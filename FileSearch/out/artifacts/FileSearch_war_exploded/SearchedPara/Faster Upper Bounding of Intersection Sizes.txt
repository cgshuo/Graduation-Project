 There is a long history of developing efficient algorithms for set intersection, which is a fundamental operation in infor-mation retrieval and databases. In this paper, we describe a new data structure, a Cardinality Filter , to quickly compute an upper bound on the size of a set intersection. Knowing an upper bound of the size can be used to accelerate many applications such as top-k query processing in text mining. Given finite sets A and B , the expected computation time for the upper bound of the size of the intersection | A  X  is O (( | A | + | B | ) /w ), where w is the machine word length. This is much faster than the current best algorithm for the exact intersection, which runs in O (( | A | + | B | ) / exp ected time. Our performance studies show that our im-plementations of Cardinality Filters are from 2 to 10 times faster than existing set intersection algorithms, and the time for a top-k query in a text mining application can be reduced by half.
 E.1 [ Data Structures ]: Arrays; H.3.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval Indexing; data structure; set intersection; text mining; data mining; top-k
Set intersection, which computes the set of the common elements in given sets, is a fundamental operation in many query processing tasks including relational query process-ing, data mining, and other search tasks. While the output of the set intersection is an enumeration of the elements in the intersection, some applications require only the size of the intersection. One example of such applications is top-k query in the context of text mining, which extracts top-k most frequent terms in documents for a given search query Fi gure 1: Top-k query retrieves the terms with the top-k largest intersections with the set of documents retrieved by a search query. ( hit documents ). To process this query, we compute, for each term, the set intersection between (1) the set of hit docu-ments and (2) the set of documents containing the term as illustrated in Figure 1, and then sort the terms with respect to their intersection sizes. An important observation in this kind of top-k query is that an enumeration of the elements of intersections is not necessary, but their sizes alone are sufficient to solve the problem.

The first contribution of our work is to show that the per-formance of top-k query can be improved by a fast algorithm for computing good upper bounds on the sizes of intersec-tions instead of the exact ones. The top-k terms retrieved by a top-k query are special in having high frequencies or strong correlations with the search query. Most of the other terms, however, are infrequent and not correlated with the search query. That means they have very small intersec-tions with a set of hit documents compared to those of the top-k terms. Our observations show that intersections for approximately 80% of the terms evaluated in the currently used top-k algorithm are less than 5% of the smallest inter-section size (threshold) of the ranked terms. Focusing on this difference between the actual intersection sizes and the threshold, we found that more than 80% of the evaluations of the intersection sizes can be skipped by replacing them with the evaluations of their upper bounds. Our approach of using upper bounds is not limited to the top-k query, but it can be applied to other data analysis algorithms such as frequent item set mining using an inverted index.
The second contribution of our work is to show how to quickly compute an upper bound of the size of the inter-section. We propose two data structures for this problem: Single Cardinality Filter (SCF) and Recursive Cardinality F ilter (RCF), both of which share some features with the current best algorithm for exact set intersection [13] (the DK algorithm). To make clear our new ideas in SCF and RCF, we should start with the DK algorithm. The DK algo-rithm constructs, for a given set A , a bit sequence to store a sketch of the set A and an auxiliary integer set to keep the original information. The term sketch used here means a data structure that compactly stores a summary of a set to quickly test the membership of an element for the set at the price of some false positive errors. The DK algorithm first computes a sketch of A  X  B using the corresponding sketches (bit sequences) for A and B to filter out unneces-sary elements and to select candidate elements of A  X  B in the auxiliary integer sets. Then it compares the candidate elements in the auxiliary integer sets to obtain the exact result. The efficiency of the DK algorithm comes from the filtering by the sketch of A  X  B , but we found that the actual computation time of the DK algorithm is dominated by the comparison step. In SCF, we also use the bit sequence to store a sketch for a given set, but unlike the DK algorithm, we use the sketches not for filtering elements but for count-ing the common elements, and use the auxiliary integer sets not for comparing the elements but for correcting the count. That makes the auxiliary integer sets for SCF far smaller than those for the DK algorithm. Our improvement is that we significantly reduced the time for comparing the elements of the auxiliary integer sets based on the fact that our pur-pose is to obtain an upper bound rather than the exact value of | A  X  B | . In our methods, the computation time for the bitwise AND operations dominates the whole computation time. The RCF is a variant of SCF that uses another SCF re-cursively to store the auxiliary integer set. We show that the expected time of the upper bound computation with RCF is O (( | A | + | B | ) /w ), where w is the machine word length, which is faster than the DK algorithm whose expected computa-tion time is O (( | A | + | B | ) / studies, SCF and RCF are shown to perform from 2 to 10 times faster than the DK algorithm.

In this paper, we use the w -bit word RAM model for the theoretical analysis of the algorithms. In this model, we have access to a random access memory consisting of w -bit words, and the basic operations (such as arithmetic oper-ations, comparison operations, and bitwise Boolean opera-tions) take constant time. In particular, we assume that the popcount operation, which counts the number of true bits in a w -bit machine word, takes constant time. This is a nat-ural assumption because the state-of-the-art CPUs support this operation with this performance. In this model, we can count the number of true bits in a bit sequence of length  X  in O (  X /w ) time. Note that, in computation models that do not support the popcount operation in constant time, the popcount operation typically takes O (log w ) time [23]. Thus the time complexity of our analysis may become worse by a factor of O (log w ) in another computation model. For ex-ample, it may take O (  X  log w/w ) time to count the number of true bits in a bit sequence of length  X  .

This paper is structured as follows: In the next section, we refer to the related work. Then we describe two new data structures in Section 3 and introduce their applications in Section 4. Section 5 describes our implementations of the applications in detail. We present our experimental results in Section 6. Finally, we conclude in Section 7.
The study of exact set intersection has a long history, and it plays an important role in databases and information retrieval [5, 8]. The linear merge algorithm is one of the simplest approaches to intersecting two sets A and B . This algorithm iterates over each element of the sorted arrays of the elements for A and B and compares them in O ( | A | + |
B | ) time. However, when A and B have greatly different sizes, | A | &lt; | B | for example, it is faster to search for each element of A in B . The binary search algorithm performs a binary search to quickly check if B contains the elements in A . The theoretical studies of acceleration by exploiting the asymmetry of set sizes go back to [16], which reduced the number of comparisons to | A | +log |
B | . Further improvements include adaptive approaches [4, 5, 12], the use of balanced trees [9, 11], and the use of multi-cores [21, 22].

The fastest practical algorithm was proposed in [13], ef-fectively using bitwise AND operations to accelerate the al-gorithm and universal hash functions to skip comparisons between unmatched pairs. It can be seen as a practical im-provement of the theoretically fastest algorithm [8], whose expected time complexity is O (( | A | + | B | ) log 2 w/w + B | ). Our algorithm with RCF, whose time complexity is O (( | A | + | B | ) /w ), runs faster than these algorithms both in theory and in practice.
There are many efficient algorithms for computing approx-imate set intersection sizes [7, 17, 19, 20]. However, since all of these algorithms are sampling-based, the obtained results can be less than the exact value. Our Cardinality Filter is designed to avoid underestimation.

A Bloom filter [10] and its variant, a counting filter [15], are data structures for a set to test whether a given value is a member of the set without false negatives. These data structures can be used to compute an upper bound of the size of the set intersection of two given sets A and B by per-forming a membership test for each element in the smaller set with the Bloom filter (or the counting filter) of the larger set and count the true cases. Since the membership test of the Bloom filter has only false positives and no false nega-tives, this approach outputs an upper bound of | A  X  B | in O (min {| A | , | B |} ) time. However, in Section 6, we will show that this is faster than our method only when A and B are strongly asymmetric and that it performs quite poorly in the other cases.

A conjunctive filter [18] is a space-efficient data structure for approximate set intersection, and it can compute a su-perset of the intersection set. Since the size of the superset of the intersection set is at least the intersection size, we can use the conjunctive filter to calculate the upper bound. However, its computation is not fast because the design of the conjunctive filter puts more emphasis on space-efficiency rather than time-efficiency.
The target application of our new data structures pre-sented in this paper is the top-k query for text mining, which retrieves the top-k most frequent terms in the documents for a given search query. This is a kind of top-k problem Fi gure 2: An example of SCFs with N = 3 for two [3, 6, 14], which is generally the retrieval of the k highest scoring entries such as the documents having top-k highest relevance scores for a given query. The top-k query corre-sponds to the case where the entries are the terms and the score of each entry is the intersection size (see Figure 1). The general top-k studies that do not specify the scoring function are orthogonal to our approach since our technique improves the comparison of the scores for the specific case of intersection.

Simitsis et al. [20] studied the top-k query and presented techniques based on pruning and approximate intersection. In this paper, the pruning algorithm is the baseline for eval-uating the improvement by using our data structures. We did not consider using their approximate intersection tech-nique since it may return incorrect top-k terms. The scope of our research covers only the exact top-k .
In this section, we propose a new data structure, Cardi-nality Filter (CF), for computing an upper bound on the intersection size of two given sets. First, we give a simple implementation of CF, the Single Cardinality Filter (SCF), in Section 3.1 and then a more efficient variant of SCF, the Recursive Cardinality Filter (RCF), in Section 3.2. Finally we formulate CF as a general data structure in Section 3.3.
Throughout this paper, X denotes a finite set, and the two sets A and B are subsets of X . Our CF works for any set X , but we assume here X is an integer set X = { 0 , 1 , . . . , | X | X  1 } for simplicity. In this section, we describe a new data structure, a Single Cardinality Filter , for computing an upper bound of | A  X  for two given sets A and B . This data structure is similar to that used by the current fastest algorithm [13] for exact set intersection, in that it uses a bit array. The SCF is de-signed so that the computation time is significantly reduced because our aim is not to compute the exact intersection A  X  B but to compute an upper bound of | A  X  B | .
 Let us show how to construct the SCF for a given set A . Let h be a universal hash function that maps an el-ement from X to { 0 , 1 , . . . ,  X  X  X | /N  X   X  1 } , where N is an integer parameter for SCF. The SCF consists of a pair of elements in A and c ( A ) is the set of integers that stores all of the elements in A except those that are the smallest in each collision for h . Figure 2 shows an example of the SCFs for sets A and B , where we use the same parameter N = 3 for both of the sets. In this example, the elements 7, 8, 10, 12, and 14 in A are mapped to 2, 0, 2, 4, and 4, respectively, by the hash function h . Hence the h ( A ) is equal to { 0 , 2 , 4 there are three collisions for h : { 8 } , { 7 , 10 } , and Here, we define the collision as the set of elements that are mapped to the same value by h . That means we refer to a set of a single element as a collision. Hence the set c ( A ) is equal to { 10 , 14 } because the other elements in A are the smallest in their collisions. In the SCF, we store h ( A ) as a bit sequence of length  X  X  X | /N  X  and c ( A ) as an integer array. Intuitively, we can see that the set h ( A ) stores a sketch of set A and c ( A ) stores auxiliary information for set A .
Using the SCFs for the two sets A and B with the same hash function h and the integer parameter N , we can effi-ciently compute an upper bound of | A  X  B | . As we will show in a rigorous proof later, | h ( A )  X  h ( B ) | + | c ( A ) upper bound of the intersection size. (Recall the example in Figure 2, where | h ( A )  X  h ( B ) | = 3, | c ( A )  X  c ( B ) |
A  X  B | = 3.) Intuitively, this is because an element in A  X  is counted in one of two ways: When different elements in A and B have the same hash value, the intersection size may be overcounted (8  X  A and 5  X  B in Figure 2). Another case of overcounting is the case of 14 where one element is counted twice by both | h ( A ) h ( B ) | and | c ( A )  X  c ( B ) | . We can obtain the upper bound quickly by using the fast bitwise AND operations to compute h ( A )  X  h ( B ) and the linear merge algorithm to compute far smaller than | A  X  B | ).

Now we give a rigorous definition of the SCF. Given a pos-itive integer N ( the compression ratio parameter ) and a uni-versal hash function h that maps from X to H = { 0 , 1 , . . . ,  X  X  X | /N  X  X  X  1 } , the SCF  X  is defined as a map that maps A  X  X to  X ( A ) = ( h ( A ) , c ( A ))  X  2 H  X  2 X where We also respectively define the binary operation  X  between  X ( A ) and  X ( B ) and the size function | X | by Using these definitions, we have Theorem 3.1.
 Theorem 3.1. For any A, B  X  X , | A  X  B |  X  |  X ( A )  X   X ( B ) | .

Proof. Dividing A  X  B into two disjoint sets, we have |
A  X  B | = | A  X  B  X  c ( A )  X  c ( B ) | + | ( A  X  B ) \ ( c ( A ) and we have |  X ( A )  X   X ( B ) | = | c ( A )  X  c ( B ) | + by definition. Since | A  X  B  X  c ( A )  X  c ( B ) |  X  | c ( A ) it is sufficient to prove that | ( A  X  B ) \ ( c ( A )  X  c ( B )) | h ( A )  X  h ( B ) | .

We can prove this by showing that, for all x and y in ( A  X  B ) \ ( c ( A )  X  c ( B )) such that x  X  = y , we have In other words, we must show the restriction of h that maps ( A  X  B ) \ ( c ( A )  X  c ( B )) to h ( A )  X  h ( B ) is injective.
First, we verify the image of the restriction of h is con-tained in h ( A )  X  h ( B ). In other words, Without loss of generality, we can assume x &lt; y . Since y /  X  c ( A )  X  c ( B ), we know that y /  X  c ( A ) or y /  X  y /  X  c ( A ), then, by the definition of c , there is no z  X  that Hence, by the fact that x  X  A and the assumption that x &lt; y , we know that h ( x )  X  = h ( y ) holds. Using a similar injectivity holds.
 No w we will analyze the space and time complexities of SCF. We first present Lemma 3.1.
 Lemma 3.1. The expected size of c ( A ) is at most Pr oof. Let  X  A be the bit array that corresponds to h ( A ). The length of  X  A is | H | =  X  X  X | /N  X  . Since the probability that the hash function h maps an element in A to an arbi-trary bit in  X  A is 1 / | H | , the probability that this bit in  X  true bits in  X  A is | H | (1  X  (1  X  1 / | H | ) j A j ).
The expected size of c ( A ) is |
A | X  X  h ( A ) | = | A | X  X  H | wh ere the first inequality follows from the inequality (1  X  ) n  X  1  X  n X  + n 2  X  2 / 2 for n  X  1 and 0  X   X   X  1.
F rom Lemma 3.1 and the fact that the length of bit vector  X  A is | H | =  X  X  X | /N  X  , Theorem 3.2 follows directly. Theorem 3.2. The expected space usage of SCF for a set A is at most  X  X  X | /N  X  + wN | A | 2 / 2 | X | bits. There is a second theorem that follows from Lemma 3.1:
Theorem 3.3. The expected time complexity of SCF for computing an upper bound of | A  X  B | is O ( | X | /N w + N ( |
B | 2 ) / | X | ) . Fi gure 3: An illustration of a 3-layer RCF for set A .
Proof. The computation time required for bitwise AND operations between the bit sequences  X  A and  X  B is O (( | X | /N )  X  (1 /w )). The computation time for the linear merge algorithm to compute | c ( A )  X  c ( B ) | is O ( | | c ( B ) | ) = O ( N ( | A | 2 + | B | 2 ) / | X | ) by Lemma 3.1.
F inally we evaluate the accuracy of the SCF. Let us define the approximation ratio of SCF  X  for two sets A and B such that A  X  B  X  =  X  as |  X ( A )  X   X ( B ) | / | A  X  B | . This number is at least 1 by definition and indicates how close the upper bound is to the exact size. Although we omit the proof due to the space limitation, we present Theorem 3.4.

Theorem 3.4. The expected approximation ratio of SCF is at most
Here we give a variant of the SCF, the Recursive Cardi-nality Filter  X  (RCF), which minimizes the use of the slow linear merge algorithm for computing | c ( A )  X  c ( B ) | the same notation  X  for an RCF as used for an SCF, but in this section,  X  always denotes an RCF.

Here is how to construct an RCF for a set A . We first construct an SCF ( h 1 ( A ) , c 1 ( A )) for the set A as the first layer of the RCF. Here h i is a hash function h i : X  X  { 0 , . . . ,  X  X  X | /N i  X  X  X  1 } , c i is the collision set of h a compression ratio parameter for the i -th layer. Then we as the second layer of the RCF, and replace the set c 1 ( A ) in the first layer data structure with this new SCF. We re-peat this process until we obtain the l -th layer, where l is a parameter. Figure 3 illustrates an RCF with three layers. The resulting l -layer RCF can be represented as As with an SCF, we define the binary operation and the size function for an RCF as  X ( A )  X   X ( B ) = ( H 1 ( A )  X  H 1 ( B ) , H 2 ( A )  X  Th e upper bound |  X ( A )  X   X ( B ) | is represented as and Theorem 3.5 can be easily proved by induction on l . Theorem 3.5. | A  X  B | X |  X ( A )  X   X ( B ) | The best choice of N i depends on the application, and in Section 5.2, we show how to set N (or N i ) for a specific application. Using an appropriate parameter setting, the time complexity for computing an upper bound of | A  X  B | with an RCF can be improved over that with an SCF.
Theorem 3.6. Let l be  X  log max {| A | , | B |} X  , r be a con-stant larger than 1, and  X  be an l -layer RCF constructed with parameters N i =  X  2 i 1 | X | /r max {| A | , | B |} X  1 , 2 , . . . , l . The time complexity of computing an upper bound of | A  X  B | for sets A and B with this l -layer RCFs is O (( | A | + | B | ) /w ) .

Proof. By Lemma 3.1, the expected size of C 1 ( A ) is at most N 1 | A | 2 / 2 | X | . Since N 1 =  X  X  X | /r max {| expected size of C 1 ( A ) is at most | A | / 2 r . Similarly, using Lemma 3.1 for C 1 ( A ) and the definition N 2 =  X  2 | X | /r max {| A | , | B |} X  , we get the expected size of C ( A ) is at most | A | / 2 2 r . Repeating this for l layers, we get the expected size of C l ( A ) is at most | A | / 2 l The length of the bit sequence for H i ( A ) is  X  X  X | /N  X  puting an upper bound of | A  X  B | with RCF is at most
The time complexity for RCF is better than that for the current fastest exact set intersection algorithm [13], whose time complexity is O (( | A | + | B | ) /
We have shown two data structures, SCF and RCF, and they are similar in the sense that they are designed to quickly compute an upper bound of | A  X  B | for two given sets A and B . We define a Cardinality Filter (CF) as in Definition 3.1, so both the SCF and RCF can be seen as implementations of a CF. Figure 4 shows the correspondence between the sets and the CFs.

Definition 3.1. Let X be a set and  X  be a set with op-erations  X  :  X   X   X   X   X  and | X | :  X   X  R +  X  X  0 } = { x  X  R | x  X  0 } . A map  X  : 2 X  X   X  is said to be a Cardinality Filter if it satis es for  X  A, B  X  X ,
In this section, we present some applications of CFs. First we describe the general usage of a CF in Section 4.1. Then we give some examples of the top-k query for text mining in Section 4.2 and the frequent item set mining with an inverted index in Section 4.3.
 Fi gure 4: The operation  X ( A )  X   X ( B ) for Cardinality Filters  X ( A ) and  X ( B ) is analogous to the set inter-section A  X  B for two sets A and B .
We can use CFs to accelerate the evaluation of the in-equality of intersection sizes | A  X  B | &gt; c . Note that the condition for the CFs is a necessary condition of the condi-tion for the original sets: | A  X  B | &gt; c  X  X   X ( A )  X   X ( B ) That means these two conditions are logically equivalent: 1. | A  X  B | &gt; c 2. |  X ( A )  X   X ( B ) | &gt; c  X  | A  X  B | &gt; c . Our approach to the acceleration is to substitute the in-equality for the intersection size | A  X  B | &gt; c in the target the logical equivalence of the two conditions, this does not change the operation of the target algorithms. In the situa-tion where |  X ( A )  X   X ( B ) |  X  c holds for the majority of the inputs, we can skip the computation of | A  X  B | . Therefore, if the upper bound can be calculated much faster than the exact value, we can save time.
First, we show how to apply CF to the top-k query de-scribed in Section 1, which is the retrieval of the top-k most frequent terms in documents that are dynamically re-turned by a search query. Algorithm 1 is known as an ef-ficient algorithm for top-k query [20] and is widely used. In this algorithm, we assume that each term is associated with a posting list, which is a list of IDs of the documents containing a specific term. Algorithm 1 loads the posting scending order of frequency ( | P [0] |  X  | P [1] |  X   X  X  X  calculates the intersection size between the set of document IDs for the search query S and the posting list P [ i ]. Pairs the temporary queue Q that stores the pairs with the top-k intersection sizes (Lines 7 and 9). The queue is updated only when it is not full (Line 6) or the intersection size | S is larger than the minimum intersection size in the queue |
S  X  P [min( Q )] | , where min( Q ) denotes the term with the smallest intersection size in the queue (Line 8). This algo-rithm can terminate its iterations, when both of these con-Fi gure 5: The ratio of the intersection size to the threshold. ditions are met: (1) the queue is full, and (2) the document frequency of the current term is not more than the minimum intersection size in the queue (Line 3).
 A lgorithm 1 The top-k query Inp ut: 1: Q  X  X  X  // a set of the temporary top-k terms 2: for i = 0 , 1 , . . . , | P | X  1 do 3: if | Q | = k and | P [ i ] | X | S  X  P [min( Q )] | then 4: break // early out 5: end if 6: if | Q | &lt; k then 7: insert ( i, | S  X  P [ i ] | ) into Q 8: else if | S  X  P [ i ] | &gt; | S  X  P [min( Q )] | then 9: insert ( i, | S  X  P [ i ] | ) into Q 10: remove the pair (min( Q ) , | S  X  P [min( Q )] | ) from Q 11: end if 12: end for 13: return Q
The most time consuming part of this algorithm is the evaluation of the intersection size | S  X  P [ i ] | &gt; | in Line 8. Note that the elements of the intersection S  X  are not referred to here, and that only the size of the in-tersection is needed. One important observation is that the intersection size | S  X  P [ i ] | is typically much smaller than the threshold | S  X  P [min( Q )] | for most of the posting lists. An example appears in Figure 5, which shows the distribu-tion of the ratios of the intersection size to the threshold |
S obtained by testing 309 396 inequalities with 50 top-k term queries ( k = 100) with | S | ranging from 100 to 100 000 on the 528 545 documents of the NHTSA data [1]. The vertical axis shows the proportion (as a percentage) of the values of |
S  X  P [ i ] | / | S  X  P [min( Q )] | that fall into each range on the horizontal axis. The range  X 1.00- X  corresponds to the cases for which the inequalities were evaluated as true. The inter-section size is at most 20 times as small as the threshold in approximately 80% of the inequalities.

Considering the gaps in the inequalities of the threshold evaluations, we can speed up Algorithm 1 by replacing the threshold condition | S  X  P [ i ] | &gt; | S  X  P [min( Q )] with As described in Section 4.1, this replacement does not change the results of the algorithm. The CFs for the posting lists phase, and the CF for the hit documents  X ( S ) is created at the beginning of the query processing. The implementation details and the performance improvements will be discussed in Sections 5 and 6, respectively.
Another target application of CF is frequent item set min-ing. Frequent item set mining is a task to retrieve frequent subsets of items from a transaction database where each transaction is represented as a set of items. An example application is to find frequent combinations of items in pur-chase records. Given T , a transaction database, T [ t ], the items, and s m , an integer parameter for the minimum size (the minimum support) of the item subsets, our task is to generate where s ( C ) is the support of C defined by We say C has the minimum support if and only if s ( C )  X 
The Apriori algorithm [2] is widely used as a solution for frequent item set mining. Algorithm 2 shows the pseudocode of the Apriori algorithm using an inverted index. It extracts the frequent item sets by iteration for the size of item set n (Line 2). In each iteration, it creates a list of sets L n stores the sets of n items having the minimum support, from the lists L 1 and L n 1 . It scans each of the candidate sets of items C  X  X  i } , which consists of a set in L n 1 and a set in L 1 (Line 7). Then it checks if the candidate sets have the minimum support: where it computes the intersection P [ C ]  X  P [ { i } ] (Line 8). The candidate sets having the minimum support are added to L n (Line 9). Finally it generates all of the item sets in L 1 , L 2 , . . . (Line 13). This algorithm is slightly different from the original Apriori algorithm, which calculate s ( C { i } ) by scanning all of the transactions. The difference is that Algorithm 2 uses the inverted index for the items. The inverted index has an advantage when L 1 is small, since the time for the transaction scan is independent of | L 1 | .
We can apply CF to Algorithm 2 by replacing the inequal-ity in Line 8 with |
 X ( P [ C ])  X   X ( P [ { i } ]) | X  s m  X  | P [ C ]  X  P [ { i W e can skip most of the intersections if their sizes are much smaller than the minimum support s m .
 A lgorithm 2 The Apriori algorithm on an inverted index Inp ut: 1: L 1  X  X { i }| i  X  I, s ( { i } )  X  s m } 2: for n = 2 , 3 , . . . do 3: if L n 1 =  X  then 4: break 5: end if 6: L n  X  X  X  7: for each C  X  L n 1 and each { i } X  L 1 such that i /  X  8: if | P [ C ]  X  P [ { i } ] | X  s m then 9: add C  X  X  i } to L n 10: end if 11: end for 12: end for 13: return
In this section, we explain the details of our implementa-tion of top-k query processing using a CF.
First we explain the structure of the data created in the index and in memory. We use the SCF or RCF described in Section 3 for the implementation of the CF. As illustrated in Figure 6, we store in the index the posting list for each term and a term CF for each posting list. The posting lists, which are shown as lists of boxes, are stored in the form of a compressed integer array and physically sorted in descend-ing order of size, which is the number of document IDs for each term. The term CFs, which are shown as the boxes be-low  X  X erm CFs X , are also sorted in the same order, but the posting lists and the term CFs are stored in physically sepa-rate locations so that the index reader can skip reading the unnecessary parts of the posting lists. In the online phase, given the document ID list retrieved by a search query, a query CF is created for the document ID list. The docu-ment ID list and the query CF are shown on the left side of Figure 6 in the same form as for the posting lists and the term CFs. The query CF is created in memory, and the upper bound operation is performed on the query CF and each term CF.

One important configuration setting is the compression ratio parameter N . A CF works efficiently when N is set to an appropriate value for its set size, but at the same time, the upper bound can be calculated only for a pair of CFs having same N . In our implementation, we define several candidate values of N (e.g. N = 1 , 2 , 5), and each term CF is created with the most appropriate value in the can-didates. In Figure 6, the configurations of N are indicated in the boxes for the term CFs. For the query CF, given the document ID list for a search query, candidate CFs are created for each candidate value of N . In Figure 6, three Fi gure 7: The ratio of the intersection size to the threshold is approximated by | P [ n ] | / | P [ k  X  1] | candidate CFs are created with N = 1 , 2 , 5 to be used as a query CF. Then the upper bound is computed between each term CF and one in the candidate CFs with the same value of N as the term CF.

Another feature of the configuration is that we do not create term CFs for very short posting lists because such CFs are not space-efficient. For such posting lists, we use the linear merge algorithm to compute the size of the set intersection.
The best configuration of the compression ratio parame-ter N depends on its application, but the example of the configuration for the top-k query gives us clues for general applications. Here, we show how to configure N for the n + 1-th posting list P [ n ] using the notation of Section 4.2. Figure 7 illustrates the term posting lists in the form of a list of boxes and the correspondence between the estimated values and the related posting lists. First | S  X  P [min( Q )] which is the threshold of the intersection size for a term to be put in the temporary queue Q , is at least | S  X  P [ k since the first k terms are put into Q in every case, and Q is updated only when some P [ i ]( i  X  k ) satisfies | S  X  |
S expected value of | S  X  P [ k  X  1] | under the assumption of no correlation between S and P [ k  X  1], is a good approximation for | S  X  P [min( Q )] | . Although | S  X  P [ k  X  1] | may become larger or smaller when S and P [ k  X  1] have a positive or negative correlation, | S  X  P [ k  X  2] | or | S  X  P [ k ] threshold in either case, and these values should be close to |
S || P [ k  X  1] | / | X | . Similarly, we can estimate | S  X  |
S || P [ n ] | / | X | for n  X  k . Thus, we set N to an integer close
Na me | A | | B | Cr (A ) Large / no correlation 1M 1 M 1. 0 (B) Middle / no correlation 1 00K 10 0K 1. 0 (C) Small / no correlation 1 0K 10 K 1. 0 (D) Asymmetric / no correlation 1M 10 K 1. 0 (E) Middle / positive correlation 1 00K 10 0K 10 .0 (F) Middle / negative correlation 1 00K 10 0K 0. 1 Table 1: Input data with varying the data size (A), (B), and (C), the asymmetry (A) and (D), and the correlation (E), (B), and (F). factor. In other words, for this application, we can set N for a set | A | to  X  X  X | /r | A | X  where r is a constant larger than 1 since | P [ k  X  1] | is linear with the number of documents. Using this setting of N , the expected space usage of the SCF for a set A is O ( | A | (1+ w )) bits (by Theorem 3.2). The expected time complexity for the upper bound of | A  X  B | is O ( | A | + | B | ) when N = | X | /r max {| A | , | B rem 3.3).
In this section, we present our speed and size evaluations of CFs compared with known exact set intersection algo-rithms and the Bloom filter. We compared the computation times for the sizes of two-set intersections or their upper bounds for seven algorithms: LM the linear merge algorithm described in Section 2.1, BS the binary search algorithm described in Section 2.1, DK1H the most practical of the algorithms proposed in DK2H the two-hash version of DK1H, BF (upper bound) the algorithm using the Bloom filter SCF (upper bound) the CF using a single bit array as RCF (upper bound) the recursive CF using the bit ar-All of the algorithms were implemented using C++ and eval-uated on a quad core, 64-bit, 2.5-GHz CPU with 4.0 GB of memory. The intersections were calculated in memory for the test data of set pairs which were preloaded into memory before the tests were started. We generated 100 set pairs for each test case and measured the average time for 100 itera-tions, totalling 10 000 operations. The approximation ratio was also measured to see the relation between the speed and the accuracy.

We used the synthetic test data for set pairs described in Table 1, varying the set sizes, the asymmetry, and the correlation. All of the input sets A and B are subsets in the universe set of 10M elements: | X | = 10 7 . The column  X  X r X  is for the correlation between A and B , which we define by | negative correlation). The data sets (A), (B), and (C) differ Fi gure 9: The time breakdown shows that CFs effi-ciently use bitwise AND operation. in the size of the sets, but the two input sets of each data set are the same size (the symmetric case). The data sets (A) and (D) are intended to compare the degree of asymmetry for the size differences between the two input sets. The size of the first input set, | A | , is set to 1M in both cases, but the size of the other set, | B | , differs significantly. The data sets (E), (B), and (F) are used to measure the influence of the correlations between sets. All of these tests have the input sets of the same size, but the intersection size varies from the minimum size (100 elements) to the maximum size (10K elements). We do not consider any cases of biased data such as an integer set that includes dense small numbers and sparse large numbers. Since the tested algorithms except LM and BS are mainly based on hashed values, they do not much depend on the distribution of the elements in each set. Therefore, we used synthetic data in which the elements of each set were uniformly distributed in the universe set. For the compression ratio N , we adopted the policy of  X  X low but accurate X  in this experiment: N =  X  Alt hough we can set N to  X  X  X | /r | A | X  ( r is constant) for the top-k query based on the estimate in Section 5.2, we assume a broader application here. The RCF is a 2-layer RCF, and N 2 for the second layer is set to 2 N 1 .

Figure 8 shows the computation time and the approxima-tion ratio for each algorithm and each set of data. Each of the six graphs represents the performance on the data set indicated above the graph. The axis for times runs upwards on the left side of each graph, and all of the times are shown in microseconds in all of the graphs. The axis for approxima-tion ratio runs upwards on the right side of each graph. The approximation ratio for the exact algorithms are 1 for all of the data sets. The labels for the series and for the vertical axes are omitted in the graphs for the data sets (B), (C), (D), (E), and (F). As a result, our methods outperformed the other methods by a factor of from 2 to 10 in all of the cases except the asymmetric case, (D), where the Bloom fil-ter has the advantage. Clear differences between SCF and RCF were also observed with the large input sets, (A) and (D). For DK1H and DK2H, we were unable to tune them to run as fast as those in the original paper, which is approx-imately twice as fast as the linear merge. Even considering this, our proposed methods SCF and RCF still perform the best in all of the cases.

The advantage of SCF and RCF comes from the exploita-tion of the bitwise AND operations. Figure 9 shows the time breakdown for the data set (B). While the bitwise AND op-eration takes more time in SCF and RCF than in the DK algorithms, the time for the other operations in SCF and RCF is extremely small compared to the DK algorithms. Although both DK algorithms and CFs use bitwise AND operations to compute the necessary condition for some of the elements to be contained in the intersection, DK algo-rithms need to check for each element if it really is a member of the intersection, due to the requirement of exactness. This checking step consumes at least O ( | A  X  B | ) time. In contrast, the CFs check the membership of the intersection only for the elements that are potentially uncounted. This difference is the essential benefit of computing the upper bound.
In this section, we show the performance improvements for the top-k queries. The data used in the experiments was the text data of NHTSA [1], which includes defect and maintenance information for motor vehicles. The total data size was 150 MB and the number of documents was 528 545. We tokenized the text of each document using whitespace as a delimiter and used each token as a term. We varied the number of documents retrieved by each search query ( | S | in Section 4.2) from 100 to 100 000. We used five search terms for each size and measured the average time for 50 queries (10 queries for each term). All the algorithms were implemented using Java and evaluated on a quad core, 64-bit, 2.5-GHz CPU with 4.0 GB of memory.
 We compared the performance of the top-k query using Algorithm 1 with LM, LM + BS (switching LM and BS at |
A | / | B | = 1 / 3 , 3), and DK1H as described in Section 6.1 and the improved version of Algorithm 1 with SCF and RCF as described in Sections 4.2 and 5. We set N for SCF and ( N 1 , N 2 ) for RCF as in Table 2 using the estimate we intro-duced in Section 5.2. Figure 10 shows the query processing time for each method. The value of k was set to 100, since this is a frequently used value in text mining. The numbers of documents retrieved by the search queries are displayed on the horizontal axis. If this value is large, each inter-section consumes a large amount of time, but only a small number of posting lists are loaded. If this value is small, the computation time for the intersections is small, but many posting lists are read. We can see SCF and RCF performed approximately twice as fast as the other methods.
Figure 11 shows, for each size of the hit document set for the search query, the ratio of the exact intersections skipped by SCF or RCF to the number of evaluated posting lists, except those for the terms that actually rank in the top-k result (skip ratio). We can see that more than 80% of the threshold conditions of intersection were eliminated by the CFs before loading the posting lists of the terms.
In this paper, we studied the speed of algorithms to com-pute the upper bounds of the intersection sizes, which has not previously been a focus in the research for the intersec-tion computation. We introduced new data structures, the Single Cardinality Filter and the Recursive Cardinality Fil-ter, to quickly upper bound the size, and we showed they perform much faster than the exact set intersection algo-rithms and can accelerate the text mining query processing.
Computation time
Fig ure 10: Performance of top-k queries (k=100). Fig ure 11: The ratio of skipped exact intersections. Since a comparison between intersection sizes is a common operation in mining and discovery tasks, CFs are expected to be applied to many analytical queries.

CF presents at least three exciting challenges. First, it would be interesting to study the optional algebraic prop-erties of SCF and RCF that we did not use in this paper. In addition to the property | A  X  B |  X  |  X ( A )  X   X ( B ) and RCF have other algebraic properties such as monotonic-ity A  X  B  X  |  X ( A ) |  X  |  X ( B ) | and follow the commuta-tive and associative laws for  X  , which are important when intersecting three or more sets. Second, although we de-fined intersection-like operations for CF, union-like opera-tions and exclusions were not studied. A suite of set opera-tions might expand the applications of CF to databases and search engines. Third, a non-parametric implementation of CF is also challenging, since both SCF and RCF have the compression ratio parameter N , which restricts the combi-nations of sets. To conclude, CF was shown to be capable of speeding up some known algorithms, but still has areas for further algebraic and algorithmic research. [1] National Highway Traffic Safety Administration. [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] R. Akbarinia, E. Pacitti, and P. Valduriez. Best [4] R. A. Baeza-Yates. A fast set intersection algorithm [5] J. Barbay, A. L  X opez-ortiz, and T. Lu. Faster adaptive [6] H. Bast, D. Majumdar, R. Schenkel, M. Theobald, and [7] K. Beyer, R. Gemulla, P. J. Haas, B. Reinwald, and [8] P. Bille, A. Pagh, and R. Pagh. Fast evaluation of [9] G. E. Blelloch and M. Reid-miller. Fast set operations [10] B. H. Bloom. Space/time trade-offs in hash coding [11] M. R. Brown and R. E. Tarjan. A fast merging [12] E. D. Demaine, A. L  X opez-Ortiz, and J. I. Munro. [13] B. Ding and A. C. K  X  onig. Fast set intersection in [14] R. Fagin, A. Lotem, and M. Naor. Optimal [15] L. Fan, P. Cao, J. Almeida, and A. Z. Broder. [16] F. K. Hwang and S. Lin. A simple algorithm for [17] R. Krauthgamer, A. Mehta, V. Raman, and A. Rudra. [18] D. Okanohara and Y. Yoshida. Conjunctive filter: [19] O. Papapetrou, W. Siberski, and W. Nejdl.
 [20] A. Simitsis, A. Baid, Y. Sismanis, and B. Reinwald. [21] S. Tatikonda, F. Junqueira, B. B. Cambazoglu, and [22] D. Tsirogiannis, S. Guha, and N. Koudas. Improving [23] H. S. Warren. Hacker's Delight . Addison-Wesley
