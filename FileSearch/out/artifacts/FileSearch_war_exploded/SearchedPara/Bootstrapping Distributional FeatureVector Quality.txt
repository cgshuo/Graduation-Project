 Bar-Ilan University Bar-Ilan University weighting in distributional word similarity. The method was motivated by attempts to utilize
Our analysis revealed that a major reason for the rather loose semantic similarity obtained by deficient feature weighting. This observation led to the definition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors. The under-lying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted. This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space. The superior performance of the bootstrapping method was assessed in two different experiments, one based on direct human gold-standard annotation and the other based on an automatically created disambiguation dataset. These results are further supported by applying a novel quanti-tative measurement of the quality of feature weighting functions. Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied. 1. Introduction 1.1 Motivation
Distributional word similarity has long been an active research area (Hindle 1990; Ruge 1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and
Weir 2005). This paradigm is inspired by Harris X  X  distributional hypothesis (Harris 1968), which states that semantically similar words tend to appear in similar contexts.
In a computational realization, each word is characterized by a weighted feature vector, where features typically correspond to other words that co-occur with the characterized word in the context. Distributional similarity measures quantify the degree of similarity between a pair of such feature vectors. It is then assumed that two words that occur within similar contexts, as measured by similarity of their context vectors, are indeed semantically similar.
 inferences. The first type is making similarity-based generalizations for smoothing word co-occurrence probabilities, in applications such as language modeling and dis-ambiguation. For example, assume that we need to estimate the likelihood of the verb X  object co-occurrence pair visit X  X ountry , although it did not appear in our sample corpus.
Co-occurrences of the verb visit with words that are distributionally similar to country , such as state , city ,and region , however, do appear in the corpus. Consequently, we may infer that visit X  X ountry is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan,
Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts.
 is meaning-preserving lexical substitution. Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) sum-marization, need to recognize that one word can be substituted by another one in a given context while preserving, or entailing the original meaning. Naturally, recogniz-ing such substitutable lexical entailments is a prominent component within the textual entailment recognition paradigm, which models semantic inference as an application-independent task (Dagan, Glickman, and Magnini 2006). Accordingly, several textual entailment systems did utilize the output of distributional similarity measures to model entailing lexical substitutions (Jijkoun and de Rijke 2005; Adams 2006; Ferrandez et al. 2006; Nicholson, Stokes, and Baldwin 2006; Vanderwende, Menezes, and Snow 2006).
In some of these papers the distributional information typically complements man-ual lexical resources in textual entailment systems, most notably WordNet (Fellbaum 1998).
 the meaning of the other. For instance, in question answering, the word company in a question can be substituted in an answer text by firm , automaker ,or subsidiary , whose meanings entail the meaning of company . However, as it turns out, traditional distributional similarity measures do not capture well such lexical substitution relationships, but rather capture a somewhat broader (and looser) notion of semantic similarity. For example, quite distant co-hyponyms such as party and company also come out as distributionally similar to country , due to a partial overlap of their semantic properties. Clearly, the meanings of these words do not entail each other.
 distributional similarity scheme may be improved to yield tighter semantic similarities, and eventually better approximation of lexical entailments. This article presents one component of this research plan, which focuses on improving the underlying semantic 436 quality of distributional word feature vectors. The article describes the methodology, definitions, and analysis of our investigation and the resulting bootstrapping scheme for feature weighting which yielded improved empirical performance. 1.2 Main Contributions and Outline
As a starting point for our investigation, an operational definition was needed for evaluating the correctness of candidate pairs of similar words. Following the lexical substitution motivation, in Section 3 we formulate the substitutable lexical entailment relation (or lexical entailment , for brevity), refining earlier definitions in Geffet and
Dagan (2004, 2005). Generally speaking, this relation holds for a pair of words if a possible meaning of one word entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. Lexical entailment overlaps partly with traditional lexical semantic relationships, while capturing more generally the lexical substitution needs of applications. Empirically, high inter-annotator agreement was obtained when judging the output of distributional similarity measures for lexical entailment.
 to the lexical entailment criterion. Choosing the commonly used measure of Lin (1998) as a representative case, the analysis shows that quite noisy feature vectors are a major cause for generating rather  X  X oose X  semantic similarities. On the other hand, one may expect that features which seem to be most characteristic for a word X  X  meaning should receive the highest feature weights. This does not seem to be the case, however, for common feature weighting functions, such as Point-wise Mutual Information (Church and Patrick 1990; Hindle 1990).
 the original feature weights (Section 4), leading to better feature vectors and better similarity predictions. The general idea is to promote the weights of features that are common for semantically similar words, since these features are likely to be most char-acteristic for the word X  X  meaning. This idea is implemented by a bootstrapping scheme, where the initial (and cruder) similarity measure provides an initial approximation for semantic word similarity. The bootstrapping method yields a high concentration of semantically characteristic features among the top-ranked features of the vector, which also allows aggressive feature reduction.
 respond to the two types of applications for distributional similarity. First, it achieved significant improvements in predicting lexical entailment as assessed by human judg-ments, when applied over several base similarity measures (Section 5). Additional analysis relative to the lexical entailment dataset revealed cleaner and more charac-teristic feature vectors for the bootstrapping method. To obtain a quantitative analysis of this behavior, we defined a measure called average common-feature rank ratio .
This measure captures the idea that a prominent feature for a word is expected to be prominent also for semantically similar words, while being less prominent for unrelated words. To the best of our knowledge this is the first proposed measure for direct analysis of the quality of feature weighting functions, without the need to employ them within some vector similarity measure.
 prediction of co-occurrence likelihood within a typical pseudo-word sense disambigua-tion experiment, obtaining substantial error reductions (Section 7). Section 8 concludes this article, suggesting the relevance of our analysis and bootstrapping scheme for the general use of distributional feature vectors. 1 2. Background: Distributional Similarity Models
This section reviews the components of the distributional similarity approach and specifies the measures and functions that were utilized by our work.
 similar contexts, suggesting that semantic similarity can be detected by comparing contexts of words. This is the underlying principle of the vector-based distributional similarity model, which comprises two phases. First, context features for each word are constructed and assigned weights; then, the weighted feature vectors of pairs of words are compared by a vector similarity measure. The following two subsections review typical methods for each phase. 2.1 Features and Weighting Functions In the typical computational setting, word contexts are represented by feature vectors.
A feature represents another word (or term) w with which w co-occurs, and possibly specifies also the syntactic relationship between the two words, as in Grefenstette (1994),
Lin (1998), and Weeds and Weir (2005). Thus, a word (or term) w is represented by a feature vector, where each entry in the vector corresponds to a feature f .Padoand
Lapata (2007) demonstrate that using syntactic dependency-based features helps to distinguish among classes of lexical relations, which seems to be more difficult when using  X  X ag of words X  features that are based on co-occurrence in a text window. where fw is a context word (or term) that co-occurs with w under the syntactic depen-fw in the syntactic dependency, being either the head (denoted h ) or the modifier (de-noted m ) of the relation. For example, given the word company ,thefeature earnings, gen, h corresponds to the genitive relationship company X  X  earnings ,and investor, pcomp of, m corresponds to the prepositional complement relationship the company of the investor . Throughout this article we use syntactic dependency relationships generated by the
Minipar dependency parser (Lin 1993). Table 1 lists common Minipar dependency relations involving nouns. Minipar also identifies multi-word expressions, which is 438 advantageous for detecting distributional similarity for such terms. For example,
Curran (2004) reports that multi-word expressions make up between 14 X 25% of the synonyms in a gold-standard thesaurus.
 relationship instances of the form w,f , where each pair corresponds to a single co-occurrence of w and f in the corpus. f is termed as a feature of w . Then, a word w is represented by a feature vector, where each entry in the vector corresponds to one feature f . The value of the entry is determined by a feature weighting function weight ( w , f ), which quantifies the degree of statistical association between w and f in the set S . For example, some feature weighting functions are based on the logarithm of the word X  X eature co-occurrence frequency (Ruge 1992), or on the conditional probability of the feature given the word (Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999; Lee 1999).
 Information ( MI ) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch,
Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004;
Weeds, Weir, and McCarthy 2004), defined by:
We calculate the MI weights by the following statistics in the space of co-occurrence instances S : where count ( w , f ) is the frequency of the co-occurrence pair w,f in S , count ( w )and count ( f ) are the independent frequencies of w and f in S ,and nrels is the size of S .High
MI weights are assumed to correspond to strong word X  X eature associations. tically correlated with their corresponding headword. Thus, they suggest that any statistical test used for collocations is a good starting point for improving feature-weight functions. In their experiments the t-test-based metric yielded the best empirical performance.
 functions used for collocation extraction, including t-test and  X  inflate the weights for rare features (Dunning 1993). In addition, a major property of lexical collocations is their  X  X on-substitutability X , as termed in Manning and Schutze (1999). That is, typically neither a headword nor a modifier in the collocation can be substituted by their synonyms or other related terms. This implies that using modifiers within strong collocations as features for a head word would provide a rather small amount of common features for semantically similar words. Hence, these functions seem less suitable for learning broader substitutability relationships, such as lexical entailment.

In particular, a common practice is to filter out features by minimal frequency and weight thresholds. Then, a word X  X  vector is constructed from the remaining (not filtered) features that are strongly associated with the word. These features are denoted here as active features.
 quality and word similarity performance. 2.2 Vector Similarity Measures
Once feature vectors have been constructed the similarity between two words is de-fined by some vector similarity measure. Similarity measures which have been used in the cited papers include weighted Jaccard (Grefenstette 1994), Cosine (Ruge 1992), and various information theoretic measures, as introduced and reviewed in Lee (1997, 1999). In the current work we experiment with the following three popular similarity measures. 1. The basic Jaccard measure compares the number of common features with 2. The standard Cosine measure ( COS ), which is popularly employed for 440 3. A popular state of the art measure has been developed by Lin (1998),
It is interesting to note that a relatively recent work by Weeds and Weir (2005) inves-tigates a more generic similarity framework. Within their framework, the similarity of two nouns is viewed as the ability to predict the distribution of one of them based on that of the other. Their proposed formula combines the precision and recall of a potential  X  X etrieval X  of similar words based on the features of the target word. The precision of w  X  X  prediction of v  X  X  feature distribution indicates how many of the features of the word w co-occurred with the word v . The recall of w  X  X  prediction of v  X  X  features indicates how many of the features of v co-occurred with w . Words with both high precision and high recall can be obtained by computing their harmonic mean, mh (or F-score), and a weighted arithmetic mean. However, after empirical tuning of weights for the arithmetic mean, Weeds and Weir X  X  formula practically reduces to Lin X  X  measure, as was anticipated by their own analysis (in Section 4 of their paper).
 as representative for the state of the art and utilize it for data analysis and as a starting point for improvement. To further explore and evaluate our new weighting scheme, independently of a single similarity measure, we conduct evaluations also with the other two similarity measures of weighted Jaccard and Cosine. 3. Substitutable Lexical Entailment
As mentioned in the Introduction, the long term research goal which inspired our work is modeling meaning X  X ntailing lexical substitution. Motivated by this goal, we proposed in earlier work (Geffet and Dagan 2004, 2005) a new type of lexical relationship which aims to capture such lexical substitution needs. Here we adopt that approach and formulate a refined definition for this relationship, termed substitutable lexical entailment . In the context of the current article, utilizing a concrete target notion of word similarity enabled us to apply direct human judgment for the  X  X orrectness X  (relative to the defined notion) of candidate word pairs suggested by distributional similarity. Utilizing these judgments we could analyze the behavior of alternative distributional vector representations and, in particular, conduct error analysis for word pair candidates that were judged negatively.
 applications need to identify term pairs whose meanings are both entailing and sub-stitutable. Such pairs seem to be most appropriate for lexical substitution in a meaning preserving scenario. To model this goal we present an operational definition for a lexical semantic relationship that integrates the two aspects of entailment and substitutability, which is termed substitutable lexical entailment (or lexical entailment , for brevity).
This relationship holds for a given directional pair of terms ( w , v ), saying that w entails v , if the following two conditions are fulfilled: 1. Word meaning entailment : the meaning of a possible sense of w implies a 2. Substitutability : w can substitute for v in some naturally occurring sentence, the meaning of terms by existential statements of the form  X  X here exists an instance of the meaning of the term w in some context X  (notice that, unlike propositions, it is not intuitive for annotators to assign truth values to terms). For example, the word company would correspond to the existential statement  X  X here exists an instance of the concept company in some context. X  Thus, if in some context  X  X here is a company  X  (in the sense of  X  X ommercial organization X ) then necessarily  X  X here is a firm  X  in that context (in the corresponding sense). Therefore, we conclude that the meaning of company implies the meaning of firm . On the other hand,  X  X here is an organization  X  does not necessarily imply the existence of company ,since organization might stand for some non-profit association, as well. Therefore, we conclude that organization does not entail company . text in which the lexical substitution would satisfy entailment between the modified sentence and the original one. Practically, in our experiments presented in Section 5 the human assessors could consult external lexical resources and the entire Web to obtain all the senses of the words and possible sentences for substitution. We note that the task of identifying the common sense of two given words is quite easy since they mutually dis-ambiguate each other, and once the common sense is known it naturally helps finding a corresponding common context. We note that this condition is important, in particular, in order to eliminate cases of anaphora and co-reference in contexts, where two words quite different in their meaning can sometimes appear in the same contexts only due to the text pragmatics in a particular situation. For example, in some situations worker and demonstrator could be used interchangeably in text, but clearly it is a discourse co-reference rather than common meaning that makes the substitution possible. Instead, we are interested in identifying word pairs in which one word X  X  meaning provides a reference to the entailed word X  X  meaning. This purpose is exactly captured by the existential propositions of the first criterion above. 442 for candidate word similarity pairs was quite intuitive for annotators, and yielded good cross-annotator agreement. Overall, substitutable lexical entailment captures directly the typical lexical substitution scenario in text understanding applications, as well as in generic textual entailment modeling. In fact, this relation partially overlaps with several traditional lexical semantic relations that are known as relevant for lexical substitution, such as synonymy, hyponymy, and some cases of meronymy. For example, we say that the meaning of company is lexically entailed by the meaning of firm (synonym) or automaker (hyponym), while the word government entails minister (meronym) as The government voted for the new law entails A minister in the government voted for the new law. but it is rather designed to select those sub-cases of other lexical relations that are needed for applied entailment inference. For example, lexical entailment does not cover all cases of meronyms (e.g., division does not entail company ), but only some sub-cases of part-whole relationship mentioned herein. In addition, some other relations are also covered by lexical entailment, like ocean and water and murder and death , which do not seem to directly correspond to meronymy or hyponymy relations.
 which word of the pair entails the other, the relation may hold in both directions for a pair of words, as is the case for synonyms. More detailed motivations for the substitutable lexical entailment relation and analysis of its relationship to traditional lexical semantic relations appear in Geffet (2006) and Geffet and Dagan (2004, 2005). 4. Bootstrapping Feature Weights
To gain a better understanding of distributional similarity behavior we first analyzed the output of the LIN measure, as a representative case for the state of the art, and regarding lexical entailment as a reference evaluation criterion. We judge as correct, with respect to lexical entailment, those candidate pairs of the distributional similarity method for which entailment holds at least in one direction.
 the summer period entails the sentence There is no rain in subtropical areas during the summer period. As another example, democracy is a type of country in the political sense, thus the existence entailment holds and also the sentence Israel is a democracy in the Middle East entails Israel is a country in the Middle East.
 suggested by distributional similarity measures do not correspond to  X  X ight X  semantic relationships. In particular, many word pairs suggested by the LIN measure do not satisfy the lexical entailment relation, as demonstrated in Table 2.
 for these lexical entailment prediction errors. Most relevant for the scope of the cur-rent article, in many cases highly ranked features in a word vector (when sorting the features by their weight) do not seem very characteristic for the word meaning. This is demonstrated in Table 3, which shows the top 10 features in the vector for country .
As can be seen, some of the top features are either too specific ( landlocked , airspace ), and are thus less reliable, or too general ( destination , ambition ), thus not indicative and may co-occur with many different types of words. On the other hand, intuitively more characteristic features of country , like population and governor , occur further down the sorted feature list, at positions 46 1and 832. Overall, features that seem to characterize the word meaning well are scattered across the ranked feature list, while many non-indicative features receive high weights. This behavior often yields high similarity scores for word pairs whose semantic similarity is rather loose while missing some much tighter similarities.
 receive higher weights, are expected to be common for w and other words that are semantically similar to it. This observation suggests a computational scheme which would promote the weights of features that are common for semantically similar words.
Of course, there is an inherent circularity in such a scheme: to determine which features should receive high weights we need to know which words are semantically similar, while computing distributional semantic similarity already requires pre-determined feature weights.
 compute initial distributional similarity values, based on an initial feature weighting function. Then, to learn more accurate feature weights for a word w ,wepromote features that characterize other words that are initially known to be similar to w .By the same rationale, features that do not characterize many words that are sufficiently similar to w are demoted. Even if such features happen to have a strong direct statis-tical association with w they would not be considered reliable, because they are not supported by additional words that have a similar meaning to that of w . 444 4.1 Bootstrapped Feature Weight Definition
The bootstrapped feature weight is defined as follows. First, some standard word similarity measure sim is computed to obtain an initial approximation of the similarity space. Then, we define the word set of a feature f , denoted by WS ( f ), as the set of words for which f is an active feature. Recall from Section 2.2 that an active feature is a feature that is strongly associated with the word, that is, its (initial) weight is higher than an empirically predefined threshold,  X  weight .The semantic neighborhood of w , denoted by
N ( w ), is defined as the set of all words v which are considered sufficiently similar to
The bootstrapped feature weight, denoted weight B , is then defined by:
That is, we identify all words v that are in the semantic neighborhood of w and are also characterized by f , and then sum the values of their similarities to w .
 balance between feature specificity and generality, addressing the observations in the beginning of this section. Some features might characterize just a single word that is very similar to w , but then the sum of similarities will include a single element, yielding a relatively low weight. This is why the sum of similarities is used rather than an average value, which might become too high by chance when computed over just a single element (or very few elements). Relatively generic features, which occur with many words and are thus less indicative, may characterize more words within N ( w ) but then on average the similarity values of these words with w is likely to be lower, contributing smaller values to the sum. To receive a high overall weight a reliable feature has to characterize multiple words that are highly similar to w .
 than a direct function of word X  X eature association values, which is the more common approach. It thus does not depend on the exact statistical co-occurrence level between w and f . Instead, it depends on a more global assessment of the association between f and the semantic vicinity of w . We notice that the bootstrapped weight is deter-mined separately relative to each individual word. This differs from measures that are global word-independent functions of the feature, such as the feature entropy used in
Grefenstette (1994) and the feature term strength relative to a predefined class as em-ployed in Pekar, Krkoska, and Staab (2004) for supervised word classification. 4.2 Feature Reduction and Similarity Re-Computation
Once the bootstrapped weights have been computed, their accuracy is sufficient to allow for aggressive feature reduction. As shown in the following section, in our experiments it sufficed to use only the top 100 features for each word in order to obtain optimal word similarity results, because the most informative features now receive the highest weights.
 sim function with weight B replacing the original feature weights. The resulting similarity measure is further referred to as sim B . 5. Evaluation by Lexical Entailment
To test the effectiveness of the bootstrapped weighting scheme, we first evaluated whether it contributes to better prediction of lexical entailment. This evaluation was based on gold-standard annotations determined by human judgments of the substi-tutable lexical entailment relation, as defined in Section 3. The new similarity scheme, sim
B , based on the bootstrapped weights, was first computed using the standard LIN method as the initial similarity measure. The resulting similarity lists of sim original LIN method) and sim B LIN ( Bootstrapped LIN ) schemes were evaluated for a sam-ple of nouns (Section 5.2). Then, the evaluation was extended (Section 5.3) to apply the bootstrapping scheme over the two additional similarity measures that were presented in Section 2.2, sim WJ (weighted Jaccard) and sim COS (Cosine). Along with these lexical entailment evaluations we also analyzed directly the quality of the bootstrapped fea-ture vectors, according to the average common-feature rank ratio measure, which was defined in Section 6. 5.1 Experimental Setting Our experiments were conducted using statistics from an 18 million token subset of the
Reuters RCV1 corpus (known as Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19), parsed by Lin X  X  Minipar dependency parser (Lin 1993). 30 randomly selected nouns whose corpus frequency exceeds 500. In our primary exper-iment we computed the top 40 most similar words for each noun by the sim sim
LIN measures, yielding 1,200 pairs for each method, and 2,400 pairs altogether. About 800 of these pairs were common for the two methods, therefore leaving approximately 1,600 distinct candidate word similarity pairs. Because the lexical entailment relation is directional, each candidate pair was duplicated to create two directional pairs, yielding and ( v , w ) were created to be judged separately for entailment in the specified direction (whether the first word entails the other). Consequently, a non-directional candidate similarity pair w , v is considered as a correct entailment if it was assessed as an entailing pair at least in one direction.
 information and could consult any available dictionary, WordNet, and the Web.
The judgment criterion follows the criterion presented in Section 3. In particular, the judges were asked to apply the two operational conditions, existence and sub-stitutability in context, to each given pair. Prior to performing the final test of the annotation experiment, the judges were presented with an annotated set of entailing and non-entailing pairs along with the existential statements and sample sentences for substitution, demonstrating how the two conditions could be applied in different cases of entailment. In addition, they had to judge a training set of several dozen pairs and then discuss their judgment decisions with each other to gain a better understanding of the two criteria.
 { company, organization } two directional pairs are created: (company, organization) and (organization, company) . The former pair is judged as a correct entailment: the existence of a company entails the existence of an organization, and the meaning of the sentence: John works for a large company entails the meaning of the sentence with substitution:
John works for a large organization. Hence, company lexically entails organization ,butnot 446 vice versa (as shown in Section 3.3), therefore the second pair is judged as not entailing.
Eventually, the non-directional pair { company, organization entailment.
 judged by three native English speaking assessors, each of whom possessed a Bach-elors degree in English Linguistics. For each subset a different pair of assessors was assigned, each person judging the entire subset. The judges were grouped into three different pairs (i.e., JudgeI+JudgeII, JudgeII+JudgeIII, and JudgeI+JudgeIII). Each pair was assigned initially to judge all the word similarities in each subset, and the third assessor was employed in cases of disagreement between the first two. The majority vote was taken as the final decision. Hence, each assessor had to fully annotate two thirds of the data and for a third subset she only had to judge the pairs for which there was disagreement between the other two judges. This was done in order to measure the agreement achieved for different pairs of annotators.
 a pair with the method that proposed it. We note that this evaluation methodology, in which human assessors judge the correctness of candidate pairs by some semantic substitutability criterion, is similar to common evaluation methodologies used for para-phrase acquisition (Barzilay and McKeown 2001; Lin and Pantel 2001; Szpektor et al. 2004).
 cisions were 93.5% between Judge I and Judge II, 90% for Judge I and Judge III, and 91.2% for Judge II and Judge III. The corresponding kappa values are 0.83, 0.80, and 0.80, which is regarded as  X  X ery good agreement X  (Landis and Koch 1997). It is interesting to note that after some discussion most of the disagreements were settled, and the few remaining mismatches were due to different understandings of word meanings. These findings seem to have a similar flavor to the human agreement findings reported for the Recognizing Textual Entailment challenges (Bar-Haim et al. 2006; Dagan, Glickman, and
Magnini 2006), in which entailment was judged for pairs of sentences. In fact, the kappa values obtained in our evaluation are substantially higher than reported for sentence-level textual entailment, which suggests that it is easier to make entailment judgments at the lexical level than at the full sentence level.
 similarity pairs generated for 10 additional nouns, distinct from the 30 nouns used for the test set. The parameters were optimized by running the algorithm systematically with various values across the parameter scales and judging a sample subset of the results. weight MI = 4wasfoundastheoptimal MI threshold for active feature weights (features included in the feature vectors), yielding a 10% precision increase of sim removing over 50% of the data relative to no feature filtering. Accordingly, this value also serves as the  X  weight threshold in the bootstrapping scheme (Section 4). As for the  X  sim parameter, the best results on the development set were obtained for  X   X  sim = 0 . 02, and  X  sim = 0 . 0 1when bootstrapping over the initial similarity measures
LIN , WJ ,and COS , respectively. 5.2 Evaluation Results for sim B LIN
We measured the contribution of the improved feature vectors to the resulting preci-sion of sim LIN and sim B LIN in predicting lexical entailment. The results are presented in
Table 4, where precision and error reduction values were computed for the top 20, 30, and 40 word similarity pairs produced by each method. It can be seen that the
Bootstrapped LIN method outperformed the original LIN approach by 6 X 9 precision points at all top-n levels. As expected, the precision for the shorter top 20 list is higher for both methods, thus leaving a bit less room for improvement.
 pairs than the other measure and reduced the number of errors by almost 15%. We also computed the relative recall, which shows the percentage of correct word similarities found by each method relative to the joint set of similarities that were extracted by both methods. The overall relative recall of the Bootstrapped LIN was quite high (94%), exceeding LIN  X  X  relative recall (of 78%) by 16 percentage points. We found that the bootstrapped method covers over 90% of the correct similarities learned by the original method, while also identifying many additional correct pairs.
 mined not just by the quality of the feature vectors but significantly by the nature of the vector comparison measure itself (i.e., the LIN formula, as well weighted Jaccard and
Cosine as reported in Section 5.3). It was observed in other work (Geffet and Dagan 2005) that these common types of vector comparison schemes exhibit certain flaws in predicting lexical entailment. Our present work thus shows that the bootstrapping method yields a significant improvement in feature vector quality, but future research is needed to investigate improved vector comparison schemes.
 reduction allowed by having the most characteristic features concentrated at the top ranks of the vectors. The vectors of active features of LIN , as constructed after standard feature filtering (Section 5.1), could be further reduced by the bootstrapped weighting to about one third of their size. As illustrated in Figure 1, changing the vector size significantly affects the similarity results. In sim B LIN the top 100 features per word, while using less than 100 or more than 150 features caused a 5 X 10% decrease in performance. On the other hand, an attempt to cut off the lower ranked features of the MI weighting always resulted in a noticeable decrease in precision. These results show that for MI weighting many important features appear further down in the ranked vectors, while for the bootstrapped weighting adding too many features adds mostly noise , since most characteristic features are concentrated at the top ranks. Thus, in addition to better feature weighting, the bootstrapping step provides effective feature reduction, which improves vector quality and consequently the similarity results.
 for example, by Widdows (2003), Curran (2004), and Curran and Moens (2002) X  X ho also used reduced vectors of up to 100 features as optimal for learning hyponymy and synonymy, respectively. In Widdows the known SVD method for dimension reduction 448 of LSA-based vectors is applied, whereas in Curran, and Curran and Moens, only the strongly associated verbs (direct and indirect objects of the noun) are selected as  X  X anonical features X  that are expected to be shared by true synonyms.
 tion over the similarity results of sim B LIN . The resulting increase in precision was much smaller, of about 2%, showing that most of the potential benefit is exploited in the first bootstrapping iteration (which is not uncommon for natural language data). On the other hand, computing the bootstrapping weight twice increases computation time significantly, which led us to suggest a single bootstrapping iteration as a reasonable cost-effectiveness tradeoff for our data. 5.3 Evaluation for sim B WJ and sim B COS
To further validate the behavior of the bootstrapping scheme we experimented with two additional similarity measures, weighted Jaccard ( sim WJ in Section 2.2). For each of the additional measures the experiment repeats the main three steps described in Section 4: Initially, the basic similarity lists are calculated for each of the measures using MI weighting; then, the bootstrapped weighting, weight computed based on the initial similarities, yielding new word feature vectors; finally, the similarity values are recomputed by the same vector similarity measure using the new feature vectors.
 similarity lists, using the sim WJ and sim COS similarity measures, each with the weight and weight B weighting functions. The four lists were judged for lexical entailment by three assessors, according to the same procedure described in Section 5.1. To make the additional manual evaluation affordable we judged the top 20 similar words in each list for each of the 30 target nouns of Section 5.1.
 both weight MI and weight B . As shown in the table, bootstrapped weighting consistently contributed between 4 X 6 points to the accuracy of each method in the top 20 similarity list. We view the results as quite positive, considering that improving over top 20 similarities is a much more challenging task than improving over longer similarity lists, while the improvement was achieved only by modifying the feature vectors without changing the similarity measure itself (as hinted in Section 5.2). Our results are also compatible with previous findings in the literature (Dagan, Lee, and Pereira 1999;
Weeds, Weir, and McCarthy 2004) that found LIN and WJ to be more accurate for similarity acquisition than COS . Overall, the results demonstrate that the bootstrapped weighting scheme consistently produces improved results.
 features for a given target word converge across the different initial similarity measures, as exemplified in Table 6. In particular, although the initial similarity lists overlap only partly, 4 the overlap of the top 30 features for our 30-word sample was ranging between 88% and 100%. This provides additional evidence that the quality of the bootstrapped weighting is quite similar for various initial similarity measures. 6. Analyzing the Bootstrapped Feature Vector Quality
In this section we provide an in-depth analysis of the bootstrapping feature weighting quality compared to the state-of-the-art MI weighting function. 6.1 Qualitative Observations
The problematic feature ranking noticed at the beginning of Section 4 can be revealed more objectively by examining the common features which contribute most to the word similarity scores. To that end, we examine the common features of the two given words and sort them by the sum of their weights in both word vectors. Table 7 shows the top 10 common features by this sorting for a pair of truly similar (lexically entailing) words ( country X  X tate ), and for a pair of non-entailing words ( country X  X arty ). For each common feature the table shows its two corresponding ranks in the feature vectors of the two words. 450 scattered across the pair of feature vectors, making it difficult to distinguish between the truly similar and the non-similar pairs. We suggest, on the other hand, that the desired behavior of effective feature weighting is that the common features of truly similar words would be concentrated at the top ranks of both word vectors. In other words, if the two words are semantically similar then we expect them to share their most characteristic features, which are in turn expected to appear at the higher ranks of each feature vector. The common features for non-similar words are expected to be scattered all across each of the vectors. In fact, these expectations correspond exactly to the rationale behind distributional similarity measures: Such measures are designed to assign higher similarity scores for vector pairs that share highly weighted features. the observations regarding the original LIN method, using the same running example.
Table 8 shows the top 10 features of country . We observe that the list now contains features that are intuitively quite indicative and reliable, while many too specific or idiomatic features, and too general ones, were demoted (compare with Table 3). Table 9 shows that most of the top 10 common features for country X  X tate are now ranked highly for both words. On the other hand, there are only two common features (among the top 100 features) for the incorrect pair country X  X arty , both with quite low ranks (compare with Table 7), while the rest of the common features for this pair did not pass the top 100 cutoff.
 try , where many incorrect (non-entailing) word similarities, like party and company , were demoted. Instead, additional correct similarities, like kingdom and land , were promoted (compare with Table 2). In this particular case all the remaining errors correspond to words that are related quite closely to country , denoting geographic concepts. Many of these errors are context dependent entailments which might be substitutable in some cases, but they violate the word meaning entailment condition (e.g., country X  X eighbor , country X  X ort ). Apparently, these words tend to occur in contexts that are typical for country in the Reuters corpus. Some errors violating the substitutability condition of lexical entailment were identified as well, such as industry X  X roduct . These cases are quite hard to differentiate from correct entailments, since the two words are usually closely related to each other and also share highly ranked features, because they often appear in similar characteristic contexts. It may therefore be difficult to filter out such 452 non-substitutable similarities merely by the standard distributional similarity scheme, suggesting that additional mechanisms and data types would be required. 6.2 The Average Common-Feature Rank Ratio
It should be noted at this point that these observations regarding feature weight be-havior are based on subjective intuition of how characteristic features are for a word meaning, which is quite difficult to assess systematically. Therefore, we next propose a quantitative measure for analyzing the quality of feature vector weights.
 their average common-feature rank with respect to the top-n common features, denoted acfr n , as follows: where rank ( w , f ) is the rank of feature f in the vector of the word w when features are sorted by their weight, and F ( w )isthesetoffeaturesin w  X  X  vector. top-n is the set of top n common features to consider, where common features are sorted by the sum of their weights in the two word vectors (the same sorting as in Table 7). In other words, acfr n ( w , v ) is the average rank in the two feature vectors of their top n common features. typically yield lower values of acfr n for truly similar words (as low ranking values correspond to higher positions in the vectors) than for non-similar words. Hence, given a pre-judged test set of pairs of similar and non-similar words, we define the ratio, acfr-ratio , between the average acfr n of the set of all the non-similar words, denoted as
Non-Sim , and the average acfr n of the set of all the known pairs of similar words, Sim ,to be an objective measure for feature weighting quality, as follows: and acfr 10 ( country , party ) = 64. Both values are quite high, showing no principal differ-ence between the tighter lexically entailing similarity versus a pair of non-similar (or rather loosely related) words. This behavior indicates the deficiency of the MI feature weighting function in this case. On the other hand, the corresponding values for the two pairs produced by the Bootstrapped LIN method (for the features in Table 9) are acfr desired distinction between similar and non-similar words, showing that the common features of the similar words are indeed concentrated at much higher ranks in the vectors than the common features of the non-similar words.
 variety of alternative weighting functions were compared. However, the quality of these weighting functions was evaluated only through their impact on the performance of a particular word similarity measure, as we did in Section 5. Our acfr-ratio measure provides the first attempt to analyze the quality of weighting functions directly, relative to a pre-judged word similarity set, without reference to a concrete similarity measure. 6.3 An Empirical Assessment of the acfr-ratio
In this subsection we report an empirical comparison of the acfr-ratio obtained for the MI and BootstrappedLIN weighting functions. To that end, we have run the Minipar system on the full Reuters RCV 1corpus, which contains 2.5 GB of English news stories, and then calculated the MI -weighted feature vectors. The optimized threshold on the feature weights,  X  weight , was set to 0.2. Further, to compute the Bootstrapped LIN feature weights a  X  sim of 0.02 was applied to the LIN similarity values. In this experiment we employed the full bootstrapped vectors (i.e., without applying feature reduction by the top 100 cutoff). This was done to avoid the effect of the feature vector size on the acfr which tends to naturally assign higher scores to shorter vectors.
 larity pairs, we utilized the annotated test sample of candidate pairs of word similarities described in Section 5, which contains both entailing and non-entailing pairs. the mean acfr n scores for weight B range within 110 X 264 for n = 10...100, while the corresponding range for weight MI is by an order of magnitude higher: 780 X 1,254, despite the insignificant differences in vector sizes. Therefore, we conclude that the common features that are relevant to establishing distributional similarity in general (regardless of entailment) are much more scattered across the vectors by MI weighting, while with bootstrapping they tend to appear at higher positions in the vectors. These figures 454 reflect a desired behavior of the bootstrapping function which concentrates most of the prominent common features for all the distributionally similar words (whether entailing or not) at the lower ranks of their vectors. In particular, this explains the ability of our method to perform a massive feature reduction as demonstrated in Section 5, and to produce more informative vectors, while demoting and eliminating much of the noise in the original vectors.
 distinguish between entailing and non-entailing pairs. To this end we calculated the acfr-ratio , which captures the difference in the average common feature ranks between entailing vs. non-entailing pairs, for both the MI -based and bootstrapped vectors. are consistently higher for Bootstrapped LIN than for MI . That is, the bootstrapping method assigns much higher acfr n scores to entailing words than to non-entailing ones, whereas for MI the corresponding acfr n scores for entailing and non-entailing pairs are roughly equal. In particular, we notice that the largest gaps in acfr-ratio occur for lower numbers of top common features, whose weights are indeed the most important and influential in distributional similarity measures. Thus, these findings suggest a direct indication of an improved quality of the bootstrapped feature vectors. 7. A Pseudo-Word Sense Disambiguation Evaluation
The lexical entailment evaluation reported herein corresponds to the lexical substitution application of distributional similarity. The other type of application, as reviewed in the
Introduction, is similarity-based prediction of word co-occurrence likelihood, needed for disambiguation applications. Comparative evaluations of distributional similarity methods for this type of application were commonly conducted using a pseudo-word sense disambiguation scheme, which is replicated here. In the next subsections we first describe how distributional similarity can help improve word sense disambiguation (WSD). Then we describe how the pseudo-word sense disambiguation task, which corresponds to the general WSD setting, was used to evaluate the co-occurrence like-lihood predictions obtained by alternative similarity methods. 7.1 Similarity Modeling for Word Sense Disambiguation
WSD methods need to identify the correct sense of an ambiguous word in a given context. For example, a test instance for the verb save might be presented in the con-text saving Private Ryan . The disambiguation method must decide whether save in this particular context means rescue , preserve , keep , lay aside , or some other alternative. annotated training corpus. For example, the system might learn from the annotated training data that the word soldier is a typical object for the rescuing sense of save ,asin:
They saved the soldier . In this setting, distributional similarity is used to reduce the data sparseness problem via similarity-based generalization. The general idea is to predict the likelihood of unobserved word co-occurrences based on observed co-occurrences of distributionally similar words. For example, assume that the noun private did not occur as a direct object of save in the training data. Yet, some of the words that are
Thus, a WSD system may infer that the co-occurrence save private is more likely for the rescuing sense of save because private is distributionally similar to soldier , which did co-occur with this sense of save in the annotated training corpus. In general terms, the WSD method estimates the co-occurrence likelihood for the target sense and a given context word based on training data for words that are distributionally similar to the context word.
 in Dagan, Marcus, and Markovitch (1995) to enhance WSD performance in machine translation and recently in Gliozzo, Giuliano, and Strapparava (2005), who employed a
Latent Semantic Analysis (LSA)-based kernel function as a similarity-based represen-tation for WSD. Other works employed the same idea for pseudo-word sense dis-ambiguation, as explained in the next subsection. 7.2 The Pseudo-Word Sense Disambiguation Setting
Sense disambiguation typically requires annotated training data, created with consid-erable human effort. Yarowsky (1992) suggested that when using WSD as a test bed for comparative algorithmic evaluation it is possible to set up a pseudo-word sense disambiguation scheme. This scheme was later adopted in several experiments, and was popular for comparative evaluations of similarity-based co-occurrence likelihood estimation (Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). We followed closely the same experimental scheme, as described subsequently.
 pseudo word. In our experiment each pseudo-word constitutes a pair of randomly chosen verbs, ( v , v ), where each verb represents an alternative  X  X ense X  of the pseudo-word. The two verbs are chosen to have almost identical probability of occurrence, which avoids a word frequency bias on the co-occurrence likelihood predictions. pseudo-word and n is a noun representing the object of the pseudo-word. Such pairs are constructed from all co-occurrences of either v or v with the object n in the corpus.
For example, given the pseudo-word (rescue, keep) and the verb X  X bject co-occurrence in the corpus rescue X  X rivate we construct the pair private, (rescue, keep) . Given such a test 456 pair, the disambiguation task is to decide which of the two verbs is more likely to co-occur with the given object noun, aiming to recover the original verb from which this pair was constructed. In this example we would like to predict that rescue is more likely to co-occur with private as an object than keep .
 the co-occurrence statistics for the original known verb in each pair (i.e., either n, v or n, v X  ). From the remaining 20% of the pairs those occurring in the training corpus were discarded, leaving as a test set only pairs which do not appear in the training part.
Thus, predicting the co-occurrence likelihood of the noun with each of the two verbs cannot rely on direct frequency estimation for the co-occurrences, but rather only on similarity-based information.
 ilarity scores for all pairs of nouns based on the training set statistics, where the co-occurring verbs serve as the features in the distributional vectors of the nouns. Then, given a test pair (v,v X ), n our task is to predict which of the two verbs is more likely to co-occur with n . This verb is thus predicted as being the original verb from which the pair was constructed. To this end, the noun n is substituted in turn with each of its k distributionally most similar nouns, n i , and then both of the obtained  X  X imilar X  pairs n i , v and n i , v are sought in the training set.
 n, v X  is the one for which more pairs of similar words were found in the training set.
Several approaches were used in the literature to quantify this decision procedure and we have followed the most recent one from Weeds and Weir (2005). Each similar noun n is given a vote, which is equal to the difference between the frequencies of the two more frequently. The votes for each of the two verbs are summed over all k similar nouns n i and the one with most votes wins. The winning verb is considered correct if it is indeed the original verb from which the pair was constructed, and a tie is recorded if the votes for both verbs are equal. Finally, the overall performance of the prediction method is calculated by its error rate: where T is the number of test instances.

Reuters corpus (of Section 5.1). The training and test data were created as described herein, using the Minipar parser (Lin 1993) to produce verb X  X bject co-occurrence pairs.
The k=40 most similar nouns for each test noun were computed by each of the three examined similarity measures LIN, WJ ,and COS (as in Section 5), with and without bootstrapping. The six similarity lists were utilized in turn for the pseudo-word sense disambiguation task, calculating the corresponding error rate. 7.3 Results
Table 11 shows the error rate improvements after applying the bootstrapped weighting for each of the three similarity measures. The largest error reduction, by over 15%, was obtained for the LIN method, with quite similar results for WJ . This result is better than the one reported by Weeds and Weir (2005), who achieved about 6% error reduction compared to LIN . proved bootstrapped feature vectors, correlates also with better similarity-based infer-ence for co-occurrence likelihood prediction. Furthermore, we have seen once again that the bootstrapping scheme does not depend on a specific similarity measure, reducing the error rates for all three measures. 8. Conclusions
The primary contribution of this article is the proposal of a bootstrapping method that substantially improves the quality of distributional feature vectors, as needed for statistical word similarity. The main idea is that features which are common for similar words are also most characteristic for their meanings and thus should be promoted. In fact, beyond its intuitive appeal, this idea corresponds to the underlying rationale of the distributional similarity scheme: Semantically similar words are expected to share exactly those context features which are most characteristic for their meaning. context of the two primary applications of distributional word similarity. The first is lexical substitution, which was represented in our work by a human gold standard for the substitutable lexical entailment relation. The second is co-occurrence likelihood prediction, which was assessed by the automatically computed scores of the common pseudo-word sense disambiguation evaluation. An additional outcome of the improved feature weighting is massive feature reduction.
 strapping scheme is robust and performs well when applied over different measures.
Notably, our experiments show that the underlying assumption behind the boot-strapping scheme is valid, that is, available similarity metrics do provide a reason-able approximation of the semantic similarity space which can be then exploited via bootstrapping.
 1. Utilizing a refined definition of substitutable lexical entailment both as an 2. A thorough error analysis of state of the art distributional similarity 458 3. Inspired by the qualitative analysis, we proposed a new analytic measure
The ability to identify the most characteristic features of words can have additional ben-efits, beyond their impact on traditional word similarity measures (as evaluated in this article). A demonstration of such potential appears in Geffet and Dagan (2005), which presents a novel feature inclusion scheme for vector comparison. That scheme utilizes our bootstrapping method to identify the most characteristic features of a word and then tests whether these particular features co-occur also with a hypothesized entailed word. The empirical success reported in that paper provides additional evidence for the utility of the bootstrapping method.
 rections by future work on acquiring lexical entailment or other lexical-semantic rela-tions. One direction is to explore better vector comparison methods that will utilize the improved feature weighting, as shown in Geffet and Dagan (2005). Another direc-tion is to integrate distributional similarity and pattern-based acquisition approaches, which were shown to provide largely complementary information (Mirkin, Dagan, and
Geffet 2006). An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods. As a parallel direction, future research should explore in detail the impact of different lexical-semantic acquisition methods on text understanding applications.
 improving feature vector quality in additional unsupervised settings. We thus hope that this idea will be explored further in other NLP and machine learning contexts. References 460
