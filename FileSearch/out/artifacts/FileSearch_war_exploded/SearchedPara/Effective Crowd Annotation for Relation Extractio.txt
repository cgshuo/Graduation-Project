 Relation extraction (RE) is the task of identifying instances of relations, such as nationality (person, country) or place of birth (person, location) , in pas-sages of natural text. Since RE enables a broad range of applications  X  including question answering and knowledge base population  X  it has attracted atten-tion from many researchers. Many approaches to RE use supervised machine learning, e.g., (Soderland et al., 1995; Califf and Mooney, 1997; Lafferty et al., 2001), but these methods require a large, human-annotated training corpus that may be unavailable.
In response, researchers developed methods for distant supervision (DS) in which a knowledge base such as Wikipedia or Freebase is used to automati-cally tag training examples from a text corpus (Wu and Weld, 2007; Mintz et al., 2009). Indeed, virtu-ally all entries to recent TAC KBP relation extraction competitions use distant supervision (Ji and Grish-man, 2011). However, distant supervision provides noisy training data with many false positives, and this limits the precision of the resulting extractors (see Section 2). A natural assumption is that human-annotated training data, either alone or in conjunc-tion with distant supervision, would give better pre-cision. In particular, Snow et al. (2008) showed that, for many NLP tasks, crowdsourced data is as good as or better than that annotated by experts.
It is quite surprising, therefore, that researchers who have applied crowdsourced annotation to rela-tion extraction argue the opposite , that crowdsourc-ing provides only minor improvement:  X  Zhang et al. (2012) conclude that  X  X uman feed- X  Pershina et al. (2014) assert  X  X imply taking the  X  Angeli et al. (2014) explored a novel active
This paper reports quite a different result, show-ing up to a 0.20 boost to F1. By carefully de-signing a quality-controlled crowdsourcing work-flow that uses Gated Instruction (GI), we are able to create much more accurate annotations than those produced by previous crowdsourcing methods. GI (summarized in Figure 2) includes an interactive tu-torial to train workers, providing immediate feed-back to correct mistakes during training. Workers are then screened by their accuracy on gold-standard questions while doing the annotation. We show that GI generates much better training data than crowd-sourcing used by other researchers, and that this leads to dramatically improved extractors.

Adding GI-crowdsourced annotations of the ex-ample sentences selected by Angeli et al. X  X  active learning method provides a much larger boost to the performance of the learned extractors than when their traditional crowdsourcing methods are used. In fact, the improvement due to our crowdsourcing method substantially outweighs the benefits of An-geli et al. X  X  active learning strategy as well. In total, this paper makes the following contributions:  X  We present the design of the Gated Instruction  X  We demonstrate that Gated Instruction increases  X  Augmenting distant supervision with 10K of  X  We demonstrate that improved crowdsourcing  X  In contradiction to Zhang et al. X  X  prior claims, we  X  Gated Instruction may also reduce the cost of
Our results provide a clear lesson for future re-searchers hoping to use crowdsourced data for NLP tasks. Extreme care must be exercised in the details of the workflow design to ensure quality data and useful results. Distant supervision (DS) is a method for training extractors that obviates the need for human-labeled training data by heuristically matching facts from a background knowledge base (KB) to a large textual corpus. Originally developed to extract biological relations (Craven and Kumlien, 1999), DS was later extended to extract relations from Wikipedia in-foboxes (Wu and Weld, 2007) and Freebase (Mintz et al., 2009). Specifically, distant supervision uses the KB to find pairs of entities E 1 and E 2 for which a relation R holds. Distant supervision then makes that assumption that any sentence that contains a mention of both E 1 and E 2 is a positive training in-stance for R ( E 1 , E 2 ) .

Unfortunately, this assumption leads to a large proportion of false positive training instances. For example, Freebase asserts that Nicolas Sarkozy was born in Paris, but nearly all sentences in a news corpus that mention Sarkozy and Paris do not give evidence for a place of birth relation. To address this shortcoming, there have been attempts to model the relation dependencies as multi-instance multi-class (Bunescu and Mooney, 2007; Riedel et al., 2010) leading to state-of-the art extraction learners MultiR (Hoffmann et al., 2011) and MIML-RE (Sur-deanu et al., 2012).

Additionally, other techniques developed to study the relation extraction problem have achieved cer-tain success, including universal schemas (Riedel et al., 2013), and deep learning (Nguyen and Gr-ishman, 2014). Despite these technical innova-still require substantial human effort, typically hand-written extraction rules (Surdeanu and Ji, 2014).
Recently researchers have explored the idea of augmenting distant supervision with a small amount of crowdsourced annotated data in an effort to im-prove relation extraction performance (Angeli et al., 2014; Zhang et al., 2012; Pershina et al., 2014).
Zhang et al. (2012) studied how the size of the crowdsourcing training corpus and distant supervi-sion corpus affect the performance of the relation extractor. They considered the 20 TAC KBP re-lations that had a corresponding Freebase relation. They added up to 20K instances of crowd data to 1.8M DS instances using sparse logistic regression, tuning the relative weight of crowdsourced and DS training. However, they saw only a marginal im-provement from F1 0.20 to 0.22 when adding crowd-sourced training to DS training, and conclude that human feedback has little impact.

Angeli et al. (2014) also investigated meth-ods for infusing distant supervision with crowd-sourced annotations in the Stanford TAC-KBP sys-tem. They experimented with several methods, in-cluding adding a random sample of annotated sen-tences to the training mix, and using active learning to select which sentences should be annotated by humans. Their best results were what they termed  X  X ample JS, X  training a committee of MIML-RE classifiers and then sampling the sentences to be crowdsourced weighted by the divergence of clas-sifications.

Surprisingly, they found that the simple approach of just adding crowdsourced data to the training mix hurt extractor performance slightly. They conclude that the most important use for crowdsourced an-notations is as a way to initialize MIML-RE, miti-gating the problem of local minima during learning. When they initialized MIML-RE with 10K Sam-ple JS crowdsourced instances and then trained on a combination of Sample JS crowdsourced and DS instances, this raised F1 from 0.34 to 0.38.
Pershina et al. (2014) also exploited a small set of highly informative hand-labeled training data to improve distant supervision. Rather than crowd-sourcing, they used the set of 2,500 labeled instances from the KBP 2012 assessment data. They state that  X  X imply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data. X  Instead they use the hand-annotated data to learn guidelines that are included in a graphical prediction model that extends MIML-RE, trained using distant super-vision. This raised F1 from 0.28 to 0.32 over a com-parison system without the learned guidelines.
Gormley et al. (2010) filtered crowdsourced workers by agreement with gold questions and by noting which workers took fewer than three seconds per question. They reported good inter-annotator agreement, but did not build a relation extractor from their data.

Both Zhang et al. and Angeli et al. used tradi-tional methods to ensure the quality of their crowd-sourced data. Zhang et al. replicated each ques-tion three times and included a gold question (i.e., one with a known answer) in each set of five ques-tions. They only used answers from workers who answered at least 80% of the gold-standard ques-tions correctly.
Angeli et al. included two gold-standard ques-tions in every set of 15. They discarded sets in which both controls were answered incorrectly, and additionally discarded all submissions from workers who failed the controls on more than one third of their submissions. They collected five annotations for each example, and used the majority vote as the ground truth in their training. They did not report the resulting quality of their crowdsourced annotations, but did release their data, allowing us to measure its precision and recall (see Section 4.1).

We argue that all these systems would have got-ten better performance by focusing attention on the quality of their crowdsourced annotation. We demonstrate that by improving the crowdsourcing workflow, we achieve a higher F1 score, both with the crowdsourced training alone and in combination with distant supervision.

Our work adds to the existing large body of work that shows that crowdsourcing can be and is an ef-fective and efficient method for training machine learning algorithms. Snow et al. (2008) showed that multiple workers can simulate an expert worker in a variety of natural language tasks. Many re-searchers (e.g., (Dawid and Skene, 1979; Whitehill et al., 2009)) have designed methods to aggregate crowd labels in order to reduce noise, and Sheng et al. (2008) showed that paying multiple crowd work-ers to relabel examples, as opposed to labeling new ones, can increase the accuracy of a classifier.
The effectiveness of crowdsourcing is dependent on a number of human factors. Several researchers have studied how worker retention is affected by payment schemes (Mao et al., 2013), recruitment techniques (Ipeirotis and Gabrilovich, 2014), or at-tention diversions (Dai et al., 2015). Ipeirotis and Gabrilovich show that volunteer workers may pro-vide higher quality work. By contrast, we show that paid workers, too, can produce high quality work through careful attention to worker training and test-ing. We used Amazon Mechanical Turk for our crowd-sourcing, but designed our own website to imple-ment the Gated Instruction (GI) protocol, rather than use the platform Amazon provides directly. This al-lowed us greater control over the UI and the worker Gated Instruction Crowdsourcing Protocol experience. The primary benefit of GI is worker training, which is necessary across platforms, so we expect to see comparable results on other platforms, such as CrowdFlower.

The ideas behind Gated Instruction are summa-rized in Figure 2. The workflow proceeds in three phases: tutorial, weed-out, and work (described be-low) with a focus on well-known user interface prin-ciples (rapid feedback and availability of extra help). While conceptually simple, we show this approach has a much bigger effect on the resulting learned NLP system than a more complex graphical model. 3.1 Interactive Tutorial Design The most important step in crowdsourcing is ensur-ing that workers understand the task. To this end we required workers to complete an interactive tutorial to learn the criteria for the relations to be annotated.
Since we wanted to test our extractor against of-ficial answers for the TAC-KBP Slot Filling evalu-ation, our tutorial taught workers to follow the offi-cial KBP guidelines. These guidelines require tag-ging only relations directly stated in the sentence, and discourage plausible inferences. For example, if a sentence states only that a person works in a city, then annotating a place of residence relation with that city is counted as an error, even if it is proba-ble that the person lives there.

Figure 3 shows a page from the tutorial that explains annotation guidelines for nationality and lived in (i.e., place of residence ). This figure shows the first page of the tutorial  X  as more relations are taught, those relations are added to the question. The real questions are asked in the same format later on for consistency. The worker can click on a link to see the relation definitions at any time during the tu-torial or while doing the actual task. If workers make a mistake during the tutorial, they are given immedi-ate feedback along with an explanation for the cor-rect answer. The workers cannot proceed without correcting all errors on all problems in the tutorial. 3.2 Adaptive Worker Screening After examining worker mistakes in a preliminary experiment, we manually selected a set of gold ques-tions (i.e., questions with unambiguous, known an-swers) that workers are likely to get wrong if they don X  X  clearly understand the annotation criteria. The gold questions are grouped into sets of 5 questions that represent all relations being annotated. The first 5 questions (the screening phase) are used to elimi-nate spammers and careless workers early on. These questions look no different from normal questions, but we give feedback to workers with the right an-swers if workers give wrong answers to any of these questions. If a worker fails a majority of such ques-tions, the worker is disqualified from the task.
We then place additional sets of gold questions among real test questions without feedback in order to spot-check workers X  responses. In our experience, workers who start out with high accuracy maintain that accuracy throughout the entire session. There-fore, we place the gold questions in exponentially decreasing frequency among the batches of 20 ques-tions (5 gold questions in batches 2, 4, 8, etc.), and allow only workers who maintain at least 80% ac-curacy on the most recent 10 gold questions to con-tinue with the task. Our task was not large enough to attract problems of collusion, but more lucrative or long-running tasks may require continual gen-eration of new gold questions in order to combat sharing of answers among workers (Oleson et al., 2011). Techniques such as expectation maximiza-tion (Dawid and Skene, 1979) can be used to pro-duce new gold questions based on worker answers. 3.3 Motivational Feedback We want workers to stay motivated, so our crowd-sourcing system also provides feedback to work-ers. In particular, workers receive adaptive per-batch message feedback at the end of each batch of ques-tions (every 20 questions) about how well they did on the gold questions in the past batches, how much they have earned so far, and a reminder of the bonus for finishing all 10 batches. We paid workers $0.50 for each batch of 20 questions with a bonus of $1.00 for finishing 10 batches. In this section, we address the following questions:  X  Does Gated Instruction produce training data  X  Does higher quality crowdsourced training data  X  How does the boost in extractor performance  X  How does extractor performance increase with  X  What X  X  the most cost-effective way to aggre-4.1 Quality of Gated Instruction Training We took the best training set of 10,000 instances from Angeli et al. X  X  2014 system that selected train-ing instances using active learning (their Sample JS data). In order to focus on the effect of crowdsourc-ing, we restricted our attention to four distinct re-lations between person and location that were used by previous researchers: nationality , place of birth , place of residence , and place of death 2 . We then sent these sentences to crowdsourced workers using the Gated Instruction protocol.

To evaluate the crowdsourced training data qual-ity, we hand-tagged the crowdsourced annotations from both our Gated Instruction system and Angeli et al. X  X  work on 200 random instances. Annotations were considered correct if they followed the TAC-KBP annotation guidelines. Two authors tagged the sample with 87% agreement and then reconciled opinions to agree on consensus labels.

The training precision, recall, and F1 are shown in Figure 4. In this and all other experiments, aggre-gate statistics are macro-averaged across relations. We also include the training quality from Zhang et al., although this is on a different set of sentences and only for place of birth , place of residence , and place of death .

Our Gated Instruction protocol gives higher F1 for the training set of each of the four relations we com-pared with Angeli X  X  crowdsourcing on the same sen-tences. Our overall F1 was 0.77, compared to 0.58 for Angeli et al. and 0.24 for Zhang et al. The differ-ence in precision is most dramatic, with our system achieving 0.77 compared to 0.50 and 0.15.
 Worker agreement with GI was surprisingly high. Two workers agreed on between 78% to 97% of the instances, depending on the relation. The average agreement was 88%. The data is available for re-4.2 Integrating Crowdsourced Data with the The pipeline of our relation extraction system is as follows. First we collected sentences of training data from the TAC-KBP newswire corpus that contain a person and a location according to the Stanford NER tagger (Finkel et al., 2005). We represent them us-ing the features described by Mintz et al. (2009). These features include NER tags of the two argu-ments, the dependency path between two designated arguments, the sequence of tokens between the ar-guments, and the order of the arguments in the sen-tence.

We then split the data into 700K used for distant supervision and much smaller sets for crowdsourc-ing and for a held-out test set. For the experiments presented, unless otherwise noted, we used a variant of majority vote to create a training set. We obtained annotations from two workers for each example sen-tence and kept the instances where both agreed as our training data.

Finally, we ran a learning algorithm on the distant supervision training data, the crowdsourced training data, and a combination of the two. The results were evaluated on the hand-labeled test set. 4.3 Effect of Data Quality on Extractor We now study how the higher quality training data from our crowdsourcing protocol affects extractor performance, when it is added to a large amount of distantly-supervised data.

We compared adding the 10K crowdsourced in-stances from the previous experiment to 700K in-stances from distant supervision, where the crowd-sourced data had tags from either Gated Instruction or the original crowdsourcing from Angeli et al. We compare only with Angeli et al. as we did not have annotations from Zhang et al. for the same training sentences.

We experimented using three learning algorithms: logistic regression, MultiR, and MIML-RE. We found that logistic regression gives the best results when applied to the crowdsourced training alone. With logistic regression, training on the 10K Sam-ple JS instances gave F1 of 0.31 with Angeli et al. X  X  crowdsourced labels and 0.40 with Gated Instruc-tion. Logistic regression is not a good fit for dis-tant supervision  X  we had F1 of 0.34 from logistic regression trained on DS only.

MultiR and MIML-RE gave the best results for combining crowdsourcing with distant supervision. Each of these multi-instance multi-class learners had similar results, so we present results only for MIML-RE in the remainder of our experiments, as it is the learning algorithm used by other researchers.
We included no special mechanisms to prevent distant supervision data from swamping the smaller amount of crowdsourced data. MIML-RE has a built-in mechanism to combine supervised and dis-tant supervision. It automatically builds a classifier from the supervised instances, uses this to initialize the distant supervision instance labels, and locks the supervised labels. With MultiR, we put the crowd-sourced instances in separate singleton  X  X ags X  of training instances, since MultiR always takes at least one instance in each bag as truth.

As Angeli et al. found, it is important to use the crowdsourced training to initialize MIML-RE. With the default initialization, Angeli et al. report no gain in F1. We found a small gain in F1 even with the default initialization, but larger gains with crowd-sourced initialization, which we use for the follow-ing experiments.

To see how much of the boost over distant su-pervision comes from the active learning that went into Angeli et al. X  X  sample JS training, we also used Gated Instruction on a randomly selected set of 10K newswire instances from the TAC KBP 2010 corpus (LDC2010E12) that contained at least one NER tag for person and one for location .

As Figure 5 shows, adding the Sample JS training with Gated Instruction crowdsourcing had a positive impact on performance, increasing precision from 0.40 to 0.43, recall from 0.41 to 0.51, and F1 from 0.40 to 0.47. With the original crowdsourced tag-ging from Angeli et al., adding the crowdsourced instances actually caused a small drop in precision, a smaller gain in recall than Gated Instruction, and F1 of 0.43  X  substantially less than achieved with labels from Gated Instruction.

Furthermore, in an apples-to-oranges comparison, we found that our improved crowdsourcing proto-col had a much bigger impact than Angeli et al. X  X  active learning mechanism. Adding 10K randomly selected newswire instances tagged with Gated In-struction gave higher precision (0.43), recall (0.51), and F1 (0.46) than adding instances selected by ac-tive learning (Sample JS) when labeled using Angeli et al. X  X  protocol. In fact Gated Instruction gave dou-ble the improvement (6 points gain in F1 vs. 3). Of course, both of these numbers are small  X  bigger gains come from using the techniques together, and especially from using more crowdsourced data. 4.4 Effect of Data Quantity on Extractor Zhang et al. reported negligible improvement in F1 from adding 20K instances with their crowdsourc-ing to distant supervision, and Angeli et al. reported a gain of 0.04 F1 from adding 10K instances with active learning and their crowdsourcing.
 As Figure 6 shows, Gated Instruction can raise F1 from 0.40 to 0.60 over distant supervision alone from adding 20K random newswire instances. This experiment uses all five relations that we crowd-sourced, adding travel to to the relations from Fig-ure 5 that we had in common with Angeli et al. The results for DS only and 10K random instances are not significantly different from those in Figure 5 in which travel to was omitted. 4.5 Comparison between Ways to Aggregate In this section we explore the cost-effectiveness of alternate methods of creating training from Gated Instruction annotations. We compare a policy of us-ing the majority vote of two out of three, or three out of five workers, and so forth, as opposed to solic-iting a single annotation for each training sentence (unilabeling). Lin et al. (2014) show that in many settings, unilabeling is better because some classi-fiers are able to learn more accurately with a larger, noisier training set than a smaller, cleaner one.
With a given budget, single annotation gives three times as many training instances as the policy that uses three votes and five times as many as the policy that requires five votes, and so forth. Is the quality of data produced by Gated Instruction high enough to rely on just one annotation per instance?
We randomly select 2K examples from the 20K newswire instances and use Gated Instruction to ac-quire labels from 10 workers for each sentence. Fig-ure 7 shows that when training a logistic regression classifier with high quality crowdsourcing data, a single annotation is, indeed, more cost effective than using a simple majority of three, five, or more anno-tations (given a fixed budget). The learning curves in Figure 7 use uncertainty sampling (US) to select examples from the 2000 available with the curves la-beled US 1/1 for single votes, US 2/3 for two out of three, and so forth.

This is not to say that a single vote is always the best policy. It is another example of the impact of GI X  X  high quality annotation. In the same domain of relation extraction, Lin et al. (2016) also show that with a more intelligent and dynamic relabeling policy, relabeling certain examples can still help. This paper describes the design of Gated Instruc-tion, a crowdsourcing protocol that produces high quality training data. GI uses an interactive tuto-rial to teach the annotation task, provides feedback during training so workers understand their errors, refuses to let workers annotate new sentences until they have demonstrated competence, and adaptively screens low-accuracy workers with a schedule of test questions. While we demonstrate GI for the task of relation extraction, the method is general and may improve annotation for many other NLP tasks.
Higher quality training data produces higher ex-tractor performance for a variety of learning algo-rithms: logistic regression, MultiR, and MIML-RE. Contrary to past claims, augmenting distant supervi-sion with a relatively small amount of high-quality crowdsourced training data gives a sizeable boost in performance. Adding 10K instances that Angeli et al. selected by active learning, annotated with Gated Instruction, raised F1 from 0.40 to 0.47  X  substan-tially higher than the 0.43 F1 provided by Angeli et al. X  X  annotations. We also find that Gated Instruction is more effective than a complicated active learning strategy. Adding 10K randomly selected instances raises F1 to 0.46, and adding 20K random instances gave F1 of 0.60.

Our experimental results yield two main takeaway messages. First, we show that in contrast to prior work, adding crowdsourced training data substan-tially improves the performance of the resulting ex-tractor as long as care is taken to ensure high quality crowdsourced annotations. We haven X  X  yet experi-mented beyond person-location relations, but we be-lieve that Gated Instruction is generalizable, partic-ularly where there are clear criteria to be taught. We believe that Gated Instruction can greatly improve training data for other NLP tasks beside relation ex-traction as well.

Second, we provide practical and easily insti-tuted guidelines for a novel crowdsourcing proto-col, Gated Instruction, as an effective method for ac-quiring high-quality training data. It X  X  important to break complex annotation guidelines into small, di-gestible chunks and to use tests (gates) to ensure that the worker reads and understands each chunk of the instructions before work begins. Without these ex-tra checks, many poor workers pass subsequent gold tests by accident, polluting results.

