 In this paper we address the issue of learning a ranking model for search result diversification. In the task, a model concerns with both query-document relevance and document diversity is automatically created with training data. Ide-ally a diverse ranking model would be designed to meet the criterion of maximal marginal relevance, for selecting doc-uments that have the least similarity to previously selected documents. Also, an ideal learning algorithm for diverse ranking would train a ranking model that could directly optimize the diversity evaluation measures with respect to the training data. Existing methods, however, either fail to model the marginal relevance, or train ranking models by minimizing loss functions that loosely related to the eval-uation measures. To deal with the problem, we propose a novel learning algorithm under the framework of Perceptron, which adopts the ranking model that maximizes marginal relevance at ranking and can optimize any diversity evalu-ation measure in training . The algorithm, referred to as PAMM (Perceptron Algorithm using Measures as Margins), first constructs positive and negative diverse rankings for each training query, and then repeatedly adjusts the model parameters so that the margins between the positive and negative rankings are maximized. Experimental results on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Retrieval Models Algorithms search result diversification; maximal marginal relevance; di-rectly optimizing evaluation measures c  X 
It has been widely observed that users X  information needs, described by keyword based queries, are often ambiguous or multi-faceted. It is important for commercial search engines to provide search results which balance query-document rel-evance and document diversity, called search result diver-sification [1, 30]. One of the key problems in search result diversification is ranking, specifically, how to develop a rank-ing model that can sort documents based on their relevance to the given query as well as the novelty of the information in the documents.

Methods for search result diversification can be catego-rized into heuristic approaches and learning approaches. The heuristic approaches construct diverse rankings with hand-crafted ranking rules. As a representative method in the category, Carbonell and Goldstein [2] propose the maximal marginal relevance (MMR) criterion for guiding the con-struction ranking models. In MMR, constructing of a diverse ranking is formulated as a process of sequential document selection. At each iteration, the document with the highest marginal relevance is selected. The marginal relevance can be defined as, for example, a linear combination of the query-document relevance and the maximum distance of the docu-ment to the selected document set. A number of approaches have been proposed [8, 23, 24, 25] on the basis of the crite-rion and promising results have been achieved. User studies also shows that the user browsing behavior matches very well with the maximal marginal relevance criterion: usually users browse the web search results in a top-down manner, and perceive diverse information from each individual doc-ument based on what they have obtained in the preceding maximal marginal relevance has been widely accepted as a criterion for guiding the construction of diverse ranking models.

Recently, machine learning approaches have been proposed for the task of search result diversification [14, 20, 22, 29, 31], especially the methods that can directly optimize evaluation measures on training data [16, 28]. Yue and Joachims [28] propose SVM-DIV which formulates the task as a problem of structured output prediction. In the model, the measure of subtopic diversity is directly optimized under the struc-tural SVM framework. Liang et al. [16] propose to conduct personalized search result diversification via directly opti-mizing the measure of  X  -NDCG, also under the structural SVM framework. All of these methods try to resolve the mis-match between the objective function used in training and the final evaluation measure used in testing. Experimen-tal results also showed that directly optimizing the diversity evaluation measures can indeed improve the diverse ranking performances [16, 28]. One problem with the direct opti-mization approaches is that it is hard, if not impossible, to define a ranking model that can meet the maximal marginal relevance criterion under the direct optimization framework.
In this paper, we aim to develop a new learning algorithm that utilizes the maximal marginal relevance model for rank-ing as well as can directly optimize any diversity evaluation measure in training. Inspired by the work of R-LTR [31] and Perceptron variations [7, 15], we propose a new algorithm for search result diversification, referred to as PAMM (Percep-tron Algorithm using Measures as Margins). PAMM utilizes a sequential document selection process as its ranking model. In learning, it first generates positive rankings (ground truth rankings) and negative rankings for the training queries. It then repeats the process of estimating the probabilities for the rankings, calculating the margins between the positive rankings and negative rankings in terms of the ranking prob-abilities, and updating the model parameters so that the margins are maximized. We show that PAMM algorithm minimizes an upper bound of the loss function that directly defined over the diversity evaluation measures.

PAMM offers several advantages: 1) adopting the ranking model that meets the maximal marginal relevance criterion; 2) ability to directly optimize any diversity evaluation mea-sure in training; 3) ability to use both positive rankings and negative rankings in training.

To evaluate the effectiveness of PAMM, we conducted extensive experiments on three public TREC benchmark datasets. The experimental results showed that our methods significantly outperform the state-of-the-art diverse ranking approaches including MMR, SVM-DIV, and R-LTR. We an-alyzed the results and showed that PAMM makes a good balance between the relevance and diversity via maximizing marginal relevance in ranking. We also showed that by di-rectly optimizing a measure in training, PAMM can indeed enhance the ranking performances in terms of the measure.
The rest of the paper is organized as follows. After a sum-mary of related work in Section 2, we describe the general framework of learning maximal marginal relevance model in Section 3. In Section 4 we discuss the proposed PAMM al-gorithm. Experimental results and discussions are given in Section 5. Section 6 concludes this paper and gives future work.
Methods of search result diversification can be categorized into heuristic approaches and learning approaches.
It is a common practice to use heuristic rules to construct a diverse ranking list in search. Usually, the rules are cre-ated based on the observation that in diverse ranking a doc-ument X  X  novelty depends on not only the document itself but also the documents ranked in previous positions. Carbonell and Goldstein [2] propose the maximal marginal relevance criterion to guide the design of diverse ranking models. The criterion is implemented with a process of iteratively select-ing the documents from the candidate document set. At each iteration, the document with the highest marginal rel-evance score is selected, where the score is a linear combi-nation of the query-document relevance and the maximum distance of the document to the documents in current re-sult set. The marginal relevance score is then updated in the next iteration as the number of documents in the result set increases by one. More methods have been developed under the criterion. PM-2 [8] treats the problem of finding a diverse search result as finding a proportional representa-tion for the document ranking. xQuAD [25] directly models different aspects underlying the original query in the form of sub-queries, and estimates the relevance of the retrieved documents to each identified sub-query. See also [3, 9, 10, 11, 21]
Heuristic approaches rely on the utility functions that can only use a limited number of ranking signals. Also, the parameter tuning cost is high, especially in complex search settings. In this paper, we propose a learning approach to construct diverse ranking models that can meet the maximal marginal relevance criterion.
Methods of machine learning have been applied to search result diversification. In the approaches, rich features can be utilized and the parameters are automatically estimated from the training data. Some promising results have been obtained. For example, Zhu et al. [31] proposed the rela-tional learning to rank model (R-LTR) in which the diverse ranking is constructed with a process of sequential document selection. The training of R-LTR amounts to optimizing the likelihood of ground truth rankings. More work please refer to [14, 20, 22, 29]. All these methods, however, formulate the learning problem as optimizing loss function that loosely related to diversity evaluation measures.

Recently methods that can directly optimize evaluation measures have been proposed and applied to search result diversification. Yue and Joachims [28] formulate the task of constructing a diverse ranking as a problem of predict-ing diverse subsets. Structural SVM framework is adopted to perform the training. Liang et al. [16] propose to con-duct personalized search result diversification, also under the structural SVM framework. In the model, the loss func-tion is defined based on the diversity evaluation measure of  X  -NDCG. Thus, the algorithm can be considered as di-rectly optimizing  X  -NDCG in training. One issue with the approach is that it is hard to learn a maximal marginal rel-evance model under the structural SVM framework.

In this paper, we propose a Perceptron algorithm that can learn a maximal marginal relevance model, at the same time directly optimizing diversity evaluation measures.
We first describe the general framework of learning max-imal marginal relevance model for search result diversifica-tion.
Suppose that we are given a query q , which is associated with a set of retrieved documents X = { x 1 ,  X  X  X  , x M } , where each document x i is represented as a D -dimensional rele-vance feature vector. Let R = R M  X  M  X  K denotes a 3-way tensor representing relationship among the M documents, where R ijk stands for the k -th relationship feature of docu-ment x i and document x j . Algorithm 1 Ranking via maximizing marginal relevance Input: documents X , document relation R , and ranking Output: ranking y 1: S 0  X   X  2: for r = 1 ,  X  X  X  ,M do 5: end for 6: return y
The maximal marginal relevance model creates a diverse ranking over X with a process of sequential document selec-tion. At each step, the document with the highest marginal Specifically, let S  X  X be the set of documents have been selected for query q at one of the document selection step. Given S , the marginal relevance score of each document x i  X  X \ S at current step is defined as a linear combina-tion of the query-document relevance and diversity of the document to the documents in S : where x i denotes the relevance feature vector of the docu-ment, R i  X  R M  X  K is the matrix representation of the re-lationship between document x i and the other documents (note that R ij  X  X  K denotes the relationship feature vector of document pair ( x i , x j )), and  X  r and  X  d are the weights for the relevance features and diversity features, respectively. The first term in Equation (1) represents the relevance of document x i to the query and the second term represents the diversity of x i w.r.t. documents in S . Following the practice in [31], the relational function h S ( R i ) is defined as the minimal distance:
According to the maximal marginal relevance criterion, sequential document selection process can be used to create a diverse ranking, as shown in Algorithm 1. Specifically, given a query q , the retrieved documents X , and document relationship R , the algorithm initializes S 0 as an empty set. It then iteratively selects the documents from the candi-date set. At iteration r ( r = 1 ,  X  X  X  ,M ), the document with the maximal marginal relevance score f S r  X  1 is selected and ranked at position r . At the same time, the selected docu-ment is inserted to S r  X  1 .
Machine learning approaches can be used to learn the maximal marginal relevance model. Suppose we are given N labeled training queries { ( X ( n ) ,R ( n ) ,J ( n ) ) } denotes the human labels on the documents, in the form of a binary matrix. J ( n ) ( i,s ) = 1 if document x ( n ) i contains the s -th subtopic of q n and 0 otherwise 1 . The learning process, thus, amounts to minimize the loss over all of the training queries:
Some datasets also use graded judgements. In this paper, we assume that all labels are binary.
 where  X  y ( n ) is the ranking constructed by the maximal marginal relevance model (Algorithm 1) for documents X ( n ) , and dicted ranking y ( n ) compared with the human labels J ( n )
In search result diversification, query level evaluation mea-sures are used to evaluate the  X  X oodness X  of a ranking model. These measures include  X  -NDCG [5], ERR-IA [4], and NRBP [6] etc. We utilize a general function E ( X, y ,J )  X  [0 , 1] to rep-resent the evaluation measures. The first argument of E is the set of candidate documents, the second argument is a ranking y over documents in X , and the third argument is the human judgements. E measures the agreement between y and J .

As an example of diversity evaluation measures,  X  -NDCG [5] is a variation of NDCG [13] in which the newly found subtopics are rewarded and redundant subtopics are penalized. The  X  -NDCG score at rank k can be defined by replacing the raw gain values in standard NDCG@ k with novelty-baised gains: where NG ( r ) = P s J ( y ( r ) ,s )(1  X   X  ) C s ( r  X  1) biased gain at rank r in ranking y , C s ( r  X  1) = P r  X  1 denotes the number of documents observed within top r  X  1 that contain the s -th subtopic, NG  X  ( r ) is the novelty-biased gain at rank r in a positive ranking, and y ( k ) denotes the index of the document ranked at k . Usually the parameter  X  is set to 0.5.

ERR-IA [4] is another popular used diversity evaluation measure. Given a query with several different subtopics s , the probability of each intent Pr( s | q ) can be estimated, where P s Pr( s | q ) = 1. The intent-aware ERR at rank k can be computed as: where ERR@ k ( s ) is the expected reciprocal rank score at k in terms of subtopic s .

Table 1 gives a summary of the notations described above.
We aim to maximize the diverse ranking accuracy in terms of a diversity evaluation measure on the training data. Thus, the loss function in Equation (2) becomes convex function.

We resort to optimize the upper bound of the loss func-tion under the framework of structured output prediction. According to Theorem (2) in [27], we know that the loss function defined in Equation (5) can be upper bounded by the function defined over the ranking pairs: where Y +( n ) is the set of all possible  X  X ositive X  rankings (rankings whose  X  -NDCG/ERR-IA equals to one) for the n -th query, Y  X  ( n ) is the set of all possible  X  X egative X  rank-ings (rankings whose  X  -NDCG/ERR-IA is less than one) for the n -the query, J  X  K is one if the condition is satisfied other-wise zero, and F ( X,R, y ) is the query level ranking model. F takes the document set X , document relationship R , and ranking over the document y as inputs. The output of F is the confidence score of the ranking y . The predicted  X  y in Equation (5) can be considered as the ranking that max-imizes F : where Y ( n ) is the set of all possible rankings over X ( n ) F is defined as the probability of generating the ranking list y with a process of iteratively selecting the top ranked documents from the remaining documents, and using the marginal relevance function f S in Equation (1) as the selec-tion criterion: where y ( r ) denotes the index of the document ranked at ranked at the top r  X  1 positions in y , f S r  X  1 ( x i ,R marginal relevance score of document x i w.r.t. the selected documents in S r  X  1 , and S 0 =  X  is an empty set. With the definition of F , it is obvious that the maximal marginal relevance process of Algorithm 1 actually greedily searches the solution for optimizing the problem of Equation (7).
To conduct the optimization under the Perceptron frame-work, the upper bound of Equation (6) is further relaxed, by replacing the max with sum and moving the term The upper bound of Equation (6) becomes:
X This is because P i x i  X  max i x i if x i  X  0 holds for all i , and J x  X  y  X  z K  X  z  X  J x  X  y K holds if z  X  [0 , 1]. Please note that we assume E ( X, y + ,J )  X  [0 , 1] and thus
The loss function in Equation (9) can be optimized un-der the framework of Perceptron. In this paper, inspired by the work of structured Perceptron [7] and Perceptron algo-rithm with uneven margins [15], we have developed a novel learning algorithm to optimize the loss function in Equation (9). The algorithm is referred to as PAMM and is shown in Algorithm 2.
 PAMM takes a training set { ( X ( n ) ,R ( n ) ,J ( n ) ) } put and takes the diversity evaluation measure E , learn-ing rate  X  , number of positive rankings per query  X  + , and number of negative rankings per query  X   X  as parameters. For each query q n , PAMM first generates  X  + positive rank-ings PR ( n ) and  X   X  negative rankings NR ( n ) (line (2) and line (3)). PR ( n ) and NR ( n ) play as the random samples of Y +( n ) and Y  X  ( n ) , respectively. PAMM then optimizes the model parameters  X  r and  X  d iteratively in a stochastic man-ner over the ranking pairs: at each round, for each pair be-tween a positive ranking and a negative ranking ( y + , y the gap of these two rankings in terms of the query level ranking model  X  F = F ( X,R, y + )  X  F ( X,R, y  X  ) is calcu-lated based on current parameters  X  r and  X  d (line (9)). If  X  F is smaller than the margin in terms of evaluation mea-sure  X  E = E ( X, y + ,J )  X  E ( X, y  X  ,J ) (line (10)), the model parameters will be updated so that  X  F will be enlarged (line (11) and line (12)). The iteration continues until conver-gence. Finally, PAMM outputs the optimized model param-eters (  X  r , X  d ).

Next, we will explain the key steps of PAMM in detail.
In PAMM, it is hard to directly conduct the optimization over the sets of positive rankings Y +( n ) and negative rank-ings Y  X  ( n ) , because in total these two sets have M ! rankings if the candidate set contains M documents. Thus, PAMM samples the rankings to reduce the training time.

For each training query, PAMM first samples a set of posi-tive rankings. Algorithm 3 illustrates the procedure. Similar to the online ranking algorithm shown in Algorithm 1, the positive rankings are generated with a sequential document selection process and the selection criteria is the diversity evaluation measure E . After generating the first positive ranking y (1) , the algorithm constructs other positive rank-ings based on y (1) , by randomly swapping the positions of two documents whose subtopic coverage are identical.
For each training query, PAMM also samples a set of neg-ative rankings. Algorithm 4 shows the procedure. The algo-rithm simply generates random rankings iteratively. If the generated ranking is not a positive ranking and satisfies the Algorithm 2 The PAMM Algorithm Output: model parameters (  X  r , X  d ) 1: for n = 1 to N do 2: PR ( n )  X  PositiveRankings( X ( n ) ,J ( n ) ,E, X  + ) { Algo-3: NR ( n )  X  NegativeRankings( X ( n ) ,J ( n ) ,E, X   X  ) { Algo-4: end for 5: initialize {  X  r , X  d } X  random values in [0 , 1] 6: repeat 7: for n = 1 to N do 8: for all { y + , y  X  } X  PR ( n )  X  NR ( n ) do 11: calculate  X   X  ( n ) r and  X   X  ( n ) d { Equation (10) 12: (  X  r , X  d )  X  (  X  r , X  d ) +  X   X  (  X   X  ( n ) r ,  X   X  13: end if 14: end for 15: end for 16: until convergence 17: return (  X  r , X  d ) user predefined constraints (e.g,  X  -NDCG@20  X  0 . 8), the ranking will be added into the ranking set NR .
 Please note that in some extreme cases Algorithm 3 and Algorithm 4 cannot create enough rankings. In our imple-mentations, the algorithms are forced to return after running enough iterations.
Given a ranking pair ( y + , y  X  )  X  PR ( n )  X  NR ( n ) , PAMM updates  X  r and  X  d as if F ( X,R, y + )  X  F ( X,R, y  X  )  X  E ( X, y + ,J )  X  E ( X, y The goal of the update is to enlarge the margin between y and y  X  in terms of query level model:  X  F = F ( X,R, y + F ( X,R, y  X  ). For convenience of calculation, we resort to the problem of because F ( X,R, y ) &gt; 0 and log(  X  ) is a monotonous increas-ing function. Thus,  X   X  r can be calculated as the gradient: Algorithm 3 PositiveRankings Input: documents X , diversity labels J , evaluation mea-Output: positive rankings PR 1: for r = 1 to | X | do 4: end for 5: PR  X  X  y (1) } 6: while | PR | &lt;  X  + do 7: y  X  y (1) 8: ( k,l )  X  randomly choose two documents whose hu-9: y ( k )  X  y ( l ) { swap documents at rank k and l } 10: if y /  X  PR then 11: PR  X  PR  X  X  y } 12: end if 13: end while 14: return PR Algorithm 4 NegativeRankings Input: documents X , diversity labels J , evaluation mea-Output: NR 1: NR =  X  2: while | NR | &lt;  X   X  do 3: y  X  random shuffle (1 ,  X  X  X  , | X | ) 4: if y /  X  NR and E ( X, y ,J ) is as expected then 5: NR  X  NR  X  X  y } 6: end if 7: end while 8: return NR where =
Similarly,  X   X  d can be calculated as where  X  log F ( X,R, y ) Intuitively, the gradients  X   X  r and  X   X  d are calculated so that the line 12 of Algorithm 2 will increase F ( X,R, y and decrease F ( X,R, y  X  ).
We analyzed time complexity of PAMM. The learning pro-cess of PAMM (Algorithm 2) is of order O ( T  X  N  X   X  +  X   X  Table 2: Statistics on WT2009, WT2010 and WT2011.
 M 2  X  ( D + K )), where T denotes the number of iterations, N the number of queries in training data,  X  + the number of positive rankings per query,  X   X  the number of negative rankings per query, M the maximum number of documents for queries in training data, D the number of relevance fea-tures, and K the number of diversity features. The time complexity of online ranking prediction (Algorithm 1) is of order O ( M 2 ( D + K )).

PAMM is a simple yet powerful learning algorithm for search result diversification. It has several advantages com-pared with the existing learning methods such as R-LTR [31], SVM-DIV [28], and structural SVM [26].
 First, PAMM employs a more reasonable ranking model. The model follows the maximal marginal relevance criterion and can be implemented with a process of sequential docu-ment selection. In contrast, structural SVM approaches [26] calculate all of the ranking scores within a single step, as that of in relevance ranking. The marginal relevance of each doc-ument cannot be taken into consideration at ranking time.
Second, PAMM can incorporate any diversity evaluation measure in training, which makes the algorithm focus on the specified measure when updating the model parameters. In contrast, R-LTR only minimizes loss function that is loosely related to diversity evaluation measures and SVM-DIV is trained to optimize the subtopic coverage.

Third, PAMM utilizes the pairs between the positive rank-ings and the negative rankings in training, which makes it possible to leverage more information in training. Specif-ically, it enables PAMM algorithm to enlarge the margins between the positive rankings and negative rankings when updating the parameters. In contrast, R-LTR only uses the information in the positive rankings and the training is aimed to maximizing the likelihood. We conducted experiments to test the performances of PAMM using three TREC benchmark datasets for diversity tasks: TREC 2009 Web Track (WT2009), TREC 2010 Web Track (WT2010), and TREC 2011 Web Track (WT2011). Each dataset consists of queries, corresponding retrieved documents, and human judged labels. Each query includes several subtopics identified by TREC assessors. The docu-ment relevance labels were made at the subtopic level and the labels are binary 2 . Statistics on the datasets are given in Table 2.
 All the experiments were carried out on the ClueWeb09 Category B data collection 3 , which comprises of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied
WT2011 has graded judgements. In this paper we treat them as binary. http://boston.lti.cs.cmu.edu/data/clueweb09 to the documents as preprocessing. We conducted 5-fold cross-validation experiments on the three datasets. For each dataset, we randomly split the queries into five even subsets. At each fold three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the five trials.

As for evaluation measures,  X  -NDCG@ k (Equation (3)) with  X  = 0 . 5 and k = 20 is used. We also used ERR-IA@ k (Equation (4)) with k = 20 to evaluate the performances.
We compared PAMM with several types of baselines. The baselines include the conventional relevance ranking models in which document diversity is not taken into consideration. Query likelihood (QL) [18] language models for informa-ListMLE [17] a representative learning-to-rank model for
We also compared PAMM with three heuristic approaches to search result diversification in the experiments. MMR [2] a heuristic approach to search result diversifica-xQuAD [25] a representative heuristic approach to search PM-2 [8] another widely used heuristic approach to search Please note that these three baselines require a prior rele-vance function to implement their diversification steps. In our experiments, ListMLE was chosen as the relevance func-tion.

Learning approaches to search result diversification are also used as baselines in the experiments.
 SVM-DIV [28] a representative learning approach to search Structural SVM [26] Structural SVM can be configured R-LTR [31] a state-of-the-art learning approach to search
As for features, we adopted the features used in the work of R-LTR [31]. There are two types of features: the rele-vance features which capture the relevance information of a query with respect to a document, and the diversity features which represent the relation information among documents. Table 3 and Table 4 list the relevance features and diversity features used in the experiments, respectively. http://www.dmoz.org Table 3: Relevance features used in the experiments. The first 4 lines are query-document matching fea-tures, each applied to the fields of body, anchor, title, URL, and the whole documents. The latter 3 lines are document quality features. [31] Table 4: The seven diversity features used in the ex-periments. Each feature is extracted over two doc-uments. [31]
In the experiments, we made use of the benchmark datasets of WT2009, WT2010, and WT2011 from the TREC Web Track, to test the performances of PAMM.

PAMM has to tune some parameters. The learning rate parameter  X  was tuned based on the validation set during each experiment. In all of the experiments in this sub-section, we set the number of positive rankings per query  X  + = 5, and number of negative rankings per query  X   X  = 20. As for the parameter E of PAMM,  X  -NDCG@20 and ERR-IA@20 were utilized. The results for PAMM using  X  -NDCG@20 in training are denoted as PAMM(  X  -NDCG). The PAMM results using ERR-IA@20 as measures are de-noted as PAMM(ERR-IA).

The experimental results on WT2009, WT2010, and WT2011 are reported in Table 5, Table 6, and Table 7, respectively. Numbers in parentheses are the relative improvements com-pared with the baseline method of query likelihood (QL). Boldface indicates the highest score among all runs. From the results, we can see that PAMM(  X  -NDCG) and PAMM(ERR-IA) outperform all of the baselines on all of the three datasets in terms of both  X  -NDCG@20 and ERR-IA@20. We con-ducted significant testing (t-test) on the improvements of PAMM(  X  -NDCG) over the baselines in terms of  X  -NDCG@20 and ERR-IA@20. The results indicate that all of the im-provements are statistically significant (p-value &lt; 0 . 05). We also conducted t-test on the improvements of PAMM(ERR-IA) over the baselines in terms of  X  -NDCG@20 and ERR-IA@20. The improvements are also statistically significant. All of the results show that PAMM is effective for the task of search result diversification.
 We observed that on all of the three datasets, PAMM(  X  -NDCG) trained with  X  -NDCG@20 performed best in terms of  X  -NDCG@20 while PAMM(ERR-IA) trained with ERR-IA@20 performed best in terms of ERR-IA@20. The results indicate that PAMM can enhance diverse ranking perfor-mances in terms of a measure by using the measure in train-ing. We will further discuss the phenomenon in next section. Table 5: Performance comparison of all methods in official TREC diversity measures for WT2009.
 Table 6: Performance comparison of all methods in official TREC diversity measures for WT2010.
 Table 7: Performance comparison of all methods in official TREC diversity measures for WT2011.

We conducted experiments to show the reasons that PAMM outperforms the baselines, using the results of the WT2009 dataset as examples.
We found that PAMM makes a good tradeoff between the query-document relevance and document diversity via max-imizing marginal relevance. Here we use the result with regard to query number 24 ( X  X iversity X  which contains 4 subtopics), to illustrate why our method is superior to the baseline method of Structural SVM trained with  X  -NDCG@20 (denoted as StructSVM(  X  -NDCG)). Note that structural Figure 1: Example rankings from WT2009. Each shaded block represents a document and the num-ber(s) in the block represent the subtopic(s) covered by the document.
 SVM cannot leverage the marginal relevance in its ranking model. Figure 1 shows the top five ranked documents by StructSVM(  X  -NDCG), as well as four intermediate rankings generated by PAMM(  X  -NDCG) (denoted as f S 0 ,f S 1 ,f and f S 3 ). The ranking denoted as f S r is generated as: first sequentially selecting the documents for ranking positions of 1 , 2 ,  X  X  X  ,r  X  1 with models f S 0 ,f S 1 ,  X  X  X  ,f S r  X  2 then ranking the remaining documents with f S r  X  1 . For ex-ample, the intermediate ranking denoted as f S 2 is generated as: selecting one document with f S 0 and setting it to rank 1, then selecting one document with f S 1 and set it to rank 2, and finally ranking the remaining documents with f S 2 and putting them to the tail of the list. Each of the shaded block indicates a document and the number(s) in the block indi-cates the subtopic(s) assigned to the document by the hu-man annotators. The performances in terms of  X  -NDCG@5 are also shown in the last column. Here we used  X  -NDCG@5 because only the top 5 documents are shown.

The results in Figure 1 indicate the effectiveness of the maximal marginal relevance criterion. We can see that the  X  -NDCG@5 increases steadily with the increasing rounds of document selection iterations. In the first iteration, f selects the most relevant document and puts it to the first position, without considering the diversity. Thus, the  X  -NDCG@5 of the ranking generated by f S 0 is lower than that of by StructSVM(  X  -NDCG). In the second iteration, the ranking function f S 1 selects the document associated with subtopics 1 and 3 and ranks it to the second position, ac-cording to the maximal marginal relevance criterion. From the view point of diverse ranking, this is obviously a better choice than StructSVM(  X  -NDCG) made, which selects the document with subtopics 1 and 4. (Note that both Struc-tural SVM and PAMM select the document with subtopics 2 and 4 for the first position.) In the following steps, f f 3 select documents for ranking positions of 3 and 4, also following the maximal marginal relevance criterion. As a re-sult, f S 1 ,f S 2 , and f S 3 outperforms StructSVM(  X  -NDCG).
We conducted experiments to see whether PAMM has the ability to improve the diverse ranking quality in terms of a measure by using the measure in training. Specifically, we trained models using  X  -NDCG@20 and ERR-IA@20 and F igure 2: Performance in terms of  X  -NDCG@20 when model is trained with  X  -NDCG@20 or ERR-IA@20. F igure 3: Performance in terms of ERR-IA@20 when model is trained with  X  -NDCG@20 or ERR-IA@20. evaluated their accuracies on the test dataset in terms of both  X  -NDCG@20 and ERR-IA@20. The experiments were conducted for each fold of the cross validation and perfor-mances on each fold are reported. Figure 2 and Figure 3 show the results in terms of  X  -NDCG@20 and ERR-IA@20, respectively. From Figure 2, we can see that on all of the 5 folds (except fold 1), PAMM(  X  -NDCG) trained with  X  -NDCG@20 performs better in terms of  X  -NDCG@20. Sim-ilarly, from Figure 3, we can see that on all of the 5 folds (except fold 4), PAMM(ERR-IA) trained with ERR-IA@20 performs better in terms of ERR-IA@20. Similar results have also been observed in experiments on other datasets (see the results in Table 5, Table 6, and Table 7). All of the results indicate that PAMM can indeed enhance the diverse ranking quality in terms of a measure by using the measure in training.
We examined the effects of the number of positive rank-ings generated per query (parameter  X  + ). Specifically, we compared the performances of PAMM(  X  -NDCG) w.r.t. dif-ferent  X  + values. Figure 4 shows the performance curve in terms of  X  -NDCG@20. The performance of R-LTR base-F igure 4: Ranking accuracies and training time w.r.t.  X  + . F igure 5: Ranking accuracies and training time w.r.t  X  . line is also shown for reference. From the result, we can see that the curve does not change much with different  X  + val-ues, which indicates the robustness of PAMM. Figure 4 also shows training time (in hours) w.r.t. different  X  + The training time increased dramatically with large  X  + , be-cause more ranking pairs are generated for training. In our experiments  X  + was set to 5.

We further examined the effect of the number of nega-tive rankings per query (parameter  X   X  ). Specifically, we compared the performances of PAMM(  X  -NDCG) w.r.t. dif-ferent  X   X  and the results are shown in Figure 5. From the results, we can see that the performance of PAMM increas-ing steadily with the increasing  X   X  values until  X   X  which indicates that PAMM can achieve better ranking per-formance with more information from the negative rankings. As the cost, the training time increased dramatically, be-cause more training instances are involved in training. In our experiments,  X   X  was set to 20.

We also conducted experiments to show the effect of sam-pling the negative rankings with different  X  -NDCG values. Specifically, in each of the experiment, we configured the Al-gorithm 4 to choose the negative rankings whose  X  -NDCG@20 values are 0.5, 0.6, 0.7, 0.8, and 0.9, respectively. Figure 6 shows the performances of PAMM(  X  -NDCG) w.r.t. different  X  -NDCG@20 values of the sampled negative rankings. From the results, we can see that PAMM performs best when the F igure 6: Ranking accuracies w.r.t. different  X  -NDCG@20 values of the negative rankings.
F igure 7: Learning curve of PAMM(  X  -NDCG).  X  -NDCG@20 of the sampled negative rankings ranges from 0.6 to 0.9. The results also indicate that PAMM is robust and not very sensitive to different methods of sampling the negative rankings.
Finally we conducted experiments to show whether PAMM can converge in terms of the diversity evaluation measures. Specifically, we showed the learning curve of PAMM(  X  -NDCG) in terms of  X  -NDCG@20 and ERR-IA@20 during the train-ing phase. At each training iteration the model parame-ters are outputted and evaluated on the test data. Figure 7 shows the performance curves w.r.t. the number of train-ing iterations. From the results, we can see that the rank-ing accuracy of that PAMM(  X  -NDCG) steadily improves in terms of both  X  -NDCG@20 and ERR-IA@20, as the training goes on. PAMM converges and returns after running about 60 iterations. We also observed that in all of our experi-ments, PAMM usually converges and returns after running 50  X  100 iterations. Similar phenomenon was also observed from the learning curve of PAMM(ERR-IA). The results in-dicates that PAMM converges fast and conducts the training efficiently.
In this paper we have proposed a novel algorithm for learn-ing ranking models in search result diversification, referred to as PAMM. PAMM makes use of the maximal marginal relevance model for constructing the diverse rankings. In training, PAMM directly optimizes the diversity evaluation measures on training queries under the framework of Percep-tron. PAMM offers several advantages: employs a ranking model that follows the maximal marginal relevance crite-rion, ability to directly optimize any diversity evaluation measure, and ability to utilize both positive rankings and negative rankings in training. Experimental results based on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods including SVM-DIV, structural SVM, and R-LTR.

Future work includes theoretical analysis on the conver-gence, generalization error, and other properties of the PAMM algorithm, and improving the efficiency of PAMM in both offline training and online prediction. This research work was funded by the 973 Program of China under Grants No. 2014CB340401, No. 2012CB316303, the 863 Program of China under Grants No. 2014AA015204, the National Natural Science Foundation of China under Grant No. 61232010, No. 61425016, No. 61173064, No. 61472401, No. 61203298, and No. 61202214.

We would like to express our gratitude to Prof. Chengx-iang Zhai who has offered us valuable suggestions in the academic studies.
