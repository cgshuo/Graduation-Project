 Wenyuan Dai  X  dwyak@apex.sjtu.edu.cn Qiang Yang  X  qyang@cse.ust.hk Gui-Rong Xue  X  grxue@apex.sjtu.edu.cn Yong Yu  X  yyu@apex.sjtu.edu.cn Clustering (Jain &amp; Dubes, 1988) aims at partition-ing objects into groups, so that the objects in the same groups are relatively similar, while the objects in different groups are relatively dissimilar. Clustering has a long history in machine learning (MacQueen, 1967), and recent works on clustering research have focused on improving the clustering performance us-ing the prior knowledge in semi-supervised clustering (Wagstaff et al., 2001) and supervised clustering (Fin-ley &amp; Joachims, 2005).
 In the past, semi-supervised clustering incorporates pairwise supervision, such as must-link or cannot-link constraints (Wagstaff et al., 2001), to bias clustering results. Supervised clustering methods learn distance functions from a small sample of auxiliary labeled data (Finley &amp; Joachims, 2005). Different from these clus-tering problems, in this paper, we address a new clus-tering task where we use a large amount of auxiliary unlabeled data to enhance the clustering performance of a small amount of target unlabeled data. In our problem, we do not have any labeled data or pairwise supervisory constraint knowledge. All we have are the auxiliary data which are totally unlabeled and may be irrelevant to the target data. Our target data consist of a collection of unlabeled data from which it may be insufficient to learn a good feature representation. Thus, applying clustering directly on these target data may give very poor performance. However, with the help of auxiliary data, we are able to uncover a good feature set to enable high quality clustering on the tar-get data.
 Our problem can be considered as an instance of trans-fer learning, which makes use of knowledge gained from one learning task to improve the performance of an-other, even when these learning tasks or domains fol-low different distributions (Caruana, 1997). However, since all the data are unlabeled, we can consider it as an instance of unsupervised transfer learning (Teh et al., 2006). This unsupervised transfer learning prob-lem could also be viewed as a clustering version of the self-taught learning (Raina et al., 2007), which uses irrelevant unlabeled data to help supervised learning. Thus, we refer to our problem as self-taught clustering (or STC for abbreviation).
 To tackle the problem, we observe that the perfor-mance of clustering highly relies on data represen-tation when the objective function and the distance measure are fixed. Therefore, to improve the clus-tering performance, one alternative way is to seek a better data representation. We observe that different objects may share some common or relevant features. For example, in Figure 1, diamond and ring share quite a lot of features about  X  X iamond X ; ring and platinum share quite a lot of features about  X  X lat-inum X ; moreover, platinum and titanium share quite a lot of features about  X  X etal X . In this situation, the auxiliary data can be used to help uncover a bet-ter data representation to benefit the target data set. Our approach to tackling this problem is by using co-clustering (Dhillon et al., 2003), so that the commonal-ity can be found in the feature spaces that corresponds to similar semantic meanings.
 In our solution to the self-taught clustering problem, two clustering operations, on the target data and the auxiliary data are respectively performed together. This is done through co-clustering. We extend the information theoretic co-clustering algorithm (Dhillon et al., 2003) which minimizes loss in mutual informa-tion before and after co-clustering. An iterative al-gorithm is proposed to monotonically reduce the ob-jective function. The experimental results show that our algorithm can greatly improve the clustering per-formance by effectively using auxiliary unlabeled data, as compared to several other state-of-the-art clustering algorithms. For clarity, we first define the self-taught clustering task. Let X and Y be two discrete random variables, taking values from two value sets { x 1 , . . . , x n } and { y 1 , . . . , y m } , respectively. X and Y correspond to the target and auxiliary data. Let Z be a discrete random variable, taking values from the value set { z 1 , . . . , z that corresponds to the common feature space of both target and auxiliary data.
 Let p ( X, Z ) be the joint probability distribution with respect to X and Z , and q ( Y, Z ) be the joint probabil-ity distribution with respect to Y and Z . In general, p ( X, Z ) and q ( Y, Z ) can be considered as two n  X  k and m  X  k matrices respectively, which can be estimated from data observations. For example, consider the case that x 1 = { z 1 , z 3 } , x 2 = { z 2 } , and x 3 = { z 2 Then, the joint probability distribution p ( X, Z ) can be estimated as We wish to cluster X into N partitions  X  X = {  X  x 1 , . . . ,  X  x N } and Y into M clusters Furthermore, Z can be clustered into K feature clus-ters  X  Z = {  X  z 1 , . . . ,  X  z K } . We use C X : X 7 X  C
Y : Y 7 X  ing functions, which map variables in the three value sets to their corresponding clusters. For brevity, in the following, we will use  X  X ,  X  Y and  X  Z to denote C X C Y ( Y ) and C Z ( Z ), respectively.
 Our objective is to find a good clustering function C X for the target data, with the help of the clusters C Y on the auxiliary data and C Z on the common feature space. In this section, we present our co-clustering based self-taught clustering (STC) algorithm, and then discuss its theoretical properties based on information theory. 3.1. Objective Function for Self-taught We extend the information theoretic co-clustering (Dhillon et al., 2003) to model our self-taught clus-tering algorithm. In the information theoretic co-clustering, the objective function of co-clustering is defined as minimizing loss in mutual information be-tween instances and features, before and after co-clustering. Formally, using the target data X and their feature space Z for illustration, the objective function can be expressed as where I (  X  ;  X  ) denotes the mutual information between two random variables (Cover &amp; Thomas, 1991) that I (  X  X,  X  Z ) corresponds to the joint probability distribu-tion p (  X  X,  X  Z ) which is defined as For example, for the joint probability p ( X, Z ) in Equa-tion (1), suppose that the clustering on X is  X  X = {  X  x 1 = { x 1 , x 2 } ,  X  x 2 = { x 3 }} , and the clustering on Z is  X  Z = {  X  z 1 = { z 1 , z 2 } ,  X  z 2 = { z 3 }} . Then, In this work, we model our self-taught clustering al-gorithm (STC) as performing co-clustering operations on the target data X and auxiliary data Y , simultane-ously, while the two co-clusters share the same features clustering  X  Z on the feature set Z . Thus, the objective function can be formulated as J = I ( X, Z )  X  I (  X  X,  X  Z ) +  X  h I ( Y, Z )  X  I (  X  In Equation (2), I ( X, Z )  X  I (  X  X,  X  Z ) is computed on the co-clusters on the target data X , while I ( Y, Z )  X  I (  X 
Y ,  X  Z ) on the auxiliary data Y .  X  is a trade-off parameter to balance the influence between the tar-get data and the auxiliary data which we will test in our experiments. From Equation (5), we can see that, although the two co-clustering objective func-tions I ( X, Z )  X  I (  X  X,  X  Z ) and I ( Y, Z )  X  I (  X  formed separately, they share the same feature cluster-ing  X  Z . This is the  X  X ridge X  to transfer the knowledge between the target and auxiliary data.
 Our remaining task is to minimize the value of the objective function in Equation (5) 1 . However, min-imizing Equation (5) is not an easy task, since it is non-convex and there are no good solutions currently to directly optimize this objective function. In the fol-lowing, we will rewrite the objective function in Equa-tion (5) into the form of Kullback-Leibler divergence (Cover &amp; Thomas, 1991) (KL divergence), and mini-mize the reformulated objective function. 3.2. Optimization for Co-clustering We first define two new probability distributions  X  p ( X, Z ) and  X  q ( Y, Z ) as follows.
 Definition 1 Let  X  p ( X, Z ) denote the joint probability distribution of X and Z with respect to the co-clusters ( C
X , C Z ) ; formally, where x  X   X  x and z  X   X  z . Therefore, with regard to Equations (1) and (4) ,  X  p ( X, Z ) is given by Likewise, let  X  q ( Y, Z ) denote the joint probability dis-tribution of Y and Z with respect to the co-clusters ( C
Y , C Z ) . We have where y  X   X  y and z  X   X  z .
 Using the probability distributions  X  p ( X, Z ) and  X  q ( Y, Z ) defined above, we can reformulate the objec-tive function in Equation (5) into a form based on KL divergence (Cover &amp; Thomas, 1991).
 Lemma 1 When the clusters C X , C Y and C Z are fixed, the objective function in Equation (5) can be re-formulated as I ( X ; Z )  X  I (  X  X ;  X  Z ) +  X  h I ( Y ; Z )  X  I (  X  where D (  X || X  ) denotes the KL divergence between two probability distributions (Cover &amp; Thomas, 1991), where D ( p || q ) = P x p ( x ) log p ( x ) q ( x ) . Proof Based on the Lemma 2.1 in (Dhillon et al., fore, Lemma 1 can be proved straightforwardly. Lemma 1 converts the loss in mutual information to the KL divergence between the distributions p and  X  p , and between q and  X  q , respectively. However, the prob-ability distributions in Lemma 1 are joint distribu-tions, and are therefore difficult to optimize. Hence, in Lemma 2, we rewrite the objective function in Lemma 1 as a conditional probability form. We then show how to optimize the objective function in the new form. Lemma 2 The KL divergence with respect to joint probability distributions can be reformulated as
D ( p ( X, Z ) ||  X  p ( X, Z )) Similarly,
D ( q ( Y, Z ) ||  X  q ( Y, Z )) Proof We only give the proof to Equation (10). Using an identical argument, Equations (11), (12) and (13) can be easily derived.
 D ( p ( X, Z ) ||  X  p ( X, Z )) = X .
 From Lemma 2 and Equation (10), we can see that the value of D ( p ( X, Z ) ||  X  p ( X, Z )) and thus can then decrease global optimization function in Equation (9). Therefore, if we iteratively choose the best cluster  X  x for function will be minimized monotonically. Formally, Using a similar argument on Y and Z , we have and Based on Equation (14), (15) and (16), an alternative way to minimize the objective function in Equation (9) is derived, as shown in Algorithm 1.
 In Algorithm 1, in each iteration, our self-taught clus-tering algorithm (STC) minimizes the objective func-tion by choosing the best  X  x ,  X  y and  X  z for each x , y and Algorithm 1 The Self-taught Clustering Algorithm: STC Input: A target unlabeled data set X ; an auxiliary unlabeled data set Y ; the feature space Z shared by both X and Y ; the initial clustering functions C (0) X , C
Y and C Output: The final clustering function C ( T ) X on the target data X .
 Procedure STC 1: Initialize p ( X, Z ) and q ( Y, Z ) based on the data 2: Initialize  X  p (0) ( X, Z ) based on p ( X, Z ), C (0) 3: Initialize  X  q (0) ( Y, Z ) based on q ( Y, Z ), C (0) 4: for t  X  1 , . . . , T do 5: Update C ( t ) X ( X ) based on p ,  X  p ( t  X  1) , and Equa-6: Update C ( t ) Y ( Y ) based on q ,  X  q ( t  X  1) , and Equa-7: Update C ( t ) Z ( Z ) based on p , q ,  X  p ( t  X  1) ,  X  q 8: Update  X  p ( t ) based on based on p ( X, Z ), C ( t ) 9: Update  X  q ( t ) based on based on q ( Y, Z ), C ( t ) 10: end for 11: Return C ( T ) X as the final clustering function on the z based on Equations (14), (15) and (16). As we dis-cussed above, this can reduce the value of the global objective function in Equation (9). In the following theorem, we show the monotonically decreasing prop-erty of the objective function of the STC algorithm. Theorem 1 In Algorithm 1, let the value of objective function J in the t -th iteration be Then, Proof (Sketch) Since in each iteration, the cluster-ing functions are updated based on Equations (14), (15) and (16), which locally minimize the values of jective function is monotonically non-increasing as a result. Theorem 1 follows as a consequence.
 Note that, although STC is able to minimize the ob-jective function value in Equation (9), it is only able to find a locally optimal one. Finding the global optimal solution is NP-hard. The next corollary emphasizes the convergence property of our algorithm STC. Corollary 1 Algorithm 1 converges in a finite number of iterations.
 Proof (Sketch) The convergence of our algorithm STC can be proved straightforwardly based on the monotonical decreasing property in Theorem 1, and the finiteness of the solution space. 3.3. Complexity Analysis We now analyze the computational cost of our algo-rithm STC. Suppose that the total number of ( x, z ) co-occurrences in the target data set X is L 1 , and the total number of ( y, z ) co-occurrences in the aux-iliary data set Y is L 2 . In each iteration, updating the target instance clustering C X takes O ( N  X  L 1 ). Updating the auxiliary instance clustering C Y takes O ( M  X  L 2 ). Moreover, updating the feature clustering C
Z takes O ( K  X  ( L 1 + L 2 )). Since the number of it-erations is T , the time complexity of our algorithm is O ( T  X  (( K + N )  X  L 1 + ( K + M )  X  L 2 ))). In the follow-ing experiments, it is shown that T = 10 is enough for convergence. Usually, the number of clusters N , M and K can be considered as constants, so that the time complexity of STC is O ( L 1 + L 2 ).
 Considering space complexity, our algorithm needs to store all the ( x, z ) and ( y, z ) co-occurrences and their corresponding probabilities. Thus, the space complex-ity is O ( L 1 + L 2 ). This indicates that the time com-plexity and the space complexity of our algorithm are all linear on the input. We conclude that the algorithm scales well. In this section, we evaluate our self-taught cluster-ing algorithm STC on the image clustering tasks, and show effectiveness of STC. 4.1. Data Sets We conduct our experiments on eight clustering tasks generated based on the Caltech-256 image corpus (Griffin et al., 2007). There are a total of 256 cate-gories in the Caltech-256 data set, where we randomly chose 20 categories from this corpus. For each cat-egory, 70 images are randomly selected to form our clustering tasks. Six binary clustering tasks, one 3-way clustering task, and one 5-way clustering task were generated using these 20 categories, as shown in Table 1. The first column in Table 1 presents the categories with respect to the target unlabeled data. For each clustering task, we used the data from the correspond-ing categories as target unlabeled data, while the data from the remaining categories as the auxiliary unla-beled data.
 For data preprocessing, we used the  X  X ag-of-words X  method (Li &amp; Perona, 2005) to represent images in our experiments. Interesting points in images are found and described by SIFT descriptor (Lowe, 2004). Then, we clustered all the interesting points to get the code-book, and set the number of clusters to 800. Using this codebook, each image can be represented as a vector in the subsequent learning processes. 4.2. Evaluation Criteria In these experiments, we used entropy to measure the quality of clustering results, which reveals the purity of clusters. Specifically, the entropy for a cluster  X  x c represents a category label in the evaluation cor-where ` ( x ) denotes the true label of x in the evalu-ation corpus. The total entropy for the whole clus-tering is defined as the weighted sum of the entropy with respect to all the clusters; formally, H (  X  X ) = P uated using the entropy H (  X  X ). 4.3. Empirical Analysis We compared our algorithm STC to several state-of-the-art clustering methods as baseline methods. For each baseline method considered below, we have two different options: one is to apply the baseline method on the target data only, which we refer to as separate , and the other is to apply on the combined data con-sisting of target data and the auxiliary data, which we refer as combined . The first baseline method is a tradi-tional 1D-clustering solution CLTUO (Zhao &amp; Karypis, 2002) using its default parameter. The second baseline method is clustering on the target data under a new feature representation that is first constructed through feature clustering (on the target or the combined data set); this baseline is designed to evaluate the effec-tiveness of co-clustering based method as opposed to naively constructing new data representation for clus-tering. We refer to this class of baseline methods as Feature Clustering . The third baseline method is an information theoretic co-clustering method applied to the target (or the combined) data set (Dhillon et al., 2003), which we refer to as Co-clustering . This base-line is designed to test the effectiveness of our special co-clustering model for self-taught clustering. Table 1 presents the clustering performance in en-tropy according to each data set and each evaluation method. From this table, we can see that Feature Clustering and Co-clustering perform somewhat worse than CLUTO . This is a little different from the re-sults shown in the previous literatures such as (Dhillon et al., 2003). In our opinion, it is because our self-taught clustering problem focuses on a different situ-ation from the previous ones; that is, the target data are insufficient for traditional clustering algorithms. In our experiments, there are only 70 instances in each category, which is too few to build a good fea-ture clustering partition. Therefore, the performance of Feature Clustering and Co-clustering declines. Moreover, the performance with respect to combined is worse than that with respect to separate in gen-eral. We believe that it is because the target data and the auxiliary data are more or less independent of each other, and thus the topics in the combined data set may be biased towards the auxiliary data and thus harm the clustering performance on the target data. In general, our algorithm STC greatly outperforms the three baseline methods. We observe that the reason for the outstanding performance of STC is that the co-clustering part of STC makes feature clustering re-sult consistent with the clustering result on both the target data and the auxiliary data. Therefore, using this feature clustering as the new data representation, the clustering performance of the target data is im-proved.
 In our STC algorithm, it is assumed that we have al-ready known the number of feature clusters K . How-ever, in reality, this number should be carefully tuned. In these experiments, we tuned this parameter em-pirically. Figure 2 presents the entropy curves with respect to different number of feature clusters given by CLUTO , Feature Clustering , Co-clustering and STC respectively. The entropy in Figure 2 is the aver-age over 6 binary image clustering tasks. Note that the curve given by CLUTO never changes, since CLUTO does not incorporate feature clustering. From this figure, we can see Feature Clustering and Co-clustering perform somewhat unstably as a function of the in-creasing number of feature clustering. We believe the reason is that there are only too few instances in each clustering task, which makes the traditional clustering results unreliable. Our algorithm STC incorporates a large amount of auxiliary unlabeled data, so that its variance is much smaller than that of traditional clus-tering algorithms. STC performs increasingly better in general, along with the increasing number of fea-ture clustering, until the number of feature clusters reaches 32. When the number of feature clusters is greater than 32, the performance of STC becomes in-sensitive to the number of feature clusters. We believe a number of feature clustering which is no less than 32 will be sufficient to make STC perform well. In these experiments, we set the number of feature clustering to 32.
 We next tested the choice for the trade-off parame-ter  X  in our algorithm STC (refer to Equation (5)). Generally, it is difficult to theoretically determine the value of the trade-off parameter  X  . Instead, in this work, we tuned this parameter empirically on the data set fern vs starfish . Figure 3 presents the entropy curve given by STC along with changing trade-off pa-rameter  X  . From this figure, it can be seen that, when  X  decreases, which implies that the weights of the aux-iliary unlabeled data lower, the performance of STC declines rapidly. On the other hand, when  X  is suffi-ciently large, i.e.  X  &gt; 1, the performance of STC is relatively insensitive to the parameter  X  . This indi-cates the auxiliary data can help the clustering on the target data in our clustering tasks. In these experi-ments, we set the trade-off parameter  X  to one, which is the best point in Figure 3.
 Since our algorithm STC is iterative, the convergence property is also important to evaluate. Theorem 1 and Corollary 1 have already proven the convergence of STC theoretically. Here, we analyze the conver-gence of STC empirically. Figure 4 shows the entropy curve given by STC corresponding to different num-ber of iterations on the data set fern vs starfish . From this figure, we can see that STC converges very well after 7 iterations, while the performance of STC reaches the lowest point when STC converges. This indicates that our algorithm STC converges very fast and very well. In these experiments, we set the num-ber of iterations T to 10. We believe 10 iterations are enough for STC to converge. In this section, we review several past research works that are related to our work, including semi-supervised clustering, supervised clustering and transfer learning. Semi-supervised clustering improves clustering perfor-mance by incorporating additional constraints pro-vided by a few labeled data, in the form of must-links (two examples must in the same cluster) and cannot-links (two examples cannot in the same clus-ter) (Wagstaff et al., 2001). It finds a balance be-tween satisfying the pairwise constraints and optimiz-ing the original clustering criteria function. In addition to (Wagstaff et al., 2001), Basu et al. (2002) used a small amount of labeled data to generate initial seed clusters in K -means and constrained K -means algo-rithm by labeled data. Basu et al. (2004) generalized the previous semi-supervised clustering algorithms and proposed a probabilistic framework based on hidden Markov random fields that combines the constraints and clustering distortion measures in a general frame-work. Recent semi-supervised clustering works include (Nelson &amp; Cohen, 2007; Davidson &amp; Ravi, 2007). Supervised clustering is another branch of work de-signed to improve clustering performance with the help of a collection of auxiliary labeled data. To address the supervised clustering problem, Finley and Joachims (2005) proposed an SVM-based supervised clustering algorithm by optimizing a variety of different cluster-ing functions. Daum  X e III and Marcu (2005) developed a Bayesian framework for supervised clustering based on Dirichlet process prior.
 Transfer learning emphasizes the transferring of knowl-edge across different domains or tasks. For example, multi-task learning (Caruana, 1997) or clustering (Teh et al., 2006) learns the common knowledge among dif-ferent related tasks. Wu and Dietterich (2004) investi-gated methods for improving SVM classifiers with aux-iliary training data sources. Raina et al. (2006) pro-posed to learn logistic regression classifiers by incorpo-rating labeled data from irrelevant categories through constructing informative prior from the irrelevant la-beled data. Raina et al. (2007) proposed a new learn-ing strategy known as self-taught learning , which uti-lizes irrelevant unlabeled data to enhance the classifi-cation performance.
 In this paper, we propose a new clustering framework called self-taught clustering which is an instance of un-supervised transfer learning . The basic idea is to use irrelevant unlabeled data to help the clustering of a small amount of target data. To our best knowledge, our self-taught clustering problem is novel in capturing a large class of machine learning problems. In this paper, we investigated an unsupervised trans-fer learning problem called self-taught clustering , and developed a solution by using an unlabeled auxiliary data to help improve the target clustering results. We proposed a co-clustering based self-taught clustering algorithm (STC) to solve this problem. In our al-gorithm, two co-clusterings are performed simultane-ously on the target data and the auxiliary data to un-cover the shared feature clusters. Our empirical results show that the auxiliary data can help the target data to construct a better feature clustering as data rep-resentation. Under the new data representation, the clustering performance on the target data is indeed enhanced, and our algorithm can greatly outperform several state-of-the-art clustering methods in the ex-periments.
 In this work, we tackled the self-taught clustering by finding a better feature representation using co-clustering. In the future, we will explore several other ways in finding common feature representations. Qiang Yang thanks Hong Kong CERG grants 621307 and CAG grant HKBU1/05C. We thank the anony-mous reviewers for their greatly helpful comments. Bach, F. R., Lanckriet, G. R. G., &amp; Jordan, M. I. (2004). Multiple kernel learning, conic duality, and the smo algorithm. Proceedings of the Twenty-first
International Conference on Machine Learning (pp. 6 X 13).
 Basu, S., Banerjee, A., &amp; Mooney, R. J. (2002).
Semi-supervised clustering by seeding. Proceedings of the Nineteenth International Conference on Ma-chine Learning (pp. 27 X 34).
 Basu, S., Bilenko, M., &amp; Mooney, R. J. (2004). A probabilistic framework for semi-supervised cluster-ing. Proceedings of the Tenth ACM SIGKDD In-ternational Conference on Knowledge Discovery and Data Mining (pp. 59 X 68).
 Caruana, R. (1997). Multitask learning. Machine Learning , 28 , 41 X 75.
 Cover, T. M., &amp; Thomas, J. A. (1991). Elements of information theory . Wiley-Interscience.
 Daum  X e III, H., &amp; Marcu, D. (2005). A bayesian model for supervised clustering with the dirichlet process prior. Journal of Machine Learning Research , 6 , 1551 X 1577.
 Davidson, I., &amp; Ravi, S. S. (2007). Intractability and clustering with constraints. Proceedings of the Twenty-fourth International Conference on Ma-chine Learning (pp. 201 X 208).
 Dhillon, I. S., Mallela, S., &amp; Modha, D. S. (2003).
Information-theoretic co-clustering. Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 89 X  98).
 Finley, T., &amp; Joachims, T. (2005). Supervised clus-tering with support vector machines. Proceedings of the Twenty-second International Conference on Machine Learning (pp. 217 X 224).
 Griffin, G., Holub, A., &amp; Perona, P. (2007). Caltech-256 object category dataset (Technical Report 7694). California Institute of Technology.
 Jain, A. J., &amp; Dubes, R. C. (1988). Algorithms for clustering data . Englewood, NJ: Prentice-Hall. Li, F.-F., &amp; Perona, P. (2005). A bayesian hierarchical model for learning natural scene categories. Proceed-ings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Vol-ume 2 (pp. 524 X 531).
 Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision , 60 , 91 X 110.
 MacQueen, J. B. (1967). Some methods for classi-fication and analysis of multivariate observations.
Proceedings of Fifth Berkeley Symposium on Math-ematical Statistics and Probability (pp. 1:281 X 297). Nelson, B., &amp; Cohen, I. (2007). Revisiting probabilis-tic models for clustering with pair-wise constraints.
Proceedings of the Twenty-fourth International Con-ference on Machine Learning (pp. 673 X 680).
 Raina, R., Battle, A., Lee, H., Packer, B., &amp; Ng, A. Y. (2007). Self-taught learning: transfer learning from unlabeled data. Proceedings of the Twenty-fourth
International Conference on Machine Learning (pp. 759 X 766).
 Raina, R., Ng, A. Y., &amp; Koller, D. (2006). Construct-ing informative priors using transfer learning. Pro-ceedings of the Twenty-third International Confer-ence on Machine Learning (pp. 713 X 720).
 Teh, Y. W., Jordan, M. I., Beal, M. J., &amp; Blei, D. M. (2006). Hierarchical Dirichlet processes. Journal of the American Statistical Association , 101 , 1566 X  1581.
 Wagstaff, K., Cardie, C., Rogers, S., &amp; Schr  X odl, S. (2001). Constrained k-means clustering with back-ground knowledge. Proceedings of the Eighteenth
International Conference on Machine Learning (pp. 577 X 584).
 Wu, P., &amp; Dietterich, T. G. (2004). Improving svm accuracy by training on auxiliary data sources. Pro-ceedings of the Twenty-first International Confer-ence on Machine Learning (pp. 110 X 117).
 Zhao, Y., &amp; Karypis, G. (2002). Evaluation of hierar-chical clustering algorithms for document datasets.
Proceedings of the Eleventh International Confer-ence on Information and Knowledge Management
