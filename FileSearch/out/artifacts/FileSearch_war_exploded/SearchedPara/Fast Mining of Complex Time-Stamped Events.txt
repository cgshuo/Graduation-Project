 Given a collection of complex, time-stamped events, how do we find patterns and anomalies? Events could be meetings with one or more persons with one or more agenda items at zero or more loca-tions (e.g., teleconferences), or they could be publications with au-thors, keywords, publishers, etc. In such settings, we want to solve the following problems: (1) find time stamps that look similar to each other and group them; (2) find anomalies; (3) provide inter-pretations of the clusters and anomalies by annotating them; (4) au-tomatically find the right time-granularity in which to do analysis. Moreover, we want fast, scalable algorithms for all these problems.
We address the above challenges through two main ideas. The first ( T3 ) is to turn the problem into a graph analysis problem, by carefully treating each time stamp as a node in a graph. This view-point brings to bear the vast machinery of graph analysis meth-ods (PageRank, graph partitioning, proximity analysis, and Cen-terPiece Subgraphs, to name a few). Thus, T3 can automatically group the time stamps into meaningful clusters and spot anomalies. Moreover, it can select representative events/persons/locations for each cluster and each anomaly, as their interpretations. The second idea ( MT3 ) is to use temporal multi-resolution analysis (e.g., min-utes, hours, days). We show that MT3 can quickly derive results from finer-to-coarser resolutions, achieving up to 2ordersofmag-nitude speedups. We verify the effectiveness as well as efficiency of T3 and MT3 on several real datasets.
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithm, experimentation multi-resolution analysis, scalability, graph mining
In many real applications, datasets are often collected at different time stamps. At each time stamp, we might observe a set of events, where each event consists of a set of entities. Furthermore, each en-tity can have its own attributes. For example, in social networks, we might observe activities (events) at each day (time), where each ac-tivity involves a set of different people (entities)  X  each with his/her own attributes (e.g., job title). Another example is the yearly DBLP datasets, where a time stamp is  X  X ublish year X ; an event is a  X  X aper X ; and entities are  X  X uthor, X   X  X onference, X  etc.

How can we analyze time in such a complex context. For ex-ample, are there any two time stamps that look similar with each other? Can we find any abnormal time stamp whose behavior is very different from other time stamps? How can we interpret our findings? Furthermore, how can we do such analysis on multiple scales in an efficient way?
In this paper, we address the above challenges in multiple di-mensions. First in a single scale, our method (T3) can automat-ically group time stamps into meaningful clusters as well as spot the abnormal stamps. For each cluster/abnormal time stamp, it also outputs the selective subsets of events/entities/attribute values as their interpretations. Here, the main idea is (1) to adopt a graph representation for the datasets at different time stamps and (2) to explore the proximity among different nodes (time/events/entities/ attribute values), based on this we will find clusters and anomalies as well as their interpretations. Our experiments on several real datasets demonstrate that T3 always outputs results (i.e., clusters and anomalies as well as their interpretations) that are consistent with human intuitions. Furthermore, we propose MT3 to allow ef-ficient analysis on multiple scales. Here, the key idea is to explore the  X  X moothness X  (i.e., redundancy) among different scales. Our experiments show that MT3 leads to exactly the same results (i.e, no quality loss ), but achieves significant speed-ups (up to 2 orders of magnitude ).

The main contributions of the paper are summarized as follows:
The rest of the paper is organized as follows. We begin in Sec-tion 2 with the formal problem definition. We present T3 for the single scale analysis and MT3 for the multiple scale analysis in Section 3 and Section 4, respectively. The experimental results are reported in Section 5. We review the related work in Section 6 and conclude the paper in Section 7. = { a 1 , ..., a object to the y th object ( x, y =1 , ..., 2+ p + q ) ( i, j ) and D x,y ( i, j )=0 ( i = j ) n type of object , and i, j =1 , ..., n 1 ) )  X  n 1 , and i =1 , ..., n  X  n 1 ,j =1 , ..., n 1 )
In this section, we first introduce our notations and data repre-sentation, and then give the formal problem definitions.
Table 1 lists the main symbols we use throughout this paper. Fol-lowing standard notation, we use calligraphic letter for sets (e.g., (e.g., W ), and lower case bolded letters for vectors (e.g., g ). We denote the transpose with a prime (i.e., W is the transpose of W ), and we use superscripts to denote the indices for object types (e.g., (e.g., W x,y is a block matrix of the matrix W ). For matrix/vector, we use the subscript to represent the size of the matrix/vector (e.g. 0 k  X  l means a matrix of size k  X  l , whose elements are all zero). If the size of a matrix/vector is clear from the context, we omit such subscripts. Also, we represent the elements in a matrix using a convention similar to Matlab, e.g., W ( i, j ) is the element at the i th row and j th column of the matrix W , and W ( i, :) is the row of W , etc.
 In our setting, the datasets are collected at different time stamps. At each time stamp, we observe a set of events, where each event consists of a set of entities. Furthermore, each entity may or may not have its own attributes. For example, in the running example in etc). The events are spreaded among 6 time stamps ( t 1 , ..., t of which is a day (e.g., t 1 is  X  X onday X , t 2 is  X  X uesday X , etc). Fur-thermore, each event involves 2 entities ( b 1 , ..., b 8 is a person (e.g., b 1 is  X  X ohn X , b 2 is  X  X mith X , etc) .
To simplify the description, we refer to  X  X ime X ,  X  X vent X , each type of  X  X ntity X , and each  X  X ttribute X  as one type of object, respectively. If we have p types of entities (in the running example, p =1 ), and q types of attributes (in the running example, q =0 ), we define the object is always  X  X ime X ; the second type of object is always  X  X vent X ; each of the next p objects is one type of  X  X ntity X ; and each of the next q objects is one type of  X  X ttribute X . For the running example in Table 2(a), we have 3 types of objects in the object set O 1 , 2 , 3 ). They are  X  X ime X ,  X  X vent X , and  X  X ntity X , respectively. (There is no  X  X ttribute X  in this example.) Each object type has a set of instances. For example, the instances for the  X  X ime X  object ( O different time stamps (e.g., t 1 ,t 2 , ... ).

In this paper, we use a graph representation for the whole dataset covering all time stamps. To be specific, we treat each instance for each type of object as a node in the graph. For example, Table 2(b) gives the graph representation for the original time-stamped datasets (depicted in Table 2(a))  X  where each time stamp, each event in-stance, and each entity instance is represented as a single node in the graph. Furthermore, the relationship between different types of objects are modeled by the adjacency matrices ( W x,y ( 1 , ..., 2+ p + q ) ). For example, we can use W 1 , 2 to model the relationship between the  X  X ime X  object and  X  X vent X  object, where W W 1 , 2 ( i, j )=0 otherwise. Similarly, we can use W 2 , 2+ x 1 , .., p ) to model the relationship between the  X  X vent X  object and the volves the j th instance of the x th type of entity; W 2 , 2+ x otherwise. We can use W 2+ x, 2+ p + y ( x =1 , ..., p, y model the relationship between the x th type of  X  X ntity X  object the of the y th type of  X  X ttribute X ; W 2+ x, 2+ p + y ( i, j )=0 otherwise. For the running example, two such adjacency matrices ( W 1 , 2 W 2 , 3 ) are enough to model all the relationships (see Table 2(c)).
If we always reserve the first n 1 rows/columns for the time nodes; the next n 2 rows/columns for the event nodes; followed by rows/ columns for entity nodes and attribute nodes respectively; we can define W = W x,y ( x, y =1 , ..., 2+ p + q ) as the overall adjacency matrix for the whole graph. Note that if there is no relationship be-tween the x th and the y th objects, the corresponding block matrix W x,y = 0 . Also, by this notation, we allow additional relationship
Figure 1: The outputs for the running example in Table 2. within the same type of object. For example, if we want to consider the continuous property of time, we can put extra links between consecutive time nodes, which will lead to a non-zero block matrix W 1 , 1 . For the running example in Table 2, its overall adjacency matrix W has the following format (Eq. ( 1)):
With the above notation, our datasets can be denoted by the ob-matrix W . Our goal is to find (1) similar/anomalous time stamps and (2) their interpretations. In this paper, we define an anomalous time stamp as a special time cluster, which contains a single time stamp. Therefore, we define the cluster membership function g as an n 1  X  1 vector, and each element in g as an integer between 1 and z ( z is the cluster number for time stamps), indicating to which cluster it belongs. To provide an interpretation for each time clus-ter, we want to select a representative subset of instances from each type of object (except  X  X ime X  object). Thus, our problem ( The Single Scale Analysis ) can be formally defined as follows: P ROBLEM 1. The Single Scale Analysis Given: The datasets collected at different time stamps: {O Find: (i) The cluster membership function g for time stamps (as
For example, Fig. 1(a) shows the output of the proposed T3 (for the single scale analysis) applied to the datasets we list in Table 2, where we find 2 clusters of time stamps ( { t 1 ,t 2 } and { t and 1 abnormal time stamp ( t 3 ). Therefore, our cluster membership function satisfies: g =[1 , 1 , 3 , 2 , 2 , 2] . For each time cluster as well as the abnormal time stamp, we also output a representative
Besides the finest scale, we might also want to do the same anal-ysis (i.e., to find the time cluster/anomaly as well as their inter-pretations) on some coarser scale. To this end, we introduce the aggregation function f , which is an n 1  X  1 vector. For exam-ple, if we aggregate the time by every two time stamps for the datasets in Table 2, the aggregation function u is a 6  X  1 vector: f =[ 1 , 1 , 2 , 2 , 3 , 3 ] . Also, let  X  g be the cluster membership function and  X  z be the cluster number at the aggregated scale, re-spectively. With this notation, our problem ( The Multiple Scale Analysis ) can be formally defined as follows: P ROBLEM 2. The Multiple Scale Analysis Given: (i) The datasets collected at different time stamps: Find: (i) The cluster membership function  X  g for time stamps (as
For example, Fig. 1(b) shows the output of the proposed MT3 applied to the datasets in Table 2 if we aggregate the time by every two time stamps. Notice that in this case, the abnormal time stamp (i.e., t 3 at the finest scale) disappears.
In this section, we propose T3 to address problem 1. We first give an overview of the proposed algorithm (T3), and then introduce each component of T3 in detail.
Alg. 1 gives the overview of the proposed T3 for single scale analysis. In T3, we first construct the graph representation W from the original raw datasets as introduced in Section 2 (step 1). Then (step 2), we will compute two proximity matrices from the adjacency matrix W : the time-to-time proximity matrix ( ttP ) and the time-to-others proximity matrix ( toP ). The time-to-time prox-imity matrix ( ttP ) will be used to find the time cluster member-ship function g (step 3); while the time-to-others proximity matrix ( toP ) will be used to find the representative subset of instances as the interpretations for time cluster (step 4).
The key point in T3 is to construct two proximity matrices ( ttP and toP ), based on which we will find the time cluster membership function g and its interpretations, respectively.

Alg. 2 lists detailed procedures to compute these two proxim-ity matrices. Overall, we adopt the well-studied model of random in the figure. Algorithm 1 Overview of T3 1: construct the graph W from the raw datasets 2: compute the proximity matrices ttP and toP 3: find time cluster membership function g based on ttP 4: find the interpretation for each time cluster based on toP walk with restart [ 19, 27, 33] for this purpose (steps 7-12). Sup-pose a random particle starts from the time node j , the particle iteratively transmits to its neighborhood with the probability that is proportional to the edge weight between them; and also at each step, it has some probability ( 1  X  c ) to return to the starting node j . The proximity score r i,j is defined as the steady-state probabil-ity that the particle will finally stay at node i . A subtle point in computing the proximity matrices is how to normalize the original adjacency matrix W . In Alg. 2, we propose to normalize it by ob-ject type (steps 1-7). That is, suppose the random particle stays at some node of type x and overall there are s x different types of ob-jects connected to the x th type of object; then at the next step, the particle will have equal chance ( 1 s objects.
 Algorithm 2 Compute the Proximity Matrices ttP and toP Input: the adjacency matrix W and c Output: the proximity matrices ttP and toP 1: for x =1:2+ p + q do 2: for y =1:2+ p + q do 3: normalize by object type: W x,y  X  1 s 4: end for 5: end for 6: set W  X  [ W x,y ] 7: for j =1: n 1 do 8: let e = 0 n  X  1 ; then set e ( j )=1 9: solve r from the equation r = c W r +(1  X  c ) e 10: set ttP (: ,j )= r (1 : n 1 ) 11: set toP (: ,j )= r ( n 1 +1: n ) 12: end for
Here, we want to find the cluster membership function g for time stamps based on the time-to-time proximity matrix ttP . The algo-Algorithm 3 Find the Time Cluster Input: the time-to-time proximity matrix ttP Output: the cluster membership function g 1: do eigen value decomposition for ttP ; let {  X  1 , ...,  X  2: find the cluster number z =argmax i (  X  i  X  1  X   X  i ) 3: let V =[ v 1 , ..., v z ] 4: treat each row of V as a data point in z -dimensional space 5: use k-means to find z clusters on V and output the correspond-In Alg. 3, we first use the eigen-gap [ 9] (step 2) to choose cluster number z . Then, we treat the first z eigen vectors as the embedding of the time nodes in the z -dimensional space (steps 3-4) and run k-means to find the final cluster membership function g (step 5).
As mentioned before, if we find some cluster which contains a single time stamp, we flag it as the abnormal time stamp.
One benefit of using spectral clustering method is that we can use the first few eigen vectors as the embedding of the time stamps in some low dimensional space. For example, we can visualize the time stamps by plotting its first two eigen vectors in Fig. 1 for the running example.
For each time cluster, we want to select a representative subset of instance nodes from each type of object (except the  X  X ime X  object) as the interpretations for that time cluster.

Suppose we want to find the interpretations for the time cluster the time cluster u to the instance node j : ing methods. We can plug in any clustering algorithm that takes a proximity matrix between nodes as input. For example, we could transfer the time-to-time proximity matrix ttP to be the normal-ized graph Laplacian and find its eigen-decomposition instead (step 1). Alternatively, we can normalize each row of V to have the unit length in step 3 as suggested in [ 25]. where I( . ) is an indicator function, which is 1 if the condition in the parenthesis is true and 0 otherwise.
 for each instance node j w.r.t. the given time cluster u as follows:
The intuition of Eq. ( 3) is that we want to find the node j which age. Finally, we can output a subset of instance nodes with high representative scores r ( j, u ) from each type of object as the inter-pretations for the time cluster u .
In this section, we propose MT3 to address problem 2. Concep-tually, we can apply T3 for each scale of interest independently. Here, the challenge is to make the analysis on the coarser scales as efficient as possible, given that we have already done the analysis at the finest scale.
 Algorithm 4 Update the Proximity Matrices Input: the proximity matrices ttP and toP , the normalized adja-Output: the proximity matrices  X  ttP and  X  toP at the aggregated 1: set up the normalized adjacency matrix  X  W =[  X  W x,y 2: initialize the transformation matrices: T 1 = 0  X  n 1 3: for  X  i =1:  X  n 1 do 4: find time stamps at the finest scale: J = { i : g ( i )=  X  i } 5: for each i  X  X  do 6: set T 1 (  X  i, i )= h ( i ) / 7: set T 2 ( i,  X  i )=1 8: end for 9: end for 12: update  X  toP = toP ( ttP )  X  1 T 1  X  ttP
In Alg. 1, the computational bottleneck lies in ste p 2  X  i.e., to compute the two proximity matrices ttP and toP . For example, our experiments show that the time for this step usually accounts for more than 95% of the overall running time of the algorithm. Therefore, our goal in Multiple Scale Analysis is to efficiently up-scale, given that we have already computed the proximity matrices ( ttP and toP ) at the finest scale.

We introduce the following vector h n 1  X  1 , where h ( i ):= num-ber of event/entity/attribute nodes connected to the time node i at the finest scale. Suppose that we will have  X  n 1 time stamps at the aggregated scale (i.e.,  X  n 1 =max( f ) ). Alg. 4 gives the detailed procedure to update the proximity matrices. In Alg. 4, after we get the overall normalized adjacency matrix  X  W at the aggregated scale (step 1), we set up two transformation matrices T 1 (steps 2-9). Then (steps 10-12), we need two matrix inversions (one n  X  n 1 in step 10 and one  X  n 1  X   X  n 1 in step 11) to get the proxim-ity matrices (  X  ttP and  X  toP ) at the aggregated scale. Note that in many real applications the number of time nodes at the finest scale is usually much smaller compared to the total nodes in the graph (i.e., n 1 n ). Typically, n 1 (the number of time nodes at the finest scale) is up to a few thousand whereas n (the total nodes in the graph) could be up to a few hundred thousand. For example, in the DBLP dataset, we only have about 49 among 988,947 time nodes at the finest scale. Therefore, we can efficiently update the proximity matrices at the aggregated scale by Alg. 4.

The correctness of Alg. 4 is guaranteed by the following theo-rem:
T HEOREM 1. The proximity matrices  X  ttP and  X  toP by Alg. 4 are correct. That is, they are exactly the same as we apply Alg. 2 to the adjacency matrix  X  W .

P ROOF . To simplify the description, we re-write the normalized adjacency matrix as the following 2  X  2 block form: where
Notice that only time nodes change before/after the aggregation, we have,
Furthermore, we can verify the following equations hold for the two off-diagonal blocks in Eq. ( 4): Define the following matrix inversion:
By the property of random walk with restart [ 33], we have the following equations for the proximity matrices:
Now, apply block matrix inversion lemma [ 28] to Eq. ( 8). To-gether with Eq. ( 4)-(9), we have 1 1  X  c ( ttP ) =( I  X  c W 1 1  X  c (
In Eq. ( 10), we have four equations for four unknown variables (  X  ing this well-defined linear system, we have where  X  = I  X  c W 1 , 1  X  (1  X  c )( ttP )  X  1 , which completes the proof of theorem 1.

Based on Alg. 4, the complete algorithm for Multiple Scale Anal-ysis is given in Alg. 5.
 Algorithm 5 MT3 for Multiple Scale Analysis Input: the proximity matrices ttP and toP , the normalized adja-Output: (i) the cluster membership function  X  g at the aggregated 1: update the proximity matrices  X  ttP and  X  toP by Alg. 4 2: find the cluster membership function  X  g by Alg. 3 3: for each time cluster  X  u in  X  g , compute the representative score
In this section, we introduce four real datasets and present our experimental results. All of the experiments are designed to answer the following questions:
We use four real datasets, which are summarized in Table 3. For each dataset, Table 3 lists the number of different types of  X  X ntity X  the number of time nodes in the finest scale ( n 1 ), the number of nodes ( n ) and edges ( m ) in the whole graph in the finest scale. We verify the effectiveness of the proposed T3 and MT3 on NIPS , CIKM , and DeviceScan , and measure the efficiency of our algo-rithms using the larger DBLP and DeviceScan datasets.

The first dataset ( NIPS ) is from the NIPS proceedings. 3 The time stamps are publication years, from 1987 to 1999. We treat paper as  X  X vent X  object and author as  X  X ntity X  object; there is no  X  X ttribute X  object in this dataset. Overall, there are 13 time nodes, 1,740 paper nodes, 2,037 author nodes, and 11,460 edges at the finest scale. http://www.cs.toronto.edu/ ~ roweis/data.html Figure 2: The embedding for the time nodes of NIPS dataset. The CIKM dataset is constructed from the CIKM proceedings. 4 Again, time stamps are publication years, from 1993 to 2007. (No-tice that we do not include papers from CIKM 1992 since the ses-sion information for that year is not available.) We treat paper as  X  X vent X  object. For this dataset, we have two types of  X  X ntity X  ob-jects: the authors of the paper and the session name where the paper is presented during the conference. For the session name, we fur-ther extract 158 keywords as its attribute. Overall, there are 15 time nodes, 952 paper nodes, 1,895 author nodes, 279 session nodes, 158 keyword nodes, and 10,228 edges at the finest scale. The DBLP dataset is constructed from all the papers in the DBLP. 5 Again, time stamps are publication years, from 1959 to 2007. We treat paper as  X  X vent X  object. For this dataset, we have two types of  X  X ntity X  objects: the authors of the paper and the conference where the paper is published. There is no additional  X  X ttribute X  object for this dataset. Overall, there are 49 time nodes, 567,090 paper nodes, 418,236 author nodes, 3,571 conference nodes, and 5,216,722 edges at the finest scale.

The DeviceScan is from MIT reality mining project. 6 Here, the  X  X vent X  object is blue tooth device scanning persons, and the time stamps are the day when such scanning events happen, from Jan. 1, 2004 to May. 5, 2005. For this dataset, we have two types of  X  X ntity X  objects: the blue tooth device and the person to be scanned; there is no additional  X  X ttribute X  object. Overall, there are 294 time nodes, 114,046 scanning nodes, 103 device nodes, 97 person nodes, and 684,276 edges at the finest scale.
Here, we show the experimental results for the three real datasets, all of which are consistent with our intuition.

Fig. 2 gives the embedding of the time nodes for NIPS dataset using the first two eigen vectors ( v 1 and v 2 )of toP , which reveal a line shape of time over publication years. Using T3, we find two time clusters (green circles vs. red dots in Fig. 2) as well as their interpretations in Table 4. From Fig. 2 and Table 4, we can see that while NIPS is a relatively stable community on the whole (e.g., the majority representative authors do not change over years), there is a topic shift from early 1990s (mainly on  X  X eural network X  and  X  X eu-ral information processing X ) to late 1990s (mainly on  X  X tatistical learning X ).

Fig. 3 gives the embedding of the time nodes for CIKM dataset using the first two eigen vectors ( v 1 and v 2 )of toP , which reveal http://www.informatik.uni-trier.de/ ~ ley/db /conf/cikm/ http://www.informatik.uni-trier.de/ ~ ley/db/ http://reality.media.mit.edu/ Figure 3: The embedding for the time nodes of CIKM dataset. a line shape of time over publication years as for the NIPS dataset. Using T3, we find two time clusters (green circles vs. red dots in Fig. 3) as well as their interpretations in Table 5. (For simplicity, we do not show the representative papers in the table.) From Fig. 3 and Table 5, we can see that while there are quite a lot of research interest in deductive databases and rule systems in the CIKM com-munity in 1990s, attention has shifted to XML, statistical learning, language, etc since 2000.
 Fig. 4 shows the results of applying the proposed MT3 to the DeviceScan dataset on two different scales: (a) daily scale and (b) monthly scale. From Fig. 4(a), it can be seen that, there are two time clusters on the daily scale. We found that one time cluster (green circles) corresponds to semester breaks as well as holidays; and the other cluster (red dots) corresponds to the week days during the semester. On the other hand, we found an abnormal time stamp (red dot, which is Apr. 2004) on the monthly scale (Fig. 4(b)).
Here, we study the wall-clock time of the proposed MT3 using two relatively larger datasets: DeviceScan and DBLP . For these results, all of the experiments are done on the same machine with four 2.4GHz AMD CPUs and 48GB memory, running Linux (2.6 kernel). We vary the aggregation length (e.g., aggregate by every 2 time stamps, by every 3 time stamps, etc) and compare the wall-clock time by the proposed MT3 and that by applying T3 to each of the aggregated scale from scratch (referred to as the  X  X traight-forward X  method). Figure 4: The embedding for the time nodes of DeviceScan dataset.

Fig. 5 shows the results. Notice that time is in logarithm scale. It can be seen that the proposed MT3 is much more efficient. For ex-ample, it is 120x faster (6.1 seconds vs. 734 seconds) for DeviceS-can dataset if we aggregate the time by every three time stamps (Fig. 5(a)); and it is 263x faster (6.0 seconds vs. 1,603 seconds) for DBLP dataset if we aggregate the time by every two time stamps (Fig. 5(b)). Overall, the proposed MT3 is 25x-263x faster than the straight-forward method. We would like to emphasize that such speed-ups are totally free , i.e., the proposed MT3 leads to exactly the same outputs as we apply T3 to each aggregated scale from scratch.
In this section, we review the related work, which can be cate-gorized into three parts: graph mining, proximity measurement on graphs and relational learning.

Graph Mining. There exists a lot of research on static graph mining, including pattern and law mining [ 4, 11, 13, 7, 24], fre-quent substructure discovery [ 35], influence propagation [ 20], com-munity mining [ 14, 16, 17], etc. More recently, there has been an increasing interest in mining time-evolving graphs, such as densifi-cation laws and shrinking diameters [ 22], community evolution [ 5], dynamic communities [ 8], and proximity tracking [ 34], etc. It is worth pointing out that in these work, the focus is on utilizing the in the graphs; while in T3 and MT3 we focus on the other side of the problem, i.e., to better understand time itself based on other information (event/entity/attribute).

Measuring Proximity on Graphs. One of the most widely used proximity measurement on graphs is random walk with restart [ 19, 27, 33], which is the main idea behind Google X  X  PageRank al-gorithm [ 26]. Other representative proximity measurements on static graphs include the sink-augmented delivered current [ 12], cycle-free effective conductance [ 21], survivable network [ 18], and direction-aware proximity [ 32]. Notice that the fast algorithms to compute the proximity measurements designed for querying, such as the one in [ 33], do not apply in our settings since the pre-computational time for these algorithms will flood the overall run-ning time of T3 and MT3.
 Also, there are a lot of applications of proximity measurements. Representative work includes connection subgraphs [ 12, 21, 30], content-based image retrieval [ 19], cross-modal correlation discov-ing [ 31], ObjectRank [ 6], RelationalRank [ 15], and NetRank [ 3, 2]. Among them, the most related works are [ 27, 6 , 3, 2] in the sense that they all use a graph representation for the dataset(s). However, these approaches mainly focus on querying with or without learn-ing; while T3 and MT3 are focusing on mining time in the context of complicated events.

Relational Learning . Sharan and Neville [ 29] present a two-step approach for incorporating temporal information on links (e.g., co-authorship and citation) into a relational classifier. First, they summarize the time-varying interaction as weights on links of a static summary graph. The summarization uses an exponential weighting scheme [ 10]. Second, they incorporate these link weights into a relational Bayes classifier. Their approach requires a sum-mary parameter (  X  ), that needs to be either provided by the user or tuned by the learning algorithm. Furthermore, their approach cannot handle temporally-varying attributes. Our approach do not require a user-provided parameter and can handle time associated with any aspect of an event.
In this paper, we study how to find patterns in a collection of time-stamped, complex events. Our main contributions are the fol-lowing: 1. We propose to treat each time-stamp as a node in a carefully 2. We propose MT3 to handle multiple scale analysis, achieving 3. Finally, we verify the effectiveness as well as the efficiency
A promising research direction is to extend the T3 and MT3 to include additional continuous attributes, like geographical coordi-nates.
This material is based upon work supported by the National Sci-ence Foundation under Grants No. IIS-0326322, No. IIS-0534205, and performed under the auspices of the U.S. Department of En-ergy by Lawrence Livermore National Laboratory under contract DE-AC52-07NA27344 (LLNL-CONF-405539). This work is also partially supported by the Pennsylvania Infrastructure Technology Alliance (PITA), an IBM Faculty Award, a Yahoo Research Al-liance Gift, with additional funding from Intel, NTT and Hewlett-Packard. Any opinions, findings, and conclusions or recommenda-tions expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, or other funding parties.
