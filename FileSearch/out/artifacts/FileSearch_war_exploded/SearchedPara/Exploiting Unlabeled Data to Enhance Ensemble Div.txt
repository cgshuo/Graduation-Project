 Nanjing University, Nanjing 210093, China
In ensemble learning [8], a number of base learners are trained and then combined for prediction to achieve strong generalization ability. Numerous effective ensemble meth ods have been proposed, such as B OOSTING [9], B AGGING [4], S
TACKING [19], etc., and most of these methods work under the supervised setting where the labels of training example s are known. In many real-world tasks, however, unlabeled training examples are readily available while obtaining th eir labels would be fairly expensive. Semi-supervised learning [5] is a major paradigm to exploit unlabeled data together with labeled training data to improve learning performance automatically, without human intervention.

This paper deals with semi-supervised ensembles, that is, ensemble learning with labeled and unlabeled data. In contrast to the huge volume of literatures on ensemble learning and on semi-supervised learning, only a few work has been devoted to the study of semi-supervised ensembles. As indicated by Zhou [20], this was caused by the different philosophies of the ensemble learning community and the semi-supervised learning community. The ensemble learnin g community believes that it is able to boost the performance of weak learners to strong learners by using multiple learn-ers, and so there is no need to use unlabeled data; while the semi-supervised learning community believes that it is abl e to boost the performance of weak learners to strong learners by exploiting unlabeled data, and so there is no need to use multiple learners. However, as Zhou indicated [20], there a re several important reasons why ensemble learning and semi-supervised learning are actually mutually beneficial, amon g which an important one is that by considering unlabeled data it is possible to help augment the diversity among the base learners, as explained in the following paragraph.
It is well-known that the generalization error of an en-semble is related to the average generalization error of the base learners and the diversity among the base learners. Generally, the lower the average generalization error (or, the higher the average accuracy) of the base learners and the higher the diversity among the base learners, the better the ensemble [11]. Previous ensemble methods work under supervised setting, trying to achieve a high average accura cy and a high diversity by using the labeled training set. It is noteworthy, however, pursuing a high accuracy and a high diversity may suffer from a dilemma. For example, for two classifiers which have perfect performance on the labeled training set, they would not have diversity since th ere is no difference between their predictions on the training examples. Thus, to increase the diversity needs to sacrifice the accuracy of one classifier. However, when we have unlabeled data, we might find that these two classifiers actually make different predictions on unlabeled data. Thi s would be important for ensemble design. For example, given two pairs of classifiers, ( A, B ) and ( C, D ) , if we know that all of them are with 100 % accuracy on labeled training data, then there will be no difference taking either the ensemble consisting of ( A, B ) or the ensemble consisting of ( C, D ) ; however, if we find that A and B make the same predictions on unlabeled data, while C and D make different predictions on some unlabeled data, then we will know that the ensemble consisting of ( C, D ) should be better. So, in contrast to previous ensemble methods which focus on achieving both high accuracy and high diversity using only the labeled data , the use of unlabeled data would open a promising direction for designing new ensemble methods.
 In this paper, we propose the U DEED ( Unlabeled Data to Enhance Ensemble Diversity ) approach. Experiments show that by using unlabeled data for diversity augmentation, U
DEED achieves much better performance than its counter-part which does not consider the usefulness of unlabeled data. Moreover, U DEED also achieves highly comparable performance to other state-of-the-art semi-supervised en sem-ble methods.

The rest of this paper is organized as follows. Section II briefly reviews related work on semi-supervised ensembles. Section III presents U DEED . Section IV reports our experi-mental results. Finally, Section V concludes.

As mentioned before, in contrast to the huge volume of literatures on ensemble learning and on semi-supervised learning, only a few work has been devoted to the study of semi-supervised ensembles.

Zhou and Li [21] proposed the T RI -TRAINING approach which uses three classifiers and in each round if two classi-fiers agree on an unlabeled instance while the third classifie r disagrees, then the two classifiers, under a certain conditi on, will label this unlabeled instance for the third classifier; the three classifiers are voted to make prediction. This is a disagreement-based semi-supervised learning approach [22], which can be viewed as a variant of the famous co-training method [3]. Later, Li and Zhou [14] extended T
RI -TRAINING to C O -FOREST , by including more base classifiers and in each round the majority teach minority strategy is still adopted.

In addition to T RI -TRAINING and C O -FOREST , there are several semi-supervised boosting methods [1], [6], [7], [16], [18]. D X  X lch  X e Buc et al. [7] proposed SSMB OOST to handle unlabeled data within the margin cost functional optimiza-tion framework for boosting [17], where the margin of an en-semble H on unlabeled data x is defined as either H ( x ) | H ( x ) | . Furthermore, SSMB OOST requires the base learners to be semi-supervised algorithms themselves. Later, Benne tt et al. [1] developed A SSEMBLE , which labels unlabeled data x by the current ensemble as y = sign [ H ( x )] , and then iteratively puts the newly labeled examples into the original labeled set to train a new base classifier which is then added to H . Following the same margin cost functional optimization framework, Chen and Wang [6] added a local smoothness regularizer to the objective function used by A
SSEMBLE to help induce new base classifier with a more reliable self-labeling process. Other than the margin cost functional formalization, M CSSB [18] and S EMI B OOST [16] estimate the labels of unlabeled instances by optimizing an objective function containing two terms. The first term encodes the manifold assumption that unlabeled instances with high similarities in input space should share similar l a-bels, while the other term encodes the clustering assumption that unlabeled instances with high similarities to a labele d example should share its given label. The difference lies in that M CSSB [18] implemented the objective terms based on Bregman divergence while S EMI B OOST [16] implemented them with traditional exponential loss.

A commonness of these existing semi-supervised ensem-ble methods is that they construct ensembles iteratively, and in particular, the unlabeled data are exploited through assigning pseudo-labels for them to enlarge labeled training set. Specifically, pseudo-labels of unlabeled instances ar e estimated based on the ensemble trained so far [1], [7], [14], [21], or with specific form of smoothness or mani-fold regularization [6], [16], [18]. After that, by regardi ng the estimated labels as their ground-truth labels, unlabeled instances are used in conjunction with labeled examples to update the current ensemble iteratively.

Although various strategies have been employed to make the pseudo-labeling process more reliable, such as by incor -porating data editing [13], the estimated pseudo-labels ma y still be prone to error, especially in initial training iter ations where the ensemble is only moderately accurate. In the next section we will present the U DEED approach. Rather than working with pseudo-labels to enlarge labeled training set , U
DEED utilizes unlabeled data in a different way, i.e., help augment the diversity among base learners.
 A. General Formulation Let X = R d be the d -dimensional input space and Y = { X  1 , +1 } be the output space. Suppose L = { ( x i , y 1  X  i  X  L } contains L labeled training examples and U = { x i | L +1  X  i  X  L + U } contains U unlabeled training examples, where x i  X  X  and y i  X  X  . In addition, we use  X  L = { x i | 1  X  i  X  L } to denote the unlabeled data set derived from L .

We assume that the classifier ensemble is composed of m base classifiers { f k | 1  X  k  X  m } , where each of them takes the form f k : X X  [  X  1 , +1] . Here, the real value of f k ( x ) corresponds to the confidence of x being positive. Accordingly, ( f k ( x )+1) / 2 can be regarded as the posteriori probability of being positive given x , i.e. P ( y = +1 | x ) .
The basic idea of U DEED is to maximize the fit of the clas-sifiers on the labeled data, while maximizing the diversity of the classifiers on the unlabeled data. Therefore, U DEED generates the classifier ensemble f = ( f 1 , f 2 , , f m minimizing the following loss function: Here, the first term V emp ( f , L ) corresponds to the empirical loss of f on the labeled data set L ; the second term V div ( f , D ) corresponds to the diversity loss of f on a specified data set D (e.g. D = U ). Furthermore,  X  is the cost parameter balancing the importance of the two terms.
In this paper, U DEED calculates the first term V emp ( f , L ) in Eq.(1) as: Here, l ( f k , L ) measures the empirical loss of the k -th base classifier f k on the labeled data set L .

As shown in Eq.(1), the second term V div ( f , D ) is used to characterize the diversity among the based learners. However, it is well-known that diversity measurement is not a straightforward task since there is no generally accepted formal definition [12]. In this paper, U DEED chooses to calculate V div ( f , D ) in a novel way as follows: Here, |D| returns the cardinality of data set D . Intuitively, d ( f p , f q , D ) represents the prediction difference between any pair of base classifiers on a specified data set D . 1 In addition, the prediction difference is calculated based on the concre te output f ( x ) instead of the signed output sign[ f ( x )] . In this way, the prediction confidence of each classifier other than the simple binary prediction is fully utilized.

Then, U DEED aims to find the target model f  X  which minimizes the loss function in Eq.(1): B. Logistic Regression Implementation
In this paper, we employ logistic regression to implement the base classifiers. Specifically, each base classifier f k  X  m ) is modeled as: Here, g k : X X  [0 , 1] is the standard logistic regression function with weight vector w k  X  X  d and bias value b k  X  R . Without loss of generality, in the rest of this paper, b absorbed into w k by appending the input space X with an extra dimension fixed at value 1.

Correspondingly, the first term V emp ( f , L ) in Eq.(1) is set to be the negative binomial likelihood function on the labeled data set L , which is commonly used to measure the empirical loss of logistic regression: Here, the term BLH( f k ( x i ) , y i ) calculates the binomial likelihood of x i having label y i , when f k serves as the classi-fication model. Note that the probabilities of P ( y = +1 | x ) and P ( y =  X  1 | x ) are modeled as 1+ f k ( x ) 2 and 1  X  f respectively, BLH( f k ( x i ) , y i ) then takes the following form based on Eq.(5): BLH( f k ( x i ) , y i ) = ln =  X  Note that the first term V emp ( f , L ) can also be evaluated in other ways, such as l 2 loss: 1 mL P m k =1 P L i =1 ( f k
The target model f  X  is found by employing gradient descent -based techniques. Accordingly, the gradients of V ( f , L , D ) with respect to the model parameters  X  = { w k | 1  X  k  X  m } are determined as follows: 2  X  X   X   X   X  X   X  w k  X  BLH( f k ( x i ) , y i ) (1 + y i )(1  X  f k ( x i ))  X   X  d ( f k , f k  X  , D ) To initialize the ensemble, each classifier f k is learned from a bootstrapped sample of L , namely L k = { ( x k i , y k i  X  L } , by conventional maximum likelihood procedure. Specifically, the corresponding model parameter w k is ob-tained by minimizing the objective function 1 2 || w k || 2 +  X  P L i =1  X  BLH( f k ( x k i ) , y k i ) . Here,  X  balances the model complexity and the binomial likelihood of f k on L k . In this paper,  X  is set to the default value of 1. Note that the ensemble can also be initialized in other ways, such as instantiating each w k with random values, etc.

As shown in Eq.(1), the second term V div ( f , D ) regarding ensemble diversity is defined on a specified data set D . Given the labeled training set L and the unlabeled training set U , we consider three possibilities of instantiating D :  X  D =  X  : No data is employed to measure the diversity  X  D =  X  L : Labeled training examples are employed  X  D = U : Unlabeled training examples are employed
For L C and L CD , after the ensemble is initialized, a series of gradient descent steps are performed to optimize the model by minimizing the loss function V ( f , L , D ) as defined in Eq.(1). For L C U D however, instead of directly minimizing V ( f , L , D ) in the straightforward way of setting D = U , the loss function is firstly minimized by a series of gradient descent steps with D =  X  L . After that, by using the learned model as the starting point , a series of gradient descent steps are further conducted to finely search the model space with D = U . The purpose of this two-stage process is to distinguish the priorities of the contribution from labeled data and unlabeled data. 3
For any gradient descent -based optimization process, it is terminated if either the loss function V ( f , L , D ) or the di-versity term V div ( f , D ) does not decrease anymore. For each implementation of U DEED , the label of an unseen example z is predicted by the learned ensemble f  X  = ( f  X  1 , f  X  2 via weighted voting : 4 f  X  ( z ) = sign [ P m k =1 f  X  k
Intuitively, if the ensemble does benefit from the diver-sity augmented by the unlabeled training examples, L C U D should achieve superior performance than L C and L CD . In this section, comparative studies between U DEED (i.e. L
C U D ) and other semi-supervised ensemble methods are firstly reported. More importantly, experimental analysis on the three different implementations of U DEED are further conducted to show whether unlabeled data do benefit en-semble learning by helping augment the diversity among base learners.

Twenty-five publicly-available binary data sets are used for experiments, whose characteristics are summarized in Table I. Fifteen of them are from UCI Machine Learning Repository [2], five from UCI KDD Archive [10], four from [5] and one from [15]. Twenty regular-scale data sets (left four columns) as well as five large-scale data sets (right column) are included. The data set size varies from 57 to 581,012, the dimensionality varies from 8 to 300, and the ratio between positive examples to negative examples varie s from 0.031 to 3.844.

For each data set, 50% of them are randomly selected to form the test set T , and the rest is used to form the training set of L S U . The percentage of labeled data in training set (i.e. |L| / ( |L| + |U| ) ) is set to be 0.25. For each data set, 50 random L / U / T splits are performed. Hereafter, the reported performance of each method corresponds to the average result out of 50 runs on different splits.
Various ensemble sizes (i.e. m ) are considered in the experiments: a) m = 20 representing the case of small-scale ensemble; b) m = 50 representing the case of medium-scale ensemble; and c) m = 100 representing the case of large-scale ensemble. 5 In addition, as shown in Eq.(1), the cost parameter  X  is set to the default value of 1. Note that better performance can be expected if certain strategies such as cross-validation are employed to optimize the value of  X  . A. Comparative Studies
In this subsection, U DEED (L C U D ) is compared with two popular ensemble methods B AGGING [4] and A DA B OOST [9], and two successful semi-supervised ensemble methods A
SSEMBLE [1] and S EMI B OOST [16]. For fair comparison, logistic regression is employed as the base learner of each compared method. For U DEED , the maximum number of gradient descent steps is set to 25 and the learning rate is se t to 0.25. For the other compared methods, default parameters suggested in respective literatures are adopted.

Tables II to IV report the detailed experimental results under small-scale ( m =20), medium-scale ( m =50) and large-scale ( m =100) ensemble sizes respectively. S EMI B OOST fails to work on the large-scale data sets, due to its de-manding storage complexity ( O (( |L| + |U| ) 2 ) ) to maintain the similarity matrix for the training examples.

On each data set, the mean predictive accuracy as well as the standard deviation of each algorithm (out of 50 runs) are recorded. Furthermore, to statistically measure the significance of performance difference, pairwise t -tests at 95% significance level are conducted between the algo-rithms. Specifically, whenever U DEED achieves significantly better/worse performance than the compared algorithm on any data set, a win/loss is counted and a maker  X  /  X  is shown. Otherwise, a tie is counted and no marker is given. The resulting win/tie/loss counts for U DEED against the compared algorithms are highlighted in the last line of each table.
 In summary, when the ensemble size is small (Table II), U DEED is statistically superior to B AGGING , A DA B OOST , A
SSEMBLE and S EMI B OOST in 52% , 52% , 56% and 45% cases, and is inferior to them in much less 12% , 20% , 12% and 15% cases; When the ensemble size is medium (Table III), U DEED is statistically superior to B AGGING A 52% and 50% cases, and is inferior to them in much less 8% , 36% , 24% and 10% cases; When the ensemble size is large (Table IV), U DEED is statistically superior to B AGGING , A 52% and 40% cases, and is inferior to them in much less 8% , 40% , 20% and 15% cases. These results indicate that U DEED is highly competitive to the other compared methods. Roughly speaking, as for the time complexity, U
DEED is slightly higher than B AGGING and A DA B OOST while fairly comparable to A SSEMBLE and S EMI B OOST . B. The Helpfulness of Unlabeled Data
As motivated in Section I, U DEED aims to exploit unla-beled data to help ensemble learning in the particular way of augmenting diversity among base learners. Therefore, in addition to the above comparative experiments with other (semi-supervised) ensemble methods, it is more important to show whether U DEED (L C U D ) does achieve better per-formance than its counterparts (L C and L CD ) which do not consider using unlabeled data for diversity augmentation.
Table V reports the performance improvement (i.e. in-crease of predictive accuracy) of L C U D against L C and L
CD under various ensemble sizes. On each data set, the mean improved predictive accuracy as well as the standard deviation (out of 50 runs) are recorded. In addition, to stat is-tically measure the significance of performance difference , pairwise t -tests at 95% significance level are conducted. Specifically, whenever L C U D achieves significantly supe-rior/inferior performance than L C or L CD on any data set, a win/loss is counted and a maker  X  /  X  is shown in the Table. Otherwise, a tie is counted and no marker is given. The resulting win/tie/loss counts for L C U D against L C and L are highlighted in the last line of Table V.

In summary, when the ensemble size is small , L C U D is statistically superior to L C and L CD in 64% and 56% cases, and is inferior to them in both only 12% cases; When the ensemble size is medium , L C U D is statistically superior to L
C and L CD in both 52% cases, and is inferior to them in both only 8% cases; When the ensemble size is large , L
C U D is statistically superior to L C and L CD in 52% and 56% cases, and is inferior to them in only 8% and 12% cases. These results indicate that, by exploiting unlabeled data i n the specific way of helping augment ensemble diversity, U
DEED (L C U D ) is capable of achieving better performance than its counterparts (L C and L CD ) which do not consider employing unlabeled in ensemble generation. 6 C. Diversity Analysis
To clearly verify that U DEED (L C U D ) does increase the diversity among base learners after generating ensemble by utilizing unlabeled data, additional experiments are anal yzed in this subsection based on several existing diversity mea-sures. Specifically, four diversity measures summarized in [12] are considered, whose values are calculated based on the oracle (correct/incorrect) outputs of base learners.
Suppose m denotes the number of base classifiers in the ensemble and N denotes the number of examples in the test set T . In addition, let O = [ o ij ] m  X  N be the oracle output matrix. Here, o ij = 1 if the i -th base learner correctly classifies the j -th test example ( 1  X  i  X  m, 1  X  j  X  N ). Otherwise, o ij = 0 . The formal definitions of the four diversity measures are as follows:  X  Disagreement measure (DIS):  X  Double-fault measure (DF):  X  Entropy measure (ENT): ENT =  X  Coincident failure diversity (CFD):
CFD = Here, DIS and DF are pairwise measures while ENT and CFD are non-pairwise measures. In addition, 1-DF is used instead of DF such that for all the measures, the greater the value the higher the diversity. All the four measures vary between 0 and 1.

Table VI compares U DEED  X  X  initial diversity after ensem-ble initialization with its final diversity after ensemble learn-ing under various ensemble sizes. For each data set, pairwis e t -tests at 95% significance level are conducted between the initial and the final ensemble diversities. Whenever the fina l ensemble achieves significantly higher/lower diversity th an the initial one, a win/loss is recorded. Otherwise, a tie is recorded. The resulting win/tie/loss counts are highlight ed in the last line of Table VI.

In summary, when the ensemble size is small , U DEED statistically increases the initial ensemble diversity in 60% (DIS), 56% (DF), 60% (ENT) and 60% (CFD) cases, but decreases the initial ensemble diversity in only 16% (DIS), 20% (DF), 16% (ENT) and 8% (CFD) cases.

When the ensemble size is medium , U DEED statistically increases the initial ensemble diversity in 56% (DIS), 56% (DF), 56% (ENT) and 48% (CFD) cases, but decreases the initial ensemble diversity in only 24% (DIS), 16% (DF), 16% (ENT) and 8% (CFD) cases;
Finally, when the ensemble size is large , U DEED statisti-cally increases the initial ensemble diversity in 68% (DIS), 68% (DF), 64% (ENT) and 48% (CFD) cases, but decreases the initial ensemble diversity in only 16% (DIS), 16% (DF), 16% (ENT) and 12% (CFD) cases.

These results clearly verify that U DEED can effectively exploit unlabeled data to help augment ensemble diversity.
Previous ensemble methods try to obtain a high accuracy of base learners and high diversity among base learners by considering only labeled data. There were some studies on using unlabeled data, but focusing on using unlabeled data to improve accuracy. The major contribution of our work is to use unlabeled data to augment diversity, which suggests a new direction for ensemble design. Specifically, a novel semi-supervised ensemble method named U DEED is proposed, which works by maximizing accuracy on labeled data while maximizing diversity on unlabeled data.
Experiments show that: a) U DEED achieves highly compa-rable performance against other successful semi-supervis ed ensemble methods; b) U DEED does benefit from unlabeled data by using them to augment the diversity among base learners. In the future, it is interesting to see whether U works well with other base learners. It would be insightful to analyze why U DEED can achieve good performance the-oretically. Furthermore, designing other ensemble method s by exploiting unlabeled data to augment ensemble diversity gracefully is a direction very worth studying.

The authors wish to thank the anonymous reviewers for their helpful comments in improving this paper. This work was supported by the National Science Founda-tion of China (60635030, 60805022), the National Fun-damental Research Program of China (2010CB327903), the Ph.D. Programs Foundation of Ministry of Education of China (200802941009), the Jiangsu Science Foundation (BK2008018) and the Jiangsu 333 Program.

