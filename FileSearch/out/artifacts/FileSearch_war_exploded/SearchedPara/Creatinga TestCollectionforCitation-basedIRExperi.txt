 We present a methodology for creating a test collec-tion of scientic papers that is based on the Cran-eld 2 methodology but uses a current conference as the main vehicle for eliciting rele vance judgements from users, i.e., the authors.

Building a test collection is a long and expensi ve process but was necessary as no ready-made test col-lection existed on which the kinds of experiments with citation information that we envisage could be run. We aim to impro ve term-based IR on scien-tic articles with citation information, by using in-dex terms from the citing article to additionally de-scribe the cited document. Exactly how to do this is the research question that our test collection should help to address.

This paper is structured as follo ws: Section 2 mo-tivates our proposed experiments and, thereby , our test collection. Section 3 discusses the how test col-lections are built and, in particular , our own. Sec-tion 4 briey describes the practicalities of compil-ing the document collection and the processing we perform to prepare the documents for our experi-ments. In Section 5, we sho w that our test collection can be used with standard IR tools. Finally , Sec-tion 6 discusses the problem of the low number of rele vant documents judged so far and two ways of alle viating this problem. The idea of using terms external to a document, coming from a `citing' document, has been bor -rowed from web-based IR. When one paper cites another , a link is made between them and this link structur e is analogous to that of the web:  X hyper -links ... pro vide semantic linkages between ob-jects, much in the same manner that citations link documents to other related documents X  (Pitk ow and Pirolli, 1997). Link structure, particularly anchor text, has been used to adv antage in web-based IR. While web pages are often poorly self-descripti ve (Brin and Page, 1998) anchor text is often a higher -level description of the pointed-to page. (Da vison, 2000) pro vides a good discussion of how well an-chor text does this and pro vides experimental results in support. Thus, beginning with (McBryan, 1994), there is a trend of propagating anchor text along its hyperlink to associate it with the link ed page, as well as the page in which it is found. Google, for ex-ample, includes anchor text as inde x terms for the link ed page (Brin and Page, 1998). The TREC Web tracks have also sho wn that using anchor text im-pro ves retrie val effecti veness for some search tasks (Ha wking and Craswell, 2005).

This idea has already been applied to citations and scientic articles (Bradsha w, 2003). In Bradsha w's experiment, scientic documents are inde xed by the text that refers to them in documents that cite them. Ho we ver, unlik e in experiments with pre vious col-lections, we need both the citing and the cited article as full documents in our collection. The question of how to identify citation `anchor text' and its extent is a matter for research; this requires the full text of the citing article. Pre vious experiments and test col-lections have had only limited access to the content of the citing article: Bradsha w had access only to a x ed windo w of text around the citation, as pro vided by CiteSeer' s `citation conte xt'; in the GIR T collec-tions (Kluck, 2003), a dozen or so content-bearing information elds (e.g., title, abstract, methodologi-cal descriptors) represent each document and the full text is not available. Additionally , in Bradsha w's ex-periment, no access is given to the text of the cited article itself so that the inuence of a term-based IR model cannot be studied and so that documents can only be inde xed if the y have been cited at least once. A test collection containing full text for man y cit-ing and cited documents, thus, has adv antages from a methodological point of vie w. 2.1 Choosing a Genr e When choosing a scientic eld to study , we look ed for one that is practicable for us to compile the doc-ument collection (freely available machine-readable documents; as few as possible document styles), while still ensuring good coverage of research top-ics in an entire eld. Had we chosen the medical eld or bioinformatics, the prolic number of jour -nals would have been a problem for the practical document preparation.

We also look ed for a relati vely self-contained eld. As we aim to propagate referential text to cited papers as inde x terms, references from documents in the collection to other documents within the col-lection will be most useful. We call these internal references. While it is impossible to nd or create a collection of documents with only internal refer -ences, we aim for as high a proportion of internal references as possible.

We chose the ACL (Association for Computa-tional Linguistics) Anthology 1 , a freely available digital archi ve of computational linguistics research papers. Computational linguistics is a small, ho-mogenous research eld and the Anthology contains the most prominent publications since the beginning of the eld in 1960, consists of only 2 journals, 7 conferences and 5 less important publications, such as discontinued conferences and a series of work-shops, resulting in only 7000 papers 2 .

With the ACL Anthology , we expect a high pro-portion of internal references within a relati vely compact document collection. We empirically mea-sured the proportion of collection-interna l refer -ences. We found a proportion of internal refer -ences to all references of 0.33 (the in-factor ). We wanted to compare this number to a situation in another , lar ger eld (genetics) but no straightfor -ward comparison is possible, as there are very man y genetics journals and quality of journals probably plays a lar ger role in a bigger eld. We tried to simulate a similar collection to the 9 main jour -nals+conferences in the Anthology , by considering 10 journals in genetics with a range of impact fac-tors 3 , resulting in an in-f actor of 0.17 (dropping to 0.14 if only 5 journals are considered). Thus, our hypothesis that the Anthology is reasonably self-contained, at least in comparison with other possible collections, was conrmed.

The choice of computational linguistics has the added benet that we are familiar with the domain; we can interpret the subject matter better than we would be able to in the medical domain. This should be of use to us in our eventual experiments. To turn our document collection into a test col-lection, a parallel set of search queries and rele-vance judgements is needed. There are a number of alternati ve methods for building a test collec-tion. For TREC, humans devise queries specically for a given set of documents and mak e rele vance judgements on pooled retrie ved documents from that set (Harman, 2005). Theirs is an extremely labour -intensi ve and expensi ve process and an unrealistic option in the conte xt of our project.

The Craneld 2 tests (Cle verdon et al., 1966) in-troduced an alternati ve method for creating a test collection, specically for scientic texts. The method was subject to criticism and has not been emplo yed much since. Ne vertheless, we belie ve this method to be worth revisiting for our current situa-tion. In this section, we describe in turn the Cran-eld 2 method and our adapted method. We discuss some of the original criticisms and their bearing on our own work, then describe our returns thus far. 3.1 The Craneld 2 Test Collection The Craneld 2 tests (Cle verdon et al., 1966) were a comparati ve evaluation of inde xing language de-vices. From a base collection of 182 (high speed aerodynamics and aircraft structures) papers, the Craneld test collection was built by asking the au-thors to formulate the research question(s) behind their work and to judge how rele vant each reference in their paper was to each of their research questions, on a 5-point scale. Referenced documents were ob-tained and added to the base set. Authors were also ask ed to list additional rele vant papers not cited in their paper . The collection was further expanded in a second stage, using bibliographic coupling to search for similar papers to the referenced ones and emplo ying humans to search the collection for other rele vant papers. The resultant collection comprised 1400 documents and 221 queries (Cle verdon, 1997).
The principles behind the Craneld technique are:  X   X   X  3.2 Our Anthology Test Collection We altered the Craneld design to t to a x ed, existing document collection. We designed our methodology around an upcoming conference and approached the paper authors at around the time of the conference, to maximize their willingness to par -ticipate and to minimise possible changes in their perception of rele vance since the y wrote the paper . Due to the relati vely high in-f actor of the collection, we expected a signicant proportion of the rele vance judgements gathered in this way to be about Anthol-ogy documents and, thus, useful as evaluation data.
Hence, the authors of accepted papers for ACL-2005 and HL T-EMNLP-2005 were ask ed, by email, for their research questions and rele vance judge-ments for their references. We dened a 4-point rele vance scale, c.f. Table 1, since we felt that the distinctions between the Craneld grades were not clear enough to warrant 5. Our guidelines also in-cluded examples of referencing situations that might t each cate gory . Personalized materials for partic-ipation were sent, including a reproduction of their paper' s reference list in their response form. This meant that invitations could only be sent once the paper had been made available online.

We further deviated from the Craneld methodol-ogy by deciding not to ask the authors to try to list additional references that could have been included in their reference list. An author' s willingness to name such references will dif fer more from author to author than their naming of original references, as referencing is part of a standardized writing process. By asking for this data, the consistenc y of the data across papers will be degraded and the status of any additional references will be unclear . Furthermore, feedback from an informal pilot study conducted on ten paper authors conrmed that some authors found this task particularly dif cult.

Each co-author of the papers was invited indi vidu-ally to participate, rather than inviting the rst author alone. This increased the number of invitations that needed to be prepared and sent (by a factor of around 2.5) but also increased the lik elihood of getting a re-turn for a given paper . Furthermore, data from mul-tiple co-authors of the same paper can be used to measure co-author agreement on the rele vance task. This is an interesting research question, as it is not at all clear how much even close collaborators would agree on rele vance, but we do not address this here.
We plan to expand the collection in a second stage, in line with the Craneld 2 design. We will reapproach contrib uting authors after obtaining re-trie val results on our collection (e.g., with a stan-dard IR engine) and ask them to mak e additional rel-evance judgements on these papers. 3.3 Criticisms of Craneld 2 Both Craneld 1 (Cle verdon, 1960) and 2 were sub-ject to various criticisms; (Sp  X  arck Jones, 1981) gives an excellent account of the tests and their criticisms. The majority were criticisms of the test collection paradigm itself and are not pertinent here. Ho w-ever, the sour ce-document principle (i.e., the use of queries created from documents in the collection) at-tracted particular criticisms. The fundamental con-cern was that the way in which the queries were cre-ated led to  X an unnaturally close relation X  between the terms in the queries and those used to inde x the documents in the colection (Vick ery , 1967); any such relationship might have created a bias towards a particular inde xing language, distorting the com-parisons that were the goal of the project.
In Craneld 1, system success was measured by retrie val of source documents alone, criticized for being an over-simplication and a distortion of `real-life' searching. The evaluation procedure was changed for Craneld 2 so that source documents were excluded from searches and, instead, retrie val of other rele vant documents was used to measure success. This remo ved the problem that, usually , when a user searches, there is no source document for their query . Despite this, Vick ery notes that there were  X still verbal links between sought document and question X  in the new method: each query author was ask ed to judge the rele vance of the source doc-ument' s references and  X the questions ... were for -mulated after the cited papers had been read and has possibly inuenced the wording of his question X .
While adapting the Craneld 2 method to our needs, we have tried to address some of the crit-icisms, e.g., that authors' rele vance judgements change over time. Ne vertheless, we still have source-document queries and must consider the as-sociated criticisms. Firstly , our test collection is not intended for comparisons of inde xing languages. Rather , we aim to compare the effect of adding ex-tra inde x terms to a base inde xing of the documents. The source documents will have no inuence on the base inde xing of a document abo ve that of the other documents. The additional inde x terms, com-ing from citations to that document, will generally be `chosen' by someone other than the query author , with no kno wledge of the query terms 4 . Also, our documents will be inde xed fully automatically , fur -ther diminishing the scope of any subconscious hu-man inuence.

Thus, we belie ve that the suspect relationship be-tween queries and inde xing is negligible in the con-text of our work, as opposed to the Craneld tests, and that the source-document principle is sound. 3.4 Retur ns and Analysis Out of around 500 invitations sent to conference au-thors, 85 resulted in research questions with rele-vance judgements being returned; 235 queries in to-tal. Example queries are:  X   X 
Of the 235 queries, 18 were from authors whose co-authors had also returned data and were dis-carded (for retrie val purposes); we treat co-author data on the same paper as `the same' and keep only the rst authors'. 47 queries had no rele vant Anthology-internal references and were discarded. Another 15 had only rele vant Anthology references not yet included in the archi ve 5 ; we keep these for the time being. This lea ves 170 unique queries with at least 1 rele vant Anthology reference and an aver-age of 3.8 rele vant Anthology references each. The average in-f actor across queries is 0.42 (similar to our pre viously estimated Anthology in-f actor) 6 .
Our average number of judged rele vant docu-ments per query is lower than for Craneld, which had an average of 7.2 (Sp  X  arck Jones et al., 2000). Ho we ver, this is the nal number for the Cran-eld collection, arri ved at after the second stage of rele vance judging, which we have not yet car -ried out. Ne vertheless, we must anticipate a po-tentially low number of rele vant documents per query , particularly in comparison to, e.g., the TREC ad hoc track (Voorhees and Harman, 1999), with 86.8 judged rele vant documents per query . The Anthology documents are distrib uted in PDF , a format designed to visually render printable docu-ments, not to preserv e editable text. So the PDF col-lection must be con verted into a fully textual format. A pipeline of processing stages has been developed in the frame work of a wider project, illustrated in Figure 1.

Firstly , OmniP age Pro 14 7 , a commercial PDF processing softw are package, scans the PDFs and produces an XML encoding of character -le vel page layout information. AI algorithms for heuristically extracting character information (similar to OCR) are necessary since man y of the PDFs were created from scanned paper -copies and others do not contain character information in an accessible format.
The OmniP age output describes a paper as text blocks with typesetting information such as font and positional information. A pre-processor (Le win et al., 2005) lters and summarizes the OmniP age out-put into Intermediate XML (IXML), as well as cor -recting certain characteristic errors from that stage. A journal-specic template con verts the IXML to a logical XML-based document structure (Teufel and Elhadad, 2002), by exploiting low-le vel, presenta-tional, journal-specic information such as font size and positioning of text blocks.

Subsequent stages incrementally add more de-tailed information to the logical representation. The paper' s reference list is annotated in more detail, marking up indi vidual references, author names, ti-tles and years of publication. Finally , a citation pro-cessor identies and marks up citations in the doc-ument body and their constituent parts, e.g., author names and years. We expect that our test collection, built for our cita-tion experiments, will be of wider value and we in-tend to mak e it publicly available. As a sanity check on our data so far, we carried out some preliminary experimentation, using standard IR tools: the Lemur Toolkit 8 , specically Indri (Strohman et al., 2005), its inte grated language-model based search engine, and the TREC evaluation softw are, trec eval 9 . 5.1 Experimental Set-up We inde xed around 4200 Anthology documents. This is the total number of documents that have, at the time of writing, been processed by our pipeline (24 years of CL journal, 25 years of ACL proceed-ings, 14 years of assorted workshops), plus another  X  90 documents for which we have rele vance judge-ments that are not currently available through the Anthology website but should be incorporated into the archi ve in the future. The inde xed documents do not yet contain annotation of the reference list or ci-tations in text. 19 of our 170 queries have no rele vant references in the inde xed documents and were not included in these experiments. Thus, Figure 2 sho ws the distrib ution of queries over number of rele vant Anthology references, for a total of 151 queries.
Our Indri inde x was built using def ault parameters with no optional processing, e.g., stopping or stem-ming, resulting in a total of 20117410 terms, 218977 unique terms and 2263 `frequent' 10 terms.

We then prepared an Indri-style query le from the conference research questions. The Indri query language is designed to handle highly comple x queries but, for our very basic purposes, we created simple bag-of-w ords queries by stripping all punctu-ation from the natural language questions and using Indri' s #combine operator over all the terms. This means Indri ranks documents in accordance with query lik elihood. Again, no stopping or stemming was applied.

Ne xt, the query le was run against the Anthology inde x using IndriRunQuery with def ault parameters and, thus, retrie ving 1000 documents for each query .
Finally , for evaluation, we con verted the Indri' s rank ed document lists to TREC-style top results le and the conference rele vance judgements compiled into a TREC-style qrels le, including only judge-ments corresponding to references within the in-dexed documents. These les were then input to trec eval, to calculate precision and recall metrics. 5.2 Results and Discussion Out of 489 rele vant documents, 329 were retrie ved within 1000 (per query) documents. The mean av-erage precision (MAP) was 0.1014 over the 151 queries. This is the precision calculated at each rele-vant document retrie ved (0.0, if that document is not retrie ved), averaged over all rele vant documents for all queries, i.e., non-interpolated. R-precision, the precision after R (the number of rele vant documents for a query) documents are returned, was 0.0965. The average precision at 5 documents was 0.0728.
We investigated the effect of excluding queries with lower than a threshold number of judged rel-evant documents. Figure 3 sho ws that precision at 5 documents increases as greater threshold values are applied. Similar trends were observ ed with other evaluation measures, e.g., MAP and R-precision in-creased to 0.2018 and 0.1528, respecti vely , when only queries with 13 or more rele vant documents were run, though such stringent thresholding does result in very few queries. Ne vertheless, these trends do suggest that the present low number of rele vant documents has an adv erse effect on retrie val results and is a potential problem for our test collection.
We also investigated the effect of including only authors' main queries, as another potential way of objecti vely constructing a `higher quality' query set. Although, this decreased the average in-f actor of rel-evant references, it did, in fact, increase the average absolute number of rele vant references in the inde x. Thus, MAP increased to 0.1165, precision at 5 doc-uments to 0.1016 and R-precision to 0.1201.
These numbers look poor in comparison to the performance of IR systems at TREC but, impor -tantly , the y are not intended as performance results. Their purpose is to demonstrate that such numbers can be produced using the data we have collected, rather than to evaluate the performance of some new retrie val system or strate gy.

A second point for consideration follo ws directly from the rst: our experiments were carried out on a new test collection and  X dif ferent test collec-tions have dif ferent intrinsic dif culty X  (Buckle y and Voorhees, 2004). Thus, it is meaningless to compare statistics from this data (from a dif ferent domain) to those from the TREC collections, where queries and rele vance judgements were collected in a dif ferent way, and where there are very man y rele-vant documents.

Thirdly , our experiments used only the most basic techniques and the results could undoubtedly be im-pro ved by, e.g., applying a simple stop-list. Ne ver-theless, this notion of intrinsic dif culty means that it may be the case that evaluations carried out on this collection will produce characteristically low preci-sion values.

Lo w numbers do not necessarily preclude our data' s usefulness as a test collection, whose purpose is to facilitate comparati ve evaluations. (Voorhees, 1998) states that  X To be viable as a laboratory tool, a [test] collection must reliably rank dif ferent re-trie val variants according to their true effecti veness X  and defends the Craneld paradigm (from criticisms based on rele vance subjecti vity) by demonstrating that the relati ve performance of retrie val runs is sta-ble despite dif ferences in rele vance judgements. The underlying principle is that it is not the absolute pre-cision values that matter but the ability to compare these values for dif ferent retrie val techniques or sys-tems, to investigate their relati ve benets. A test col-lection with low precision values will still allo w this.
It is kno wn that all evaluation measures are un-stable for very small numbers of rele vant documents (Buckle y and Voorhees, 2000) and there are issues arising from incomplete rele vance information in a test collection (Buckle y and Voorhees, 2004). This mak es the second stage of our test collection com-pilation even more indispensable (asking subjects to judge retrie ved documents), as this will increase the number of judged rele vant documents, as well as bridging the completeness gap.

There are further possibilities of how the prob-lem could be countered. We could exclude queries with lower than a threshold number of rele vant docu-ments (after the second stage). Given the respectable number of queries we have, we might be able to af-ford this luxury . We could add rele vant documents from outside the Anthology to our collection. This is least preferable methodologically: using the An-thology has the adv antage that it has a real identity and was created for real reasons outside our experi-ments. Furthermore, the collection `co vers a eld', i.e., it includes all important publications and only those. By adding external documents to the collec-tion, it would lose both these properties. We have presented an approach to building a test collection from an existing collection of research pa-pers and described the application of our method to the ACL Anthology . We have collected 170 queries with rele vance data, centered around the ACL-2005 and HL T-EMNLP-2005 conferences. We have sanity-check ed the usability of our data by running the queries through a retrie val system and evaluating the results using standard softw are. The collection currently has a low number of judged rele vant documents and further experimentation is needed to determine if this poses a real problem.
We plan a second stage of collecting rele vance judgements, in line with the original Craneld de-sign, whereby authors who have contrib uted queries will be ask ed to judge the rele vance of documents in retrie val rankings from standard IR models and, ide-ally , from our eventual citation-based experiments.
Ne vertheless, our test collection is lik ely to suf fer from incomplete rele vance information. The bpref measure (Buckle y and Voorhees, 2004) gauges re-trie val effecti veness solely on the basis of judged documents and is more stable to dif fering levels of completeness than measures such as MAP , R-precision or precision at x ed document cutof fs. Thus, bpref may offer a solution to the incomplete-ness problem and we intend to investigate its poten-tial use in our future evaluations.

When nished, we hope our test collection will be a generally useful IR resource. In particular , we expect the collection to be useful for experimenta-tion with citation information, for which there is cur -rently no existing test collection with the properties that ours offers.

Ackno wledgements Thanks to the revie wers for their useful comments and to Karen Sp  X  arck Jones for man y instructi ve discussions.

