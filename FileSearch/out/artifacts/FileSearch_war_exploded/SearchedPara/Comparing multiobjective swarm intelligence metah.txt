 1. Introduction
Nowadays, we can easily find many optimization problems that require a huge computational time. There are even problems that can not be solved optimally with the existing computers. Such problems are called NP-hard problems. Currently, all known algorithms for solving NP-hard problems require an exponential time with respect to the input size. It is unknown if there will be so fast algorithms, therefore, to solve an NP-hard problem of an arbitrary size it is common to use techniques such as metaheuristics ( Glover and
Kochenberger, 2003 ). In computer science, we can define a meta-heuristics as a problem optimizati on method that applies an iterative process to improve the quality of p ossible solutions, taking into account a given fitness function. Als o, we can easily adapt metaheur-istics to several problems. These techniques do not guarantee finding the optimal solution, they find quasi-optimal solutions in a reason-able time. Within the vast world of metaheuristics it is defined the concept of swarm intelligence. This concept is taking a lot of strength in recent years. Swarm intelligenc e is the discipline that deals with systems composed of a set of decen tralized and self-organized individuals, and which are normally based on natural phenomenons. In particular, this discipline takes advantage of the collective behavior of individuals who relate to each other and the environment. These algorithms could be divided into two groups: those based on the animal behaviors and those based on physics or nature behaviors. In recent years, many algo rithms based on these co llective behaviors are being successfully applied in diffe rent problems of several fields, due to this, we have decided to analyze the behavior of swarm intelli-gence algorithms in this work, comparing two novel algorithms such as the Artificial Bee Colony (ABC) algorithm ( Karaboga, 2005 ), which is an optimization algorithm based on the intelligent foraging behavior of honey bee swarm; and the Gravitational Search Algorithm (GSA) ( Rashedi et al., 2009 ), a new optimization algorithm based on the law of gravity and mass interactions. In this way, we can compare an algorithm from each group: one based on the animal behaviors (ABC), and other based on physics and nature behaviors (GSA).
The main objective of this work is to analyze which kind of swarm intelligence algorithm is able to solve better the Motif Discovery Problem (MDP). MDP is an NP-hard optimization problem as defined task of discovering novel Transcription Factor Binding Sites (TFBS) in DNA sequences ( D X  X aeseleer, 2006 ). Predicting common patterns, motifs, is one of the most important sequence analysis problem, and it has not yet been resolved in an efficient manner. In addition, in this work we have expanded the formulation of this problem with several constraints to adapt the solution search to the real biology. In biology, finding and decoding the true meaning of these DNA patterns can help us to explain the complexity and development of living organisms. MDP maximizes three conflicting objectives: motif length, support, and similarity. Due to this, we have to use multiobjective techniques for its resolution. Moreover, we have to adapt the operation of our algorithms to the mul tiobjective context. Therefore, the swarm intelligence based algorithms compared in this paper are adapted to this context: the Multiobjective ABC (MOABC) algorithm ( Gonza  X  lez-A  X  lvarez et al., 2011a ), and the Multiobjective GSA (MO-
ABC and GSA algorithms, respectively. For the results presentation, we have used typical mul tiobjective indicators such as hypervolume ( Zitzler and Thiele, 1999 ) or coverage relation ( Zitzler et al., 2000 ), and thus, we facilitate future comparisons. We also want to empha-size that to ensure that the solutions found by our proposals are biologically relevant, we have made several analysis by using biological indicators such as Sensitivity, the Positive Predictive Value, the Performance Coefficient, and the Correlation Coefficient. On whole, our main objectives in this work are: compare novel swarm intelligence based techniques to solve a well-known bioinformatics problem that still has not been resolved in an efficient manner, incorporate new rules and constraints to adapt the MDP to the real biological world, obtaining good and relevant results.
In the remainder of the paper, we briefly mention a number of existing works dedicated to the motif discovery in Section 2 .There-after, in Section 3 , we describe the MDP in detail. Section 4 presents the metaheuristics compared, explaining their performances. In
Section 5 , we include the experimental methodology, the instances used, and the best configuration of each algorithm. In Section 6 ,we analyze the behavior of our proposals, making comparisons between them and with two standard multiobjective evolutionary algorithms.
We also compare the two proposed swarm-based algorithms with other previously proposed metaheuristics in Section 7 . Section 8 compares the motifs discovered by our algorithms with those predicted by other 14 well-known biological methods. Finally, we outline the conclusions of this paper. 2. Related work
In this section we present some of the research literature related to the MDP. First, we will describe some of the latest research that apply evolutionary computation to discover motifs in DNA sequences. Next, we will organize and analyze the biological methods most commonly used to solve this problem.
There are many proposals based on evolutionary techniques for finding DNA motifs, an example is the algorithm FMGA ( Liu et al., 2004 ), a genetic algorithm based on the SAGA operators ( Notredame and Higgins, 1996 ). Another algorithms are St-GA ( Stine et al., 2003 ) and MDGA ( Che et al., 2005 ). Although there are other proposals such as the algorithm TS-BFO ( Shao and Chen, 2009 ) which integrates Bacterial Foraging Optimization ( BFO) and Tabu Search (TS), or the
EDA/DE algorithm proposed in Shao et al. (2009) , almost all the evolutionary algorithm proposals are based on genetic algorithms.
Furthermore, all of them employ a single objective to discover motifs, and the motif length is given beforehand, assuming only one motif per sequence. Moreover, almost all of the algorithms tries to find motifs in all of the given sequences. Fogel et al. (2004 , 2008) propose a new multi-term fitness function, the objective of this process was to maximize the motif similarities while avoiding saturation of low complexity solutions. However, the best way to address some of these problems is using a multiobjective approach. Kaya (2009) proposed a multiobjective GA based method named MOGAMOD for discovering motifs, demonstrating the advantages of multiobjective approach over single objective ones to discover motifs. Due to the advantages of the multiobjective optimization, we also use it in our problem definition. Our first tests were performed by using DEPT (Differential Evolution with Pareto Tournaments, Gonza  X  lez-A et al., 2010a ) and MO-VNS (Multiobjective Variable Neighborhood
Search, Gonza  X  lez-A  X  lvarez et al., 2010b ), two new multiobjective adaptations based on Differential Evolution (DE, Storn and Price, 1997 ) and Variable Neighborhood Search (VNS, Mladenovic and
Hansen, 1997 ). When we adapted the MDP formulation to the multiobjective optimization context, we could make comparisons between our proposals and the MOGAMOD algorithm in Gonza  X  lez-
A  X  lvarez et al. (2011c) . At this moment, after better understanding the biological aspects of the MDP, we have incorporated several con-straints and improvements which allow us to discover solutions with a high biological relevance. The improvements are detailed in Section 3 , among them we highlight the complexity concept, which discards the biologically irrelevant solutions. In this work, our multiobjective proposals are based on ABC ( Karaboga, 2005 )andGSA( Rashedi et al., 2009 ). Unfortunately, we have not been able to compare the results obtained by them with those obtained by MOGAMOD, due to the changes that we have made in the problem formulation.
 results in almost all areas where it has been applied, in this particular problem (MDP), the methods or techniques that are getting better results are the biological ones. Most of the literature categorizes motif discovery methods into two major groups based on the combinatorial approach used in their design: string-based methods and probabilistic methods. The string-based methods guarantee global optimality and they are appropriate for finding totally constrained motifs (all instances are identical). The probabilistic methods imply representa-tion of the motifs by a position weight matrix. Among the string-based methods there are many proposals as Oligo/Dyad-Analysis ( van
Helden et al., 1998 , 2000 ), MITRA (Mismatch Tree Algorithm) ( Eskin and Pevzner, 2002 ), YMF ( Sinha and Tompa, 2003 ), and QuickScore ( Regnier and Denise, 2004 ). However, one of the most popular string-based methods for discovering motifs is Weeder ( Pavesi et al., 2001 ).
Regarding to the probabilistic methods, Hertz et al. (1990) presented one of the first implementations for discovering motifs by using a matrix representation of TFBS. This algorithm has been improved over the years resulting in Consensus ( Hertz and Stormo, 1999 ).
However, the probabilistic methods that best predictions are given are those that apply powerful statistical techniques, two known alternatives are Expectation Maximization (EM) algorithm and Gibbs
Sampling (GS). Algorithms like MEME (Multiple EM for Motif Elicita-tion) and MEME3 (it differs from MEME mainly in using a correction factor) ( Bailey and Elkan, 1995 ) extended the EM algorithm for identifying motifs. Another importa nt statistical method is Improbizer ( Ao et al., 2004 ). GS is another algorithm that has been successfully applied in many other methods. Based on this algorithm, one that stands out is AlignACE (Aligns Nucleic Acid Conserved Elements) ( Roth et al., 1998 ). It uses a GS similar to that described by Neuwald et al. (1995) . There are other many proposals based on GS, for instance, ANN_Spec ( Workman and Stormo, 2000 ), MotifSampler ( Thijs et al., 2001 ), GLAM ( Frith et al., 2004 ), or the recently proposed
SeSiMCMC (Sequence Similariti es by Markov Chain Monte-Carlo) ( Favorov et al., 2005 ). Thanks to the work of Tompa et al. (2005) , we can compare the results obtained by our algorithms with those obtained by the 14 biological methods described in this section.
As we will see in the following sections, our algorithms obtain results with an important biological relevance. 3. Motif discovery problem to form an RNA sequence. Then this sequence is used to produce the corresponding protein sequence. This process starts when a macro-molecule called Transcription Factor (TF) has been bounded to a short subsequence in the promoter region of the gene, called TFBS ( Zare-Mirakabad et al., 2009 ). Finding TFBSs in
DNA sequences (problem known as MDP) is important for uncovering the underlying regulatory relationship and under-standing the evolutionary mechanism of living organisms. In this paper we solve the MDP finding biologically relevant DNA patterns in sequences of different beings. 3.1. Problem formulation
Given a set of sequences S  X f S i 9 i  X  1 ; 2 , ... , D g of nucleotides defined on the alphabet B  X f A , C , G , T g . S i  X f S j sequence of nucleotides, where w i is the sequence width. The set of 1 ; 2 , ... , w i l  X  1 g ,where j i isthebindingsiteofapossiblemotif to be maximized. To obtain the values of the other two objectives we have to build the Position Indicator Matrix (PIM) A  X f A i 1 ; 2 , ... , D g of the motif, where A i  X f A j i 9 j  X  1 ; 2 , ... , w row vector with respect to a sequence S i . A i j is 1 if the position j in S a binding site, and 0 otherwise. We refer to the number of motif instances as 9 A 9  X  P D i  X  1 P w i j  X  1 A j i .Wealsorequiretofindthe consensus motif, which is a string abstraction of the motif instances.
In this work, we consider a single motif instance per sequence. Only those sequences that achieve a motif instance of certain quality with respect to the consensus motif were taken into account when we perform the final motif. This is indicated by the second objective, the support .Furthermore, S  X  A  X  X f S  X  A  X  1 , S  X  A  X  2 , ... , S  X  A  X  motif instances, where S  X  A  X  i  X  S  X  A  X  1 i S  X  A  X  2 i instance in 9 A 9 . S ( A ) can also be expanded as  X  S  X  A  X  where S  X  A  X  j  X  S  X  A  X  j 1 S  X  A  X  j 2 ... S  X  A  X  j 9 A 9 position in the motif instances. Then, we build the Position Count
Matrix (PCM) N ( A ) with the numbers of different nucleotide bases on each position of the candidate motifs ( A ) which have passed the threshold marked by the support. N  X  A  X  X f N  X  A  X  1 , N  X  A  X 
N  X  A  X   X f N  X  A  X  j b 9 b A B g ,where N  X  A  X  j b  X  9 f S  X  A  X  nant nucleotides of each position are normalized in the Position objective, the similarity , averaging all the dominance values of each PFM column, as is indicated in the following expression: Similarity  X  Motif  X  X  where f  X  b , i  X  is the score of nucleotide b in column i in the PFM and max f f  X  b , i  X g is the dominance value of the dominant nucleotide in column i .

To guide the pattern search to solutions that have biological relevance, we have imposed to the objective functions several constraints that should be satisfied by each solution: (C1) Motifs are usually short ( D X  X aeseleer, 2006 ), so that, finding long motifs can mean to lose a great computational time. To avoid this, we have restricted the motif length to the range [7,64] nucleotides, where the minimum is seven and the maximum is 64. (C2) We have also set a minimum support value of two for the motifs of the sequence data sets composed of four or less sequences, and of three for the other ones (more than four sequences). Normally, the binding sites are composed of motifs of all or nearly all sequences, and without this constraint is very easy to predict motifs with a high similarity (even 100%) formed, for example, by candidates of only one DNA sequence. (C3) We have also applied the complexity concept proposed in
These three constraints must be met by all the generated solutions, i.e., if a solution does not meet the three defined constraints, it will be discarded and it will not be part of the population. 3.2. Example
As an example, Fig. 1 illustrates an artificial MDP with motif length  X  10. By using the motif instances shown in the first column of Fig. 1 , we first obtain the consensus motif GGGATTACAG. With this consensus motif we can calculate the value of the second objective. Those sequences which candidate motif exceeds or equals a threshold value of concordance of 50 %  X  Z 5 = 10  X  , will be taken into account in the support, in this example we have support  X  5 (see second column of Fig. 1 ). By using the candidate motifs which have surpassed the threshold value of Support, we can build the final motif. In the second column, we include this final motif and the selected candidate motifs. The last step is to build the PCM and the PFM by using the nucleotides of the motif instances that have passed the concordance threshold (see last column of Fig. 1 ). Having done that, we can obtain the value of the similarity, applying Eq. (1). In this example we obtain similarity  X  0.94. With the naked eye we see the similarity differ-ences between the consensus and the final motifs. If we compare the similarities obtained by both solutions we notice as if we do not apply the threshold value of Support, we would have obtained a solution with a similarity of 72%. However, taking into account this objective, we have eliminated the candidates that distorted the final solution, obtaining a solution with a similarity of 94%. 4. Description of the algorithms
In this section we detail the representation of the individuals, and we include a brief description of the algorithms compared in this work.
 4.1. Representation of the individuals
Each individual includes the necessary information to form a possible motif. An individual in our algorithms represents the starting location ( s i ) of the potential motif on all the sequences. As our data sets have different number of sequences, the definition of the individuals should be adapted to each data set. Our definition of the individual also includes the motif length. In
Fig. 2 , we show our individual representation. 4.2. Multiobjective artificial bee colony (MOABC)
ABC is an evolutionary algorithm defined by Karaboga (2005) motivated by the intelligent behavior of honey bee swarms. In
ABC, the colony of artificial bees contains three groups of bees: employed, onlookers and scouts bees. Employed bees go to their food source and come back to hive and dance on this area. The employed bee whose food source has been abandoned becomes a scout and starts to search for finding a new food source.
Onlookers watch the dances of employed bees and choose food sources depending on them. As the MDP is formulated as a multiobjective problem, we defined a new multiobjective algo-rithm in Gonza  X  lez-A  X  lvarez et al. (2011a) named MOABC. This algorithm incorporates several changes that allow its application in multiobjective optimization problems. The first modification applies the dominance concept ( Deb, 2001) in the greedy selec-tion functions used for improving the food sources of the employed and onlooker bees. However, the most important adaptation is made in the sorting process performed after each generation. This function selects the best food sources for the next generation. It applies two features of the standard NSGA-II ( Deb et al., 2002 ) algorithm, the nondominated sort and the crowding distance. First, it ranks the colony by using the dominance concept previously mentioned (nondominated sorting function), situating on the top of the colony, the solutions that are not dominated by any other solution, behind them it will be the solutions dominated by only one solution, and so on. With the food sources sorted, we have to select the best ones. These selected food sources will be exploited by the employed bees in the next generation. However, it may happen that not all the solutions of the last selected Pareto front can be chosen for the next generation. In this case, we have to apply the crowding distance concept, calculating the corresponding value for all the solutions of the conflicting Pareto front and choosing the sources of the best crowding distances. For further information about the
MOABC algorithm see Gonza  X  lez-A  X  lvarez et al. (2011a) . 4.3. Multiobjective gravitational search algorithm (MO-GSA) (2009) . It uses the Newtonian physics theory and its searcher agents are a collection of masses. In GSA, we have an isolated system of masses. Using the gravitational force, every mass in the system can see the situation of the other masses. All these objects attract each other by the gravity force, and this force causes a global movement of all objects towards the objects with heavier masses (better solutions). Hence, masses cooperate using a direct form of communication, through gravitational force. As the MDP is formulated as a multiobjective problem, we defined a new multiobjective algorithm in Gonza  X  lez-A  X  lvarez et al. (2011b) named MO-GSA. In this algorithm the most important modifica-tion is applied to assign the fitness value to the agents. In GSA, each individual must have a single fitness value so, if we want to design a multiobjective implementation, we can not consider the dominance concept. This new algorithm classifies the population into different Pareto fronts, ranking the individuals by using the
Pareto front and the crowding distance concept from the NSGA-II algorithm. Then, it applies a linear bias b r to the r th ranked element by using the expression: b r  X  1 = r , obtaining values from 1 to 1/ N . Thus, we obtain a sorted population with a single fitness value. For further information about the MO-GSA consult ( Gonza  X  lez-A  X  lvarez et al., 2011b ). 4.4. Nondominated sorting genetic algorithm (NSGA-II)
Genetic Algorithm (NSGA) based on the classification of the population at various levels. In this algorithm, before the selec-tion, we rank the population by using the dominance concept. All nondominated individuals are classified into a category with a dummy fitness proportional to the population size. In order to maintain the diversity of the population, these individuals are distributed according to their fitness, subject to a distribution parameter (sharing parameter). This classified group is removed of the population and the remaining individuals are reclassified by the same procedure. This process continues until all indivi-duals in the population are classified. Since the first individuals are of best quality, they always get more copies than the rest of the population, allowing the search in nondominated regions.
NSGA got promising results, however, was criticized for three main reasons: its computational complexity O( MN 3 ), where M is the number of objectives and N is the population size; its non-elitist operation, and the need to specify a distribution parameter.
To repair these limitations, Deb et al. (2002) proposed an enhanced version of this method, called NSGA-II. It solves the previously described limitations through a fast nondominated sorting mechanism with a low computational complexity O( MN a selection operator used to combine the parent and child populations, and selecting the N best solutions taking into account their quality and their distribution in the Pareto front.
Thanks to the good results achieved by this algorithm in a lot of multiobjective problems, it has become a standard multiobjective algorithm. In this work we have also developed it to have a point of reference on the results of our two proposals, MOABC and MO-
GSA. For further information about the NSGA/NSGA-II algorithms see Srinivas and Deb (1995) and Deb et al. (2002) . 4.5. Strength Pareto evolutionary algorithm (SPEA2)
Zitzler and Thiele (1999) presented another important algo-rithm, the SPEA. Zitzler and Thiele (1999) introduce a new evolutionary approach to multicriteria optimization that com-bines several features of previous multiobjective evolutionary algorithms in a unique manner. It is characterized by storing nondominated solutions externally in a second, continuously updated population, evaluating an individual fitness dependent on the number of external nondominated points that dominate it, preserving population diversity using the Pareto dominance relationship, and incorporating a clustering procedure in order to reduce the nondominated set without destroying its character-istics. Two years later the same authors proposed a second algorithm called SPEA2 ( Zitzler et al., 2001 ). It eliminated the potential weaknesses of its predecessor and incorporated most recent results in order to design a powerful and up-to-date evolutionary multiobjective optimization algorithm. The main differences of SPEA2 in comparison to SPEA are an improved fitness assignment scheme is used which takes into account, for each individual, how many individuals it dominates and it is dominated by, a nearest neighbor density estimation technique is incorporated which allows a more precise guidance of the search process, and a new archive truncation method guarantees the preservation of boundary solutions. Thanks to these improve-ments, as with the NSGA-II algorithm, SPEA2 has become a key heuristic to propose new multiobjective techniques. To show the quality of the results obtained by our heuristics, we have also implemented this algorithm. For further information about the
SPEA/SPEA2 algorithms see Zitzler and Thiele (1999) and Zitzler et al. (2001) . 5. Methodology
In this section we explain the methodology followed to configure each algorithm, we detail the data sets used in our experiments, and we show the results obtained by our algo-rithms. In the following sections we will compare the results obtained by our algorithms with those obtained by several standard multiobjective algorithms, and with those obtained by other 14 well-known biological methods. We will also include a powerful statistical analysis of the results to ensure their statis-tical relevance. 5.1. Experimentation, data sets, and parameter settings
For each experiment we have performed 30 independent runs to assure its statistical relevance. The results are measured using the hypervolume indicator ( While et al., 2006 ) to facilitate the comparison of performances. The results are displayed using the average values of the hypervolumes among the 30 runs. The reference volume is calculated using the maximum values of each objective in each data set, for example, a data set with five sequences will have: Support  X  5, Motif Length  X  64, and Similarity  X  1. The values of the objectives are normalized before calculating the corresponding hypervolumes. The experiments are organized taking into account the influence of each parameter in each algorithm. To compare the algorithms we have also used the coverage relation ( Zitzler et al., 2000 ) that is useful to analyze which algorithms get the best Pareto fronts. All experiments were performed using a Pentium 4 (2.8 GHz) with 1 GB of RAM, and the algorithms were compiled using gcc with no optimization options. In our experimentation we have grouped the results into three benchmarks. In each benchmark, we have used six sequence data sets corresponding to alive beings, more concretely, from fly (those beginning by  X  X m X ), from human ( X  X m X ), from mouse ( X  X us X ), and from yeast ( X  X st X ). In the first benchmark, we compare the behavior of the algorithms on  X  X  X eneric X  X  instances (those ending in  X  X  X ). This class of instances has the binding sites planted in randomly chosen genomic promoter sequences. In the second benchmark, we use  X  X  X eal X  X  instances (ending in  X  X  X ), where the binding sites are in their real genomic promoter sequences. Finally, the last benchmark is composed of  X  X  X arkov X  X  instances (ending in  X  X  X ), where the binding sites are planted in sequences randomly generated according to a Markov chain of order 3. These 18 sequence data sets are available in Tompa et al. (2005) , and are selected from the TRANSFAC database ( Wingender et al., 1996 ). Table 1 shows the properties of each data set. We have selected data sets with different number of sequences, with different sizes (nucleotides per sequence), and of different types (Generic, Real, and Markov), to ensure that our algorithms work well with several types of instances. By using data sets from different species and with different characteristics, we get strong algo-rithms that discover motifs in all types of biological data. The established runtimes are also shown (in seconds) in the last column of Table 1 . These runtimes are directly related to the complexity of each instance. After carrying out a large number of experiments, we observed how our algorithms required around 20 s to find good solutions in the instances composed of less than 10 sequences, while they needed around 30 s to solve the instances composed of 10 or more sequences.

In all our algorithms we have adjusted the value of each parameter to obtain the best configurations and so, the best results. The methodology followed to configure a parameter in our heuristics is always the same: first we establish, within the range of each parameter value, a minimum of five more or less equidistant values, for example, if we want to configure a parameter which value can be between 0% and 100%, we can test the following values: 10%, 25%, 50%, 75%, and 90%. After this, we run the experiments. As we can notice, we have not processed the values: 0% and 100%. If we use these values we are considering, for example, that there is no mutation or that it is always obligatory, so we believe that we should not try them. In the first algorithm (MOABC) we have configured four important para-meters: the population size ( PS ), the mutation probability ( F ), the mutation shift ( M ), and the number of scout bees ( SN ) that we add in each generation. To obtain the best value for the first parameter ( PS ) we have performed experiments with the values: 50, 75, 100, 125, and 150. After analyzing the results, we obtain the best hypervolumes by using 150 individuals. So, in order to obtain more accurate conclusions, we performed more runs with 175, 200, and 225 individuals, concluding that with a PS  X  200 the
MOABC achieves better motifs. The same happened with the second parameter, the F , when we confirm that with a probability of 10% the algorithm got the best solutions, we also test the values: 8%, 5%, 3%, and 1%, achieving a better performance with a
F  X  8%. Regarding M , we were testing values from a 10% increasing by 5% each time. We could conclude that with a 30% the algorithm performed better than with any other value. Finally, we analyzed the value of SN that is necessary to get the best discoveries. We have performed experiments with 1, 2, 3, 4, and 5 bees, conclud-ing that with a single scout bee the algorithm performs better. To sum up, after performing and analyzing all these experiments we can conclude that the best parameter values for the MOABC algorithm are PS  X  200, F  X  8%, M  X  30%, and SN  X  1. In the second algorithm proposed in this work (MO-GSA), we have only adjusted the population size , performing the same tests as in the previous algorithm. Again, we have concluded that the algorithm discovers motifs of better quality with a population size of 200 individuals. The other algorithm parameters ( G 0, a , and e ) were established with the values used by Rashedi et al. (2009) . Finally, it is important to say that we have carried out the entire configuration process on the two standard multiobjective algo-rithms implemented: the NSGA-II and SPEA2 algorithms.
In Table 2 , we include the parameters configured and the best values used for each configured parameter of each algorithm. 6. Algorithm comparisons three groups, in the first one, we use  X  X  X eneric X  X  instances, the second one uses  X  X  X eal X  X  instances, and the last one applies  X  X  X arkov X  X  instances. Now we proceed to analyze the behavior of the algorithms with  X  X  X eneric X  X  instances. The first analysis was made by using the hypervolume indicator. The hypervolume indicator ( While et al., 2006 ) is a measure of quality of a set duced in a run of a multiobjective optimizer. Assuming a mini-mization problem involving d objectives, this indicator consists of the measure of the region which is simultaneously dominated by
P and bounded above by a reference point r A R d such that r Z  X  max p p 1 , ... , max p p d  X  , where p  X  X  p 1 , ... , p relation Z applies componentwise. In Fig. 3 , we include the results (median and interquartile range) of the first group of instances. These instances have been sorted taking into account its complexity, thus the easier instances stay on the top of the table (higher hypervolumes), and the more complicated ones on the bottom (lower hypervolumes). Analyzing the numerical data of Fig. 3 , we can see how the best performing algorithm in five of the six instances tested is MOABC. We also note how our second proposal (MO-GSA) does not perform well in the first two instances (the simplest ones). However, after the third instance, the MO-GSA improves significantly its results, obtaining the second best hypervolume value in the more complicated instance.
In Fig. 3 , we also include a graphical representation of the hypervolumes to better observe the algorithm behaviors. In this first set of instances, we can conclude that the algorithms that best results obtain in the first instances are SPEA2 and MOABC, and those which behave best in the more complex instances are MOABC and MO-GSA.

To ensure that the differences among our algorithms are relevant, we have performed an exhaustive statistical analysis of the results obtained by each of them. The statistical study that we have carried out is the same as described by Sheskin (2007) . First, we apply a Kolmogorov X  X mirnov test for checking what type of distributions follow the results. If distributions are not Gaussian, we use the non-parametric Kruskal X  X allis test to compare the medians of the results. On the contrary, if the distributions are
Gaussian, we perform a Levene test to check the homogeneity of variances. If the results present homogeneity of variances, we apply an ANOVA test, otherwise, the Kruskal X  X allis test is used.
The results that overcome this exhaustive statistical analysis present significant differences. In all these tests we have con-sidered a confidence level of 95%, i. e., p -value  X  0.05. It is important to emphasize that we have applied the tests for each pair of algorithms, comparing the results obtained by each one with the results obtained by the other three algorithms sepa-rately. This whole process has been also repeated for each data set. The successful tests are indicated with a  X   X   X  in the result tables, and the non-successful tests are indicated with a  X   X . In the results shown in Fig. 3 , we only find three non-successful cases of the 36 possible combinations. The first one occurs in the simplest instance, and the rest are given between the standard multiobjective evolutionary algorithms SPEA2 and NSGA-II.
After this first hypervolume analysis, we have also studied the motifs discovered by the algorithms (the nondominated solu-tions) using a second indicator, the coverage relation. This con-cept was presented by Zitzler et al. (2000) in their study in multiobjective optimization using evolutionary algorithms. Applying the dominance concept ( Deb, 2001 ), the coverage relation considers that x 1 covers x 2 iff x 1 g x 2 or x coverage relation is applied to all nondominated solutions obtained by the algorithms and it is used as a comparison criterion. The results of this last comparison are presented in Table 3 . Analyzing the results, we see how the nondominated solutions discovered by our first proposal, the MOABC algorithm, cover the 94.53% of the NSGA-II solutions, and the 97.24% of the SPEA2 solutions. However, the nondominated solutions discov-ered by these two algorithms only cover the 14.83% and the 14.68% of the MOABC solutions, respectively. The same happens for the MO-GSA algorithm, while it is able to cover the 71.37% and the 76.40% of the solutions of NSGA-II and SPEA2, these two standard algorithms only cover the 23.46% and 23.84% of its nondominated solutions. If we compare the solutions discovered by our two proposals, we can conclude that the solutions found by MOABC are better than those discovered by the MO-GSA algorithm.

Next, we compare the results of the algorithms with  X  X  X eal X  X  instances. In Fig. 4 we include the hypervolume comparison. In this second benchmark, we have also organized the instances by complexity, being hm15r the simplest instance, and hm03r the most complex one. As happened in the first benchmark, the algorithms that perform better in the first instances are MOABC and SPEA2. However, we see how this standard multiobjective algorithm (SPEA2), together with NSGA-II, suffers an important quality drop in its results when the instances increase in complexity. Being the MO-GSA which best results achieves in the most complicated instance, surpassing by 9% and 14% the hyper-volumes of SPEA2 and NSGA-II, respectively. In this second benchmark, we only have one non-successful statistical test of the 36 cases, presented between SPEA2 and NSGA-II in the mus05r instance. In Fig. 4 we also include a graphical representa-tion of the algorithm behaviors. The coverage relation obtained by the algorithms in this benchmark is included in Table 4 . We see how our two proposals cover a large percentage of the NSGA-II and SPEA2 non-dominated solutions, while they are only able to cover a 25.22% of the solutions at the most. In this second analysis, we can conclude that our proposals are the algorithms that behave better when the instance complexity is increased.
The last benchmark analyzes the algorithm behaviors by using  X  X  X arkov X  X  instances. In Fig. 5 , we show a numerical and graphical representation of the results of this last set of instances. If we focus on data, we obtain the same conclusions as in the first two benchmarks. We see how SPEA2 and MOABC are the best algorithms when we use the simplest instances, while MO-GSA is the worse one. However, if we calculate the hypervolume decrease obtained from the first instance to the last one, we can conclude that the algorithm that achieves a lower difference is
MO-GSA, with a value of 31.243%. Second, the MOABC algorithm with a 34.811%. And finally, the SPEA2 algorithm (with a 40.430%) and NSGA-II (with 45.670%), respectively. Furthermore, MOABC and MO-GSA are again the algorithms with a better behavior in the more complex instances. In this benchmark, we have only one case without statistically relevant differences, between MOABC and SPEA2 in the dm03m instance. Finally, we include the cover-age relation achieved by the algorithms in Table 5 . In this table, we can note how, again, our two proposed swarm algorithms are able to cover a large percentage of the nondominated solutions of the standard multiobjective evolutionary algorithms SPEA2 and NSGA-II. Also, if we compare the results of our two algorithms, MOABC presents a better average performance.
 obtain several conclusions. First of all, we can say that the MOABC algorithm achieves better results than the other algorithms in almost all instances. Second, we can note how the MO-GSA is an algorithm that presents a good scaling capability, i.e., as the instances are complicated, its results are better in relation to those achieved by the other algorithms. In this sense, in the result tables we see how, in the first instances, the standard multi-objective evolutionary algorithms (SPEA2 and NSGA-II) present higher hypervolumes than MO-GSA. However, in the last sequence data sets (the most complex ones) MO-GSA overcomes the results obtained by the SPEA2 and NSGA-II algorithms. For all these reasons, we can conclude that we have compared two swarm algorithms which allow us to obtain relevant results. 7. Comparison with previous works swarm-based algorithms with those achieved by our previously proposed metaheuristics. The first techniques that we applied to solve the MDP as a multiobjective optimization problem were Differential Evolution with Pareto Tournaments (DEPT, Gonza  X  lez-
A  X  lvarez et al., 2010a ) and Multiobjective Variable Neighbourhood
Search (MO-VNS, Gonza  X  lez-A  X  lvarez et al., 2010b ). At present, after better understanding the biological aspects of the addressed problem (MDP), we have incorporated several improvements and constraints that allow us to better adapt its mathematical formulation to the real-world biological requirements, improving the quality of the discovered solutions (all these improvements are detailed in Section 3 ). As the two previously proposed metaheuristics (DEPT and MO-VNS) did not include these improvements in their corresponding references, we have incor-porated them to both algorithms in order to compare their results with those achieved by the two swarm-based algorithms pro-posed in this work. Their configuration settings are described in
Table 6 , and the experimental methodology followed is the same as described in Section 5 , conducting 30 independent runs and measuring the quality of the results with the same multiobjective indicators: hypervolume ( While et al., 2006 ) and coverage rela-tion ( Zitzler et al., 2000 ). The results of the first comparison are shown in Table 7 . As we can see, the results achieved by our best swarm-based algorithm (MOABC) are better than those obtained by our best previously proposed algorithm (DEPT), achieving the highest hypervolumes in 14 of the 18 solved instances. Regarding our second swarm-based algorithm (M O-GSA), although it achieves results slightly lower than DEPT, it achieves similar hypervolumes to MO-VNS. In fact, if we compare the results of these two metaheuristics (MO-GSA and MO-VNS) by using the second multi-objective indicator: coverage relation (see Table 8 ); we note how the MO-GSA algorithm covers more MO-VNS nondominated solutions than vice versa, covering an average percentage of 53.62% of the solutions of the MO-VNS, while this latter covers a 51.44% of the MO-GSA nondominated solutions. In addition, if we compare the cov-erages obtained by both MOABC and DEPT, we see how the MOABC algorithm covers the 58.65% of the DEPT nondominated solutions, while DEPT achieves an average coverage of 52.47% of MOABC solutions, confirming the conclusions drawn from hypervolumes. Thus, we have proposed two novel swarm-based algorithms that achieve quality solutions, overcoming those obtained by two pre-viously proposed algorithms based on two classical algorithms such as Differential Evolution and Variable Neighbourhood Search. 8. Comparison with other biological methods
In this section we analyze the motifs obtained by our two proposals, MOABC and MO-GSA algorithms. To that end we have compared the best motifs (nondominated solutions) discovered by both heuristics with the best solut ions predicted by 14 well-known biological methods in motif discovery. Thus, we demonstrate that the motifspredictedbyMOABCandMO-GSAhaveanimportantbiolo-gical relevance. The biological methods compared in this section are AlignACE ( Roth et al., 1998 ), ANN_Spec ( Workman and Stormo, 2000 ), Consensus ( Hertz and Stormo, 1999 ), GLAM ( Frith et al., 2004 ), Improbizer ( Ao et al., 2004 ), MEME ( Bailey and Elkan, 1995 ), MEME3 ( Bailey and Elkan, 1995 ), MITRA ( Eskin and Pevzner, 2002 ), Motif-Sampler ( Thijs et al., 2001 ), oligo/dyad-analysis ( vanHeldenetal., 1998 , 2000 ), QuickScore ( Regnier and Denise, 2004 ), SeSiMCMC ( Favorov et al., 2005 ), Weeder ( Pavesi et al., 2001 ), and YMF ( Sinha and Tompa, 2003 ). Short descriptions of them are provided in Tompa et al. (2005) . All the listed biological methods need similar runtimes to make good predictions, these times range from 20 to 60 s ( Kaya, 2009 ). As we included in Table 1 ,wehaveestablishedruntimes within that range for our algorithms to make possible a fair comparison of the discovered results. Therefore, we can conclude that the computational cost of ou r algorithms and the biological methodsissimilar.

For each method M and each data set D , we have the set of known binding sites and the set of predicted binding sites. The correctness of M and D can be assessed at the nucleotide level, defining four statistical/biological indicators such as: the Sensi-tivity ( nSn ) also called recall rate in some fields, which measures the proportion of actual positives which are correctly identified as such: nSn  X  nTP
The Positive Predictive Value ( nPPV ) also called precision rate is the proportion of real positives which are correctly identified: nPPV  X  nTP
The Performance Coefficient ( nPC ): nPC  X  nTP And finally, the Correlation Coefficient ( nCC ) which is the
Pearson product-moment coefficient of correlation in the parti-cular case of two binary variables. The value of nCC ranges from 1 (indicating perfect anticorrelation) to  X  1 (indicating perfect correlation): where PN  X  nTP  X  nFN , NP  X  nTN  X  nFP , PP  X  nTP  X  nFP , and NN  X  nTN  X  nFN .

The values of these four biological indicators range from 1 (indicating the worst prediction) to  X  1 (indicating the best predic-tion). Thus, if the predicted motifs exactly coincide with the known binding sites, all indicators will be  X  1. This means that the higher the values of the biological indicators the better predictions. For further information about these coefficients see Tompa et al. (2005) .
Table 9 shows the results of comparing our proposals with the previously defined biological methods by using these four statistical/ biological indicators: nSn , nPPV , nPC ,and nCC . To facilitate compar-isons between the best predictions of our algorithms and the best predictions of the 14 biological methods we have selected, for each data set, the two tools that achieve the best results for each indicator. For example, the methods that better Sensitivities ( nSn )obtainforthe dm04g data set are MotifSampler and ANN_Spec with 0.022. Thus, we facilitate the understanding of all this amount of biological informa-tion. By using this methodology, we can make statements like:  X  as our heuristics gets better results than t he best biological method, we can say Table 9 (a) X (d) we include in the second and the third columns the best method, and the corresponding indicator value. In the fourth and the fifth columns, we show the second best method and its result for each instance, and finally, we include the results of our algorithms in the last columns.
 That being said, we can proceed to analyze the results obtained. In Table 9 (a) we see how only in yst09g, yst05r and yst03m (3 data sets out of 18), our heuristics fail to overcome the results of the best biological method. The same happens for the nPC and nCC indicators. If we compare Table 9 (a) X (d) with Figs. 3 X 5 , we can notice how in the data set hm03r, where MO-GSA got the best hypervolumes, it also gets the best predictions. It is worth noting that the yeast data sets ( X  X st X ) are the most worked on a biological level, and thus, our algorithm obtains a minor difference with respect to the other methods. These results demonstrate that, in addition to obtain good resultsatcomputersciencelevel(throughindicatorssuchashyper-volume or coverage relation), we get very relevant biological motifs. It is also important to notice that the best results are usually obtained by the same biological method in each data set, e.g., for dm04g the best results are always obtained by MotifSampler, or for hm03r the best results are always achieved by MEME. However, our algorithms achieve very good results in all data sets, regardless of the species (fly, human, mouse, or yeast) and the natu re of the instance (Generic, Real, or Markov). It makes possible to expect that our multiobjective versions of ABC and GSA can also obtain good results for genomes of other different beings.

Finally, in Table 10 we include all the necessary information to compose some of the better predictions discovered by our algo-rithms, so any researcher can verify that the information dis-played in this work is true. 9. Conclusions and future work
In this paper, we have compared two novel swarm intelligence based algorithms: ABC and GSA to solve the MDP. Moreover we have adapted these algorithms to the mul tiobjective context, resulting in two new algorithms named MOABC and MO-GSA. This work differs from previous approaches to MDP, because our new constraints focuses on real-world aspects of biology, e.g., the motif complexity concept. In this work, we have also combined computational and biological aspects, demonstratin g through several indicators and statistics that the results obtained are relevant in both fields. To summarize, some of the contributions of this work are the formula-tion presented and applied to solve the MDP, and the comparison made among our algorithms (MOABC and MO-GSA) and other standard multiobjective algorithms (SPEA2 and NSGA-II) and also 14 well-known biological methods, where we analyze the behavior of each of them in three benchmark scomposedofsixinstancesof different nature. As we have seen, MOABC is the algorithm that best results have obtained in most instances.

Evaluating new algorithms for this problem is a matter of future work. In particular, we have planned comparisons with other known swarm-based algorithms such as Particle Swarm
Optimization (PSO), since this is a clear example of swarm algorithm which has received a lot of attention in the literature.
Furthermore, we will also investigate the application of parallel or distributed techniques for solving the MDP.
 Acknowledgments
This work was partially funded by the Spanish Ministry of Science and Innovation and ERDF (the European Regional Development
Thanks also to the Fundacio  X  n Valhondo, for the economic support offered to David L. Gonza  X  lez-A  X  lvarez to make this research. References
