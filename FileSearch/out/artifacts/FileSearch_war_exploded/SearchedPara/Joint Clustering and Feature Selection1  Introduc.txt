 In many applications in data mining and machine learning, one is often con-fronted with very high dimensional data. High dimensional data require more time and space to process. Moreover, due to the existence of many irrelevant and/or redundant features, learning algorithms tend to overfit. To overcome this problem, feature selection techniqu es are designed to select a subset of fea-tures from the high dimensional feature set for a compact and accurate data representation.

Feature selection methods can be classified into supervised and unsupervised methods. Supervised feature selection al gorithms, such as Fisher score [1], robust between features and labels. However, in practice, the labels are expensive to ob-tain. Hence, it is important to develop unsup ervised feature sel ection algorithms by using all the data points without labels.

Recently, several unsupervised algor ithms have been proposed to leverage both the manifold structure and learning mechanism. These methods include Laplacain Score (Lap Socre) [4], Spectra l Feature Selection (SPEC) [5], Multi-Cluster Feature Selection (MCFS) [6], Unsupervised Discriminative Feature Se-lection (UDFS) [7], joint Feature Selection and Subspace Learning (FSSL) [8] and Joint Embedding Learning and Sparse Regression (JELSR) [9]. Commonly, these approaches characterize manifold structure via various graphs and select features with different learning mechanism. LapScore and SPEC rank each features by computing different metrics. MCFS achie ves feature selection by Spectral Re-gression [10]. Both of UDFS, FSSL and JELSR can viewed as an integration of embedding with different graphs and sparse subspace learning via 2 , 1 -norm regularization.

Besides, to inherit the discriminative power of supervised approach, several recent work [11 X 13] incorporate supervi sed subspace learning technique into the clustering framework. Empirical resul ts have shown performance improvement with other popular clustering algorithms.

Based on the above motivation, in this paper, we propose a new approach, called Jointly Clustering and Feature Selection (JCFS), for unsupervised feature selection. Specifically, we integrate supe rvised feature selection technique (Fisher Score) and spectral clustering (manifold learning) in a unified framework. Based on the selected features, we jointly maxi mize Fisher criterion for feature se-lection and minimize the spectral cluster ing criterion to preserve the manifold structure. Compared with traditional unsupervised feature selection approaches, our method could integrate the merits of supervised feature selection and clus-tering (manifold learning). We also dis cover the connection between JCFS and other clustering and feature selection met hods, such as spectral clustering, dis-criminative K-means, JELSR and Discriminative codebook selection [14]. Many experimental results are provided for demonstration. Suppose we have n data points { x i } n i =1  X  R d , the data matrix is denoted by X =[ x 1 ,..., x n ]  X  R d  X  n .Weuse x i to represent the i -th feature of X .We represent a clustering of the data points by a partition matrix P =[ p 1 ,..., p c ]= [ p Instead of directly using the entries of partition matrix P as the cluster labels, we assume that X has been centered with zero mean, i.e., n i x i = 0 . In this section, we briefly introduce Fisher Score [1] and Spectral Clustering [15]. 3.1 Fisher Score for Feature Selection The key idea of Fisher score [1] is to find a subset of features, based on theses selected features the distances between data points in different classes are as large as possible, while the distances between data points in the same class are as small as possible. The Fisher Score can be formulated as the following Ratio Trace optimization problem: where tr(  X  ) is the trace of a squared matrix, X m is the data matrix represented by m selected features, the total scatter matrix S t and the between-cluster scatter matrix S b are defined as follows [12]: The regularization parameter  X &gt; 0 in (1) is used to keep the total scatter matrix non-singular. To represent whether a feature is selected or not, we introduce an score in (1) can be equivalently reformulated as follows, where diag( z ) is a diagonal matrix whose diagonal elements are z i . The objective function in (3) can be further simplified as follows: It is easy to verify that Y T Y = I . Therefore, it can be further simplified as min 3.2 Spectral Clustering Spectral clustering is one of the most po pular clustering methods in recent years, which exploits the eigen-structure of a specially constructed matrix. Generally, the objective function of spectral clustering [15] algorithms can be defined as: where L is a Laplacian matrix to approximate the manifold structure with dif-ferent choices. In order to capture the lo cal data structure, one can construct a nearest neighbor graph with an affinity matrix A  X  X  n  X  n . The simplest defini-tion of A is as follows: The normalized Laplacian matrix L is then defined by L = I  X  D  X  1 / 2 AD  X  1 / 2 , where D is a diagonal matrix with the diagonal elements as D ii = n j A ij ,  X  i . The optimization problem in (6) can be solved by the eigenvalue decomposition. Based on the continuous solution, the final discrete solution is then obtained by K-means or spectral rotation [15]. In this section, we will integrate Fisher score and spectral clustering in a unified framework. The key idea of our method is to find a subset of features, based on which we jointly maximize the Fisher criterion for feature selection and minimize the spectral clustering criterion to best preserve the manifold structure. It can be mathematically formulated as follows, where  X  and  X  are two tradeoff parameters. Due to the integer constraints on z problem (8) is a mixed integer programming [16]. 4.1 Optimization of JCFS Algorithm In this section, we propose to develop an iterative algorithm to solve the opti-mization problem in (8).
 The Computation of Y for Given z Though a matrix inverse is involved, we will show that the inverse can be ef-ficiently computed without explicit inverse operation. The above optimization problem integrates Laplacian matrix an d inverse covariance matrix based on se-lected features in a unified framework, which can be solved by the eigenvalue decomposition.
 The Computation of z for Given Y . Givenanexisting Y , the above op-timizing with respect to z is equivalent to solving the following minimization problem The above problem is NP-hard due to the combination nature. To solve it ef-ficiently, we can relax the integer constraints on z using nonnegative 1 -norm regularization. The following relaxed problem is a convex optimization problem which can be solved efficiently. where  X &gt; 0 is a regularization parameter to control the sparseness of z .Note that (11) is no longer equivalent to (10) due to the relaxation. In other words, the relaxation can be seen as a tradeoff between the strict equivalence and com-putational tractability.

Remember that our goal is to selected m features, we did not solve the above convex optimization problem in this paper. Instead, we solve a sequential op-timization problem by specifyin g the number of selected features m ,whichis particularly convenient for feature selection. Suppose the first t features have been selected, i.e., x 1 ,..., x t , then the ( t + 1)-th feature can be obtained by solving the following problem: By using the Woodbury-Morrison formula [17], we have to solving the following problem which is computed on each feature independently. The above process is repeated until we have selected m features. More details about the sequential optimization scheme can be found at [14, 18]. We summarize the complete JCFS algorithm for feature selection in Algorithm (1). In this section, we discuss the connectio n between JCFS and SEC, Discriminative K-means, JELSR [9] and DCS [14]. The first two are proposed for clustering task, while the last two are designed for unsupervised feature selection. 5.1 Connection between JCFS and Spectral Clustering JCFS reduces to spectral clustering, if  X  is set as zero. Therefore spectral clus-tering is a special case of JCFS. Algorithm 1. JCFS for Feature Selection 5.2 Connection between JCFS and Discriminative K-Means Several recent work [11,12] incorporate s upervised dimension reduction such as Linear Discriminant Analysis (LDA) [19] into the clustering framework, which jointly learn the low-dimensional subspa ce and clustering. For instance, Discrim-inative Clustering methods solve the following optimization problem: There are two sets of variabl es, the projection matrix W and the scaled cluster assignment matrix Y , in (15). The above optimization problem can be simplified by optimizing Y only based on the following observation [12]: where the equality holds when W = VS ,and V is composed of the eigenvectors nonsingular matrix. Based on (16), JCFS reduces to Discriminant K-means, if  X   X  X  X  and m = d . Therefore Discriminant K-means is a special case of JCFS. 5.3 Connection between JCFS and JELSR JELSR [9] integrate embedding learning and sparse regression in a unified frame-work to perform unsupervised feature sel ection. It can be formulated as follows. The sparse subspace W and embedding Y in (17) are learned iteratively. To analysis the connection between JCFS and JELSR, we follow the updating rules of JELSR. Suppose the t -th iteration of W is given by W t ,the( t +1)-th iteration can be obtained by W t +1 =( XX T +  X U )  X  1 XY ,where U is a diagonal matrix min which can be further reformulated into where z i = || w i || 2 .When Y is fixed the optimization of W is solved by the following regularized problem, It is mathematically similar to Group Lasso problem [2] and multi-task feature selection [20]. Based on above formulation, we found that JELSR and JCFS solve similar problem to update Y , the difference lies JCFS only use selected features (0-1 weights of features) while JELSR use all features weighted by its norm || w which leads to sparsity of W , however JCFS directly select features to maximizes Fisher score.

Besides, the differences between JCFS and MRSF [21], MCFS [6] can also be obtained based on the above analysis. MRSF and MCFS are two-stage ap-proaches, which both first learn the graph embedding. MRSF then selects fea-tures by solving the optimization problem (20), while MCFS selects features via solving K independently LASSO problems. 5.4 Connection between JCFS and Discriminative Codeword DCS [14] was proposed to minimize the fitting error based on selected features, which can be formulated as follow. After substituting the optimal value of W and b into (21), the above optimization problem is equivalent to where H n = I n  X  n  X  1 n 1 n 1 T n is the centering matrix.

It is clear that the above formulation is essentially the same as the discrim-inative feature selection part in JCFS, however we derive JCFS from super-vised feature selection ( Fisher Score ) criterion, while DCS is derived from ridge regression . Besides JCFS also integrate spect ral clustering to capture mani-fold information which have been shown to be useful for unsupervised feature selection [4,6]. proposed JCFS for feature selection. We pe rform clustering and nearest neighbor classification experiments by only using the selected features. The following five unsupervised feature selectio n algorithms are compared:  X  Max Variance which selects those features of maximum variances in order  X  Laplacian Score [4] which selects the features most consistent with the Gaus- X  Multi-Cluster Feature Selection (MCFS) 1 [6] which selects features using  X  JELSR [9] which performs feature selection via unifying the graph embedding  X  Our proposed JCFS algorithm. 6.1 Data Sets We use four real world data sets in our e xperiments, the processed version of these data sets can be obtained from Cai X  X  page 2 .

The first one is the ORL face database which consists of a total of 400 face images, of a total of 40 subjects (10 samples per subject). The size of each cropped image is 32  X  32 pixels, with 256 gray levels per pixel. Thus, each face image can be represented by a 1,024-dimensional vector.

The second one is the COIL image library from Columbia. It contains 20 objects. The images of each objects wer e taken 5 degrees apart as the object is rotated on a turntable and each objects has 72 images. The size of each image is 32  X  32 pixels, with 256 gray levels per pixel. Thus, each image is represented by a 1,024-dimensional vector.

The third one is Isolet spoken letter recognition data 3 .Thisdatasetwas generated as follows. 150 subjects spoke the name of each letter of the alphabet twice. The speakers are grouped into sets of 30 speakers each, and are referred to as isolet1 through isolet5. In our experiments, we use isolet5 which consists 1559 examples with 617 features.

The fourth one is Extended Yale-B database 4 contains 16128 face images of 38 human subjects under 9 pose and 64 illumination conditions. In our experiment, we choose the frontal pose and use all the images under different illumination, thus we get 2414 image in total. They are resized to 32  X  32 pixels, with 256 gray levels per pixel. Thus each face image is represented as a 1024-dimensional vector. 6.2 Clustering We perform K-means clustering by using the selected features and compare the results of different algorithms in this test.
 Evaluation Metrics. The performance is evaluated by comparing the labels obtained using clustering algorithms with the ground truth labels. We use Clus-tering Accuracy (ACC) and Normalized Mutual Information (NMI) to measure the clustering performance.
 Parameter Settings. Several parameters need to be set beforehand for these algorithms. All the compared algorithms except Max Variance are need to de-termine the parameter k which specifies the number of nearest neighbors. Gen-erally speaking, k should be set as a small number to preserve the local manifold structure. To fairly compar e their performances, we fixe k as 5, used in [6, 7], for all the algorithms on all the data sets. For Lap Socre, MCFS, and JCFS we use binary weights to represent the nearest graph for its simplicity. The di-mensionality of graph embedding in MCFS and JELSR is fixed as the number of clusters. For JELSR, we fix  X  = 1 and search regularization parameter  X  tal number of clusters c is provided for all the clustering algorithms. To reduce the statistical variation of K-means results, we independently repeat the exper-iments for 100 times with random initializations. We report the best average results corresponding to the best objective values in terms of ACC and NMI respectively.
 Clustering Results. Table (1, 2 , 3, and 4) show the clustering performance ver-sus the number of selected features m on different data sets. As can be seen, JCFS performs better than the other methods in most cases. Comparing with second best method, JCFS achieves 8.1% (4.2%), 14.1% (10.1%), 4.9% (7.4%), and 8.8% (8.4%) relative improvements in aver age when measured by ACC (NMI) on the ORL, COIL20, ISOLET, YABLEB data sets, respectively. This show that the idea of integrating supervised feature select ion into the clustering framework is bene-ficial in designing unsupervised feature selection methods. It would be interesting to note that, on these data sets, our proposed algorithm performs surprisingly well by using only very few features, such as 5 and 15 features. 6.3 Nearest Neighbor Classification In this subsection, we evaluate different feature selection algorithms in the clas-sification task. We perform nearest neighbor classifier (1-NN) with the selected features. Classification accuracy is us ed to measure the performance. The ex-perimental results are shown in Table 5, we can observe that JCFS is compet-itive with other algorithms for nearest n eighbor classification different selected features m . 6.4 Parameters Selection Though our algorithms have three parameters, that are, regularization param-eters (  X  and  X  ) and the number of nearest neighbors k . The parameter k is commonly used for manifold learning and spectral clustering, and the results are often stable when k is small. The parameter  X  can be safely set as a small num-ber. Due to space limitation, we only show the clustering results with different  X  for different selected features m . The data set used for this test is the ORL face database. The experimental results are shown in Fig 1 In this paper, we have proposed a new unsupervised feature selection approach which selected features that jointly maxim ize the supervised Fisher criterion and best preserve the manifold information. We show that spectral clustering, dis-criminative K-means, DCS are all the sp ecial cases of JCFS. We also prove that JELSR and JCFS share similar objective function, where features in JELSR are weighted by its norm while JCFS use a 0-1 weight scheme. Our preliminary results on clustering and classificatio n with selected features show that JCFS outperforms the compared algorithms. In future, we plan to explore other super-vised feature selection techniques into th e clustering framework for unsupervised feature selection.
 Acknowledgments. We would like to thank all anonymous reviewers for their helpful comments. This work is supported in part by NSFC grant 60970045 and China National 973 project 2013CB329305.

