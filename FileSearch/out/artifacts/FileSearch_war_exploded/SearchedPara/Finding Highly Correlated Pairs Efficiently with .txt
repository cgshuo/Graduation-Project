 We consider the problem of finding highly correlated pairs in a large data set. That is, given a threshold not too small, we wish to report all the pairs of items (or binary attributes) whose (Pearson) correlation coefficients are greater than the threshold. Correlation analysis is an important step in many statistical and knowledge-discovery tasks. Normally, the number of highly correlated pairs is quite small compared to the total number of pairs. Identifying highly correlated pairs in a naive way by computing the correlation coefficients for all the pairs is wasteful. With massive data sets, where the total number of pairs may exceed the main-memory ca-pacity, the computational cost of the naive method is pro-hibitive. In their KDD X 04 paper [15], Hui Xiong et al. ad-dress this problem by proposing the TAPER algorithm. The algorithm goes through the data set in two passes. It uses the first pass to generate a set of candidate pairs whose cor-relation coefficients are then computed directly in the sec-ond pass. The efficiency of the algorithm depends greatly on the selectivity (pruning power) of its candidate-generating stage.
 In this work, we adopt the general framework of the TA-PER algorithm but propose a different candidate-generation method. For a pair of items, TAPER X  X  candidate-generation method considers only the frequencies (supports) of individ-ual items. Our method also considers the frequency (sup-port) of the pair but does not explicitly count this frequency (support). We give a simple randomized algorithm whose false-negative probability is negligible. The space and time complexities of generating the candidate set in our algorithm are asymptotically the same as TAPER X  X . We conduct ex-periments on synthesized and real data. The results show  X 
Supported by N SF grant 0331640.  X 
Supported in part by HSA RPA grant AR O-1756303, ONR grants N00014-01-1-0795 and N00014-04-1-0725, and NSF grants 0331548, 0428422, and 0534052.
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. Computing correlation coefficients for such a huge number of pairs can be prohibitively expensive. Moreover, in many cases, there will not be enough main memory to hold all the pairs. Although one can turn to external-memory com-putations, the performance deteriorates to an unacceptable level. Hence, in these situations, it is critical for the mem-ory requirement of an algorithm to be much smaller than the size of the input data set. This is possible because it is often the case for real-world data that the number of highly correlated pairs is much smaller than the total number of possible pairs. Storing all the pairs and computing all the correlation coefficients are wasteful.

To address the efficiency and the memory-shortage prob-lem in finding correlated pairs, Hui Xiong et al. [15] proposed the TAPER algorithm. The algorithm makes two passes through the data set. In the first pass, it prunes many pairs that are not highly correlated and generates a set of candi-date pairs whose correlation coefficients may be above the threshold. The correlation coefficients of these pairs in the candidate set are computed in the second pass. The pairs that are indeed highly correlated are then identified. The size of the candidate set can be much smaller than the total number of pairs. By paying a fairly small price (one more pass through the data), the algorithm saves the computation for the pruned pairs. Furthermore, the algorithm requires less memory than the naive one, making it more suitable for massive data.

Clearly, the efficiency and the memory requirement of the algorithm depend strongly on the effectiveness of the prun-ing method used in the candidate-generation stage. To de-cide whether a pair ( a, b ) should be pruned, the TAPER algorithm uses a rule that considers only the frequencies of individual items a and b (or, from an association-rule-mining point of view, the supports of a and b ). Computational sim-plicity is the advantage of this pruning rule. One needs to count only the supports of the individual items. The disad-vantage is that a relatively large group of uncorrelated pairs is missed by the pruning rule. Intuitively, the pruning rule used in TAPER can be viewed as the following: We prune the pair ( a, b ) if the support of a and the support of b differ a lot. Consider a data set with items X  support distribution shown in Fig. 1. (The x axis is the value of the supports, and the y axis is the number of items with that support value.) Consider two areas L and S . Items that fall in L have large supports, and items that fall in S have small sup-ports. TAPER X  X  pruning rule removes pairs that consist of one item from L and the other from S . The pairs that draw both items from S (or L ) are selected by TAPER as candi-dates. However, two items a and b of similar small supports are often not correlated because the item pair ( a, b )hasex-tremely small support. There can be many such pairs. In particular, in a large database, most of the items have sim-ilar small supports, but many are not correlated. Because TAPER X  X  pruning rule includes these pairs in the candidate set, the candidate set is relatively large. We observe that TAPER X  X  pruning can be further improved by considering the supports of the pairs.

However, there is a basic obstacle to making such improve-ment. At first glance, it seems the new pruning process re-quires that we get the supports of all the pairs. On the other hand, the goal of the pruning is exactly to avoid doing so. A pruning process is meaningless if it needs to get the statis-tics of all the pairs. That is the crux of the challenge we memory capacity of computers, an algorithm X  X  efficiency and applicability strongly depend on its memory requirement. We did not use extremely large data sets in the experiment. Instead, we closely examined the memory requirements of the algorithms being compared. The experiments showed that our algorithm has a much smaller (several orders of magnitude smaller) memory requirement than other algo-rithms. Hence, it is much more usable and efficient in deal-ing with massive data sets.  X  X arket basket X  is a powerful abstraction used for mod-eling data. The market-basket abstraction is particularly useful for identifying association (correlation) patterns in the data. Mining association rules [1] is an often encoun-tered problem in many data-mining tasks. There are many algorithms [2, 3, 4, 9] that exploit different constraints for efficient mining of association rules. Two measures are often considered when judging the significance of an association rule: its support and its confidence. Other quantities that measure the interestingness of association rules are studied in [6, 13, 14].

Identifying associations (correlations) according to statis-tical measures is also an important problem. It has been considered in statistics, but not primarily from an algo-rithmic point of view. The computations in the statistics literature are often done by brute-force algorithms. Previ-ously, Xiong et al. [15] considered the problem of identify-ing strongly correlated pairs, using Pearson coefficients as the measure of correlation. In this paper, we propose an algorithm that adopts the general framework of the TAPER algorithm in [15] but uses a different method for pruning weakly correlated pairs.
The rest of this paper is organized as follows: In sec-tion 2, we introduce a rule for generating a candidate set and give an efficient candidate-generation method. In sec-tion 3, we present the whole algorithm and analyze the time and space complexity of our candidate-generation method. In section 4, we provide experimental results. The results show that our algorithm generates a much smaller candi-date set and is faster than TAPER. Section 5 summarizes the paper and discusses some future work. Our pruning process is based on the following observation: For a pair ( a, b ), if the correlation coefficient of the pair is above a threshold  X  , the Jaccard distance between the set of baskets containing a and the set of baskets containing b must be smaller than f (  X  ), for a function f that we will specify later. With this observation, to generate candidates is to select pairs of items that are close in the Jaccard space. We use min-hash to estimate the Jaccard distances. A second hash is used to group together the pairs that are close to each other in the Jaccard space. The candidate set then consists of the pairs in such groups. In this way, we use the supports of the pairs in the pruning process without having to count these supports. The process does not need to consider all thepossiblepairs.

Now we describe the pruning process in detail. First, we need some notation. Data in a market-basket model can be |
R ( a )  X  R ( b ) | The last inequality comes from the fact that  X , S  X  1. Sim-ilarly, |
R ( a )  X  R ( b ) | Here the last inequality comes from the fact that S  X   X  . Given 1  X  S  X   X  , this ratio achieves its minimum value of  X  2
Candidate-Generation Rule: We put the pair ( a, b ) ( a, b ) is selected if the Jaccard distance between R ( a ) and R ( b ) is smaller than 1  X   X  2 .

Note that this bound is tight. That is, if we prune a pair remove a pair whose correlation coefficient is slightly above  X  . To see this, consider two items a and b with R ( a )  X  R ( b )= R ( a ). Also, assume sp ( a )and sp ( b ) are very small. In this case, (1  X  sp ( a ))  X  1, and (1  X  sp ( b ))  X  1. The correlation coefficient reduces to correlation coefficient is above  X  .

Next, we describe a method that can efficiently prune (or generate candidates) according to our rule. The candidate-generation method uses min hashing, which was introduced in [7, 11]. A min-hash function h min maps an item in the data set to a number and has the following property: Given two items a and b , The following is a simple min-hash function from [11]: Let is put in the candidate set. The process then stops. Note that the pair (3, 9) is not in the candidate set. This agrees with the fact that there is no vector of item 3 that is equal to a corresponding vector of item 9. Figure 2: Shape of the Function f ( x )=1  X  (1  X  x k ) t
Fig. 2 shows the function f ( x )=1  X  (1  X  x k ) t for several values of k and t . f ( x ) has an  X  X  X  shape and approximates a threshold function, i.e., a function g ( x )thattakesthe value 1 when x is larger than the threshold and 0 when x is smaller than the threshold. f ( x ) is an approximation of such a threshold function. We observe that k determines the sharpness of the transition from 0 to 1. t works with k to determine where the transition happens. For an ideal pruning result, we want the transition to be sharp, i.e., f to behave more like g . By choosing values for k and t properly, we can place into the candidate set the pairs that satisfy probability close to zero.
We describe our full algorithm in this section. Our al-gorithm adopts the general framework of TAPER. To find the highly correlated pairs, it makes two passes through the data set. After the first pass, we generate a set of candidates. In the second pass, the correlation coefficients of the candi-dates are computed, and the pai rs that are truly correlated are identified.

In the first pass, our algorithm performs min hashing for each item. The min-hash value of an item a is simply the minimum of the set { h ( r ) | r  X  R ( a ) } . It is easy to see that this computation can be performed in one pass through the data. When using multiple min-hash functions, all hashes of an item can be computed at the same time X  X here is no need for multiple passes. Recall that we use a total of k  X  t in-dependent hash functions. The parameters k and t are used to control the pruning and the probability of false negatives. k controls the effectiveness of the pruning. By using a large k , we can prune more pairs that do not satisfy our rule. In general, k can be determined by the limits on the available computational resources. t controls the probability of false negatives. The larger the t , the smaller the probability that we drop a pair satisfying our rule.

Once we have a value for k ,wechoose t according to the threshold  X  and false-negative tolerance  X  .Wefirstdis-cuss the meaning of  X  and then describe how we choose its value. Note that, when we fix the values of both k and t , the stronger a pair X  X  correlation is, the more likely it is that this pair will be placed in the candidate set. This is deter-mined by the probability function in Fig. 2. A pair ( a, b )has probability 1  X  (1  X  x k ) t to be in the candidate set, where in the candidate set. In other words, our candidate genera-tion favors strongly corre lated pairs. We choose t such that, this pair from the candidate set is below  X  . (This is why we call  X  the  X  X alse-negative tolerance. X ) Note this does not mean that our candidate generator misses every highly correlated pair with a probability  X  . Only the pairs ( a, b ) correlation) have probability of  X  being left out. The other pairs with stronger correlation have false-negative proba-bility smaller than  X  . The formula for choosing t is then
After going through the data set, we have k  X  t hash values for each item. They are grouped into t vectors each of size k . Using the equivalence relation defined in the previous section, we partition the items into equivalence classes. We can do so with the help of a hash table. The hash table takes a vector of size k as a key and hashes the item into one of its buckets. The items in the same bucket form an equivalence class, and we add to the candidate set the pairs formed by two items in the same class. Note that each item has t vectors and thus t keys. Therefore, this process is repeated t times, each time using a different key. Once we have the candidate set, the second pass is straightforward. The whole algorithm is presented in Algorithm 1.

We now analyze the space and time complexity of our candidate-generation method. We first consider the memory requirement. For each item, we store k  X  t min-hash val-ues. Because k and t are two constants determined only by the threshold  X  and the false-negative tolerance  X  ,theto-tal memory required by our candidate-generation method is O ( k  X  t  X  n )= O ( n ).

Now we consider the time complexity. While going through the data set, for each item i contained in basket (row) j ,the row number j is hashed, and the hash value is compared to the currently stored minimum value. The current minimum is replaced if necessary. Clearly, this process requires time only necessary to go through the input. After going through the data set, we partition the set of items into equivalence classes according to the equivalence relation. Using a hash table, this can be done in O ( n ) time. Since we repeat the partition process t times, the total time requirement is again O ( t  X  n )= O ( n ).

In summary, the time and space complexity of our candidate-generation method are asymptotically the same as that of TAPER X  X  candidate-generation process.
In this section, we report the results of testing our algo-rithm on several data sets. The experimental results show that our candidate-generation method prunes the unwanted pairs effectively, producing a small candidate set, and there-fore achieves small overall running time. We first show, in
We measured two types of running times. One is the overall running time of the program. The other is the run-ning time of the candidate-generation phase. In Fig. 3 and Fig. 4, we plot the overall and the candidate-generation run-ning times of our algorithm, respectively, and compare them with those of the TAPER algorithm.

Our algorithm achieves smaller overall running time in these test cases. This is mainly due to the very small candi-date set generated by the algorithm. In most cases, because the candidate set is so small, the second pass of our algo-rithm takes only a few seconds.

The comparison of the running times for candidate gen-eration is interesting. In some cases (e.g.,  X  =0 . 3forthe  X  X etail X  data set), our candidate-generation process takes much longer than TAPER X  X  candidate-generation process, while in some other cases (e.g.,  X  =0 . 5 , 0 . 7 for the  X  X etail X  data set), our candidate-generation process takes less time.
In both algorithms, there are two steps in the candidate-generation process. First, they need to go through the input data in one pass. Second, they generate the candidate set. When TAPER goes through the input data, it counts the frequencies of the items. In our algorithm, we need to com-pute the min-hash values for the items. For each item, we may compute several min-hash values. Therefore, in the first step, our algorithm takes more time than TAPER, par-ticularly when  X  is small and we compute many min-hash values. In the second step, for each pair generated as a The first one measures how small the candidate set is, com-pared to the set of all possible pairs. We call this quantity the shrink ratio ( SR ) of the candidate set. It is defined to be the ratio of the size of the candidate set to the total number of pairs. Note that the measurement called  X  X runing ra-tio X  defined in [15] is exactly 1  X  SR . Our second quantity measures how large the candidate set is, comparing to the number of the truly correlated pairs. By truly correlated pairs, we mean the pairs whose coefficients are above the threshold. We call this measurement the expand ratio ( ER ) of the candidate set. A good candidate set should have both small ER and small SR .Asmall SR meanswesavealotby pruning many unwanted pairs. A small ER means that the candidate set does not contain too many false positives. We remark that the false negative rate of our candidate sets in these experiments is below 0.005. That is, our candidate set misses less than 5 / 1000 of the truly correlated pairs. Hence, when our algorithm achieves a small ER ( SR ), it is indeed asmall ER ( SR ), not one made by removing a lot of truly correlated pairs.

We first examine the shrink ratio of the candidate set generated by our algorithm and compare it to TAPER X  X  ratio. We plot the shrink ratios in Fig. 5. The values of the ratios are represented by the heights of the bars. For each threshold  X  , we plot the candidate set X  X  SR from the two algorithms side by side. Note that the Y axis is in log scale. We make several observations here. First, the result shows that our pruning rule is much more powerful for these the number of truly correlate d pairs. This shows that our algorithm has very strong pruning power. We investigate a pruning process that uses both rules. Both our and TAPER X  X  pruning rules produce candidate sets with false positives. We analyzed how a pair can escape TAPER X  X  pruning. We gave an efficient pruning method to catch these pairs. However, there are pairs that are elusive to our rule as well. In particular, a pair X  X  passing our test does not necessarily mean that it also satisfies TAPER X  X  rule. It can be expected that pruning using both rules is more effective. Note that applying the TAPER pruning rule in our algorithm is simple. It can be done by adding a step in our candidate-generation process. Namely, before putting a pair into the candidate set, we check the two items against TAPER X  X  rule. Only those that pass the test can be put in the candidate set. We applied this double-rule pruning on the  X  X umsb X  data set. The result is plotted in Fig. 7. For each threshold  X  , we plot, side by side, the ER of the candidate sets generated by our rule and by the combined rules.

The result shows that the combination of the two rules is powerful. The size of the candidate set generated by the combined rule is of the same order as the number of truly correlated pairs. For large  X  values, the ER is less than 2. This means that the number of false-positive pairs in the candidate set is less than the number of truly correlated pairs. Figure 7: ER of the Candidate Sets Generated by Double Pruning and Our Pruning
WeusedsynthesizeddatasetslistedinTable3toexamine the scalability of our algorithm. Fig. 8 plots the execution time of the algorithm against the number of items in the data set. It shows that the execution time increases linearly with the number of items.

We also examine the scalability of the algorithm X  X  prun-ing power. That is, we investigate whether the algorithm X  X  effectiveness in pruning changes with the size of the data set. In Fig. 9, we plot the expand ratio ER of the candidate set against the number of items in the data set. We observe that, when the threshold  X  is large, the ER values stay es-sentially the same for data sets with different numbers of items. This means that, for these  X  values, our algorithm X  X  pruning power is independent of the number of items in the Figure 10: Tradeoff between Running Time and False Negative Rate
In this paper, we consider the problem of finding all pairs of highly correlated items in a large data set. In many cases, the number of highly correlated pairs is much smaller than the total number of pairs. It is wasteful to compute the coefficients of all the pairs in the process of identifying the highly correlated ones. For massive data sets, the compu-tational cost of the naive method is prohibitive. Xiong et al. in [15] addressed this problem and proposed a two-step algorithm: TAPER. During the first step, they prune many of the unwanted pairs by simple computation. Only the co-efficients of the remaining pairs are computed during the second step.

We examine the pruning rule in TAPER and observe that there is a relatively large gro up of uncorrelated pairs that are elusive to TAPER X  X  pruning. To remove these pairs, a rule needs to consider the support of the pair in addition to the supports of individual items. A straightforward prun-ing method using such a rule will require computing the supports of all the pairs, which makes it meaningless. We propose a pruning rule and a pruning method that avoid this problem. Our experimental results show that our pruning method yields a candidate set much smaller than the ones produced by TAPER. Therefore, it achieves much larger sav-ings of computational resources.

There are two directions for future work. First, we use the function 1  X  (1  X  x k ) t to approximate a threshold func-tion. When the threshold is close to one, this approxima-tion is efficient, in the sense that k and t can take reason-ably small values. When the threshold is close to zero, we need large k and t . If we had a more efficient approxima-tion for small thresholds, our algorithm could be improved. Also, there are several other quantities that measure associ-ations/correlations. It will be interesting to see whether our algorithm could be used for them.
