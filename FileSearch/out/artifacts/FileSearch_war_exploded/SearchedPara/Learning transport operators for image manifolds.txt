 It is well known that natural images occupy a small fraction of the space of all possible images. Moreover, as images change over time in response to observer motion or changes in the environ-ment they trace out particular trajectories along manifolds in this space. It is reasonable to expect that perceptual systems have evolved ways to efficiently model these manifolds, and thus mathe-matical models that capture their structure in operators that transport along them may be of use for understanding perceptual systems, as well as for engineering artificial vision systems. In this paper, we derive methods for learning these transport operators from data.
 Rather than simply learning a mapping of individual data points to a low-dimensional space, we seek a compact representation of the entire manifold via the operators that traverse it. We investigate a direct application of the Lie approach to invariance [1] utilizing a matrix exponential generative model for transforming images. This is in contrast to previous methods that rely mainly upon a first-order Taylor series approximation of the matrix exponential [2,3], and bilinear models, in which the transformation variables interact multiplicatively with the input [4,5,6]. It is also distinct from the class of methods that learn embeddings of manifolds from point cloud data [7,8,9,10]. The spirit of this work is similar to [11], which also uses a spectral decomposition to make learning tractable in extremely high dimensional Lie groups, such as those over images. We share the goal of [12] of learning a model of the manifold which can then be generalized to new data.
 Here we show how a particular class of transport operators for moving along manifolds may be learned from data. The model is first applied to synthetic datasets to demonstrate interesting cases structure. Subsequently, we apply it to time-varying natural images and extrapolate along inferred trajectories to demonstrate super-resolution and temporal filling-in of missing video frames. Let us consider an image of the visual world at time t as a point x  X  R N , where the elements of x correspond to image pixels. We describe the evolution of x as where the matrix A is a linear operator capturing some action in the environment that transforms the image. Such an action belongs to a family that occupies a subspace of R N  X  N given by for some M  X  N 2 (usually M &lt;&lt; N 2 ), with  X  m  X  R N  X  N . The amount of a particular action from the dictionary  X  m that occurs is controlled by the corresponding c m . At t = 0 , a vision system takes an image x 0 , and then makes repeated observations at intervals  X  t . Given x 0 , the solution to (1) traces out a continuously differentiable manifold of images given by x t = exp( A t ) x 0 , which we observe periodically. Our goal is to learn an appropriate set of bases,  X  , that allow for a compact description of this set of transformations by training on many pairs of related observations. This generative model for transformed images has a number of attractive properties. First, it factors apart the time-varying image into an invariant part (the initial image, x 0 ) and variant part (the trans-formation, parameterized by the coefficient vector c ), thus making explicit the underlying causes. Second, the learned exponential operators are quite powerful in terms of modeling capacity, com-pared to their linear counterparts. Lastly, the partial derivatives of the objective function have a simple form that may be computed efficiently. The model parameters are learned by maximizing the log-likelihood of the model. Consider two  X  X lose X  states of the system in isolation. Let x 0 be our initial condition, and x 1 be a second observa-tion. These points are related through an exponentiated matrix that itself is composed of a few basis elements, plus zero-mean white i.i.d. Gaussian noise, n : We assume a factorial sparse prior over the transform variables c of the form P ( c m )  X  exp(  X   X  | c m | ) . The negative log of the posterior probability of the data under the model is given by where || X || F is the Frobenius norm, which acts to regularize the dictionary element lengths. The 1-norm encourages sparsity. Given two data points, the solution of the c variables which relate them through  X  is found by a fast minimization of E with respect to c .
 Learning of the basis  X  proceeds by gradient descent with respect to E . (Note that this constitutes a variational approximation to the log-likelihood, similar to [13].) The  X  variables are initialized randomly, and adjusted according to  X   X  =  X   X   X  X   X   X  , using the solution, c , for a pair of observations x , x 1 . Figure 1 outlines the steps of the algorithm.
 The partial derivatives of E w.r.t. c and  X  can be cast in a simple form using the spectral decomposi- X   X  . Then Figure 1: Pseudo-code for the learning algorithm. Steps 1-2 initialize. A typical stopping criteria in step 3 is that the reconstruction error or sparsity on some held-out data falls below a threshold. Steps 8-9 shrink the subspace spanned by the dictionary if one or more of the elements have shrunk sufficiently in norm. where the matrix F is given by: Application of the chain rule and a re-arrangement of terms yields simplified forms for the partials of E w.r.t. c and  X  . After computing two intermediate terms P and Q , the two partial derivatives for inference and learning are: The order of complexity for both derivatives is determined by the computation of Q , which requires an eigen-decomposition and a few matrix multiplications, giving O ( N p ) with 2 &lt; p &lt; 3 . We first test the model by applying it to simple datasets where the solutions are known: learning the topology of a sphere and a torus. Second, we apply the model to learn the manifold of time-varying responses to a natural movie from complex oriented filters. These demonstrations illustrate the algorithm X  X  capability for learning significant non-linear structure.
 We have also applied the model to the Klein bottle. Though closely related to the torus, it is an example of a low-dimensional surface whose topology can not be captured by a first-order Lie operator, though our model is able to interpolate between points on the surface using a piecewise approximation (see the supplementary material accompanying this paper for further discussion of this point).
 Related pairs of points on a torus are generated by choosing two angles  X  0 , X  0 uniformly at random from [0 , 2  X  ] ; two related angles  X  1 , X  1 are produced by sampling from two von Mises distributions with means  X  0 and  X  0 , and concentration  X  = 5 using the circular statistics toolbox of [15]. For the sphere, we generate the first pair of angles using the normal-deviate method, to avoid concentration of samples near the poles. Though parameterized by two angles, the coordinates of points on these surfaces are 3-and 4-dimensional; pairs of points x t for t = 0 , 1 on the unit sphere are given by x t = Figure 2: Orbits of learned sphere operators. (a) Three  X  m basis elements applied to points (b) When superimposed on top of each other, the three sets of orbits clearly define the surface of a sphere. Figure 3: Orbits of learned torus operators. Each row shows three projections of a  X  m basis element applied to a point on the surface of the torus. The orbits shown are generated by setting x relationship, while the third varies more freely. Figure 4: Learning transformations of oriented filter pairs across time. The orbits of three three complex filter outputs in response to a natural movie. The blue points denote the complex output for each frame in the movie sequence and are linked to their neighbors via the blue line. The points circled in red were observed by the model, and the red curve shows an extrapolation along the estimated trajectory.
 For the sphere, N = 3 , thus setting M = 9 gives the model the freedom to generate the full space of A operators. The  X  are initialized to mean-zero white Gaussian noise with variance 0 . 01 , and 10 , 000 learning updates are computed by generating a pair of related points, minimizing E w.r.t. c , then updating  X  according to  X   X  =  X   X   X  X   X   X  . In all of the point set experiments,  X  = 0 . 0001 and  X  = 0 . 01 . For cases where topology can be recovered, the solution is robust to the settings of  X  and  X   X  changing either variable by an order of magnitude does not change the solution, though it may increase the number of learning steps required to get to it. In cases where the topology can not be recovered, the influence on the solution of the settings of  X  and  X  is more subtle, as their relative values effectively trade-off the importance of data reconstruction and the sparsity of the vector c . We adjust  X  during learning as follows: when  X   X  causes E to decrease, we multiply  X  by 1 . 01 ; otherwise, we multiply  X  by 0 . 99 . When the model has more parameters than it needs to fully capture the topology of the sphere this fact is evident from the solution it learns: six of the dictionary elements  X  m drop out (they have norm less than 10  X  6 ), since the F-norm  X  X eight decay X  term kills off dictionary elements that are used rarely. Figure 2 shows orbits produced by applying each of the remaining  X  m operators to points on the sphere. Similar experiments are successful for the torus; Figure 3 shows trajectories of the operators learned for the torus.
 As an intermediate step towards modeling time varying natural images, we investigate the model X  X  complex pyramid is built from each frame in a movie, and pairs of filter responses 1 to 4 frames apart are observed by the model. Four 2x2 basis functions are learned in the manner described above. Figure 4 shows three representative examples that illustrate how well the model is able to extrapolate from the solution estimated using the learned basis  X  , and complex responses from the same filter within a 4 frame time interval. In most cases, this trajectory follows the data closely for several frames. In the image domain, our model has potential applications in temporal interpolation/filling in of video, super-resolution, compression, and geodesic distance estimation. We apply the model to moving natural images and investigate the first three applications; the third will be the subject of future work. Here we report on the ability of the model to learn transformations across time, as well as across scales of the Laplacian pyramid. Our data is many short grayscale video sequences of Africa from the BBC. 5.1 Time We apply the model to natural movies by presenting it with patches of adjacent frame pairs. Using an analytically generated infinitesimal shift operator, we first run a series of experiments to determine the effect of local minima on the recovery of a known displacement through the minimization of E Figure 5: Shift operator learned from synthetically transformed natural images. The operator  X  1 , displayed as an array of weights that, for each output pixel, shows the strength of its connection to each input pixel. Each of the 15x15 arrays represents one output pixel X  X  connections. Because of the 1 /f 2 falloff in the power spectrum of natural images, synthetic images with a wildly different distribution of spatial frequency content, such as uncorrelated noise, will not be properly shifted by this operator. Figure 6: Interpolating between shifted images to temporally fill in missing video frames. Two images x 0 and x 1 are generated by convolving an image of a diagonal line and a shifted diagonal line by a 3x3 Gaussian kernel with  X  = 0 . 8 , and the operator A is inferred. The top row shows the interpolation between x 0 and x 1 . The bottom row shows the sequence of images x t = ( I + A t ) x 0 , that is, the first-order Taylor expansion of the matrix exponential, which performs poorly for shifts greater than one pixel. w.r.t. c . When initialized to zero, the c vector often converges to the wrong displacement, but this problem can be avoided with high probability using a coarse-to-fine technique [16,17]. Doing so requires a slight alteration to our inference algorithm: now we must solve a sequence of optimiza-tion problems on frame pairs convolved with a Gaussian kernel whose variance is progressively decreased. At each step in the sequence, both frames are convolved by the kernel before a patch is selected. For the first step, the c variables are initialized to zero; for subsequent steps they are ini-tialized to the solution of the previous step. For our analytical shifting operator, two blurring filters  X  first a 5x5 kernel with variance 10, then a 3x3 kernel with variance 5  X  reliably gives a proper initialization for the final minimization that runs on the unaltered data.
 For control purposes, the video for this experiment comes from a camera fly over; thus, most of selected from random locations in the video, but discarding patches near the horizon where there is little or no motion. We initialize M = 16 ; after learning, the basis function with the longest norm has the structure of a shift operator in the primary direction of motion taking place in the video sequence. Using these 16 operators, we run inference on 1,000 randomly selected pairs of patches from a second video, not used during learning, and measure the quality of the reconstruction as the trajectory is used to predict into the future. At 5 frames into the future, our model is able to maintain an average SNR of 7, compared to SNR 5 when a first-order Taylor approximation is used in place of the matrix exponential; for comparison, the average SNR for the identity transformation model on this data is 1.
 Since the primary form of motion going on in these small patches is translation, we also train a single operator using artificially translated natural data to make clear that the model can learn this case completely. For this last experiment we take a 360x360 pixel frame of our natural movie, and continuously translate the entire frame in the Fourier domain by a displacement chosen uniformly at random from [0 , 3] pixels. We then randomly select a 15x15 region on the interior of the pair of frames and use the two 225 pixel vectors as our x 0 and x 1 . We modify the objective function to be where W is a binary windowing function that selects the central 9x9 region from a 15x15 patch; thus, the residual errors that come from new content translating into the patch are ignored. After learning, the basis function  X  1 (shown in figure 5) is capable of translating natural images up to 3 pixels while maintaining an average SNR of 16 in the 9x9 center region. Figure 6 shows how this operator is able to correctly interpolate between two measurements of a shifted image in order to temporally up-sample a movie. 5.2 Scale The model can also learn to transform between successive scales in the Laplacian pyramid built from a single frame of a video sequence. Figure 7 depicts the system transforming an image patch from scale 2 to 3 of a 256x256 pixel image. We initialize M = 100 , but many basis elements shrink during learning; we use only the 16  X  m with non-negligible norm to encode a scale change. The basis  X  is initialized to mean-zero white Gaussian noise with variance 0.01; the same inference and learning procedures as described for the point sets are then run on pairs x 0 , x 1 selected at random from the corpus of image sequences in the following way. First, we choose a random frame from a random sequence, then up-sample and blur scale 3 of its Laplacian pyramid. Second, we select an 8x8 patch from scale 2 ( x 0 ) of the corresponding up-blurred image patch ( x 1 ). Were it not for the highly structured manifold on which natural images live, the proposition of finding an operator that maps a blurred, subsampled image to its high-resolution original state would seem untenable. However, our results show that in many cases, a reduced representation of such two-way mappings can be found, even for small patches. We have shown that it is possible to learn low-dimensional parameterizations of operators that trans-port along non-linear manifolds formed by natural images, both across time and scale. Our focus thus far has been primarily on understanding the model and how to properly optimize its parameters, Figure 7: Learning transformations across scale. (a) Scale 3 of the Laplacian pyramid for a natural scene we wish to code, by describing how it transforms across scale, in terms of our learned dictionary. (b) The estimated scale 2, computed by transforming 8x8 regions of the up-sampled and blurred scale 3. The estimated scale 2 has SNR 9.60; (c) shows the actual scale 2 and (d) shows the errors made by our estimation. For reconstruction we use only 16 dictionary elements. as little work has previously been done on learning such high dimensional Lie groups. A promising direction for future work is to explore higher-order models capable of capturing non-commutative operators, such as as this formulation may be more parsimonious for factoring apart transformations which are preva-lent in natural movies, such as combinations of translation and rotation.
 Early attempts to model the manifold structure of images train on densely sampled point clouds and find an embedding into a small number of coordinates along the manifold. However such an points, or moving along the manifold. One must always refer back to original data points on which the model was trained  X  i.e., it works as a lookup table rather than being an abstraction of the data. Here, by learning operators that transport along the manifold we have been able to learn a compact description of its structure.
 This model-based representation can be leveraged to compute geodesics using a numerical approxi-mation to the arc length integral: where T is the number of segments chosen to use in the piecewise linear approximation of the curve, and each term in the summation gives the length of a segment. We believe that this aspect of our model will be of use in difficult classification problems, such as face identification, where Euclidean distances measured in pixel-space give poor results.
 Previous attempts to learn Lie group operators have focused on linear approximations. Here we show that utilizing the full Lie operator/matrix exponential in learning, while computationally intensive, is tractable, even in the extremely high dimensional cases required by models of natural movies. Our spectral decomposition is the key component that enables this, and, in combination with careful mitigation of local minima in the objective function using a coarse-to-fine technique, gives us the power to factor out large transformations from data.
 One shortcoming of the approach described here is that transformations are modeled in the original pixel domain. Potentially these transformations may be described more economically by working in a feature space, such as a sparse decomposition of the image. This is a direction of ongoing work. Acknowledgments The authors gratefully acknowledge many useful discussions with Jascha Sohl-Dickstein, Jimmy Wang, Kilian Koepsell, Charles Cadieu, and Amir Khosrowshahi, and the insightful comments from our anonymous reviewers. References
