 (EBMT) [1], information retrieval (IR) and text summarization etc. 
Recently, some researchers present a hybrid corpus-based mach ine translation sys-the scratch, and the similar examples retrieved will affect the results greatly. the examples in the training corpus provide rich information, such as word alignment. 
In this paper, we provide a novel approach, which use the bilingual information of the examples to retrieve the similar examples for an input sentence. (SMT) and EBMT. SMT obtains the translation models during training, and does not need the training corpus when decoding; while EBMT retrieves the similar examples in the corpus when translating. 
A word alignment is defined as follows: given a sentence pair ( C , E ), we define a words in both of the sentences. A word alignment between a sentence pair is a set of gradually, we call the region is independent . position index. In the example, the region (1..3, 3..5) is independent. 
In our hybrid MT system, the word alignment in the corpus must satisfy the Inver-alignment. blocks . Each block is formed by combining one or more links, and must be independ-We divide the retrieval of similar examples into two phases:  X 
Fast Retrieval Phase: retrieving the similar examples from the corpus quickly, and take them as candidates. The complexity should be not too high.  X  Refining Phase: refining the candidates to find the most similar examples. through the following three similarity metrics. 3.1 The Matched Words Metric number of the matched source words between the input sentence and the source sen-tence C in the example firstly. where words in I , and ) , , ( E A C Len is the number of the words in the in C . 3.2 The Matched Blocks Metric i length of the source phrase (here M =3 or 5), i.e., M k  X  . 
For each c , there may exists more than one blocks with c as the source phrase, so we will sort them by the probability and keep the best N (here set N=5) blocks. Now we represent the input sentence as:  X  . Note, some empty, e.g.  X  =  X  2 2 gram B , since no blocks with  X   X  X  X   X  X   X  as the source phrase. 
In the same way, we represent the example ) , , ( E A C as: process and store them in the corpus. 
Now, we can use the number of the matched blocks to measure the similarity of the input and the example:  X  . 
Since each block is attached a probability, we can compute the similarity in the fol-lowing way: 
So the final similarity metric for fast retrieval of the candidates is: where 1 = + +  X   X   X  . Zhao et al.[4] provides a method to tune the weights, but here example left, and retrieve the best N examples. swallow structure of the sentences, to find the best M similar examples. 4.1 The Alignment Structure Metric blocks, at this moment the order of the source phrases in the blocks must correspond with the order of the words in the input. into several regions, where some regions are matched and some un-matched. And we use a similar edit distance method to measure the similarity. We count the number of the Deletion/Insertion/Substitution operations, which take the region as the object. 
We set the penalty for each deletion and in sertion operation as 1, while considering the un-matched region in the example may be independent or not, we set the penalty 0.5 for substituting the region (1..3,3..5) to (1..1). the input R and exmaple R are the region numbers in the input and example. 4.2 The Semantic Metric Chinese semantic lexicon. In the lexicon, the semantics of the words are divided into using a unique code. Each code may include one or more words, and each word may  X   X   X  have the same code  X  X a02 X , the code for  X   X   X  is  X  X a03 X . one code, we will choose the nearest distance. two regions in the input and the example. 
Given the same match as section 4.1, the substitution distance is the sum of seman-tic distance of the substitution regions: 
Then, we obtain the semantic metric: where 1 ' ' ' = + +  X   X   X  . Here we also use the mean values as the weights. We carried out experiments on a Chinese-English bilingual corpus, which is the train-ing corpus in the open Chinese-English translation task of IWSLT2007, consisting of of 39,953 sentence pairs, and the test set 489 Chinese sentences. We lowercased and stemmed the words in the English sentences for preprocessing. 
During the training, we obtained firstly the word alignments, which satisfy the ITG retrieved and stored the blocks in each examples, i.e. ) , , ( E A C  X  . 5.1 Evaluation of the Retrieval of Similar Examples We evaluate our retrieval method in manual way: obtain 200 input sentences from the inputs. 
We define a correct similar example as the following way: if the retrieved example can be turned to the input sentence by modifying x independent regions, where each region consists of at most y blocks (here set 2 = y ). 
To evaluate the two retrieval phases, we also get the best 10 examples for the fast accuracies are both 45%, because some of the input sentences are very short, and both of the two phases will find the exactly similar examples. But when x becomes larger, the refined retrieval will get more accurate examples. 5.2 Evaluation of the Translation systems. The results list in Table 2. 
The first column lists the MT systems: the first two systems are two SMT systems, where Moses[7] is a state-of-the-art SMT system, and SMT-CKY is our SMT system scores[8] for these systems. The results show that our hybrid MT system achieves an trieve the similar examples is effective. hybrid MT systems. The approach makes good use of the word alignment information contained implicitly in each example, it helps to improve the translation quality. the retrieved examples more effectively. No. 60573057, 60473057 and 90604007. 
