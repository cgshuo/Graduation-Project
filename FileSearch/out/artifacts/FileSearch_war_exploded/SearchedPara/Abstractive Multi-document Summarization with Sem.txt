 Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extrac t-ed from the original documents directly by rank-ing or scoring and only little post -editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009 ). Pure extraction has intrinsic limits compared to abstraction ( Carenini and Cheung, 2008 ). 
Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the text s (Zhuge, 2015a ). There are some abstractive ap-proaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Mar cu, 2002; Cohn and Lapata, 2009) , sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008 ), and sentence revision ( Tanaka et al., 2009) . However, t hese approache s are se n-tence rewriting techniques based on syntact ical analysis without semantic analysis and abstract representation.

Fully abstractive summarization approach r e-quires a separate process for the analysis of text s that serves as an intermediate step before the generation of sentences ( Genest and Lapalme, 2011) . Statistics of words or phrases and syntac-tical analysis that have been widely used in exis t-ing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on deeper seman-tic analysis . 
We defin e the concept of Basic Semantic Unit ( BSU ) to express the semantics of text s. A BSU is an action indicator with its obligatory arg u-ments which contain actor and receiver of the action. BSU is the most basic element of coher-ent information in texts, which c an describe the semantics of an event or action. The semantic information of texts is represented by extracting BSUs and constructing BSU semantic link net-work (Zhuge, 2009) . Semantic Link Network consists of semantic nodes, semantic links and reasoning rules (Zhuge, 2010; 2011; 2012; 2015b) . The semantic nodes can be any resources. In this work, the semantic nodes are BSUs ex-tracted from texts. We use semantic relatedness between BSUs as semantic links. Then summary can be generated based on the semantic link ne t-work through summary structure planning . 
The characteristics of our approaches are as follow s:  X  Each BSU describes the semantics of an  X  The BSU semantic link network is a n ab- X  Summary is buil t from sentence to sentence There are some abstractive summarization a p-proaches in recent years. A n approach TTG at-tempt s to generate abstractive summary by using text -to-text generation to generate sentence for each subject -verb -object triple (Genest and Lapalme, 2011 ). A system that attempts to gen-erate abstractive summaries for spoken meetings was proposed (Wang and Cardie, 2013 ). It ide n-tifies relation instances that are represented by a lexical indicator with an argument constituent from texts. Then the relation instances are filled into templates which are extracted by applying multiple sequence alignment. Both of these sy s-tems need to select a subset of the large volumes of generated sentences. However, our system generates summary directly by summary stru c-ture planning . It can generate well -organized and coherent summary more effectively . 
A recent work aims to generate abstractive summary based on Abstract Meaning Represen-tation (AMR) (Liu et al., 2015). It first parses the source text into AMR graphs, and then tran s-forms them into a summary graph and plans to generate text from it. This work only focuses on the graph-to-graph transformation. The module of text generation from AMR has not been d e-veloped. The nodes and edges of AMR graph are entities and relations between entities respectiv e-ly, which are sufficiently different from the BSUs semantic link network . Moreover , texts can be generated efficiently from the BSUs network. Another recent abstractive summarization met h-od generates new sentences by selecting and merging phrases from the input documents (Bing et al., 2015). It first extracts noun phrases and verb -object phrases from the input documents, and then calculates saliency scores for them. An ILP optimization framework is used to simult a-neously select and merg e informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. As the results show that the method is difficult to generat e new informative sentences really diffe r-ent from the original sentences and may generate some none factual sentences since phrases from different sentences are merged.

Open information extraction has been pr o-posed by (Banko et al., 2007; Etzioni et al., 2011) . They extract binary relations from the web, which is different from our approach that extracts events or actions expressed in texts. Our system produces an abstractive summary for a set of topic related documents. It co nsists of two major components: Information extraction and summary generation . 3.1 Information Extraction The semantic information of texts is obtained by extracting BSU s and construct ing BSU semantic link network. A BSU is represented as an actor -action -receiv er triple, which can both detect s the crucial content and incorporate s enough sy n-tactic information to facilitate the downstream sentence generation. Some actions may not have the receiver argument. For example,  X  Flight MH370  X  disappear  X  and  X  Flight MH370 -leave -Kuala Lumpur  X  are two BSU s. 
BSU Extraction . BSU s are extracted from the sentences of the documents. The text s are pr e-processed by name entity recognition ( Finkel et al., 2005) and co-reference resolution (Lee et al., 2011) . Constituent and depe ndency parses are obtained by Stanford parser ( Klein and Manning, 2003) . The eligible action indicator is restricted to be a predicate verb; the eligible actor and r e-ceiver arguments are noun phrase. Both the actor and receiver arguments take the form of consti t-uents in the parse tree. A valid BSU should have one action indicator and at least one actor arg u-ment, and satisfy the following constraints:  X  The actor argument is the nominal subject or  X  The receiver argument is the direct object or 
We create some manual rules and syntactic constraints to identify all BSU s based on the sy n-tactic structure of sentences in the input text s. Constructing BSU Semantic Link Network. The semantic relatedness between BSU s contains three parts: Arguments Semantic Relatedness (ASR), Action -Verbs Semantic Relatedness (VSR) and Co -occurrence in the Same Sentence (CSS). Arguments of BSU s include actors and receivers, which both are noun phrases and ind i-cate concepts or entities in the text. When co m-puting ASR, the semantic relatedness between concepts must be measured. We use the explicit semantic analysis based on Wikipedia to co m-pute semantic relatedness between concepts (Ga-brilovich and Markovitch, 2007) . When compu-ting VSR, WordNet -based measure is used to calc ulate the semantic relatedness between action verbs (Mihalcea et al., 2006 ). CSS is measured whether two different BSUs co-occur in the same sentence. Semantic relations between BSU s are computed by linearly combining these three parts. Then BSU s that are extracted from the texts form a semantic link network.

Semantic Link Network Reduction. A di s-criminative ranker based on Support Vector R e-gression (SVR) ( Smola and Scholkopf, 2004) is utilized to assign each BSU a summary -worthy score. Training data was constructed from the DUC 2005 datasets which contain both the source documents and human generated refe r-ence summaries. BSUs are extracted from these datasets. For each BSU in the source documents, if it has occurred in the corresponding human generated summ aries or the semantic relatedness between the BSU and one BSU in the corr e-spond ing human generated summaries is above a threshold  X  , then it is considered to be a positive sample and be assigned 1 to its summary -worthy score. Otherwise, the BSU is considered to be a negative sample and be assigned 0 to its su m-mary -worthy score. Table 1 displays the features of BSU used in the SVR mode l. Then the salien-cy score of each BSU in the semantic link ne t-work is calculated by the following equation: Where 
BSU ; 
BSU and 
BSU s in the semantic link network are clu s-tered by hierarchical complete-link clustering methods . BSUs in each cluster are semantically similar . For example, Malaysia Airlines plane -vanish and Flight MH370  X  disappear . Only the most important one with the largest saliency score is reserved in the network. These less i m-portant BSUs are eliminated. The remaining BSU semantic link network represents the important information of the texts with no redundancy . 3.2 Summary Generation The s ummary for the documents is generated directly based on the BSU semantic link network. The summary should be well -structured and well -organized. It should not just be a heap of related information, but should build from se n-tence to sentence to a coherent body of info r-mation about a topic. 
The summary structure is planned based on the BSU semantic link network . An optimal path which cover s all the nodes in the network is found. The following two factors are considered when finding the optimal path: (1) Context S e-mantic Coherent. To make the summary seman-tic coherent, all adjacent sentences should be s e-mantically related. We need to find an optimal path, in which every two adjacent nodes are strong semantically related. The optimal path is denoted as the theme of generated summary clear -cut, the important content should be put in prior position. The order of the i th node in the path is denoted as w Sal BSU = and m aximize
To combine the above two factors, we need to find an optimal path which covers each node on-ly once and has the long est distance. The biased -sum weight of all nodes in the path should be maximized. The problem can be proved to be NP -hard by reduction to TSP problem. It can be formalized as an integer linear programming (ILP) as follow. er the optimal path goes from node i to node j.
Since each node can be traversed only once, the following constraints must be satisfied. The nodes in the path are sequentially ordered. If the edge between two nodes is in the path, then 
Table 1 . Features for BSU summary -worthy scoring. We use SVM -light with RBF kernel by default parameters ( Joachims, 1999) . the order of the two nodes is sequentially close to each other, which can be formulated as follow:
At last, we ca n formulate the objective func-tion as follow: where parameter  X  tunes the effect of the two parts and n is the quantity of BSU s in the final BSU semantic link network ( after reduction ). 
Sentence Generation . After the summary structure has been planned, sentences are gener-ated for each node in the BSU semantic link net-work. As the BSU contains enough semantic and syntactic information, sentence can be generated efficiently according to the following rules:  X  Generate a Noun Phrase (NP) based on the  X  Generate a Verb Phrase (VP) based on the  X  Generate complement s for the VP when the 
The process of sentence generation for each node is based on the syntactic structure of the source sentence where the BSU is extracted from. The time and location preposition phrases which are important information of new events are kept. The generated sentences are organized according to the summary structure. If some adjacent sen-tences in the summary have the same subject, the subject of the latter can be substituted by a pr o-noun (such as it or they) to avoid repetition of noun phrases. One sample summary generated by our system for  X  X alaysia MH370 D isappear X  news is shown in Figure 1. 4.1 Dataset and Experimental Settings In order to evaluate the performance of our sy s-tem, we use two datasets that have been used in recent multi-document summarization shared tasks: DUC2005 and DUC2007. Each task has a gold standard datas et consisting of document clusters and reference summaries. In our exper i-ments, DUC2005 was used for training and p a-rameter tuning, and DUC2007 was used for tes t-ing. Based on the tuning set, t he parameter  X  is set as 10 and  X  is set as 0. 7 after tuning . 
Our system is compared with one state -of-the -art graph -based extractive approach MultiMR (Wan and Xiao, 2009) and one abstractive ap-proach TTG (Genest and Lapalme, 2011 ). In a d-dition, we have implemented another baseline RankBSU which use s the graph-based ranking methods on the BSU s network to rank BSU s and select the top ranked BSU s to generate sentences. 4.2 Results ROUGE -1.5.5 toolkit was used to evaluate the quality of summary on DUC 2007 dataset (Lin and Hovy, 2003) . The ROUGE scores of the NIST Baseline system (i.e. NIST Baseline ) and average ROUGE scores of all the participating systems (i.e. AveDUC ) for DUC 2007 main task were also listed. According to the results in T a-ble 2, our system much outperforms the NIST Baseline and AveDUC , and achieves higher ROUGE scores than the abstractive approach TTG . So the abstract representation of text s and the information extraction process in our system are effective for multi-docume nt summarization. Our system also achieves better performance than the baseline RankBSU , which demonstrates that the network reduction method is more eff i-cient than the popular graph-based ranking methods . As compared with the state -of-art graph -based extractive method MultiMR , our system also achieves better performance. Fu r-thermore , our system is abstractive with abstract representation and sentence generation. Incorrect Table 2 . Comparison results (F -measure) on DUC 2007 under ROUGE evaluation.

Table 3. Comparison results on DUC 2007 un-der the automated pyramid evaluation with two threshold value 0.6 and 0.65. parser and co -reference resolution will lead to wrong extraction of BSU . If with more accurate parser and co -reference resolution, our system will be expected to achieve better performance. 
Since ROUGE metric evaluates summaries only from word overlapping perspective , we also use the pyramid evaluation metric (Nenkova and Passonneau, 2004) which can measure the su m-mary quality beyond simply string matching. The pyramid evaluation metric involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning, which is a better metric for the abstrac-tive summary evaluation. Since the manual py r-amid evaluation is time -consuming and the eva l-uation results can  X  X  be reproducible with different groups of assessors, we use the automated ver-sion of pyramid proposed in (Passonneau et al., 2013) and adopt the same setting as in (Bing et al., 2015) . Table 3 shows the evaluation results of our system and the three baseline system s on DUC 2007. The results show that the perfo r-mance of our system is significantly better than the three baseline systems, w hich demonstrates that the summaries of our system contain more SCUs than summaries of other systems. So our system can generate more informative summary.
In addition, large volumes of news texts for popular news event s are crawled from the news websites. Figure 1 and 2 show the summaries for the  X  X alaysia MH370 Disappear X  news event generated by our system and MultiMR respec-tively. The summary by MultiMR contains some repetition of facts obviously. And it is just a heap of information about MH370. The summary by our system doesn X  X  contain much repetition of facts, so it can contain more useful information. And it is built from sentence to sentence to a c o-herent body. Obviously, the summary by our sy s-tem is more coherent and compact. The proposed summarization approach is effe c-tive in information extraction and achieves good performance on DUC datasets. Through the sample summary, we can find that the approach is very effective for summarizing texts that mai n-ly describe facts and actions of news event. Summaries generated by our system are inform a-tive, coherent and compact.

But for texts expressing opinions, the a p-proach can  X  X  settle it approp riately . For example, when the verbs of BSUs are not meaningful a c-tions, like  X  be X , the semantic relations between them can  X  X  be appropriately computed by the methods described in the paper . More efficient methods to computer semantic relations between BSU s should be developed in the following work . 
The sentence generation process described in the paper is just a preliminary scheme. It should be developed to generate sentence relying less on the original sentence structure and aggre gating infor mation from several different BSUs . 
Fig ure 1. The s ummary of  X  Malaysia MH370 Di s-appear  X  news event generated by our system.
Fig ure 2. The s ummary of  X  Malaysia MH370 Di s-
