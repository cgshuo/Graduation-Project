 KDD Cup 2007 focuses on movie rating behaviors. The goal of the task  X  X ho Rated What X  is to predict whether  X  X x-isting X  users will review  X  X xisting X  movies in the future. W e cast the task as a link prediction problem and address it via a simple classification approach. Compared with other applications for link prediction, there are two major chal-lenges in our task: (1) the huge size of the Netflix data; (2) the prediction target is complicated by many factors, such as a general decrease of interest in old movies and more ten-dency to review more movies by Netflix users due to the success of the internet DVD rental industries. We address the first challenge by  X  X elective X  subsampling and the sec-ond by combining information from the review scores, movie H.4 [ Information Systems Applications ]: Miscellaneous KDD Cup, Netflix, Link prediction One of the two tasks in KDD Cup 2007 is to predict which users rated which movies in 2006, given the Netflix Prize training data set that contains more than 100 million rating s from over 480 thousand users on nearly 18 thousand movie titles collected between 1998 and 2005. In our practice, we cast the task as a link prediction problem and address it via a simple classification approach.
 Link prediction, i.e., the task of predicting the future str uc-ture of a network given the current structure, is a funda-mental task in many data mining applications, such as so-cial network analysis, protein/genetic interaction predi ction, and collaborative filtering recommendation. Many models have been studied and applied to linkage prediction. Net-work evolution and graph generation models aim to capture how networks grow and change over time, typically based on the topological features [5; 6; 7]. Network evolution and graph generation models focus on abstract graph where at IBM T.J. Watson Research Center no vertex and link attributes are considered. Various rela-tional learning methods have been proposed to define a joint probability over the entire graph -both the node attributes and link structure [8]. Link prediction based on relational learning explores both the link structure and the descrip-tive attributes of nodes. However, for a huge graph with rich features, the computational cost becomes a problem. Other methods based on binary classification typically re-quire rich features on both nodes and graph structures [3]. One difficulty in applying machine learning algorithms to real problems is the computation expense and feasibility in large-scale applications.
 In our approach, we formulate the link prediction as a binary classification problem and solve it via a supervised learnin g task. To predict a link, we partition the Netflix training set into two non-overlapping subsets -a training set containin g ratings appearing before October 2005 and a development set containing ratings after October 2005. A pair of user and movie represents a positive example if there is a rating connection between them, negative otherwise.
 There are two major challenges in the KDD Cup  X  X ho Rated What X  task: (1) the huge size of the Netflix data; (2) the prediction target is complicated by many factors, such as a general decrease of interest in old movies and more ten-dency to review more movies by Netflix users due to the success of the internet DVD rental industries. We address the first challenge by  X  X elective X  subsampling and the sec-ond by combining features based on movie content, review scores, and graph topology effectively and projecting the features over time. We demonstrated that an effective sub-sampling based on the task requirement is able to provide an effective and efficient solution for a large-scale task. A set of meaningful features has been developed by exploring both the graph topology and movie contents collected from external resources. To solve a supervised learning problem, effective feature ex -traction is a must. For our task, we consider two types of features: one is proximity features that represent the sim-ilarity in content between the query movie and the movies that the users rated before; the other is the features based on graph topology. The movie contents are used to calculate the proximity be-tween a user and a movie. The basic idea is: if a user has rated many animation movies and no horror movies in the Figure 1: The histograms of proximity feature values from positive examples (left) and negative examples (right) year of 2005, it is more likely he or she will rate an anima-tion movie rather than a horror movie in 2006. We collect the movie content information, such as plot, director, acto r, genre, movie connections from multiple sources on the inter -net. Then for each movie, we have its content information; for each user, we model the user X  X  preference with contents of movies that have been rated before.
 The raw content information that we collect from the inter-net is unstructured, resulting in a feature set of very high dimensions (over 50,000) when we convert them into struc-tured feature vectors using the bag-of-words representati on. It raises a great challenge for any sophisticated classifica -tion models to be applied on a data set of extremely high dimensions in both the feature space and example space. Therefore we applied latent semantic indexing (LSI) [1] to obtain a low-dimensional feature representation: for each movie, we constructed one feature set based on directors, actors, genres and so on, and another feature set based on the plots of the movie. Then singular value decomposition (SVD) [2] is applied on the movie-content and movie-plot matrix respectively, and only the top 900 singular vectors are kept for later uses. In this way, each movie is repre-sented with a relatively low-dimensional feature vector so that computing the similarity scores based on dot-product of movie vectors can be executed efficiently.
 For each example, i.e., a pair of movie and user, we com-pute the proximity features in the following way: we use the dot product of two content feature vector to represent the similarity score of two movies. For each user, we retrieve the list of movies having been rated and call them user-related movies . Given an example, i.e., a movie and a user, we compute all the similarity scores between this movie and the user-related movies, and then use the mean, minimum, and maximum scores as proximity features. To capture the user X  X  preference over time, we project the proximity fea-tures into three time ranges -the year of 2003, 2004, and 2005. Figure 1 shows an example of how the proximity fea-tures (a larger value mean indicates greater similarity) di s-criminate between positive and negative examples (differen t mean, variance as well as the shape of the plots). Another useful information source is the review history of all users. A graph with users and movies as nodes can be constructed. Consider a graph G = h V, E i where each edge e = h u, m i  X  E represents an interaction between node u and m at a particular time t , i.e., user u rated movie m at time t . There is rich information contained in the graph. One of the most natural measurement for a node is how many other nodes it connects to. In our application, the connectivity represents how popular a movie is, or how active a user is, which is no doubt a meaningful factor in this problem. Therefore the features based on the number of connected edges are used. We also project such features into three time ranges -the year of 2003, 2004, and 2005, to model the trend over time.
 Graph topology contains the most important set of features and can be applied to study on all networks. Recent studies on topological features have shown that shortest distance, clustering coefficient, number of common neighbors, and so on are extremely helpful for link prediction. Due to lim-ited time to work on the project, we implemented a set of naive features based on adjacency matrix. In the adjacency matrix, each row is a movie and each column represents a user. Therefore each movie can be represented with the cor-responding row in the adjacency matrix. Similarly to our proximity feature, SVD is first applied to convert the vector into a low-dimensional space, the similarity scores betwee n a movie and the user-related movies are then computed, and finally the mean, minimum, and maximum scores are used as features. Figure 2 shows an example of how the graph topology features discriminate between positive and nega-tive examples. The first step in data preparation is to collect all the data available. Therefore we combine the Netflix Prize training data and qualification set together as the  X  X etflix KDD X  set. This results in 17,770 movies and 480,189 users, with 103,297,638 reviews. As described on the KDD Cup website, the test sets are generated as follows: the 17770 movies in the Netflix Prize training set were split randomly into two sets, one per task, resulting in 6822 movies for  X  X ho Rated What X  task and 8863 movies for  X  X ow Many Ratings X  task. Let P( M 2006 = i ) = p i be the marginal probability that the i th movie is reviewed in 2006, and P( U 2006 = j ) = q j be the marginal probability that the j th user reviews a movie in 2006. The movie-user pair ( x, y ) in the testing set for  X  X ho Rated What X  task is generated as follows: If user y has reviewed movie x before 2006, the pair ( x, y ) is removed.
 Given the enormous data in the Netflix KDD set, sampling an effective training set is essential to apply any machine learning algorithms. On the other hand, since the goal is to predict the behaviors of existing users in existing movie s, it is reasonable to explore similar behaviors in the previou s years. Therefore following the sampling methodology as the test data, we generate two sample sets with around 100,000 movie-user pairs for the year 2004 and 2005 as the training data, and two sample sets for the last quarter of year 2005 as the development set. Notice that since we are only intereste d in the behaviors of existing users to existing movies, the pairs either with the users who joined after 2005 (or 2004) Figure 2: The histograms of graph-based feature values from positive examples (left) and negative examples (right) or with the movies which are released afterwards, need to be removed. We represent the resulting set as S-2005-1, S-2005 -2, S-2004-1, S-2004-2, S-2005Q4-1, S-2005Q4-2 respective ly. The positive rate of the true labels in the sampled sets from different years is shown in Table 1. As we can see, the ratio remains similar over the years. In particular, the estimate d rate in 2005 is very close the one in the KDD Cup test set, which serves an excellent training set for our later predict ion. Table 1: Positive rate of the true labels in the sampled sets from year 2004, 2005, the last quarter of 2005, and 2006 (from the released answer set posted on the KDD Cup web-site) Even though we have only sampled a subset of the huge training set, the number of examples is still around 100,000 , which renders useless many sophisticated classification al -gorithms, such as support vector machines. After careful examination of the characteristics of the data, including : (1) an imbalanced set with only 6-8% positive examples; (2) heterogenous attributes, i.e. the features are gathered an d extracted from multiple information sources, such as movie content, review information and so on; (3) huge number of training and testing examples, we explore the following learning strategies, including: Single classifier: a straightforward solution to our task as a classification problem, is to apply some classifiers, which have been studied extensively over the past ten years. In our experiment, we have tried logistic regression, support vec tor machines (which fails to converge during the training phase and generate any reasonable predictions), decision trees, and which is an open-source MATLAB package that encapsu-lates most of popular classification algorithms to facilita te MATLABArsenal.htm the research efforts on developing and evaluating classifica -tion algorithms on real-world data sets. In our experiments , we find that ridge regression and logistic regression are effi-cient and provide most accurate results. In our submission, we use the ridge regression since its optimization criterio n agrees with the evaluation measure, i.e. root mean squared errors (RMSE).
 Ensemble of classifiers: In addition to the simple sin-gle classifiers, we also examine two ensemble approaches, including (1) building sub-classifiers on subsets of traini ng examples to alleviate the problem of too many examples; and (2) building separate classifiers for each set of feature s (from the same sources) to reduce the dimension of the raw features. To combine the prediction of the sub-classifiers, we use the pre-set weights (by human) and the weights that are learned from the other development set. In our experi-ment, we find that the first ensemble approach, i.e. building classifiers on subsets of training examples, perform consis -tently worse than building a single classifier on the whole training set. Therefore, we focus on the second approach with different combination strategies. In this section, we describe the experiment results from two settings: one is the validation setting, which is used for fe a-ture selection and classifier selection; the other is submis sion setting, which reports our final submitted results for KDD Cup 2007  X  X ho Rated What X  task. In our validation experiments, we use S-2005Q4-1 as the testing set, S-2005Q4-2 as the development set and S-2005-1, S-2005-2, S-2004-1, S-2004-2 as the training sets. The ro ot mean squared error (RMSE) is used for evaluation measure. The results are shown in Figure 3. The RMSE of the baseline method, which is calculated by assigning all the examples as the prior of the test set, is 0.2394. From the results, we can see that: (1) all the methods achieve better results than the baseline method, which requires the nontrivial estimat e of the prior of the positive examples in the test set; (2) the training sets from year 2005 are much more effective than those from 2004; (3) the ensemble approach, which learns the weights from the development set using logistic regression, perform consistently the best. For KDD cup 2007, we target at answering the question of  X  X ho Rated What X  in 2006. In the validation experiments, we have observed that the training sets sampled from the current year (compared with other years) are the most ef-fective for the predictions of the next year. Therefore we use the sets sampled from year 2005 as the training data. In addition, to reduce the variance, we build two models using the two 2005 sets, i.e. S-2005-1 and S-2005-2 respectively, and then average the predictions are with equal weights. Before the deadline of the submission, we generate the re-sults on the three models we have examined before but only submit the one generated by the single classifier (with ridge regression) using all the features in order to avoid possi-ble overfitting of the ensemble approach (whose weights are learned using S-2005Q4-2 as the development set since we Figure 3: RMSE of the validation experiments using S-2005Q4-1 as the testing set and S-2005Q4-2 as the devel-opment set do not have any data from 2006). Table 2 shows the RMSE of the three methods on the KDD Cup test set after the answer set is released and Table 3 lists the RMSE results of the top performers in this task.
 Method Baseline Single RMSE 0.268 0.265 0.266 0.263 Table 2: RMSE of our models on KDD Cup test set. The predictions from single classifier are submitted (in bold le t-ters).
 Table 3: RMSE of top performers on KDD Cup  X  X ho Rated What X  task. The KDD Cup 2007 focuses on the Netflix Prize data with rich information of the movie reviews and it has attracted the attention of many researchers and scholars in related ar -eas. There are several observations that we find interesting in both future research directions and industrial applica-tions: 1. Correct sampling is essential : The Netflix Prize data is a typical example of the data we need to handle in many real applications. A recent trend in the research of machine learning and data mining is to adapt the successful but com-plex algorithms for large-scale applications. In the exerc ise of KDD Cup, we have demonstrated that an effective sub-sampling based on the task analysis is able to provide an accurate and efficient solution. 2. The effectiveness of multi-task learning : The KDD Cup 2007 uses the same data as the Netflix Prize competition, but focuses on different tasks. An initial thought we have for the  X  X ho Rated What X  task is to explore the features or lessons that have been examined in the Netflix Prize compe-tition. However, we fail to achieve any significant progress on that direction: the models trained on the SVD features from the Netflix Prize competition cannot even help us beat the baseline. As we can see, a naive sharing of feature space is far from getting the most of multi-task learning. It would be interesting to explore how to effectively make use of shar-ing information in different tasks for learning.
 We want to thank Rick Lawrence, Naoki Abe, Prem Melville, Hisashi Kashima, Shohei Hido, Chandan Reddy, Yi Zhang, Hanghang Tong, Rong Yan, Yuan Yuan, and many others for useful discussion. [1] M. Berry, S. Dumais, and T. Letsche. Computational [2] G. Golub and C. V. Loan. Matrix Computations . Johns-[3] M. Hasan, V. Chaoji, S. Salem, and M. J. Zaki. Link [4] T. Hastie, R. Tibshirani, and J. Friedman. The Elements [5] Z. Huang. Link prediction based on graph topology: The [6] H. Kashima and N. Abe. A parameterized probabilistic [7] D. Liben-Nowell and J. Kleinberg. The link prediction [8] B. Taskar, M.-F. Wong, P. Abbeel, and D. Koller. Link
