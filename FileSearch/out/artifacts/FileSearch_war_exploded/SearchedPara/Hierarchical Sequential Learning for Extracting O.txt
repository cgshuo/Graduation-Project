 Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e.g. Kim and Hovy (2005), Popescu and Etzioni (2005), Breck et al. (2007)), determining their polarity (e.g. Hu and Liu (2004), Kim and Hovy (2004), Wilson et al. (2005)), and determin-ing their strength, or intensity (e.g. Popescu and Etzioni (2005), Wilson et al. (2006)). Most pre-vious work treats each subtask in isolation: opin-ion expression extraction (i.e. detecting the bound-aries of opinion expressions) and opinion attribute classification (e.g. determining values for polar-ity and intensity) are tackled as separate steps in opinion recognition systems. Unfortunately, er-rors from individual components will propagate in systems with cascaded component architectures, causing performance degradation in the end-to-end system (e.g. Finkel et al. (2006))  X  in our case, in the end-to-end opinion recognition sys-tem.

In this paper, we apply a hierarchical param-eter sharing technique (e.g., Cai and Hofmann (2004), Zhao et al. (2008)) using Conditional Ran-dom Fields (CRFs) (Lafferty et al., 2001) to fine-grained opinion analysis. In particular, we aim to jointly identify the boundaries of opinion expres-sions as well as to determine two of their key at-tributes  X  polarity and intensity.

Experimental results show that our proposed ap-proach improves the performance over the base-line that does not exploit the hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate systems. We define the problem of joint extraction of opin-ion expressions and their attributes as a sequence tagging task as follows. Given a sequence of to-kens, x = x 1 ... x n , we predict a sequence of defined as conjunctive values of polarity labels and intensity labels, as shown in Table 1. Then CRFs is given as (Lafferty et al., 2001) where Z x is the normalization factor.

In order to apply a hierarchical parameter shar-ing technique (e.g., Cai and Hofmann (2004), Zhao et al. (2008)), we extend parameters as fol-lows. negative) and intensity (high, medium, low) I where g O and g  X  O pinion extraction, g P and g  X  defined for P olarity extraction, and g S and g  X  feature vectors defined for S trength extraction, and
For instance, if y i = 1 , then
If y i  X  1 = 0 , y i = 4 , then This hierarchical construction of feature and weight vectors allows similar labels to share the same subcomponents of feature and weight vec-tors. For instance, all  X  f ( y i , x, i ) such that y i  X  { 1 , 2 , 3 } will share the same compo-can be other variations of hierarchical construc-tion. For instance, one can add  X   X  g I (  X , x, i ) learning for each label.

Notice also that the number of sets of param-eters constructed by Equation (1) is significantly smaller than the number of sets of parameters that are needed without the hierarchy. The former re-quires (2 + 4 + 4) + (2  X  2 + 4  X  4 + 4  X  4) = 46 sets of parameters, but the latter requires (10) + (10  X  10) = 110 sets of parameters. Because a combination of a polarity component and an in-tensity component can distinguish each label, it is not necessary to define a separate set of parameters for each label. We first introduce definitions of key terms that will be used to describe features.  X 
We obtain these prior-attributes from the polar-ity lexicon populated by Wilson et al. (2005).  X 
Words in a given opinion expression often do not share the same prior-attributes. Such dis-continuous distribution of features can make it harder to learn the desired opinion expres-sion boundaries. Therefore, we try to obtain expression-level attributes ( E XP -P OLARITY and der to derive E XP -P OLARITY , we perform simple voting. If there is a word with a negation effect, such as  X  X ever X ,  X  X ot X ,  X  X ardly X ,  X  X gainst X , then span with the same expression-level attributes are referred to as E XP -S PAN . 3.1 Per-Token Features Per-token features are defined in the form of g (  X , x, i ) , g mains of  X ,  X ,  X  are as given in Section 3. Common Per-Token Features Following features are common for all class labels. The notation  X  indicates conjunctive operation of two values.  X  based on GATE (Cunningham et al., 2002).  X   X  based on WordNet (Miller, 1995).  X  based on opinion lexicon (Wiebe et al., 2002).  X  based on CASS partial parser (Abney, 1996).  X   X   X   X  boolean to indicate whether x i is in an E XP -S PAN .  X   X  Polarity Per-Token Features These features are included only for g O (  X , x, i ) and g P (  X , x, i ) , which are the feature functions corresponding to the polarity-based classes.  X   X   X  where P olarity  X  { positive, neutral, negative } .
This feature encodes the number of positive, spectively, in the current sentence.  X   X   X   X  Intensity Per-Token Features These features are included only for g O (  X , x, i ) responding to the intensity-based classes.  X   X   X  words in the current sentence.  X  such as  X  X xtremely X ,  X  X ighly X ,  X  X eally X .  X  verb, such as  X  X ust X ,  X  X an X ,  X  X ill X .  X  verb, such as  X  X ay X ,  X  X ould X ,  X  X ould X .  X  as  X  X ittle X ,  X  X omewhat X ,  X  X ess X .  X   X   X   X  3.2 Transition Features Transition features are employed to help with boundary extraction as follows: Polarity Transition Features Polarity transition features are features that are  X   X  Intensity Transition Features Intensity transition features are features that are  X   X  We evaluate our system using the Multi-Perspective Question Answering (MPQA) cor-pus 1 . Our gold standard opinion expressions cor-Method Description r (%) p (%) f (%) Polar-Only  X  Intensity-Only 43.3 92.0 58.9 Joint without Hierarchy 46.0 88.4 60.5 Joint with Hierarchy 48.0 87.8 62.0
Table 4: Performance of Opinion Extraction respond to direct subjective expression and expres-sive subjective element (Wiebe et al., 2005). 2
Our implementation of hierarchical sequential learning is based on the Mallet (McCallum, 2002) code for CRFs. In all experiments, we use a Gaus-sian prior of 1 . 0 for regularization. We use 135 documents for development, and test on a dif-ferent set of 400 documents using 10-fold cross-validation. We investigate three options for jointly extracting opinion expressions with their attributes as follows: [Baseline-1] Polarity-Only  X  Intensity-Only: For this baseline, we train two separate sequence tagging CRFs: one that extracts opinion expres-sions only with the polarity attribute (using com-mon features and polarity extraction features in Section 3), and another that extracts opinion ex-pressions only with the intensity attribute (using common features and intensity extraction features in Section 3). We then combine the results from two separate CRFs by collecting all opinion en-tities extracted by both sequence taggers. 3 This baseline effectively represents a cascaded compo-nent approach. [Baseline-2] Joint without Hierarchy: Here we use simple linear-chain CRFs without exploit-ing the class hierarchy for the opinion recognition task. We use the tags shown in Table 1.
 Joint with Hierarchy: Finally, we test the hi-erarchical sequential learning approach elaborated in Section 3. 4.1 Evaluation Results We evaluate all experiments at the opinion entity level, i.e. at the level of each opinion expression rather than at the token level. We use three evalua-tion metrics: recall, precision, and F-measure with equally weighted recall and precision.

Table 4 shows the performance of opinion ex-traction without matching any attribute. That is, an extracted opinion entity is counted as correct if it overlaps 4 with a gold standard opinion expression, without checking the correctness of its attributes. Table 2 and 3 show the performance of opinion extraction with the correct polarity and intensity respectively.

From all of these evaluation criteria, J OINT WITH
IERARCHY performs the best, and the least effec-tive one is B ASELINE -1 , which cascades two sepa-rately trained models. It is interesting that the sim-ple sequential tagging approach even without ex-than the cascaded approach ( B ASELINE -1 ).
When evaluating with respect to the polarity at-tribute, the performance of the negative class is substantially higher than the that of other classes. This is not surprising as there is approximately twice as much data for the negative class. When evaluating with respect to the intensity attribute, the performance of the L OW class is substantially lower than that of other classes. This result reflects the fact that it is inherently harder to distinguish an opinion expression with low intensity from no opinion. In general, we observe that determining correct intensity attributes is a much harder task than determining correct polarity attributes.
In order to have a sense of upper bound, we also report the individual performance of two sep-arately trained models used for B ASELINE -1 : for the Polarity-Only model that extracts opinion bound-aries only with polarity attribute, the F-scores with respect to the positive, neutral, negative classes are Only model, the F-scores with respect to the high, tively. Remind that neither of these models alone fully solve the joint task of extracting boundaries as well as determining two attributions simultane-ously. As a result, when conjoining the results from the two models ( B ASELINE -1 ), the final per-formance drops substantially.

We conclude from our experiments that the sim-ple joint sequential tagging approach even with-out exploiting the hierarchy brings a better perfor-mance than combining two separately developed systems. In addition, our hierarchical joint se-quential learning approach brings a further perfor-mance gain over the simple joint sequential tag-ging method. Although there have been much research for fine-grained opinion analysis (e.g., Hu and Liu (2004), Wilson et al. (2005), Wilson et al. (2006), Choi and Claire (2008), Wilson et al. (2009)), 5 none is directly comparable to our results; much of previ-ous work studies only a subset of what we tackle in this paper. However, as shown in Section 4.1, when we train the learning models only for a sub-set of the tasks, we can achieve a better perfor-mance instantly by making the problem simpler. Our work differs from most of previous work in that we investigate how solving multiple related tasks affects performance on sub-tasks.

The hierarchical parameter sharing technique used in this paper has been previously used by Zhao et al. (2008) for opinion analysis. However, Zhao et al. (2008) employs this technique only to classify sentence-level attributes (polarity and in-tensity), without involving a much harder task of detecting boundaries of sub-sentential entities. We applied a hierarchical parameter sharing tech-nique using Conditional Random Fields for fine-grained opinion analysis. Our proposed approach jointly extract opinion expressions from unstruc-tured text and determine their attributes  X  polar-ity and intensity. Empirical results indicate that the simple joint sequential tagging approach even without exploiting the hierarchy brings a better performance than combining two separately de-veloped systems. In addition, we found that the hierarchical joint sequential learning approach im-proves the performance over the simple joint se-quential tagging method.
 This work was supported in part by National Science Foundation Grants BCS-0904822, BCS-0624277, IIS-0535099 and by the Department of Homeland Security under ONR Grant N0014-07-1-0152. We thank the reviewers and Ainur Yesse-nalina for many helpful comments.

