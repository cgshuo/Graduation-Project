 As a central problem in machine learning, clustering methods have been applied to data mining, area, there is still need to explore methods that are robust and efficient on large data sets, and or impose constraints that make these algorithms impractical for large data sets. been introduced. The main idea behind those algorithms is to find a good cluster around a specific node. These techniques, thanks to their scalability, has had high impact in practical applications [LLDM09, GLMY11, GS12, AGM12, LLM10, WLS + 12]. Nevertheless, the theoretical understand-ing of these techniques is still very limited. In this paper, we make an important contribution in perspective is relevant for practical applications where we are not only interested to find clusters fundamental parameter for those algorithms and, by leveraging it, it is possible to improve their performances.
 as a graph: given an undirected 1 graph G = ( V,E ), we want to find a set S that minimizes the than  X  S ). To capture this concept rigorously, we consider the cut conductance of a set S as: 2 GLMY11, GS12], and has been identified as one of the most important cut-based measures in the lit-erature [Sch07]. Many approximation algorithms have been developed for the problem, but most of trend, initiated by Spielman and Teng [ST04], and then followed by [ST08, ACL06, AP09, GT12], attempts to solve this conductance minimization problem locally , with running time only dependent on the volume of the output set.
 the existence of some set A g  X  A with at least half the volume, such that for any  X  X ood X  starting vertex v  X  A g , they output a set S with conductance  X  c ( S ) =  X  O ( Finding Well-Connectedness Clusters. All local clustering algorithms developed so far, both disconnected inside. This cannot happen in reality if A is a  X  X ood X  cluster, and in practice we are often interested in finding mostly good clusters. This motivates us to study an extra measure on We assume that, in addition to prior work, the cluster A satisfies the gap assumption ularly relevant when the edges of the graph represent pairwise similarity scores extracted from a machine learning algorithm: we would expect similar nodes to be well connected within themselves However, local algorithms based on the above gap assumption is not well studied. 3 Our Results. Under the gap assumption Gap  X   X (1), can we guarantee any better cut conduc-tance than the previously shown  X  O ( some other desirable properties. In particular, we prove: the PageRank -Nibble algorithm outputs a set S with the gap is, the more accurate S approximates A . 4 For the third property on the cut conductance  X  ( S ), we notice that our guarantee O ( p  X  / Gap )  X  O ( to the cut conductance.
 very different analysis specifically designed for our gap assumption. 5 This algorithm is simple and clean, and can be described in four steps: 1) compute the (approximate) PageRank vector starting from a vertex v  X  A g with carefully chosen parameters, 2) sort all the vertices according to their (normalized) probabilities in this vector, 3) study all sweep cuts that are those separating high-value vertices from low-value ones, and 4) output the sweep cut with the best cut conductance. See Algorithm 1 on page 12 for details.
 Theorem 2. There exists a graph G = ( V,E ) and a non-empty A  X  V with  X  and Gap =  X (1) , vector can output a set S with cut conductance better than O ( p  X  / Gap ) .
 bounds on the probabilities of reaching specific vertices (up to a very high precision). Theorem 2 does not rule out existence of another local algorithm that can perform better than O ( p  X  / Gap ). However, we conjecture that all existing (random-walk-based) local clustering algorithms share the same hard instance and do not outperform O ( p  X  / Gap ), similar to the classical case where they all provide only  X  O ( to design a flow-based local algorithm to overcome this barrier under our gap assumption. Most relevant to our work are the ones on local algorithms for clustering. On the theoretical side, after the first such result [ST04, ST08], [ACL06] simply compute a Pagerank random walk vector and then show that one of its sweep cuts satisfies cut conductance O ( of this Pagerank vector is deterministic and is essentially the algorithm we adopt in this paper. [AP09, GT12] use the theory of evolving set from [MP03]. They study a stochastic volume-biased evolving set process that is similar to a random work. This leads to a better (but probabilistic) running time and but essentially with the same cut conductance guarantee.
 conductance O ( provide a first O (log n ) approximation; and Arora, Rao and Vazirani [ARV09] provide a O ( approximation. Those results, along with recent improvements on the running time by for instance [AHK10, AK07, She09], are all global algorithms: their time complexities depend at least linearly on the size of G . There are also seminal work in machine learning to make such global algorithm practical, including the seminal work of [LC10] for spectral partitioning.
 that have a sub-linear running time in terms of the size of the training set [ZCZ + 09, SSS08]. practice [GS12, GLMY11, ACE + 13, AGM12] as they can be implemented in a distributed manner GLMY11, GS12, AGM12]. Such local algorithms have been applied for (overlapping) clustering of big graphs for distributed computation [AGM12], or community detection on huge Youtube video graphs [GLMY11]. There also exist variants of the random walk, such as the multi-agent random walk, that are known to be local and perform well in practice [AvL10].
 supportive experiments on it. Their experiments confirmed the first two properties in our Theo-well-connectedness assumption in their paper so they are forced to study random walks that start from a random vertex selected in A , rather than a fixed one like ours. In addition, they have not argued about the cut conductance (like our third property in Theorem 1) of the set they output. GS12, AGM12, LLM10]. We provide preliminaries in Section 2, and they are followed by the high level ideas of the proofs for Theorem 1 in Section 3 and Section 4. We then briefly describe how to prove our tightness result in Theorem 5, and end the main body of this paper with empirical studies in Section 6. Consider an undirected graph G ( V,E ) with n = | V | vertices and m = | E | edges. For any vertex u  X  V the degree of u is denoted by deg( u ), and for any subset of the vertices S  X  V , volume of S between A and B .
 G [ S ].
 as follows: conductance of S on the induced subgraph G [ S ].
 and  X  c ( A )  X   X . This set A is not known to the algorithm. The goal is to find some set S that vol( A ) rather than n or m .
 Our assumption. We assume that the following gap assumption: holds throughout this paper. This assumption can be understood as the cluster A is more well-connected inside than it is connected to  X  A . (This assumption can be weakened by replacing the definition of Conn ( A ) with Conn ( A ) def = 1  X  of the random walk matrix on G [ A ] . We discuss them in Appendix A.) Input parameters. Similar to prior work on local clustering, we assume the algorithm takes as input: of the following linear equation (cf. [ACL06]): possibly negative) vector s inside the proof. When it is clear from the context, we drop  X  in the subscript for cleanness.
 non-empty subset S  X  V we denote by  X  S the degree-normalized uniform distribution on S , that is,  X 
S ( u ) = is an indicator vector, and if so we abbreviate pr  X  probability of reaching each vertex in this random procedure. In its mathematical formula we have (cf. [Hav02, ACL06]): Proposition 2.1. pr s =  X s +  X  P  X  t =1 (1  X   X  ) t ( sW t ) .
 algorithm to compute them efficiently.
 Definition 2.2. An  X  -approximate PageRank vector p for pr s is a nonnegative PageRank vector p = pr s  X  r where the vector r is nonnegative and satisfies r ( u )  X   X  deg( u ) for all u  X  V . v given a vector p , one looks for the best cut: by the time to compute p (see Proposition 2.3), so it is negligible. Our proof requires the technique of Lov  X asz-Simonovits Curve that has been more or less used in In other words, for any x  X  [vol( S p j ) , vol( S p j +1 )], Note that p [ x ] is increasing and concave. level, when  X   X  the random walk will leak too much to  X  A ; while when  X   X  2 log n the random walk will not mix well inside A . In prior work,  X  is chosen to be  X ( X ), and we will instead choose in Section 3.1, and then move to the approximate vector in Section 3.2. This essentially proves the first two properties of Theorem 1. We first introduce a new notation starting at vector s but walking on the subgraph G [ A ].
 probability of leakage is upper bounded by 2 X   X  , and (2) pr v is close to that the latter implies that pr v mixes well inside A as long as v  X  A g , in a PageRank vector with teleport probability  X  starting at v , we have: Proof sketch. The proof for the first property (3.1) is classical and can be found in [ACL06]. The idea is to study an auxiliary PageRank random walk with teleport probability  X  starting at the to  X  A with probability no more than  X  / X  . Then, using Markov bound, there exists A g  X  A with implies (3.1) immediately.
 following random procedure: start from vertex v , then at each step with probability  X  let the walk stop, and with probability (1  X   X  ) follow the matrix W to go to one of its neighbors (or itself) and continue. Now, we divide this procedure into two rounds. In the first round, we run the same we let it stop and temporarily  X  X old X  this probability mass. We define l to be the non-negative round, we continue our random walk only from vector l . It is worth noting that l is non-zero only at boundary vertices in A .
 hold exactly the same amount of probability l ( u ) at boundary vertices u , and in the second round we start from l but continue this random walk only within G [ A ]. To bound the difference between pr v and e pr v , we note that they share the same procedure in the first round; while for the second round, the random procedure for pr v starts at l and walks towards V \ A (so in the worst case it may never come back to A again), while that for induces a probability vector the original PageRank random walk leaks from vertex u . Then, k l k 1  X  2 X   X  follows from the fact that the total amount of leakage is upper bounded by 2 X   X  .
 this by first lower bounding state it as the following lemma, and provide its proof in the Appendix C.2.
 Lemma 3.2. When  X   X  O ( X   X  Gap ) we have that Here deg A ( u ) is the degree of u on G [ A ] , but vol( A ) is with respect to the original graph. an  X  -approximate Pagerank vector for pr v . We choose p = pr  X  (2.1) that pr v ( u )  X  p ( u )  X  pr v ( u )  X   X  deg( u ) for all u  X  V .
 Corollary 3.3. For any v  X  A g and  X   X  O ( X   X  Gap ) , in an  X  -approximate PageRank vector to pr v denoted by p = pr  X  probability mass that will be sent to r on vertices outside A , is upper bounded by the probability of leakage. However, the latter is upper bounded by 2 X   X  when we choose A g .
 O  X   X  vol( A ) .
 and A \ S c : the first property.
 that u has to be on the boundary of A and vol( A b )  X  8 X vol( A ).
 we further have u 6 X  S c so p ( u ) &lt; c deg( u ) vol( A ) , it implies that we notice that k e pr l k 1 = k l k 1  X  2 X   X  . This in sum provides that most O (vol( A ) / Gap ) satisfying the first two properties of Theorem 1. In the classical work of [ACL06], they have shown that when  X  =  X ( X ), among all sweep cuts on vector p there exists one with cut conductance O ( under our gap assumption Gap  X   X (1).
 Proof sketch. To convey the idea of the proof, we only consider the case when p = pr v is the exact PageRank vector, and the proof for the approximate case is a bit more involved and deferred to Appendix D.1.
 then it suffices to prove E 0  X  O  X   X  (1  X  O (1 / Gap ))vol( A )) gives We introduce some classical notations before we proceed in the proof. For any vector q we denote E | a  X  A  X  b  X  B } be the set of directed edges from A to B .
 according to Corollary 3.3. In the next step, we use the definition of the lazy random walk matrix W to compute that recall | A  X  S | / | A | .
 and combining (4.1) and (4.2) we obtain that this inequality. Letting x 1 := vol( S 1 / 4 ) and x 2 := vol( S 1 / 8 ), we have that E 0  X  O  X   X  Appendix D.2, and summarize our final algorithm in Algorithm 1. the best cut conductance we can obtain from a local algorithm? We show that this is true if one sticks to a sweep-cut algorithm using PageRank vectors. Figure 1: Our hard instance for proving tightness. One can pick for instance `  X  n 0 . 4 and  X   X  1 so that n/`  X  n 0 . 6 ,  X  n  X  n 0 . 1 and  X  n`  X  n 0 . 5 .
 Algorithm 1 PageRank-Nibble Input: v,  X  and vol 0  X  [ vol( A ) 2 , vol( A )].
 Output: set S . 3: Sort all vertices in supp( p ) according to p ( u ) deg( u ) . 4: Consider all sweep sets S 0 c def = { u  X  supp( p ) : p ( u )  X  c deg( u ) vol hard instance. Consider a (multi-)graph with two chains (see Figure 1) of vertices, and there are multi-edges connecting them. 10 In particular: We let the top chain to be our promised cluster A . The total volume of A is 2 n +  X  n , while the cut conductance  X  c ( A ) =  X  n vol( A )  X   X  2 . Suppose that the gap assumption 11 Gap def = 1  X  this requirement.) probability  X  =  X  and we prove in Appendix Ethe following lemma: computes pr a exactly and looks for all possible sweep cuts, then none of them gives a better cut conductance than O ( p  X  / Gap ). More specifically, for any sweep set S : This ends the proof of Theorem 2. The PageRank local clustering method has been studied empirically in various previous work. For instance, Gleich and Seshadhri [GS12] performed experiments on 15 datasets and confirmed that PageRank outperformed many others in terms of cut conductance, including the famous Metis algorithm. Moreover, [LLDM09] studied PageRank against Metis+MQI which is the Metis algo-rithm plus a flow-based post-processing. Their experiments confirmed that although Metis+MQI outperforms PageRank in terms of cut conductance, however, the PageRank algorithm X  X  outputs are more  X  X ommunity-like X , and they enjoy other desirable properties.
 strating our theoretical discoveries in Theorem 1, without comparisons to other methods. We run our algorithm against both synthetic and real datasets.
 is a cut-conductance guarantee that ensures the output set S has small  X  c ( S ). We now provide experimental results to support them.
 Experiment 1. In the first experiment, we study a synthetic graph of 870 vertices. We carefully cannot identify A up to a very high accuracy. We let the vertices be divided into three disjoint 550 vertices. We assume that A is constructed from the Watts-Strogatz model 13 with mean degree represents the clustering accuracy. The vertical bars are 94% confidence intervals for 100 runs. to interpolate between a regular lattice (  X  = 0) that is not-well-connected and a random graph (  X  = 1) that is well-connected. We then construct the rest of the graph by throwing in random edges, or more specifically, we add an edge connectedness A enjoys, and therefore the larger the gap Gap is in Theorem 1. This should lead to a better performance both in terms of accuracy and conductance when  X  goes larger. one in A , and  X  to be sufficiently small. We then run our algorithm 100 times each time against PageRank-Nibble performs better both in accuracy and cut conductance as Gap goes larger. Experiment 2. In the second experiment, we use the USPS zipcode data set 14 that was also used in the work from [WLS + 12]. Following their experiment, we construct a weighted k -NN graph with denotes the average square distance between each point to its 20th nearest neighbor. 10 separate binary-classification problems. For each of them, we pick an arbitrary starting vertex in it, let  X  = 0 . 003 and  X  = 0 . 00005, and then run our PageRank-Nibble algorithm. We report contains all data points associated with the given digit. We then compare the cut conductance of have smaller conductance than A , because A is not necessarily the sparest cut in the graph.) In addition, one can also confirm from Table 1 that our algorithm enjoys high precision and recall. We thank Lorenzo Orecchia, Jon Kelner, Aditya Bhaskara for helpful conversations. This work is partly supported by Google and a Simons award (grant no. 284059). As mentioned in Section 2, we can relax our gap assumption to Assumption).
 Lemma 3.2 under the two new assumptions, and we provide such analysis in Appendix C.2. In this section we briefly summarize the algorithm Approximate-PR (see Algorithm 2) proposed by Andersen, Chung and Lang [ACL06] to compute an approximate PageRank vector. At high level, Approximate-PR is an iterative algorithm, and maintains an invariant that p is always equal to pr s  X  r at each iteration.
 PageRank vector according to Definition 2.2 at this initial step.
 of p , i.e., r ( u )  X   X  deg( u ), and pushes this r ( u ) amount of probability mass elsewhere: One can verify that after any push step the newly computed p and r will still satisfy p = pr s  X  r . This indicates that the invariant is satisfied at all iterations. When Approximate-PR terminates, PageRank vector.
 small: u i is the vertex chosen at the i -th iteration and T is the number of iterations. However, it is not thus Approximate-PR runs in time O 1  X  X  .
 amount of probability mass must come from r ( u ) during the algorithm, and thus vertex u must be k r k 1  X  1 because k s k 1  X  1, so the total volume for such vertices cannot exceed 2 Algorithm 2 Approximate-PR (from [ACL06]) Input: starting vector s , teleport probability  X  , and approximate ratio  X  .
 Output: the  X  -approximate PageRank vector p = pr s  X  r . 1: p  X  ~ 0 and r  X  s . 2: while r ( u )  X   X  deg( u ) for some vertex u  X  V do 3: Pick an arbitrary u satisfying r ( u )  X   X  deg( u ). 4: p ( u )  X  p ( u ) +  X r ( u ). 5: For each vertex v such that ( u,v )  X  E : 6: r ( u )  X  1  X   X  2 r ( u ). 7: end while 8: return p .
 v  X  A g , in a PageRank vector with teleport probability  X  starting at v , we have: Leakage event. We begin our proof by defining the leaking event in a random walk procedure. We start the definition of a lazy random walk and then move to a PageRank random walk. At high level, we say that a lazy random walk of length t starting at a vertex u  X  A does not leak from A In addition, if u has k bad edges, we also distinguish k self-loops at u in the lazy random walk graph, and call them bad self-loops . Now, we say that a random walk does not leak from A , if it if a random walk chooses only good edges at each step, it is equivalent to a lazy random walk on the induced subgraph G [ A ] with outgoing edges removed.
 probability  X  (1  X   X  ) t , and then performing a lazy random walk of length t starting from u . By the linearity of random walk vectors, the probability of leakage for this Pagerank random walk is exactly P  X  t =0  X  (1  X   X  ) t Leak ( u,t ).
 Upper bounding leakage. We now give an upper bound on the probability of leakage. We start random walk, the probability of leakage is upper bounded by  X  by the definition of cut conductance; u  X  A , and therefore the probability of leakage in the i -th step is upper bounded by that in the satisfying on (3.2).
 Lower bounding pr . Now we pick some v  X  A g , and try to lower bound pr v . To begin with, we is the degree of a vertex and for u  X  A we denote by deg A ( u ) the number of neighbors of u inside A ): The major difference between f W and c W is that they are normalized by different degrees in the rows, and the rows of f W sum up to 1 but those of c W do not necessarily. More specifically, if we denote by D the diagonal matrix with deg( u ) on the diagonal for each vertex u  X  A , and D A the by our definition of Leak .
 Claim C.1. There exists non-negative vectors l t for all t  X  X  1 , 2 ,... } satisfying: and Proof. To obtain the result of this claim, we write c the t -th step; or equivalently, k l t k 1 = Leak ( v,t )  X  Leak ( v,t  X  1).
 vector holds coordinate-wisely on all vertices in A : coordinate-wisely on all vertices in A according to the definition of c W : Claim C.1 we further reduce the computation on matrix c W to that on matrix f W : We next combine the above two inequalities and compute pr At last, we upper bound the one norm of l using Claim C.1 again: where the last inequality uses (C.1).
 will then extend it to the weakest assumption  X   X  O 1  X  between those three assumptions, see Appendix A.
 starting at any vertex v  X  A will land in a vertex u  X  A with probability: Now if we choose T 0 = 3+log vol( A )  X  then for any t  X  T 0 : We then convert this into the language of PageRank vectors:
X At last, we notice that  X   X  1 9 T one among all three. Proof. We only point out how to extend our proof in the exact case (see Section 4) to the case when p is an  X  -approximate PageRank vector. For any set S 1 / 4  X  S  X  S 1 / 8 , we compute that Therefore, we have In sum, we have arrived at the same conclusion as (4.1) in the case when p is only approximate, and the rest of the proof follows in the same way as in the exact case.
 We are ready to put together all previous lemmas to show the main theorem of this paper. the PageRank -Nibble algorithm outputs a set S with Proof. As in Algorithm 1, we choose  X  =  X ( X   X  Gap ) to satisfy the requirements of all previous lemmas. We define A g according to Lemma 3.1 and compute an  X  -approximate PageRank vector starting from v where  X  = 1 10vol Lemma 3.4 guarantees the first two properties of the theorem.
  X  ( S d  X  ) = O (  X  ( S d  X  ) = O ( approximate PageRank vector is the bottleneck for the running time, we conclude that Algorithm 1 In this section we show that our cut conductance analysis for Theorem 1 is tight. We emphasize and start to upper and lower bound the probabilities of reaching specific vertices up to a very high precision. This is different from the mixing time theory on Markov chains, as for instance, on a chain of ` vertices it is known that a random walk of O ( ` 2 ) steps mixes, but in addition we need to compute how faster it mixes on one vertex than another vertex.
 chain, and then in Appendix E.2 we formally prove Lemma 5.1 with the help from those lemmas. In this subsection we provide four useful lemmas about a PageRank random walk on a single chain. For instance, in the first of them we study a chain of length ` and compute an upper bound on the probability to reach the rightmost vertex from the leftmost one. The other three lemmas are similar in this format. Those lemmas require the study of the eigensystem of a lazy random walk matrix on this chain, followed by very careful but problem-specific analyses.
 vertex indexed by 0 and the rightmost vertex indexed by ` . Let pr  X  random walk starting at vertex 0 with teleport probability  X  =  X  Proof. Let us define to be the ( ` + 1)  X  ( ` + 1) lazy random walk matrix of our chain. For k = 0 , 1 ,...,` , define: i  X  X  1 , 2 ,...,`  X  1 } . Then it is routinary to verify that v k  X  W =  X  k  X  v k and thus We remark here that since W is not symmetric, those eigenvectors are not orthogonal to each other they form an orthonormal basis.
 As a consequence when t &gt; 0, using  X  ` = 0: Now it is easy to compute the exact probability of reaching the right-most vertex ` : At last, we translate this language into the PageRank vector pr  X  We remark here that the last inequality is obtained using Taylor approximation. vertex indexed by 0 and the rightmost vertex indexed by ` . Let pr  X  random walk starting at vertex 0 with teleport probability  X  =  X  Proof. Recall from the proof of Lemma E.1 that for t &gt; 0 we have Now it is easy to compute the exact probability of reaching the middle vertex ` 2 : At last, we translate this language into the PageRank vector pr  X  We remark here that the last inequality is obtained using Taylor approximation. vertex indexed by 0 and the rightmost vertex indexed by ` . Let pr  X  random walk starting at the middle vertex `/ 2 with teleport probability  X  =  X  Then, under this orthonormal basis: Then similar to the proof of Lemma E.1 we have that for all t &gt; 0 Now it is easy to compute the exact probability of reaching the middle vertex ` 2 :  X  Notice that in the last equality we have used a recent result on power sum of cosines that can be found in Theorem 1 of [Mer12]. Next we perform some classical tricks on binomial coefficients:
X and in the last inequality we have used a famous upper bound on the central binomial coefficient We remark here that the last inequality is obtained using Taylor approximation. chain is infinite both to the left and to the right of the origin. Now we study the PageRank random walk on this infinite chain that starts from the origin with teleport probability  X  =  X  by pr  X  Proof. As before we begin with the analysis of a lazy random walk of a fixed length t , and will number of steps, there are t 1  X  t number of them in which the random walk moves either to the left or to the right, while in the remaining t  X  t 1 of them the random walk stays. This happens with probability t t happens with probability t 1 t random walk is: X Here in the last inequality we have used the famous lower bound on the central binomial coefficient walk: Here in the last inequality we have used the Taylor approximation for the Gaussian error function erf.
 We are now ready to show the proof for Lemma 5.1.
 Lemma 5.1. For any  X   X  (0 , 4] and letting  X  =  X /` 2 , there exists some constant c 0 such that when already done in Appendix E.1. They together will imply that In the third step, we show that must be a large amount of probability mass moving from b to d due to the nature of PageRank bottom, giving a contradiction to pr a ( d ) deg( d ) being small.
 (E.2) and (E.3).
 Step 1: upper bounding pr a ( c ) / deg( c ). In the first step we upper bound the probability of reaching vertex c . Since removing the edges between b and d will disconnect the graph and thus language, taking into account the multi-edges, we have that Lemma 3.1. Letting teleport probability  X  , then Lemma 3.1 (and its actual proof) implies that pr a ( b )  X  since a  X  A g is a good starting vertex. We can rewrite this as Next we use Lemma E.2 and Lemma E.3 to deduce that: At last, we normalize this probability by its degree deg( b ) = 2 n/` +  X  n and get: Step 3: lower bounding pr a ( d ) / deg( d ). Since we have already shown a good lower bound apply to vertex d as well because b and d are neighbors. This is not true in general, for instance if d were connected to a very large complete graph then all probability mass that reached d would be badly diluted. However, with our careful choice of the bottom chain, we will show that this is true in our case.
 bility is the amount of probability that will leak from A , subtracted by the amount of probability that will come back to A .
 the PageRank random walk runs for a total of t steps (which happens with probability  X  (1  X   X  ) t ), way to compute the total amount of net leakage of a PageRank random walk: NetLeakage = Now we have a decent lower bound on the amount of net leakage, and we want to further lower bound pr a ( d ) using this NetLeakage quantity. We achieve so by studying an auxiliary  X  X andom walk X  procedure q ( t ) , where q (0) = p (0) =  X  a , but q ( t ) ( u ) = 0 for u 6 X  A . 17 Then we have that: further notice that  X  = pr  X  also as a PageRank vector. We highlight here that  X  is a vector that is non-zero only at to the first equality in (E.6).
 pr a ( d ) =  X ( d ) = pr  X  ( d ) = pr (  X  ( d )  X  continue pr a ( d )  X  ( NetLeakage )  X  pr d ( d )  X  pr a ( d )  X  Step 4: putting it all together. We now define (using the fact that  X  &gt; 0 and  X  &lt; 4) constant c 2 to satisfy satisfies that (using the fact that  X  ` 2 = o (1)) Next, if we choose c 1 = c 2 2 and c 0 = 2 c [ACE + 13] L. Alvisi, A. Clement, A. Epasto, S. Lattanzi, and A. Panconesi. The evolution of sybil [ACL06] Reid Andersen, Fan Chung, and Kevin Lang. Using pagerank to locally partition a [AHK10] Sanjeev Arora, Elad Hazan, and Satyen Kale. O(sqrt(log(n)) approximation to sparsest [AK07] Sanjeev Arora and Satyen Kale. A combinatorial, primal-dual approach to semidefinite [AL06] Reid Andersen and Kevin J. Lang. Communities from seed sets. WWW  X 06, pages [Alo86] Noga Alon. Eigenvalues and expanders. Combinatorica , 6(2):83 X 96, 1986. [AP09] Reid Andersen and Yuval Peres. Finding sparse cuts locally using evolving sets. STOC, [ARV09] Sanjeev Arora, Satish Rao, and Umesh V. Vazirani. Expander flows, geometric embed-[AvL10] Morteza Alamgir and Ulrike von Luxburg. Multi-agent random walks for local clustering [CKK + 06] Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yuval Rabani, and D. Sivakumar. [GLMY11] Ullas Gargi, Wenjun Lu, Vahab S. Mirrokni, and Sangho Yoon. Large-scale community [GS12] David F. Gleich and C. Seshadhri. Vertex neighborhoods, low conductance cuts, and [GT12] Shayan Oveis Gharan and Luca Trevisan. Approximating the expansion profile and [Hav02] Taher H. Haveliwala. Topic-sensitive pagerank. In WWW  X 02 , pages 517 X 526, 2002. [KVV04] Ravi Kannan, Santosh Vempala, and Adrian Vetta. On clusterings: Good, bad and [LC10] Frank Lin and William W. Cohen. Power iteration clustering. In ICML  X 10 , pages [LLDM09] Jure Leskovec, Kevin J. Lang, Anirban Dasgupta, and Michael W. Mahoney. Communi-[LLM10] Jure Leskovec, Kevin J. Lang, and Michael Mahoney. Empirical comparison of algo-[LR99] Frank Thomson Leighton and Satish Rao. Multicommodity max-flow min-cut theorems [LS90] L  X aszl  X o Lov  X asz and Mikl  X os Simonovits. The mixing rate of markov chains, an isoperi-[LS93] L  X aszl  X o Lov  X asz and Mikl  X os Simonovits. Random walks in a convex body and an improved [Mer12] Mircea Merca. A note on cosine power sums. Journal of Integer Sequences , 15:12.5.3, [MMV12] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approxi-[MP03] Ben Morris and Yuval Peres. Evolving sets and mixing. STOC  X 03, pages 279 X 286. [MR95] Rajeev Motwani and Prabhakar Raghavan. Randomized algorithms . Cambridge Uni-[Sch07] S. E. Schaeffer. Graph clustering. Computer Science Review, , 1(1):27 X 64, 2007. [She09] Jonah Sherman. Breaking the multicommodity flow barrier for o ( [SJ89] Alistair Sinclair and Mark Jerrum. Approximate counting, uniform generation and [SM00] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on [SSS08] Shai Shalev-Shwartz and Nathan Srebro. SVM optimization: inverse dependence on [ST04] Daniel Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph parti-[ST08] Daniel Spielman and Shang-Hua Teng. A local clustering algorithm for massive graphs [WLS + 12] Xiao-Ming Wu, Zhenguo Li, Anthony Man-Cho So, John Wright, and Shih-Fu Chang. [ZCZ + 09] Zeyuan Allen Zhu, Weizhu Chen, Chenguang Zhu, Gang Wang, Haixun Wang, and
 Zeyuan Allen Zhu zeyuan@csail.mit.edu MIT CSAIL, 32 Vassar St., Cambridge, MA 02139 USA Silvio Lattanzi silviol@google.com Vahab Mirrokni mirrokni@google.com Google Research, 111 8th Ave., 4th floor, New York, NY 10011 USA As a central problem in machine learning, clustering methods have been applied to data mining, computer vision, social network analysis. Although a huge num-ber of results are known in this area, there is still need to explore methods that are robust and efficient on large data sets, and have good theoretical guarantees. In particular, several algorithms restrict the number of clusters, or impose constraints that make these al-gorithms impractical for large data sets.
 To solve those issues, recently, local random-walk clus-tering algorithms (Spielman &amp; Teng, 2004; Andersen et al., 2006) have been introduced. The main idea be-hind those algorithms is to find a good cluster around a specific node. These techniques, thanks to their scal-ability, has had high impact in practical applications (Leskovec et al., 2009; Gargi et al., 2011; Gleich &amp; Se-shadhri, 2012; Andersen et al., 2012; Leskovec et al., 2010; Wu et al., 2012). Nevertheless, the theoretical understanding of these techniques is still very limited. In this paper, we make an important contribution in this direction. First, we relate for the first time the performance of these local algorithms with the inter-nal connectivity of a cluster instead of analyzing only its external connectivity. This change of perspective is relevant for practical applications where we are not only interested to find clusters that are loosely con-nected with the rest of the world, but also clusters that are well-connected internally. In particular, we show theoretically and empirically that this internal connectivity is a fundamental parameter for those al-gorithms and, by leveraging it, it is possible to improve their performances.
 Formally, we study the clustering problem where the data set is given by a similarity matrix as a graph: giv-en an undirected 1 graph G = ( V,E ), we want to find a set S that minimizes the relative number of edges going out of S with respect to the size of S (or the size of  X  S if S is larger than  X  S ). To capture this concept rigorously, we consider the cut conductance of a set S as: where vol( S ) def = P v  X  S deg( v ). Finding S with the s-mallest  X  c ( S ) is called the conductance minimization. This measure is a well-studied measure in differen-t disciplines (Shi &amp; Malik, 2000; Spielman &amp; Teng, 2004; Andersen et al., 2006; Gargi et al., 2011; Gle-ich &amp; Seshadhri, 2012), and has been identified as one of the most important cut-based measures in the lit-erature (Schaeffer, 2007). Many approximation algo-rithms have been developed for the problem, but most of them are global ones: their running time depends at least linearly on the size of the graph. A recen-t trend, initiated by Spielman and Teng (2004), and then followed by (Spielman &amp; Teng, 2008; Andersen et al., 2006; Andersen &amp; Peres, 2009; Gharan &amp; Tre-visan, 2012), attempts to solve this conductance min-imization problem locally , with running time only de-pendent on the volume of the output set.
 In particular, if there exists a set A  X  V with  X  c ( A )  X   X , these local algorithms guarantee the existence of some set A g  X  A with at least half the volume, such that for any  X  X ood X  starting vertex v  X  A g , they out-put a set S with conductance  X  c ( S ) =  X  O ( Finding Well-Connectedness Clusters. All local clustering algorithms developed so far, both theoreti-cal ones and empirical ones, only assume that  X  c ( A ) is small, i.e., A is poorly connected to  X  A . Notice that such set A , no matter how small  X  c ( A ) is, may be poorly connected or even disconnected inside. This cannot happen in reality if A is a  X  X ood X  cluster, and in practice we are often interested in finding mostly good clusters. This motivates us to study an extra measure on A , that is the connectedness of A , denoted as Conn ( A ) and we will define it formally in Section 2. We assume that, in addition to prior work, the cluster A satisfies the gap assumption which says that A is better connected inside than it is connected to  X  A . This assumption is particularly rel-evant when the edges of the graph represent pairwise similarity scores extracted from a machine learning al-gorithm: we would expect similar nodes to be well con-nected within themselves while dissimilar nodes to be loosely connected. As a result, it is not surprising that the notion of connectedness is not new. For instance (Kannan et al., 2004) studied a bicriteria optimization for this objective. However, local algorithms based on the above gap assumption is not well studied. 2 Our Results. Under the gap assumption Gap  X   X (1), can we guarantee any better cut conductance than the previously shown  X  O ( that the answer is affirmative, along with some other desirable properties. In particular, we prove: Theorem 1. If there exists a non-empty set A  X  V such that  X  c ( A )  X   X  and Gap  X   X (1) , then there exists some A g  X  A with vol( A g )  X  1 2 vol( A ) such that, when choosing a starting vertex v  X  A g , the PageRank-Nibble algorithm outputs a set S with 1. vol( S \ A )  X  O ( 1 2. vol( A \ S )  X  O ( 1 3.  X  c ( S )  X  O ( p  X  / Gap ) , and We interpret the above theorem as follows. The first two properties imply that under Gap  X   X (1), the vol-ume for vol( S \ A ) and vol( A \ S ) are both small in comparison to vol( A ), and the larger the gap is, the more accurate S approximates A . 3 For the third prop-erty on the cut conductance  X  c ( S ), we notice that our guarantee O ( p  X  / Gap )  X  O ( vious work on local clustering under this gap assump-tion. In addition, Gap might be very large in reality. For instance when A is a very-well-connected cluster it might satisfy Conn ( A ) = polylog ( n ), and as a con-sequence Gap may be as large as  X   X (1 /  X ). In this case our Theorem 1 guarantees a polylog ( n ) approximation to the cut conductance.
 Our proof of Theorem 1 uses almost the same PageR-ank algorithm as (Andersen et al., 2006), but with a very different analysis specifically designed for our gap assumption. This algorithm is simple and clean, and can be described in four steps: 1) compute the (approximate) PageRank vector starting from a ver-tex v  X  A g with carefully chosen parameters, 2) sort all the vertices according to their (normalized) prob-abilities in this vector, 3) study all sweep cuts that are those separating high-value vertices from low-value ones, and 4) output the sweep cut with the best cut conductance. See Algorithm 1 for details.
 We also prove that our analysis is tight.
 Theorem 2. There exists a graph G = ( V,E ) and a non-empty A  X  V with  X  and Gap =  X (1) , such that for all starting vertices v  X  A , none of the sweep-cut based algorithm on the PageRank vector can output a set S with cut conductance better than O ( p  X  / Gap ) . We prove this tightness result by illustrating a hard instance, and proving upper and lower bounds on the probabilities of reaching specific vertices (up to a very high precision). Theorem 2 does not rule out exis-tence of another local algorithm that can perform bet-ter than O ( p  X  / Gap ). However, we conjecture that all existing (random-walk-based) local clustering al-gorithms share the same hard instance and do not outperform O ( p  X  / Gap ), similar to the classical case where they all provide only  X  O ( Cheeger X  X  inequality. It is an interesting open question to design a flow-based local algorithm to overcome this barrier under our gap assumption.
 Prior Work. Related work is discussed in depth in the full version of this paper.
 Roadmap. We provide preliminaries in Section 2, and they are followed by the high level ideas of the proofs for Theorem 1 in Section 3 and Section 4. We then briefly describe how to prove our tightness result in Theorem 5, and end this extended abstract with empirical studies in Section 6. 2.1. Problem Formulation Consider an undirected graph G ( V,E ) with n = | V | vertices and m = | E | edges. For any vertex u  X  V the degree of u is denoted by deg( u ), and for any sub-set of the vertices S  X  V , volume of S is denoted by vol( S ) def = P u  X  S deg( u ). Given two subsets A,B  X  V , let E ( A,B ) be the set of edges between A and B . For a vertex set S  X  V , we denote by G [ S ] the induced subgraph of G on S with outgoing edges removed, by deg S ( u ) the degree of vertex u  X  S in G [ S ], and by vol S ( T ) the volume of T  X  S in G [ S ].
 We respectively define the cut conductance and the set conductance of a non-empty set S  X  V as follows: Here  X  c ( S ) is classically known as the conductance of S , and  X  s ( S ) is classically known as the conductance of S on the induced subgraph G [ S ].
 We formalize our goal in this paper as a promise problem . Specifically, we assume the existence of a non-empty cluster of the vertices A  X  V satisfying This set A is not known to the algorithm. The goal is to find some set S that  X  X easonably X  approximates A , and at the same time be local : running in time proportional to vol( A ) rather than n or m .
 Our assumption. We assume that the following gap assumption: holds throughout this paper. This assumption can be understood as the cluster A is more well-connected inside than it is connected to  X  A . (This assumption can be weakened by replacing the definition of Conn ( A ) with Conn ( A ) def = 1  X   X  mix ( A ) is the mixing time for the relative pointwise distance in G [ A ] ; or less weakly Conn ( A ) def =  X  ( A ) where  X  ( A ) is the spectral gap , i.e., 1 minus the second largest eigenvalue of the random walk matrix on G [ A ] . We discuss them in the full version of this paper.) Input parameters. Similar to prior work on local clustering, we assume the algorithm takes as input:  X  Some  X  X ood X  starting vertex v  X  A , and an oracle to output the set of neighbors for any given vertex.
This requirement is essential because without such an oracle the algorithm may have to read all inputs and cannot be sublinear in time; and without a s-tarting vertex the sublinear-time algorithm may be unable to even find an element in A .

We also need v to be  X  X ood X , as for instance the vertices on the boundary of A may not be helpful enough in finding good clusters. We call the set of good vertices A g  X  A , and a local algorithm needs to ensure that A g is large, i.e., vol( A g )  X  1 2 vol( A ).  X  The value of  X  .

In practice  X  can be viewed as a parameter and can be tuned for specific data. This is in contrast to the value of  X  that is the target cut conductance and does not need to be known by the algorithm. 5  X  A value vol 0 satisfying vol( A )  X  [vol 0 , 2vol 0 ] . 2.2. PageRank Random Walk We use the convention of writing vectors as row vectors in this paper. Let A be the adjacency matrix of G , and let D be the diagonal matrix with D ii = deg( i ), then the lazy random walk matrix W def = 1 2 ( I + D  X  1 A ). Accordingly, the PageRank vector pr s, X  , is defined to be the unique solution of the following linear equation (cf. (Andersen et al., 2006)): where  X   X  (0 , 1] is the teleport probability and s is a starting vector . Here s is usually a probability vector: its entries are in [0 , 1] and sum up to 1. For technical reasons we may use an arbitrary (and possibly nega-tive) vector s inside the proof. When it is clear from the context, we drop  X  in the subscript for cleanness. Given a vertex u  X  V , let  X  u  X  X  0 , 1 } V be the indicator vector that is 1 only at vertex u . Given non-empty sub-set S  X  V we denote by  X  S the degree-normalized u-niform distribution on S , that is,  X  S ( u ) = deg( u ) vol( S ) u  X  S and 0 otherwise. Very often we study a PageR-ank vector when s =  X  v is an indicator vector, and if so we abbreviate pr  X  v by pr v .
 One equivalent way to study pr s is to imagine the fol-lowing random procedure: first pick a non-negative in-teger t  X  Z  X  0 with probability  X  (1  X   X  ) t , then perform a lazy random walk starting at vector s with exactly t steps, and at last define pr s to be the vector describing the probability of reaching each vertex in this random procedure. In its mathematical formula we have (cf. (Haveliwala, 2002; Andersen et al., 2006)): Proposition 2.1. pr s =  X s +  X  P  X  t =1 (1  X   X  ) t ( sW t This implies that pr s is linear: a  X  pr s + b  X  pr t = pr 2.3. Approximate PageRank Vector In the seminal work of (Andersen et al., 2006), they defined approximate PageRank vectors and designed an algorithm to compute them efficiently.
 Definition 2.2. An  X  -approximate PageRank vector p for pr s is a nonnegative PageRank vector p = pr s  X  r where the vector r is nonnegative and satisfies r ( u )  X   X  deg( u ) for all u  X  V .
 Proposition 2.3. For any starting vector s with k s k 1  X  1 and  X   X  (0 , 1] , one can compute an  X  -approximate PageRank vector p = pr s  X  r for some r in time O 1  X  X  , with vol(supp( p ))  X  2 (1  X   X  )  X  . For completeness we provide the algorithm and its proof in the full version. It can be verified that:  X  u  X  V, pr s ( u )  X  p ( u )  X  pr s ( u )  X   X  deg( u ) . (2.1) 2.4. Sweep Cuts Given any approximate PageRank vector p , the sweep cut (or threshold cut) technique is the one to sort al-l vertices according to their degree-normalized prob-separate high-value vertices from low-value vertices. More specifically, let v 1 ,v 2 ,...,v n be the decreasing order over all vertices with respect to p ( u ) deg( u ) and sweep cuts are the corresponding cuts ( S p j , S Usually given a vector p , one looks for the best cut: In almost all the cases, one only needs to enumerate j over p ( v j ) &gt; 0, so the above sweep cut procedure runs in time O vol(supp( p ))+ | supp( p ) | X  log | supp( p ) | . This running time is dominated by the time to compute p (see Proposition 2.3), so it is negligible. 2.5. Lov  X asz-Simonovits Curve Our proof requires the technique of Lov  X asz-Simonovits Curve that has been more or less used in all local clus-tering algorithms so far. This technique was originally introduced by Lov  X asz and Simonovits (1990; 1993) to study the mixing rate of Markov chains. In our lan-guage, from a probability vector p on vertices, one can introduce a function p [ x ] on real number x  X  [0 , 2 m ]. This function p [ x ] is piecewise linear, and is char-acterized by all of its end points as follows (letting In other words, for any x  X  [vol( S p j ) , vol( S p j +1 Note that p [ x ] is increasing and concave. In this section, we study PageRank random walks that start at a vertex v  X  A with teleport probability  X  . We claim the range of interesting  X  is  X ( X ) ,O (  X  2 log n This is because, at a high level, when  X   X  the random walk will leak too much to  X  A ; while when  X   X  2 log n the random walk will not mix well inside A . In prior work,  X  is chosen to be  X ( X ), and we will instead choose  X  =  X (  X  2 log n ) =  X ( X   X  Gap ). Intuitively, this choice of  X  ensures that under the condition the random walk mixes inside, it makes the walk leak as little as possible to  X  A . We prove the above intuition rigorously in this section. Specifically, we first show some properties on the exact PageRank vector in Sec-tion 3.1, and then move to the approximate vector in Section 3.2. This essentially proves the first two prop-erties of Theorem 1. 3.1. Properties on the Exact Vector We first introduce a new notation PageRank vector (with teleport probability  X  ) starting at vector s but walking on the subgraph G [ A ]. Next, we choose the set of  X  X ood X  starting vertices A g to satisfy two properties: (1) the total probability of leakage is upper bounded by 2 X   X  , and (2) pr v is close to that pr v mixes well inside A as long as Lemma 3.1. There exists a set A g  X  A with volume vol( A g )  X  1 2 vol( A ) such that, for any vertex v  X  A a PageRank vector with teleport probability  X  starting at v , we have: In addition, there exists a non-negative leakage vector l  X  [0 , 1] V with norm k l k 1  X  2 X   X  satisfying (Details of the proof are in the full version.) Proof sketch. The proof for the first property (3.1) is classical and can be found in (Andersen et al., 2006). The idea is to study an auxiliary PageRank random walk with teleport probability  X  starting at the degree-normalized uniform distribution  X  A , and by simple computation, this random walk leaks to  X  A with proba-bility no more than  X  / X  . Then, using Markov bound, there exists A g  X  A with vol( A g )  X  1 2 vol( A ) such that for each starting vertex v  X  A g , this leakage is no more than 2 X   X  . This implies (3.1) immediately.
 The interesting part is (3.2). Note that pr v can be viewed as the probability vector from the following random procedure: start from vertex v , then at each step with probability  X  let the walk stop, and with probability (1  X   X  ) follow the matrix W to go to one of its neighbors (or itself) and continue. Now, we divide this procedure into two rounds. In the first round, we run the same PageRank random walk but whenever the walk wants to use an outgoing edge from A to leak, we let it stop and temporarily  X  X old X  this probability mass. We define l to be the non-negative vector where l ( u ) denotes the amount of probability that we have  X  X eld X  at vertex u . In the second round, we continue our random walk only from vector l . It is worth noting that l is non-zero only at boundary vertices in A . Similarly, we divide the PageRank random walk for e pr v into two rounds. In the first round we hold exact-ly the same amount of probability l ( u ) at boundary vertices u , and in the second round we start from l but continue this random walk only within G [ A ]. To bound the difference between pr v and that they share the same procedure in the first round; while for the second round, the random procedure for pr v starts at l and walks towards V \ A (so in the worst case it may never come back to A again), while that for e pr v starts at l and walks only inside G [ A ] so induces a probability vector At last, to see k l k 1  X  2 X   X  , one just needs to verify that l ( u ) is essentially the probability that the original PageRank random walk leaks from vertex u . Then, of leakage is upper bounded by 2 X   X  .
 As mentioned earlier, we want to use (3.2) to lower bound pr v ( u ) for vertices u  X  A . We achieve this by first lower bounding dom walk on G [ A ]. Given a teleport probability  X  that is small compared to  X  2 log vol( A ) , this random walk should mix well. We formally state it as the following lemma, and provide its proof in the the full version. Lemma 3.2. When  X   X  O ( X   X  Gap ) we have that Here deg A ( u ) is the degree of u on G [ A ] , but vol( A ) is with respect to the original graph. 3.2. Properties of the Approximate Vector From this section on we always use  X   X  O ( X   X  Gap ). We then fix a starting vertex v  X  A g and study an  X  -approximate Pagerank vector for pr v . We choose For notational simplicity, we denote by p this  X  -approximation and recall from Section 2.3 that p = pr  X  v  X  r where r is a non-negative vector with 0  X  r ( u )  X   X  deg( u ) for every u  X  V . Recall from (2.1) We now rewrite Lemma 3.1 in the language of approx-imate PageRank vectors using Lemma 3.2: Corollary 3.3. For any v  X  A g and  X   X  O ( X   X  Gap ) , in an  X  -approximate PageRank vector to pr v denoted by p = pr  X  v  X  r , we have: In addition, there exists a non-negative leakage vector l  X  [0 , 1] V with norm k l k 1  X  2 X   X  satisfying  X  u  X  A, p ( u )  X  Proof. The only inequality that requires a proof is P at the algorithm to compute an approximate Pager-ank vector (see the full version), the total probability mass that will be sent to r on vertices outside A , is upper bounded by the probability of leakage. Howev-er, the latter is upper bounded by 2 X   X  when we choose A We are now ready to state the main lemma of this section. We show that for all reasonable sweep sets S on this probability vector p , it satisfies that vol( S \ A ) and vol( A \ S ) are both at most O  X   X  vol( A ) . Lemma 3.4. In the same definition of  X  and p from Corollary 3.3, let sweep set S c def = u  X  V : p ( u )  X  c vol( A ) for any constant c &lt; following guarantees on the size of S c \ A and A \ S c : 1. vol( S c \ A )  X  2 X   X c vol( A ) , and 2. vol( A \ S c )  X  2 X  Proof. First we notice that p ( S c \ A )  X  p ( V \ A )  X  owing to Corollary 3.3, and for each vertex u  X  S c \ A vol( S c \ A )  X  2 X   X c vol( A ) proving the first property. We show the second property in two steps. First, let A b be the set of vertices in A such that 4 5 deg A ( u ) &lt; 7 8 deg( u ). This implies that u has to be on the boundary of A and vol( A b )  X  8 X vol( A ).
 Next, for a vertex u  X  A \ A b we have (using Corol-e pr l ( u )  X  ( volume for such vertices (i.e., vol( A \ ( A b  X  S c ))) can-a non-negative probability vector coming from a ran-dom walk procedure, so k sum provides that Note that if one chooses  X  =  X ( X   X  Gap ) in the above lemma, both those two volumes are at most O (vol( A ) / Gap ) satisfying the first two properties of Theorem 1. In the classical work of (Andersen et al., 2006), they have shown that when  X  =  X ( X ), among all sweep cuts on vector p there exists one with cut conductance O (  X  under our gap assumption Gap  X   X (1).
 Lemma 4.1. Letting  X  =  X ( X   X  Gap ) , among all sweep there exists one, denoted by S c  X  , with cut conductance  X  ( S c  X  ) = O ( p  X  / Gap ) .
 Proof sketch. To convey the idea of the proof, we only consider the case when p = pr v is the exact PageRank vector, and the proof for the approximate case is a bit more involved and deferred to the full version. Suppose that all sweep sets S c for c  X  [ 1 8 , 1 4 ] satisfy | E ( S c ,V \ S c ) | X  E 0 for some value E 0 , then it suffices to prove E 0  X  O  X   X   X  vol( A ). This is because, if so, then there exists some S c  X  with | E ( S c  X  ,V \ S c  X  ) | X  E and this combined with the result in Lemma 3.4 (i.e., vol( S c  X  ) = (1  X  O (1 / Gap ))vol( A )) gives  X  We introduce some classical notations before we pro-ceed in the proof. For any vector q we denote by q ( S ) def = P u  X  S q ( u ). Also, given a directed edge a set of directed edges E 0 we let p ( E 0 ) def = P e  X  E 0 We also let E ( A,B ) def = { ( a,b )  X  E | a  X  A  X  b  X  B } be the set of directed edges from A to B .
 Now for any set S 1 / 4  X  S  X  S 1 / 8 , we compute that =  X  (1  X   X  ) p ( S )  X   X  (1  X  p ( S )) + (1  X   X  )( pW )( S ) =  X  (1  X   X  ) p ( S )  X  2 X  + (1  X   X  )( pW )( S ) Here we have used the fact that when p = pr v is exact, it satisfies 1  X  p ( S ) = p ( V  X  S )  X  2 X  / X  according to Corollary 3.3. In the next step, we use the definition of the lazy random walk matrix W to compute that Here the first inequality is due to the definition of the Lov  X asz-Simonovits curve p [ x ], and the second inequal-ity is because p [ x ] is concave. Next, suppose that in addition to S 1 / 4  X  S  X  S 1 / 8 , we also know that S is a sweep set, i.e.,  X  a  X  S,b 6 X  S we have p ( a ) deg( a )  X  This implies p ( S ) = p [vol( S )] and combining (4.1) and (4.2) we obtain that Since we can choose S to be an arbitrary sweep set between S 1 / 4 and S 1 / 8 , we have that the inequality end points x  X  [vol( S 1 / 4 ) , vol( S 1 / 8 )] on the piecewise linear curve p [ x ]. This implies that the same inequality holds for any real number x  X  [vol( S 1 / 4 ) , vol( S as well. We are now ready to draw our conclusion by repeatedly applying this inequality. Letting x 1 := vol( S 1 / 4 ) and x 2 := vol( S 1 / 8 ), we have where the first inequality uses the definition of S 1 / 4 the fifth inequality uses the definition of S 1 / 8 last inequality uses Lemma 3.4 again. After re-arranging the above inequality we conclude that E 0  X  O  X   X   X  vol( A ) and finish the proof.
 The lemma above essentially shows the third property of Theorem 1 and finishes the proof of Theorem 1. For completeness of the paper, we still provide the formal proof for Theorem 1 in the full version, and summarize our final algorithm in Algorithm 1. It is a natural question to ask under our newly in-troduced assumption Gap  X   X (1): is O ( p  X  / Gap ) the best cut conductance we can obtain from a local al-gorithm? We show that this is true if one sticks to a sweep-cut algorithm using PageRank vectors.
 Algorithm 1 PageRank-Nibble Input: v,  X  and vol 0  X  [ vol( A ) 2 , vol( A )]. Output: set S . 4: Consider all sweep sets S 0 c def = { u  X  supp( p ) : More specifically, we show that our analysis in Sec-tion 4 is tight by constructing the following hard in-stance. Consider a (multi-)graph with two chains (see Figure 1) of vertices, and there are multi-edges con-necting them. 8 In particular:  X  the top chain (ended with vertex a and c and with  X  the bottom chain (ended with vertex d and e ) con- X  vertex b and d are connected with  X  n edges.
 We let the top chain to be our promised cluster A . The total volume of A is 2 n +  X  n , while the total volume of the entire graph is 4 n + 2 X  n . The mixing time for A is  X  mix ( A ) =  X ( ` 2 ), and the cut conductance  X  c ( A ) = (For instance one can let `  X  n 0 . 4 and  X   X  1 n 0 . 9 achieve this requirement.) We then consider a PageRank random walk that starts at vertex v = a and with teleport probability  X  =  X  ` 2 for some arbitrarily small constant  X  &gt; 0. 10 Let pr a this PageRank vector, and we prove in the full version the following lemma: Lemma 5.1. For any  X   X  (0 , 4] and letting  X  =  X /` 2 , there exists some constant c 0 such that when study-ing the PageRank vector pr a starting from vertex a in This lemma implies that, for any sweep-cut algorith-m based on this vector pr a , even if it computes pr exactly and looks for all possible sweep cuts, then none of them gives a better cut conductance than O ( p  X  / Gap ). More specifically, for any sweep set S :  X  if c 6 X  S , then | E ( S,V \ S ) | is at least n  X  if c  X  S , then d must be also in S because it This ends the proof of Theorem 2. The PageRank local clustering method has been s-tudied empirically in various previous work. For in-stance, Gleich and Seshadhri (2012) performed exper-iments on 15 datasets and confirmed that PageRank outperformed many others in terms of cut conduc-tance, including the famous Metis algorithm. More-over, (Leskovec et al., 2009) studied PageRank a-gainst Metis+MQI which is the Metis algorithm plus a flow-based post-processing. Their experiments confirmed that although Metis+MQI outperforms PageRank in terms of cut conductance, however, the PageRank algorithm X  X  outputs are more  X  X ommunity-like X , and they enjoy other desirable properties. Since our PageRank-Nibble is essentially the same PageRank method as before with only theoretical changes in the parameters, it certainly embraces the same empirical behavior as those literatures above. Therefore, in this section we perform experiments only for the sake of demonstrating our theoretical discover-ies in Theorem 1, without comparisons to other meth-ods. We run our algorithm against both synthetic and real datasets, and due to the page limit, we defer the details of our experiment setups to the full version. Recall that Theorem 1 has three properties. The first two properties are accuracy guarantees that ensure the output set S well approximates A in terms of volume; and the third property is a cut-conductance guarantee that ensures the output set S has small  X  c ( S ). We now provide experimental results to support them. In the first experiment, we study a synthetic random graph of 870 vertices. Our desired cluster A is con-structed from the Watts-Strogatz random model with a parameter  X   X  [0 , 1] to control the connectivity of G [ A ]: the larger  X  is the larger Gap is. We there-fore present in Figure 2 our experimental results as two curves, both in terms of  X  : the cut conductance over  X  ratio, i.e.,  X  c ( S )  X  , and the clustering accuracy, i.e., 1  X  | A  X  S | | V | . Our experiment confirms our result in Theorem 1: PageRank-Nibble performs better both in accuracy and cut conductance as Gap goes larger. In the second experiment, we use the USPS zipcode dataset that was also used in the work from (Wu et al., 2012). This dataset has 9298 images of handwritten digits between 0 to 9, and we treat them as 10 separate binary-classification problems. We report our results in Table 1. For each of the 10 binary-classifications, we have a ground-truth cluster A that contains all da-ta points associated with the given digit. We then compare the cut conductance of our output set  X  c ( S ) against the desired cut conductance  X  =  X  c ( A ), and our algorithm consistently outperforms the desired one on all 10 clusters. (Notice that it is possible to see an output set S to have smaller conductance than A , be-cause A is not necessarily the sparest cut in the graph.) In addition, one can also confirm from Table 1 that our algorithm enjoys high precision and recall.
 Acknowledgments. We thank Lorenzo Orecchia, Jon Kelner, Aditya Bhaskara for helpful conversations. Alamgir, Morteza and von Luxburg, Ulrike. Multi-agent random walks for local clustering on graphs. ICDM  X 10, pp. 18 X 27, 2010.
 Alon, Noga. Eigenvalues and expanders. Combinator-ica , 6(2):83 X 96, 1986.
 Alvisi, L., Clement, A., Epasto, A., Lattanzi, S., and
Panconesi, A. The evolution of sybil defense via social networks. In IEEE Symposium on Security and Privacy , 2013.
 Andersen, Reid and Lang, Kevin J. Communities from seed sets. WWW  X 06, pp. 223 X 232, 2006.
 Andersen, Reid and Peres, Yuval. Finding sparse cuts locally using evolving sets. STOC, 2009.
 Andersen, Reid, Chung, Fan, and Lang, Kevin. Using pagerank to locally partition a graph. 2006. An extended abstract appeared in FOCS  X 2006.
 Andersen, Reid, Gleich, David F., and Mirrokni, Va-hab. Overlapping clusters for distributed computa-tion. WSDM  X 12, pp. 273 X 282, 2012.
 Arora, Sanjeev and Kale, Satyen. A combinatorial, primal-dual approach to semidefinite programs. S-TOC  X 07, pp. 227 X 236, 2007.
 Arora, Sanjeev, Rao, Satish, and Vazirani, Umesh V.
Expander flows, geometric embeddings and graph partitioning. Journal of the ACM , 56(2), 2009. Arora, Sanjeev, Hazan, Elad, and Kale, Satyen.
O(sqrt(log(n)) approximation to sparsest cut in  X o(n 2 ) time. SIAM Journal on Computing , 39(5): 1748 X 1771, 2010.
 Chawla, Shuchi, Krauthgamer, Robert, Kumar, Ravi,
Rabani, Yuval, and Sivakumar, D. On the hardness of approximating multicut and sparsest-cut. Com-putational Complexity , 15(2):94 X 114, June 2006. Gargi, Ullas, Lu, Wenjun, Mirrokni, Vahab S., and Y-oon, Sangho. Large-scale community detection on y-outube for topic discovery and exploration. In AAAI Conference on Weblogs and Social Media , 2011.
 Gharan, Shayan Oveis and Trevisan, Luca. Approx-imating the expansion profile and almost optimal local graph clustering. FOCS, pp. 187 X 196, 2012. Gleich, David F. and Seshadhri, C. Vertex neighbor-hoods, low conductance cuts, and good seeds for local community methods. In KDD  X 2012 , 2012.
 Haveliwala, Taher H. Topic-sensitive pagerank. In WWW  X 02 , pp. 517 X 526, 2002.
 Kannan, Ravi, Vempala, Santosh, and Vetta, Adrian.
On clusterings: Good, bad and spectral. Journal of the ACM , 51(3):497 X 515, 2004.
 Leighton, Frank Thomson and Rao, Satish. Multicom-modity max-flow min-cut theorems and their use in designing approximation algorithms. Journal of the ACM , 46(6):787 X 832, 1999.
 Leskovec, Jure, Lang, Kevin J., Dasgupta, Anirban, and Mahoney, Michael W. Community structure in large networks: Natural cluster sizes and the ab-sence of large well-defined clusters. Internet Mathe-matics , 6(1):29 X 123, 2009.
 Leskovec, Jure, Lang, Kevin J., and Mahoney,
Michael. Empirical comparison of algorithms for network community detection. WWW, 2010.
 Lin, Frank and Cohen, William W. Power iteration clustering. In ICML  X 10 , pp. 655 X 662, 2010.
 Lov  X asz, L  X aszl  X o and Simonovits, Mikl  X os. The mixing rate of markov chains, an isoperimetric inequality, and computing the volume. FOCS, 1990.
 Lov  X asz, L  X aszl  X o and Simonovits, Mikl  X os. Random walks in a convex body and an improved volume algorith-m. Random Struct. Algorithms , 4(4):359 X 412, 1993. Makarychev, Konstantin, Makarychev, Yury, and Vi-jayaraghavan, Aravindan. Approximation algo-rithms for semi-random partitioning problems. In STOC  X 12 , pp. 367 X 384, 2012.
 Merca, Mircea. A note on cosine power sums. Journal of Integer Sequences , 15:12.5.3, May 2012.
 Morris, Ben and Peres, Yuval. Evolving sets and mix-ing. STOC  X 03, pp. 279 X 286. ACM, 2003.
 Motwani, Rajeev and Raghavan, Prabhakar. Random-ized algorithms . Cambridge University Press, 1995. Schaeffer, S. E. Graph clustering. Computer Science Review, , 1(1):27 X 64, 2007.
 Shalev-Shwartz, Shai and Srebro, Nathan. SVM op-timization: inverse dependence on training set size. In ICML , 2008.
 Sherman, Jonah. Breaking the multicommodity flow barrier for o ( cut. FOCS  X 09, pp. 363 X 372, 2009.
 Shi, J. and Malik, J. Normalized cuts and image seg-mentation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 22(8):888 X 905, 2000. Sinclair, Alistair and Jerrum, Mark. Approximate counting, uniform generation and rapidly mixing markov chains. Information and Computation , 82 (1):93 X 133, 1989.
 Spielman, Daniel and Teng, Shang-Hua. Nearly-linear time algorithms for graph partitioning, graph spar-sification, and solving linear systems. STOC, 2004. Spielman, Daniel and Teng, Shang-Hua. A local clus-tering algorithm for massive graphs and its applica-tion to nearly-linear time graph partitioning. CoRR , abs/0809.3232, 2008.
 Wu, Xiao-Ming, Li, Zhenguo, So, Anthony Man-Cho,
Wright, John, and Chang, Shih-Fu. Learning with partially absorbing random walks. In NIPS , 2012. Zhu, Zeyuan Allen, Chen, Weizhu, Zhu, Chenguang,
Wang, Gang, Wang, Haixun, and Chen, Zheng. In-verse time dependency in convex regularized learn-
