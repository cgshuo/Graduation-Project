 With the dramatic increase in the amount of textual information available in digital archives, there has been growing interest in Information Extraction (IE), which identifies relevant information (usually of pre-defined types) from text documents in a certain domain and put them in a structured format. 
According to the scope of the ACE program (ACE 2000-2005), IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper will focus on the ACE RDC task, which detects and classifies various semantic relations between two entities. For example, we want to determine whether a person is a citizen of a country, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query  X  X ho is the president of the United States? X , and information retrieval, e.g. to expand the query  X  X eorge W. Bush X  with  X  X he president of the United States X  via his relationships with the country  X  X he United States X . 
One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005). As the arguably largest annotated corpus in relation extraction, the ACE RDC 2003 corpus shows that different relation subtypes are much unevenly distributed and a few subtypes, such as the subtype  X  X ounder X  under the type  X  X OLE X , suffers from a small amount of annotated data. While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector m achines (Zhao et al 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the data sparseness problem. 
This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes. Through organizing various classes hierarchically according to their relatedness, a discriminative function is determined for a given class in a top-down way by capturing the commonality among related classes to guide the training of the given class. Here, support vector machines (SVM) is applied as the basic classifier learning approach. The reason that we choose SVM is that SVM represents the state-of-the-art in the machine learning community and support both feature-based and kernel-based learning. For SVM, the guidance is done by, when training a lower-level class, discounting those negative training instances which do not belong to the support vectors of the upper-level class. By doing so, the data sparseness problem can be well dealt with and much better performance can be achieved, especially for those relations with small or medium amounts of annotated examples. Evaluation on the ACE RDC 2003 and 2004 corpora shows that the hierarchical learning strategy achieves much better performance than the flat learning strategy on least-and medium-frequent relations. It also shows that our system much outperforms the previous best-reported system by addressing the data sparseness issue using the hierarchical learning strategy. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using SVM. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. The relation extraction task was formulated at MUC-7(1998) and extended at ACE (2000-2005). With the increasing popularity of ACE, it is starting to attract more and more researchers within the NLP community. Representative related works can be classified into three categories according to different approaches they used: generative models (Miller et al 2000), tree kernel-based approaches (Zelenko et al 2003; Culotta et al 2004; Bunescu et al 2005; Zha ng et al 2005), and feature-based approaches (Kambhatla 2004; Zhao et al 2005 1 ; Zhou et al 2005). 
Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition and relation extraction. The problem is that such integration using a generative approach may impose big challenges such as the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically apply some smoothing techniques to integrate different scales of contexts in parameter estimation, e.g. the back-off approach in Miller et al (2000). 
Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus 2 . Bunescu et al (2005) proposed a shortest path dependency kernel. They argued that the information to model a relationship between two entities can be typically captured by the shortest path between them in the dependency graph. It achieved the F-measure of 52.5 on the 5 relation types in the ACE RDC 2003 corpus. Zhang et al (2005) adopted a clustering algorithm in unsupervised relation extraction using tree kernels. To overcome the data sparseness problem, various scales of sub-trees are applied in the tree kernel computation. Although tree kernel-based approaches are able to explore the huge implicit feature space without much feature engineering, further research work is necessary to make them effective and efficient. 
Comparably, feature-based approaches achieved much success recently. Kambhatla (2004) employed maximum entropy models with various features derived from word, entity type, mention level, overlap, dependency tree, parse tree and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. Zhao et al (2005) combined various kinds of knowledge from tokenization, sentence parsing and deep dependency analysis through SVM and achieved the F-measure of 70.1 on the 7 relation types of the ACE RDC 2004 corpus 3 . Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through SVM and achieved F-measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively. To overcome the data sparseness problem, feature-based approaches normally incorporate various scales of contexts into the feature vector extensively. These approaches then depend on adopted learning algorithms to weight and combine each feature effectively. For example, an exponential model and a linear model are applied in the maximum entropy models and support vector machines respectively to combine each feature via the learned weight vector. 
As discussed above, although various approaches have been employed in relation extraction, they only apply some implicit approaches to overcome the data sparseness problem. Until now, there are no explicit ways to resolve the data sparseness problem in relation extraction. Currently, all the current approaches apply the flat learning strategy which equally treats training examples in different relations independently and ignore the commonality among different relations. This paper proposes a novel hierarchical learning strategy to resolve this probl em by considering the relatedness among different relations and capturing the commonality among related relations. By doing so, the data sparseness problem can be well dealt with and much better performance can be achieved, especially for those relations with small or medium amounts of annotated examples. Traditional classifier learning approaches apply the flat learning strategy. That is, they equally treat training examples in different classes independently and ignore the commonality among related classes. The flat strategy will not cause any problem when there are a large amount of training examples for each class, since, in this case, a classifier learning approach can always learn a nearly optimal discriminative function for each class against the remaining classes. However, such flat strategy may cause big problems when there is only a small amount of training examples for some (or nearly optimal) discriminative function for a class with a small amount of training examples, and, as a result, may significantly affect the performance of the class and even the overall performance. 
To overcome the inherent problems in the flat learning strategy, this paper proposes a hierarchical learning strategy which explores the inherent commonality among related classes through a class hierarchy. In this way, the training examples of related classes can help in learning a reliable discriminative function for a class with only a small amount of training examples in a top-down way. Here, the state-of-the-art support vector machines (SVM) is applied as the basic classifier learning approach. For SVM, the guidance is done by, when training a lower-level class, discounting those negative training instances which do not belong to the support vectors of the upper-level class. 
In the following, we will first describe SVM, followed by the hierarchical learning strategy using SVM. Finally, we will consider several ways in building the class hierarchy. 3.1 Support Vector Machines Support Vector Machines (SVM) is a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998). Based on the structural risk minimization of the statistical learning theory, SVM seeks an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set. 
Basically, SVM is only for binary classification. Therefore, we must extend it to multi-class classification, such as the ACE RDC task. For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. Moreover, we map the SVM output into the probability by using an additional sigmoid model: model trust alorithm as described in Platt (1999). The final decision of an instance in multi-class classification is determined by the class which has the maximal probability from the corresponding SVM. 
The reason why we choose SVM is that SVM represents the state-of X  X he-art in the machine learning community and supports both feature-based learning and kernel-based learning. Moreover, there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight 4 deleveloped by Joachims (1998). By default, the simple linear kernel is applied unless specified. 3.2 Hierarchical Learning Strategy Using Support Vector Machines Assume we have a class hierarchy for a task, e.g. the one in the ACE RDC 2003 corpus as shown in Table 1 of Section 4.1. The hierarchical learning strategy explores the inherent commonality among related classes in a top-down way. For SVM, the guidance of related classes in training a discriminative function for a given class is done by discounting those negative training instances which do not belong to the support vectors of the upper-level class. Th at is, the lower-level discriminative function has the preference toward the discriminative function of its upper-level class. Here, all the positive relation examples (i.e. those under the  X  X ES X  relation) are always kept since their number is always much less that of the negative relation examples (i.e. those under the  X  X ON X  relation) in most classification tasks, such as the ACE RDC task using the ACE RDC 2003 and 2004 corpora. 
For an example, let X  X  look at the training of the  X  X pouse X  relation subtype in the class hierarchy as shown in Table 1:  X  Train a SVM for the  X  X ES X  relation vs. the  X  X ON X  relation.  X  Train a SVM for the  X  X OCIAL X  relation type vs. all the remaining relation types  X  Train a SVM for the  X  X pouse X  relation subtype vs. all the remaining relation  X  Return the above trained SVM as the classifier for the  X  X pouse X  relation 
Please note: considering the argument order of the two mentions, one more step is necessary to differentiate the two different argument orders for a non-symmetric relation subtype. For this issue, please see Section 4.1 for more details. 
In the case of using SVM as the basic classifier learning approach, the support vectors of a given class are much biased to the ones of the upper class which covers the given class and its related classes du e to discounting negative relation examples which do not belong to the support vectors of the upper level SVM classifier. In some sense, related classes help by discounting the effect of un-related classes when determining the support vectors for a given class. As a result, this can largely reduce the noisy effect. Moreover, this can largely reduce the search space in training and make SVM training converge much faster. Our experimentation shows that similar performance is achieved when the discounting weight ranges from 0.1 to 0.3. Therefore, the discounting weight is set to 0.2 throughout the paper. That is, the cost of a discounted training example is multiplied by 0.2. 
In this way, the training examples in different classes are not treated independently any more, and the commonality among related classes can be captured via the hierarchical learning strategy. The intuition behind this strategy is that the upper-level class normally has more positive training examples than the lower-level class so that the corresponding discriminative function can be determined more reliably. In this discriminative function for a class with only a small amount of training examples in a top-down way and thus alleviate its data sparseness problem. 3.3 Building the Class Hierarchy We have just described the hierarchical learning strategy using a given class hierarchy. Normally, a rough class hierarchy can be given manually according to human intuition, such as the one in the ACE RDC 2003 corpus. In order to explore more commonality among sibling classes, we make use of binary hierarchical using the flat learning strategy to learn th e discriminative functions for individual classes and then iteratively combining the two most related classes using the cosine similarity function between their discriminative functions, e.g. using the support vector sets in SVM, in a bottom-up way. The intuition is that related classes should have similar hyper-planes to separate from othe r classes and thus have similar sets of support vectors in SVM.  X  Lowest-level hybrid: Binary hierarchical clustering is only done at the lowest  X  All-level hybrid: Binary hierarchical clustering is done at all levels in a bottom-This paper focuses on the ACE RDC task with evaluation on the ACE RDC 2003 and 2004 corpora. 4.1 Experimental Setting Evaluation is mainly done on the ACE RDC 2003 corpus. The training data consists of 674 documents (~300k words) with 9683 relation examples while the held-out testing data consists of 97 documents (~50k words) with 1386 relation examples. Table 1 lists various types and subtypes of relations for the corpus, along with their occurrence frequencies in the training data. It shows that this corpus suffers from a small amount of annotated data for a few subtypes such as the subtype  X  X ounder X  under the type  X  X OLE X . Here, we also divide various relation subtypes into three bins: large/middle/small, according to their training data sizes. For this corpus, 400 is used as as the lower threshold for the large bin and 200 as the upper threshold for the small bin 5 . As a result, the large/medium/small bin includes 5/8/11 relation subtypes in the corpus, respectively. SOCIAL Associate 91 Small 
Detailed evaluation has been also done on the ACE RDC 2004 corpus, which contains 451 documents and 5702 relation instances. For comparison with Zhao et al (2005), we only use the same 348 nwire and bnews documents, which contain 125k words and 4400 relation instances. Evaluation is done using 5-fold cross-validation and shows similar tendency with the ACE RDC 2003 corpus. To avoid redundancy, we will only report the final performance on the ACE RDC 2004 corpus. 
In this paper, we adopt the same feature set as applied in Zhou et al (2005): word, entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic information. By default, we use the SVM with the simple linear kernel unless specified. Moreover, we also explicitly model the argument order of the two mentions involved. F or example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2-ROLE.Citizen-Of-m1. Note that, in the ACE RDC 2003 task, 6 of th ese 24 relation subtypes are symmetric:  X  X EAR.Relative-Location X ,  X  X OCIAL.Associate X ,  X  X OCIAL.Other-Relative X ,  X  X OCIAL.Other-Professional X ,  X  X OCIAL.Sibling X , and  X  X OCIAL.Spouse X . In this way, we model relation extraction in the ACE RDC 2003 task as a multi-class classification task with 43 (24X2-6+1) classes, two for each relation subtype (except the above 6 symmetric subtypes) and a  X  X ONE X  class for the case where the two mentions are not related. For the ACE RDC 2004 task, 6 of these 23 relation subtypes are symmetric:  X  X HYS.Near X ,  X  X ER-SOC.Business X ,  X  X ER-SOC.Family X ,  X  X ER-SOC.Other X ,  X  X MP-ORG.Partner X , and  X  X MP-ORG.Other X . In this way, we model relation extraction in the ACE RDC 2004 task as a multi-class classification task with 41 (23X2-6+1) classes, two for each relation subtype (except the above 6 symmetric subtypes) and a  X  X ONE X  class for the case where the two mentions are not related. 4.2 Experimental Results Table 2 compares the hierarchical learning strategy and the flat learning strategy with the existing class hierarchy on the ACE RDC 2003 corpus using SVM. It shows that the hierarchical strategy outperforms the flat strategy by 1.9 (57.4 vs. 55.5) in F-measure, largely due to its gain in recall. As a result, the hierarchical strategy achieves the F-measure of 57.4 using SVM. Flat 63.1 49.5 55.5 Class Hierarchy P% R% F Existing 63.7 52.3 57.4 Entirely Automatic 63.5 52.5 57.4 Lowest-level Hybrid (based on the existing one) 64.1 52.8 57.9 All-level Hybrid (based on the existing one) 64.3 52.9 58.0 
Table 3 compares the performance of the hierarchical learning strategy using different class hierarchies on the ACE RDC 2003 corpus. It shows that, the lowest-level hybrid approach, which only automatically updates the existing class hierarchy at the lowest level, slightly improves the performance by 0.5 (57.9 vs. 57.4) in F-measure while further updating the class hierarchy at upper levels in the all-level hybrid approach only has very slight effect (58.0 vs. 57.9) in F-measure. This is largely due to the fact that the major data sparseness problem occurs at the lowest level, i.e. the relation subtype level in the ACE RDC 2003 corpus. As a result, the hierarchical learning strategy using the class hierarchy built with the all-level hybrid approach achieves the F-measure of 58.0 using SVM, which outperforms the flat learning strategy by 2.5 (58.0 vs. 55.5) in F-measure. In order to justify the usefulness of our hierarchical learning strategy when a rough class hierarchy is not available and difficult to determine manually, we also experiment using entirely automatically built class hierarchy without considering the existing class hierarchy. Table 3 shows that using automatically built class hierarchy performs comparably (63.7%/52.3%/57.4 vs. 63.5%/52.5%/57.4) with using only the existing one. (Similarity) P% R% F P% R% F P% R% F Flat 68.6 57.3 62.4 67.8 34.7 45.9 35.0 24.7 29.0 
With the major goal of resolving the data sparseness problem for the classes with a small amount of training examples, Table 4 compares the hierarchical and flat learning strategies on the relation subtypes of different training data sizes using SVM on the ACE RDC 2003 corpus. Here, we divide various relation subtypes into three bins: large/middle/small, according to their available training data sizes. Please see Table 1 for details. Table 4 shows that th e hierarchical learning strategy outperforms the flat learning strategy by 1.0(63.4 vs. 62.4)/5.9(51.8 vs. 45.9)/6.9(35.9 vs. 29.0) in F-measure on the large/middle/small bin respectively. This indicates that the hierarchical learning strategy performs much better than the flat learning strategy for those classes with a small or medium amount of annotated examples although the hierarchical learning strategy only performs slightly better by 1.0 than the flat learning strategy on those classes with a large size of annotated corpus. This suggests that the proposed hierarchical learning strategy can well deal with the data sparseness problem in the ACE RDC 2003 corpus using SVM. 
An interesting question is about the similarity between the discriminative functions learned using the hierarchical and flat learning strategies. Table 4 compares the similarities between the weighted support vector sets of the discriminative functions for the two strategies and different bins. Since different relation subtypes contain quite different numbers of training examples, the similarities are weighted by the training example numbers of different rel ation subtypes. Table 4 shows that the discriminative functions learned using the two strategies are very similar (with the similarity about 0.95) for the relation subtypes belonging to the large bin, while there exist quite differences between the discriminative functions learned using the two strategies for the relation subtypes belonging to the medium/small bin with the similarity about 0.90/0.76 respectively. This means that the use of the hierarchical learning strategy over the flat learning st rategy only has very slight change on the discriminative functions for those classes with a large amount of annotated examples while its effect on those with a small amount of annotated examples is obvious. This contributes to and explains (the degree of) the performance difference between the two strategies on the different training data sizes as shown in Table 4. 
Due to the difficulty of building a lar ge annotated corpus, another interesting question is about the adaptability of the hierarchical learning strategy and its comparison with the flat learning strategy. Figure 1 shows the effect of different training data sizes for some major relation subtypes while keeping all the training examples of remaining relation subtypes on the ACE RDC 2003 corpus using SVM. It shows that the hierarchical learning strategy performs much better than the flat learning strategy when only a small amount of training examples is available. It also shows that the hierarchical learning strategy can achieve stable performance much faster than the flat learning strategy. Finally, it shows that the ACE RDC 2003 corpus suffer from the lack of training examples. Among the three major relation subtypes, only the subtype  X  X ocated X  achieves steady performance. 
Finally, we also compare our system with the previous best-reported systems, such as Zhou et al (2005) and Zhao et al (2005) on the ACE RDC 2003 and 2004 corpora respectively. For the ACE RDC 2003 corpus, Table 6 shows that our system with the hierarchical learning strategy outperforms th e previous best-reported system by 2.5 (58.0 vs. 55.5) in F-measure using SVM with linear kernel, in the extraction of the 24 relation subtypes. Moreover, we also evaluate our system using the polynomial kernel with degree d=2 in SVM. It shows that this can further improve the F-measure by 2.2 (60.2 vs. 58.0) in the extraction of 24 relation subtypes. As a result, our system significantly outperforms Zhou et al (2005) by 4.7 (60.2 vs. 55.5) in F-measure on the 24 relation subtypes. For the ACE RDC 2004 corpus, Table 6 shows that our system with the hierarchical learning strategy outperforms the previous best-reported system by 0.5 (70.8 vs. 70.3) in F-measure using SVM with linear kernel on the 7 relation types. Moreover, we also evaluate our system using the polynomial kernel with degree d=2 in SVM. It shows that this can further improve the F-measure by 2.1 (64.9 vs. 62.8) on the 23 relation subtypes. As a result, our system significantly outperforms Zhao et al (2005) by 2.3 (72.6 vs. 70.3) in F-measure on the 7 relation types. 
Ours: Hierarchical+SVM (linear) 
Ours: Hierarchical+SVM (polynomial) Zhou et al(2005): 
Flat+SVM(linear) 
Zhao et al(2005):Flat+SVM (composite polynomial) This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes. For each class in a class hierarchy, a discriminative function is determined in a top-down way. In this way, the upper-level discriminative function can effectively guide the lower-level discriminative function learning. In this paper, the state-of-the-art Support Vector Machines is applied as the basic classifier learning approach using the hierarchical learning strategy and the guidance is done by, when training a lower-level class, discounting those negative training instances which do not belong to the support vectors of the upper-level class. Evaluation on the ACE RDC 2003 and 2004 corpora shows that the hierarchical learning strategy performs much better than the flat learning strategy in resolving the critical data sparseness problem in relation extraction. 
In the future work, we will explore the hierarchical learning strategy using other guidance principle. Moreover, just as indicated in Figure 1, most relation subtypes in the ACE RDC 2003 corpus (arguably the la rgest annotated corpus in relation extraction) suffer from the lack of training examples. Therefore, a critical research in relation extraction is how to rely on semi-supervised learning approaches (e.g. bootstrap) to alleviate its dependency on a large amount of annotated training examples and achieve better and steadier performance. 
