 Mining useful patterns in sequential data is a challenging task. Many studies have been proposed for mining interesting patte rns in sequence databases [9]. Sequen-tial pattern mining is probably the most popular research topic among them. A subsequence is called sequential pattern or frequent sequence if it frequently appears in a sequence database, and its f requency is no less than a user-specified minimum support threshold minsup [1]. Sequential pattern mining plays an im-portant role in data mining and is essential to a wide range of applications such as the analysis of web click-streams, program executions, medical data, biologi-cal data and e-learning data [9]. Several efficient algorithms have been proposed for sequential pattern mining such as ClaSP [7], CloSpan [12], GSP [11], PrefixS-pan [10], SPADE [13] and SPAM [3]. Sequential pattern mining algorithms can be categorized as using a horizontal database format (e.g. CloSpan, GSP and PrefixSpan) or a vertical database format (e.g.ClaSP,SPADE,SPAM).Using the vertical format provides the advantage of generating patterns and calculat-ing their supports without performing costly database scans [3,7,13]. This allows vertical algorithms to perform better on datasets having dense or long sequences than algorithms using the horizontal format, and to have excellent overall perfor-mance [2,3,7]. However, a crucial perform ance bottleneck of vertical algorithms is that they use a generate-candidate-and-test approach, which can generate a large amount of patterns that do not appear in the input database or are in-frequent. An important research questions that arises is: Could we design an effective candidate pruning method for vertical mining algorithms to improve mining performance? Answering this question is challenging. It requires design-ing a candidate pruning mechanism (1) that is effective at pruning candidates and (2) that has a small runtime and memory cost. Moreover, the mechanism should preferably be generic. i.e. applicable to any vertical mining algorithms.
In this paper, we present a solution to this issue based on the study of item co-occurrences. Our contribution is threef old. First, to store item co-occurrence information, we introduce a new data structure named Co-occurrence MAP (CMAP). CMAP is a small and compact structure, which can be built with a single database scan.

Second, we propose a generic candidate pruning mechanism for vertical se-quential pattern mining algorithms based on the CMAP data structure. We describe how the pruning mechanism is integrated in three state-of-the-art algo-rithms ClaSP, SPADE and SPAM. We name the resulting algorithms CM-ClaSP, CM-SPADE and CM-SPAM.

Third, we perform a wide experimental evaluation on six real-life datasets. We compare the performance of CM-ClaSP, CM-SPADE and CM-SPAM with state-of-the-art algorithms for mining sequential patterns (GSP, PrefixSpan, SPADE and SPAM) and closed sequential patterns (ClaSP and CloSpan). Results show that the modified algorithms (1) prune a large amount of candidates, (2) and are up to eight times faster than the corresponding original algorithms and (3) that CM-ClaSP and CM-SPADE have respectively the best performance for sequential pattern mining and closed sequential pattern mining.

The rest of the paper is organized as follows. Section 2 defines the problem of sequential pattern mining and reviews th e main characteristics of ClaSP, SPADE and SPAM. Section 3 describes the CMAP structure, the pruning mechanism, and how it is integrated in ClaSP, SPADE and SPAM. Section 4 presents the experimental study. Finally, Sect ion 5 presents the conclusion. Definition 1 (sequence database). Let I = { i 1 ,i 2 , ..., i l } be a set of items (symbols). An itemset I x = { i 1 ,i 2 , ..., i m } X  I is an unordered set of distinct items. The lexicographical order lex is defined as any total order on I . Without loss of generality, it is assumed in the following that all itemsets are ordered according to lex .A sequence is an ordered list of itemsets s = I 1 ,I 2 , ..., I n such that I k  X  I (1  X  k  X  n ). A sequence database SDB is a list of sequences SDB = s 1 ,s 2 , ..., s p having sequence identifiers (SIDs) 1 , 2 ...p . Example. A sequence database is shown in Fig. 1 (l eft). It contains four sequences having the SIDs 1, 2, 3 and 4. Each single letter represents an item. Items between { e } contains five itemsets. It indicates that items a and b occurred at the same time, were followed by c ,then f,g and lastly e .
 Definition 2 (sequence containment). A sequence s a = A 1 ,A 2 , ..., A n is said to be contained in a sequence s b = B 1 ,B 2 , ..., B m iff there exist integers 1  X  i as s a s b ). Example. Sequence 4 in Fig. 1 (left) is contained in Sequence 1. Definition 3 (prefix). A sequence s a = A 1 ,A 2 , ..., A n is a prefix of a se-and the first | A n | items of B n according to lex are equal to A n . Definition 4 (support). The support of a sequence s a in a sequence database SDB is defined as the number of sequences s  X  SDB such that s a s and is denoted by sup SDB ( s a ).
 Definition 5 (sequential pattern mining). Let minsup be a threshold set by the user and SDB be a sequence database. A sequence s is a sequential pattern and is deemed frequent iff sup SDB ( s )  X  minsup .The problem of mining sequential patterns is to discover all sequential patterns [11]. Example. Fig. 1 (right) shows 6 of the 29 sequential patterns found in the database of Fig. 1 (left) for minsup =2.
 Definition 6 (closed sequential pattern mining). A sequential pattern s a is said to be closed if there is no other sequential pattern s b , such that s b is a superpattern of s a , s a s b , and their supports are equal. The problem of closed sequential patterns is to discover the set of closed sequential patterns, which is a compact summarization of all sequential patterns [7,12].
 Definition 7 (horizontal database format). A sequence database in hori-zontal format is a database where each entry is a sequence. Example. Fig. 1 (left) shows an horizontal sequence database.
 Definition 8 (vertical database format). A sequence database in vertical format is a database where each entry represents an item and indicates the list of sequences where the item appears and th e position(s) where it appears [13]. Example. Fig. 2 shows the vertical representation of the database of Fig. 1 (left).
 sociated with each pattern. IdLists allow calculating the support of a pattern quickly by making join operations with IdLists of smaller patterns. To discover sequential patterns using IdLists, a single database scan is required to create IdLists of patterns containing a single items, since IdList of larger patterns are obtained by performing the aforementioned join operation (see [13] for details). Several works proposed alternative representations for IdLists to save time in join operations, being the bitset repre sentation the most efficient one [3,2].
The horizontal format is used by Apriori-based algorithms (e.g. GSP) and pattern-growth algorithms (e.g. CloSpan and PrefixSpan). The two main algo-rithms using the vertical database format are SPADE and SPAM. Other algo-rithms are variations such as bitSPADE [2] and ClaSP [7]. SPADE and SPAM differ mainly by their candidate generation process, which we review thereafter. Candidate Generation in SPAM. The pseudocode of SPAM is shown in Fig. 3. SPAM take as input a sequence database SDB and the minsup thresh-old. SPAM first scans the input database SDB once to construct the vertical representation of the database V ( SDB ) and the set of frequent items F 1 .For each frequent item s  X  F 1 , SPAM calls the SEARCH procedure with s , F 1 , { e  X  F 1 | e lex s } ,and minsup . The SEARCH procedure outputs the pattern { s } and recursively explore candidate patterns starting with the prefix { s } . The SEARCH procedure takes as parameters a sequential pattern pat and two sets of items to be appended to pat to generate candidates. The first set S n repre-sents items to be appended to pat by s -extension. The s -extension of a sequential set S i represents items to be appended to pat by i -extension. The i -extension of For each candidate pat generated by an extension, SPAM calculate its support to determine if it is frequent. This is done by making a join operation (see [3] for details) and counting the number of sequences where the pattern appears. The IdList representation used by SPAM is based on bitmaps to get faster operations [3]. If the pattern pat is frequent, it is then used in a recursive call to SEARCH to generate patterns starting with the prefix pat . Note that in the recursive call, only items that resulted in a fr equent pattern by extension of pat are considered for extending pat . SPAM prunes the search space by not extending infrequent patterns. This can be done due to the property that an infrequent sequential pattern cannot be extended to form a frequent pattern [1].
 Candidate Generation in SPADE. The pseudocode of SPADE is shown in Fig. 4. The SPADE procedure takes as input a sequence database SDB and the minsup threshold. SPADE first constructs the vertical database V ( SDB )and identifies the set of frequent sequential patterns F 1 containing frequent items. Then, SPADE calls the ENUMERATE procedure with the equivalence class of size 0. An equivalence class of size k is defined as the set of all frequent patterns containing k items sharing the same prefix of k  X  1items.Thereis only an equivalence class of size 0 and it is composed of F 1 . The ENUMERATE procedure receives an equivalence class F as parameter. Each member A i of the equivalence class is output as a frequent sequential pattern. Then, a set T i , representing the equivalence class of all frequent extensions of A i is initialized to the empty set. Then, for each pattern A j  X  F such that i lex j , the pattern A i is merged with A j to form larger pattern(s). For each such pattern r , the support of r is calculated by performing a join operation between IdLists of A i and A j . If the cardinality of the resulting IdList is no less than minsup ,itmeansthat r is a frequent sequential pattern. It is thus added to T i .Finally,afterallpattern A j have been compared with A i ,theset T i contains the whole equivalence class of patterns starting with the prefix A i . The procedure ENUMERATE is then called with T i to discover larger sequential patterns having A i as prefix. When all loops terminate, all frequent seque ntial patterns have been output (see [13] for the proof that this procedure is correct and complete).

SPADE and SPAM are very efficient for datasets having dense or long sequences and have excellent overall performance since performing join operations to cal-culate the support of candidates does not require scanning the original database unlike algorithms using the horizontal format. For example, the well-known Pre-fixSpan algorithm, which uses the horizontal format, performs a database projec-tion for each item of each frequent sequential pattern, in the worst case, which is extremely costly. The main performance bo ttleneck of vertical mining algorithms is that they use a generate-candidate-and-test approach and therefore spend lot of time evaluating patterns that do not appear in the input database or are in-frequent. In the next section, we present a novel method based on the study of item co-occurrence information to prune candidates generated by vertical mining algorithms to increase their performance.
 In this section, we introduce our approach, consisting of a data structure for storing co-occurrence information, and its properties for candidate pruning for vertical sequential pattern mining. Then, we describe how the data structure is integrated in three state-of-the-art vertical mining algorithms, namely ClaSP, SPADE and SPAM. 3.1 The Co-occurrence Map Definition 9. An item k is said to succeed by i-extension to an item j in a sequence I 1 ,I 2 , ..., I n iff j, k  X  I x for an integer x such that 1  X  x  X  n and Definition 10. An item k is said to succeed by s-extension to an item j in a sequence I 1 ,I 2 , ..., I n iff j  X  I v and k  X  I w for some integers v and w such that 1  X  v&lt;w  X  n .
 Definition 11. A Co-occurrence MAP (CMAP) is a structure mapping each item k  X  I to a set of items succeeding it. We define two CMAPs named CMAP i and CMAP s . CMAP i maps each item k to the set cm i ( k ) of all items j  X  I succeeding k by i-extension in no less than minsup sequences of SDB . CMAP s maps each item k to the set cm s ( k ) of all items j  X  I succeedings k by s-extension in no less than minsup sequences of SDB . Example. The CMAP structures built for the sequence database of Fig. 1(left) are shown in Table 1, being CMAP i on the left part and CMAP s on the right part. Both tables have been created considering a minsup of two sequences. For instance, for the item f , we can see that it is associated with an item, cm i ( f )= { g } ,in CMAP i ,whereas it is associated with two items, cm s ( f )= { e, g } ,in CMAP s . This indicates that both items e and g succeed to f by s-extension and only item g does the same for i-extension, being all of them in no less than minsup sequences. Size Optimization. Let n = | I | be the number of items in SDB . To implement aCMAP,asimplesolutionistousean n  X  n matrix (two-dimensional array) M where each row (column) correspond to a distinct item and such that each entry m item i by i-extension or s-extension. The size of a CMAP would then be O ( n 2 ). However, the size of CMAP can be reduced using the following strategy. It can be observed that each item is succeeded by only a small subset of all items for most datasets. Thus, few items succeed anothe r one by extension, and thus, a CMAP may potentially waste large amount of memory for empty entries if we consider them by means of a n  X  n matrix. For this reason, in our implementations we instead implemented each CMAP as a hash table of hash sets, where an hashset corresponding to an item k only contains the items that succeed to k in at least minsup sequences.
 3.2 Co-occurrence-Based Pruning The CMAP structure can be used for pruning candidates generated by vertical sequential pattern mining algorithms based on the following properties. Property 1 (pruning an i-extension). Let be a frequent sequential pattern A andanitem k . If there exists an item j in the last itemset of A such that k belongs to cm i ( j ), then the i-extension of A with k is infrequent. Proof. If an item k does not appear in cm i ( j ), then k succeed to j by i-extension in less than minsup sequences in the database SDB . It is thus clear that appending k by i-extension to a pattern A containing j in its last itemset will not result in a frequent pattern.
 Property 2 (pruning an s-extension). Let be a frequent sequential pattern A and an item k . If there exists an item j  X  A such that the item k belongs to cm s ( j ), then the s-extension of A with k is infrequent. Proof. If an item k does not appear in cm s ( j ), then k succeeds to j by s-extension in less than minsup sequences from the sequence database SDB . It is thus clear that appending j by s-extension to a pattern A containing k will not result in a frequent pattern.
 Property 3 (pruning a prefix). The previous properties can be generalized to prune all patterns starting with a given prefix. Let be a frequent sequential pattern A andanitem k . If there exists an item j  X  A (equivalently j in the last all supersequences B having A as prefix and where k succeeds j by s-extension (equivalently i-extension to the last itemset) in A in B are infrequent. Proof. If an item k does not appear in cm s ( j ) (equivalently cm i ( j )), therefore k succeeds to j in less than minsup sequences by s-extension (eq uivalently i-extension to the last itemset) in the database SDB . It is thus clear that no frequent pattern containing j (equivalently j in the last itemset) can be formed such that k is appended by s-extension (equivalently by i-extension to the last itemset). 3.3 Integrating Co-occurrence Pruning in Vertical Mining Integration in SPADE. The integration in SPADE is done as follows. In the ENUMERATE procedure, consider a pattern r obtained by merging two patterns A i = P the item that is appended to A i to generate r .If r is an i-extension, we use the CMAP i structure, otherwise, if r is an s-extension, we use CMAP s .Ifthelast item a of r does not have an item x  X  cm i ( a ) (equivalently in cm s ( a )), then the pattern r is infrequent and r can be immediately discarded, avoiding the join operation to calculate the support of r . This pruning strategy is correct based on Properties 1, 2 and 3.

Note that to perform the pruning in SPADE, we do not have to check if items of the prefix P are succeeded by the item y  X  A j . This is because the items of P are also in A j . Therefore, checking the extension of P by y was already done, and it is not necessary to do it again. Integration in SPAM. The CMAP structures are used in the SEARCH pro-cedure as follows. Let a sequential pattern pat being considered for s -extension ( x  X  S n )or i -extension ( x  X  S i ) with an item x (line 3). If the last item a in pat does not have an item x  X  cm s ( a ) (equivalently cm i ), then the pattern resulting from the extension of pat with x will be infrequent and thus the join operation of x with pat to count the support of the resulting pattern does not need to be performed (by Property 1 and 2). Furthermore, the item x should not be considered for generating any pattern by s -extension ( i -extension) having pat as prefix (by Property 3). Therefore x should not be added to the variable S temp ( I temp ) that is passed to the recursive call to the SEARCH procedure.
Note that to perform the pruning in SPAM, we do not have to check for extensions of pat with x for all the items since such items, except for the last one, have already been checked for extension in previous steps.
 IntegrationinClaSP. We have also integrated co-occurrence pruning in ClaSP [7], a state of the art algorithm for closed sequential pattern mining. The integration in ClaSP is not described here since it is done as in SPAM since ClaSP is based on SPAM. We performed experiments to assess the performance of the proposed algorithms. Experiments were performed on a computer with a third generation Core i5 pro-cessor running Windows 7 and 5 GB of free RAM. We compared the performance of the modified algorithms (CM-ClaSP, CM-SPADE, CM-SPAM) with state-of-the-art algorithms for sequential pattern mining (GSP, PrefixSpan, SPADE and SPAM) and closed sequential pattern mining (ClaSP and CloSpan). All algo-rithms were implemented in Java. Note that for SPADE algorithms, we use the version proposed in [2] that implement IdLists by means of bitmaps. All mem-ory measurements were done using the J ava API. Experiments were carried on six real-life datasets having varied chara cteristics and representing four differ-ent types of data (web click stream, text from books, sign language utterances and protein sequences). Those datasets are Leviathan , Sign , Snake , FIFA , BMS and Kosarak10k . Table 2 summarizes their chara cteristics. The source code of all algorithms and datasets used in our experiments can be downloaded from http://goo.gl/hDtdt .

The experiments consisted of running all the algorithms on each dataset while decreasing the minsup threshold until algorithms became too long to execute, ran out of memory or a clear winner was observed. For each dataset, we recorded the execution time, the percentage of candidate pruned by the proposed algo-rithms and the total size of CMAPs. The comparison of execution times is shown in Fig. 5. The percentage of candidates pruned by the proposed algorithms is shown in Table 3.
 Effectiveness of Candidate Pruning. CM-ClaSP, CM-SPADE and CM-SPAM are generally from about 2 to 8 times faster than the corresponding original algorithms (ClaSP, SPADE and SPAM). This shows that co-occurrence pruning is an effective technique to improve the performance of vertical mining algorithms. The dataset where the performance of the modified algorithms is closer to the original algorithms is Sna ke because all items co-occurs with each item in almost all sequences and therefore fewer candidates could be pruned. For other datasets, the percentage of candidates pruned range from 50% and to 98 %). The percentage slowly decrease as minsup get lower because less pairs in CMAP had a count lower than minsup for pruning.
 Best Performance. For mining sequential patterns, CM-SPADE had the best performance on all but two datasets (Kosarak and BMS). The second best algo-rithm for mining sequential patterns is CM-SPAM (best performance on BMS and Kosarak). For mining closed sequential patterns, CM-ClaSP has the best performance on four datasets (Kosarak, BMS, Snake and Leviathan). CM-ClaSP is only outperformed by CloSpan on two datasets (FIFA and SIGN) and for low minsup values.
 Memory Overhead. We also studied the memory overhead of using CMAPs. We measured the total memory used by a matrix implementation and a hashmap implementation of CMAPs (cf. section 3.1) for all datasets for the lowest minsup values from the previous experiments. Results are shown in Table 4. Size is measured in terms of memory usage and number of entries in CMAPs. From these results, we conclude that (1) the matrix implementation is smaller for datasets with a small number of distinct items (Snake and SIGN), while (2) the hashmap implementation is smaller for datasets with a large number of items (BMS, Leviathan, Kosarak and FIFA) and (3) the hashmap implementation has a very low memory overhead (less than 35 MB on all datasets). Sequential pattern mining algorithms using the vertical format are very efficient because they can calculate the support of candidate patterns by avoiding costly database scans. However, the main perfo rmance bottleneck of vertical mining algorithms is that they usually spend lot of time evaluating candidates that do not appear in the input database or are infrequent. To address this problem, we presented a novel data structure named CMAP for storing co-occurrence information. We have explained how CMAPs can be used for pruning candi-dates generated by vertical mining algorithms. We have shown how to integrate CMAPs in three state-of-the-art vertical algorithms. We have performed an ex-tensive experimental study on six real-life datasets to compare the performance of the modified algorithms (CM-ClaSP, CM-SPADE and CM-SAPM) with state-of-the-art algorithms (ClaSP, CloSpan, GSP, PrefixSpan, SPADE and SPAM). Results show that the modified algorithms (1) prune a large amount of candi-dates, (2) are up to 8 times faster than the corresponding original algorithms and (3) that CM-SPADE and CM-ClaSP have respectively the best performance for mining sequential patterns and closed sequential patterns.

The source code of all algorithms and datasets used in our experiments can be downloaded from http://goo.gl/hDtdt .

For future work, we plan to develop additional optimizations and also to integrate them in sequential rule mining [5], top-k sequential pattern mining [4] and maximal sequential pattern mining [6].
 Acknowledgement. This work is partially financed by a National Science and Engineering Research Council (NSERC ) of Canada research grant, a PhD grant from the Seneca Foundation (Regional Agency for Science and Technology of the Region de Murcia), and by the Spa nish Office for Science and Innovation through project TIN2009-14372-C03-01 and PlanE, and the European Union by means of the European Regional Development Fund (ERDF, FEDER).

