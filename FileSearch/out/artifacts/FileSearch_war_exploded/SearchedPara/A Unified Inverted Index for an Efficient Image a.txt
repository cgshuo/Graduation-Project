 We present an efficient method for approximate search in a combination of several metric spaces  X  which are a general-ization of low level image features  X  using an inverted index. Our approximation gives very high recall with subsecond response time on a real data set of one million images ex-tracted from Flickr. We further exploit the inverted index to improve efficiency of the query processing by combining our search in metric features with search in associated textual metadata.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Systems]: Information Search and Retrieval General Terms: Algorithms, Experiments Keywords: Multimedia, IR, Approximation, Inverted In-dex, Metric Spaces
With the recent proliferation of Web 2.0 applications, we are seeing a new trend in multimedia production where users are becoming mass producers of audio-visual content. This calls for a solution to index and search this data.
Search in audio-visual content can be generalized to a search in metric spaces [3, 1] by assuming some distance function on low-level features such as Color and Textures for image search. State-of-the-art solutions for search in a single metric space already reveal linear scalability in the collection size [2] due to the cost and number of distance computations. Combining several low level features makes this query processing even more expensive.

As a result, search in large collections is generally limited to associated text metadata. However, it is widely accepted that a system that can search by several low-level features, combined with text, is expected to yield best results.
We present the PCA -Pivots Crossing Approximation model, an approximate similarity search, designed for search over a combination of metric spaces. The proposed approx-imation uses a fixed number of distance computations per query and thus can be quite efficient. We show an efficient implementation using inverted index with a dynamic  X  X er
This work was partially funded by the European Commis-sion FP6 project SAPIR. We also like to thank CNR for the CoPhir collection.
 query X  aggregate function to combine the different metric spaces.

We further show a novel approach to add text as a filter over the search in metric spaces: by using IR methods with Boolean constraints, we can process text and image queries at a same time, solving the efficiency problem mentioned above while improving the effectiveness of the image search. We present a possible implementation and some experiments on a real dataset of one million images annotated with text.
This section describes our approach for approximate con-tent based search in collections defined over several Metric Spaces. We remind, that a Metric Space is an ordered pair ( S, d ), where S is a domain and d is a distance function d : S  X  S  X  R between any two objects in S .

Assume C  X  S , a collection of objects over n metric spaces. For each metric space ( S l ,d l ) ,l  X  X  1 ,...n lect P pivots: piv j ( l )  X  S l , j  X  X  1 ,...P } . Given an object o =( o 1 ,...,o n ) ,o l  X  S l , we select for each o l ,the p&lt;P pivots with the minimum distance d l ( o l ,piv j ( l )) among all piv j ( l ). Typically, we choose p&lt;&lt;P .These n  X  P pivots can be selected randomly from the collection or by a clustering algorithm. We denote by P j ( l ) the set of all objects that were mapped to piv j ( l ).

Given a query object q =( q 1 ,...q n ) ,q l  X  S l , we similarly map each q l to the p closest pivots among all piv j ( l )andwe denote their corresponding sets by P q j ( l ). We define C as the set of all objects that were mapped to one or more of the closest pivots to the query q . They are, thus, candidates to be the objects that are close to the query object.
Figure 1 shows an example of two metric spaces ( n =2), seven objects ( o 1  X  o 7), four pivots ( P = 4) and a query object Q . In the example, we select the closest p =3pivots for each of the query features. For the first feature (in L space), we select P 2 (1) ,P 3 (1) and P 4 (1) and for the second feature, we select P 1 (2) ,P 2 (2) and P 3 (2).

C can still be large; therefore, for each object o  X  C , we define tf ( o )tobethenumberofsets P q j ( l )inwhich o appears. We denote C ( m )  X  C , the set of the m objects in C with highest tf value. Our motivation is that objects o  X  C that are close to the query q are probably in C ( m ) since they were mapped to many pivots that are close to the query. The top-k results can, then, be scored with a fixed number of n distance computations d l ( q l ,o l )foreach o  X  C ( m ) followed by an aggregate function to combine them.

In the example in Figure 1, if we set m =4,wewould Figure 1: PCA with two metric spaces (L1,L2), seven objects ( o 1  X  o 7 ) and four pivots ( P 1  X  P 4 ) select the 4 documents with the highest tf which are o 1 with tf ( o 1 )=4and o 2 ,o 3 and o 4 ,eachwith tf =3.
This section presents our Inverted Index implementation for the PCA , and shows how textual attached data (free-text, fielded text or structured data) is indexed using the same indexing structure.

A typical Inverted Index is composed of lexicon and post-ing lists. To implement the PCA , we keep each pivot as a lexicon entry and the documents mapped to it are kept in its corresponding posting-list. A document with n low-level features is indexed by mapping the low-level features to their corresponding nearest pivots (similarly to a lemmatization step in text). Once this mapping is done, the document is a bag of words ( pivots and the associated textual terms) and can be indexed in regular manner.

The query processing is done in a similar way: the low-level features of the query are mapped to their nearest terms in the lexicon and a large disjunction of these terms with the eventual textual constraint is created. This Boolean expres-sion is executed on the inverted index and returns a set of m documents with the highest score. For an object o ,the score tf ( o ) corresponds to the number of sets P q ( i )towhich it belongs; this is efficiently obtained from the Inverted In-dex. This set of m documents corresponds to the set C ( m ) we defined in Section 2 and can be fully scored using their distances to the query.
Our experiments were done using Apache Lucene 1 on a collection of one million images crawled from Flickr, part of the CoPhir 2 test-collection. For each page, we indexed the image and the associated text. For the image, we indexed five MPEG7 visual descriptors  X  Scalable Color, Edge His-togram, Homogeneous Texture, Color Structure and Color Layout. To make each of the five features a Metric Space, we applied L 2 (Euclidean distance) for the ColorLayout fea-http://lucene.apache.org http://cophir.isti.cnr.it/ ture and L 1 (Manhattan distance) for the other features. We used sum for aggregating the five distances.

We estimated the efficiency with two factors: the distance computations needed and the disk accesses (approximated by the length of the involved posting-lists). We estimated the effectiveness by using the recall measure, comparing the results of a full similarity search versus our approach.
Table 1 summarizes the results for image only search over 150 randomly selected query images, with varied lexicon size ( P ) from 1000 to 32000, number of nearest pivots ( p )equal 30 and number of high scored documents ( m ) 1000. For each query, we obtained the top 25 results.

Results show that too short and too large lexicons may have low recall: lexicon size of 1000 and 32000 were too short and too large respectively, thus, the recall values were much lower than those of the  X  X edium X  size lexicons.
The  X #Comp X   X  X istance computations X  and  X  X osting list size X  columns in Table 1 describe the efficiency. As expected, increasing the lexicon size increases the number of distance computations and decreases the posting lists size. The  X  X ize of C  X  corresponds to the set C as described in Section 2. It should be noted that even though the size of C is rel-atively high, we compute full distances only on the top m documents, using the tf measure. In our setting, we got very high recalls for value of m = 1000.

Next, we generated a set of 20 queries composed of im-age and its corresponding text. We compared the involved posting list size for an approach that starts with an image query and adds the text as a filter afterwards, versus our ap-proach, which queries the text and the image concurrently. For a lexicon size of 2000, the posting list size is 28364 for our  X  X mage+text X  approach, whereas, for the  X  X mage-&gt; text X  approach, the size explodes quickly and reaches the level of 3429604. Since the posting list size is an approximation to the number of disk accesses, it confirms that using im-age+text significantly improves the efficiency.

Lexicon Recall Posting size | C | #Comp
We presented an efficient method for approximate content-based search over several metric spaces, optionally combined with text query. We showed an implementation using a uni-fied inverted index and reported experiments that show the effectiveness and efficiency of the method.
