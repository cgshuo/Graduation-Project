 1. Introduction
The issue of automatic disambiguation of word senses has been an interest and concern since the 1950s, and word sense disambiguation involves the association of a given word in a text or discourse with a particular machine translation, man X  X achine communication, message understanding in language understanding appli-query  X  X  depression  X  X , should the system return documents about illness, weather systems, or economics?
Though current IR systems do not use explicitly WSD and rely on the user typing enough context in the query ments suggested that reliable IR would require at least 90% disambiguation accuracy for explicit WSD to be of benefit [28] . In addition, WSD has been more recently shown to improve cross-lingual IR and document flagging of all the references illegal drugs , rather than medical drugs . More generally, the Semantic Web requires automatic annotation of documents according to a reference ontology: all textual references must co-reference determination, and acronym expansion can also be cast as WSD problems for proper names. WSD is only beginning to be applied in these areas.
 Since its inception, many approaches have been proposed for WSD in the literature (see [11] for a survey).
During the last two decades, many supervised machine learning algorithms have been used for this task, see e.g. [25] , including Naive Bayesian (NB) model, decision trees, exemplar-based model, support vector machines, maximum entropy models, etc. On the other hand, as observed in studies of pattern recognition may potentially offer complementary information about patterns to be classified. In other words, features and classifiers of different types complement one another in classification performance. This observation to WSD as in [6,7,10,12,15,27,32] .
 posed a common theoretical framework for combining classifiers which leads to many commonly used decision rules used in practice. This framework has been also applied to the problem of WSD in [17,20] .In based on Dempster X  X hafer (DS) theory of evidence [29] and OWA operators [33] .

In [2] , Al-Ani and Deriche have proposed a new technique for combining classifiers using DS theory, in ability assignments (BPAs). These BPAs are then combined making use of Dempster X  X  rule of combination to obtain a new output vector that represents the combined confidence in each class label. Different from their
BPAs, making use of the discount operation in DS theory and then combine the resulted BPAs to obtain the context in WSD as distinct representations of a polysemous word under consideration, and then all these rep-resentations are used jointly to identify the meaning of the target word. On the one hand, various ways of using the context could be considered as providing different information sources to identify the meaning of the target word. Moreover, each of these information sources does not by itself provide 100% certainty as a whole piece of evidence for identifying the sense of the target. Then by considering the problem as that weights seems to be appropriate when defining weights in terms of the accuracy of individual classifiers.
On the other hand, by considering each representation of the context as information inspired by a semantics should be worth mentioning that the use of OWA operators in classifier combination has been studied, for commonly used decision rules but without some strong assumptions made in the work by Kittler et al. [14] .
Experimentally, we design a set of individual classifiers, each of which corresponds to a distinct represen-tation type of context considered in the WSD literature, and then the proposed combination strategies are operators. Section 3 devotes to the theoretical framework for combining classifiers in WSD based on these ing remarks. 2. Preliminaries
In this section we briefly review basic notions of DS theory of evidence and OWA operators. 2.1. Dempster X  X hafer theory of evidence
In DS theory, a problem domain is represented by a finite set H of mutually exclusive and exhaustive hypotheses, called frame of discernment [29] . In the standard probability framework, all elements in H are assigned a probability. And when the degree of support for an event is known, the remainder of the support is automatically assigned to the negation of the event. On the other hand, in DS theory mass assignments are carried out for events as they know, and committing support for an event does not necessarily imply that the function m :2 H ! [0,1] verifying m ( H ) = 1 and m ( A ) = 0 for all A 5 H .

Two evidential functions derived from the basic probability assignment m are the belief function Bel the plausibility function Pl m , defined as one correspondence with each other.
 Two useful operations that play a central role in the manipulation of belief functions are discounting and discount rate , which results in a new BPA m a defined by
Consider now two pieces of evidence on the same frame H represented by two BPAs m rule of combination is then used to generate a new BPA, denoted by ( m sum of m 1 and m 2 ), defined as follows where
Note that the orthogonal sum combination is only applicable to such two BPAs that verify the condition j &lt;1. 2.2. OWA operators
The notion of OWA operators was first introduced in [33] regarding the problem of aggregating multi-cri-teria to form an overall decision function. A mapping is called an OWA operator of dimension n if it is associated with a weighting vector W =[ w (1) w i 2 [0,1] and (2) where b i is the i th largest element in the collection a
OWA operators provide a type of aggregation operators which lay between the  X  X  X nd X  X  and the  X  X  X r X  X  aggre-gation. As suggested by Yager [33] , there exist at least two methods for obtaining weights w approach is to use some kind of learning mechanism. That is, we use some sample data, arguments and asso-
The fuzzy linguistic quantifiers were introduced by Zadeh in [36] . According to Zadeh, there are basically membership function of relative quantifiers can be defined [9] as with parameters a , b 2 [0,1].
 follows: 3. Weighted combination of classifiers for WSD
Consider a pattern recognition problem where pattern w is to be assigned to one of the M possible classes c according to the Bayesian theory [14] , then the pattern w should be assigned to class c riori probability of that class is maximum, i.e.
Begin with the decision rule (7) , under the conditional independence assumption of the representations used greatly from the prior ones, the authors in [14] developed a theoretical framework for combining classifiers which leads to many commonly used decision rules used in practice. At the same time, the authors also con-ceded that these assumptions seem to be unrealistic in many situations. Particularly, to our opinion, these assumptions are difficult to be accepted and verified in the context of WSD. In the following, we will focus on a framework for combining classifiers in WSD based on the DS theory and OWA operators. This frame-work also interestingly yields many commonly used decision rules for WSD but without the strong assump-tions mentioned above. 3.1. WSD with multi-representation of context
Given a polysemous word w , which may have M possible senses (classes): c in the bag-of-words approach , the context is considered as words in some window surrounding the target word w ;inthe relational information based approach , the context is considered in terms of some relation semantic categories, etc. As such, for a target word w , we may have different representations of context C corresponding to different views of context. Assume we have such R representations of C , say f ing for the aim of identifying the right sense of the target w . Clearly, each f semantical representation of w . Each representation f i of context has its own type depending on which way context is used.
 thermore, assume that each i th classifier (expert) is associated with a weight a of representations f i associated with various interpretations of corresponding weights a sifier combination schemes serving for identifying the sense of the target w .
 3.2. DS theory based combination scheme Given a target word w in a context C and S  X f c 1 ; c 2 ; ... ; c vocabulary of DS theory, S can be called the frame of discernment of the problem. As mentioned above, var-meaning of the target word. Each of these information sources does not by itself provide 100% certainty as a making the final decision on the sense of w given as follows  X  R probability distributions P (  X  | f i )( i =1, ... , R )on S ,  X  the weights a i of the individual information sources ( i =1, ... , R ).
From the probabilistic point of view, we may straightforwardly think of the combiner as a weighted mix-ture of individual classifiers defined as
Then the target word w should be naturally assigned to the sense c
However, by considering the problem as that of weighted combination of evidence for decision making, we now formulate a general rule of combination based on DS theory. To this end, we first adopt a probabilistic classifier. This interpretation of weights seems to be especially appropriate when defining weights in terms of the accuracy of individual classifiers.

Under such an interpretation of weights, the piece of evidence represented by P (  X  | f a discount rate of (1 a i ). This results in a BPA m i defined by discernment.

We are now ready to formulate our belief on the decision problem by aggregating all pieces of evidence represented by m i s in the general form of the following where m is a BPA and is a combination operator in general.

By applying different combination operations for , we may have different aggregation schemes for obtain-with the problem of how to make a decision based on m .As m does not in general provide a unique probability distribution on S , but only a set of compatible probabilities bounded by the belief function Bel probability function P m on S derived from m for the purpose of decision making via the so-called pignistic when a decision must be made, the belief at the credal level induces the probability function P making. 3.2.1. The discounting-and-orthogonal sum combination strategy
As discussed above, we consider each P (  X  | f i ) as the belief quantified from the information source f ( i =1, ... , R ) at corresponding rates (1 a i )( i =1, ... , R ) before combining them.
Thus, Dempster X  X  rule of combination now allows us to combine BPAs m the orthogonal sum operation with respect to a combinable collection of BPAs m tioned property essentially form the basis for developing a recursive algorithm for calculation of the BPA m [34] . This can be done as follows.
 result of combining the first i BPAs m j , for j =1, ... , i . Let us denote p for k =1, ... , M , i =1, ... , R 1, and j I ( i +1) is a normalizing factor defined by S derived from m via the pignistic transformation as follows and we have the following decision rule: uations as pointed out in [37] . Fortunately, in the context of the weighted combination of classifiers, by discounting all P (  X  | f i )( i =1, ... , R ) at corresponding rates (1 a between the individual classifiers before combining them. 3.2.2. The discounting-and-averaging combination strategy
In this strategy, instead of using Dempster X  X  rule of combination after discounting P (  X  | f of (1 a i ), we apply the averaging operation over BPAs m for any A 2 2 S . By definition, we get above, we use it as a normalization factor and easily obtain the decision rule (9) .
 on the weighted mixture of individual classifiers is the same as that based on the probability function P m defined by (22) X (24) via the pignistic transformation. 3.3. OWA operator based combination scheme
Let us return to the problem of identifying the sense of a given word w as described above. As discussed on the role of context in the task of determining the most appropriate sense of w , each representation f word w in the form of a posterior probability P ( c k | f
Under such a consideration, we now can define an overall decision function D , with the help of an OWA operator F of dimension R , which combines individual opinions to derive a consensus decision as follows: where p i is the i th largest element in the collection P ( c suggests that the target word w should be assigned to class c tics to various weighting vectors W , we can obtain many commonly used decision rules as following. 3.3.1. Max rule
W = [1,0, ... ,0], which yields from (27) and (28) the Max Decision Rule as 3.3.2. Min rule (27) and (28) the Min Decision Rule as 3.3.3. Median rule
In order to have the Median decision rule, we use the absolute quantifier at least one which can be equiv-ian decision rule as 3.3.4. Fuzzy majority voting rules We now use the relative quantifier at least half with the parameter pair (0,0.5) for the membership function the corresponding weighting vector W =[ w 1 , ... , w R ] for the decision rule, denoted by FM1, as where p i is the i th largest element in the collection P ( c FM2.
 Interestingly also, from the following relation it suggests that the Max and Min decision rules can be approximated by the upper or lower bounds appro-upper bound yields the Sum rule.
 In addition, from the classical voting strategy, we can also obtain the following decision rule. 3.3.5. Majority vote rule
Majority voting follows a simple rule as: it will vote for the class which is chosen by maximum number of individual classifiers. This can be done by hardening the a posteriori probabilities P ( c
D ki defined as follows: then the right class (sense) c j is determined as follows: 4. Experimental study
In this section we will design an experiment to test the classifier combination schemes discussed. 4.1. Representations of context for WSD
As mentioned previously, context representation plays an essentially important role in WSD. For predict-words. In [26] , Ng and Lee proposed a use of more linguistic knowledge resources that then became popular for determining word sense in many papers. The knowledge resources used in their paper included topic con-and Matsumoto [32] .

Particularly, Pedersen [27] considered several context windows on both the left and the right and grouped window sizes. Finally, the best of each kind according to the majority voting procedure is then chosen. Wang context, we also carry out an experiment on Pedersen X  X  representation of context. We borrowed this feature space division from Pedersen [27] and used the maximum window size in each kind, consequently nine different representations were generated based on nine different combinations of left and right windows as follows: (2,2), (2,5), (2,50), (5,2), (5,5), (5,50), (50,2), (50,5), and (50,50).

For context representation, we observe that two of the most important information sources for determin-tural relations between the target word and the surrounding words in a local context. Under such an f is a set of part-of-speech tags assigned with their positions in the local context; f dered words in the large context with different windows: small, median and large respectively. Symbolically, we have where w i is the word at position i in the context of the ambiguous word w and p appears on the left (right) of w .

In the experiment, we set n 1 = 3 (maximum of collocations), n text). For topic context, we foresee three different window sizes: n relations. Even the unordered words in a local context may also contain structure information, but colloca-tions and words as well as part-of-speech tags assigned with their positions may bring richer information. 4.2. Test data Concerning evaluation exercises in automatic WSD, three corpora so-called Senseval-1, Senseval-2 and
Senseval-3 have been built on the occasion of three corresponding workshops held in 1998, 2001, and 2004 will be tested on English lexical samples of Senseval-2 and Senseval-3. These two datasets are more precise than the one in Senseval-1 and widely used in current WSD studies.
 A total of 73 nouns, adjectives, and verbs are chosen in Senseval-2 with the sense inventory is taken from
WordNet 1.7. The data came primarily from the Penn Treebank II corpus, but was supplemented with data from the British National Corpus whenever there was an insufficient number of Treebank instances (see [13] pus. The sense inventory used for nouns and adjectives is taken from WordNet 1.7.1, which is consistent with the annotations done for the same task during Senseval-2. Verbs are instead annotated with senses from Wordsmyth. 3 There are 57 nouns, adjectives, and verbs in this data (see [23] for more detail).
In these datasets, each polysemous word is associated with its corresponding training dataset and test data-proposal in [22] , which provides a scoring method for exact matches to fine-grained senses as well as one for partial matches at a more coarse-grained level. Note that, like most related studies, we just compute the fine-grained score in the following experiments. 4.3. Experimental results
Table 1 shows the experimental results conducted on Senseval-3 which are obtained by using various strat-the results obtained by applying the max, min, median, FM1, and FM2 rules, respectively. Further, DS denotes the Dempster rule of combination with discounting factor, while DS of combination without discounting factor, or equivalently, a probability a i for classifier C i ( i =1, ... ,6) used in DS (72.4% of DS 1 in comparison with 64.1% of C 4 ).
Table 2 shows an experimental comparison between the combination strategies discussed above and the others studied in the literature including majority voting, weighted voting, a stacking method using a maxi-mum entropy model and Naive Bayesian combination rule. These first three methods were used for WSD resentation of context affects on the performance of combination strategies, we conducted an experiment with those combination strategies on Pedersen X  X  representations of context, of which the result is also shown in Table 2 .

From the obtained results we see that the combination rule DS dence with our context representation gives the best result on average. Interestingly also, some combination strategies using OWA operators such as median and FM2 rules provide high accuracies as well. Note that though DS 1 also requires an assumption of conditional independence between individual classifiers, it seems each context representation, which is used to build an individual classifier, does not provide fully enough
DS much more effective than Pedersen X  X  ones. Note that while majority voting has been widely used in many stud-context of WSD.

The best accuracies obtained by the DS 1 rule, 64.7% for Senseval-2 and 72.4% for Senseval-3, are compa-systems (classifiers) which were built based on different machine learning algorithms were merged by using eval-3 contest used the Regularized Least Square Classification (RLSC) algorithm with a correction of the a based methods. The detail of this comparison is shown in Table 3 .
 5. Conclusions
In this paper we have discussed and formalized various ways of using context in WSD as distinct represen-tify the meaning of the target word. This consideration allowed us to develop a framework for combining semous word as different information sources serving for identifying the right sense of the target word, we have applied Dempster rule of evidence combination to derive decision rules, denoted by DS making the final decision of identification. On the other hand, considering each representation of a polyse-cation, we have also applied the notion of OWA operators for aggregating multi-criteria to define an overall decision function, which leads to numerous combination rules such as Max, Min, Median, FM1 and FM2, with the help of linguistic fuzzy quantifiers.

We have experimentally explored all developed combination strategies on the datasets of English lexical samples of Senseval-2 and Senseval-3. It has been shown that individual classifiers corresponding to dif-ferent types of representation suitably offer complementary information about the target to be assigned a sense; it consequently makes combination strategies would help in making more correct decisions. The experimental result has shown that the discussed framework of classifier combination also yields several decision rules in WSD that perform well comparable to the best systems in the contests of Senseval-2 and Senseval-3.

For the future work, we are planning to integrate the classifier combination schemes discussed in this paper with knowledge-based WSD methods as comprehensively studied in [24] for further improving the perfor-mance of disambiguation methods.
 Acknowledgements The constructive comments and helpful suggestions from anonymous reviewers are greatly appreciated. This research is partly conducted as a program for the  X  X  X ostering Talent in Emergent Research Fields X  X  in Special Coordination Funds for Promoting Science and Technology by the Japanese Ministry of Education, Culture, Sports, Science and Technology.

References
