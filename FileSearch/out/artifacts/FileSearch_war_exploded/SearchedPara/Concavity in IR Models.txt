 We study the impact of concavity in IR models and propose to use a generalized logarithm function, the  X  -logarithm to weight words in documents. We extend the family of infor-mation based Information Retrieval (IR) models with this function. We show that that concavity is indeed an impor-tant property of IR models. Experiments conducted for IR tasks, Latent Semantic Indexing and Text Categorization show improvements.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Theory concavity, retrieval constraints, log-logistic, IR models
Although the main probabilistic IR models differentiate from each other, either by a underlying theoretical frame-work or by a distinct choice of a word frequency distribution, all these well performing models share common properties which would allow one to describe these models in a sin-gle framework. This is precisely the aim of the axiomatic theory.

Axiomatic methods were pioneered by Fang et al. [4] and used since then in several studies including [2]. In a nutshell, axiomatic methods provide formal constraints that IR func-tions should satisfy in order to be valid, i.e. to perform well on IR tasks. According to [2], the four main constraints for an IR function to be valid can be phrased as: the weighting function should (a) be increasing and (b) concave wrt term frequencies (referred as TFC2 constraint in [4]) , (c) have an IDF effect and (d) penalize long documents.

For example, concavity favors documents that contain dif-ferent query terms (ie aspects) against documents that con-tains one query term. In practice, all state of the art IR models are concave functions with the term frequency in documents [1, 2, 6]. Many of IR weighting functions actu-ally rely on the logarithm function to ensure concavity in term frequencies, such as language models. A priori, there is no intrinsic reason for the log function to have the best analytical properties in an IR setting. As concavity is an important aspect of retrieval models, we want to further assess its impact by testing a family of concave functions parametrized by the  X   X  logarithm, a generalization of the log function. This family of models will be discussed in the next section. In section 3, several experiments will stress the importance of concave functions in IR and demonstrate the benefit of our approach.
First of all, we would like to discuss the concavity effect in IR models from a theoretical standpoint. Suppose that retrieval functions can be written as where q w is the query term frequency, t wd is normalized fre-quency of w in d ,  X  w a corpus statistic for word w ,  X  is a set of parameters and h a positive function. The concave effect [4, 2] is defined by the condition  X  (  X , X  ) ,  X  2 This constraint guarantees that the increase in the retrieval score should be smaller for larger term frequencies. Let us discuss further the implications of the concave effect. Let a and b be two words with equal idf or collection frequency, ie  X  a =  X  b . Imagine that all documents in the collection have the same length l , let s a constant, representing the number of occurrences of word a and b , ie t a + t b = s . We want to show that concave functions favor a uniform distribution of occurrences in documents. Let f the univariate function defined by f ( t ) = h ( t, X , X  ) Now, consider the following op-timization problem: So, A gives the score of a document whose frequencies for word a and b are equal to t a = t and t b = s  X  t . The solution of this problem gives the preferred repartition of frequencies for both words in documents.
The Kuhn-Tucker conditions implies that either the con-straints are active and t = 0 or t = s , or they are inactive and  X  =  X  = 0, which gives f 0 ( t ) = f 0 ( s  X  t ) and t = Overall this gives the two possible solutions: As f is concave the optimal solution is t  X  = s 2 . Hence, concave functions favors the equipartition of frequencies. In other words, concave functions favor documents with as many occurrences of word a as word b that is to say docu-ments that cover both aspects of a query. On the contrary, if f was convex, it would favor the other solution, when we choose only one word. In other words, convex functions fa-vor documents with either word a alone or word b alone. Note that these arguments are only valid for a fixed docu-ment length l and a predetermined s and that they could be generalized with more than two words.

Of course, the previous development is a theoretical ar-gument with limiting assumptions and what is lacking is the ability to test empirically whether concavity is indeed a critical aspect of IR models. What is needed to study con-cavity/convexity in IR models is a way to modify a state of the art IR model to adjust its concavity property. Ideally, a state of the art IR model would only be a particular case of this generalization. This is exactly what we propose with the use of a generalized logarithm function: the (  X  -)logarithm [5] 1 . The  X  -logarithm [5] comes from the statistical physics community and can understood as a generalization of the logarithm function. The  X  -logarithm is defined by  X  t &gt; 0: The interesting properties of this curved logarithm are: Several IR weighting functions actually rely on the logarithm function to ensure concavity in term frequencies, such as information models and the language modeling approach to IR. A priori, there is no intrinsic reason for the log function to have the best analytical properties in an IR setting and this is why our idea is to replace the logarithm component in IR models by this generalized logarithm.

To do so, we build on the recent information-based model presented in [2] to test the idea of the  X  -logarithm . Information-based IR models are defined by the following equation: where q w is the query term frequency, T w is a continuous random variable modeling normalized term frequencies t wd and  X  w is a set of parameters of the probability distribu-tion considered. This ranking function corresponds to the mean information a document brings to a query. [2] con-sider two distributions for word frequencies: the log-logistic and a smooth power law distribution. We adopt here the log-logistic model as it is a simpler model.
We changed the notation of the q  X  logarithm to  X   X  logarithm to avoid confusion with query notation.
Thus, we propose a generalized information model of the form:
This model, noted ELL (Eta Log-Logistic), has then two parameters: c the standard parameter in information and DFR models which normalizes the term frequencies and  X  which sets the curvature of the logarithm. Detailed formulas for this model are given in the appendix.

Let h ( t, X  ) =  X  ln  X  P ( T w  X  t |  X  ). We show in figure 1 the variations of h for several value of  X  and for a corpus frequency of  X  = 0 . 005 which illustrates that we are indeed parametrizing a family of concave functions. The second derivative wrt to t is equal to: Hence,  X  must be smaller than 2 to assure that the model is concave in term frequencies. But, the interesting property of this family of IR models is in fact the possibility of hav-ing a smooth transition between concave or convex models. Therefore, it is possible to adjust the curvature of the model in order to assess the role of concavity and to validate the importance of the concave effect as we will do in the next section.
First of all, we want to study the correlation between con-cavity and performance. Then, we will show that the previ-ous framework can be applied to Latent Semantic Indexing and Text Categorization as well.
To assess the impact of concavity, we used standard IR col-lections, from two evaluation campaigns: TREC and CLEF. We use the ROBUST (TREC), TREC3, and CLEF Do-main Specific Task 2004-2006 collections and apply stan-dard preprocessing including either stemming or lemmati-zation. We varied the two parameters in the following range c  X  X  0 . 5 , 1 , 3 , 5 , 7 , 9 } and Figure 2: MAP against c ,  X  on ROBUST, TREC3, CLEF dataset with title and descriptions queries.
 Then, we measured the performance (MAP) for each dataset. Figure 2 show a greyscale map of the performance on each dataset and with either short queries ( suffix -T) or long queries ( suffix -D). Recall that  X  = 1 amounts to the log-arithm and when  X  &lt; 2 the model is concave. The plots clearly show that convex model obtains the worst perfor-mance, roughly dividing the performance by a factor of 2. In other words, linear functions do not provide good per-formance in ad-hoc IR. Interestingly, it also show that the case  X  = 1 . 8 close to linear does not perform well albeit concave. On average, concave models perform better than convex ones.

In addition, the case where  X  = 1 . 1 , 1 . 2 in figure 2 are consistently the best on all collections and type of queries. This is why we compare in figure 3 the performance of the standard log-logistic model to the ELL model with  X  = 1 . 1. The generalized information model outperforms the log-logistic model for most values of c showing that the new model is not more sensitive to the setting of c
Second, we compared the generalized information model to the BM25 baseline [6] as it has also two important param-eters. BM25 [6] weighting schema has roughly 2 the form: where t is term frequency in the document . The parameter k somehow describes a family of concave functions: ( k 1 +1) x 10 random splits were used in order to optimize ( c ,  X  ) on each collection. Table 1 compares the performance to the BM25 [6] trained with two varying parameters ( k 1 and b ), with the statistical significance.

In most cases, the generalized information model outper-forms BM25. So, changing the curvature of the models al-it use a variant of IDF coming from the Probability Rank-ing Principle Figure 3: MAP of Log-Logistic Model vs  X  = 1 . 1 on the IR collections Table 1: ELL versus BM25 after 10 splits: bold in-dicates statistical significance lows to obtain significant improvements over the log-logistic model and BM25. One could argue that one additional free parameter,  X  , should be determined possibly with ex-tra amount of labeled data. However, BM25 has been used by the community for more than a decade despite having two parameters to set. Furthermore, our proposal outper-form BM25 and one can always a prior value of 1 . 1 as we showed it provide good result consistently on all collections.
Moving from IR models to document clustering or classi-fication tasks is straightforward: a document can be repre-sented by the following feature vector Latent Semantic Indexing (LSI) [3] computes a Singular Value Decomposition over a term-document matrix in order to find latent topics. We tried several variants of LSI on one dataset. The term document matrix is weighted with a TF -IDF baseline, or an informative content with log-logistic distribution as in information models or with our proposal with the generalized informative content. We used c = e  X  1 as a default value for the parameter c since the occurrences of document of average length are not renormalized (ie the normalization factor is equal to 1). Figure 4 shows the Mean Average Precision for the dataset queries once the SVD has been performed for several latent dimension. This figure shows the generalized informative with  X  = 1 . 2 outperform the two other baseline in most cases. Overall, this shows that the proposal is also adequate for computing similarities between documents as better performances are obtained. Figure 4: Mean Average Precision for different La-tent Semantic Indexing variants
Application to Text Classification. We then train a logistic regression with L2 regularization with different embedding of term frequencies for several text classifica-tion dataset. For each dataset, we keep the best value of  X  among { 0 . 5 , 0 . 75 , 1 , 1 . 2 , 1 , 5 } and compare the performance against several TF -IDF weighting [7] and a standard log function. We chose a default value for c = e  X  1 and We chose the regularization parameter on the validation set among accuracy for 3 standards test collections.
 Table 2: Mean Accuracy on 10 random splits (test set)
Our model is consistently the best on all collections. If one can argue that these are minor improvements compared to the classical tfidf baseline, we want to mention that such difference in performance do have practical value in litiga-tion scenarios.
 Concerning text classification, the diffusion Kernel and Battacharya Kernel seem to be the more effective accord-ing to [7]. The Diffusion Kernel compares two multinomial probability distributions  X  and  X  by: The main ingredient of these kernels seems to be to the square rooting of the term frequencies. For example, [7] ex-plains that the NGD kernel, the Bhattacharyya kernel and the information diffusion kernel on the multinomial mani-fold, all invoke the square-root squashing function Note that the  X  -logarithm can be understood as a scaled power func-tion and thus contain the particular case of the square root. However, the best performances with the  X  -logarithm are of-ten obtained when  X  = 1 . 2 or  X  = 1 . 1, a different case from the square root.
Concavity is indeed a desirable property of IR models and convex models performed poorly. Weighting words in documents require concave functions and we propose the  X  -logarithm to parametrize a family of weighting functions. We then generalize the family of information-based model with this curved logarithm and show experimentally the ben-efits of this approach for IR, document clustering and text categorization.

Moreover, the generalized information models can be valu-able in the context of the axiomatic theory, so as to under-stand why and when IR models fail and to develop novel constraints. In particular, concave functions that fails to provide good performance are interesting candidate to study. Finally, an interesting extension would be to have different values of  X  for different words, depending eventually on lin-guistic properties or statistical behavior in the corpora. In information models [2] discrete word occurrence x wd are first renormalized with the following schema into continuous values t wd to account for different document length l d . This renormalization use a parameter c and the average document length in the collection L avg .

All in all, the ELL model we propose with a log-logistic distribution amounts to log  X  P ( T w &gt; t wd |  X  w ) = 1 where N is the number of documents in the collection and N w the number of documents containing word w .
 [1] G. Amati and C. J. V. Rijsbergen. Probabilistic models [2] S. Clinchant and E. Gaussier. Information-based models [3] S. Deerwester. Improving Information Retrieval with [4] H. Fang, T. Tao, and C. Zhai. A formal study of [5] J. Naudts. The q -exponential family in statistical [6] S. E. Robertson and S. Walker. Some simple effective [7] D. Zhang, X. Chen, and W. S. Lee. Text classification
