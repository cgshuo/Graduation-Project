 Modelling the document scores returned from an IR system for a given query using parameterised score distributions is an area of research that has become more popular in re-cent years. Score distribution (SD) models are useful for a number of IR tasks. These include data fusion, query performance prediction, determining thresholds in filtering applications, and tasks in the area of distributed retrieval. The inference of performance metrics, such as average pre-cision, from these SD models is an important consideration. In this paper, we study the accuracy of a number of methods of inferring average precision from an SD model.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval: Query formulation General Terms: Experimentation, Measurement, Perfor-mance Keywords: Information Retrieval, Score Distributions, In-ference
Modelling the document scores returned from informa-tion retrieval (IR) systems using score distributions (SD) models [16] is both theoretically principled and practically useful [12]. A number of works [1, 6, 9] have attempted using expectation-maximisation to infer the distributions of relevant and non-relevant document scores from unlabelled data. However, there are a number of unanswered questions with regard to SD models even when labelled data (i.e. rel-evance judgments) are available. Before dealing with  X  X oisy X  and unlabelled data, it is important that we know how to correctly model and accurately infer performance metrics from  X  X lean X  labelled data. This paper deals with determin-ing the best method of inferring average precision from SD models that use labelled data and are theoretically consis-tent with many known IR principles.

In this section, we discuss related work regarding SD mod-els and we outline the contribution made by this work.
A number of recent works [13, 11, 2, 7] in this area have aided in constraining the search for models that are con-sistent with various theories and observed phenomena in the domain of IR. Robertson X  X  Recall-Fallout Convexity Hy-pothesis [13] generalises previous work [4] and extends the probability ranking principle (PRP) to the continuous score domain where one ranking of the collection, given a query, is a sample ranking drawn from an infinite collection which is assumed to adhere to the PRP. This effectively invali-dates the normal-exponential SD model used in many works. Others [2] have postulated both  X  X trong X  and  X  X eak X  SD hy-potheses that help to constrain the choice of distributions that comprise an SD model. These hypotheses suggest that the SD model should be able to support perfect retrieval (i.e. full separation of relevant and non-relevant distributions).
Normalisation of retrieval scores prior to parameter esti-mation can affect the theoretical validity of an SD model. For example, min-max (0-1) normalisation used in many pre-vious works [1, 9] effectively prohibits scores below 0 and above 1. In such circumstances, any distribution that sup-ports values outside of that range (i.e. 0-1) would be invalid, as it would assign a non-zero probability to an observation that is unobservable (i.e. impossible) [13].
As it is the query that generates both relevant and non-relevant document scores, it would be unlikely that the pair of distributions that comprise a binary SD model are from two different families of distributions (e.g. it is unclear why relevant documents should be drawn from a normal, when non-relevant documents are drawn from an exponential dis-tribution). In most cases, an IR system does not know the underlying relevance of each document and can only strive to separate the two distributions as much as possible (as im-plied by the  X  X trong X  and  X  X eak X  SD hypothesis). Therefore, it is likely that both distributions should be of the same family (type) of distribution (at least for an initial ad hoc retrieval run where no relevance information is available).
Using only a portion of the ranked-list (i.e. the top 1000 yes yes [0 :  X  ] [0 :  X  ] yes yes [0 :  X  ] [0 :  X  ] yes yes [  X   X  :  X  ] [  X   X  :  X  ] document scores) from which to infer the SD model parame-t ers may negatively affect the performance of a model. Trun-cating the list at arbitrary points may ignore useful infor-mation regarding the document scores of a large number of documents. Although most of the documents that are dis-carded below the truncation point may not be relevant, they are important for the inference of the non-relevant document score distribution. This can, in turn, affect the inference of performance metrics. Conversely, modelling the entire col-lection of documents as a ranked-list leads to problems where the actual  X  X it X  of the model is poor, due to the number of documents that receive no score (due to not matching any query-term). This would manifest itself as a large probabil-ity mass (spike) at a score of zero. In fact, for many models of retrieval, documents that do not match a query-term are excluded from the ranking (i.e. they are not ranked), rather than receiving a score of zero 1 .
Table 1 lists a number of SD models used in the literature and outlines the hypotheses that they each adhere to, when assuming that the expected score of relevant documents is greater than the expected score of non-relevant documents [13]. Inferring average precision from these distributions is an important consideration in determining the practical use-fulness of the model and the practical usefulness of any task using these models. However, average precision has been in-ferred using two different methods thus far in the literature [8, 9]. In this paper, we determine empirically which of these methods is more accurate. In doing so we use SD models that are consistent with all of the aforementioned hypothe-ses. Furthermore, we do not apply score normalisation or ranked-list truncation.
In this work we model a document ranking using an SD model that adheres to the recall-fallout convexity hypothe-sis (RFCH) [13], where f ( s | 1) and f ( s | 0) are the probability density function of the relevant and non-relevant document scores respectively, and where  X  is the mixing parameter. Therefore, the scores returned in a ranked list can be mod-use two gamma distributions 2 to model the scores of both relevant and non-relevant documents.
F or models of retrieval that allow negative scores, assign-ing a score of zero to documents that match no query-terms is not a practical, or theoretically sound, solution. Deem-ing the documents not-returned would seem a practical and more theoretically sound approach.
A two-lognormal model that adheres to the RFCH yields higher actual correlation values, but comparatively similar
Given a ranked list of all document scores s 1 , s 2 , s 3 returned in response to a query Q (i.e. where ret is the re-turned set of documents that match at least one query-term) and the known binary relevance labels for the documents at each of those scores, we can estimate the SD model param-eters for that ranking. For simplicity, we use method-of-moment estimates and ensure that the model adheres to the RFCH by equating the scale parameters of both distribu-tions using the values obtained from the non-relevant doc-ument scores following previous research [8]. We estimate  X  using only the relevance labels in the returned set (i.e. uments). It is important to note that we do not perform any score normalisation on the output of the scores from any of the IR systems used. We now review two methods that can be used to infer average precision from an SD model and the available ranking.
Research using the Maximum Entropy Method to analyse performance metrics [3] has shown that the expected average precision E[ap] of a ranking can be calculated as follows: where R is the number of relevant documents for a query, N is the number of documents in the ranking, and p i is the probability that a document at rank i is relevant. The prob-ability of relevance at rank i can be inferred from the SD ous research [9]. Adherence of the SD model to the RFCH ensures that this probability of relevance decreases as i in-creases, and therefore, adheres to the PRP. Given that doc-uments that do not match a query-term are not modeled in our approach, N = ret is the number of returned documents and therefore, R can be estimated as is used in recent research [9] and can be computed in O ( N ) time.
Average precision corresponds to the area under the pre-cision recall curve [15]. This can be expressed in terms of a score s as follows: where prec ( s ) is the precision at score s on the score line results as regards the accuracy of the two approaches to i nferring average precision studied in this paper. R s f ( s | 1)  X  ds , while prec ( s ) is calculated as As s is supported on [0 :  X  ), we can approximate this using numerical integration. In particular, N uniformly spaced integration points on [0 : B ] can be used where B is some upper bound where f ( s )  X  0. We have found by experimen-tation that 2  X  s 1 (i.e. two times the top score) is a suitable point 3 for the collections used here. AuPR can be calcu-lated in O ( N ) time where N is the number of samples used. By starting at B , we can estimate recall, fallout, and pre-cision at each integration point down to 0. The estimate of AuPR can be calculated cumulatively as the algorithm progresses (as outlined in Algorithm 1). These samples can be viewed as documents that are uniformly dispersed on the score line s from 0 to B .
 Algorithm 1 Calculate AuPR with N samples and mixing parameter  X  score = 2  X  s 1 recall = 0 fallout = 0 ap = 0 ds = score/N for i=0 to N do end for return ap
In the experiments that follow, we compare both methods of inferring average precision (i.e. E[ap] and AuPR ) us-ing a large number of queries on three test collections from TREC disks 1-5 (two Newswire collections and one Web collection 4 ). We submit each query to an IR system and estimate the parameters of an SD model using the relevance judgments for that query. We then estimate the inferred av-erage precision for each of the methods outlined. We report the Kendall- X  correlation and RMS (root mean square) error of real average precision to inferred average precision over a set of queries. We conducted these experiments on two IR systems (BM25 with default parameters and a language model LM using Jelinek-Mercer smoothing set to 0.2). It is important to note that both of these systems return positive scores as we used the modified BM25 [10] and the language model with JM smoothing from [17].
While the integral can be transformed into an integral over a finite interval, the method used here produces a suitable approximate of average precision. It also enables us to con-trol the number of uniformly sampled documents ( N ) on the score line.
Outlined in Table 2 and available from http://trec.nist.gov/ AP 242,918 149 051-200 3.6 FT 210,158 188 251-450 2.5 WT10G 1,692,096 100 451-550 2.5 We can see from Figure 1 for all three collections that AuPR outperforms E[ap] as there is a higher correlation with real average precision. Another point to note is that AuPR needs relatively few points (i.e. documents on the score line s ) to reach maximum performance. On all three collections, after 64 points have been sampled, the corre-lation of AuPR with actual average precision is near its maximum. A similar trend is reported for the RMS error (Figure 2) and the results are consistent for both IR sys-tems used. Overall AuPR is the more accurate approach of the two analysed and can be calculated using far fewer documents ( N ).

Using an SD model that adheres to the RFCH smooths the probabilities of relevance over the entire score range s in a principled manner. Using only a limited interval on the score line s , (as is the case for E[ap] which uses the initial dis-crete ranking) may ignore useful information at high scores. Approximating the integral (as is the case for AuPR ) is a more accurate approach. We hypothesise that using only a part of this range effectively reduces the accuracy of the in-ference from the model. Furthermore, as score distributions effectively model an infinite collection of document scores in a principled manner, it is more intuitive to infer the ef-fectiveness metric from the continuous domain. Modelling the actual ranking of documents as a sample drawn from an infinite collection was proposed recently [14] and it has been shown that average precision values, when smoothed appropriately, tend to follow a normal distribution. It must be noted that the E[ap] was not originally developed for the task of inferring average precision from SD models, and so this research does not dispute its X  importance in other areas of IR.
We have presented an empirical study of two methods of inferring average precision from SD models. We have shown that approximating the area under the precision-recall curve directly from the SD model is the better of the two ap-proaches in terms of accuracy.
 The first author is funded by the Irish Research Council for Science, Engineering and Technology (IRCSET), co-funded by Marie Curie Actions under FP7. [1] Avi Arampatzis, Jaap Kamps, and Stephen [2] Avi Arampatzis and Stephen Robertson. Modeling [3] Javed A. Aslam, Emine Yilmaz, and Virgiliu Pavlu. [4] Abraham Bookstein. When the most pertinent [5] Ronan Cummins. Measuring the ability of score [6] Ronan Cummins. Predicting query performance [7] Ronan Cummins. Investigating performance predictors [8] Ronan Cummins and Colm O X  X iordan. On [9] Keshi Dai, Virgil Pavlu, Evangelos Kanoulas, and [10] Hui Fang and ChengXiang Zhai. An exploration of [11] Evangelos Kanoulas, Keshi Dai, Virgiliu Pavlu, and [12] R. Manmatha, T. Rath, and F. Feng. Modeling score [13] Stephen Robertson. On score distributions and [14] Stephen Robertson. On smoothing average precision. [15] Stephen E. Robertson, Evangelos Kanoulas, and [16] John A. Swets. Information retrieval systems. Science , [17] Chengxiang Zhai and John Lafferty. A study of
