 Probabilistic data exist in many fields, including spatial databases, biology informa-tion system, sensor network and so on. A number of factors, such as data collection, data transformation and data reservation, contribute to the randomness of data [2, 5, 6]. Now many methods have been proposed to mine knowledge from probabil-ble worlds seriously limits its feasibility. Therefore, there is still a challenge to handle traditional mining problems in the field of probabilistic data mining. 
Some researchers have done some interesting exploratory works on probabilistic data mining. For example, Smith [7] proposed an uncertain decision tree by consider-ing both probabilistic information and the difference between real values. Based on the same goal, [8] proposed a new Na X ve Bayes classification for probabilistic data by existing methods cannot effectively handle the datasets with high dimensions [1]. Therefore, it is necessary to do some preprocessing work by reducing the dimension randomness of real value, but they neglect the distance of the real values. 
Feature selection is a powerful tool to re duce redundant dimensions and select in-and redundant features. The irrelevant feature is used to indicate a feature with lower or no relevance to the class. The redundancy means some features can be omitted or maximal relevance and ignores relativity among the features. 
This paper has made the following contributions.1) We propose a distance-based information measure model. 2) We design a novel feature selection method, which deletes irrelevant as well as redundant features. In this section, we first present a novel probabilistic data model used in our paper, and then give a new definition of distance on such data. The main symbols used in this paper are listed in Table 1. 2.1 Probabilistic Data The problem of modeling probabilistic data has been studied in many researches [4]. assumed to be Gaussian distribution. For example, the value measured by a sensor could be modeled as A:[60, 80] with a Gaussian distribution  X   X  70,10  X   X  . 
When a numerical feature is represented as an interval with an independent proba-bility distribution, we call it random value feature (RVF), denoted by A  X  . Further, we tuple here consists of random value features A  X  and class D. 2.2 The New Distance Measure In this subsection, we discuss how to measure the distance of such probabilistic data. where x, y are random variables of  X   X   X   X ,  X   X  , and  X  X  X   X   X   X ,  X   X   X  X  X  X 0,1 X  . In fact, any type of distance formula can be used to construct the distance metric, as long as it satisfies non-negative, symmetric and triangle inequality characteristics. where  X   X   X   X   X  =1 if  X   X  and  X   X  are in the same class, else  X  X  X   X   X =0 . two condition features. dis represents the discernible distance of tuple P. The discern-ible distance is a minimum distance to distinguish P from tuples with different classes. In this section, a new way to measure the information hidden in probabilistic databas-introduce a novel feature selection method of probabilistic databases. In our paper, the information means the discernible power in probabilistic databases. 3.1 Distance-Based Information Measure Definition 4 (Discerned-Pair). If a pair  X  X   X   X  ,  X   X  X  X , X  X  X  X  X  X  and  X  X  X   X   X  X  X  X  X  X   X   X  ,  X  ,  X   X  is called a discerned-pair of dataset  X  . Definition 5 (Discernible Space). The discernible space of feature subset R satisfies So Discerned-Pair represents the sample pa ir with different decision class, and dis-cernible space of R consists of all discerned-pairs distinguished by feature set R. Definition 6 (Discernible Power of R). The discernible power of R is defined as the cardinality of discernible space  X  X  X   X   X   X  :  X  X   X   X   X  = |  X  X  X   X   X   X  X  . Definition 6 shows  X  X   X   X   X  can measure the ability of R to discern each pair in a non-power R. In a sense,  X  X   X   X   X  can be seen as a measure for information hidden in prob-abilistic data, and  X  X   X   X   X  holds all information hidden in probabilistic data. 3.2 Distance-Based Feature Selection In the above subsection,  X  X   X   X   X  can be as a measure of information hidden in proba-the information used to classify different class tuples keeps unchanged. In general, we can select the features, which is with maximal discernibility and minimal relativity. 
When we just find a subset with the maximal discernibility, the relativity among the features still exists. For example, a feature ranking method [3] selects two features both with bigger discernibility, but they might distinguish almost the same discerned-pairs. Here redundancy happens. To solve such problem, the concept of minimal relativity is mentioned. Minimal relativity between features mainly embodies in the maximal dif-ference of their discerned-pair space. In general, we can select the features, which have the higher discernible power as different discernible space as possible. 4.1 A Heuristic Algorithm Generally speaking, the na X ve algorithm is first to find all subsets of the original fea-ture set and compute their own discernible power. The subset with the fewest features Obviously, such a na X ve method is NP complete problems. 
Hence, we propose a heuristic algorithm to find a final subset. The algorithm starts with an empty set, and keeps adding features into a pool until the discernible power of all features in the pool is the biggest. The algorithm is described as follows. 4.2 The Accelerative Algorithm Meanwhile, two accelerative algorithms are designed to get better performance. 
The first accelerative algorithm makes full use of uniqueness to measure whether a be distinguished by the feature selected in priority, are being deleted from discerned-We call this accelerative algorithm discer ned degree-based algorithm (FSDD). 
The second accelerative algorithm is a s earch and delete algorithm (FSSD). It saves execution time by reducing space. First, it searches the feature with the biggest discerned power through the original discerned-pair space, and then deletes the dis-final subset and we keep searching the next feature until the reduced space is empty. 4.3 The Scalable Algorithm with a Variable Although the accelerative algorithms improve the performance of feature selection process, they are still sensitive to noise. This problem can be analyzed with Figure 2. Point g is an object with noise. When we compute the discernible distance for object P, we get a smaller discernible distance dis represented with the red line. This smaller dis leads to an obvious question: object P is completely isolated. 
To solve such problem, we redefine Definition 3, 5 with variable  X  . In Definition adding variable , X  the effect of object P that was isolated with others could be some-what neglected. After replacing old definitions, we get the general algorithms with variable  X  , which are less sensitive to noise. Here,  X  X  X 0,1 X  X  . In this section, we present the experiments to evaluate the algorithms. 4 datasets which contain numerical features are chosen from the UCI (See Table 2). The original numerical data are converted into interval values with probability distribution. The procedure is as follows: Scan each feature and get its maximum value  X   X  X  X  X  and mini-use Gaussian distribution over the range without loss of generality. For each fea- X  where  X  is a percentage parameter that can control randomness degree of the objects. 
There are two factors to measure the performances of our algorithms. One is the size of the final subset, and the other is the execution time. In addition, there are two main parameters  X  and  X  in our algorithms. 5.1 Effect of  X  When we increase the value of  X  , the execution time and the number of feature sub-set of our three algorithms are recorded in Figure 3. Since datasets D1-D4 have simi-lar results, we only take parkinsons as an example and fix  X  =0.9. scale of subset keeps unchanged, the execution time is basically unchanged. The ran-discernibility of features. Effect of randomness on different algorithms in the former features lead to the scale of the feature subset varies to a certain extent. 5.2 Effect of  X  The parameter  X  is used to decrease the effect of noise. To avoid the effect of  X  , we fix 0.05= X  . In Figure 4, each subgraph draws the execution time with the value of  X  table, we can see that our algorithms decrease the scale of features effectively. 
Figure 4 shows FSSD mostly costs the fewest execution time among the three al-ability to find a smaller subset weakens when datasets with more features. On the creases. This phenomenon can be explained from the view of the parameter  X  , which reduces the effect of noise to keep the discernibility of features. So FSDD makes full the fewest execution time whereas FSDD is good at selecting a smaller feature subset. Probabilistic data models have been discussed in many applications, including spa-tial databases, sensor networks and biology information systems. However, less ef-randomness and distance. In this paper, we propose a new distance-based feature selection method. Acknowledge. This work was supported by the National Science Foundation of Chi-na ( Grant No. 612 02114, 61070056,61033010, 61272137 ). 
