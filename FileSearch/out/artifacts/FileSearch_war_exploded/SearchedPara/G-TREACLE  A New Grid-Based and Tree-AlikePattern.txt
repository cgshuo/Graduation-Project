 Cluster analysis in data mining is a critical business application, which has re-cently become a highly active topic in data mining research [1]-[7]. Most of existing clustering techniques have high computational time, or may have pat-tern recognition problems when using large databases. To solve limitations of the previous existing clustering methods, this work presents a new algorithm named  X  G rid-based and TRE e-A like C lustering technique for L arge databas E s X  (G-TREACLE) by integrating with grid-based, density-based and hierarchical clus-tering approaches. Performance studies show that the proposed G-TREACLE approach is a highly robust clustering technique.
 Several clustering algorithms regarding this work are described as follows.
K-means is the one of popular partitional algorithm [4]. It takes the input parameter, k , and partitions a set of n objects into k clusters. K-means always converges to a local optimum and it can not filter noise.

The grid-based clustering algorithm defines clusters as a multisolution grid data structure. It quantizes the object space into a finite number of cells that form a grid structure on which all of the operations for clustering are performed. The major advantage of the approach is its fast processing time. CLIQUE is one of the most famous grid-based techniques [7]. However, its cluster boundaries are either horizontal or vertical, due to the nature of the rectangular grid.
To identify clusters with arbitrary shape, density-based clustering approaches have been proposed. Those typically regar d clusters as dense re gions of objects in the data space that are separated by regions of low density (representing noise). DBSCAN is the one of well-know density-based approaches. Although it can accurately recognize any arbitrary pattern and different size clusters, and filters noise [5]. However, the time complexity of DBSCAN is high when the database size is large.

GDH integrates the idea of grid-based, d ensity-based and h ierarchical clus-tering methods, developed by Wang [2]. GDH refers the conception of density function and gradient decrease and concept of sliding window [2]. Although GDH can significantly eliminate the problem of indentation boundaries resulted from traditional grid-based algorithms, it may fail in grouping objects to the right position if two clusters are the same time in the populated hypercube. This section describes the concepts of th e proposed new G-TREACLE clustering algorithm. Ideally, the G-TREACLE algorithm creates a feature space through  X  X ypercubes map constructing X  in which all of objects are located on appropriate position. Then,  X  X ecognizing solid framework X  is employed to fleetly identify the framework of clusters, and subsequently adopt  X  X ree-alike pattern X  within  X  X dge shaping X  to discover  X  X lurred region X , which may contain noises and cluster objects. Finally, the parts resulted from the above concepts will be integrated to acquire the complete clusters. The implemented details of concepts are illustrated with four parts as follows: (1) Hypercubes map constructing: Reducing the number of searching spaces is the main idea of this step. Initially, G-TREACLE constructs a hy-percubes map by splitting the feature space in accordance with a hypercube X  X  length. Then, each object is assigned to an appropriate hypercube. If the to-tal number of objects in the hypercube is greater than the threshold Hd ,this hypercube is named  X  X opulated hypercube X  [6]. Fig. 1 illustrates the concept. The searching expansion through the initial point will be performed. Notably, a populated hypercube is called  X  X nitial point X  of search space if it has the highest number of objects among all populated hypercubes. (2) Recognizing solid framework: This investigation adopts the  X  X ynamic-gradient-threshold X  as a measure of hypercube-volume, namely the number of objects in the populated hypercube, det ecting preprocesses to discover the solid framework of clusters excluding the blurred region. The dynamic-gradient-threshold is obtained as follows: where | HC | indicates the number of objects in the most populated hypercube HC in the cluster, and PSV is the percentage of the submontane value, which is an input parameter. Fig. 2 depicts an example of the usage of dynamic-gradient-threshold. Every bar in Fig. 2 indicates the number of objects in each populated hypercube. Since every bar within a cluster may be different, dynamic-gradient-threshold can dynamically determine whether a populated hypercube can be treated as the solid framework o f clusters in which every object can be assigned to a cluster without calculation. In Fig. 2, NC 1, NC 2and NC 3rep-resent the complete cluster. After computing the dynamic-gradient-threshold, such as DGT 1, DGT 2and DGT 3 in Fig. 2, for each cluster, the solid frame-work of clusters will be identified and assigned directly to a cluster but excluding the  X  X lurred region X  representing the areas whose number of objects is under dynamic-gradient-threshold, given as IC 1, IC 2, IC 3 and the areas between the clusters. Subsequently, the edge shaping step has to be utilized to detect those  X  X lurred region X , as displayed on populated hypercubes A, B, D, F and G of Fig. 3.
 (3) Edge Shaping: The aim of this step is to define accurately the blurred region of a cluster. In this work, the new density-based clustering method is proposed. In contrast to conventional density-based clustering algorithms, e.g., DBSCAN, the proposed density-based method processes searching expansion through a  X  X ree-alike pattern X  comprising many centroids for each cluster, thus decreasing time complexi ty. Fig. 4 displays the procedure of how does the pro-posed density-based method work. In the 2-D hypercubes map, displayed in the c choosing process is defined as: where w is the radius of the search circle and the distance function d ( x j ,c p )is the Euclidean distance function: where k represents the dimension. If the centroid set C is empty or the distance between the object x j and each centroid c p in C is greater than w , the object x j is chosen as new centroid. Otherwise, the object x j is assigned to its closest centroid c p in C . As displayed in the diagram (b) of Fig. 4, each zone surrounded by dotted circle is termed  X  X eighbor-area X  in which the largest point is illustrated as centroid. And the neighbor-area NA p must satisfy: where c p is the centroid of NA p . Subsequently, we need to identify which neighbor-area consisting of noise. In order to achieve this purpose, the den-sity of every neighbor-area NA p is determined by deriving density function [6] rather than directly counting the numbe r of objects contained in the neighbor-area. The assumption is that the density value of the neighbor-area (namely region) comprising noise is generally lower than that of the populated neighbor-area containing normal clusters objects since its distribution is always sparser than that of the populated neighbor-area [6]. In other words, this means that although the neighbor-areas consisting of noise have the same number equivalent to the ones consisting of normal clusters objects, but the derived density value of former generally lower than that of latter. Consider some neighbor-areas within the clusters displayed in the diagram (b) of Fig. 4 that are not surrounded com-pletely by dotted circle, those areas cons ist of fewer normal objects but cannot be labeled as noise-area since the density of those areas is greater than the density of noise-areas that not belong to any cluster.

In [6], influence function is defined as a mathematical description that the influence of an object has within its neighborhood, while the density function is defined as the sum of influence function of all objects in the region, and can be any arbitrary function. For simplicity, this work applies the Euclidean density function and Gaussian representation. The Gaussian density function is given by [6]: where N represents the number of objects within the region, d ( x i ,x j ) denotes the distance between x i and x j ,and  X  is the standard deviation. If derived density value of the neighbor-area is greater than the threshold MinDensityVal ,it will be preserved as a  X  X ode X . Otherwise, the neighbor-area will be pruned and labeled as noise-area.

After the pruning process, each node searches its neighbor nodes and links them through the virtual edges, which are illustrated in the diagram (c) of Fig. 4. The connection between the nodes means t hat their distance is less than twice the w stated above. After neighbor nodes searching recursively, a  X  X ree-alike pattern X  can be constructed as a cluster mapping. On the other hand, a broken connection between the patterns makes th em into different clusters or noises. The complete algorithm is described as follows.
 PartialDataSets represents a partial dataset. Width is a search radius, and MinDensityVal denotes the minimal density threshold value in the region.
The neighbor node searching process searchNeighborNode() is as follows:
After running the new density-based clustering method TAClustering() ,a set of sub-clusters can be gained from the populated hypercube that not be-longs to the solid framework of the cluster. These populated hypercubes may contain objects belonging to two differ ent clusters, as mentioned above and depicted on populated hypercubes F and G in Fig. 3. Border objects of sub-cluster and noise can be recognized at the same time [5]. In order to produce the precise combination, the proposed algorithm connects sub-cluster resulted from TAClustering() run with the solid framework of cluster through the border objects of sub-cluster. Border objects a re redefined as objects resulting from a TAClustering() run that are close to the populated hypercube X  X  border. This redefinition shortens the computational time in TAClustering() . The light color objects (on the border) on populated hypercubes A, B, D, F and G of Fig. 3 indicate border objects. (4) Consolidation stage: After the edge shaping stage, the algorithm merges the parts resulted from method TAClustering() with the solid framework of the cluster, depending on which border objects are close to the solid framework of cluster. The proposed algorithm repeats the process to recognize all clusters. The complete clustering algorithm described as follows: DataSets is an entire database. Cl represents the length of a hypercube, PSV denotes the percentage of the submontane value, and Hd is the threshold of the populated hypercube X  X  volume. Width represents a search radius, and MinDensityVal denotes the minimal density threshold value in the region.
The neighbor searching process searchNeighborHCubes() is as follows:
The process is repeated to con struct the entire cluster. In this study, G-TREACLE was implemented in a Java-based program, and run on a desktop computer with 256MB RAM, an Intel 1.5GHz CPU on Mi-crosoft MS Windows XP professional Operational System. For simple visual-ization, seven synthetic 2-D datasets w ere utilized to evaluate the performance of the proposed algorithm [3]. Among these datasets, the patterns of dataset 1, 2 and 4 were sampled from [2] and [5], Fig. 5 shows the original datasets. The results of the proposed algorithm were compared with DBSCAN, K-means, CLIQUE and GDH. Four kinds of data sizes in seven synthetic 2-D datasets, with 11,500, 115,000, 230,000 and 575,000 objects in seven synthetic 2-D datasets, and all with 15% noise, were employed in this experiment. For clustering per-formance comparisons, the clustering correctness rate (CCR) and noise filtering rate (NFR) are introduced. Notably, CCR represents the percentage of cluster objects correctly recogn ized by algorithm, while NF R denotes the percentage of noise objects correctly filtered by algorithm. Due to the computational time of DBSCAN increases significantly as the number of databases increases, hence Table 1 does not list the simulation results for DBSCAN (N/A means that the simulations were not performed). Table 1 shows the clustering experimental re-sults with G-TREACLE, K-means, DBSCAN, CLIQUE and GDH by utilizing 575,000 object datasets. Owing to the limitation of length, not all experimen-tal results are shown. It is observed that G-TREACLE can handle arbitrary patterns for clustering, while K-means cannot recognize arbitrary shapes. Al-though CLIQUE and GDH could handle the complex patterns in Dataset 4 to 7, CLIQUE could not smoothly identify clusters X  edge due to the nature of the rectangular grid, and then it caused in inaccurate results. Additionally, the gradient decrease function in GDH pla ced some clusters the wrong position if the populated hypercubes were neighbor s but the gradient decrease between the populated hypercubes was too high. In complex datasets such as DataSets 4, 5, 6 and 7, GDH and CLIQUE need to set small capacity of populated hypercube for distinction between cluster X  X  borders that are close to each other. Therefore, the time cost of GDH and CLIQUE raises with increasing numbers of populated hypercubes to be searched and processed. As shown in Table 1, G-TREACLE usually yields more accurate results and performs fast than K-means, DBSCAN, CLIQUE and GDH. This work develops a new clustering algorithm named G-TREACLE for data mining. It can accurately identifies large patterns that are close to each other by using tree-alike pattern and is capable of successfully eliminate edge indention, so that it may improve the clustering performance of large databases as well as eliminate outliers. In addition, simulation results demonstrate that the proposed new clustering approach performs better than some existing well-known methods such as the K-means, DBSCAN, CLIQUE and GDH algorithms.
 Acknowledgments. The authors would like to thank the National Science Council of the Republic of China, Taiwan for financially supporting this research under Contract No. NS C 96-2221-E-020-027.

