 1. Introduction cally, compared to previous work, our proposal is able to deal with the following complexities. 1.1. Sentence relations three types of sentence relation naturally (Section 2.1 ). 1.2. Interactions among questions with the use of the structural SVMs) performs the extraction for multiple questions in a single pass. 1.3. Thread structure itly utilized in Cong et al. (2008) and Ding et al. (2008) . 1.4. Performance preference et al., 2008 ) when adapted to different system requirements.
 The contributions of this paper can be summarized as follows.
 threads, which are not fully exploited in ( Cong et al., 2008; Ding et al., 2008 ). wider choice than the previous Conditional Random Field (CRF) based model ( Ding et al., 2008 ). performance measures.
 2. Problem formalization representation and then on the basis of the new representation introduce a structural model. graphs) as well. 2.1. Notations and graphical representation
Assume a given thread contains p posts { p 1 , ... , p p }, which are authored by a set of users { u u
Among the n sentences, m question sentences q  X f x q 1 ; ... ; x m elements f y 1 ; q and y . j .
 ple, the question sentences are q ={ x 1 , x 2 , x 3 } and the nodes { y to model interactions among questions, too.
 2.2. Structural model tences and m identified questions. y represents the m n label matrix to be predicted. scenario. We focus on hypothesis functions that take the form h  X  x ; w  X  X  argmax eral, it has theoretically proved convergence in polynomial time ( Joachims et al., 2009 ). respectively. 3. Encoding relations and answer extraction can be defined as follows, where the sub-mappings W n ( x , y ), W h ( x , y ) and W sentence).
 denote the post identities and the author identities for the n sentences as p x  X  p x can formally define the candidate sets for y i . as
Below, we formally describe the definitions of the three feature sub-mappings. 3.1. The node feature mapping W n ( x , y ) encodes the relations between sentence and label pairs, we define it as follows, where w n ( x j , y ij ) is a feature mapping for a given sentence x where denotes a tensor product, and / q coded sentence and its details are described in Table 2 . K ( y where k C ( y ij ) is equal to one if y ij = C , otherwise zero. k ( x j , y ij ) for y ij = C one gets,
Note that the node feature mapping does not incorporate the relations between sentences. 3.2. The horizontal edge feature mapping context and answer sentences. It can be defined as follows, swer edges, respectively. Their formal definitions are given as follows, question and plain. Note that we use complete context X  X nswer ( skipping ) edges in W 3.3. The vertical label group feature mapping W v ( x , y ) is defined as follows, where w v ( x j , y . j ) encodes each label group pattern into a vector. 4. Structural SVMs and inference
OP1 . (1-Slack Structural SVM ) taridis et al. (2005) and Joachims et al. (2009) .
 to solving algorithms are essential for the success of customizing structural SVMs to our problem. 4.1. Exact inference
The exact inference algorithm is designed for a simplified model with only two sub-mappings W does not include W v ).
 for each row of the label matrix. However, it would be intractable for large candidate sets. tence. In most forums, a post contains a couple of sentences which is tractable for 2 Viterbi-like algorithm to efficiently select the most violated constraint.
Fig. 2 . This graph transform merges all the nodes in the context candidate set to one node with 2 posed algorithm ( Algorithm 1 ) can be summarized in three steps: (1) enumerate all the 2 (lines 6 and 7).
 requires that ( a ) the loss function should be decomposable such that D  X  y
W v is excluded from the model . 4.2. Approximate inference
The exact inference cannot handle the complete model with three sub-mappings, W erating approaches ( Finley &amp; Joachims, 2008 ).

Algorithm 1. Exact Inference Algorithm 1: Input:  X  C i ; A i  X  for each q i , w , x , y 2: for i 2 {1, ... , m } do 3: for C s # C i do 4:  X  p  X  C s  X  ; ^ y i :  X  C s  X  Viterbi  X  w ; x ; C s  X  5: end for 6: C s  X  argmax C s # C i p  X  C s  X  7: ^ y i :  X  ^ y i :  X  C s  X  8: end for 9: return ^ y during the last pass. This indicates that at least a local optimal solution is obtained.
Algorithm 2. Greedy Inference Algorithm 1: Input: w , x , y 2: initialize solution: ^ y y 0 3: repeat 4: y 0 ^ y 5: for i 2 {1, ... , m } do 6: for j 2 {1, ... , n } do 7: ^ y ij argmax ^ y ij w T W  X  x ; ^ y  X  X  D  X  y ; ^ y  X  8: ^ y ij ^ y ij 9: end for 10: end for 11: until ^ y  X  y 0 12: ^ y ^ y 13: return ^ y first add three variables, y C ij ; y A ij and y P ij for each label in the label matrix. If y where / ij ( L ) denotes the decomposed scores for node features or the decomposed loss, and / summarized as follows: P P one. 5. Loss functions this section, we introduce the loss functions used in our work. 5.1. Basic loss function where I [.] is an indicative function that equals to one if the condition holds and zero otherwise. 5.2. Sequence-based loss function tion as follows: dicted labels (including context s and answer s) and the true labels are treated as two sequences. 5.3. Recall-vs-precision loss function precision, where y ij  X  P (recall that P denotes the label  X  X lain X ) means that y ( y  X  X lain X  texts. Obviously, this type of error decreases recall. Similarly, I  X  y note the loss function with c p / c r = 2 and that with c
Section 4.2 and train the structural SVMs. 6. Experiments swers of questions. 6.1. Experimental setup 6.1.1. Data set that questions are easier to annotate while it is more difficult to link answers with questions. of sentences because one sentence can be annotated with multiple labels. 6.1.2. Experiment details
We calculated the standard precision ( P ), recall ( R ) and F cross validation. 6.2. Baseline methods non-answer), respectively. We used the feature mapping / q detail, the multiclass SVMs learn the weight vector w which predicts x 6.3. Experimental results
In Table 5 , S-SVM represents the structural SVMs only using the node features W S-SVM-HC.
 improve the baseline methods binary SVMs by about 6% and 20% in terms of F 6.3.1. The use of different loss functions based loss function ( D d ), recall-vs-precision loss function ( D swer loss function ( D c c favoring recall about context extraction and D section, we will verify the flexibility by testing the loss functions empirically. tures as S-SVM-HCV but are trained on different loss functions.
 From Table 6 , we can observe that: matrix in Table 6 .

The method denoted by D p p does well in precision and the method denoted by D expect.

Fig. 3 illustrates the capability of the loss function D p c and c r in Eq. (6) . The horizontal axis of Fig. 3 represents log( c comes larger, the precision increases but the recall decreases; and vice versa. 7. Related work dents X  query with reply posts from an annotated corpus of archived threaded discussions. questions, but not those across all the questions. 8. Conclusion and future work of questions from online forums and then customized a structural SVM approach to solve it. adapt our model to fit for different application requirements.
 the requirements of different applications.
 from online forums.
 References
