 This paper evaluates undergraduate students X  knowledge, interests and experiences with 20 topics from the TREC Robust Track collection. The goal is to charact erize these topics along several dimensions to help researchers make more informed decisions about which topics are most appr opriate to use in experimental IIR evaluations with undergraduate student subjects. H.1.2 [Information Storage and Retr ieval]: Models and Principles -User/Machine Systems -Human factors. Design, Experimentati on, Human Factors. Topic selection, task difficulty , motivation, assigned tasks. Interactive information retrieva l (IIR) experiments frequently use available test collections, such as TREC collections, which have predetermined topics for users. These collections usually contain a large number of topics (~50), but in studies with human subjects it is not always feasible that all topics can be searched. Instead, researchers select a smaller set of topics for use in their studies (~4-8). However, there is little guidance on how to select these topics and little information about the topics other than the number of relevant documents that exist in the corpus. Often researchers select topics inform ally, or through examination of corpus-specific measures of difficulty. While research has shown that subject motivation is an im portant issue, especially with assigned search tasks, and that researchers should tailor tasks and topics to the interests of target s ubjects [1], the selection of topics is rarely more than a cursory exercise constrained by available choices and information rather than careful selection by the experimenter. This paper eval uates 20 topics from the TREC Robust Track [6] by collecting data from undergraduate student subjects  X  a typical subject type in IIR experiments  X  about the topics themselves in order to find topics that are relevant to that particular group. The results can be used to inform future topic selection as well as provide a starting point for additional study. We used the TREC Robust Track co llection [6], which consists of a 3GB corpus of newswire text, 50 topics and relevance judgments. We selected 20 topics by ranking the topics using normalized discounted cumulated ga in (nDCG) [3] at depth 50 for queries that were collected from subjects in a prior study [4] that used all 50 topics. The number of topics examined was based on having enough for meaningful comparisons while keeping the experiment within a reasonable size. The nDCG values were averaged for each topic and topics were sorted into four bins: easy, medium, moderate, and difficult. Five topics from each bin were selected by considering targ et subjects X  potential interests in the topics and the number of rele vant documents in the corpus (these are selection heuristics we would like to move beyond, but as a starting point we used them to narrow the set) (Table 1). 55 subjects participated in this study (33 females, 22 males), with a mean age of 21 years. All were undergraduates except one; a range of majors were represented across the humanities, sciences and social sciences. Subjects were given 4 topics, one of each difficulty level, and asked to fi nd and save relevant documents (15-minutes per topic). Subject s used two experimental IIR systems. These systems recommended queries and terms to subjects and were built on top of the Lemur IR toolkit. The basic KL-divergence retrieval model was used for retrieval and ranking. Details about these systems can be found in [5]. Topics were rotated across subjects and counter-balanced across system. Subjects completed pre-search and post-search questionnaire items (Table 2) for each topic. The pre-search items were: How much do you know about this topic? How difficult do you think it will be to find relevant documents for this topic? How relevant is this topic to your life? and How interested are you to learn more about this topic? The post-search items asked subjects to describe experienced difficulty (How difficult was it to find relevant documents for the topic?) and their affective states, measured using Csikszentmihalyi  X  X  flow [2] and operationalized using three standard items. These items were: How would you rate your experiences searching fo r information about the topic? How would you rate your mood while you searched? How hard was it to concentrate while you searched? Table 2 shows subjects X  responses to the pre-search and post-search items for each topic. In general, subjects indicated that most topics were not relevant to their lives. Topics that were rated as most relevant to subject s X  lives were airport security (341) and tropical storms (408). The topic that was rated least relevant to subjects X  lives wa s international art crime (322). Despite these low ratings, subjects indicated that they were interested in learning more about some of the topics, but this was correlated significantly with relevance ( r =.557, p &lt; .05). Subjects were most interested to learn more about mental illness drugs (383) and curbing population growth (435). Subjects rated the expected difficulty of most topics around the midpoint; these ratings were not correlated with nDCG. However, subjects X  post-search ratings of difficulty we re more variable and were correlated positively with nDCG ( r =.475, p &lt; .05). Post-search difficulty also correlated positively with subjects X  ratings of the search experience ( r =.487, p &lt; .05). In general, subjects did not know a lot about the topics. Prior topic knowledge correlated positiv ely with many measures, including pre-task difficulty ( r =.799, p &lt; .01), relevance ( r =.636, p &lt; .01), and interest ( r =.729, p &lt; .01). There were also significant correlations between it and flow measures of mood particularly interesting when noting that the scores for prior topic knowledge are very low for many of the selected topics. There were also correlations between the post-search measures of topic difficulty and search experience ( r =.751, p &lt; .01), mood ( r =.651, knowledge and post-search diffi culty correlate with these measures of flow, but did not directly correlate with each other. We report preliminary results of undergraduate student subjects X  evaluations of 20 topics from the TREC Robust Track collection. The use of test collections is common in many IIR evaluations and researchers often need to select topics to assign to subjects. To date, this selection process has been based on rough heuristics and assumptions about what might a ppeal to target subjects. The goal of this work was to provide additional information about the topics contained in the TREC Robust Track collection (a collection that is often used in IIR studies) and, in particular, information about difficulty, app eal and engagement. Our hope is that this will help researchers ma ke more informed choices about which topics to include in their evaluations. Results for this exploratory study indicate possible uses for nDCG rankings as partial indicators of post-search measures of topic difficulty. This could be particularly useful given that this measure of difficulty also correlated with the affective state of the subjects, as indicated by the measures of flow. [1] Borlund, P. (2000). Experime ntal components for the [2] Csikszentmihalyi, M., &amp; Larson, R. (1987). Validity and [3] J X rvelin, K. &amp; Kek X l X inen, J. (2002). Cumulated gain-based [4] Kelly, D., &amp; Fu, X. (2006). Elicitation of term relevance [5] Kelly, D., Gyllstrom, K., &amp; Bailey, E. W. (2009). A [6] Voorhees, E. M. (2006). Overview of the TREC 2005 
