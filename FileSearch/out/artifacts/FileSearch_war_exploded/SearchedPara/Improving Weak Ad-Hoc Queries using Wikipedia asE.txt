 In an ad-hoc retrieval task, the query is usually short and the user expects to find the relevant documents in the first several result pages. We explored the possibilities of using Wikipedia X  X  articles as an external corpus to expand ad-hoc queries. Results show promising improvements over mea-sures that emphasize on weak queries.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Relevance Feedback, Retrieval Models Algorithms, Experimentation pseudo-relevance feedback, external corpus, Wikipedia
In web retrieval tasks, the number of terms in a query is usually small (two to three on average) [1]. According to [2], if the terms cannot provide enough information of the user X  X  need, the retrieval result may be poor. These are known as weak queries [3]. Also, the relevant documents are likely to be scattered along the retrieval list. In this case, the user may give up after inspecting the first one or two result pages without finding a relevant document. Pseudo Relevance Feedback (PRF) is a well-known method for improving retrieval effectiveness. However, it is based on the assumption that top retrieved documents are relevant, and thus may actually harm performance possibly when the initial retrieval X  X  top ranked documents are irrelevant.
Some previous works have been done to address the issue of weak queries in ad-hoc retrieval. Web assistance and data fusion method [3] probe a web search engine (e.g. Google) to form new queries, and then combine the corresponding retrieval lists. Our experiments, however, use a local reposi-tory of Wikipedia [4] articles as external corpus. New queries are formed by analyzing Wikipedia articles and a second re-trieval on the target corpus is then performed. Results show that retrieval effectiveness, especially for weak queries, are improved.

The TREC Robust Track [5] was started in 2003 to fo-cus on poor performing queries. Several new measures were introduced to evaluate the effectiveness on weak queries. Among them,  X #0p5 X ,  X #0p10 X ,  X #0p15 X , and  X #0p20 X  are the number of queries that have zero precision at top 5, 10, 15, and 20 retrieved documents, respectivety.  X  X rea X  is a weighted sum of the average precision of the 25% worst per-forming queries (e.g. 12 out of 50). The weight is  X  x k = where r is the query rank (weakest has r =1)and x is size of set.  X  X rea X  measures the overall performance of weakest queries in a set and weight weaker ones heavier.

Since 2004, another new measure Geometric MAP (GMAP) [6] was introduced as an alternative to the mean average pre-cision (MAP). GMAP takes the geometric mean of average precisions of all the queries instead of their arithmetic mean, in order to emphasize scores close to 0.

The following table shows a comparison between our ini-tial (BM term weights [7]) and PRF results, on the TREC Robust 2005 Track. 40 terms are picked from the top 20 documents for PRF query expansion. As can be seen from the figures, although PRF improves MAP significantly, most other measures favoring weak queries are lower.
The performances of certain queries are significantly harmed by PRF. Since PRF is based on the assumption that top N documents are relevant, when the initial retrieval is unsatis-factory, the top N documents are likely to be irrelevant and may produce ineffective PRF terms.
Wikipedia is a multilingual, web-based, free-content ency-clopedia. Its articles are contributed by users worldwide via Internet. Therefore, unlike tra ditional encyclopedias, the articles in Wikipedia are constantly being updated and new entries are created everyday. Wikipedia also has detailed guidelines governing its content quality. These characteris-tics make it an ideal repository as background knowledge.
All the articles in Wikipedia are available for download through their weekly data dumps. We keep a local copy of such data for faster access. We indexed the articles using the Indri search engine [8]. Retrieval is performed using the Markov Random Field model for Term Dependencies [9] following the query formulation in [10]. 100 articles are retrieved for each query.

The Wikipedia articles are stored in their own markup language called Wikitext which preserves features such as categories, hyperlinks, subsections, tables, pictures, etc. We utilized the categorical information in each article to re-rank the retrieval list. Each category c is given a weight W which equals to the number of articles in the retrieval list that belongs to c . Each article d is then given a weight W
S d =  X  where S d is the original score and  X  is a constant equals to 0.8. The 100 articles are re-ranked according to S d and 40 terms are picked from the top 20 articles to expand the query. New retrieval is performed and shown as Wiki below. Although PRF outperforms Wiki in MAP, Wiki beats PRF in most other measures favoring weak queries. A look into the per query result shows that, among the 15 queries that are hurt by PRF, 14 are improved by Wiki. For the other 35 queries, PRF and Wiki both produced better result over initial retrieval, though Wiki is not as effective as PRF. This is probably because of the differences in language and context between Wikipedia (up-to-date general knowledge Encyclopedia) and the target corpus (news archive within a time range). Also, since the size of Wikipedia is limited, some topics may not be covered well. GMAP .0761 .0256 .1042 .1756 .2645 .2157
For the 15 queries hurt by PRF, Wilcoxon signed-ranks test with p&lt; 0 . 05 shows significant increases of Wiki over PRF. For the rest, the test shows that both have significant increases over initial retrieval.

Similarly, there are also 15 queries hurt by Wiki. The dif-ference between those hurt by PRF, however, is that these queries performed well in the initial retrieval, with MAP = 0 . 2094, and GM AP =0 . 1279. Manually inspecting the Wikipedia articles reveals that some queries are not well covered while for the other queries, term selection failed to pick a good set of terms for expansion.

The above result shows two possible ways for improve-ment. The first is to identify topics that are not well covered in Wikipedia. This could be done by analyzing the top N retrieved Wiki articles. The second is to integrate Wiki and PRF methods, hopefully to eliminate queries hurt by either.
Many other works have used external corpus to improve retrieval. However, many of them use large corpus based on the belief that the likelihood of finding good expansion terms increases as the database size increases. Some probe the web which have billions of documents. Diaz and Met-zler [11] stated that the quality of the corpus also matters, as they were able to significantly improve the TREC Ro-bust 2005 track [10] with the BIGNEWS corpus (6.4 million documents), which consists of similar content (both news articles). Wikipedia, with 1.6 million documents at the mo-ment, is much smaller than BIGNEWS. However, with all the policies and guidelines regulating Wikipedia, as well as the constant addition of articles, we believe it can be a high quality corpus for general purpose term expansion. Also, with most Wikitext markup features (e.g. links, sections, references) remain unexplored, it certainly has a lot of po-tential. The challenge is how to make use of such features to find the information accurately and select terms more effectively.
We explored Wikipedia as external corpus to expand the query. Wikipedia is especially useful to improve weak queries which PRF is unable to improve. Despite its comparatively smaller size, we believe the content quality and the evolving nature make it a good resource. In the future, we will look for more effective ways to find information and integrate it with existing systems. [1] A. Spink, B. J. Jansen, D. Wolfram, and T. Saracevic. [2] C. Buckley. Why current IR engines fail. In SIGIR , [3] K.L. Kwok, L. Grunfeld and P. Deng. Improving weak [4] Wikipedia, the free encyclopedia. [5] E.M. Voorhees. Overview of TREC 2003. In TREC , [6] S.E. Robertson. On GMAP: and other [7] S.E. Robertson and S. Walker. Some simple effective [8] T. Strohman, D. Metzler, H. Turtle and W.B. Croft. [9] D. Metzler and W.B. Cro ft. A markov random field [10] D. Metzler, F. Diaz, T. Strohman, and W.B. Croft. [11] F. Diaz and D. Metzler. Improving the estimation of
