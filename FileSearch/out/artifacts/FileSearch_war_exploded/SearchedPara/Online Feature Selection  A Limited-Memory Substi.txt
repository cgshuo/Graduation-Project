 This paper considers the feature selection scenario where only a few features are accessible at any time point. For example, features are generated sequentially and visible one by one. Therefore, one has to make an online decision to identify key features after all features are only scanned once or twice. The optimization based approach is a powerful tool for the online feature selection.

However, most existing optimization based algorithms ex-plicitly or implicitly adopt L 1 norm regularization to identify important features, and suffer two main disadvantages: 1) the penalty term for L 1 norm term is hard to choose; and 2) the memory usage is hard to control or predict. To overcome these two drawbacks, this paper proposes a limited-memory and model parameter free online feature selection algorithm, namely online substitution (OS) algorithm. To improve the selection efficiency, an asynchronous parallel extension for OS (Asy-OS) is proposed. Convergence guarantees are pro-vided for both algorithms. Empirical study suggests that the performance of OS and Asy-OS is comparable to the bench-mark algorithm Grafting, but requires much less memory cost and can be easily extended to the parallel implementa-tion.
 Feature Selection; Online Learning; Asynchronous Parallel Optimization Feature selection plays a key role in many learning and mining tasks [12]. Substantial research efforts have focused on the batch selection, where all features are assumed to be accessible at any time point and can be accessed for arbi-However, the batch selection may meet the bottleneck in terms of computation and memory for the big data appli-cation, while the ultimately wanted features only occupy a tiny space. Some works [18, 28] can handle large number of features, but all the features are assumed to be accessible in the selection process.

Online feature selection [14, 32] relaxes the requirement in the batch selection and makes a series of decisions to identify key features, to fit important applications where features cannot be accessed at one time, for example,  X  features are too many to store locally;  X  features can only be queried remotely and one needs to identify important features after scanning all the features one time immediately.

There are usually two types of online feature selection algorithms: statistical algorithms [32, 23, 33, 22] and opti-mization based algorithms (or embedded methods) [14, 34], which have different application scenarios. The statistical al-gorithms usually do not have a given objective and features are selected based on certain statistical quantity, e.g. mu-tual information or correlation. Thus, selected features can be used for many different tasks, but are usually sub-optimal for any specific task (e.g. a given objective function).
This paper mainly focuses on the optimization based ap-proaches, which are target oriented with a clear objective (e.g. loss function) in feature selection, for example, mini-mizing square loss or classification error. The key difference to statistical methods is that optimization based methods give different selection result for different objective.
Most existing optimization based algorithms such as Graft-ing [14] or its variation [34] essentially solve an L minimization problem similar to batch methods, for exam-ple, LASSO [19] and L 1 logistic regression [12]. There are two main drawbacks in practice: 1) As pointed out in [21], it is hard to choose the model parameter (that is, L 1 norm penalty) for online methods. The hyper-parameter in batch methods is usually decided by cross validation (CV). How-ever, the CV strategy is unavailable for the online feature selection scenario, since we can only see a few features. 2) The L 1 norm based online method cannot strictly control the sparsity of the solution path. In the worst case, the required space is comparable to the batch methods.

To overcome these two practical disadvantages, this pa-per proposes an online feature selection algorithm, namely online substitution (OS) algorithm. OS essentially aims at solving an L 0 norm constraint problem: where X  X  R n  X  p is a feature matrix of n samples with p features, y  X  R n is the label vector, and s is the total number of features we want to select. Comparing to Grafting types approaches, it has the following advantages:  X  (Limited-memory) The memory usage is strictly controlled up to O ( ns ). The memory restriction has a very special meaning for big data applications. Imagine the following scenario: Given p = 2 M features and n = 0 . 5 M samples, one wants to select top s = 100 important features. Using the batch method, one needs terabytes of space to save it and solve an optimization to identify top 100 key features using the batch selection. Typically, this work needs to involve parallel computation to solve the batch problem (e.g., L 1 norm minimization). If one can reduce the mem-ory cost to s  X  n  X  1GB to obtain top 100 features, the whole work can be done on a single PC.  X  (Model parameter free) One only needs to specify the to-tal number of desired features s (that is much easier to decide the L 1 norm penalty), thus it avoids the dilemma of setting hyper parameters (depending all features).
To improve the computational efficiency in big data appli-cation, we propose an asynchronous parallel variation for OS (Asy-OS). Asynchronous parallelism is proven to be more ef-ficient than synchronous parallelism in solving many large scale problems in deep learning, sparse learning, etc. It is mainly because that asynchronous parallelism substantially reduces the communication and coordination cost which can-not be avoided in synchronous parallelism. This paper ap-plies this efficient scheme to parallelize the proposed OS al-gorithm. Besides of improving the computational efficiency, the memory cost can be shared by multiple machines in Asy-OS, which is able to tackle even larger scale applications.
The contribution of our work can be summarized as fol-lows:  X  We propose a limited-memory and model parameter free online feature selection algorithm, which avoids disadvan-tages of some benchmark algorithms and can be easily extended to the parallel implementation;  X  We propose a novel asynchronous parallel algorithm to improve the efficiency of the proposed OS algorithm, while only slightly sacrificing the accuracy. To the best of our knowledge, this is the first parallel algorithm for online feature selection.  X  Theoretical convergence guarantees are provided for both algorithms.
 Throughout this paper, we use the following notations and definitions.  X  n is the number of samples.  X  p is the number of features.  X  y ( i ) is the label for the i -th sample.  X  x ( i )  X  R p is a column vector representing i -th sample.  X  x j  X  R n is a column vector representing the j -th feature.  X  X  X  R n  X  p is the data matrix: X := [ x 1 ,  X  X  X  , x  X  S  X  X  1 ,  X  X  X  ,p } is the index set of features, and S denotes its complementary set.  X  X S is the matrix containing features in set S .  X  w is the p dimensional vector containing coefficients of all the features.  X  w j  X  R p is the coefficient of the j -th feature.  X  w S is the coefficients of the features in set S .  X  w k is the coefficient vector in the k -th iteration.
Our work relates to topics including batch feature selec-tion, streaming feature selection, and asynchronous parallel optimization. In this section, we will introduce methods on all the areas and emphasize the ones which are closely re-lated to ours.
Batch feature selection is the prototype used in most tra-ditional feature selection methods. The entire set of fea-tures is given before doing selection. Most batch feature selection methods can be classified as statistical methods or optimization methods. Statistical methods select features according to certain criteria which are usually built based on statistical rules. For example, method based on mutual information [13], selecting features based on relevance and redundancy [24], and method based on dependency [16].
Optimization based feature selection methods are also called embedded methods in some literature. Its basic idea is doing feature selection and learning the model concurrently. These methods usually have a linear model w , and require it to be sparse, i.e. k w k 0  X  s . Different feature selection methods handle the sparsity requirement differently. For instance, L 1 regularization based methods [19, 12, 3] relax the hard constraint to L 1 penalty, and transform the problem to be convex. Besides using an overall sparse regularization, there are many works [25, 2, 6] proposed for structured sparsity. Unsupervised objective is also used in some works [8, 9]. However, we cannot directly control the number of selected features by using the L 1 regularized methods. Different with the original L 0 constraint, they use a regularization parame-ter  X  to control the sparsity, but there is no explicit relation between s and  X  , which means we need to tune  X  if we want to get a reasonable result, i.e. will not select too much or too little features.

An alternative way to handle the sparsity requirement is optimizing the problem just with hard L 0 constraint [5, 26]. Although this leads to an NP-hard problem and we may never get the global optimal solution, there is still theoreti-cal guarantee for the error bound [26]. The basic approach of these methods is projected gradient descent. Thanks to the simplicity of projection onto L 0 ball, their efficiency is somehow satisfactory.
The basic problem setting of streaming feature selection is we only have access to a small part of features, e.g., the input feature. Previously input features are allowed to be retained, but we also should consider the limit on memory. This ob-stacle is an issue when feature selection method considers the relation between different features. Many streaming feature selection methods can be seen as extension of some batch feature selection approach, so they also belong to statisti-cal methods or optimization methods. For example, Zhou et.al. [32] proposed the alpha-investing criterion to select features. OSFS [22] dynamically selects the features based on the online analysis and the online redundancy analysis. Both of their criteria are based on statistical quantities.
In this paper, we focus on optimization based streaming feature selection methods. Grafting method [14] is built based on the L 1 regularized formulation, and it can be used in any problem with L 1 regularization theoretically. It mainly select features at two time points. When the feature just come, grafting test it with a simple criterion and reject it if it fails. When the feature pass the test, grafting will include it into the other selected features and solve a small scale batch sparse learning problem, finally only the features with nonzero coefficient can be retained. Since grafting is actu-ally the online version of L 1 regularization method, it has all the problems of L 1 based method. What X  X  worse, tuning  X  seems harder since we even cannot access all the features, and we do not know how much space we need in the process of feature selection. Besides that, it lacks theoretical analy-sis about convergence rate and error bound with respect to the number of iteration. It also has another practical prob-lem that it takes too much time if the feature pass the test, since a full optimization need to be conduct. This defect has been pointed out and improved by grafting-light [34].
Asynchronous parallelism is a new parallel mechanism to speedup the optimization efficiency and received broad at-tention recently. It avoids the coordination and synchroniza-tion cost comparing the traditional synchronous parallelism and received remarkable successes in optimization and ma-chine learning for solving deep learning [7, 30, 31], SVM [11], linear programming [17], linear equations [1], LASSO [10], matrix completion [15], and many others [29].
The online feature selection problem considered in this paper can be formally defined in the following: while fea-tures come one by one, given a memory budget and a target (e.g., sample labels or measurements on samples), we need to decide what features to retain.
 The loss function L in (1) can be in many different forms. For linear regression, the most common L is in the squared loss form For classification task, it could be the squared hinge loss where y ( i )  X  X  X  1 , 1 } . L could also be logistic regression loss, SVM regression loss, etc. In this section, we do not restrict the form of L .

There are several methods [5, 26] that work on analyzing and solving problem (1). The main idea of their algorithm is projected gradient descent (Proj-GD), which makes a nor-mal gradient descent step and projects the current point onto the L 0 ball. Yuan et.al. [26] derive the convergence rate and error bound for general convex and smooth loss function. In a word, Proj-GD works very well on the L constrained problem.

However, in order to use Proj-GD to do feature selection, we must have the access to all the features. For streaming feature selection, grafting [14] and grafting-light [34] uses the stage wise strategy to update the model w . Since they are all based on the L 1 regularized formulation, it is easy to guarantee descent on objective value, and the global op-timum can be attained if we can scan all the features again and again. But L 0 constraint based formulation is another story, even batch method Proj-GD can only get the approx-imate solution [26]. Furthermore, how can we guarantee descent and convergence is not clear.
The basic reason that we cannot do Proj-GD for stream-ing feature selection is that we can nether get the gradient, nor update the coefficients for the unseen feature. We are only allowed to access couple of features at the same time, which are actually the new coming feature, and some fea-tures temporarily retained. Since new features keep coming, and we cannot retain all of them, this means we need to make decision on rejecting or accepting features in this process.
We propose a method which is motivated by Proj-GD. We maintain a set S containing features temporarily retained. The maximum size of S is s . When S is not full, new features are always accepted. If the set S is full, we have to reject an existing feature in S if we decide to accept a new feature, that is, the new feature will substitute the old feature. We call this process  X  X nline substitution X . The criteria for sub-stitution is comparing the potential of the new feature with the worst features in S . At iteration k , if the coming feature has index j , then the procedure is: 1. Update coefficients of the retained features with step length feature with step length  X  : w j =  X   X   X  j L ; 2. Project w onto  X ( s ): w = P  X ( s ) ( w ). where m  X  1 is a parameter, P  X ( s ) ( w ) means projecting w onto the set  X ( s ) := { w |k w k 0  X  s } , that is, retain top k largest (in the sense of magnitude) elements in w and set the rest to zero. In section 4.2, We will see that the parameter m is useful to the convergence guarantee of our asynchronous parallel method. Since w in step 2 has at most s + 1 nonzero elements, so the projection step is just setting the minimum (in magnitude) nonzero element as zero.

The pseudo code is shown in Algorithm 1. From the al-gorithm we can see that the partial gradient  X  S L is written a function of u , for least square loss (2), it is: and for the squared hinge loss (3), it is where  X  means element-wise multiplication. Because we only make one partial gradient step (W.R.T. S  X  X  j } ), our method OS is much efficient for handling each Algorithm 1: Online Substitution (OS) algorithm.
 Data : Label y .

Result : Set S consisting of selected features. set S =  X  ; repeat 3 Receive a feature x j from the pool S with index j ; 4 u := X S w S ; 7 update S = S  X  X  j } ; 8 if | S | &gt; s then 9 w k  X  = 0, S = S \{ k  X  } , where until Reach convergence ; coming feature when compare with grafting. However, if the new features are input too frequently, the online substitution speed may not catch up its speed. One promising way to improve the efficiency is using parallel optimization. We extend our method OS to an asynchronous method.

The asynchronous parallelism implementation has a sim-ilar procedure with Algorithm 1, but here we have multiple workers selecting features. Each worker t uses set S ( t ) with size s/q , where q is the number of workers. In addition, computing the gradient needs information from other work-ers, we can use a central machine to collect all updates in different workers as in Algorithm 2. The procedure for the central machine is very simple: it just receives all the  X  u ( t ) sent by each worker t , and add it to the central state variable u C , which actually represents X w .

The procedure for workers is shown in Algorithm 3. Firstly, pull the central state variable u C from the central machine, then save the current local state variable u ( t ). Secondly, use u C to compute the partial gradient W.R.T. w S ( t ) and w , and perform update according to the gradient and pro-jection. At last, calculate the difference variable  X  u ( t ) and send it to the central machine. In the whole procedure of central machine and workers, there is no synchronization process.

Algorithm 2: Asy-OS: procedure of the central ma-chine. set u C = 0 ; repeat 3 if Receive  X  u ( t ) from a certain worker t . then 4 u C = u C +  X  u ( t ); 5 end until Workers stop pushing  X  u ( t );
In this section, we show the main result of our theoreti-cal analysis, including convergence of OS and Asy-OS algo-rithm, and the property of their local minimum. Proofs are provided in the Appendix A.

Firstly, we make some certain Lipschitzian assumptions to prepare the following theoretical analysis. Lipschitzian assumptions are commonly used in analyzing optimization algorithms. Define function f ( w ) as f ( w ) := L ( X w , y ) and Algorithm 3: Asy-OS: procedure of worker t .
 Data : Label y .

Result : Set S ( t ) consisting of selected features. set S ( t ) =  X  ; repeat 3 Receive a feature x j from the pool S with index j ; 4 Read u C from the central machine; 8 update S ( t ) = S ( t )  X  X  j } ; 9 if | S ( t ) | &gt; s/q then 10 w k  X  = 0, S ( t ) = S ( t ) \{ k  X  } , where 12 Push  X  u ( t ) = X S ( t ) w S ( t )  X  u ( t ) to the central until Reach convergence ; construct function F w ( t ,r ) by: where t  X  R | S | , r  X  R . Assume we have:
Assumption 1. (Lipschitz Gradient.) There exists con-stant L F &lt; +  X  which satisfies:  X  t  X  R | S | and r  X   X  X  X  X  S F w k ( 0 , 0) , t  X  + r  X  j F w k ( 0 , 0) + L F There exists a constant L f &lt; +  X  satisfying:  X  u , v  X 
Then we are ready to present the convergence rate for the proposed OS algorithm (Algorithm 1) with the following Theorem:
Theorem 1. For K iterations, the average distance of w between two successive iterations in the OS algorithm satis-fies when  X  &lt; 1 /L F establishes.
 The left hand side of (8) is nothing but the k X  f ( w if using gradient descent to solve an unconstrained opti-mization. Theorem 1 basically suggests that the sequence k w k +1  X  w k k 2 converges and its average rate is O (1 /k ). It is also worth to point out that the optimal choice for  X  is 0 . 5 /L F . Next we establish the convergence of the proposed Asy-OS algorithm. For the asynchronous version in Algorithm 2 and 3, the convergence is guaranteed under some conditions. We assume that we have bounded staleness, i.e.
Assumption 2. (Bounded staleness.) In one update iteration of any worker, the central state variable u not update more than  X  max times. Thus u C updates at most  X  max times between the each pair of line 4 and line 12 in Algorithm 3.
 Formally, we have the following theorem:
Theorem 2. The average distance of two successive it-erations the Asy-OS algorithm with q workers converges in the following sense: if the step length  X  is appropriately chosen such that  X  := 1 Note that w consists of all disjoint pieces from q workers and the iteration counts the change happening to w in any single worker. Form the above theorem we can see the relation between the step length  X  and the staleness bound  X  Larger  X  max requires that  X  to be smaller. Assume that we have the following Restricted Strong Convexity assumption:
Assumption 3. There exists  X   X  ( s ) &gt; 0 which satisfies: Then our local optimum has the property
Theorem 3. If w  X  is the local optimal point that we get by our method. Then where w  X  is the global optimal point. Theorem 3 shows that our local minimum is better with smaller value of  X  , which also has a relationship with step length  X  . Larger step length  X  leads to better local optimum. However, as shown in Theorem 1 and 2, there is an upper bound for  X  to guarantee convergence.
In this section, we demonstrate the empirical performance and the effects of some parameters. The experiments are conducted on two different models: linear regression and hinge loss classification.

Linear regression. Feature selection in linear regres-sion aims at recovering the sparse vector w  X   X  R p , given the training data X  X  R n  X  p and the corresponding observations y = X w  X  + , where  X  R n is the noise. In our experiment, the training data X are generated from standard i.i.d. Gaus-sian distribution. We force the ground truth vector w  X  to be sparse, i.e. k w  X  k 0  X  s . The nonzero positions of w are randomly selected and their values follow the standard i.i.d. Gaussian distribution. Noise is generated from i.i.d. Gaussian distribution with mean 0 and variance 0 . 1 2 . We adopt the cost function (2) for linear regression.

Hinge loss classification. Besides regression, we also try our method for classification model. The data matrix X and the sparse ground truth vector w  X  are generated exactly the same way as in linear regression settings. Class labels y  X  { X  1 , +1 } n are generated by taking the sign of X w Another data matrix X test  X  R n  X  p and vector y test are gen-erated exactly the same way as X and y . We use them as the test data. We adopt the squared hinge loss function (3).
In addition, we also test our method on the privacy image classification problem [27]. Its target is to distinguish im-ages which could contain some privacy information from the public images. In the experiment, we use a collected data set in [20] with roughly 3400 images in both classes i.e. privacy and public. We combine them as a dataset with 6914 im-ages, and randomly select one percent data as training set, leave the rest as testing set. Each image is represented by 7488 features generated by several image feature extraction methods including color histogram, linear binary pattern, histogram of oriented gradients, etc.

Baseline Methods. To demonstrate our online feature selection method, we compare it with L 0 constrained batch method (Proj-GD), L 1 regularized batch method i.e. LASSO and L 1 hinge loss classification, grafting method, and two naive online implementations which are based on coordi-nate descent method and projected or proximal operation. We adopt proximal gradient descent to solve LASSO, and use the implementation of LIBLINEAR [4] to solve L 1 hinge loss classification.

Performance Measure. To compare the performance, we use feature selection recall, estimation error and classi-fication error. Feature selection recall is the ratio that cor-rectly selected features (at most s ) over the total amount of used features in the true model. For linear regression, we show the recovery error which is the normalized distance between the recovered model and the true model. For classi-fication problem, we evaluate the classification error. For all the methods, we use the selected features to fit the training data again.

To demonstrate the efficiency improvement of our parallel method Asy-OS, we show the running time and training error (i.e. fitting error for linear regression and classification error for hinge loss classification). We also show the speedup compared with the sequential version.

Implementation Details. In our experiment, we focus on methods based on two basic sparsity formulations: L constraint and L 1 regularization. For L 0 methods including our method OS, the objective function is just equation (1). The L 1 regularized formulation is min w L +  X  k w k 1 . In this formulation, L is the specific loss function, i.e. L r in (2) or L c in (3). L 1 regularization parameter  X  is set to guarantee the batch methods selecting at least s features.

In our synthetic data experiments, the total number of features p is set to 2000  X  6000 for linear regression and 1000  X  5000 for classification. The number of nonzero ele-ments is set s = 100 both. The number of samples n is set to 1 . 2 s log 2 p . For all the online feature selection method, we make the limitation that we can only scan all the features twice. The results are averaged on 30 repeated experiments with different random data.
In the first column of Figures 1 and 2, we show the per-formance comparison on linear regression and classification problems respectively. We can see that for both problems, L 0 based methods i.e. Proj-GD and our method OS, have higher feature selection recall. For linear regression, our method OS has almost the same performance with the L 0 batch counterpart Proj-GD. But we can find that the es-timation error and the feature selection recall are not very consistent. The L 1 methods also have low recovery error, although they do not have the highest feature selection re-call. In the first column of Figure 4, we can find that the L batch method has higher error when s is small, and other methods are not influenced too much.
One important issue in practice is how many times of scan-ning features is sufficient. To demonstrate this, we test our method by scanning features for different number of times, and compare the performance in the second column of Fig-ures 1, 2 and 4, which suggests that 2 or 3 times would be enough since the error does not clearly improve after that.
In this section, we study the performance of Asy-OS. We first simulate Asy-OS on a single machine, such that the staleness value  X  max can be easily controlled. We set  X  to be the number of workers, that is, each worker uses the information from other workers  X #Workers X  iterations iter-ates ago. From the right column of Figures 1 (for linear regression), 2 (for classification) and 4, we can observe that the recall almost monotonically degrade while the number of workers increases. One interesting finding is that the clas-sification problem seems to be more robust to the staleness (or #Workers).

Next we implement Asy-OS on a computer network con-sisting of 6 machines. We use Message Passing Interface (MPI) to implement the parallel mechanism. The central node (i.e. Algorithm 2) runs on a single machine, and every two worker-processes run on a specific machine. All the ma-chines have the same hardware (Intel Xeon E5-2430 CPU with 24 cores, 64 GB RAM). The data are generated with the similar way but here we set n = 1000 ,p = 8192 ,s = 64 for linear regression and s = 32 for classification. To com-pute the speedup, we set an error threshold and record the time used for attaining such error. We repeat the experi-ment 10 times and report the average result.

In Figure 3, we show the running time of Asy-OS. In the first two sub-figures we can observe that the error decreases rapidly when more workers are involved. When the num-ber of workers is larger than 8, there is almost no improve-ment of efficiency. We think this is caused by two reasons. First, since every worker will communicate with the central machine, the communication cost over the network will in-crease when the number of workers becomes larger. Second, the staleness upper-bound  X  max will be larger when using more workers. In the speedup result, we can find we have more than linear speedup for both applications. The reason is that we separate both the total iterations and the work-load for each iteration. For each iteration, each worker only needs to compute its local X S ( t ) w S ( t ) and retrieves u the central machine (shown in Algorithm 3).
This paper proposes a limited-memory and model parame-ter free online feature selection algorithm, namely OS, which overcomes the disadvantages of existing optimization based online feature selection algorithm such as Grafting and its variation. To improve the computational efficiency and solve problems with huge scale problem, we propose an asyn-chronous parallel OS algorithm. Theoretical convergence analysis is provided for both algorithms. Empirical study suggests that the performance of OS and Asy-OS is com-parable to the benchmark algorithm Grafting, but requires much less memory cost and is much easier to set sparsity parameter.
We thank the reviewers for constructive comments. This project is supported by the NEC fellowship and the NSF grant CNS-1548078. The implementation of Asy-OS signifi-cantly benefits from the first author X  X  course project on Pro-fessor Sandhya Dwarkadas X  X  Parallel and Distributed Sys-tems course at University of Rochester. [1] H. Avron, A. Druinsky, and A. Gupta. Revisiting [2] F. Bach, R. Jenatton, J. Mairal, G. Obozinski, et al. [3] E. Candes and T. Tao. The dantzig selector: [4] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [5] S. Foucart. Hard thresholding pursuit: an algorithm [6] D. Kong, R. Fujimaki, J. Liu, F. Nie, and C. Ding. [7] M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G.
 [8] Z. Li, J. Liu, Y. Yang, X. Zhou, and H. Lu.
 [9] Z. Li and J. Tang. Unsupervised feature selection via [10] J. Liu and S. J. Wright. Asynchronous stochastic [11] J. Liu, S. J. Wright, C. R  X e, V. Bittorf, and S. Sridhar. different number of workers for Asy-OS. result with different number of workers for Asy-OS. [12] A. Y. Ng. Feature selection, l 1 vs. l 2 regularization, [13] H. Peng, F. Long, and C. Ding. Feature selection [14] S. Perkins and J. Theiler. Online feature selection [15] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A [16] L. Song, A. Smola, A. Gretton, K. M. Borgwardt, and [17] S. Sridhar, S. Wright, C. Re, J. Liu, V. Bittorf, and [18] M. Tan, I. W. Tsang, and L. Wang. Towards ultrahigh [19] R. Tibshirani. Regression shrinkage and selection via [20] L. Tran, D. Kong, H. Jin, and J. Liu. Privacy-cnh: A [21] J. Wang, M. Wang, P. Li, L. Liu, Z. Zhao, X. Hu, and [22] X. Wu, K. Yu, H. Wang, and W. Ding. Online [23] K. Yu, X. Wu, W. Ding, and J. Pei. Towards scalable [24] L. Yu and H. Liu. Efficient feature selection via [25] M. Yuan and Y. Lin. Model selection and estimation [26] X.-T. Yuan, P. Li, and T. Zhang. Gradient hard [27] S. Zerr, S. Siersdorfer, J. Hare, and E. Demidova. [28] Y. Zhai, M. Tan, Y. S. Ong, and I. W. Tsang.
 [29] R. Zhang and J. Kwok. Asynchronous distributed [30] S. Zhang, A. E. Choromanska, and Y. LeCun. Deep [31] W. Zhang, S. Gupta, X. Lian, and J. Liu.
 [32] J. Zhou, D. Foster, R. Stine, and L. Ungar. Streaming [33] J. Zhou, D. P. Foster, R. A. Stine, and L. H. Ungar. [34] J. Zhu, N. Lao, and E. P. Xing. Grafting-light: fast,
Define S k as the set of selected features and j k as the new input feature in the ( k + 1)-th iteration. The OS algorithm following the update rule that: Suppose that ( w k +1  X  w k ) S k = t , ( w k +1  X  w k ) j we already know that w k +1 and w k are the same at other positions. So we have: According to inequality (6) from the Assumption 1, we have: f ( w k +1 )  X  f ( w k ) = F w k ( t ,r )  X  F w k ( 0 , 0)  X  X  X  X  S k F w k ( 0 , 0) , t  X  + r  X  j k F w k ( 0 , 0) + L = 1 m + L F  X  L F where the last inequality comes from the fact that Summing up inequality (9) from k = 1 to K , we get Since we know that function f is bounded below, i.e., f ( w ) &gt;  X  X  X  ,  X  w  X   X ( s ). If 1 2  X   X  L F 2 &gt; 0, we get 1 K It completes the proof.

To analyze the proposed asynchronous algorithm, we mon-itor the values of w concatenating all disjoint pieces from q workers. The central node actually records the value of u = X w . To be convenient, we define a vector function where t k is the worker index which makes the update at the ( k + 1)-th iteration, and S k ( t k ) denotes the set of selected features at worker t k .

Then the ( k + 1)-th update happens in the central node follows: where S k ( t k )  X  X  j k } denotes the complementary set of S { j k } .

In the asynchronous parallelism, we can not guarantee  X  w k = w k in general, but we have the following equation when the staleness is limited: where  X  ( k )  X  { k  X  1 ,k  X  2 ,...,k  X   X  max } . Since w w k only differ in S k ( t k )  X  X  j k } , we have the inequality: k w k +1  X  ( w k  X   X g k (  X  w k )) k 2  X k w k  X  ( w k  X   X g so we get: With similar steps of getting inequality (9), we have f ( w k +1 )  X  f ( w k )  X  X  g k ( w k ) , w k +1  X  w k  X  + L F =  X  g k (  X  w k ) , w k +1  X  w k  X  + L F +  X  g k ( w k )  X  g k (  X  w k ) , w k +1  X  w k  X   X  L F where the last inequality uses (10). Introduce  X  &gt; 0 , X  &gt; 0, we obtain: We use U k to denote the union of S k ( t k ): U k =  X  q t =1 Then we take the expectation of T 2 : where the second inequality comes from (7) in Assumption 1. Combine with equation (11) and choose  X  =  X  to be 1 /L F , then So we have  X  X  X  1 Summing (12) from k = 1 to K , we get
E f ( w K +1 )  X  f ( w 1 )  X  X  X  1 +  X  X  X  1 + =  X  1
X If the condition is satisfied, that is, Finally, we get It completes the proof.
 Let us prove a lemma first.

Lemma 4. We have
Proof. From the restricted strong convexity, i.e. As-sumption 3, we have: f (  X  w )  X  f ( w k )  X  X  X  X  f ( w k ) ,  X  w  X  w k  X  +  X   X  where  X   X  = supp(  X  w ) It completes the proof.
 Then we are ready to prove Theorem 3.

Proof. From Lemma 4, we have: f ( w  X  )  X  f ( w  X  )  X  s where | w  X  min | = min i | w  X  i | .

From Assumption 3, we also have: k w  X  k 2  X  2 ( f ( 0 )  X  f ( w then we have (1 +  X  ) ( f ( w  X  )  X  f ( w  X  ))  X   X  ( f ( 0 )  X  f ( w  X  It completes the proof.
