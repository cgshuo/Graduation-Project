
Our work aims to the acquisition of deep gram-matical information for nouns, because having in-formation such as countability and complementa-tion is necessary for different applications, espe-cially for deep analysis grammars, but also for question answering, topic detection and tracking, etc. 
Most successful systems of deep lexical acquisi-tion are based on the idea that distributional fea-tures (i.e. the contexts where words occur) are as-sociated to concrete lexical types. The difficulties are, on the one hand, that some filtering must be applied to get rid of noise, that is, contexts wrongly assessed as cues of a given type and, on the other hand, that for a pretty large number of words, their occurrences in a corpus of any length are very few, making statistical treatment very difficult. 
The phenomenon of noise is related to the fact that one particular context can be a cue of different lexical types. The problem of sparse data is pre-dicted by the Zipfian distribution of words in texts: there is a large number of words likely to occur a very reduced number of times in any corpus. Both of these typical problems are maximized in the case of nouns. 
The aim of the work we present here is to build a learner that can handle noise, but, more interest-ingly, that is able to overcome the problem of sparse data. The learner must predict the correct type both when there is a large number of occur-rences as well as when there are only few occur-rences, by learning on features that maximize gen-eralization capacities of the learner while control-ling overfitting phenomena. We have based our work on two main points. Firstly, we have used morphosyntactic information as features. Secondly, we made the learner deal with all occurrences of a word as a complex unit. In our system, linguistic cues of every occurrence are collected in the signature of the word (more technically a pair lema + part of speech ) in a par-ticular corpus. In the next sections we give further details about the features used, as well as about the use of signatures. 
The rest of the paper is as follows. Section 2 presents an overview of the state of the art in deep lexical acquisition. In section 3, we introduce de-tails about our selection of linguistically motivated cues to be used as features for training a Decision Tree (DT). Section 4 shortly introduces the meth-odology and data used in the experiments whose results are presented in section 5. And in section 6 we conclude by comparing with the published re-sults for similar tasks and we sketch future re-search. 
Most of the work on deep lexical information acquisition has been devoted to verbs. The existing acquisition systems learn very specialized linguis-tic information such as verb subcategorization frame 2 . The results for verb subcategorization are mostly around the 0.8 of precision. Briscoe &amp; Car-roll (1997) reported a type precision of 0,76 and a type recall of 0.43. Their results were improved by the work of Korhonen (2002) with a type precision of 0.87 and a recall of 0.68 using external re-sources to filter noise. Shulte im Walde (2002) re-ports a precision of 0.65 and a recall of 0.58. Chesley &amp; Salmon-Alt (2006) report a precision of 0.86 and a recall of 0.54 for verb subcategorization acquisition for French. 
Lexical acquisition for nouns has been con-cerned mainly with ontological classes and has mainly worked on measuring semantic similarity on the basis of occurrence contexts. As for gram-matical information, the work of Baldwin and Bond (2003) in acquisition of countability features for English nouns also tackles the very important problem of feature selection. Other work like Car-roll and Fang X  X  (2004) and Baldwin X  X  (2005) have focused on grammatical information acquisition for HPSG based computational grammars. The latter is the most similar exercises to our work. Baldwin (2005) reports his better results in terms of type accuracy has been obtained by using syn-tactic information in a chunked and parsed corpus. The type F-scores for the different tested catego-ries for English were: for verbs 0.47, for nouns 0.6 and for adjectives 0.832. 
One of the most important tasks in developing machine learning applications is the selection of the features that leads to the smallest classification error. For our system, we have looked at distribu-tional motivated features that can help in discrimi-nating the different types that we ultimately use to classify words. 
The lexical types used in deep analysis gram-mars are linguistic generalizations drawn from the distributional characteristics of particular sets of words. For the research we present here, we have taken the lexicon of a HPSG-based grammars de-veloped in the LKB platform (Copestake, 2002) for Spanish, similarly to the work of Baldwin (2005). In the LKB grammatical framework, lexical types are defined as a combination of features. Lexical typology of nouns for Spanish, for instance, can be seen as a cross-classification of noun countability vs. mass distinctions, and subcategorization frame or valence, including prepositional selection. For example nouns as  X  X emor X  ( X  X ear X ) and  X  X dicci X n X  n_ppde_pcomp_a_count as they take two com-plements: one with de and the other with a bound preposition a , as in  X  X l temor de la ni X a a los fan-tasmas X  ( X  X he girl X  X  fear to ghosts X ) vs.  X  X a adic-ci X n a la coca X na X  ( X  X he addiction to cocaine X ). 
We decided to carry out the classification for each of the grammatical features that conform the cross-classified types as a better level of generali-zation than the type: mass and countable , on the one hand and, on the other hand, for subcategoriza-tion information three further basic features: trans , for nouns with thematic complements introduced by the preposition de , intrans , when the noun can appear with no complements and pcomp for nouns having complements introduced by a bound prepo-sition. The complete type can be recomposed with the assigned features.  X  X emor X  and  X  X dicci X n X  will be examples of trans and pcomp_a . They both have also to be assigned the feature countable . The combination of features assigned corresponds to the final type which is a definition of the complete behaviour of the noun with respect, for instance, optional complements. 
We have used 23 linguistic cues, that is, the pat-terns of contexts that can be indicative of a particu-lar feature. The most frequent cue that can be re-lated to countable is for the noun to be found with plural morphology. A singular noun without de-terminer after a verb or a preposition is a cue of the noun being mass :  X  X ay barro en el sal X n X  ( X  X here is mud in the living room X ) vs.  X  X ay hombres en el sal X n X  ( X  X here are men in the living room X ). A fur-ther cue for mass is the presence of particular quantifiers, such as  X  X  X s X  ( X  X ore X ),  X  X enos X  ( X  X ess X ), etc. But these cues, based on a collection of lexical items, are less productive than other characteristics such as morphological number or presence of determiners, as they appear very scarcely in texts. Nevertheless, we should mention that most of mass nouns in Spanish can also appear in the contexts of countables, as in the case of  X  X eer X  when in constructions such as  X  X hree beers, please X . 
More difficult was to find cues for identifying the transitive nature of a noun. After some empiri-cal work, we found a tendency of argumental com-plements to have a definite article:  X  X emor de la ni X a X  ( X  X ear of the girl X ), while modifiers tend to appear without determiners:  X  X esa de juegos X  ( X  X a-ble of games X ). Besides, we have taken as a cue the morphological characteristics of deverbal nouns. Suffixes such as  X -ci X n X ,  X -si X n X , and  X -miento X , are very much indicative of transitive nouns. Fi-nally, to find the bound preposition of comple-ments, we used a pattern for each possible preposi-tion found after the noun in question. 
We used Regular Expressions to implement the linguistic motivated patterns that check for the in-formation just mentioned in a part of speech tagged corpus. The various patterns determine whether the linguistic cues that we have related to syntactic features are found in each occurrence of a particu-lar word in a corpus. The positive or negative re-sults of the n pattern checking are stored as binary values of a n dimensional vector, one for each oc-currence. All vectors produced, one per occurrence of the word in question, are stored then in a kind of vector of vectors that we have called its signature . The term signature wants to capture the notion that the data it embodies is truly representative of a par-ticular item, and that shows the details of its typical behavior. Particularly, we wanted linguistic cues appearing in different occurrences of the same word to be observed as related information. We have not dealt with ambiguity at all, however. One of the reasons was our focus on low frequency nouns. l X  X ULA , a multilingual part of speech tagged corpus which consists of domain specific texts. The sec-tion used for our evaluation was the Spanish with 1,091,314 words in the domain of economy and 4,301,096 for medicine. A dataset of 289 nouns, present in both subcorpora, was selected. It was important to compare the behavior of the same nouns in both corpus to check whether the learner was subject to unwanted overfitting. sifier 3 . DT X  X  are one well known and successful technique for this class of tasks when there is enough pre-annotated data available. DT X  X  have the additional benefit that the results can be in-spected. The signatures of the words in the Gold-Standard lists were extr acted from the corpus of medicine and of the economy one. There was a further test set of 50 nouns with a single occur-rence in the corpus of economy for testing pur-poses. The DT was trained with the signatures of the economy corpus, and the medicine ones as well as the singles set were used for testing. The purpose of the evaluation was to validate our system with respect to the two problems men-tioned: noise filtering and generalization capacity by measuring type precision and type recall. We understand type precision as a measure of the noise filtering success, and recall as a measure of the generalization capacity. the different experiments. In Table 1, there is a view of the results of the experiment after training and testing with the signatures got in the smaller corpus. The results are for the assignment of the grammatical feature for the two values, yes and no . And the column named global refers to the total percentage of correctly classified instances. 
The most difficult task fo r the learner is to iden-tify nouns with bound prepos itions. Note that there are only 20 nouns with pr epositional complements of the 289 test nouns, and that the occurrence of the preposition is not mandatory, and hence the signatures are presented to the learner with very little information. 
Table 2 shows the results for 50 nouns with only one occurrence in the corpus. The performance does not change significantly, showing that the generalization capacity of the learner can cope with low frequency words, and that noise in larger signatures has been adequately filtered. 
Table 3 shows that there is little variation in the results of training with signatures of the economy corpus and testing with ones of the medicine cor-pus. As expected, no variation due to domain is relevant as the information learnt should be valid in all domains. 
The obtained results show that the learning of grammatical features of nouns are learned success-fully when using distributional linguistic informa-tion as learning features that allow the learner to generalize so as to maintain the performance in cases of nouns with just one occurrence. 
There are however issues that should be further investigated. Grammatical features with low preci-sion and recall results ( mass and pcomp ) show that some more research should be carried out for find-ing relevant linguistic cues to be used as learning features. In that respect, the local cues based on morphosyntactic tagging have proved to be useful, minimizing the text preprocessing requirements for getting usable results. The authors would like to thank Jordi Porta, Daniel Chicharro and the anonymous reviewers for helpful comments and suggestions. 
