 1. Introduction
The general approach for looking for a pattern in a file that is stored in its compressed form, is first decompressing and then applying one of the known pattern matching algorithms to the decoded file. In many cases, however, in particular on the Internet, files are stored in their original form, for if they were compressed, the host computer would have to provide either memory space for each user in order to store the decoded file, or appropriate software to support on the fly decoding and matching. Both requirements are not reasonable, as many users can simultaneously quest the same information reservoir which will either demand a large quantity of free memory, or put a great burden on the host CPU. Another possibility is transferring the compressed files to the personal computer of the user, and then decoding the files. However, we then assume that the user knows the exact location of the information she or he is looking for; if this is not the case, much unneeded information will be transferred.
 There is therefore a need to develop methods for directly searching within a compressed file.
This so-called compressed matching problem has been introduced by Amir and Benson (1992), and has recently got a lot of attention (Amir, Benson, &amp; Farach, 1996; Ga  X  sieniec &amp; Rytter, 1999; Farach &amp; Thorup, 1995; K  X  arkk  X  ainen, Navarro, &amp; Ukkonen, 2000; Kida, Takeda, Shinohara, &amp; Arikawa, 1999; DeMoura, Navarro, Ziviani, &amp; Baeza-Yates, 1998; Navarro &amp; Raffinot, 1999;
Shibata, Kida, Takeda, Shinohara, &amp; Arikawa, 2000). It is a variant of the classical pattern matching problem, in which one is given a pattern P and a (usually much larger) text T , and one tries to locate the first or all occurrences of P in T . In the compressed version of this problem, the text is supposed to be stored in some compressed form.

For complementary encoding and decoding functions E and D , that is, functions such that for approach which searches for the pattern P in the decompressed text D  X  E  X  T  X  X  . A necessary con-dition is then that the pattern P should be encoded in the same way throughout the text, which is not the case for arithmetic coding and for dynamic methods such as adaptive Huffman coding.
The various Lempel X  X iv variants are also dynamic methods, but for them compressed matching is possible: all of the fragments of the pattern P appear in the compressed text, though not neces-sarily contiguously and not necessarily in the same order as in the pattern, since parts of the compressed text are pointers to an external dictionary or to previous locations in the given text itself. Much of the previous work on compressed pattern matching concentrates on Lempel X  X iv encodings. A different approach is not to adhere to a known compression scheme, but to devise a new one that is specially adapted to allow efficient searches directly in the compressed file (Manber, 1997; Klein &amp; Shapira, 2000).

Fukamachi, Shinohara, and Takeda (1992) propose a pattern matching algorithm for Huffman encoded strings, based on the Aho X  X orasick algorithm. In order to reduce the processing time due to bit per bit state transitions, they use a special code in which the lengths of the codewords are multiples of four bits and present an algorithm for pattern matching in this kind of compressed files. Shibata, Matsumoto, Takeda, Shinohara, and Arikawa (2000) present an efficient realization of pattern matching for Huffman encoded text, substituting t consecutive state transitions of the original machine by a single one. When t is a multiple of 4, this results in a speedup. Takeda et al. (2002) build a pattern matching machine by embedding a DFA that recognizes a set of codewords into an ordinary Aho X  X orasick machine, and then make it run over a text byte after byte. Their technique can handle any prefix code including Huffman codes.

DeMoura, Navarro, Ziviani, and Baeza-Yates (2000) propose a compression scheme that uses a word based byte oriented Huffman encoding. The first bit of each byte is used to mark the beginning of a word. Exact and approximate pattern matching can be done on the encoded text without decoding. Their algorithm runs twice as fast as agrep , but compression performance is slightly hurt. Moreover, the compression method is not applicable to texts like DNA sequences, which cannot be segmented into words.

In the present work, we are interested in searching within the original Huffman encoded text without any modification. We concentrate on static Huffman coding, for which the problem might at first sight seem trivial. It is, however, not always straightforward, since an instance of E  X  P  X  in the compressed text is not necessarily the encoding of an instance of P in the original text T , and might be crossing codeword boundaries. Consider for example the Huffman code f 00 ; 010 ; 011 ; string 1000101100 is the encoding of the string one . Suppose, however, that we are searching for the pattern two : we could find E ( two ) starting at the third bit and extending to the end of E ( one ), as shown in Fig. 1.

The problem is thus one of verifying that the occurrence detected by the pattern matching algorithm is aligned on a codeword boundary. In the next section, we suggest an algorithm for compressed matching in Huffman encoded files. Section 3 analyses the probability of getting false matches and experimental results are presented in Section 4. 2. Compressed pattern matching for Huffman codes
For a given text T over some alphabet R , we consider the Huffman encoded text E  X  T  X  . In order to locate a pattern P in T , we start by encoding the pattern and then apply one of the known pattern matching techniques to find E  X  P  X  in E  X  T  X  . Note that Boyer and Moore X  X  (1977) algorithm, with its sub-linear performance might not be the best choice here, as we deal with the binary alphabet f 0 ; 1 g . An attractive alternative in our case is Karp and Rabin X  X  (1987) probabilistic pattern matching, specifically because our suggested solution is also probabilistic in nature.
If the algorithm does not find any occurrence, we know that P does not occur in T . On the other hand, if an occurrence of E  X  P  X  is detected, we cannot be sure that it corresponds to an occurrence of P in T , unless we scan the encoded text from its beginning to locate all the codeword boundaries. This means that we effectively decode the text, which is what we wanted to avoid.
No decoding is necessary, if we also keep the list I of the indices f i each codeword. Once the compressed pattern has been located, the index of its location can be searched for in I . This would just take O  X  log j T j X  time using binary search, but keeping the list I might sometimes more than double the size of the compressed file. Consider, for example, a file
E  X  T  X  of one KB, consisting of about a thousand codewords of average length 8 bits. The cor-responding list I would have about a thousand entries, each requiring 13 bits! It would not really help to record, instead of the list I , the sequence L of codeword lengths , which are the differences bits are then necessary for each length, where m is the maximum codeword length, so that the size of the list L would still be O  X j T j X  . For the above example, if m  X  16, the size of L would be
Even though the space overhead is reduced, the required time can be just as bad as for the sequential decompression from scratch: instead of decoding the text, it is the list L that has to be processed from its beginning. In fact, if already one agrees to double the size of the file, a simpler solution avoiding the necessity for binary search would be to keep a bit-string B of size identical to that of the compressed file; a bit in a position corresponding to the beginning of a codeword would be set to 1 in B , and all the others to 0.

A possible simple solution would be some tradeoff between recording all codeword boundaries or none of them by preparing a small list of possible entry points into the compressed text. Choose a parameter b and partition the compressed file into blocks of b bits; then move, when necessary, each partition point to the closest preceding codeword boundary, and record the index of the first provides the starting point of the block containing the compressed pattern, so this block can be decompressed. The additional required space is thus O  X j T j = b  X  and decoding time is reduced to entry of D . For certain values of b , this may be a recommendable solution, with small storage overhead and fast performance. However, the more we wish to reduce the size of D , the larger b will be, implying longer processing. If one agrees to change the encoded file slightly, one can get rid of the list D and force alignment on block boundaries. The total number of additional bits, less than one codeword per block, could be kept very low.

As alternative we suggest a solution that does not alter the compressed file by exploiting the tendency of Huffman codes to resynchronize quickly after errors (Klein &amp; Wiseman, 2000): if the pattern has been found at index i , jump back by some constant number of bits K and start decoding from there. It might well be that the bit indexed i K is not the beginning of a codeword in E  X  T  X  , so that the decoding will be erroneous at the beginning. However, if K is chosen large enough, the decoding of the last bits preceding bit i will generally be correct, regardless of possible errors before. One can therefore decide, with a small error probability, whether to announce a match at location i or not, depending on whether bit i is the beginning of a new codeword in the decoding that started at i K . The formal algorithm for finding the occurrences of pattern P in T is given in Fig. 2. It uses the Huffman tree of the given alphabet R , and refers to its root as root .It the substring of y that starts at its i th position. If no such index exists ( x does not occur as substring of y ), the procedure returns 1 . The decoding then starts at position i K , or at the beginning of the string in case K &gt; i 1. The procedure search can be implemented using any of the known pattern matching algorithms X  X  X e shall refer specifically to the Karp Rabin algorithm in Section 3. 2below X  X  X ut the details have been omitted here to keep the focus on the solution of the synchronization problem.

There are codes for which this algorithm does not work better than without the backward jump of K bits. Indeed, suppose we start the decoding of a given compressed string at two different points, yet according to the same Huffman tree, and suppose that at some point, these two decodings synchronize. Let x and y denote the last codewords for the two decodings before reaching the synchronization point. Then either x is a suffix of y or y is a suffix of x . In any case, the underlying Huffman code cannot have the so-called suffix-property , asserting that no code-word can be the suffix of any other, similarly to the well-known prefix-property of all Huffman codes. Accordingly, codes having both the prefix and the suffix property have been called never-self-synchronizing in Gilbert and Moore (1959); they are called affix codes in Fraenkel, Mor, and
Perl (1983). There are infinitely many different complete variable-length affix codes, e.g., {01, 000, 100, 110, 111, 0010, 0011, 1010, 1011}, but they are nonetheless extremely rare (Fraenkel &amp; Klein, 1990). In particular, the code used in Fig. 1 is not affix, since the codeword for o is a suffix of the codeword for e . Returning now to our compressed matching algorithm, if the code is an affix code and bit i K does not happen to be the first of a codeword, the erroneous decoding will extend to the end of the file, for any size of K .

In practice, however, synchronization is often achieved after a small number of bits, typically less than 100. It seems therefore that by choosing K as a few hundred should generally be enough to avoid errors like declaring a match when in fact there is none, or failing to declare a match even though there actually is one. We bring some experimental results below. 3. Estimating the number of errors 3.1. False matches in the pattern matching process
We shall compute an estimated number of false matches using two different models of the probabilistic process underlying the text creation. Both models assume that the text has been generated by choosing repeatedly, and independently from each other, characters from R according to their probability of occurrence p 1 ; ... ; p assumption is of course an approximation in many cases, in particular for natural language texts which generally exhibit many dependencies. One could even argue that due to the independence of symbols, regular patterns should not exist and therefore there is no basis for any pattern matching. We consider, however, also very large alphabets, the elements of which are not nec-essarily single characters, but rather words or even phrases. Such models are frequently used in large information retrieval systems (Witten, Moffat, &amp; Bell, 1994).

We refer in this section to the number of false matches caused by the search function only, as if the algorithm of Fig. 2were used with backskip parameter K  X  0. The experiments below suggest that for large enough K , the number of false matches generally decreases to zero.

The first model relies on the fact that the string E  X  T  X  is the result of a Huffman encoding process,  X  ; ... ; X  n , respectively, and assumes that the probabilities of the occurrence of the characters in the is called dyadic . The resulting approximation may be justified by the fact that since the original and the corresponding dyadic distributions yield the same Huffman code, they must be quite similar. A formal definition of this similarity can be found in Longo and Galasso (1982), in which the set of probability distributions is given a pseudo-metric, and an upper bound is derived for the distance of any probability distribution to the dyadic distribution giving the same Huffman tree.
One can then use a theorem shown in Klein, Bookstein, and Deerwester (1989), stating that with these assumptions, the output of Huffman decoding is indistinguishable from a random binary string with probability of occurrence of a 1-bit being equal to this would then imply that any binary pattern of length k , with k P 1, occurs with probability 2 k .
We shall use this approximation even though E  X  T  X  is finite and the occurrences of characters are not really independent of each other.

To estimate the number of false matches, we proceed as follows: let m  X j E  X  P  X j be the length in bits of the encoding of the pattern, and assume P occurs t times in T . Consider a text string T obtained from T by purging all occurrences of P . The Huffman encoding of T sequence of length j E  X  T  X j tm (assuming that there are no overlaps of suffixes of P with prefixes of P ). Since this too is a Huffman encoded string, the probability of occurrence of E  X  P  X  is 2 m .No occurrence of E  X  P  X  in E  X  T 0  X  corresponds to a true match of P in T number of false matches
In fact, we have used here two more approximations: by eliminating all the occurrences of P , the original probabilities may have been changed, which could affect the lengths of the corre-sponding Huffman codewords. If t and m are small relative to the size of the encoded text, the change in the probabilities might be small enough to yield the same Huffman tree (Longo &amp;
Galasso, 1982), and even if the tree is altered, the change of the average codeword length will often be negligible. The second approximation is that by removing a true match, a new false match might appear that spans over the gap.

The second model takes the probabilities p 1 ; ... ; p n into account and assumes a complete prefix code, though not necessarily one derived from Huffman X  X  algorithm. For convenience, we shall still use the terminology of Huffman codes, but the analysis is also valid for any other complete prefix code with associated probabilities. The following notations will be used below. Let T denote the Huffman tree corresponding to a given Huffman code. The elements which are encoded appear with probabilities p 1 ; ... ; p n in the text, and the lengths of the corresponding Huffman codewords are  X  1 ; ... ; X  n , respectively. We shall also use the notation p element corresponding to the leaf y . Denote by L the set of the leaves of T , and by I the set of its internal nodes. For each x 2 I , we define T x as the subtree of T rooted at x , and we denote by
L x  X  L \ T x the set of its leaves. The internal nodes I correspond to the possible positions within a codeword at which a match of the pattern P can be found. In particular, the root r of the tree, which belongs to I , corresponds to the special case where position i , returned by the pro-cedure search in the algorithm, is the beginning of a codeword, i.e., a true match has been found.
Consider the fact of having a possible match in a certain position as if it were generated by the following random process: the compressed text consisting of a given sequence of zeros and ones, we pick randomly bit positions which shall act as the starting position of the matches. In this sense, we can speak about the probability of having a possible match in a certain position.
We thus assume that the position i returned by the procedure search occurs at random in any possible location, that is, at any internal node of T . For a given internal node x 2 I , the probability P  X  x  X  of the position corresponding to x being returned by the algorithm will be proportional to p i  X  i , and not just to p i , since we deal with a random process on the compressed text and not on the original one. Each leaf of the Huffman tree is associated with a probability p and the probability associated with an internal node y is the sum of the probabilities associated with the two children of y . Thus, when adding the probabilities associated with all the internal nodes, we get W  X  given by This is indeed a probability distribution, as
Similarly, for a leaf y 2 L , the probability of seeing the codeword corresponding to y in the compressed text, which we shall denote by P  X  y  X  to differentiate it from the above probabilities defined for internal nodes, will be proportional to p i  X  ability will be and again
As an example for these definitions, consider again the Huffman code mentioned in Section 1 0.12, 0.11, 0.11, 0.06, 0.05 and 0.08, respectively. The corresponding Huffman tree is depicted in
Fig. 3. The probability associated with any node v of L [ I appears underneath v , the proba-bilities P  X  x  X  for x 2 I appear in grey ellipses to the right of the internal (black) nodes, and the probabilities P  X  y  X  for y 2 L appear in white boxes to the left of the leaves.
 Let F denote the event of getting a false match at a given position. We evaluate the probability P  X  F  X  by conditioning on the position x 2 I returned by the algorithm:
If x is the root, P  X  F j algorithm returned x  X  X  0, because of the prefix property of the Huffman codes. If x is some other internal node, we have to consider several possibilities, which are schematically displayed in Fig. 4.

Since we deal with a complete code, any binary sequence such as E  X  P  X  we try to locate, can be decoded (i.e., mapped into a sequence of codewords), even if the traversal of the tree T does not start at its root. One possibility is that E  X  P  X  is a substring of a codeword, without being its prefix or suffix. This corresponds to a path in T starting at an internal node x and ending at another it could be such a suffix followed by several other codewords. The most general case is given in
Fig. 4(c): E  X  P  X  consists of the suffix of some codeword, followed by (zero or more) codewords and ending with the proper prefix of some codeword.
 starting at the internal node x , and proceeding to left or right children as directed by the binary corresponds to t  X  1, and if the decoding happens to finish at the end of a codeword (as in the case y x ; t  X  is the sum of the probabilities of the leaves in the subtree rooted by y case y 0  X  x ; t  X  X  root , this sum is 1. Assuming independence of the events, we get
We can therefore derive the estimated number of false matches j E  X  T  X j P  X  F  X  as
In any case, we see that the probability of a false match decreases sharply when the length m  X j E  X  P  X j increases. We bring below experimental results comparing the formulas with empiric data. It should be noted that one can argue that similarly, the expected number of true matches can be evaluated; but true matches of E  X  P  X  in E  X  T  X  correspond to matches of P in T , and these are given since P and T are fixed. There is therefore no probabilistic scenario on which calculating this probability could be based. For the false matches, however, our assumption of random occur-rence seems reasonable, yielding the above analysis. 3.2. False matches resulting from probabilistic pattern matching
We stated above that if x , the node of the Huffman tree corresponding to the position returned by the algorithm, is the root, then P  X  F j algorithm returned x  X  X  0, i.e., there cannot be a false match, because the encoded pattern has been found at a codeword boundary and a false match would imply a violation of the prefix property. However, this assumes that we can assure that if the pattern matching algorithm declares a match at position i , there is indeed a match at that position. This is true for deterministic algorithms, but not necessarily for probabilistic ones. For instance, Karp and Rabin X  X  (1987) algorithm searches for E  X  P  X  in E  X  T  X  by scanning substrings Z of E  X  T  X  , each of the same length m as the encoded pattern, and instead of comparing Z it compares Z i mod Q with E  X  P  X  mod Q , where Q is a large randomly chosen prime number. If the moduli are equal, a match is declared, even though obviously there are many numbers a and b such that a 6  X  b but a mod Q  X  b mod Q . Two probabilities have thus to be dealt with: 1. the probability R 1 that the match declared by the probabilistic pattern matcher might be an erroneous one; it might not correspond to a match of P in T .

The second probability R 2 has been evaluated in the previous section. As to R can easily turn the probabilistic algorithm into a deterministic one, by checking at the declared position if it indeed holds a match. Moreover, such a check is generally not really needed. The probability to get a false match by the Karp Rabin algorithm is bounded by mn = 2 q , where m and n are the sizes of the pattern and the scanned text, respectively, and q  X d log bits of the prime number Q . One can therefore choose q large enough (since m and n are given) to make this probability negligible relative to R 2 . Summarizing, we may safely ignore R pattern E  X  P  X  is shorter than q , then working modulo Q is in fact a real comparison and not a probabilistic one, so R 1  X  0; if on the other hand, m is larger than q , then R extremely small. 3.3. Erroneous decisions of the algorithm
When running the pattern matching algorithm with the backskips, a correct performance identifies the true and false matches for each of the occurrences of E  X  P  X  in E  X  T  X  . These matches are called below true positives and negatives. The algorithm can, however, also fail in two quite different ways: 1. It could announce a match, while in reality the occurrence of E  X  P  X  in E  X  T  X  does not correspond 2. It could fail to announce a match, while in reality the occurrence of E  X  P  X  in E  X  T  X  does corre-
In an analogy to information retrieval literature, we wish to retrieve all, and only, occurrences of P in T . The first type of error reduces then precision (the ratio of relevant retrieved items to all retrieved items), since it retrieves also elements which are not occurrences of P in T ; the second type of error reduces recall (the ratio of relevant retrieved items to all relevant items), since it does not retrieve all occurrences of P in T . The following table summarizes the four possibilities for a given occurrence of E  X  P  X  in E  X  T  X  : the columns correspond to the actual situation (match or non-match of P in T ), the rows to what is announced by the algorithm.

If the algorithm does not skip backwards ( K  X  0), every occurrence of E  X  P  X  in E  X  T  X  would be declared as a match, so there are no false negatives, but possibly many false positives. For small values of K , it is probable that synchronization with the true decoding is not achieved before the K bits are used up, which could imply a large number of false decisions; but by increasing K , the probability of synchronization, regardless of the starting point, increases, and the number of false positives or negatives will decrease. 4. Experimental results The algorithm was tested on several files of different nature. The first one was paper1 from the
Calgary corpus, an English text with editing instructions. The second was a DNA file (of the declarematch True positives False positives declarenon-match False negatives True negatives tobacco chloroplast genome), including also blanks and newlines for clarity; the alphabet thus consisted of six characters. Finally, to cancel the bias introduced by the independence assumption, a new text was created based on the distribution of characters in paper1 , but with each character generated independently from the others.

The first set of tests was performed on paper1 , searching for arbitrary patterns which were chosen randomly within the file. We used a canonical Huffman encoding and considered patterns of lengths 3 X 50, four of each. The compressed forms of these 19 2patterns occurred in total 1077 times in the compressed text. Of these occurrences, 1040 corresponded to appearances of P in T , and 37 were false matches, all of which occurred for the shorter patterns up to length 5. For example, when searching for P  X  fro , for which E  X  P  X  X  110010-10001-0011, the pattern
P 0  X  k t h was retrieved ( t denoting a blank), for which E  X  P 0  X  X  111100101-000-10011, including E  X  P  X  as suffix.

The algorithm was then applied several times, each time with a different size of the backward skip, which was chosen as an integral number of bytes. The first column of Table 1 gives the size K of the backward skip in bits, the following columns list the number of occurrences of true and false positives and negatives.

Obviously, true positives and false negatives add up to the number of actual matches, while true negatives and false positives add up to the number of non-matches. One sees that false positives (wrongly announced matches) are rare, independently of the algorithm, and that already for small backskips (less than 1 2bytes for all our examples), both types of errors may be corrected.
Table 2brings some example patterns P , comparing for each the actual number of wrong matches (occurrences of E  X  P  X  in E  X  T  X  which do not correspond to occurrences of P in T ) with the expected number on the basis of formulas (1) and (2).
 We see that there is generally a good fit, with no formula consistently outperforming the other.
Interestingly, on the random file, the number of wrong matches was much higher than the number of true matches for many of the examples.
 5. Concluding remarks
Searching for a pattern directly in a Huffman encoded file seems to be an easy task because of the static nature of the compression scheme. There are however synchronization problems, which we tried to overcome in this work. If the pattern is long enough, the probability of finding a wrong match is often very low, independently of the algorithm. For the other patterns, a proper choice of the backskip parameter lets us control the error.
 Acknowledgements The authors would like to thank two anonymous referees for their helpful comments.
 References
