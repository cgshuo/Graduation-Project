 Online social networks and mash-up services create oppor-tunities to connect different web services otherwise isolated. Specifically in the case of news, users are very much exposed to news articles while performing other activities, such as so-cial networking or web searching. Browsing behavior aimed at the consumption of news, especially in relation to the vi-sits coming from other domains, has been mainly overlooked in previous work. To address that, we build a BrowseGraph out of the collective browsing traces extracted from a lar-ge viewlog of Yahoo News (0.5B entries) and we define the ReferrerGraph as its subgraph induced by the sessions with the same referrer domain. The structural and temporal pro-perties of the graph show that browsing behavior in news is highly dependent on the referrer URL of the session, in ter-ms of type of content consumed and time of consumption. We build on this observation and propose a news recom-mender that addresses the cold-start problem: given a user landing on a page of the site for the first time, we aim to predict the page she will visit next. We compare 24 fla-vors of recommenders belonging to the families of content-based, popularity-based, and browsing-based models. We show that the browsing-based recommender that takes into account the referrer URL is the best performing, achieving a prediction accuracy of 48% in conditions of heavy data sparsity.
 H.4 [ Information Systems Applications ]: Miscellaneous BrowseGraph; cold-start; news recommendation; browsing behavior; sessions This work has been performed when the author was a Visiting Scientist at Yahoo Labs, Barcelona, within the framework of the FREP grant.
 Figure 1: An article page from Yahoo News (compacted layout). Right rail boxes and the infinite-scroll section at the bottom allow the user to browse to other articles.
In recent years the consumption of online news has in-creased rapidly, in contrast with the decline of traditional newspapers 1 . Between 2009 and 2012 the percentage of users visiting news portals raised steadily up to the point to represent the major portion of overall Web traffic 2 , com-parable to the volume of visits to top domains like Google search 3 . For its importance, richness of content, and abun-dant user participation, the field of online news has become a crowded arena for research in several areas including re-trieval, ranking, recommendation, and personalization [6, 2, 30 ]. Despite the vast amount of work in the field, there are two aspects of news consumption that are still largely unexplored. First, modern online news providers have tur-ned into more globally connected systems able to attract a wider audience than their core of regular users. News ar-ticles are very often shared on different external websites and social media platforms, thus providing a growing num-ber of browsing shortcuts to news portals. To mention two examples, modern search engines serve queries relevant to news stories by directly featuring news articles from major providers, and social media are increasingly used as daily tools for journalists and casual news readers 4 to spread and consume news provided by external parties [ 2, 17 , 28 ]. De-spite such increasing level of integration and mashup, news portals have been studied mostly in isolation. The latter aspect that has drawn very little attention is user browsing behavior . Although recent literature is rich in studies about browsing patterns in several online platforms [ 18 , 9, 25 ], lit-tle has been done with respect to the news domain. One reason for this is that the browsing sessions in online news outlets have been found to be short, aimed in most cases to quick catch ups on news [9 ].

In this work we address these two aspects in combination and exploit them in a task of news recommendation in a cold-start scenario. We study the way users browse news content in relation to the type of online domain they we-re browsing before landing on the news page, also known as referrer domain. Our contribution begins with finding that browsing is a meaningful phenomenon to study also for online news, as the browsing graphs have a coherent and well-formed structure also in this domain. We find that the referrer partly determines the type of news consumed and the time when it is consumed. In the wake of previous stu-dies about the impact of the referrer domains on user brow-sing behavior [4 ], we use the browsing graph induced by the referrer of the browsing sessions to predict the next article a newcomer will visit right after she lands on a page of the site. Using a very large sample of viewlogs from Yahoo News (  X  500M entries), we compare 24 flavors of recommenders for next-article consumption, including popularity-, item-, and browsing-based models. We find that browsing-based recom-mendations achieves the best overall precision@1 among all the methods: up to 48% in conditions of heavy volatility of news articles and of high data sparsity arising from the large amount of candidate articles to recommend.

We summarize our main contributions as follows:
Our work differs from the state-of-the-art in the fact that we consider previous users navigational patterns and the re-ferrer domain to recommend the news article that users are likely to visit next; we are not aware of any other work that use the BrowseGraph for news recommendation. Moreover, we introduce and study for the first time the ReferrerGraph , and show how its information helps to improve the accuracy of the recommendation. Cold-Start Recommendation. Cold-start problem re-commendation refers either to new items or to new users, and we consider the latter case. Some solutions to the pro-blem require an initial, even though small, set of preferences about the newcomer user [ 22 , 21 , 1]( warm-start scenario). The user profile can be populated by asking to the users to rate a set of items first or importing their preferences from external systems [19 ]. When a minimal user profile is availa-ble, a common approach is to apply association rules in order to expand the user profile. Sobhanam et al. [22] used clu-stering algorithms to find the most similar known items and then they extended the profile of the users with item rela-ted to their tastes. Shaw et al. [21] used also non-redundant rule sets showing how they can improve the results. Yang et al. [31] presented a Bayesian-inference based recommen-dation system that exploit the social network structure of the user in order to perform personalized recommendation. In this paper, the only information we have is the external referrer URL from which the user is coming from, and the news article the user is currently watching. While no one has considered the first information in the field of recommender system, the latter one could be integrated in a content-based approach, that is one of the baseline we implemented (  X  5.2 ). News Recommendation. Despite the progress of recom-mender systems in general, news recommendation is still a very active area 5 . The majority of news recommender sy-stems are based on user information collected over time [12 , 5, 10 , 11 ], relying on logged-in users only and combining col-laborative filtering and content-based approaches. However, when reading news website the user is most often not logged-in, making impossible (or at least very difficult) the creation of a reliable user profile. There are approaches based on models of similarity among news articles [ 16, 10 , 20 ] consi-dering different key factors like textual similarity, recency, coherence, novelty and popularity. Tsagkias and Blanco [26 ] proposed a language intent model that extracts a query from the user browsing session. In this way they model the user interest using the queries of the users. There are also ap-proaches looking for the most authoritative news sources as a proxy for quality [6 , 24 ]. However, most of previous work relies on the user historical profiles. Our work is different since it is based on the BrowseGraph , using previous collec-ted behavior of users grouped based on the external domain where they come from.
 BrowseGraph and Referrer URL. In recent years there have been numerous studies on user traffic in term of navi-gation/browsing of web pages. Browsing sessions have been leveraged for different tasks such as recommending photos and photostreams [ 3], predicting the users demographic [ 8], or optimizing the web crawler [13 ]. Liu et al. [14 ] introduced the BrowseGraph , a graph built by the users navigation pat-terns where the nodes are webpages and the edges are tran-sitions made by users. They compared different centrality metrics computed on the standard hyperlink graph model, on the BrowseGraph , and on their combination, finding that the BrowseGraph rank has higher quality. The BrowseGra-ph has been used to rank items like photos in Flickr [ 25 ], where the external referrer domain ( i.e. the last domain vi-sited by the user before entering in Flickr), was considered to attribute more importance to the images with highest ex-ternal visibility. The referrer domain has been proved to be useful for studying the popularity of media items (such as YouTube videos [7 ]), and to characterize the type of session the user is likely to perform [ 4]. However, we are not awa-re about studies that exploit this important information in the context of news or that use the ReferrerGraphs for new recommendation purposes as we do.
To investigate the activity of news consumption and brow-sing we analyze the Yahoo News viewlog, also considering the browsing activity of users coming from different ( fami-lies of) domains. In this section, we describe the raw data we use and its preprocessing (  X  3.1 ) and how we leverage it to generate the BrowseGraphs (  X  3.2 ).
Each article page in Yahoo News contains a nearly ine-xhaustible variety of options to transition to other article pages. As shown in Figure 1, the typical news page con-tains a right rail with several boxes of recommended news ( e.g. , recent articles) and an infinite-scroll list of persona-lized news at the bottom. To capture the user X  X  browsing activity we consider a sample of the site X  X  viewlog 6 , where each pageview is recorded by the following fields: BCookie is an anonymized user identifier computed from the browser cookie. CurrentURL and ReferrerURL represent, respectively, the URL of the page the user is currently visi-ting and the URL of the page from which she transitioned to the current one, possibly including external domains. Last, the User-Agent identifies the browser in use, and the Time indicates when the page was visited. By preserving only the traffic from well-known browser identifiers (to remove spider-generated requests), we are left with approximately 0 . 5 billion pageviews.

To model browsing transitions we group pageviews in ses-sions , consisting of a sequence of pageviews made by the same user in a continuous time frame. As is standard prac-tice [ 23 ], we split sessions by timeout (inactivity between two pageviews is longer than 25 minutes) but also when the user leaves the site for another domain, disregarding the timeout. In addition to the browsing transitions, we gather also the information about the topic of each article page (assigned by editors), the article headline and its body. There are 22 article topics in total, some of which are listed in Table 3.
We aggregate the session data over all users to generate a BrowseGraph [14], namely a graph where the nodes are pages and the directed links connect pages that have been visited in sequence during a browsing session. The edges of the graph therefore have the following structure: where weight is the total volume of transitions, aggregated over all sessions. Because the BrowseGraph captures pat-terns of user flow, it has been leveraged in several applica-tions, ranging from estimation of page importance [ 14 ] to multimedia ranking [ 3, 25 ]. In the news domain, the Brow-seGraph is particularly valuable as the links between articles may vary depending on time ( e.g. , turnover of the  X  X op sto-ries X  during the day) or even on the user, as the set of links shown may vary in a personalized fashion. In addition, we go beyond the study of the general browsing patterns by comparing the browsing behavior of users who land on the news website from different domains . We aim to verify em-pirically the idea that users coming from different types of external web services are interested (or exposed) to different types of content and therefore behave differently.
 To decompose the overall BrowseGraph G into subgraphs G d that account for the sessions originated from a specific domain d , we use only the sessions whose first referrer URL matches the domain d . For instance, a user who accesses a news page from a tweet will start a new session that will be part of the Twitter ReferrerGraph . We refer to the subgra-ph G d as the ReferrerGraph for the domain d . In particular, we consider 9 source domains. Three search engines: Bing , Google , and Yahoo ; three social networks: Facebook , Reddit , and Twitter ; and the homepage of the news portal, a special case of referrer URL that represents a significant entry point for Yahoo News users. In addition, we also consider two ag-gregated ReferrerGraphs created by the union of the graphs in the search engines and social networks respectively (we call them Search and Social ), as some of the characteristics of the ReferrerGraphs of domains belonging to the same fa-mily are quite similar (see  X  4 ). Last, since the consumption of news items is strictly dependent on time, we define a temporal BrowseGraph G t as the BrowseGraph originated by the browsing sessions occurring in an hourly time slot t . We partition the BrowseGraphs on hourly intervals, ending up with 1 , 440 temporal graphs for each domain, for a total of 12 , 960 graphs.
In this section we report the structural properties of the full BrowseGraph and of the ReferrerGraphs induced by the different domains of origin (  X  4.1 ) and a study of their evolution in time (  X  4.2 ).
We find the distribution of the number of hops per session to be broadly distributed (not shown) but with very low ave-rage values (Table 1), in agreement with previous work that found the user interaction with news portals being short and time-constrained [9 ]. Despite that, the BrowseGraph built from the full set of user sessions is connected, with a grea-test weakly connected component that spans up to 95% of all the pages and whose nodes are  X  5 hops away on average (statistics are summarized in Table 2). This means that al-though the individual browsing interactions with the news portal are short, the collective browsing behavior weaves an implicit network of associations between articles whose points are on average 5 hops away. The connectivity of the BrowseGraph appears to be scale-invariant, as very similar connectivity values are found for the ReferrerGraphs , the most disconnected one being Twitter, with 87% of nodes in its giant component. The average in-degree can be conside-rably high due to the large number of possible connections that an article page has with others (as illustrated in  X  3 ), and it is by far the largest in the homepage graph, which dominates in terms of traffic volume. Although the degree Table 1: Average number of hops during browsing sessions with different referrer domains.
 Table 2: Structural statistics of the ReferrerGraphs (  X  d  X  in-dicates the average shortest path length and GCC indicates the Giant Connected Component). Figure 2: Distributions of indegree ( k in ) and edge weight ( w ) in some ReferrerGraphs . Search graphs are collapsed in one curve due to their similar distributions. distributions vary considerably (Figure 2), the edge weight distributions are closer to each other, the vast majority of edges having very low weights.

A natural question is whether the graphs are different just in terms of structural properties or also with respect to the type of their nodes. To measure the overlap between graphs we compute the Jaccard similarity between the set of their nodes (Figure 3a ). As one might expect, similarity is lower between the two major families: search and social. Surpri-singly though, there are conspicuous differences also within each group. For instance, Twitter and Reddit have only  X  20% of the overall amount of their nodes being covered by both. This means that the users coming from Twitter are visiting only a small portion of the news articles visited by users coming from Reddit. In other words, the users inte-rest is strongly dependent by the type of website they are coming from. We spot also significant differences in the ty-pe of news content consumed in the different networks. To measure that, we count the frequency of articles belonging to each of the news topics (see  X  3.1 ), and we rank topics by their frequency in each network (Table 3). Naturally, the most popular type of news content in all the cases is related to general-type news. But except for the top position, the rankings show substantial differences, with celebrity-related news being the main interest for users coming from search engines, while blogs, sports, and entertainment are the most popular topics in Facebook, Twitter and Reddit respectively.
The differences in terms of graph structure, their size and (a) Jaccard similarity of node sets Figure 3: Node overlap between graphs and article ranking comparison. type of nodes impact directly the type of articles that are consumed the most or that are perceived as most intere-sting by the users. To gauge that, we consider two metrics of news importance, namely the pageview count and the Pa-geRank centrality computed on the weighted graphs. We apply each metric separately to the ReferrerGraphs and we compute pairwise the Kendall X  X   X  similarity between the ra-nks. Figure 3b displays the values for PageRank, similar results are found for the viewrank (not shown for brevity). To discount for the different dimensionality,  X  is measured only on the elements contained in the intersection of the two sets. To account for the noise that can be potentially intro-duced by the permutations on the latest positions on the rank (i.e., articles with very similar scores in the long-tail of the popularity curve), we repeat the same measure on the top 100 and 1000 articles, obtaining very similar results. Rankings tend to be more similar within domain families rather across families (  X   X  [0 . 14 , 0 . 4]).
Time plays a central role in news content consumption as news articles tend by their nature to become rapidly stale. In Figure 4 (left) we plot the distribution of the relative volume of views that articles receive in time (hours). In Figure 4 (right) we show the same measure but on a normalized time axis that starts from the article publishing time ( t = 0) up to the last visit received ( t = 1). We refer to this normalized timeline as the article lifespan . Consistently with previous work, we find that 80% of visits are received within the first 30 hours after the article publication and before the first 20% of the overall article lifespan.

One question, however, is whether the temporal aspect of news consumption depends on the type of network the user is coming from. To investigate this aspect, we repeat the previous measures separately on the three ReferrerGra-phs homepage , search , and social in separation. For each of them, we measure the distribution of the total volume of visi-ts at each point of the article normalized lifespan (Figure 5). A phenomenon of rapid decay emerges in all the three cases, however the curves exhibit major differences in skew. While news consumption through the homepage tends to happen in earlier stages of the lifespan, accesses through social ne-tworks and search engines are shifted towards later stages. In particular, for the social domain we observe evident pea-Table 3: Most popular news topics for different browse gra-phs. We grouped Google , Bing , and Yahoo into the same network ( search ) since their ranks are nearly identical. ks of accesses during late stages of the article life. Even if these peaks account for rather small percentage of the whole traffic (up to 2%), they still represent a non-negligible num-ber of accesses. Examples of news that largely contribute to the visit volume inside the peaks belong to the  X  X rivia X  type of stories that are most commonly seen on social networks
The referrer domain is just one variable that might impact the temporal patterns of content consumption for news, and the type (or topic) of the article can also have a role on that. In Figure 6 we show the aggregated volume of views in time for news articles belonging to six different categories. The relative positions of the accesses from homepage, social si-tes, and search engines change depending on the topic. The baseline consumption behavior is given by the general ty-pe of news, for which the view volume from the homepage is consistently higher than the volume from search engines, which is in turn higher than the one from social networks. For blogs instead, the view volume is more similar across the three macro-networks and the curves intersect more often, with two clear phases. First, the number of visits from social networks goes above search for a short time, likely explai-ned by the fact that blog posts in important news sites are usually written by bloggers who are heavily involved in the activity of online social networking. Last, the accesses from homepage and search become comparable in volume. Si-milar observations hold for other categories: sports, movies, and celebrities get a higher volume of accesses in later stages of the article life. In the case of celebrities in particular, we observe that accesses from social exceed even the ones from the homepage, after a certain point. This may happen when news about specific events ( e.g. , academy awards) cause an outburst in the social media discourse. Last, an interesting case emerges from visual news such as photo-galleries rela-ted to news events. In this case, the accesses from social and search are comparable in the early life of the news, whi-le the volume from search and homepage are comparable in the later stages. This delineates a scenario in which images lend themselves to spread easily in social media.
Besides studying the attention received in time by the ove-rall set of articles, it is interesting to check how the atten-tion received by an article changes in relation to others. In other words, if we rank articles by viewcount, we can explo-Figure 5: PDF of the number of views received in each of th-ree ReferrerGraphs over the normalized lifespan of the news, from the publication ( x = 0) to the last visit ( x = 1). re how the rank changes in time and across ReferrerGraphs . To quantify that, we consider Homepage , Search , and Social ReferrerGraphs separately and for each of them we compu-te an hourly view rank R t for all the articles they contain. Then, for each set of articles published in the hourly time slot t i , we compute the Kendall X  X   X  between their view rank at t i and the view ranks in subsequent hours, more formally  X  ( R ti ,R ti + j ) ,  X  j  X  1. Then, we shift each measurement ba-ck in time by i hours, so that all sets of articles start from time 0, and we average all the measurements, with resulting curves in Figure 7. The lower the value of  X  , the farther the ranking at time t is from the ranking of at the original publi-cation time. In all the cases we observe the values decrease rapidly in the first 5 hours and a steady-state occurs within the first 24 hours. This finding is consistent with the volu-me of views dropping of several orders of magnitude in few hours. The  X  value after 2 days is, on average, not higher than 0 . 55, meaning that the final ranking changes conside-rably from the initial one. Although the trend of the three curves is analogous, they have different offsets. Articles ac-cessed via search change their relative position less and the ranking stabilize slightly quicker, while on the other extreme accesses from social networks impact the view rank more.
Item recommendation is a crucial task in news sites as they have to deal with a rapidly changing pool of thousands of fresh articles and millions of users, each one with a speci-fic range of interests. In such a scenario, profiling users with their explicit ( e.g. , comments, article saved, printed, sha-red) and implicit ( e.g. , views, time spent) activities on-site is an effective way to recommend new content that matches the user interest. However, personalization is not possible in cases of cold-start , when a user who is a newcomer or is not logged-in lands on the site. In this context, the in-formation of the BrowseGraph can help, as the activity of Figure 6: Number of views in each ReferrerGraph in time, breakdown by news topic. Figure 7: Kendall X  X   X  calculated between the view rank at time t and the view rank at time 0. previous users provide a collective trace of previous browsing patterns that can be recommended also to the new user. In particular, we show that the ReferrerGraphs are particular-ly effective to this end. Next, we formally define the recom-mendation problem (  X  5.1 ), describe a number of methods to address it (  X  5.2 ), compare them on a large scale dataset (  X  5.3 ), and discuss the results. The prediction problem we address is defined as follows. A newcomer user u  X  U is given, who begins a new session at time t on page p start  X  P , with referrer domain d  X  D . The task consists in predicting a page p next that u will visit right after p start , given that we restrict the problem to users whose sessions will include at least two pageviews, i.e. , an additional pageview after p start . We consider, for simplicity, a time line quantized in discrete 1-hour slots and we assume to know the information about the browsing sessions genera-ted by other users in the previous time slot t-1. To be able to draw a comparison also with recommendation methods based on textual content or item popularity, we consider an additional set of metadata for every page p  X  P , specifically, v p : cumulative number of pageviews at time t  X  1; cat p : the page X  X  topical category; h p : the page textual headline; b : the textual body of the page.
All the prediction algorithms we consider are determined by the combination of three components we describe next. Full neighbors set (full) . After the initial visit of p the target user could transition, in principle, to any other pa-ge in P . However, we measure that in the 95% of the cases, p next is included among the set  X  t  X  1 G ( p start ) = { p E ( G t  X  1 ) } , namely the out-neighbors of p start in the Browse-Graph created from the browsing sessions occurring during the timeslot t-1. This happens because, even though the cardinality of  X  t  X  1 G ( p start ) can be very big (recall the degree distribution in Figure 2), most of the browsing links between p start and its neighbors are created shortly after the news are published. For this reason, an effective strategy would be to consider only the set  X  t  X  1 G ( p start ) as output range for the prediction. We call this selection strategy  X  X ull X  , after the full BrowseGraph we use to perform the selection. Referrer neighbors set (ref) . In  X  4 we observed that the type of referrer URL determines to a certain extent the type of news consumed. One might argue that adapting the candidate page selection based on the user domain of origin d , could potentially improve the prediction accuracy. We could therefore restrict the output range to the neighbors of p start in the domain-dependent graph G t  X  1 d . Using G instead of the full graph G t  X  1 implies a drop in the chance of finding p next in the set  X  t  X  1 G ( p start ) from 95% to a minimum of 48% for the Yahoo graph and a maximum of 72% for Homepage. However, based on our previous analysis, the subset of remaining pages could have a higher likelihood of being good candidates for prediction. Similarly to the previous case, we name this selection strategy  X  X ef X  . Mixed neighbors set (mix) . A natural extension is to combine the ref and full approaches in cascade. By defi-nition, G t  X  1 d has a subset of the nodes in G t . Therefore, it may happen that the node p start is not present in the sub-graph or does not have any out-neighbor. So, we adopt the following strategy: if p start  X  N ( G t  X  1 d )  X  k out then use the ref strategy, otherwise rollback to full . In the following, we refer to this strategy as  X  X ix X  .

In the following, we call C the set of candidate nodes, disregarding the strategy used to obtain it.
As we report in Table 4, the probability of transitioning from a page with a topical category cat p to another page wi-th the same category, computed over all the sessions, varies depending on the domain of origin for that session. In the cases of Twitter and Facebook , there is a slight tendency to stick to the same topic, whereas for the other domains two consecutive pages in the session tend to belong to different categories. We leverage this information to enrich the ini-tial candidate selection strategy keep only those articles in C that belong to the same topic as p start for Twitter and Facebook , or to a different topic for the other domains.
All the methods use the BrowseGraph information to se-lect an initial set of candidate pages C , according to one of the strategies defined above. After that, a criterion for the selection of the predicted next page among the ones in the set is needed. Next, we describe four algorithms, with their shortnames in parenthesis.
 Random (rand). A simple baseline that selects at random a node in C .
 Content-based (cb). A standard approach to recommend Table 4: Probability that a user navigates between pageviews of the same category. items at cold-start is to select the most similar article to the one the user is currently consuming, according to text-based metrics. When the body of the article is available (35% of articles in our dataset) the similarity is com-puted between the bodies, otherwise their headline (always available) is used. We compute the cosine similarity of the vector representation weighted with TD-IDF of p start with the ones of every p i  X  C . Text is preprocessed with stopword removal and stemming [ 29 ].
 Most Popular (pop). Another typical cold-start recom-mendation approach is to select the most popular item. We recommend the node in C with the highest view count, considering the views until time t  X  1.
 Edge-based (edge). Consider the weight on the edges that encode the likelihood of transition between nodes according to the browsing traces recorded at time t  X  1. Hence, we pre-dict p next to be the node in  X  t  X  1 G ( p start ) with highest weight on the incoming edge from p start . Depending on the initial candidate selection strategy (  X  5.2.1 ), the edges considered (and their weights) will be either the ones in the BrowseGra-ph (for the full selection) or the ones in the ReferrerGraph (for the ref selection).
We apply our prediction strategies to the sessions of 1 , 438 hourly timeslots, for an average of 350 K users per time-slot. We evaluate the goodness of the prediction by measu-ring its overall Precision@1 : a true positive occurs when the predicted page is equal to p next , a false positive when that condition does not hold. Additionally, since all the me-thods we presented lend themselves to produce ranking of pages (based on popularity, similarity, etc. ), we also measure their Mean Reciprocal Ranking for the top 3 news articles ( MRR@3 ). As noticed earlier, there is always a chance that the correct article cannot be possibly predicted be-cause p next might be not included in the set of candidates  X 
G ( p start ) (for example because the article is published at time t and does not exists yet at time t  X  1). We adopt a con-servative approach and we count also these cases as false po-sitives. Figure 8 summarizes the prediction results. To have a more detailed picture of the cases in which the different approaches work best, we report separate evaluation results for the sessions with different referrer domains. Twelve bars for each group represent the precision and MRR scores for the combinations of the three selection strategies ( full , ref , mix ) with the next-node selection methods ( random , cb , pop , edge ). The maximum precision achieved for the diffe-rent domains partially depends on the dimensionality of the session volume for that domain. This is mainly because the smaller the  X  t G ( p start ) set, the higher the probability of gi-ving in output a correct prediction just by chance. The most interesting experimental findings, lie instead in the offsets between different methods X  results within each referrer do-main. First, the random baseline achieves always the worst performance, followed by the content, popularity and edge strategies in order. About the low performance of cb , our hypothesis is that the selection of next article is not driven by patterns of content similarity. In other words, after ha-ving read an article on a topic the user is likely not motivated to keep reading about the same (or similar) topic right after. The pop approach works only slightly better, as it relies on aggregate information about the amount of page visits, but disregarding where such visits came from. The best method by far is edge , meaning that previous transitions from p and p next constitute the stronger signal for the prediction of future transitions. For the social referrer domains it is able to reach up to 48% and 54% in P@1 and MRR@3. For the search domains instead, it reaches up to 27% and 34%. Re-garding the node selection strategies, ref outperforms full in all the three social domains (except for the ref-edge com-bination in Facebook). The fact that a more specific type of recommendation works better suggests that people coming from social networks tend to retrace the same browsing pa-ths other people from the same referrer domain have already explored, with limited serendipitous discovery. The opposite occurs for the search domains, where full beats ref . This may happen because query-driven systems provide a wider range of entry points to the news site than the links posted on social networks, thus making the prediction task harder. The same happens for the homepage, where the variability of content displayed is very wide and dynamic. However, for both families, mix is the most effective strategy that is able to significantly boost even more the precision for social networks and to fill the performance gap with full for the search domain. Homepage, which is the domain originating the highest number of sessions, is the only one in which full has top precision. In this case, the behavior of users is so varied that restricting the options to a subgraph turns out to be detrimental for the prediction quality. Last, when the topical filtering is applied (  X  5.2.2 ), the precision experiences a drop in performance loosing from 10 . 6% up to 69 . 5% (not shown in plots). This happens because discarding too many nodes introduces the high risk of ruling out very good can-didates ( e.g. , a node connected to p start with a high-weight edge). In our case, as shown in Table 4, the probability of transitioning to the same topic (or to a different one) is not far from 0 . 5 in all cases, therefore the topical information is not discriminative enough to filter out nodes without losing the most likely next pages.

The experiments highlight how the referrer URL of a brow-sing session can help to understand the user behavior, pre-dicting the navigation pattern and improving the next-hop recommendation in news browsing. A recommender that uses the weights of the BrowseGraph edges appears to be an effective way to anticipate user needs and keep them longer on site, especially for people coming from social media. This is particularly important, as it as been shown [ 2, 17 , 28 ] that social media platforms are playing an increasingly important role on the news propagation 8 and they are becoming event more critical connections with the news world.
We present an analysis of the browsing traces extracted from a very large viewlog from Yahoo News, introducing the definition of a special case of the BrowseGraph model, na-mely the ReferrerGraph , that consists of a subgraph built from the browsing sessions with homogeneous referrer URL. We find that the browsing graphs of news sites are well-connected despite the tendency to rapid staleness of con-tent and to the typically short user sessions. ReferrerGraphs built considering 9 major domains appear to be quite non-overlapping, to cover articles of different topics, and to lead to the emergence of different sets of most popular articles. Traffic traces coming from different families of referrer do-mains have different time consumption patterns: for exam-ple, the sessions originating from search engines and social networks tend to consume content slightly after the visits coming from the news site homepage, and with some bursty consumption peaks for social networks caused by occasional spread of viral stories. Last, we build on our analytical fin-dings by showing that the ReferrerGraph can be applied to effective article recommendation in a cold-start scenario. In terms of modeling our prediction setup could be extended using continuous-time models, and a comparison between BrowseGraphs built at multiple time scales is also a natural extension. Our experiments mainly prove the point that the referrer domain has a big predictive potential that should be considered when building any browsing user model. As our goal is limited to the prediction of the next page visited, mo-re general content-based techniques for cold-start [ 1, 27 ] are not directly comparable with our approach, although a more extensive comparison would be valuable to gain a broader view on the problem. At any rate, we hope the findings hig-hlighted in this paper lead to a greater consideration of the referrer domain with particular focus on cold-start problems. This research is partially supported by the Ministry of Scien-ce and Innovation of Spain.
