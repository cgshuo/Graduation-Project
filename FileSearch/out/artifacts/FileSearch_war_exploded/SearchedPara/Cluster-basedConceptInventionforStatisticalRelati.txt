 We use clustering to deriv e new relations whic h augmen t database schema used in automatic generation of predictiv e features in statistical relational learning. Entities deriv ed from clusters increase the expressivit y of feature spaces by creating new rst-class concepts whic h con tribute to the cre-ation of new features. For example, in CiteSeer, pap ers can be clustered based on words or citations giving \topics", and authors can be clustered based on documen ts they co-author giving \comm unities". Suc h cluster-deriv ed concepts become part of more complex feature expressions. Out of the large num ber of generated features, those whic h impro ve predictiv e accuracy are kept in the mo del, as decided by sta-tistical feature selection criteria. We presen t results demon-strating impro ved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.
 Categories and Sub ject Descriptors: I.2.6 [Arti cial Intelligence]: Learning General Terms: Algorithms.
 Keyw ords: Relational Learning, Clustering, Feature Gen-eration.
Statistical relational learning and related metho ds searc h a space of database queries or logic expressions to nd those whic h generate new predictiv e features. A given schema, de-scribing bac kground data, is used to structure a searc h over database queries. Eac h query generates a table, whic h in turn is aggregated to pro duce scalar feature candidates. The pro cess pro duces a stream of features, from whic h statisti-cally signi can t predictors are selected. The expressivit y of the generated features is determined by the set of relational entities participating in the searc h.
 Suite 400, Piscata way, NJ 08854.

In this pap er we argue that considerably more powerful mo dels can be built when the original schema is augmen ted with new relations whic h are deriv ed via clustering ( cluster-relations ). Clustering can be used to create rst-class re-lational concepts whic h are not deriv able otherwise from the original relations. The addition of cluster-relations to the schema results in the creation of richer, more expres-sive, feature spaces, resulting in more accurate mo dels than those built from the original relational concepts. In addi-tion to summarizing information (e.g. \Is this documen t on a given topic?"), cluster deriv ed concepts participate in more complex relationships (e.g., \Do es the database con-tain another documen t on the same topic and published in the same conference?"). The creation of these new high-level concepts allo ws more accurate and robust mo deling from complex data sources not simply through information reduction, but, more imp ortan tly, through the increased ex-pressivit y of the language used to describ e patterns in the data [3].
We use a form of statistical relational learning whic h in-tegrates regression with feature generation from relational data. In this pap er we use logistic regression, giving a metho d we call Structural Logistic Regression (SLR). SLR com bines the strengths of classical statistical mo deling with the high expressivit y of features automatically generated from a relational database.

Cluster-relations enter the form ulation of the searc h space used to generate predictiv e features exactly as the original relations. The original database schema is used to decide whic h entities to cluster and whic h attributes to use, for example documen ts clustered by words or by citations cre-ate alternativ e clusterings of the same objects. Once the schema is expanded by adding deriv ed cluster relations to it, the underlying statistical relational learning metho dology is rep eated, i.e. database queries of the feature generation searc h space are evaluated, and the resulting tables per ob-serv ation are aggregated to pro duce scalar feature columns, Figure 1. The new relations added are treated exactly the same as the original relations.

In the rest of the section we brie y describ e SLR. A more detailed description of the metho d and the speci cation of its feature generation algorithm is given in [11]. Section 2.1 describ es the similarit y measure we use for clustering.
SLR is an extension of logistic regression to mo deling rela-tional data. It com bines the strengths of classical statistical Figure 1: Cluster-relations augmen t database schema used to pro duce feature candidates. mo dels with the higher expressivit y of features automati-cally generated from a relational database. SLR dynami-cally couples two main comp onen ts: generation of feature candidates from relational data and their selection using statistical mo del selection criteria. Relational feature gen-eration is a searc h problem. It requires form ulation of the searc h in the space of queries to a relational database. At eac h searc h node, feature candidates are constructed and considered for mo del inclusion. Thus, the pro cess incre-men tally learns predictiv e data patterns, possibly enco ding complex regularities in a domain. The pro cess results in a statistical mo del where eac h selected feature is the evalua-tion of a database query enco ding a predictiv e data pattern.
As men tioned above, relational feature generation is a searc h problem. We use top-do wn searc h of re nemen t graphs [16, 2] as our main searc h space speci cation metho d. Eac h node in the re nemen t graph is a database query . The searc h starts with simpler queries about learning examples and progresses by re ning its nodes, i.e. adding more rela-tion instances and conditions to a paren t query . Since we are building statistical mo dels, rather than logic clauses as is the case in inductiv e logic programming where re nemen t graphs are used, we are not limited to searc hing in the space of binary logic-v alued clauses. In our case, eac h node of the graph is a query evaluating into a table of all satisfying so-lutions. Within eac h node we apply a num ber of aggregate operators to pro duce both boolean and real-v alued features. Eac h node of the re nemen t graph pro duces multiple fea-ture candidates. Aggregations can be applied to a whole table or to individual columns, as appropriate given type restrictions, e.g. ave cannot be applied to a column of a categorical type.

Top-do wn searc h of re nemen t graphs allo ws a num ber of optimizations, e.g. i) the results of queries (prior to applying the aggregations) at a paren t node can be reused at the children nodes, ii) a node resulting in an empt y table for eac h observ ation should not be re ned any further as its re nemen ts will also be empt y.
Throughout the exp erimen ts in this pap er we use k -means clustering algorithm (e.g. [5]) with vector-space cosine sim-ilarit y [14]. In the form ulas presen ted here, documen t d can stand for any object type we want to cluster, and words w are the attributes whic h are used to cluster d . For exam-ple, authors d , can be clustered using the documen ts w they write.

Eac h documen t is view ed as a vector whose dimensions corresp ond to words in the vocabulary; the comp onen t mag-nitudes are the tf-idf weigh ts of the words. Tf-idf is the pro duct of term frequency tf ( w; d )|the num ber of times word w occurs in the corresp onding documen t d |and in-verse documen t frequency where j D j is the num ber of documen ts in a collection and df ( w ) is the num ber of documen ts in whic h word w occurs at least once. The similarit y between two documen ts is then where d i and d j are vectors with tf-idf coordinates as de-scrib ed above.

In the exp erimen ts rep orted here we use binary tf-idf , where the tf comp onen t is a binary indicator. We use this measure to be consisten t with the bac kground relation HasWord . Since we are not using coun ts in the original relation HasWord , we do not bring in this additional information into the at-tributes used to cluster documen ts. Other deriv ed cluster relations use naturally binary attributes, e.g. citation or authorship based clusters. Other similarit y measures and varian ts of tf-idf exist.

The searc h can be extended to include sev eral types of similarit y measures and a searc h over k , the num ber of groups in clustering. These would result in a higher num-ber of features tested in the regression. If needed, on-the-y optimization using subsampling and ecien t linear time clustering algorithm could be used, but in this pap er we did not nd them necessary .
We explore two tasks using CiteSeer data: classifying doc-umen ts into their publication venues, conferences or jour-nals, and predicting the existence of a citation between two documen ts. The target concept pair is &lt;Document, Venue&gt; and &lt;Document, Document&gt; resp ectiv ely. In the case of venue prediction, value of the resp onse variable is one if the pair's venue is a true publication venue of the corresp onding doc-umen t and it is zero otherwise. Similarly , in link predic-tion, value of the resp onse variable is one if there exists a citation between two documen ts and it is zero otherwise. In both tasks, the searc h space con tains queries based on sev eral relations about documen ts and publication venues, suc h as citation information, authorship and word con ten t of the documen ts. Mo deling of laten t structure of entities in this domain, suc h as topics of documen ts or comm unities of authors, is capable of pro ducing more accurate predictiv e mo dels that the original relational represen tation. Clusters can be deriv ed by clustering entities in the domain based on the variet y of alternativ e sources of attributes. The fol-lowing are descriptions of basic relations we use, follo wed by a description and discussion of the deriv ed cluster relations we use to augmen t the searc h space: -PublishedIn( doc :Document, vn :Venue) . Publication venues are extracted by matc hing information with the DBLP database, http://dblp.uni-trier.d e/ . Publication venues are kno wn for 60,646 CiteSeer documen ts. All other relations are pop-ulated with information about these documen ts. There are 1,560 unique conferences and journals. Training and test examples are sampled from this bac kground relation. Rela-tion size: 60,646. -Author( doc :Document, auth :Person) . 53,660 out of the total of 60,646 documen ts have authorship information avail-able; there are 26,740 unique last names of authors. Relation size: 131,582. -Citation( from :Document, to :Document) . This relation con tains all citations among our \univ erse" of 60,646 docu-men ts. It con tains 42,749 unique citing documen ts, 31,603 unique cited documen ts, and the total of 49,398 documen ts. Relation size: 173,410. -HasWord( doc :Document, wor d :Word ). This is by far the largest relation even for relativ ely small vocabularies. It is populated by binary word occurrence vectors, i.e. there is a tuple for eac h word in the vocabulary if it is con tained in a corresp onding documen t. The relation con tains word data available for 56,104 documen ts, the size of vocabulary is 1,000 words. (the vocabulary con tains top coun t words in the entire collection after Porter stemming and stop word remo val). Relation size: 6,894,712.

We use k -means to deriv e cluster relations; any other hard clustering algorithm can be used for this purp ose. The re-sults of clustering are represen ted by binary relations &lt;ClusteredEntity,ClusterID&gt; . Cluster relations can be generated lazily , or they can be precomputed and added to the relational schema before feature generation phase. E-cien t clustering algorithms can be used for documen t cluster-ing based on the regularities characteristic of citation struc-ture in corp ora of scien ti c publications [12].

The original database schema con tains sev eral entities whic h can be clustered based on a num ber of alternativ e crite-ria. Eac h man y-to-man y relation in the original schema presen ted above can pro duce two cluster relations. Three out of four relations are man y-to-man y (with the exception of PublishedIn ), this results in six new cluster-relations. The follo wing is the list of these six cluster relations whic h we add to the relational database schema: -ClustDocsByAuthors( doc :Document , clust :Clu st0) 53,660 documen ts are clustered based on the iden tity of their 26,740 authors. Relation size: 53,660. -ClustAuthorsByDocs( auth :Perso n, clus t :Clust1 ) 26,740 authors are clustered based on 53,660 documen ts they wrote. Relation size: 26,740. -ClustDocsByCitingDocs( doc :Docum ent, clust :Clus t2) 31,603 documen ts are clustered based on 42,749 documen ts citing them (the num bers are sligh tly lower in link prediction where target concept links do not participate in clustering). Relation size: 31,603. -ClustDocsByCitedDocs( doc :Docume nt, clust :Clust 3) 42,749 documen ts are clustered based on 31,603 documen ts cited from them (the num bers are sligh tly lower in link pre-diction where target concept links do not participate in clus-tering). Relation size: 42,749. -ClustDocsByWords( doc :Document, clust :Clust 4) 56,104 documen ts are clustered based on the vocabulary of top 1,000 words. Relation size: 56,104. -ClustWordsByDocs( wor d :Word, clust :Clus t5) The vocabulary of 1,000 words is clustered based on their occurrence in this collection of 56,104 documen ts. Relation size: 1,000.

An imp ortan t asp ect of optimizing cluster utilit y in gen-eral, and of the use of cluster relations in our setting in par-ticular, is the choice of k , the num ber of groups into whic h the entities are clustered. In our case, for eac h poten tial value of k we would ideally compute separate clusters. For simplicit y and speed in the exp erimen ts presen ted here we x k to be equal to 100 in all cluster relations except for the last one, ClustWordsByDocs , where the num ber of clusters is 10. The latter is clustered into few er groups than the rest of the clusters to re ect the fact that there is roughly an order of magnitude few er objects, words, to be clustered; we selected the vocabulary of size 1,000 to mak e the size of HasWord relation smaller and more manageable. The accu-racy of the cluster-based mo dels rep orted below can poten-tially be impro ved even further if one is willing to incur the additional cost of optimizing the choice of k .
We compare mo dels learned from the feature space gener-ated from four original non-cluster relations with the mo dels learned from the original four relations plus six deriv ed clus-ter relations ( clustersNO and clustersYES mo dels). Mo d-els are learned with sequen tial feature selection whic h uses Bayesian Information Criterion (BIC) [15], i.e. as eac h fea-ture is generated it is added to the mo del permanen tly if the BIC impro ves, or is permanen tly dropp ed otherwise. Sequen tial feature selection di ers from standard step-wise mo del selection in that the latter requires kno wing in ad-vance all features whic h will be generated; step-wise mo del selection is much more exp ensiv e as it requires the com-putation of the objectiv e function for all available feature candidates when deciding whic h one to add or drop next, se-quen tial feature selection, on the other hand, re-trains only one additional mo del per one generated feature.

The size of feature streams used in training eac h mo del is set to 3,500 numerically unique features. A numeric sig-nature of partially evaluated features is main tained to avoid fully generating numerically equiv alen t (or rather, at least, nearly collinear within hashing error) features; note that this is di eren t from avoiding syn tactically equiv alen t nodes of the searc h space: two di eren t queries can pro duce numer-ically equiv alen t feature columns, e.g. all zeros, whic h is a common case as feature generation progresses deep er in the searc h space.

We use 10-fold cross validation to sho w accuracy impro ve-men t when using cluster-relations and to deriv e error bounds on the impro vemen t. All observ ations are split equally into 10 sets. Eac h of the sets is used to train a mo del. Eac h of the mo dels is tested on the remaining observ ations. This results in 10 values per eac h tested level, whic h are used to deriv e error bounds. In venue prediction, the total num-ber of observ ations is 10,000: 5,000 positiv e examples of &lt;Document,Venue&gt; target pairs uniformly sampled from the relation PublishedIn , and 5,000 negativ e examples where documen t is uniformly sampled from the remaining docu-men ts and the venue is uniformly sampled from the domain of all venues, suc h that the sampled venue is not a true venue of the documen t. Sampled positiv e pairs are remo ved from the bac kground relation PublishedIn , as well as the tuples involving documen ts sampled for the negativ e set. The size of the bac kground relation PublishedIn reduces by 10,000 after remo ving tuples involved in training and test Figure 2: Learning curv es: venue prediction aver-age test set accuracy against the num ber of features generated from the training sets in 10-fold cross validation (in eac h of 10 runs N train = 1 ; 000 and N test = 9 ; 000 ). Balanced positiv e/negativ e priors sets. In link prediction, the total num ber of observ ations is 5,000: 2,500 positiv e examples of &lt;Document,Document&gt; target pairs uniformly sampled from the Citation relation, and 2,500 negativ e examples uniformly sampled from empt y links in the citation graph. Sampled positiv e pairs are re-moved from the bac kground relation Citation . The size of the bac kground relation Citation reduces by 2,500, the num ber of sampled positiv e examples.

Figure 2 and Figure 3 presen t test accuracy learning curv es for mo dels learned with and without cluster relations in venue prediction and link prediction resp ectiv ely. Curv e coordinates are averages over the runs in 10-fold cross val-idation (see below separate gures and discussion of error bounds of the di erence between accuracies in both mo d-els). The learning curv es sho w test set accuracy changing with the num ber of features, in interv als of 250, generated and sequen tially selected from the training set. The average test set accuracy of the cluster based mo dels after exploring the entire feature stream is 87.2% in venue prediction and 93.1% in link prediction, whic h is, resp ectiv ely, 4.75 and 3.22 percen tage points higher than the average accuracy of the mo dels not using cluster relations.

Figure 4 and Figure 5 presen t Gaussian 95% con dence in-terv als of the di erence in mean test accuracies of clusterYES and clusterNO mo dels in venue prediction and link predic-tion resp ectiv ely. In venue prediction, after exploring ap-pro ximately half of the feature stream the impro vemen t in accuracy by the cluster-based mo dels is statistically signif-ican t at 95% con dence level according to the t -test (con-dence interv als do not intersect with y =0). In the early Figure 3: Learning curv es: link prediction aver-age test set accuracy against the num ber of fea-tures generated from the training sets in 10-fold cross validation (in eac h of 10 runs N train = 500 and N test = 4 ; 500 ). Balanced positiv e/negativ e priors feature generation, when considering the streams of about 1,000 features, cluster-based mo dels perform signi can tly worse: at this learning phase, additional cluster-based fea-tures while not yet signi can tly impro ving accuracy may dela y the disco very of signi can t non-cluster based features. In link prediction, while the signi cance of the impro vemen t from cluster-based features is reduced early in the stream, it con tinuously increases throughout the rest of the stream. At the end of the stream the impro vemen t in accuracy of the cluster-based mo del is 3.22 percen tage points, statisti-cally signi can t at the 99.8% con dence level. The highest level accuracies (after seeing 750 features by clustersNO and after seeing 3500 features by clustersYES ) also sta-tistically di er: the accuracy impro vemen t in cluster-based mo dels is 1.49 percen tage points, signi can t at the 99.9% con dence level. The average num ber of features selected in 10 clusterYES mo dels is 32.0 in venue prediction and 32.3 in link prediction; resp ectiv ely, 27.9 and 31.8 features on average were selected into clusterNO mo dels from equally man y feature candidates (3,500).

The impro ved accuracy of the cluster-based mo del in venue prediction comes mostly from a single cluster-based feature. This feature was selected in all cross validation runs. It is a binary feature involving laten t documen t topics, i.e. the cluster relation of documen ts clustered by their word con-ten t. The feature is ON for target documen t/v enue pair &lt;D,V&gt; , if there exists a documen t D1 in the cluster where D belongs suc h that D1 is published in the same venue as D . Using a logic based notation, the feature is the follo wing Figure 4: Mean venue prediction accuracy dif-ference, accur acy ( cluster sY ES ) accur acy ( cluster sN O ) with 95% con dence interv als (bounds based on N =10 points, t -test distribution) Figure 5: Mean link prediction accuracy dif-ference, accur acy ( cluster sY ES ) accur acy ( cluster sN O ) with 95% con dence interv als (bounds based on N =10 points, t -test distribution) (abbreviate here clustDocsByWords by topic ): 1 exists [ publ ishedI n ( D 1 ; V ) ; topic ( D; C ) ; topic ( D 1 ; C )] :
The follo wing are examples of other signi can t features automatically generated and selected in the venue prediction task: documen t D is more likely to be published in a confer-ence or journal V : i) if V is a large venue, i.e. publishes man y pap ers ( size [ publ ishedI n ( ; V )]), ii) if documen t D cites an-other documen t whic h is published in the same venue V , iii) if documen t D is cited by another documen t whic h is pub-lished in the same venue V , iv) if the author of D published another pap er in the same venue V .

The follo wing three cluster-based features were selected in more than ve cross validation runs (9, 9 and 6 times resp ectiv ely) in the link prediction task (target: &lt;D1,D2&gt; ): exists [ docsB yCitedD ocs ( D 1 ; C ) ; docsB yCitedD ocs ( D 2 ; C )] ; exists [ docsB yW ords ( D 1 ; C ) ; docsB yW ords ( D 2 ; C )] ;
Clustering and other laten t space mo deling metho ds suc h as principal comp onen ts analysis often used in prop ositional predictiv e mo deling as a means for dimensionalit y reduc-tion. Dimensionalit y reduction is achiev ed by replacing the original at features with the iden ti ers of clusters they are elemen ts of, or by the coordinates of their pro jections onto a lower dimensional space. For example, words can be clus-tered into groups replacing individual words for documen t classi cation. Structure together with the at features can result in more accurate predictiv e mo dels, for example in the con text of maxim um entrop y mo deling [9].

One researc h direction in relational learning addresses clus-tering of relational entities with novel distance metrics de-ned over the interlink ed relational represen tation. Man y people have address clustering from relational represen ta-tion (see e.g. [6]). It is not our goal to nd a single \best" data partitioning. Instead, we iden tify a num ber of alterna-tive clusterings whic h are involved in more complex features impro ving predictiv e accuracy of statistical mo dels. Ob jects may be clustered based on di eren t attributes, using di er-ent similarit y measures, and with di eren t num bers of clus-ters found. The usefulness of a grouping can be assessed only in relation to a particular set of predictions being made.
Cluster-based concept and relation invention, as describ ed in this pap er, di ers imp ortan tly from using aggregation, in a sense commonly used in databases, as a means of sum-marization. Aggregation is essen tial in statistical relational learning and is also used to create new, rich types of features from relational represen tation [10]. Using aggregates creates richer features than mo deling a boolean, table empt y/non-empt y feature as is the case in classical logic-based relational learning approac hes [8]. The need for aggregates in rela-tional learning comes from the fact that the cen tral type of relational represen tation is a table (set); the data is repre-sen ted by a num ber of tables, and database queries result in tables. Statistical mo dels, on the other hand, work with scalar values, real num bers, integers, or categorical vari-1 Note that D1 is distinct from D as the tuple with publica-tion venue of documen t D is remo ved from the bac kground relation PublishedIn . ables. Aggregates are essen tial to our approac h; eac h node in our searc h space evaluates into a table, whic h in turn is aggregated to pro duce a num ber of scalar feature candidates. The adv antage of clusters comes at another level to create cen tral relational entities from whic h features are generated; aggregates are applied at the next step to the tables result-ing from queries whic h can involve both the cluster relations and the original relations.

The idea of augmen ting the existing represen tation with new relations or predicates is, of course, not new. In in-ductiv e logic programming it is kno wn as \predicate inven-tion". For example, Statistical Predicate Invention [1] whic h was prop osed for learning in hypertext domains, represen ts classi cations pro duced by Naiv e Bayes as a new predicate added to FOIL [13]. Our approac h di ers in that we use statistics rather than logic as a cen tral mo deling comp onen t, and more imp ortan tly in this con text, we adv ocate the use of cluster-based relation invention as a means to enric h fea-ture spaces by adding to schema man y types of clusters, not only those of a resp onse concept, thus creating rst-class re-lational concepts, suc h as \topics" or \comm unities", whic h have a clean \iden tity" as the world represen tation entities.
Concept invention could also, in theory be done in other types of relational learning, suc h as in those using graphi-cal mo dels, e.g. Probabilistic Relational Mo dels (PRMs) [4], whic h are generativ e mo dels of join t probabilit y distribution capturing probabilistic in uences between entities and their attributes in a relational domain. However, suc h generativ e mo dels are not conduciv e to searc hing for complex features, as is done in inductiv e logic programming and in this pap er.
We presen ted a framew ork for learning predictiv e statisti-cal mo dels from relational data where alternativ e \cluster-relations" are deriv ed from the attributes in the original database schema and included in the feature generation pro-cess. The metho d was used for predicting the publication venue of scien ti c pap ers from the CiteSeer data including the citation graph, pap er authorship and word con ten t. We used clustering to deriv e new rst class relational entities re ecting hidden topics of pap ers, author comm unities and word groups. New cluster relations included into the feature generation pro cess, in addition to the original relations, re-sulted in the creation of richer cluster-based features, where clusters enter into more complex relationships with existing bac kground relations rather than only pro vide dimensional-ity reduction. Using relation invention gives more accurate mo dels than those built only from the original relations.
Adding cluster-deriv ed concepts as relations to a database schema used to generate features can increase the predictiv e accuracy of statistical relational mo dels. Section 4 presen ts exp erimen tal results for venue prediction and link predic-tion. In both tasks, we have sho wn that mo dels built using a schema augmen ted with cluster-deriv ed relations result in statistically signi can t increase in accuracy over the mo dels built using the original relational schema.

Tw o mo dels, with and without cluster relations, were com-pared on feature streams of 3,500 unique features. In the middle of the feature generation pro cess for the venue pre-diction task, the cluster-based feature generation was able to disco ver a highly signi can t feature from the clusters of documen ts group ed by their word con ten t whic h con tributed a mean of 4.75 percen tage point increase in the nal test set accuracy . Through the end of the feature generation pro-cess the cluster-based mo del main tained a stable accuracy impro vemen t, and was statistically signi can t at the 95% con dence level based on the t -test over 10-fold cross valida-tion. The average impro vemen t from using cluster-deriv ed features at the end of the link prediction feature stream was 3.22 percen tage points, also statistically signi can t at the 95% con dence level.

We envision sev eral impro vemen ts to the relation inven-tion metho dology . Ric her types of clusters can be deriv ed from more complex sets of attributes than those immedi-ately available in a single relation. For example, publication venues and authorship data are in two separate relations whic h both can be used to cluster publication venues based on the authors who publish in them. Also, clustering can be performed lazily as a corresp onding depth in the fea-ture searc h space is reac hed by the feature generation. In con trast to \prop ositionalization" [7], whic h implies a de-coupling of relational feature generation and mo deling, SLR is dynamic and allo ws for a more natural introduction of this extension.
