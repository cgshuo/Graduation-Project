 Peking University basic methods (including lexicon-based methods and corpus-based methods) for cross-lingual sentiment classification by simply leveraging machine translation services to eliminate the view and the Chinese view based on additional unlabeled Chinese data. Experimental results on two test sets show the effectiveness of the proposed approach, which can outperform basic methods and transductive methods. 1. Introduction
Sentiment classification is the task of identifying the sentiment polarity of a given text, which is traditionally categorized as either positive or negative. In recent years, senti-ment classification has drawn much attention in the natural language processing (NLP) field and it has many useful applications, such as opinion mining and summarization (Liu, Hu, and Cheng 2005; Ku, Liang, and Chen 2006; Titov and McDonald 2008). for sentiment classification. The lexicon-based methods rely heavily on a sentiment lexicon containing positive terms and negative terms. The corpus-based methods rely heavily on an annotated corpus for training a sentiment classifier. The sentiment lexicon and corpus are considered the most valuable resources for the sentiment classification task. However, such resources in different languages are rather unbalanced. Because most previous work focuses on English sentiment classification, many annotated sen-timent lexica and corpora for English sentiment classification in various domains are freely available on the Web. However, the annotated resources for sentiment classifica-tion in many other languages are not abundant and it is time-consuming to manually label a rich and reliable sentiment lexicon or corpus in those languages. The challenge before us is leveraging rich English resources for sentiment classification in other languages. In this study, we focus on the problem of English-to-Chinese cross-lingual sentiment classification, leveraging only English sentiment resources for sentiment clas-sification of Chinese product reviews, without using any Chinese sentiment resources.
Note that this problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages. The proposed approach in this study can also be applied for generic cross-lingual text categorization tasks.
 classification in Romanian (Mihalcea, Banea, and Wiebe 2007; Banea et al. 2008), and the methods are very straightforward. First, they use machine translation for translating resources (such as a lexicon or corpus) between Romanian and English, and then they employ the lexicon-based or corpus-based method for subjectivity classification in either Romanian or English. Similar experiments have been performed for subjectivity classification in Spanish (Banea et al. 2008). However, our empirical study shows that sentiment classification performance using these methods is far from satisfactory be-cause the machine translation quality is not very good according to the recent NIST open machine translation evaluation results, and thus a language gap between the original language and the translated language still exists.
 classification, and then propose a bilingual co-training approach to improve the accuracy of corpus-based polarity classification of Chinese product reviews. Unlabeled
Chinese reviews can be fully leveraged in the proposed approach. First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews. Then, we can view the classification problem in two different ways: the
Chinese view with only Chinese features and the English view with only English features. We then use the co-training approach to make full use of the two redundant views of features. The SVM classifier (Joachims 2002) is adopted as the basic classifier in the proposed approach.

Bing Translate ) are used for review translation in the experiments. The experimental results on two test sets show that the proposed approach based on any machine transla-tion service can outperform a few popular baselines, including advanced transductive methods. We also find that the balanced growth of the positive and negative instances at each iteration in the co-training algorithm is very important for the success of the algorithm.

Section 3 introduces several basic methods. The proposed co-training approach is de-scribed in detail in Section 4. Sections 5 and 6 present the evaluation set-up and results, respectively. Lastly, we conclude this article and discuss future work in Section 7. 2. Related Work 2.1 Sentiment Classification
Sentiment classification can be performed on words, sentences, or documents. In this article we focus on document-level sentiment classification, and research in this area has followed a lexicon-based (i.e., rule-based) or a corpus-based (i.e., classification-based) approach.
 sentiment lexica. Turney (2002) predicts the sentiment orientation of a review as the average semantic orientation of the phrases in the review that contain adjectives or ad-verbs, which is known as the semantic orientation method. Kim and Hovy (2004) build 588 three models to assign a sentiment category to a given sentence by combining the indi-vidual sentiments of sentiment-bearing words. Kanayama, Nasukawa, and Watanabe (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. and they use a labeled corpus to train a sentiment classifier. Since the work of Pang, Lee, and Vaithyanathan (2002), various classification models and linguistic features have been proposed to improve classification performance (Mullen and Collier 2004; Pang and Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005). More recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of a text at varying levels of granularity. Blitzer, Dredze, and Pereira (2007) investigate domain adaptation for sentiment classifiers, focusing on on-line reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. A non-negative matrix tri-factorization approach has been proposed for sentiment classification, which learns from lexical prior knowledge in the form of domain-independent sentiment-laden terms in conjunction with domain-dependent unlabeled data and a few labeled data (Li, Zhang, and Sindhwani 2009). Dasgupta and
Ng (2009) propose a semi-supervised approach to sentiment classification where they first use spectral techniques to mine the unambiguous reviews and then exploit them to classify the ambiguous reviews by a novel combination of active learning, transductive learning, and ensemble learning.
 such work uses similar lexicon-based or corpus-based methods for Chinese sentiment classification.
 sources for sentiment analysis in other languages. Standard naive Bayes and SVM classifiers have been applied for subjectivity classification in Romanian and Spanish (Mihalcea, Banea, and Wiebe 2007; Banea et al. 2008), and the results show that auto-matic translation is a feasible alternative for the construction of resources and tools for subjectivity analysis in a new target language. Wan (2008) focuses on leveraging both
Chinese and English lexica to improve Chinese sentiment analysis by using lexicon-based methods. Wei and Pal (2010) apply structural correspondence learning (SCL) to minimize the noise introduced by machine translations. In this study, we focus on developing novel approaches to improve the corpus-based method for cross-lingual sentiment classification of Chinese product reviews. 2.2 Cross-Domain Text Classification
Cross-domain text classification can be considered as a more general task than cross-lingual sentiment classification. In this task, the labeled and unlabeled data come from different domains and their underlying distributions are often different from each other, which violates the basic assumption of traditional supervised learning.
 dressing the cross-domain text classification problem by transferring knowledge across domains, and such algorithms include Transductive SVM (Joachims 1999), EM (Nigam et al. 2000), EM-based naive Bayes classifier (Dai et al. 2007a), Topic-bridged PLSA (Xue et al. 2008), Co-Clustering X  X ased classification (Dai et al. 2007b), and the two-stage approach (Jiang and Zhai 2007). Dai et al. (2007b) use co-clustering as a bridge to propagate the class structure and knowledge from the in-domain to the out-of-domain.
Jiang and Zhai (2007) look for a set of features generalizable across domains at the first generalization stage, and then pick up useful features specific to the target domain at the second adaptation stage. Daum  X  e III and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model. In recent years, a few methods/algorithms have been proposed for cross-domain sentiment classification, including structural correspondence learning (Blitzer, Dredze, and Pereira 2007), cross-domain graph ranking (Wu et al. 2009), and spectral feature alignment (Pan et al. 2010). sification, which can be considered a special case of cross-domain text classification.
Bel, Koster, and Villegas (2003) empirically investigate three translation strategies for cross-lingual text categorization: document translation, terminology translation, and profile-based translation. A few novel models have been proposed to address the problem X  X or example, the EM-based algorithm (Rigutini, Maggini, and Liu 2005), the information bottleneck approach (Ling et al. 2008), multilingual domain models (Gliozzo and Strapparava 2005), and the structural correspondence learning approach (Prettenhofer and Stein 2010; Wei and Pal 2010). Shi et al. (2010) introduce a method to transfer classification knowledge across languages by translating the model features and using an EM algorithm. The most recent related work includes multilingual text categorization based on multi-view learning (Amini, Usunier, and Goutte 2009; Amini and Goutte 2010). To the best of our knowledge, co-training has not yet been investi-gated for cross-domain or cross-lingual text classification. 3. The Basic Methods
A straightforward method for cross-lingual sentiment classification is to use machine translation for transferring lexica or corpora of reviews between English and Chinese, and then apply the lexicon-based or corpus-based method for sentiment classification in either the English or Chinese language. Therefore, the basic methods consist of two main steps: resource translation and sentiment classification. According to different transla-tion directions and classification methods, four basic methods are introduced as follows. 3.1 Lexicon-Based Method in English Language: LEX(EN)
This method first translates Chinese reviews into English reviews, and then identifies the sentiment polarity of the translated English reviews based on English sentiment lexica, as illustrated in Figure 1.

Wan (2008) to compute the semantic orientation value of a review. The unsupervised approach is quite straightforward and it makes use of the following sentiment lexica: positive Lexicon (Positive Dic) containing terms expressing positive polarity, Lexicon (Negative Dic) containing terms expressing negative polarity,
Lexicon (Negation Dic) containing terms that are used to reverse the semantic polarity of a particular term, and Intensifier Lexicon (Intensifier Dic) are used to change the degree to which a term is positive or negative. The semantic orientation value for a review is computed by summing the polarity values of all terms in the review, making use of both the word polarity defined in the positive and negative lexica and the contextual valence shifters defined in the negation and intensifier lexica. 590
For example, given a review of the image quality is not good positive term, the use of the negation term not reverses the polarity orientation value, and the overall polarity orientation value of the review is negative. Given a review of the image quality is very good , the use of the intensifier term orientation value of good , and the overall polarity orientation value of the review is positive. In our study, the scope of a negation or intensifier term is simply determined by using a distance window of two words. We do not use a parser to determine the scope because the parsing results for the translated reviews are not reliable. labeled as negative; otherwise, the review is labeled as positive. 3.2 Lexicon-Based Method in Chinese Language: LEX(CN)
This method first translates English sentiment lexica into Chinese lexica, and then identifies the sentiment polarity of Chinese reviews based on the translated Chinese lexica, as illustrated in Figure 2.
 semantic orientation value computation used in Wan (2008) to predict the polarity orientation of the Chinese reviews. Each Chinese review is first segmented into Chinese terms/words by using our in-house conditional random field (CRF) X  X ased Chinese word segmentation tool, and then the polarity orientation value for the Chinese review is computed by summing the polarity values of all terms in the review. The terms defined in the negation lexicon are used to reverse the polarity values of the nearby
Chinese terms, and the terms defined in the intensifier lexicon are used to intensify the polarity values of the nearby Chinese terms. The scope of a negation or intensifier term is also simply determined by using a distance window of two words. 3.3 Corpus-Based Method in English Language: SVM(EN) then translate test Chinese reviews into English reviews. Lastly, we use the classifier to classify the translated English reviews. In this study, we use the widely used SVM classifier for classification. We also use a transductive variant of the SVM classifier for making use of unlabeled Chinese reviews, which will be described in Section 5.5. All
English unigrams and bigrams are used as features, and the feature weight is simply set polarity orientation of the review. 3.4 Corpus-Based Method in Chinese Language: SVM(CN)
As illustrated in Figure 4, we first translate labeled English reviews into Chinese re-views, and then learn a classifier based on the translated Chinese reviews with labels. 592
Lastly, we use the classifier to classify test Chinese reviews. We also use the SVM classifier (and a transductive variant) for classification, and all Chinese unigrams and 4. The Bilingual Co-Training Method 4.1 Overview These two basic corpus-based methods have been used in Banea et al. (2008) for
Romanian subjectivity analysis. As shown in our later experiments, the two methods do not perform well for Chinese sentiment classification, because the term distributions in the original reviews and the translated reviews are different. One reason is attributed to machine translation. Because current machine translation services cannot accurately translate reviews, it is inevitable that they bring errors into the translated texts. More-over, it may happen that different terms are used to express the same meaning in the original texts and the translated texts, because each machine translation service uses particular resources and corpora for model building. The other reason is attributed to inherent domain difference. The review sets in different languages are generally in very different domains, because they are written by different users in different countries, and the writing styles, lengths, and term usages of the reviews are very different. make use of some amounts of unlabeled Chinese reviews to improve the classification accuracy. The co-training approach can make full use of both the English features and the Chinese features in a unified framework. The framework of the proposed approach is illustrated in Figure 5.
 phase, the input is the labeled English reviews and some amount of unlabeled Chinese the unlabeled Chinese reviews are translated into unlabeled English reviews by using machine translation services. Therefore, each review is associated with an English ver-sion and a Chinese version. The English features (i.e., all English unigrams and bigrams) and the Chinese features (i.e., all Chinese unigrams and bigrams) for each review are considered two different and redundant views of the review. The co-training algorithm is then applied to learn two classifiers, and finally the two classifiers are combined into a single sentiment classifier. In the classification phase, each unlabeled Chinese review 594 for testing is first translated into an English review, and then the learned classifier is applied to predict the polarity orientation of the review as either positive or negative. 4.2 The Co-Training Algorithm
The co-training algorithm (Blum and Mitchell 1998) is a bootstrapping method; it starts with a set of labeled data, and increases the amount of annotated data using some amount of unlabeled data in an incremental way. One important aspect of co-training is that two conditionally independent views are required for co-training to work, but the independence assumption can be relaxed. In the past, co-training has been successfully applied to statistical parsing (Sarkar 2001), reference resolution (Ng and Cardie 2003), part-of-speech tagging (Clark, Curran, and Osborne 2003), word sense disambigua-tion (Mihalcea 2004), and e-mail classification (Kiritchenko and Matwin 2001). Co-training has not yet been used for cross-domain or cross-lingual text categorization, however.
 predict the class of an example, it can provide one more training example for the other it does not mean that this example will be easily classified by the second classifier, so the second classifier will get useful information to improve itself, and vice versa (Kiritchenko and Matwin 2001).
 or unlabeled Chinese review has two sets of features: English features and Chinese fea-tures. Here, a review is used to indicate both its Chinese version and its English version, unless stated otherwise. Now we describe the details of the co-training algorithm. the model during the bootstrapping phase, and only for these reviews are the labels obtained using an average of the normalized prediction values determined by the component classifiers. In the algorithm, p and n are two parameters controlling the growth size in the labeled data. At each iteration, at most 2( into L . The two parameters also maintain the class distribution in the labeled data by balancing the parameter values of p and n at each iteration. If is called balanced growth , otherwise the growth is called the co-training algorithm used in this study differs slightly from the original co-training algorithm in that the original co-training algorithm is dependent on the sequence of the two component classifiers, whereas our co-training algorithm is independent of the classifier sequence. Moreover, each classifier in our co-training algorithm not only makes use of a few examples confidently predicted by the other classifier, but also makes use of a few examples confidently predicted by itself.

C en and C cn . Typical text classifiers include Support Vector Machine (SVM), naive Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN), and so forth. In this study, we adopt the widely used SVM classifier (Joachims 2002), as in the basic corpus-based methods. Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in this space by maximizing the margin between the two data sets. The output value of the SVM classifier for a review indicates the confidence level of the review X  X  classification. The sentiment polarity of a review is indicated by the sign of the prediction value. Note that we use all unigrams and bigrams in each language as features and the feature weight is simply set to term frequency.
 and C cn . Therefore, in the classification phase, we can obtain two prediction values for a test review. We normalize the prediction values into [  X  1, 1] by dividing the maximum absolute value. Finally, the average of the normalized values is used as the overall learning field. Blum and Mitchell (1998) prove that co-training can be successful if the two sufficient and redundant views are conditionally independent of each other. Abney (2002) shows that weak dependence between the two views can also guarantee success-ful co-training. Balcan, Blum, and Yang (2005) prove that a weaker assumption called  X  -expansion is sufficient for iterative co-training to succeed. Wang and Zhou (2010) view the co-training process as a combinative label propagation over two views, and they 596 provide the sufficient and necessary condition for co-training to succeed. As can be seen, the assumption about the dependence between the two views is much relaxed, which can guarantee that although the English features and the Chinese features are not conditionally independent of each other, the use of the two views for co-training is acceptable. In the extreme case, if both classifiers agree on all the unlabeled data, labeling the data does not create new information, and thus the co-training algorithm will not work at all. We will show in the experiments that the English classifier and the
Chinese classifier disagree on many unlabeled examples, which can also guarantee the success of the co-training approach. 5. Evaluation Set-up 5.1 English Sentiment Resources
The basic LEX(EN) and LEX(CN) methods require English sentiment lexica. In this study, we collected and used the following popular and publicly available English and Hoffmann 2005; Wilson et al. 2005). The clues in this file were collected from a number of sources. Some were culled from manually developed resources (e.g., and unannotated data. A majority of the clues were collected as part of the work reported in Riloff and Wiebe (2003).
 the same file.
 110,834 entries for projecting English lexica into Chinese lexica via term-to-term trans-lation. If an English term corresponds to multiple Chinese terms, we simply use the first Chinese term for translation because the first one is the dominant translation. beled English sentiment corpus. In this study, we used the following popular English sentiment corpus: available on the Web; we used the corpus constructed for multi-domain sentiment and it was within similar domains to the test set. The data set consists of 8,000 Amazon product reviews (4,000 positive reviews + 4,000 negative reviews) for four different product types: books, DVDs, electronics, and kitchen appliances. 5.2 Chinese Review Sets The following two data sets were collected and used as test sets in the experiments: performance of the proposed approach, we collected and labeled 886 product reviews (451 positive reviews + 435 negative reviews) from the popular Chinese IT product Web cameras, and laptop computers. lected and labeled 930 product reviews (560 positive reviews + 370 negative reviews) on such products as electronics and furniture.
 polarity tags of the reviews were first annotated by one subject and then checked by the other subject. The conflicts were resolved by discussion.
 methods and the co-training method: unlabeled set and the IT168 test set were in the same domain and had similar underlying feature distributions, but the unlabeled set and the 360BUY test set may be in different domains.
 and the test set was blind to the training phase. All these data sets are available upon request. 5.3 Review Translation
For all the data sets described in Sections 5.1 and 5.2, each Chinese review was translated
Therefore, each review has two views: the English view and the Chinese view. A review is represented by both its English view and its Chinese view.
 NLP field (Lopez 2008), though the translation performance is far from satisfactory.
A few commercial machine translation services can be publicly accessed, for ex-ample, Google Translate (GoogleTranslate), 16 Yahoo Babel Fish 598
Microsoft Bing Translate (MicrosoftTranslate). 18 The three MT systems are considered to be state-of-the-art commercial machine translation systems, and all three MT systems provide Chinese-to-English and English-to-Chinese translation services. However, it is not easy to accurately compare the translation performance of the three MT systems because the three systems are updated frequently. In the experiments, we adopt all of them for both English-to-Chinese translation and Chinese-to-English translation. English reviews, including the manual translation results (HumanTranslate): Chinese reviews: 5.4 Evaluation Metric We used the standard precision ( P ), recall ( R ), and F-measure ( formance of positive and negative classes, and employed the accuracy metric ( measure the overall performance of each system. The metrics are defined in the same way as in generic text categorization tasks. 5.5 Baseline Methods
In the experiments, the proposed co-training approach (CoTrain) is compared with two groups of baseline methods. sentiment classification of Chinese reviews based on Chinese resources.
 tion, and it is lexicon-based. It uses the most popular and publicly available Chinese as LEX(CN) for semantic orientation value computation. The four Chinese lexica were collected as follows: released by HOWNET.
 clumsy ) were collected from the Chinese VSA released by HOWNET.
 from related papers.
 collected from the Chinese VSA released by HOWNET.
 in the Chinese language. We downloaded a very large number of product reviews and their associated tags from the popular Chinese online shopping Web site Amazon
The reviews are about various products such as consumer electronics, mobile phones, digital products, books, and so on. The polarity tag of each review was automatically judged by the number of the user-assigned stars attached to the review. If the star number is equal to or less than two, the review is labeled as negative, and otherwise the review is labeled as positive. We adopt the inductive SVM classifier and use the Chinese reviews.
 tion in the Chinese language. We adopt the transductive SVM classifier, and use the automatically crawled corpus used in BaseCN2 and the unlabeled Chinese reviews for training.
 randomly partitions the original test set into five subsets. During each cross-validation process, a single subset is retained as the validation set, and the remaining four subsets are used as the training set. The inductive SVM classifier is trained on the training set and tested on the validation set. The cross-validation process is then repeated five times, and the five results are then averaged. Note that the results are produced by five differ-ent classification models that are different from other methods. The performance of the cross-validation method can be seen as an upper bound for the monolingual methods, because the method uses human-labeled Chinese reviews for training, and moreover, the training reviews and the test reviews come from the same Web site and thus they are in the same domain. The method is denoted UpperBound(CrossValidation) perform sentiment classification of Chinese reviews based only on English resources. 600 described in Section 3.2.
 described in Section 3.1.
 sentiment classification in the Chinese view, as described in Section 3.4. Only English-to-Chinese translation is needed. The inductive SVM learner aims to build a decision function based on the training set, and the unlabeled set is not used by this method. sentiment classification in the English view, as described in Section 3.3. Only Chinese-to-English translation is needed. The unlabeled set is not used by this method. averaging the prediction values of the two SVM classifiers in the same way as in the co-training approach.
 for sentiment classification in the Chinese view. Only English-to-Chinese translation is needed. The unlabeled set is used by this method. Transductive SVM has been widely used to treat partially labeled data in semi-supervised learning. Different from inductive
SVM, it can leverage unlabeled data and try to separate both labeled and unlabeled data with a maximum margin. For more details, refer to Joachims (1999).
 for sentiment classification in the English view. Only Chinese-to-English translation is needed. The unlabeled set is used by this method.
 averaging the prediction values of the two TSVM classifiers.
 the unlabeled set for sentiment classification in the Chinese view. The algorithm is a single-view weakly supervised algorithm. It starts with a set of labeled reviews, and builds a SVM classifier. The classifier is then applied to the unlabeled reviews, and the p positive and n negative most confidently predicted reviews are added to the labeled set. The classifier is then retrained on the new labeled set. The process continues for iterations. The parameters p , n ,and I are defined in the same way as for the co-training algorithm.
 for sentiment classification in the English view.

Train(CN) by averaging the prediction values of the two self-training classifiers. It is noteworthy that SelfTrain(ENCN) differs from CoTrain in that there is no mutual inter-action between the English component classifier and the Chinese component classifier in SelfTrain(ENCN).
 strong baselines because they have been widely used for improving classification accu-the linear kernel and default parameter values for both inductive SVM classification and transductive SVM classification.

Gain [IG], and Mutual Information [MI]) can be used for dimension reduction, we use all the features in the experiments for comparative analysis because there is no significant performance improvement after applying the feature selection techniques in our empirical study. 6. Evaluation Results 6.1 Method Comparison
In the experiments, we first compare the proposed co-training approach with the base-line methods. The parameter values for CoTrain and SelfTrain are set as p = n = 5. The three parameters are empirically set by considering the total number (i.e., 2,000) of the unlabeled Chinese reviews. In our empirical study, the proposed approach can perform well with a wide range of parameter values, which will be shown later. bound on the two test sets, respectively. Tables 2 through 4 show the comparison results for the cross-lingual methods based on the three machine translation services on the
IT168 test set, respectively. Tables 6 through 8 show the comparison results for the cross-lingual methods based on the three machine translation services on the 360BUY test set, respectively. Note that we also present the classification results for the two component classifiers (Chinese component classifier C cn and English component classifier our proposed co-training approach. Tables 9 and 10 show the results of significance tests between CoTrain and the baseline methods on the two test sets, respectively. We sign tests in the experiments. The p-values for sign tests are presented; the performance difference between CoTrain and a baseline method is statistically significant at a 95% level if the p-value is smaller than 0.05.
 is used, the proposed co-training approach (CoTrain) outperforms all baseline methods on the overall accuracy metric and most other metrics on the two test sets. In particular, on the IT168 test set, the best accuracy is achieved by CoTrain with GoogleTranslate, and on the 360BUY test set, the best result is achieved by CoTrain with YahooTranslate.
Even the two component classifiers in CoTrain can perform as well as or better than the baseline methods. As can be seen from Tables 9 and 10, the performance difference 602 between CoTrain and any baseline method is always statistically significant when
GoogleTranslate or YahooTranslate is used for machine translation. We can also see that the performance difference between CoTrain and any baseline method is almost always statistically significant when MicrosoftTranslate is used for machine translation, except for the TSVM(CN) baseline on the IT168 test set and the TSVM(ENCN) and TSVM(CN) baselines on the 360BUY test set.
 very similar to CoTrain, and it combines the results of two classifiers in the same way.
However, the co-training approach can train two more effective component classifiers than those used in TSVM(ENCN). As suggested from the tables, the accuracy values of the component classifiers ( C cn and C en ) in CoTrain are almost always higher than those of the corresponding TSVM(CN) and TSVM(EN), based on any machine translation service. The reason is that TSVM(CN) and TSVM(EN) leverage the unlabeled data independently, while the two component classifiers in the co-training approach leverage the unlabeled data in a mutual way, and more useful knowledge in the unlabeled data can be incorporated into the co-training approach. We can also see that the co-training approach outperforms the baseline self-training approach, which further demonstrates the great importance of the mutual influence of the two views during the bootstrapping phase.
 co-training approach are required to disagree on some unlabeled examples, and we 604 show the disagreement ratio between the two classifiers at each iteration in Figure 6.
At each iteration in the co-training algorithm, we use the two classifiers to predict the polarity tags of the unlabeled examples, respectively. The disagreement ratio is computed by dividing the number of the consistently predicted examples by the size of the unlabeled set. We can see from the figure that the disagreement ratio is always higher than 20%, which guarantees the success of the co-training approach. the LEX(EN) method performs better than the BaseCN1 method, but the LEX(CN) method performs worse than the BaseCN1 method. The reason is that the sentiment lexica used in LEX(CN) are automatically translated from the original English lexica, and the translation is very inaccurate because there are no contexts or clues for sense disambiguation during the translation process.
 the BaseCN2 and BaseCN3 methods outperform the BaseCN1 method. However, the BaseCN2 and BaseCN3 methods cannot outperform the strong cross-lingual baseline 606 methods (e.g., TSVM(ENCN), SelfTrain(ENCN)), because the Chinese training corpus is automatically collected without human checking and thus about 10% of the reviews are mistakenly labeled. Moreover, the corpus is collected from a different Web site, and thus the training set and the test set may be in different domains. We also note that no methods can outperform the monolingual upper bound (the cross-validation method), because it leverages in-domain human-labeled training set for model learning. most always outperform the corresponding inductive SVM classifiers on the two test sets. More specifically, the BaseCN3 method outperforms the BaseCN2 method; the TSVM(CN), TSVM(EN), and TSVM(ENCN) methods almost always outperform the
SVM(CN), SVM(EN), and SVM(ENCN) methods, respectively, except that TSVM(CN) cannot outperform SVM(CN) on the IT168 test set with GoogleTranslate. In most cases, SelfTrain(CN), SelfTrain(EN), and SelfTrain(ENCN) can outperform the SVM(CN),
SVM(EN), and SVM(ENCN) methods, respectively. The results demonstrate that the use of unlabeled reviews is beneficial to the classification task.
 views are beneficial to the final classification accuracy, and the co-training approach is more suitable for making use of the unlabeled Chinese reviews than the transductive SVM and the self-training approach.
 the two test sets, and no particular service can always outperform the other two services on the two test sets. Although machine translation is very important in the proposed methods, the quality of the three machine translation services offers no significant differences. 6.2 Influences of Iteration Number ( I )
Figures 7 and 8 show the accuracy curves of the co-training approach and two strong baselines (SVM(ENCN) and SelfTrain(ENCN)) with respect to different numbers of iterations on the two test sets with GoogleTranslate, respectively. The parameter values for CoTrain and SelfTrain are set as p = n = 5. The iteration number 100. When I is set to 1, both the co-training approach and the self-training approach de-generate into SVM(ENCN). The accuracy curves of the component English and Chinese classifiers learned in the co-training approach are also shown in the figures. We omit the very similar figures obtained with YahooTranslate and MicrosoftTranslate.
 two strong baselines after a few iterations. After a large number of iterations, the performance of the co-training approach does not rise any more, because the algorithm runs out of all useful examples in the unlabeled set. The performance finally has a slight decline because some noisy training examples may be selected from the remaining unlabeled set. Fortunately, the proposed approach performs well with a wide range of iteration values.
 training approach. It is encouraging that either the component English classifier or the component Chinese classifier alone can perform better than the strong baselines after a 608 few iterations. The results show that the effectiveness of the co-training approach can be attributed to the effectiveness of its two component classifiers. 6.3 Influences of Growth Size ( p , n )
Figures 9 and 10 show how the growth size at each iteration ( confident examples) influences the accuracy of the proposed co-training approach on the two test sets with GoogleTranslate, respectively. In these experiments, we set which is considered a balanced growth. When p differs very much from considered unbalanced. Balanced growth of (2, 2), (5, 5), (10, 10), and (15, 15) examples and unbalanced growth of (1, 5), (5, 1), (1, 10), and (10, 1) examples are compared in the figures. We omit the very similar figures obtained with YahooTranslate and MicrosoftTranslate.
 can be improved after a few iterations. The performance of the co-training approach with larger p = n will rise more sharply, because the approach can make use of more selected examples to improve the classifiers at each iteration. Also, the performance of the co-training approach with larger p = n will become stable more quickly, because the approach runs out of the limited examples in the unlabeled set more quickly. declines quite rapidly, however, because the selected unbalanced examples hurt the performance at each iteration. We also find that the more the performance declines. Actually, in the generic text categorization task, unbalanced training data will lead to poor classification results (Japkowicz and Stephen 2002). growth can lead to performance improvement, but an unbalanced growth can hurt the final performance. 6.4 Influences of Feature Selection
In these experiments, all features (unigrams + bigrams) are used. As mentioned earlier, feature selection techniques are widely used for dimensionality reduction. In this sec-tion, we conduct further experiments to investigate the influences of feature selection techniques on the classification results. We use the simple but effective DF for feature selection. Figures 11 and 12 show the comparison results of different feature sizes for the co-training approach and two baselines on the two test sets with GoogleTranslate, respectively. The feature size is measured as the proportion of the selected features against the total features (i.e., 100%), and we select 10%, 25%, and 50% features in the experiments. We omit the very similar figures obtained with YahooTranslate and MicrosoftTranslate.
 influence on the classification accuracy of each individual method. This can be ex-plained by the fact that sentiment classification is different from topic-based text clas-sification, and the useful feature sets for the two classification tasks are very different.
The popular feature selection techniques are helpful for topic-based text classification, but they cannot select good features for sentiment classification. Though the feature selection techniques cannot improve the sentiment classification accuracy significantly, they can reduce the feature size to 10% while not significantly lowering the classification accuracy. The large reduction of feature size can improve system efficiency. 610 form the inductive SVM baseline, and the co-training approach can always outperform all the three baselines with different feature sizes. The results further demonstrate the effectiveness and robustness of the proposed co-training approach. 6.5 Influences of Different Training Sets
In the experiments, the training set provided by Blitzer, Dredze, and Pereira (2007) is a very balanced set (4,000 positive reviews + 4,000 negative reviews). In this sec-tion, we sample the following two training sets from the original set: One training set consists of 4,000 positive reviews and randomly selected 2,000 negative reviews (#pos:#neg=2:1), and the other training set consists of 2,000 randomly selected positive reviews and 4,000 negative reviews (#pos:#neg=1:2). The two sampled training sets are not balanced. The proposed co-training approach is compared with the three strong baselines (SVM(ENCN), TSVM(ENCN), and SelfTrain(ENCN)) on the two training sets. Figures 13 and 14 show the comparison results on the two training sets, respectively.
We can see that based on the two training sets, our proposed co-training approach can consistently outperform all three baselines, which further demonstrates the robustness of our proposed approach. 7. Conclusion and Discussion
In this article, we proposed to use the co-training approach to address the problem of cross-lingual sentiment classification. The approach leverages only labeled English reviews and unlabeled Chinese reviews for Chinese sentiment classification. First, the labeled English reviews are translated into labeled Chinese reviews by using English-to-Chinese machine translation services, and the unlabeled Chinese reviews are trans-lated into unlabeled English reviews by using Chinese-to-English machine translation services. The English view and the Chinese view are considered two redundant views.
Then, the co-training algorithm is employed to learn two component classifiers in the two views by mutually helping each other. Finally, given a test Chinese review and its translated English review, the two classifiers are used to obtain two prediction values, and the final polarity tag of the review is decided by the average of the two prediction values. 612 for evaluation, and the evaluation results show the overall effectiveness and robustness of the proposed co-training approach. The approach can significantly outperform the lexicon-based baselines, the inductive classification baselines, the transductive classifi-cation baselines, and the self-training baselines. We also find that the growth size (i.e., the numbers of positive and negative examples selected in the labeled data) is a very important factor in the proposed approach, which has great influence over the final performance. In particular, a balanced growth leads to performance improvement, but an unbalanced growth hurts the final performance.
 this study, the proposed approach can be easily applied to cross-language sentiment classification in other languages, because the three machine translation services cover many of the most frequently used language pairs. For most western languages, feature extraction is very easy because word segmentation is not required. However, for some
Asian languages (e.g., Japanese, Korean), the step of word segmentation is required in order to split a text into words, and thus a word segmentation tool for the specific language is necessary. Fortunately, with the progress of NLP research, word segmenta-tion tools with good performance can be easily obtained for each specific language, and unigram/bigram features can be easily extracted after word segmentation.
 language are still different due to the inaccuracy of the machine translation service and the domain difference between the training set and the test set. In future work, we will try to develop advanced methods to minimize the feature gap in the two review sets. Moreover, we will translate both English and Chinese reviews into a few other languages, and then exploit the multi-view learning techniques for making use of the multiple views in different languages.
 Acknowledgments References 614
