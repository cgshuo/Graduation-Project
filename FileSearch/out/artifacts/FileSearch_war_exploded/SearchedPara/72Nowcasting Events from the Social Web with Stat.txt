 VASILEIOS LAMPOS and NELLO CRISTIANINI, University of Bristol, UK It has not been a long time since snapshots of real life started to appear on the social side of the Web. Social networks such as Facebook and Twitter have grown stronger, forming an electronic substitute for public expression and interaction. Twitter, in par-ticular, counting a total of 200 million users worldwide, 1 came up with a convention that encouraged users to make their posts, commonly known as tweets , by default pub-licly available. Tweets being limited to a length of 140 characters (similarly to text mes-sages in mobile phones) forced their authors to produce more topic specific statements. By adding user X  X  location when posting (via the mobile phone service provider or IP address) to this piece of public information, Twitter ushered in a new era for social Web media and at the same time enabled a new wave of experimentation and research on text stream mining. Now, it has been shown that this vast amount of data encap-sulates useful signals driven by our everyday life and therefore, statistical learning methods could be applied to extract them (several examples are provided in Section 2).
The term nowcasting , commonly used in finance, expresses the fact that we are mak-ing inferences regarding the current magnitude M (  X  ) of an event  X  . For a time interval u =[ t  X  t , t ], where t denotes the current time instance, consider M  X  ( u ) as a latent variable. The Web content W ( u ) for this time interval is a partially observed variable; in particular, data from a social network, denoted as S ( u )  X  W ( u ) are being observed. In this work, S ( u ) is used to directly infer M  X  ( u ) . For short time intervals u , we are inferring the present value of the latent variable, that is, we are nowcasting the mag-nitude of an event. We have already presented preliminary results on a methodology for tracking the level of a flu epidemic from Twitter content using unigrams [Lam-pos and Cristianini 2010] and demonstrated an online tool 2 for this purpose [Lampos et al. 2010]. Here we extend our previous findings and present a general framework for exploiting user input published in social media.

Sparse learning enables us to select a consistent set of features (e.g., unigrams or bigrams) and then use it to perform inference via regression. The performance of the proposed methodology is evaluated by investigating two case studies. In the first, we infer the daily amount of rainfall in five UK locations by using tweets; this forms a benchmark problem testing the limits of our approach given that rainfall has a very inconsistent behavior in the UK [Jenkins et al. 2008]. Ground truth consists of rainfall observations taken from weather stations located in the vicinity of the target locations. The second case study focuses on inferring the level of Influenza-like Illness (ILI) in the population of three UK regions based again on geolocated Twitter content. Results are validated by being compared with actual ILI rates measured by the Health Protection Agency (HPA). 3 In both case studies, experimental results are very positive in terms of the semantic correlation between selected features and target topics, and strong given the general inference performance.

The specific procedure that we followed, namely using Bolasso [Bach 2008] for feature selection from a large set of candidates has proven to work best compared to another relevant state-of-the-art approach [Ginsberg et al. 2008], but the general claim is that statistical learning techniques can be deployed for the selection of features and, at the same time, for the inference of a useful statistical estimator. Com-parisons with other variants of Machine Learning methods may be of interest, though they would not change the main message: that one can learn the estimator from data, by means of supervised learning. In the case of ILI, other methods (e.g., Corley et al. [2009]; Polgreen et al. [2008]) propose to simply count the frequency of the disease name. This can work well when people can diagnose their own disease (maybe easier in some cases than others) and no other confounding factors exist. However, from our experimental results (see Sections 5, 6, and 7), one can conclude that this is not an optimal choice. Furthermore, it is not obvious that a function of Twitter content should correlate with the actual health state of a population. There are various possible sampling biases that may prevent this signal from emerging. An important result of this study is that we find that it is possible to make up for any such bias by calibrating the estimator on a large dataset of Twitter posts and actual HPA readings; similar results are derived for the rainfall case study. While it is true that Twitter users do not represent the general population and Twitter content might not represent any particular state of theirs, we find that actual states of the general population (health or weather oriented) can be inferred as a linear function of the signal in Twitter.
The content of this article is laid out as follows: related work and background theo-retical foundations are provided in Section 2; the proposed methodology, the perfor-mance evaluation procedure and the baseline approach, to which we compare our results, are described in Section 3; Section 4 is concerned with the technical details of information collection and retrieval explaining how Twitter is sampled and also de-fines the classes of features used in our approach; Sections 5 and 6 include a detailed presentation and analysis of the experimental results for the case studies of rainfall and flu nowcasting respectively; finally, Section 7 further discusses the derivations of this work, followed by the conclusions and future work in Section 8. Recent work has been concentrated on exploiting user-generated Web content for con-ducting several types of inference. A significant subset of papers, examples of which are given in this paragraph, focuses on methodologies that are based either on man-ually selected textual features related to a latent event, for instance, flu related key-words, or the application of sentiment/mood analysis, which in turn implies the use of predefined vocabularies, where words or phrases have been mapped to sentiment or mood scores [Pang and Lee 2008]. Corley et al. [2009] reported a 76.7% correlation between official ILI rates and the frequency of certain hand-picked influenza related words in blog posts [Corley et al. 2009], whereas similar correlations were shown be-tween user search queries that included illness related words and CDC 4 rates [Pol-green et al. 2008]. Furthermore, sentiment analysis has been applied in the effort of extracting voting intentions [Tumasjan et al. 2010] or box-office revenues [Asur and Huberman 2010] from Twitter content. Similarly, mood analysis combined with a non-linear regression model derived an 87.6% correlation with daily changes in Dow Jones Industrial Average closing values [Bollen et al. 2011]. Finally, Sakaki et al. [2010] presented a method that exploited the content, time stamp and location of a tweet to detect the existence of an earthquake.

However, in other approaches feature selection is performed automatically by ap-plying statistical learning methods. Apart from the obvious advantage of reducing human involvement to a minimum, those methods tend to have an improved inference performance as they are enabled to explore the entire feature space or, in general, a greater amount of candidate features [Guyon and Elisseeff 2003]. In Ginsberg et al. [2008] Google researchers proposed a model able to automatically select flu related user search queries, which later on were used in the process of tracking ILI rates. Their method, a core component of Google Flu Trends, achieved an average correla-tion of 90% with CDC data, much higher than any other previously reported method. An extension of this approach has been applied on Twitter data achieving a 78% cor-relation with CDC rates [Culotta 2010]. In both those works, features were selected based on their individual correlation with ILI rates; the subset of candidate features (user search queries or keywords) appearing to independently have the highest lin-ear correlations with the target values formed the result of feature selection. Another technique, part of our preliminary results, which applied sparse regression on Twit-ter content for automatic feature selection, resulted to a greater than 90% correlation with HPA X  X  flu rates for several UK regions [Lampos and Cristianini 2010]; an im-proved version of this methodology has been incorporated in Flu Detector [Lampos et al. 2010], an online tool for inferring flu rates based on tweets.

Besides minor differences regarding the information extraction and retrieval techniques or the datasets considered, the fundamental distinction between Ginsberg et al. [2008] and Culotta [2010] and Lampos and Cristianini [2010] lies on the feature selection principle; a sparse regressor, such as LASSO, does not handle each candidate feature independently but searches for a subset of features that satisfies its constraints [Tibshirani 1996] (see Section 2.2). In this work, we extend and generalize the method-ology and preliminary results presented in Lampos and Cristianini [2010]. The main theoretical concept is again feature selection by sparse learning, though we aim to make this selection consistent considering, at the same time, more types of features. Least Absolute Shrinkage and Selection Operator (LASSO), presented in Tibshirani [1996], being a constrained version of ordinary least squares (OLS) regression, pro-vides a sparse regression estimate  X   X  computed by solving the following optimization problem: where x  X  X  denote the input data ( N observations of p variables), y  X  X  are the N target values,  X   X  X  the N coefficients or weights,  X  0 is the regression bias and t  X  0 is referred to as the regularization or shrinkage parameter since it controls the regularization (or shrinkage) amount on the L1-norm of  X   X  X . Least Angle Regression (LARS) provides an efficient algorithm for computing the entire regularization path of LASSO [Efron et al. 2004], that is, all LASSO solutions for different choices of the regularization parameter t . However, it has been shown that LASSO selects more variables than necessary [Lv and Fan 2009] and that in many settings it performs an inconsistent model selection [Zhao and Yu 2006].

Bootstrap , presented in Efron [1979], was introduced as a method for assessing the accuracy of a prediction but has also found applications in improving the prediction itself (see for example Bagging [Breiman 1996]). Suppose that we aim to fit a model to a training dataset T . The basic idea of bootstrapping is to draw n random datasets B with replacement from T , forcing each sample to have the same size as | T | ; the drawn datasets are referred to as bootstraps. Then, refit the model into each element of B and examine the behavior of the fits [Efron and Tibshirani 1993]. The bootstrapped version of LASSO, conventionally named as Bolasso , intersects the supports of LASSO bootstrap estimates and addresses its model selection inconsistency problems [Bach 2008]. Throughout this work we have applied Bolasso X  X  soft version (see Sections 3.1 and 3.2) in our effort to select a consistent subset of textual features. In this section, a general description of the proposed methodology is given, introducing the notation that is going to be used throughout this script. An abstract summary of the methodology includes the following three main operations. (1) Candidate Feature Extraction . A vocabulary of candidate features is formed by us-(2) Vector Space Representation . For a fixed time period and set of locations, the Vector (3) Feature Selection and Inference . A subset of the candidate features is selected by boolean function g indicates whether a candidate marker c i is contained in a user post p j or not: Given the user posts P ( u ) , we compute the score s of a candidate marker c i as follows: Therefore, the score of a candidate marker is the number of tweets containing this marker divided by the total number of tweets for a predefined time interval. The scores of all candidate markers for the same time interval u are kept in vector x given by: In our study, u takes the length of a day d ; from this point onwards, consider a time interval equal to the duration of a day. However, u  X  X  length is a matter of choice depending on the inference task at hand.
 of the candidate markers C . Those are held in a | D | X | C | array X ( D ) : For the same set of | D | days, we retrieve the values of the target variable y ( D ) : X
D ) and y ( D ) are used as an input in Bolasso. In each bootstrap, LASSO selects a sub-set of the candidates and at the end Bolasso, by intersecting the bootstrap outcomes, attempts to make this selection consistent. LASSO is formulated as follows: where t is the regularization parameter controlling the shrinkage of w  X  X  L1-norm. In turn, t can be expressed as where w OLS is the OLS regression solution and a denotes the desired shrinkage percent-age of w OLS  X  X  L1-norm. Bolasso X  X  implementation applies LARS, which is able to explore the entire regularization path at the cost of one matrix inversion and decides the value of the regularization parameter ( t or  X  ) using the largest consistent region, that is, the largest continuous range on the regularization path, where the set of selected variables remains the same [Efron et al. 2004].

After selecting a subset F = { f i } , i  X  X  1 , ..., | F |} of the feature space, where F  X  C , the VSR of the initial vocabulary X ( D ) is reduced to an array Z ( D ) of size | D | X | F | .We learn the weights of the selected features by performing OLS regression: where vector w s denotes the learned weights for the selected features and scalar  X  is regression X  X  bias term.

It is important to notice that statistical bounds exist linking LASSO X  X  expected per-formance to the one derived on the training set (empirical), the number of dimensions, number of training samples and 1-norm of w . For example in Bartlett et al. [2009] it is shown that LASSO X  X  expected loss L ( w ) up to polylogarithmic factors in W 1 , | C | and | D | is bounded by where  X  L ( w ) denotes the empirical loss, | C | is the number of candidate features, | D | is the number of training samples and W 1 is an upper bound for the 1-norm of w ,that is, w 1  X  W 1 . Therefore, to minimize the prediction error using a fixed set of training samples and given that the empirical error is relatively small, one should either reduce the dimensionality of the problem ( | C | ) or increase the shrinkage of w  X  X  1-norm (which intuitively might result in sparser solutions). A strict application of Bolasso implies that only features with a nonzero weight in all bootstraps are going to be considered. In our methodology a soft version of Bolasso is applied (named as Bolasso-S in Bach [2008]), where features are considered if they ac-quire a nonzero weight in a fraction of the bootstraps, which is referred to as Consensus Threshold (CT). CT ranges in (0 , 1] and obviously is equal to 1 in the strict application of Bolasso. The value of CT, expressed by a percentage, is decided using a validation set. To constrain the computational complexity of the learning phase, we consider 21 discrete CTs from 50% to 100% with a step of 2.5%.
 Overall, performance evaluation includes three steps: (a) training, where for each CT we retrieve a set of selected features from Bolasso and their weights from OLS regression, (b) validating CT, where we select the optimal CT value based on a valida-tion set, and (c) testing, where the performance of our previous choices is computed. Training, validation and testing sets are by definition disjoint from each other. Output :  X  C 1: p  X   X   X   X  C while i  X  k do end p  X  argmin return  X  C 1: p ;
The Mean Squared Error (MSE) between inferred ( X w ) and target values ( y ) forms theloss( L ) during all steps. For a sample of size | D | this is defined as: where the loss function ( x i w , y i )=( x i w  X  y i ) 2 . The Root Mean Squared Error (RMSE)  X  the square root of MSE  X  has been used as a more comprehensive metric (it has the same units with the target variables) for presenting results in Sections 5 and 6.
 To summarise CT X  X  validation, suppose that for all considered consensus thresholds CT and set of features, it is given by: phase.

Taking into consideration that both target values (rainfall and flu rates) can only be zero or positive, we threshold the negative inferred values with zero during testing, that is, x i  X  max { x i , 0 } . We perform this filtering only in the testing phase; during CT X  X  validation, we want to keep track of deviations in the negative space as well.
As part of the evaluation process, we compare our results with a baseline approach that encapsulates the methodologies in Ginsberg et al. [2008] and Culotta [2010]. Those approaches, as explained in Section 2, mainly differ in the feature selection pro-cess that is performed via correlation analysis (Algorithm 1). Briefly, given a set C of n candidate features, their computed VSRs for training and validation X ( train ) and X ( v al ) a) computes the Pearson correlation coefficients (  X  ) between each candidate feature and the response values in the training set, b) ranks the retrieved correlation coeffi-cients in descending order, c) computes the OLS-fit loss ( L ) of incremental subsets of the top-k correlated terms on the validation set and d) selects the subset of candidate features with the minimum loss. The inference performance of the selected features is evaluated on a (disjoint) test set.
 For the experimental purposes of this work we use millions of tweets collected via Twit-ter X  X  Search API and ground truth from authoritative sources. Based on the fact that information geolocation is a key concept in both case studies, we are considering only tweets tagged with the location (longitude and latitude coordinates) of their author. We use UK X  X  54 most populated urban centers and collect tweets geolocated within a 10km range from each one of them. Our crawler exploits Atom feeds and periodically retrieves the 100 most recent tweets per urban center. 5 The time interval between con-secutive queries for an urban center varied from 5 to 10 minutes but has been stable on a daily basis and always the same for all locations. Therefore, a sampling method is carried out during collection; we try to reduce sampling biases (for the purposes of our work) by using the same sampling frequency per urban center. Collecting all tweets, apart from being a much more resource demanding process, would also have resulted in exceeding data collection limits set by Twitter. Nonetheless, the daily number of collected tweets (more than 200,000) is considered adequate for the experimental part of this work.

All collected tweets are stored and indexed in a MySQL database. Text preprocess-ing such as stemming by applying Porter X  X  Algorithm for English language [Porter 1980], stop word and punctuation removal as well as the computation of VSRs are performed by our software libraries. VSRs are formed using a TF binary vector space model as already described in Section 3.1.

Candidate features, that is, the pool or vocabulary of n -grams on which feature se-lection is applied, are extracted from encyclopedic, scientific, or more informal Web references related to the inference topic. 6 By performing feature extraction in this way, we secure the existence of good candidate features, but we are also enabled to test the feature selection capability of our method, since most candidates are not di-rectly related to the target topic. A typical information retrieval approach would have implied the creation of a vocabulary index from the entire Twitter corpus [Manning et al. 2008]; our choice is extensively justified in the discussion section. Neverthe-less, acquired results indicate that our simplification in the feature extraction process results in a significant inference performance. Three classes of candidate features have been investigated: unigrams or 1-grams (de-noted by U ), bigrams or 2-grams ( B ) and a hybrid combination ( H ) of 1-grams and 2-grams. 1-grams being single words cannot be characterized by a consistent semantic interpretation in most of the topics. They take different meanings and express distinct outcomes based on the surrounding textual context. 2-grams on the other hand can be more focused semantically. However, their frequency in a corpus is expected to be lower than the one of 1-grams. Particularly, in the Twitter corpus, which consists of very short pieces of text (tweets are at most 140 characters long), their occurrences are expected to be sometimes close to zero.

The hybrid class of features exploits the advantages of classes U and B and reduces the impact of their disadvantages. It is formed by combining the training results of U and B for all CTs. Validation and testing are performed on the combined datasets. Suppose that for all considered consensus thresholds CT i , i  X  X  1 , ..., | CT |} , 1-grams and 2-grams selected via Bolasso are denoted by F ( U ) i and F ( B ) i respectively. Then, the pseudo-selected n -grams for all CTs for the hybrid class F ( H ) i are formed by their union, where Z denotes the VSR of each feature class (using Section X  X  3.1 notation). Valida-tion and testing are performed on Z ( H ) i as it has already been described in Section 3.2. Note that compiling an optimal hybrid scheme is not the main focus here; our aim is to investigate whether a simple combination of 1-grams and 2-grams is able to deliver better results. The experimental results (see Sections 5 and 6) do indeed indicate that feature class H performs on average better than U and B . In the first case study, we exploit the content of Twitter to infer daily rainfall rates (measured in millimetres of precipitation) for five UK cities, namely Bristol, London, Middlesbrough, Reading and Stoke-on-Trent. The choice of those locations has been based on the availability of ground truth, that is, daily rainfall measurements from weather stations installed in their vicinity.

We consider the inference of precipitation levels at a given time and place as a good benchmark problem, in that it has many of the properties of other more useful scenarios, while still allowing us to verify the performance of the system, since rainfall is a measurable variable. The event of rain is a piece of information available to the significant majority of Twitter users and affects various activities that could form a discussion topic in tweets. Furthermore, predictions about it are not always easy due to its nonsmooth behavior [Jenkins et al. 2008].
 The candidate markers for this case study are extracted from weather related Web references, such as Wikipedia X  X  page on Rainfall, an English language course on weather vocabulary, a page with formal weather terminology and several others. As already mentioned in Section 4, the majority of the extracted candidate features is not directly related to the target topic, but there exists a subset of markers that could probably offer a good semantic interpretation. Markers with a count  X  10 in the Twitter corpus used for this case study are removed. Hence, from the extracted 2381 1-grams, 2159 have been kept as candidates; likewise the 7757 extracted 2-grams have been reduced to 930. A year of Twitter data and rainfall observations (from the July 1, 2009 to the June 30, 2010) formed the input data for this experiment. For this time period and the considered locations, 8.5 million tweets have been collected. In each run of Bolasso the number of bootstraps is proportional to the size of the training sample (approximately 13% using the same principle as in Bach [2008]), and in every bootstrap we select at most 300 features by performing at most 900 iterations. A bootstrap is completed as soon as one of those two stopping criteria is met. This is an essential trade-off that guarantees a quicker execution of the learning phase, especially when dealing with large amounts of data.

The performance of each feature class is computed by applying a 6-fold cross valida-tion. Each fold is based on 2 months of data starting from the month pair July-August (2009) and ending with May-June (2010). In every step of the cross validation, 5 folds are used for training, the first half (a month-long data) of the remaining fold for validating CT and the second half for testing the performance of the selected markers and their weights. Training is performed by using the VSRs of all five locations in a batch dataset, CT X  X  validation is carried out on the same principle (i.e., we learn the same markers-weights under the same CT for all locations), and finally testing is done both on the batch dataset (to retrieve a total performance evaluation) and on each location separately. Finally, we also compute the inference performance of the baseline approach for feature selection (Algorithm 1) for the same training, validation, and testing sets, considering the top k = 300 correlated terms. The derived CTs as well as the numbers of selected features for all rounds of the 6-fold cross validation are presented on Table I. In most rounds CT values are close to 90% meaning that a few markers were able to capture the rainfall rates signal. However, in the last round, where the validation dataset is based on July 2009, CTs for all feature classes are significantly lower, which can be interpreted by the fact that July is the 2nd most rainy month in our dataset, but also a summer month; therefore, tweets for rain could be followed or preceded by tweets discussing a sunny day, creating instabilities during the validation process. In addition, our dataset is restricted to only one year of weather observations, and therefore seasonal patterns like this one are not expected to be captured properly.

Detailed performance evaluation results (total and per location for all feature classes) are presented on Table II. For a better interpretation of the numerical values (in mm), consider that the average rainfall rate in our dataset is equal to 1 . 8witha standard deviation of 3 . 9 and a range of [0 , 65]. Our method outperforms the baseline approach (see Algorithm 1 in Section 3.2) in all-but-one intermediate RMSE indica-tions as well as in total for all feature classes, achieving an improvement of 10.74% (derived by comparing the lowest total RMSEs for each method). The overall perfor-mance for our method indicates that feature class H performs better than both U and B  X  in the results per location, feature class H has the best performance 11 times (out of 30), the same holds for B ,and U is better 8 times.

Presenting all intermediate results for each round of the cross validation would have been intractable. In the remaining part of this Section, we present the results of learning and testing for cross-validation X  X  round 5 only, where the month of testing is October 2009. Tables III, IV, and V list the selected features in alphabetical order together with their weights for feature classes U , B and H respectively. For class H we have also compiled a word cloud with the selected features as a more comprehen-sive representation of the selection outcome (Figure 1). The majority of the selected 1-grams (Table III) has a very close semantic connection with the underlying topic; stem  X  X uddl X  holds the largest weight, whereas stem  X  X unni X  has taken a negative weight and interestingly, the word  X  X ain X  has a relatively small weight. There also ex-ist a few words without a direct semantic connection, but the majority of them has negative weights and in a way they can act as mitigators of non weather related uses of the remaining rainy weather oriented and positively weighted features. The se-lected 2-grams (Table IV) have a clearer semantic connection with the topic with  X  X our rain X  acquiring the highest weight. In this particular case, the features for class H are formed by the exact union of the ones in classes U and B , but take different weights (Table V and Figure 1).
 Inference results per location for cross validation X  X  round 5 are presented in Figures 2, 3, and 4 for U , B ,and H feature classes respectively. Overall, inferences follow the pattern of actual rain; for feature class B , we see that inferences in some occasions appear to have a positive lower bound (see Figure 3(e)) that is actually the positive bias term of OLS regression appearing when the selected markers have zero frequencies in the daily Twitter corpus of a location. As mentioned before this problem is resolved in H since it is very unlikely for 1-grams to also have a zero frequency. Results for class H depicted in Figures 4(a) (Bristol), 4(b) (London) and 4(d) (Reading) demonstrate a good fit with the target signal.
 In the second case study, we use the content of Twitter to infer regional flu rates in the UK. We base our inferences in three UK regions, namely, Central England and Wales, North England and South England. Ground truth, that is, official flu rate measure-ments, is derived from HPA. HPA X  X  weekly reports are based on information collected from the Royal College of General Practitioners (RCGP) and express the number of GP consultations per 100,000 citizens, where the result of the diagnosis was ILI. According to HPA, a flu rate less than or equal to 30 is considered as baseline, flu rates below 100 are normal, between 100 and 200 are above average and over 200 are characterized as exceptional. 7 To create a daily representation of HPA X  X  weekly reports we use linear interpolation between the weekly rates. Given the flu rates r i and r i +1 of 2 consequent weeks, we compute a step factor  X  from and then produce flu rates for the days in between using the equation where d j denotes the flu rate of week X  X  day j and d 1 = r i . The main assumption here is that a regional flu rate will be monotonically increasing or decreasing within the duration of a week.

Candidate features are extracted from several Web references, such as the Web sites of National Health Service, BBC, Wikipedia, and so on, following the general principle, that is, including encyclopedic, scientific, and more informal input. Similarly to the previous case study, extracted 1-grams are reduced from 2428 to 2044, and extracted 2-grams from 7589 to 1678. Here, we have removed n -grams with a count  X  50 since the number of tweets involved is approximately 5 times larger compared to the rainfall case study.

In the flu case study, not enough peaks are present in the ground truth time se-ries, as the collected Twitter data cover only one flu period with above average rates (Swine Flu epidemic in June-July 2009). During performance evaluation, this results in training mostly on nonflu periods where there is no strong flu signal; hence, feature selection under those conditions is not optimal. To overcome this and assess prop-erly the proposed methodology, we perform a random permutation of all data points based on their day index. The randomly permuted result for South England  X  X  flu rate is shown on Figure 10(c); we apply the same randomized index on all regions during performance evaluation. For this experiment, we considered tweets and ground truth in the time period be-tween the June 21, 2009 and April 19, 2010 (303 days). The total number of tweets used reaches approximately 50 million. Similarly to the previous case study, we are applying 5-fold cross validation using data of 60 or 61 days per fold. In each round of the cross-validation, 4 folds are used for training. From the remaining fold, 30 days of data are used for validating CT and the rest for testing. Bolasso settings are identical to the ones used in the rainfall case.

Notice that in the following experiments data points are not contiguous in terms of time since they have been permuted randomly based on their day index (as explained in the previous section). However, we have included an example at the end of the next section, where contiguous (time-wise) training, validating and testing data points have been used. The derived CTs and numbers of selected features for all rounds of the 5-fold cross validation are presented on Table VI. In the flu case study, most CTs (especially in U and H feature classes) get a value close to the lower bound (50%) after validation, and on average more features (compared to the rainfall case) are being selected. This is due to either the existence of only one significant flu period in the ground truth data or the general inadequacy of 1-grams to describe the underlying topic as effectively as in the previous case study.

Table VII holds the performance results for all rounds of the cross validation. For a more comprehensive interpretation of the numerical values consider that the average ILI rate across the regions used in our experiments is equal to 26 . 659 with a standard deviation of 29 . 270 and ranges in [2 , 172]. Again, feature class U performs better than B , whereas H outperforms U and B . In the regional results per fold, class H has the best performance 8 times, B 5 times and U only 2 times (out of 15 subcases in total). Similarly to the previous case study, our method improves on the performance of the baseline approach by a factor of 9.05%.
 For this case study, we present all intermediate results for cross validation X  X  round 1. Tables VIII, IX, and X show the selected features for U , B and H feature classes re-spectively. From the selected 1-grams (Table VIII), stem  X  X rrig X  8 has the largest weight. Many illness related markers have been selected such as  X  X ough X ,  X  X ealth X ,  X  X edic X ,  X  X urs X ,  X  X hroat X  and so on, but there exist also words with no clear semantic relation. Surprisingly, stem  X  X lu X  has not been selected as a feature in this round (and has only been selected in round 5). On the contrary, almost all selected 2-grams (Table IX) can be considered as flu-related;  X  X onfirm swine X  has the largest weight for both feature classes B and H (Table X and Figure 5). As a general remark, keeping in mind that those features have been selected using data containing one significant flu period, they cannot be considered as very generic ones.

Regional inference results are presented on Figures 6, 7, and 8 for classes U , B , and H respectively. There is a clear indication that the inferred signal has a strong correlation with the actual one; for instance, for feature class H (Figure 8) the lin-ear correlation coefficients between the inferred and the actual flu rate for Central England &amp; Wales, North England and South England are equal to 0.933, 0.855 and 0.905 respectively. Using all folds of the cross validation, the average linear correla-tion for classes U , B ,and H is equal to 0.905, 0.868 and 0.911 respectively, providing additional evidence for the significance of the inference performance. 9
Finally, we present some additional experimental results where training, validating and testing have been carried out in a contiguous time wise manner. From the 303 days of data, we used days 61 X 90 for validating CT, 91 X 121 for testing (from Septem-ber 19 to October 19, 2009) and the remaining days have been used for training. In this formation setting, we train on data from the Swine Flu epidemic period and then test on a period where influenza existed but its rate was within a normal range. In Figure 9, we show the inference outcome for South England for all feature classes. 10 We have also included a smoothed representation of the inferences (using a 7-point moving average) to induce a weekly trend. Class H has the best performance; in this example, class B performs better than U . The experimental results provided practical proof for the effectiveness of our method in two case studies: rainfall and flu rates inference. Rain and flu are observable pieces of information available to the general public and therefore are expected to be parts of discussions in the social Web media. While samples from both rainfall and ILI rates can be described by exponentially distributed random variables, those two phenom-ena have a distinctive property. Precipitation, especially in the UK, is rather unsta-ble, that is, prone to daily changes, whereas a flu rate evolves much more smoothly. Figures 10(a) (rainfall in London) and 10(b) (flu rate in South England) provide a clear picture for this. Consequently, rainfall rates inference is a much harder problem: users discussing the weather of a preceding day or the forecast for the next one, especially when the current weather conditions contradict, affect not only the inference process but learning as well. For example, in the 6th round of the cross-validation, where we derived the worst inference performance, we see that in the VSRs of the test set (which includes 67 rainy out of 155 days in total for all 5 locations), 1-gram  X  X lood X  has the exact same average frequency during rainy and non rainy days; furthermore, the average frequency of stem  X  X ain X  in days with no rain was equal to 68% of the one in rainy days. Similar statistics are also observed in the training set or for 2-grams; for instance, the average frequencies of  X  X ain hard X  and  X  X our rain X  in the training set (716/1515 rainy days) for nonrainy days are equal to 42% and 13% of the ones in rainy days respectively.

The proposed method is able to overcome those tendencies by selecting features with a more stable behavior to the extent possible. However, the figures in the two previous Sections make clear that inferences have a higher correlation with the ground truth in the flu case study; even when deploying a randomly permuted version of the dataset, which in turn encapsulates only one major flu period, and therefore is of worse quality compared to the rainfall data. Based on those experimental results and the properties of the target events that reach several extremes, we argue that the proposed method is applicable to other events as well, which are at least drawn from an exponential distribution.

Another important point in this methodology regards the feature extraction ap-proach. A mainstream information retrieval technique implies the formation of a vo-cabulary index from the entire corpus [Manning et al. 2008]. Instead, we have chosen to form a more focused and restricted in numbers set of candidate features from online references related with the target event, a choice justified by LASSO X  X  risk bound (see Equation (10)). The short time span of our data limits the amount of training samples, and therefore directs us in the choice of reducing the number of the candidate features to minimize the risk error and avoid overfitting. Indeed, in the flu case study, where only a small variation in the ILI rates is observed, when we formed an index from the entire Twitter corpus, the method tended to select non-illness-related features as well. Some of those features, for example, were describing a popular movie released in July 2009, the same period with the peak in the flu rates signal. By having fewer and slightly more focused on the target event X  X  domain candidates, we constrain the di-mensionality over training samples ratio and this issue is resolved. Nevertheless, the size of 1-gram vocabularies in both case studies was not small (approx. 2400 words) and 99% of the daily tweets for each location or region contained at least one candidate feature. However, for 2-grams this proportion was reduced to 1.5% and 3% for rainfall and flu rates case studies respectively, meaning that this class of features required a much higher number of tweets in order to properly contribute.

The experimental process made also clear that a manual selection of very obvious keywords that logically describe a topic, such as  X  X lu X  or  X  X ain X , might not be optimal es-pecially when using 1-grams; more rare words ( X  X uddl X  or  X  X rrig X ) exhibited more stable indications about the target events X  magnitude. Finally, it is important to note how CT operates as an additional layer in the feature selection process facilitating the adapta-tion on the special characteristics of each dataset. CT X  X  validation showed that a blind application of strict bolasso (CT = 1) would not have performed as good as the relaxed version we applied; only once in 22 validation sets the optimal value for CT was set equal to 1. We have presented a supervised learning framework for nowcasting events by ex-ploiting unstructured textual information published on the social Web. The proposed methodology is able to turn geo-tagged user posts on the microblogging service of Twitter to topic-specific geolocated signals by selecting textual features that capture semantic notions of the inference target. Sparse learning via a soft version of Bolasso, the bootstrapped LASSO L1-norm regulariser, performs a consistent feature selection, which increases the inference performance approx. by a factor of 10% compared to previously proposed methods [Culotta 2010; Ginsberg et al. 2008].

We have displayed results drawn from two case studies, that is, the benchmark problem of inferring rainfall rates and the real-life task of detecting the diffusion of Influenza-like Illness from tweets. In both case studies, the majority of selected fea-tures was directly related with the target topic and inference performance has been significant; for instance, for the important task of nowcasting influenza, inferred flu rates reached an average correlation of 91.11% with the actual ones. As expected, se-lected 2-grams showed a better semantic connection with the target topics. However, during inference they did not perform as well as 1-grams. Combining both feature classes into a hybrid approach resulted in an overall better performance.
 Future work could be focused on improving various subtasks in our methodology. Feature extraction can become more sophisticated by identifying self diagnostic statements in the corpus (e.g.,  X  X  got soaked today X  or  X  X  have a headache X ) or by in-corporating entities (gender, names, brands, etc.). Similarly to other work (mentioned in Section 2), using sentiment or mood analysis on the text can offer an additional di-mension of input information. Exploiting the temporal behavior of an event combined with more sophisticated inference techniques able to model non linearities or even a generative approach, could also improve the inference performance as well as provide interesting insights (e.g., the identification of latent variables that influence the inference process). Finally, on a conceptual basis, the detection of multivariate signals, where target variables may be interdependent (e.g., electoral voting intentions), could form an interesting task for future research.

