 to exploit context-specific independence to achieve efficie nt and accurate inference. achieve widespread use? We believe it would need to be: contained factors to the power zero if the gate is off, or one i f it is on: ( Q used in both directed Bayesian networks and undirected grap hs. nected to nothing).
 different key values, true and false. If c is true, x will have distribution N ( m will have distribution N ( m is used and the key values are the integers 1,2,3.
 factor and mean/precision variables inside a plate, so that they are replicated N times. guities, gates cannot partially overlap, nor can a gate cont ain its own selector variable. variant g dependency represented by the structure inside the gate is p resent or absent. are contained in gates  X  this example is described in the foll owing section. 2.1 Examples of models with gates boring image pixels x edge e where the aim is to detect associations between a genetic var iant g variable c switches on or off a linear model of the genetic variant X  X  con tribution y across all individuals. When the gate is off, y explained only by a Gaussian-distributed background model z of c allows associations between the genetic variation and the t rait to be detected. of message passing algorithms on mixture models.
 As a motivating example, consider the mixture model of figure 1b when the precisions p mean-field approximation to this model, we obtain the follow ing fixed-point system: between 0 and 1). For example, the update for q ( m m blur the message from f to m the 1 function followed by KL-projection. 3.1 Why gates are not equivalent to  X  X ick X  factors try to write the model using a factor pick ( x | c, h We can introduce latent variables ( h downward messages from ( m upward from x before it reaches the factor f , which is incorrect. Another approach is to pick from ( m x to f is not blurred, and the upward messages to ( m the downward messages from ( m 3.2 Variables inside gates is: p ( x, c, m q ( c = k )  X  p ( c = k ) exp X q ( y | c = k )  X  exp X Notice that only the messages to x and m as the evidence for the submodel containing f pling [7] to allow each of these algorithms to support contex t-specific independence. ized approximation q ( x ) = Q ministic factors, that is, factors which have the form f variables. For both algorithms the marginal distributions are obtained as q ( x cept for derived child variables in VMP where q ( x derived variables contribute 1 under VMP.

EP Any Y
VMP Stochastic Y
EP s
VMP s Table 1: Messages and evidence computations for EP and VMP The top part of the table shows messages between a variable x contributions for variables and factors in each algorithm. 4.1 Variational Message Passing with gates its key k The messages out of a gate are modified as follows: contained nodes: where s deterministic factor f The child variable x where m To allow for nested gates, we must also define an evidence mess age for a gate: 4.2 Expectation Propagation with gates is denoted g , its selector variable c and its key k The messages into a gate are as follows: an intermediate evidence-like quantity s where m is used to cancel the denominators of s out of a gate may now be specified: Finally, the evidence contribution of a gate block with sele ctor c is: 4.3 Gibbs sampling with gates and sending these values to their neighboring factors. Then for each variable x increase the cost.
 as a gate enlargement (figure 3). By enlarging the gate block t o include u the multiplication factor and u since u suggested by [14], which can be interpreted as enlarging the gate. distribution over models.
 algorithm.

