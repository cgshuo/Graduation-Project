 Collaborative Filtering (CF) have been extensively investigated due to the fact that there is massive volume of information available on the Web and CF it is readily applicable to real-world applications such as recommendation systems. For example, a number of recommendation systems have been developed to predict the movie rating given by users in the Netflix dataset very high accuracy. CF discovers the association of user-item ratings and predict the rating to a previously unseen item given by a user. One of the challenges in CF is to handle the sparsity and high dimensionality of the user-item rating matrix. Due to this, the similarity between users are difficult to be computed directly.
 item rating matrix, is one of the most common techniques used in CF. By treating the user-item rating matrix as the target matrix, the objective of matrix factor-ization is to discover the user matrix and item matrix, whose dot-product can approximate the target matrix. Each column of the user matrix and item matrix essentially represent a user and an item respectively. The user and item matrices are normally of lower rank to address the sparsity and the dimensionality prob-lem and improve the efficiency in the prediction of unknown rating. However, one major limitation of low rank matrix factorization is that the similarity between two users will be unavoidably distorted because the column vectors in the user and item matrices corresponding to the smallest eigenvalues will be discarded and only a few significant columns will be retained. For example, let be the user-rating matrix where ( i, j )-th entry corresponds to the rating given by user i to item j . The Euclidean distances between users 1 and 2, users 1 and 3, and users 2 and 3 are 1, 1, and 1.4142 respectively. If we apply low rank matrix factorization and set the rank k = 2 to solve R  X  U  X V where U, V  X   X  R 2  X  2 2 . The results are If we set  X  R = U  X V and compute the Euclidean distance according to distances between users 1 and 2, users 1 and 3, and users 2 and 3 are 0.7946, 0.6738, and 1.4081 respectively. As a result, low rank matrix factorization does not consider the distance between users/items in the learned low rank space. As we can observe in the above example, the Euclidean distance between users 1 and 2 is reduced from 1 to 0.7946, while the Euclidean distance between users 1 and 3 is reduced from 1 to 0.6738. The relative changes of the distance from the original space to the new space are different, even though the two distances are the same in the original space. More importantly, such changes completely depend on the user-item rating matrix and do not consider other useful information in a social network. For example, the distance between users 1 and 2 in the new space should be smaller than the distance between users 1 and 3 if users 1 and 2 are  X  X riends X  while users 1 and 3 are not in a social network. Regularized Singular Value Decomposition (RSVD) is a common technique used to solve the low rank matrix factorization problem and identify the low-rank user matrix and item matrix. Regularization is originally applied in the model to tackle the problem of model complexity and over-fitting. Several approaches have been proposed to use different regularizers to incorporate additional or prior information in learning the model. For example, Ma proposed to consider the user similarity and item similarity in the regularizer [ 1 ]. Essentially, it imposes soft constraints that given a pair of similar users, the two column vectors of the user matrix representing the two users are required to be close to each other. Similarly, the two column vectors of the item matrix representing the two items are required to be close to each other. Empirical results illustrate that prior information in the form of regularizer can substantially improve the performance in prediction. One limitation of this approach is that the closeness of two users/items is represented by the Frobenius norm of the difference between two column vectors. In other words, the distance metric is needed to be designed in advance. More importantly, the distance metric chosen does not take the data collected and the goal of the task into account.
 We have developed a framework for jointly conducting collaborative filtering and distance metric learning, aiming at simultaneously discovering the user and item matrices for predicting unknown ratings, and learning the distance metric for other applications, in the new low rank space. Unlike existing works which only address the CF problem, or apply the pre-defined similarity measures to represent the closeness between users/items in the learned model, our approach can automatically discover the similarity metric when computing the user and item matrices when solving RSVD. The major idea of our approach is that given an item, a pair of similar users should give similar rating to this item. Moreover, from the discriminative perspective, the distance between them should be as close as possible in the low rank space. On the contrary, the distance between dissimilar users should be as far as possible in the new space. To achieve this, we have incorporated the parameterized Mahalanobis distance, which essentially is a linear transformation of the distance from the original space to a new space, into the regularizer of RSVD. When solving the RSVD, the user matrix, item matrix, and the parameters of the Mahalanobis distance will be learned jointly in our model. In our designed regularizer, we can easily incorporate the similarity information in the original space in our model. For example, trust information is commonly available in social networks. Trusted users can be considered to similar, while untrusted users can be considered to be dissimilar. With this trust information, the solution will naturally consider both user-item rating informa-tion and trust information. As a result, the learned user matrix, item matrix, and the parameters of Mahalanobis distance can be applied to coherently tackle both rating prediction and trust prediction problems, reducing possible conflict between the two tasks. Another characteristic of our approach is that collabora-tive filtering and distance metric learning serve as regularization to each other, leading to the smoothing effect and reducing overfitting.
 1. We have developed a framework for jointly learning the user and item matrices 2. Our model can easily incorporate the prior social network information such as 3. We showed that in our model derived from RSVD, collaborative filtering and 4. We have conducted extensive experiments to evaluate our framework and Recommendation systems have been extensively investigated by researchers [ 2 ]. Memory-based methods aims at measure the user-user similarity based on the user profile or historical record to predict the rating of items given by a user [ 3  X  6 ]. However, one common shortcoming is the sparsity problem of the raw data. Normally, a user may only rate a relatively small number of the items, out of hundreds or thousands. Given two users, the number of items that are commonly rated is very small. Model-based methods aim at train a model for prediction [ 7  X  9 ]. For example, Zhang and Koren proposed Bayesian hierarchical linear model to tackle the CF problem [ 10 ]. In this model, the profile of each user is mod-eled by a linear model, whose parameters are drawn from a prior distribution. The rating to an item given by a user is then predicted by applying the model with relevant input. Xue et al. proposed a clustering-based method, which first generates clusters of similar users using K-means algorithm [ 8 ]. These generated clusters are then exploited to smooth the unknown rating, and hence improve the prediction performance for each individual user. ListCF predicts the ranking of items by a user by measuring the user-user similarity based on the Kullback-Leibler divergence between users X  probability distributions over permutations of commonly rated items [ 11 ].
 tive of matrix factorization is to discover the user matrix and the item matrix in a low-rank space, such that the dot-product can approximate the original user-item ratings. To address the sparsity problem, regularized singular value decomposition (RSVD) is applied [ 12 , 13 ]. Empirical results have also demon-strated that matrix factorization methods achieved promising performance. For example, Srebro and Jaakola proposed an approximation method to discover the low rank matrices using EM algorithm and applied in CF [ 14 ]. Srebro et al. then proposed another matrix factorization method based on maximum margin principal [ 15 ]. This method imposes constraints on the norm of the factorized matrices. Salakhutdinov and Mnih developed different probabilistic matrix fac-torization models [ 6 , 16 ]. These two models consider the uncertainty involved in the user-item ratings. Instead of predicting the rating, Liu and Yang proposed a method to predict the ranking of items by a user [ 17 ].
 A number of methods aiming at incorporating additional information in the learned model have been proposed [ 18 , 19 ]. One common method to consider the additional information is to make use of the regularizer in RSVD. For example, Noel et al. proposed to incorporate different forms of regularizer such as fea-ture social regularizer and co-preference regularizer into the objective function when solving RSVD [ 20 ]. Ma et al. proposed two regularization models, namely, average-based regularization and individual-based regularization, and applied different similarity measures to consider the social information [ 21 ]. Later, Ma developed another method to incorporate the user-user similarity and item-item similarity [ 1 ]. Szummer and Yilmaz proposed a method to consider preference regularization to tackle the learning to rank problem in a semi-supervised set-ting [ 22 ]. In matrix factorization, there are m users and n items. User i gives item j a rating r ij =1 , 2 ,...,r max , where r max is the maximum value for a rating. Let R  X  R m  X  n be the rating matrix where the ( i, j )-th entry is equal to r has rated item j and 0 otherwise. Note that a user may only rate a few items, hence R is very sparse. Let E X { r ij } for some pairs of i and j be the set of training examples consisting of ratings that user i has rated item j . CF aims at predicting the value of unknown ratings by making use of and V  X  R d  X  n , where d min ( m, n ), be the user matrix and item matrix. We denote u i and v j be the i -th column vector of U and j -th column vector of V respectively. Matrix factorization treats R as the target matrix and aims at computing U and V such that R  X  U V . As a result, the unknown rating to item j given by user i can be predicted by computing  X  r ij Regularized Singular Value Decomposition (RSVD) is a common technique applied to address the sparsity problem in matrix factorization problem. A quadratic loss function is defined as follows: where  X  F refers to the Frobenius norm. The last two terms are regularizers. The objective of regularization is to avoid large values of U and V , and hence controlling the model complexity and reducing over-fitting.  X  defined weighting parameters of the two regularizers. Training of RSVD aims at finding U and V by minimizing the loss function in Eq. 1 .
 and solving the system the linear equations is not feasible. Instead, stochastic gradient descent is a common technique for finding the nearly optimal u v . u i and v j are updated iteratively as follows: where u t i and v t i refer to the u i and v j at the t -th iteration;  X  the learning rate of the algorithm. This updating rules are applied for each r ij  X  X  until the maximum number of iterations is reached. As mentioned in Sect. 1 , one shortcoming of typical RSVD in collaborative filter-ing is that the distance between two users in the low rank space will be distorted. Moreover, it does not consider prior social network information when computing U and V . Though some existing social recommendation approach attempt to incorporate the similarity between users, the pre-defined distance metric can-not effectively capture the characteristics of the data and directly accomplish the goal of the task. In this section, we first discuss the idea of distance metric learning. Next, we will present our joint model for collaborative filtering and distance metric learning 4.1 Distance Metric Learning Following the notation used above, U  X  R d  X  m denotes to the user matrix where the j -th column refers to the j -th user. Mahalanobis distance, denoted by d ( u i ,u j ), between users i and j is defined as follows: where A  X  R d  X  d is a semi-definite, A 0. In Mahalanobis distance, A refers to the covariance matrix. If we assume all users are independent, A = I and d ( u i , u j ) becomes the Euclidean distance between u i and u acts as a linear transformation of the distance between u original space to a new space. In many applications, we may collect a set of similar or dissimilar objects. For example, in social network, we may treat a pair of users who are friends as similar users. On the contrary, two users who do not know each other are dissimilar. Distance metric learning aims at automatically learning the distance function based on the collected data. In our approach, we consider A in Mahalanobis distance as parameters, which can be learned from the training examples. The objective is the discover A such that the distance between similar users can be linearly transformed to a new space such that they are as close as possible. On the contrary, the distance between dissimilar users should be linearly transformed such that their distance in the new space is as far as possible. We denote D be the set of pairs of similar users and dissimilar users respectively. We can formulate the distance metric problem as an constrained optimization problem as follows: The first constraint ensure that the distance between dissimilar users cannot be smaller than 1; the second constraint ensure that the A needs to be semi-positive definite. Note that it is a convex optimization problem. To simplify the learning and improve the efficiency, we set A to a diagonal matrix. As a result, the problem can further be derived to an unconstrained optimization problem as follows: where A is a diagonal matrix. Similarly, regularization is commonly applied to avoid overfitting in learning [ 23 ]. 4.2 RSVD with Distance Metric Learning Recall that the objective of our framework is to jointly solve RSVD and dis-tance metric learning. To achieve this, we develop a regularizer based on the aforementioned metric learning problem and integrate to RSVD. The rationale of our approach is to simultaneously solve the RSVD and distance metric learning in a single coherent model. In essence, the loss function of RSVD with distance metric learning is expressed as follows: The first three terms and the fourth term on the right hand side refer to the loss function of RSVD and metric learning respectively. To solve RSVD and distance metric learning, we jointly minimize Loss new with respect to U , V ,and A . The first derivatives of the loss function with respect to u expressed as follows:
We can then solve the optimization problem by iterative methods like the effi-cient gradient descent method. One characteristic of our approach is that U , V , and A are jointly varied to optimize Loss new . This leads to a solution optimizing both tasks of collaborative filtering and distance metric learning. On the other hand, collaborative filtering and distance metric learning serve regularization to each other resulting to the smoothing effect and reducing over-fitting. Recalled that our preference regularizer in Eq. 8 contains the similarity between users. In this paper we employ three different similarity measures to discover similar users.
 Jaccard Similarity. Jaccard similarity mainly consider the items that both users have rated, without considering the actual ratings given to these items. Let Q h and Q i be the set of items that users h and i have rated respectively. Jaccard similarity is defined as follows: Pearson Correlation Coefficient. Pearson correlation coefficient (PCC) aims at measuring the relationship between the ratings given to the items that are rated by two users. Let Q h and Q i be the set of items that users h and i have rated respectively. PCC is defined as follows: where  X  r h refers to the mean of the ratings to all items given by user h . Since  X  1  X  pcc ( h, i )  X  1, we define our similarity as follows: Kendall Rank Correlation Coefficient. Unlike PCC, Kendall rank correla-tion coefficient, denoted as  X  , is to measure the relation between the ranking of the items that are rated by two users. Let Q h and Q i be the set of items that users h and i have rated respectively.  X  ( h, i ) is defined as follows: Since  X  1  X   X  ( h, i )  X  1, we define our simiarity as follows: The computation of PCC and  X  coefficient are computationally expensive. To reduce the computational time, for any pair of users, we randomly sample N items that are rated by them to compute pcc and  X  coefficient. In our exper-iments, N is set to 10. Next, given a user i , the top-K similar users such that the similarity is greater than 0.75 are considered to be similar to user i and constitute S ( i )inEq. 8 . We have conducted experiments on two real-world datasets to evaluate the effec-tiveness of our framework. The first dataset we used is the MovieLens dataset This dataset consists of 100,000 ratings (between 1 and 5) from 943 users on 1,642 movies. We call this dataset ml-100k . Another dataset is the Epinions dataset 4 . This dataset consists of 664,823 ratings (between 1 and 5) from 49,290 users on 139,738 different items. We call this dataset epinions . In each dataset, we randomly divided the data into five portions, namely u1 to u5, with equal number of ratings. In each run of the experiments, we treated four portions as the set of training examples and the remaining portion as the test data. For example, we utilized u1-u4 for training and u5 for testing. As a result, we con-ducted 5 runs of experiments, each of which utilized different portions as testing data, for each dataset.
 Three sets of experiments were conducted to evaluate our framework. In the first set of experiments, we applied the standard RSVD method on the datasets. This can be regarded as our baseline method. We call this RSVD approach .Inthe second set of experiments, we implemented the existing method described in [ 1 ] and applied it on the datasets. We implemented the SR u + in [ 1 ]. We call this Ma X  X  approach . We compared with this approach because it also aims at improving collaborative filtering via regularization. However, it only considers the closeness between users and the closeness between items in the learned model. In the third sets of experiments, we applied our framework, using different similarity measures as described above. We call this Our approach . In all these approaches, we set the dimension d in matrix factorization to 10. We also followed [ 1 ] to set the parameters  X  1 ,  X  2 ,  X  0.005, and 0.005 respectively. In our approach, we also set  X  all regularizers have the same weighting. The maximum iteration when running stochastic gradient descent optimization was set to 50,000. Since the ratings of the datasets we used in the experiments are discrete, we round the predicted ratings of the three approaches to the nearest integer.
 Error (MAE), which is defined as follows: where T refers to the set of testing data.
 of the table refers to a run of the experiments. The first column of the table refers to the portion of the dataset used as testing data in this run. The second and third columns contains the prediction performance of RSVD approach and Ma X  X  approach respectively. The fourth column is divided into three sub-columns, each of which contains the prediction performance of our approach using different sim-ilarity measures. The first, second, and third sub-columns refer to the Jaccard similarity, Pearson correlation coefficient (PCC), and Kendall rank correlation coefficient (  X  ) respectively. The last row of the table shows the average prediction performance. The average MAE of our approach using Jacaard similarity, PCC, and  X  coefficient are 0.712, 0.706, and 0.707 respectively. They outperform RSVD approach and Ma X  X  approach, whose average MAE are 0.751 and 0.723 respec-tively. Among the three different similar measure, our approach achieves similar prediction performance. Table 2 shows the prediction performance of different approaches on the dataset epinions. The format of Table 2 is the same as that of Table 1 . Similarly, our approach achieves the best performance, with average MAE of 0.782, 0.785, and 0.783 for Jaccard similarity, PCC, and  X  coefficient respectively. We have developed a framework for improving rating prediction in collaborative filtering by making use of preference regularization. Our framework is designed based on the idea that similar users should retain the distance in the low-rank space after RSVD. One characteristic of our framework is that collaborative fil-tering and distance metric learning serve as regularization to each other and naturally reduce overfitting to both. Another characteristic is that social com-munity information and similarity information can be easily considered in our framework. We have conducted several sets of experiments on two real-world datasets to evaluate our framework. We have compared our framework with exiting works. The experimental results show that our framework achieves a very promising performance.

