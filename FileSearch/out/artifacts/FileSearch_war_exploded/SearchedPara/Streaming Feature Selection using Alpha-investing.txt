 In Streaming Feature Selection (SFS), new features are se-quentially considered for addition to a predictive model. When the space of potential features is large, SFS offers many advantages over traditional feature selection method s, which assume that all features are known in advance. Fea-tures can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for addi ng features to the model. We describe  X  -investing, an adap-tive complexity penalty method for SFS which dynamically adjusts the threshold on the error reduction required for adding a new feature.  X  -investing gives false discovery rate-style guarantees against overfitting. It differs from standa rd penalty methods such as AIC, BIC or RIC, which always drastically over-or under-fit in the limit of infinite number s of non-predictive features. Empirical results show that SF S is competitive with much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with over a million potential features.
 G.3 [ Probability and Statistics ]: Statistical computing; I.5.2 [ Pattern Recognition ]: Design Methodology X  Fea-ture evaluation and selection Algorithms Classification, Multiple Regression, Feature Selection, F alse Discovery Rate
In many predictive modeling tasks one has a fixed set of observations from which a vast (or even infinite) set of poten -tially predictive features can be computed. Of these, often only a small number are expected to be useful in the pre-dictive model. Pairwise interactions and data transforma-tions of an original set of features are frequently importan t in obtaining superior statistical models, but consequentl y expand the number of feature candidates while leaving the number of observations constant. For example, in a recent bankruptcy prediction study described below (see Section 6), pairwise interactions between the 365 original candida te features led to a set of over 67 , 000 resultant candidate fea-tures, of which about 40 proved to be significant. The fea-ture selection problem is to identify and include features from a candidate set with the goal of building a statistical model with minimal out-of-sample (test) error. As the set of potentially predictive features becomes ever larger, care ful feature selection to avoid overfitting and to reduce compu-tation time becomes ever more critical.

In this paper, we present streaming feature selection (SFS), a class of methods in which features are considered sequen-tially for addition to a model, and either added to the model or discarded. By modeling the candidate feature set as a dynamically generated stream, we can handle candidate fea-ture sets of unknown, or even infinite size, since not all po-tential features need to be generated and tested. Enabling selection from a set of features of unknown size is useful in many settings. For example, in statistical relational lear n-ing [16, 8, 7], an agent may search over the space of SQL queries to augment the base set of candidate features found in the tables of a relational database. The number of candi-date features generated by such a method is limited by the amount of CPU time available to run SQL queries. Gen-erating 100,000 features can easily take 24 CPU hours[19], while millions of features may be irrelevant due to the large numbers of individual words in text. Another example oc-curs in the generation of transformations of features alrea dy included in the model (e.g. pairwise or cubic interactions) . When there are millions or billions of potential features, j ust generating the entire set of features(e.g. cubic interacti ons or three-way table merges in SQL) is often intractable. Tra-ditional regularization and feature selection settings as sume that all features are pre-computed and presented to a learne r before any feature selection begins. SFS does not.
We present here both the general SFS approach and a sim-ple algorithm,  X  -investing, that exploit the streaming fea-ture setting to produce useful models.  X  -investing is moti-vated from a desire to control the false discovery rate (FDR) of added features. As described below, FDR measures the percentage of  X  X purious X  features added to the model, i.e., the percentage of features that do not improve accuracy on a hypothetical infinite validation set, and indirectly cont rols the degree of overfitting, as measured by test error.
SFS with  X  investing applies to a wide variety of models where p-values or similar measures of feature significance are generated. We evaluate  X  -investing using linear and lo-gistic regression, where a large variety of selection crite ria have been developed and tested previously in traditional fe a-ture selection settings. Empirical evaluation shows that, as predicted by theory,  X  -investing provides superior models under the SFS setting compared to traditional feature selec -tion penalty criteria including AIC [3], BIC [21], and RIC [6, 9].

Although, as noted above, SFS is designed for settings in which the feature set size is unknown, in order to compare it with stepwise regression, we applied SFS in traditional feature selection settings, i.e. those of fixed feature set s ize. In such settings, we find that SFS provides competitive per-formance to stepwise regression for smaller feature sets, a nd offers significant computational savings and higher predic-tion accuracy when the feature set size becomes moderately large.
Traditional feature selection typically assumes a setting consisting of n observations and a fixed number p of candi-date features. The goal is to select the feature subset that will ultimately lead to the best performing predictive mode l. The size of the search space is therefore 2 p , and identifying the best subset is NP-complete. Many commercial statistica l packages offer variants of a greedy method,stepwise feature selection: an iterative procedure in which at each step the best possible feature is selected and added to the model. Stepwise regression thus performs hill climbing in the spac e of feature subsets. Stepwise selection is terminated when either all candidate features have been added, or none of the remaining features lead to increased expected benefit according to some measure, such as a p-value threshold. We will show that an even greedier search, in which each feature is considered only once (rather than at every step) gives com -petitive performance. Variants of stepwise selection abou nd, including forward (adding features deemed helpful) back-wards (removing features no longer deemed helpful) and mixed methods that alternate. Our evaluation and discus-sion will assume a simple forward search.
 Table 1: Symbols used throughout the paper and their definitions.

Symbol Meaning n Number of observations p Number of candidate features q Number of features currently included in a model Table 2: Different choices for the model complexity penalty F .
 Name Nickname Penalty Akaike Information Criterion AIC 2 Bayesian Information Criterion BIC log( n ) Risk Inflation Criterion RIC 2 log( p )
There are many methods for assessing the benefit of adding a feature. Computer scientists tend to use cross-validatio n, where the training set is divided into several (say k ) batches with equal sizes. k  X  1 of the batches are used for train-ing while the remainder batch is used for evaluation. The training procedure is run k times so that the model is eval-uated once on each of the batches and performance is aver-aged. The approach is computationally expensive, requirin g k separate retraining steps for each evaluation. A second ob-jection is that when observations are scarce the method does not make good use of the observations. Finally, when many different models are being considered (e.g. different combi-nations of features), there is a serious danger of over-fitti ng, when cross-validation is used. One, in effect, is selecting t he model to fit the test set.

Penalized likelihood ratio methods [5] for feature selec-tion are preferred to cross-validation by many statisticia ns, as they have an advantage in not requiring the multiple re-training of the model, and have attractive theoretical prop -erties. Penalized likelihood can be represented as: where F is a function designed to penalize model complex-ity, q represents the number of features currently included in the model at a given point. We continue to denote the total number of candidate features as p and the number of observations in the training set as n . Table 1 contains these definitions which we use throughout the paper. The first term in the equation represents a measure of the in-sample error given the model, while the second is a model complexity penalty.

Using Equation 1, only features that decrease scores get added. In other words, the benefit of adding the feature to the model embodied by the likelihood ratio must surpass the penalty incurred by increasing the model complexity. We focus now on choice of F , where several methods have been developed and applied for regression. Many different functions F have been used, defining different criteria for feature selection. The most widely used of these criteria ar e the Akaike Information Criterion (AIC), the Bayesian In-formation Criterion (BIC), and the Risk Inflation Criterion (RIC). Table 2 summarizes the the penalties F used in these methods.

This paper presents SFS using  X  -investing. A virtually identical method can also be derived using a minimum de-scription length formalism [22], giving what we have called the information investing criterion (IIC) [23]. For exposi -tion we find it useful to compare the different choices of F as alternative coding cost schemes for use in a minimum description length (MDL) criterion framework. Equation 1 can be viewed as the length of a message encoding a statis-tical model plus the training set response values given that model. A model encoding scheme for feature selection must identify which features are selected for inclusion in addit ion to encoding the parameters of the included features. Differ-ent choices for F correspond to different coding schemes for the model. In contrast the cost of encoding the training set response values is independent of F .

Generally, encoding schemes work better when they more optimally encode the model: producing the most accurate depiction of the model using the fewest number of bits. AIC corresponds to an encoding of each model parameter in two bits (F=2). BIC X  X  choice of F = log( n ) employs more bits to encode each parameter as the training set size grows larger. Using BIC, each zero parameter (feature not included in the model) is coded with one bit, and each non-zero parameter is coded with 1 + 1 2 log( n ) bits. (All logs are base 2.) Recalling that the log likelihood of the data given a model gives the number of bits to code the model error, leads to the BIC criterion for feature selection: ac-cept a new feature x i only if the change in log likelihood from adding the feature is greater than 1 2 log( n ), i.e. if lent to a Minimum Description Length (MDL)[20] criterion if the number of features considered, p is much less than the number of observations, n . However, BIC is not a valid code for p  X  n .

Both AIC and BIC methods suffer as p grows larger than n , a situation common in SFS settings. We confirm this theory through empirical investigation in Section 6. RIC corresponds to a penalty of F = 2  X  log ( p ) [12]. Though the criteria is motivated by a minimax argument, following [22] we can view RIC as an encoding scheme where log p bits encode the index of which feature was added. Such an encoding is most efficient when few of the p candidates enter the model.

RIC can be problematic in SFS settings since RIC requires that we know p in advance, which is often not the case (See Section 3). We are forced to guess a likely p , and when our guess is inaccurate, the method may be too stringent or not stringent enough. By plugging F = 2 log p into Equation 1 and examining the resulting chi-squared hypothesis test, i t can be shown that the p-value required to reject the null hy-pothesis must at least be smaller than 0 . 05 p . In other words, RIC may be viewed as a Bonferroni p-value thresholding method. Bonferroni methods are known to be overly strin-gent [4], a problem exacerbated in SFS applications when p should technically be chosen to be the largest number of features that might be examined. On the other hand, if p is picked to be a lower bound of the number of predictors that might be examined, then it is too small and there is increased risk that some feature will appear by chance to give significant performance improvement.
In streaming feature selection (SFS), candidate features are sequentially presented to the modeling code for poten-tial inclusion in the model. As each feature is presented, a decision is made using an adaptive penalty scheme as to whether or not to include the feature in the model. Each feature need be examined at most once.

The  X  X treaming X  view generalizes the process by which features are generated and the order in which they are pre-sented. For instance, one implementation may allow cycli-cal streams that loop through previously dismissed feature s, while others may allow features to only be presented to the selection strategy once. In our experiments, we found that using a second pass through the features was not helpful for our real data sets. Most importantly, the stream structure does not assume a fixed number of features, or even that the feature stream be determined in advance. Features can be generated dynamically based on which features have already been added to the model. 1 Note that the theory provided below is independent of the feature generation scheme used. All that is required is a method of generating features, and an estimation package which given a proposed feature for addition to the model returns a p-value for the correspond-ing coefficient or, more generally, the change in likelihood o f the model resulting from adding the feature.

For concreteness, consider a binary classification task to be modeled using logistic regression with feature selectio n. The data set has n observations and p original predictors. In addition to the p original predictors, there are p 2 pair-wise interaction terms formed by multiplying all p 2 pairs of features together ( e.g. x 2 1 , x 1 x 2 , . . . ). (Almost half of these features are, of course, redundant with the other half due to symmetry, and so need not be generated and tested.) We re-fer to the p 2 interaction terms as generated predictors, and consider them as members of a more general class of pre-dictors formed from transformations of the original featur es (square root, log, etc. ), or combinations of them including, for instance, principle components analysis (PCA) or other unsupervised clustering of the observations. Such strate-gies are frequently successful in obtaining better predict ive models.

In recent years, advances in computing power and data storage technology have lead to novel methods of feature generation. For example, Statistical Relational Learning (SRL) methods often generate tens or hundreds of thousands of potentially predictive features. SRL and related meth-ods  X  X rawl X  through a database or other relational struc-ture and generate features by building increasingly comple x compound relations [16, 8, 7]. For example, when building a model to predict the journal in which an article will be published, potentially predictive features include the wo rds in the target article itself, the words in the articles cited by the target article, the words in articles that cite articl es written by the authors of the target article, and so forth.
Traversing such relational structures can easily generate several billion features, since there are many words, autho rs, and journals. A hundred billion numbers do not fit easily into memory on most contemporary computers, and so com-puting all possible features in advance is ill-advised. Eve n if memory were not an issue, traditional stepwise regression o n a hundred billion features is not affordable on most modern computers. Some other strategy is required. A natural al-ternative is to interleave the generation of features with t he assessment of model improvement. Features that are dis-
One cannot use the coefficients of the features that were not added to the model, as this would lead to overfitting. Initialize Do forever missed as unhelpful do not burden memory or computation time from that point on.

By interleaving the generation of features with the assess-ment of model improvement, we may prune the search over features to those which are likely to lead to improvement, and thus make a potentially intractable search tractable. I n structural logistic regression and inductive logic progra m-ming applications, the approach is to search further in thos e branches of a refinement graph if at least one of the compo-nent terms has proven predictive. In the pairwise interac-tion term example, as p gets large we see that the number of interactions grows quadratically, creating a similar co m-putational burden to feature selection. In the case of inter -action terms we might first perform feature selection on the untransformed p predictors, selecting q examples, and then perform feature selection on the q p interaction terms formed from the selected base predictors. Frequently, q &lt;&lt; p and so a great deal of computational time is saved. More im-portantly, one does not need to penalize complexity for the many interaction terms which are never examined.  X  -investing controls the false discovery rate by dynami-cally adjusting a threshold on the p-statistic for a new fea-ture to enter the model. It is easy to implement, and gives provable control of the false discovery rate. The algorithm is shown in figure 1.) The threshold,  X  i , corresponds to the probability of including a spurious feature at step i . It is adjusted using the wealth, w i , which represents the current acceptable number of future false positives. Wealth is in-creased when a feature is added to the model (presumably correctly, and hence permitting more future false positive s without increasing the overall false discovery rate). Weal th is decreased when a feature is not added to the model, in order to save enough wealth to add future features.
More precisely, a feature is added to the model if its p-value is greater than  X  i . The p-value is the probability that a feature coefficient could be judged to be non-zero when it is in fact zero. It is computed by using the fact that  X  X oglikelihood is equivalent to t-statistic. The idea of  X  -investing is to adaptively control the threshold for adding features so that when new (probably predictive) features ar e added to the model, one  X  X nvests X   X  increasing the wealth, raising the threshold, and allowing a slightly higher futur e chance of incorrect inclusion of features. We increase weal th by  X   X   X   X  i . Note that when  X  i is very small, this in-crease amount is roughly equivalent to  X   X  . Each time a feature is tested and found not to be significant, wealth is  X  X pent X , reducing the threshold so as to keep the guaran-tee of not adding more than a target fraction of spurious features. There are two user-adjustable parameters,  X   X  and W 0 , which can be selected to control the false discovery rate; we set both of them to 0.5 in all of the experiments presented in this paper.  X  -investing allows us to bound, in expectation, the relative fraction of features incorrectly and correctly added to the model.

Theorem 1. Let M i be the number of correct features in-cluded in the model, let N i be the number of spurious fea-tures (those with true coefficient zero) included and w i wealth, all at iteration i , and let  X   X  &lt; 1 be a user selected value. Then if the algorithm in Figure 1 is followed: Proof. The proof relies on the fact that is a super-martingale [15]. I.e., S i is, in expectation, non-increasing in each iteration: E ( S i )  X  S i  X  1 . Thus but since we start out with N i = 0, and M i = 0, Further, since w i &gt; 0 by construction,
The proof that S i is a super-martingale is straightforward by considering the cases when the feature is or is not in the true model and is or is not added to the estimated model. Our result, which can be written as or, as many features are added to the model, is very similar to a classic false discovery rate, which prov ides a bound on E ( N i /M i )
The selection of  X  i as w i / 2 i gives the slowest possible decrease in wealth such that all wealth is used; i.e., so that as many features as possible are included in the model without systematically over-fitting. More formally:
Theorem 2. Computing  X  i as w i / 2 i gives the slowest possible decrease in wealth such that if no feature is added, then
Proof. The proof has two parts. (a) Setting  X  i to w i / 2 i would lead to not all of the wealth being used (i.e. not enough features added to the model) and (b) setting  X  i to w / 2 i 1  X   X  would lead to the wealth potentially going to zero after a finite number of features was seen, thus prohibiting any more features from being added, regardless of how sig-nificant they were. If no features are added to the model, wealth is w i =  X  i (1  X  1 / 2 i ). Noting that as i becomes large,  X   X  1 / 2 i becomes small, and taking the limit as an infinite number of features are considered,
This is zero only if P  X  i =  X  , which requires the update rule to decrease as 1 /i or faster. If wealth were updated more slowly, the corresponding sum P 1 /i 1+  X  would be finite and wealth would not go to zero.
SFS also provides another, much more subtle, guarantee against over-fitting. For the case of  X  X ard X  problems, where the coefficients to be estimated are just barely distinguish-able above the noise, the cost (increase in out-of-sample er -ror) of adding a  X  X alse X  feature is comparable to the benefit of adding a true features. One then wants to have a false discover rate of just under fifty percent.

This is a property of using a so-called testimator . A testi-mator tests for significance and then estimates by the usual estimator if it is significant, and estimates by zero other-wise. If a feature has a true coefficient of zero, then when it is falsely included, it will be biased by about t  X  SE , where t is the critical value used for testing significance, and SE is the standard error of the coefficient. On the other hand, the hardest to detect coefficients will have a coefficient of about t  X  SE . Hence leaving them out will bias their esti-mated value by about the same amount, namely t  X  SE . We can thus get optimal test error by adding as many features as possible while not exceeding a specified ratio of false to true features added to the model. This is very similar to controlling the False Discovery Rate (FDR) [4], the number of features incorrectly included in the model divided by the total number of features included in the model, which has become popular in recent years. In the regime that we are working, correctly adding a feature always reduces both the FDR and the out-of-sample error, and incorrectly adding a feature always increases both FDR and error.
We compared streaming feature selection using  X  -investing against both streaming and stepwise feature selection usin g the AIC, BIC and RIC penalties on a battery of synthetic and real data sets.

The base synthetic data set contains 100 observations each of 1,000 features, of which 5 are predictive. We generate the features independently from a normal distribution, N (0 , 1), with the true model being the sum of five of the predic-tors plus noise, N (0 , 0 . 1 2 ). The artificially simple structure of the data (the predictors are uncorrelated and have rel-atively strong signal) allows us to easily see which feature selection methods are adding spurious features or failing t o find features that should be in the model.

The results are presented in Table 3. As expected, AIC massively overfits, always putting in as many features as there are observations. BIC overfits severely, although sli ghtly less badly when streaming is used rather than the less greedy stepwise selection procedure. RIC gives performance com-parable to  X  -investing. As one would also expect, if all of the true features in the model are first in the stream,  X  -investing does much (three times) better than RIC, while if all of the true features in the model are last,  X  -investing does much (four times) worse than RIC. In practice, if one is not tak-ing advantage of known structure of the features, one can randomize the feature order to avoid such bad performance.
Stepwise regression gave noticeably better results than streaming feature selection (SFS) for this problem. Using AIC and BIC still resulted in n features being added, but at least all of the correct features were found. RIC gave half th e error of its streaming counterpart. However, using standar d code from R, the stepwise regression was much slower than SFS. Running stepwise regression on data sets with tens of thousands of features, such as the ones presented below, was not possible.

One might hope that adding more spurious features to the end of a feature stream would not severely harm an algorithm X  X  performance. 2 However, AIC and BIC, since their penalty is not a function or p , will add even more spurious features (if they haven X  X  already added a feature for every observation!). RIC (Bonferroni) puts a harsher penalty as p gets large, adding fewer and fewer features. As Table 4 shows,  X  -investing is clearly the superior method in this case.
Table 5 provides a summary of the characteristics of the 10 real data sets that we used. All 10 data sets involve binary classification tasks. The first seven data sets were taken from the UCI repository. The other three contain gene expression data, in which each feature represents a gene expression value for each individual sample (patient with cancer or healthy donor). In the aml data set, samples consist of patients with acute myeloid leukemia and patient s with acute lymphoblastic leukemia. The classification task is to identify which patients have which cancer. In the ha and hung data sets, samples consist of healthy donors and patients with either Sezary Syndrome (ha data) or Mycosis Fungoides (hung data). The classification task is to distin-guished diseased patients from the controls. Observations which contain missing feature values were deleted.
The baseline accuracy is the accuracy on the whole data set when the majority class is used as the prediction. The feature selection methods were tested on these data sets us-ing ten-fold cross-validation. Since the three biology dat a sets have large feature sets, we shuffled their features five times (in addition to the cross validations), applied SFS on each feature order, and averaged the five evaluation results .
For each data set, we did two kinds of experiments. The first experiments used only the original feature set. The second interleaved feature selection and generation, init ially testing PCA components and the original features, and then generating interaction terms between any of the features which had been selected and any of the original features. In each cross-validation, there were 50 observations in the training set, and the remaining observations were in test set. A small training set size was selected to make sure the problems were difficult enough that the methods gave clearly different results.
One would not, of course, intentionally add features known not to be predictive, but, as described above, there is often a natural ordering on features so that some features such as interactions have a smaller fraction of predictive feature s. Synthetic data: x  X  N (0 , 1) , y is linear in x with noise  X  = 0 . 1 .  X  X irst X  and  X  X ast X  denote the true features being first or last in the data stream.
 samples being used. data sets are gene expression data.
We used R to implement our evaluation. We were unable to run stepwise regression on the internet data set when interaction terms and PCA were included, and on biology data sets, because the stepwise regression algorithm in R had stack overflow problems on these large feature sets. This problem could no doubt be overcome, but it is indicative of the difficulty of running stepwise regression on large data se t. Table 6 shows the evaluation results when only the original features are used; table 7 shows the evaluation results when interaction terms and PCA components are included.
When only the original feature set is used, (Table 6)  X  -investing has better performance than streaming AIC and BIC only on two of the seven UCI data sets: the internet and wpbc data sets. On the other data sets, for the rela-tively fewer numbers of features the less stringent penalti es do as well as or better than SFS. When interaction terms and PCA components are included (Table 7),  X  -investing gives better performance than streaming AIC on five data sets, than streaming BIC on three data sets, and than streaming RIC on two data sets. In general, when the feature set size is small, there is no significant difference in the prediction ac -curacies between  X  -investing and the other penalties. When the feature set size is larger(i.e., when new features are ge n-erated)  X  -investing begins to show its superiority over the other penalties.

We also compared SFS with stepwise regression using the same penalties on each data set. We find that when the original feature set is used, SFS does not differ significantl y from stepwise regression. SFS has better performance than stepwise regression in five cases, and worse performance in four. (Here a  X  X ase X  is defined as a comparison of SFS and stepwise regression under the same penalty on a data set.) However, when interaction terms and PCA components are included, SFS gives better performance than stepwise re-gression in nine cases, and stepwise regression has better performance than SFS in only three cases (all on the spam data set). Thus, in our tests, SFS is comparable to step-wise regression on the smaller data sets and superior on the larger ones.

For the three biology data sets (aml, ha and hung), when comparing  X  -investing with streaming AIC, streaming BIC, and streaming RIC, we find that when the original features are used, RIC gives better performance than  X  -investing. (Table 8) But when interaction terms and PCA are included (Table 9), RIC is often too conservative to select even only one feature, whereas  X  -investing has stable performance and higher accuracy than RIC. Note that, regardless of whether or not interaction terms and PCA are included,  X  -investing always has much higher accuracy than AIC and BIC.
We also tested SFS on a problem of predicting personal bankruptcies[11]. The data set is highly un-balanced, con-taining 2,244 bankruptcy events and hundreds of thousands of non-bankruptcy observations. The real world loss func-tion for predicting bankruptcy is quite asymmetric: the cos t of failing to predict a bankruptcy when one does occur is much higher than the cost of predicting a bankruptcy when none occurs. We call the ratio of these two costs  X  . We compared Streaming Feature Selection against boosted C4.5, doing 5-fold cross-validation, where each pass of the cross-validation uses 100,000 non-bankruptcies and about one fifth of the bankruptcies. SFS with  X  -investing was run once, and then the out-of-sample costs were estimated for Table 10: Loss as a function of the loss ratio,  X  , for boosted C4.5 and for SFS with  X  -investing  X  199 99 19 6 4 1 C.45 Loss 132 76 18.6 7.2 5.09 1.45
SFS+alpha Loss 61 41 15.3 6.9 5.02 1.54 each cost ratio  X  using the predicted probability of bankruptcy. C4.5 was run separately for each value of  X  .

Table 10 shows that for low cost ratios, the two methods give very similar results, but at higher cost ratios, SFS wit h  X  -investing gives around half the loss of C4.5. Using AIC, one would expect over 1,000 variables to be falsely included in the model, based on the fact that an f-statistic-based penalty of 2 corresponds to a t-statistic of wildly generous threshold when considering 67,000 feature s. BIC also massively overfits, although less severely.
Recent developments in statistical variable selection tak e into account the size of the feature space, but only allow for finite, fixed feature spaces, and do not support sequential (or streaming) feature selection. The risk inflation criter ion (RIC) produces a model that possesses a type of competitive predictive optimality. RIC chooses a set of features from th e potential feature pool so that the loss of the resulting mode l is within a factor of log( p ) of the loss of the best such model. In essence, RIC behaves like a Bonferroni rule [9]. Each time a predictor is considered, there is a chance that it will ente r the model even if it is merely noise. In other words, the tested null hypothesis is that the proposed feature does not improve the prediction of the model. Doing a formal test generates a p-value for this null hypothesis. Suppose we only add this predictor if its p-value is less than  X  j when we consider the j th predictor. Then the Bonferroni rule keeps the chance of adding even one extraneous predictor to less than, say, 0.05 by constraining P  X  j  X  0 . 05.

Bonferroni methods like RIC are conservative, limiting the ability of a model to add factors that improve its pre-dictive accuracy. The connection of RIC to  X  -spending rules leads to a more powerful alternative. An  X  -spending rule is a multiple comparison procedure that bounds its cumula-tive type 1 error rate at a small level, say 5%. For example, suppose one has to test the p hypotheses H 1 , H 2 , . . . , H we test the first using level  X  1 , the second using level  X  and so forth with P j  X  j = 0 . 05, then we have only a 5% chance of falsely rejecting one of the p hypotheses. If we as-sociate each hypothesis with the claim that a predictor adds to value to a regression, then we are back in the situation of a Bonferroni rule for variable selection. Bonferroni metho ds and RIC simply fix  X  j =  X /p for each test.

Alternative multiple comparison procedures control a dif-ferent property. Rather than control the cumulative  X  (also known as the family wide error rate), these control the so-called false discovery rate [4]. Control of the false discov -ery rate at 5% implies that at most 5% of the rejected hypotheses are false positives. In variable selection, thi s implies that of the included predictors, at most 5% de-grade the accuracy of the model. The Benjamini-Hochberg method for controlling the false discovery rate suggests th e  X  -investing rule used in SFS, which keeps the false discov-ery rate below  X  : Order the p-values of the independents stepwise regression. the results using the software at hand. See Section 3. earlier tables. format as earlier tables. tests of H 1 , H 2 , . . . , H p so that p 1  X  p 2  X  p p the largest p-value for which p k  X   X / ( p  X  k ) and reject all H i for i  X  k . Thus, if the smallest p-value p 1  X   X /p , it is rejected. Rather than compare the second largest p-value to the RIC/Bonferroni threshold  X /p , reject H 2 if p 2  X  2  X /p . Our proposed  X  -investing rule adapts this approach to eval-uating an infinite sequence of predictors. There have been many papers that looked at procedures of this sort for use in variable selection from an FDR perspective [2], an empiri-cal Bayesian perspective [13, 17], an information theoreti cal perspective [10] or simply a data mining perspective [11]. But all of these require knowing the entire list of possible variables ahead of time. Further, most of them assume that the variables are orthogonal and hence tacitly assume that p &lt; n . Obviously, the Benjamini-Hochberg method fails as p gets large; it is a batch-oriented procedure.

The  X  -investing rule of SFS controls a similar character-istic. Framed as a multiple comparison procedure, the  X  -investing rule implies that, with high probability, no more than  X  times the number of rejected tests are false positives. That is, the procedure controls a difference rather than a rate. As a sequential feature selector, if one has added, say 20 features to the model, then with high probability (tend-ing to 1 as the number of accepted features grows) no more than 5% (i.e., one) are false positives.
A variety of machine learning algorithms have been devel-oped over the years for online learning where observations are sequentially added. Algorithms such as the Streaming Feature Selection (SFS) presented in this paper, which are online in the features being used are much less common. For some problems, all predictors are known in advance, and a large fraction of them are predictive. In such cases, regu-larization or smoothing methods work well and streaming feature selection does not make sense. For other problems, selecting a small number of features gives a much stronger model that trying to smooth across all potential features. (See [1, 14] for a range of feature selection problems and approaches.) For example, in predicting what journal an article will be published in, we find that roughly 10-20 of the 80,000 features we examine are selected [18]. For the problems in citation prediction and bankruptcy prediction that we have looked at, generating potential features (e.g. by querying a database or by computing transformations or combinations of the raw features) takes far more time than the streaming feature selection. Thus, the flexibility that SFS using  X  -investing provides to dynamically decide which features to generate and add to the feature stream provides potentially large savings in computation.

Empirical tests show that for the smaller data sets where stepwise regression can be done, SFS gives comparable re-sults.For smaller feature sets, any of a number of penalty methods can be used. However, unlike stepwise regression, SFS scales will to large feature sets, and unlike the AIC, BIC and RIC penalties, SFS with  X  -investing works well for all values of p and n . Key to this guarantee is the use of an  X  -investing rule which controls the false discovery rate by increasing the threshold on the p-value necessary for addin g a variable to the model each time a variable is found sig-nificant, and decreasing the threshold each time one is not found to be significant. Given any code which incrementally considers features for addition and calculates their p-val ue or entropy reduction, SFS using  X  -investing is extremely easily to implement. For linear and logistic regression, we have found that it can easily handle a million features. We thank Andrew Schein for his help in this work and Malik Yousef for supplying the gene expression data sets. [1] Special issue on variable selection. In Journal of [2] F. Abramovich, Y. Benjamini, D. Donoho, and [3] H. Akaike. Information theory and an extension of the [4] Y. Benjamini and Y. Hochberg. Controlling the false [5] P. Bickel and K. Doksum. Mathematical Statistics . [6] D. L. Donoho and I. M. Johnstone. Ideal spatial [7] S. Dzeroski and N. Lavrac. Relational Data Mining . [8] S. Dzeroski, L. D. Raedt, and S. Wrobel.
 [9] D. P. Foster and E. I. George. The risk inflation [10] D. P. Foster and R. A. Stine. Adaptive variable [11] D. P. Foster and R. A. Stine. Variable selection in [12] E. I. George. The variable selection problem. Journal [13] E. I. George and D. P. Foster. Calibration and [14] I. Guyon. Nips 2003 workshop on feature extraction. [15] J. Jacod and A. Shiryaev. Limit Theorems for [16] D. Jensen and L. Getoor. IJCAI Workshop on [17] I. M. Johnstone and B. W. Silverman. Needles and [18] A. Popescul and L. H. Ungar. Structural logistic [19] A. Popescul and L. H. Ungar. Cluster-based concept [20] J. Rissanen. Hypothesis selection and testing by the [21] G. Schwartz. Estimating the dimension of a model. [22] R. A. Stine. Model selection using information theory [23] L. H. Ungar, J. Zhou, D. P. Foster, and R. A. Stine.
