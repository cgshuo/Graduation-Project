 Spatial collocation patterns associate the co-existence of non-spatial features in a spatial neighborhood. An example of such a pattern can associate contaminated water reservoirs with certain deceases in their spatial neighborhood. Previ-ous work on discovering collocation patterns converts neigh-borhoods of feature instances to itemsets and applies mining techniques for transactional data to discover the patterns. We propose a method that combines the discovery of spatial neighborhoods with the mining process. Our technique is an extension of a spatial join algorithm that operates on mul-tiple inputs and counts long pattern instances. As demon-strated by experimentation, it yields significant performance improvements compared to previous approaches.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining, Spatial Databases and GIS Algorithms Collocation Pattern, Spatial Databases
Spatial database management systems (SDBMSs) [17] man-age large collections of multidimensional data which, apart from conventional features, include spatial attributes (e.g., location). They support efficient operations of basic re-trieval tasks like spatial range queries, nearest neighbor search, spatial joins, etc. On the other hand, SDBMSs do not ex-plicitly store patterns or rules that associate the spatial re-lationships between objects with some of their non-spatial work supported by grant HKU 7149/03E from Hong Kong RGC.
 Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. features. As a result, there is an increasing interest in the automatic discovery of such information, which finds use in a variety of disciplines including environmental research, government services layout, and geo-marketing.

Classic spatial analysis tasks include spatial clustering [16, 14, 15], spatial characterization [8], spatial classification [6], and spatial trend detection [9]. In this paper, we focus on a recent mining problem; that of identifying groups of partic-ular features that appear frequently close to each other in a geo-spatial map. This problem is also referred to as the mining collocation patterns problem [18, 4, 11]. As an ap-plication, consider an E-commerce company that provides different types of services, such as weather, timetabling and ticketing queries [10]. The requests for those services may be sent from different locations by (mobile or fixed-line) users. The company may be interested in discovering types of services that are requested by geographically neighboring users, in order to provide location-sensitive recommenda-tions to them for alternative products. For example, hav-ing known that ticketing requests are frequently asked close to timetabling requests, the company may choose to ad-vertise the ticketing service to all customers that ask for a timetabling service. As another example, a collocation pattern can associate contaminated water reservoirs with a certain decease in their spatial neighborhood.

As a more concrete definition of the problem, consider a number n of spatial datasets R 1 ,R 2 ,...,R n , such that each R i contains objects that have a common non-spatial fea-ture f i . For instance, R 1 may store locations of weather requests, R 2 may store locations of timetabling requests, etc. Given a distance threshold , two objects on the map (independently of their feature labels) are neighbors iff their distance is at most . We can define a collocation pattern P by an undirected connected graph, where each node corre-sponds to a feature and each edge corresponds to a neigh-borhood relationship between th e corresponding features. For example, consider a pattern with three nodes, labeled  X  X imetabling X ,  X  X eather X , and  X  X icketing X , and two edges connecting  X  X imetabling X  with  X  X eather X  and  X  X imetabling X  with  X  X icketing X . An instance of a pattern P is a set of objects that satisfy the unary (feature) and binary (neigh-borhood) constraints specified by the pattern X  X  graph. An instance of our example pattern is a set { o 1 ,o 2 ,o 3 } label ( o 1 )= X  X imetabling X , label ( o 2 )= X  X eather X , label ( o 3 )=  X  X icketing X  (unary constraints) and dist ( o 1 ,o 2 )  X  , dist ( o 1 , o 3 )  X  (spatial binary constraints).

Interestingness measures for collocation patterns express the statistical significance of their instances. They can as-sist the derivation of useful rules that associate the instances of the features. We adopt the measures of [18, 4] (to be reviewed in Section 2), which can be used to solve several variants of the mining problem. Based on the interestingness measures, we propose a method that combines the discov-ery of spatial neighborhoods with the mining process. Note that the discovery of object neighborhoods (or else pattern instances) is in fact a (multi-way) spatial join problem. We extend a hash-based spatial join algorithm to operate on multiple feature sets in order to identify such neighborhoods. The algorithm divides the map and partitions the feature sets using a regular grid. While identifying object neigh-borhoods in each partition, at the same time, the algorithm attempts to discover prevalent and/or confident such pat-terns by counting their occurrences at production time. If the memory is not sufficient to discover the frequent patterns in one pass over all partitions, we employ the partitioning mining paradigm to solve the problem. As demonstrated by experimentation, our technique yields significant perfor-mance improvements compared to previous methods that do not integrate the mining process with the spatial query processing part.

The rest of the paper is organized as follows. Section 2 provides backgound in spatial data mining. Section 3 de-scribes our method for discovering collocation patterns and related association rules. Sections 4 and 5 present our ex-perimental results and conclude the paper.
Past research on mining spatial associations is based on two models; the reference feature model and the colloca-tion patterns model. In this section we briefly review these models and discuss their pros and cons.
The problem of mining association rules based on spa-tial relationships (e.g., adjacency, proximity, etc.) of events or objects was first discussed in [5]. The spatial data are converted to transactional according to a centric reference feature model. Consider a number n of spatial datasets R 1 ,R 2 ,...,R n , such that each R i contains all objects that have a particular non-spatial feature f i . Given a feature f we can define a transactional database as follows. For each object o i in R i a spatial query is issued to derive a set of features I = { f j : f j = f i  X  X  X  o j  X  R j ( dist ( o i ,o The collection of all feature sets I for each object in R fines a transactional table T i . T i is then mined using some itemsets mining method (e.g., [1, 19]). The frequent feature sets I in this table, according to a minimum support value, can be used to define rules of the form:
The support of a feature set I defines the confidence of the corresponding rule. For example, consider the three object-sets shown in Figure 1. The lines indicate object pairs within distance from each other. The shapes indicate different fea-tures. Assume that we want to extract rules having feature a on their left-hand side. In other words, we want to find
Note that we used a distance relationship ( dist ( o i ,o in this definition; in general, any spatial relationship (i.e., topological, distance, directional) between R i and R j could be prescribed by the data analyst. features that occur frequently close to feature a .Foreach instance of a , we generate an itemset; a 1 generates { b, c because there is at least one instance of b (e.g., b 1 and b 2 ) and one instance of c (e.g., c 1 )closeto a 1 . Similarly, a 2 generates itemset { b } (due to b 2 ). Let 75% be the minimum confidence. We first discover frequent itemsets (with min-imum support 75%) in T a = { b, c } , { b } ,whichgivesusa sole itemset { b } . In turn, we can generate the rule with confidence 100%. For simplicity, in the rest of the discussion, we will use f i  X  I to denote rules that associate instances of feature f i with instances of feature sets I , f within its proximity. For example, the rule above can be expressed by a  X  X  b } . The mining process for feature a can be repeated for the other features (e.g., b and c )to discover rules having them on their left side (e.g., we can discover rule b  X  X  a, c } with conf. 100%). Note that the features on the right hand side of the rules are not required to be close to each other. For example, rule b  X  X  a, c does not imply that for each b the nearby instances of a and c are close to each other. In Figure 1, observe that although b 2 is close to instances a 1 and a 2 of a and instance c 2 of c , c 2 is neither close to a 1 ,norto a 2 . Methods for discovering rules requiring the right-hand side features to form a neighborhood will be reviewed in the next subsection.
Recently [10, 18, 4, 11], the research interest shifted to-wards mining collocation patterns , which are feature sets with instances that are located in the same neighborhood. A pattern P of length k is described by a set of features { f 1 ,f 2 , ..., f k } . A valid instance of P is a set of objects { o 1 ,o 2 , ..., o k } :(  X  1  X  i  X  k, o i  X  R i )  X  (  X  1 k, dist ( o i ,o j )  X  ). In other words, all pairs of objects in a valid pattern instance should be close to each other, i.e., the closeness relationships between the objects should form a clique graph . Consider again Figure 1 and the pattern P = { a, b, c } . { a 1 ,b 1 ,c 1 } is an instance of P , but is not. [18, 4] define some useful measures that characterize the interestingness of collocation patterns. The first is the par-ticipation ratio pr ( f i ,P )ofafeature f i in pattern P ,which is defined by the following equation:
Using this measure, we can define col location rules that associate features with the existences of other features in their neighborhood. In other words, we can define rules of the form label ( o )= f i  X  o participates in an instance of P with confidence pr ( f i ,P ). These rules are similar to the ones defined in [5] (see previous subsection); the difference here is that there should be neighborhood relationships be-tween all pairs of features on the right hand side of the rule. For example, pr ( b, { a, b, c } )=0 . 5 implies that 50% of the instances of b (i.e., only b 1 ) participate in some instance of pattern { a, b, c } (i.e., { a 1 ,b 1 ,c 1 } ).

The prevalence prev ( P ) of a pattern P is defined by the following equation:
For example, prev ( { b, c } )=2 / 3, since pr ( b, { b, c and pr ( c, { b, c } )=2 / 3. The prevalence captures the mini-mum probability that whenever an instance of some f i  X  P appears on the map, then it will participate in an instance of P . Thus, it can be used to characterize the strength of the pattern in implying collocations of features. In addi-tion, prevalence is monotonic; if P  X  P then prev ( P )  X  prev ( P ). For example, since prev ( { b, c } )=2 / 3, we know that prev ( { a, b, c } )  X  2 / 3. This implies that the a priori property holds for the prevalence of patterns and algorithms like Apriori [1] can be used to mine them in a level-wise man-ner [18].

Finally, the confidence conf ( P ) of a pattern P is defined by the following equation:
For example, conf ( { b, c } )=1,since pr ( b, { b, c } )=1and pr ( c, { b, c } )=2 / 3. The confidence captures the ability of the pattern to derive collocation rules using the participa-tion ratio. If P is confident with respect to a minimum confidence threshold, then it can derive at least one collo-cation rule (for the attribute f i with pr ( f i ,P )= conf ( P )). In Figure 1, conf ( { b, c } ) = 1 implies that we can find one feature in { b, c } (i.e., b ) every instance of which participates in an instance of { b, c } . Given a collection of spatial objects characterized by different features, a minimum prevalence threshold min prev , and a minimum confidence threshold min conf , a data analyst could be interested in discover-ing prevalent and/or confident patterns, and the collocation rules derived by them. The confidence of a collocation rule is prevalent with respect to min prev ) [18].

Previous methods on mining (prevalent or confident) col-location patterns [10, 18, 4, 11] separate the part that evalu-ates the spatial relationships from the mining part. The typ-ical approach is to initially perform a spatial (distance) join [17] to retrieve object pairs within distance to each other to generate the instances of 2-length patterns. The preva-lence of these patterns is then evaluated and only prevalent ones (and their instances) are kept. From two k -length pat-terns P 1 and P 2 , a candidate pattern P 3 of length k +1 is generated, by Apriori-based candidate generation [1] (the first k  X  1featuresof P 1 and P 2 should be common and the last ones are the additional features in P 3 ). Each candidate pattern (whose subsets are all prevalent) is then validated by joining the instances of P 1 and P 2 where the first k feature instances are common. The distance of the last two feature instances in P 3 is then checked to validate whether they are close to each other. After finding all valid instances of a candidate pattern P 3 , we need to validate whether they are prevalent by counting the participation ratio of each fea-ture in them.

Consider again the example of Figure 1. After spatially joining the object-set pairs ( a, b ), ( b, c ), and ( a, c ), we re-trieve the instances of the corresponding patterns. For ex-( a 2 ,b 2 ) } . Moreover, their prevalences are computed (100% for P 1 ). Note that in order to compute the prevalences, we need a bitmap for each set, that marks the objects that are found in each pattern. For example we use a bitmap for a to mark its instances that participate in P 1 and after counting the number of 1 X  X  in it, we can find that pr ( a, { a, b } If min prev = 30%, observe that also P 2 = { b, c } and P 3 = { a, c } are prevalent. From P 1 and P 3 , we can gen-erate P 4 = { a, b, c } (since its subset P 2 is also prevalent). The instances of P 4 are generated by joining the instances of P 1 with those of P 3 on their agreement on the instance of a . Since the only instance of P 3 is ( a 1 ,c 1 ), the only poten-verifying that b 1 is close to c 1 . The prevalence of P 4 (=1 / 3) is then verified.

This technique is adapted in [18, 4, 11] to mine preva-lent and confident patterns. For confident patterns [4], the method is slightly changed since confidence is not mono-tonic, but has a weak monotonicity property; given a k -length pattern P at most one ( k  X  1)-length sub-pattern P of P may have lower confidence than P . In [10], a simi-lar approach is followed. However, a particular instance of afeature f i is not allowed to participate into multiple in-stances of a pattern P that includes f i . Thisaffectsthe quality of the results, since the actual number of pattern instances is underestimated. Also, depending on which pat-tern instance a particular feature is assigned to, we can have different mining results.

The techniques discussed so far suffer from certain effi-ciency problems. First, they require a potentially large num-ber of ( k  X  1)-length pattern instances to be computed before k -length patterns can be discovered. As discussed in [11], these instances can be too many to fit in memory. Also the computational cost of processing them and computing par-ticipation ratios from them is very high. In addition, spatial joins are only used to find the instances of 2-length patterns only, and afterwards no special techniques are used to prune the space using the distribution of the feature instances in space. In this paper, we build on the work of [18, 4] and pro-pose an efficient technique that integrates the computation of spatial neighbor relationships with the mining algorithm.
Before we describe our methodology, let us briefly discuss the requirements for an efficient spatial collocations mining tool that operates on a SDBMS:
Our proposed mining tool is designed to meet the above requirements. In addition, it is appropriate for mining collo-cation patterns based on prevalence and/or confidence and deriving the rules from them. Last but not least, it can dis-cover patterns and rules based on both reference feature and collocation models, discussed in Sections 2.1 and 2.2. The next paragraphs describe our approach in detail.
The model described and used in [18, 4, 11] considers only collocation patterns, in each valid instance of which all pairs of objects should satisfy some spatial relationship (e.g., they should be close to each other). The same requirement should hold for the patterns mined in [10]. On the other hand, the data analyst may be interested in patterns with arbitrary or no relationships between some feature pairs. In general, we can represent a pattern by a graph, where each node represents a variable labeled by a feature and each edge represents the spatial relationship that should hold between the corresponding variables in valid pattern instances.
Figure 2 shows examples of a star pattern, a clique pat-tern and a generic one. A variable labeled with feature f is only allowed to take instances of that feature as values. Variable pairs that should satisfy a spatial relationship (i.e., constraint) in a valid pattern instance are linked by an edge. In the representations of Figure 2, we assume that there is a single constraint type (e.g., close to), however in the general case, any spatial relationship could label each edge. More-over, in the general case, a feature can label more than two variables. Patterns with more t han one variables of the same label can be used to describe spatial autocorrelations on a map.

By definition, the instances of a k -length pattern are the results of a multi-way spatial join that combines k spatial datasets (indicated by the labels of the variables) using the spatial relationships described by the graph edges [7]. For example, the instances of the pattern in Figure 2a can be de-scribed by the following extended SQL statement, assuming that the instances of feature f i arestoredinrelation R f i
Note that this generic definition for spatial patterns, does not affect the definitions of interestingness measures. For example, the participation ratio of a variable in a pattern P is the percentage of its instances that participate in in-stances of P . The participation ratios of pattern variables can be used to derive useful rules, associating the label (i.e., feature) of the variable with the existence of instances of other variables that qualify the specified constraints of the pattern graph. The participation ratio of the variable la-beled c in the pattern of Figure 2c captures the probability that the existence of c implies an instance of b in its neigh-borhood, which forms a clique with an instance of a and an instance of d .

We will initially confine our discussion in patterns that form a star or clique graph and no label constrains more than one variable, because of their high relevance to previous work. Later, we will discuss how our methods can be used to mine more gene ric patterns.
We will first focus on methods for mining star-like pat-terns (like the one in Figure 2a), which have been defined by Koperski and Han [5] (see Section 2.1). In other words we want to find rules that associate the existence of a fea-ture with the existence of other feature instances near it. As an example, consider the rule:  X  X iven a pub, there is a restaurant and a snack bar within 100 meters from it with confidence 60% X .

Without loss of generality, we assume that the input is n datasets R 1 ,R 2 , ..., R n , such that for each i , R instances of feature f i .Adataset R i may already exist as a spatial relation in the SDBMS (e.g., a relation with cities) or it can be constructed on-the-fly by the analyst (e.g., a relation with large cities, not explicitly stored). Our method imposes a regular grid over the space to mine and hashes the objects into partitions using this grid. Given a distance threshold , each object is extended by to form a disk and hashed into the partitions intersected by this disk. Figure 3 shows an example. The space is partitioned into 3  X  3 cells. Object a 1 (which belongs to dataset R a , corresponding to feature a ) is hashed to exactly one partition (corresponding to the central cell C 5 ). Object b 1 is hashed to two partitions ( C 2 and C 5 ). Finally, object c 1 is hashed into four partitions and hashing them to potentially multiple partitions will be explained shortly. The size of the grid is chosen in a way such that (i) the total number of feature instances (for all features) that are assigned to a grid cell are expected to fit in memory and (ii) the side of a cell is at least 2 long. Due to (i), we can find the patterns using an efficient main memory algorithm for each cell. Due to (ii), no feature instance is assigned to more than four cells (replication is controlled).
Thus the mining algorithm operates in two phases; the hashing phase and the mining phase. During the hashing phase, each dataset R i is read and the instances of the cor-responding feature are partitioned into a number of buckets (as many as the cells of the grid). The mining phase employs a main memory algorithm to efficiently find the association rules in each cell. This method is in fact a multi-way main memory spatial join algorithm based on plane sweep [13, 2, 7]. The sketch of the algorithm is given in Figure 4. The synch sweep procedure extends the plane sweep technique used for pairwise joins to (i) apply for multiple inputs and (ii) find for each instance of one input if there is at least one instance from other inputs close to it. synch sweep takes as input a feature f i (also denoted by the index i ) and a set of partitions of all feature instances hashed into the same cell C , and finds the maximal patterns each feature instance is included directly (without comput-ing their sub-patterns first). The objects in the partition R (corresponding to feature f i )incell C are scanned in sorted order of their x -value (lines 3 X 17). Objects outside the cell are excluded for reasons we will explain shortly. For each object o i , we initialize the maximal star pattern L ,where o can participate as L  X  X  center. Then for each other feature, we sweep a vertical line along x -axis to find if there is any instance (i.e., object) within distance from o i ;ifyes,we add the corresponding feature to L . Finally, L will contain the maximal pattern that includes f i ; for each subset of it we increase the support of the corresponding collocation rule.
The algorithm requires two database scans; one for hash-ing and one for reading the partitions, performing the spatial joins and counting the pattern supports. If the powerset of all features but f i cannotfitinmemory,wecanstillap-ply the algorithm with three database scans instead of two. First, the instances are hashed as usual. Then for each cell C , for each feature f i : (i) the maximal patterns for all in-stances of f i in C are found and stored locally as itemsets, (ii) a itemsets mining method (e.g., [19]) is used to find the locally frequent patterns in cell C and the itemsets are written to a temporary file T C i corresponding to f i and C . Finally, after all cells have been visited, for each feature f the global frequences of patterns which are locally frequent in at least one cell are counted at a single scan of all T and from the globally frequent patterns corresponding spa-tial collocation rules are generated. Note that the number of local max-patterns for a particular feature f i and cell C are at most as many as the number of instances of f i in C , and therefore are guaranteed to fit in memory (and mined there). Finally, note from Figure 3 that an object o i can be hashed to many cells. However the maximal pattern in-stance that contains o i as center will be counted only once, since we peform mining for o i as center only at the (exactly one) cell, where the object is inside (line 5 of synch sweep ). However, we still need the replicated instances at the neigh-bor cells (if any) to assist finding the patterns having other features as center and o i as neighbor object.

Our algorithm is related to hash-based spatial join tech-niques [12] and multiway spatial joins [7]. [12] proposes an algorithm that joins two spatial datasets, retrieving the sub-set of their cartesian product that qualifies a spatial predi-cate (usually overlap). It hashes the two datasets into buck-ets using a grid, in the same way as our algorithm and then joins bucket pairs to find the qualifying join pairs. [7] pro-poses methods that extend binary join algorithms to apply on multiple inputs. In this case, the problem is to find the subset of the Cartesian product of multiple relations, where the instances qualify some constraint graph, like the ones shown in Figure 2. Our method is essentially different since, even though it applies (and joins) multiple datasets, it finds the maximal patterns that each instance of a relation qual-ifies. Thus, the join results in our case may contain fewer feature instances than the total number of features.
The algorithm of Figure 4 may need to perform a large number of computations in order to find the maximal collo-cation pattern for each object o i . In the worst case, we may need to compute the distance between o i and every instance of every other feature f j , f j = f i ,incell C .Inordertode-crease this cost (exponential to the number of features), we propose the following heuristic. For a given cell C ,before processing the join, we perform a secondary spatial hashing in memory, this time using a fine grid F ; we divide C into smaller cells with  X  = / all buckets R C i (for every feature f i ) are hashed into smaller buckets in memory, this time without replication; each ob-ject goes to exactly one micro-cell that includes it. Figure 5a shows an example of a cell C , where a number of instances of feature a have been hashed. C corresponds to the shaded region. Observe that the bucket R C a also contains two in-stances which are outside C (but they are within distance from it). The area defined by C extended by at all sides is divided into smaller micro-cells as shown in Figure 5. 2 Af-ter partitioning the instances using the micro-cells, we know the number of objects of R a in each of them, as indicated by the numbers in the figure. Figure 5b shows partition R corresponding to another feature b , but to the same cell C .
While trying to find patterns that are centered with an a in C , recall that we need to see for every instance of a ,which
For the ease of discussion, in this example we assume that the side of each cell C is a multiple of  X  ; this technique is still applicable in the general case. /* R i stores the coordinates of all objects with feature f Algorithm find centric collocations ( R 1 ,R 2 , ..., R n 1. /* 1. Spatial-hashing phase */ 2. super-impose a regular grid G over the map; 3. for each feature f i 4. hash the objects from R i to a number of buckets: 5. each bucket corresponds to a grid cell; 6. each object o is hashed to the cell(s) intersected by 7. the disk centered at o with radius ; 8. /* 2. Mining phase */ 9. for each cell C of the grid; 10. for each feature f i 11. load bucket R C i in memory; 12. /* R C i containing objects in R i hashed into C */ 13. sort points of R C i accordingtotheir x co-ordinate; 14. for each feature f i 15. synch sweep ( f i , R C 1 ,R C 2 , ..., R C n ); function synch sweep (feature f i , buckets R C 1 ,R C 2 , ..., R 1. for each 1  X  j  X  n, j = i 2. a j = 1; /* pointer to the first object in R C j */ 3. while there are more objects in R C i 4. o i := next object in R C i ; 5. if o i is in C then /* exclude objects outside C */ 6. L := ; /* initialize an empty feature-set */ 7. for each 1  X  j  X  n, j = i 8. while a j  X | R C j | and R C j [ a j ] .x &lt; o i  X  9. a j := a j +1; 10. p := a j ;/*startmovingapointerfrom a j */ 11. while p  X | R C j | and R C j [ p ] .x  X  o i + then 12. /* o i is x-close to R C j [ p ]*/ 13. if dist ( o i ,R C j [ p ])  X  then 14. L := L  X  f j ; /* found feature f j near o i */ 15. j := j +1; go to line 8; 16. for each I  X  L 17. conf ( f i  X  I ):= conf ( f i  X  I )+1; Figure 4: An algorithm for reference feature collo-cations is in R C a and inside C , if there are some nearby instances of the other features. By only looking at the numbers of the small cells in Figure 5a and Figure 5b, we can directly know that the objects in the first two circled micro-cells certainly have a b neighbor since the corresponding micro-cell in R C b is occupied. Also, we can infer that the objects in the circled micro-cell at the bottom-right corner can have no b neighbor, since this and the surrounding cells (marked by the thick line) are empty in R C b . Thus, we can adjust the algorithm of Figure 4 as follows: This optimization is expected to be effective when the fea-ture instances are skewed in space. In this case, we can save a lot of computations, by (i) inferring patterns directly (if the corresponding micro-cells are non-empty) and (ii) ex-
Figure 5: Using the finer grid to infer patterns cluding features (if the corresponding and neighbor micro-cells are empty).
The methodology described in the previous paragraphs can be extended to mine (prevalent or confident) clique col-location patterns. In this case, we are interested in finding patterns like the one in Figure 2b. In instances of such pat-terns every feature instance is close to every other feature instance.

The algorithm of Figure 4 stops when it finds the features with at least one instance close to the current object o i suffices to discover the unique maximal pattern instance cen-tered at o i . However, when it comes to discovery of cliques the problem becomes more complicated, since there could be multiple maximal cliques that can contain the current object o . For example, consider an instance a 1 of feature a which is part of two cliques; { a 1 ,b 1 ,c 1 } and { a 1 ,b 1 ,d 1 a clique. As another example, consider cliques { a 1 ,b 1 ,c 1 and { a 1 ,b 2 ,d 1 } ;inthiscase, a 1 is in instances of patterns { a, b, c } and { a, b, d } , but the instances of b in them is not common.

The implication is that for each instance o i (i) we have to find all maximal clique patterns it participates in ( { a, b, c and { a, b, d } for a 1 in the previous example) and (ii) we should not count the subpatterns more than once (we should not count the occurence of a 1 in { a, b } twice, although it participates in two super-patterns of { a, b } ).

Our algorithm extends the synch sweep function to com-pute instances (and prevalences) of clique patterns as fol-lows. For a given o i , the plane sweep algorithm finds all clique pattern instances where o i participates in. This is performed by checking the distance between the instances of other features that are close to o i , using a search heuris-tic based on backtracking [7]. After obtaining all clique in-stances that contain o i , we then mark the corresponding patterns and all their subpatterns, such that each distinct pattern where o i takes part is marked only once. The preva-lences and confidences of all those marked patterns are then increased accordingly, before the algorithm proceeds to the next o i . For example, consider an instance a 1 of feature a which is part of two cliques; { a 1 ,b 1 ,c 1 } and { a 1 ,b 2 ,d 1 patterns whose prevalences and confidences will be increased are { a, b } , { a, c } , { a, d } , { a, b, c } ,and { a, b, d
We note here that, since our method attempts to count at one scan the instances of the powerset of all possible pat-terns, the space required is too high for a large number of database features (exponential to the total number of fea-tures). This makes our approach slow (for clique patterns) when the total number of features is large. However, we observe that in typical applications, although there can be a large number of features only few of them usually partic-ipate in long patterns [17]. In order to alleviate the bot-tleneck of our approach, when there are many features we perform mining in two steps; first we apply our technique to find the prevalences (and/or confidences) of all pairs of features only. We then prune those features which do not appear in any prevalent (and/or confident) 2-pattern and reduce the powerset of patterns, which we have to count in our algorithm.
Generic collocation patterns associate the existence of fea-tures by an arbitrary (connect ed) graph (like the one of Fig-ure 2c). We could find patterns that form arbitrary graphs, by extending the synch sweep function to find all combi-nations of feature instances where o i appears. However, the space of possible patterns is huge in this case. For a given set of features the number of possible graphs that connect them is exponential to the size of the set. Moreover, we have to consider the powerset of the total number of features to consider. For example, if we have 4 features { a, b, c, d should consider all possible graphs that connect { a, b, c, d all possible graphs that connect { a, b, c } , etc. Nevertheless our techniques are still applicable when the space of con-sidered graphs is restricted (e .g., patterns where all but one features form a clique). In the future, we plan to investigate the discovery of arbitrary patterns by alternative means (i.e., use of Apriori-based methods).
In this section, we compare our methods with previous bottom-up Apriori-based approaches [5, 18, 4, 11]. We eval-uate the performance of the algorithms using synthetic and real datasets. The algorithms were implemented in C++ and the experiments were run using a 700MHz Pentium III machine with 4Gb of main memory. In the next subsection, we describe the synthetic data generator. Section 4.2 com-pares the algorithm of Section 3.2 with the methods pro-posed by [5, 18] for reference feature collocation patterns (i.e., star patterns). Our algorithm for clique patterns (Sec-tion 3.3) is compared with the methods of [18] and [4] in Section 4.3. Last, Section 4.4 compares the performance of the algorithms using a real dataset.
Table 1 summarizes the parameters used in the data gen-erator. Firstly, we set L features, which we call non-noise features and can appear in the longest collocation pattern which is generated. We also set n noise noise features. The number of points for noise features is r noise  X  N . We assign these points to the noise features uniformly. The remaining points are assigned to non-noise features uniformly. The participation ratio of a feature in the longest pattern which has participation ratio larger than the confidence threshold is  X  max +  X  .Thenumberofpoints N i ,whichmustap-pear in the instances of longest pattern of a feature f i (  X  pation ratios are  X  min +  X  and the number of points in the instances of longest pattern is (  X  min +  X  )  X  N  X  (1  X  r noise Parameter Meaning n noise the number of noise features We generate instances of the longest pattern as follows. We divide the map using a regular grid of cell-side length . At first, we generate a point randomly. We use the point as the center and r as the radius to generate points for a feature in the longest pattern around a circle. The coordinates for the i  X  th point of the feature f j is ( x c + r  X  sin 2  X i r assign r s the value s ( 2 L ), (1  X  s  X  L ). In this way, any point in the cell can participate in an instance of the longest pattern. After selecting the first center point, we mark the cells in the grid which intersect the circle centered at it with radius , such that no other longest pattern instance can be generated in them. Next, we continue generating pattern instances from random points, whose extended circle does not intersect used cells. After generating pattern instances d times, the process ends. The remaining points of the features which appear in the longest pattern, are generated randomly on the map. Finally, we generate the points of noise features randomly on the map.

The generator described above generates instances of a long pattern with length L . The number of features which have participation ratios larger the confidence threshold in the longest pattern is set to m . First, we experimented with the discovery of star patterns. We compare the performance of the algorithm of Section 3.2 (called FC for  X  X ast collocations X ) with the methods pro-posed by [5, 18] that (i) generate 2-patterns using binary spatial joins (ii) perform level-wise mining [1] to discover the patterns from the instances of 2-patterns (called LW for  X  X evel-wise X  approach). For this set of experiments, we used synthetic datasets.

First, we study the effect of the number of points in a dataset generated using a square map 7070  X  7070. Table 2 shows the standard generation parameter values used in this set of experiments. Note that we use a rather small dataset size (30K), because the level-wise mining methods are quite slow and it would be quite hard to compare our technique with them for larger databases. Because the constraints in star patterns are quite loose compared to those of clique patterns, the number of patterns and their instances is very large. This causes the Apriori-like algorithm to be very slow as shown in Figure 6. However, our algorithm is scalable, since the long patterns are discovered directly, without going through the discovery (and TID-join) of their subpatterns. Table 2: Standard parameter values for star pattern mining
In the next experiment, we compare the performance of the two techniques as a function of the number of non-noise features in the datasets. Figure 7 shows that our algo-rithm maintains great advantage compared to the Apriori-like technique.

In the previous experiments, we used only 2 noise fea-tures. When the number of noise features increases, the number of 2-patterns increases since more combinations of features are likely to be found frequently together. Figure 8 shows that the time for Apriori-like mining is not affected by the change of this parameter. The cost of our technique increases slightly, however, it is still much faster.
In this section, we validate the performance of our algo-rithm in mining clique patterns, by comparing it with the previous approaches proposed in [18] and [4].

The effect of number of points in the dataset was evalu-ated with datasets generated on a square map 8000  X  8000. Table 3 shows the generator X  X  parameters for this set of ex-periments. Figure 9 shows the results. LW-prev corresponds Table 3: Standard parameter values for clique pat-tern mining to the method that mines prevalent patterns using  X  (as de-scribed in [18]). LW-conf corresponds to the method that mines confident patterns using  X  (as described in [4]). Our method (FC) can mine prevalent and confident patterns at the same cost (and at the same time) since it is not a level-wise technique. As we can see from the figure, it is much faster compared to the previous techniques. As the number of points increases, the number of instances for all patterns is increases greatly and this affects all algorithms; the level-wise methods are affected more by the boost of the pattern instances at the various levels.

Next, we evaluated the performance of the three tech-niques as a function of the number L of non-noise features. We used the same parameters as the previous experiment, after fixing N =30 K . Figure 10 plots the results. Ob-serve that as the number of features increases, with fixed number N of points, the number of points for each feature decreases. On the other hand, the number of candidates increases, which in general affects more the cost of the algo-rithms. When the effect of the number of candidates domi-nates the reduction of points per feature, the time for mining increases. Note that in all cases, our method maintains sig-nificant advantage over level-wise techniques.

Next, we test the performance effect of the number of m confident features in the longest pattern (i.e., the num-ber of features with participation ratio larger than  X  in the longest pattern). As m increases, the number of confident sub-patterns of the longest pattern increases. For the level-wise algorithms, the number of candidates increases as m increases. FC, on the other hand, needs to discover longer maximal patterns. Figure 11 compares the three methods. The x-axis is the number of confident features in the longest pattern. Note that the level-wise methods are more sensitive to m , due to the larger number of candidates to be gener-ated and counted for both prevalent and confident patterns. On the other hand, our technique is almost insensitive to m .
Finally, we test the performance of the algorithms as a function of the number of noise features in the database. Figure 12 illustrates the effects. Observe that our algorithm maintains its advantage over previous techniques, due to the heuristic we apply to remove irrelevant features after a preprocessing step that discovers the prevalent/confident 2-pattens (discussed in Section 3.3). We downloaded a real dataset from Digital Chart of the World 3 (DCW), which is an Environmental Systems Re-search Institute. We downloaded 8 layers of Minnesota state, as shown in Figure 13 and described in Table 4. We treat each layer as a feature so that there are 8 features in our experiments.

Figure 14 compares the mining algorithms for star pattern http://www.maproom.psu.edu/dcw/ Figure 14: Mining Star Patterns in a Real Dataset mining. The performance of our algorithm for star patterns is almost not affected by the the distance threshold. On the other hand, the LW approach deteriorates fast with . Figure 15 shows the performance of algorithms for clique pattern mining. Both figures plot the mining cost as a func-tion of the distance threshold used for mining. Note that the performance trends are the same compared to the ex-periments with synthetic data; our method is always much faster compared to the level-wise approaches.

Figure 16 shows the relative performance for clique pat-tern mining as a function of the prevalence (confidence) threshold  X  . Note that our method is very fast even for small values of  X  , as opposed to level-wise methods which are very sensitive to it. Notably, the prevalent patterns mining algo-rithm is also quite fast. This is attributed to the fact that there are very few (and short) prevalent patterns, which are discovered quite fast.

Some long real patterns found include a reference feature pattern with  X  X opulated places X  as center and  X  X eronauti-cal X ,  X  X rainage Supplemental X ,  X  X ypsography X ,  X  X ypsog-raphy Supplemental X ,  X  X and Cover X  as neighbor features; Figure 15: Mining Clique Patterns in a Real Dataset Figure 16: Mining Clique Patterns in a Real Dataset and a confident clique pattern with  X  X ultural Landmarks X ,  X  X rainage X ,  X  X rainage Supplemental X ,  X  X ypsography X ,  X  X yp-sography Supplemental X . Both are found for = 600 and  X  =0 . 05.
In this paper, we have extended the collocation pattern model and proposed a fast technique for mining collocation patterns. The extended model generalizes collocation pat-terns by a constraint graph, where each node corresponds to a feature and the edges correspond to spatial constraints.
As stated in [17] (p. 205), there are typically much fewer features in a spatial mining problem (never more than a few dozens), compared to the number of items in transactional mining problems. Thus the enumeration of spatial neigh-borhood computations for each feature instance dominates the mining cost. Based on this ground truth, we proposed a mining technique which naturally extends multi-way spatial join algorithms to discover pattern instances of any length and directly compute the participation ratios of features in them, which can be used to derive confidences and preva-lences of collocation patterns. We proposed a number of heuristics that optimize the mining process and deal with memory constraints.

Finally, we conducted a comprehensive experimental eval-uation using synthetic and real datasets. The results show that our technique is orders of magnitude faster in the dis-covery of long collocation patterns, compared to previous methods. Another notable advantage of our method is that it can compute both prevalent and confident patterns for multiple values of prevalence and confidence thresholds at a single process, since no candidate pruning takes place. Thus, after the join process it suffices to scan the features and their participation ratios in pattern instances in order to derive the prevalences and confidences of all patterns, and finally keep the ones that are more interesting.
 The authors would like to thank Zhong Zhi for his help with the implementation of the mining algorithm. [1] R. Agrawal and R. Skrikant. Fast algorithms for [2] T. Brinkhoff, H.-P. Kriegel, and B. Seeger. Efficient [3] A. Guttman. R-trees: a dynamical index structure for [4] Y. Huang, H. Xiong, S. Shekhar, and J. Pei. Mining [5] K.Koperski and J.Han. Discovery of spatial association [6] K.Koperski, J.Han, and N.Stefanovic. An efficient [7] N. Mamoulis and D. Papadias. Multiway spatial joins. [8] M.Ester, A.Frommelt, H.-P.Kriegel, and J.Sander. [9] M.Ester, A.Frommelt, J.Han, and J.Sander. Spatial [10] Y. Morimoto. Mining frequent neighboring class sets [11] R. Munro, S. Chawla, and P. Sun. Complex spatial [12] J. M. Patel and D. J. DeWitt. Partition based [13] F. P. Preparata and M. I. Shamos. Computational [14] R.T.Ng and J.Han. Efficient and effective clustering [15] J. Sander, M. Ester, H.-P. Kriegel, and X. Xu. [16] S.Guha, R.Rastogi, and K.Shim. CURE: an efficient [17] S. Shekhar and S. Chawla. Spatial Databases: A Tour . [18] S. Shekhar and Y. Huang. Discovering spatial [19] M. J. Zaki and K. Gouda. Fast vertical mining using
