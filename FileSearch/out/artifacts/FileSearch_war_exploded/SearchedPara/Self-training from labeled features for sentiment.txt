 1. Introduction
With the explosion of people X  X  attitudes and opinions expressed in social media including blogs, discussion forums, tweets, etc., detecting sentiment or opinion from the Web is becoming an increasingly popular way of interpreting data. domains. Moreover, the rapid evolution of user-generated contents demands sentiment classifiers that can easily adapt to new domains with minimum supervision. This thus motivates the investigation of weakly-supervised or unsupervised sen-timent analysis approaches.
 While supervision for a sentiment classifier can come from labeled documents, it can also come from labeled words. For example, the word  X  X  excellent  X  X  typically conveys positive sentiment. A simple approach of using such polarity words for sentiment classification is to compare the frequency of occurrence of positive and negative terms in a document.  X 
However, it does not normally give good results. In recent years, much effort has been devoted to incorporate prior belief of word-sentiment associations from a sentiment lexicon into classifier learning by combining such lexical knowledge with 2009 ).
 the next iteration. Other approaches use ensemble techniques by combining lexicon-based and corpus-based algorithms ( Tan, Wang, &amp; Cheng, 2008 ). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. More recently, Dasgupta and Ng (2009) proposed a weakly-supervised sentiment classification clustering can be considered as sentiment-oriented topics. Nevertheless, human judgement of identifying the most impor-tant dimensions during spectral clustering is required.

In this paper, 1 we propose a simple and robust strategy that works by providing weak supervision at the level of features by constraining the model X  X  predictions on unlabeled instances.

We evaluate our proposed framework on the movie-review data and the multi-domain sentiment dataset and show that our method attains comparable or better performance than other previously proposed weakly-supervised or semi-super-
Related work on weakly-supervised and semi-supervised sentiment classification are discussed in Section 2 . The proposed cludes the paper and outlines directions for future research. 2. Related work
The pioneer work on sentiment classification that does not require labeled data is that of Turney (2002)[Turney X  X  (2002)] which classifies a document as positive or negative by the average semantic orientation of the phrases in the document that contain adjectives or adverbs. The semantic orientation of a phrase is calculated as the pointwise mutual information (PMI) automobile reviews and 66% for movie-reviews. In the same vein, Read and Carroll (2009) proposed three different ways, lexical association (using PMI), semantic spaces, and distributional similarity, to measure the similarity between words and Carroll chose seven polarity prototypes which were obtained from Roget X  X  Thesaurus and WordNet and selected based on their respective frequency in the Gigaword corpus. Still the best result was achieved using PMI with 69.1% accuracy ob-tained on the movie-review data.

There have also been much interests in incorporating prior information from sentiment lexicon containing a list of words bearing positive or negative polarity into sentiment model learning, which we call weakly-supervised sentiment classifica-tion. Sentiment lexicons can be constructed in many different ways, ranging from manual approaches ( Whitelaw, Garg, &amp;
Kim &amp; Hovy, 2004), and even almost fully automated approaches (Kaji &amp; Kitsuregawa, 2006; Kanayama &amp; Nasukawa, 2006; Turney &amp; Littman, 2002). When incorporating such prior information into model learning, Andreevskaia and Bergler (2008) integrate a corpus-based classifier trained on a small set of annotated in-domain data and a lexicon-based system trained on WordNet for sentence-level sentiment annotation across different domains. Li et al. (2009) employ lexical prior knowledge for semi-supervised sentiment classification based on non-negative matrix tri-factorization, where the domain-independent prior knowledge was incorporated in conjunction with domain-dependent unlabeled data and a few labeled documents. Melville et al. (2009) also combine lexical information from a sentiment lexicon with labeled documents where word-class probabilities in Na X ve Bayes classifier learning are calculated as a weighted combination of word-class distribu-tions estimated from the sentiment lexicon and labeled documents respectively. Lin and He (2009) proposed a joint senti-ment-topic (JST) model to model both sentiment and topics from text and they incorporate sentiment prior information by modifying conditional probabilities used in Gibbs sampling during JST model learning.

Instead of incorporating prior information into model learning through sentiment lexicons, Dasgupta and Ng (2009) proposed an unsupervised sentiment classification algorithm where user feedbacks are provided in the spectral clustering process in an interactive manner to ensure that text are clustered along the sentiment dimension. Features induced for each dimension of spectral clustering can be considered as sentiment-oriented topics. Nevertheless, human judgement of identifying the most important dimensions during spectral clustering is required. We compare with the methods of ( Das-or better performance on the movie-review data and outperforms their on the multi-domain sentiment dataset.
Other weakly-supervised sentiment classification approaches typically adopt the self-training strategy. Zagibalov and Car-roll (2008b) start with a one-word sentiment seed vocabulary and use iterative retraining to gradually enlarge the seed vocabulary by adding more sentiment-bearing lexical items based on their relative frequency in both the positive and neg-ative parts of the current training data. Sentiment direction of a document is then determined by the sum of sentiment scores of all the sentiment-bearing lexical items found in the document. The problem with this approach is that there is no principal way to set the optimal number of iterations. They then suggested an iteration control method in ( Zagibalov vious two iterations. However, this does not necessarily correlate to the best classification accuracy.
Similar to Zagibalov and Carroll (2008b), Qiu et al. (2009) also use a lexicon-based iterative process as the first phase to 2008b ), they start with a much larger HowNet Chinese sentiment dictionary 2 as the initial lexicon. Documents classified by stage.

The above self-training approach utilizes self-labeled instances in the training loop. While the current model could be im-proved by iteratively adding the most confident self-labeled examples generated at each iteration, this is not always true beled which makes the model even worse in the next iteration. Much recent work has thus been conducted to explore labeled features in model learning without labeled instances. For example, some approaches use human annotated labeled features to generate pseudo-labeled examples that are subsequently used in standard supervised learning ( Schapire, Rochery, Rahim, features and unlabeled instances using generalized expectation (GE) criteria. Labeled features can come from human anno-tations or through unsupervised feature clustering with latent Dirichlet allocation (LDA). For LDA-generated features, the expressed as GE criteria.

In contrast to the aforementioned methods, our proposed framework does not use human annotations to generate labeled features. Instead, we use the generalized expectation criteria to express preferences on expectations of sentiment labels of those lexicon words from a sentiment lexicon. Moreover, our framework further induces domain-specific features automat-ically from a large corpus of un-annotated data. 3. Proposed framework
We propose a novel framework for sentiment classifier learning from unlabeled documents as shown in Fig. 1 . The pro-classifier is trained by incorporating prior information from the sentiment lexicon which consists of a list of words marked with their respective polarity. For example, the word  X  X  good  X  X  typically conveys a positive sentiment. We refer such prior information as labeled features and use them directly to constrain model X  X  predictions on unlabeled instances using general-uments labeled with high confidence are fed into the self-learned features extractor to acquire domain-dependent features set to obtain the final results.

The remainder of the section will describe the proposed framework in details. 3.1. Classifier training using generalized expectation criteria
Assuming that we have a total number of S sentiment labels denoting by S  X f positi v e ; negati v e g in a typical sentiment word in the document is an item from a vocabulary V index with V distinct terms denoted by {1,2, ... , V }.
Suppose we have a classifier parameterized by K , the sentiment label s of a document w is found by maximizing the fol-lowing equation:
Assume we have some labeled features where words are given with their prior sentiment orientation, we could construct data that should also hold of the model distribution. document label j co-occur in an instance.
 We define the expectation of the features as parameterized at K .
 E constraints used in model learning. The jk th entry denotes the expected number of instances that contain feature k and have label j .
 tion on the set of instances containing feature k , i.e. a target value.
 represent a positive orientation. We would expect that this word more likely appears in positive documents. For each labeled feature k 2 K , a single GE term is
The gradient of Eq. (6) with respect to the model parameter for feature k and label j is:
If a maximum entropy model is used as the classifier, the probability of sentiment label s conditioned on document w is given by a proper probability. Plug Eq. (8) into (7) , we get
The final objective function consists of a GE term for each labeled features k 2 K with a zero-mean r 2 variance Gaussian prior on parameters for regularization.
 In our experiments, we set r = 0.1 and use L-BFGS to estimate model parameters.
 ious ways in doing these in Section 4.1 . 3.2. Self-Learned features extraction
An initial classifier is built using the labeled features obtained from a sentiment lexicon and is subsequently applied on the model prediction is error prone and therefore pseudo-labeled examples with wrong labels might degrade the model per-formance during the iterative training process. Various pseudo-labeled example selection strategies ( Medlock &amp; Briscoe, 2007; Daum X , 2008 ) have been proposed. But these heuristic choices often require careful parameter tuning. In our algo-rithm presented here, we simply choose pseudo-labeled examples based on their posterior probability of class membership and use the class prediction probability threshold s to filter out low confidence pseudo-labeled examples. As will be shown in Section 4.4 , our proposed algorithm is not sensible to the setting of s . Hence, we can simply ignore this parameter and select all the self-labeled examples for word-class distributions estimation.

Given the pseudo-labeled examples generated, we first select the most indicative feature words for each class based on information gain. We set the information gain threshold c as the mean of the information gain scores of the top 200 most predictive features. The expected word-class distribution for a given word w 2 V is defined as a vector ^ f  X  w  X 2 R S where Such a distribution can be estimated from pseudo-labeled examples as defined in Eq. (4) . 4. Experiments
We evaluate our proposed framework on the two datasets, the movie-review (MR)dataset 4 and the multi-domain senti-ment (MDS) dataset. 5 The MR dataset consists of 1000 positive and 1000 negative movie-reviews downloaded from the IMDB movie archive. The MDS dataset contains four different types of product reviews extracted from Amazon.com including Books, DVDs, Electronics and Kitchen appliances, with 1000 positive and 1000 negative reviews for each domain.
Preprocessing was performed on both of the datasets by removing punctuation, numbers, non-alphabet characters and stopwords. Summary statistics of the datasets before and after preprocessing are shown in Table 1 . It can be seen that the MR data appears to be the largest dataset, nearly doubling in its vocabulary size compared to that of Books and DVDs. The Electronics and Kitchen datasets are smaller with their vocabulary size being only half of that of Books and DVDs. The MPQA subjectivity lexicon is used as a sentiment lexicon in our experiments. It contains 2,718 positive and 4,911 negative words. It should be noted that the MPQA subjectivity lexicon is domain-independent and does not bear any domain-specific information about the datasets used here. Algorithm 1. (Self-learned features extraction)
Input : The document collection D  X f w 1 ; w 2 ; ... ; w D g , the sentiment lexicon L , the class prediction probability
Output : Self-learned features with their expected distribution, F  X f X  w 1 ; ^ f  X  w 1  X  X  ;  X  w 2 ; ^ f  X  w 2  X  X  ; ... g 1: Construct an initial classifier upon samples of D  X f w 1 ; w 2 ; ... ; w D g with prior information obtained from L 2: for each document w i 2 D do 3: Infer its sentiment class label as s i = arg max s P ( s j w i ; K ) 4: if P ( s i j w i ; K )&gt; s then 5: Add labeled sample ( w i , s i ) into a labeled document pool B 6: end if 7: end if 8: for each distinct word w t from the labeled document pool B do 9: Calculate the information gain IG ( w t ) based on B 10: if IG ( w t )&gt; c do 11: Calculate the target expectation of w t ; ^ f  X  w t  X  from B 12: Add  X  w t ; ^ f  X  w t  X  X  into the self-learned feature list F 13: end if 14: end for
We randomly split the data with 90% data used as the training set and the remaining 10% data used as the test set. For all the results reported in this section, we performed 10 random splits and report the classification accuracy averaged over 10 such runs. It has to be noted that no labeled instances were used in the classifier training in our proposed framework. 4.1. Overall comparison We compare our proposed approach with several other methods as described below:
Lexicon labeling . We implemented a baseline model which simply assigns a score +1 and 1 to any matched positive and negative word respectively based on a sentiment lexicon. A review document is then classified as either positive or neg-ative according to the aggregated sentiment score. Thus, in this baseline model, a document is classified as positive if there are more positive words than negative words in the document and vice versa.

Heuristic labeling . For a dataset, the matched polarity words from a sentiment lexicon are extracted as features which are assumed to be highly predictive of their corresponding polarity class. It should be noted that features generated in this way is domain-independent and does not bear any domain specificity. For GE-based training, a target or reference expec-2008; Schapire et al., 2002) is adopted that a majority of the probability mass for a feature is distributed uniformly among its associated class(es), and the remaining probability mass is distributed uniformly among the other non-associated class(es). Since we are dealing with the binary classification problem here, the target expectation of a feature having its prior polarity (or associated class label) is 0.9 and 0.1 for its non-associated class.
 from the Na X ve Bayes classifier trained using document vectors with binary features. There are many different strategies in selecting the pseudo-labeled examples to be added into training set. We chose those self-labeled instances whose label prediction probability exceeds 0.8.
 the feature with the class label and the target expectation of each feature is re-estimated from the pseudo-labeled exam-ples. A second classifier is then trained using these self-learned features .

Oracle labeling . Similar to Heuristic labeling , features used here are also extracted from a sentiment lexicon. However, instead of using the simple heuristic to specify the target expectation of the features, we assume the availability of doc-ument labels and compute the exact target expectations from the labeled instances.

Na  X  ve Bayes . For comparison purposes, we also trained supervised classifiers including Na X ve Bayes (NB), support vector machines (SVMs), and maximum entropy (ME) models. We preprocessed documents by stopword removal and stem-ming, and performed 10-fold cross validation. The results show that NB consistently outperforms SVM and ME and rep-resenting document vectors by binary features (word presence and absence) gives better results than using TFIDF features. Thus, we only report results from NB trained on document vectors with binary features.

Table 2 shows sentiment classification accuracies on the MR and MDS datasets using the different methods mentioned above. All the approaches outperform the baseline model which classifies documents based solely on the aggregated senti-ment scores calculated from a sentiment lexicon. Incorporating the prior knowledge from the sentiment lexicon and training
We also notice that Self-labeled instances only improves the classification accuracy marginally compared to Heuristic label-better choice since it does not introduce new examples with incorrect labels. Instead, it calculates word-class association probabilities by averaging over many pseudo-labeled examples which essentially has a smoothing effect and makes it more tolerant to class prediction errors. Thus, contrary to self-training, it avoids the incestuous bias problem.
If the true document labels are revealed, the exact target expectation for each feature can be calculated from the labeled racies on the MR and Electronics datasets compared to the supervised Na X ve Bayes approach. 4.2. Results by filtering polarity words by frequency
We have also conducted experiments by filtering infrequent polarity words. Different word frequency cutoff threshold has been tested, ranging between 0 and 50. It can be observed from Fig. 2 that without any filtering, the total number of matched polarity words in the MR dataset against the MPQA subjectivity lexicon is about 3000. After removing the polarity words that occurred less than five times in MR, the total number of matched polarity words is reduced to 1500 and the clas-quency cutoff point 20 and the total number of matched polarity words is nearly 600. The performance gap between Heuristic labeling and Self-learned features seems to diminish when too few features words are selected.
We also notice that varying the polarity word frequency cutoff does not affect the performance using Oracle labeling much. The performance is the best when no filtering was done and only drops slightly with more polarity words filtered. tures reflect the true distributions over class labels.
 Self-learned features increases with more infrequent polarity words removed and it levels off before drops when too few polarity words are used as the initial labeled features. As mentioned earlier, Books and DVDs are larger corpora and thus the number of matched polarity words without filter is about 2000. While for Electronics and Kitchen, only about 750 polar-the corpus, the number of matched polarity words drops dramatically with only about 500 matched words for Books and in the range of 5.32% and 8.1%. The optimal polarity word frequency cutoff point is 10 for Books and DVDs and five for
Electronics and Kitchen. At this point, the total number of matched polarity words is about 270 for Books and DVDs, and 160 for Electronics and Kitchen.

Using as few as 50 polarity words as features when the polarity word frequency cutoff point is 40 for Books and DVDs and labeling for all the datasets.

Oracle labeling seems more robust to changes of the polarity word frequency cutoff. It performs fairly stable and only drops dramatically when too few polarity words were incorporated as prior knowledge, for example, when there are only 23 polarity words were selected at the cutoff point 40 for the Kitchen dataset. 4.3. Comparison with existing approaches Li et al. (2009) employed lexical prior knowledge extracted from a sentiment lexicon that was developed in the IBM India tion based on non-negative matrix tri-factorization. Such domain-independent prior knowledge was incorporated in con-junction with domain-dependent unlabeled data and a few labeled documents for model learning. With 10% of labeled documents for training, the non-negative matrix tri-factorization approach performed much worse than our approach with a difference of 8 X 11% for Heuristic labeling and 13 X 15% for Self-learned features on both MR and MDS. Even with 40% labeled documents, their approach is still slightly worse than Self-learned features which uses no labeled documents.
Lin, He, and Everson (2010) conducted a comparative study of three closely related Bayesian models for sentiment clas-sification, namely, the latent sentiment model (LSM), the joint sentiment-topic (JST) model, and the Reverse-JST model. They incorporated sentiment prior information extracted from both the MPQA subjectivity lexicon and the appraisal lexicon 6 by modifying conditional probabilities used in Gibbs sampling during model learning. The best sentiment classification results were obtained using LSM with 74.1% on MR and 69.3% on MDS. Our Heuristic labeling slightly outperforms LSM on MDS. Self-learned features gives a similar result on MR, but outperforms LSM on MDS by nearly 6%.

Dasgupta and Ng (2009) proposed a weakly-supervised sentiment classification algorithm where user feedbacks are pro-vided on the spectral clustering process in an interactive manner to ensure that text are clustered along the sentiment dimension. Users are allowed to specify the dimension along which they want the data points to be clustered via inspecting a small number of words. They removed words that occur in only a single review and the top 1.5% words after sorting the vocabulary by document frequency. And we did not perform such preprocessing. Their proposed approach achieved 70.9% classification accuracy on MR and an average of 68.95% on the MDS dataset. Our Heuristic labeling gives slightly better per-formance on both datasets and Self-learned features outperforms their approach by a margin of nearly 4% on MR and 6% on MDS. 4.4. Sensitivity analysis
In this section, we explore the sensitivity to the class prediction probability threshold s used in Algorithm 1 . We measure the accuracy of our proposed approach with s varying between 0.5 and 0.9. When s is set to 0.5, essentially there is no fil-tering and all the pseudo-labeled examples are selected. By increasing s , more pseudo-labeled examples with low confidence fairly robust to changes of s varying between 0.5 and 0.7 with accuracies changing in the range of 0.15% and 0.8%. However, as s is increased beyond 0.8, accuracies drop more noticeably with the biggest drop of 4.5% observed for the Books dataset. This is in contrast to self-training approaches that it is important to only add the most confident pseudo-labeled examples into the training set in order to iteratively improve the model. Our proposed approach does not utilize pseudo-labeled exam-ples directly and instead use them to estimate the word-class distributions which makes it more robust to the pseudo-la-beled example selection strategies. The results also indicate that we can essentially ignore s and simply choose all the pseudo-labeled examples for automatic feature acquisition. 4.5. Domain-specific polarity words
While a generic sentiment lexicon provides useful prior knowledge for sentiment analysis, the contextual polarity of a word may be quite different from its prior polarity. Positive words may appear in sentences describing negative sentiment, ically distinguish between prior and contextual polarity. Our proposed framework starts with a generic sentiment lexicon and estimates word-polarity association probabilities from pseudo-labeled examples. Indeed, as seen from Table 3 , the pro-posed framework is able to extract domain-dependent feature words and estimate the word-class probabilities from a par-ticular domain and thereby reflect a domain-specific sentiment polarity for each word.
 Table 3 lists some extra polarity words extracted by our approach which are not found in the MPQA subjectivity lexicon. We can see that the proposed framework is able to identify domain-specific polarity words. For example, complicated is gen-erally considered as negative in other context, but it expresses positive opinion in describing movie plots. We also observe other domain-specific terms for the MR dataset, such as the actress name winslet (kate Winslet) with positive polarity and the movie name batman bearing negative polarity. For the MDS datasets, example domain-specific terms include foreign for
Books, oscar for DVDs, small and crashes for Electronics, and contemporary and burned for Kitchen. 5. Conclusions and future work
In this paper, we have proposed a novel framework where prior knowledge from a generic sentiment lexicon is used to build a classifier where preferences on expectations of sentiment labels of those lexicon words are expressed using general-ture words whose word-class distributions are estimated and are subsequently used to train another classifier by constraining the model X  X  predictions on unlabeled instances. Experiments on both the movie-review data and the multi-do-main sentiment dataset show that our approach attains comparable or better performance than existing weakly-supervised sentiment classification methods despite using no labeled documents. Moreover, our approach is simple and robust and does not require careful parameter tuning. Although this paper primarily studies sentiment analysis, the proposed approach is applicable to any text classification task where some relevant prior knowledge is available.

A promising direction for future work is to incorporate ontology engineering into weakly-supervised model learning. By incorporating domain-independent knowledge from a sentiment lexicon as well as domain knowledge from ontologies, we are hoping to reveal both topics and sentiment labels of a document simultaneously.
 References
