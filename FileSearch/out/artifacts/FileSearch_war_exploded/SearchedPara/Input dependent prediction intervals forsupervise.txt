 Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia 1. Introduction
Predicting for the future is one of the hardest things imaginable, yet we do it anyway. For that, machine learning algorithms are often utilized, often for search of the best model in an exploratory fashion. We then wonder, does the model represent the data well and do the predicted values conform to the dataset or has the model learned a wrong concept. We want to test arbitrary regression models, so we are limited to black-box modelling and we can not have many assumptions. As we focus on regression in the supervised learning framework, what is available is the set of observations ( x i ,y i ) ,i =1 ...n, where x i are attribute vectors and y i are observed values of the continuous variable that is to be learned and later predicted for new observations. One fundamental assumption is independent sampling, so we assume we are dealing with representative datasets. We also presume the data follows some continuous function and is somehow corrupted with additive noise: For neural networks, the common approach is to estimate the model X  X  error through bootstrap and to use the residuals to estimate the data error (or noise) by maximum likelihood estimation [6,17]. Both error estimates are then combined to form prediction intervals. We compare them with procedures that form prediction intervals directly from the individual samples X  local neighborhoods. As such, these can be applied even in cases where the learning algorithm is not available for bootstrap.

If we assume for the noise to have a Gaussian distribution with zero mean and constant variance throughout the attribute space, then the prediction intervals are straightforward to analytically calculate with elementary statistics. Many papers presume the error is independent of the data and inadvertently dismiss the possibility of heteroscedasticity. Even though heterogeneous variance can have little effect on estimation of the conditional mean, the presence of heteroscedasticity does affect the reliability of predictions where variability is greater.

Figure 1 illustrates the relationship between the prediction interval, the confidence interval and the three relevant values. Confidence intervals are concerned with the accuracy of the model X  X  estimate  X  y accuracy of the model X  X  output with respect to the realized observations y i . Prediction intervals should capture the distribution of individual future points and are concerned with the quantity y i  X   X  y i .Ifwe expand the first term, we see that the corresponding confidence interval should necessarily be enclosed within the prediction interval. In real world applications, prediction intervals are more practical than confidence intervals because the former are concerned with the accuracy with which it is possible to predict the observed value itself, and not just the accuracy of the estimate of the true conditional mean [13].
In the following section we first overview the related work. Then we focus on the two families of non-parametric approaches for construction of prediction intervals for supervised regression. Testing methodology and the experiments are described, followed by results and discussion, where we also address some visualization. We conclude with a summary and possible further work. 2. Related work
Theoretic origins of interval estimates date back to beginnings of modern statistics, more specifically the work of Neyman [9], in which he introduced confidence intervals. In the machine learning commu-nity, Nix and Weigend were among the first to address the problem of estimating the mean and variance of the target distribution, when it is not assumed the noise is uniform across the input space [10]. The method extends neural networks with an additional output neuron that computes the localized estimate of the total variance of the target around the true underlying function. This provides a measure of un-certainty of the usual network output for each input. Tibshirani has performed a comparison of three statistical methods for forming confidence intervals for homoscedastic data: the delta method based on the Hessian, bootstrap estimators, and the sandwich estimator [14]. Tests performed with neural net-works have shown that bootstrap methods provide the most accurate estimates of the standard errors of the predicted values.

There exist many different approaches that are tailored to specific models, even frameworks to en-hance models, i.e. Conformal Predictors [11], but as we want to test arbitrary regression models, we are restrained to black-box modelling. A survey of various point-wise reliability estimators based on perturbing learning data, use of unlabeled data and sensitivity analysis was done in [1]. In recent years, the publications on prediction intervals and error estimation were mostly published in neural network journals, where the uncertainty of prediction errors was decomposed into two independent components.
While confidence intervals deal with the deviation of the models X  predictions and the mean of the target, prediction intervals need to account for the variation of the target variable. Heskes proposed a method for computing prediction intervals in two stages [6]. Confidence intervals are formed for the ensemble of neural networks from bootstrap replicates of the original data set. The variance of the target distribution is estimated with the samples left out of the bootstrap sample. Residuals of these samples are then handed over to a separate neural network with an exponential activation function for the output neuron to ensure positive variance. Zapranis and Livanis [17] have in a sense combined the works of Heskes and Tibshirani, and compared the methods to analytic approaches. Their results indicate clear superiority of the combination of the bootstrap and maximum likelihood approaches to constructing prediction intervals.

A different approach to constructing prediction in tervals was presented in [13]. They partition the input space into fuzzy clusters and form prediction intervals for each cluster based on the empirical distribution of the errors associated with all instances within the cluster. Prediction intervals for new samples are formed according to the grade of their membership in identified clusters, without the calculation of confidence intervals. Predictions from random forests can be interpreted as a weighted mean of the observed response variables [7]. The concept of random forests was generalized to Quantile Regression Forests in [8] and these are naturally capable of forming prediction intervals.

The previous two paragraphs represent an overview of the state of the art methodologies found in the literature for making prediction intervals. In the following section, we will generalize these existing approaches so as to level the playing field for further evaluation. To our knowledge, this paper presents the first comparison and the most extensive evaluation of these approaches to date. 3. Bootstrap and maximum likelihood
The first family of methods is based on the idea of explaining the total prediction error as a sum of the model X  X  error and the error caused by noise inherent to the data [6]. Noise in data and non-uniform distribution of examples represent a challenge for learning algorithms, leading to different prediction accuracies in different parts of the problem space. This component is called the data noise variance and is the accuracy of prediction models: their generalization ability, bias, resistance to noise, avoidance of over-fitting, etc. These factors form the component called the model uncertainty variance,  X  2 m .Thetwo components are assumed to be independent of each other and their sum is the total prediction variance:
The most straightforward approach was formed in [17]. Given a fixed model with available learn-ing algorithm and training set, the reinterpretation of the method goes as follows. To estimate  X  2 m ( x ) , bagging is done with the training data, using the model at hand. Confidence intervals are formed by assuming the normal distribution and calculating  X   X  2 m ( x ) , the variance of the bagged predictions. Then a radial basis function network (RBFN) is trained on the residuals of the bagging predictions on the prediction of the model. This method will be noted in tables as BagMLa .

Generalizing the method from [6], we label it BagMLb . Here,  X   X  2 m ( x ) is again obtained by calculating the variance of the bagged model. Estimation of  X  2 d ( x ) is done by a RBFN trained on the out X  X f X  sample bagged residuals. Assuming a locally normal distribution, the BagMLb prediction interval is  X  y ( x )  X  z of [17] have actually gone further in lowering the variance of their estimates by means of an ensemble of ensembles to form  X  y bag ( x ) . 4. Local neighborhood The second family assumes that samples, which are close in the attribute domain, will behave similarly. These approaches estimate the conditional prediction variance with use of the local neighborhoods for algorithm or the bootstrap procedure would be just too time consuming. To deal with mixed data types, both Clustering and Nearest neighbors approaches use the Gower X  X  distance. 4.1. Clustering
Adopting the idea from [13], we implemented simple k-means clustering on the training data. To keep the method as simple as possible, we did not utilize any partition optimizations and to bypass the diffi-culties arising from determining the optimal number of clusters, the number of clusters is defined with the common heuristic k = n/ 2 ,where n is the size of the training set. LNcl prediction intervals are constructed for each cluster directly from the empiric distribution of residuals, by taking the appropri-ate percentiles, i.e. the 2.5 and 97.5 percentiles for (1  X   X  )= 95% prediction intervals. For an unseen example, the nearest cluster defines the prediction interval. In the following, pseudo-algorithm initLNcl is used to learn the prediction intervals and pseudo-algorithm LNcl returns prediction intervals for new samples: function initLNcl (Model, TrainingSet,  X  ) Residuals end. function LNcl ( x , Model, Clusters) end. 4.2. Nearest neighbors
Instead of using clustering, we can simplify this idea and use the nearest neighbor algorithm to con-struct prediction intervals in the following way. First, signed residuals are obtained from the training set. Bias correction is performed by enforcing zero mean of the residuals. For the distribution of the near-est neighbors residuals, we assume the normal distribution and calculate the standard deviation, which ants with different sizes of neighborhoods. With method LN5 , the size of the neighborhood is 5% of the total population. The second variant LN100 is computationally even simpler, as it covers the whole (100%) population and is therefore equivalent to analytic methods that assume constant variance. The size of the used neighborhood is a balance between how local and how accurate the intervals are. As it is the primary parameter of this method it could be a subject of further investigation. The following pseudo-algorithm summarizes this method: function LN ( x , Model, TrainingSet, P,  X  ) end. 4.3. Quantile regression forest
Natural progression of local neighborhood based approaches would suggest the use of adaptive neigh-borhood procedures. These procedures often have some inherent parameterization, though Lin and Jeon [7] have reinterpreted predictions from random forests as a weighted mean of the observed re-sponse variables. Random forests [3], grow an ensemble of tree predictors such that each tree depends on the values of a random vector sampled independently from the set of attributes and with the same distribution for all trees in the forest. The trees used are the Classification And Regression Trees [4] and as such, random forests are regarded as one of the best non-parametric machine learning algorithms known. The concept of random forests was generalized to Quantile Regression Forests in [8]. In contrast to the original random forests, trees in these ensembles are not pruned and the values of all observations are kept in leafs. This preserves information about the underlying distribution and makes estimation of conditional quantiles possible. The conditional distribution function of Y given X = x can be written as where 1 Y i y is an indicator variable The last expression can be approximated by the weighted mean over the observations of indicator vari-ables and the estimator is The weights w i are averaged weights from the collection of trees and a weight in the individual tree is the inverse of the number of observations in the corresponding leaf. Weights sum to one, so together they give a representation of the distribution of possible values for the response variable. Estimates of conditional quantiles are obtained by finding the infimum value of y ,forwhich a new sample is dropped trough the collection of trees, the weights are obtained. The corresponding y i values are sorted in ascending order and as we are constructing 95% prediction intervals, we need to find our sought prediction interval. As such, this method X  X  prediction intervals are independent of the model X  X  predictions. 5. Experiments
The experiments are done in the R environment for statistical computing [12]. We use seven popular and widely used regression models. Because we want to show the true nature of different approaches to making prediction intervals, we chose to not do any model parameter tuning. Even the obligatory parameterization of some models was kept to a bare minimum, using basic heuristics. The idea is not to try and achieve the best possible accuracy, but to investigate what prediction intervals can tell about arbitrary (including mediocre) models. The used models are:  X  LR  X  Linear Regression,  X  RT  X  pruned regression tree ( rpart library; minimum number of observations for splits to be at- X  kNN  X  11-nearest neighbors (our implementation with gower distance; the number of neighbors is  X  SVM  X  regression Support Vector Machine ( e1071 library),  X  BAG  X  Bagging of 50 regression trees ( ipred library; number of trees used is the same as in  X  NN  X  Multilayer perceptron with 5 hidden neurons (from nnet library: 5 neurons is generally not too  X  RBFN  X  Radial Basis Function Network (our implementation uses n basis and Gaussian distribu-
For the test datasets, we created a compilation of 36 artificial and real-world data sets, which is avail-able from the corresponding author on request. Seven artificial datasets comprise of one-dimensional sets of 3000 instances of the normal Gaussian noise; linear function with added 10% noise from uniform and standard normal distributions; three-region piece-wise constant function, which regions have either 1%, 2% and 5% or 10%, 20% and 50% added standard normal noise; a quadratic function with added standard normal noise multiplied by the square of x ; sinusoidal function with mixed Gaussian noise. Five multi-dimensional artificial datasets from [15] have five features, 1000 instances and comprise of linear, polynomial and trigonometric functions and we included the artificial data set used in [17] for comparison. The 24 real-world data sets are compiled from datasets in Weka X  X  [5] numeric, regression and UCI collections, and contain a minimum of 100 samples.

Artificial datasets vary in complexity and noise levels. With knowing how individual models work, it is possible to have some ideas which models will perform well on which datasets, i.e. we know that linear regression is best suited for the simple linear function and inappropriate for the piece-wise constant function, whereas with regression trees it is the complete opposite. So the artificial datasets are used mainly for validation, whereas the real world datasets are much more complex and are used to evaluate their practical use.
 When evaluating the prediction intervals, we are interested in two quantities. The first quantity is Prediction Interval Coverage Probability (PICP) and is defined as the percentage of test instances for which the target value is contained within the prediction intervals. By definition, the prediction intervals should on average enclose the realized observations in (1  X   X  ) % of all cases (traditionally 90%, 95% or 99%, we focus on 95%), hence PICP is a quantization of validity .
 The second quantity of interest is the average interval length or the Mean Prediction Interval (MPI). As we want to compare this measure of optimality from different approaches, we introduce the measure Relative Mean Prediction Interval (RMPI), for which we need a baseline or default for comparison. For data set by taking the appropriate quantiles (i.e. 2.5 and 97.5 percentiles for 95% prediction intervals). MPI def is the length of the prediction interval constructed in this way. RMPI is then calculated by taking the method X  X  MPI and normalizing it with MPI def . For comparison, we also compute naive prediction intervals. These are constructed in the same way as the default intervals, using the distribution of all target values in the training set. Figure 2 shows the difference between constant and data-dependent prediction intervals. How the naive prediction intervals look on the artificial data set generated from a linear function with Gaussian noise of constant variance is shown in Fig. 2(a), whereas Fig. 2(b) shows data-dependent prediction intervals constructed with the LN100 method.

Testing is performed in the following way. Every data set is first randomly split into two equally sized sets, the training set and the test set. We did not use bootstrap because it is already used by BagML approaches and our set-up should give more credible results when enough data is present. For training only for evaluation. On this set the PICP and RMPI scores are calculated for all methods. The run-times are also measured so we can compare the methods X  efficiency. For stability, this procedure is repeated 50 times, and then the median results are used. 6. Results and discussion
Evaluation on artificial data sets of the six different methods for constructing prediction intervals is summed up in Table 1. The first three columns describe the distribution of the obtained PICP values, namely their 2.5 percentile, mean and 97.5 percentile. As we are constructing 95% prediction intervals, the target PICP is 0.95. Method that would be on average most correct would have PICP 50 0.95 with PICP 2 . 5 and PICP 97 . 5 as close to 0.95 as possible.

On the real-world domains, as presented in Table 2, local methods LNcl , LN5 and LN100 underesti-mate the mean PICP. With real-world datasets, these methods can fail and this manifests in their low PICP 2 . 5 values. On the other hand, BagML methods strive to produce valid prediction intervals no mat-ter how bad the model is. Figure 3 demonstrates this behavior with non-optimal models. These methods clearly produce symmetric intervals but what is interesting is that the BagMLb method better adapts to the residuals. In Fig. 4 we see how intervals produced by LNcl and LN5 look like. We can notice that the local methods seem almost model independent and are able to capture the targets better than inappro-priate models. Prediction intervals produced with QRF are model independent and this is demonstrated in Fig. 5(a). Figure 5(b) demonstrates how the simple method LN100 can produce valid prediction in-tervals, thus the produced prediction intervals are clearly not confirmatory with the underlying data and the corresponding RMPI is greater than 1.

As is evident from both tables, local neighborhood methods on average produce narrower intervals (lower mean RMPI) and are orders of magnitude quicker than the BagML methods. Results from quantile regression forest are quite similar to those of BagMLb , but the method X  X  time complexity is very similar to LN5 .

In Fig. 6 the densities of the PICP di stributions from all the evaluated methods are plotted against each other. Distribution of the set of all calculated PICP values is shown with the bold line. We see that on average, the methods give various low results but the peek of the distribution of all values is at 0.951, which is remarkably close to the sought 0.95 coverage probability (the vertical line). Bootstrap and Max-imum Likelihood methods are shown with dark gray lines, Local Neighborhood methods with light gray and Quantile Regression Forests with solid gray. It i s clearly visible on the plot that Local Neighborhood based estimates on average fall short of the target PICP, and on the other side, Bootstrap and Maximum Likelihood methods and Quantile Regression often overestimate the intervals. The method with the clos-est peek at 0.955 is achieved with the method BagMLa , whose distribution also has a significantly larger spread around the target PICP than other methods. Second closest are PICPs from the method LN5 with a peek at 0.942. Quantile Regression Forests follow with their PICP peek at 0.967. Close are BagMLb at 0.968 and the analytic method, LN100 with its peek at 0.931. The furthest away from the target PICP is LNcl at 0.910.
 When a method X  X  prediction intervals enclose the whole test target space, it gets a PICP value 1.0. The percentage of PICP values of exactly 1.0 is displayed in Table 6, where we see that clustering never made this sort of mistake. On the other hand, BagMLb encloses all values in 5.2% of cases and BagMLa in 3.5%. When ignoring these cases in the distribution of PICP values, the closest to the target PICP is still BagMLa with PICP value of 0.947, but closely followed by LN5 with a peak at 0.941. Not that far off are QRF at 0.965, LN100 with a peak at 0.933 and BagMLb at 0.966. PICP of LNcl does not change (0.910) and is still furthest away from the target PICP.

Figure 7 shows how the lowest 10% achieved PICP values and the 10% closest to the target PICP value are distributed among the used methods. In this regard, using LNcl looks like the most risky option and QRF the safest. Interestingly, prediction intervals from LN100 are more likely to be better than from all other methods. We have seen in Fig. 5(b) that these intervals are usually valid but rarely confirmatory.
It is equally important to consider and compare the achieved RMPI values. If two methods achieve the same PICP, the one with a lower RMPI value would have more narrow intervals and should be regarded as the better option. Figure 8 presents which methods scored the smallest (best) and the largest (worst) RMPI values. Small RMPI values are mostly achieved by LN methods due to their nature of producing very narrow intervals. The lowest PICP value 0 was achieved by LNcl , LN5 ,and LN100 on the dataset servo and in conjunction with the neural network model. The corresponding RMPI values are extremely small, between 0.0015 and 0.002. Largest RMPI values were produced by BagML methods. On the dataset triazines , using a neural network, the largest RMPI value is 5.3 from the method BagMLa , though the corresponding PICP value is 1.0. This means the method enclosed all the test examples on the account of very broad prediction intervals. According to the figure, QRF produced the smallest amount of extreme RMPI values and is in this respect, on average, the safest method to use.

A really useful tool for analyzing models is the residuals versus fitted values plot. In this plot, the predicted values are on the abscissa and the residuals on the ordinate. The residuals should be randomly distributed and the better the model fits the data, the closer to 0 are the residuals. It is often very easy to identify inadequate models with these plots as in many cases the models predictions do not cover the target space correctly. We use the already familiar dataset, the sinusoid function with mixed noise, for illustrative purposes in Fig. 9. The figure shows the residuals versus predicted values plots for all models together with the corresponding root mean squared errors (RMSE) and relative root mean squared errors (rRMSE). The horizontal lines represent the distribution of the residuals and show how the residuals are spread around zero. It is easy to see that bagged regression trees, ordinary regression tree, support vector machine and linear regression do not fit well to this dataset. Other models fit the data rather well and it is not really clear which model one would use. Prediction intervals can visually help in respect that the busyness of prediction intervals implies a less reliable or more error prone model. Two of the less busy plots are for the neural network and the random forest models and though visually quite similar, the peaks are from different prediction intervals.

To visually evaluate prediction intervals, we use a plotting technique from [8]. Test cases are ordered according to the length of the corresponding prediction intervals. For centering, the means of the upper and lower ends of the prediction interval are subtracted from all target values and prediction intervals. Prediction intervals and the corresponding targets for the sinusoid function with mixed noise are pre-sented in Fig. 10 for the neural network (left column) and the random forest (right column). In this example, all the prediction intervals look and work better with the neural network. LNcl produces the most confirmatory prediction intervals for this dataset. Their achieved PICP is closest to the target PICP value and the RMPI is considerably smaller than 1. Validity, optimality and variability in lengths of the intervals suggest that cases with short prediction intervals can thus be predicted more accurately than cases with long intervals. 7. Conclusions
We reinterpreted existing approaches that use bootstrap and maximum likelihood estimation for use with arbitrary regression models and presented simpler methods that use local neighborhoods to form prediction intervals. A method that uses clustering was adopted and we tried an even simpler method where we used signed residuals of the nearest neighbors. We also tested Quantile Regression Forest, as it can be interpreted as a form of adaptive neighbors.
The comparison of the two families of methods shows that methods based on bootstrap and maximum likelihood estimation produce prediction intervals that often enclose the models X  predictions (due to their nesting of confidence intervals) and therefore most often produce valid prediction intervals. But when a model does not fit the data well, the RMPI of these methods is considerably larger than that of the local neighborhood based methods, whose intervals may not include the models X  predictions. Nearest neighbor methods have proven to be much more time efficient with promising results on the artificial datasets, but the results on real-world datasets have shown that these methods can suffer from low pre-diction interval coverage probability. Because this method estimates local variance from rather small subsets (i.e. 5 from a dataset with 200 instances), better coverage may be achieved on smaller datasets with use of the t -distribution instead of the Gaussian. Quantile regression forest is a good alternative to the bootstrap and maximum likelihood methods, especially when considering the advantage in time complexity. Prediction intervals from quantile regression forests were closer to the targeted PICP than prediction intervals from BagMLa . This could be attributed to their ability to adapt and to learn a con-firmatory representation of the underlying conditional distribution. Finally, we demonstrated that when prediction intervals are confirmatory, they are informative as well as useful. If prediction intervals were to be used, the methods should be compared with help of the informative graphic representations, as the test statistics are unable to show the complete picture.

Just like with regular variance estimation, more data is generally better. Because small sample statistics are inherently hard and standard techniques unreliable, we chose to not use datasets smaller than 100 samples (though this still counts as a small number of samples). As the number of samples increases, prediction intervals converge to the unreliability inherent to the data. If the dimensionality of the data is great, standard problems from the curse of dimensionality arise  X  if the prediction intervals seem to be too wide, considerable amounts of additional data would be needed. On the other hand if dealing with imbalanced data, prediction intervals would show which parts of the problem domain are to what extent more (or less) reliable. A proper comparison for example with bi-clustering would make perfect sense in future work.

It would be useful if there existed a connection between the size of the error and the width of the prediction intervals or if some combination of these methods could be utilized for automated model selection or refinement. The size of the error may be further explained by individual attributes, as was demonstrated in a classification setting [16]. The prediction intervals represent bounds of the target val-ues so this could be utilized for outlier detection. Their length represents the reliability of an individual prediction so we intend to compare the presented approaches with other reliability estimators [2]. What could be studied further are other approaches to quantile regression and various graphic representations of prediction intervals for multi-dimensional data would also be of great benefit to the end users.
Further research could be focused on time series data and data-streams where new observations arrive continuously at a rapid pace. Presented approaches assume that data comes from a static distribution and could be used as concept drift detectors. Other example where the presented ideas could be used is cluster monitoring operation where data is continuously evaluated to obtain an overall cluster quality for identifying the currently best clusters.
 Acknowledgements
The authors are extremely grateful to Erik  X trumbelj for many supportive discussions and to the re-viewers for their valuable comments. This work was supported by a grant from the Slovenian Research Agency (P2-0209).
 References
