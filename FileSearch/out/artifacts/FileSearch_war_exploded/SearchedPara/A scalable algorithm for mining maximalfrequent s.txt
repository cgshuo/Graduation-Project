 REGULAR PAPER Congnan Luo  X  Soon M. Chung Abstract In this paper, we propose an efficient scalable algorithm for mining M aximal S equential P atterns using S ampling (MSPS). The MSPS algorithm re-duces much more search space than other algorithms because both the subse-quence infrequency-based pruning and the supersequence frequency-based prun-ing are applied. In MSPS, a sampling technique is used to identify long frequent sequences earlier, instead of enumerating all their subsequences. We propose how to adjust the user-specified minimum support level for mining a sample of the database to achieve better overall performance. This method makes sampling more efficient when the minimum support is small. A signature-based method and a hash-based method are developed for the subsequence infrequency-based pruning when the seed set of frequent sequences for the candidate generation is too big to be loaded into memory. A prefix tree structure is developed to count the candidate sequences of different sizes during the database scanning, and it also facilitates the customer sequence trimming. Our experiments showed MSPS has very good performance and better scalability than other algorithms.
 Keywords Data mining  X  Maximal frequent sequences  X  Sampling  X  Signatures  X  Prefix tree  X  Performance analysis 1 Introduction Mining sequential patterns from large databases is an important problem in data mining. With numerous practical applications, such as consumer market-basket data analysis and Web-log analysis, it has become an active research topic. Since it was introduced in Agrawal and Srikant [ 3], many algorithms have been proposed, but most of them are to discover the full set of frequent sequences.
 [12 ], only subsequence infrequency-based pruning is used to reduce the number of candidate sequences. So, if a sequence with length l is frequent, all of its 2 l sub-sequences must be enumerated first. Thus, if some frequent sequences are long, the overhead of enumerating all of their subsequences is so much that mining the full set of frequent sequences is impractical. An alternative approach is mining only the maximal frequent sequences. A frequent sequence is maximal if none of its supersequences is frequent. Mining only the maximal frequent sequences is ef-ficient because the search space can be reduced a lot by using the supersequence frequency-based pruning. Another approach is to mine only the closed frequent sequences [ 19, 20]. A frequent sequence is closed if none of its supersequences has the same support. Obviously, the relationship between the set of all frequent sequences (FS), the set of closed frequent sequences (CFS), and the set of maximal frequent sequences (MFS) is MFS  X  CFS  X  FS. An advantage of mining CFS is that the count information for all frequent sequences is also obtained. However, in many cases, CFS could be still too large and orders of magnitude larger than MFS. Thus, mining MFS can be most efficient and scalable. In interactive data min-ing, after mining MFS quickly, we can selectively count the interesting patterns subsumed by MFS by scanning the database just once. Moreover, managing and querying a small set of maximal patterns is easy, time-saving, and space-saving. frequent sequences at a reasonable cost. If the look-ahead is not cost-effective, its cost can offset the gain from the supersequence frequency-based pruning, like the cases of AprioriSome and DynamicSome algorithms [ 3].
 cannot efficiently mine the maximal frequent sequences because of the unique characteristics of the sequence mining. For example, an item can appear multiple times in a sequence at different positions. Thus, the search space becomes much larger. We consider the look-ahead technique used in Max-Miner [ 5], DepthPro-ject [ 1], and MAFIA [ 6] algorithms to find potential maximal frequent itemsets. They use a Lexicographic tree of itemsets to represent the search space, where each node is associated with a frequent itemset, called head , and a set of extension items, called tail . In this case, the union, head  X  tail , is the only one candidate maximal itemset to be checked for the node. But in the case of maximal frequent sequence mining, the number of candidate maximal sequences to be checked for the node is unlimited because each item in the tail can be included many times in a candidate. For example, if the head contains only one item A and the tail contains items B and C for a node, then possible candidate maximal sequences could be A  X  B  X  C , A  X  B  X  C  X  C , A  X  B  X  C  X  C  X  C , and so on. Thus, the look-ahead method of those maximal frequent itemset mining algorithms cannot work well for the mining of maximal frequent sequences.
 traversal of the Lexicographic sequence tree to mine long patterns. But they cannot use the subsequence infrequency-based pruning effectively because the informa-tion about infrequent short patterns is not enough. This is not so serious in the case of maximal frequent itemset mining, like DepthProject [ 1], because the search space is not usually very large. But, it could be a problem in sequence mining. of cost-effective look-ahead in mining maximal frequent sequences. With sam-pling, we can combine the Apriori candidate generation method [ 2, 3, 17 ]andthe supersequence frequency-based pruning. This enables us to avoid counting most nonmaximal patterns against the whole database, while finding the maximal pat-terns quickly.
 first. But after the pass 2 over the database, we mine a small random sample database first, starting with the candidate 3-sequences (i.e., sequences of three items) generated from the set of frequent 2-sequences. The local maximal frequent sequences that are found from the sample database starting with the global candi-date 3-sequences, are verified in a top-down fashion against the original database, so that we can efficiently collect the longest frequent sequences covered by them. Then, the bottom-up search is resumed from the pass 3, and the supersequence frequency-based pruning is applied at each pass by using the long patterns discov-ered in the previous step. Thus, the MFS mined by MSPS provides a border under which all frequent sequences exist.
 (1) A new MSPS algorithm is developed for mining maximal frequent sequences. (2) How to trade-off the cost and the quality of sampling is studied thoroughly. (3) Optimization components are developed to reduce the computation complex-cepts of sequence mining. Section 3 reviews some related works. Section 4 de-scribes the MSPS algorithm, and Sect. 5 theoretically analyzes the sampling in MSPS. The experimental results and performance analyses are presented in Sect. 6. Section 7 contains some conclusions and future work. 2 Sequence mining Let I ={ i 1 , i 2 ,..., i n } beasetofitems.An k-itemset i is a set of k items denoted an ordered list of itemsets denoted by s 1 , s 2 ,..., s k , where each s i ,1  X  i  X  k , is an itemset . A sequence s a = a 1 , a 2 ,..., a p is contained in another sequence s subsequence of s b ,and s b is a supersequence of s a . An item may appear at most sequence. If there are k items in a sequence, the length of the sequence is k ,and wecallita k -sequence. For example, a 3-sequence { A } , { B , C } is a subsequence of a 5-sequence { C } , { A , D } , { B , C } . For simplicity, these two sequences can be represented as A  X  BC and C  X  AD  X  BC .
 customer-id, transaction-time, and an itemset that includes all the items purchased by the customer in that single transaction. All the transactions of a customer can be viewed as a customer sequence, where these transactions are ordered by their transaction times. We denote a customer sequence t as T 1 , T 2 ,..., T m ,which means the customer has m transactions in the database and each transaction T i , 1  X  i  X  m , contains all the items purchased in that transaction. A customer supports a sequence if the sequence is contained by the customer sequence. The support for a sequence in database D is defined as the fraction of total customers who support the sequence. Given a user-specified minimum support, denoted by minsup , a sequence is frequent if its support is greater than or equal to minsup. The problem of sequence mining is to find all the frequent sequences in the database with respect to a user-specified minsup. If a sequence is frequent and none of its supersequences is frequent, then it is a maximal frequent sequence.
 the sequence mining: (1) Any supersequence of an infrequent sequence is not fre-quent, so it can be pruned from the set of candidates. This is called subsequence infrequency-based pruning. (2) Any subsequence of a frequent sequence is also frequent, so it can be pruned from the set of candidates. This is called superse-quence frequency-based pruning. 3 Related work Mining sequential patterns was introduced in Agrawal and Srikant [ 3] with Aprio-riAll, AprioriSome, and DynamicSome algorithms. Although AprioriSome and DynamicSome try to generate and count long candidate sequences before enu-merating all their subsequences, their performance is usually worse than that of AprioriAll. The reason is that too many false candidates are generated without be-ing pruned by the subsequence infrequency-based pruning. The performance gain from the supersequence frequency-based pruning is not enough to offset the cost of counting so many false candidates.
 ple passes on the database. At pass k , the set of candidate k -sequences are counted on the database, and frequent k -sequences are determined. Then, the candidate ( k + 1 ) -sequences are generated by joining frequent k -sequences for the next pass. This process will continue until no candidate is generated. Even though GSP is much faster than AprioriAll, it has a very high overhead of enumerating every sin-gle frequent subsequence when there are some long patterns. This is also the main weakness of other Apriori-like algorithms, such as PSP [ 12]. For PSP, a prefix tree was developed as the internal data structure to organize and count candidates more efficiently. The differences between the PSP X  X  prefix tree and the one developed for our MSPS will be discussed later.
 of (customer-id, transaction-time) pairs is associated with each sequence, and the candidates are counted by intersecting the id-lists. A lattice-theoretic approach is used to decompose the search space into small pieces so that all working id-lists can be loaded into memory. PrefixSpan [ 15] projects a large sequence database re-cursively into a set of small postfix subsequence databases based on the currently mined frequent prefix subsequences. Then, the subsequent mining is confined to each small projected database. A memory-based pseudoprojection technique is de-veloped to save the computation cost of projection and the memory space for pro-jected databases. SPAM [ 4] uses a vertical bitmap representation of the database for candidate generation and counting. A bitmap is created for each item in the database, where each bit corresponds to a transaction. If transaction j contains item i ,thenbit j in the bitmap for item i is set to 1; otherwise, it is set to 0. SPAM also uses a depth-first traversal of the Lexicographic sequence tree and an Apriori-based pruning of candidates.
 to prune the search space. The basic idea is that, when mining k -sequences, it sorts the customer sequences in the database based on their k-minimum subsequences , which are k -sequences smaller than any other k -sequences in terms of alphabetical order and the transaction id of their items. As the customer sequences containing the same k -minimum subsequences are listed consecutively after sorting, we can easily check if the first k -minimum subsequence is frequent or not. Then, from the customer sequences containing this first k -minimum subsequence, we can identify a group of conditional k -minimum subsequences and update the sorted database based on them. This procedure is repeated until all the frequent k -sequences are found. In this way, many infrequent candidate k -sequences which are not appear-ing in the database can be skipped without being counted. Furthermore, DISC-all also combines the subsequence infrequency-based pruning, database projection, and customer sequence trimming to improve its performance.
 GSP. However, their performance may not be scalable in certain cases. For SPADE, if the database is in the horizontal format, where the transactions form the tuples in the database, transforming it to the vertical format requires extra disk space that is almost the same as its size. This may be a problem in practice if the database is large. Even if the database is in the vertical format, to efficiently count 2-sequences, SPADE proposes transforming it back to the horizontal format on the fly. This usually requires much time and memory for very large databases and results in a performance degradation, which is shown in our tests. SPAM is more efficient in mining long patterns than other algorithms. However, it con-sumes more memory space than SPADE and PrefixSpan. It is claimed to be a memory-based algorithm. According to our tests, its scalability is much more sen-sitive to the number of items and the database size than other algorithms. The comparison between MSPS, GSP, SPADE, and SPAM is presented in detail in the performance analysis section.
 fixSpan may be challenged when the database has a large number of customer sequences and items. A large number of items often produce many combinations at the early stage of mining with a small minimum support level, and it requires PrefixSpan to construct more projected databases. If the database is very large, the cost of projection will be high and much more memory is necessary. In addition, PrefixSpan performs a depth-first search. Thus, the largest number of projected databases to be resident in memory at the same time is same as the length of the longest frequent sequence. When the minimum support is small, even with the pseudoprojection of the database, the memory requirement can be easily much more than the available memory space, because the size of the projected databases based on the frequent 1-sequences alone can be almost the same as the original database size without including the amount of memory required for the pseudo-projection at the lower levels during the depth-first search.
 on their k -minimum subsequences. When the database is large, for example, with millions of customer sequences, the sorting could be very time-consuming, be-cause the comparison between customer sequences with certain length is not triv-ial and we may not have enough memory to perform a quick memory-based sorting. Moreover, DISC-all needs to update the sorted database many times to mine all the frequent sequences with the same length. For a large database with many items, say 10,000 items, the number of distinct k -sequences appearing in the database is usually very big, and thus results in a very high cost for the re-sorting. The test results reported in Chiu et al. [ 8] could not show the scalability of DISC-all even though the biggest database used is just about 50 Mbytes and contains only 1000 items, which is about 10% of the biggest database used in our research (500 Mbytes and 10,000 items).
 due to its complexity. CloSpan [ 20] performs a depth-first search on the Lex-icographic sequence tree. At each node, CloSpan applies a pruning technique called Early Termination by Equivalence to see if the subtree rooted at the current node can be absorbed by any other potential closed frequent sequence already found. Finally, a candidate set of closed frequent sequences is determined, and a postpruning step is needed to exclude nonclosed frequent sequences. As dis-cussed in Wang and Han [ 19], CloSpan follows the candidate maintenance-and-test paradigm, which requires much memory to maintain all the historical candi-date closed frequent sequences to do the closure checking for the newly found closed sequence. Thus, the algorithms with this paradigm have rather poor scala-bility when the minimum support is low or patterns are long. The BIDE algorithm [19 ] can solve this problem by adopting a novel sequence closure checking scheme named Bidirectional Extension. With this, BIDE does not need to store any histor-ical closed patterns. BIDE also performs a depth-first search on the Lexicographic sequence tree. At each node, the forward directional extension is used to grow the frequent prefix sequences and check the closure of the prefix sequences, while the backward directional extension is used for both closure checking and search space pruning. However, unlike other algorithms, the current version of BIDE can mine only the closed frequent sequences of single items, instead of the sequences of itemsets. It is noticed that, like PrefixSpan, both CloSpan and BIDE perform the pseudoprojection of the database along the path of the depth-first search. This poses the same scalability problem as in PrefixSpan when the minimum support is low because we must have enough memory to hold almost the whole database and associated pseudoprojection data structures.
 two aspects: (1) how to choose the sample size, and (2) how to avoid missing patterns in the sample. In Chen et al. [ 7], the FAST algorithm progressively refines the initial sample to obtain a small final sample and reports the set of frequent itemsets in the final sample as the result. In Toivonen [18] and Zaki et al. [22], the Chernoff boundary was used to choose the sample size and to lower the user-specified minsup to mine the sample.
 sequences in a noisy environment. However, the proposed method defines a prob-abilistic match only between the sequences of items, not between the sequences of itemsets. For example, the match between A  X  B and AB is not defined. Thus, it cannot mine the frequent sequences of itemsets. The Chernoff boundary was also used in Yang et al. [ 21] to estimate if a sequence is frequent or not. The sequences whose match is very close to the user-specified minimum match are considered as ambiguous patterns, and they have to be verified against the whole database. They theoretically proved that if a sequence is frequent, then its probability of being missed in the sample is small. However, to reduce the number of misses in the sample, they also suggested to lower the minimum match for the sample as in Toivonen [18] and Zaki et al. [22].
 sample size based on the estimated support of the candidate itemsets, which is expected to be tighter than the sample size based on the Chernoff boundary. The basic idea is that a relatively small sample can be used if the support of a candidate itemset is far from the minimum support. The transactions are randomly selected from the database and added into the sample one by one. Each candidate itemset is evaluated based on the current sample to see if it needs to be evaluated with more transactions at the next step. However, they tested this method with only one candidate itemset. Thus, as suggested in Domingo et al. [ 10, 11], more work needs to be done to combine this adaptive sampling method with some mining algorithms, like Apriori, and more experiments are required to test the cost of the online sampling.
 model the support of a sequence in the sample, instead of using the Chernoff boundary. Moreover, instead of trying to reduce the misses in the sample ei-ther by increasing the sample size too much or by lowering the user-specified minsup support to mine the sample, we proposed to increase the small user-specified minsup a little bit to mine the sample, so that we can achieve the best overall performance. 4 MSPS algorithm Like GSP, MSPS also uses the candidate generation then counting approach to per-form the mining, but the performance is improved very much by combining the supersequence frequency-based pruning into its bottom-up, breadth-first search. It has the following original components: (1) A signature-based approach and a hash-based approach are developed to perform a partial subsequence infrequency-based pruning when the set of frequent k -sequences is too big to be loaded into memory totally for the generation of candidate ( k + 1 ) -sequences. (2) To effi-ciently count candidates of different sizes, a prefix tree structure is developed, and it also facilitates the customer sequence trimming. (3) To support supersequence frequency-based pruning, sampling is used to find long frequent patterns early. (4) To make the sampling more efficient and robust in sequence mining, a theoret-ical method of adjusting the small user-specified minsup for mining the sample database is proposed.
 candidate generation and pruning, candidate counting, and sampling in detail. The following notations will be used in our description: DB is the original database and db is a small random sample of DB . If a sequence is frequent in DB ,itis called a global frequent sequence. L DB k is the set of all global frequent k -sequences is the set of all global maximal frequent sequences in the whole database. If a sequence is frequent in the sample db , we call it a local frequent sequence. L db k , C k ,and MFS sequences generated from L db k  X  1 , and the set of local maximal frequent sequences in the sample, respectively. LongFS DB is the set of verified long global frequent sequences found from the sample result. 4.1 Description of MSPS The basic idea of MSPS is simple: if some long frequent patterns are found early, they can be used to prune the search space so that the mining can speed up. To find long frequent patterns, a small sample db is mined first. We must balance the gain from the supersequence frequency-based pruning and the cost for mining the sample and then verifying the sample result. MSPS consists of three phases: Phase 1: L DB 1 and L DB 2 are determined. Candidate 3-sequences are generated Phase 2: A random sample is drawn from DB , then how much the user-specified Phase 3: The bottom-up search suspended at the end of Phase 1 is resumed from quickly, the bottom-up mining of MSPS still generates all the frequent sequences. Some of them are actually counted against the whole database, and others are not counted as they are pruned by using the long frequent sequences found from the sample. Thus, we will not miss any real maximal frequent sequences in our final mining result.
 A random sample db is 50% of DB . The user-specified minsup is 50%, and it is also used to mine the sample. In Phase 2, we mine db starting with C DB 3 and finally obtain MFS db . Then, a top-down search is performed to verify the patterns in MFS db . In this example, all the three patterns in MFS db are globally frequent, so they are put into LongFS DB . In Phase 3, at each pass, we use LongFS DB to prune the candidates. In each C DB k , the candidate sequences in grey color are the ones pruned, and only the ones in black color are actually counted against DB . Thus, with sampling and supersequence frequency-based pruning, MSPS largely reduces the number of candidates to be counted against DB . To mine this database, GSP needs to count 26 candidates against DB from pass 3, but MSPS counts only three candidate sequences. 4.2 Candidate generation and pruning In both Phases 2 and 3, we have performed the bottom-up, breadth-first search on the sample and the original database, respectively. At pass k , the candidates are generated in two steps: Join Step: we generate local (global) candidate ( k + 1 ) -sequences by joining L db k Prune Step: In both Phases 2 and 3, the subsequence infrequency-based pruning specified minsup is very small, L DB k could be too large to be loaded into mem-ory totally. For this case, GSP proposed to use a relational merge-join technique to generate candidates. But in this manner, the subsequence infrequency-based pruning cannot be applied because the whole L DB k is not available in memory and retrieving the relevant portions of L DB k from a disk requires too many swaps. With-out the subsequence infrequency-based pruning, usually the performance of GSP degrades a lot.
 pruning and signature-based pruning. Figure 2 shows an example to illustrate the two pruning methods, and it also demonstrates that the signature-based pruning is more effective when the same amount of memory is used. For each frequent sequence, we can calculate its hash value and signature by using a hash function and a signature function, respectively. In the hash-based pruning, we use a bit vector. If there is at least one frequent sequence hashed to a bit address in the bit vector, the corresponding bit is  X 1 X ; otherwise, the bit is  X 0. X  In the signature-based pruning, a signature vector is used to contain all the unique signatures of the frequent sequences. These signatures are sorted in ascending order in the signature vector. Compared with the case of loading all the frequent sequences into memory, the bit vector and the signature vector take much less memory, especially when the frequent sequences are long. Both vectors can be loaded into memory totally. comparison, we used the same memory space for the bit vector and the signature vector. For the signature vector, we need 7 integers of 4 bytes for all the unique signatures. So, the bit vector can have 7  X  4  X  8 = 224 bits to use the same amount of memory. The typical hash function used in this example was also used in Park et al. [14] and Shintani and Kitsuregawa [16], and I 1 and I 2 denote the lexicographic order of the items. In the hash function, we use the modulo operation to map the value of I 1  X  10 + I 2 into the range between 0 and 223, because the bit vector has only 224 bits. For example, the hash value of AW is ( 1  X  10 + 23 ) mod 224 = 33, so that the bit vector has  X 1 X  at its bit address 33.
 sents the number of itemsets in the frequent sequence. For example, the signature of W  X  G is (( 2  X  1 ) mod 2 6 ) 2 26 + ( 23  X  10 + 7 ) mod 2 26 = 67109101. It is the fifth element in the signature vector. We will explain the design idea of this signature function later. are shown in Fig. 2. In the case that L 2 can be totally loaded into memory, we are able to perform the subsequence infrequency-based pruning used in Apriori, and all candidate 3-sequences can be pruned. With the hash-based pruning, we cannot prune the first three candidates due to the collision in hashing. For example, the candidate ABC is pruned by the Apriori pruning because its subsequence AC is not in L 2 . But in the hash-based pruning, ABC is not pruned because the hash value of AC is 13 and the corresponding bit in the bit vector is  X 1, X  which was actually set by a frequent 2-sequence W  X  G . However, in the signature-based pruning, the signature of AC , which is 13, is not in the signature vector. Thus, we can detect that AC is infrequent and then remove ABC .
 encode the items and their order in the sequence, and (2) the highest 6 bits encode the number of itemsets in the sequence. The first part of the signature is determined by ( I 1  X  10 + I 2 ) mod 2 26 in the signature function, and it is similar to the hash function used in our example. But, the difference is that the value of I 1  X  10 + I 2 is mapped into a much bigger range [0, 2 26  X  1], which largely reduces the chance of collision between signatures. The second part of the signature is determined by ((
NumItemsets  X  1 ) mod 2 6 ) 2 26 in the signature function, and it can distinguish the sequences like BC and B  X  C . Thus, the information of a sequence is better represented by a signature than a hash value, and the signature-based pruning is usually more effective than the hash-based pruning when the same amount of memory is used.
 of a sequence has nothing to do with the signature vector size. In our example, as we expect the length of the longest frequent sequence would be less than 64, the second part of the signature takes only 6 bits, and the remaining 26 bits are used for the first part.
 quence can be expressed as follows: hash value = signature = ((( NumItemsets  X  1 ) mod 2 p ) 2 q + r ) in the sequence; NumItemsets is the number of itemsets in the sequence; and NumItems j is the number of items in the j th itemset in the sequence. parts. The lowest r bits encode the items and their order in the sequence, the mid-dle q bits encode the information about the number of items in each itemset in the sequence, and the highest p bits encode the number of itemsets in the sequence. This general signature function is a little bit different from the one used for the example in Fig. 2. By encoding the information about the number of items in each itemset in the sequence, we can distinguish the sequences like AB  X  DA and ABD  X  A . In practice, we can set the parameters p , q ,and r as 5, 7, and 20, respectively, which show very good pruning effectiveness.
 more effective when the same memory space is used. Our devised signature func-tion encodes not only the items and their order, but also the number of itemsets and the size of each itemset. By encoding these information into different bits, it actually partition the signatures of the sequences with different number of itemsets and itemset sizes into different value ranges. Such partitioning reduces the chance of collisions. In addition, because the signature value can be as big as the max-imum value that a unsigned integer can represent, the signatures can have much bigger value range than hash values, and hence have less probability of collisions. have millions of bits, a lot of them have  X 0 X  value. A good hash function can produce randomly distributed hash values, but it is not so easy to get a good ran-domizing hash function for the databases containing various data distributions and skewness. For the signature-based pruning, if the signature vector has N entries, we can guarantee that each of those entries covers at least one pattern. So, no entry is empty and the memory space allocated is fully utilized. Thus, when the same memory space is used, the signature technique can distinguish more patterns. plexity is O ( log N ) due to the binary search in the signature vector when N is the number of entries in the signature vector. However, it is O ( 1 ) for hash-based pruning. The extra search cost can be easily compensated as more candidate se-quences can be pruned by the signature-based pruning, because counting those false candidates against millions of customer sequences usually costs even more. forms much better than GSP when the seed set of frequent sequences for gener-ating the candidates cannot be loaded into memory totally. If the memory cannot hold all the candidates generated, they can be processed part by part. 4.3 Counting of candidate sequences During the top-down search for long patterns covered by MFS db , to reduce the number of passes, we need to count candidates of different sizes at each pass over the database. For that purpose, we developed a new prefix tree structure. Since it is much more efficient than the hash tree, we also use it to count the candidates of the same size during the bottom-up search in Phases 2 and 3. We first describe our prefix tree and the customer sequence trimming technique, and then compare it with the prefix tree used for PSP [ 12]. 4.3.1 Overview of the prefix tree and the customer sequence trimming The following example shows how the prefix tree works. Suppose we have 10 candidates of length 2 or 3. The prefix tree is constructed as shown in Fig. 3. Each node is associated with a pointer. If the path from the root to a node represents a candidate, the pointer points to the candidate; otherwise, it is NULL. A node may have two types of children. The  X  X -extension X  child means the item represented by the child node is in the same itemset with the item represented by its parent node. The  X  X -extension X  child means the item represented by the child node starts a new itemset. All the S-extension (I-extension) children of a node are linked together, and only the first child is linked to their parent node by a dashed (solid) line. For example, nodes 4 and 5 are the S-extension children of node 1, and the corresponding paths represent the candidates A  X  A and A  X  E , respectively. Nodes 6 and 7 are I-extension children, and their paths represent AC and AD , respectively. facilitate the customer sequence trimming. In this example, we have eight items in the database: A , B , C , D , E , F , and H .Since B , F ,and G do not appear in any candidate, they should be ignored during the counting. Thus, the bit vector is set as ( 10111001 ) , where 1 at the i th bit position means item i appears in the prefix tree. All the bits are initialized to 0, and the corresponding bits are set to 1 as we insert candidates into the prefix tree.
 to s = ACD  X  ADE  X  DH using the bit vector. Then, a recursive method is used to count all the candidates contained in s . At the root node, we check each item in ACD  X  ADE  X  DH to see if it is in the root node X  X  S-extension children. The first item of s is A , and it appears as the first S-extension child of the root node. So we recursively call the count function at the root node with two sequence segments. The segment CD  X  ADE  X  DH is used in the call for node 1 X  X  I-extension link, while ADE  X  DH is for its S-extension link. Then, we can locate the second item of s , C , at node 2. Since node 2 has no S-extension child, only one recursive call with the segment D  X  ADE  X  DH is made for its I-extension link. The third item of s , D , is the last item of the first itemset in s . Only one call with segment ADE  X  DH is made for node 3 X  X  S-extension link. The fourth item of s , A , can be located at node 1 again, and we make two recursive calls. One is for the node 1 X  X  I-extension link with DE  X  DH , and the other one is for its S-extension link with DH . Then, we process the remaining items in s , one by one, in the same way. Whenever we locate an item at some node, if the pointer associated with the node is not NULL and the count of the corresponding candidate is not increased yet (for the current customer sequence), it should be increased.
 there is no constraint on which items in the customer sequence should be checked against the root X  X  S-extension link, because the first item of a candidate can appear anywhere in the customer sequence. At other nodes, there are some constraints. Let us see how to make recursive calls at node 1 along its I-extension link. Recall that we have made two recursive calls at the root node with segments, CD  X  ADE  X  DH and DE  X  DH , for node 1 X  X  I-extension. Now we process them at node 1. Since the two segments are specified for node 1 X  X  I-extension link, we just check the items in their first itemsets, CD and DE , against node 1 X  X  I-extension link, because only those items are in the same itemset with item A represented by node 1. For CD  X  ADE  X  DH ,since C appears at node 6 which has no child, we stop there by just increasing the count of AC . Another item, D , appears at node 7. We increase the count of AD and make recursive calls for node 7 X  X  links. Since D is the last item of the first itemset in CD  X  ADE  X  DH , only one recursive call with the segment ADE  X  DH is made for node 7 X  X  S-extension link. For another sequence segment DE  X  DH at node 1, two items of the first itemset, D and E ,are checked. D is located at node 7. Since the count of AD is already increased before, we should not increase it again. Two recursive calls are made at node 1 for node 7 X  X  links. One is with E  X  DH for node 7 X  X  I-extension link and the other is with DH for the S-extension link. We can ignore E because it is not an I-extension child of node 1. This process will continue until a leaf node is reached or the sequence segment is empty. 4.3.2 Features of the prefix tree and the customer sequence trimming There are some major differences between our prefix tree and the PSP X  X  prefix tree: (1) Our prefix tree is used to count candidates of different sizes, whereas PSP X  X  prefix tree is only used to count the candidates of the same size. (2) To improve the candidate counting, a bit vector is associated with our prefix tree to facilitate the customer sequence trimming. (3) The supersequence frequency-based pruning reduces the size of our prefix tree when we count the candidates against the whole database.
 prefix tree structure, the I-extension children and S-extension children of a node are linked together, respectively. During the candidate counting, we frequently need to locate the items in the customer sequences along these links. This search could be either sequential or binary depending on how the links are implemented. tions can enhance the counting process. In MSPS, by performing supersequence frequency-based pruning in Phase 3, only a part of the candidate set needs to be processed. Thus, our prefix tree is usually much smaller than PSP X  X  prefix tree at each pass. Moreover, we also reduce the number of search operations by trim-ming the customer sequences. In PSP, the items not in the prefix tree are not trimmed from the customer sequence. Thus, when these items are processed, they are searched along the corresponding links exhaustively even though they are not in those links. This unnecessary search cost is not trivial when the number of cus-tomer sequences is large. MSPS can avoid this problem. As the mining process makes progress, fewer and fewer items would remain in the longer candidate pat-terns, and the customer sequence trimming can save a lot of time.
 technique proposed in Park et al. [ 14]. In Phase 3 of MSPS, those candidates removed by the supersequence frequency-based pruning are not counted, so we do not know exactly how many times an item in a customer sequence appears in the candidate sequences. This prevents us to do the customer sequence trimming as in Park et al. [ 14]. We can only trim the customer sequences based on the items appearing in the prefix tree. 5 Sampling in MSPS For both frequent itemset mining and sequence mining, if a pattern is found fre-quent in db but turns out to be infrequent in DB ,itisan overestimate .However,if a pattern is infrequent in db but actually frequent in DB ,itisa miss . of sampling. While we focused on how to maximize the performance improve-ment, more attention was given in Toivonen [ 18] on how to reduce the probability of misses. To achieve that goal, two methods were suggested in Toivonen [ 18]: (1) mine a large sample, and (2) lower the user-specified minsup for mining the sample. These two methods can reduce the misses but also potentially degrade the overall performance. Mining a large sample cuts the merit of sampling, while lowering the user-specified minsup may generate a large number of overestimates. Obviously, a complete sample result without misses does not necessarily mean the best overall performance. In MSPS, the cost related to sampling includes all the overhead of mining the sample and verifying the sample result, whereas the perfor-mance gain is from the supersequence frequency-based pruning. The effectiveness of this pruning is determined by how many long frequent patterns can be found from the sample. As different settings of sample size and the adjusted minsup for mining the sample are used, the overall performance varies accordingly. Thus, we pay our attention to the sample size and the adjusted minsup in the following discussion. 5.1 Sample size In Toivonen [18] and Zaki et al. [22], the minimum sample size that guarantees a small chance of misses with certain confidence is given by the Chernoff boundary. conservative. In MSPS, a large sample can improve the quality of sample result with fewer misses and overestimates. Consequently, verifying the sample result can be done quickly and the supersequence frequency-based pruning can be very effective. But the overhead of mining a large sample is high. However, with a small sample, the overhead of mining sample is low, but MFS db may be in bad quality. Then, the cost of verifying the sample result containing many overestimates would be high. If the small sample size makes the minimum support count for mining thesample(i.e., minsup  X | db | or lowered minsup  X | db | ) very small, mining the sample itself may take a long time. Thus, a small sample does not necessarily mean a lower cost. Furthermore, if only few long frequent sequences are found under the border formed by MFS db , then the supersequence frequency-based pruning will not be effective, either. That is why the sample should not be too large or too small.
 to be mined, so it is hard to determine the best sample size. MSPS allows users to choose a plausible sample size empirically. In our experiments, we set the sample size as 10% of the original database size. By using a default sample size, how to balance the cost related to the sampling and the quality of sample result mainly depends on the adjusted minsup for mining the sample. Even though the default sample size may not be the best one all the time, with the method of adjusting the minsup, it works very well in practice according to our extensive experiments. In Sect. 6.3 , we will show the effect of different sample sizes. 5.2 Adjusting the user-specified minimum support for mining the sample In the sample mining result, a certain rate of misses is tolerable. Our tests show that, for a missing k -sequence, if most of its long subsequences, such as subse-quences with length k  X  1or k  X  2, are found, then the supersequence frequency-based pruning is not affected much. In practice, as long as the sample size is not too small, the probability that most of these subsequences are also missed is quite low. Compared to misses, overestimates could be a bigger problem. Once an in-frequent k -sequence is identified frequent in db at pass k , then it may be joined with many other k -sequences to generate a large number of false candidates in mining the sample. Most importantly, the situation may become even worse when the minimum support count for mining the sample is very small. We found this is more serious for sequence mining than for frequent itemset mining, because the search space is much larger. For MSPS, it not only degrades the efficiency of mining the sample, but also causes a high cost to identify the overestimates. however they did not consider the case that the user-specified minsup is very small. In that case, it is dangerous to lower the minsup further. In this research, we inves-tigated how to avoid the overestimates in the case of small user-specified minsup, because such mining task is more time-consuming.
 user-specified minsup is big, simply using it or even a lowered one to mine the sample works fine. Only a small number of misses and overestimates occur in our tests. This is usually safe because our default sample size is not very small. (2) If the user-specified minsup is small, the sampling technique is challenged. Using a lowered minsup or even the original user-specified minsup for mining the sample often causes many overestimates because lo w er ed minsup  X | db | or minsup  X | db | is too small. Even though increasing the sample size could be a solution for this case, it limits the merit of sampling. Thus, we consider increasing the minsup a little to mine the sample, hoping it will limit the overestimates to a reasonable level. In that case, more misses may occur. However, even though there is a missing pattern, as long as most of its long subsequences are still contained in the sample result, the supersequence frequency-based pruning is not affected much. (3) In some rare cases, the user-specified minsup is extremely small. Then, just increasing the minsup for mining the sample cannot solve the problem. We must consider increasing the sample size too. Actually, both cases (2) and (3) raise the same technical question: when user-specified minsup is small, how to increase the minsup for mining the sample of a certain size? We must keep in mind that if the increase in the minsup for mining the sample is not enough, the problem of overestimates cannot be solved. However, if it is increased too much, we may not find any long patterns from the sample.
 Theorem 1 For an arbitrary sequence X in a database DB, whose global support in DB is P ( X ) , the distribution of its local support P ( X ) in a random sample d b can be approximated by a normal distribution N ( X ,  X  2 ) : where | db | is the sample size, i.e., the number of customer sequences in the sample. Proof Because the support of X in DB is P ( X ) , the probability that a customer sequence randomly selected from DB contains X is also P ( X ) . For a random sam-ple db with | db | customer sequences that are independently drawn from DB with replacement, the random variable T ( X ) , which represents the total number of cus-tomer sequences containing X in db , has a binomial distribution of | db | trials with the probability of success P ( X ) . In general, if | db | is greater than 30, the distribu-tion of T ( X ) can be approximated by a normal distribution whose mean is | db | X  P ( X ) and the standard deviation is pose that we draw a random sample db from DB and then use the point estimator P ( X ) = T ( X )/ | db | to estimate the support of X in the population of DB . Then, P ( X ) is also an unbiased estimator with mean | db | X  P ( X )/ | db |= P ( X ) and standard deviation the user-specified minimum support is small. Thus, we investigated how much the minimum support should be increased to mine the sample of a certain size. Theorem 2 In the mining of a database DB for the user-specified minimum sup-port (mi nsu p  X  50% ), if the adjusted minimum support used to mine the sample db is S (S &gt; mi nsu p), the probability that an infrequent sequence Y can be over-estimated as frequent in d b is lower than 1  X  P Z ,where P Z is the probability of the z-score Z = ( S  X  minsup )/ Proof By Theorem 1, for an arbitrary sequence X , its local support observed from a sample, P ( X ) , has a normal distribution with the mean equal to its global support P ( X ) , and the standard deviation is of [ 0 , 1 / 2 ] , the standard deviation of P ( X ) is also increasing in this interval. user-specified minimum support, i.e., P ( Y 1 ) = minsup . Based on the nor-mal distribution, if the adjusted minimum support used for mining the sam-ple is set to S with S &gt; P ( Y 1 ) , i.e., S &gt; minsup , the probability that Y 1 can be found as a local frequent sequence in db is 1 the z -score Z = ( S  X  P ( Y 1 ))/ minsup )/ in db , P ( Y 2 ) , also has a normal distribution with mean P ( Y 2 ) and standard deviation usually smaller than 50%, in our analysis both P ( Y 1 ) and P ( Y 2 ) are considered to be in that range. As P ( Y 2 )&lt; P ( Y 1 ) , both the mean and the standard deviation of P ( Y with the distribution curve of P ( Y 1 ) , the distribution curve of P ( Y 2 ) is shifted left and sharper. That means, if we set the adjusted minimum support for mining the sample as S , the probability that an infrequent sequence Y 2 is overestimated as frequent should be lower than 1  X  P Z , which is the probability that Y 1 can be found as a local frequent sequence in db .
 than 1  X  P Z . In our experiments, the critical value of Z is set to 1.28, where P formula of S provides a theoretical guideline for adjusting the user-specified min-imum support to mine the sample. Even though this adjusted minimum support value may not be the best one all the time, it worked well in most of our experi-ments. In Sect. 6.3 , we will show the effect of different adjusted minimum support values used for mining the sample. 6 Performance analysis To compare MSPS with other algorithms, we implemented GSP and obtained the source codes of SPAM and SPADE from their authors X  Web sites. All the exper-iments were performed on a SuSE Linux PC with a 2.6 GHz Pentium processor and 1 Gbytes main memory.
 scalability of these algorithms in terms of the number of items and the number of customer sequences. We also investigated how the sample size and the adjusted minsup for mining the sample affect the performance of MSPS. Since the sam-pling technique is probabilistic, we ran MSPS 100 times for each test. The aver-age execution time of the 100 runs was reported as the performance result. The default sample size was fixed as 10% of the test database for all experiments. The databases used in our experiments are synthetically generated as in Agrawal and Srikant [ 3]. The database generation parameters are described in Table 1.Forall databases, N S = 5000 and N I = 25,000; and the names of the databases reflect other parameter values used to generate them. 6.1 Performance comparison We ran MSPS, GSP, SPADE, and SPAM on four databases with medium sizes of about 100 Mbytes. The number of items in these databases is 10,000. In our tests, SPAM could not mine these databases, and its run was terminated by the operating system. Our machine is a 32-bit system, but the user address space is limited to 2 Gbytes. In all these tests, SPAM always required more than 2 Gbytes of memory, and hence caused the termination.
 it or a lowered one to mine the sample may cost too much due to so many over-estimates. In practice, we may not know the data distribution characteristics of the database to be mined. Thus, we conservatively assumed that all user-specified minsups in our tests are small and simply increased them a little bit for mining the sample. The adjusted minsup for each test is computed using the formula derived in Sect. 5.2 . The probability that an overestimate occurs is set to 10% at most, i.e., Z = 1 . 28. Some typical adjusted minsups computed using the formula are listed in Table 2.
 tegrated, MSPS performs much better than GSP because it processes fewer can-didates in a much more efficient way. The advantage of SPADE is the efficient counting of the candidates by intersecting the id-lists. However, when mining a medium-size database with 400,000 customers, the counting for L DB 2 in SPADE is inefficient and degrades the overall performance very much. Considering both fac-tors, we can say that if there are not enough number of candidates to be counted, SPADE cannot show its efficiency. That is why SPADE is even worse than GSP when the minsup is big, as shown in some of the figures.
 mining. In that case, the overhead of GSP in candidate generation, pruning, and especially counting using a huge hash tree increases drastically. For MSPS, this situation is considerably improved by using the supersequence frequency-based pruning, the prefix tree structure, and the customer sequence trimming. Figure 5 shows how much the search space can be reduced by MSPS compared to GSP in mining D400K-C20-T2.5-S10-I1.25-N10K. Since counting C DB 1 and C DB 2 can be done quickly on the database in the horizontal format, we focused on the total number of candidates of length greater than 2, i.e., the candidates from pass 3 in GSP and the candidates in Phase 3 of MSPS. We can see that MSPS can reduce the search space by 55 X 65%. When many passes are required for the mining, most candidates usually appear after pass 2, hence MSPS can outperform GSP further when the minsup is decreased. This improvement also makes MSPS better than SPADE in most tests on the medium-size databases. Only when the minsup is very small, SPADE can beat MSPS.
 the smallest minsup used on them in the tests. Mining D400-C20-T2.5-S10-I1.25-N10K with the minsup of 0.12% produced most frequent sequences, about 2.6 million, while mining D400-C10-T5-S5-I1.25-N10K with the minsup of 0.06% produced only 526K frequent sequences. This difference is reflected in the corre-sponding execution times of GSP: 4397.5 s versus 1230.5 s. On the contrary, the numbers of maximal frequent sequences in these two databases with respect to the corresponding minsup values are 87K and 54K, respectively, and the correspond-ing execution times of MSPS are 1112.4 and 429.7 s. That means, even if there is a big difference between the numbers of frequent sequences in two databases, the difference between the numbers of maximal frequent sequences is usually much smaller. Thus, compared to frequent sequence mining algorithms, MSPS can be more robust in dealing with the databases with different data distribution charac-teristics.
 and maximal frequent sequences in the D400K-C10-T5-S10-I2.5-N10K database with respect to various minsups. When the minsup is decreased from 0.33% to 0.18%, the number of frequent sequences increases very quickly, from 47K to 1.7 million. But the number of maximal frequent sequences increases much slowly, from 4.8K to 28K. That means, mining maximal frequent sequences could be much more scalable. Another interesting observation is that the number of closed frequent sequences is not much smaller than that of frequent sequences for all the cases. For example, when the minsup is 0.18%, the number of closed frequent sequences is 1.2 million, while the number of frequent sequences is 1.7 million. That means, for certain cases, mining closed frequent sequences also might be impractical. 6.2 Scalability evaluation Both SPADE and SPAM need to store a huge amount of intermediate data to save their computation cost. When the memory space requirement is over the mem-ory size available, CPU utilization drops quickly due to the frequent swapping. Compared with them, MSPS and GSP process the customer sequences one by one, hence only a small memory space is needed to buffer the customer sequences being processed. MSPS can also handle the situation that L DB k or C DB k cannot be totally loaded into memory by using the signatures as explained in Sect. 4. There-fore, MSPS does not require the memory space as much as GSP, SPADE, and SPAM.
 items and millions of customers, so we evaluated the scalability of the mining algorithms in these two aspects. First, we started with a very small database D1K-C10-T5-S10-I2.5 and changed the number of items from 500 to 10,000. The user-specified minsup was 0.5%. To run MSPS on such a small database with only 1000 customers, we selected the whole database as the sample and used the user-specified minsup to mine it. When there are only 500 items, the database be-comes very dense because each item has a higher probability of being selected by the synthetic database generation program to construct the customer sequences. As a result, all the four algorithms spent much more time to finish the mining. Since MSPS does not apply the sampling on such a small database, supersequence frequency-based pruning is not performed in mining. Thus, in this case, SPADE and SPAM performed better than MSPS and GSP as long as their memory require-ment is satisfied.
 Theoretically, the memory space required to store the whole database into bitmaps in SPAM is D  X  C  X  N / 8 bytes. For the id-lists in SPADE, it is about D  X  C  X  T  X  4 bytes. But we found these values are usually far less than their peak memory space requirement during the mining, because the amount of intermediate data in both algorithms is quite large. As shown in Fig. 8, even though the D1K-C10-T5-S10-I2.5-N8000 database takes only 260 Kbytes, and the theoretical memory space requirement to store the database in SPAM is about 1000  X  10  X  8000 / 8 bytes  X  10 Mbytes, it could not finish the mining when the minsup was 0.5% because it needed more than 2 Gbytes of memory. Compared with SPAM, SPADE divides the search space into small pieces as only the id-lists being processed need to be loaded into memory. Another advantage of SPADE is that the id-lists become shorter and shorter with the progress in mining, whereas the length of the bitmaps does not change in SPAM. These two differences make SPADE much more space-efficient than SPAM.
 to 100K customer sequences. SPAM cannot mine the databases with more than 20K customers due to the memory problem. Our tests showed that SPAM is very sensitive to the number of items and the number of customers, which mainly limits its applicability.
 the user-specified minsup is 0.18%. We fixed the number of items as 10,000 and increased the number of customers from 400K to 2000K. SPAM cannot per-form the mining due to the memory problem. For SPADE, we partitioned the test database into multiple chunks for better performance when its size was increased. Otherwise, the counting of C DB 2 for a large database could be extremely time-consuming. We made each chunk contain 400K customers so that it is only about 100 Mbytes, which is one-tenth of our main memory size. Thus, D400K-C10-T5-S10-I2.5-N10K is processed as one chunk, D800K-C10-T5-S10-I2.5-N10K is divided into two chunks, and so on. Figure 9 shows that the scalability of both MSPS and GSP is quite linear. As the database size is increased, MSPS performs much better than the others.
 performed best X  X bout 20% faster than MSPS. But SPADE cannot maintain a reasonable scalability as the database becomes larger, and MSPS starts outper-forming SPADE. When the database size is increased from 1600K customers to 2000K customers, there is a sharp performance drop in SPADE, such that it is even slower than GSP. In that case, MSPS is faster than SPADE by a factor of about 8. As discussed before, counting C DB 2 is a performance bottleneck for SPADE because the transformation of a large database from the vertical format to the hor-izontal format takes too much time. When the database is very large, the transfor-mation also requires a large amount of memory and frequent swapping, hence the performance drops drastically. Partitioning the database can relieve this problem to some extent but does not solve it completely. In addition, for the database with a large number of items and customers, SPADE needs more time to intersect more and longer id-lists.
 takes about 500 Mbytes, for various minsups. This database was partitioned into five chunks for SPADE, and the results are shown in Fig. 10 .
 For medium size databases, MSPS performs better for relatively big minsups, while SPADE is faster for small minsups. When the database is large, SPADE X  X  performance drops drastically, and MSPS outperforms SPADE very much. If the user-specified minsup is big and there are very few long patterns, GSP may per-form as well as, or even better than the others due to its simplicity and effective subsequence infrequency-based pruning. 6.3 Effect of the sample size and the adjusted minsup on the performance of MSPS In MSPS, the overall performance improvement from the supersequence frequency-based pruning is determined by two factors: (1) how much search space can be reduced in Phase 3, and (2) the cost of Phase 2 for mining the sample and verifying the sample result. In this section, we will discuss how the sample size and the adjusted minsup affect the performance of MSPS. 6.3.1 Effect of the sample size on the performance We ran MSPS on D400K-C10-T5-S10-I2.5-N10K for the minsup of 0.18% with different sample sizes: 5, 10, 20, 40, and 60% of the original database. To observe how the sample size alone affects the performance of MSPS, we kept the user-specified minsup unchanged for mining the sample. The experimental results are shown in Table 3.
 of the original database to some extent. Even though there may be more misses for a small sample, there is no big difference in how much the search space is reduced in Phase 3. This is because most long subsequences of those missing sequences are still found from the sample. Thus, the supersequence frequency-based pruning is not affected much. However, for a small sample, like 5% of the database, the minimum support count (i.e., | db | X  minsup ) for mining the sample is very low, hence more overestimates would be produced. The cost of Phase 2 shows that a small sample does not necessarily mean a lower mining cost. The average performance of the 5% sample is not good; and in the worst case, it takes more time than GSP does: 3240 versus 2359 s.
 fewer overestimates are occurred. For the 60% sample, even though the quality of sample result is improved with much fewer misses and overestimates, it requires too much time for mining the sample itself. As a result, we cannot achieve the best performance, either. Compared with others, the 20% sample was the best based on all the measures shown in Table 3: average performance is best (704 s), more than 40% of runs were done in less than 650 s, and only 18 out of 100 runs took over 800 s. It is worth to mention that when we used the 10% sample with the adjusted minsup of 0.201% (instead of the user-specified minsup of 0.18%), we obtained a better average performance of 603 s. It demonstrates that adjusting the minsup (for mining the sample) enables us to use a relatively small sample to achieve a better performance. 6.3.2 Effect of the adjusted minsup on the performance To evaluate the effect of the adjusted minsup for mining the sample, we used D400K-C10-T5-S10-I2.5-N10K with the user-specified minsup of 0.18%. The sample size was fixed as 10% of the original database. The adjusted minsup values listed in Table 4 were chosen empirically, except for 0.20%, which is based on the formula proposed in Sect. 5.2 . too small, the overall performance was degraded. Simply using the small user-specified minsup of 0.18% to mine the sample was not the best choice, either. The best adjusted minsup was 0.20%, which is very close to the value computed using our formula (0.201%): 71 out of 100 runs were finished within 650 s. With this adjusted minsup, MSPS outperformed GSP (taking 2359 s) by a factor of about 3. This result demonstrates that our proposed formula for the adjusted minsup value is very reasonable.
 0.17% to mine the sample, the average performance was much worse than other adjusted minsup values. If it is lowered further, mining the sample itself becomes very hard and more overestimates will occur. When we increased the minsup to 0.5%, which was relatively too big, MSPS mined the sample and verified the sam-ple result within just 11 s. Actually, it did not find long frequent sequences in the sample that could speed up the mining process. In such cases, MSPS works with-out the benefit of the supersequence frequency-based pruning. The performance gain comes mainly from the efficient counting of the candidates using the prefix tree structure and the customer sequence trimming. Compared to this case, the adjusted minsup of 0.20% reduced the total execution time by 34%, from 947 to 622 s.
 better to use it without any change, or even a lowered one, for mining the sample. In Toivonen [ 18 ], how to avoid the misses in mining the sample by lowering the user-specified minsup is described. However, as mentioned before, without know-ing whether a user-specified minsup is big or not for the database, lowering it to mine the sample could be very risky. In our research, we simulated the practical situations and adopted a safe approach of adjusting the minsup to a slightly bigger value by using the proposed formula to mine the sample. Our method is more use-ful when the user-specified minsup is small, and it is the case the mining process takes a lot of time. 7 Conclusions and future work In this paper, we proposed a new algorithm, named MSPS, for mining max-imal frequent sequences using sampling. MSPS combined the subsequence infrequency-based pruning and the supersequence frequency-based pruning to-gether to reduce the search space. In MSPS, a sampling technique is used to identify potential long frequent patterns early. When the user-specified minsup is small, we proposed how to adjust it to a little bigger value for mining the sam-ple to avoid many overestimates. This method makes the sampling technique more efficient in practice for sequence mining. Both the supersequence frequency-based pruning and the customer sequence trimming used in MSPS improve the candi-date counting process on the new prefix tree structure developed. Our extensive experiments proved that MSPS is a practical and efficient algorithm. Its excel-lent scalability makes it a very good candidate for mining customer market-basket databases which usually have tens of thousands of items and millions of customer sequences.
 retically analyzing the side-effect of sampling on the maximal sequential patterns. (2) Improving the sample quality: if a random sample does not represent the con-tent of the original database well, the performance of MSPS is affected. We are considering the combination of sequence clustering and stratified sampling to im-prove the sample quality. (3) Integrating the proposed sampling technique with other sequence mining algorithms. It can improve their performance in certain application domains. (4) Investigating if there is a cost-effective way, other than sampling, that can detect long potential frequent sequences. If yes, combining it with the depth-first search could be an interesting topic.
 References Author Biographies
