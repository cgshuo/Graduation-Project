 Crowdsourcing services such as Amazon Mech anical Turk have made it possible to col-lect large amount of labels at relatively low cost. Nonetheless, since the reward is small and the ability of workers is not certified, the labeling quality of crowd labelers is often much lower than that of an expert. In the worst case, some workers just submit ran-dom answers to get the fee deviously. One approach to dealing with low quality labels is repeated-labeling. Sheng et al. [16] empirically showed that under certain assump-tions, repeated-labeling can improve the label quality. Thus in crowdsourcing, people while in traditional supervised learning, one instance x i corresponds to one label y i .
The problem remains as how to learn a reliable predictive model with the unreliable crowd labels. Various methods have been proposed to infer the ground truth [4, 10] or learn from crowd labels directly [8, 15]. The basic idea is employing generative models for the labeling processes of crowd labelers. While these models are useful under certain conditions, their assumptions on labelers are not easy to verify for a certain task.
This situation motivates us to investigate making full use of opinions collected from crowds by incorporating some expert labels, which seems more sensible than trying to verify the behavior of each labeler. Intuiti vely, combining expert labels with crowd labels is expected to achieve higher learning quality than solely using crowd labels though, little work has been done under this configuration since most of the existing work has focused on crowd labels.

This paper proposes to improve the performance of crowdsourcing learning tasks with a minimum number of expert labels by maximizing the utilization of both the crowd and expert labels 1 . Our major contribution is a formalized framework for utiliz-ing expert labels in crowdsourcing. Following a series of existing work [8, 15, 19], our work focuses on supervised classification problems.

Some existing models [8, 13, 15] are capable of combining expert labels by straight-forward extensions. The major difference between our method and these models is that we use prior beliefs on experts much more explicitly. 1.1 An Illustrative Example In what follows, we illustrate the limitation of crowd data with an example and explain the idea which forms the basis of our framework. Fig. 1(a) shows a synthetic dataset for binary classification. For each class, we samp le 100 points respectively from two differ-ent Gaussian distributions, and get four underlying clusters. We simulate two labelers whose opinions differ in one cluster as shown in Figs. 1(b) &amp; 1(c). Here no model that uses the crowd labels without extra information can weight one labeler over the other since there is simply not enough evidence. Nonetheless, these two labelers provide very informative labels. Labeler 1 actually gave all correct labels. If we can identify this fact by a few expert labels, we achieve an efficient method.

However, the problem is not trivial even for this toy data set. Supposing that we choose a controversial point and let an expert label it, we will find that Labeler 1 gave the correct answer. This is far from enough to conclude that Labeler 1 gave true labels for all controversial points given that in practice we only have crowd labels and are not aware of the underlying data distribution. Adding more expert labels may increase our confidence on Labeler 1 , still a formalized mechanism i s needed to combine the ground truth with crowd data.

We address the problem by a model combination process. We train a logistic re-gression classifier for each labeler separatel y with the labels provided by that labeler, colored in blue. We treat the values of f ( x i ) as features in a new space, shown in Fig-ure 1(d). This is referred to as the intermediate feature space [11]. The final prediction is made by another classifier in t his intermediate feature space.

By summarizing the opinions of labelers using personal classifiers, the separation between classes becomes clearer and the cont roversial area is projected to the bottom right in the new space and becomes more compact. Incorporating expert label evidence in this space is much easier compared with th e crowd labels in the original space. A few ground truth labels in the controversial area will enables most classifiers built in this space to favor Labeler 1 over Labeler 2 n aturally. We leave the the crucial step of combining expert evidence to the experiment section after we formalize our framework. With the arising of crowdsourcing services, crowd workers have shown their power in applications such as sentiment tracking [3], machine translation [1] and name entity annotating [5]. A key problem in crowdsour cing research is modeling data from multi-ple unreliable sources for inferring the ground truth. The problem has its origin in the early work [4] for combinin g multiple diagnostic test results. Recent work addressed problems with the same formulation by methods such as message transferring [10] and graphical models [13].
 Our framework adopts the idea of learning a classifier from crowd data directly. Raykar et al . [15] and Yan et al . [19] treat true labels as hidden variables which are inferred by the EM algorithm. Kajino et al . [8] infer only the true classifier by personal classifiers without considering true labels e xplicitly. The nature of our method is similar to that of Kajino et al . [8], focusing on the final learning tasks and not being tangled with the correctness of a certain label.

To the best of our knowledge, very little work considered the case of learning from crowd and expert data simultaneously. Kajino et al . [9] addressed this problem by ex-tending some existing models straightforwardly. Wauthier and Jordan [18] also used some expert labels. In their model crowd labels only make effects through the shared latent factors which express labelers. Our method differs from these work in both moti-vation and formulation.

We treat combining opinions of labelers as model combination. Getting the opti-mal combination of a group of pattern classifiers has been studied thoroughly for a long time and various methods have been proposed to employ the intermediate feature space. Merz [14] proposed to do feature extractio n using singular decomposition in this space and Kuncheva et al . [12] proposed to combine classifiers giving soft labels using de-cision templates. In traditional model combination framework, multiple classifiers are obtained by different models trained on the same data set. Here the scenario is different, i.e., we have multiple unreliable label sets to train multiple classifiers, and we propose to use some reliable labels to combine them. Under the crowdsourcing setting the idea of absorbing the evidence of true labels in the intermediate feature space is also original. In this paper we focus on binary classification problems with crowdsourcing training data. The extension to multi-class cases is conceptually straightforward. 3.1 Problem Formulation x gives labels to all N data instances. 2 The label given by the  X  th labeler for instance x i is denoted as y i where y i  X  X  X  1 , 1 } . All labels corresponding to x i are collected in the L -dimensional vector y i .

Different from most of the existing methods , we use some additional expert-labeled instances to improve the model quality. If there are N 0 expert labels, then the expert and y 0 j is the true label provided by the expert . Note that an expert-labeled instance x f : R D  X  [0 , 1] for unseen data by taking both training sets D and D 0 as inputs where f ( x )= p ( y =1 | x ) is the posterior probability of the positive class. We denote the predictive function in this way for the convenience of the following steps. 3.2 Building Intermediate Feature Space We extract the crowd opinions by treating labelers as personal classifiers. For the  X  th classification model that expresses predictions as posterior probabilities of classes is compatible with our approach. Here we follow the work [8] and use a logistic regression model for each labeler, which is given by where w is the model parameter and the logistic sigmoid function is defined as  X  ( a )= 1 / (1 + e  X  a ) . We express all prediction functions of classifiers as an ensemble F = { f 1 ,f 2 ,...,f L on instance x . The outputs of all L classifiers for a particular instance x i is organized decision profile [11]. In what follows, we denote this vector as dp i with the  X  th element dp i = f ( x i ) . We treat values of dp i as features in a new feature space, namely the intermediate feature space, and use anothe r classifier taking these values as inputs for making the final prediction. 3.3 Combination of Evidence from Crowds and Experts The next step is to train a classifier in the intermediate feature space by utilizing expert labels. As expert labels are much more reliable than crowd labels, we should put more weights on them. However, if we discard crowd labels and use expert labels solely, building a stable model can be costly even in the more compact and representative intermediate feature space. Thus a balan ce has to be made between the crowd opinions and expert evidence.

We address the problem by imposing a Bayesian treatment on the model parame-ters of the classifier in the intermediate f eature space. We use some straightforward combination of personal classifiers as the p rior distribution of model parameters, and absorb expert label evidence by updating the posterior distribution sequentially. We be-lieve that a fully Bayesian method is essential here for utilizing the prior distribution on parameters, which is informative in our framework as we will show later.

Specifically, we use the Bayesian logistic regression model [7] as our classifier in the intermediate feature space. The model achieved a tractable approximation of the posterior distribution over parameter w in Equation (1) by using accurate variational techniques. In our problem, the decision profile dp i in the new space corresponding to the instance x i in the original space is an ( L +1) -dimensional vector consisting of all values of dp i , X  =1 ,...,L and an additional constant 1 corresponding to the bias in parameter w . The corresponding true label is y 0 i . The model assumes that the prior distribution over w is Gaussian with mean  X  and covariance matrix  X  . Absorbing the evidence of expert-labeled instance dp and the true label y amounts to updating the mean and covariance matrix by process is iterative and converges very fast (about two iterations) [7].

While one common criticism of the Bayesian approach is that the prior distribution is often selected on the basis of mathematical convenience rather than as a reflection of any prior beliefs [2], the prior distribution here is informative with a specific mean and an isotropic covariance matrix given by The mean is chosen such that all personal classifiers are combined by weighting them equally, and the bias is  X  0 . 5 to fit the shape of the logistic sigmoid function which is equal to 0 . 5 for a =0 .

There is a single precision parameter  X  governing the covariance matrix. We can interpret  X  as our confidence on the crowds. A large  X  will cause the prior distribution over w to peak steeply on the mean, thus the affect of absorbing one expert label will be relatively small, leading to a final classifier depending heavily on the mean of prior, which is the simple combination of personal classifiers. On the other hand, a small  X  means that the prior is close to uniform, causing the final classifier to make predictions mainly based on expert labels.

Intuitively, we should use a large  X  when personal classifiers are generally good, and use a small one when crowd labels are inaccurate. In a crowdsourcing scenario however, we usually do not have such knowledge. One alternative is to let  X  be related to the number of expert labels N 0 given by  X  =1 /N 0 . As this number increases, we decrease the confidence on crowds to let the final model put more weight on expert labels. Experiments show that with such selection of  X  , our model achieves relatively stable performance under various values of N 0 .

Once the prior over w is chosen, we update its posterior distribution sequentially with Equations (2) &amp; (3) by adding one expert label each time. If an instance x j labeled by the expert is not in D , we should first calculate its predictions dp j by personal classifiers and use these values to update the model. We collect all dp j in a set DP = { dp j } N 0 j =1 . The complete steps of learning our model are summarized in Algorithm 1. 3.4 Classification To classify a new coming instance x k using the above results, we firstly get the predic-tions dp k of personal classifiers on x k , and calculate the predictive distribution of the true label y 0 k in the intermediate feature space by marg inalizing w.r.t. the final distribu-tion N ( w |  X  ,  X  ) . The predictive likelihood is given by where subscript k assigned to  X  and  X  refers to the posterior distribution over w after absorbing the evidence of dp k and y 0 k . 3.5 Missing Labels In real crowdsourcing tasks, workers may label part of the instances instead of the whole set. Our model handles this problem naturally by training multiple personal classifiers independently. A worker only labels a few instances may lead to a pool personal classi-fier. But this is not fatal as he uses only a tiny proportion of the whole budget. Also in practice, we can avoid such cases simply by designing HITs with a moderate size. We use synthetic data to illustrate the process of absorbing expert evidence, and evaluate the performance of our method on both UCI benchmark data and real crowdsourcing data. 4.1 Synthetic Data We complete our example in Figure 1 by illustrating the process of absorbing expert labels, shown in Figure 2. For clarity, we only show the decision boundaries given by means of the distributions over model parameter w . Dotted lines are priors before adding expert labels. This prior is give n by weighting each labeler equally following our framework.

In the left sub-figure, we add one expert label and get the posterior. Since the true label is blue, the decision boundary moves downward to suggest that data points near this labeled instance is more likely to be blue. In the right sub-figure, we add four expert labels for each class. The final decision bounda ry separates the actual class very well using merely eight expert labels. In this experiment we adjusted the model parameter  X  to get the best illustrative effect.
 4.2 UCI Data We test our method on three data sets from UCI Machine Learning Repository [6], Waveform 1(5000 points, 21 di mensions), Wine Quality( 6497 points, 12 di mensions) and Spambase(4601 points, 57 dimensions). These data sets have moderate sizes which enable us to perform experiments when number of crowd labels varies.

Since multiple labelers for these UCI datasets are unavailable, we simulate L label-ers for each dataset. We firstly cluster the data into L clusters using k -means and assign some labeling accuracy to each cluster for every labeler. Thus each labeler can have different labeling qualities for different clusters. We use an L  X  L matrix A =[ a ij ] L  X  L to express the simulation process, in which a ij is the probability that labeler i gives the true label for an instance in the j th cluster, thus a row corresponds to a labeler and a column to a cluster. We set L =5 and use three different accuracy matrices A 1 , A 2 , and A 3 to simulate different situations of labelers as follows.

A 1 simulates severely biased labelers. A 2 simulates labelers whose labels are both noisy and biased. A 3 simulates simply noisy labels. Note that A 3 satisfies the model assumption in the work by Raykar et al. [15].
 We choose three baseline methods that learn with crowd data solely for comparison. To verify the ability of our method to utilize the crowd labels, we compare the results trained on expert labels solely. For comparison with existing methods we use the model proposed by Kajino et al. [9], which is a state-of-art model that addresses the same problem. We use the results trained on the original datasets which have all ground truth labels as the approximate upper bounds of the classification performance. Methods used in experiments are summarized as follows.  X  Majority Voting (MV) method learns from the single-labeled training set esti- X  All-in-One-Classifier (AOC) treats all labels as in one training set.  X  Multiple Labelers (ML) method [15] learns from crowd labels directly.  X  Kajino et al. [9] extended their personal classifier model [8] to incorporate expert  X  We refer to our method as Classifier Combination with Experts (CCE) . CCE-N 0  X  Training with expert labels solely is referred as Expert Labels (EL) classifiers.  X  Ground Truth (GT) classifier uses the original datasets for training.

For MV, AOC, GT, and EL, we use a logistic regression model respectively to train the classifiers. For PCE, CCE and EL, the set of expert labels are randomly chosen from the original datasets given the number of expert labels N 0 which is restricted to a small proportion of N . We divide each dataset into a 70% training set and a 30% test set and each result is averaged on 10 runs.

Tables 1 X 3 show the results for different datasets respectively. Results are in the form of classification accuracy and averaged on 10 trials. The GT classifier is independent of crowd labels thus it has only one result on each dataset. Our CCE outperforms EL, and also outperforms MV, ACL and ML in most cases. This validates the ability of CCE for combining crowd and expert labels. The only exception appears in ML under A 3 where labelers are not biased. CCE outperfo rms PCE with clear advantages. There are a number of cases that PCE performs worse than EL, which suggests that in the PCE model expert evidences are easily dist urbed by inaccurate crowd labels.

Table 4 shows the results under the variation of numbers of crowd labels. We show the results on Spambase data under A 3 since under this situation all methods seem to work well. The top number of each column repr esents the number of labels provided by each labeler. This is also the number of expert labels used for GT. We use 50 expert labels for EL, PCE and CCE. EL has only one result as it is independent of crowd labels.
There is no surprise that ML performs very well in this experiment as the configura-tion here meets ML X  X  model assumption. Yet we should not forget that ML fails in many cases as shown in Tables 1 X 3. We do not choose those cases because showing groups of failed results does not make any sense. Generally PCE and CCE outperform MV and AOC by using extra expert labels. CCE performs slightly worse than PCE when the number of crowd labels is small, while the performance raise of PCE is quite limited when using more crowd labels.

In summary, our method CCE achieved reasona ble performance on different data sets with various labeler properties. Th e accuracy and stability of our CCE increase as we use more expert labels. On the other hand, learning solely from crowd labels is risky, especially when crowd labels are biased . PCE X  X  performance is limited compared with CCE when we have enough crowd labels. 4.3 Affective Text Analysis Data In this section we show results on the data for affective text analysis collected by Snow et al. [17]. The data is collected from Amazon Mechanical Turk. Annotators are asked to rate the emotions of a list of short headlines. The emotions include anger, disgust, fear, joy, sadness, surprise and the overall positive or negative valence. The former six There is a total number of 100 headlines la beled by 38 workers. For each headline 10 workers rated for each of the seven emotions. Most workers labeled 20 or 40 instances thus more than one half labels are missing. All 100 instances are also labeled by the experts and have an average rating for each emotion, which we treat as ground truth.
We design the classification task which predicts the surprising level of a headline using other emotion ratings as features. W e define that a headline of which the surprise rating is above 20 is a surprise, while others not, and use ratings of other six emotions provided by the experts to express a headline. Thus we get a binary classification task in a 6 -dimensional feature space.

Figure 3 shows classification accuracy when continually adding expert labels. Re-sults of MV, AOC and ML are not shown in this figure, which are three horizontal lines below GT and stay close to each other. PCE only performs similarly with EL, which collapses to GT when using all expert labels.
The result of CCE is promising. The value of GT is 0.65, which suggests that accord-ing to the experts, there is no strong correlation between the surprising level and other emotions. However, CCE only uses about 20 expert labels to get a similar performance level with GT, and when adding more expert labels, CCE outperforms GT and achieves an accuracy up to 0.8. We attribute this fact to the power of our CCE model as a  X  X eature extractor X . Among the 38 workers, one or more of them did give ratings in manners that relate surprising levels to other emotions even if experts did not do so. Personal clas-sifiers trained from these workers will then be able to predict the target and our model identifies these classifiers successfully using expert labels. In this paper, we have proposed a framework for improving the performance of crowd-sourcing learning tasks by incorporating the evidence of expert labels with a Bayesian logistic regression classifier in the intermed iate feature space. Experimental results have verified that by combining crowd and expert labels, our method has achieved better performance as compared with some existing methods, and has been stable under the variation of the number of expert labels and crowd labeler properties.

A promising direction of future work is to investigate actively querying for the expert labels, for which we can develop models by adopting basic ideas from active learning and considering the particular situation of crowdsourcing.
 Acknowledgment. This work is supported by the National Key Technology R&amp;D Program of the Chinese Ministry of Science and Technology under Grant No. 2012BAH94F03.

