 Information Retrieval; Learning-to-Ranking; Personaliza-tion
Traditional information retrieval systems, and partic-ularly Web search engines, have focused on keyword-matching. In this search paradigm, users typically input their information needs as a set of keywords and the search engines match the keywords with documents and use some additional signals, such as document popularity (e.g., doc-ument historical click through rate, PageRank etc.) to find relevant documents. This paradigm makes it very simple and easy for users to use. Moreover, it allows the efficient retrieval of documents at the Web scale. However, while this paradigm has been effective for the majority of the queries on generic Web search engines, it does not work that well for LinkedIn job search. In our use case, user information needs are typically rather complicated. For example, when a user issues query  X  X oftware engineer Cambridge Microsoft X , he or she does not mean to find a job that its description con-tains these keywords. Instead, he or she looks for a job with title software engineer at company Microsoft in Cambridge city . Thus, it is important to understand the structure of the information need. Moreover, even if we can structure the query as above, it is still unclear which city does  X  X am-bridge X  refers to. Even assuming we know that  X  X ambridge X  refers to the city in Massachusetts (USA), there could still be plenty of Microsoft software engineer jobs in the city. In this case, which specific ones match best with the user X  X  interests and expertise? Thus, it is crucial to go beyond keyword-matching when scoring documents to provide per-sonally relevant results on LinkedIn job search.

Semantic search, on the other hand, represents informa-tion needs and documents in a structured way and semanti-cally matches the information needs with the documents. A challenge with semantic search is that it is typically difficult for an end-user to describe his or her information need in a semantic representation. Moreover, semantic search is often restricted by concepts and relations predefined in a knowl-edge base. Thus, it does not scale well to open and dynamic document sets like the Web. 2. Presenting practical challenges when deploying the 3. Experimenting and demonstrating significant bene-
The rest of the paper will be organized as follows. Section 2 reviews related work. Section 3 presents query-document matching features based on standardized entities in queries and documents. Section 4 discusses searcher-document fea-tures capturing expertise homophily. Section 5 details how we construct entity-faceted historical click-through-rate fea-tures. In Section 6, we present a learning-to-rank approach to combine the proposed features and the existing ones. Of-fline and online experimental results are discussed in Section 7. Finally, concluding remarks are in Section 8. Semantic search has been an active research area recently. Generally speaking, semantic search uses semantics to make search systems more effective. More specifically, semantic search approaches extract semantic meanings and structures from search queries and documents and exploits them in search process [6]. The main approach to semantic search typically represents queries and documents in a structured way and applies semantic query languages like SPARQL [19] to retrieve results.

SHOE system [12] is one of the early work in this di-rection. It annotates Web pages with semantic informa-tion. On query side, users specify their information needs based on nodes and relations in an ontology. Swoogle [8] and Corese [4] are Semantic Web search engines. These en-gines store documents in Resource Description Framework (RDF) format and provide structured query languages to re-trieve the documents. Similarly, Zhong et al. [28] represent queries and documents by conceptual graphs then propose an ontology-based matching algorithm estimating the simi-larity between a query and a document.

A common problem of these approaches is that it is chal-lenging for end users to specify their information needs in such structured ways [24]. Also, these approaches are often restricted by concepts and relations predefined in ontologies. Thus, they do not scale to open and dynamic document sets like the Web [7]. Moreover the algorithm matching queries and documents in the structured representation are typi-cally computationally expensive. Thus, it is challenging for industrial search engines to apply them in real time at the Web scale.
Another research direction related to our work is using semantic technology to enhance traditional keyword-based information retrieval. Guha et al. [9] propose an approach to improve keyword-based search by using data retrieved from the Semantic Web. Specifically, given a text query, the plicitly select some of the predefined topics to specify their interests. Another direction is to implicitly model user X  X  in-terests. Sugiyama et al. [22] and Sontag et al. [21] propose approaches to model user X  X  long-term interests. Some other work like [20, 25] focuses on immediate history to repre-sent user X  X  short-term interests. When data is too sparse to model user X  X  interests individually, personalization could be achieved by grouping users in cohorts [27]. When a user issues a query, the corresponding cohorts can be used to customize results for the user.

A novel aspect of our work in terms of personalization is that we propose a new concept of expertise homophily to personalize job search results. Expertise homophily is mea-sured by similarity between user X  X  expertise and expertise requirement of each job. Moreover, to represent user X  X  ex-pertise, we do not only use standardized skills explicitly in a user profile but also infer skills that the user might have. The inference step is done by collaborative filtering tech-nique exploiting skill co-occurrence patterns in the whole member base.
As mentioned before, when a user issues a query like  X  X oft-ware engineer Microsoft X , the user implicitly links the key-words to different typed entities, such as, title and company and expects the results matching with the information need in terms of the structure he or she has in mind. To sat-isfy this, we propose an approach that indexes documents in a structured way. At searching time, the user query is segmented and linked to one of the typed entities used in the document index. Then, we construct various features matching typed entities mentioned in the query with the cor-responding ones in the documents. In the next subsections, we describe how documents are structured and indexed, how to segment a query and link the segments to entities and how to construct features semantically matching the query and documents.
To help job seekers search and discover jobs, we build a search index on some of the key attributes of the job. Jobs on LinkedIn are structured to present the following key attributes: job title , company , location , industry and skills . An example is shown in Figure 1. When a job is posted on LinkedIn, it goes through a standardizer which looks at above mentioned fields to extract out standardized entities. The extracted entities are based on curated dictionaries built over time from our member profiles. The standardizer has been engineered through multiple iterations to understand what parts of the job posting are critical for different entities. The standardized job is then indexed and is searchable both on the entities as well as the free text as entered by the job poster. Since there is huge research literature on mapping textual mentions to entities, this paper does not emphasize this step. Instead, through out the paper, we focus on how to use these entities in job search ranking. When a searcher enters a query, e.g.  X  X oftware engineer Microsoft Cambridge X , we first apply a query tagger to seg-ment the query and tag the segments into entity types that match title entity ( X  X oftware engineer X ) with the title entity in each job document. Similarly, we match company entity and location entity with the corresponding ones in the job description. We consider two types of entity-matching: hard matching and soft matching. The former checks if the two entities (in the query and in the document) have the same identifier. This matching is able to capture synonymy rela-tionship amongst different textual forms of the same entities, e.g.,  X  X oftware engineer X  and  X  X oftware developer X .
We also consider the semantic similarity (soft matching) between two different but related standardized entities, such as, between title  X  X oftware engineer X  and title  X  X oftware ar-chitect X  or between skill  X  X nformation retrieval X  and skill  X  X eb search X . To measure such similarity, we use an ap-proach leveraging the uniqueness of LinkedIn data collection including entity co-occurrence and career trajectory of more than 400 million members on LinkedIn. Intuitively, if two skills tend to co-occur in similar groups of members, they are likely to be related. Likewise, if there are a significant number of employees transferring between two companies, they are also likely to be similar.
As briefly presented before, for a query like  X  X oftware en-gineer Microsoft X  the searcher is not equally interested in every software engineer job at the company. Instead, if the searcher happens to be a machine learning expert, he or she is much more likely to be interested in software engineer jobs related to this field rather than software engineer jobs focus-ing on other domains. Thus, in many cases, user queries are not enough to represent user information need and interest. To complement the query, we exploit an idea of expertise homophily that captures the similarity between searcher X  X  expertise and job expertise requirements to make job search results more personally relevant.

Homophily has been extensively studied in the context of social networks analysis [17, 14]. The main idea is that in a social network, a node tends to be connected or interact with other nodes that are similar to it. In the context of job search, we hypothesize that a job searcher tends to be interested in the jobs requiring similar expertise as his or hers. In this section, we first present how we represent ex-pertise of searchers and expertise requirement of jobs via standardized skills. Then, we conduct an analysis to ver-ify the hypothesis. Finally, we propose features capturing searcher-document expertise homophily.
 LinkedIn allows members to add skills to their profiles. Typical example of skills for a software engineer would be - X  X lgorithm X ,  X  X ata Mining X ,  X  X ython X , etc. On LinkedIn, there are about 35 thousand standardized skills. Members can also endorse skills of other members in their network. Thus, skills are an integral part of members X  profiles to help them showcase their professional expertise (see Figure 2). On document side, as described in Section 3, each job de-scription also associates with a set of standardized skills. Thus, in this paper, we use skills to represent searcher X  X  ex-pertise and job expertise requirement.

We conduct an analysis to verify the hypothesis that a job searcher tends to be interested in the jobs similar to his or her expertise using search logs. To avoid confounding fac-tors, e.g., a feature in the original ranking function produc-stance, a machine learning researcher could have  X  X onprofit fundraising X  skill [11]. To overcome these challenges, we es-timate expertise scores of a member on the explicit skills and the ones he might have. Figure 3 describes the offline process to estimate the expertise scores. In the first step, we use a supervised learning algorithm combining various signals on LinkedIn such as skill-endorsement graph page rank, skill-profile textual similarity, member X  X  seniority, etc. to estimate the expertise score, i.e., p ( expert | member,skill ). After this step, the expertise matrix ( E 0 ) is very sparse since we can be certain only for a small percentage of the pairs. In the second step, we factorize the matrix into member and skill matrices in K-dimensional latent space. Then, we compute the dot-product of the matrices to fill in the  X  X n-known X  cells. The intuition is that if a member have  X  X a-chine learning X  and  X  X nformation retrieval X  skills, based on skill co-occurrence patterns from all of our member base, we could infer that the member is likely to also know  X  X earning-to-rank X .

Since the dot-product results in a large number of non-zero scores of each member on the 35K skills, the scores are then thresholded. If a member X  X  score on a skill is less than a threshold, the member is assumed not to know the skill and the score is zeroed out. Thus, outlier skills are removed. At the same time, the final expertise matrix ( E 1 ) is still relatively denser than E 0 since it includes scores of inferred skills. We refer interested readers to our recent work [10] for more details.

Since matrix factorization is computationally complex, to guarantee efficiency, we apply a two-phase approach. An offline process periodically runs on distributed computing platforms like Hadoop to infer member skills. The online phase then simply consumes the latest version of the data at ranking time. Given a set of skills that a searcher has and a set of skills that a job requires, we compute Jaccard similarity between the the two sets. One future direction is to use weighted Jaccard similarity in which the weights are determined by searcher X  X  expertise scores on the skills.
In this section, we propose entity-faceted document-popularity features based on user past behaviors. These features aim to indicate the quality of documents and are independent of query and searcher. The idea of using user X  X  historical actions on documents in Web search ranking func-tions has been shown effective in the literature [3]. However, a key challenge when deploying these features is the sparse-ness of data. The challenge is even more serious for job search since the lifetime of a job document is much shorter. To overcome this, instead of computing a single historical CTR separately for every job in the corpus, we estimate his-torical CTRs of a job on different entity-facets, such as, title, company or location. Please also note that in this paper we use historical CTR as an example, the idea is also appli-cable to other user-action based popularity features where data sparseness is an issue.
Given a job corpus, we compute multiple historical CTRs on different facets of the jobs. These facets are defined based on standardized entities mentioned in the jobs, such as, ti-CTRs at a large scale. At ranking time, the online process takes the latest data to compute features in realtime.
The default value  X  controls an interesting trade off for the cases where the number of impressions are low, e.g., jobs from a new company. If we use a too small default value, the new jobs that were not previously shown have little chance to be shown. Thus, we could not explore these results. However, many of these jobs could be good results and could get clicked if they are shown to the users. On the other hand, if we assign  X  to a too high value, historical CTRs become too smooth and we do not exploit much of the insight from user historical actions.
We tune the values of  X  on validation set. Each instance in the data set, which corresponds to a search result, contains a feature vector and a graded relevance label. The label indicates how relevant the result is to the query and the searcher. The details on how the data set is generated will be described in Section 6.2. For each entity-faceted historical CTR (e.g., company-faceted historical CTR), we select the value for  X  that maximizes the correlation between the CTR and the labels (See Equation 2).

To illustrate the effect of parameter tuning, Figures 4 and 5 show company-faceted historical CTRs with  X  equaling to zero and the optimal value. In each figure, the upper plot shows the relationship between the CTR feature and labels and the lower plot reveals company distribution over CTR values. On Figure 4, when the default value is zero, there are a significant number of companies having the historical CTR of zero because they have zero or few impressions in the period before we collect the training data. However, the average label of these results is not low (the first data point in Figure 4(a)), i.e., many of them are actually good results. Thus, as shown in the figure, the CTR does not strongly predict result relevance. When we assign  X  to the optimal value (Figure 5), however, the company-faceted historical CTR becomes strongly correlated with the label. Quanti-tatively, between the two values for  X  , the later improves Pearson correlation coefficient between the feature and the label by 55%.
This section describes how the proposed features are in-tegrated into the job search ranking function. Specifically, we apply learning-to-rank approach to learn a new ranking function combining the entity-aware features with the exist-ing ones. We first give a high level description on the exist-ing features. Then, we present an approach for extracting personalized training data from search log and the learning-to-rank algorithm used to learn a new ranking function from the training data.
The existing features are generally divided into the fol-lowing categories.
Textual features The most traditional type of features in information retrieval is textual features. These features match the keywords in queries with different sections of job descriptions, such as, title, company, etc. The key difference between these features and entity-aware matching features in Section 3 is that the former does not take into account standardized entity information when matching queries and documents.

Geographic features (personalized features) Job search on LinkedIn is highly personalized. For instance, a query like  X  X oftware developer X  from a job seeker will pro-duce very different results if the searcher is in New York City, USA as opposed to (say) Montreal, Canada. Location plays an important role in personalizing the results. We create multiple features capturing this.

Social features (personalized features) Another im-portant aspect of personalization is to capture how the re-sults socially relate to the searcher. We leverage a variety of the signals on LinkedIn, such as, how the searcher socially connects with the company posting the job, e.g., if he or she follows the company or he or she has friends working at the company, etc. to generate the features in this category. We refer the readers to [10] for a more detailed description of the existing features.
A traditional way to obtain training data is to use human experts to label the results. However, as we are dealing with a large amount training data for personalized search, it is expensive to use human experts. At the same time, it is very hard for people other than the searcher to know the true relevance judgment of the results. For example, for the same query  X  X oftware engineer X , a new college graduate in the US and an experienced candidate in Canada could be interested in very different results. Thus, similar to [13], we use log data as implicit feedback from searchers to generate training data.

One problem with the log data is position bias as the users tend to interact (e.g., click) on top results. Thus, labels in-ferred from the user actions are biased towards the ranking function generating the data. In order to counter the po-sition bias, we randomize the ranking of search results and show them to a small percentage of traffic. Then, we collect user actions on the results. For instance, as shown in Figure 6, a user issues query  X  X oftware engineer X , and six results are shown. There are different actions the user can take on the results and the corresponding ones will have differ-ent graded relevance labels based on the importance of the actions. In the example shown, the searcher clicks on the second result and decides to apply to the job at the fourth position. Since applying to a job is a stronger signal of rel-evance than clicking on a job, we assign a higher label to applied results (label=3, i.e., considered as perfect results) and a lower label to clicked results (label=1, i.e., good re-sults). For the first and third results, the searcher scans through them but chooses not to take any action. Thus, we consider that these results are irrelevant (label=0, i.e., bad results). For the results ranked below the last interacted one (below result 4 in the Figure), we cannot be certain about the relevance judgement of them since the searcher may not have looked at them. Therefore, we discard these results.
It is worth noting that when collecting the data, we count every query issued as a unique search. For instance, if the as described above. Parameter K is set to 25 , which is the number of results shown in the first result page of LinkedIn job search.
In this section, we evaluate the proposed features in both offline testing and online A/B testing.
Baseline: The baseline model uses all of the existing features described in Section 6.1 and does not contain any entity-aware features.

Entity-Aware Document-Query Matching (DQM): This model uses the existing features and entity-aware document-query matching features discussed in Section 3.
 Document-Searcher Expertise Homophily (DSH): This model uses the existing features and searcher-document expertise homophily features described in Section 4.
Entity-Faceted Historical CTR (ECTR): This model uses the existing features and entity-facted histotical CTRs described in Section 5. In this paper, we experiment with company and title facets.

All of Entity-Aware Features (ALL): This model uses the existing features and all of the entity-aware fea-tures.

All of the models are trained on the same training set and are tuned extensively on the validation set to get the optimal parameter setting for each of them.
The Figure 7 shows performance lifts of the proposed mod-els over the baseline on the test set in terms of Precision at 1 (P@1), Mean Reciprocal Rank (MRR), NDCG@15 and NDCG@25. As shown in the figure, all of the models with entity-aware features (DQM, DSH and ECTR) are consis-tently better than the baseline across all of the metrics. This confirms the benefit of using entity-aware features in job search. For instance, on NDCG@25, entity-aware document-query matching features, document-searcher expertise ho-mophily features and entity-faceted historical CTR features can improve the baseline by 1 . 2%, 5 . 4% and 4 . 8% respec-tively.

Interestingly, amongst the three feature categories, document-query matching features yield the least improve-ment and document-searcher matching features achieve the highest improvement. This emphasizes the importance of using standardized entities to personalize search results. Fi-nally, when combining all of the features together, not so sur-prisingly ALL model has the best performance. Compared to the baseline, ALL model is 20%, 12 . 1%, 8 . 3% and 7 . 9% better in terms of P@1, MRR, NDCG@15 and NDCG@25.
In our online experiment, we take the model with the best offline performance, which is the one using all of the entity-aware features and compare it with the model currently in production. The model in production uses the features in Section 6.1 and some simple entity-aware document-query matching features. The later could be viewed as a simplified version of the features described in Section 3. The model Table 2: Online A/B test results on click-through-rate and apply rate metrics. Both of the improve-ments are statistically significant. the improvements are statistically significant (details of the statistical testing is described in [26]) This re-confirms the advantage of entity-aware features in personalized job search ranking.
In this paper, we propose an approach to applying stan-dardized entity data to improve job search quality and to personalize search results. To match queries and job doc-uments, we structure the queries and the documents in a schema specific to job search domain including critical en-tity types in the domain such as job title, skill, company and location, etc. Then, we construct various features semanti-cally matching the queries and the job documents based on the structured schema. To close the gap between a query and the searcher X  X  information need and also to personalize search results, we propose a concept of expertise homphily capturing the similarity between the searcher X  X  expertise and the requirement of the job. We use standardized skills to represent searcher X  X  expertise and job expertise requirement and measure the similarity based on these skills. Finally, to model document popularity, we introduce an idea of entity-faceted historical CTRs. These capture quality of each job on different aspects such as job title, company and location, etc. Grouping job documents based on the entities does not only make the groups meaningful but also alleviates the data sparseness issue when estimating historical CTRs.

Throughout the paper, we also detail how we resolve prac-tical challenges when deploying standardized entities in job search ranking, including: ambiguity, missing values (such as members X  skills), scalability and online efficiency (espe-cially for sophisticated signals such as skill expertise scores), sparseness of historical searchers X  actions (particularly in job search domain where job lifetime is typically short), param-eter tuning for feature engineering, confounding factors and biases in log data and how to optimally combine the new features with the current ones in the existing ranking func-tion.

We conduct offline experiment on randomized data and online A/B test to understand the effectiveness of the entity-aware features. In offline experiment, a new model with the features improves over the baseline +20%, +12 . 1% and +8 . 3% in terms of Precision@1, MRR and NDCG@25, re-spectively. Online A/B test shows that the features can improve click-through-rate and apply rate, the two most important metrics of the product, +11 . 3% and +5 . 3%, re-spectively. The results confirm the benefits of the proposed features in job search. As of this writing, the new model serves all live traffic on LinkedIn job search.
 ACKNOWLEDGMENT: We would like to thank Deepak Agarwal for his valuable feedback during the course of this work. [15] H. Li. A short introduction to learning to rank. IEICE [16] T. Liu, T. Joachims, H. Li, and C. Zhai. Introduction [17] M. McPherson, L. Smith-Lovin, and J. M. Cook. [18] D. Metzler and W. B. Croft. Linear feature-based [19] E. Prud X  X ommeaux, A. Seaborne, et al. Sparql query [20] X. Shen, B. Tan, and C. Zhai. Implicit user modeling [21] D. Sontag, K. Collins-Thompson, P. N. Bennett, [22] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive [23] B. Tan and F. Peng. Unsupervised query segmentation [24] V. S. Uren, Y. Lei, and E. Motta. Semsearch: Refining [25] R. W. White, P. N. Bennett, and S. T. Dumais. [26] Y. Xu, N. Chen, A. Fernandez, O. Sinno, and [27] J. Yan, W. Chu, and R. W. White. Cohort modeling [28] J. Zhong, H. Zhu, J. Li, and Y. Yu. Conceptual graph
