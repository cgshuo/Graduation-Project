 Cristian Sminchisescu CRISMIN @ CS . TORONTO . EDU University of Toronto, Department of Computer Science, 6 King X  X  College Road, Toronto, Ontario, Canada M5S 3G4 Many successful visual tracking approaches are based on high-dimensional, physically inspired, non-linear genera-tive models of shape, intensity or motion [11, 6, 15, 18]. Although usually hard to construct, such models offer intu-itive representations, counterpoint coherence to image clut-ter and offer the analytical advantage of a global coordinate system for continuous optimization or sampling. However, despite good progress, inference in these frameworks re-mains difficult, mostly due to the lack of learning and repre-sentation adaption beyond the initial design choice. This in-flexibility leads to either high-dimensional, ill-conditioned state spaces [18], or to a lack of representational power that restricts model usage to oversimplified scenarios. The use of priors in the original state space may alleviate this problem [10, 6, 15] while conserving continuous represen-tations, but still the state space dimension (and search com-plexity) remains unchanged. Another approach is to use forms of non-linear dimensionality reduction [4, 24, 25] but then lose the global nature of the representation [4, 24] or the continuity of the generative mapping [25] that makes efficient optimization possible. In this paper, we propose an algorithm that learns reduced generative models that are global, continuous and consistent during inference. These properties are motivated as follows: ( i ) Learning non-linear low-dimensional global models re-quires a dimensionality reduction method that recovers manifolds having intrinsic curvature ( e.g . holes). These arise in many practical modeling settings, e.g . physical con-straints of an articulated figure or occlusion [8]. To preserve the local manifold geometry, we use a low-dimensional rep-resentation extracted using Laplacian eigenmaps [2] ( 2), but other methods with similar properties e.g .[14,7,26] would also apply. Estimating the intrinsic dimensionality of the model based on the Hausdorff dimension is demon-strated in 4.1. ( ii ) Continuous generative model. Continuous optimiza-tion in the low-dimensional space requires not only a re-duced global coordinate system but also a globally con-tinuous generative mapping. Assuming the original high-dimensional model is continuous, the one obtained by re-ducing its dimensionality should also be. In 2.1, we esti-mate a smooth mapping between the learned and the origi-nal model state space, based on kernel regression. Smooth-ness allows the use of efficient continuous methods for high-dimensional optimization [5, 20, 18, 19]. While we aim at dimensionality reduction, it is likely that for many complex processes, even reduced representations would still have rather large dimensionality ( e.g . 10 X 15). ( iii ) Consistent estimates require not only a prior on the probable regions of the low-dimensional manifold, as pre-dicted by the typical training data density, but also sepa-rating holes produced by insufficient sampling from gen-uine intrinsic space curvature. The inherent sparsity of high-dimensional training sets makes this disambiguation difficult. (An analysis based on the training data distri-bution usually requires restrictive sampling assumptions [16]). In 2.2 we propose an analytic solution that com-bines a smoothing Gaussian mixture, and a prior flattening method. This exploits the layered structure of our learned generative model, in order to push down sharp curvature constrains in the low-dimensional space. ( iv ) Geodesics for Interpolation: To obtain a complete low-dimensional generative model for analysis and synthesis, interpolation is also necessary. A  X  X eodesic X  cost function for this computation is given in 2.3.
 Related Work: There is important work involving track-ing using constrained generative models [11, 4, 24], but we are not aware of algorithms that allow continuous op-timization over a learned non-linear manifold. Bregler &amp; Omohundo [4] track 2D lip contours using a high-dimensional Gaussian Mixture prior (GMM) learned from training data and gradient descent. They optimize in the original high-dimensional space, and regularize the esti-mates using GMM projection. Toyama &amp; Blake [24] track 2D exemplars over a GMM index and Euclidean similari-ties using a discrete method and a set of local-coordinate system charts. Globally post-coordinating a local mixture representation of the manifold [21] would not be appli-cable for continuous optimization because the coordinates are uniquely defined only w.r.t. the considered training set. Thus, the coordinates of new configurations sampled dur-ing optimization may not be unique. Wang et al [25] use isometric embeddings [22] to restrict variations of high-dimensional 2D shape coordinate sets to low-dimensions (2d in their case) and compute local non-parametric, not necessarily continuous mappings, between their intrinsic and embedding spaces. Consider a generative model (fig. 1a) representing smooth non-linear transformations that re-produce the variability, but also the strong correlations, en-countered in some observation domain . The model is defined over an original state space , subject to prior , and has additional parameters . 1 A common difficulty with many intuitive, physically in-spired generative models like , is that they usually have too general, high-dimensional state spaces, that are diffi-cult to estimate and prior knowledge cannot be flexibly used during the model state inference. An additional dif-ficulty (in many vision problems) is caused by the non-linearity and non-convexity of the original representation space. This may be produced, e.g . by (physical) domain constraints, present in the model.
 To learn a consistent reduced model, we use Laplacian Eigenmaps [2], a non-linear embedding method that can, in principle reconstruct low-dimensional manifolds [14, 7, 26] could also be used). These algorithms recover embeddings that minimally distort the local geometry of a typical distribution from . The geometry is ap-proximated based on a training set , and the resulting embedded set of coordinates is alternative embeddings that preserve the global geometry would also apply [22]. An advantage of spectral embed-dings [22, 14, 2] is their good generalization [3]. A continuous embedded generative model (fig. 1a) can be obtained by learning the parameters of a global smooth mapping between and and by construct-ing a prior on the embedded manifold (fig. 1a).
 For consistent inference in , the prior has to reflect the data density in the training set , but also intrinsic cur-vature induced by existing priors at other layers in the gen-erative model ( and beyond). Details are given in the fol-lowing sections. 2.1. Globally Smooth Generative Mappings The construction of the learned generative model requires the estimation of a forward mapping based on points in the training set in (stored column-wise in a matrix ) and corresponding points in the embedded space (stored in a matrix ). Consider a row operator that extracts the -th row of a matrix and the corresponding column operator. We employ a sparse kernel regressor and estimate mappings from tant for efficient low-dimensional generative models. Con-sider representatives , , and kernels tors in map to the dimension in is , where map into dimension and trix of size x , where is the dimension of the train-ing set. The parameter vector is thus .
 Consequently, and the mapping can be derived as: pseudo-inverse of , computed once for all mappings and . 3 Differentiation of the generative mapping to second order for continu-ous optimization can be obtained using the chain rule and the derivation of the Jacobian of : . 2.2. Embedded and Layered Generative Priors Consistent inference in the embedded space requires a prior over the probable regions of the low-dimensional manifold , determined by the training data density. Here we use a mixture prior , where are Gaussian functions with parameters obtained by -means clustering the embedded training set [12]. 4 Sampling artifacts and problem domain constraints may in-teract in a way that is difficult to separate in . In particular, the constraints may generate unfeasible regions having in-trinsic curvature. Geometrically these will be holes, in both and in . For human kinematic representations based on joint angles , the intrinsic curvature is produced by the limits of articulations and by the body non-self intersection constraints. These exclude certain state variable combina-tions (see also 4). While for many domain models, ana-lytic characterizations of unfeasible regions may be avail-able (in ), directly separating sampling artifacts from in-trinsic curvature in is nearly impossible, under general, unrestrictive, sampling assumptions. The reason is that one cannot assume that e.g . the training data available in has been sampled uniformly and / or densely from the unknown [16], and the prior is simply blind to such effects ( i.e . it smooths them). In fact, it may assign unfeasible regions a moderately high probability, especially if these are sur-rounded by densely sampled zones.
 Because the learned model is layered, sharper curva-ture constraints may be induced in the embedded space by existing priors in the original representation space, where these may be available in simple analytic form. For a lay-ered continuous generative model , one can exploit the modular structure of its forward transformation chain. Since evaluation and differentiation of with respect to its state variables is the main computational machinery of the model, analytic forms for intermediate function val-ues and derivatives on the generative transformation chain are available. For a two-layer embedded-embedding model slice with , and pri-ors and respectively, we combine the dis-tribution over probable regions in with flattened priors from the embedding space : prior is not normalized and it requires a state-dependent Ja-cobian scaling factor. Analytically differentiating is possible, given , and the parametric form of the mapping in the embedded space (see 3). Priors at subsequent layers can be discarded, being already absorbed in . 2.3. Geodesics for Interpolation The construction of geodesics can be framed as optimal inference where we synthesize a trajectory that is smooth and consistent with the prior on the manifold . As-sume a trajectory with endpoints , and its discretization with knots . The en-ergy function for geodesics can be written as: ference operator square matrix of dimension [ x ] con-sisting of band-diagonal blocks of -dimensional iden-tity matrices . Priors encoding higher degree of smoothness can be obtained by self-multiplication, e.g . for second order as , etc . The function is differentiable and can be sampled or optimized for a lo-cal MAP solution from a trivial initialization ( e.g . points ize using Floyd X  X  dynamic programming algorithm (DP). This is run off-line to find all shortest paths on the set of mixture centers obtained from clustering (see 2.2). This roadmap can be effectively used at geodesic query time: given known endpoints, link to the closest mixture component at each end and use the precomputed road (see fig. 2(d) for an oriented bounded box decomposition used in nearest neighbor queries). The DP trajectory is then refined using the consistent geodesic function . We apply Bayes rule to compute the  X  X tatic X  total poste-rior probability over the learned manifold space given (data) observation : . Here, the observation likelihood, that can be computed in terms of , the probability of observa-tion as predicted by the generative model feature at configuration (see fig. 1a). For tracking using dynamic observations, the prior at time combines the previous pos-terior and the dynamics , where we have collected the observations at time into vector becomes: , where gether, and form the time prior the propagating density, we use Covariance Scaled Sam-pling (CSS) [18]. This probabilistic method represents the posterior distribution of hypotheses in state space , as a Gaussian mixture, whose weights, centers and covari-ances are obtained as follows. Random samples are gener-ated from the temporal prior , and each is opti-mized by nonlinear local optimization (respecting any prior constraints, etc .) to maximize the local posterior likelihood encoded by . The optimized likelihood value and position gives the weight and center of a new component, and the inverse Hessian of the log-likelihood gives a scale matrix that is well adapted to the contours of the cost func-tion, even for very ill-conditioned problems like monocular human tracking. The likelihood and temporal prior distribu-tions are then composed and pruned to a maximum number of mixture components, in order to produce the posterior Representation Learning is based on a physically in-spired 3D body model that consists of a kinematic  X  X kele-ton X  of articulated joints controlled by angular joint vari-ables, covered by a  X  X lesh X  built from superquadric ellip-soids with deformations. The model has internal propor-tions, shape and surface color parameters . The state space consists of 29 joint angle variables (for shoulder, elbow, hip, knee joints, etc .) and 6d global rigid motion variables , encoded in the state . We learn a low-dimensional rep-resentation for training vector slices of , that do not include the rigid components , using manifold em-bedding on a set of body joint angle training data, obtained with a motion capture system (courtesy of the motion cap-ture database at the CMU graphics laboratory [1]). We es-timate a mixture model for by -means clustering the embedded eigenvectors, to build the prior . We also learn the parameters of a forward mapping into the original joint angle space using Gaussian kernel regression. In use, model superquadric surfaces are discretized into 2D meshes and the mesh nodes (and their colors, updated after each tracked image, e.g . by texture mapping) are mapped to 3D points using knowledge of the kinematic state variables predicted at configuration by . These map to each body kinematic chain and then predict image positions and pixel colors, using perspective image projection, transfor-mations that are all encoded in 6 . The Observa-tion Model is based on sums of predicted-to-image match-ing likelihoods (and their gradient and Hessian metrics) evaluated for each model feature prediction . As image features, we use a robust combination of intensity-based alignment metrics, silhouettes and robustified normalized edge distances [18]. Flattened Embedded Priors consist of soft joint angle limits and body non self-intersection con-straints [18]. For the experiments here, we work with the negative log-likelihood energy function in 3 and the prior is not normalized and not scaled. For temporal state infer-ence (tracking), we use CSS [18], as explained in 3. 4.1. Experiments The experiments we show include image-based visual tracking of human activities in monocular video. This un-derlines the importance of using prior knowledge because often the motion of subsets of body limbs is unobserved for long periods, e.g . when a tracked subject is sideways or not facing the camera. However, information about unobserved variables is present indirectly in the observed ones and this constrains their probability distribution. Learning a global, non-linear, low-dimensional representation, produces a model that couples the state variables. We derive models based on various training datasets, including walking, running and human interaction (gestures in conversations). Analysis of the walking manifold involves a corpus of 2500 frames coming from 5 subjects, and thus contains significant variability. Fig. 2 shows walking data analysis and various structures necessary for optimization. Fig. 2(a) (left) gives estimates of the data intrinsic dimensionality based on the Hausdorff dimension , where is the radius of a sphere centered at each point, and are the number of points in that neighborhood (the plot is averaged over many nearby points). The slope of the curve in the linear domain corresponds roughly to a 1d hypothesis. Fig. 2(b) plots the embedding distortion, computed as the normalized Euclidean SSE over each neighborhood in the training set graph. Notice its sta-bility across different neighborhood sizes, and contrast it with the larger distortion of more variate training sets, in fig. 5(c). Fig. 2(c) and fig. 2(d) show embeddings into 2d and 3d. The latter representation is more flexible, and al-lows more variability. The results correspond to spherical neighborhood sizes of and Gaussian standard de-viation . The figures show the embedded manifold as defined by the GMM prior (3 stdev). Notice the shape has similarities with the position-velocity plot of a harmonic oscillator. Fig. 2(d) shows the spatial decomposi-tion of the data based on oriented bounding boxes OBB [9]. This is used for fast nearest-neighbor queries in geodesic calculations ( 2.3). The embedded generative model used for tracking is based on a forward mapping ( 2.1) that has 500 kernels.
 The image based tracking of walking is based on 2s of video of a subject moving against a cluttered background in a monocular sequence (fig. 3). We use a 9d state model con-sisting of a 3d embedded coordinate (for the 2500 walking dataset above) ( ) + 6d rigid motion ( ). and track using CSS with 5 hypotheses. Aside from clutter, the sequence is difficult due to the self-occlusion of the left side of the body. This occasionally makes the state variables associated to the invisible limbs close to singular. While singularity can be artificially resolved with stabilization priors, the more se-rious problem is that without prior knowledge, the related state variables would be mistracked, thus making recovery from failure extremely unlikely. Also notice the elimination of timescale dependence present in classical dynamic pre-dictive models. The manifold is traversed at a speed driven by image evidence, as opposed to a prespecified one. Embedded vs. original model comparison for walking in fig. 4 is based on 60 frames of left out test motion capture data, synthesized using the articulated 3D model. We se-lect 15 (3D) joint positions (shoulders, hips, elbows, etc .), perturb them with 1cm spherical noise to simulate model-ing errors and project them onto a virtual monocular cam-era image plane (440x358 pixels). This input data is used to define a SSD reprojection error (Gaussian likelihood), for body joints. We track with 2 hypotheses, using both the 35d original model (having joint angle limit and body non self-intersection priors) and the 9d embedded walking model. The left and middle figures 4(a), (b) show the aver-age pixel reprojection error per joint, whereas fig. 4(c) gives the average joint angle error with respect to ground truth (for the embedded model we plot the estimated radi-ans , average range of uncertainty of the kernel regres-sor with errorbars). Both models maintain track, but the original one overfits the data, leading to low reprojec-tion errors, but larger variance in joint angle estimates. This is caused by tracks that follow equivalent class (monocular reflective) neighboring minima w.r.t. ground truth, more clearly noticeable at the beginning and the end of the se-quence. The region between the frames 40-60 corresponds to moments where the model puppet is situated sideways in straight-stand positions with respect to the camera ray of sight. The accuracy of the original model improves during this period, perhaps because some of the depth ambiguities are eliminated due to physical constraints. The embedded model is biased for walking and has thus larger reprojec-tion error but significantly smaller 3D variance, having the error rather uniformly distributed among its joint angles. The average error in fig. 4(c) is about , and the maxi-mum error during tracking was in one left hip joint an-gle. The original model tends to have large localized errors caused by reflective ambiguities at particular limbs. The av-erage error in fig. 4(c) is about , but the maximum error was in one right shoulder joint angle. For the limited computational resources used, and for the limited walking task, the learned embedded model is clearly more accurate. Analysis of the running, walking and human interaction manifold is illustrated in fig. 5 where we show a 600 point training set consisting of samples drawn from an activity set consisting of walks, runs and conversations. Left plots in fig. 5(a),(b) show 3d projections of neighborhood graphs ( ) for 6d and 5d embeddings onto their 3 lead-ing Laplacian eigenvectors. Note that the the submanifolds of these activities mix, therefore pathways between these are probable (this can be also qualitatively checked by con-nected component analysis in the training set graph). Cir-cular structures related to periodic walks and runs are less observable for 5d embeddings but are more clearly visible for 6d ones. The plot in fig. 5(c) confirms that the embed-ded neighborhood distortion decreases monotonically with increasing dimension. In practice, the stability of optimiza-tion in the embedded space becomes satisfactory beginning at about 5-6d, ruling out the use of very low-dimensional 2-4d models. The performance of the optimizer is based on both the latent space structure, and the accuracy of the map-ping . Indeed, we found that the constrained topology of low-dimensional spaces (2-4d) collapses data from em-bedded runs and walks into nearly overlapping cycles (not shown), and this leads to estimation instability. In fig. 5(d) we show the good accuracy of a mapping (based on 100 kernels) from the 6d embedded data in fig. 5(a) into the original 29d training set.
 Tracking of human activities is exemplified in fig. 6 where we analyze a 5s video using a 12d model consisting of 6d rigid state + 6d embedded coordinate obtained from a 9000 element training set consisting of 2000 walking, 2000 running and 5000 human interaction samples. The 6d-29d mapping is based on 900 kernels. Fig. 6 shows snap-shots from the original sequence together with image-based tracking and monocular 3D reconstructions of the most probable configurations rendered from a synthetic scene viewpoint. The algorithm tracks and reconstructs 3D mo-tion with good accuracy using 7 hypotheses. Missing data resulting from frequent occlusion / disocclusion of limbs would make monocular tracking with quasi-global cost sen-sitive search [18] or optima enumeration methods [19], alone difficult without prior-knowledge, or at least a sophis-ticated image-based limb detector. On the other hand, the presence of multiple activities and complex scenarios of hu-man interaction demands a flexible learned representation, and makes dedicated dynamic predictors ( e.g . walking, run-ning) [6, 15] difficult to apply. In fig. 7 we show various components failure modes. Fig. 7(a),(b) shows the behavior of the system in a run that does not use the flattened embed-ded priors for physical constraints. Indeed, these are useful  X  notice unfeasible configurations of the right hand inside the back and right upper-arm inside the torso. The effects of missing training data on tracking behavior are explored in fig. 7(c)-(f) where an embedded model computed with-out conversation training data is used to track the sequence. The model tracks the first part of the sequence and the be-ginning of the conversation, but eventually looses lock of the arms when the gestures deviate significantly from the training set. We have presented a learning and inference framework that reduces visual tracking to low-dimensional spaces com-puted using non-linear embedding. Because existing ap-proaches to optimization over learned, constrained gener-ative representations are based on only locally valid mod-els, they can X  X  easily exploit both the convenience of low-dimensional modeling and the one of efficient continuous search. Therefore they may operate either discretely or in hybrid non-convergent regimes. To address these dif-ficulties, we introduce a layered generative model having learned, embedded representation, that can be estimated us-ing efficient continuous optimization methods. We analyze the structure of reduced manifold representations for a va-riety of human walking, running and conversational activi-ties, and demonstrate the algorithm by providing quantita-tive and qualitative results of human tracking and 3D mo-tion reconstruction based on learned low-dimensional mod-els, in monocular video.
 Future and ongoing work will explore the construction of flexible dynamic predictors for tracking, low-dimensional shape representations, and activity recognition.
 Acknowledgments Special thanks to Kyros Kutulakos and Nigel Morris for generous help with video capture.
