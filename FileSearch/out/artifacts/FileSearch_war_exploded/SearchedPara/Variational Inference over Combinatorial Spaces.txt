 The framework we propose is applicable in the following setup: let C denote a combinatorial space , by which we mean a finite but large set, where testing membership is tractable, but enumeration is not, and suppose that the goal is to compute P x  X  X  f ( x ) , where f is a positive function. This setup subsumes many probabilistic inference and classical combinatorics problems. It is often intractable to compute this sum, so approximations are used.
 is larger than C , but paradoxically it is often possible to find such a decomposition where for each i , P x  X  X  i f ( x ) is tractable. We give many examples of this in Section 3 and Appendix B. describes an effective way of using this type of decomposition to approximate the original sum. Another way of viewing this setup is in terms of exponential families. In this view, described in detail in Section 2, the decomposition becomes a factorization of the base measure. As we will show, the exponential family view gives a principled way of defining variational approximations. In order to make variational approximations tractable in the combinatorial setup, we use what we call an implicit message representation . The canonical parameter space of the exponential family enables such representation. We also show how additional approximations can be introduced in cases where the factorization has a large number of factors. These further approximations rely on an outer bound of the partition function, and therefore preserve the guarantees of convex variational objective functions.
 While previous authors have proposed mean field or loopy belief propagation algorithms to approx-imate the partition function of a few specific combinatorial models X  X or example [7, 8] for parsing, and [9, 10] for computing the permanent of a matrix X  X e are not aware of a general treatment of variational inference in combinatorial spaces.
 There has been work on applying variational algorithms to the problem of maximization over combi-natorial spaces [11, 12, 13, 14], but maximization over combinatorial spaces is rather different than summation. For example, in the bipartite matching example considered in both [13] and this paper, there is a known polynomial algorithm for maximization, but not for summation. Our approach is also related to agreement-based learning [15, 16], although agreement-based learning is defined within the context of unsupervised learning using EM, while our framework is agnostic with respect to parameter estimation.
 The paper is organized as follows: in Section 2 we present the measure factorization framework; in Section 3 we show examples of this framework applied to various combinatorial inference problems; and in Section 4 we present empirical results. In this section, we present the variational measure factorization framework. At a high level, the first step is to construct an equivalent but more convenient exponential family. This exponential family will allow us to transform variational algorithms over graphical models into approximation algo-rithms over combinatorial spaces. We first describe the techniques needed to do this transformation in the case of a specific variational inference algorithm X  X oopy belief propagation X  X nd then discuss mean-field and tree-reweighted approximations.
 To make the exposition more concrete, we use the running example of approximating the value and gradient of the log-partition function of a Bipartite Matching model (BM) over K N,N , a well-known #P problem [17]. Unless we mention otherwise, we will consider bipartite perfect matchings; non-bipartite and non-perfect matchings are discussed in Section 3.1. The reader should keep in mind, however, that our framework is applicable to a much broader class of combinatorial objects. We develop several other examples in Section 3 and in Appendix B. 2.1 Setup Since we are dealing with discrete-valued random variables X , we can assume without loss of generality that the probability distribution for which we want to compute the partition function and moments is a member of a regular exponential family with canonical parameters  X   X  R J : for a J -dimensional sufficient statistic  X  and base measure  X  over F = 2 X , both of which are assumed (again, without loss of generality) to be indicator functions :  X  j ,  X  : X  X  { 0 , 1 } . Here X is a superset of both C and all of the C i s. The link between this setup and the general problem of computing P x  X  X  f ( x ) is the base measure  X  , which is set to the indicator function over C :  X  ( x ) = 1 [ x  X  X  ] , where 1 [  X  ] is equal to one if its argument holds true, and zero otherwise. equal to the expectation of the sufficient statistic  X  j under the exponential family with base measure  X  [5]). We want to exploit situations where the base measure can be written as a product of I measures  X  ( x ) = Q I i =1  X  i ( x ) such that each factor  X  i : X  X  { 0 , 1 } induces a super-partition typically done using dynamic programming (DP). We also assume that the gradient of the super-partition functions is tractable, which is typical for DP formulations.
 In the case of BM, the space X is a product of N 2 binary alignment variables, x = x the sufficient statistic takes the form  X  j ( x ) = x m,n . The measure factorization we use to enforce the matching property is  X  =  X  1  X  2 , where: and similarly for the denominator: After plugging in the reparameterization of the numerator and denominator back into the logit function in Equation (5) and doing some algebra, we obtain the more efficient update  X  i,j = the logit function applied to each entry of the vector v . See Figure 1 for a summary of the BPMF algorithm. 2.5 Other variational algorithms The ideas used to derive the BPMF updates can be extended to other variational algorithms with minor modifications. We sketch here two examples: a naive mean field algorithm, and a TRW approximation. See Appendix A.2 for details.
 In the case of naive mean field applied the graphical model described in Section 2.2, the updates take a form similar to Equations (4), except that the reverse incoming message is not omitted when computing an outgoing message. As a consequence, the updates are not directional and can be associated to nodes in the graphical model rather than edges: This yields the following implicit updates: 5 and the moment approximation  X   X  = logistic(  X  ) .
 In the case of TRW, lines 3 and 6 in the pseudocode of Figure 1 stay the same, while the update in line 4 becomes: where  X  i  X  j are marginals of a spanning tree distribution over K I,J . We show in Appendix A.2 how the idea in Section 2.4 can be exploited to reuse computations of super-partition functions in the case of TRW as well. 2.6 Large factorizations In some cases, it might not be possible to write the base measure as a succinct product of factors. Fortunately, there is a simple and elegant workaround to this problem that retains good theoretical guarantees. The basic idea is that dropping measures with domain { 0 , 1 } in a factorization can only increase the value of the partition function. This solution is especially attractive in the context of outer approximations such as the TRW algorithm, because it preserves the upper bound property of the approximation. We show an example of this in Section 3.2. In this section, we show three examples of measure factorizations. See Appendix B for two more examples (partitions of the plane, and traveling salesman problems). Figure 2: (a) An example of a valid multiple alignment between three sequences. (b) Examples of invalid 3.1 More matchings Our approach extends naturally to matchings with higher-order (augmented) sufficient statistic, and to non-bipartite/non-perfect matchings. Let us first consider an Higher-order Bipartite Model (HBM), which has all the basic sufficient statistic coordinates found in SBM, plus those of the form  X  ( x ) = x m,n  X  x m +1 ,n +1 . We claim that with the factorization of Equation (2), the super-partition functions A 1 and A 2 are still tractable in HBM. To see why, note that computing A 1 can be done by building an auxiliary exponential family with associated graphical model given by a chain of cient statistic coordinates  X  j ( x ) = x m,n are encoded as node potentials, and the augmented ones as edge potentials in the chain. This yields a running time of O ( N 3 ) for computing one super-partition function and its gradient (see Appendix A.3 for details). The auxiliary exponential family technique used here is reminiscent of [21].
 Extension to non-perfect and non-bipartite matchings can also be done easily. In the first case, a dummy  X  X ull X  node is added to each bipartite component. In the second case, where the original space is the set of N 2 alignment indicators, we propose a decomposition into N measures. Each 3.2 Multiple sequence alignment We start by describing the space of pairwise alignments (which is tractable), and then discuss the extension to multiple sequences (which quickly becomes infeasible as the number of sequences increases). Consider two sequences of length M and N respectively. A pairwise sequence alignment is a bipartite graph on the characters of the two sequences (where each bipartite component has the characters of one of the sequences) constrained to be monotonic : if a character at index m  X  m is aligned to index n 0 , then we must have n 0 &gt; n . A multiple alignment between K sequences of of the k -th sequence, and such that the following three properties hold: (1) each pair of components forms a pairwise alignment as described above; (2) the alignments are transitive , i.e., if character c is aligned to c 2 and c 2 is aligned to c 3 then c 1 must be aligned to c 3 ; (3) the alignments satisfy a partial order property: there exists a partial order p on the connected components of the graph with the property that if C 1 &lt; p C 2 are two distinct connected components and c 1  X  C 1 , c 2  X  C 2 are in the same sequence, then the index of c 1 in the sequence is smaller than the index of c 2 . See Figure 2(a,b) for an illustration.
 We use the technique of Section 2.6, and include only the pairwise alignment and transitiv-ity constraints, creating a variational objective function that is an outer bound of the origi-nal objective. In this factorization, there are K 2 pairwise alignment measures, and T = P sages for one iteration can be computed in time O ( T ) . 3.3 Linearization of partial orders A linearization of a partial order p over N objects is a total order t over the same objects such that x  X  p y  X  x  X  t y . Counting the number of linearizations is a well-known #P problem [22]. Equivalently, the problem can be view as a matching between a DAG G = ( V, E ) and the integers { 1 , 2 , . . . , N } with the order constraints specified on the edges of the DAG.
 straightforward generalization of the algorithm used to compute HBM X  X  super-partition can be used. This generalization is simply to use sum-product with graphical model G i instead of sum-product on a chain as in HBM (see Appendix A.5 for details). Again, the state space of the node of the of the current forest. 4.1 Matchings As a first experiment, we compared the approximation of SBM described in Section 2 to the Fully Polynomial Randomized Approximation Scheme (FPRAS) described in [23]. We performed all our experiments on 100 iid random bipartite graphs of size N , where each edge has iid appearance prob-ability p , a random graph model that we denote by RB ( N, p ) . In the first and second experiments, we used RB (10 , 0 . 9) . In this case, exact computation is still possible, and we compared the mean Root Mean Squared (RMS) of the estimated moments to the truth. In Figure 3(a), we plot this quantity as a function of the time spent to compute the 100 approximations. In the variational approximation, we measured performance at each iteration of BPMF, and in the sampling approach, we measured performance after powers of two sampling rounds. The conclusion is that the variational approxi-regime.
 Next, we show in Figure 3(b) the behavior of the algorithms as a function of p , where we also added the mean field algorithm to the comparison. In each data point in the graph, the FPRAS was run no less than one order of magnitude more time than the variational algorithms. Both variational strategies outperform the FPRAS in low-density regimes, where mean field also slightly outperforms BPMF. On the other hand, for high-density regimes, only BPMF outperforms the FPRAS, and mean field has a bias compared to the other two methods.
 The third experiment concerns the augmented matching model, HBM. Here we compare two types of factorization and investigate the scalability of the approaches to larger graphs. Factorization F1 is a simpler factorization of the form described in Section 3.1 for non-bipartite graphs. This ignores the higher-order sufficient statistic coordinates, creating an outer approximation. Factorization F2, Table 1: Average SP scores in the ref1/test1 directory of BAliBASE. BPMF-i denotes the average SP of the described in Section 3.1 specifically for HBM, is tighter. The experimental setup is based on a gen-erative model over noisy observations of bipartite perfect matchings described in Appendix C.2. We show in Figure 3(c) the results of a sequence of these experiments for different bipartite component sizes N/ 2 . This experiments demonstrates the scalability of sophisticated factorizations, and their superiority over simpler ones. 4.2 Multiple sequence alignment To assess the practical significance of this framework, we also apply it to BAliBASE [6], a standard protein multiple sequence alignment benchmark. We compared our system to Clustal 2.0.12 [24], the most popular multiple alignment tool, and ProbCons 1.12, a state-of-the-art system [25] that also relies on enforcing transitivity constraints, but which is not derived via the optimization of an objec-tive function. Our system uses a basic pair HMM [26] to score pairwise alignments. This scoring function captures a proper subset of the biological knowledge exploited by Clustal and ProbCons. 6 The advantage of our system over the other systems is the better optimization technique, based on the measure factorization described in Section 3.2. We used a standard technique to transform the pairwise alignment marginals into a single valid multiple sequence alignment (see Appendix C.3). Our system outperformed both baselines after three BPMF parallel message passing iterations. The algorithm converged in all protein groups, and performance was identical after more than three itera-tions. Although the overall performance gain is not statistically significant according to a Wilcoxon signed-rank test, the larger gains were obtained in the small identity subset, the  X  X wilight zone X  where research on multiple sequence alignment has focused.
 One caveat of this multiple alignment approach is its running time, which is cubic in the length of the longest sequence, while most multiple sequence alignment approaches are quadratic. For exam-ple, the running time for one iteration of BPMF in this experiment was 364.67s, but only 0.98s for Clustal X  X his is why we have restricted the experiments to the short sequences section of BAliBASE. Fortunately, several techniques are available to decrease the computational complexity of this algo-rithm: the transitivity factors can be subsampled using a coarse pass, or along a phylogenetic tree; and computation of the factors can be entirely parallelized. These improvements are orthogonal to the main point of this paper, so we leave them for future work. Computing the moments of discrete exponential families can be difficult for two reasons: the struc-ture of the sufficient statistic that can create junction trees of high tree-width, and the structure of the base measures that can induce an intractable combinatorial space. Most previous work on vari-ational approximations has focused on the first difficulty; however, the second challenge also arises frequently in machine learning. In this work, we have presented a framework that fills this gap. It is based on an intuitive notion of measure factorization, which, as we have shown, applies to a variety of combinatorial spaces. This notion enables variational algorithms to be adapted to the combinatorial setting. Our experiments both on synthetic and naturally-occurring data demonstrate the viability of the method compared to competing state-of-the-art algorithms.
