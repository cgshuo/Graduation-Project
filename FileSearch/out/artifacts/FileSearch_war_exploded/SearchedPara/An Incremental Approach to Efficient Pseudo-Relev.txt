 Pseudo-relevance feedback is an important strategy to im-prove search accuracy. It is often implemented as a two-round retrieval process: the first round is to retrieve an ini-tial set of documents relevant to an original query, and the second round is to retrieve final retrieval results using the original query expanded with terms selected from the previ-ously retrieved documents. This two-round retrieval process is clearly time consuming, which could arguably be one of main reasons that hinder the wide adaptation of the pseudo-relevance feedback methods in real-world IR systems.
In this paper, we study how to improve the efficiency of pseudo-relevance feedback methods. The basic idea is to re-duce the time needed for the second round of retrieval by leveraging the query processing results of the first round. Specifically, instead of processing the expand query as a newly submitted query, we propose an incremental approach, which resumes the query processing results (i.e. document accumulators) for the first round of retrieval and process the second round of retrieval mainly as a step of adjusting the scores in the accumulators. Experimental results on TREC Terabyte collections show that the proposed incremental ap-proach can improve the efficiency of pseudo-relevance feed-back methods by a factor of two without sacrificing their effectiveness.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Search process, Relevance feed-back General Terms: Performance; Experimentation Keywords: Pseudo-relevance Feedback; Incremental Ap-proach; Efficiency; Query Expansion
Pseudo-relevance feedback is an important technique that can be used to improve the effectiveness of IR systems. Dif-ferent pseudo-relevance feedback methods have been pro-posed and studied for various retrieval models [8, 19, 26 X 28, 37, 41], but they all boil down to the problem of expanding an original query with useful term selected from a certain number of pseudo-relevant documents, i.e., top-ranked doc-uments from an initial retrieval run. In particular, the im-plementation of pseudo-relevance feedback methods always require two rounds of retrieval, with the first round retriev-ing an initial set of documents with respect to an original query and the second round retrieving documents based on an expanded query, which is generated by expanding the original query with relevant terms selected from the previ-ously retrieved document set.

Although pseudo-relevance feedback methods can lead to large performance gain in terms of effectiveness, there is one major downside that limits their usability in real retrieval application: its low efficiency [6, 13, 18]. In particular, the second round of retrieval could significantly slow down the performance due to the time spent on selecting terms for expansion and on executing the expanded query that are often much longer than the original one [13, 18]. It is clear that such drastically increased execution cost limits the ap-plicability of pseudo-relevance feedback methods in real IR applications.

Compared with continual efforts on improving the effec-tiveness and robustness of pseudo feedback methods in the past decades [8, 10, 11, 14, 15, 19 X 21, 21, 22, 24, 26 X 28, 32, 37, 38, 41], less attention has been paid to make these methods more efficient [6, 13, 18]. Billerbeck and Zobel [6] proposed a summary based method for efficient term selection from the feedback methods. Lavrenko and Allan [18] proposed a fast relevance model, and Cartright et. al. [13] studied approx-imation methods to reduce the storage and computational cost for the fast relevance model. These proposed methods were designed for either a specific feedback method [13, 18] or a bottleneck in the feedback implementation, i.e., how to efficiently select expansion terms from the feedback doc-uments [6]. However, it remains unclear whether there is a general solution to address other bottlenecks for efficient pseudo-relevance feedback implementation.

In this paper, we propose a general solution that can improve the efficiency of pseudo-relevance feedback meth-ods. The basic idea is to reduce the execution cost for the expanded query by using the the query processing sta-tus of the original query. Existing IR systems often imple-ment the pseudo-feedback methods as a two-round retrieval, where the second round is processed independently to the first round. However, since the query used in the second round retrieval is an expanded version of the original query used in the first round, it would be beneficial to leverage the results of the first round of retrieval to reduce the query processing time for the second round. Although this is an intuitive idea, we are not aware of any previous work that successfully implemented the idea. Experimental results on TREC Terabyte collections show that the proposed strat-egy can improve the efficiency by a factor of 2 to 4 without sacrificing the effectiveness.

The remainder of this paper is organized as follows. We first discuss related work on pseudo-relevance feedback in Section 2, and then provide background knowledge about indexing and query processing in Section 3. We then de-scribe the motivation and details of the proposed incremen-tal approach in Section 4. We explain the experiment set up, present experiment results and summarize our findings in Section 5. Finally, we conclude and discuss the future work in Section 6.
Pseudo-relevance feedback is an effective technique for improving retrieval accuracy. The commonly used feed-back method for vector space models is the Rocchio algo-rithm [27,28]. In classical probabilistic models, the feedback method is based on the Robertson/Sparck-Jones weight-ing [26]. Two representative feedback methods for the lan-guage modeling approaches are the relevance model [19] and the model-based feedback [41].

Despite the difference in selecting expansion terms (i.e., estimating relevance models in language modeling frame-work), all these methods share similar implementation strate-gies, i.e., initial retrieval to find documents relevant to an original query; term selection to identify useful expansion terms from the feedback documents; and second-round re-trieval to return documents relevant to the expanded query. This commonality in implementation makes it possible to derive a general optimization strategy that can improve the efficiency of these pseudo-relevance feedback methods, which is the focus of our paper.

More recently, continuous efforts have been put on im-proving the effectiveness and robustness of the pseudo-feedback methods [8, 10, 11, 14, 15, 20 X 22, 24, 32, 38]. For example, Tao and Zhai [32] extended model-based feedback methods using a regularized EM algorithm to reduce the parameter sensi-tivity, and Lv and Zhai [21] extended the relevance model to exploit term proximity.
Compared with the effectiveness of an IR system, its ef-ficiency is equally important. In particular, high query la-tency can decrease user satisfaction significantly [23]. Un-fortunately, only a few previous studies focused on efficient pseudo-relevance feedback methods [5, 6, 13, 18].

Billerbeck and Zobel [5,6] examined the bottlenecks of the pseudo-relevance feedback implementation and proposed to leverage tf-idf document summaries to reduce the cost of se-lecting expansion terms from the feedback documents. They also explored other approaches such as query association, us-ing reduced-size collections and carrying accumulator infor-mation from the first round of retrieval to the second round, but all of them were unsuccessful. Our approach is in the same line of the third unsuccessful strategy they mentioned in the paper. However, we reach a different conclusion with our implementation, i.e., our method can improve the ef-ficiency without obvious sacrificing the effectiveness. The main reason behind this fact is that our method allows the expanded terms to incrementally evaluate the accumulators in the initial retrieval rather than merely re-rank the top documents of the initial runs.

Lavrenko and Allan [18] proposed a fast relevance model which computes the original relevance-based ranking based on the cross-entropy between two documents (i.e., one rep-resenting the relevance model and the other representing the document to be scored). The re-arrangement used in the derivation makes it possible to shift the main computa-tional cost from the query processing time to indexing time. Their experiment results show that the new method can re-duce the query execution time significantly. However, the downside is the time taken to compute the document simi-larity matrix could be rather long (e.g., 90 hours for a small collection), which could makes it impractical for larger col-lections. Cartright et. al. [13] took one step further and propose approximation methods to reduce the storage and computational cost of this offline computing. Since these methods are specifically designed for relevance models, it remains unclear whether similar strategy can be leveraged for other feedback methods.

Cartright and Allan [12] have focused on optimizing the efficiency for interpolating subqueries. In particular, they investigated four cases of such an interpolation with varying amounts of accessibility for the original queries and expan-sion terms, and then proposed methods for each case. Our problem formulation is similar to the case when having ac-cess to both original queries and expansion terms, and our proposed approach bears similarity to the proposed Rewind algorithm. However, we use a different query processing technique, i.e,. score at a time (SAAT), from the one used in the previous study, i.e., document at a time (DAAT). Since SAAT makes it easier to resume the ranking status and adjust the document scores based on new terms, the improvement of our results is more significant than observed in the previous study.

Pseudo-relevance feedback is a specific type of query ex-pansion methods where expansion terms are selected from highly-ranking documents. We now describe a couple of efforts on efficient query expansion [33, 35]. Theobald et. al. [33] proposed an optimization strategy for query expan-sion methods that are based on term similarities such as those computed based on WordNet. Their pruning method was designed based on a similarity based scoring function, and it remains unclear how to generalize it to other query expansion methods. Wang et. al. [35] proposed a solution for efficient query expansion for advertisement search. They focused on solving a domain specific challenge, i.e., the place-ment of similar bid phrases. The proposed strategies in these studies are very problem-specific, and can not be directly applied to pseudo-relevance feedback methods.
Most current IR systems are based on inverted indexes [42]. An inverted index contains one inverted list for each term in the document collection. An inverted list of a term contains a list of postings, and each posting describes the information of a document containing the term such as doc-ument ID and the term occurrence in the document. Query processing involves traversing the inverted lists of query terms, computing the relevance score for each document based on a retrieval function and then ranking the documents based on their scores.

Significant efforts have been made on making this process more efficient over the past decade [1 X 4,7,9,17,25,29,30,42]. Due to the limited space, we mainly provide some back-ground about a few influential studies including those used in our system [1 X 4, 25, 29, 30].
Traditional inverted indexes are document sorted indexing , where the postings of an inverted list are sorted based on the document IDs [36]. The inverted lists of common terms could consist a huge number of postings. To enable faster access to the lists, various compression techniques such as delta encoding for document IDs have been used [42]. As the document IDs are usually sorted based on an increasing order, the delta codes are usually much smaller than the original IDs. It is therefore possible to reduce the space usage by storing delta codes instead of the original document IDs. Indexing compression not only saves the usage of disk and memory, but also improves the efficiency by minimizing cache/memory misses.

Another commonly used indexing organization strategy is impact-sorted indexing , where the postings are sorted based on their impact, i.e., their contribution to the relevance score of a document [1 X 4, 29]. With the impact-sorted indexing, the retrieval score of a document D for query Q can be eval-uated by summing up the impact scores of D in the inverted lists of all the terms in Q . To further improve the efficiency, the impact scores are binned into a smaller number of dis-tinct values and only the binned integer values are stored in the index. This strategy brings two benefits. First, the impact score can be stored using a very small number of bits (e.g., 6 bits for 64 distinct binned integers) instead of using 32 or 64 bits for a floating point variable. Second, with the binned scores, the number of documents with the same score is pretty large so that we can separate an inverted list into segments, one for each distinct value of the impact scores. Within each segment, documents are then ranked based on their document IDs, and the compression techniques used for document-sorted indexing can be applied here to achieve high level of compression. Formally, the retrieval score of document D for query Q is computed as follows, where B w,D is the binned integer score representing the im-pact of query term w in D . Previous studies have shown that such binning strategy would not significantly affect the retrieval accuracy [3, 29].

Skipping is another mechanism for fast accessing the in-formation from the postings. Skips are forward pointers within an inverted list, and make it possible to pass over non-promising information on the postings with minimal ef-forts. They are often inserted into the indexes and stored as additional information. With the help of the skips, the system can jump to required records without going through the posting list one record by one record.
When processing a query, there are three different tech-niques to traverse the indexes and compute the relevance score for each document.
To reduce the computational efforts, pruning has been introduced to all these three strategies [4, 9, 34] with the goal of limiting the number of accumulators and the scan depth on the index lists. The main idea is that the process can be terminated once the system identifies that further processing will not change the ranking of top k documents.
We follow the previous studies [4, 29] and use a state-of-the-art four-stage SAAT query processing strategy with the impact-based indexing since it can achieve very good per-formance in many cases. Note that we use query generation model with Dirichlet prior smoothing as the basic retrieval model [40] (i.e., the one without using feedback) and the rel-evance model [19] as the pseudo-relevance feedback method. However, our proposed incremental approach can be applied to other retrieval functions and feedback methods and we plan to study it as one of our future work.

To compute the impact scores (i.e., B w,D in Equation (1), we use the derived equation from previous study [29]: where n is the number of distinct values used to represent impact scores and set to 64. C w is the minimum value of P ( w | D ) in the collection (i.e., C w = min D logP ( w | D )). M is used to ensure that the binned score is within the range, and s is a saturation parameter which makes sure that all the bin values, i.e., [0 ,n ], can be fully utilized. The details of the derivation can be found in the references [29]. More-over, the term probability is estimated using Dirichlet Prior smoothing [40], i.e., The parameter  X  is set to 2,500 and the skip length is set to 128 bytes.

For query processing, we use the SAAT with a four-stage pruning. The four stages are: OR, AND, REFINE and IG-NORE. The processing begins with the OR stage by process-ing the postings in the inverted lists based on the decreasing order of the impact values. Document accumulators are cre-ated whenever it is necessary, i.e., when a new document is seen in one of the inverted lists. The processing switches to AND stage when we can prove that we have created accumu-lators for all the documents that could possibly enter the top n. In AND stage, we ignore all the documents without an accumulator. The processing continues and we update the scores for the existing accumulators with newly processed information. The processing switches to REFINE stage as soon as we know exactly what the top n documents are with-out knowing their exact ranking. REFINE stage works with the accumulators of the top n documents. Once we can de-termine that rank order of the top n documents, the process enters the final IGNORE stage, which means that all the remaining information from the inverted list can be ignored.
The basic idea of the pruning strategy is to gradually re-duce the number of active accumulators when we gain more confidence on knowing that other accumulators would not change the final top-k search results. In particular, OR stage is the only one that can add accumulators, AND stage only updates existing accumulators, and REFINE stage only pro-cess accumulators that can make to the top k results. More-over, we further speed up the process by using skipping and accumulator trimming [29]. Skipping enables fast processing of long postings, while accumulator trimming can reduce the computational cost in the AND stage by dynamically reduce the number of enabled accumulators.
As described in Section 2.1, many pseudo-relevance feed-back methods have been proposed and studied. Despite the differences on how to exploit feedback information, they all require the following three-step implementation: 1. Initial retrieval: finding documents relevant to an 2. Term selection: identifying useful expansion terms 3. Second-round retrieval: returning documents rele-
We now provide more details on how each of these three steps is implemented in existing IR systems.
 Figure 1: Processing time of the three steps in pseudo-relevance feedback implementation
The initial retrieval step can be implemented with any existing top-k query processing techniques as described in Section 3. There have been significant efforts on optimizing the efficiency for this step [1 X 4, 7, 9, 17, 25, 29, 30, 42].
The term selection step is to select important terms from the feedback documents with the expectation that the se-lected terms can bring more relevant documents in the sec-ond round of retrieval. Traditionally, these terms are se-lected directly from the top ranked documents. However, this step could takes lots of time when the documents are long and the information about the documents need to be read from the disk. To solve this problem, Billerbeck and Zobel [6] proposed to generate a short tf-idf based summary for each document and select expansion terms from the sum-maries of the feedback documents. These summaries are small enough to be pre-loaded into the memory, and can lead to more efficient term selection. Their experimental results showed that this strategy is efficient with ignorable loss in terms of the effectiveness. In this paper, we use this strategy for term selection. One difference is that we use term probability p ( t | D ) instead of tf-idf weighting to gen-erate document summaries because the retrieval function used is based on language modeling approach. And we set the length of document summary to 20 terms.

The second round retrieval step aims to retrieve final re-trieval results with the expanded query. The expanded query is often formulated as a linear interpolation of the origi-nal query and the expansion terms selected from the second step. As an example, in the relevance model [19], this step is to retrieve documents with an updated query model (i.e.,  X 
Q ) by linearly combining the original query model (i.e.,  X  ) with the relevance model estimated from the feedback documents (i.e.,  X  F ) as follows: where the original query model  X  Q is estimated using the maximum likelihood estimation of query Q , the relevance model  X  F estimated from the feedback documents F using the methods described in previous study [19], and  X  is to control the amount of feedback.

In the second round of retrieval, existing IR systems such as Indri would process the expanded query in the same way as a newly submitted query. In other words, the two rounds of retrieval are processed independently .

Figure 1 shows how much time each step takes when us-ing the existing implementation methods described above for the relevance feedback method with 20 expansion terms. It is clear that the third step takes the most of computa-tional time while the other two stages share a very small part the time usage. Thus, the key to efficient pseudo-relevance feedback methods is to reduce the execution time for the second-round retrieval , which is the focus of our paper.
Compared with the initial retrieval, the expanded query processed in the second round is often much longer than the original query because it has much larger number of query terms to be processed. For example, the average length of Web queries is around 3, while the number of expansion terms is often set to 20. As a result, the computational cost for expanded query is significantly higher than that for the basic retrieval. Take the implementation used in our paper as an example, a longer query means that each accumulator needs to collect more postings to produce a final result and more accumulators would be created and evaluated. As a result, the system performs much more index accesses and computing, which leads to extremely long processing time compared with that for shorter queries.

One limitation of existing implementation for feedback methods is that the two rounds of retrieval are processed independently . Each round starts with an empty set of ac-cumulators and gradually adds new accumulators in the OR stage. When switched to AND stage, accumulators can be updated but no new accumulated can be added. When switched to REFINE stage, only top k accumulators are processes. And in the final IGNORE stage, all the infor-mation from the inverted lists can be ignored. Note that the accumulators used the two rounds of retrieval are com-puted from the scratch separately. This implementation is illustrated in the upper part of Figure 2
However, unlike processing a new query, the expanded query in the second round retrieval is related to the query used in the initial retrieval. In particular, the query terms in the initial retrieval is a subset of those in the second round of retrieval, and these terms will be processed twice in this two rounds of retrieval process. Moreover, the results of these two rounds of retrieval might have a great overlap. Thus, it would be interesting to study how to leverage the results of the first round of retrieval to reduce the computational cost. But how to leverage them? This is not a simple problem without significant challenges.

The idea of exploiting the results of initial retrieval to improve the efficiency of the second round retrieval was dis-cussed in previous study [6], but was not found useful. In the next subsection, we re-visit this basic idea and propose an incremental approach that is shown to be both efficient and effective based on the experimental results.
Our basic idea is that the initial retrieval process should be treated as part of the query processing for the second round retrieval. Instead of processing the expanded query in the second round from the scratch, we should be able to resume the query processing results of the initial query, and continue the processing for the expanded query terms. But how to resume the results?
One possible strategy is to resume the last ranking-related stage, i.e., REFINE, in the query processing results of the initial retrieval. Recall that REFINE stage is designed to process only the accumulators of top K ranked documents. Thus, if we resume the status from the REFINE stage in the second round retrieval, it is equivalent to re-ranking those top K ranked documents using the expanded query. This strategy is illustrated in the middle part of Figure 2. Since the number of accumulators used in REFINE stage is very small, this re-ranking method would be quite efficient. However, it would suffer significant loss in terms of the ef-fectiveness because one of the major benefits of feedback methods is to find relevant documents that were not among top ranked results for the initial retrieval and the re-ranking strategy seems to disable this nice benefit. Clearly, this is not an optimal solution. Can we do better?
Intuitively, if more document accumulators can be in-cluded in re-ranking process, the retrieval effectiveness could be improved. On the extreme case, when all the documents are considered for the re-ranking, the cost would be the same as submitting a new query. Thus, the main challenge here is how to select a set of documents to be re-ranked so that we can increase the efficiency without sacrificing the effec-tiveness.
 Recall that the pruning technique consists of four stages: OR, AND, REFINE and IGNORE. The number of active accumulators becomes smaller as the system switches from one stage to the other. As discussed earlier, resuming from the REFINE stage is equivalent to re-ranking only top k documents, which hurts the effectiveness. On the contrary, resuming from the OR stage would not hurt the effective-ness. However, since the number of expanded terms is much larger than the number of original query terms, we might not be able to reduce the number of accumulators significantly. Thus, we propose to resume from the AND stage.

The main idea of our incremental approach is shown in the lower part in Figure 2. REFINE is the last ranking-related stage in the initial retrieval. Thus, in order to resume from the AND stage of the initial retrieval, we have to first rewind the query processing results from the end of REFINE stage back to the end of the AND stage. This new stage is referred to as RECOVERY stage in our system. The RECOVERY stage has two tasks: (1) re-enable the accumulators that were disabled at the REFINE stage; and (2) turn back the inverted list pointers to the positions where they were at the end of AND mode. Our experimental results show that this stage takes a very short ignorable time. After the RECOV-ERY stage, we will switch to the new AND stage to update the accumulators based on the expanded terms, and then continue to the other two stages as usual.
Our proposed incremental approach can improve the effi-ciency because of the following two reasons.

First, query processing results of the original query terms can provide useful information for effective pruning in the second round retrieval. Specifically, accumulator trimming is used in the AND stage to dynamically reduce the num-ber of active accumulators, and a threshold is used to de-cide whether an accumulator should be kept active or not. The threshold is to estimate a lower bound of the relevance scores for the top K ranked documents. This threshold is set to  X  inf at the beginning of the retrieval process, and will be updated at the later stage of the retrieval when more information is gathered. Since initial value of the threshold is rather small, little pruning is applied at the early stage of the retrieval process. If the system could be informed with the range of the threshold, more pruning can be done, which would lead to shorter processing time. Since resuming the process of the initial retrieval can provide a much larger ini-tial value for the pruning threshold, the proposed approach can reduce the query processing time.

Second, the efficiency is closely related to the number of accumulators that need to be processed. Long queries usually lead to a huge number of accumulators, which sig-nificantly hurt the efficiency. However, when resuming the query processing results of the initial query, we are able to start with a much smaller set of accumulators. Note that this strategy is not ranking safe when the expanded query is Table 1: Efficiency comparison using TREC Ter-abyte ad hoc queries (i.e., average query processing time (ms) to retrieve 1K documents) Table 2: Comparison of the average size of accumu-lator lists per query significantly different from the initial query (i.e., when  X  in Equation 2 is very small). However, as shown in Section 5, the optimal value of  X  is often large and this strategy does not affect the effectiveness significantly.
In our study, we use three standard TREC data sets which were created for TREC Terabyte tracks in 2004 to 2006. These three sets are denoted as TB04, TB05 and TB06 in this paper. All the data sets use Gov2 collection as the document set, and the collection consists of 25.2 million web pages crawled from the .gov domain. All experiments were conducted on a single machine with dual AMD Lisbon Opteron 4122 2.2GHz processors, 8GB DDR3-1333 memory and four 2TB SATA2 disks.
 The basic retrieval model used in our experiments is the Dirichlet Prior smoothing method [40], where the parame-ter  X  was set empirically to 2500. This method is labeled as NoFB . We use the relevance model [19] as the pseudo-relevance feedback method. This model is chosen because of the following two reasons. First, it is a state-of-the-art pseudo-relevance feedback method that has been shown to be both effective and robust. Second, it is implemented in the Indri 1 toolkit, which makes it possible to verify the cor-rectness of our implementation. This method is labeled as FB .
 The implementation of our basic retrieval system (i.e., NoFB ) is described in Section 3.3. Based on the basic re-trieval system, we implemented two pseudo-relevance feed-back systems: (1) FB-BL: traditional method of imple-menting pseudo-relevance feedback methods, i.e., the ex-panded query is processed independently to the original query; (2) FB-Incremental: our proposed approach that exploits the query processing results of the original query to speed up the processing of the expanded query.

Although we implemented our own indexing and query processing modules, we did not build the impact-based in-dexing directly from the collection. Instead, we built an initial index using Indri toolkit and use Indri API to trans-fer the indri index to our impact-sorted index. The size of the impact-based index for Gov2 collection is about 11.8GB with 442MB for term lookup, 7.5GB for inverted lists and http://www.lemurproject.org/indri/ Table 3: Effectiveness comparison using TREC Ter-abyte ad hoc queries (MAP@1000) 3.8GB for document summaries that are needed for the term selection step in the feedback methods. We decided to lever-age the Indri index because (1) it saves our unnecessary efforts on writing codes to parse documents and count the term statistics; and (2) it enables direct comparison between our system and Indri since they now use the same infor-mation about the collection. When evaluating a query, we also leveraged Indri API to handle the tasks of converting terms/documents to their IDs.

Since we have to work with two indexes (the original Indri index and new Impact-sorted index) and all the information are read directly from disk rather than memory, our system could be slower than those using similar strategies. How-ever, this should not affect our comparison on the two FB systems since they are implemented using the same strategy. Moreover, even working with two indexes, our developed sys-tem is still much faster than the Indri toolkit (version 5.1), which takes about 1 minute to process a query when using the relevance model on the Gov2 collection.
The first two sets of experiments were conducted with the ad hoc queries used in TREC Terabyte tracks. Each data set has 50 queries, and we use title only field to formulate queries. The third set of experiments was conducted with the efficiency queries used in TREC Terabyte tracks. One data set has 50K queries while the other has 100K queries.
We first examine whether the incremental approach can improve the efficiency. In particular, we set the number of feedback documents to 10, the number of expansion terms to 20 and the expansion weight  X  is set to 0.6. These pa-rameters are chosen because they can lead to near-optimal performance in terms of the effectiveness. Table 1 shows the average processing time (ms) for each query when 1000 doc-uments are retrieved. The trends on the three collections are similar. Both FB systems are slower than the NoFB system. It takes around 5 seconds for FB-BL to process a query, and takes around 2.5 seconds for FB-Incremental to do so. Clearly, FB-Incremental can achieve a speed up of 2 compared with FB-BL, which shows that the proposed in-cremental approach can improve the efficiency. Note that we also evaluate the efficiency of the Indri toolkit, a widely used open source IR system. It takes around 1 minute for the Indri toolkit to process a query on the same collection. This suggests that the baseline system we implemented, i.e., FB-BL, is very efficient.

As discussed earlier, the speed up of FB-Incremental is achieved for two reasons. First, FB-Incremental leverages the query processing results of the initial retrieval, which avoids processing the original query terms twice. Second, the accumulator list inherited from the initial retrieval by FB-Incremental is much smaller. Table 2 shows the compar-ison of the accumulator list size between the two methods. Our incremental method creates only about 1/7 accumu-lators as those of baseline method. As a result, the new method avoids a lot of unnecessary index access and com-puting for those accumulators it does not include. Note that our baseline system, i.e., FB-BL, is a very strong baseline since it applied the accumulator trimming techniques which can reduce unnecessary accumulators during the process.
We also conduct experiments to examine whether the pro-posed approach would hurt the performance. Since our in-cremental approach leverages the accumulators used in the AND stage of the initial retrieval, it is possible that we may miss some relevant documents because they were not rele-vant to the original query and thus were not included in its accumulator lists. As a result, it is possible that the incre-mental approach might hurt the effectiveness, but it only happens when the expanded query is very different from the original query. Recall that the optimal value of  X  is 0.6 which indeed indicates that the original query and expanded query are similar. Table 3 shows the results measured with MAP@1000. To ensure the correct implementation of our system, we compare the baseline performance with the ones reported in the previous study [30] and find that the results are similar, which confirms our implementation of NoFB is correct. Moreover, we find that both FB methods can improve the retrieval accuracy about 4% to 8%, which is also similar to the performance improvement of the feedback method implemented in Indri. One interesting observation is that the effectiveness of the two FB systems are similar, which indicates that our proposed approach improve the ef-ficiency without sacrificing the effectiveness.
There are multiple parameters in the pseudo-relevance feedback methods such as the number of expansion terms, the number of feedback documents and the expansion weight (i.e.,  X  in Equation (2)). Since we use impact based docu-ment summary for term select, the impact of the number of feedback documents on efficiency is not very significant. Thus, we only focus on the other two parameters. query) 0.5 0.6 0.7 0.8 0.9 Figure 3: The speed-up rate on different expansion weight  X  (top 1000 documents returned)
As discussed earlier, the expansion weight is an important parameter and we should examine the impact of its value on both efficiency and effectiveness. The results are shown in Table 4 and Table 5. We see that the optimal value of  X  is around 0.6, which means that we should put more weights to the original query terms. According to Equation (2), the higher value of  X  means that we put more trust on the orig-inal query and less weights to the expansion terms. Given the characteristics of SAAT pruning technique, postings of terms with smaller weights are more likely to be pruned. As a result, we can observe a clear trend that both methods can improve the efficiency more when the value  X  is larger.
Figure 3 shows how the speed-up rate of FB  X  Incremental compared with FB changes with the expansion weight. The higher expansion weight we choose, the more the original query determines the final ranking list and the more time the incremental method can save. However when the ex-pansion weight is too high (e.g. 0.9), SAAT pruning tech-nique is rather efficient and leave small room for further im-provement. As a result, the speed-up rate at high expansion weight decreases.

We further test the impact of number of expansion terms on efficiency. Table 4 shows the speed up rate of FB  X  Figure 4: The speed-up rate on different number of expansion terms Incremental compared with FB  X  BL . It shows that our new method benefits more when the system adds more ex-pansion terms into the original query. The main reason of this trend is that our method efficiently controls the grows of accumulator list size when the query becomes longer.
Another important factor that could affect the retrieval speed is the number of documents retrieved for each query. Figure 5 shows how the number of retrieved documents af-fect the speed up of FB  X  Incremental over FB  X  BL . The results indicate that the new method can achieve more speed-up when the system returns fewer results to users. We believe it is due to the reason that returning fewer doc-uments brings higher cut-off threshold. And in our method, the high cut-off threshold which is got from the initial run provides a strong and efficient guidance on the accumulator selections in the resumed retrieval. As a result, the pool of candidate documents/accumulators is kept in a very small size and final result is generated soon. In opposite, the base-line method which does the second round retrieval indepen-dently cannot get the benefit from the high cut-off threshold and it still generates large number of accumulators in which most of them are unnecessary. Figure 5: The speed-up rate on different number of retrieved documents per query Figure 6: The speed-up rate on different expansion weight  X  (top 10 documents returned)
We also report the efficiency when retrieving only 10 doc-uments for each query in Table 6. It is clear that when retrieving fewer documents, FB-Incremental can achieve a higher speed up, i.e., around four. This indicates that this method could be very useful for Web search domain since search users only need to look at 10 results per page. Fi-nally, Figure 6 shows impact of the expansion weight on the speed-up when only 10 documents are retrieved. The trend is very similar to Figure 3.
To further test the scalability of the incremental method and study how original query length affects the pseudo feed-back system speed, we test both methods on 50k 2005 effi-ciency queries and 100k 2006 efficiency queries. For each query, 5 expansion terms are added and it only returns top 10 documents. The results show that the incremental method can stably improve the efficiency at a rate more than 2 at different original query length. For short queries, the baseline is fast and it leaves less space for further improve-ment. For long queries, the processing time is dominated Table 7: Average query processing time (ms) on dif-ferent query length (2005 50K queries) FB-Incremental 124 18 43 97 204 316 540 Table 8: Average query processing time (ms) on dif-ferent query length (2006 100K queries) FB-Incremental 300 26 43 110 248 441 797 by the original query terms and the optimizations in pseudo feedback part will not affect the total time too much. As a result, it is reasonable to observe a speed up rate summit at medium length query (i.e., 3-4 terms).
This paper focuses on efficient implementation of pseudo-relevance feedback methods, an important yet under-studied problem. Motivated by the fact that original query terms are often processed twice in the two-round retrieval process, we proposed an incremental approach that can exploits the query processing results of the first round retrieval for com-puting the document scores in the second round. Although similar idea was mentioned in a previous study [6], it was concluded as a unsuccessful strategy. On the contrary, our method has been shown to be useful and can achieve a speed up of 2 over TREC collections. This paper is one of a few studies that try to bring the gap between the continuous re-search efforts on improving the effectiveness of the pseudo-relevance feedback methods and the increasing efforts on efficient IR systems.

There could be many interesting directions for our future work. First, we plan to implement other pseudo feedback methods including both basic models such as model-based feedback [41] and more sophisticated models such as posi-tional relevance model [22], and evaluate whether the pro-posed approach can improve their performance. Second, it would be interesting to study how to leverage the incremen-tal approach to improve the efficiency of query processing for long queries. Finally, our system is built on impact-sorted indexing with SAAT traverse. We plan to continue our efforts and study whether it is possible to improve the efficiency for feedback methods when using other query pro-cessing strategies such as DAAT and TAAT .
This material is based upon work supported by the Na-tional Science Foundation under Grant Number IIS-1017026. We thank the anonymous SIGIR reviewers for their useful comments. [1] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space [2] V. N. Anh and A. Moffat. Impact transformation: [3] V. N. Anh and A. Moffat. Simplified similarity scoring [4] V. N. Anh and A. Moffat. Pruned query evaluation [5] B. Billerbeck and J. Zobel. Techniques for efficient [6] B. Billerbeck and J. Zobel. Efficient query expansion [7] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and [8] C. Buckley. Automatic query expansion using [9] C. Buckley and A. F. Lewit. Optimization of inverted [10] C. Buckley and S. Robertson. Relevance feedback [11] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting [12] M.-A. Cartright and J. Allan. Efficiency optimizations [13] M.-A. Cartright, J. Allan, V. Lavrenko, and [14] K. Collins-Thompson and J. Callan. Estimation and [15] J. V. Dillon and K. Collins-Thompson. A unified [16] S. Ding and T. Suel. Faster top-k document retreival [17] R. Fagin. Combining fuzzy information: an overview. [18] V. Lavrenko and J. Allan. Real-time query expansion [19] V. Lavrenko and B. Croft. Relevance-based language [20] K.-S. Lee, W. B. Croft, and J. Allan. A cluster-based [21] Y. Lv and C. Zhai. Positional relevance model for [22] Y. Lv, C. Zhai, and W. Chen. A boosting approach to [23] M. Mayer. Scaling google for every user. In Seattle [24] J. Miao, J. Huang, and Z. Ye. Proximity-based [25] A. Moffat and J. Zobel. Self-indexing inverted files for [26] S. Robertson and K. Sparck Jones. Relevance [27] J. Rocchio. Relevance feedback in information [28] G. Salton and C. Buckley. Improving retrieval [29] T. Strohman. Efficient processing of complex features [30] T. Strohman and W. B. Croft. Efficient document [31] T. Strohman, H. Turtle, and W. B. Croft.
 [32] T. Tao and C. Zhai. Regularized estimation of mixture [33] M. Theobald, R. Schenkel, and G. Weikum. Efficient [34] H. Turtle and J. Flood. Query evaluation: strategies [35] H. Wang, Y. Liang, L. Fu, G. rong Xue, and Y. Yu. [36] I. H. Witten, A. Moffat, and T. C. Bell. Managing [37] J. Xu and W. B. Croft. Improving the effectivness of [38] Y. Xu, G. J. F. Jones, and B. Wang. Query dependent [39] H. Yan, S. Shi, F. Zhang, T. Suel, and J.-R. Wen. [40] C. Zhai and J. Lafferty. The dual role of smoothing in [41] C. Zhai and J. Lafferty. Model-based feedback in the [42] J. Zobel and A. Moffat. Inverted files for text search
