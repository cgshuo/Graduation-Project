 The goal of clustering is to identify distinct groups in a dataset. Compared to non-parametric clustering methods like complete linkage, hierarchical model-based clustering has the a~vantage of offering a way to estimate the number of groups present in the data. However, its computational cost is quadratic in the number of items to be clustered, and it is therefore not applicable to large problems. We review an idea called Fractionation, originally conceived by Cutting, Karger, Pedersen and Tukey for non-parametric hi-erarchical clustering of large datasets, and describe an adap-tation of Fractionation to model-based clustering. A further extension, called Refractionation, leads to a procedure that can be successful even in the difficult situation where there are large numbers of small groups X  1.5.3 [Pattern Recognition]: Clustering; 1.5.1 [Pattern Recognition]: Models--Statistical Model-based Clustering Model-based Clustering, Fractionation, Refractionation 
The goal of clustering is to identify distinct groups in a dataset 2d = {~vz,... ,xn} C R m. For example, when *Supported by NSA grant 62-1942. iSupported by NSF grant DMS-9803226 and NSA grant 62-1942. permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. presented with (a typically higher dimensional version of) a dataset like the one in Figure 1 we would like to detect that there appear to be (perhaps) five or six distinct groups, and assign a group label to each observation. (Throughout this paper we distinguish between "groups" and "clusters", which are estimates for the groups.) 
To cast clustering as a statistical problem we regard the data xt,.  X   X  , x, as a sample from some unknown probability density p(x). There are two statistical approaches to clus-tering. Nonparametric clustering [18, 13, 2, 10] is based on the premise that groups correspond to modes of the density p(x). The goal then is to estimate the modes and assign each observation to the "domain of attraction" of a mode. In con-trast, model-based clustering (see [15] and references therein) assumes that each group g is represented by a density pg(x) that is a member of some parametric family, such as the mul-tivariate normal family. The density p(x) then is a mixture of the group densities, and the parameters of the mixture components as well as their number can be estimated from the data. The ability to estimate the number of groups is an important strength of the model-based approach. There is, as yet, no comparable method for nonparametric clustering in more than one dimension. 
In this paper we focus on model-based clustering of large datasets. We review an idea called Fractionation, originally conceived by Cutting, Karger, Pedersen and Tukey [7] for nonparametric hierarchical clustering of large datasets, and describe an adaptation of Fractionation to model-based clus-tering. A further extension, called Refractionation, leads to a procedure that can be successful even in the difficult situa-tion where there are large numbers of small groups. Refrac-tionation is the principal new idea presented in our paper. that the data xl,... , x~ are a sample from a mixture den-sity p(x) = ~-17rg pg(x). Here, 7rg is the prior probability that a rando~a~ chosen observation belongs to group g, and pg is the density modeling group g. A popular model, and the one we focus on, is to assume that the group densities pg are multivariate Gaussian with mean #9 and covariance matrix ~. a given number G of mixture components is where  X (.;/~, E) is the Gaussian density with mean vector/~ and eovarianee matrix E. This log-likelihood can be opti-chapter 2.8.) or components, in a mixture model ([15], chapter 6.) For example, we can find the number G that maximizes the 
While attractive conceptually, this approach to fitting mix-
Unfortunately, straightforward implementation of hierar-
There are several ways of extending model-based cluster-Fractionation was originally presented by Cutting, Karger, 
The original Fractionation algorithm proceeds as follows: with cr &lt; 1. Summarize each cluster by its mean. We refer to these cluster means as meta-observations. 
M, return to step (1), with the meta-observations tak-ing the place of the original data. closest mean. 
The number of fractions in the i-th iteration is c~-ln/M 
If we use hierarchical model-based clustering as the base 
We do not want to assume that the number of groups is 
A major problem with Fractionation is that once observa-tions from different groups have been assigned to the same meta-observation this error will never be corrected. Such er-roneous assignments are less likely to occur if fractions are pure, i.e. contain observations from few groups or, equiva-lently, if groups are split over few fractions. We could form purer fractions if we knew the group labels of the obser-vations. This observation suggests applying Fractionation repeatedly and forming the fractions for Step 1 of the i-th pass based on the clustering produced in the (i -1)st pass. Conceptually, Step 4 of the Fractionation algorithm is replaced by two steps, both involving hierarchical model-based clustering of the meta-observations generated by Step 3: 4a Cluster the meta-observations into G clusters, where G is determined by the AWE criterion X  4b Define the fractions for the i-th pass: as soon as a cluster formed during the merging represents more than M observations, make those observations into a fraction and remove the cluster from the merge process X  
We stop the Refractionation iterations when the change in the number G of clusters and the cluster compositions is small enough X  
To illustrate how Refractionation works, consider a simple example in two dimensions with 25 equally spaced Gaussian groups containing 16 points each. Figure 2 shows the data and the component densities of the model. The circles in this and the following figures are isopleths of the component densities containing 95% of the mass. 
We randomly split the data into four fractions of 100 ob-servations each (Step 1 of the Fractionation algorithm), and then use model-based hierarchical clustering to cluster each fraction into M/iO = 10 clusters (Step 2 of the algorithm) X  The fractions and their clusters are shown in Figure 3. 
The number of meta-observations produced by clustering the fractions in this case is 40 which is less than M = 100 (Step 3) and we can therefore proceed to steps 4a and 4b. 
Clustering the 40 meta-observations into 25 clusters (Step 4a) produces the mixture model whose component densities are shown in Figure 4. Clearly, this clustering in no way reflects the structure of the data. 
Clustering the 40 meta-observations into new fractions (Step 4b) results in fraction sizes of 97, 108, 104, and 91. Figure 5 shows the new fractions. 
We now start the second pass of Fractionation. Each frac-tion again is clustered into 10 clusters (Step 2) shown in Figure 5. 
Clustering the 40 meta-observations into 25 clusters (Step 4a) produces the mixture model shown in Figure 6. We have essentially recovered the structure of the data. 
A third pass of Fractionation (Figures 7 and 8) leads to almost the same mixture model (Figure 8) as the second pass (Figure 6), and the Refractionation process stops. 
Table 1 gives numerical summaries of the purity of the fractions. At the beginning of the first Fractionation pass, each of the 25 groups is scattered over all four fractions, whereas at the beginning of the third pass only one of the groups is split across multiple fractions X  ..:.~. ......%.. .",.. ,.~,' . . :.. 
Figure 5i Meta-observations obtained by clustering the four fractions in the second pass of Fractiona-tion.  X  ;f.~....:::~.....,... :,, ....., 
Figure 7: Meta-observations obtained by clustering the four fractions in the third pass of Fraetionation. 
Figure 6: Clusters after the second pass of Fraction-ation. 
Table 1: The distribution of the number of fractions each group resides in at the start of each Fractiona-tion pass. 
Figure 8: Clusters after the third pass of Fractiona-tion. 
In order to gain some insight into the scope and limita-tions of (Re)Fractionation, we consider an idealized situa-tion where the groups are so well separated that it is unam-biguous whether or not two observations or meta-observa-tions belong to the same group. This allows us to separate performance of the base clustering method from the perfor-mance of Fractionation and Refractionation. 
Let ng be the number of groups in the data, let nf be the number of fractions, and let n X  be the number of clusters generated from each fraction in Step 2 of the Fractionation algorithm. Clearly, if ng&lt; nc then Fractionation will work and Refractionation is unnecessary. On the other hand, if tions from more than nc groups, which will lead to impure clusters, and therefore the groups will not be recovered per-fectly. 
Even in our simple scenario it is difficult to make such simple statements about Refractionation. We can only prove that Refractionation works for ng= nc + 1, under some restrictions about the group sizes. However our examples show that the range of applicability is much larger. It is also clear that refractionation will not recover the groups if that contains observations from more than nc groups, and clustering this fraction will lead to impure clusters. 
In order to investigate how well model-based 1Fractiona-tion and Refractionation can find groups in a dataset, we apply them to four datasets for which the group labels are known. 
In our examples we know the true group labels of the ob-servations, and we want to measure the degree of agreement between the groups and their estimates, the clusters. We use the Fowlkes-Mallows index [11] as a measure of agree-ment. The index is the geometric mean of two probabilities: the probability that two randomly chosen observations are in the same cluster given that they are in the same group, and the probability that two randomly chosen observations are in the same group given that they are in the same clus-ter. Hence a Fowlkes-Mallows index near 1 means that the clusters are a good estimate of the groups. 
To compute the Fowlkes-Mallows index we construct a contingency table of the groups and the clusters, as shown in Table 2. Let nl. be the sum over the i-th row of the table, and let n.j be the sum over the j-th column. Then the Fowlkes-Mallows index is given by: 
Our examples are derived from a dataset of 1,131 doc-uments that is part of the Topic Detection and Tracking document collection [1]. The 1,131 documents were manu-ally classified into a total of 25 groups or topics. Six of the topics have less than eight documents each, and contain a true groups 19 
Figure 9: Fowlkes-Mallows index vs number of clus-ters chosen by AWE for Example 1. 21 22 23 
Figure 10: Fowlkes-Mallows index vs number of clusters chosen by AWE for Example 2. choosing the final number of clusters. In ten replications of the experiment the AWE criterion chose 21 clusters once, 22 clusters 3 times and 23 clusters 6 times. The values of the Fowlkes-Mallows index were between 0.83 and 0.94 (see 
Figure 10.) and all the groups are large. They could certainly have been recovered by clustering a random sample of manageable size. Example 3 is more challenging. cating the TDT dataset 19 times, replacing each group by a scaled and shifted version of the entire dataset: Let Pi and ]El be the mean vector and covariance matrix of the i-th group. We obtained the i-th replicate by scaling and shifting the entire dataset to have mean vector tti and co-variance matrix ~i. We end up with 19 x 19 = 361 groups and 19 x 1100 = 20, 900 observations. fractions of 1,045 observations each and clustered fractions into 100 clusters. Because the number of groups (361) is 
Pass Min Median Max [11] E. B. Fowlkes and C. L. Mallows. A method for [12] C. Fraley and A. Raftery. How many clusters? which [13] J. Hartigan. Statistical theory in clustering. Journal of [14] G. E. Hinton, P. Dayan, and M. Revow. Modeling the [15] G. McLachlan and D. Peel. Finite Mixture Models. [16] G. Schwartz. Estimating the dimension of a model. [17] D. Scott. Multivariate Density Estimation. Wiley, [18] D. Wishart. Mode analysis: A generalization of 
