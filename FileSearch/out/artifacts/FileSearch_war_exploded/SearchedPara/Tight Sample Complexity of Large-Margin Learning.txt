 be ensured. For instance, for large-margin classification w e know that if P  X  i.e. the actual number of samples required to get low error, i s much lower than the bound. a specific class of distributions (e.g. all those with P a precise characterization of the sample complexity, for in stance in low dimensions. In particular, we present the  X  -adapted-dimension k by minimizing the  X  -margin-violations is both lower-bounded and upper-bound ed by  X   X ( k more than  X  can be bounded from above and from below by: moderate error rates, which are more relevant for machine le arning. complexity that we establish, can be used to compare large-m argin ( L establish a sample complexity gap between L In this paper we focus only on large L bounds also for other learning rules.
 Related work assumptions.
 learnability claim do not lend themselves easily to such ana lysis. this work also does not provide true distribution-specific s ample complexity. Let D be a distribution over R d  X  { X  1 } . D interested in linear separators, parametrized by unit-nor m vectors in B d P  X  by  X   X  the margin loss with respect to S is denoted by  X   X  sification error is  X   X  ( w, S ) , 1 Since many predictors minimize the margin loss, we define: margin  X  , a training sample S = { ( x which outputs a predictor  X  w  X  argmin  X  w = A  X  ( S,  X  S X ) .
 a random test sample, each of size m , and define  X  S,  X  error minimization algorithm A , and for any m  X  m (  X ,  X , D ) ,  X  Sub-Gaussian distributions D presently see. We also require a more restrictive condition  X  namely that D multivariate Gaussian distribution.
 Definition 2.2 (See e.g. [11, 12]) . A random variable X is sub-Gaussian with moment B (or B -sub-Gaussian) for B  X  0 if We further say that X is sub-Gaussian with relative moment  X  = B/ p E [ X 2 ] . mixture random variable with mean zero is included in this fa mily. Definition 2.3. A distribution D moment  X  if there exists some orthonormal basis a sub-Gaussian random variables, each with a relative moment  X  . mension, for a small fixed constant  X  . For instance, the family D sg depend on the dimensionality of the space or the variance of t he distribution. happens if the dimensionality is arbitrarily high while the average-norm is bounded. Definition 3.1. Let b &gt; 0 and k a positive integer. Definition 3.2. The  X  -adapted-dimension of a distribution or a set, denoted by k k such that the distribution or set is (  X  2 k, k ) limited.
 It is easy to see that k matrix of X , then k previously in [14]. k Lemma 3.3. For 0 &lt;  X  &lt; 1 ,  X  &gt; 0 and a distribution D We proceed to provide a sample complexity upper bound based o n the  X  -adapted-dimension. Definition 4.1. Let F be a set of functions f : X  X  R , and let  X  &gt; 0 . The set { x  X  -shattered by F if there exist r that  X  i  X  [ m ] , y set in X that is  X  -shattered by F .
 The sample complexity of  X  -loss minimization is bounded by  X  O ( d functions restricted to the domain X . For any set we show: ( B 2 / X  2 + k + 1) . Consequently, it is also at most 3 k  X  ( X ) + 1 .  X  P of size ( d + 1)  X  ( d + 1) such that  X  i  X  [ m ] , k  X  X  X   X  X We then proceed similarly to the proof of the norm-only fat-s hattering bound [17]. We have  X  X =  X  X  X  P +  X  X ( I  X   X  P ) . In addition,  X  Xw Denote row i of T by t P P From the inequality E [ X 2 ]  X  E [ X ] 2 , it follows that l 2  X  (1 +  X  ) 2 B 2  X  &gt; 0 , we can set  X  = 0 and solve for m . Thus m  X  ( k + 1) + B 2 2  X  2 + Corollary 4.3. Let D be a distribution over X  X { X  1 } , X  X  R d . Then independently sub-Gaussian distributions as well (see app endix for proof): that D  X  as well. We consider this problem in following section. drawn from D m algorithm A , such that  X   X  S For every x  X  R d there is a label y  X  X  X  1 } such that P examples in S margin loss on S , but loss of at least 1 Theorem 5.2. Let S = ( X denotes the n  X  X h largest eigenvalue of X .
 Lemma 5.3. Let S = ( X S is 1 -shattered at the origin.
 Proof. If  X  we have k y k =  X  m and y  X  ( XX  X  )  X  1 y  X k y k 2  X  sample is 1 -shattered at the origin.
 and denote X then there exists a margin-error minimization algorithm A such that  X  Theorem 5.4 follows by scaling X  X  m &gt; 0 Theorem 5.4 can thus be used to derive a distribution-specifi c lower bound. Define: Then for any  X  &lt; 1 / 4  X   X   X  within reasonable error with less than m complexity from Section 4 was  X  O ( k k  X  and m d  X   X  &lt; 1 , and the coordinates of each example are identically distri buted: Theorem 6.1 (Theorem 5.11 in [18]) . Let X random variables with mean zero, variance  X  2 and finite fourth moments. If lim then lim This asymptotic limit can be used to calculate m m . If d, m are large enough, we have by Theorem 6.1: Solving  X  2 ( adapted-dimension for this distribution to get k k any finite-sample lower bounds for families of distribution s. rem 6.3, stated below, which constitutes our final sample com plexity lower bound. covariance matrix  X   X  I and trace ( X )  X  L has moment B , then for any m  X   X  trace ( X ) Where X and an integer L  X  &gt; 0 and any  X  &lt; 1 4  X   X   X   X  ( D ) , Proof. The covariance matrix of D diag (  X  D set  X  and L  X  Case I: Assume  X  The random matrix X  X   X  each of its coordinates has sub-Gaussian moment  X  and covariance matrix  X   X  trace ( X   X  Case II: Assume  X  diag (0 , . . . , 0 , 1 / X  2 , . . . , 1 / X  2 ) , with k is drawn from an independently sub-Gaussian distribution w ith covariance matrix  X   X  that all its coordinates have sub-Gaussian moment  X  . In addition, from the properties of k discussion in Section 2), trace ( X   X  holds for X  X   X  In both cases P [  X  an algorithm A such that for any m  X   X  ( k following bound, which holds for any  X  &gt; 0 and  X   X  (0 , 1 distribution of labels D to compare L Gaps between L L regularization guarantee a sample complexity of O (log( d )) for an L order to compare this with the sample complexity of L one must use a lower bound on the L distributions with k X k independent Bernoulli variable, the distribution is sub-G aussian with  X  = 1 , and k Gaps between generative and discriminative learning for a G aussian mixture . Consider two large distance 2 v &gt;&gt; 1 between the class means, such that d &gt;&gt; v 4 . Then P classes by looking for a large-margin separator. Indeed, we can calculate k Acknowledgments of Sciences and Humanities. This work was supported by the NA TO SfP grant 982480.
