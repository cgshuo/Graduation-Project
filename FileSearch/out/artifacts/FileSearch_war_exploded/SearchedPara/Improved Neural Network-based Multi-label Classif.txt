 In multi-label text classification, one text can be associated with multiple labels ( label co-occurrence ) (Zhang and Zhou, 2014). Since la-bel co-occurrence itself contains information, we would like to leverage the label co-occurrence to im-prove multi-label classification using a neural net-work (NN). We propose a novel NN initialization method that treats some of the neurons in the final hidden layer as dedicated neurons for each pattern of label co-occurrence. These dedicated neurons are initialized to connect to the corresponding co-occurring labels with stronger weights than to oth-ers. While initialization of an NN is an important research topic (Glorot and Bengio, 2010; Sutskever et al., 2013; Le et al., 2015), to the best of our knowl-edge, there has been no attempt to leverage label co-occurrence for NN initialization.

To validate our proposed method, we focus on multi-label Natural Language Query (NLQ) classifi-cation in a document retrieval system in which users input queries in natural language and the system re-turns documents that contain answers to the queries. For NLQ classification, we first train a model from training data that contains pairs of queries and cor-responding one or more than one document labels, and then predict the appropriate document labels for new queries with the trained model.

Through experiments with a real-world document retrieval system and publicly available multi-label data set, simply and directly embedding label co-occurrence information into an NN with our pro-posed method improved accuracy of NLQ classifi-cation. Along with the recent success in NNs (Collobert et al., 2011; Kim, 2014), NN-based multi-label classi-fication has been proposed. An NN for NLQ classi-fication needs to accept queries with variable length and output their labels. Figure 1 shows a typical NN architecture (Collobert et al., 2011). This NN first transforms words in the input query into word em-beddings (Mikolov et al., 2013), then applies Con-volutional Neural Network (CNN) and Max-pooling over time to extract fixed-length feature vectors, and feed them into the output layer to predict the label for the query (Collobert and Weston, 2008; Col-lobert et al., 2011; Yih et al., 2014). To take care of multi-labels, label co-occurrence has been incor-porated into loss functions such as pairwise ranking loss (Zhang and Zhou, 2006). More recently, Nam et al. (2014) reported that binary cross entropy can out-perform the pairwise ranking loss by leveraging rec-tified linear units (ReLUs) for nonlinearity (Nair and Hinton, 2010), AdaGrad for optimization (Duchi et al., 2011), and dropout for generalization (Srivastava et al., 2014). Considering the training efficiency and superior performance, we used the binary cross en-tropy as one of the baselines in our experiments in Section 4 in addition to negative log-likelihood and cross entropy .

Let x denote the feature vector of a query, y be the vector representation of the label, o be the output value of the NN, and  X  be the parameters of the NN. Note that the representation of y differs depending on the loss function. For simplicity in the following explanation, assume that we have a finite set of la-bels  X  = {  X  1 , X  2 , X  3 , X  4 , X  5 } and that a query x has multiple labels {  X  1 , X  4 } : Negative Log Probability With minimization of negative log probability, a single label is assumed. To circumvent this limitation, we used copy trans-formation (Tsoumakas et al., 2010) and obtained two training examples (( x , y (1) ) , ( x , y (2) )) , where loss for each example becomes l ( X  , ( x , y (1) )) =  X  log( o 1 ) and l ( X  , ( x , y (2) )) =  X  log( o 4 ) , where softmax activation is used to calculate o in the out-put layer.
 Cross Entropy We assumed multi-labels as prob-abilistic distribution as y = (0 . 5 , 0 , 0 , 0 . 5 , 0) . The cross entropy loss for the training example ( x , y ) becomes l ( X  , ( x , y )) =  X  y log( o ) , where softmax activation is used in the output layer.
 Binary Cross Entropy As Nam et al. (2014) in-dicated, minimizing binary cross entropy is supe-rior for handling multi-labels. By representing the target labels as y = (1 , 0 , 0 , 1 , 0) , the binary cross entropy loss for the training example ( x , y ) be-comes l ( X  , ( x , y )) =  X  y ) log(1  X  o k )) , where sigmoid activation is used in the output layer. In this section, we explain our proposed method in detail. 3.1 Weight Initialization Leveraging Label We propose an NN initialization method to treat some of the neurons in the final hidden layer as dedicated neurons for each pattern of label co-occurrence. These dedicated neurons simultane-ously activate the co-occurring labels. Figure 2 shows the key idea of the proposed method. We first investigate the training data and list up patterns of label co-occurrence. Then, for each pattern of la-bel co-occurrence, we initialize a matrix row so that the columns corresponding to the co-occurring la-bels have a constant weight C and the other columns Binary Cross Entropy 49.75  X  50.51 70.81  X  71.32 48.09  X  48.34 have a weight of 0 , as shown in Figure 2 (above). Note that the remaining rows that are not associated with the pattern of label co-occurrence are randomly initialized. This initialization is equivalent to treat-ing some of the neurons in the final hidden layer as dedicated neurons for each pattern of label co-occurrence, where the dedicated neurons have con-nections to the corresponding co-occurring labels with an initialized weight C and to others with an initialized weight of 0 , as shown in Figure 2 (below). Finally, we conduct normal back-propagation using one of the loss functions, as discussed in the previ-ous section. Note that all the connection weights in the NN including the connection weights between the dedicated neurons and all labels are updated through back-propagation.

Since (1) computation of proposed initialization itself is negligible and (2) computation of back-propagation and the architecture of NN does not change with or without the proposed initialization, our proposed method does not increase computation in training and evaluation. 3.2 Weight Setting for Dedicated Neurons For the weight value C for initialization, we used the upper bound UB of the normalized initializa-tion (Glorot and Bengio, 2010), which is determined by the number of units in the final hidden layer n h and output layer n c as UB = ally, we changed this value in accordance with the frequency of the label co-occurrence patterns in the training data. The background idea is that the pat-terns of label co-occurrence that appear frequently (i.e., the number of queries with this pattern of label co-occurrence is large) are more important than less frequent patterns. Assuming that a specific pattern of label co-occurrence appears in the training data f times, we try f  X  UB and to emphasize this pattern.
 We conducted experiments with the real-world NLQ classification data and the publicly available data to confirm the advantage of the proposed method. 4.1 Real-world NLQ classification Data Experimental Setup We used NLQs for a docu-ment retrieval system in the insurance domain for the experiments. Users of the system input queries in natural language, and the system returns the la-bels of the documents that contain answers. We used 3 , 133 queries for training and 394 queries for eval-uation, 1 , 695 and 158 of which had multiple labels, respectively. The number of unique document labels assigned to the training data was 526 .

We used the NN shown in Figure 1. The dimen-sion of word embedding was 100 , number of ker-nels for the CNN was 1 , 000 , which means 1 , 000 units exist in the final hidden layer on top of Max-pooling over time, and number of output units was 526 . We used this NN configuration in common for all the experiments. The word embedding was pre-trained with the skip-gram model of word2vec using the dumped English Wikipedia data and the docu-ments of the target insurance domain (Mikolov et al., 2013). The NN except the word embedding layer was randomly initialized in accordance with the nor-malized initialization (Glorot and Bengio, 2010). We used the ReLU for nonlinearity, AdaGrad for op-timization, and dropout for generalization. We fixed the number of training epochs to 1 , 000 1 .
For the proposed method, we investigated the 1 , 695 queries with multiple labels in the training data and found 252 patterns of label co-occurrence. We then embedded this information in a 1 , 000  X  526 weight matrix between the final hidden and output layers. In other words, we treated 252 neurons in the final hidden layer as dedicated neurons in weight initialization.

For the hyper-parameter settings, we first tuned the hyper-parameters including L2-regularization and learning rate so that the accuracy of the baseline system with random initialization was maximized. For the proposed initialization, we used the same hyper-parameters obtained in the former tuning.
We used three evaluation metrics that are closely related to the usability of the document retrieval sys-tem: (1) 1-best accuracy judges if the 1 -best result Recall@5 judges if the 5 -best results of a system contain at least one of the correct labels. (3) Full accuracy investigates the j -best results of a system and judges if they match the correct labels when j Different Loss Functions Table 1 shows the ex-perimental results using three different loss func-tions. Comparing the values to the left of the arrows, which did not use the proposed initialization, supe-riority of binary cross entropy (Nam et al., 2014) was confirmed in full accuracy, while cross entropy was the best in 1-best accuracy in this experiment. As shown to the right of the arrows, we obtained improvement for all loss functions with every eval-uation metric with the proposed method. Overall, cross entropy training with the proposed initializa-tion achieved the best in all three metrics, where 1-best accuracy improvement from 50 . 51% to 52 . 54% was statistically significant ( p&lt; 0 . 05 ). Different Weight Initialization Table 2 shows the results of emphasizing the frequent patterns of la-bel co-occurrence. We used the cross entropy loss function, which was the best in the previous exper-iments. Using ment in 1-best accuracy and full accuracy, though using f  X  UB deteriorated in all metrics compared with UB . This suggests that there is room for im-provement if we can appropriately emphasize fre-quent patterns of label co-occurrence.
 Analysis on Trained Neural Network We inves-tigated if the dedicated neurons for patterns of la-bel co-occurrences still simultaneously activate the corresponding labels after back-propagation. Table 3 shows the analysis on the NNs trained in the ex-periments for Table 1. In the # Survived Neurons columns, we investigated if the dedicated neurons initialized for the pattern of k -label co-occurrence still had the k largest weights to the correspond-ing k labels after back-propagation. Large por-tions of dedicated neurons  X  X urvived X  after back-propagation. In the Weights columns, we calculated the mean of the connection weights between the dedicated neurons and corresponding co-occurring labels and compared them with the mean of all con-nections in this weight matrix. The trained weights for the connections between the dedicated neurons and corresponding co-occurring labels ( Weights-Dedicated ) were much stronger than the average weights ( Weights-All ). This analysis suggests that the proposed initialization yields dedicated neurons that simultaneously activate the co-occurring labels even after back-propagation.

There can be an overlap in label co-occurrence patterns. One typical case is  X  X , B X  and  X  X , B, C X , and another case is  X  X , E X ,  X  X , G X , and  X  X , E, F, G X . While we prepared the dedicated neu-rons for each co-occurrence pattern before back-propagation, some overlapped co-occurrences might be explained by the superset or combination of sub-sets after back-propagation. Table 3 suggests that some of the dedicated neurons did not survive af-ter back-propagation. We confirmed that about half of the label co-occurrence patterns whose dedicated neurons did not survive were covered by the patterns whose neurons survived.  X  X over X  means that if a neuron for  X  X , B X  did not survive, a neuron for  X  X , B, C X  survived, or if a neuron for  X  X , E, F, G X  did not survive, neurons for  X  X , E X  and  X  X , G X  survived. If we change the network structure by connecting the dedicated neurons only to the corresponding units or preparing the special output units for co-occurring labels (label powerset (Read, 2008)), this flexibility might be lost. 4.2 Publicly Available Data We used multi-label topic categorization data ( RCV1-v2 ) (Lewis et al., 2004) to validate our method. We used the same label assignment and the same training and evaluation data partition with the LYRL2004 split (Lewis et al., 2004) where 23 , 149 training texts and 781 , 265 evaluation texts with 103 topic labels are available. We used the bag-of-word (BoW) feature for each text prepared by Chang and Lin (2011) whose dimension was 47 , 236 and con-structed a feed-forward NN that has an input layer that accepts the BoW feature, hidden layer of 2 , 000 units, and output layer of 103 output units with the cross entropy loss function. By embedding the la-bel co-occurrence information between the hidden and output layers with the initial weights set to UB , which corresponded to treating 758 neurons out of 2 , 000 hidden units as the dedicated neurons, we im-proved 1-best accuracy of topic label classification from 93.95% to 94.60%, which was statistically sig-nificant ( p&lt; 0 . 001 ).

To the best of our knowledge, 1-best accuracy of 94 . 18 % ( 5 . 82 % one-error) 4 (Rubin et al., 2012) was the best published result with using the standard LYRL2004 split of RCV1-v2. Our proposed method has advantages in a sufficiently competitive setup. We proposed an NN initialization method to lever-age label co-occurrence information. Through ex-periments using the data of a real-world document retrieval system and publicly available data, we con-firmed that our proposed method improved NLQ classification accuracy. The advantage of the pro-posed method also includes no computational over-head during training and evaluation.

When we have large training data, the number of label co-occurrence patterns can be larger than that of hidden units. In such a case, one option is to select an appropriate set of label co-occurrence patterns with certain criteria such as the frequency in the training data. Another option is to make a larger weight matrix using all patterns and then to reduce its dimension with such as Principal Com-ponent Analysis (PCA) in advance of NN training. Our future work also includes setting the initializa-tion weight in a more sophisticated way and combin-ing the proposed method with other NN-based meth-ods (Kim, 2014; Johnson and Zhang, 2015).
 We would like to show our gratitude to Dr. Ramesh M. Nallapati of IBM Watson for supporting the ex-periments. We are grateful to Dr. Yuta Tsuboi, Dr. Ryuki Tachibana, and Mr. Nobuyasu Itoh of IBM Research -Tokyo for the fruitful discussion and their comments on this and earlier versions of the paper. We thank the anonymous reviewers for their valu-able comments.
