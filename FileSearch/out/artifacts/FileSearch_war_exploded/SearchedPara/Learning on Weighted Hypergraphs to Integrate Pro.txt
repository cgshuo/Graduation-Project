
Building reliable predictive models from multiple com-plementary genomic data for cancer study is a crucial step towards successful cancer treatment and a full under-standing of the underlying biological principles. To tackle this challenging data integration problem, we propose a hypergraph-based learning algorithm called HyperGene to integrate microarray gene expressions and protein-protein interactions for cancer outcome prediction and biomarker identification. HyperGene is a robust two-step iterative method that alternatively finds the optimal outcome predic-tion and the optimal weighting of the marker genes guided by a protein-protein interaction network. Under the hypoth-esis that cancer-related genes tend to interact with each other, the HyperGene algorithm uses a protein-protein in-teraction network as prior knowledge by imposing a con-sistent weighting of interacting genes. Our experimental results on two large-scale breast cancer gene expression datasets show that HyperGene utilizing a curated protein-protein interaction network achieves significantly improved cancer outcome prediction. Moreover, HyperGene can also retrieve many known cancer genes as highly weighted marker genes.
Finding gene predictors of cancer outcome from ge-nomic data is becoming an increasingly important focus in cancer research under the assumption that the genomic in-formation can shed light on the molecular mechanisms un-derlying cancer development and progression. In the past decade, enormous amount of large-scale microarray gene expression profiles have been produced to study different cancers such as breast cancer [18, 19], lung cancer [16] and prostate cancer [7] for the purposes of 1) detecting marker genes for cancer-relevant phenotypes and 2) building reli-able predictive models for cancer prognosis or diagnosis. The two tasks are closely intervened with each other be-cause on one hand, a predictive model built from highly predictive marker genes is often more accurate in outcome prediction; on the other hand, a highly accurate predic-tion model can also be analyzed to reveal unknown cancer marker genes. Different machine learning and data mining strategies for feature selection have been applied to iden-tifying a subset of genes that can maximize the prediction performance of a classifier [18].

Although many interesting and promising findings have been reported in these studies, the reliabilities of the studies have been questioned with the concern on the unstable and inconsistent results in cross-validations and cross-platform comparisons due to the relatively small sample sizes in the studies [6]. To overcome this difficulty, it has been pro-posed to include other complementary genomic informa-tion such as pathway information or functional annotations to aid the process of model building and biomarker discov-ery such that the prior knowledge from the complementary data can improve the robustness of the model and result in more consistent discoveries across independent datasets [4, 3, 13]. The availability of large protein-protein inter-action networks, which contain information on gene func-tions, pathways and modularity of gene regulations, pro-vides a desirable source of data for this purpose. Protein-protein interactions can be derived from a number of ex-perimental techniques such as yeast two-hybrid system and mass spectrometry [11]. The high consistency between the networks derived from different organisms allows integra-tion of many small networks into a large scale network. It has been observed that cancer genes tend to be highly con-nected with each other in large scale protein-protein inter-action networks [3]. It has been shown in [4] that by incor-porating protein-protein interaction network into the model built from microarray gene expressions, the authors can im-prove cancer outcome prediction and get more reproducible results on two large scale gene expression datasets. In their approach, the integration of gene expressions and protein-protein interactions is achieved by two independent proce-dures: discriminative subnetworks are first identified from a curated protein-protein interaction network and the subnet-works are then used as features to predict cancer metasta-sis. Authors in [13] proposed a method which first com-putes the spectral graph structure of a gene network and then, uses the spectral graph structure to smooth microarray gene expressions before used for sample classification. A statistics-based method is proposed in [3] to identify can-cer genes by scoring genes by their degree in a cancer-specific interaction network, their differential expressions in microarray data and their structural, functional and evo-lutionary properties. However, designing a unified strategy for integrating protein-protein interactions and microarray gene expressions is still a challenging problem due to the complexity of a joint learning on two different data types.
In this paper, we propose a hypergraph-based iterative learning algorithm called HyperGene to integrate microar-ray gene expressions with protein-protein interactions for robust cancer outcome prediction and marker gene identifi-cation. The HyperGene algorithm minimizes a cost func-tion under a unified regularization framework which ele-gantly takes a protein-protein interaction network as con-straints on a hypergraph built from microarray gene expres-sions. The HyperGene algorithm is a natural extension of label propagation algorithms on hypergraphs [2, 1, 22]. Hy-perGene is based on a hypergraph in which each sample is denoted by a vertex and each gene is denoted by two hyper-edges: a  X  X p-regulated X  hyperedge and a  X  X own-regulated X  hyperedge. The two edges group samples by the expres-sion state (up/down) of the gene in the samples (Figure 1 A&amp;B). Our cluster assumption on the hypergraph is that the samples of the same type tend to have similar gene expres-sion patterns and thus are highly connected by the hyper-edges. Since the original hypergraph-based learning algo-rithms assume uniform weighting of the hyperedges [1, 22], direct application of these algorithms to high-dimensional and noisy genomic data results in inferior prediction accu-racy. The HyperGene algorithm is fundamentally different in reformulating the optimization problem as learning la-bels and hyperedge weights together with the assignment of edge weights constrained by a protein-protein interac-tion network. Essentially, to avoid overfitting training data, the HyperGene algorithm tries to find a weighting of hyper-edges that nicely balances the two-class separation on the hypergraph and the consistency with the protein-protein in-teraction network. These properties of the HyperGene algo-rithm promise to improve prediction accuracy and provide more robust identification of marker genes. Furthermore, the resulted weights on the genes can be used to discover highly weighted subnetworks in the protein-protein interac-tion network, which might also suggest important pathways related to cancer outcomes.
In Figure 1, we show the regularization framework in our formulation. We first discretize gene expression profiles into three states: basal or up/down-regulated (Figure 1A), and build a hypergraph with (positive/negative/test) samples as vertices and gene expression states as hyperedges (Figure 1B). The regularization framework seeks for a global solu-tion to both outcome prediction and gene weighting by con-sidering the connectivities in the hypergraph, and the incor-poration of the protein-protein interaction network provides useful prior knowledge on weighting interacting genes with similar values (Figure 1C). The cost function is defined on three loss terms: 1) inconsistent labeling of samples that are highly connected in the hypergraph; 2) inconsistent labeling of training samples with known outcomes; 3) inconsistent weighting of the hyperedges associated with the interacting genes in the protein-protein interaction network. Our objec-tive is to find a solution that can minimize the weighted sum of the three loss terms.
A hypergraph is a special graph which contains hyper-edges. In a simple graph, each edge connects a pair of vertices, but in a hypergraph each edge can connect arbi-trary number of vertices in the graph. Hypergraphs are of-ten used with algorithms for exploring higher order corre-lation between objects in data mining and bioinformatics [17, 20, 21]. Let V = { v 1 ,v 2 ,...,v | V | } be a set of vertices and E = { e 1 ,e 2 ,...,e | E | } be a set of edges defined on V : for any hyperedge e  X  E , e = { v ( e ) 1 ,v ( e ) 2 ,...,v { v a vertex v are called incident if v  X  e . A non-negative real number (a weight) can be assigned to each hyperedge by a function w ( w can also be defined as a vector variable and we will use both notations interchangeably). The vertex set V , hyperedge set E and the weight function w fully defines a weighted hypergraph denoted by G ( V,E,w ) . The inci-dence matrix H for hypergraph G ( V,E,w ) is a | V | X | E | matrix with elements defined as h ( v,e ) = 1 when v  X  e and 0 otherwise. The degree of a vertex v is defined as d ( v ) = P e  X  E h ( v,e ) w ( e ) , which is the sum of the weights of the hyperedges incident with v . The degree of a hyper-edge e is defined as d ( e ) = |{ v | v  X  e }| , which is the num-ber of vertices incident with e . Finally, we define W as the diagonal matrix whose elements on the diagonal are weights of hyperedges, and D v and D e as the diagonal matrices with elements on the diagonal being the degrees of vertices and hyperedges (the row and column sum of H ). Note for a hyperedge i , D e ( i,i ) = d ( i ) . However, for a vertex j , D v ( j,j ) = d ( j ) if and only if P e  X  E h ( j,e ) w ( e ) = d ( j ) .
We use a weighted hypergraph G ( V,E,w ) to model the gene expression data: each sample is denoted by a vertex v  X  V and each hyperedge denotes one of the two ex-pression states (up/down-regulated) of a gene (Figure 1A). Thus, each gene will be associated with two hyperedges in the hypergraph. The incidences between the V and E are decided by the gene expression values on the samples. If the expression value of a gene i is positive for sample set V and negative on sample set V 2 , the up-state hyperedge e is incident with V 1 and the down-state hyperedge e down i incident with V 2 . Note that V 1  X  V 2 is a proper subset of V if the expression levels of the gene in some of the samples are zero (basal). After the hypergraph is constructed, we de-fine a function y to assign initial labels to the corresponding vertices in the hypergraph. If a vertex v is in the positive group, y ( v ) = +1 ; If it is in the negative group, y ( v ) =  X  1 and if v is a test sample, y ( v ) = 0 (Figure 1B).
For cancer outcome prediction, our goal is to find the correct labels for the unlabeled vertices of the test samples in the hypergraph. Let f be the objective function (vector) of labels to be learned. Intuitively, there are two criteria for learning optimal f : 1) we want to assign the same label to vertices that share many incidental hyperedges in com-mon; 2) assignment of the labels should be similar to the initial labeling y . For criteria 1), we define the following cost function,
 X ( f,w ) = If the predicted labels on the vertices are consistent with the incidences with the hyperedges, the value of  X ( f ) should be minimized. For criteria 2), we directly measure the 2-norm distance between the vectors of the predicted and the original labels as follows,
To introduce protein interactions as prior knowledge into the hypergraph-based learning, we assume that interacting genes should receive similar weights on their associated hy-peredges. We define a binary indicator  X  ij to capture the interaction between a pair of hyperedge e i and e j . The in-dicator  X  ij = 1 if the two genes associated with e i and e have the nearest distance k in the protein-protein interac-tion network, otherwise 0. The distance k picked larger than 1 can relax the definition of interaction between two genes by allowing indirect interactions through neighbors. When k = 1 , it is reduced to measure the direct interac-tion between genes. In our experiments in this paper, we set k = 2 . To assign weights to hyperedges consistent with the prior knowledge in the protein-protein interaction network, we define the following cost function over the hyperedge weights, where  X  ( e i ) = P | E | j =1  X  i,j , which is the number of hyper-edges interacting with the hyperedge e i . Minimizing  X ( w ) ensures that hyperedges associated with interacting genes will get similarly weighted. When there is no prior knowl-edge, we can simply set  X ( w ) = || w || 2 .
After the prior knowledge is introduced from a protein-protein interaction network, our task is to minimize the sum of the three cost terms defined as where  X  and  X  are positive real numbers. This objective can be achieved with the following optimization problem, subject to The intuition of adding P e  X  E h ( v,e ) w ( e ) = d ( v ) as con-straints is to maintain the hypergraph structure. The weight-ing of the hyperedges should not be biased towards some samples (such as training samples) and thus, the degree of each vertex, the sum of the hyperedge weights on the vertex, should be kept the same as in the initial graph. Mathemat-ically, these constraints also guarantee that the covariance matrix in  X ( f,w ) is positive semi-definite with respect to f [22], which makes our learning problem solvable.
 identity matrix and W is the diagonal matrix with W ii = w ( e i ) . We can show  X ( f,w ) = f T  X  f by  X ( f,w ) = X
HyperGene ( y,H,A, X , X  ) Step three in the above derivation shows that  X ( f,w ) = f
T  X  f if and only if P P e  X  E h ( v,e ) w ( e ) = d ( v ) for  X  v  X  V in equation 3 keep D v unchanged during the optimization and thus make  X  al-ways positive semi-definite. Finally, let A be the adjacency matrix defined on the protein-protein interaction network with A ij =  X  ij , where i and j are the indexes of hyper-edges, and D be the diagonal matrix with D ii = P j A ij , the optimization problem in equation (3) can be written in the following matrix form, minimize subject to
The objective function  X ( f,w ) in the optimization prob-lem defined by equation 3 is not convex in ( f , w ). However, our formulation contains two sub-optimization-problems, both of which are convex if we independently optimize  X ( f,w ) with respect to f or w . Specifically, if we fix w to be a specific weighting w t satisfying the constraints w t  X  0 and Hw t = diag ( D v ) , the objective function  X ( f,w = w t ) is convex in f ; if we fix f to be a specific la-beling of the vertices f t ,  X ( f = f t ,w ) is also convex in w . Thus, a local optimal solution can be found by solving the two optimizations alternatively by iteration. Our assump-tion is that f and w can be independently optimized and this assumption does not guarantee a global optimal solution to the optimization problem.

For solving the optimization problem in the regulariza-tion framework, the HyperGene algorithm is a two-step iter-ative method that alternatively finds the optimal f and w in each step. The outline of the HyperGene algorithm is given in Figure 2. The HyperGene algorithm first initializes w with a uniform weighting 1 over the hyperedges. Note that w = 1 is a solution to the linear system Hw = diag ( D v ) by definition of D v and thus, a valid solution to Equation 3. In the first step in each iteration, HyperGene fixes w and optimizes  X ( f,w = w t ) with respect to f in the following optimization problem, The cost term  X ( w = w t ) is removed from  X ( f,w = w t ) since it is a constant in the above optimization problem. In the cost term  X ( f,w = w t ) = f T  X  f (Equation 4),  X  is positive semi-definite given  X ( f,w = w t )  X  0 for any f (Equation 1), which also implies that  X ( f,w = w t ) is con-vex in f . Therefore, we can simply take derivative with respect to f to get the optimal solution f  X  = ((1  X   X  ) I +  X   X )  X  1 y , where  X  =  X  1+  X  [22]. This is equivalent to solv-ing the linear system ((1  X   X  ) I +  X   X ) f = y , which can be efficiently computed by Jacobi Iteration method [14].
In the second step in each iteration, the HyperGene algo-rithm fixes f = f t learned in the previous step to learn the optimal weighting of hyperedges w by solving the quadratic programming problem: subject to The cost  X  || f  X  y || 2 is removed from  X ( f,w = w t ) since it is a constant in the above optimization problem, and  X ( f = f ,w ) is a linear function of w (Equation 1). Since  X ( w ) = w T ( I  X  D  X  1 / 2 AD  X  1 / 2 ) w  X  0 for any w (Equation 2), I  X  D  X  1 / 2 AD  X  1 / 2 is positive semi-definite, which implies that  X ( f = f t ,w ) is convex in w . In both steps, the total cost  X ( f,w ) is guaranteed to be reduced until there is only very small change. Thus, our algorithm will finally stop at a small total cost. We implemented the HyperGene algorithm in MATLAB and use ILOG/CPLEX package (version 11.1) for quadratic programming.
We evaluate the HyperGene algorithm on both artificial datasets and two breast cancer gene expression datasets us-ing as a prior a large curated protein-protein interaction net-work constructed by [4]. This protein-protein interaction network contains 57,235 interactions among 11,203 pro-teins integrated from yeast two-hybrid experiments, pre-dicted interactions from orthology and co-citatioin, and other literature reviews [4]. We compare the classifica-tion performance of HyperGene with three baselines, the hypergraph-based learning algorithm [22] and SVMs with linear kernel and RBF kernel (Matlab Bioinformatics Tool-box (V3.0)). The classification performance of all meth-ods are evaluated using the receiver operating characteris-tics (ROC) score: the normalized area under a curve plot-ting the number of true positives against the number of false positives by varying a threshold on the decision values [9]. To mimic the noisy nature of microarray data, we test the HyperGene algorithm on artificial hypergraphs with many noisy hyperedges. In all experiments, we label 50% vertices for training and hold out the other 50% vertices in the hy-pergraphs for testing. We randomly generate hypergraphs with a large number of non-informative hyperedges and a certain number of special hyperedges, each of which alone is not very informative but in combination is highly infor-mative. We first generate a highly discriminative hyperedge incident with 80% of vertices in one class and 20% of ver-tices in the other class, and the hyperedge is split into 5 weak informative hyperedges with equally number of ver-tices. The informative hyperedges are generated to simulate the expression behavior of cancer genes, which are often non-informative unless combined as a module. The prior knowledge is introduced as the interactions between the in-formative hyperedges and some other random interactions between non-informative hyperedges are also introduced as noise.

The algorithms are tested on 100 randomly generated such hypergraphs. We report the average ROC score of the baselines and HyperGene with different percentage of in-formative hyperedges in Figure 3A. Because the results are similar for different choice of  X  and  X  parameters, we only plot the case with (  X , X  ) = (0 . 5 , 1) . It is clear in the plot that, when the prior knowledge gives useful information about interactions between informative hyperedges, the per-formance of our algorithm is significantly better than SVMs and the hypergraph-based algorithm with uniform weights. Since in this simulation, only very high-order combination of the hyperedges can provide good classification perfor-mance, SVMs perform poorly in all cases. To check the
HyperGene and the baseline algorithms in each experiment are reported. versus HyperGene versus HyperGene convergence of the HyperGene algorithm, we also mea-sure the value of the cost function in each iteration on the two real microarray gene expression datasets with selected 1,465 genes (see section 4.2). The change of the cost func-tion for different  X  and  X  parameters is shown in Figure 3B. It is clear that the HyperGene algorithm converges very fast. We also found that the value of f and w variables stay unchanged after the first 2 to 3 iterations.

To test if the HyperGene algorithm can select infor-mative hyperedges, we design one additional experiment with more diverse prior knowledge on both informative hy-peredges and non-informative hyperedges. We generate 400 non-informative hyperedges and 200 informative hy-peredges. The 200 informative hyperedges are grouped into 10 fully connected cliques in the interaction network. We also group 200 random hyperedges into 2 fully connected cliques. The adjacency matrix is shown in Figure 3C. The 2 cliques of non-informative hyperedges are on the top-left of the matrix and the 10 cliques of informative hyperedges are on the bottom-right of the matrix. The weights learned by HyperGene is plotted in Figure 3D. It is evident that informative hyperedges are assigned much larger weights, which shows that the HyperGene algorithm is capable of selecting true informative interaction components even un-der the presence of abundant irrelevant interactions. This result also suggests that the HyperGene algorithm assigns weights to hyperedges based on both the predictability and modularity of the hyperedges, instead of the number of in-teractions that they have in the interaction network. Accord-ingly, the HyperGene algorithm achieves the highest ROC score 0.874 in this experiment, while the hypergraph-based algorithm and SVM with linear kernel and RBF kernel only score 0.750, 0.596 and 0.604 respectively.
We next test the HyperGene algorithm for cancer out-come prediction on two breast cancer gene expression datasets, the van X  X  Veer et al dataset [18] and the van de Vijver et al dataset [10]. The van X  X  Veer et al dataset and the van de Vijver et al dataset contain 24,481 gene expres-sions of 97 and 295 patients respectively. The patients are divided into two groups based on whether the patient had been free of disease after their diagnosis for an interval of at least 5 years or had developed distant metastasis within 5 years after a poor prognosis. The details for quantiza-tion and normalization of scanned microarray images are described in [18, 10]. In the experiments on the van X  X  Veer et al dataset, two subsets of gene expressions, 231 genes suggested by [18] and the top ranked 500 genes selected by the correlation coefficients between the gene expressions and the cancer outcomes, are used for classification. Note that the two subsets of genes are selected on a training set of 78 patients and the remaining 19 patients are held out as the test set as suggested by [18]. In the experiments on the van de Vijver et al dataset [10], we use for classifica-tion two subsets of hypothetical cancer susceptibility genes, 326 genes from Ingenuity 1 and 1,465 genes from Cancer Genomics tool 2 . We randomly run 5-fold cross-validation multiple times on the van de Vijver et al dataset and mea-sure the average ROC. Note that within each experiment of a 5-fold cross-validation, another 4-fold cross-validation is used on the training set to pick the best parameters for HyperGene and the baseline algorithms to test the held-out set. The classification results in Table 1 show that Hyper-Gene performs significantly better than both SVMs and the hypergraph-based learning algorithm in all the experiments. Particularly, HyperGene outperforms the three baseline al-gorithms in classifying the 19 test samples on the van X  X  Veer et al dataset by around 4 to 6 percents when the 231 genes are used, and around 2.5 to 5 percents when the 500 genes are used. It is interesting that the optimal values of  X  for HyperGene in the two experiments are both 1. When  X  is large, the prior knowledge from the protein-protein interac-tion network is emphasized, and the interacting genes will get very similar weights in the optimizations of the Hyper-Gene algorithm. When the interaction network contains ac-curate and helpful information, larger  X  s will be picked in cross-validation to take advantage of the prior knowledge. However, when the quality of the interaction network is poor, larger  X  s will lead to deteriorated classification perfor-mance. Thus, we speculate that the protein-protein interac-tion network plays important role in learning the better clas-sifiers, given the relatively large value for  X  . On the van de Vijver et al dataset, HyperGene achieves an improvement of 1.3 to 2.4 percents on the average ROC score. Although the improvement seems to be small, pairwise comparisons between HyperGene and the baseline algorithms show that in many more cases, HyperGene outperforms the other al-gorithms. 4.3.1 Identification of known biomarkers To demonstrate that HyperGene is capable of identifying true cancer susceptibility genes, we examine the weights of genes obtained by the HyperGene algorithm. In this ex-periment, we construct a hypergraph with the 1,465 can-didate cancer genes and all the labeled patient vertices on the van de Vijver et al dataset. We compare the genes that are highly weighted by HyperGene with those known breast cancer causative genes reported in previous litera-tures. We collect a list of 30 breast cancer causative genes, 16 of which are presented in our data, from [18] and the overview section of breast cancer (MIM 114480) in On-line Mendelian Inheritance in Man (May, 2007) 3 . While Correlation Coefficients give very low ranking to the 16 known breast cancer causative genes, the HyperGene algo-rithm in two different settings (  X  = 1 and 0.001) assigns high ranks to most of the genes, with 14 out of 16 genes ranked in the top 300 genes (Table 2). The difference of the ranking of known breast cancer causative genes cal-culated in the two  X  values is small, which indicates that the Hypergene algorithm is not sensitive to  X  parameter to identify marker genes in this case. Notable examples of the biomarker genes are tumor protein p53 (TP53), estro-gen receptor 1 (ESR1), v-Ha-ras Harvey rat sarcoma viral oncogene homolog (HRAS), and v-Ki-ras2 Kirsten rat sar-coma viral oncogene homolog (KRAS), all of which are not identified in [18] but are highly ranked by the Hy-perGene algorithm. Other novel susceptibility candidates that are not in our list of known causative genes but have a large number of interactions with known susceptibility genes such as CREB binding protein (CREBBP), B-cell CLL/lymphoma 2 (BCL2), and Mdm2 p53 binding protein homolog (MDM2) are also highly ranked by the HyperGene algorithm. 4.3.2 Functional enrichment and pathway analysis We also analyze the biological functions of the biomarker genes by Gene Ontology (GO) annotations and pathway analysis with Ingenuity (version 5.5). We investigate whether the identified marker genes involve significantly over-represented GO categories and biological pathways that are related with breast cancer. With the top 100 marker genes as input, Ingenuity identifies 17 enriched functions scoring a p -value less than 1 . 0 e  X  9 on the van de Vijver et al dataset. Figure 4 shows the enriched biological func-tions from the van de Vijver datasets. All the 17 enriched functions of top 100 marker genes shows strong consis-tency with those identified by [5, 19], indicating that these processes are significantly involved with the progression of cancer. Especially, the most significant functions such as cell cycle ( p -value = 4 . 03 e  X  47 ), cell death ( p -value = 3 . 44 e  X  44 ) , gene expression ( p -value = 2 . 43 e  X  43 ), and cellular growth and proliferation ( p -value = 2 . 7 e  X  36 ) are well known to be functionally involved with metasta-sis and development of breast cancer [15, 19, 4, 18]. Note that among the 17 functions, 11 functions are closely or ex-actly matched with the 21 functions discovered previously in [19].

In Figure 5, we show the identified sub-networks among the top 100 marker genes. The genes in the same pro-tein complex or biochemical pathway tend to perform sim-
Table 2. The ranking of known breast can-cer susceptibility genes. We compare the ranking of the known cancer genes obtained by the HyperGene algorithm with the ranking calculated by Correlation Coefficients (CC).

We set  X  = 0 . 5 and  X  = 1 and 0 . 001 to test the HyperGene algorithm.
 ilar biological functions and may lead to same or simi-lar diseases [12, 8]. As shown in Figure 5, many known causative cancer genes play critical roles and are present with other susceptibility candidate genes in the pathway networks. TP53-subnetwork is involved with glucocorti-coid receptor signaling, p53 signaling and B cell recep-tor signaling pathways, and BRCA1-subnetwork is over-represented with glucocorticoid receptor signaling, estrogen receptor signaling, and RAR activation. Other networks are also involved with glucocorticoid receptor signaling, RAR activation, estrogen receptor signaling and other canonical pathways. All those over-represented biological pathways are closely linked with breast cancer 4 . This observation again supports the hypothesis that cancer genes share spe-cific pathways involved with disease and they often inter-act with each other in a protein-protein interaction network [3, 19, 4, 8].
Utilizing the prior knowledge introduced from a protein-protein interaction network, the HyperGene algorithm out-performs SVMs and the original hypergraph-based learn-the enriched functions have p -value less than 1 . 0 e  X  9 . ing algorithm in experiments on both artificial datasets and two real breast cancer datasets. HyperGene is also capa-ble of retrieving maker genes highly relevant to the cancer. Thus, HyperGene is an effective algorithm to integrate gene expressions and protein-protein interactions for cancer out-come prediction and biomarker identification.

As large volume of human genomic and proteomic data is becoming available for cancer studies, data integration for improving cancer prognosis and treatment is turning into one of the central problems in biomedical research. Our results suggest that large scale protein-protein interaction networks contain complementary information that can po-tentially aid cancer outcome prediction and biomarker iden-tification with microarray gene expression data. The Hy-perGene algorithm is a powerful tool for handling this data integration problem.

We plan to extend the HyperGene algorithm to handle other types of prior knowledge such as Gene Ontology or pathways for learning with microarray gene expressions. The other prior knowledge might indicate other types of pri-ors on the genes, which will need to be handled differently with variants of HyperGene. We also plan to apply the Hy-perGene algorithm to study other cancers such as lung can-cer to improve the diagnosis and prognosis of these cancers. Ze Tian is supported by the Biomedical Informatics and Computational Biology (BICB) Graduate Traineeship Pro-gram at University of Minnesota.

