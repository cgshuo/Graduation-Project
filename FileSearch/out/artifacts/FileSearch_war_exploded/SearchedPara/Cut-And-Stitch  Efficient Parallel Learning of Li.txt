 Multi-core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing. O ur goal is to make use of the multi-core as well as multi-process or architectures to speed up data mining algorithms. Specifica lly, we present a parallel algorithm for approximate learning of Li near Dy-namical Systems (LDS), also known as Kalman Filters (KF). LD Ss are widely used in time series analysis such as motion captur e mod-eling, visual tracking etc. We propose Cut-And-Stitch (CAS), a novel method to handle the data dependencies from the chain s truc-ture of hidden variables in LDS, so as to parallelize the EM-b ased parameter learning algorithm. We implement the algorithm u sing OpenMP on both a supercomputer and a quad-core commercial desktop. The experimental results show that parallel algor ithms us-ing Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the serial version. In addition, Cut-And-Stitch can be generalized to other models with similar linear structur es such as Hidden Markov Models (HMM) and Switching Kalman Filters (SKF).
 Categories and Subject Descriptors: I.2.6 Artificial Intelligence: Learning -parameter learning D.1.3 Programming Technique s: Con-current Programming -parallel programming G.3 Probabilit y and Statistics: Time series analysis General Terms: Algorithms; Experimentation; Performance.
 Keywords: Linear Dynamical Systems; Kalman Filters; OpenMP; Expectation Maximization (EM); Optimization; Multi-core .
Time series appear in numerous applications, including mot ion capture [11], visual tracking, speech recognition, quanti tative stud-ies of financial markets, network intrusion detection, fore casting, etc. Mining and forecasting are popular operations we want t o do. Two typical statistical models for such problems are hid den Markov models (HMM) and linear dynamical systems (LDS, also known as Kalman filters). Both assume linear transitions on h id-den (i.e.  X  X atent X ) variables which are considered discret e for the former and continuous for the latter. The hidden states or va ri-ables in both models are inferred through a forward-backwar d pro-cedure involving dynamic programming. However, the maximu m likelihood estimation of model parameters is difficult, req uiring the well-known Expectation-Maximization (EM) method [1]. The EM algorithm for learning of LDS/HMM iterates between computi ng conditional expectations of hidden variables through the f orward-backward procedure (E-step) and updating model parameters to maximize its likelihood (M-step). Although EM algorithm ge n-erally produces good results, the EM iterations may take lon g to converge. Meanwhile, the computation time of E-step is line ar in the length of the time series but cubic in the dimensionali ty of observations, which results in poor scaling on high dimensi onal data. For example, our experimental results show that on a 93 -dimensional dataset of length over 300, the EM algorithm wou ld take over one second to compute each iteration and over ten mi n-utes to converge on a high-end multi-core commercial comput er. Such capacity may not be able to fit modern computation-inten sive applications with large amounts of data or real-time constr aints. While there are efforts to speed up the forward-backward pro ce-dure with moderate assumptions such as sparsity or existenc e of low-dimensional approximation, we will focus on taking adv an-tage of the quickly developing parallel processing technol ogies to achieve dramatic speedup.

Traditionally, the EM algorithm for LDS running on a multi-core computer only takes up a single core with limited proces sing power, and the current state-of-the-art dynamic paralleli zation tech-niques such as speculative execution [6] have few benefits du e to the nontrivial data dependencies. As the number of cores on a sin-gle chip keeps increasing, soon we may be able to build machin es with even a thousand cores, e.g. an energy efficient, 80-core chip not much larger than the size of a finger nail was released by In tel researchers in early 2007 [10]. This paper is along the line t o inves-tigate the following question: how much speed up could we obt ain for machine learning algorithms on multi-core? There are al ready several papers on distributed computation for data mining o pera-tions. For example,  X  X ascade SVMs X  were proposed to paralle lize Support Vector Machines [9]. Other articles use Google X  X  ma p-reduce techniques [8] on multi-core machines to design effic ient parallel learning algorithms for a set of standard machine l earning algorithms/models such as na X ve Bayes and PCA, achieving al most linear speedup [4, 12]. However, these methods do not apply t o HMM or LDS directly. In essence, their techniques are simila r to dot-product-like parallelism, by using divide-and-conqu er on inde-pendent sub models; these do not work for models with compli-is the length of longest acyclic path in its graphical repres entation. For example, the diameter of the LDS in Figure 1 is N . Symbol Definition Y a multi-dimensional observation sequence
Z the hidden variables ( = { z 1 , . . . , z N } ) m the dimension of the observation sequence H the dimension of hidden variables N the duration of the observation F the transition matrix, H  X  H G the project matrix from hidden to observation, m  X  H
In this paper, we propose the Cut-And-Stitch method (CAS), which avoids the data-dependency problems. We show that CAS can quickly and accurately learn an LDS in parallel, as demon -strated on two popular architectures for high performance c omput-ing. The basic idea of our algorithm is to (a) Cut both the chain of hidden variables as well as the observed variables into sm aller blocks, (b) perform intra-block computation, and (c) Stitch the local results together by summarizing sufficient statistics and u pdating model parameters and an additional set of block-specific par ame-ters. The algorithm would iterate over 4 steps, where the mos t time-consuming E-step in EM as well as the two newly introduced ste ps could be parallelized with little synchronization overhea d. Further-more, this approximation of global models by local sub-mode ls sacrifices only a little accuracy, due to the chain structure of LDS (also HMM), as shown in our experiments, which was our first go al. On the other hand, it yields almost linear speedup, which was our second main goal.

The rest of the paper is organized as follows. We first describ e the Linear Dynamical System in Section 2 and present our pro-posed Cut-And-Stitch method in Section 3. Then we describe the programming interface and implementation issues in Sectio n 4. We present experimental results Section 5, the related work in Sec-tion 6, and our conclusions in Section 7.
Here we give a brief introduction to Linear Dynamical System s (LDS), including its formalization, its learning algorith m and its connections to hidden Markov models (HMM).
 Consider a multi-dimensional sequence Y = y 1 , . . . , y length N . For example, Y could be a sequence of marker posi-tion vectors captured by video cameras, where each vector of dimensionality m . Suppose the evolution of the observation is driven by a hidden Markov process. For example, in motion cap ture modeling, hidden variables may correspond to a sinusoid mov ing pattern, while the observed motion could be periodic walkin g cy-cles. In LDS, both the transitions among the hidden variable s as well as their projections to the observations are described as linear Gaussian models (Eq (2-2)). We denote them as a matrix F for the transition ( H  X  H ) with noises {  X  n } ; and a matrix G ( m  X  H ) for the projection with the noises {  X  n } at each time-tick n . Fig-ure 1 provides the graphical representation of following eq uations defining an LDS: where z 0 is the initial state of the whole system, and  X   X  ( i = 1 . . . M ) are multivariate Gaussian noises: Figure 1: A Graphical Representation of the Linear Dynamical System: z 1 , . . . , z N indicate hidden variables; y 1 , . . . , y cate observation. Arrows indicate Linear Gaussian conditional probabilistic distributions.

Given the observation sequence, the goal of the learning algo-rithm is to compute the optimal parameter set  X  = ( 0 ,  X  , F,  X  , G,  X ) The optimum is obtained by maximizing the log-likelihood l ( Y ;  X  ) over the parameter set  X  . As mentioned in Section 1, the typical learning method for LDS is the EM algorithm [1], which iteratively maximizes the expected complete log-likelihood in a coordinate-ascent manner: In brief, the algorithm first guesses an initial set of model param-eters  X  0 . Then, at each iteration, it uses a forward-backward algo-rithm to compute expectations of the hidden variables  X  z Y ;  X  0 ] ( n = 1 , . . . , N ) as well as the second moments and covari-ance terms, which is the E-step. In the M-step, it maximizes the ex-pected complete log-likelihood of E [ L ( Y , z 1 ... N )] the model parameters. Since the computation of E [ z n | Y ] depends on
E [ z n  X  1 | Y ] and E [ z n +1 | Y ] , the straightforward implemen-tation of the EM algorithm can not exploit much instruction level parallelism.
 Although we will focus on LDS in the rest of this paper, our Cut-And-Stitch method could also be adapted to HMMs with a careful replacement of context, because their graphical models are very similar. Figure 1 shows the structure of the graphical representation of an LDS; notice that the structure remains the same for hidden Markov models, with the only differences that the hidden (and pos-sibly observed) variables are discrete and that the conditional distri-butions should be replaced by multinomial distributions. Accord-ingly, the forward-backward algorithm of HMM is still tractable and can be implemented in a similar manner, and the M-step in the learning algorithm can be modified as well.
In the standard EM learning algorithm described in Section 2, the chain structure of the LDS enforces the data dependencies in both the forward computation from z n (e.g. E [ z n | Y ;  X  ] ) to z and the backward computation from z n +1 to z n In this section, we will present ideas on overcoming such dependencies and describe the details of Cut-And-Stitch parallel learning algorithm.
Our guiding principle to reduce the data dependencies is to di-vide LDS into smaller, independent parts. Given a data sequence Y and k processors with shared memory, we could cut the sequence into k subsequences of equal sizes, and then assign one proces-sor to each subsequence. Each processor will learn the parameters, say  X  1 , . . . ,  X  k , associated with its subsequence, using the basic, sequential EM algorithm. In order to obtain a consistent set of pa-rameters for the whole sequence, we use a non-trivial method to Figure 2: Graphical illustration of dividing LDS into block s in the Cut step. Note Cut introduces additional parameters for each block. summarize all the sub-models rather than simply averaging. Since each subsequence is treated independently, our algorithm w ill ob-tain near k -fold speedup. The main design challenges are: (a) how to minimize the overhead in synchronization and summarizat ion, and (b) how to retain the accuracy of the learning algorithm. Our Cut-And-Stitch method (or CAS) is targeting both challenges.
Given a sequence of observed values Y with length of N learning goal is to best fit the parameters  X  = ( 0 ,  X  , F,  X  , G,  X ) The Cut-And-Stitch (CAS) algorithm consists of two alterna ting steps: the Cut step and the Stitch step. In the Cut step, the Markov chain of hidden variables and corresponding observations a re di-vided into smaller blocks, and each processor performs the l ocal computation for each block. More importantly, it computes t he ini-tial beliefs (marginal expectation of hidden variables) fo r its block, based on the neighboring blocks, and then it computes the im-proved beliefs for its block, independently. In the Stitch step, each processor computes summary statistics for its block, a nd then the parameters of LDS are updated globally to maximize the EM learning objective function (also known as the expected complete log-likelihood ). Besides, local parameters for each block are also updated to reflect changes in the global model. The CAS algori thm iterates between Cut and Stitch until convergence.
The objective of Cut step is to compute the marginal posterior distribution of z n , conditioned on the observations y 1 the current estimated parameter  X  : P ( z n | y 1 , . . . , y the number of processors k and the observation sequence, we first divide the hidden Markov chain into k blocks: B 1 ,. . . , each block containing the hidden variables z , the observations and four extra parameters  X  ,  X  ,  X  ,  X  . The sub-model for B i is described as follows (see Figure 2): where the block size T = N  X  i could be viewed as messages passed from next block, through the introduction of an extra hidden variable z  X  i,T .

Intuitively, the Cut tries to approximate the global LDS model by local sub-models, and then compute the marginal posterio r with the sub-models. The blocks are both logical and computation al, meaning that most computation about each logical block resi des on one processor. In order to simultaneously and accurately co mpute all blocks on each processor, the block parameters should be well chosen with respect to the other blocks. We will describe the pa-rameter estimation later but here we first describe the crite ria. From the Markov properties of the LDS model, the marginal posteri or of z i,j conditioned on Y is independent of any observed y outside the block B i , as long as the block parameters satisfy: Therefore, we could derive a local belief propagation algor ithm to compute the marginal posterior P ( z i,j | y i, 1 . . . y Both computation for the forward passing and the backward pa ss-ing can reside in one processor without interfering with oth er pro-cessors except possibly in the beginning. The local forward pass computes the posterior up to current time tick within one blo ck P ( z i,j | y i, 1 . . . y i,j ) , while the local backward pass calculates the whole posterior P ( z i,j | y i, 1 . . . y i,T ) (to save space, we omit the parameters). Using the properties of linear Gaussian condi tional distribution and Markov properties (Chap.2 &amp;8 in [1]), one c an eas-ily infer that both posteriors are Gaussian distributions, denoted as:
We can obtain the following forward-backward propagation e qua-tions from Eq (4-8) by substituting Eq (9-12) and expanding. The initial values are given by: The backward passing equations are: The initial values are given by: Except for the last block:
In the Stitch step, we estimate the block parameters, collect the statistics and compute the most suitable LDS parameters for the whole sequence. The parameters  X  = ( 0 ,  X  , F,  X  , G,  X ) dated by maximizing over the expected complete log-likelih ood function:
Now taking the derivatives of Eq 27 and zeroing out give the up -dating equations (Eq (34-39)). The maximization is similar to the M-step in EM algorithm of LDS, except that it should be comput ed in a distributed manner with the available k processors. The solu-tion depends on the statistics over the hidden variables, wh ich are easy to compute from the forward-backward propagation desc ribed in Cut .
 where the expectations are taken over the posterior margina l distri-statistics of each block on every processor. To ensure its correct execution, statistics collecting sho uld be run after all of the processors finish their Cut step, enabled through the synchronization among processors. With the local statistics for each block, where Cov ( Y ) is the covariance of the observation sequences and could be precomputed.

As we estimate the block parameters with the messages from the neighboring blocks, we could reconnect the blocks. Reca ll the conditions in Eq (9-10), we could approximately estimate th e block parameters with the following equations. Except for the first block (no need to compute  X  k and  X  k last block):
In summary, the parallel learning algorithm works in the fol low-ing two steps, which could be further divided into four sub-s teps:
Cut divides and builds small sub-models (blocks), and then each
Stitch estimates the parameters through collecting (C) local statis-
To extract the most parallelism, any of the above equations i nde-pendent of each other could be computed in parallel. Computa tion of the local statistics in Eq (31-33) is done in parallel on sors. Until all local statistics are computed, we use one pro cessor to calculate the parameter using Eq (34-39). Upon the comple tion of computing the model parameters, every processor compute s its own block parameters in Eq (40-44). To ensure the correct exe cu-tion, Stitch step should run after all of the processors finish their Cut step, which is enabled through the synchronization among pr o-cessors. Furthermore, we also use synchronization to ensur e Maxi-mization part after Collecting and Re-estimate after Maximization . An interesting finding is that our method includes the sequen tial version of the learning algorithm as a special case. Note if t he number of processors is 1, the Cut-And-Stitch algorithm falls back to the conventional EM algorithm sequentially running on si ngle processor.
In the first iteration of the algorithm, there are undefined in i-tial values of block parameters  X  ,  X  ,  X  and  X  , needed by the for-ward and backward propagations in Cut . A simple approach would be to assign random initial values, but this may lead to poor p er-formance. We propose and use an alternative method: we run a sequential forward-backward pass on the whole observation , esti-mate parameters, i.e. we execute the Cut step with one processor, and the Stitch step with k processors. After that, we begin nor-mal iterations of Cut-And-Stitch with k processors. We refer to this step as the warm-up step. Although we sacrifice some speedup, the resulting method converges faster and is more accurate. Fig ure 3 illustrates the time line of the whole algorithm on four CPUs .
We will first discuss properties of our proposed Cut-And-Stitch method and what it implies for the requirements of the comput er architecture: Figure 3: Graphical illustration of Cut-And-Stitch algorithm on 4 CPUs. Arrows indicates the computation on each CPU. Tilt-ing lines indicate the necessary synchronization and data t rans-fer between the CPUs and main memory. Tasks labeled with  X  X  X  indicate the (parallel) estimation of the posterior marginal distribution, including the forward-backward propagatio n of beliefs within each block as shown in Figure 2. (C) indi-cates the collection of local statistics of the hidden variables in each block; (M) indicates the maximization of the expected log-likelihood over the parameters, and then it re-estimates (R) the block parameters. The current Symmetric MultiProcessing (SMP) technologies pro-vide opportunities to match all of these assumptions. We imp le-ment our parallel learning algorithm for LDS using OpenMP, a multi-programming interface that supports shared memory o n many architectures, including both commercial desktops and sup ercom-puter clusters. Our choice of the multi-processor API is bas ed on the fact that OpenMP is flexible and fast, while the code gener ation for the parallel version is decoupled from the details of the learning algorithm. We use the OpenMP to create multiple threads, sha re the workload and synchronize the threads among different proce ssors. Note that OpenMP needs compiler support to translate parall el di-rectives to run-time multi-threading. And it also includes its own library routines (e.g. timing) and environment variables ( e.g. the number of running processors).

The algorithm is implemented in C++. Several issues on config -uring OpenMP for the learning algorithm are listed as follow s: To evaluate the effectiveness and usefulness of our propose d Cut-And-Stitch method in practical applications, we tested our imple-mentation on SMPs and did experiments on real data. Our goal i s to answer the following questions: We will first describe the experimental setup and the dataset we used. Table 2: Wall-clock time for the case of Walking Motion (#22) on multi-processor/multi-core (in seconds), and the avera ge of normalized running time on 58 motions (serial time = 1 ). Table 3: Rough estimation of the number of arithmetic oper-ations ( + ,  X  ,  X  , / ) in E, C, M, R sub steps of Cut-And-Stitch . Each type of operation is equally weighted, and only the larg est portions in each step are kept.
We run the experiments on a supercomputer as well as on a com-mercial desktop, both of which are typical SMPs. We used a 17MB motion dataset from CMU Motion Capture tions, each with 93 bone positions in body local coordinates . The motions span several hundred frames long (100  X  500). We use our method to learn the transition dynamics and projection matr ix of each motion, using H =15 hidden dimensions.
We did experiment on all of the 58 motions with various number of processors on both machine. The speedup for k processors is defined as According to Amdahl X  X  law, the theoretical limit of speedup is where p is the proportion of the part that could run in parallel, and (1  X  p ) is the part remains serial. To determine the speedup limit, we provide an analysis of the complexity of our algorithm by c ount-ing the basic arithmetic operations. Assume that the matrix multi-plication takes cubic time, the inverse uses Gaussian elimi nation, there is no overhead in synchronization, and there is no memo ry contention. Table 3 lists a rough estimate of the number of ba sic arithmetic operations in the Cut and Stitch steps with E, C, M, and R sub steps. As we mentioned in Section 3, the E,C,R sub steps can run on k processors in parallel, while the M step in principle, has to be performed serially on a single processor (or up to fo ur processors with a finer breakdown of the computation).

In our experiment, N is around 100-500, m = 93 , H = 15 , thus p is approximately 99 . 81%  X  99 . 96% .

Figure 4 shows the wall clock time and speedup on the super-computer with a maximum of 128 processors. Figure 5 shows the wall clock time and speedup on the multi-core desktop (maxim um 4 cores). We also include the theoretical limit from Amdahl X  s law. Table 2 lists the running time on the motion set. In order to co m-pute the average running time, we normalized the wall clock t ime relative to the serial one, defined as where t k is wall clock time with k processors.

The performance results show almost linear speedup as we in-crease the number of processors, which is very promising. Ta king a closer look, it is near linear speedup up to 64 processors. T he speedup for 128 processors is slightly below linear. A possi ble ex-planation is that we may hit the bus bandwidth between proces sors and memory, and the synchronization overhead increases dra mati-cally with a hundred processors.
In order to evaluate the quality of our parallel algorithm, w e run our algorithm on a different number of processors and compar e the error against the serial version (EM algorithm on single pro cessor). Due to the non-identifiability problem, the model parameter s for different run might be different, thus we could not directly compute the error on the model parameters. Since both the serial EM le arn-ing algorithm and the parallel one tries to maximize the data log-likelihood, we define the error as the relative difference be tween log-likelihood of the two, where data log-likelihood is com puted from the E step of the EM algorithm. where Y is the motion data sequence,  X   X  k are parameters learned with k processors and l (  X  ) is the log-likelihood function. The er-ror from the experiments is very tiny, with a maximum 0 . 3% mean 0 . 17% , and no clear evidence of increasing error with more processors. In some cases, the parallel algorithm even foun d higher ( 0 . 074% ) likelihood than the serial EM. Note there are limitations of the log-likelihood criteria, namely higher likelihood d oes not necessarily indicate better fitting, since it might get over -fitting. The error curve shows the quality of parallel is almost ident ical to the serial one.
In order to show the visual quality of the parallel learning a lgo-rithm, we observe a case study on two different sample motion s: walking motion (Subject 16 #22, with 307 frames), jumping mo -tion (Subject 16 #1, with 322 frames), and running motion (Su bject 16 #45, with 135 frames). We run the CAS algorithm with 4 cores to learn model parameters on the multi-core machine, and the n use versus number of cores k . these parameters to estimate the hidden states and reconstr uct the original motion sequence. The test criteria is the reconstr uction error (NRE) normalized to the variance, defined as where y i is the observation for i -th frame and  X  y i is the reconstructed with model parameters from 4-core computation. Table 4 show s the reconstruction error: both parallel and serial achieve ver y small er-ror and are similar to each other. Figure 6 and Figure 7 show th e reconstructed sequences of the feet coordinates. Note our r econ-struction (red lines) is very close to the original signal (b lue lines).
Data mining and parallel programming receives increasing i nter-est. Parthasarathy et al. [2] develop parallel algorithms f or mining terabytes of data for frequent itemsets, demonstrating a ne ar-linear scale-up on up to 48 nodes.

Reinhardt and Karypis [13] used OpenMP to parallelize the di s-covery of frequent patterns in large graphs, showing excell ent speedup of up to 30 processors.

Cong et al. [7] develop the Par-CSP algorithm that detects cl osed sequential patterns on a distributed memory system, and rep ort good scale-up on a 64-node Linux cluster.

Graf et al. [9] developed a parallel algorithm to learn SVM th rough cascade SVM. Collobert et al. [5] proposed a method to learn a mix-ture of SVM in parallel. Both of them adopted the idea of split ting dataset into small subsets, training SVM on each, and then co mbin-ing those SVMs. Chang et al. [3] proposed PSVM to train SVMs on distributed computers through approximate factorizati on of the kernel matrix.

There is an attempt to use Google X  X  Map-Reduce [8] to paral-lelize a set of learning algorithm such as na X ve-Bayes, PCA, linear regression and other similar algorithms [4,12]. Their fram ework re-quires the summation form (like dot-product) in the learnin g algo-rithm, and hence could distribute independent calculation s to many processors and then summarize them together. Therefore the same techniques could hardly be used to learn long sequential gra phi-cal models such as Hidden Markov Models and Linear Dynamical Systems.
In this paper, we explore the problem of parallelizing the le arning algorithm for LDS models on symmetric multiprocessor archi tec-tures. The main contributions are as follows:
Future work could extend our Cut-And-Stitch method to models with similar chain structure such as HMMs. Another directio n is to extend Cut-And-Stitch for switching Kalman filters (SKF).
Acknowledgements. This material is based upon work sup-ported by the National Science Foundation under Grants No.I IS-0326322. The data used in this project was obtained from mo-cap.cs.cmu.edu supported by NSF EIA-0196217. This work is a lso partially supported by the Pennsylvania Infrastructure Te chnology Alliance (PITA), an IBM Faculty Award, a Yahoo Research Al-liance Gift, and a SPRINT gift. Any opinions, findings, and co n-clusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the f unding parties. We would like to thank Pittsburgh Supercomputing C enter and National Center for Supercomputing Applications for pr ovid-ing machines and support, Wanhong Xu for his help on processi ng data, and Erika Chin and David Zhu for their valuable suggest ions. [1] C. M. Bishop. Pattern Recognition and Machine Learning . [2] G. Buehrer, S. Parthasarathy, S. Tatikonda, T. Kurc, and [3] E. Chang, K. Zhu, H. Wang, H. Bai, J. Li, Z. Qiu, and [4] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. R. Bradski, A. Y. [5] R. Collobert, S. Bengio, and Y. Bengio. A Parallel Mixtur e [6] C. B. Colohan, A. Ailamaki, J. G. Steffan, and T. C. Mowry. [7] S. Cong, J. Han, and D. Padua. Parallel mining of closed [8] J. Dean and S. Ghemawat. Mapreduce: Simplified data [9] H. P. Graf, E. Cosatto, L. Bottou, I. Dourdanovic, and [10] Intel. Intel research advances  X  X ra of tera X : [11] L. Li, J. McCann, C. Faloutsos, and N. Pollard. Laziness is a [12] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and [13] S. Reinhardt and G. Karypis. A multi-level parallel a set of purple lines; this illustrates the high accuracy of Cut-And-Stitch . running motion (subject 16 #45).
