 Document network is a kind of intriguing dataset which can provide both topical (textual content) and topological (rela-tional link) information. A key point in viably modeling such datasets is to discover proper denominators beneath the two different types of data, text and link. Most previous work in-troduces the assumption that documents closely linked with each other share common latent topics. However, the het-erophily (i.e., tendency to link to different others) of nodes is neglected, which is pervasive in social networks. In this paper, we simultaneously incorporate community detection and topic modeling in a unified framework, and appeal to Canonical Correlation Analysis (CCA) to capture the latent semantic correlations between the two heterogeneous laten-t factors, community and topic . Despite of the homophily (i.e., tendency to link to similar others) or heterophily, CCA can properly capture the inherent correlations which fit the dataset itself without any prior hypothesis. Logistic normal prior is also employed in modeling network to better capture the community correlations. We derive efficient inference and learning algorithms based on variational EM methods. The effectiveness of our proposed model is comprehensively verified on three different types of datasets which are namely hyperlinked networks of web pages, social networks of friends and coauthor networks of publications. Experimental result-s show that our approach achieves significant improvements on both topic modeling and community detection compared with the current state of the art. Meanwhile, our model is impressive in discovering correlations between extracted topics and communities.
 H.0 [ Information Systems ]: General; H.3.5 [ Information Storage and Retrieval ]: Online Information Services Algorithms, Experiments Document Networks, Probabilistic Topic Model, Communi-ty Detection, CCA
The prevalence of social media has strongly propelled the popularity of User Generated Content (UGC) online. Un-like the past websites, social networks thrive on relationship-s. The rich connectivity patterns make documents contain much more than a set of plain texts. Rather, they can be viewed as text-associated nodes in graphs, or we call them document networks. Previous studies have evolved to two trains of thoughts in modeling the heterogeneous sources, text and link, in document networks. The first kind tries to absorb link information into topic modeling [38, 18, 9, 31, 8, 22]. The emphasis of such models is to improve the topic quality. Link information is only treated as additional knowledge. Generally, these models are always extensions of traditional topic models, such as LDA [7]. Latent patterns beneath the massive links are neglected. The second kind of thought tries to absorb textual information to better decom-pose the network [20, 40, 44, 21]. Such work always focuses on community detection. Topic similarity is only considered as latent links to reinforce or supply the observed edges. Though very different in methods, the two thoughts share a common assumption that two nodes closely linked with each other have very similar topic distribution. However, we must challenge this intuition to its limitations. Homophi-ly (i.e., tendency to link to similar others) is a ubiquitous attribute of nodes in networks. For example, users like to follow others with similar interests, and web pages tend to cite relevant sites. However, heterophily (i.e., tendency to link to different others) is also important and prevalent. In twitter, people like not only users that share similar topics but also ones who post interesting but unseen news or jokes. Creative and original content always draws more attention, which may have nothing in common with the past posts of an interested user. Meanwhile, it is not strange that friends in a social network have conflict interests. Only considering homophily may neglect or even wrongly model such infor-mation.

In this paper, we would like our model to inherently cap-ture the latent correlations between text and link so that it can properly fit the dataset and has no strong prior hypoth-esis. To achieve better efficiency and more interpretable results, we first reduce text and link to lower dimensional representations, topic and community. Canonical Correla-tion Analysis (CCA) [3, 17] which is a well-known machine learning algorithm is then applied to capture the latent se-mant ic association between them. CCA aims to maximize the covariance between two random sets, thus it builds a reasonable bridge for topic and community to interact with each other. This not only makes our model elegant in al-gebra but also totally unsupervised in implementation. On account of these advantages, we name our model Canoni-cal Topical-Topological Analysis (CTTA). Specifically, it is an unified model which can simultaneously perform topic modeling and community detection, as well as discovering the correlations between them. Some previous work [42, 29] also aims at the same problem as we do, but it is a pity that they can not jump out of the homophily assumption mentioned above.

Discovering association between topic and community al-lows us to predict links when only given texts (or vise ver-sa) [28, 4, 5]. Similarly, finding topic-topic (community-community) relations would enable us to predict similar con-tent (links) when given specific content (links), which is also what we expect our model to achieve. It is feasible if we carefully select the topic modeling and community detec-tion algorithms. Parametric topic models (i.e., LDA [7] and PLSA [19]) and nonparametric Bayesian methods (i.e., HD-P [37]) have achieved great success in extracting meaningful topics from document collections. Correlated Topic Model (CTM), as a famous variation of LDA, replaces the Dirich-let prior with normal prior [2] so that it overcomes LDA X  X  limitation in describing topic correlations. Coincidentally, in the probabilistic interpretation of CCA [3, 17], the two random sets are also drawn from normal distribution (Sec-tion 3.2). Thus we can naturally combine CTM and CCA by overlapping their sharing part. When it comes to commu-nity detection, probabilistic models [1, 24] also show great power in projecting nodes to low-dimensional communities. One of the state-of-the-art algorithms is Mixed Member-ship Stochastic Blockmodels (MMSB) [1]. Similar to LDA, MMSB also uses Dirichlet prior for the community member-ship of each node. Inspired by the improvement from LDA to CTM, we replace this prior with normal distribution as well. This operation also enhances the ability of MMSB in capturing the correlations between communities. Now we can take the same step to combine MMSB and CCA as we did for CTM and CCA. With CTM modeling topics, MMSB detecting communities, and CCA capturing mutual interac-tions, our model links everything together.

The experimental evaluation is comprehensively conduct-ed on three different datasets, hyperlinked networks of web pages, social networks of friends and coauthor networks of publications. The results demonstrate not only the prepon-derance of our model in modeling topics and detecting com-munities, but also the capability in discovering correlations between them. The main contributions of this paper are as follows:
The rest of this paper is organized as follows: Section 2 introduces the related work. Section 3 describes the details of our proposed CTTA model. Section 5 shows the demon-stration and experiment results compared with baselines. Conclusions are given in Section 6.
Statical topic models such as LDA [7] and PLSA [19] use a set of topic distributions over a fixed vocabulary to describe a corpus of documents. The extracted topics can be con-sidered as low-dimensional and semantic coherent represen-tations for the massive document collection. The main im-provement from PLSA to LDA is the introduction of Dirich-let prior for the multinomial distributions, which enables us to incorporate prior knowledge when modeling topics. Such attribute leads to many subsequent studies [25, 35, 32] which aim to improve the topic coherence by absorbing prior knowledge into the prior parameters. Mimno et al. [32] propose an extremely flexible model named Dirichlet-multinomial regression (DMR) which can encode arbitrary features of documents into the parameters of Dirichlet pri-or. A number of extensions [38, 18] of DMR demonstrate its effectiveness. Network structure can also be converted to proper document features and as well be modeled by DMR. Hefny et al. [18] use path-constrained random walks algo-rithm [27] to express complex relations and encode graph information into their DMR based topic model. Wahabza-da et al. place Gaussian process (GP) prior for DMR to better incorporate node features. Specifically, the covari-ance function of GP is a function of document attributes and relations, in which p-steps random walk kernel [45] is selected to express the link structures. Another extension of LDA without applying DMR to model document networks is Relational Topic Model (RTM) [9]. In RTM, each doc-ument is first generated from topics as in LDA. The links between documents are then modeled as binary variables, one for each pair of documents. The similarity between a pair of documents is measured by the Hadamard (element-wise) product of their topic distributions. Though powerful, the Hadamard product only allows the-same-topic interac-tions. Chen et al. [11] overcome this limitation of RTM by replacing the Hadamard product with a full weight matrix which allows all pairwise topic interactions, which is also more suitable for asymmetric networks. Other than exten-sions of LDA, Mei et al. augment the PLSA objective func-tion with a network harmonic regularization penalty that encourages topic mixtures of related documents to be sim-ilar. All the models mentioned above are general and can be applied to any document networks. However, their main concern is topic modeling.

On the other hand, researches also fall into a specific kind of dataset which is publications (writings) with coauthor or citation networks. Such kind of dataset is a special case of document networks. Researchers are concerned with not only the texts and links, but also the authors and contexts. However, we can still gain much inspiration from them. The Auth or-Topic (AT) model [36] learns the topic of a document conditioned on the mixture of authors, which implicitly as-sumes that the distribution over topics of a certain author keeps the same for all his/her documents. Kataria et al. [23] model the influence of cited authors along with the interests of citing authors. They hypothesize that apart from the c-itations present in documents, the context surrounding the citation mention provides extra topical information about the cited authors. Zheng et al. [44] conduct community detection algorithm considering both the participations of authors and textual contents of documents. Mccallum et al. [30] propose the Topic-Author-Recipient model to consid-er sender and receiver information so that the distribution over topics is conditioned distinctly on both the sender and recipient.

When switching our emphasis from the textual content to the network structure in documents networks, we would gain another train of studies which aim to discover patterns hid-den in the intricate edges. Mixture Membership Stochastic Blockmodel (MMSB) [1] is one of the state-of-the-art genera-tive models, which can simultaneously factorize the network and infer the latent communities. Traditionally, communi-ty is a group of nodes with denser interactions amongst its members than those between its members and the remain-der. Tremendous studies [34, 14, 26, 41] such as spectral clustering have paid much attention in discovering commu-nities in massive networks. To achieve a tradeoff between analyzing documents and exploiting network, Yin et al. [42] handle topic modeling and community detection in the same framework. In their proposed model LCTA (Latent Com-munity Topic Analysis), one community can correspond to multiple topics and multiple communities can share the same topic. Liu et al. [29] also consider that topic and communi-ty are inner-dependent. Whether a link exists between two documents follows a binomial distribution parameterized by the similarity between topic mixtures and community mix-tures. Our proposed model is more powerful in both topic modeling and community detection. Meanwhile, it is also elegant in capturing the semantic relations between topics and communities compared to previous work. In this section, we give the instantiation of our proposed Canonical Topical-Topological Analysis (CTTA) model with Correlated Topic Model (CTM) [6], Mixed Membership S-tochastic Blockmodels (MMSB) [1] and Canonical Correla-tion Analysis (CCA) [17], which are three main ingredients of our model. The model description that follows assumes the reader is familiar with these three basic models. How-ever, what is still worth mentioning first would be the prob-abilistic interpretation of CCA.
Given two random sets of variables x 1 and x 2 , CCA is concerned with finding a pair of linear transformations so that one component within each set of transformed vari-ables is correlated with a single component in the other set. Meanwhile, the correlation between x 1 and x 2 is maximized. Bach et al. [3] provide a probabilistic interpretation of CCA , which enables the use of local CCA models as components of a larger probabilistic model.
 on the latent correlation factor z  X  R d . The generative process of CCA can be described as follows: A very important feature that CCA possesses is that it can measure the correlation between two sets in different dimen-sional spaces. In our CTTA model, this feature enables us to set different number of topics and communities in ex-ploring the document network. Though word and link are heterogeneous, CCA captures the latent semantic associa-tion between their low-dimensional representations, namely topic and community. In this section, we present the details of our proposed Canonical Topical-Topological Analysis (CTTA) model, which simultaneously explores the topical and topological factors in document networks. For the latent topic factor, we appeal to CTM, which improves LDA by replacing the Dirichlet prior with normal distribution to capture the topical corre-lations. By coincidence, both random sets x 1 and x 2 are drawn from normal distribution in the generative process of CCA. Thus we can naturally integrates CCA and CTM by fitting x 1 (or x 2 ) as the per-document topic proportion prior. The topical correlation is explicitly captured by the covariance matrix of the normal distribution. For the la-tent community factor, we employ MMSB to factorize the network and infer latent clusters. However, an intractable problem which we confront first is that the mixed communi-ty (block) membership for each node (document) is drawn from Dirichlet prior. Inspired by CTM, we also appeal to log normal distribution and generate the block membership mally distributed. Now the integration of MMSB and CCA is the same as that of CTM and CCA. Topic modeling and community detection are elegantly coupled together with the help of CCA, and both do not loss their adept ability either.
The directed graphical model of CTTA is depicted in Fig. 1. Let D be the number of documents in the corpus where each document d has N d words. The vocabulary size is de-noted by V . K is the manually specified topic size. T is the number of communities. L is the dimension of latent corre-lation factor in CCA.  X  1: K are K multinomials over the word vocabulary. Y ( p, q ) denotes the measurement taken on the pair of documents ( p, q ). In this paper, we consider the doc-ument network as a directed graph, where Y ( p, q )  X  { 0 , 1 } . If there is a link from document p to q , Y ( p, q ) equals 1. Y ( p, q ) also need not equal Y ( q, p ). Each document d is as-sociated with a topic proportion vector  X  d and a community membership vector  X  d , where f (  X  d,k ) denotes the probabil-ity of document d selecting topic k , and f (  X  d,t ) denotes the probability of d belonging to community t . The probabilities of interactions between different communities are defined by having a connection from a node in community t 1 to a node in community t 2 . For edge Y ( p, q ), the vector resents the community membership of document p when it connects to q , and bership of document q when it is connected from p . We Figure 1: The graphical model of Canonical Topical-Topo logical Analysis spare no efforts to keep the symbols similar to those in the original paper of CTM and MMSB so that the generative process could be more readable. The generative process of CTTA is as follows: 1. Draw a global L-dimensional Gaussian correlation fac-2. For each document d  X  X  1 , ..., D } 3. For each document d  X  X  1 , ..., D } 4. For each pair of documents ( p, q )  X  D  X  D :  X  X  X  The f ( x ) function mentioned above refers to: f ( x i We make the following notes for the Canonical Topical-Topological Analysis model: (1) The generative steps (step 2a and 3) form the core of the Correlated Topic Model. In step 2a, the per-document topic proportion  X  d is drawn from Gaussian prior which de-pends on the latent correlation factor y of CCA. Step 2b and 4 form the main body of Mixed Membership Stochas-tic Blockmodels. The Dirichlet prior in MMSB is replaced by Gaussian distribution in Step 2b. Then  X  d is mapped to the topic simplex with function detailed in step  X  X  X  . The Gaussian distribution also reinforces the ability of MMSB in capturing the correlations between communities. Step 1 and 2 are the generative steps of CCA, from which we can easily find that communities and topics are closely coupled together and significantly interact with each other. (2) The model parameters in CTTA comprise the set  X  = latent community indicators are denoted by { [1 , D ] } = latent variables form the variable set  X  = { y,  X ,  X , z, Note that the parameters M z and M c are matrices with dimensions K  X  L and T  X  L respectively. They linearly transform topic and community from their own spaces to the space of y (step 2a and 2b). The observable variables are the documents D and links Y .
 The following equation gives the probability that D and Y arise from the CTTA model given the model parameters  X : Now our task turns to find the optimal model parameters  X  which can maximize the posterior probability given the observed documents and links: Unfortunately, it is computationally intractable to deter-mine the true posterior distribution, because the normal (or the logistic normal when considering the f ( x ) function) prior is not conjugate to the multinomial. Meanwhile, too many latent variables are coupled together. In this paper, we in-troduce the variational EM algorithm [39] which is widely applied by Blei et al [6, 7]. In many optimal problems, it is more computationally manageable than existing standard methods, such as Markov Chain Monte Carlo.
Though the unified CTTA model is elegant in simultane-ously modeling documents and accompanied network, poste-rior inference is the central challenge to use it. In this paper, we make use of variational EM methods to efficiently obtain an approximation of the posterior distribution. The deriva-tion is complicated, which however considerably remedies the computational burden when executing.
During the E-step, we update the posterior distribution over the unobservable variables  X  = { y,  X ,  X , z, mean-field variational methods, we posit a simple distribu-tion of the latent variables with free parameters so that the approximation is close in the Kullback-Leibler divergence to the true posterior. In this paper, we introduce a factorized distribution q ( X ) in which the latent variables are indepen-dent of each other. The details of q ( X ) a re listed as follows: Note that  X  d (  X  d,i where i  X  [1 , K ]) is set as K indepen-dent univariate Gaussians, and  X  d (  X  d,i where i  X  [1 , T ]) is set as T independent univariate Gaussians. It comes true since these variational parameters are fit using a single ob-served document d (In Eqs. 17-24, the sum of documents is removed from both sides of the equations). There is no ad-vantage in introducing a non-diagonal variational covariance matrix. Now we have a set of free variational parameters Following Jordan et al. [39], we bound the log likelihood using Jensen X  X  inequality. That is where log P ( D , Y ,  X  |  X  ,  X  ) is the log likelihood function for the complete data set which contains both latent variables and observed document network. We expand the lower bound of the log likelihood as follows: Then we further expand the above equation in terms of the model parameters  X  and the variational parameters  X . Each term on the right-hand side is shown below: wh ere tr ( X  y ) is the trace of the covariance matrix  X  y This term is expanded utilizing the property of matrix nor-mal distribution X  X  X N ( M, U, V ), of which E ( X  X  BX ) = V tr ( UB  X  ) + M  X  BM . Similar expansion goes for where w d,n denotes the n th word in document d . The per-document topic proportion  X  d is drawn from Gaussian dis-tribution, from which a topic z is sampled with the equa-gacy leads to the difficulty in computing the expected log probability of a topic assignment. We lower bound the ex-pectation of the log likelihood with the Taylor expansion log( x ) = log(  X  ) +  X   X  1 ( x  X   X  ) + O ( x ). Thus where  X  1 d is an additional variational parameter. Similar derivations are conducted that
E q (log P (
E q (log P ( E (log P ( Y ( p, q ) | where p = g (or q = h ) refers to that document p (or q ) belongs to the community g (or h ).
 H ( q ) = 1 2 log de t ( X  y ) + +
X  X 
X We substitute Eqs. 1-9 into the log likelihood function, and maximize the lower bound by taking the partial derivatives with respect to the variational parameters  X  and setting them to zero. The update equations are listed below: y =  X  y  X  X  X   X   X  X  X   X  For the variational parameters {  X   X  d ,  X   X d ,  X   X  d ,  X  no analytic solution. Thus we apply the Newton-Raphson method. For  X   X  d , we have X The second order derivative for  X  d is The diag ( v ) function returns a diagnose matrix by placing the vector v on its diagnose. The derivative for each coordi-nate of  X   X d is  X  Similarly, we can also derive the first and second derivatives for {  X   X  d ,  X   X d } as follows:  X  Sparsity : In large networks such as Facebook, there are always a large amount of non-interactions. In other word-s, they are sparse . However, we can hardly say that people with no links between each other in Facebook are not friends in reality, or they have little in common. Analogically, we either can not say that web pages contain no mutual hyper-links are irrelevant. Such nodes may just lack opportunity aware of each other X  X  existence in the network. In these cases, treating the missing links as unobserved variables is more faithful to the underlying semantics of the data, which is also accepted in [9, 1]. Thus the D  X  D pair-wise space of step 4 in the generative process is reduced to the space of all observed links. This operation can not only reflect our lack of knowledge about the relationship in the network but also considerably decrease the computational cost.
In the E-step, we update the variational parameters  X  to find the tightest lower bound on the log likelihood based on the observations of documents and accompanied network. In the M-step, we update the model parameters  X  to maximize the log likelihood based on the updated variational param-eters . Similar to the process in the E-step, we calculate the partial derivatives with respect to each model parameter and set them to zero. The update equations are listed as follows: where the  X  ( w d,n = v ) is the Kronecker delta function which returns 1 only when w d,n equals v . We iteratively execute the E-step with Eqs. 10 -24 and M-step with Eqs.25 -32 until the model parameters converge.
In this section, we present the quantity and quality eval-uations for our proposed model with some state-of-the-art baselines from three aspects: topic modeling , community de-tection and topic-community correlation analysis . By joint-ly modeling documents and alongside network, we consider that the analysis of communities and topics would mutually enhance each other, and the quality of both would achieve a prominent improvement. On the other hand, CCA reveal-s the latent semantic correlations between community and topic. To better present the advantages brought by CCA, we visually construct a relational graph with the discovered associations as edges. Four different models are compared in the experiments: since its perplexity scores reach ten thousands)
Datasets: We apply our model on three public dataset-s: Foursquare check-in dataset with friendship relations 1 blog posts with hyperlink network 2 and DBLP dataset with coauthor graph 3 . From the Foursquare dataset, we extract the check-ins of people registered at Los Angeles and aggre-gate all tips (texts posted by users to describe a location) of a person as a single document. In order to make a docu-ment contain certain number of words, we remove the users who post less than 30 tips. Then we link the documents ac-cording to the friendship relations between the correspond-ing users. The blog posts dataset contains technical blogs which cite others with hyperlinks. Since this dataset is of grea t quality, we take no more operations to remove noise. Dealing with the DBLP dataset is a little complicate. We first extract four different conferences, SIGIR, WWW, KDD and NIPS from the large xml file. For each paper, the title in text and all its authors are extracted. We then construct the coauthor network by linking two unique authors with an edge if they have coauthored in at least one paper. To make the coauthor network not so sparse, we remove authors who publish papers less than 7. The three datasets are denoted as LA, Tech and DBLP respectively in the following parts for brevity. The statistics of the three processed datasets are shown in Table 1.

Parameter Settings: We implement the LDA and RTM with Gibbs sampling methods. Both models are trained us-ing 2000 Gibbs iterations to ensure convergence. We set the hyperpameters  X  = 50 /K and  X  = 0 . 01 heuristically, which reports the best model quality in [16]. LCTA and NetPLSA are inferred by EM algorithm. We conduct the inference al-gorithm until the relative change in the log likelihood is less than 10  X  3 , which is also the threshold set in our model.
Many objective metrics have been applied to measure the quality of generated topics. In language modeling, the Per-plexity is used in convention, which is algebraicly equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization perfor-mance. Thus we first compare the perplexity score to mea-s ure how well our model fits the data. However, previous work also points out that human judgments are sometimes contrary to the perplexity measure [10, 13]. To better re-flect the advantage of introducing network information, we secondly apply Topic Coherence metric [33] in this paper. This metric is firstly proposed by Mimno et al. [33]. They point out that the topic coherence score corresponds well with human coherence judgments. This metric is also wide-ly used by Chen et al [12, 13]. Good model would generate coherent topics with accurate semantic clustering, and high-er coherence score indicates higher quality of topics. In our experiment, 20 most probable words in each topic are used and the average coherence value is calculated for compari-son. We fix the number of communities to T = 10 in LCTA and CTTA, which are the only two models capable to dis-cover communities.

Figure 2 illustrates the perplexity and coherence scores on different number of topics. Note that the perplexity scores of LCTA incredibly reach ten thousands, which are much bigger than the other models. Thus we do not plot it in the figure for clarity. We can see that our proposed model achieves impressive improvement on all the three dataset-s. In Figure 5, we directly show three example topics of which the top 10 words are extremely coherent and inter-pretable. RTM yields very close perplexity scores to that of LDA, but it outperforms LDA in topic coherence with a large margin. NetPLSA performs extraordinarily well on Tech dataset. However, it gets much worse results on LA and DBLP. This may come from its assumption that topic similar documents tend to have a link, which is practical to webpages with hyperlinks, but friendship and coauthorship may not follow this hypothesis. Our model is elegant in cap-turing the latent semantic correlations between topics and communities. It aims to fit the observed data and relies on no prior hypothesis. Thus it is general for all the corpora with network structure.
An important advantage of CTTA model is its ability to si-multaneously model topics and discover communities. How-ever, it is always hard to intuitively depict the latent clus-ters beneath the network. Previous work [31] directly plots out the network with some standard graphs layout algorith-m such as  X  Spring Embedder  X . However, when it comes to large graph with thousands of edges, this method loses its ef-fectiveness. The generated figure is always a massive gather of nodes, and we can hardly detect any community visually. In this paper, we measure the grouping result and cut edge weights to evaluate the performance. We compare our model with LCTA and Spectral Clustering (SC). SC is a powerful tool in analyzing communities on large networks [41], which is widely applied by researchers all over the world.
Figure 3 depicts the grouping results. The number of communities is restricted to 10. We can see that our model yields similar clustering patterns as SC but has more bal-anced clusters. This means that CTTA correctly reveals the topological information in the datasets. But with in-corporating topical information, we make the results better. LCTA outputs extremely balanced clusters which are total-ly different from those of SC. This is unacceptable. Fig 4 show s the cut edge weights when the number of communities is set to 10 and 15. This metric presents the total weight of edges across different communities. Lower value indicates better clustering effect. On all the three datasets, CTTA gets much lower scores. The performance of LCTA is not so stable with the changing of community size. This may be partially caused by its assumption that two linked users are drawn from the same community. Generally, with the increase of community size, CTTA performs better.
In this part, we give a direct evaluation of the results gen-erated by our model. With the topic-topic (T-T) correlation-s captured by CTM, the community-community (C-C) cor-relations captured by MMSB, and the topic-community (T-C) correlations captured by CCA, the CCTA model builds a complete graph that links every component together. Thus we visually depict the extracted topics and communities, as well as the edges (T-T, C-C, T-C correlations) amongst them. Note that only the pair which has prominently high-er relevancy in the covariance matrix is connected with an edge. It is a pity that nodes in LA and Tech are common users and web pages, which makes it hard for readers to sub-jectively judge the performance without knowing the users X  interests or carefully reading the whole web page content. While authors in the DBLP dataset are more familiar to readers. What X  X  more, most authors only concentrate on their major research interests, which are also easy to access on the web. Thus the relational graph on DBLP is much easier to understand and judge. Here the 10 most probable words in the topic-word distribution are extracted to repre-sent each research topic. Communities are labelled with the names of authors who publish the most papers in it.
From Figure 5 we can see that the Bayesian Methods topic is associated with all the extracted communities and topics, which may be on account of its popularity in different field-s. The correlations between communities and corresponding topics are accurately captured by the latent correlation fac-tors. We also find an edge connecting the communities of Information Retrieval and Language Modeling , which may be because the numerous overlapping algorithms applied by both. Meanwhile, the top 10 words in each topic are very coherent, and we can give very accurate semantic interpre-tation to label them.
This paper proposes a novel model CTTA which simulta-neously performs topic modeling and community detection on document networks. The introduction of CCA makes our model capable to analyze correlations between the het-Figure 5: Correlations amongst topics and commu-niti es erogeneous topic and community, as well better model the heterophily (i.e., tendency to link to different others) and ho-mophily (i.e., tendency to link to similar others) attributes of nodes. Comprehensive evaluations on three different dataset-s show that CTTA outperforms state-of-the-art baselines with significant improvements. With increasing studies fo-cusing on document networks, we believe that our proposed model is promising to advance the researches in this field. The application of CCA also offers inspirations for studies which would like to discover correlations between two sets of variables in other fields.
The research of authors is partially supported by the Na-tional Natural Science Foundation of China under Grant No. 91218301 and No. 61202383, HongKong, Macao and Taiwan Science &amp; Technology Cooperation Program of China un-der Grant No. 2013DFM10100, the Program for New Cen-tury Excellent Talents in University (NCET) under Grant No. NCET-12-0414, and the Natural Science Foundation of Shanghai under Grant No. 12ZR1451200. [1] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. [2] J. Aitchison. The statistical analysis of compositional [3] F. R. Bach and M. I. Jordan. A probabilistic [4] L. Backstrom and J. Leskovec. Supervised random [5] N. Barbieri, F. Bonchi, and G. Manco. Who to follow [6] D. Blei and J. Lafferty. Correlated topic models. [7] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [8] D. Cai, X. Wang, and X. He. Probabilistic dyadic data [9] J. Chang and D. M. Blei. Relational topic models for [10] J. Chang, S. Gerrish, C. Wang, J. L. Boyd-graber, and [11] N. Chen, J. Zhu, F. Xia, and B. Zhang. Generalized [12] Z. Chen and B. Liu. Mining topics in documents: [13] Z. Chen, A. Mukherjee, B. Liu, M. Hsu, [14] A. Clauset, M. E. J. Newman, and C. Moore. Finding [15] E. Erosheva, S. Fienberg, and J. Lafferty.
 [16] T. L. Griffiths and M. Steyvers. Finding scientific [17] J. F. Hair, W. C. Black, B. J. Babin, R. E. Anderson, [18] A. Hefny, G. Gordon, and K. Sycara. Random walk [19] T. Hofmann. Probabilistic latent semantic indexing. In [20] B. Hu and M. Ester. Social topic modeling for [21] B. Hu, M. Jamali, and M. Ester. Spatio-temporal [22] S. Huh and S. E. Fienberg. Discriminative topic [23] S. Kataria, P. Mitra, C. Caragea, and C. L. Giles. [24] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, [25] H. Kim, Y. Sun, J. Hockenmaier, and J. Han. Etm: [26] J. Kleinberg. The small-world phenomenon: An [27] N. Lao, T. Mitchell, and W. W. Cohen. Random walk [28] D. Liben-Nowell and J. Kleinberg. The link-prediction [29] Y. Liu, A. Niculescu-Mizil, and W. Gryc. Topic-link [30] A. McCallum, A. Corrada-Emmanuel, and X. Wang. [31] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [32] D. Mimno and A. McCallum. Topic models [33] D. Mimno, H. M. Wallach, E. Talley, M. Leenders, [34] A. B. N. Pathak and K. Erickson. Social topic models [35] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [36] M. Steyvers, P. Smyth, M. Rosen-Zvi, and [37] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [38] M. Wahabzada, Z. Xu, and K. Kersting. Topic models [39] M. Wainwright and M. Jordan. A variational principle [40] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank: [41] L. Wu, X. Wu, A. Lu, and Z.-H. Zhou. A spectral [42] Z. Yin, L. Cao, Q. Gu, and J. Han. Latent community [43] H. Zhang, C. L. Giles, H. C. Foley, and J. Yen. [44] G. Zheng, J. Guo, L. Yang, S. Xu, S. Bao, Z. Su, [45] X. Zhu, J. Kandola, J. Lafferty, and Z. Ghahramani.
