 Interpretation of events X  X etermining what the author claims did or did not happen X  X s impor-tant for many NLP applications, such as news arti-cle summarization or biomedical information ex-traction. However, detecting events and assessing their factuality is challenging. For example, while most non-copular verbs are events, words in gen-eral vary with use (e.g.  X  X rade route X  vs  X  trade with Iraq X ). Events also have widely varying, context-dependent factuality cues, such as event interactions (e.g.  X  prevent easy access ) and cue words (e.g.  X  ordered to X  vs.  X  expected to X ). As shown in Figure 1, these are common challenges that a model of event factuality must address.
In this paper, we present new data and mod-els for these tasks, demonstrating that non-experts can provide high-quality annotations which en-able fine-grained, scalar judgments of factuality. Unlike previous work, we do not use a detailed (1) U.S. embassies and military installations (2) Intel X  X  most powerful computer chip has trade route.  X  X here are no shipments at the moment. X  Figure 1: Example annotations with italicized event mentions and crowdsourced scalar factuality values u  X  [  X  3 . 0 , 3 . 0] . Positive (or negative) val-ues indicate the extent to which the author claims the events happened (or not). specification of exactly what events and factual-ity classes should be. Instead, we simply ask non-experts to find words describing things that the author claims could have happened, and rate each possibility on a scale of -3 (certainly did not happen) to 3 (certainly did). Figure 1 shows that non-expert workers X  X hen their judgments are aggregated X  X onsistently find a wide range of events and recognize the subtle differences in im-plied factuality. For example, the event set gets a score of 2.6, indicating that it likely but not cer-tainly occurred, since it was ordered , whereas the ordered event, gets a score of 3.0.

We gather data for event detection and factual-ity, reusing sentences from the TempEval-3 cor-pus (Uzzaman et al., 2013). Our approach pro-duces high-quality labels with modest costs. We also introduce simple but highly effective models for both tasks that outperform strong baselines. In particular, our factuality regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By pro-viding scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical chal-lenge. Instead of definition -driven instructions, we propose example -driven instructions and show their effectiveness.

Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prab-hakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they re-lied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing fac-tuality from the reader X  X  perspective as a distribu-tion of categories, but their annotation process re-quires manual normalization of the text. In con-trast, we model factuality from the author X  X  per-spective with scalar values, and we have an end-to-end crowdsourced annotation pipeline.

More recently, Soni et al. (2014) investigated a related problem for quoted statements on Twitter, and they also crowdsourced factuality annotations to learn regression models. While their approach is similar, we focus on predicting factuality for events that occur in every sentence. Without the restrictions of their task, we must reason about a larger variety of contextual cues.

Our method of evaluating annotator agreement (Section 3) is related to the crowdsourcing study by Snow et al. (2008), who showed that pooled non-experts can match or outperform single ex-pert annotators. In contrast, we approximate ex-pert judgments by independently sampling and ag-gregating sets of non-expert judgments. Train 192 2909 73220 We use a two-stage annotation pipeline to create the labels shown in Figure 1. Event mentions are first detected, followed by factuality judgments. As motivated in Section 1, we use instructions that are easily understandable by workers with no lin-guistic training and improve overall quality by ag-gregating multiple judgments to get the final label. Event Annotation Given a sentence, we high-light one token at a time and ask workers if it refers to an event. We use the following instructions:
We consider events to be things that may or may not occur either in the past, present or fu-ture (e.g., earthquake, meeting, jumping, talk-ing, etc.). In some cases, it is not so clear whether a word is referring to an event or not.
Consider these harder cases to be events. along with 25 example annotations that covered a large variety of cases such as nominals, statives, generic events, light verbs, and non-events. These examples include both toy sentences and sentences from the corpus to annotate. For efficiency, we did not annotate a short list of stop words, copular verbs, and auxiliaries.
 Factuality Annotation For factuality, we present a sentence with one highlighted event token at a time with the following prompt:
On a scale from 3 to -3, rate how likely the high-lighted event did or will happen according to the author of the sentence. along with 17 examples to calibrate the annota-tor X  X  judgments, including negated, conditional, hedged, generic, and nested events. The responses -3, 0, and 3 were given explicit interpretations. 3 and -3 denote respectively that the target event cer-tainly did or did not happen according to the au-thor. 0 denotes that the author is neutral and ex-presses no bias towards the event X  X  factuality. Figure 3: Agreement statistics as a function of k , the number of judgments aggregated. We choose k = 5 in both tasks for our experiments, as de-noted by the red square.
 Data collection We gathered data on Crowd-domly presented test questions with known an-swers. For each example, we collect and aggregate 5 judgments, as described below. For comparison, we annotated TempEval-3 (Uzzaman et al., 2013), keeping the existing test split and randomly hold-ing out a quarter of the training examples to create a development set. Figure 2 shows data statistics. The annotation cost is 0.5 X  per judgment for de-tection and 2 X  per judgment for factuality.
 Aggregated Agreement We introduce a simple scheme to measure agreement with aggregate data, for example when the majority class from a pool of judgments is used for the final label. Instead of comparing individuals, we want to know how often the aggregates will agree, if we were to have different groups of annotators doing the task.
Formally, we assume N samples { ( x i ,y i ) | i = 1 ,...,N } , where each x i is a token within a sen-tence, and y i = { y j M judgments for x i . Let Y be the set of possible labels, Y = { X  1 , 1 } for detection and Y = [  X  3 , 3] for factuality. Let A GG : Y k  X  Y be an ag-gregation function, which maps k judgments to a single aggregate one. For event detection, we set A
GG ( y 1 ,...,y k ) to return the majority value from the set of judgments { y 1 ,...,y k } . For factual-ity, we set A GG ( y 1 ,...,y k ) = 1 computes the mean value.

To estimate the agreement between aggregates of k judgments, we collect pairs of disjoint sub-sets of size k from the M judgments. Given y i , we define the set of aggregate judgment pairs: Figure 4: Confusion matrix between FactBank la-bels and our discretized factuality ratings. { ( A GG ( y 0 ) , A GG ( y 00 )) | y 0 ,y 00  X  y i  X  | y 0 | = | y 00 | = k  X  y 0  X  y 00 =  X  X  . 3 To measure how well these aggregates agree, we treat A GG ( y 0 ) as a candidate hypothesis and A GG ( y 00 ) as the gold la-bel and compute the appropriate evaluation metric to measure aggregate agreement. We use the F1 score for detection and Pearson X  X  correlation for factuality, as described in Section 5.

We experiment with k = 1 ,..., 9 for 100 sen-tences, allowing aggregates of up to 9 judgments, as seen in Figure 3. Aggregate agreement for both tasks improve with larger k , but returns quickly diminish. Therefore, we chose k = 5 for the full data collection to reasonably trade off between quality and quantity. In absolute terms, the agree-ment at this level is strong (92.6% F1 for detection and 83.1% correlation for factuality), demonstrat-ing that aggregate non-expert judgments can pro-duce high-quality annotations.
 Comparison to FactBank We compare our fac-tuality ratings, rounded to the nearest integer, to FactBank annotations (author source only) for overlapping events. The confusion matrix from Figure 4 shows there is strong correlation between our ratings and FactBank labels with specified cer-tainties and polarities. These labels are CT-, PR-, PS-, PS+, PR+, and CT+, corresponding to events that are seen as (certainly/probably/possibly) (not happening/happening).
 We differ most significantly in events labeled Uu (underspecified) by FactBank, which consist largely of nested events, such as  X  X andors said he X  X  double his money X  or  X  X andors hoped he X  X  double his money. X  While FactBank annotators would label both double events as Uu, our anno-tations can indicate nuances based on the author X  X  wording (i.e., said vs. hoped ). The large variation in the Uu column of the confusion matrix suggests that the factuality of an event is rarely perceived as completely neutral, even when the author does not commit to a belief in the event X  X  occurrence. Learning For the detection task, we learn a lin-ear SVM classification model. For the factuality task, we assume a dataset with N examples of la-beled events { ( x i ,y i ) | i = 1 ,...,N } , and we learn a regression model: y i = w &gt;  X  ( x i ) . We in-troduce a learning objective for regression: min that combines the advantages of LASSO (Tibshi-rani, 1996) and support vector regression (Drucker et al., 1997). It induces sparse feature weights while being insensitive to errors less than . Features For the detection model, we include features given the input word x : (1) lemma of x , (2) part of speech of x , (3) indicator for whether x is a hyponym of the event synset in WordNet and the part of speech of x , (4) Brown clusters of x and its part of speech, and (5) all dependency paths from x up to length 1. For the factuality model, given the input event mention x , we include: (1) lemma of x , (2) part of speech of x , and (3) all dependency paths from x up to length 2.

For dependency paths, we include all edge labels, the target word is omitted, and each node may or may not be lexicalized; we include all possible configurations. For example in  X  X ohn did not expect to return  X , the dependency path: not  X  [ neg ] X  expect  X  X  xcomp ]  X  return , would produce the following features: These dependency features allow for context-dependent reasoning, including many of the cases in Figure 1 where the factuality of an event de-pends on the identity of a neighboring verb. Baselines For detection, we include a baseline reimplementation of the N AVY T IME (Chambers, 2013) classification detector, one of the top per-formers in the TempEval-3 event detection task.
For factuality, we include three baselines: (1) A one-vs.-rest multi-class classifier (D ISCRETE ) us-ing our features (Section 4) and labels that are dis-cretized by rounding to the nearest integer, (2) a regression model (SVR) trained with the standard SVR objective using our features, and (3) a re-gression model (P RABHAKARAN ) trained with the standard SVR objective using features from Prab-hakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set of manually defined words.
 Implementation Details The SVM models (N and our detection model) were trained with SVM-the linear program optimizing the regression ob-jective in Section 4. All hyperparameters were tuned on the development set.
 We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hyponym features. Brown clusters with 100, 320, 1000, and 3200 clusters from Turian et al. (2010) are used in the detection features.
 Evaluation Metrics We use the standard F1 score for the evaluation of detection. For event factuality, we report two metrics, the mean abso-lute error (MAE) relative to the gold standard la-bels and Pearson X  X  correlation coefficient. While MAE is an intuitive metric that evaluates the abso-lute fit of the model, Pearson X  X  r better captures how well a system is able to recover the varia-tion of the annotations. Pearson X  X  r is also con-veniently normalized such that r = 0 for a system that blindly chooses the best a priori output and r = 1 for a system that makes no error. Detection Results Figure 5 shows development see a small drop in precision and large gains in recall, but a significant increase in F1, primarily due to the use of distributional features and more general dependency features.
 Factuality Results Figure 6 shows development and test results for predicting the factuality of gold-labeled event mentions. Our system shows an overall improvement in performance over all baselines, demonstrating that the regression model works well for this data. It is able to make more graded judgments that correlate with the aggre-gate opinions of untrained annotators. As shown in Figure 8, which compares the mean average er-ror for different buckets of factuality labels, we observe the largest gains over P RABHAKARAN in examples with low factuality, where lexical cues are especially critical.
 Error Analysis We manually studied 50 devel-opment samples where our factuality model pro-duced the largest absolute errors. Figure 7 summa-rizes the error types. The biggest challenge is the wide variety of sparse lexical cues. For example, the sentences  X  X ong Kwan will be lucky to break even X  and  X  X hat sale could still fall through if fi-nancing problems develop X  require modeling the influence of  X  X ucky to X  and  X  X all through. X  Even when these types of features do appear in the train-ing data, they tend to be very rare.

We also find cases that require inference over longer distances than our model permits. Con-sider the sentence  X  X esa had rejected a general proposal from StatesWest to combine the two car-riers. X  To know that combine is not likely to hap-pen, we must infer that it is conditioned on the proposal, which was rejected. Finally, we find that world knowledge and pragmatic inference is sometimes required. For example, in the sen-tence  X  X here was no hint of trouble in the last Figure 7: Error types for the 50 examples with the largest absolute development error. Figure 8: Mean absolute error in the development set for different labels rounded to the nearest inte-ger. Our system X  X  improvement is greater when predicting events with low factuality, which re-quires modeling sparse lexical cues. conversation between controllers and TWA pilot Steven Snyder, X  the pragmatic implication that trouble likely happened requires common knowl-edge about flights. We studied event detection and scalar factuality prediction, demonstrating that non-expert annota-tor can, in aggregate, provide high-quality data and introducing simple models that perform well on each task. There is significant room for fu-ture work to improve the results, including jointly modeling the factuality of multiple events and in-tegrating factuality models into information ex-traction and question answering systems.
 This research was supported in part by the NSF (IIS-1252835, IIS-1408287), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. The authors thank Mark Yatskar, Luheng He, and Mike Lewis for help-ful discussions, and the anonymous reviewers for helpful comments.
