 How does one make a message  X  X uccessful X ? This question is of interest to many entities, including political parties trying to frame an issue (Chong and Druckman, 2007), and individuals attempting to make a point in a group meeting. In the first case, an important type of success is achieved if the national conversation adopts the rhetoric of the party; in the latter case, if other group members repeat the originating individual X  X  point.

The massive availability of online messages, such as posts to social media, now affords re-searchers new means to investigate at a very large scale the factors affecting message propagation, also known as adoption, sharing, spread, or vi-rality. According to prior research, important fea-tures include characteristics of the originating au-thor (e.g., verified Twitter user or not, author X  X  messages X  past success rate), the author X  X  social network (e.g., number of followers), message tim-ing, and message content or topic (Artzi et al., 2012; Bakshy et al., 2011; Borghol et al., 2012; Guerini et al., 2011; Guerini et al., 2012; Hansen et al., 2011; Hong et al., 2011; Lakkaraju et al., 2013; Milkman and Berger, 2012; Ma et al., 2012; Petrovi  X  c et al., 2011; Romero et al., 2013; Suh et al., 2010; Sun et al., 2013; Tsur and Rappoport, 2012). Indeed, it X  X  not surprising that one of the most retweeted tweets of all time was from user BarackObama, with 40M followers, on November 6, 2012:  X  X our more years. [link to photo] X .
Our interest in this paper is the effect of alterna-tive message wording , meaning how the message is said, rather than what the message is about. In contrast to the identity/social/timing/topic features mentioned above, wording is one of the few fac-tors directly under an author X  X  control when he or she seeks to convey a fixed piece of content. For example, consider a speaker at the ACL business meeting who has been tasked with proposing that Paris be the next ACL location. This person can-not on the spot become ACL president, change the shape of his/her social network, wait until the next morning to speak, or campaign for Rome instead; but he/she can craft the message to be more hu-morous, more informative, emphasize certain as-pects instead of others, and so on. In other words, we investigate whether a different choice of words affects message propagation, controlling for user and topic : would user BarackObama have gotten significantly more (or fewer) retweets if he had used some alternate wording to announce his re-election?
Although we cannot create a parallel universe fortunately, a surprising characteristic of Twitter allows us to run a fairly analogous natural exper-iment : external forces serendipitously provide an environment that resembles the desired controlled setting (DiNardo, 2008). Specifically, it turns out to be unexpectedly common for the same user to post different tweets regarding the same URL  X  pairs are shown in Table 1; we see that the paired tweets may differ dramatically, going far beyond word-for-word substitutions, so that quite interest-ing changes can be studied.

Looking at these examples, can one in fact tell from the wording which tweet in a topic-and author-controlled pair will be more successful? The answer may not be a priori clear. For example, for the first pair in the table, one person we asked found t 1  X  X  invocation of a  X  X candal X  to be more attention-grabbing; but another person preferred t because it is more informative about the URL X  X  content and includes  X  X ight media portrayal X . In an Amazon Mechanical Turk (AMT) experiment (  X  4), we found that humans achieved an average accuracy of 61.3%: not that high, but better than chance, indicating that it is somewhat possible for humans to predict greater message spread from different deliveries of the same information.
Buoyed by the evidence of our AMT study that wording effects exist, we then performed a battery of experiments to seek generally-applicable, non-Twitter-specific features of more successful phras-ings.  X  5.1 applies hypothesis testing (with Bonfer-roni correction to ameliorate issues with multiple comparisons) to investigate the utility of features like informativeness, resemblance to headlines, and conformity to the community norm in lan-guage use.  X  5.2 further validates our findings via prediction experiments, including on completely fresh held-out data, used only once and after an We achieved 66.5% cross-validation accuracy and 65.6% held-out accuracy with a combination of our custom features and bag-of-words. Our clas-sifier fared significantly better than a number of baselines, including a strong classifier trained on the most-and least-retweeted tweets that was even granted access to author and timing metadata. The idea of using carefully controlled experiments to study effective communication strategies dates back at least to Hovland et al. (1953). Recent studies range from examining what characteris-tics of New York Times articles correlate with high re-sharing rates (Milkman and Berger, 2012) to looking at how differences in description affect the spread of content-controlled videos or images (Borghol et al., 2012; Lakkaraju et al., 2013). Simmons et al. (2011) examined the variation of quotes from different sources to examine how tex-tual memes mutate as people pass them along, but did not control for author. Predicting the  X  X uccess X  of various texts such as novels and movie quotes has been the aim of additional prior work not al-ready mentioned in  X  1 (Ashok et al., 2013; Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Pitler and Nenkova, 2008; McIntyre and Lapata, 2009). To our knowledge, there have been no large-scale studies exploring wording effects in a both topic-and author-controlled setting. Em-ploying such controls, we find that predicting the more effective alternative wording is much harder than the previously well-studied problem of pre-dicting popular content when author or topic can freely vary.

Related work regarding the features we consid-ered is deferred to  X  5.1 (features description). Our main dataset was constructed by first gath-ering 1.77M topic-and author-controlled (hence-forth TAC ) tweet pairs 5 differing in more than just lines of 236K user ids that appear in prior work (Kwak et al., 2010; Yang and Leskovec, 2011) via the Twitter API. This crawling process also yielded 632K TAC pairs whose only difference was spacing, and an additional 558M  X  X npaired X  tweets; as shown later in this paper, we used these extra corpora for computing language models and other auxiliary information. We applied non-obvious but important filtering  X  described later in this section  X  to control for other external fac-tors and to reduce ambiguous cases. This brought us to a set of 11,404 pairs, with the gold-standard labels determined by which tweet in each pair was the one that received more retweets according to the Twitter API. We then did a second crawl to get an additional 1,770 pairs to serve as a held-out dataset. The corresponding tweet IDs are available online at http://chenhaot.com/pages/ wording-for-propagation.html . (Twit-ter X  X  terms of service prohibit sharing the actual tweets.)
Throughout, we refer to the textual content of the earlier tweet within a TAC pair as t 1 , and of the later one as t 2 . We denote the number of retweets received by each tweet by n 1 and n 2 , respectively. We refer to the tweet with higher (lower) n i as the  X  X etter (worse) X  tweet.
 Using  X  X dentical X  pairs to determine how to compensate for follower-count and timing ef-fects. In an ideal setting, differences between n 1 and n 2 would be determined solely by dif-ferences in wording. But even with a TAC pair, retweets might exhibit a temporal bias because of the chronological order of tweet presentation ( t 1 might enjoy a first-mover advantage (Borghol et al., 2012) because it is the  X  X riginal X ; alternatively, Figure 1: (a): The ideal case where n 2  X  n 1 when t 1  X  t 2 is best approximated when t 2 oc-curs within 12 hours of t 1 and the author has at least 10,000 or 5,000 followers. (b): in our chosen setting (blue circles), n 2 indeed tends to track n 1 , whereas otherwise (black squares), there X  X  a bias towards retweeting t 1 . t might be preferred because retweeters consider t to be  X  X tale X ). Also, the number of followers an author has can have complicated indirect effects on which tweets are read (space limits preclude discussion).

We use the 632K TAC pairs wherein t 1 and t 2 are identical 7 to check for such confounding effects: we see how much n 2 deviates from n 1 in such settings, since if wording were the only explanatory factor, the retweet rates for identical tweets ought to be equal. Figure 1(a) plots how the time lag between t 1 and t 2 and the author X  X  follower-count affect the following deviation esti-mate: where pairs whose t 1 is retweeted n 1 times. (Note that the number of pairs whose t 1 is retweeted n 1 times decays exponentially with n 1 ; hence, we condi-tion on n 1 to keep the estimate from being domi-nated by pairs with n 1  X  0 , and do not consider n 1  X  10 because there are too few such pairs to es-timate the setting where we (i) minimize the confound-ing effects of time lag and author X  X  follower-count and (ii) maximize the amount of data to work with is: when t 2 occurs within 12 hours after t 1 and the author has more than 5,000 followers. Figure 1(b) confirms that for identical TAC pairs, our cho-sen setting indeed results in n 2 being on average close to n 1 , which corresponds to the desired set-ting where wording is the dominant differentiating Focus on meaningful and general changes. Even after follower-count and time-lapse filtering, we still want to focus on TAC pairs that (i) ex-hibit significant/interesting textual changes (as ex-emplified in Table 1, and as opposed to typo cor-rections and the like), and (ii) have n 2 and n 1 suf-ficiently different so that we are confident in which t is better at attracting retweets. To take care of (i), we discarded the 50% of pairs whose similar-ity was above the median, where similarity was ing pairs by n 2  X  n 1 and retained only the top and overfit to the idiosyncrasies of particular authors, we cap the number of pairs contributed by each author to 50 before we deal with (ii). We first ran a pilot study on Amazon Mechan-ical Turk (AMT) to determine whether humans can identify, based on wording differences alone, which of two topic-and author-controlled tweets is spread more widely. Each of our 5 AMT tasks involved a disjoint set of 20 randomly-sampled TAC pairs (with t 1 and t 2 randomly reordered); subjects indicated  X  X hich tweet would other peo-ple be more likely to retweet? X , provided a short justification for their binary response, and clicked a checkbox if they found that their choice was a  X  X lose call X . We received 39 judgments per pair in aggregate from 106 subjects total (9 people com-pleted all 5 tasks). The subjects X  justifications were of very high quality, convincing us that they the third TAC pair in Table 1 were:  X  X  t 1 makes] the cause relate-able to some people, therefore show-ing more of an appeal as to why should they click the link and support X  and, expressing the opposite view,  X  X  like [ t 2 ] more because [ t 1 ] starts out with a generalization that doesn X  X  affect me and try to make me look like I had that experience before X .
If we view the set of 3900 binary judgments for our 100-TAC-pair sample as constituting in-dependent responses, then the accuracy for this set is 62.4% (rising to 63.8% if we exclude the 587 judgments deemed  X  X lose calls X ). However, if we evaluate the accuracy of the majority response among the 39 judgments per pair, the number rises to 73%. The accuracy of the majority response generally increases with the dominance of the ma-jority, going above 90% when at least 80% of the judgments agree (although less than a third of the pairs satisfied this criterion).

Alternatively, we can consider the average ac-curacy of the 106 subjects: 61.3%, which is bet-ter than chance but far from 100%. (Variance was high: one subject achieved 85% accuracy out of 20 pairs, but eight scored below 50%.) This re-sult is noticeably lower than the 73.8%-81.2% re-ported by Petrovi  X  c et al. (2011), who ran a sim-ilar experiment involving two subjects and 202 tweet pairs, but where the pairs were not topic-or
We conclude that even though propagation pre-diction becomes more challenging when topic and author controls are applied, humans can still to some degree tell which wording attracts more retweets. Interested readers can try this out themselves at http://chenhaot.com/ retweetedmore/quiz . We now investigate computationally what word-ing features correspond to messages achieving a broader reach. We start (  X  5.1) by introducing a set of generally-applicable and (mostly) non-Twitter-specific features to capture our intuitions about what might be better ways to phrase a message. We then use hypothesis testing (  X  5.1) to evaluate the importance of each feature for message prop-One-sided paired t-test for feature efficacy  X  X  X  X  : p  X  1e-20  X  X  X  X  : p  X  1-1e-20  X  X  X  : p  X  0.001  X  X  X  : p  X  0.999  X  X  : p  X  0.01  X  X  : p  X  0.99
 X  : p  X  0.05  X  : p  X  0.95  X  : passes our Bonferroni correction
One-sided binomial test for feature increase (Do authors prefer to  X  X aise X  the feature in t 2 ?) agation and the extent to which authors employ it, followed by experiments on a prediction task (  X  5.2) to further examine the utility of these fea-tures. 5.1 Features: efficacy and author preference
What kind of phrasing helps message propaga-tion? Does it work to explicitly ask people to share the message? Is it better to be short and concise or long and informative? We define an array of fea-tures to capture these and other messaging aspects. We then examine (i) how effective each feature is for attracting more retweets; and (ii) whether au-thors prefer applying a given feature when issuing a second version of a tweet.

First, for each feature, we use a one-sided paired t-test to test whether, on our 11K TAC pairs, our score function for that feature is larger in the bet-ter tweet versions than in the worse tweet versions, for significance levels  X   X  . 05 ,. 01 ,. 001 , 1 e-20 . Given that we did 39 tests in total, there is a risk of obtaining false positives due to multiple test-ing (Dunn, 1961; Benjamini and Hochberg, 1995). To account for this, we also report significance re-sults for the conservatively Bonferroni-corrected ( X  X C X ) significance level  X  = 0.05/39=1.28e-3.
Second, we examine author preference for ap-plying a feature. We do so because one (but by no means the only) reason authors post t 2 after having already advertised the same URL in t 1 is that these authors were dissatisfied with the amount of atten-tion t 1 got; in such cases, the changes may have been specifically intended to attract more retweets. We measure author preference for a feature by the percentage of our TAC pairs 13 where t 2 has more  X  X ccurrences X  of the feature than t 1 , which we de-note by  X  % p f 2  X  f 1 q  X . We use the one-sided bi-nomial test to see whether % p f 2  X  f 1 q is signifi-cantly larger (or smaller) than 50%. Table 3: Explicit requests for sharing (where only occurrences POS-tagged as verbs count, according to the Gimpel et al. (2011) tagger). Not surprisingly, it helps to ask people to share. (See Table 3; the notation for all tables is ex-plained in Table 2.) The basic sanity check we performed here was to take as features the number of occurrences of the verbs  X  X t X ,  X  X etweet X ,  X  X lease X ,  X  X pread X ,  X  X ls X , and  X  X lz X  to capture explicit re-quests (e.g.  X  X lease retweet X ).
 Informativeness helps. (Table 4) Messages that are more informative have increased social ex-change value (Homans, 1958), and so may be more worth propagating. One crude approxima-tion of informativeness is length, and we see that found that shorter versions of memes are more likely to be popular. The difference may result from TAC-pair changes being more drastic than the variations that memes undergo.

A more refined informativeness measure is counts of the parts of speech that correspond to content. Our POS results, gathered using a Twitter-specific tagger (Gimpel et al., 2011), echo those of Ashok et al. (2013) who looked at predict-Table 5: Conformity to the community and one X  X  own past, measured via scores assigned by various language models. ing the success of books. The diminished effect of hashtag inclusion with respect to what has been re-ported previously (Suh et al., 2010; Petrovi  X  c et al., 2011) presumably stems from our topic and author controls.
 Be like the community, and be true to yourself (in the words you pick, but not necessarily in how you combine them). (Table 5) Although dis-tinctive messages may attract attention, messages that conform to expectations might be more eas-ily accepted and therefore shared. Prior work has explored this tension: Lakkaraju et al. (2013), in a content-controlled study, found that the more up-voted Reddit image titles balance novelty and fa-miliarity; Danescu-Niculescu-Mizil et al. (2012) (henceforth DCKL X 12) showed that the memora-bility of movie quotes corresponds to higher lexi-cal distinctiveness but lower POS distinctiveness; and Sun et al. (2013) observed that deviating from one X  X  own past language patterns correlates with more retweets.

Keeping in mind that the authors in our data types of language-conformity constraints an au-thor might try to satisfy: to be similar to what is normal in the Twitter community, and to be similar to what his or her followers expect. We measure a tweet X  X  similarity to expectations by its score according to the relevant language model, the unigrams (unigram model) or all and only bi-community language model from our 558M un-paired tweets, and personal language models from each author X  X  tweet history.
 Imitate headlines. (Table 6) News headlines are often intentionally written to be both informative and attention-getting, so we introduce the idea of
Table 6: LM-based resemblance to headlines. scoring by a language model built from New York Use words associated with (non-paired) retweeted tweets. (Table 7) We expect that provocative or sensationalistic tweets are likely to make people react. We found it difficult to model provocativeness directly. As a rough approximation, we check whether the changes in t 2 with respect to t 1 (which share the same topic and author) involve words or parts-of-speech that are associated with high retweet rate in a very large separate sample of unpaired tweets (retweets and replies discarded). Specifically, for each word w that appears more than 10 times, we compute the probability that tweets containing w are retweeted more than once, denoted by rs p w q . We define the rt score of a tweet as max w where T is all the words in the tweet, and the rt score of a particular POS tag z in a tweet as max w Include positive and/or negative words. (Ta-ble 8) Prior work has found that including posi-tive or negative sentiment increases message prop-agation (Milkman and Berger, 2012; Godes et al., 2005; Heath et al., 2001; Hansen et al., 2011). We measured the occurrence of positive and negative words as determined by the connotation lexicon of Feng et al. (2013) (better coverage than LIWC). Measuring the occurrence of both simultaneously was inspired by Riloff et al. (2013).
 Refer to other people (but not your audience). (Table 9) First-person has been found useful for success before, but in the different domains of sci-entific abstracts (Guerini et al., 2012) and books (Ashok et al., 2013). Table 8: Sentiment (contrast is measured by pres-ence of both positive and negative sentiments). Generality helps. (Table 10) DCKL X 12 posited that movie quotes are more shared in the culture when they are general enough to be used in multi-ple contexts. We hence measured the presence of indefinite articles vs. definite articles.
 The easier to read, the better. (Table 11) We measure readability by using Flesch reading ease (Flesch, 1948) and Flesch-Kincaid grade level (Kincaid et al., 1975), though they are not de-signed for short texts. We use negative grade level so that a larger value indicates easier texts to read. Final question: Do authors prefer to do what is effective? Recall that we use binomial tests to determine author preference for applying a feature more in t 2 . Our preference statistics show that au-thor preferences in many cases are aligned with feature efficacy. But there are several notable ex-ceptions: for example, authors tend to increase the use of @-mentions and 2nd person pronouns even though they are ineffective. On the other hand, they did not increase the use of effective ones like proper nouns and numbers; nor did they tend to increase their rate of sentiment-bearing words. Bearing in mind that changes in t 2 may not always be intended as an effort to improve t 1 , it is still in-teresting to observe that there are some contrasts between feature efficacy and author preferences. 5.2 Predicting the  X  X etter X  wording
Here, we further examine the collective efficacy of the features introduced in  X  5.1 via their perfor-mance on a binary prediction task: given a TAC pair ( t 1 ,t 2 ), did t 2 receive more retweets? Our approach. We group the features introduced in  X  5.1 into 16 lexicon-based features (Table 3, 8, 9, 10), 9 informativeness features (Table 4), 6 language model features (Table 5, 6), 6 rt score features (Table 7), and 2 readability features (Ta-ble 11). We refer to all 39 of them together as custom features. We also consider tagged bag-of-words ( X  X OW X ) features, which includes all the unigram (word:POS pair) and bigram features that appear more than 10 times in the cross-validation data. This yields 3,568 unigram features and 4,095 bigram features, for a total of 7,663 so-called 1,2-gram features . Values for each feature are nor-malized by linear transformation across all tweets in the training data to lie in the range r 0 , 1 s . 18
For a given TAC pair, we construct its feature vector as follows. For each feature being consid-ered, we compute its normalized value for each tweet in the pair and take the difference as the fea-ture value for this pair. We use L2-regularized lo-gistic regression as our classifier, with parameters chosen by cross validation on the training data. (We also experimented with SVMs. The perfor-mance was very close, but mostly slightly lower.) A strong non-TAC alternative, with social infor-mation and timing thrown in. One baseline re-sult we would like to establish is whether the topic and author controls we have argued for, while intuitively compelling for the purposes of trying to determine the best way for a given author to present some fixed content, are really necessary in practice. To test this, we consider an alterna-tive binary L2-regularized logistic-regression clas-sifier that is trained on unpaired data, specifically, on the collection of 10,000 most retweeted tweets (gold-standard label: positive) plus the 10,000 least retweeted tweets (gold-standard label: neg-ative) that are neither retweets nor replies. Note that this alternative thus is granted, by design, roughly twice the training instances that our clas-sifiers have, as a result of having roughly the same number of tweets, since our instances are pairs. Moreover, we additionally include the tweet au-thor X  X  follower count, and the day and hour of posting, as features. We refer to this alternative classifier as TAC+ff+time . (Mnemonic:  X  X f X  is used in bibliographic contexts as an abbreviation for  X  X nd the following X .) We apply it to a tweet pair by computing whether it gives a higher score to t 2 or not.
 Baselines. To sanity-check whether our classifier provides any improvement over the simplest meth-ods one could try, we also report the performance of the majority baseline, our request-for-sharing features, and our character-length feature.
 Performance comparison. We compare the ac-curacy (percentage of pairs whose labels were cor-rectly predicted) of our approach against the com-peting methods. We report 5-fold cross validation results on our balanced set of 11,404 TAC pairs 1,770 TAC pairs; this set was never examined dur-ing development, and there are no authors in com-mon between the two testing sets.
 Figure 2(a) summarizes the main results. While
TAC+ff+time outperforms the majority base-line, using all the features we proposed beats
TAC+ff+time by more than 10% in both cross-validation (66.5% vs 55.9%) and heldout valida-tion (65.6% vs 55.3%). We outperform the aver-age human accuracy of 61% reported in our Ama-zon Mechanical Turk experiments (for a different data sample); TAC+ff+time fails to do so.

The importance of topic and author con-trol can be seen by further investigation of
TAC+ff+time X  X  performance. First, note that it yields an accuracy of around 55% on our cross-validation accuracy on the larger most-and least-retweeted unpaired tweets averages out to a high 98.8%. Furthermore, note the superior per-formance of unigrams trained on TAC data vs
TAC+ff+time  X  which is similar to our uni-grams but trained on a larger but non-TAC dataset that included metadata. Thus, TAC pairs are a use-ful data source even for non-custom features. (We also include individual feature comparisons later.)
Informativeness is the best-performing custom feature group when run in isolation, and outper-forms all baselines, as well as TAC+ff+time; and we can see from Figure 2(a) that this is not due just to length. The combination of all our 39 custom features yields approximately 63% accu-racy in both testing settings, significantly outper-forming informativeness alone (p  X  0.001 in both cases). Again, this is higher than our estimate of average human performance.

Not surprisingly, the TAC-trained BOW fea-tures (unigram and 1,2-gram) show impressive predictive power in this task: many of our custom features can be captured by bag-of-word features, in a way. Still, the best performance is achieved by combining our custom and 1,2-gram features together, to a degree statistically significantly bet-ter than using 1,2-gram features alone.

Finally, we remark on our Bonferroni correc-tion. Recall that the intent of applying it is to avoid false positives. However, in our case, Fig-ure 2(a) shows that our potentially  X  X alse X  posi-tives  X  features whose effectiveness did not pass the Bonferroni correction test  X  actually do raise performance in our prediction tests.
 Size of training data. Another interesting obser-vation is how performance varies with data size. For n  X  1000 , 2000 ,..., 10000 , we randomly sampled n pairs from our 11,404 pairs, and com-puted the average cross-validation accuracy on the sampled data. Figure 2(b) shows the averages over 50 runs of the aforementioned procedure. Our cus-tom features can achieve good performance with little data, in the sense that for sample size 1000 , they outperform BOW features; on the other hand, BOW features quickly surpass them. Across the board, the custom+1,2-gram features are consis-tently better than the 1,2-gram features alone. Top features. Finally, we examine some of the top-weighted individual features from our ap-proach and from the competing TAC+ff+time classifier. The top three rows of Table 12 show the best custom and best and worst unigram features for our method; the bottom two rows show the best and worst unigrams for TAC+ff+time. Among custom features, we see that community and per-sonal language models, informativeness, retweet scores, sentiment, and generality are represented. As for unigram features, not surprisingly,  X  X t X  and  X  X etweet X  are top features for both our approach and TAC+ff+time. However, the other unigrams for the two methods seem to be a bit different in spirit. Some of the unigrams determined to be most poor only by our method appear to be both surprising and yet plausible in retrospect:  X  X cymi X  (abbreviation for  X  X n case you missed it X ) tends to indicate a direct repetition of older information, so people might prefer to retweet the earlier ver-sion;  X  X hanks X  and  X  X orry X  could correspond to personal thank-yous and apologies not meant to be shared with a broader audience, and similarly @-mentioning another user may indicate a tweet intended only for that person. The appearance of [hashtag] in the best TAC+ff+time unigrams is consistent with prior research in non-TAC settings (Suh et al., 2010; Petrovi  X  c et al., 2011). Table 12: Features with largest coefficients, de-limited by commas. POS tags omitted for clarity. In this work, we conducted the first large-scale topic-and author-controlled experiment to study the effects of wording on information propagation.
The features we developed to choose the bet-ter of two alternative wordings posted better per-formance than that of all our comparison algo-rithms, including one given access to author and timing features but trained on non-TAC data, and also bested our estimate of average human perfor-mance. According to our hypothesis tests, help-ful wording heuristics include adding more infor-mation, making one X  X  language align with both community norms and with one X  X  prior messages, and mimicking news headlines. Readers may try out their own alternate phrasings at http: //chenhaot.com/retweetedmore/ to see what a simplified version of our classifier predicts.
In future work, it will be interesting to examine how these features generalize to longer and more extensive arguments. Moreover, understanding the underlying psychological and cultural mecha-nisms that establish the effectiveness of these fea-tures is a fundamental problem of interest.
 Acknowledgments. We thank C. Callison-Burch, C. Danescu-Niculescu-Mizil, J. Kleinberg, P. Mahdabi, S. Mullainathan, F. Pereira, K. Raman, A. Swaminathan, the Cornell NLP seminar par-ticipants and the reviewers for their comments; J. Leskovec for providing some initial data; and the anonymous annotators for all their labeling help. This work was supported in part by NSF grant IIS-0910664 and a Google Research Grant.
