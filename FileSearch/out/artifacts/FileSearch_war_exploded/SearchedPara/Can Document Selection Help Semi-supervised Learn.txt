 The goal of event extraction is to identify instances of a class of events in text. In addition to identifying the event itself, it also identifies all of the participants and attributes of each event; these are the e ntities that are involved in that event. The same event might be presented in various expressions, and an expression might represent different events in different contexts. Moreover, for each event type, the event participants and attributes may also appear in multiple forms and exemplars of the different forms may be required. Thus, event extraction is a difficult task and requires substantial training data. However, annotating events for training is a tedious task. Annotators need to read the whole sentence, possibly several sentences, to decide whether there is a specific event or not, and then need to identify the event participants (like Agent and Patient), and attributes (like place and time) t o complete an event annotation. As a result, for event extraction ta sks like MUC4, MUC6 (MUC 1995) and ACE2005, from one to several hundred annotated documents were needed.

In this paper, we apply a novel self -training process on an existing state -of -the -a rt baseline system. Alt h ough traditional self -training on normal newswire does not work well for this specific task, we managed to use information retrieval (IR) to select a better corpus for bootstrapping. Also, taking advantage of properties of this corp us, cross -document inference is applied to obtain more  X  X nformative X  probabilities. To the best of our knowledge, we are the first to apply information retrieval and global inference to semi -supervised learning for event extraction. Automa tic Content Extraction (ACE) defines an event as a specific occurrence involving participants 1 ; it annotates 8 types and 33 subtypes of events. 2 We first present some ACE terminolog y to understand this task more easily :  X  Event mention 3 : a phrase or sentence within  X  Event trigger : the main word that most  X  Event mention arguments (roles) : the entity 
Here is an example: (1) Bob Cole was killed in France today; h e was attacked ... 
Table 1 shows the results of the preprocessing , including name identification, entity mention classification and coreference , and time stamping . Table 2 shows the results for event extraction .
 M ention ID
Table 2. An example of event triggers and roles Self -training has been applied to several natural language processing tasks. For event extraction, there are several studies on bootstrapping from a seed pattern set. Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. (2000 , 2003) incorporated this into a bootstrapping app roach , extended by Surdeanu et al. (2006) to co -training. Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. Liao and Grishman (2010b) combined these two approaches to build a filtered ranking algorithm. However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling. Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self -trained r elevant sentence classifier and automatically learn ed domain -relevant extraction patterns. Liu (2009) proposed the BEAR system, which tagged both the events and their roles. However, the new patterns were boostrapped based on the frequencies of sub -pattern mutations or on rules from linguistic contexts, and not on statistical models.

The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995 ). Yangarber et al. (Yangarber and Jokipii, 2005; Yangarbe r, 2006; Yangarber et al., 2007) applied cross -document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of information about CEOs (name, start y ear, end year). Later, Ji and Grishman (2008) employed a rule -based approach to propagate co nsistent triggers and arguments across topic -related documents . Gupta and Ji ( 2009 ) used a similar approach to recover implicit time information for events . Liao an d Grishman (2010a) use a statistical model to infer the cross -event information within a document to improve event extraction. We use a state -of -the -art English IE system as our baseline ( Grishman et al. 2005 ) . This system extracts events independently for each sentence, because the definition of event mention argument s in ACE con strains them to appear in the same sentence. The system combines pattern matching with statistical models. In the training process, for every even t mention in the ACE training corpus, patterns are constructed based on the sequences of constituent heads separating the trigger and arguments. A set of Maximum Entropy based classifiers are also trained:  X  Argument Classifier: to distinguish  X  Role Classifier: to classify argume nts by  X  Reportable -Event Classifier (Trigger 
In the test procedure, each document is scanned for instances of triggers from the training corpus. When an instance is found, the system tries to match the environment of the trigger again st the set of patterns associated with that trigger. If t his pattern -matching process succe eds , t he argument classifier is applied to the entity mentions in the sentence to assign the possible arguments ; for any argument passing that classifier, the role c lassifier is used to assign a role to it. Finally, once all arguments have been assigned, the reportable -event classifier is applied to the potential event mention; if the result is successful, this event mention is reported. In self -training, a classifier is first trained with a small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re -trained and the procedure repeated. As a result, the criterion for selecting the most confident examples is critical to the effectiveness of self -training.
To acquire confident samples, we need to first decide how to evaluate the confidence for each ev ent. However, as an event contains one trigger and an arbitrary number of roles, a confident event might contain unconfident arguments. Thus, instead of taking the whole event, we select a partial event, containing one confident trigger and its most confid ent argument, to feed back to the training system. 
For each mention m i , its probability of filling a role r in a reportable event whose trigger is t is computed by: where P Arg (m i ) is the probability from the argument classifier, P Role (m i ,r) is that from the role classifier, and P Event (t) is that from the trigger classifier. In each iteration, we added the most confident &lt;role, trigger&gt; pairs to the training data, and re -trained the system. 5.1 Problems of Traditional Self -training However, traditional self -training does not perform very well (see our results in Table 3). The newly added samples do not improve the system performance; instead, its performance stays stable, and even gets worse after several iterations. 
W e analyzed the data, and found that this is caused by two common problems of traditional self -training. First, the classifier uses its own predictions to train itself, and so a classification mistake can reinforce itself. This is particularly true for event extraction, due to its relatively poor performance, compared to other NLP tasks, like Named Entity Recognition, parsing, or part -of -speech tagging, where self -training has been more successful. Figure 1 shows that the precision using the original training data is not very good: while precision improves with increasing classifier threshold, about 1/3 of the roles are still incorrectly tagged at a threshold of 0.90.
Figure 1. Precision on the original training data 
Another problem of self -training is that nothing  X  X ovel X  is added because the most confident examples are those frequently seen in the training data and might not provide  X  X ew X  information. Co -training is a form of self -training which can address this problem to some extent. However, it requires two views of the data , where each example is described using two different feature sets that provide different, complementar y information . Ideally, the two views are conditionally independent and each view is sufficient (Zhu, 2008) . Co -training has had some success in training (binary) semantic relation extractors for some relations, where the two views correspond to the arguments of the relation and the context of these arguments (Agichtein and Gravano 2000). However, it has had less success for event extraction because event arguments may participate in multiple events in a corpus and individual event instances may omit some arguments. 5.2 Self -training on Information Retriev al To address the first problem (low precision of extracted events), we tried to select a corpus where the baseline system can tag the instances with greater confidence . (Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. Thus, we believe that bootstrapping on a corpus of topic -related documents should perform better than a regular newswire corpus .
We followed Ji and Grishman (2008) X  X  approach and use d the IN DRI retrieval system 4 (Strohman et al., 2005) to obtai n the top N related documents for each annotated document in the training corpus . The query is event -based to insure that related documents contain the same events. For each training document, w e const ruct an INDRI query from the triggers and arguments . For example, for sentence (1) in section 2, we use the keywords  X  killed  X ,  X  attacked  X ,  X  France  X ,  X  Bob Cole  X , and  X  today  X  to extract related documents. Only names and nominal arguments will be used; pronou ns appearing as arguments are not included. For each argument we also add other names coreferential with the argument. 5.3 Self -training using Global Inference Although bootstrapping on related documents can solve the problem of  X  X onfidence X  to some ex t ent, the  X  X ovelty X  problem still remains: the top -ranked extracted events will be too similar to those in the training corpus . To address this problem , we propose to use a simple form of global inference based on the special characteristics of related -to pic documents . Previous studies pointed out that information from wider scope , at the document or cross -document level, could provide non -local information to aid event extraction ( Ji and Grishman 2008 , Liao and Grishman 2010a) . There are two common assump tions within a cluster of related documents (Ji and Grishman 2008 ):  X  Trigger Consistency Per Cluster: if one  X  Role Consistency Per Cluster: if one en tity 
Based on these assumptions, i f a t rigger/role has a low probability from the baseline system, but a high one f rom global inf eren ce, it means that the local context of this trigger/role tag is not frequently seen in the training data, but the tag is still confident. Thus, we can confidently add it to the training data and it can provide novel information which the samples confid ently tagged by the baseline system can not provide.
To start, the baseline system extracts a set of events and estimates the probability that a particular instance of a word triggers an event of that type, and the probability that it takes a particular argument. The global inference process then begins by collecting all the confident triggers and arguments from a cluster of related documents. 5 For each trigger word and event type, it records the highest probability (over all instances of that word in the cluster) that the word triggers an event of that type. For each argument, within -document and cross -document coreference 6 are used to collect all instances of that entity; we then compute the maximum probability (over all instances) of that ar gument playing a particular role in a particular event type. These maxima will then be used in place of the locally -computed probabilities in computing the probability of each trigger -argument pair in exampl e, if the entity  X  X raq X  is tagged confidently (probability &gt; 0.9) as the  X  X ttacker X  role somewhere in a cluster, and there is another instance where from local information it is only tagged with 0.1 probability to be an  X  X ttacker X  role, we use probability of 0.9 for both instances. In this way, a trigger pair containing this argument is more likely to be added into the training data through bootstrapping, because we have global evidence that this role probability is high, although its local confidence is lo w. In this way, some novel trigger -argument pairs will be chosen, thus improving the baseline system. We randomly chose 20 newswire texts from the ACE 2005 training c o rpora (from March to May of 2003) as our evaluation set, and use d the system ( Grishman et al. 2005 ), and a simple re maining n ewswire texts as the original training data (83 documents). For self -training, we picked 10,000 consecutive newswire texts from the TDT5 corpus from 2003 8 for the ST experiment. For ST_IR and ST_GI, we retrieved the best N ( using N = 25, which ( Ji and Gris hman 2008) found to work best ) related texts for each training document from the English TDT5 corpus consist ing of 278,108 news texts (from April to S eptember of 2003). In total we retrieved 1650 texts; the IR system returned no texts or fewer than 25 text s for some training documents. In each iteration, we extract 500 trigger and argument pairs to add to the training data.

Results (Table 3) show that bootstrapping on an event -based IR corpus can produce improvements on all three evaluations, while global i nference can yield further gains. 
Table 3. Performance (F score) with different We propos ed a novel self -training process for event extraction that involves information retrieval (IR) and global inference to provide more accurate and informative instances. Experiments show that using an IR -select ed corpus improves trigger labeling F score 1.7%, and role labeling 2.3%. G lobal inference can achieve further improvement of 1.1% for trigger labeling, and 1.3% for role labeling. Also, this bootstrapping involves processing a much smaller but more closel y related corpus, which is more efficient. Such pre -selection of documents may benefit bootstrapping for other NLP tasks as well, such as name and relation extraction.

