 We describe an application of relational knowledge discov-ery to a key regulatory mission of the National Associa-tion of Securities Dealers (NASD). NASD is the world X  X  largest private-sector securities regulator, with responsibil-ity for preventing and discovering misconduct among secu-rities brokers. Our goal was to help focus NASD X  X  limited regulatory resources on the brokers who are most likely to engage in securities violations. Using statistical relational learning algorithms, we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future. Our models incorporate organizational relationships among brokers (e.g., past coworker), which domain experts consider important but have not been easily used before now. The learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of effort by NASD staff. Model predictions were found to correlate highly with the subjective evaluations of experienced NASD examiners. Furthermore, in all performance measures, our models performed as well as or better than the handcrafted rules that are currently in use at NASD.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Design, Experimentation.
 Fraud detection, statistical relational learning, relational probability trees.

The National Association of Securities Dealers (NASD) is the world X  X  largest private-sector securities regulator, with responsibility for preventing and discovering misconduct among securities brokers such as fraud and other violations of securities regulations. In accomplishing this regulatory mission, it is critical for NASD to target its limited resources on those brokers who are most likely to be engaged in fraud-ulent behavior. This paper describes an application of rela-tional knowledge discovery methods to identify such brokers, which was a joint effort between NASD and researchers at the University of Massachusetts (UMass) Amherst.

Using publicly available data, we learned statistical rela-tional models of broker behavior that provide a ranking of active brokers with respect to their probability of commit-ting a serious securities violation in the near future. The intention is to use this ranking to improve NASD X  X  assign-ment of field examinations X  X rokers who are ranked higher would be more likely to receive additional examinations by NASD staff. This approach limits the effects of false pos-itives as human analysts will further evaluate the brokers identified by the model.

NASD currently identifies higher-risk brokers using a set of handcrafted rules. These rules are based on informa-tion intrinsic to the brokers such as the number and type of past violations. They do not exploit social, professional, and organizational relationships among brokers even though NASD experts believe this information is central to the task. Indeed, fraud and malfeasance are usually social phenom-ena, communicated and encouraged by the presence of other individuals who also wish to commit fraud [4]. It is, how-ever, difficult to accurately specify these patterns manually. As such, relational learning methods have the potential to improve current techniques.

Our approach to modeling in this domain exploits recent work on learning accurate, interpretable models of relational data [9, 10]. We learned relational probability tree (RPT) models, an extension of probability estimation trees for re-lational domains [12]. These models have three attractive characteristics. First, they provide a ranking of brokers (with respect to estimated probability of misconduct) rather than the binary classification provided by the handcrafted rules. Second, they are able to represent and reason with the relational context information analysts believe to be impor-tant. And third, due to their selectivity and intuitive rep-resentation, tree models are usually easily interpretable X  X  quality that is often important in order for domain experts to trust, and make regular use of, the models.

The learned models were subjected to an extensive evalua-tion by NASD staff that took over two person weeks of effort. This evaluation showed that the models ranked brokers in a manner consistent with the subjective ratings of experienced examiners. Furthermore, in all performance measures, our models performed as well as or better than the handcrafted rules that are currently in use at NASD. Most notably, our models identified higher-risk brokers not previously detected with the handcrafted rules and combined with the current NASD process to significantly increase the accuracy of pre-dicting higher-risk brokers.

In the remainder of this paper, we relate our experience developing statistical relational models for this task. We start with a description of the regulatory mission of NASD and the data used to train the models. We then outline the prediction task and our modeling approach. We continue with an empirical evaluation of the models and conclude with implications and future research directions.
NASD is the world X  X  largest private-sector securities reg-ulator. It regulates every firm in the United States that conducts securities business with the public (called broker-dealers ), and it is subject to oversight by the U.S. Securi-ties and Exchange Commission (SEC). Established in 1939, NASD has a nationwide staff of more than 2,000, and its regulatory responsibility now includes 5,200 securities firms that operate more than 100,000 branch offices and employ 660,000 individual securities brokers.

NASD rules regulate every aspect of the brokerage busi-ness for NASD members. NASD responsibilities include ex-amination, licensing, testing and registration; enforcement; market surveillance; rule writing; professional training; dis-pute resolution; and investor education. NASD examines broker-dealer firms for compliance with NASD rules, Mu-nicipal Securities Rulemaking Board rules, and federal se-curities laws. NASD also disciplines those who fail to com-ply and in 2004 filed 1,400 enforcement actions, barred or suspended 830 brokers from the securities industry, and col-lected $104 million in fines. In addition, NASD monitors all trading on the NASDAQ Stock Market, which covers more than 70 million orders, quotes, and trades per day.
NASD examines firms both on a periodic basis (called cycle examinations) and also in response to complaints or other reasons (called cause examinations). In 2004, NASD X  X  Member Regulation Department conducted 2,275 cycle ex-aminations and 5,967 cause examinations, which required more than 500 field examiners as well as headquarters staff. Properly targeted examinations are critical to protecting in-vestors and the integrity of securities markets. Early discov-ery of securities violations can prevent serious harm, recover fraudulently obtained funds, and lead to swift punishment of perpetrators. It can also prevent future violations through increased regulatory scrutiny.

It is critically important for NASD to identify firms and brokers who have a higher probability of committing serious violations in the future because this allows efficient alloca-tion of the limited resources of examiners and other NASD staff. Currently, NASD uses a variety of methods to identify higher-risk brokers and firms, particularly highlighting firms and individual brokers who have had regulatory or financial problems in the past. Because of the difficulty of this task, NASD continually seeks methods to both predict violations and assign its examinations more precisely.
One key tool for accomplishing NASD X  X  regulatory mis-sion is its Central Registration Depository (CRD c  X  ) sys-tem. CRD was established to aid in the licensing and reg-istration of its broker-dealers and the brokers who work for them. CRD maintains information on all federally regis-tered broker-dealers and brokers for the SEC, NASD, the states, and other federally authorized private sector regula-tors, such as the New York Stock Exchange.

Originally implemented in June 1981, CRD has grown to include data on approximately 3.4 million brokers, 360,000 branches, and 25,000 firms. For firms, CRD information in-cludes data such as ownership and business locations. For individual brokers, CRD includes qualification and employ-ment information. Information in CRD is self-reported by the registered firms and brokers, although incorrect or miss-ing reports can trigger regulatory action by NASD. Figure 1 shows a relational schema for the NASD data, indicating entities and relationships that were used in our analysis. Although the CRD database employs a much more com-plex schema, Figure 1 provides a guide to the major types of objects and links provided to the relational learning algo-rithms. The frequency counts in Figure 1 refer to a subset of the CRD used for this analysis, which was restricted to firms and brokers who have had an approved NASD registration.
One of the most important categories of data in CRD captures disciplinary information from a number of sources, including state regulators, SEC, NASD, New York Stock Exchange, American Stock Exchange, and FBI, as well as from the registered brokers and the brokerage firms them-selves. This disciplinary information, generally referred to as disclosures , includes information on criminal, regulatory, and civil judicial actions; customer complaints; and termi-nation actions. Other disclosure types report financial prob-lems such as bankruptcies, bond denials, and liens. Disclo-sure information on individual brokers is provided free of charge to the public through NASD X  X  BrokerCheck system (www.nasdbrokercheck.com).

Because one indicator of future problematic behavior is past behavior, NASD uses disclosure counts of individual brokers from the CRD to assist it in targeting its examina-tions toward those who are at higher risk to commit future violations.
Our goal was to develop a statistical model to identify which brokers warrant additional attention from NASD ex-aminers. There are two reasons to instigate reviews: (1) to uncover broker violations, and (2) to prevent future vio-lations by increasing supervision on those brokers who are believed to be most likely to commit them. Unfortunately, there is no attribute in the data that records, in retrospect, whether examiners should have reviewed particular brokers. Instead, we use the existence of serious violations as a sur-rogate measure.

To quantify broker misconduct, we used a ranking of dis-closure severity provided by NASD experts. We regarded disclosures of type investigation or regulatory-action as  X  X e-rious violations X  and labeled the brokers who have had a se-rious violation in a given time period as positive examples. In other words, the surrogate measure we used is whether a broker will have an investigation or regulatory-action dis-closure in the near future, under the assumption that ex-aminers would have wanted to review these brokers before they committed these actions. We restricted our analysis to small and moderate sized firms with fewer than 15 bro-kers. These firms account for almost half of the firms under NASD jurisdiction. There were two reasons for this restric-tion. First, the patterns of behavior differ between small and large firms. Second, large firms typically have more extensive compliance mechanisms in place.

Currently, NASD generates a list of higher-risk brokers (HRB) using a set of handcrafted rules they have formed using their domain knowledge and experience. This ap-proach has two weaknesses we aim to address. First, the handcrafted rules simply categorize the brokers as  X  X igher-risk X  and  X  X ower-risk X  rather than providing a risk-ordered ranking. A ranking would be more useful to examiners as it would allow them to focus their attention on brokers con-sidered to have the highest risk. Second, NASD X  X  hand-crafted rules use only information intrinsic to the brokers. In other words, they do not utilize relational context in-formation such as the conduct of past and current cowork-ers. NASD experts believe that organizational relationships can play an important role in predicting serious violations. For example, brokers that have had serious violations in the past may influence their coworkers to participate in future schemes. Furthermore, some firms tend to be associated with continuous misconduct (i.e., they do not regulate their own employees and may even encourage violations). Lastly, higher-risk brokers sometimes move from one firm to an-other collectively, operating in clusters, which heightens the chance of regulatory problems. A model that is able to use relational context information has the potential to capture these types of behavior and provide more accurate predic-tions.
NASD X  X  task of ranking brokers for examination has three characteristics that are common to many knowledge discov-ery tasks, but that are rarely addressed in combination. Ac-curate ranking of brokers is inherently probabilistic, rela-tional, and temporal.

Probabilistic  X  Any attempt to predict the future be-
Relational  X  The majority of the patterns discussed by
Temporal  X  NASD wishes to predict behavior in the rel-
All three of these problem characteristics indicate the po-tential for a statistical relational model to provide better indicators for examiners than a broker X  X  actual disclosures. Specifically, a relational model can capture dependencies among broker characteristics, past behavior, and future be-havior that go beyond what can be captured in simple filter-ing rules. In addition, it can capture dependencies that go beyond an individual broker to consider the behavior of the broker X  X  past and present coworkers, branches, and firms. Finally, a statistical relational model might be able to iden-tify and represent complex temporal trends of behavior that suggest particularly high risk for serious violations in the next year, even though past behavior has been relatively benign.
We use relational probability trees (RPTs) [12] for this task. RPTs extend probability estimation trees [13] to a relational setting. Due to their selectivity and intuitive representation of knowledge, tree models are often easily interpretable. This makes RPTs an attractive modeling approach for NASD examiners. The RPT learning algo-rithm adjusts for biases towards particular features due to the unique characteristics of relational data. Specifically, three characteristics X  X oncentrated linkage, degree dispar-ity, and relational autocorrelation X  X an complicate efforts to construct good statistical models, leading to feature se-lection bias and discovery of spurious correlations [9, 10]. By adjusting for these biases, the RPT algorithm is able to learn relatively compact and parsimonious tree models.
RPT models estimate probability distributions over class labels in a manner similar to conventional tree models. How-ever, the learning algorithm looks beyond the attributes of the object for which the class label is defined and considers the effects of attributes in the relational neighborhood of the object being classified. The RPT learning algorithm uses subgraphs as training examples. Each subgraph includes different types of objects (e.g., firms, disclosures), links that represent relationships between these objects (e.g., employ-ment links between a broker and a branch), and attributes on these objects and links. In each subgraph, there is a single target object to be classified; the other objects and links in the subgraph form the target X  X  relational neighbor-hood. To classify brokers, we constructed subgraphs around brokers, including information about their current and past employment, and their disclosures (see section 6.1 for more detail). A hypothetical subgraph for this task is shown in Figure 2.

The RPT algorithm automatically constructs and searches over aggregated relational features to model the distribu-tion of the class label. For example, to predict the value of an attribute (e.g., broker-has-serious-violation-next-year ) based on the attributes of related objects (e.g., characteris-tics of the broker X  X  coworkers), a relational feature may ask whether the average employment length of the coworkers is less than 12 months. The algorithm constructs features from the attributes of different object/link types in the sub-graphs using multiple methods of aggregation (mode, av-erage, count, proportion, and degree) to group the values of those attributes. Count, proportion, and degree features consider a number of different thresholds (e.g., proportion &gt; 10%). The algorithm searches for the best binary discretiza-tion of continuous attributes for features (e.g., count(disclo-sure.year &gt; 2004)). For the experiments reported in this pa-per, we considered 10 thresholds and 10 discretizations per feature. The algorithm uses pre-pruning in the form of a p -value cutoff and a depth cutoff to limit tree size. All ex-periments reported in this paper used  X  = 0 . 2 / | attributes | and depth cutoff= 7.

Given an RPT model learned from a set of training ex-amples, the model can be applied to unseen subgraphs for prediction. The chosen feature tests are applied to each sub-graph and the example travels down the tree to a leaf node. The model then uses the probability distribution estimated for that leaf node to make a prediction about the class label of the example. Alternatively, an ensemble of RPT mod-els can be used to improve the probability estimates for each instance. Bagging is an ensemble method that reduces variance without increasing bias [8]. The bagging proce-dure involves learning multiple trees, each from a different bootstrapped pseudosample (i.e., sample N instances with replacement from the original sample), and then comput-ing probability estimates by averaging the predictions of the trees on the test set.
This work was conducted as a joint project between NASD and the Knowledge Discovery Laboratory at UMass Amherst Department of Computer Science. The project proceeded in two iterations of a four-stage process of task specification, data preparation, data mining, and evaluation (with rough time estimates in parentheses): Scoping and task selection (one month)  X  We dis-
Data preparation (three months)  X  NASD staff pre-
Data mining (one month)  X  UMass researchers con-
Evaluation (one month)  X  UMass researchers evaluated
Task refinement (two months)  X  The task specifica-
Data refinement (two months)  X  In addition to re-
Data mining (one month)  X  Based on the new class
Evaluation (one month)  X  As before, the RPTs were
In retrospect, several findings of prior work on the knowl-edge discovery process [6, 2] were largely borne out. The analysis process of this project followed a sequence quite similar to the ones described in this prior work. In addition, the vast majority of time was spent on task specification, data preparation, and evaluation, rather than on the data mining step.
In this section, we present an empirical evaluation of the claim that RPT models provide a useful ranking of brokers with respect to their likelihood of committing securities vi-olations in the near future. We examine two surrogate mea-sures of misconduct. The first is the class label used to train the RPT models, namely whether any serious disclosures were filed on the target broker. The second is a subjective evaluation of brokers by NASD examiners.
 Where appropriate, we compare the performance of the RPT models to two baseline models. The first, referred to as Base , is an RPT model learned using the same algorithm, but without the attributes in the relational neighborhood surrounding the target broker. Base models used only the attributes on the target brokers themselves. The second, referred to as HRB , is the binary classification produced by NASD X  X  higher-risk broker list.
The training and test instances were subgraphs centered on a target broker and included information about the bro-ker X  X  current and past employment. These subgraphs were extracted from the data using the visual query language QGraph [1]. Queries in this language allow for variation in the number and types of objects and links that form the subgraphs and return collections of all matching subgraphs from a database.

Figure 3 shows an example of the type of query used to construct training and test instances. This query is dated December 31, 2000. It returns one subgraph for each broker who, on that date, was working for a firm that employed fewer than 15 brokers. In each subgraph, the relational neighborhood includes the following: (1) any disclosures that have been filed on this broker until the query date, (2) the broker X  X  current branch (at query date), all cowork-ers at this branch, the firm this branch belongs to, and the regulators associated with this firm, and (3) the broker X  X  past branches, past coworkers at those branches, and firms and regulators associated with these past branches. Figure 2 shows a hypothetical match to this query: a broker who has had two disclosures and who has worked at a single branch.
To address the temporal nature of the prediction task, we created multiple samples, where each sample was a static view of the dataset at a particular point in time. More specifically, the samples we created reflected a static view of the dataset at the end of the calendar years 1996-2001. For example, the 1996 sample was constructed using the data available on December 31, 1996. The samples include a subgraph for each broker active at that date; the relational neighborhoods of the target brokers reflect what was known about the brokers at that date.

The target class label was broker-has-serious-violation-next-year , indicating whether at least one disclosure of type regulatory-action or investigation was filed on the broker within the next calendar year. For example, for the 1996 sample, the target class label was whether the broker had a serious violation during the calendar year 1997. One char-acteristic of the resulting training samples is that there are few positive instances but many negative instances. Table 1 lists the distribution of positive and negative examples in each sample. On average, only 1% of the examples are pos-itive. To increase the absolute number and distribution of positive instances, and to avoid overfitting to the trends of a single year, we constructed training sets by combining sam-ples from three consecutive years. For example, we merged the samples from 1996, 1997, and 1998 into a single training set. If brokers were active during the entire time interval, they were included as three separate examples, with sub-graphs that reflect their relational neighborhood at the end of each year.

From these merged samples, we produced 10 pseudosam-ples for bagging. Each pseudosample was produced using stratified bootstrap resampling (i.e., the positive and the negative examples were separately sampled with replace-ment from the original sample). The number of positive examples in the pseudosamples was chosen to be the same as in the original sample, but we limited the negative ex-amples to 1500 to increase the overall proportion of positive examples. This also limits the number of times each broker is added to the sample (since brokers are unlikely to have a positive class label for three consecutive years).
The RPT models had 55 attributes available for classifi-cation, including information on the broker (e.g., has other business), past disclosures (e.g., event date), current em-ployment (e.g., branch location), past employment (e.g., ter-mination reason), and coworkers (e.g., time in industry).
In this section, we evaluate the performance of RPT mod-els in predicting whether a broker will have a serious viola-tion in the following calendar year. We present results on four test samples from 1999-2002 1 .

The RPTs were trained on samples that combined the samples for the three previous years using the procedure outlined above. For example, the test year 1999 means that the model was trained on samples dated 1996, 1997, and 1998 and the test sample was the sample dated 1999. Recall that each training or test sample builds subgraphs using the data available at the end of the sample year and assigns the class label using the disclosures filed in the following calendar year.

We examine four measures of performance. We use Re-ceiver Operating Characteristic (ROC) curves and area un-
We obtained class label information for the 2002 sample after the second iteration of model development and eval-uation. We include post-hoc evaluation on this sample to improve understanding of the evaluations reported in sec-tion 6.3. der the ROC curve (AUC) to evaluate the ranking of the different models. In addition, because the HRB does not provide a ranking but only a binary classification of the bro-kers, we present precision and recall results.
 Figure 4 shows the ROC curves on the four test samples. ROC curves show the quality of the ranking provided by the classifier [14]. The curve shows how the false positive rate and the false negative rate vary as the probability threshold between classes is varied between zero and one. If a model dominates the ROC space it can be regarded as the model that provides the best ranking of the brokers. A random ranking is expected to produce a diagonal line with equal true positive and false positive rates.

The figure shows ROC curves for Base and RPT, but only a single point for HRB. This is because both Base and RPT provide a ranking of the brokers, therefore multiple ways of setting the threshold between classes, while HRB provides only a binary class label and therefore a single threshold.
The figure shows that all three models performed better than random. The steep slope of the curves in the true positive range [0 . 0 , 0 . 4] indicates that the models accurately rank brokers at the top of the list. In addition, the figure reveals that the three models are roughly comparable at the single threshold produced by the HRB list. The relational information produces the largest improvement when ranking the 1999 and 2000 samples and the improvement is most pronounced for the true positive range [0 . 4 , 0 . 9].
On average the RPT model produces an equivalent, or better, ranking when compared to the baseline model. The ROC information is summarized in Figure 5, which plots the AUC for the Base and RPT models. The benefit of the relational information differs significantly between 1999-2000 and 2001-2002. We are still investigating the reason for this change in performance. Our initial hypothesis, first suggested by NASD staff familiar with the disclosures during this period, is that it is due to the bursting of the  X  X ech bubble X  in mid-2000, which may have changed the nature and pattern of disclosures and caused concept drift.
Figure 6 shows precision and recall results. To obtain Figure 5: AUC performance comparison of the base-line and full RPT models. these results, we used the rankings provided by RPT and Base models to generate broker lists of the same size as the HRB list. For example, in the 1999 test sample, which in-cluded 118 brokers on the HRB list, the RPT list included the 118 brokers ranked most highly by the RPT model. Ta-ble 1 lists the size of the HRB list for each sample.
Precision refers to the proportion of brokers on the list who have a positive class label. Recall is the proportion of brokers with a positive class label who appear on the list. Due to the small number of positives and the size of the HRB list, 0.40 is the maximum precision any model can hope to achieve. Also, given the low proportion of positives, random performance would result in approximately 0.01 pre-cision and 0.03 recall. Clearly all the models are performing above random. In all test samples, RPT precision and re-call performance were equivalent to or higher than HRB. The relative performance of RPT and Base paralleled the ROC-AUC performance reported in the previous section.
To quantify the amount of relational information included in the RPT models, we computed the proportion of tree nodes that use relational features weighted by the propor-tion of training instances that traveled through the node. In each RPT model, the weighted proportion of nodes that used relational information was more than 50%. This indicates that the RPTs made substantial use of the relational infor-mation. Recall that RPTs performed substantially better than the Base models, which ignored the relational informa-tion, in two of the four years evaluated.
We next evaluated the RPT models with respect to the subjective ratings of brokers from NASD examiners. These Figure 6: Precision-recall performance comparison of the RPT models and the higher-risk broker list. ratings were not part of the CRD data but were produced on a small set of brokers in February 2005 with the sole purpose of evaluating our models.

Examiners spent a considerable amount of time (approxi-mately 30 minutes per broker) to determine the ratings. As a consequence, we could obtain ratings for only a small set of brokers. Because of this limitation, we chose to evaluate a single bagged RPT model. We trained this model 2 using the 1999-2001 samples, which constituted the most recent three-year span in the CRD data available to us.

Using this model, we obtained predictions for the 2002 test sample. We selected 80 brokers from this sample and asked four NASD examiners to rate these brokers on a five-point scale, indicating the degree to which each broker warranted additional attention from an NASD examiner in 2003. A rating of 1 indicated that the broker deserved no additional attention; a rating of 5 indicated they deserved the high-est attention. We asked examiners to use any information to which they have access, including the data accumulated since 2003 and any useful sources outside of CRD. We be-lieve that these ratings provide a better measure of the util-ity of the RPT rankings than the class label we used to train our models (i.e., whether a broker will have a serious vio-lation in the following year): They reflect the judgment of experienced examiners in the light of extensive information (not limited to the CRD data) and the hindsight provided by the data accumulated from 2003 until February 2005.
We selected the brokers using the HRB and RPT lists
This RPT model was learned at an earlier stage of our analysis than those reported in Section 6.2, and used a less conservative p-value cutoff (  X  = 0 . 05). Table 2: Overlap between HRB and RPT lists.
 Table 3: Pairwise correlations of examiner ratings. and partitioned the 2002 test sample into the following cat-egories: Table 2 contains the number of brokers in each category. We selected 20 brokers from each category as follows: We ranked the brokers in each category with respect to the prob-ability estimates produced by the RPT model. Within each category, we created 20 bins by frequency (i.e., we placed an equal number of brokers in each bin) and selected the broker with the median probability value in each bin as represen-tative of that bin.

Each of the four NASD examiners independently rated the 80 selected brokers. The examiners were not aware of the procedure used to select the set of brokers; and the UMass author communicating the results to examiners was not aware of which category each broker belonged to. Fur-thermore, to avoid systematic biases caused by evaluation order, each examiner received the list of brokers in a differ-ent random order.

We present the agreement between the examiners X  ratings in Table 3. The pairwise correlations among the examiner ratings indicate a relatively consistent ranking of brokers with the exception of examiner 3. This examiner rated 70% of the brokers with a rating of 1 whereas the other examiners rated brokers more uniformly in the range [1 , 5].
Figure 7 shows the distribution of brokers X  average exam-iner rating for each of the four categories. Table 4 shows the mean of the distribution for each category and the results of two-tailed t -tests comparing the RPT-Only distribution to each of the other three distributions. The distributions for the RPT-Only and HRB-Only categories are nearly identi-cal, but they are significantly higher than the distribution for the Neither category and significantly lower than the distribution for the Both category ( p &lt; 0 . 01).
These results indicate that the RPT model is competitive with the HRB list because it identifies brokers with similar ratings. We note here that examiners X  subjective judgments and the HRB criteria are likely to be highly correlated X  the HRB list was created to correlate well with examiners X  judgments, and their judgments may have been influenced by the existing criteria for the HRB list X  X o it not surprising that brokers on the HRB list were rated highly by examiners. Figure 7: Distributions of brokers X  average examiner rating per category.
 What is more surprising is that brokers identified by the RPT (which was tuned to a surrogate class label) did as well as HRB in terms of examiner ratings.

Furthermore, the results indicate that the RPT model identifies novel cases previously unidentified by the HRB list but with equivalent ratings. This suggests that the RPT model can be used successfully to extend the set of brokers currently assessed by examiners. And finally, the results show that brokers identified by the combination of models ( Both ) have significantly higher ratings than those identified by either model in isolation. This indicates that an ensemble of models may be useful in prioritizing examiner attention. These results prompted us to add membership on the HRB list as a feature to the RPT learning algorithm. The evaluation of this modified RPT model revealed that its per-formance was slightly better than the RPT model presented here, particularly on the 2001 test set. Future work will include additional investigation in this direction.
Figure 8a shows a scatterplot of the average rating as-signed to each broker and the probability of a positive class label assigned to the same broker by the RPT model. Note that we have not sampled uniformly from this space. The bottom left corner of the plot is a very dense region that contains a large number of brokers that are not on the HRB list and who also have a very low probability of a positive class label. These brokers are in the Neither category in Table 2. We selected only 20 brokers from this category of over 4000 individuals.

Figure 8b shows a variation of the same plot in which brokers are placed into ten bins with respect to their RPT probabilities (bin width=0 . 1). The figure shows a scatter-plot of the mean broker rating and the mean RPT probabil-ity in each bin. Both figures reveal that RPT probabilities correlate well with the ratings of NASD examiners. This in-dicates that the RPT model can be used to rank brokers in a manner that would be consistent with examiners ratings if the examiners were to rate each case individually. In other words, the RPT model can be used to prioritize examiners attention on brokers more likely to warrant investigation. Figure 8: Correlation between RPT predictions and examiner ratings. (a) corr = 0 . 549 , R 2 = 0 . 293 , p = 1 . 30 e  X  07 , (b) corr = 0 . 945 , R 2 = 0 . 876 , p = 3 . 94 e  X  04 .
Examiners also provided some anecdotal evidence that the model produced rankings that corresponded to their expert judgments. Without prompting, one NASD examiner made the following comment when returning his ratings: Further examination revealed that this broker was identified only by the RPT-Only model and had an average rating of 4.75 from the four examiners.

Finally, to evaluate the utility of the surrogate class label we examined the distribution of examiner ratings in light of additional class label information provided for the 2002 sample. Figure 9 shows the distribution of average rat-ings for the brokers with positive and negative class labels. There are only six brokers in our evaluation set with positive class labels but the ratings for these brokers are significantly higher than for the rest of the brokers. However, the aver-age rating for negative examples is still much higher than we expected. This indicates that while there is useful informa-tion in our surrogate class label, there remains a significant amount of untapped information in the negative examples that could be used to improve the models.
NASD staff began this project contending that informa-tion about the professional and organizational networks that connect brokers would provide useful information for deter-mining their risk for serious violations of securities regu-lations. The results of this research have borne out those beliefs.

Our relational models provide predictions that are com-petitive with, but significantly different from, the predic-tions provided by NASD X  X  hand-tuned rules, which only ex-amined brokers and their disclosures, ignoring additional re-lational information such as coworkers at present and past firms. These models show important potential for NASD X  X  screening process. They identified higher-risk brokers not previously identified by the NASD rules, and thus provided additional targets for NASD examinations. Furthermore, Figure 9: Average broker rating by class label (Pos-itive: mean = 3 . 96 , Negative: mean = 2 . 39 ). being identified as higher-risk by both our models and the HRB model was found to be more predictive of future prob-lems than being identified by either model alone, thus per-mitting NASD to focus examinations on those most likely to have a serous violation in the near future. And finally, the probability estimates assigned to brokers by our mod-els in general agreed with the subjective ratings of NASD examiners, thus the ranking provided by our models can be used to prioritize examiners X  attention.

Our models made substantial use of relational features. In addition, we showed how the statistical models that ignored this relational information performed substantially worse in two of the four years evaluated.

That said, the available data provide only relatively weak abilities to exploit the relational aspects of the domain. In CRD, individual brokers are directly related only through firms. Even branch relationships have to be inferred from address information, although this limitation will be obvi-ated beginning this October when each broker will be sys-tematically linked to a branch. More importantly, we do not know which individual brokers work together directly, nor what other social or organizational relationships they may share. To enhance their knowledge of potential links among individuals, NASD is investigating other recent technolo-gies, most notably the NORA (Non-Obvious Relationship Awareness) system produced by Systems Research and De-velopment, a Nevada-based company recently acquired by IBM. Such relationships could add substantially to the data analyzed in the work reported here, which could only use branch and firm relations present in CRD.

The work reported here also exemplifies a framework that may be useful to projects that seek to develop screening tools to aid field examiners working in other domains such as heath care, insurance, banking, and environmental health and safety. In such cases, development of a labeled train-ing set may be impractical in the initial stages of a project. While the most accurate class labels would be the judgments of examiners, examiners X  time is typically limited and orga-nizations may be understandably skeptical about devoting large amount of examiners X  time to labelling data sets.
As we demonstrated here, however, initial classifiers can be developed using a surrogate for the ideal class label (here we used the occurrence of serious violations as a surrogate for examiners X  judgments about the utility of an examina-tion). Evaluations of models constructed with the surrogate label can determine how well it matches examiners X  judg-ments and can serve to guide and motivate additional work.
Our research to date suggests a wide variety of directions for future work. First, the inferences described in this paper did not exploit a key feature of relational data X  X he poten-tial of inferences about one object to inform inferences about others. This approach, called collective inference [3, 16, 11] has been shown to improve the accuracy of inferences in relational data. We suspect that this approach could im-prove accuracy if inferences were made collectively about all brokers and firms.

Second, some of the knowledge conveyed by NASD ex-aminers to the UMass researchers was too complex to be captured by the features currently available to RPTs. In particular, examiners described a set of temporal changes in employment that they believed were strongly associated with higher risk brokers. We suspect that representing and using this temporal information would significantly improve model accuracy. Temporal-relational models are a promising direction for future work that researchers have only recently started to explore [15]. In a similar way, we hope to use additional types of connections among brokers to enhance our knowledge of the social and professional networks that affect broker behavior.

Third, the evaluation we conducted with the help of ex-aminers indicates that it would be possible to obtain class labels directly from examiners. This would allow us to aban-don our surrogate label (serious violations) and attempt to reproduce examiners X  screening judgments directly. In the ideal case, we would faithfully reproduce a consensus judg-ment on the part of examiners, allowing them to focus on in-depth examinations, rather than initial screening of higher-risk brokers.

Fourth, we hope to focus on a wider range of firms in future work. Here we examined only the brokers who work at small to medium-sized firms. The promising results we obtained in this task encourages us to continue to develop models for larger firms.

Finally, we hope to account for the apparent concept drift that caused the relational information to show greater im-provement in 1999 and 2000. Preliminary investigations show that the 2001-2002 period has a different profile of dis-closures, perhaps resulting from the precipitous decline in tech stocks in 2000 and the subsequent rash of complaints from customers in subsequent months and years. NASD staff suggested normalizing disclosure rates based on mar-ket performance, and this seems a promising approach. [1] H. Blau, N. Immerman, and D. Jensen. A visual query [2] C. Brodley and P. Smyth. Applying classification [3] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [4] C. Cortes, D. Pregibon, and C. Volinsky. Communities [5] T. Fawcett and F. Provost. Adaptive fraud detection. [6] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. From [7] L. Getoor, N. Friedman, D. Koller, and A. Pfeffer. [8] T. Hastie, R. Tibshirani, and J. Friedman. The [9] D. Jensen and J. Neville. Linkage and autocorrelation [10] D. Jensen and J. Neville. Avoiding bias when [11] D. Jensen, J. Neville, and B. Gallagher. Why [12] J. Neville, D. Jensen, L. Friedland, and M. Hay. [13] F. Provost and P. Domingos. Tree induction for [14] F. Provost and T. Fawcett. Analysis and visualization [15] S. Sanghai, P. Domingos, and D. Weld. Dynamic [16] B. Taskar, P. Abbeel, and D. Koller. Discriminative
