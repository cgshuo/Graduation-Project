 Features in many real world applications such as Chem-informatics, Bioinformatics and Information Retrieval have complex internal structure. For example, frequent patterns mined from graph data are graphs. Such graph features have different number of nodes and edges and usually over-lap with each other. In conventional data mining and ma-chine learning applications, the internal structure of features are usually ignored.

In this paper we consider a supervised learning problem where the features of the data set have intrinsic complexity, and we further assume that the feature intrinsic complexity may be measured by a kernel function. We hypothesize that by regularizing model parameters using the information of feature complexity, we can construct simple yet high qual-ity model that captures the intrinsic structure of the data. Towards the end of testing this hypothesis, we focus on a regression task and have designed an algorithm that incor-porate the feature complexity in the learning process, using a kernel matrix weighted L 2 norm for regularization, to obtain improved regression performance over conventional learning methods that does not consider the additional information of the feature. We have tested our algorithm using 5 different real-world data sets and have demonstrate the effectiveness of our method.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms, Experimentation Data Mining, Regression, Regularization
Data with complex features are becoming abundant in many application domains such as Cheminformatics, Bioin-formatics, Information Retrieval and among others. For ex-ample in Cheminformatics researchers usually model chemi-cal structures as a graphs and extract frequent subgraphs as features [2]. Such subgraph features have different number of nodes, different number of edges, and usually overlap with each other. In Bioinformatics and Information Retrieval, given a set of protein sequences or documents, if we use fre-quent subsequences of amino acids or words, each feature has its own complexity such as subsequence length [9].
In this paper we focus on learning from graph data, due to the wide range of applications where graphs are utilized as a modeling tool. In particular, we focus on the subgraph based graph learning problem where features are (frequent) subgraphs mined from the training graph data sets and we present each graph as a feature vector. Once we have trans-formed a graph to a feature vector, mining and learning from graphs is similar to any other type of vectorial data. Typically there are two types of learning tasks: unsuper-vised and supervised and we focus on the supervised graph learning problem in this paper.

Subgraph based graph learning problem has attracted re-search interest in the data mining community. For example, Tsuda [20] proposed a graph regression framework in which he employed L 1 norm regularization algorithm Lasso [19] to graph data and conducted forward stepwise feature selec-tion for regression. Saigo [15] applied partial least square regression to graph data and implemented feature selection during the pattern mining process. Yan et al. [21] performed a comprehensive study on mining significant graph patterns for graph classification. Fei &amp; Huan [4] studied structure consistency relationship of subgraph features, developed a subgraph feature selection method and employed Support Vector Machine to perform graph classification.

However, none of the existing methods considers the in-ternal structure of subgraph features and utilizes their com-plexity to construct accurate models. Our current working hypothesis is that complexity of subgraphs should be incor-porated into model construction in order to build simple yet high quality model predicting labels of graph data. To il-lustrate that point, we show an example in Figure 1. There are three graphs G 1 , G 2 and G 3 in Figure 1. F 1 and F frequent subgraph features if we use the minimal support threshold min sup = 2 3 . Using F 1 and F 2 as features, the object-feature matrix X , where each row is an graph and each column is a feature, is represented as: Figure 1: Three graphs G 1 , G 2 , G 3 and two subgraph features F 1 , F 2
In this example, we can see that no matter what labels the three graphs have, the two features F 1 and F 2 have exactly the same correlation with labels. Lasso [19] based regression method will randomly pick up a feature and assign coeffi-cient to it because F 1 and F 2 have the same correlation with labels. Ridge [7] will assign equal weights to F 1 and F 2 cause Ridge shrinks more on the direction where the singular value of X is smaller. In this case, the singular values of X are equal, hence Ridge will shrink the two features with the same ratio. However, intuitively, we would like to assign more weight to F 1 than F 2 since F 1 much simpler than F The reason why current regularization methods fail to do so is that they treat the feature as an atomic element and neglect the internal complexity of features.

In this paper towards the end of incorporating feature complexity in the model selection process, we investigate two approaches. First, we study the approach for measuring the complexity of subgraph features. Second, we investigate the algorithms that select simpler features to construct super-vised learning models. Specifically in our study, we utilize graph kernel functions to measure the complexity of graph features. To solve the supervised graph learning problem, we propose a L 2 norm based regularization method for re-gression using subgraph features. Though we evaluate our algorithms primarily using data sets from chemical struc-ture activity relationship study, these algorithms in principle should be applicable for any types of graph data.
Specifically our contributions in this paper are:
The rest of the paper is organized as following. In Section 1.1, we discuss related work. In Section 2 we present back-ground information and in Section 3 we show our detailed methodology. In section 4 we present the experimental study of our algorithm, followed by a conclusion and a discussion of the future work. Regularization based linear regression is not a new topic. Hoerl and Kennard [7] developed ridge regression based on L2 norm regularization. Tibshirani[19] proposed the Lasso method which is a shrinkage and selection method for linear regression. Lasso minimizes the sum of squared errors, with an upper bound on the L 1 norm of the regression coeffi-cients. Efron &amp; Hastie [3] designed a novel algorithm, Least Angel Regression (LARS), to solve the optimization problem in Lasso efficiently. Zou &amp; Hastie [24] developed a regres-sion framework based upon penalizing on L 1 and L 2 norm of coefficients simultaneously. Recently, a new direction is in feature selection in regularized learning is to explore the relationship of features. Yuan &amp; Lin [22] studied the case when features have a natural group structure and designed a technique to select grouped features called group Lasso. Zhao &amp; Yu [23] integrated a hierarchical relation on features to regression and proposed a method called iCAP. Quanz &amp; Huan [13] assumed a general undirected graph relationships of features and employed the feature graph Laplacian in lo-gistic regression for graph classification.

Though regularized regression has been studied for a long time, none of the existing method considers the special char-acteristics of graph data and subgraph features and hence may not provide the optimal results for graph regression. We develop a graph regression method incorporating feature in-formation and our experiment study shows that our method works very well on several real-world data sets compared with other regression models.
Here we introduce basic notations for graph, frequent sub-graph mining, graph kernel functions and regularized linear regression.
A labeled graph G is described by a finite set of nodes V and a finite set of edges E  X  V  X  V . In most applications, a graph is labeled, where labels are drawn from a label set  X  . A labeling function  X  : V  X  E  X   X  assigns labels to nodes and edges. In node-labeled graphs , labels are assigned to nodes only and in edge-labeled graphs , labels are assigned to edges only. In fully-labeled graphs , labels are assigned to nodes and edges. We may use a special symbol to represent missing labels. If we do that, node-labeled graphs, edge-labeled graphs, and graphs without labels are special cases of fully-labeled graphs. Without loss of generality, we handle fully-labeled graphs only in this paper. We do not assume any structure of label set  X  now; it may be a field, a vector space, or simply a set.
 Following convention, we denote a graph as a quadruple G = ( V, E,  X  ,  X  ) where V, E,  X  ,  X  are explained before. A graph G = ( V, E,  X  ,  X  ) is a subgraph of another graph G ( V 0 , E 0 ,  X  0 ,  X  0 ), denoted by G  X  G 0 , if there exists a 1-1 mapping f : V  X  V 0 such that
In other words, a graph is a subgraph of another graph if there exits a 1-1 node mapping such that preserve the node labels, edge relations, and edge labels.
 The 1-1 mapping f is a subgraph isomorphism from G to G 0 and the range of the mapping f , f ( V ), is an embedding of G in G 0 . Given a graph database GD , the support of a subgraph G , denoted by sup G , is the fraction of the graphs in GD of which G is a subgraph, or:
Given a user specified minimum support threshold min sup and graph database GD , a frequent subgraph is a subgraph whose support is at least min sup (i.e. sup G  X  min sup ) and the frequent subgraph mining problem is to find all fre-quent subgraphs in GD .

In this paper, we use frequent subgraph mining to extract features in a set of graphs. Each mined subgraph is a feature. Each graph is transformed to a feature vector indexed by the extracted features with values indicate the presence or absence of the feature as did in [8]. We use binary feature vector as contrast to occurrence feature vector (where the value of a feature indicates the number of occurrences of the feature in an object) due to its simplicity. Empirical study shows that there is negligible difference between the two representations in graph classification.
Kernel functions are powerful computational tools to an-alyze large volumes of graph data [6]. The advantage of kernel functions is due to their capability to map a set of data to a high dimensional Hilbert space without explicitly computing the coordinates of the structure. This is done through a special function K . Specifically a binary function K : X  X  X  X  R is a positive semi-definite function if for any m  X  N , any selection of samples x i  X  X ( i = [1 , n ]), and any set of coefficients c i  X  R ( i = [1 , n ]). In ad-dition, a binary function is symmetric if K ( x, y ) = K ( y, x ) for all x, y  X  X . A symmetric, positive semi-definite func-tion ensures the existence of a Hilbert space H and a map  X  : X  X  X  such that for all x, x 0  X  X .  X  x, y  X  denotes an inner product between two objects x and y . The result is known as the Mercer X  X  theorem and a symmetric, positive semi-definite function is also known as a Mercer kernel function [16], or kernel function for simplicity.

Several graph kernel functions have been studied. Re-cent progresses of graph kernel functions could be roughly divided into two categories. The first group of kernel func-tions consider the full adjacency matrix of graphs and hence measure the global similarity of two graphs. These include product graph kernels [5], random walk based kernels [10], and kernels based on shortest paths between pair of nodes [11]. The second group of kernel functions try to capture the local similarity of two graphs by counting the shared sub-components of graphs. These include the subtree kernels [14], cyclic kernels [18], spectrum kernel [2], and recently frequent subgraph kernels [17]. In this paper, we focus on graph random walk based kernels, where we use subgraph as features and kernels are defined on pairwise subgraph fea-tures.
In statistics and machine learning, regularization is a pow-erful tool to prevent overfitting. Regularization usually in-troduces additional constraints on the model as a form of a penalty for complexity. Consider a typical linear regression problem: where Y is a n  X  1 vector, X is a n  X  p matrix and  X  is a coefficient vector with the size of p  X  1 and  X  is gaussian noise with mean 0 and standard deviation  X  . Ordinary Least Square (OLS) minimizes the sum of squared errors k Y  X  X X  k 2 , where k . k is L 2 norm. But even though the solution of OLS is unbiased estimator,it is well known that OLS often does poorly in both prediction and interpretation and the model is very unstable.

Regularized linear regression not only minimizes the sum of squared errors, but bounds on the norm of regression co-efficients. For example, ridge regression [7] minimizes the residual sum of squares subject to a bound on the L2-norm of the coefficients. As a continuous shrinkage method, ridge re-gression achieves its better prediction performance through a bias variance trade-off. Lasso [19] is a penalized least squares method imposing an L1-penalty on the regression coefficients and does both continuous shrinkage and auto-matic variable selection simultaneously.
Our L 2 Norm Regularized Feature Kernel Regression method has two steps: (1) feature extraction and (2) regression. In the feature extraction step, we mine frequent subgraphs in the training samples as features. We then build a regression model, as discussed below, to predict the numerical labels of testing attribute graphs.
In this paper, we use capital letters, such as G , for a sin-gle graph and upper case calligraphic letters, such as G = G , G 2 , . . . , G n , for a set of n graphs. We assume each graph G i  X  G has an associated class label c i from a label set C . We use F = F 1 , F 2 , . . . , F n for a set of n features.
In this work we consider combining feature complexity into regression. Also we assume feature intrinsic complex-ity may be measured by a kernel function. Towards that goal, we build feature kernel first. An advantage of sub-graph features is that the kernel function defined on graphs can also be applied to subgraph features. In our article, we apply Marginalized kernel [10] to the L 2 penalty function. Marginalized kernel for graphs is described as: where G, G 0 are two graphs, z = [ G, h ], and h, h 0 are hid-den variables defined as a sequence of vertex indices, which is generated by random walks on the graph. K z ( z, z 0 ) is the kernel between the sequences of vertex and edge labels traversed in the random walk.

To avoid singularity, we add an dirac kernel matrix K d after marginal feature kernel matrix. That is, K = K m + K This will not jeopardize the kernel feature regression setting because the sum of two kernel matrices is still a valid kernel matrix. The dirac feature kernel matrix is defined as:
To introduce feature complexity to regularization, we con-struct a weighted complete graph where each node repre-sents a feature F i and the weight of each edge E i j equals to a K ( i, j ) in kernel matrix. With the feature graph, we build graph Laplacian matrix L = D  X  K to capture complexity of features, where K is the feature kernel matrix and D is a diagonal matrix defined as:
We do not normalize the graph Laplacian since we not only consider pairwise features X  X  complexity but each fea-ture X  X  own internal complexity. Suppose that the data set contains n observations and p predictors, with response vec-tor Y = ( y 1 , . . . , y n ) T and the data matrix X = ( ~x that the predictors are standardized and the response is cen-tered so that for all j , P i =1 y i = 0 . The regression function is linear with the fol-lowing form: Y = X X  , where  X  is a n  X  1 coefficient vector. The Lagrange form of the objective function is: where  X  &gt; 0 is the regularization parameter.

Our goal is to find  X  such that equation 5 is minimized. It is nontrivial to solve this optimization problem because the objective function is in quadratic form. Compute the first derivative of equation 5 with respect to  X  , we have: then by setting the derivative to zero, we can obtain: where  X   X  is our estimation and L is p  X  p laplacian matrix.
Ridge regression is a classical L 2 norm regularization based linear regression. In our framework, ridge regression is a spe-cial case. Ridge regression minimizes k Y  X  X X  k 2 +  X  X  T which is exactly the same when we set the Laplacian matrix to the identical matrix.

Next, we will show that our feature kernel regression frame-work shrinks more on the directions where singular value of X  X  L  X  1 2 is smaller. Similarly Ridge regression penalizes more on the directions where singular value of X is smaller.
Applying eigen decomposition to the positive definite ker-nel matrix L , we can factor L into the product of a ma-trix and the transpose of the matrix L r , represented as L = L T r L r , where L r = D 1 2 V T and D is the diagonal matrix with eigen values and V is the matrix with columns as eigen vectors. The solution of our framework can be rewritten as  X   X  = ( X T X +  X L T r  X  L r )  X  1 X T Y . By employing Generalized Singular Value Decomposition for X ( n  X  p ) and K r ( p  X  p ), we denote X = U  X  1 [0 , R ] Q T and L r = V  X  2 [0 , R ] Q factorization satisfies following properties:
Since rank ( L r ) = rank ( L T r  X  L r ) = rank ( L ) and L is non-singular in our framework, [0 , R ] = R . Our estimation of Y is:  X  Y = X  X   X  Figure 2: Left: Lasso estimates as a function of P 3 i =1 where u i is the column vector of U and d i =  X  i / X  i . From the result, we observe that L 2 norm regularized feature ker-nel regression first projects Y on the basis of U generated by singular value decomposition of XL  X  1 2 where L is the Laplacian matrix, then the projected values are re-scaled according to the values encoded in the diagonal matrix:
Finally the stretched values are re-described in the coor-dinate system by using the basis (columns) of U .
Compared with Ridge regression purely data driven, which just projects Y on the principle component of X and then shrinks coefficients along the direction lower singular value of X , we consider both data and features.
The purpose of this simulation is to show that the L 2 norm feature kernel regression not only stabilizes the regression coefficients but assigns coefficient values based on the com-plexities of features. We generate multi-variate Gaussian data with n samples having zero mean and p features. For simplicity, we generate n = 200 samples and p = 3 features x , x 2 , x 3 , where x 1 and x 2 are correlated with correlation coefficient  X  = 0 . 9 and x 3 is independent from the rest two features. The response value Y is generated by
Where  X  = [1 . 1 , 1 . 0 , 0 . 5] T . Assume we have additional information about the features and the feature kernel matrix is given by:
We run Lasso, ridge and our method on this data set over a wide range of regularization parameters and show the reg-ularization pathways of feature kernel regression, lasso and Ridge in Figure 2. From Figure 2, Lasso selects x 1 first re-gardless of the fact that x 1 is much more complicated than x , and the simple feature x 2 will not enter the active set until very small penalty; Ridge regression assigns almost equal coefficients to x 1 and x 2 ; for feature kernel regression, it is clear that the x 1 with high complexity obtains small coefficient and x 2 with low complexity is assigned large co-efficient. This is desirable for building a regression model because the chance complex feature from training data oc-cur in test data is very low and simple features will give better generalization performance.
We have performed a comprehensive study of the perfor-mance of our regression framework using 5 chemical struc-ture graph data sets. We have compared our method with 2 representative regularized regression methods: Lasso [19] and Ridge Regression [7].

For each data set, we used the FFSM algorithm [8] to extract frequent subgraph features from the data sets. We measured the regression performance of our regression method and compared ours with those from state-of-the-art methods using cross validation.
 Figure 3: Experimental work flow for a single cross
We select 4 chemical data sets from Binding Database [12] and 1 data set EDKB from http://edkb.fda.gov/databasedoor. html . For each data set, the response values are chemical X  X  binding affinity to a particular receprot. In this case, the affinity is measured by the concentration, which represents how much of this chemicals is needed to observe binding ac-tivity to a certain protein. See BindingDB [12] and ChemDB [1] for further details regarding the nature of the data sets.
We follow the same procedure [8] to use a graph to model a chemical structure: a vertex represents an atom and an edge represents a chemical bond. Hydrogen atoms are removed in our graph representation of chemicals, as commonly done in the cheminformatics field. The characteristics of the data set is shown in Table 1.
 Table 1: Data set: the symbol of the data set. S : total For each data set, we mined frequent subgraphs using the FFSM algorithm [8] with min support = 25% and with at least 2 nodes and no more than 10 nodes. Empirical study shows that there is no significant changes if we replace the fixed value 25 with a relatively wide range of values. We then treated each subgraph as a feature. We adopted two ways of extracting feature values: exact subgraph matching and approximate subgraph matching. For exact subgraph matching, We create a binary feature vector for each graph in the data set, indexed by the mined subgraphs, with val-ues indicate the existence (1) or absence (0) of the related features. For approximate matching, the feature vector con-struction is exactly the same except that the feature value is a real number between 0 and 1 representing the ratio of the matching size and the feature size. To build feature ker-nel matrix, we use CHEMCPP, a public library available at http://chemcpp.sourceforge.net/html/index.html .

As indicated before, we compared our method with other 2 regression methods. To have a fair comparison, we use 5-fold cross validation to derive training and testing samples and run all the methods. Since we have regularization pa-rameter  X  in all three methods, we did internal 5 fold cross validation within the training data to obtain the best tuning parameter of each method, obtain regression model on the whole training data and apply the trained model to test data to make prediction. We repeat the whole process 10 times and report average performance. Figure 3 gives an overview of our experimental set up.

For one cross validation, the prediction accuracy is mea-sured by R 2 . R 2 is close to 1 when the regression function fits good, and is close to 0 when it does not.
 where y i is true value,  X  y i is the prediction and n is total number of samples. For each data set, we repeat 5 fold cross validation 10 times and report the average R 2 value and standard deviation. We perform all of our experiments on a desktop computer with a 3Ghz Pertium 4 processor and 4 GB of RAM. Figure 4: Average prediction accuracy comparison over
In this section, we present the performance of our method compared with two additional methods: Lasso and Ridge. Based on different feature extraction ways, we have six varia-tions of methods to compare: LassoE (Lasso with exact sub-graph matching), LassoA (Lasso with approximate subgraph matching), RidgeE (Ridge with exact subgraph matching), RidgeA (Ridge with approximate subgraph matching), FKE (feature kernel with exact subgraph matching) and FKA (feature kernel with approximate subgraph matching). The prediction performance is measured by average R 2 of vali-dation results and is shown in Figure 4.
 In Figure 4, the X-axis is labeled with data set name and Y-axis is average R 2 value. The performance of the three methods varies with the same trend. A clear trend is that prediction accuracy from approximate feature value extrac-tion is better than that of exact feature value extraction for all the three methods. Among approximate feature ex-traction experiments, our method outperforms LassoA and RidgeA in 3 out of 5 data sets, and 1 comparable. For exact subgraph matching, LassoE outperforms RidgeE and FKE. A future work is to investigate why the performance of fea-ture kernel regression is not consistent in different feature extraction ways.

Table 2 shows Average R 2 value and standard deviation for three methods under approximate subgraph matching. From Table 2, we observe that our method is relatively more stable than Lasso and Ridge with smaller standard deviation and higher prediction accuracy.
Since we have parameter min sup in the feature extraction step, we changed the min sup during the feature generation process to see the robustness of our method. In the follow-ing study, we have singled out a data set (the CathepsinK data set) and test the robustness of our method using this data set by varying the min sup in frequent subgraph feature extraction process.

In Figure 5, we changed min sup during feature generation process to see the robustness of our method. We change min sup from 20% to 50%, and compute average R 2 based on the same experiment protocol. From Figure 5, we can see that our method remains stable with variant of min sup.
Overall, our L 2 norm feature kernel method is effective and achieves good accuracy within a wide range minimum support. Our method is not constrained by marginal graph Figure 5: Average prediction accuracy for 5 fold cross kernel or Dirac Kernel, any other graph kernel function can be combined with our framework.
In this paper, we studied the regression problem in which the feature has intrinsic complexity and presented a novel L 2 norm feature kernel regression method for graph data. By incorporating laplacian induced from subgraph feature kernel matrix into penalize function, we solved this new op-timization problem and revealed its connection with Ridge regression. Compared with current state-of-the-art methods as evaluated on 5 real world data sets, our method signifi-cantly outperforms the Lasso and ridge on majority of the tested data sets. In this framework, we penalize on L 2 norm of feature kernel only and that will not introduce sparseness to our model. In the future, we will design a new model that combine L 1 and L 2 norm penalization on features together to achieve both sparsity and stability of regression model. Also we will test more kernel function for this framework to see whether the performance is consistent.
 This work has been partially supported by the Office of Naval Research (award number N00014-07-1-1042). [1] J. Chen, S. J. Swamidass, J. B. Y. Dou, and P. Baldi. [2] M. Deshpande, M. Kuramochi, and G. Karypis. [3] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. [4] H. Fei and J. Huan. Structure feature selection for [5] T. G  X  artner, P. Flach, and S. Wrobel. On graph [6] D. Haussler. Convolution kernels on discrete [7] A. Hoerl and R. W. Kenard. Ridge regression: biased [8] J. Huan, W. Wang, and J. Prins. Efficient mining of [9] I. Jolliffe. Principal Component Analysis. Springer; [10] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [11] B. K.M. and K. H.-P. Shortest-path kernels on graphs. [12] T. Liu, Y. Lin, X. Wen, R. N. Jorissen, and M. K. [13] B. Quanz and J. Huan. Aligned graph classification [14] J. Ramon and T. G  X  artner. Expressivity versus [15] H. Saigo, N. Kr  X  amer, and K. Tsuda. Partial least [16] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels . [17] A. Smalter, J. Huan, and G. Lushington.
 [18] S. W. Tamas Horvath, Thomas Gartner. Cyclic [19] R. Tibshirani. Regression shrinkage and selection via [20] K. Tsuda. Entire regularization paths for graph data. [21] X. Yan, H. Cheng, J. Han, and P. Yu. Mining [22] M. Yuan and Y. Lin. Model selection and estimation [23] P. Zhao and B. Yu. Grouped and hierarchical model [24] H. Zou and T. Hastie. Regularization and variable
