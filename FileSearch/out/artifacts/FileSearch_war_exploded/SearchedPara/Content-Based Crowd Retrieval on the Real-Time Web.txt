 In this paper, we propose and evaluate a novel content-driven crowd discovery algorithm that can efficiently iden-tify newly-formed communities of users from the real-time web. Short-lived crowds reflect the real-time interests of their constituents and provide a foundation for user-focused web monitoring. Three of the salient features of the algo-rithm are its: (i) prefix-tree based locality-sensitive hashing approach for discovering crowds from high-volume rapidly-evolving social media; (ii) efficient user profile updating for incorporating new user activities and fading older ones; and (iii) key dimension identification, so that crowd detection can be focused on the most active portions of the real-time web. Through extensive experimental study, we find signifi-cantly more efficient crowd discovery as compared to both a k-means clustering-based approach and a MapReduce-based implementation, while maintaining high-quality crowds as compared to an offline approach. Additionally, we find that expert crowds tend to be  X  X tickier X  and last longer in com-parison to crowds of typical users.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Information networks Algorithms, Experimentation clustering, social media, community detection, real-time web
The real-time web has grown at an astonishing rate in the past several years. As one example, Twitter has rapidly grown from handling 5,000 tweets per day in 2007 to 50 mil-lion tweets per day in 2010 to 140 million per tweets per d ay in 2011. During the recent run-up and immediate af-termath of President Obama X  X  announcement about Osama Bin Laden, Twitter boasted a peak of 5,000 tweets per sec-ond (corresponding to 432 million tweets per day) and a sus-tained average rate of 3,000 tweets per second over several hours (corresponding to 259 million tweets per day). 1 At an order of magnitude higher, Facebook reported in 2009 that it was handling around 1 billion chat messages per day, 2 there is widespread evidence of massive growth in web-based commenting systems (like on Reddit, Digg, and NYTimes) and other real-time  X  X ocial awareness streams X .

While long-lived communities have been one of the key organizing principles of the Web, the real-time web sup-ports the near instantaneous formation of ad-hoc commu-nities linked by the real-time interests of their constituents. These X  X rowds X  X ange from groups of loosely-connected Twit-ter users responding to a live presidential address, to users sharing pictures about a chemical fire at a nearby refinery, to flash mobs congregating at particular locations (and re-vealing this location via services like Foursquare), and so on. For example, Figure 1 shows example of two content based crowds, one discussing the public release of Jay-Z and Be-yonce X  X  baby pictures with 3 users (eonline, ap, ravengood-win), and another crowd about NY Knicks vs LA Lakers basket ball game with 2 users (bharris901, geneforeman). intervals. Crowds are shown using a red boundary.
In contrast to traditional long-lived communities, these crowds are dynamically formed and potentially short-lived, often with only implicit signals of their formation within the massive scale of the real-time web. While much research has focused on community mining from large-scale systems [10, 21, 25, 26], of event detection on the Web and in social media [7, 8, 17, 24], and topic detection and tracking over streaming news items [3, 4, 12], there is a great opportu-nity to explore the near real-time extraction of crowds in the critical moments of their initial formation. Efficiently identifying these crowds of related users as they form and as they evolve can benefit many domains including epidemi-ological and disease control experts searching for evidence of new outbreaks and the reaction of the public to new vac-cines, municipalities interested in responding to local events (like the recent Vancouver riots), finance experts monitor-ing stock price jumps or crashes, political scientists track-ing chatter about presidential debates, as well as average users interested in local crowds (what restaurants are hot?) and crowds associated with particular topics (e.g., sports, movies).

In this paper we formalize the problem of crowd discovery over rapidly evolving social media and provide solutions for efficiently identifying crowds. Although we focus on text-based social media streams popularized by Twitter and re-lated services, the discussion and techniques are designed for generic application to other temporally ordered social me-dia resources. Concretely, this paper makes the following contributions:
In addition to the works cited in the introduction, there have been many efforts aimed at detecting cluster structure in text-based collections [19, 9, 5]. But, these approaches, however, are typically not designed for high-volume incre-mentally updated domains as on the real-time web. Al-ternatively, there is a large body of stream-oriented clus-tering work for finding correlations in streaming data. For example, StatStream [28] clusters evolving time series data using the Discrete Fourier Transform. Both [3] and [11] ex-plore two-stage approaches for finding clusters in low dimen-sional data (unlike the case of text clustering, which typi-cally is very high-dimensional due to the number of tokens observed). Clustering over text streams has been studied in [2, 18, 14]. These efforts have focused on the clustering of independent text elements (e.g., new messages), whereas our focus is on finding groups of related users by their sequences of related posts to the real-time web.

The solution approach in this paper relies on locality-sensitive hashing (LSH) for finding nearest-neighbors as a primitive for crowd detection. Nearest-neighbor and approx-imate nearest-neighbor search in a high-dimensional vector space is a difficult problem that Indyk and Motwani [15, 13] approach through the use of a family of randomized hash functions that generate similar vector signatures if the vec-tors are closer to each other in the high-dimensional space. In [6], Charikar constructed the LSH function for cosine similarity, which supports fast similarity between two high-dimensional vectors by reducing them to bit-arrays of much smaller dimensions. This result has been used in several problems, including efficient noun clustering [23, 20, 22]. In previous work, we studied crowd detection based on user communication, without regard for the content of the mes-sages as we do here [16].
Let U = { u 1 , u 2 , . . . , u i . . . } be a (potentially) unbounded set of users posting messages to a real-time web stream such as Twitter or Facebook. Each user may contribute an arbi-trary number of messages, where the messages are ordered in a non-decreasing fashion using the time-stamp values of the messages. We say that a crowd C = { u i , u 2 . . . u given time, is defined as a subset of users that are close to each other at that time, where closeness is measured using a similarity function sim ( u i , u j ). For example Fig-ure 2, shows a simple scenario where users are mapped into a 2-dimensional space (say, by using TF-IDF weights of the w ords in the messages). In the initial figure at time t n are sparsely distributed in the space and there are no clear crowds. As users generate more messages, we see in the fol-lowing two intervals the formation of several tight clusters of users ( X  X rowds X ). Intuitively, these crowds correspond to collections of users who are posting messages about similar topics (e.g., the Super Bowl on one day and Presidential elections the next day).

Given a user similarity measure sim ( ~u i , ~u k ) and a user similarity threshold  X  , we formulate crowd detection as an operation that preserves the following two properties: Property 1: Every user in a crowd has at least one other user in the same crowd, such that the similarity between them is at least  X  . That is,  X  u i  X  C  X  u k : u k  X  C, u u k and sim ( ~u i , ~u k )  X   X  Property 2: Every user in a crowd has no other user out-side the crowd, such that the similarity between them is at least  X  .  X  u i  X  C  X  X  u k : u k  X  S \ C and sim ( ~u i , ~u
These two properties ensure that (i) all users within a crowd are more similar to users within the crowd than out-side of the crowd; and that (ii) there does not exist any user outside of a crowd who is similar to users within a crowd.
By viewing crowd detection in this way, we can avoid memory-intensive approaches that require maintaining the overall cluster structure (which may be unreasonable for high-volume text); instead, we can formulate the crowd de-tection problem using nearest-neighbor search as a primi-tive, as illustrated in Algorithm 1. That is, for every new message posted to the real-time web, we determine the user nearest to the user posting the new message. If the sim-ilarity between the user posting the new message and the nearest user is at least  X  , we add the user to the crowd to which this nearest user belongs, if he is not already in it. If the similarity does not exceed  X  , we create a new crowd for the given user. K t is the set of all current crowds at time t . While such an approach may allow long chains of users (where the first user in a crowd is quite distant from the last user), it has the compelling advantage of efficiency. Algorithm 1 C rowd Discovery for ( u , d, t )  X  I do end for
Towards efficiently discovering crowds from the real-time w eb, we make note of the following three challenges:
In the following, we approach each of these three chal-lenges in turn, before turning to an experimental evaluation.
In this section, we first develop a vector representation for users that decays temporally, so that users are assigned to crowds that reflect their current interests and then we show how to efficiently update these user profiles as new messages a re generated.
Adopting a vector space model for users, let ~u i be the vec-tor representation for user u i , where the elements of the vec-tor correspond to tokens parsed from u i  X  X  messages. There are many domain-dependent choices for parsing messages, including language-dependent parsers, entity extraction, stem-ming, and so forth; for simplicity, we adopt a simple unigram parser that treats all strings separated by whitespace as valid tokens. Since the number of unique tokens corresponding to dimensions are not known in advance, we represent each user profile vector using an infinite co-ordinate space F [1]. Under this model, a user u i at time t is represented as: where the User Vector Dimension (UVD) value V t im , is the value for u i in the m th dimension at time t . Let x t im the number of times u i generates m at time t , and X t l { x by u i until t l , then V t l im is defined as: where F is a function of x im , t and t l and is called the UVD function.

In this way, a user is represented as the sum of his en-tire message history. However, since crowds are designed to reflect users with a similar current interest, such an ap-proach may favor crowds of users who are similar in the long-term. For example, we may identify crowds of students, of entertainers, and of politicians, but miss cross-cutting crowds that are drawn together by their current situation (e.g., emergency-oriented crowds reacting to a local earth-quake). An alternate approach is to construct user profile vectors using the latest messages only. While such an ap-proach has the advantage of being memory-less (and so, old messages may be dropped with no penalty), grouping users based only on their most recent messages may result in high crowd fluctuation since crowd assignments may vary with each new message.

To balance these two extremes, we propose to adopt a rep-resentation that fades user vectors such that recently used dimensions have higher values and older dimensions have lower values. To decay user vectors, we design another UVD function D , which decreases the score of inactive dimensions and increases the score of active dimensions in user vectors. The function D , re-calculates scores for x t o im at time t shown: where  X  u  X  [0 , 1] is a constant know as the user dimension score decay rate . Hence, we can re-write V t l im as: Note that when  X  u = 1, the value of V t l im is same as that calculated using F as the UVD function.
To calculate V t l im using (3), we have to maintain the entire set X t l im . In the context of the real-time web, this can be inefficient since it requires maintaining X t l im for all users and all dimensions and since the calculation of V t l im would be O( | X t l im | ). To solve this problem we prove a proposition that will help us calculate the value of V t l im efficiently in O (1) time without requiring us to maintain the set X t l im .
Proposition 3.1. If t n  X  k is the latest time when u i gen-erated a message with dimension m until t n , then the value of the dimension at time t n , is given by: where, V t n  X  k im and V t n im are the values of dimension m for u at time t n  X  k and t n respectively.

Proof. Let X t n  X  k im be the set all occurrences of dimension m in the messages generated by u i up to time t n  X  k . Then, using (3) we get: Using (2),  X  x t im  X  X t n  X  k im we can write: where t is the time-stamp of every occurrence of m in mes-sages generated by u i .
 Using (3) again, we write, Using (4) and (6) we can now write Since u i did not generate any messages with dimension m after t n  X  k until t n ,  X  n  X   X  [ n  X  k + 1 .. n  X  1], we have: Hence, Note that, by definition x t n im 6 = 0 if u i generates a message with m , else it is 0. This proves the proposition.
In brief, we have described an approach to represent users i n high-dimensional vector space that reflects their current interests and we have shown how to update this user profile efficiently upon the arrival of each new user message.
G iven the user profile developed in the previous section, we now turn to the challenge of assigning users to crowds as outlined in Algorithm 1. This is the core step in crowd detec-tion and is, in essence, a nearest-neighbor problem. To find nearest neighbors there are several possible methods. The simplest algorithm to determine nearest neighbor is through linear O ( n ) search, which is not efficient due to the large number of users on the real-time web. Alternatively, we can use efficient space-partitioning methods like k-d trees, which have a complexity of O (log n ). Here, we propose a special-ized variation of the randomized approach to discover near-est neighbors by using locality sensitive hashing (LSH). In this specialized version, we use an additional prefix tree data structure to support O (1) lookup of a user X  X  nearest neigh-bor, at a cost of requiring O ( n ) to look up the user X  X  next nearest neighbor. But by constructing crowd detection as a requiring only user X  X  single nearest neighbor (recall the two properties at the beginning of this section), we can support efficient crowd detection over the real-time web.
We first describe a function to calculate the similarity be-tween two vectors using LSH and then describe how we can use this similarity function to determine nearest neighbors efficiently using a prefix tree. Since users are represented as vectors, we can use a metric like cosine similarity to deter-mine the nearest neighbor. But, as described in [15], deter-mining nearest neighbors using cosine similarity is inefficient in high dimensions. Hence, we calculate the approximate cosine distance between two vectors using the approach pro-posed by Charikar [6].

In [6], the author proposed using LSH functions generated using random hyperplanes to calculate approximate cosine distance. Consider a set of vectors in the collection R m Let ~r be a m -dimensional random vector, such that each dimension in it is drawn from a 1-dimensional gaussian dis-tribution with mean 0 and variance 1. Then the hashing function h ~r corresponding to ~r is: random vectors, then for a vector ~v , we can generate its signature  X  v = ( h ~r 1 ( ~v ) , h ~r 2 ( ~v ) , . . . h vectors ~u i and ~u j , the approximate cosine similarity between them is given as: sim ( ~u i , ~u j ) = cos (  X  ( ~u i , ~u j )) = cos ((1  X  P r [  X  u So, the closer the signatures, the greater is the cosine sim-ilarity, and the more dissimilar the signatures, the lesser is their cosine similarity. This equation measures approximate cosine distance, and accuracy of this approximation can be improved by using a longer signature, i.e, a larger R .
We now describe the procedure to find the nearest user u n for a user u , from whom we can determine the nearest crowd C . We determine u n using a set of permutation functions of the form: where, p is a prime number and a, b are chosen randomly.
Let P be a collection of | P | prefix trees, where every prefix tree corresponds to a permutation function  X   X  P . Now, to add a vector ~v to P , first its signature  X  v is determined, and then the signature is inserted into every prefix tree in P after permuting it using the corresponding permutation function. So for a given vector, | P | permutations of its signature are stored in P . Every time we observe a new user vector it is added to P . Similarly, every time we modify a user vector, we remove its old signature from all the prefix trees in P and add the new one.

To determine the crowd nearest to ~u in P , we first cal-culate its signature  X  u . Then for every prefix tree in P , we permute this signature using the corresponding permutation function and find the nearest signature in the prefix tree, by iterating through the tree one level at a time starting from the root. After doing this step we end up with | P | signatures, of which the crowd corresponding to the signa-ture with smallest hamming distance is picked as the nearest neighbor of ~u . As a result, we see that using a prefix tree in combination with LSH, we can design an efficient algorithm to assign users to crowds.
The final challenge is a consideration of the purpose of the crowd monitoring application in the selection of the key dimensions for representing user vectors. For exam-ple, if the crowd detection system is intended for topic-focused crowd detection (e.g., identify all  X  X arthquake X  re-lated crowds, find all crowds related to  X  X olitics X ), then the user vectors could be weighted toward these key dimen-sions (e.g., as in a scheme for weighting the dimensions corresponding to the tokens  X  X bama X ,  X  X ebate X ,  X  X epubli-can X  X s more important dimensions than non-politics dimen-sions). Potential solutions include pre-seeding the crowd detection system with expert-labeled keywords or in identi-fying high value terms by their inverse document frequency (IDF) , which weights key terms by their relative rarity across all documents.

In this paper, we propose to select as key dimensions those that reflect the general consensus of the real-time web. That is, we seek to identify tokens that are globally popular at a particular time for biasing the crowd detection toward these tokens. In this way, crowds are defined both by users who have posted similar messages recently (as described in the previous section) and by reflecting topics of great impor-tance to the overall system.

Concretely, our goal is to select from all dimensions the most m significant dimensions. As the real-time web evolves the list of top-m dimensions can then be updated frequently to remove old dimensions and add new ones. Hence, we require a metric to score the dimensions observed so far. To score the dimensions observed in the stream, we use an approach similar to the one used in scoring dimension score for a user vector in Section 3.1.

Let y t d be the number of times a dimension d appeared in the stream at time t . Then the score for y t o d at time t Algorithm 2 C rowd Discovery
Create R : Create the set R = { ~r 1 , ~r 1 . . . ~r | R | Gaussian vectors such that | R | &lt;&lt; m .
 Initialize P : Create the set of permutation functions
P = {  X  1 ,  X  2 , . . . ,  X  | P | } , where each permutation function is defined using a prime number p and values a, b chosen randomly. Initialize P as a collection of | P | prefix trees and assign a unique permutation function from P to every prefix tree in P . for ( u, d, t )  X  I do end for t  X  t o , is given by a function E , defined as: where  X  d  X  [0 , 1] is a constant know as the dimension score decay rate .

Since a dimension can be observed several times in a stream, the score for a dimension d at time t , W t d , is cal-culated as shown in Proposition 3.2
Proposition 3.2. If t n  X  k is the latest time when dimen-sion d was observed on the stream until t n , then the dimen-sion score for the dimension at time t n , is given by: where, W t n  X  k d and W t n d are the dimension scores at time t n  X  k and t n respectively.

Proof. The proof for this is similar to the proof of Propo-sition 3.1.

Hence, we can identify dimensions that reflect the con-s ensus of the current activity of the real-time web, so that crowd detection can be focused on the most active portions of the real-time web and so resources are not wasted.
Taken together, the high-level crowd discovery algorithm described in Algorithm 1 and the three methods developed  X  efficient user profile updating, efficient crowd assignment, and identifying key dimensions  X  give us the crowd discovery algorithm in Algorithm 2.
In this section, we report a series of experiments to study crowd discovery. We evaluate the running time performance of the proposed crowd discovery algorithm with other al-gorithms for crowd discovery. We define metrics to mea-sure quality of crowds discovered and using these metrics we evaluate the quality of crowds discovered by several crowd discovery algorithms. We study the factors impacting the performance of the proposed algorithm, and finally we an-alyze the properties of crowds discovered over two Twitter datasets.
To simulate a Twitter stream, we selected a set of Twitter users and crawled their tweets using Twitter API. The users in this set are labeled using 4 classes  X  technology, entertain-ment, politics and sports. To collect this labeled dataset we used the snowball sampling approach. This approach is as follows:
Following these steps resulted in a  X  X nowball X  or chain of crawling actions, which we stopped once we observed suffi-cient users. At the end of this crawl, we were left with a set of users, lists they belong to, with lists labeled with the class they belong to. Using this information, for each domain we selected around 1,200 top users and used their tweets to simulate a labelled Twitter stream, resulting in about 1.6 million tweets for 30 days. A similar approach for sampling class specific Twitter data is described in [27]. In addition to this dataset (which we shall call the Experts dataset), we collected a location-based dataset of users tweeting from the Houston region who were selected through random sam-pling. A 30-day sampling of this stream had about about 15 million tweets from about 107 thousand users. We use the Experts dataset for all of our experiments, except for the experiments in Section 4.7.
We compare the crowd discovery algorithm ( CDA ) pro-posed in this paper with four alternatives: k-means cluster-ing ( k-means ), a Map-Reduce implementation of k-means clustering ( MR k-means ), a deterministic batched version of the CDA approach ( Iterative-CDA )  X  in which we iterate through all the pairs of user vectors to find the best crowds possible, and a Map-Reduce implementation of Iterative-CDA ( MR-CDA ).
For user vector processing, we set the following parame-t ers: number of dimensions m = 199 , 999, user dimension score decay rate  X  u = 0 . 75 and dimension score decay rate  X  d = 0 . 75. For efficeint crowd assignment, we set signature length | R | = 23, number of permutation functions | P | = 13 and  X  = 0 . 005.

In initial experiments, we varied the choice of k for k-means, finding in many cases that k-means identified many singleton crowds. For the experiments reported here, we set the number of clusters as k = 0 . 95  X  number of items to cluster.
To evaluate the running time performance of the proposed approach, we perform two experiments: (i) we use tweet sets of varying sizes as input to all the algorithms and determine the time taken by them to discover crowds; (ii) we mea-sure the tweet processing rate of the algorithms. For these experiments we use a 30 day sample of Experts stream. Running Time with Clustering Algorithms : The plot in Figure 4(a), shows the running times for the two k-means clustering algorithms and CDA to discover crowds on data collection of varying sizes. The running times graph is a log-log graph, hence there are orders of magnitude differ-ence between the running times of the algorithms. We see that the time required to discover crowds using the pro-posed algorithm is significantly lesser than that required by the clustering algorithms. As the size of the message collec-tion increases, both the clustering algorithms become slower. This behavior is expected in case of iterative k-means, be-cause of the extra iterations required by the algorithm, but was not expected in the Map-Reduce version. Generally, the Map-Reduced running time increases at a much slower rate, but is still lesser than that of the iterative version. We believe the worsening performance is because of the large value of k . Larger k results is passing of greater number of centroids to a map job which slows down the algorithm. Hence, either of these algorithms are not efficient to discover crowds.
 Running Time with CDA Algorithms : We now run similar experiments with the other crowd discovery algo-rithms. As in the case of the clustering algorithms, we see that CDA, in Figure 4(b), performs much better than the batched CDA algorithms. The Iterative-CDA performs the worst while the MR-CDA performs better after about 10 4 messages. The bad performance of MR-CDA on initial mes-sage sets can be attributed the time spent by the MR cluster in setting up the job and passing messages between various workers.
 Message Processing Rate with CDA Algorithms : To compare the rate at which the algorithms process messages as they arrive, we note the number of messages that the algorithms have processed at equally spaced time intervals. This comparison is shown in Figure 4(c). As expected, we observe that the number of messages processed by the pro-posed algorithm is more than that for the other CDA al-gorithms. This result supports the result we observed with running time Figure 4(b). Similar results were observed for k-means clustering as well but are omitted due to the space constraint.
We now evaluate the quality of crowds discovered using the proposed crowd discovery approach. We know the class to which users in our Twitter stream belong, hence, to eval-uate crowd quality we can compare the crowds discovered to this  X  X round truth X . While we do not expect all users be-longing to a particular class (e.g.,  X  X ports X ) to form a single large crowd, we do expect that crowds that form will tend to be composed of users belonging to these classes. We use the same 30 day sample of the stream that we used in Sec-tion 4.3. Like before, the experiments are run with the same value for parameter m . We next describe evaluation metrics that we use to measure quality of crowds and then present performance of CDA against k-means clustering algorithms and deterministic CDAs.
 Quality metrics : Consider the set of crowds K = { C 1 , C . . . , C n } for users in set U and a set of classes  X  = {  X  . . . ,  X  w } to which users in U belong. To measure the quality of crowds generated using crowd discovery algorithms we use the following metrics.
 Purity : To compute purity, we assign crowd to the domain which is most frequent in it, and then the accuracy of this assignment is measured by calculating the ratio of correctly assigned users. NMI : Purity gives a good understanding of quality. But, it is susceptible because high purity can be achieved when there are large number of crowds, which we expect in crowd discovery problem. Hence, to deal with this issue, we use a secondary information theory based quality metric called Normalized Mutual Information (NMI). It is defined as: w here, I ( K,  X ) is mutual information and H entropy. Comparison with Clustering Algorithms : The com-parison between quality of crowds discovered using the It-erative k-means and that discovered using CDA is shown Figure 5(a). We see that despite the significant improve-ments in running time, the crowds discovered by the CDA are still of high quality. We also notice, for all the metrics, the quality of crowds generated using CDA is better than the quality of crowds generated using a clustering algorithm. The relatively poor performance of the clustering algorithm can be attributed to the difficulties in estimating the number of clusters k .
 Comparison with CDA Algorithms : The comparison between quality of crowds discovered using the Iterative-CDA and that discovered using CDA is shown in Figure 5(a). We see that crowds discovered by Iterative-CDA are always better than that discovered using CDA. The lower values for these metrics is expected in case of CDA, as it is a random-ized and an approximate algorithm whereas Iterative-CDA is an deterministic algorithm.
In Section 3.1, we described the method to exponentially decay user vectors to help us discover temporally relevant crowds. We evaluate the effectiveness of this approach by analyzing the performance of CDA when the user vectors are exponentially decayed and when they are not. To evalu-ate the performance of the algorithm without decay, we set  X  u = 1 . 0. The difference in quality of the crowds generated by the algorithm using these two approaches is shown in Figure 5(b).

The top plot of Figure 5(b), shows the running time of the algorithms for this experiment. We observe that, thought the running times for the algorithms is almost the same ini-tially, the difference between them increases with time. This is because, as time increases, the algorithm that decays user vector and uses techniques to score dimensions, has the abil-ity to remove dimensions when they become stale. This fea-ture is not possible when the algorithm is run without decay.
As shown in the bottom plot of Figure 5(b), the quality of crowds discovered using exponential decay is much better than the crowds discovered without decay. When user vec-tors are not decayed, old dimensions are not removed from it, resulting in crowds being discovered which contain users from different domains. This results in lower quality crowds.
We next analyze the impact of using prefix trees on effi-cient crowd assignment. An alternative approach described in [23] suggests representing P as a collection of sorted lists of signatures rather than prefix trees. Such a structure is robust in the sense that signatures are sorted and hence nearest neighbor can be found faster than linear search, but has the downside that determining the nearest neighbor and adding a new vector takes O (log n ) time, considering | P | is constant. To characterize the impact of the prefix-tree based locality-sensitive hashing approach, we run CDA both with prefix trees and with sorted lists. The results are shown in Figure 5(c).

The top plot shows the running time and the bottom plot shows the quality of crowds discovered. We see that by using prefix trees, we can discover crowds at speeds several times the speed using sorted lists. As mentioned before, the improved speed efficiency is because of the constant time required to retrieve crowds in case of prefix tree instead of O (log n ) as in case of sorted lists.

The quality of crowds generated varies initially when the number of crowds in the prefix tree is small because of ran-domization involved in determining the nearest neighbor. This variance is overcome as the number of crowds in the prefix tree increases and the mean quality of crowds discov-ered remains almost the same. After sometime, once we have observed sufficient crowds, we observe that the crowds qual-ity is almost same while using both prefix tree and sorted lists.
Finally, we explore the impact of the kind of users on crowd formation. We compare the crowd size distribution, followed by the lifespan distribution of the crowds. Then we plot these two properties towards understanding crowding behaviors in these two datasets.
 The distribution of crowd sizes is shown in Figure 7(a). We see that the Houston dataset tends to have larger crowds in comparison to the Experts dataset. These larger crowds may be attributed to the fact that the Houston dataset has relatively more users in comparison to the Experts dataset, and hence more users talking about a particular event re-sulting in the formation of larger crowds. To understand these dynamics better, we show the lifespan of these crowds in Figure 7(b). The lifespan distribution shows that expert crowds, despite being smaller, are mostly longer lasting than the larger crowds discovered in Houston. Based on further analysis, we find that the experts stream is more sticky  X  that is, crowds in the experts stream added new users over time and decayed more slowly.

We attribute this finding to the crowd formation proper-ties of the Experts dataset, whereby crowds are initiated by users who are popular within a particular domain and hence tend to tweet similar things more often. This shared inter-ests among users forms crowds that discuss chains of events resulting in longer lifespans. While users in the Houston dataset, which is made up of users with relatively varied in-terests, form crowds that last only as long as the event they are discussing is popular. Continuing this avenue of investi-gation, we plot crowd size versus life span in Figure 7(c). If the crowds in the Experts dataset are really sticky, as we ex-pect, this should be observed across all the crowds of differ-ent sizes, i.e, only larger crowds should not have contributed in making the life span distribution in Figure 7(b) appear the way it does. We observe that irrespective of crowd size, expert crowds always seem to have a higher lifespan than Houston crowds. This clearly shows the way users in ex-pert crowds are tweeting and the content of their tweets is making them stick together longer than Houston crowds. In addition to this observation, we also see that the stickiness of the crowds increases with crowd size. This is observed both for the experts and Houston crowds.

We also find that events that last for a long time have more number of crowds that are spread across the event X  X  dura-tion. An example of such a long term event is the revolution in Libya, and crowds related to this appear throughout the experiment duration following a daily pattern based on users activity, as shown in Figure 6.
We have seen how the proposed content-driven crowd dis-covery algorithm can efficiently identify newly-formed com-munities of users from the real-time web. The approach leverages optimizations to locality-sensitive hashing via pre-fix trees, incorporates efficient user profile updating, and identifies key dimensions for supporting crowd detection. In our continuing work we are interested to understand the dy-namics if crowds formation and their evolution. We are also interested in analyzing the impact geography and culture has on crowd formations. This work was supported in part by NSF grant IIS-1149383, DARPA grant N66001-10-1-4044 and a Google Research Award. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not nec-essarily reflect those of the sponsors. [1] Infinite coordinate space. wikipedia: Examples of [2] C. C. Aggarwal. A framework for clustering massive [3] C. C. Aggarwal, J. Han, J. Wang, and P. S. Yu. A [4] J. Allan. Introduction to topic detection and tracking , [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] M. S. Charikar. Similarity estimation techniques from [7] S. Chen, H. Wang, S. Zhou, and P. S. Yu. Stop [8] H. Choi and H. Varian. Predicting the present with [9] D. R. Cutting, D. R. Karger, J. O. Pedersen, and [10] I. Dhillon, Y. Guan, and B. Kulis. A fast kernel-based [11] W. Fan, Y. Koyanagi, K. Asakura, and T. Watanabe. [12] M. Franz, T. Ward, J. S. McCarley, and W.-J. Zhu. [13] A. Gionis, P. Indyk, and R. Motwani. Similarity [14] L. Gong, J. Zeng, and S. Zhang. Text stream [15] P. Indyk and R. Motwani. Approximate nearest [16] K. Y. Kamath and J. Caverlee. Transient crowd [17] C. X. Lin, B. Zhao, Q. Mei, and J. Han. PET: A [18] Y.-B. Liu, J.-R. Cai, J. Yin, and A. W.-C. Fu. [19] C. D. Manning, P. Raghavan, and H. Schtze.
 [20] F. Moerchen, K. Brinker, and C. Neubauer. Any-time [21] M. E. J. Newman. Fast algorithm for detecting [22] S. Petrovi  X c, M. Osborne, and V. Lavrenko. Streaming [23] D. Ravichandran, P. Pantel, and E. Hovy.
 [24] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [25] L. Tang and H. Liu. Community Detection and [26] X. Wang, L. Tang, H. Gao, and H. Liu. Discovering [27] S. Wu, J. M. Hofman, D. J. Watts, and W. A. Mason. [28] Y. Zhu and D. Shasha. Statstream: statistical
