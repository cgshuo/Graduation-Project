 dividual processing modules are unaware of the larger context in which they operate; an integrated system will likely do better.
We describe a first research prototype for integrated CDIP, to enable work on these important research problems (see also [4]). Elsewhere [2] we describe our work on building a testbed collection for CDIP, based on document images from the Legacy Tobacco Documents Library (http://legacy.library.ucsf.edu/).

The prototype currently contains modules that extract and ana-lyze text, signatures, and logos from complex documents, enabling integrated document image retrieval. The system (Fig. 1) con-tains a set of ingestion modules, which process different forms of document image data, importing them into a database schema which includes traditional document indices by keyword and rela-tions between documents and their components (logos, signatures, and named entities). Retrieval operates via SQL queries on this unified database.

Each component in the figure is a separate thread, so that pro-cessing is fully parallelized and pipelined. Image files are served to processing modules dealing with different types of document im-age information. (Future development will add preprocessing for noise removal, skew-correction, orientation determination, and re-gion zoning.) The ABBYY OCR engine (www.abbyy.com) is used to extract text from the document image. This text is fed to the ClearForest (www.clearforest.com) information extraction module, which finds and classifies various named entities and relations. Sig-nition system [3] which matches document signatures to known signatures in a database. Logos are segmented and matched using the DocLib package [1]. These three threaded processing paths are then synchronized, and the data extracted are transformed into a unified database schema for retrieval and analysis. ulated by hand-segmentation and labeling. Figure 3: Prototype results showing signatures associated with the most total dollars (see text).
The rich collection of attributes our system associates with each document (including words, linguistic entities such as names and amounts, logos, and signatures) enables both novel forms of text retrieval, and the evidence combining capabilities of a relational database. While we have, as yet, no quantitative evaluations to re-port, we give examples here of the kinds of capabilities that our prototype currently supports. The mini-corpus used for this con-sists of 800 documents taken from the testbed we are building.
We consider integrated queries that our prototype makes possi-ble for the first time. We apply conjunctive constraints on document image components to a straightforward document ranking based on total query-word frequency in the OCRed document text; in Fig-ure 2 we show document images retrieved for two such queries. The first is the unique document found containing both of the words  X  X ncome X  and  X  X orecast X  as well as the American Tobacco Com-pany logo and a dollar amount (a recognized entity type) greater than $500K. The second example is the top-ranked document for  X  X iltration X  and  X  X fficiency X , that also has the R.J. Reynolds logo and a signature. Note that neither of these documents would have been found just based on their printed text, as neither contains the company name explicitly.

In Figure 3 we show a ranked retrieval results for a document component query which asks for the five signatures with the high-est total of dollar amounts mentioned in documents with each sig-nature. This shows another novel way of integrating useful in-formation extracted from document images which is easily imple-mentable in our framework.
 [1] K. Chen, S. Jaeger, G. Zhu, and D. Doermann. DOCLIB: A [2] D. Lewis, S. Argamon, G. Agam, O. Frieder, D. Grossman, [3] S. N. Srihari, M. K. Kalera, and A. Xu. Offline signature [4] S. Stein, S. Argamon, and O. Frieder. The effect of OCR on
