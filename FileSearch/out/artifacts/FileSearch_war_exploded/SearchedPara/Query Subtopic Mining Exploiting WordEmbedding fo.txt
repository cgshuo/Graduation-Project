 Md Zia Ullah ( According to user search behavior analysis, query is usually unclear, ambiguous, or board [ 10 ]. Issuing the same query, different users may have different search intents, which correspond to different subtopic [ 8 ]. For example, with an ambigu-ous query such as  X  X clipse, X  users may seek different interpretations, including  X  X clipse IDE, X   X  X clipse lunar, X  and  X  X clipse movie. X  With a broad query such as  X  X rogramming languages, X  users may be interested in different subtopic, includ-ing  X  X rogramming languages java, X   X  X rogramming languages python, X  and  X  X ro-gramming languages tutorial. X  However, it is not clear which subtopic of a broad query is actually desirable for a user [ 11 ]. Search engine often fails to capture the diversified search intents of a user if the issued query is ambiguous or broad and results in a list of redundant documents. As these documents may cover a few subtopic or interpretations, the user is usually unsatisfied.
 In this paper, we address the problem of query subtopic mining ,which or disambiguates the search intent of the original query. X  In this regard, our contributions are threefold: (1) some new features based on word embedding, (2) a bipartite graph based ranking for estimating the relevance of the subtopic, and (3) estimating the novelty of the subtopic by combining a mutual informa-tion based similarity and categorical similarity.
 posed subtopic mining approach. Section 3 discusses the overall experiments and results that we obtained. Finally, concluding remarks and some future directions of our work are described in Sect 4 . In this section, we describe our approach to query subtopic mining, which is composed of subtopic extraction, features extraction, and ranking. Given a query, first we extract candidate subtopics from multiple resources. Second, we extract multiple semantic and content-aware features to estimate the relevance of the candidate subtopics, followed by a supervised feature selection and a bipartite graph based ranking. Third, to cover the possible search intents, we introduce a novelty measure to diversify the subtopic. 2.1 Subtopic Extraction Inspired by the work of Santos [ 9 ], our hypothesis is that suggested queries in across search engines hold some intents of the query. For a query, we utilize the suggested queries, provided by the search engines. If a query is matched with the title of a Wikipedia disambiguation page, we extract the different meanings from that page. Then, we aggregate the subtopic by filtering out the candidates, which is the part of the query or exactly similar. 2.2 Features Extraction Let q  X  X  represents a query and S = { s 1 ,s 2 , ....., s didate subtopics extracted in Sect. 2.1 . We extract multiple local and global features, which are broadly organized as word embedding and content-aware features. We propose two semantic features based on locally trained word embed-ding and make use of word2vec 1 model [ 6 ].
 first propose a new feature, the maximum word similarity (MWS) as follows: where t and w are the word vector representations from word2vec model, cor-responding to two words t and w , respectively. The function f cosine similarity between two word vectors. To estimate the global importance of a query with a subtopic, we propose our second feature, the mean vector similarity (MVS) as follows: Among content-aware features, we extract features based on term frequency, including DPH [ 9 ], PL2 [ 9 ], and BM25 [ 9 ]; language modeling, including Kullback-Leibler ( KL )[ 9 ], Query Likelihood with Jelinek-Mercer ( QLM-JM )[ 9 ], Query Likelihood with Dirichlet smoothing ( QLM-DS )[ 9 ], and Term-dependency Markov random field ( MRF )[ 9 ]; lexical, including edit distance, sub-string match including normalized hit count ( NHC ), point-wise mutual information ( PMI ), and word co-occurrence ( WC ); and some query independent features, including aver-age term length ( ATL ), topic cohesiveness ( TC )[ 9 ], and subtopic length ( SL ). 2.3 Subtopic Ranking To remove noisy and redundant features, we normalize the features using Min-Max and employ elastic-net regularized regression.
 Bipartite Graph Based Ranking. Many real applications can be modeled as a bipartite graph, such as Entities and Co-List [ 1 ] in a Web page. We hypothesize that a relevant subtopic should be ranked at the higher position by multiple effec-tive features and intuitively, an effective feature should be ranked at higher posi-tion by multiple relevant subtopics. On this intuition, we represent a set of fea-tures
F = { f as a bipartite graph, G =( F X  X  , E ), and introduce weight propagations from both sides. The weight w i,j =1 / log 2 ( rank ( L f i ,s a feature f i and a subtopic s j , where rank( L f i ,s j subtopic s j in the ranked list L f i for the feature f i Let M be a bi-adjacency matrix of G , W 1 = D  X  1 F M ,and where D F and D S are the row-diagonal and the column-diagonal matrices of The weight propagations from the set S to the set F and vice versa are represented as follows: where 0 &lt; X  1 , X  2 &lt; 1, F 0 and S 0 are the initial weight vectors, F the weight vectors after the k -th iterations.
 From the iterative solution of the Eq. ( 3 ), we have Given  X  1 ,  X  2 , W 1 , W 2 , F 0 ,and S 0 , we estimate the scores S applying Eq. ( 4 ). These scores S  X  are considered as the relevance scores, rel(q, which is utilized in diversification.
 Subtopic Diversification. To select the maximum relevant and the minimum redundant subtopic, we diversify the subtopic using the MMR [ 2 ] framework, which can be defined as follows: where  X   X  [0 , 1], rel ( q, s i ) is the relevance score, and novelty ( s score of the subtopic s i . R is the ranked list of subtopic retrieved by Eq. ( 4 ). C is the collection of subtopic that have already been selected at the i -th iteration and initially empty.
 We hypothesize that if two subtopics represent the similar meaning, they may belong to the similar categories and retrieve similar kinds of documents from a search engine. Therefore, we propose to estimate the novelty of a subtopic by combining the contextual and categorical similarities as follows: where JSD ( s i ,s ) is estimated through the Jensen-Shannon divergence of the word probability distributions of the top-k documents refer to the subtopics s and s . X is the set of clusters obtained by applying the frequent phrase based soft clustering on the candidate subtopics, | x | is the number of subtopics belong to the cluster x ,and[( s i ,s )  X  x ] = 1 if true, zero, otherwise. In this section, we evaluate our proposed method ( W2V-BGR-Nov ) and com-pare the performance with previous methods, including [ 3 , 4 , 7 ], the diversifica-tion methods MMR [ 2 ], XQuAD [ 9 ], and the baseline, MergeBGY, merging of query completions from Bing, Google, and Yahoo. For relevance estimation, lin-ear ranking is used in the MergeBGY, whereas Eq. ( 4 ) is used for MMR and XQuAD. Moreover, the cluster label of the frequent phrase based soft clustering of candidates is considered as the sub-topics for XQuAD. For estimating novelty, cosine similarity is utilized for MergeBGY, MMR, and XQuAD. We estimate evaluation metrics, including I-rec@10, D-nDCG@10, and D#-nDCG@10; and use the two-tailed paired t-test for statistical significance testing (p &lt; 0.05). 3.1 Dataset The INTENT-2 and IMINE-2 [ 12 ] test collections include 50 and 100 topics, respectively. As resources, query completions from Bing, Google, and Yahoo were collected and included in the datasets. To estimate the features, including Eqs. ( 1 ) and ( 2 ), we retrieved the top-1000 documents from the clueweb12-b13 corpus based on language model for each topic and locally trained word2vec . The parameters in the word2vec tool are Skip-gram architecture, window width of 10, dimensionality of 200, and the sampling threshold of 10 3.2 Important Features and Parameter Tuning We trained elastic-net on INTENT-2 dataset, however, we employed on IMINE-2 dataset, and vice versa. We extracted in total 27 features and the selected features were as follows: MWS , MVS , DPH , QLM-JM , MRF , SSM , TSO , NHC , WC ,and ATL . It turned out that our proposed features MWS and MVS are important and were chosen during feature selection. Through empirical evalu-ation, we found the optimal insensitive range of values of  X  as [0.6  X  0.8] and [0.4  X  0.6], respectively. We found the optimal value of  X  in Eq. ( 5 )as0 . 85, which reflects that MMR rewards relevance than diversity in mining subtopic. 3.3 Experimental Results The comparative performances are reported in Tables 1 and 2 for INTENT-2 and IMINE-2 topics. The results show that overall W2V-BGR-NOV is the best. In terms of diversity (i.e. I-rec@10), W2V-BGR-NOV significantly outperforms all baselines except [ 3 ] for INTENT-2 topics. Though previous methods utilize mul-tiple resources which often cause noisy subtopics, however, our proposed estima-tion of subtopic novelty in Eq. ( 6 ) eliminates redundant subtopics and benefits more diverse subtopics. In terms of relevance (i.e. D-nDCG@10), W2V-BGR-NOV outperforms all baselines except HULTECH-Q-E-1Q for IMINE-2 topics. Our proposed word embedding based features, followed by bipartite graph based ranking capture better semantics to estimate the relevance of the subtopics. In terms of D#-nDCG@10, which is a combination of I-rec@10 (0.5) and D-nDCG@10 (0.5), W2V-BGR-NOV significantly outperforms all the baselines. The overall result demonstrates that our proposed W2V-BGR-NOV is effective in query subtopic diversification. In this paper, we proposed mining and ranking query subtopic by exploiting word embedding and short-text similarity measure. We introduced new features based on word embedding and bipartite graph based ranking to estimate the relevance of the subtopic. To diversify the subtopic covering multiple intents, we proposed to estimate the novelty of a subtopic by combining the contextual and categorical similarities. Experimental results demonstrate that our proposed approach outperforms the baseline and known previous methods. In the future, we will evaluate the effectiveness of the mined subtopics by employing search diversification.

