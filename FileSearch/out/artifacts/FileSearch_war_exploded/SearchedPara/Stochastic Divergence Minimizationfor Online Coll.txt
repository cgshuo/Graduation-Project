 The collapsed variational Bayes zero (CVB0) inference is a vari-ational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vec-tor must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K , the CVB0 inference requires O ( N K ) memory. A stochas-tic approximation of the CVB0 (SCVB0) inference can reduce O ( N K ) to O ( V K ) , where V denotes the vocabulary size. We re-formulate the existing SCVB0 inference by using the stochastic di-vergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 infer-ence in four datasets.
 G.3 [ Probability and Statistics ]: Nonparametric statistics Algorithms
Latent Dirichlet allocation (LDA) [1] has been one of the most studied probabilistic latent variable models over the past decade. Originally, the variational Bayes (VB) inference was used for learn-ing LDA [1]. The collapsed Gibbs (CG) sampler [2] was then pro-posed for LDA.

Teh et al. [3, 4] developed the collapsed variational Bayes (CVB) inference as an alternative (deterministic) inference to solve some of the problems of the VB inference and CG sampler. The CVB inference is a variational inference improved by marginalizing out parameters the same as with the CG sampler [5, 2]. Sung et al.[6] c  X  Update cost / mini-batch -O ( V K ) -generalized the CVB inference for conjugate-exponential family models..

Since the CVB inference for LDA requires intractable integrals, the original work involved a second-order Taylor expansion. Asun-cion et al. [7] proposed another approximation that uses only the zero-order information, called the CVB0 inference. The CVB0 in-ference for LDA converges more quickly than does the CG sam-pler since it is deterministic. Furthermore, their empirical results suggest that the CVB0 inference learns models that are as good or better (predictively) than those learned using the CG sampler. We [8] simplified the CVB0 inference for the hierarchical Dirichlet process (HDP) [5] and argued that the CVB0 inference performed well in terms of not only perplexity but also a link prediction of a social network and a nearest neighbor search of documents. A drawback of the CVB0 inference is the memory requirements. A probability vector must be maintained for latent topics for every token in a corpus. We denote w d,i as the i -th word in document d and z d,i as a latent variable corresponding to w d,i . The notation q ( z d,i = k ) is a probability that latent variable z d,i The CVB0 inference must maintain topic probability q ( z d,i for every token w d,i in the corpus because it has to subtract the previous value of q ( z d,i = k ) from expected counts when updating q ( z d,i = k ) . If the total number of tokens is N and the number of topics is K , the CVB0 inference requires O ( N K ) memory. The purpose of this paper is to reduce O ( N K ) to O ( V K ) where V denotes the vocabulary size.

Foulds et al.[9] developed a stochastic approximation of CVB0 (SCVB0) inference for LDA to solve the problem of the CVB0 in-ference. They ignored the subtraction of q ( z j,i = k ) and applied a Robbins-Monro type stochastic approximation [10] for calculating the expected counts. Therefore, their SCVB0 inference does not need to maintain q ( z d,i ) for all tokens. They showed that their SCVB0 inference outperformed the stochastic variational Bayes (SVB) inference proposed by Hoffman et al. [11]. However, there are three concerns with this SCVB0 inference. (1) It cannot be applied to LDA with the asymmetric Dirich-let distribution in which we have to estimate the parameters of the Dirichlet distribution. The derivation of the SCVB0 inference by Foulds et al.[9] is based on the fact that the CVB0 inference by ignoring the subtraction of q ( z j,i = k ) is identical to the maxi-mum a posterior (MAP) inference with smoothing by incrementing the Dirichlet parameters by one. That is, since we have to manu-ally adjust the parameters, we cannot estimate the parameters in the mathematically right way.

It is well known that it is important to use the asymmet-ric Dirichlet distribution over document-topic distribution. Wal-lach et al. [12] found that the asymmetric Dirichlet distribution over document-topic distributions has substantial advantages over the symmetric one. The asymmetric Dirichlet distribution over document-topic distribution can learn meaningful topics even when no stop words are removed. (2) We have a question regarding why the SCVB0 inference out-performed the SVB inference because the SCVB0 inference actu-ally approximates the MAP inference rather than the CVB0 infer-ence. When the number of data poins is large, Bayesian inference, including the VB inference, can be approximated by the MAP in-ference. However, this is not the answer to the question since the SVB0 inference works better than the SVB inference. (3) The computational cost of one update of the SCVB0 is O ( V K ) , which is too expensive when we update parameters for every document. Therefore, using minibatch updates was sug-gested. However, in a large-scale dataset, the number of minibatchs is also large; thus, the cost O ( V K ) per minibatch can be also ex-pensive when V is too large.
 For these problems, we propose an SCVB0 inference by using the stochastic divergence minimization (SDM) algorithm. We found that the SCVB0 inference proposed by Foulds et al.[9] is a good approximation of the SDM algorithm.

Recall that the CVB0 inference was derived as the Taylor ap-proximation method of the CVB inference in the first place; thus, its derivation was not mathematically formulated as an optimiza-tion problem. To solve this problem, we [13] derived the CVB0 inference as the  X  -divergence minimization problem. This refor-mulation showed that the CVB0 inference is not just an approxi-mation method of the CVB inference and clarified the properties of the CVB0 inference for LDA, e.g., the CVB0 inference is not af-fected by the zero-forcing effect which induces the mode-seeking property of the statistical inference [14].

By using the reformulation of the CVB0 inference as an opti-mization problem, we can apply the stochastic approximation to this optimization problem. That is, the proposed SDM algorithm is a stochastic approximation of the divergence minimization refor-mulation of the CVB0 inference. There are four contributions in this work. (1) We modify our previously proposed divergence minimization formulation of the CVB0 inference [13], which enables us to un-derstand a novel property of the CVB0 inference and provide an estimation method of the asymmetric Dirichlet distribution of LDA in the CVB0 inference. (2) The proposed SCVB0 inference more accurately approxi-mate the CVB0 inference, which can lead to a higher predictive performance of LDA. (3) The proposed SCVB0 inference can be analyzed in terms of Martingale convergence theory. (4) Since the SDM algorithm is a word-wise optimization, we do not need minibatchs whose computational cost is O ( V K ) .
On contribution 1: We [8] proposed a simplified CVB0 infer-ence with the parameter estimation of the asymmetric Dirichlet dis-tribution, which is a truncation approximation of the hierarchical Dirichlet process (HDP) [5]. This approach is based on the objec-tive function of the CVB inference, not that of the CVB X 0 X  infer-ence. However, we [13] previously found that the CVB0 inference is not just an approximation method of the CVB inference.
Our SCVB0 inference is not based on the CVB inference but on the divergence minimization framework. Therefore, we need an estimation method for the parameters of the asymmetric Dirichlet distribution in terms of the divergence minimization framework for mathematical coherent.

To solve this problem, we have to modify the original divergence minimization formulation of the CVB0 inference that we previ-ously proposed. Interestingly, this novel reformulation reveals that the CVB0 inference is related to the leave-one-out (LOO) likeli-hood estimation, which is also related to the LOO peplexity. We use this property to estimate the parameters of the asymmetric Dirichlet distribution.

On contribution 3: In the stochastic approximation literature, the update is usually interpolated by a step size in the space of pa-rameters. However, since the CVB0 inference integrates out model parameters, the SCVB0 update needs to be interpolated by a step cannot directly use the logic of the original stochastic approxima-tion to explain the convergence of the proposed SCVB0 inference. Therefore, its convergence analysis is a challenging theme. In this work, by using the SDM reformulation, we can analyze the con-vergence of the proposed SCVB0 inference in terms of Martingale convergence theory.

Table 1 summarizes this work. The remainder of this paper is or-ganized as follows. In Sections 3, 4, 5, and 6, we give an overview of related work such as LDA, CVB0 and SCVB0. In Section 7, we propose the SDM algorithm to derive the SCVB0 inference, and analyze it in Section 8. In Section 9, we evaluate the SDM-based SCVB0 inference.
The notations Dir (  X  ) and Multi (  X  ) denote the Dirichlet distribu-tion and the multinomial distribution, respectively. The notation D denotes the total number of documents, N the total number of words (tokens), V the vocabulary size, K the total number of top-ics, d a document index, i.e., d = 1 ,  X  X  X  , D , v a vocabulary index, i.e., v = 1 ,  X  X  X  , V , n d the number of words in documents d , n the number of times word v appears in whole documents, n d,k number of times topic k appears in document d , n d,k,v the number of times word v generated from topic k in document d , and n the number of times word v appears in topic k .

The bold face letters denote sets of the corresponding variables, e.g., w d,i denotes the i -th word in document d , w d = { and w = { w d } D d =1 . The notation z d,i denotes the assigned topic at the i -th word in document d ,  X  d,k the probability of topic k ap-pearing in document d ,  X  k,v the probability of word v appearing function, and ( x ) the digamma function.
In LDA, the document-topic distribution  X  d and topic-word dis-tribution  X  k are generated by  X  d  X  Dir (  X  ) ( d = 1 ,  X  X  X  , D ) ,  X  k  X  Dir (  X  ) ( k = 1 , where  X  = (  X  1 ,  X  X  X  ,  X  K ) is a K -dimensional vector and  X  = (  X  1 ,  X  X  X  ,  X  V ) is a V -dimensional vector. For each document d , generate the i -th topic z d,i and word w d,i :
Wallach et al. [12] explored the effects of choosing  X  and  X  in LDA. They found that using asymmetric  X  and symmetric  X  = (  X ,  X  X  X  ,  X  ) results in better performance. In particular, asym-metric  X  is useful for robustness against stop words. It can learn meaningful topics even when no stop words are removed. There-fore, we use symmetric  X  = (  X ,  X  X  X  ,  X  ) in this paper.
Teh et al. [4] proposed the CVB inference for LDA. They marginalize over  X  and  X  in the CVB inference the same as with the CG sampler. By integrating out  X  d and  X  k , a join distribution over w and z is given by The CVB inference approximates the posterior distribution p ( z | w ,  X  ,  X  ) by using the variational posterior distribution whose update equations are given by where  X  \ d, i  X  denotes subtracting the counts related to w z
The problem is that the expectation in the above update cannot be analytically calculated. Therefore, we need an approximation for this expectation.

Asuncion et al. [7, 15] showed the usefulness of a simple ap-proximation that uses only zero-order information of the Taylor ap-proximation, called the CVB0 inference, in LDA. An update with the CVB0 inference is given by We [8] argued that the CVB0 inference performed well in terms of not only perplexity but also a link prediction of a social network and a nearest neighbor search of documents.
A disadvantage of the CVB0 inference is the memory require-ments. The q ( z d,i = k ) must be maintained for every token in the corpus because it has to subtract the previous value of q ( z from expected counts when updating q ( z j,i = k ) . Foulds et al.[9] proposed a SCVB0 inference for LDA. They showed that the SCVB0 inference outperformed the SVB inference proposed by Hoffman et al. [11].

They ignored the subtraction of q ( z j,i = k ) and applied a stochastic approximation for calculating the expected counts E where N  X  d,k , N  X  k,v , and N Z k are expected counts estimated by a stochastic approximation, i.e., N  X  d,k  X  E [ n d,k ] , N and N Z k  X  E [ n k,  X  ] .

These calculations of the approximated counts are performed as follows. We denote q d,i = ( q ( z d,i = 1) , q ( z 2) ,  X  X  X  , q ( z d,i = K )) . Let Q d,i be a K -by-V matrix with the v -th column being q d,i and with zeros in the other entries when w d,i = v , N  X  be a K -by-V matrix whose ( k, v ) -th element is N and N Z be a K -dimensional vector whose k -th element is N They used one step size,  X   X  t , for N  X  and N Z , and another step size  X   X  t for N  X  d .
 For each token w d,i , after using (7), update where t is the update counts of N  X  d and is not equal to i because Eqs. (7) and (8) are performed throughout a document in a small number of times as a burn-in period before updating N  X  and N Since it is too expensive to update the N  X  after every document, Foulds et al. suggested the use of minibatch updates. Let B the t -th subset of whole documents and | B t | be the total number of words (tokens) in B t . After processing B t by using Eqs.(7) and (8), update
In this section, we review a statistical inference based on the  X  -divergence minimization and reformulate the divergence min-imization formulation of the CVB0 inference that we previously proposed [13].
The  X  -divergence is a generalization of the Kullback-Leibler (KL) divergence [16, 17, 18], indexed by  X   X  (  X  X  X  ,  X  ) . In this paper, we define  X  -divergence used by Minka [14] as
D  X  [ p || q ] = If p = q ,  X  -di vergence is zero.

The important property of this definition in this work is that p ( x ) and q ( x ) do not need to be normalized. For example,  X  = 1 corre-sponds to the unnormalized KL divergence (see Sato et al. [13]).
Let our task be to approximate a complex probabilistic distribu-i =1 q ( x i ) . A basic approach to obtaining q ( x ) is to minimize the divergence. We focus on the  X  -divergence minimization, i.e.,
This minimization is intractable in many cases. The alternative way is the local  X  -divergence minimization given by This means that we replace p ( x ) = p ( x i | x \ i ) p ( x p ( x i | x \ i ) q ( x \ i ) . In other words, we replace q ( x When  X  = 1 , i.e., KL divergence, this local divergence minimiza-tion is equal to the expectation propagation algorithm.
We [13] showed that the CVB0 inference is formulated by the local  X  -divergence minimization, which means that the CVB0 in-ference is a kind of power (alpha) expectation propagation. The divergence minimization in LDA is formulated as Since this minimization is intractable, we [13] solved the local di-vergence minimization problem instead of this problem, which re-covers the CVB0 inference. However, this formulation cannot give the framework to estimate parameter  X  of the asymmetric Dirichlet distribution Dir (  X  d |  X  ) . Therefore, we modify this formulation.
We consider estimating the variational distribution over w and z given by q ( w , z ) = q ( z | w ) q ( w ) = We estimate q ( w , z ) by using the  X  -divergence minimization: min = min Since this minimization is intractable, we use the following local divergence minimization.

We parameterize q ( w d,i , z d,i ) with a \ d,i d,k , b \
Note that
W e sometimes use the notations q ( w d,i , z d,i | b q ( w , z | b ) to explicitly emphasize that q ( w d,i , z
We define
By replacing b \ d,i k,v with n \ d,i k,v +  X  , we define q
We now focus on the local divergence minimization for b \ given by min where we replace p ( w \ d,i |  X  ,  X  ) with empirical word distribu-tion p emp ( w \ d,i ) because the expectation over p ( w tractable. The empirical word distribution is formulated by the product of the delta function  X  ( w d,i = v ) when the i -th word, w in document d is actually word v .

In fact, the local divergence minimization (21) recovers the result of Sato et al.[13]. Therefore, the solution of this minimization is ), Similarly, are closed form solutions of min min
By substituting optimized a \ d,i d,k , b \ d,i k,v , and c v, z d,i = k ) of Eq. (26), we have Note that we have q ( w d,i = v ) = Therefore, we recover Eq. (6) of the CVB0 inference by calculating q ( z d,i = k | w d,i = v ) = q ( w d,i = v, z d,i = k ) in Eq. (26).
In this section, we describe the relationship between our novel reformulation of the CVB0 inference and the leave-one-out like-lihood and perplexity. This relationship provides us the pa-rameter estimation method of asymmetric Dirichlet Distribution, Dir (  X  d |  X  ) .
Note that we estimate variational joint distribution q ( w , z ) = q ( z | w ) q ( w ) = and q ( w ) indicates the approximated marginal likelihood. The marginal likelihood may be referred to as the evidence or data evi-dence.

From Eq. (27), since q ( w ) depends on  X  and  X  , we expressly denote q ( w ) as q ( w |  X  ,  X  ) . Since we want to maximize the data evidence, it is natural to estimate  X  and  X  by
It is well known that the maximization of the true marginal likelihood, sometimes called the evidence, of the observed data, estimation. Therefore, our approach is a variational approximation of the empirical Bayes estimation. This is typically used for per-forming model selection. The general idea is that a higher marginal likelihood for a given model indicates a better fit of the data.
We now analyze q ( w |  X  ,  X  ) in more detail. The LOO prediction of w d,i is given by = = The leave-one-out log-likelihood estimation for  X  and  X  is given by The LOO likelihood cross-validation is often used for model se-lections [19, 20, 21, 22]. Optimization (29) can be considered an approximation of the LOO likelihood estimation Eq. (31). More interestingly, q ( w |  X  ,  X  ) is related to the LOO perplexity. We calculate the perplexity for held-out test words w test where N test is the total number of test words. From Eq. (27), we find that indicates the LOO perplexity. This pseudo perplexity can approx-imately represent the held-out perplexity. Therefore, minimizing this pseudo perplexity is expected to lead to a lower perplexity of held-out data.

We show the relationship between LOO and Test perplexities for the NIPS corpus in Figs. 1 (1) and (3) and NewYork Times corpus in Figs. 1 (2) and (4). The NIPS corpus includes randomly chosen D = 1 , 500 documents with vocabulary size V = 12 , 245 . The NewYork Times corpus includes randomly chosen D = 1 , 500 documents with vocabulary size of V = 44 , 520 . Stop and low frequency words were eliminated. These were created from the corpora of Frank and Asuncion [23]. We ran the CVB0 inference 3: for d = 1 ; ; D //We randomly order documents. 8: end for 9: end for 10: for word-type v in document d 13: end for 15: Increment t = t + 1 . 16: end for of LDA with the symmetric Dirichlet prior. The number of itera-tions was 100 . We randomly split the words in each document into training words ( 80% ) and test words ( 20% ). Test perplexity was calculated using 20% of the test words, and LOO perplexity was calculated using 80% of the training words (lower perplexity is bet-ter). We tried several hyperparameters ( K  X  X  50 , 100 , 150 { and five random initializations. Each point in Figs. 1 (1) and (2) in-dicates performance with a certain hyperparameter setting. Figures 1 (3) and (4) show the results of the best LOO perplexity for all hyperparameter setting in terms of the number of iterations. These results suggested that the CVB0 inference minimizes the LOO per-plexity, and the LOO perplexity highly correlates with the test per-plexity. Therefore, we can use the LOO perplexity as a criterion of model selection.
We proposed a stochastic divergence minimization (SDM) framework for deriving the SCVB0 inference. In the divergence minimization, a \ d,i d,k can be optimized in a document and c be obtained as c \ d,i k = focus on the SDM for b \ d,i k,v .
We start with stochastically approximating the calculation of where w d,i = v . This calculation can be rewritten by the fixed point iteration where  X  t is a stepsize.
 We also rewrite
E We replace these statistics with where ( d  X  , i  X  )  X  = ( d, i ) is a random sample with w
Therefore, we approximate Eq. (35) by
It is worth noting that we do not iterate update Eq. (38) for each w d,i many times. Update Eq. (38) is actually a word-type-wise update. Moreover, we process a document in an online situation. Thus, we can use a previously processed word for q ( z d  X  in the previously processed document. That is, update Eq. (38) can be reformulated as follows.

When we process word w d  X  ,i  X  = v , we can update b k,v next word with w d,i = v as follows. where t ( v ) is an update count for word v , i.e., it indicates how many times word v previously appeared until the next word w approximation of E [ n \ d,i k,v ] +  X  .

Note that the divergence minimization is not a document-wise optimization but a word-wise optimization, i.e., a coordinate-wise but update it only if word v appears in the document. Therefore, we do not need a minibatch operation. Moreover, by using b calculated with the previously processed documents, we can paral-setting.

The remaining problem is to show that update Eq. (38) min-imizes the divergence in Eq. (21). This problem is not obvi-E We present the theoretical analysis of update Eq. (38) in terms of Martingale convergence theory in Sec. 8.
Although this update should be performed for each distinct token in each document, we actually perform this update for each word-type in each document by approximating E [ n d,k  X   X  We proposed the stochastic approximation of Optimization (29). In this section, we focus on estimating  X  . We explain the setting of value from the result of Asuncion et al. [7].

From LOO perplexity (33), we estimate  X  by where N is the total number of words. This formulation is useful because the optimal solution is the same as Eq. (29) and the scale of the gradient of the objective function does not depend on N . We can use the stochastic gradient method by where  X  k log q ( w d,i = v
In this work, we use the construction of  X  given by This is called a truncated stick-breaking process (TSBP) construc-tion.

Ishwaran and James [24] give a bound for the error introduced by truncating the SBP. They argued that truncating the number of mixture components to a moderate level is sufficient to successfully approximate the SBP. They give a bound on the truncation error in the order of exp { X  ( K  X  1) / X  0 } . Therefore, we expect that we can estimate the effective number of topics K + if we set K &gt; K
The modeling by the asymmetric Dirichlet distribution is useful for robustness against stop words. Wallach et al. [12] found that the asymmetric  X  has substantial advantages over the symmetric one which means fixed  X  k =  X  0 /K . The asymmetric  X  can learn meaningful topics even when no stop words are removed.
We can use the stochastic gradient method for estimating  X  given by  X  0 =  X   X   X  0 log q ( w d,i = v |  X  ) =  X  k =  X   X   X  k log q ( w d,i = v |  X  ) = This approach is similar to that of Liang et al. [25], in which they use the point estimation for the TSBP. The main difference with their approach and ours is that they use the VB inference; thus, they can use the variational lower-bound as an objective function, i.e., they can use the gradient method to maximize the variational lower-bound in terms of  X  . However, we cannot use this approach because the CVB0 inference is not directly related to the variational lower-bound.
We analyze the convergence of the stochastic approximation of the divergence minimization introduced in Sec.7.1.

For mathematical convenience, we denote y d,i complete data, i.e., Again, we sometimes use the notation q ( y d,i | b \ q ( w d,i , z d,i | b \ d,i ) to explicitly emphasize that q ( w Also, for simplicity, we define, D 1 ( b That is, Optimization (21) is formulated as min We have the convergence results given by number and satisfies update Eq. (38) (or Eq. (39) )) satisfies the following hold with probability one: (1) The sequence D 1 ( b \ d,i ( t ) ) converges. (2) We have (3) Every limit point of b \ d,i ( t ) is a stationary point of
P R OOF . The proof is provided in Appendix 1 . In this section, we compared the proposed algorithm with the SCVB0 inference proposed by Foulds et al., [9] and the SVB infer-ence by Hoffman et al. [11]. We also used the Bayesian optimiza-tion framework [26, 27] for tuning hyperparamters of the original SCVB0 and SVB inferences. We explain experimental settings for the SVB, original and SDM-based SCVB0 inference.
 Setup of SVB: SVB denotes the SVB inference proposed by Hoffman et al. [11]. Snoek et al. [28] used Bayesian optimization for tuning the step size of SVB, which we call SVB-BO. We used their framework as follows.
 We defined the form of the step size as  X  t = a/ ( b + t ) gradient method of  X  , with which we formulate  X  as the stick-breaking construction and the objective function is the variational lower-bound. We assumed that a = 1 , b  X  X  1 , 10 , 100 , 1000  X  and a  X  { 0 . 0001 , 0 . 001 , 0 . 01 , 0 . 1 , 1 } , b  X  { the initialization of  X  k  X  X  0 . 01 , 0 . 05 , 0 . 1 , 0 . 2 , 0 . 5
Setup of SCVB0: SCVB0 denotes the original SCVB0 infer-ence proposed by Foulds et al. [9] that uses the setting in which and the minibatch size is 100 documents.
 We used Bayesian optimization for tuning the hyperparamters of SCVB0, denoted as SCVB0-BO. We assumed that the form of the step size is  X  t = a/ ( b + t )  X  , a = 1 , b  X  { 1 , 10 , 100 , 1000 and  X   X  { 0 . 51 , 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 , 1 } for  X  {  X   X  X  0 . 01 , 0 . 05 , 0 . 1 , 0 . 2 , 0 . 5 } .
Longer version: http://www.r.dl.itc.u-tokyo.ac. jp/~sato/SIGKDD2015_scvb0_long.pdf W ikipedia1M 0.01 0.01 1 / (1 + t ) 0 . 6 1 / (1 + t ) 0 . 7
Setup of SDM: SDM-S and SDM denote the proposed SDM-based SCVB0 inference with symmetric and asymmetric Dirichlet priors. We used simple forms of stepsize  X  t ( v ) = 1 / (1 + t ( v )) S, we fixed  X  k = 0 . 01 and  X  = 0 . 01 . For SDM, we conducted the estimation of  X  after processing 1,000 documents because the estimation of the first set of documents was too noisy. The initial-ization of  X  k was 0 . 01 and we set  X  = 0 . 01 from the result of the CVB0 inference given by Asuncion et al. [7]. Note that we did not use Bayesian optimization for SDM and we show that these simple settings of SDM outperformed fully tuned SCVBO-BO and SVB-BO.

Common Setup: We used the minibatch size of 1000 documents for SVB-BO, SCVB0-BO, and SDM because the computational complexity of the minibatch of the SVB and SCVB0 inferences is O ( KV ) and minibatch size 100 induces a slow running time. The number of iterations in the burn-in period was L b = 5 . We set the number of topics to K = 1000 . We used four datasets:  X  X ubmed corpus (Pubmed1M and Pubmed5M), X   X  X ikipedia corpus (Wikipedia) X , and  X  X BLP ab-stract corpus (DBLP). X  The Pubmed corpus was created by Frank and Asuncion [23]. We randomly chose one million (1M) docu-ments for Pubmed1M and 5M for Pubmed5M.

We chose documents that include more than 50 words. Stop and low frequency words were eliminated. In the Wikipedia corpus, we used the first 300 words for each document and randomly chose 1M English documents because we need a large amount of time for SVB-BO and SCVB0-BO. The basic information of these datasets is summarized in Table 2.

We used test set perplexity as the evaluation metric to compare performances. We calculated the perplexity as follows. For each dataset, we used 2000 documents as a test set D test . For testing, we split words into two parts, w d = ( w d, 1 ; w d, 2 )  X  D used the first part w d, 1 ( 80% of the words) to estimate the topic statistics of document d and computed the perplexity of the second part w d, 2 ( 20% of the words) conditioned on the first part and the training data.
We first analyzed the effect of Bayesian optimization for hy-perparameter tuning, Figure 2 shows the perplexities with respect to the number of trials of Bayesian optimization in the DBLP, Wikipedia, Pubmed1M datasets, respectively. For speeding up Bayesian optimization, we used only one random initialization, i.e., did not use Pubmed5M for Bayesian optimization because we need a large amount of time for SVB-BO and SCVB0-BO.

As shown in Fig. 2, SCVB0-BO outperformed SCVB0 tuned by experts [9]. Interestingly, SVB-BO also outperformed SCVB0. This means that we need to tune hyperparameters when we compare algorithms and Bayesian optimization is useful. Since SCVB0-BO outperformed SVB-BO, SCVB0 is considered to be a better inference even where we tune hyperparameters.
Figures 3 shows the perplexities with respect to the proportion of the number of training documents in the DBLP, Wikipedia, Pubmed1M and Pubmed5M datasets, respectively. We show the results of SCVB0-BO and SDM because we found that SCVB0-BO outperformed SCVB0 and SVB-BO. All results are averaged values with error bars from five experimental runs, i.e., five seeds, rameters tuned with Pubmed1M to SCVB0-BO in Pubmed5M.

SDM outperformed SCVB0-BO for each dataset and the per-plexity of SDM was much lower than that of SCVB0-BO. For ex-ample, as shown in Fig. 3 (a) DBLP, even SDM with 10% training documents was competitive with SCVB0-BO with 100% training documents. In Pubmed datasets shown in Fig. 3 (c) and (d), SDM with 30% training documents was competitive with SCVB0-BO with 100% training documents . In Wikipedia dataset shown in Fig. 3 (b), we needed 60% training documents for SDM to outper-form SCVB0-BO with 100% training documents.
 These results mean that SDM more accurately approximates the CVB0 inference. Moreover, since we use a simple hyperparameter setting for SDM, it is more easy to use SDM than SCVB0s. We may improve the results of SDM by using Bayesian optimization but it is important that SDM has better results even if we do not use hyperparameter tuning.

Table 4 shows that the running time of SDM is faster than that of SCVB0-BO excluding the DBLP corpus. Since the vocabulary size of DBLP is smaller than those of over datasets, the computa-tional complexity of a minibatch was not matter. Note that we took additional processing time for estimating  X  in SDM. Table 4 in-dicates that SDM showed an advantage in terms of computational cost when the vocabulary size was large, such as Wikipedia corpus.
We proposed a stochastic approximation of the CVB0 inference for LDA by using the stochastic divergence minimization. We an-alyzed the proposed SCVB0 inference in terms of Martingale con-vergence theory. Its predictive performance is better than that of the existing SCVB0 inference. We also reveal the property of the CVB0 inference in terms of the leave-one-out likelihood or per-plexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. For future work, we will apply recent advances in the variational inference to our approximated CVB0 inference such as the sparse update combined with the sampling method proposed by [29, 30, 31] and the split-merge update pro-posed by [32]. trials where each perplexity is minimum value in past trials. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet [2] T. L. Griffiths and M. Steyvers. Finding scientific topics. [3] Y. W. Teh, D. Newman, and M. Welling. A Collapsed [4] Y. W. Teh, K. Kurihara, and M. Welling. Collapsed [5] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [6] J. Sung, Z. Ghahramani, and S.-Y. Bang. Latent-Space [7] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. On [8] I. Sato, K. Kurihara, and H. Nakagawa. Practical collapsed [9] J. Foulds, L. Boyles, C. Dubois, P. Smyth, and M. Welling. [10] H.Robbins and S.Monro. A stochastic approximation [11] M. D. Hoffman, D. M. Blei, and F. R. Bach. Online Learning [12] H. Wallach, D. Mimno, and A. McCallum. Rethinking LDA: [13] I. Sato and H. Nakagawa. Rethinking Collapsed Variational [14] T. Minka. Divergence Measures and Message Passing. [15] A. Asuncion. Approximate Mean Field for Dirichlet-Based [16] S. Amari. Dif ferential-Geometrical Methods in Statistic. [17] M. Trottini and F. Spezzaferri. A generalized predictive [18] H. Zhu and R. Rohwer. Information Geometric [19] M. Stone. Cross-validatory Choice and Assessment of [20] R. P. W. Duin. On the Choice of Smoothing Parameters for [21] C. Stone. An Asymptotically Optimal Window Selection [22] B. Silverman. A Fast and Efficient Cross-validation Method [23] A. Frank and A. Asuncion. UCI Machine Learning [24] H. Ishwaran and L. F. James. Gibbs Sampling Methods for [25] P. Liang, S. Petrov, M. Jordan, and D. Klein. The Infinite [26] H. J. Kushner. A new method of locating the maximum point [27] J. Mo  X  ckus, V. Tiesis, and A. Zilinskas. The Application of [28] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian [29] I. Porteous, D. Newman, A. Ihler, A. Asuncion, P. Smyth, [30] L. Yao, D. Mimno, and A. McCallum. Efficient methods for [31] D. M. Mimno, M. D. Hoffman, and D. M. Blei. Sparse [32] M. Bryant and E. Sudderth. Truly Nonparametric Online
