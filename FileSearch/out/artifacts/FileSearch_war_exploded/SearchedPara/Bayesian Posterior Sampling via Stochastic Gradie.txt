 Dept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA Anoop Korattikara AKORATTI @ ICS . UCI . EDU Dept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA Dept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA When a dataset has a billion data-cases (as is not uncom-mon these days) MCMC algorithms will not even have gen-erated a single (burn-in) sample when a clever learning al-gorithm based on stochastic gradients may already be mak-ing fairly good predictions. In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of  X  X umber of bits learned per unit of computation X , an al-gorithm as simple as stochastic gradient descent is almost optimally efficient. We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimization literature.
 A first attempt in this direction was proposed by Welling and Teh (2011) where the authors show that (uncorrected) Langevin dynamics with stochastic gradients (SGLD) will sample from the correct posterior distribution when the stepsizes are annealed to zero at a certain rate. While SGLD succeeds in (asymptotically) generating samples from the posterior at O ( n ) computational cost with ( n N ) it X  X  mixing rate is unnecessarily slow. This can be traced back to its lack of a proper pre-conditioner: SGLD takes large steps in directions of small variance and re-versely, small steps in directions of large variance which hinders convergence of the Markov chain. Our work builds on top of Welling and Teh (2011). We leverage the  X  X ayesian Central Limit Theorem X  which states that when N is large (and under certain conditions) the posterior will be well approximated by a normal distribution. Our al-gorithm is designed so that for large stepsizes (and thus at high mixing rates) it will sample from this approximate normal distribution, while at smaller stepsizes (and thus at slower mixing rates) it will generate samples from an in-creasingly accurate (non-Gaussian) approximation of the posterior. Our main claim is therefore that we can trade-in a usually small bias in our estimate of the posterior distri-bution against a potentially very large computational gain, which could in turn be used to draw more samples and re-duce sampling variance.
 From an optimization perspective one may view this algo-rithm as a Fisher scoring method based on stochastic gradi-ents (see e.g. (Schraudolph et al., 2007)) but in such a way that the randomness introduced in the subsampling process is used to sample from the posterior distribution when we arrive at its mode. Hence, it is an efficient optimization al-gorithm that smoothly turns into a sampler when the correct (statistical) scale of precision is reached.
 We will start with some notation, definitions and prelimi-naries. We have a large dataset X N consisting of N i.i.d. data-points { x 1 ...x N } and we use a family of distributions parametrized by  X   X  R D to model the distribution of the x  X  X . We choose a prior distribution p (  X  ) and are inter-ested in obtaining samples from the posterior distribution, p (  X  | X N )  X  p ( X N |  X  ) p (  X  ) .
 As is common in Bayesian asymptotic theory, we will also make use of some frequentist concepts in the develop-ment of our method. We assume that the true data gen-erating distribution is in our family of models and denote the true parameter which generated the dataset X N by  X  0 We denote the score or the gradient of the log likelihood w.r.t. data-point x i by g i (  X  ) = g (  X  ; x i ) =  X   X  We denote the sum of scores of a batch of n data-points X r = { x r 1 ...x r n } by G n (  X  ; X r ) = P drop the argument X r and instead simply write G n (  X  ) and g (  X  ) for convenience.
 The covariance of the gradients is called the Fisher infor-mation defined as I (  X  ) = E x [ g (  X  ; x ) g (  X  ; x ) denotes expectation w.r.t the distribution p ( x ;  X  ) and we shown that I (  X  ) =  X  E x [ H (  X  ; x )] , where H is the Hessian of the log likelihood.
 Since we are dealing with a dataset with samples only from p ( x ;  X  0 ) we will henceforth be interested only in I (  X  0 ) which we will denote by I 1 . It is easy to see that the Fisher information of n data-points, I n = nI 1 . The empirical covariance of the scores computed from a batch of n data-points is called the empirical Fisher information, (Scott, 2002). Also, it can be shown that V (  X  0 ) is a consis-tent estimator of I 1 = I (  X  0 ) .
 We now introduce an important result in Bayesian asymp-totic theory. As N becomes large, the posterior distribution becomes concentrated in a small neighbourhood around  X  0 and becomes asymptotically Gaussian. This is formalized by the Bernstein-von Mises theorem, a.k.a the Bayesian Central Limit Theorem, (Le Cam, 1986), which states that under suitable regularity conditions, p (  X  |{ x 1 ...x proximately equals N (  X  0 ,I  X  1 N ) as N becomes very large. We are now ready to derive our Stochastic Gradient Fisher Scoring (SGFS) algorithm. The starting point in the deriva-tion of our method is the Stochastic Gradient Langevin Dy-namics (SGLD) algorithm (Welling &amp; Teh, 2011) which we describe in section 3.1. SGLD can sample accurately from the posterior but suffers from a low mixing rate. In section 3.2, we show that it is easy to construct a Markov chain that can sample from a normal approximation of the posterior at any mixing rate. We will then combine these methods to develop our Stochastic Gradient Fisher Scoring (SGFS) algorithm in section 3.3. 3.1. Stochastic Gradient Langevin Dynamics The SGLD algorithm has the following update equation:  X  Here is the step size, C is called the preconditioning ma-trix (Girolami &amp; Calderhead, 2010) and  X  is a random vari-able representing injected Gaussian noise. The gradient of the log likelihood G N (  X  ; X N ) over the whole dataset is ap-proximated by scaling the mean gradient g n (  X  t ; X t n puted from a mini-batch X t n = { x t 1 ...x t n } of size n N . Welling &amp; Teh (2011) showed that Eqn. (1) generates sam-ples from the posterior distribution if the step size is an-nealed to zero at a certain rate. As the step size goes to zero, the discretization error in the Langevin equation disap-pears and we do not need to conduct expensive Metropolis-Hasting(MH) accept/reject tests that use the whole dataset. Thus, this algorithm requires only O ( n ) computations to generate each sample, unlike traditional MCMC algorithms which require O ( N ) computations per sample.
 However, since the step sizes are reduced to zero, the mix-ing rate is reduced as well, and a large number of iterations are required to obtain a good coverage of the parameter space. One way to make SGLD work at higher step sizes is to introduce MH accept/reject steps to correct for the higher discretization error, but our initial attempts using only a mini-batch instead of the whole dataset were unsuccessful. 3.2. Sampling from the Approximate Posterior Since it is not clear how to use Eqn. (1) at high step sizes, we will move away from Langevin dynamics and explore a different approach. As mentioned in section 2, the poste-rior distribution can be shown to approach a normal distri-bution, N (  X  0 ,I  X  1 N ) , as the size of the dataset becomes very large. It is easy to construct a Markov chain which will sample from this approximation of the posterior at any step size. We will now show that the following update equation achieves this: The update is an affine transformation of  X  t plus injected independent Gaussian noise,  X  . Thus if  X  t has a Gaussian distribution N (  X  t ,  X  t ) ,  X  t +1 will also have a Gaussian dis-tribution, which we will denote as N (  X  t +1 ,  X  t +1 ) . These distributions are related by:  X 
 X  If we choose C to be symmetric, it is easy to see that the variant distribution of this Markov chain. Since Eqn. (2) is not a Langevin equation, it samples from the approximate posterior at large step-size and does not require any MH accept/reject steps. The only requirement is that C should be symmetric and should be chosen so that the covariance matrix of the injected noise in Eqn. (2) is positive-definite. 3.3. Stochastic Gradient Fisher Scoring In practical problems both sampling accuracy and mixing rate are important, and the extreme regimes dictated by both the above methods are very limiting. If the posterior is close to Gaussian (as is usually the case), we would like to take advantage of the high mixing rate. However, if we need to capture a highly non-Gaussian posterior, we should be able to trade-off mixing rate for sampling accuracy. One could also think about doing this in an  X  X nytime X  fashion where if the posterior is somewhat close to Gaussian, we can start by sampling from a Gaussian approximation at high mixing rates, but slow down the mixing rate to capture the non-Gaussian structure if more computation becomes available. In other words, one should have the freedom to manage the right trade off between sampling accuracy and mixing rate depending on the problem at hand.
 With this goal in mind, we combine the above methods to develop our Stochastic Gradient Fisher Scoring (SGFS) al-gorithm. We accomplish this using a Markov chain with the following update equation:  X  When the step size is small, we want to choose Q = C so that it behaves like the Markov chain in Eqn (1). Now we will see how to choose Q so that when the step size is large and the posterior is approximately Gaussian, our algorithm behaves like the Markov chain in Eqn. (2). First, note that if n is large enough for the central limit theorem to hold, we have: g n (  X  t ; X t n )  X  X  E x [ g (  X  t ; x )] , Here Cov [ g (  X  t ; x )] is the covariance of the scores at  X  . Using N Cov [ g (  X  t ; x )]  X  I N and N E x [ g (  X  t G
N (  X  t ; X N ) , we have: Now,  X  log p (  X  t ) + G N (  X  t ; X N ) =  X  log p (  X  t gradient of the log posterior. If we assume that the posterior is close to its Bernstein-von Mises approximation, we have  X  log p (  X  t | X N ) =  X  I N (  X  t  X   X  0 ) . Using this in Eqn. (6) and then substituting in Eqn. (4), we have: where, Comparing Eqn. (7) and Eqn. (2), we see that at high step sizes, we need: Thus, we should choose Q such that: where we have defined  X  = N + n n . Since dominates 2 when is small, we can choose Q = C  X  2 4  X CI N C for both the cases above. With this, our update equation be-comes: Now, we have to choose C so that the covariance matrix of the injected noise in Eqn. (9) is positive-definite. One way to enforce this, is by setting:
C  X  where B is any symmetric positive-definite matrix. Plug-ging in this choice of C in Eqn. 9, we get: However, the above method considers I N to be a known constant. In practice, we use N  X  I 1 ,t as an estimate of I where  X  I 1 ,t is an online average of the empirical covari-ance of gradients (empirical Fisher information) computed at each  X  t . where  X  t = 1 /t . In the supplementary material we prove that this online average converges to I 1 plus O (1 /N ) cor-rections if we assume that the samples are actually drawn from the posterior: Theorem 1. Consider a sampling algorithm which generates a sample  X  t from the posterior distribution of the model parameters p (  X  | X N ) in each itera-tion t . In each iteration, we draw a random mini-batch of size n , X t n = { x t 1 ...x t n } , and compute the empirical covariance of the scores V (  X  t ; X t Let V T be the average of V (  X  t ) across T iterations. For large N , as T  X   X  , V T converges to the Fisher information I (  X  0 ) plus O ( 1 N ) corrections, i.e. lim Note that this is not a proof of convergence of the Markov chain to the correct distribution. Rather, assuming that the samples are from the posterior, it shows that the online av-erage of the covariance of the gradients converges to the Fisher information (as desired). Thus, it strengthens our confidence that if the samples are almost from the posterior, the learned pre-conditioner converges to something sensi-ble. What we do know is that if we anneal the stepsizes according to a certain polynomial schedule, and we keep the pre-conditioner fixed, then SGFS is a version of SGLD which was shown to converge to the correct equilibrium distribution (Welling &amp; Teh, 2011). We believe the adap-tation of the Fisher information through an online average is slow enough for the resulting Markov chain to still be valid, but a proof is currently lacking. The theory of adap-tive MCMC (Andrieu &amp; Thoms, 2009) or two time scale stochastic approximations (Borkar, 1997) might hold the key to such a proof which we leave for future work. Putting it all together, we arrive at algorithm 1 below.
 The general method still has a free symmetric positive-definite matrix, B , which may be chosen according to our convenience. Examine the limit  X  0 . In this case our method becomes SGLD with preconditioning matrix B  X  1 and step size .
 If the posterior is Gaussian, as is usually the case when N is large, the proposed SGFS algorithm will sample correctly for arbitrary choice of B even when the step size is large. Algorithm 1: Stochastic Gradient Fisher Scoring (SGFS) Input: n , B , {  X  t } t =1: T 1: Initialize  X  1 ,  X  I 1 , 0 3: for t = 1 : T do 4: Choose random minibatch X t n = { x t 1 ...x t n } 6: V (  X  t )  X  8: Draw  X   X  X  [0 , 4 B ] 10: end for However, for some models the conditions of the Bernstein-von Mises theorem are violated and the posterior may not be well approximated by a Gaussian. This is the case for e.g. neural networks and discriminative RBMs, where the identifiability condition of the parameters do not hold. In this case, we have to choose a small to achieve accurate sampling (see section 5). These two extremes can be com-bined in a single  X  X nytime X  algorithm by slowly annealing the stepsize. For a non-adaptive version of our algorithm (i.e. where we would stop changing  X  I 1 ) after a fixed num-ber of iterations) this would according to the results from Welling and Teh (2011) lead to a valid Markov chain for posterior sampling.
 We recommend choosing B  X  I N . With this choice, our method is highly reminiscent of  X  X isher scoring X  which is why we named it  X  X tochastic Gradient Fisher Scoring X  (SGFS). In fact we can think of the proposed updates as a stochastic version of Fisher scoring based on small mini-batches of gradients. But remarkably, the proposed algo-rithm is not only much faster than Fisher scoring (because it only requires small minibatches to compute an update), it also samples approximately from the posterior distribution. So the knife cuts on both sides: SGFS is a faster optimiza-tion algorithm but also doesn X  X  overfit due to the fact that it switches to sampling when the right statistical scale of precision is reached. Clearly, the main computational benefit relative to stan-dard MCMC algorithms comes from the fact that we use stochastic minibatches instead of the entire dataset at every iteration. However, for a model with a large number of pa-rameters another source of significant computational effort is the computation of the D  X  D matrix  X N  X  I 1 ,t + 4 B and multiplying its inverse with the mean gradient resulting in a total computational complexity of O ( D 3 ) per iteration. In the case n &lt; D the computational complexity per iteration can be brought down to O ( nD 2 ) by using the Sherman-Morrison-Woodbury equation. A more numerically stable alternative is to update Cholesky factors (Seeger, 2004). In case even this is infeasible one can factor the Fisher in-formation into k independent blocks of variables of, say size d , in which case we have brought down the complexity to O ( kd 3 ) . The extreme case of this is when we treat every parameter as independent which boils down to replacing the Fisher information by a diagonal matrix with the vari-ances of the individual parameters populating the diagonal. While for a large stepsize this algorithm will not sample from the correct Gaussian approximation, it will still sam-ple correctly from the posterior for very small stepsizes. In fact, it is expected to do this more efficiently than SGLD which does not rescale its stepsizes at all. We have used the full covariance algorithm (SGFS-f) and the diagonal co-variance algorithm (SGFS-d) in the experiments section. Below we report experimental results where we test SGFS-f, SGFS-d, SGLD, SGD and HMC on three different mod-els: logistic regression, neural networks and discriminative RBMs. The experiments share the following practice in common. Stepsizes for SGD and SGLD are always se-lected through cross-validation for at least five settings. The minibatch size n is set to either 300 or 500 , but the results are not sensitive to the precise value as long as it is large enough for the central limit theorem to hold (typi-cally, n &gt; 100 is recommended). Also, we used  X  t = 1 t 5.1. Logistic Regression A logistic regression model (LR) was trained on the MNIST dataset for binary classification of two digits 7 and 9 using a total of 10,000 data-items. We used a 50 dimen-sional random projection of the original features and ran SGFS with  X  = 1 . We used B =  X I N and tested the algo-rithm for a number of  X  values (where  X  = 2  X  ). We ran the algorithm for 3,000 burn-in iterations and then collected 100,000 samples. We compare the algorithm to Hamil-tonian Monte Carlo sampling (Neal, 1993) and to SGLD (Welling &amp; Teh, 2011). For HMC, the  X  X eapfrogstep X  size was adapted during burn-in so that the acceptance ratio was around 0.8. For SGLD we also used a range of fixed step-sizes.
 In figure 1 we show 2-d marginal distributions of SGFS compared to the ground truth from a long HMC run where we used  X  = 0 for SGFS. From this we conclude that even for the largest possible stepsize the fit for SGFS-f is al-most perfect while SGFS-d underestimates the variance in this case (note however that for smaller stepsizes (larger  X  ) SGFS-d becomes very similar to SGLD and is thus guaran-teed to sample correctly albeit with a low mixing rate). Next, we studied the inverse autocorrelation time per unit computation (ATUC) 1 averaged over the 51 parameters and compared this with the relative error after a fixed amount of computation time. The relative error is computed as fol-lows: first we compute the mean and covariance of the C the long HMC run which we indicate with  X   X  and C  X  . Finally we compute In Figure 2 we plot the  X  X rror at time T X  for two val-ues of T (T=100, T=3000) as a function of the inverse ATUC, which is a measure of the mixing rate. Top plots show the results for the mean and bottom plots for the covariance. Each point denoted by a cross is obtained from a different setting of parameters that control the mix-ing rate:  X  = [0 , 1 , 2 , 3 , 4 , 5 , 6] for SGFS, stepsizes = and number of leapfrog steps s = [50 , 40 , 30 , 20 , 10 , 1] for HMC. The circle is the result for the fastest mixing chain. For SGFS and SGLD, if the slope of the curve is nega-tive (downward trend) then the corresponding algorithm was still in the phase of reducing error by reducing sam-pling variance at time T. However, when the curve bends upwards and develops a positive slope the algorithm has reached its error floor corresponding to the approximation bias. The situation is different for HMC, (which has no bias) but where the bending occurs because the number of leapfrog steps has become so large that it is turning back on itself. HMC is not faring well because it is computa-tionally expensive to run (which hurts both its mixing rate and error at time T). We also observe that in the allowed running time SGFS-f has not reached its error floor (both for the mean and the covariance). SGFS-d is reaching its error floor only for the covariance (which is consistent with Figure 1 bottom) but still fares well in terms of the mean. Finally, for SGLD we clearly see that in order to obtain a high mixing rate (low ATUC) it has to pay the price of a large bias. These plots clearly illustrate the advantage of SGFS over both HMC as well as SGLD. 5.2. SGFS on Neural Networks We also applied our methods to a 3 layer neural network (NN) with logistic activation functions. Below we describe classification results for two datasets. The goal of this competition is to predict how many days between [0  X  15] a person will stay in a hospital given his/her past three years of hospitalization records used the same features as the team market makers that won the first milestone prize. Integrating the first and second year data, we obtained 147,473 data-items with 139 fea-ture dimensions and then used a randomly selected 70% for training and the remainder for testing. NNs with 30 hidden units were used because more hidden units did not noticeably improve the results. Although we used  X  = 6 for SGFS-d, there was no significant difference for values in the range 3  X   X   X  6 . However,  X  &lt; 3 did not work for this dataset due to the fact that many features had values 0 . For SGD, we used stepsizes from a polynomial anneal-ing schedule a ( b + t )  X   X  . Because the training error de-creased slowly in a valid range  X  = [0 . 5 , 1] , we used  X  = 3 , a = 10 14 , b = 2 . 2  X  10 5 instead which was found optimal through cross-validation. (This setting reduced the stepsize from 10  X  2 to 10  X  6 during 1e+7 iterations). For SGLD, a = 1 , b = 10 4 , and  X  = 1 reducing the step size from 10  X  4 to 10  X  6 was used. Figure 3 (left) shows the classi-fication errors averaged over the posterior samples for two regularizer values,  X  = 0 and the best regularizer value  X  found through cross-validation. First, we clearly see that SGD severely overfits without a regularizer while SGLD and SGFS prevent it because they average predictions over samples from a posterior mode. Furthermore, we see that when the best regularizer is used, SGFS (marginally) out-performs both SGD and SGLD. The result from SGFS-d submitted to the actual competition leaderboard gave us an error of 0.4635 which is comparable to 0.4632 obtained by the milestone winner with a fine-tuned Gradient Boosting Machine. We also tested our methods on the MNIST dataset for 10 digit classification which has 60,000 training instances and 10,000 test instances. In order to test with SGFS-f, we used inputs from 20 dimensional random projections and 30 hid-den units so that the number of parameters equals 940. Moreover, we increased the mini-batch size to 2,000 to re-duce the time required to reach a good approximation of the 940  X  940 covariance matrix. The classification error aver-aged over the samples is shown in Figure 3 (right). Here, we used a small regularization parameter of  X  = 0 . 001 for all methods as overfitting was not an issue. For SGFS,  X  = 2 is used while for both SGD and SGLD the stepsizes were annealed from 10  X  3 to 10  X  7 using a = 1 , b = 1000 , and  X  = 1 . 5.3. Discriminative Restricted Boltzmann Machine We trained a DRBM (Larochelle &amp; Bengio, 2008) on the KDD99 dataset which consists of 4,898,430 datapoints with 40 features, belonging to a total of 23 classes. We first tested the classification performance by training the DRBM using SGLD, SGFS-f, SGFS-d and SGD. For this experiment the dataset was divided into a 90% training set, 5% validation and 5% test set. We used 41 hidden units giving us a total of 2647 parameters in the model. We used  X  = 10 and B =  X I N . We tried 6 different (  X , ) com-binations for SGFS-f and SGFS-d and tried 18 annealing schedules for SGD and SGLD, and used the validation set to pick the best one. The best results were obtained with an  X  value of 8.95 for SGFS-f and SGFS-d, and [ a = 0.1, b = 100000,  X  = 0.9] for SGD and SGLD. We ran all algorithms for 100,000 iterations. Although we experimented with dif-ferent burn-in iterations, the algorithms were insensitive to this choice. The final error rates are given in table 1 from which we conclude that the samplers based on stochastic gradients can act as effective optimizers whereas HMC on the full dataset becomes completely impractical because it has to compute 11.7 billion gradients per iteration which takes around 7.5 minutes per sample (4408587 datapoints  X  2647 parameters).
 To compare the quality of the samples drawn after burn-in, we created a 10% subset of the original dataset. This time we picked only the 6 most populous classes. We tested all algorithms with 41, 10 and 5 hidden units, but since the posterior is highly multi-modal, the different algorithms ended up sampling from different modes. In an attempt to get a meaningful comparison, we therefore reduced the number of hidden units to 2. This improved the situation to some degree, but did not entirely get rid of the multi-modal and non-Gaussian structure of the posterior. We compare results of SGFS-f/SGLD with 30 independent HMC runs, each providing 4000 samples for a total of 120,000 sam-ples. Since HMC was very slow (even on the reduced set) we initialized at a mode and used the Fisher information at the mode as a pre-conditioner. We used 1 leapfrog step and tuned the step-size to get an acceptance rate of 0.8. We ran SGFS-f with  X  = [2 , 3 , 4 , 5 , 10] and SGLD with fixed step sizes of [5e-4, 1e-4, 5e-5, 1e-5, 5e-6]. Both algorithms were initialized at the same mode and ran for 1 million iterations. We looked at the marginal distribu-tions of the top 25 pairs of variables which had the highest correlation coefficient. In Figure 4 (top-left and bottom-left) we show a set of parameters where both SGFS-f and SGLD obtained an accurate estimate of the marginal poste-rior. In 4 (top-right and bottom-right) we show an example where SGLD failed. The thin solid red lines correspond to HMC runs computed from various subsets of the sam-ples, whereas the thick solid red line is computed using the all samples from all HMC runs. We have shown marginal posterior estimates of the SGFS-f/SGLD algorithms with a thick dashed blue ellipse. After inspection, it seemed that the posterior structure was highly non-Gaussian with re-gions where the probability very sharply decreased. SGLD regularly stepped into these regions and then got catapulted away due to the large gradients there. SGFS-f presumably avoided those regions by adapting to the local covariance structure. We found that in this region even the HMC runs are not consistent with one another. Note that the SGFS-f contours seem to agree with the HMC contours as much as the HMC contours agree with the results of its own subsets, in both the easy and the hard case.
 Finally, we plot the error after 6790 seconds of computa-tion versus the mixing rate. Figure 5-left shows the results for the mean and the right for the covariance (for an ex-planation of the various quantities see discussion in section 5.1). We note again that SGLD incurs a significantly larger approximation bias at the same mixing rate as SGFS-f. We have introduced a novel method,  X  X tochastic Gradient Fisher Scoring X  (SGFS) for approximate Bayesian learn-ing. The main idea is to use stochastic gradients in the Langevin equation and leverage the central limit theorem to estimate the noise induced by the subsampling process. This subsampling noise is combined with artificially in-jected noise and multiplied by the estimated inverse Fisher information matrix to approximately sample from the pos-terior. This leads to the following desirable properties.  X  Unlike regular MCMC methods, SGFS is fast because it uses only stochastic gradients based on small mini-batches to draw samples.  X  Unlike stochastic gradient descent, SGFS samples (ap-proximately) from the posterior distribution.  X  Unlike SGLD, SGFS samples from a Gaussian approx-imation of the posterior distribution (that is correct for N  X  X  X  ) for large stepsizes.  X  By annealing the stepsize, SGFS becomes an any-time method capturing more non-Gaussian structure with smaller stepsizes but at the cost of slower mixing.  X  During its burn-in phase, SGFS is an efficient optimizer because like Fisher scoring and Gauss-Newton methods, it is based on the natural gradient.
 For an appropriate annealing schedule, SGFS thus goes through three distinct phases: 1) during burn-in we use a large stepsize and the method is similar to a stochastic gradient version of Fisher scoring, 2) when the stepsize is still large, but when we have reached the mode of the distribution, SGFS samples from the asymptotic Gaussian approximation of the posterior, and 3) when the stepsize is further annealed, SGFS will behave like SGLD with a pre-conditioning matrix and generate increasingly accurate samples from the true posterior.

