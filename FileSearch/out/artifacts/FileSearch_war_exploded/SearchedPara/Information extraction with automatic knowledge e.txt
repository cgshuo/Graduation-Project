 1. Introduction
Information extraction is a process that takes unseen documents as input and produces a tabular structure as output. As Internet growth accelerates, information extraction is attracting considerable attention from the Web intelligence community. Traditional information extraction tasks involve locating specific information from a plain text written in a natural language. Thus, the task biases an information extraction onl yas one area of natural language processing. In-formation extraction, as a fundamental front-end technique for knowledge discovery, data min-ing, and natural language interface to databases, on the Web has transformed a major Web application technolog y(Jung, Lee, Choi, Min, &amp; Seo, 2003; Nahm, 2001; Nahm &amp; Moone y, 2000).

One crucial challenge in information extraction as a Web application technolog yis to acquire domain portability. Since most previous systems require human annotated data to learn extrac-tion rules or patterns, domain experts manuall yannotate the training data. Worse, when a new domain is added, a considerable portion of the time to graft for the domain is poured into la-borious annotation. To circumvent this problem, recent research develops weakl ysupervised and unsupervised learning algorithms. However, the new techniques do not yet satisfy the back-end applications (Eikvil, 1999; Zechner, 1997).

Domain portabilit yis greatl yaffected b ythe manner in which Web document t ypes are used and b ywhich point in time domain experts or users are involved. Thus, we propose two strategies: first, replacing laborious annotation with automatic knowledge extraction from structured Web documents, 1 and second, placing users at the end of a learning cycle in a deployment phase.
To incrementall yimprove extraction performance, POSIE combines user-oriented learning to produce context rules with separate-context learning to generalize the rules.

The remainder of the paper is organized as follows. Section 2 reviews important related research on information extraction. Section 3 proposes our information extraction model based on a question answering framework. The detail architecture and the knowledge of the POSIE spectivel ydescribed in Sections 4 and 5. Section 6 explains the techniques to expand this knowledge through user-oriented and separate-context machine learning. Section 7 analyzes experimental re-sults for the practical  X  X  X ontinuing education X  X  domain. To conclude this paper, Section 8 discusses the functional characteristics of information extraction systems and future works. 2. Related research Information extraction (IE) systems using an automatic training approach (Grishman, 1997;
Sasaki, 1999; Yangarber &amp; Grishman, 1998) have a common goal: to formulate effective rules to recognize relevant information. The yachieve this goal in the manner of annotating training data and running a learning algorithm (Knoblock, Lerman, Minton, &amp; Muslea, 2000; Riloff, 1996; Riloff &amp; Jones, 1999; Sudo, Sekine, &amp; Grishman, 2001).

Recent IE research concentrates on the development of trainable information extraction sys-tems for the following reasons. First, annotating texts is simpler and faster than writing rules by hand. The rapid growth of the Web contents increases the need for a series of automatic pro-cessing steps. Second, automatic training ensures domain portabilit yand full coverage of ex-amples. However, training data which is expensive to acquire interrupts the predominance of trainable IE systems over other approaches.

Core machine learning algorithms to reduce the burden of the training data were adopted in man yNLP 3 applications including information extraction. Weakl ysupervised learning algo-rithms such as co-training and co-EM were developed for text categorization (Blum &amp; Mitchell, 1998; Nigam &amp; Ghani, 2000). The yreduce the annotated data b yusing a small set of seed rules and b yadding unlabeled text with the best score. Despite these efforts, error propagation in the total learning cycle is too severe to obtain the refined rules. Another strategy is active learning, which is a full ysupervised learning algorithm, but the user as a domain expert is required to locate the examples (RayChaudhuri &amp; Hamey, 1997). Due to this laborious work, knowledge changes are difficult to incorporate into the system.

Pierce and Cardie indicate some limitations of co-training for natural language processing, and propose user-oriented learning (Pierce &amp; Cardie, 2001a, 2001b). The yare concerned about the scalability and portability of information extraction systems. During the learning cycle, the user confirms the desirabilit yof new examples. Although the user is not an expert, either in machine learning or in information extraction, he or she is competent to identif ythe specific information to extract as an end user. To leverage the user  X  s ability, user-oriented learning puts the user into a deployment cycle. However, the algorithm still requires the user both to annotate seed training data and to select new candidate examples. The user is onl ysporadicall yinvolved in the learning process; users would not concentrate further on handcrafted works.

Recently, some systems address automatic constriction of extraction rules without any anno-1999). For DIPRE system, Brin (1998) uses a bootstrapping method to obtain patterns and re-lations from Web documents without pre-annotated data. The process is initiated with small samples, such as the relation of (author, title) pairs. Next, DIPRE searches a large corpus for patterns in which one such pair appears. Similarly, Yangarber and Grishman apply an automatic bootstrapping to seek patterns for name classification (Yangarber &amp; Grishman, 2000). This method requires a named-entit ytagger and a parser to mark all the instances of people  X  s names, companies, and locations. Kim et al. improves these automatic bootstrapping algorithms using the types of the Web documents (Kim, Cha, &amp; Lee, 2002; Kim, Jung, &amp; Lee, 2003). They focus more on declarative-style knowledge, which can be extended with human interaction for practical-level performance in a deployed commercial system. To generate extraction patterns, this model combines declarative DTD-style patterns and an unsupervised learning algorithm: SmL. The elimination of human pre-processed documents for training produces great portabilit yto new domains. However, the model sacrifices a portion of extraction precision to acquire a high domain portability. Without a dedicate process by hand, a fully automatic extraction system does not always ensure stable results. 4
User-oriented learning is a promising strateg ywhich eliminates the deficienc yof task coverage and provides feedback to the extraction system in both the learning and extraction phases. On the other hand, the automatic acquisition of rules from structured Web documents is a great benefit to the WWW. To maximize the efficienc yof information extraction on the Web, we propose a nice hybridized technique of automatic bootstrapping and user-oriented learning, i.e., the annotation aspect of user-oriented learning is replaced with bootstrapping. That is, the user becomes involved in onl ythe confirmation aspect of learning. The combination of user-oriented learning and sep-arate-context learning incrementall yexpands domain knowledge. As the framework of our IE system, a question answering system (Lee et al., 2001) is redesigned and adopted in the following four steps: first, automaticall yextract instances from structured Web documents; second, construct instance rules through sentence-to-LSP 5 transfer; third, confirm context rules b yuser-oriented learning; finally, generalize the context rules with separate-context learning. 3. Information extraction as question answering
The goal of question answering is to develop a system that retrieves answers rather than doc-uments in response to a question (Lee et al., 2001). As an ordinar yprocedure, a question answering system focuses on possible answers, how to determine the answer type and how to select the an-swers for each answer type. The system classifies possible answers, designs a method to determine the answer types, and searches answer candidates. There are three major steps: question processing, passage selection, and answer processing. Question processing analyzes the input question to understand what the user wishes to find. Passage selection ranks the passages in retrieved docu-ments. Answer processing selects and ranks answer candidates matching the answer type.
Question answering is closel yrelated to information extraction in that its purpose concerns the acquisition of user-requested information. However, information extraction as question an-swering is much easier than question answering itself for the following reasons. First, information extraction has a set of pre-selected questions for a target, which removes the need for question processing. Second, a pre-classified document as input is read yto be directl yprocessed, while question answering should identif yrelated documents in unrestricted open domains. Third, the relation between slots is available in information extraction, i.e., pre-defined slots help to deter-mine their instances b yusing the relation. Thus, recasting the question answering into information extraction can produce a better performance than state of the art question answering systems (Harabagiu et al., 2000; Moldovan et al., 1999).

Information extraction as a question answering also simplifies the extraction processes. We can easil yintroduce the techniques verified b yquestion answering, such as domain knowledge and instance rules. Fig. 1 shows the similarities between information extraction and question an-swering. A slot in information extraction corresponds to a pre-selected question in question an-swering. Thus, information extraction can exclude the question processing which generates many ambiguities for answer types. Domain knowledge, which is common to the two applications, includes a categor ydictionar y, a thesaurus, and collocation information. As a shared feature between the two, instance rules are applied to obtain instance hypotheses or answer candidates from the input document.

The IE model on a question answering framework improves the building process of domain knowledge by separately applying the types of Web documents. Structured Web documents provide a set of instances for each slot. Instance-to-LSP transfer automaticall yconstructs instance rules for IE from the instances obtained from automatic bootstrapping. The following section explains the system architecture based on the IE as a question answering model described here. 4. System architecture for extraction
Our system, POSIE, consists of three major phases: building, learning, and extraction. The building phase constructs several classes of extraction knowledge (see Section 5), such as collo-cation DB (database) for NE (named entity) tagging and instance rule DB for instance finding.
The learning phase generalizes the rules to enhance the extraction coverage (see Section 6). Fig. 2 shows onl ythe s ystem architecture to extract target frames using the knowledge obtained and generalized b ythe building and learning phase. 4.1. HTML pre-processing and morphological analysis
DQTagger, an HTML pre-processor, removes most HTML tags except &lt;title&gt; and &lt;key-word&gt; for an HTML document (Shim, Kim, Cha, Lee, &amp; Seo, 2002). The pre-processor keeps the layout of tables and determines the boundary of the body. All the processes after this pre-pro-cessing are performed on HTML tag-removed documents, that is, almost plain texts. A mor-phological analyzer (MA) segments and analyzes Korean sentences. Each eojeol produces pairs of morphemes and the part-of-speech (POS) tag. The MA post-editing of the analysis restores the incorrect morpheme sequences using an error DB (Table 1). 4.2. Category dictionary and thesaurus
SiteQ, a question answering system, uses a category dictionary and a thesaurus to construct lexico-semantic patterns for both questions and retrieved passages (Lee et al., 2001; Kim, Kim,
Lee, &amp; Seo, 2001). Since POSIE shares the concept of language processing with a question an-swering system such as SiteQ, we use a category dictionary and a thesaurus as the main semantic information sources. In TREC 10, 8 SiteQ has 66 semantic tags and man yuser-defined semantic classes. The semantic tags are now expanded to 83 in POSIE.

The categor ydictionar yhas approximatel y67,280 entries which consist of four components: semantic tag, user-defined semantic class, part-of-speech tag, semantic tags is a flat form. In a lexico-semantic pattern, each semantic tag follows a  X  X  X  X  X  symbol. User-defined semantic classes are the tags for syntactically or semantically similar lexical groups.
For example, a user-define semantic class  X  X %each X  X  includes the words, such as  X  X  , X  X   X  X  , X  X   X  X  , X  X   X  X  , X  X   X  X  , X  X   X  X  , X  X   X  X  , X  X  and  X  X   X  X  in Korean.

The thesaurus, which is an assistant of the categor ydictionar y, discovers sense codes for general unknown words. The codes are matched with categor ysense-code mapping table (CSMT) to acquire the most similar semantic tags. Currently, the thesaurus has about 90,000 words. The vertical bar means  X  X  X r. X  X 
Semantic tags 66 tags for Q/A Artificial language, action, artifact, belief, bird, book, building, city,
Extended 17 tags Address, appliance, art, computer, course, deed, examination, hobby, [Thesaurus entries with sense codes] 386DX 03010173091001010o0202 [Categor ysense-code mapping table (CSMT)] @computer 03010173091001010o02 @action 0B0E j 0B0K0Q062C04 j 0B0K0Q07 [Mapping results] 386DX fi @computer 4.3. Sentence-to-LSP transfer
A lexico-semantic pattern is a structure where linguistic entries and semantic types can be used in combination to make an abstraction of certain sequences of words in a text (Lee et al., 2001;
Mikheev &amp; Finch, 1995). Linguistic entries consist of words, phrases, and part-of-speech tags, such as  X  X  X MCA, X  X   X  X  X oung Men  X  s Christian Association, X  X  and  X  X  X q_loc. X  X  clude slot name instances, semantic tags (categories), and user-defined semantic classes, for ex-ample,  X  X #ce_c_teacher, X  X  11  X  X  X person, X  X  and  X  X %each. X  X 
Sentence-to-LSP transfer makes a lexico-semantic pattern from a given sentence (Jung et al., 2003). Lexico-semantic patterns enhance the coverage of extraction b yinformation abstraction through many-to-one mapping between phrases and a lexico-semantic pattern (Table 2).
The lexico-semantic patterns obtained from structured Web documents become the left-hand sides of instance rules. The average compression ratio 12 sentences are transferred into one lexico-semantic pattern. Results show that the distribution of compression ratio has a high deviation according to slot names. Experimentally, the type of slot names influences recall and precision (Table 3, see Section 7).

The transfer consists of two phases: named entit y(NE) recognition and NE tagging. NE rec-ognition discovers all possible semantic types for each word by consulting a category dictionary and a thesaurus (Rim, 2001). When a semantic type for a given word does not exist in the category dictionary, we attempt to discover the semantic types using the thesaurus. A category sense-code mapping table converts the sense code on the thesaurus into semantic tags used in the category dictionary. The table consists of pairs of semantic tags and sense codes. Each word without a semantic type becomes the key for the thesaurus search. If the search succeeds and some sense codes are retrieved, we calculate the semantic distance between the retrieved codes and the codes contained in each semantic tag in CSMT. The semantic distance lows:
NE tagging selects a semantic type for each word so that a sentence can map into our lexico-semantic pattern only. Collocation DB has the form of a trigram and is utilized for the tagging.
The components of the trigram, like lexico-semantic patterns, are lexical entries and semantic types. The examples for the trigrams and the frequencies for the  X  X  X ontinuing education X  X  domain are given in Table 4. 4.4. Instance finding To find extractable instances, we appl ytwo major features: instance rules and context rules.
The instance rules automaticall yobtained from structured Web documents discover instance hypotheses in a given document (see Section 6.1). The lexico-semantic pattern and the slot name are the components of the instance rules, as follows: We match the left-hand sides with a lexico-semantic pattern from the sentence-to-LSP transfer. If matching succeeds, the lexico-semantic pattern becomes an instance hypothesis for the slot name on the right-hand side. Next, to expand the coverage of instance rules, POSIE merges the instance hypotheses. Recursively, the instance merge applies to the algorithm given below:
Let A and B be instance hypotheses. [Basic conditions] 1. The two are the instance hypotheses of the same slot name. 2. The two are in the same sentence in a document. (We do not consider HTML tags because they
The two are merged into a new hypothesis: 1. If the scope 14 of A includes that of B, and vice versa. 2. If the scope of A overlaps with that of B, and vice versa. 3. If A  X  s end position meet with B  X  s start position, and vice versa. 4. If there is a symbol between A  X  s end position and B  X  s start position, and vice versa.
The following shows some of the examples of the instance merge: [Sentence for  X  X  X ourse time X  X  slot] [Lexico-semantic pattern] [Instance hypotheses] [The result of instance merges]
After instance search and merge, we have all possible instance hypotheses. The remaining modules of the extraction phases filter the hypotheses using context rules, dynamic slot grouping, and a slot relation check. We have two versions of context rules (see Section 5.2): context rules from user-oriented learning (see Section 6.2) and generalized context rules from separate-context learning (see Section 6.3). These rules verif ythe extracted instance h ypotheses using the left and right context. 4.5. Target frame filling
A context rule represents onl yone slot instance using the left and right context. On the other hand, WHISK (Soderland, 2001) permits rule description for multi-slots, which is a major reason wh yWHISK gives accurate results in discovering multi-slot and their relations. However,
WHISK requires learning all the types of permutations because the rule description depends on the ordering of slots.
 Our dynamic slot grouping removes the two major limitations of previous systems such as
WHISK, i.e., the number of slots to describe and the learning load to permute. Two or more context rules are woven into a rule after discovering instance hypotheses. For example, if two context rules  X  X  X  #ce _ c _ teacher sym_:} $TEACHER {sym_par %picture @action @action sym_, %picture @po-same boundar y X  X #ce_c_period, X  X  then the slot grouping d ynamicall ycombines the two rules as  X  X  X  #ce _ c _ teacher sym_:} $TEACHER {sym_par %picture @action @action sym_, %picture @po-sition sym_par #ce _ c _ period sym_:} $PERIOD { #ce _ c _ time }. X  X  An yrestriction on the number of slots to describe does not exist, because slots are freel ygrouped in running time. This eventuall y makes POSIE extract multi-slots without an ytraining or rule for them.

The ordering of slots does not affect the learning load of permutation because the source of learning is a simple context rule, not a combined form of either two or more rules. Thus, dynamic slot grouping is a prospective algorithm for the multi-slot extraction that other information extractors regard as a burdensome chore. The following examples show our dynamic slot grouping: [Input document after HTML pre-processing] [Slot instances and their context] [Dynamic slot grouping with left and right context 19 ]
Finally, slot relation check (SR) determines the number of target frames and slots to fill by the inspection of the groups to which slots belong as follows: Let M be the group with the largest number of slots.

Let slot-num  X  A  X  be the number of slots belonging to group A . Let n be the number of groups where slot-num  X  A  X  &gt; 1.
 For each group A do If n is 1, the number of target frames is also 1.

After the target frame filling, the user can confirm the extraction results. POSIE provides several types of information, i.e., instance type, position, context score, and group number, to the user to help the confirmation. The feedback updates context rules and then generalized context rules to incrementall yimprove the extraction performance. 5. Rules for information extraction
Information extraction requires several pieces of knowledge to be closel yrelated to a pre-defined target. Man ys ystems attempt to minimize domain-dependent knowledge and handcrafted features. Machine learning algorithms such as co-training (Blum &amp; Mitchell, 1998) and inductive follow mainstream research b yreusing semantic information, minimizing manual annotation, and generalizing rules using machine learning.

In contrast to other systems, we use two kinds of rules to extract and filter instances: in-stance rules and context rules, including a generalized version. A context rule consists of a slot name and two context separated into left and right. This two-level rule description increases extraction coverage without keeping all the possible permutations between instances and the context. 5.1. Instance rules
From each table field in structured Web documents, instance rules are automaticall yacquired through a sentence-to-LSP transfer process (see Section 6.1). Instance rules, a tool of instance finding, consist of lexico-semantic pattern and slot name pairs. Automatic acquisition of instance rules overcomes the current major barriers of information extraction: domain portabilit ywith minimal human intervention while maintaining a high extraction performance. POSIE auto-maticall yextracts the instances for each slot from structured Web documents, and uses them as seed instance examples.

Instance rules are the knowledge required to find slot instance hypotheses. The role of the rules in information extraction resembles that of question answering, whose rules have named entities to discover answer candidates. In information extraction, however, the rules consist of slot in-stances. The rules are applied to test documents to obtain all possible instance hypotheses. Thus, instance filtering is a role of the context rules.

Feedback on user-oriented learning helps to select instance rules to be added later. With the positive confirmation of the user, new instance rules from the merged instances are added into instance rules. Next, the system automatically applies these newly updated rules to extract in-stance hypotheses. This process assures an incremental improvement of the system by enhanced recall.
 5.2. Context rules and generalized context rules
In this section, we describe both context rules produced b yuser-oriented learning and gener-alized context rules b yseparate-context learning. The two learning algorithms are sequentiall y applied to produce the two rules.

Context rules composed of a left and right context are the knowledge to represent the context of selected instances. The sample context and their context rules are given in Table 5. The rules proposed b yCaliff and Moone y(1998) consist of a filler, a pre-filler, and a post-filler.
The context rule of POSIE also has three similar parts. However, several differences exist between tags, semantic classes, and words regarded as independent features. Our rules are lexico-semantic patterns tightl ycoupled with linguistic components. Second, instance representation: the yrep-and context rules are separated. A context rule, a meta-instance rule, has onl ya slot name. The two-level architecture enhances coverage and reduces the size of the rules. Third, context range: the ydefine the range as the number of common features between the examples, which causes the context too short to include all of the clue words. On the other hand, POSIE selects the furthest slot name instance 20 within pre-defined window size, currentl y10, as the context boundar y.
As described above, POSIE adopts a two-level rule architecture. However, the absence of rule generalization does not ensure reliable coverage. We propose a separate-context learning and a sequential covering algorithm, to produce a generalized version of context rules. In their formats, generalized context rules (Table 6) are different from context rules; the latter consists of three slot name, context type, context pattern and coverage score (see Section 6.3).

As with context rules, the slot name is the upper level of two-level rule description. The context hypothesis matches with a negative rule, then the hypothesis would be discarded. Where no context rules are applied, we selectivel ymatch with generalized rules according to the context t ype. The source of a generalized rule is context rules described with lexico-semantic patterns. Occasionally, however, the generalized rule would include an incomplete component. For example,  X  X #ce_c_t X  X  originall yfrom  X  X #ce_c_teacher X  X  or  X  X #ce_c_time, X  X  because our learning algorithm produces it as the common string among context patterns. Coverage score is the number of context covered b ya current generalized rule among the entire context with the same slot name. 6. Incremental expansion of knowledge
To ensure an incremental extraction performance, new reliable knowledge should be added into an extraction system as training proceeds. POSIE automatically extract instances, the source of instance rules, for each slot using mDTD 22 (Kim et al., 2003). Whenever a Web robot gathers documents for a given domain, for the structured documents among them, the mDTD rules extract the instances. This process graduall yincreases instance rules through sentence-to-LSP transfer. Further, POSIE incrementall yexpands domain knowledge using a sequence of user-oriented learning and separate-context learning. We adapt original user-oriented learning to re-ducing the user  X  s involvement b yreplacing manual annotation with automatic bootstrapping.
User-oriented learning, a promising algorithm which applies to both learning and extraction phases, is combined with separate-context learning to produce a generalized version of the context rules confirmed b ythe user.

Our knowledge expansion on instances and context is similar with Jones and his colleagues  X  work in that the yalso use two distinct knowledge: phrases and extraction patterns (Jones et al., 1999). However, we do not use their mutual bootstrapping-like methodolog ybecause iterative bootstrapping loop on different knowledge would cause error propagation although each loop chooses the highest scoring pattern. We prevent error propagation b yexcluding iterative mutual learning between the knowledge, and filter instance hypotheses by applying dynamic slot grouping and the validation with both instance and context rules. 6.1. Extracting instances from automatic bootstrapping
The instance extractor focuses more on declarative-style knowledge, which can be extended with human interaction for practical-level performance in an actual deployed commercial system.
The extractor applies a new extraction method to combine declarative DTD-style extraction patterns and a machine learning algorithm without an annotated corpus to generate the extraction patterns.
 The DTD concept is generall yused for markup languages, such as SGML, XML, and HTML.
In these documents, DTD is usuall ylocated in some external files, and defines the elements which belong to this document type (Flynn, 1998). Using DTD, SGML documents can encode the ele-ments included in the documents, and also parse those elements that appear in the document. We introduce the concept of mDTD, an extension of the conventional DTD concept of SGML, which we modif yfor applicabilit yto HTML-based Web document extraction. The background idea of mDTD is similar to DTD usage in SGML. Hence, mDTD is used to encode and decode the textual elements of the extraction target. In the learning phase, mDTD rules are learned and added to the set of seed mDTDs for the extraction task. In the extracting phase, a learned mDTD rule set is used as extraction patterns to identif ythe elements in HTML documents from Web sites. The idea of mDTD gives a more structured encoding abilit yto an otherwise degenerated HTML document.

A Web Robot gathers Web pages according to the seed URL lists for a given domain. The robot downloads onl ystructured Web documents among the pages. Next, the instance extractor parses seed mDTD rules and then uses token sequences to construct hierarchical mDTD object graphs. In this token sequence, the extraction process is the same as the instance classification task, where each token is classified into an instance of the extraction target based on the HTML table structure, and the possible name instances of each class.

The extractor identifies various types of extraction targets, which are defined by the template with slots (same as the schema attributes in the relational database), and then fills the empty template slots with the identified instances. Table 7 shows an example of the output template, with its filled slots. We process part-of-speech tagging and rule matching tests. Basically, the extractor uses exact matching techniques between the input token with POS tag information and the symbolic rule object. If the input token partially matches the symbolic object, decision-making for matching depends on the ratio of the matched characters compared with the total length of the input token, where the threshold level is up to half the total length.

The POS tag sequence rules are used onl yto test for exact matching. If none of the s ymbolic rules from lexical similarit ymatch, this module evaluates the POS tag rules; otherwise, POS tag rules are applied to confirm the matching results between the token and mDTD rules. SmL (Kim et al., 2002; Kim et al., 2003), the forerunner of POSIE, describes the whole process in detail. 6.2. Adapting user-oriented learning User-oriented learning, a moderatel ysupervised learning algorithm introduced b yPierce and
Cardie, concentrates on two main issues in information extraction: scalabilit yand portabilit y locate and extract. Real users ma ynot be experts at machine learning or text processing, but ma y be qualified experts at judging their goals. The authors believe that users can specif ytheir in-formation needs b yproviding training examples; the user is proficient at judging an information structure as adequate or inadequate. User-oriented learning performs three steps: annotation, location, and confirmation. Users confirm examples as positive, negative, and unconfirmed (no of new examples. Definitive judgments from the user also differentiate the form of weakly supervised learning. However, user involvement in the final decision step is inevitable to acquire an acceptable qualit yfor target extraction, as shown in Fig. 3.
 The followings show the context score (confirmation score) calculation formula:
Using structured Web documents as annotated sources removes the need for manual annota-tion (Kim et al., 2002). Users can concentrate on confirmation without an ymanual annotation of training corpus. Thus, the learning steps in POSIE are reduced to two steps which differ from the original user-oriented learning: location and confirmation. The judgment of the user greatly influences the location of new candidate examples. The context score ranges from 6.3. Generalization of context rules
The sequential covering algorithm, which is a famil yof algorithms for inductive learning, learns one rule and then removes the examples which the rule covers (Mitchell, 1998). This iteration is called a one-rule-learning X  X iscarding process. Using this process, we introduce a separate-context learning algorithm to generalize the characteristic of context rules (Fig. 4).

CalculateMinScore( ) and CalculateMaxScore( ) calculates the minimal ( s ( s max ) covering scores for current context rule ( c i ). The covering score represents the number of rules covered b ythe current rule.

Covering score (for positive)
Covering score (for negative)
GeneralizeContext( ) returns the new context rule as long as possible with the maximal ( s covering score. The function repeatedl ygrows the size of a given rule and calculates its own covering score while the maximal ( s max ) covering score would not be dropped. The enlargement will be stopped when the maximal ( s max ) covering score changes down. Like a standard separate-and-conquer algorithms such as IREP (Cohen, 1995), our separate-context learning trains rules in a greed yfashion. However, we attempt to find the best context rule set strictl yholding the highest context score.

Table 8 compares the three sequential covering algorithms using three important features: example selection, rule scoring, and example types to demonstrate the general ability of our al-gorithm. CN2 (Clark &amp; Niblett, 1989) measures the performance for the generated rules using information gain, which resembles FOIL (Quinlan, 1990), while, sequential m-DTD learning (SmL) (Kim et al., 2002) calculates the lexical similarit yand coverage rate. Our separate-context learning selects examples b yboth context score and rule frequenc y. Positive and negative ex-amples for learning are the field instances simpl yextracted from structured Web documents.
Unlike the other two algorithms, our algorithm learns four sets of examples: positive + left, positive + right, negative + left, and negative + right. When each set of generalized rules entirely covers its own examples, the learning stops. 7. Experimental results
A Web robot searched Web sites, such as universities and education centers, which provide information on  X  X  X ontinuing education. X  X  We manuall ygathered and filtered 431 Web documents on course information from tens of education-related Korean Web sites such as http://oun. knou.ac.kr/, http://www.ajou.ac.kr/~lifetime/ , and http://ncle.kedi.re.kr/ . Two hundred and forty eight of them were semi-structured Web documents 23 and the others were structured Web doc-uments. One thousand seven hundred and ninet ysix instance rules were automaticall yextracted from 3715 instances in the structured Web documents (Table 3). The rules determine instance hypotheses from the semi-structured Web documents in the first extraction phase.

POSIE extracts instances for seven slots: prescribed number ($NUMBER), teacher ($TEACHER), course name ($NAME), start time ($START), period ($PERIOD), tuition fee ($MONEY) and school hours ($TIME). The semi-structured Web documents include several multi-slots to handle. 24 We divide the documents into two sets: a training and a test set. One hundred and sevent yout of 248 are randoml yselected as the test set. The training set consists of three subsets: 24, 30 and 24. POSIE measures the extraction performance after learning each training subset. Finally, we applied the context score (CS; see Section 6.2), generalized context (GC; see Section 6.3) and slot relation check strategies (SR; see Section 4.5).
Table 9 shows that user-oriented learning enhances extraction performance. As the criteria of the performance, we measure recall, precision and F 1-measure on three techniques: context score (CS), context score + generalized context (CS + GC) and context score + generalized context + slot relation check (CS + GC + SR). We define the baseline of the system as the performance with no user training, i.e., no manual confirmation is applied. F 1-measure on the baseline is 0.477. Recall is sufficientl yhigh to appl yto this domain without human intervention. The high recall and low precision impl ythat the automatic knowledge construction from structured-documents helps to search instance hypotheses, but does not have useful information to select one. As the size of the user training documents grows, the system obtains a higher performance, up to 0.75 for F 1-measure.

The performance after learning 24 documents rapidl yincreases. While the user training doc-uments less than 10% of total are used for user-oriented learning, precision and F 1-measure reach almost the peak of our extraction performance. Almost all of our improved performance comes in precision, while recall stays almost completely flat. Only the merged instances are added into new instance rule set when the user positivel yconfirms, that is, an yinstance rules completel yseparated from existed rule set would not be added into the set. Providing the wa yto consider the separated rules during the user-oriented learning will certainl yincrease recall and overall performance, and which is one of our future works.

CS + GC + SR strateg yis alwa ys superior to those of CS and CS + GC except for 78 documents in F 1-measure, but no exception exists in precision. The reason would be that the high perfor-mance of CS and GC decreases SR  X  s effect. Indeed, in a case of omitting CS or GC, the per-formance gap b ySR distinctl yincreased for several randoml yselected documents. General context rules ensure a high precision. User-oriented learning makes a set of context rules, and separate-context learning generates the generalized version that determines whether the current instance hypothesis extracted from an unseen document is a real instance. From the above results, we can see that the greater the number of user learning documents, the less the role of the slot relation check.

We also experimented with the case that CS + GC + SR does not include dynamic slot grouping strategy. Recall is the same as the result in Table 9, and precision drops to 0.721 at 78 docs. The little effect of the grouping is caused b ythe high performance of context rules, that is, CS and GC.
As we expected, in the case without CS and GC strategies, SR with dynamic slot grouping has 0.61 as mentioned in the above table, and SR without dynamic slot grouping has 0.47 in precision at 78 docs.
 Fig. 5 shows the extraction performance on each seven slot. From four slots such as $NUM-
BER, $PERIOD, $MONEY and $TIME, we obtain F 1-measures higher than 0.8. On the other hand, course name and teacher slots have the range of 0.5 X 0.6 for F 1-measure. The two slots have more variations in their forms than the other slots, which proves that the compression ratios of the two are less than the others.

However, the low performance on some specific slots does not discourage POSIE. Through an indirect comparison with other systems on semi-structured documents, WHISK (Soderland, 2001) and SRV (Freitag, 1998a, 1998b), for teacher slot, 26 POSIE achieves a much better performance.
For WHISK, the recall for  X  X  X peaker X  X  slot is onl y0.111 at precision 0.526, and for SRV, precision is 0.62. POSIE shows a remarkable result that precision for  X  X  X eacher X  X  slot is 0.769 at recall 0.435. This result is noteworth ybecause the  X  X  X eacher X  X  slot often requires more than one name, occasionall yeven five or six persons, for an instance in the  X  X  X ontinuing education X  X  domain. The high recall for the  X  X  X eacher X  X  slot would be obtained from a categor ydictionar ywhich is crucial knowledge for answering questions. Even in the worst case, precision and recall always are above 0.4, which ensures the reliabilit yof our extraction algorithm. While previous IE s ystems are weak to extract persons, locations, and organizations, question answering systems endeavor to discover them. To extract these types, POSIE adopts many methodologies from the question answering system, which eventually ensure higher recall than other IE systems.

User-oriented learning and separate-context learning enrich extraction knowledge (see Table 10). New instance rules and context rules are incrementall yincreased as learning proceeds.
General context rules oscillate in their size due to the conflict between context rules, because our learning produces onl ycorrect generalized rules which do not cover an yopposite rules.
We also experimented on  X  X  X ob offering. X  X  We manuall ygathered and filtered 190 Web docu-ments from http://www.jobkorea.co.kr/ , http://www.joblink.co.kr/ , and http://www.guinbank. com/ and so on.

POSIE extracts instances for seven slots: category, number, age, schooling, salary, area and period. We divide the documents into two sets as the above experiment: a training and a test set.
One hundred and thirt yfive out of 190 are randoml yselected as the test set. The training set consists of three subsets: 24 and 55. POSIE measures the extraction performance after learning each training subset. Finally, we applied the mixture of the context score, generalized context and slot relation check strategies to the test documents (Table 11). 8. Conclusion 8.1. Characteristics of the information extraction systems
Table 12 summarizes the functional characteristics of well-known information extraction sys-tems (Eikvil, 1999). 27 The first four rows have a background in the wrapper generation com-munities (Kushmerick, 2000; Muslea, Minton, &amp; Knoblock, 1998), i.e., the ygenerate wrappers for structured Web documents with delimiter-based patterns, and the others in the traditional in-formation extraction communities. RAPIER (Califf &amp; Mooney, 1998), SRV and WHISK adopt relational learning algorithms to handle a wider range of texts. The last two are systems developed from our recent research. SmL applies automatic bootstrapping to the instances in structured Web documents (Kim et al., 2003). While SmL shows the optimal reduction of human inter-vention and guides an adequate use of the Web document types, it suffers from unstable extraction results due to the lack of natural language processing capabilities.

POSIE satisfies all the functions required b yTable 12. To handle multi-slot extraction, POSIE links related instances using shared boundaries between context rules and slot relation check, and detects missing instances that frequentl yappear in semi-structured and free text documents. Since
POSIE dynamically combines the context rules extracted from a document, dering of the instances does not degrade its performance. 8.2. Discussion
POSIE is an information extraction system with the hybridization of automatic bootstrapping and a sequence of learning algorithms on a question answering framework. POSIE uses structured Web documents, dictionaries, and semantic information to seed patterns of instances and context. Minimal human effort is used to validate these patterns, and then iterativel ydiscover new ones.
The system has several strong points. First, minimal intervention is required by users who are domain experts. Second, question answering techniques give a high performance with a reliable recall of slot instances. Third, a wide linguistic combination from lexical forms to semantic fea-tures is employed. Fourth, a sequence of learning algorithms in both the learning and extraction phases ensures incremental extraction performance. Future work includes the following topics: adding new domains to ascertain domain portability, 29 designing flexible generalization algo-rithm to obtain maximal coverage for unseen documents, and updating user-oriented learning interface to add separated instance rules to increase recall.
 Acknowledgements
This work was supported b yBK21 (Ministr yof Education) and mid-term strategic funding (MOCIE, ITEP).
 References
