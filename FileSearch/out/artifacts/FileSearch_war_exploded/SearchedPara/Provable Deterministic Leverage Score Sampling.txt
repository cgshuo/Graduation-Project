 We explain theoretically a curious empirical phenomenon:  X  X pproximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate X . In this work, we provide a novel theoretical analysis of deterministic leverage score sampling . We show that such sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empiri-cal evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques. G.1.3 [ Mathematics of Computing ]: Numerical Analysis-Numerical Linear Algebra; E.m [ Data ]: Miscellaneous Subset selection; low-rank matrix approximation; leverage scores; deterministic sampling; power law distributions
Recently, there has been a lot of interest on selecting the  X  X est X  or  X  X ore representative X  columns from a data matrix [13, 26]. Qualitatively, these columns reveal the most important information hidden in the underlying ma-trix structure. This is similar to what principal compo-nents carry, as extracted via Principal Components Analysis (PCA) [23]. In sharp contrast to PCA, using actual columns of the data matrix to form a low-rank surrogate o  X  ers inter-pretability, making it more attractive to practitioners and data analysts [33, 5, 34, 26].

To make the discussion precise and to rigorously charac-terize the  X  X est X  columns of a matrix, let us introduce the following Column Subset Selection Problem (CSSP).
Column Subset Selection Problem. Let A 2 R m  X  n and let c&lt;n be a sampling parameter. Find c columns of A  X  X enotedas C 2 R m  X  c  X  X hatminimize where C  X  denotes the Moore-Penrose pseudo-inverse.
State of the art algorithms for the CSSP utilize both de-terministic and randomized techniques; we discuss related work in Section 5. Here, we describe two algorithms from prior literature that su ce to highlight our contributions.
A central part of our discussion will involve the leverage scores of a matrix A , which we define below.

Definition 1. [Leverage scores] Let V k 2 R n  X  k contain the top k right singular vectors of a m  X  n matrix A with rank  X  = rank ( A ) k .Then,the(rank-k )leveragescoreof the i -th column of A is defined as Here, [ V k ] i, : denotes the i -th row of V k .
One of the first algorithms for column subset selection dates back to 1972: in [21], Joli  X  e proposes a determin-istic sampling of the columns of A that correspond to the largest leverage scores ` ( k ) i , for some k&lt; rank( A ). Although this simple approach has been extremely successful in prac-tice [21, 22, 29, 8], to the best of our knowledge, there has been no theoretical explanation why the approximation er-rors k A CC  X  A k F and k A CC  X  A k 2 should be small.
One way to circumvent the lack of a theoretical analysis for the above deterministic algorithm is by utilizing ran-domization. Drineas et al. [13] proposed the following ap-proach: for a target rank k&lt; rank( A ), define a probabil-ity distribution over the columns of A , i.e., the i th column is associated with a probability p i = ` ( k ) i /k ; observe that P i p i =1 , since dependent and identically distributed passes, sample with replacement c columns from A , with probabilities given by p . Drineas et al. [13], using results in [30], show that this random subset of columns C 2 R m  X  c approximates A ,with constant probability, within relative error: k A CC  X  A k (1 + " ) k A A k k F , when the number of sampled columns is c = O ( k log k/ " 2 ) , for some 0 &lt; " &lt; 1. Here, A the best rank-k matrix obtained via the SVD.

There are two important remarks that need to be made: (i) the randomized algorithm in [13] yields a matrix esti-mate that is  X  X ear optimal X , i.e., has error close to that of the best rank-k approximation; and (ii) the above random sampling algorithm is a straightforward randomized version of the deterministic algorithm of Joli  X  e [21].

From a practical perspective, the deterministic algorithm of Joli  X  e [21] is extremely simple to implement, and is com-putationally e cient. Unfortunately, as of now, it did not admit provable performance guarantees. An important open question [13, 29, 8] is: Can one simply keep the columns hav-ing the largest leverage scores, as suggested in [21], and still have a provably tight approximation?
In this work, we establish a new theoretical analysis for the deterministic leverage score sampling algorithm of Jo-li  X  e [21]. We show that if the leverage scores ` ( k ) a su ciently steep power-law decay, then this determinis-tic algorithm has provably similar or better performance to its randomized counterparts (see Theorems 2 and 3 in Section 2). This means that under the power-law decay assumption, deterministic leverage score sampling provably obtains near optimal low-rank approximations and it can be as accurate as the  X  X est X  algorithms in the literature [4, 18].
From an applications point of view, we support the power law decay assumption of our theoretical analysis by demon-strating that several real-world data-sets have leverage scores following such decays. We further run several experiments on synthetic and real data sets, and compare deterministic leverage score sampling with the state of the art algorithms for the CSSP. In most experiments, the deterministic algo-rithm obtains tight low-rank approximations, and is shown to perform similar, if not better, than the state of the art.
We use A , B ,... to denote matrices and a , b ,... to denote column vectors. I n is the n  X  n identity matrix; 0 m  X  n m  X  n matrix of zeros; e i belongs to the standard basis (whose dimensionality will be clear from the context). Let C =[ a i 1 ,..., a i c ] 2 R m  X  c contain c columns of A .Wecan equivalently write C = AS , where the sampling matrix is S =[ e i 1 ,..., e i c ] 2 R n  X  c . We define the Frobenius and the spectral norm of a matrix as k A k 2 F =
In this section, we describe the details of the deterministic leverage score sampling algorithm. In Section 3, we state our approximation guarantees. In the remaining of the text, given a matrix A of rank  X  , we assume that the X  X arget rank X  is k&lt;  X  . This means that we wish to approximate A using a subset of c k of its columns, such that the resulting matrix has an error close to that of the best rank-k approximation.
The deterministic leverage score sampling algorithm can be summarized in the following three steps: Step 1: Obtain V k , the top-k right singular vectors of A . This can be carried by simply computing the singular value decomposition (SVD) of A in O (min { m, n } mn )time. Step 2: Calculate the leverage scores ` ( k ) i . For simplicity, we assume that ` ( k ) i are sorted in descending order, hence the columns of A have the same sorting as well. 1 Otherwise, one needs to sort them in O ( n log n )time-cost. Step 3: Output the c columns of A that correspond to the largest c leverage scores ` ( k ) i such that their sum is more than  X  . This ensures that the selected columns have accumulated  X  X nergy X  at least  X  . In this step, we have to carefully pick  X  , our stopping threshold . This parameter es-sentially controls the quality of the approximation .
In Section 7, we provide some guidance on how the stop-ping parameter  X  should be chosen. Note that, if  X  is such that c&lt;k , we force c = k . This is a necessary step that pre-vents the error in the approximation from  X  X lowing up X  (see Section 7). The exact steps are given in Algorithm 1. Algorithm 1 LeverageScoresSampler ( A ,k,  X  ) 1: Compute V k 2 R n  X  k (top k right sing. vectors of A ) 3: Find index c 2 { 1 ,...,n } such that: 4: If c&lt;k, set c = k .

Algorithm 1 requires O (min { m, n } mn ) arithmetic oper-ations. In the full version of this paper [28], we discuss modifications that improve the running time.
Our main technical innovation is a bound on the approx-imation error of Algorithm 1 in regard to the CSSP; the proof of the following theorem can be found in Section 6.
Theorem 2. Let  X  = k " , for some " 2 (0 , 1) ,andlet S be the n  X  c output sampling matrix of Algorithm 1. Then, for C = AS and  X  = { 2 , F } ,wehave k A CC  X  A k 2  X  &lt; (1 " ) 1  X  k A A k k 2  X  .
 Choosing " 2 (0 , 1 / 2) implies (1 " ) 1  X  1+2 " and, hence, we have a relative-error approximation:
Algorithm 1 extracts at least c k columns of A .How-ever, an upper bound on the number of output columns c is not immediate. We study such upper bounds below.

From Theorem 2, it is clear that the stopping parameter  X  = k " directly controls the number of output columns c . This number, extracted for a specific error requirement " , depends on the decay of the leverage scores. For example, if the leverage scores decay fast, then we intuitively expect P
Let us for example consider a case where the leverage scores follow an extremely fast decay: Then, in this case puts the c =2 k columns of A that correspond to the 2 k largest leverage scores. Due to Theorem 2, this subset of columns C 2 R n  X  2 k comes with the following guarantee: Hence, from the above example, we expect that, when the leverage scores decay fast, a small number of columns of A will o  X  er a good approximation of the form CC  X  A .
However, in the worst case Algorithm 1 can output a num-ber of columns c that can be as large as  X  ( n ). To highlight this subtle point, consider the case where the leverage scores are uniform ` ( k ) i = k n . Then, one can easily observe that if we want to achieve an error of " according to Theorem 2, we have to set  X  = k " . This directly implies that we need to sample c&gt; ( n/k )  X  columns. Hence, if " = o (1) , then, Hence, for " ! 0 we have c ! n, which makes the result of Theorem 2 trivial.

We argued above that when the leverage scores decay is  X  X ast X  then a good approximation is to be expected with a  X  X mall X  c. We make this intuition precise below 2 . The next theorem considers the case where the leverage scores follow a power-law decay; the proof can be found in Section 6.
Theorem 3. Let the leverage scores follow a power-law decay with exponent  X  k =1+  X  ,for  X  &gt; 0 : Then, if we set the stopping parameter to  X  = k " , for some " with 0 &lt; " &lt; 1 , the number of sampled columns in C = AS that Algorithm 1 outputs is and C achieves the following approximation error k A CC  X  A k 2  X  &lt; 1
We compare the number of chosen columns c in Algorithm 1 to the number of columns chosen in the randomized lever-age scores sampling case [13]. The algorithm of [13] requires
We chose to analyze in detail the case where the leverage scores follow a power law decay; other models for the lever-age scores, example, exponential decay, are also interesting, and will be the subject of the full version of this work. columns for a relative-error bound with respect to the Frobe-nius error in the CSSP: Assuming leverage scores follow a power-law decay, Algo-rithm 1 requires fewer columns for the same " when: where C is an absolute constant. Hence, under the power law decay, Algorithm 1 o  X  ers provably a matrix approximation similar or better than [13].

Let us now compare the performance of Algorithm 1 with [4], which are the current state of the art for the CSSP. Theorem 1.5 in [4] provides a randomized algorithm which selects columns in C such that holds in expectation. This result is in fact optimal, up to a constant 2, since there is a lower bound indicating that such a relative error approximation is not possible unless c = k/ " (see Section 9.2 in [4]). The approximation bound of Algo-rithm 1 is indeed better than the upper/lower bounds in [4] for any  X  &gt; 1. We should note here that the lower bound in [4] is for general matrices; however, the upper bound of Theorem 3 is applied to a specific class of matrices whose leverage scores follow a power law decay.

Next, we compare the spectral norm bound of Theorem 3 to the spectral norm bound of Theorem 1.1 in [4], which in-dicates that there exists a deterministic algorithm selecting c&gt;k columns with error This upper bound is also tight, up to constants, since [4] pro-vides a matching lower bound. Notice that a relative error upper bound requires c =  X  ( n/ (1 + " )) in the general case. However, under the power law assumption in Theorem 3, we provide such a relative error bound with asymptotically fewer columns. To our best knowledge, fixing  X  to a con-stant, this is the first relative-error bound for the spectral norm version of the CSSP with c =poly( k, 1 / " ) columns.
In this section, we first provide evidence that power law decays are prevalent in real-world data sets. Then, we in-vestigate the empirical performance of Algorithm 1 on real and synthetic data sets.

Our experiments are not meant to be exhaustive; however, they provide clear evidence that: ( i ) the leverage scores of real world matrices indeed follow  X  X harp X  power law decays; and ( ii ) deterministic leverage score sampling in such ma-trices is particularly e  X  ective.
We demonstrate the leverage score decay behavior of many real-world data sets. These range from social networks and product co-purchasing matrices to document-term bag-of-words data sets, citation networks, and medical imaging leverage scores that decay slowly (  X  k &lt; 1 ). samples. Their dimensions vary from thousands to millions of variables. The data-set description is given in Table 1.
In Figure 1, we plot the top 1 , 000 leverage scores ex-tracted from the matrix of the right top-k singular vectors V k . In all cases we set k =10. 3 For each dataset, we plot
We performed various experiments for larger k , e.g., k =30 or k = 100 (not shown due to space limitations). We found that as we move towards higher k , we observe a  X  X moothing X  a fitting power-law curve of the form  X  x  X  k , where  X  k the exponent of interest.

We can see from the plots that a power law indeed seems to closely match the behavior of the top leverage scores. What is more interesting is that for many of our data sets we observe a decay exponent of  X  k &gt; 1: this is the regime where deterministic sampling is expected to perform well. It of the speed of decay. This is to be expected, since for the case of k = rank( A ) all leverage scores are equal. seems that these sharp decays are naturally present in many real-world data sets.

We would like to note that as we move to smaller scores (i.e., after the 10 , 000-th score), we empirically observe that the leverage scores tail usually decays much faster than a power law. This only helps the bound of Theorem 2.
In this subsection, we are interested in understanding the performance of Algorithm 1 on matrices with (i) uniform and (ii) power-law decaying leverage scores.

To generate matrices with a prescribed leverage score de-cay, we use the implementation of [20]. Let V k 2 R n  X  k denote the matrix we want to construct, for some k&lt;n . Then, [20] provides algorithms to generate tall-and-skinny orthonormal matrices with specified row norms (i.e., lever-age scores). Given the V k that is the output of the matrix generation algorithm in [20], we run a basis completion al-gorithm to find the perpendicular matrix V ? k 2 R n  X  ( n k ) such that V T k V ? k = 0 k  X  ( n k ) . Then, we create an n  X  n ma-trix V =[ V k V ? k ] where the first k columns of V are the columns of V k and the rest n k columns are the columns of V ? k ; hence, V is a full orthonormal basis. Finally we gen-erate A 2 R m  X  n as A = U  X  V T ; where U 2 R m  X  m is any orthonormal matrix, and  X  2 R m  X  n any diagonal matrix with min { m, n } positive entries along the main diagonal. Therefore, A = U  X  V T is the full SVD of A with leverage scores equal to the squared ` 2 -norm of the rows of V k . In our experiments, we pick U as an orthonormal basis for an m  X  m matrix where each entry is chosen i.i.d. from the Gaussian distribution. Also,  X  contains min { m, n } positive entries (sorted) along its main diagonal, where each entry was chosen i.i.d. from the Gaussian distribution.
We set the number of rows to m = 200 and the number of columns to n = 1000 and construct A = U  X  V T 2 R m  X  n as described above. The row norms of V k are chosen as follows: First, all row norms are chosen equal to k/n , for some fixed k . Then, we introduce a small perturbation to avoid singularities: for every other pair of rows we add 2 N (0 , 1 / 100) to a row norm and subtract the same from the other row norm  X  hence the sum of ` ( k ) i equals to k .
We set k to take the values { 5 , 10 , 50 , 100 } and for each k we choose: c = { 1 , 2 ,..., 1000 } . We present our find-ings in Figure 2, where we plot the relative error achieved k A A k k 2 2 , where the n  X  c matrix C contains the first c columns of A that correspond to the k largest leverage scores of V k , as sampled by Algorithm 1. Then, the leftmost verti-cal cyan line corresponds to the point where k = c , and the rightmost vertical magenta line indicates the point where the c sampled columns achieve an error of k A A k k 2 2 , where A is the best rank-k approximation.

In the plots of Figure 2, we see that as we move to larger values of k , if we wish to achieve an error of k A CC  X  A k k A A k k 2 2 , then we need to keep in C , approximately almost half the columns of A . This agrees with the uniform scores example that we showed earlier in Subsection 3.1. However, we observe that Algorithm 1 can obtain a moderately small relative error, with significantly smaller c . See for example the case where k = 100; then, c  X  200 sampled columns su ce for a relative error approximately equal to 2, i.e., k A CC  X  A k 2 2  X  2  X  k A A k k 2 2 . This indicates that our analysis could be loose in the general case.
In this case, our synthetic eigenvector matrices V k have leverage scores that follow a power law decay. We choose two power-law exponents:  X  k =0 . 5 and  X  k =1 . 5. Observe that the latter complies with Theorem 3, that predicts the near optimality of leverage score sampling under such decay.
In the first row of Figure 3, we plot the relative error vs. the number of output columns c of Algorithm 1 for  X  k =0 . 5. Then, in the second row of Figure 3, we plot the relative error vs. the number of output columns c of Algorithm 1 for  X  k =1 . 5. The blue line represents the relative error in terms of spectral norm. We can see that the performance of Algorithm 1 in the case of the fast decay is surprising: c  X  1 . 5  X  k su ces for an approximation as good as of that of the best rank-k approximation. This confirms the approximation performance in Theorem 3.
We will now compare the proposed algorithm to state of the art approaches for the CSSP, both for  X  = 2 and  X  = F. We report results for the errors k A CC  X  A k 2  X  / k A A k k 2  X  . A comparison of the running time complexity of those algorithms is out of the scope of our experiments.
Table 2 contains a brief description of the datasets used in our experiments. We employ the datasets used in [15], which presents exhaustive experiments for matrix approximations obtained through randomized leverage scores sampling. We compare Algorithm 1 against three methods for the CSSP. First, the authors in [4] present a near-optimal de-terministic algorithm, as described in Theorem 1.2 in [4]. Given A ,k&lt; rank( A ) and c&gt;k , the proposed algorithm selects  X  c  X  c columns of A in C 2 R m  X   X  c with
Second, in [16], the authors present a deterministic piv-oted QR algorithm such that: This bound was proved in [17]. In our tests, we use the qr (  X  ) built-in Matlab function, where one can select c = k columns of A as: where A = QR , Q 2 R m  X  n contains orthonormal columns, R 2 R n  X  n is upper triangular, and  X  is a permutation in-formation vector such that A : ,  X  = QR .

Third, we also consider the randomized leverage-scores sampling method with replacement, presented in [13]. Ac-cording to this work and given A ,k &lt; rank( A ) , and c =  X  ( k log k ), the bound provided by the algorithm is which holds only with constant probability. In our exper-iments, we use the software tool developed in [20] for the randomized sampling step.

We use our own Matlab implementation for each of these approaches. For [13], we execute 10 repetitions and report the one that minimizes the approximation error.
Table 3 contains a subset of our results; a complete set is reserved for an extended version of this work. We ob-serve that the performance of Algorithm 1 is particularly ap-pealing: It is almost as good as randomized leverage scores sampling in almost all cases -when randomized sampling is better, the di  X  erence is on the first or second decimal digit.
Figure 4 shows the leverage scores for the three matri-ces used in our experiments. Although the decay for the first data sets does not fit a  X  X harp X  power law as it is re-quired in Theorem 3, the performance of the algorithm is still competitive in practice. Interestingly, we do observe good performance for the third data set (Enron). For this case, the power law decay fits the decay profile needed to establish the near optimality of Algorithm 1.
One of the first deterministic results regarding the CSSP goes back to the seminal work of Gene Golub on pivoted QR factorizations [16]. Similar algorithms have been devel-oped in [16, 19, 10, 11, 17, 35, 32, 3, 27]; see also [6] for a recent survey. The best of these algorithms is the so-called Strong Rank-revealing QR (Strong RRQR) algorithm in [17]: Given A , c = k, and constant f 1 , Strong RRQR requires O ( mnk log f n ) arithmetic operations to find k columns of A in C 2 R m  X  k that satisfy k A CC  X  A k 2  X 
As discussed in Section 1, [21] suggests column sampling with the largest corresponding leverage scores. A related result in [35] suggests column sampling through selection over V T k with Strong RRQR. Notice that the leverage scores sampling approach is similar, but the column selection is based on the largest Euclidean norms of the columns of V T
From a probabilistic point of view, much work has fol-lowed the seminal work of [14] for the CSSP. [14] introduced the idea of randomly sampling columns based on specific probability distributions. [14] use a simple probability dis-tribution where each column of A is sampled with probabil-ity proportional to its Euclidean norm. The approximation bound achieved, which holds only in expectation, is Figure 4: The plots are for k =10 and are in loga-rithmic scale. The exponent is listed on each figure as  X  . The leverage scores are plotted with a red  X  marker, and the fitted curves are denoted with a solid blue line. [13] improved upon the accuracy of this result by using a distribution over the columns of A where each column is sampled with probability proportional to its leverage score. From a di  X  erent perspective, [12, 18] presented some opti-mal algorithms using volume sampling. [4] obtained faster optimal algorithms while [7] proposed optimal algorithms that run in input sparsity time.

Another line of research includes row-sampling algorithms for tall-and-skinny orthonormal matrices, which is relevant to our results: we essentially apply this kind of sampling to the rows of the matrix V k from the SVD of A . See Lemma 5 in the Section 6 for a precise statement of our result. Similar results exist in [1]. We should also mention the work in [37], which corresponds to a derandomization of the randomized sampling algorithm in [13].
Before we proceed, we setup some notation and defini-tions. For any two matrices A and B with appropriate dimensions, k A k 2  X  X  A k F  X  k A k F k B k 2 , and k AB k F  X  X  A k 2 k B k F . k A k  X  an expression holds for both  X  =2 , F. The thin (compact) Singular Value Decomposition (SVD) of a matrix A 2 R m  X  n with rank( A )=  X  is: with singular values 1 ( A ) ... k ( A ) k +1 ( A ) ... ( A ) &gt; 0. The matrices U A and V A contain the left and right singular vectors, respectively. It is well-known that A k A X k  X  over all matrices X 2 R m  X  n of rank at most k  X  rank( A ). The best rank-k approximation to A satisfies k A A k k 2 = k +1 ( A ) and k A A k k 2 F = A  X  denotes the Moore-Penrose pseudo-inverse of A .Let B 2 R m  X  n ( m  X  n ) and A = BB T 2 R m  X  m ; then, for all i =1 ,...,m , i ( A )= 2 i ( B ) is the i -th eigenvalue of A . To prove Theorem 2, we will use the following result. Lemma 4. [Eqn. 3.2, Lemma 3.1 in [4]] Consider A = AZZ T + E 2 R m  X  n as a low-rank matrix factorization of A ,with Z 2 R n  X  k , and Z T Z = I k .Let S 2 R n  X  c ( c k )be any matrix such that Let C = AS 2 R m  X  c .Then,for  X  =2 , F: We will also use the following novel lower bound on the smallest singular value of the matrix V k , after deterministic selection of its rows based on the largest leverage scores.
Lemma 5. Repeat the conditions of Theorem 2. Then,
Proof. We use the following perturbation result on the sum of eigenvalues of symmetric matrices.

Lemma 6. [Theorem 2.8.1; part (i) in [9]] Let X and Y be symmetric matrices of order k and, let 1  X  i, j  X  n with i + j  X  k +1 .Then, Let S 2 R n  X  c sample c columns from A with c k . Similarly, let  X  S 2 R n  X  ( n c ) sample the rest n c columns from A . Hence, Let in Lemma 6. Notice that i + j  X  k +1 , and k ( X + Y )=1; hence: Replacing  X  = k " and k ( V T k SS T V k )= 2 k ( V T k S ) con-cludes the proof.

The proof of Theorem 2 is a straightforward combina-tion of the Lemmas 4 and 5. First, by picking Z = V k in Lemma 4 we obtain: In the above, we used the facts that and the spectral norm of the sampling matrix S equals one. Also, we used that rank( V T k S )= k , which is implied from Lemma 5. Next, via the bound in Lemma 5:
Let  X  k =1+  X  for some  X  &gt; 0. We assume that the leverage scores follow a power law decay such that: ` ( k ) ` columns such that ber of columns c required to achieve an " := k  X  approxi-mation in Theorem 2. To this end, we use the extreme case P
For our analysis, we use the following well-known result.
Proposition 7. [Integral test for convergence] Let f (  X  ) 0 be a function defined over the set of positive reals. Fur-thermore, assume that f (  X  ) is monotone decreasing. Then, over the interval [ j, . . . , J ] for j, J positive integers.
In our case, consider f ( i )= 1 i 1+  X  . By definition of the leverage scores, we have: By construction, we collect c leverage scores such that k  X  = " . This leads to: To bound the quantity on the right hand side, we observe where the first inequality is due to the right hand side of the integral test and the third inequality is due to 1 + P The above lead to the following two cases: if we have: whereas in the case where we get
In the proof of Theorem 2, we require that For this condition to hold, the sampling matrix S should preserve the rank of V T k in V T k S , i.e., choose  X  such that rank( V T k S )= k .

Failing to preserve the rank of V T k has immediate impli-cations for the CSSP. To highlight this, let A 2 R m  X  n of rank k&lt; min { m, n } with SVD A = U k  X  k V T k . Further, as-sume that the k th singular value of A is arbitrary large, i.e., ( A ) !1 . Also, let rank( V T S )= &lt;k and C = AS . Then, The second equality is due to the fact that both spectral and Frobenius norms are invariant to unitary transforma-tions. In the third equality, we used the fact that ( WZ ) Z
W  X  if W T W is the identity matrix. Then, set X =  X  k V T k S 2 R k  X  c where rank( X )= . Using this notation, let U X 2 R m  X  be any orthonormal basis for span ( X ). Ob-serve U X U T X = XX  X  . The last inequality is due to U X being an m  X  m diagonal matrix with ones along its main diagonal and the rest zeros. Thus, we may conclude that for this A :
We provided a rigorous theoretical analysis of an old and popular deterministic feature selection algorithm from the statistics literature [21]. Although randomized algorithms are often easier to analyze, we believe that deterministic al-gorithms are simpler to implement and explain, hence more attractive to practitioners and data analysts.

One interesting path for future research is understand-ing the connection of this work with the so-called  X  X pectral graph sparsification problem X  [31]. In that case, edge se-lection in a graph is implemented via randomized leverage scores sampling from an appropriate matrix (see Theorem 1 in [31]). Note that in the context of graph sparsification, leverage scores correspond to the so-called  X  X   X  ective resis-tances X  of the graph. Can deterministic e  X  ective resistances sampling be rigorously analyzed? What graphs have e  X  ec-tive resistances following a power law distribution? [1] H. Avron and C. Boutsidis. Faster subset selection for [2] K. Bache and M. Lichman. UCI machine learning [3] C. H. Bischof and G. Quintana-Ort  X  X . Computing [4] C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near [5] C. Boutsidis, M. W. Mahoney, and P. Drineas. [6] C. Boutsidis, M. W. Mahoney, and P. Drineas. An [7] C. Boutsidis and D. Woodru  X  . Optimal cur matrix [8] M. E. Broadbent, M. Brown, K. Penner, I. Ipsen, and [9] A. A. E. Brouwer and W. H. Haemers. Spectra of [10] T. F. Chan and P. C. Hansen. Low-rank revealing [11] S. Chandrasekaran and I. C. F. Ipsen. On [12] A. Deshpande and L. Rademacher. E cient volume [13] P. Drineas, M. W. Mahoney, and S. Muthukrishnan. [14] A. Frieze, R. Kannan, and S. Vempala. Fast [15] A. Gittens and M. W. Mahoney. Revisiting the [16] G. H. Golub. Numerical methods for solving linear [17] M. Gu and S. Eisenstat. E cient algorithms for [18] V. Guruswami and A. K. Sinop. Optimal [19] Y. P. Hong and C. T. Pan. Rank-revealing [20] I. C. Ipsen and T. Wentworth. The e  X  ect of coherence [21] I. Jolli  X  e. Discarding variables in a principal [22] I. Jolli  X  e. Discarding variables in a principal [23] I. Jolli  X  e. Principal Component Analysis . Springer; [24] J. Kunegis. Konect: the koblenz network collection. In [25] J. Leskovec. Snap stanford network analysis project. [26] M. W. Mahoney and P. Drineas. Cur matrix [27] C. T. Pan. On the existence and computation of [28] D. Papapailiopoulos, A. Kyrillidis, and C. Boutsidis. [29] P. Paschou, E. Ziv, E. G. Burchard, S. Choudhry, [30] M. Rudelson and R. Vershynin. Sampling from large [31] N. Srivastava and D. Spielman. Graph sparsifications [32] G. Stewart. Four algorithms for the e cient [33] J. Sun, Y. Xie, H. Zhang, and C. Faloutsos. Less is [34] H. Tong, S. Papadimitriou, J. Sun, P. S. Yu, and [35] E. Tyrtyshnikov. Mosaic-skeleton approximations. [36] R. Zafarani and H. Liu. Social computing data [37] A. Zouzias. A matrix hyperbolic cosine algorithm and
