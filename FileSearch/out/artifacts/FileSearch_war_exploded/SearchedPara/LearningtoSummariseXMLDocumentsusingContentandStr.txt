 Documents formatted in eXtensible Markup Language (XML) are becoming increasingly available in collections of various document types. In this paper , we present an approach for the summarisation of XML documents. The novelty of this approach lies in that it is based on features not only from the content of documents, but also from their logical structure. We follo w a machine learning lik e, sentence extraction-based summarisation technique. To find which features are more effecti ve for producing summaries this approach vie ws sentence extraction as an ordering task. We evaluated our summarisation model using the INEX dataset. The results demon-strate that the inclusion of features from the logical structure of documents increases the effecti veness of the summariser , and that the learnable system is also effecti ve and well-suited to the task of summarisation in the conte xt of XML documents.
 H.4.m [ Inf ormation Systems ]: [Miscellaneous-T ext Summarisa-tion]; I.2.6 [ Computing Methodologies ]: [Learning-P arameter] Algorithms, Experimentation, Performance Summarisation, machine learning, ranking algorithms, XML doc-uments, content and structure features
With the gro wing availability of on-line text resources, it has be-come necessary to pro vide users with systems that obtain answers to queries in a manner which is both efficient and effecti ve. Single document text summarisation (SDS) can be coupled with con ven-tional search engines and help users to evaluate the rele vance of documents [5] for pro viding answers to their queries.

In this paper we follo w a summarisation approach that is based on the machine learning (ML) paradigm and investigate the effec-tiveness of an XML summarisation approach by combining struc-tural and content features to extract sentences for summaries. Lik e most pre vious work on ML for summarisation we rely on super -vised learning, where a set of training documents and their extract summaries are available. We explore an ML approach for SDS based on the Area under the ROC curv e (A UC). The main ratio-nale of this approach is to automatically combine dif ferent features, each being a numerical representation of a given extraction crite-rion. The summariser learns how to best combine sentence features based on its effecti veness at assigning higher scores to summary sentences than to non-summary ones.

The contrib utions of this work are therefore twofold: first, we propose and justify the effecti veness of a new algorithm that opti-mises the AUC ordering criterion, instead of the mostly used clas-sification error criterion in ML approaches for SDS [1], and sec-ond, we investigate the summarisation of XML documents by tak-ing into account features relating both to the content and the logical structure of the documents.
In order to evaluate which of the classification or the AUC cri-teria are better suited to the SDS task, we used in our experiments the same logistic model in both frame works.
 Logistic model for classification In the particular case of a logistic classifier , one mak es the follo w-ing assumption on the form of the posterior probability of the class rele vant given a sentence s = ( s 1 ; :::; s n ) represented by a vector of scores: p ( relevant j s ) = 1 of the feature combination = ( 1 ; :::; n ) can then be learnt by maximizing the binomial log-lik elihood:
L ( D ; ) = 1 jS j X where D is the set of training documents, S 1 and S 1 are respec-tively the set of rele vant and irrele vant sentences in the training set and jS j is the number of sentences in the aforesaid set. Logistic model for AUC The logistic assumption, adapted to AUC, becomes p ( relevant j by: L
A ( D ; ) = Follo wing [3], one can asymptotically sho w that the population minimizers of the expected binomial log-lik elihood for AUC and E h e H ( s 0 ) H ( s ) i coincide. This is an interesting finding which re-inforces the duality between classification and AUC.
In this paper , we tak e the logical structure of documents into ac-count when producing summaries, as well as the content, and we learn an effecti ve combination of features for summarisation. The structural features we use in our approach are (i) the depth of the el-ement in which the sentence is contained (e.g. section, subsection, subsubsection, etc.), (ii) the sibling number of the element in which the sentence is contained (e.g. 1st, middle, last), (iii) the number of sibling elements of the element in which the sentence is contained and (iv) the position in the element of the paragraph in which the sentence is contained (e.g. first, or not).

Our basic content-only query (COQ) comprises terms in the title of the document ( Title query), as well as the title keyw ords aug-mented by the most frequent terms in the document (up to 10 such terms) ( Title-MFT query). The importance of title terms for SDS can also be extended to components of finer granularity (e.g. sec-tions, subsections, etc.), by using the title of the document to find rele vant sentences within any component, or, where appropriate, by using meaningful titles of components.

Since the Title query may be very short, we have also emplo yed query-e xpansion techniques such as Local Conte xt Analysis (LCA) or thesaurus expansion methods (i.e. WordNet), we also included two queries using word clusters. This is another source of infor -mation about the rele vance of sentences to summaries. It is a more conte xtual approach compared to the title-based queries, as it seeks to tak e adv antage of the co-occurrence of terms within sentences all over the corpus, as opposed to the local information pro vided by the title-based queries. We used the cosine measure in order to compute a preliminary score between any sentence of a document and these generic queries. The scoring measure doubles the cosine scoring of sentences containing acron yms or cue-terms.
In our experiments, we used the INEX test collection and ran 3 algorithms -a logistic model optimising the ordering AUC metric using an iterati ve scaling scheme, a logistic classifier optimising the classification binomial log-lik elihood criteria (1) and the Rank-Boost algorithm [2]. To measure the effect of structure features we have learnt the best learning algorithm using COQ features alone and COQ features with the aforementioned structure features.
To obtain sentence-based extract summaries for all articles in both datasets, for training and evaluation purposes, we applied an algorithm proposed by Marcu [4] in order to generate extracts from the abstracts. This algorithm has sho wn a high degree of correlation to sentence extracts produced by humans. We therefore evaluate the effecti veness of our learning algorithm on the basis of how well it matches the automatic extracts.

In figure 1 we present the precision and recall graph that we ob-tained through the combination of content and structure features for the INEX dataset when using the three learning algorithms.
A first, non-surprising, result is that the combination of features by learning outperforms each feature alone. The results also sho w that the two ordering algorithms are more effecti ve than the logistic classifier . Ho we ver, the comparison between the AUC algorithm and the logistic classifier leads to the conclusion that an ordering criterion is better suited to SDS than a classification criterion. When comparing the two ordering algorithms, we see that the AUC algorithm slightly outperforms the RankBoost algorithm for high recall values. Since both ordering algorithms optimise the same criteria, the dif ference in performance can be explained by the class of functions that each algorithm learns. The RankBoost algorithm outputs a nonlinear combination of the features while Figur e 1: Pr ecision-Recall cur ves at 10% compr ession ratio for the lear ning effects on INEX dataset. Each point repr esents the mean perf ormance for 10 cross-v alidation folds. The bars sho w standard deviations for the estimated perf ormance. with the AUC algorithm we obtain a linear combination of these features. As the space of features is small, the non-linear Rank-Boost model has low bias and high variance and hence attempts to overfit the data.
The results that we presented in the pre vious section are encour -aging in relation to our two main moti vations: a novel learning algorithm for SDS, and the inclusion of structure, in addition to content, features for the summarisation of XML documents. The ultimate aim of our approach for the summarisation of XML doc-uments is to produce summaries for components at any level of granularity (e.g. section, subsection, etc.). The content and struc-ture features that we used can be applied to any level of granularity . In particular , the most effecti ve content (expanded concepts with word clusters and projected concepts on word clusters), and struc-ture features (depth of element and position of paragraph in the el-ement), can be applied to various granularity levels within an XML tree. By looking at the results of this study as a whole, we can say that the work presented here achie ved its main aim, to effecti vely summarise XML documents by combining content and structure features through using novel machine learning approaches. This work has howe ver a greater impact, as we belie ve that it can be applied to datasets containing documents of other types. The avail-ability of XML data will continue to increase as, for example, XML is becoming the W3C standard for representing documents (e.g. in digital libraries where content can be of any type). The availability of intelligent summarisation approaches for XML data with there-fore become increasingly important, and we belie ve that this work has pro vided a step towards this direction.
