 Extensive games are a powerful model of sequential decision-making with imperfect information, subsuming finite-horizon MDPs, finite-horizon POMDPs, and perfect information games. The past few years have seen dramatic algorithmic improvements in solving, i.e., finding an approximate Nash equilibrium, in two-player, zero-sum extensive games. Multiple techniques [1, 2] now exist for solving games with up to 10 12 game states, which is about four orders of magnitude larger than the previous state-of-the-art of using sequence-form linear programs [3].
 Counterfactual regret minimization (CFR) [1] is one such recent technique that exploits the fact that the time-averaged strategy profile of regret minimizing algorithms converges to a Nash equilibrium. The key insight is the fact that minimizing per-information set counterfactual regret results in min-imizing overall regret. However, the vanilla form presented by Zinkevich and colleagues requires the entire game tree to be traversed on each iteration. It is possible to avoid a full game-tree traver-sal. In their accompanying technical report, Zinkevich and colleagues discuss a poker-specific CFR variant that samples chance outcomes on each iteration [4]. They claim that the per-iteration cost reduction far exceeds the additional number of iterations required, and all of their empirical studies focus on this variant. The sampling variant and its derived bound are limited to poker-like games where chance plays a prominent role in the size of the games. This limits the practicality of CFR minimization outside of its initial application of poker or moderately sized games. An additional disadvantage of CFR is that it requires the opponent X  X  policy to be known, which makes it unsuit-able for online regret minimization in an extensive game. Online regret minimization in extensive games is possible using online convex programming techniques, such as Lagrangian Hedging [5], but these techniques can require costly optimization routines at every time step.
 In this paper, we present a general framework for sampling in counterfactual regret minimization. We define a family of Monte Carlo CFR minimizing algorithms (MCCFR), that differ in how they sample the game tree on each iteration. Zinkevich X  X  vanilla CFR and a generalization of their chance-sampled CFR are both members of this family. We then introduce two additional members of this family: outcome-sampling , where only a single playing of the game is sampled on each iteration; and external-sampling , which samples chance nodes and the opponent X  X  actions. We show that under a reasonable sampling strategy, any member of this family minimizes overall regret, and so can be used for equilibrium computation. Additionally, external-sampling is proven to require only a constant-factor increase in iterations yet achieves an order reduction in the cost per iteration, thus resulting an asymptotic improvement in equilibrium computation time. Furthermore, since outcome-sampling does not need knowledge of the opponent X  X  strategy beyond samples of play from the strategy, we empirically by using them to compute approximate equilibria in a variety of games. An extensive game is a general model of sequential decision-making with imperfect information. As with perfect information games (such as Chess or Checkers), extensive games consist primarily of a game tree: each non-terminal node has an associated player (possibly chance) that makes the deci-sion at that node, and each terminal node has associated utilities for the players. Additionally, game states are partitioned into information sets where a player cannot distinguish between two states in the same information set. The players, therefore, must choose actions with the same distribution at each state in the same information set. We now define an extensive game formally, introducing the notation we use throughout the paper.
 Definition 1 [6, p. 200] a finite extensive game with imperfect information has the following com-ponents: In this paper, we will only concern ourselves with two-player, zero-sum extensive games. Further-more, we will assume perfect recall , a restriction on the information partitions such that a player can always distinguish between game states where they previously took a different action or were previously in a different information set. 2.1 Strategies and Equilibria of a strategy for each player,  X  1 ,..., X  n . We let  X   X  i refer to the strategies in  X  excluding  X  i . Let  X   X  ( h ) be the probability of history h occurring if all players choose actions according to  X  . We  X  ( h ) is the contribution to this probability from player i when playing according to  X  . Let  X   X   X  i ( h ) be the product of all players X  contribution (including chance) except that of player i . For I  X  H ,  X  this notation, we can define the expected payoff for player i as u i (  X  ) = P h  X  Z u i ( h )  X   X  ( h ) . Given a strategy profile,  X  , we define a player X  X  best response as a strategy that maximizes their expected payoff assuming all other players play according to  X  . The best-response value for player i is the value of that strategy, b i (  X   X  i ) = max  X  0 approximation of a Nash equilibrium; it is a strategy profile  X  that satisfies If = 0 then  X  is a Nash Equilibrium : no player has any incentive to deviate as they are all playing best responses. If a game is two-player and zero-sum, we can use exploitability as a metric for determining how close  X  is to an equilibrium,  X  = b 1 (  X  2 ) + b 2 (  X  1 ) . 2.2 Counterfactual Regret Minimization Regret is an online learning concept that has triggered a family of powerful learning algorithms. To define this concept, first consider repeatedly playing an extensive game. Let  X  t i be the strategy used by player i on round t . The average overall regret of player i at time T is: information set I  X  X  i , for each a  X  A ( I ) , define: There is a well-known connection between regret, average strategies, and Nash equilibria. Theorem 1 In a zero-sum game, if R T i  X  X  1 , 2 }  X  , then  X   X  T is a 2 equilibrium. An algorithm for selecting  X  t i for player i is regret minimizing if player i  X  X  average overall regret (regardless of the sequence  X  t  X  i ) goes to zero as t goes to infinity. Regret minimizing algorithms in self-play can be used as a technique for computing an approximate Nash equilibrium. Moreover, an algorithm X  X  bounds on the average overall regret bounds the convergence rate of the approximation. Zinkevich and colleagues [1] used the above approach in their counterfactual regret algorithm (CFR). The basic idea of CFR is that overall regret can be bounded by the sum of positive per-information-set immediate counterfactual regret. Let I be an information set of player i . Define  X  ( I  X  a ) to be a strategy profile identical to  X  except that player i always chooses action a from information set I . Let Z I be the subset of all terminal histories where a prefix of the history is in the set I ; for z  X  Z I let z [ I ] be that prefix. Since we are restricting ourselves to perfect recall games z [ I ] is unique. Define counterfactual value v i (  X ,I ) as, The immediate counterfactual regret is then R T i, imm ( I ) = max a  X  A ( I ) R T i, imm ( I,a ) , where Let x + = max( x, 0) . The key insight of CFR is the following result.
 Theorem 2 [1, Theorem 3] R T i  X  P I  X  X  Using regret-matching 2 the positive per-information set immediate counterfactual regrets can be driven to zero, thus driving average overall regret to zero. This results in an average overall regret bound [1, Theorem 4]: R T i  X   X  u,i |I i | p | A i | / this bound, tightening it further, in Section 4.
 This result suggests an algorithm for computing equilibria via self-play, which we will refer to as vanilla CFR . The idea is to traverse the game tree computing counterfactual values using Equation 4. Given a strategy, these values define regret terms for each player for each of their information sets using Equation 5. These regret values accumulate and determine the strategies at the next iteration using the regret-matching formula. Since both players are regret minimizing, Theorem 1 applies and so computing the strategy profile  X   X  t gives us an approximate Nash Equilibrium. Since CFR only needs to store values at each information set, its space requirement is O ( |I| ) . However, as previously mentioned vanilla CFR requires a complete traversal of the game tree on each iteration, which prohibits its use in many large games. Zinkevich and colleagues [4] made steps to alleviate this concern with a chance-sampled variant of CFR for poker-like games. The key to our approach is to avoid traversing the entire game tree on each iteration while still having the immediate counterfactual regrets be unchanged in expectation . In general, we want to restrict the terminal histories we consider on each iteration. Let Q = { Q 1 ,...,Q r } be a set of subsets of Z , such that their union spans the set Z . We will call one of these subsets a block . On each iteration we will sample one of these blocks and only consider the terminal histories in that block. Let q j &gt; 0 be the probability of considering block Q j for the current iteration (where P r j =1 q j = 1 ). iteration. The sampled counterfactual value when updating block j is: Selecting a set Q along with the sampling probabilities defines a complete sample-based CFR algo-rithm. Rather than doing full game tree traversals the algorithm samples one of these blocks, and then examines only the terminal histories in that block.
 Suppose we choose Q = { Z } , i.e., one block containing all terminal histories and q 1 = 1 . In this case, sampled counterfactual value is equal to counterfactual value, and we have vanilla CFR. Suppose instead we choose each block to include all terminal histories with the same sequence of chance outcomes (where the probability of a chance outcome is independent of players X  actions as in poker-like games). Hence q j is the product of the probabilities in the sampled sequence of chance outcomes (which cancels with these same probabilities in the definition of counterfactual value) and we have Zinkevich and colleagues X  chance-sampled CFR.
 Sampled counterfactual value was designed to match counterfactual value on expectation. We show this here, and then use this fact to prove a probabilistic bound on the algorithm X  X  average overall regret in the next section.
 Lemma 1 E j  X  q j [  X  v i (  X ,I | j )] = v i (  X ,I ) Proof: Equation 8 follows from the fact that Q spans Z . Equation 9 follows from the definition of q ( z ) . This results in the following MCCFR algorithm. We sample a block and for each information regrets, and the player X  X  strategy on the next iteration applies the regret-matching algorithm to the accumulated regrets. We now present two specific members of this family, giving details on how the regrets can be updated efficiently.
 Outcome-Sampling MCCFR. In outcome-sampling MCCFR we choose Q so that each block contains a single terminal history, i.e.,  X  Q  X  X  , | Q | = 1 . On each iteration we sample one terminal history and only update each information set along that history. The sampling probabilities, q j must specify a distribution over terminal histories. We will specify this distribution using a sampling that q ( z ) &gt;  X  , thus ensuring Equation 6 is well-defined.
 traversed forward (to compute each player X  X  probability of playing to reach each prefix of the history,  X  ( h ) ) and backward (to compute each player X  X  probability of playing the remaining actions of the history,  X   X  i ( h,z ) ). During the backward traversal, the sampled counterfactual regrets at each visited information set are computed (and added to the total regret). One advantage of outcome-sampling MCCFR is that if our terminal history is sampled according to sampling MCCFR for online regret minimization. We would have to choose our own actions so that  X  i  X   X  t i , but with some exploration to guarantee q j  X   X  &gt; 0 . By balancing the regret caused by exploration with the regret caused by a small  X  (see Section 4 for how MCCFR X  X  bound depends upon  X  ), we can bound the average overall regret as long as the number of playings T is known in advance. This effectively mimics the approach taking by Exp3 for regret minimization in normal-form games [9]. An alternative form for Equation 10 is recommended for implementation. This and other implementation details can be found in the paper X  X  supplemental material or the appendix of the associated technical report [10]. External-Sampling MCCFR. In external-sampling MCCFR we sample only the actions of the opponent and chance (those choices external to the player). We have a block Q  X   X  Q for each pure strategy of the opponent and chance, i.e., , for each deterministic mapping  X  from I  X  I c  X  I
N \{ i } to A ( I ) . The block probabilities are assigned based on the distributions f c and  X   X  i , so q z consistent with  X  , that is if ha is a prefix of z with h  X  I for some I  X  I  X  i then  X  ( I ) = a . In practice, we will not actually sample  X  but rather sample the individual actions that make up  X  only iterates over i  X  N and for each doing a post-order depth-first traversal of the game tree, sampling actions at each history h where P ( h ) 6 = i (storing these choices so the same actions are sampled at all h in the same information set). Due to perfect recall it can never visit more than one history from the same information set during this traversal. For each such visited information set the sampled counterfactual regrets are computed (and added to the total regrets). Note that the summation can be easily computed during the traversal by always maintaining a weighted sum of the utilities of all terminal histories rooted at the current history. We now present regret bounds for members of the MCCFR family, starting with an improved bound for vanilla CFR that depends more explicitly on the exact structure of the extensive game. Let ~a i be a subsequence of a history such that it contains only player i  X  X  actions in that history, and let ~ A i be player i  X  X  action sequence up to that information set is ~a i . Define the M -value for player i of the game to be M i = P ~a being realized by some game. We can strengthen vanilla CFR X  X  regret bound using this constant, which also appears in the bounds for the MCCFR variants.
 Theorem 3 When using vanilla CFR for player i , R T i  X   X  u,i M i p | A i | / We now turn our attention to the MCCFR family of algorithms, for which we can provide probabilis-tic regret bounds. We begin with the most exciting result: showing that external-sampling requires only a constant factor more iterations than vanilla CFR (where the constant depends on the desired confidence in the bound).
 Theorem 4 For any p  X  (0 , 1] , when using external-sampling MCCFR, with probability at least 1  X  p , average overall regret is bounded by, R T i  X  1 + Although requiring the same order of iterations, note that external-sampling need only traverse a fraction of the tree on each iteration. For balanced games where players make roughly equal numbers meaning external-sampling MCCFR requires asymptotically less time to compute an approximate equilibrium than vanilla CFR (and consequently chance-sampling CFR, which is identical to vanilla CFR in the absence of chance nodes).
 Theorem 5 For any p  X  (0 , 1] , when using outcome-sampling MCCFR where  X  z  X  Z either  X   X  i ( z ) = 0 or q ( z )  X   X  &gt; 0 at every timestep, with probability 1  X  p , average overall regret is bounded by R T i  X  1 + The proofs for the theorems in this section can be found in the paper X  X  supplemental material and as an appendix of the associated technical report [10]. The supplemental material also presents a slightly complicated, but general result for any member of the MCCFR family, from which the two theorems presented above are derived. Table 1: Game properties. The value of | H | is in millions and |I| in thousands, and l = max h  X  H | h | . t , t os , and t es are the average wall-clock time per iteration 4 for vanilla CFR, outcome-sampling MCCFR, and external-sampling MCCFR. We evaluate the performance of MCCFR compared to vanilla CFR on four different games. Goof-spiel [11] is a bidding card game where players have a hand of cards numbered 1 to N , and take turns secretly bidding on the top point-valued card in a point card stack using cards in their hands. Our version is less informational: players only find out the result of each bid and not which cards were used to bid, and the player with the highest total points wins. We use N = 7 in our exper-iments. One-Card Poker [12] is a generalization of Kuhn Poker [13], we use a deck of size 500 . Princess and Monster [14, Research Problem 12.4.1] is a pursuit-evasion game on a graph, neither player ever knowing the location of the other. In our experiments we use random starting positions, a 4-connected 3 by 3 grid graph, and a horizon of 13 steps. The payoff to the evader is the number of steps uncaptured. Latent Tic-Tac-Toe is a twist on the classic game where moves are not disclosed until after the opponent X  X  next move, and lost if invalid at the time they are revealed. While all of these games have imperfect information and roughly of similar size, they are a diverse set of games, varying both in the degree (the ratio of the number of information sets to the number of histories) and nature (whether due to chance or opponent actions) of imperfect information. The left columns of Table 1 show various constants, including the number of histories, information sets, game length, and M-values, for each of these domains.
 We used outcome-sampling MCCFR, external-sampling MCCFR, and vanilla CFR to compute an approximate equilibrium in each of the four games. For outcome-sampling MCCFR we used an epsilon-greedy sampling profile  X  0 . At each information set, we sample an action uniformly ran-domly with probability and according to the player X  X  current strategy  X  t . Through experimentation we found that = 0 . 6 worked well across all games; this is interesting because the regret bound suggests  X  should be as large as possible. This implies that putting some bias on the most likely outcome to occur is helpful. With vanilla CFR we used to an implementational trick called pruning to dramatically reduce the work done per iteration. When updating one player X  X  regrets, if the other player has no probability of reaching the current history, the entire subtree at that history can be pruned for the current iteration, with no effect on the resulting computation. We also used vanilla CFR without pruning to see the effects of pruning in our domains.
 Figure 1 shows the results of all four algorithms on all four domains, plotting approximation quality Nodes touched is an implementation-independent measure of computation; however, the results are nearly identical if total wall-clock time is used instead. Since the algorithms take radically different amounts of time per iteration, this comparison directly answers if the sampling variants X  lower cost fixed game (and degree of confidence that the bound holds), the algorithms X  average overall regret is falling at the same rate, O (1 / performance will differ.
 The graphs show that the MCCFR variants often dramatically outperform vanilla CFR. For example, in Goofspiel, both MCCFR variants require only a few million nodes to reach  X  &lt; 0 . 5 where CFR the tightest theoretical computation-time bound, outperformed CFR and by considerable margins (excepting LTTT) in all of the games. Note that pruning is key to vanilla CFR being at all practical in these games. For example, in Latent Tic-Tac-Toe the first iteration of CFR touches 142 million nodes, but later iterations touch as few as 5 million nodes. This is because pruning is not possible Figure 1: Convergence rates of Vanilla CFR, outcome-sampled MCCFR, and external-sampled MC-CFR for various games. The y axis in each graph represents the exploitability of the strategies for the two players  X  (see Section 2.1). in the first iteration. We believe this is due to dominated actions in the game. After one or two traversals, the players identify and eliminate dominated actions from their policies, allowing these subtrees to pruned. Finally, it is interesting to note that external-sampling was not uniformly the best choice, with outcome-sampling performing better in Goofspiel. With outcome-sampling performing worse than vanilla CFR in LTTT, this raises the question of what specific game properties might favor one algorithm over another and whether it might be possible to incorporate additional game specific constants into the bounds. In this paper we defined a family of sample-based CFR algorithms for computing approximate equi-libria in extensive games, subsuming all previous CFR variants. We also introduced two sampling schemes: outcome-sampling, which samples only a single history for each iteration, and external-sampling, which samples a deterministic strategy for the opponent and chance. In addition to pre-senting a tighter bound for vanilla CFR, we presented regret bounds for both sampling variants, which showed that external sampling with high probability gives an asymptotic computational time improvement over vanilla CFR. We then showed empirically in very different domains that the re-duction in iteration time outweighs the increase in required iterations leading to faster convergence. There are three interesting directions for future work. First, we would like to examine how the algorithmic or theoretical improvements, as well as practical suggestions, such as how to choose a sampling policy in outcome-sampled MCCFR. Second, using outcome-sampled MCCFR as a general online regret minimizing technique in extensive games (when the opponents X  strategy is not known or controlled) appears promising. It would be interesting to compare the approach, in terms of bounds, computation, and practical convergence, to Gordon X  X  Lagrangian hedging [5]. Lastly, it seems like this work could be naturally extended to cases where we don X  X  assume perfect recall. Imperfect recall could be used as a mechanism for abstraction over actions, where information sets are grouped by important partial sequences rather than their full sequences. [1] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret mini-[2] Andrew Gilpin, Samid Hoda, Javier Pe  X  na, and Tuomas Sandholm. Gradient-based algorithms [3] D. Koller, N. Megiddo, and B. von Stengel. Fast algorithms for finding randomized strategies [4] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret min-[5] Geoffrey J. Gordon. No-regret algorithms for online convex programs. In In Neural Informa-[6] Martin J. Osborne and Ariel Rubinstein. A Course in Game Theory . MIT Press, 1994. [7] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equi-[8] D. Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Math-[9] Peter Auer, Nicol ` o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged [10] Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sam-[11] S. M. Ross. Goofspiel  X  the game of pure strategy. Journal of Applied Probability , 8(3):621 X  [12] Geoffrey J. Gordon. No-regret algorithms for structured prediction problems. Technical Report [13] H. W. Kuhn. Simplified two-person poker. Contributions to the Theory of Games , 1:97 X 103, [14] Rufus Isaacs. Differential Games: A Mathematical Theory with Applications to Warfare and
