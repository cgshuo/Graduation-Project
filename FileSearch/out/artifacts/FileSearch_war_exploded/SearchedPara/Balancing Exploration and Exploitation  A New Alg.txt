
Active machine learning algorithms are used when large numbers of unlabeled examples are available and getting labels for them is costly (e.g. requiring consulting a human expert). Many conventional active learning algorithms fo-cus on refining the decision boundary, at the expense of ex-ploring new regions that the current hypothesis misclassi-fies. We propose a new active learning algorithm that bal-ances such exploration with refining of the decision bound-ary by dynamically adjusting the probability to explore at each step. Our experimental results demonstrate improved performance on data sets that require extensive exploration while remaining competitive on data sets that do not. Our algorithm also shows significant tolerance of noise.
In active machine learning (also known as active sam-pling or selective sampling), the learning algorithm A has access to a large set U of unlabeled examples and some or-acle O that A can query to get the label of an individual ex-ample x  X  U ( x is then moved from U to a set L of labeled examples). As A  X  X  queries to O are assumed to be expensive (e.g. consulting a human expert), it is only feasible to label a small subset of U . Thus the goal is for A to actively se-lect examples from U such that a good hypothesis is learned while using as few (carefully chosen) labeled examples as possible. Applications of active learning include areas in which there is so much data available that it X  X  infeasible to manually label it all, so one needs to manually label a small subset to train a classifier. Successful applications of ac-tive learning include include text classification [20], image classification [12], speech recognition [7, 8], and software testing [3].

Many conventional active learning algorithms choose to label points that are near the decision boundary of the cur-rent hypothesis. This can work well if the active learner is aware of all the important regions of the instance space, i.e. there are no large  X  X ockets X  of examples that the learner X  X  hypothesis will misclassify since it hasn X  X  seen labeled ex-amples from them. Such active learners are good at  X  X x-ploitation X  (labeling examples near the boundary to refine it), but they do not conduct  X  X xploration X  (searching for large regions in the instance space that they would incor-rectly classify). An example of a problem requiring explo-ration is the exclusive OR problem (Figure 1(a)), where the negative examples are in the upper-left and lower-right re-gions (respectively numbered 1 and 4) and the positives are in the other two regions (numbered 2 and 3). If all the la-beled points are from regions 1 X 3 and none from 4, then an active learner that is not designed to explore (e.g. one that always chooses from near the decision boundary) may never discover that region 4 contains negatively-labeled examples and thus never adjust its hypothesis to accommodate it. The result is that all new negative examples that appear in region 4 would be misclassified, which could lead to high gener-alization error. However, one must not focus too much on exploration. Algorithms that do nothing but explore will re-quire many calls to the labeling oracle to refine its decision boundary.

Our algorithm addresses this problem by randomly choosing between exploration and exploitation at each round, and then receives feedback on how effective the ex-ploration phase was, measured by the change induced in the learned classifier when an exploratory example is labeled and added to the training set. Like a simple reinforcement learning algorithm, our active learner updates the probabil-ity of exploring in subsequent rounds based on the feedback it received. In this way, our algorithm is similar to Baram et al. X  X  [1]  X  COMB  X  algorithm, except that ours is simpler.
Figure 1. (a) An example of the exclusive OR problem, aka a 2 board data ( d =2 and n =4 ).

We found that in synthetic data with many regions that require exploration, our algorithm showed a distinct advan-tage over others in the literature, including COMB .Further, on data when extensive exploration was unnecessary, our al-gorithm performed competitively with the others despite its desire to explore. Finally, we discovered that even on data when extensive exploration was unnecessary, our algorithm showed improvements over the others when the labels were corrupted by noise.

In the next section we summarize related work in active learning. Then in Section 3 we describe our algorithm. Sec-tion 4 presents experimental results on noise-free and noisy data. Finally, we conclude in Section 5.
An active learner tries to select the most informative ex-ample from the unlabeled pool U . One basic idea is to select examples that effectively shrink the current version space (the set of hypotheses consistent with the training data). As discussed by Tong and Koller [20], such a heuris-tic would be probabilistically optimal if it could always ex-actly halve the version space each time A makes a query. Strong theoretical results using this idea are yielded by the Query by Committee algorithm [6, 19]. The  X  X losest sam-pling X  method [5, 18, 20] (sometimes called  X  X ncertainty sampling X ) can be thought of as a heuristic to shrink the version space. It greedily selects points that are closest to the current decision boundary. The intuition behind this is that points that are closest to the current decision boundary are points that A is most uncertain about. By labeling these, A can have better knowledge about the correctness of its decision. Tong and Koller explain why this method often works: in a large margin classifier such as a support vector machine (SVM), the current hypothesis lies approximately  X  X n the center X  of the version space and by choosing an ex-ample (a hyperplane in version space) that is closest to it, it also cuts the version space approximately in half. Although this method is elegant and easy to implement, focusing on examples near the decision boundary prevents exploration of the feature space for regions of examples that the current hypothesis misclassifies [1].

Another approach [6, 9, 11, 17] is to select examples that are helpful in building up confidence in low future er-ror. It is impossible to know the exact future error without knowing the target concept, but approximations make this method feasible. For example, Roy and McCallum [17] suggest to directly minimize the expected error on the dataset. One challenge to this approach is that a na  X   X ve im-plementation results in prohibitive computational complex-ity (though some heuristics are available to mitigate this). Further, a fair amount of initial labeled data is needed to establish a good approximation of the prior class distribu-tion in order for this method to work, and if labeled exam-ples are not chosen randomly, the class probability estima-tion may not be statistically valid. In line with this work, Nguyen and Smeulders [14] chose examples that have the largest contribution to the current expected error: they built the classifier based on centers of clusters and then propa-gated the classification decision to the other samples via a local noise model. During active learning, the clustering is adjusted using a coarse-to-fine strategy in order to balance between the advantage of large clusters and the accuracy of the data representation. Yet another approach [13] is spe-cific to SVM learning. Conceptually, in SVM learning if we can find all the true support vectors and label all of them, we will guarantee low future error. Mitra et al. assigned a confidence factor c to examples within the current deci-sion boundary and 1  X  c to examples outside each indicat-ing the confidence of whether they are true support vectors, and then chose those examples probabilistically according to this confidence. It is noteworthy that their approxima-tion of c is done using a separate validation set (with known labels) and c indicates balance of number of positive and negative examples within the neighborhood of current sup-port vectors.

The third category contains active learning algorithms that try to quickly  X  X oost X  or  X  X tabilize X  an active learner. Active learning is unstable, especially with limited labeled examples, and the hypothesis may change dramatically each round it sees a new example. One way to boost active learn-ing algorithms is simply combining them in some way. For example, Baram et al. X  X  [1] algorithm COMB combines three different active learners by finding and fast-switching to the one that currently performs the best. These three algorithms are  X  X imple X  [20] (which selects the example nearest the decision boundary),  X  X elf-Conf X  [17] (which attempts to di-rectly minimize generalization error), and, to better explore the feature space, their own heuristic  X  X ernel Farthest First X  (KFF). Given a set L of labeled examples, the next example x chosen by KFF to be labeled is the one farthest from L in the feature space induced by the SVM X  X  kernel K : where
 X  K ( x )  X   X  K ( y ) = K ( x, x )+ K ( y, y )  X  2 K ( x, y ) is the Euclidean distance from  X  K ( x ) to  X  K ( y ) , which are the images of x and y in the feature space induced by the kernel K .

The COMB algorithm works by having each  X  X xpert X  (ac-tive learner) in a pool score each candidate unlabeled exam-ple (higher score implies more suitable for labeling). These scores are exponentially weighted and normalized to proba-bilities. After low-probability instances are filtered out, the remaining probabilities are combined into a single probabil-ity vector, which is used to randomly select the next point x from U to be labeled. After x is chosen, it is labeled and added to L , which is used to train a new classifier C .Next, the unlabeled examples in U are labeled by C and a  X  X e-ward utility X  of x is computed based on the classification entropy maximization (CEM) score: H ( | C + ( U ) | / | U where C + ( U )  X  U is the subset of examples of U that C labels positive and H ( p )=  X  p log 2 p  X  (1  X  p )log 2 (1 is the entropy function. Given the previous CEM score H (before x was chosen) and the new one H (after x was cho-sen), the reward utility of x is The reward utility is then used to update the weights of the active learners in the pool. While it is possible to prove guaranteed relative loss bounds on the performance of this algorithm, it is complex to implement.

Related to Baram et al. X  X  approach, Pandey et al. [15] proposed an algorithm based on Kalai and Vempala X  X  [10]  X  X ollow the perturbed leader X  algorithm. Pandey et al. X  X  al-gorithm chooses between Simple and randomly selecting from U . As with COMB , their algorithm updates the weights of the  X  X xperts X  (active learners) based on their effective-ness. Pandey et al. measure effectiveness by the change in classification error from the current hypothesis to the new one. A drawback to this approach is that the classification error is measured on the labeled training set. Measuring error on the training set can yield a biased estimate of the error and requires a larger starting set of labeled examples.
We now describe our algorithm for exploratory active learning. As with Baram et al. [1], the problem we ad-dress is how to balance the exploitation of labeling exam-ples that are near the current decision boundary and the ex-ploration of searching for examples that are far from the already-labeled points and labeled inconsistently with how the current hypothesis would predict.

Our work addresses this problem by, at each step, ran-domly deciding whether to explore or exploit. If an ex-ploratory step is taken, then our algorithm considers the  X  X uccess X  of the exploration to adjust its probability of ex-ploring again. Our algorithm is similar to that of Baram et al., but is simpler to implement since its decision-making process is more straightforward and its probabilistic model is easier to update. Further, our algorithm measures the im-pact of a selected example on the hypothesis via a simple empirical measure rather than Baram et al. X  X  classification entropy maximization score.

Formally, our algorithm flips a biased coin with proba-bility p of coming up heads. If the result is heads, then we apply the KFF algorithm to select the next example to label. If tails, then we choose the next example with Simple [20], which chooses a point near the current decision boundary. After training using this new example, we get a new hy-pothesis h (let h be the hypothesis before the new example was labeled). To determine how successful an exploration step is, we look at the change induced from h to h , denoted d ( h, h )  X  [  X  1 , +1] . If this is positive (implying signifi-cant change from h to h ), we assume the exploration was successful and we want to keep the probability p high to encourage further exploration. If d ( h, h ) is negative, we reduce p . Formally, we update the exploration probability p to p as follows: where is a parameter that upper-and lower-bounds the value of p (so there X  X  always a chance of exploring and ex-ploiting) and  X  is a learning rate for updating p . We now define the function d ( h, h ) that we use 1 .Let S = { x 1 ,...,x m } = L  X  U be the set of labeled and unlabeled training examples that we have. Then for each of the two real-valued hypotheses h (  X  ) ,h (  X  ) ,wedefine the vectors H =( h ( x 1 ) ,h ( x 2 ) ,...,h ( x m )) and H = ( h ( x 1 ) ,h ( x 2 ) ,...,h ( x m )) , i.e. vectors of the real-valued predictions of h and h on S .Nowwedefine i.e. the inner product of H and H normalized by the prod-uct of their lengths. Thus s 1 ( h, h )  X  [  X  1 , +1] is the cosine of the angle between H and H . Despite the fact that math-ematically, s 1 ( h, h ) could be as small as  X  1 , in our exper-iments, we found that s 1 ( h, h ) was always 2 in the interval [1 / 2 , 1] . Thus we rescaled and translated s 1 as follows: Now mathematically, d 1 could be as large as 7, but in prac-tice d 1 ( h, h )  X  [  X  1 , +1] . Negative values of d 1 correspond to large values of s 1 , i.e. negative values of d 1 mean that h and h predicted similarly on S , implying that relatively lit-tle change was induced by exploration.

The time complexity of our algorithm obviously depends on the active learners used as subroutines (KFF and Simple in our case). For each round of active learning, our algo-rithm takes constant time to choose between KFF and Sim-ple, then we add the time complexity of the chosen algo-rithm, and we add time linear in | S | to update p . Specifi-cally, our algorithm X  X  expected time complexity in the cur-rent round is pT KFF +(1  X  p ) T Simple + | S | ,where T KFF is the time to run KFF and T Simple is the time to run Simple. The time complexities of KFF and Simple are similar. Given w as the weight vector of the current hypothesis h , Simple computes the dot product w,  X  K ( x ) for every x  X  U . This requires computing the kernel K ( x, v ) for ev-ery support vector v  X  V h . This takes time  X ( | U || V h where T K is the time to evaluate kernel K . Similarly, com-puting the argmax (1) for KFF takes time  X ( | U || L | T K Since V h  X  L and S = L  X  U , we have that our algorithm X  X  time complexity is O ( | U || L | T k ) per round.
In the worst case, our time complexity each round asymptotically matches that of COMB .However,anexam-ination of the full page of COMB  X  X  pseudocode [1] reveals that it requires more steps in practice. E.g. COMB executes all of its active learning subroutines each round, whereas we choose a single one to execute. It also performs significant bookkeeping, which takes time to run and is more complex to implement.
We implemented our algorithm in Matlab using the Spi-der 3 package and SVM light 4.0. We used d 1 as the measure between hypotheses and set the parameters =0 . 01 and  X  =0 . 1 . We evaluated our algorithm on noise-free and noisy data.
The data we used in our first experiment was a general-ization of exclusive OR: a d -dimensional, n  X  n  X  X  X  X  X  n  X  X heckerboard X  (e.g. Figure 1(b)). We randomly generated 250 points per square in the checkerboard and put them in aset T (so | T | = 250 n d ). The results in Figure 2 are based on 50 runs of the following experiment: We randomly se-lected 100 points from T to serve as the test set and placed the remaining points into U , the unlabeled set. Each curve in each figure is the average of these 50 experiments. Our results ( X  X xploration X ) are contrasted against Simple, KFF, random sampling (each point labeled is chosen uniformly at random from U ), and COMB , which is Baram et al. X  X  [1] combination of Simple, Self-Conf, and KFF. In each plot, we see that our approach starts out roughly even with the others, but then surpasses them in generalization accuracy after 50 X 250 examples have been labeled 4 . Our conclusion is that our method starts by emulating KFF then switches to Simple once exploratory learning is no longer fruitful, i.e. after it has sufficiently sampled all n d regions.
To determine if our algorithm performs worse on on data that does not require extensive exploration, we tested it on three data sets from the UCI repository [2]: Landsat Satel-lite (SAT), Waveform, and Image Segmentation. We also tested it on data extracted from the Corel CD Image Col-lection, with features based on color histograms, color co-herence, and moment invariants. Table 1 describes these d =3 and n =3 .  X  X xploration X  is our algorithm,  X  COMB  X  and  X  X ff X  are from Baram et al. [1],  X  X imple X  is from Tong and Koller [20], and  X  X andom X  is uniform random sampling. four data sets and also gives the parameters  X  (for the Gaus-sian RBF kernel) and C (for the soft-margin SVM) that we used for all active learners in each experiment. Since each of these data sets is multi-class, we ran our experiments in a one-versus-rest fashion (for brevity, only a subset of the results are shown).

Figure 3 summarizes results of 20 runs of each exper-iment using random, Simple, KFF, and our algorithm 5 . These along with Figure 4(a) show that our algorithm quickly learns when to stop exploring and keeps pace with Simple. Thus on these data sets, using Exploration does not hurt, even when extensive exploration is unnecessary. We also evaluated our algorithm on noisy data, using the Image Segmentation dataset from the UCI repository [2]. We performed 25 trials, where each trial started with two randomly chosen labeled examples (one positive and one negative), and each algorithm was run until it chose 150 examples to label. When an active learner chose an example to label, the label was randomly flipped with probability  X  { 0 , 0 . 05 , 0 . 10 , 0 . 20 } .

In Figure 4, we see that COMB initially fares better than our algorithm for all noise rates, but only until 20 X 70 exam-ples are labeled. After that point, for noise rates  X &gt; 0 , our algorithm shows improved performance over the others, and the amount of improvement increases with the noise rate.
It is not obvious why our algorithm is so effective in the presence of high noise rates. One possible explanation of the advantage of our algorithm over Simple is as follows. If Simple gets a noisy label early on in learning (when | L | small), that will induce a large change in the hypothesis, po-tentially moving it far from the optimal hypothesis. Simple labels points near the decision boundary since it assumes that its current hypothesis is near the optimal one. When this assumption is violated due to an early noisy point, it can take many small steps to reach the optimum 6 . In con-trast, our algorithm starts with frequent exploration before it begins to sample near the boundary (the presence of noise will likely prolong the exploratory phase). Thus by the time our algorithm is making small steps due to closest sampling, it has a more diverse set of labeled points available, result-ing in a more optimal  X  X tarting hypothesis X  to refine (i.e. it starts its small steps closer to the optimal hypothesis).
Further, while COMB also chooses KFF early on, it ap-pears that it over-commits to KFF by letting KFF X  X  weight far exceed those of Simple and random (this is evident in Figure 4 where COMB  X  X  curves tend to follow those of KFF). Since COMB  X  X  weight updates are done multiplica-tively, such an over-commitment can be difficult to reverse. In contrast, our algorithm X  X  use of an bound on p in (2) helps to avoid this. Further experimentation (including run-ning COMB with bounds on the ratios of the algorithms X  weights) is necessary to confirm this.
Other measures of hypothesis change can be used in our algorithm. One that we have started to evaluate is specific to linear threshold units, e.g. support vector machines. Let w be the weight vector for hypothesis h and w be the weight vector for hypothesis h . Then we can use which for a support vector machine can be computed using where V h ( V h )isthesetof h  X  X  ( h  X  X ) support vectors, y the { X  1 , +1 } label of x i ,  X  is the representation of SVM h , and K (  X  ,  X  ) is the kernel used by the SVM. We complete the computation of (3) by noting that w = w, w . Finally, we set d 2 ( h, h )=  X  s 2 ( h, h )  X  [  X  1 , +1] .
We have completed preliminary experiments with 2  X  2 checkerboard, COREL, and SAT, with and without noise. So far we have not found any significant difference between d 1 and d 2 , but more experiments are necessary.
In some active learning applications, it is necessary to early on emphasize exploration of the instance space in or-der to familiarize the learner with the  X  X ockets X  of examples that it would otherwise misclassify. We presented a fast, simple alternative to Baram et al. X  X  [1] COMB algorithm. In experiments, our algorithm fared better than others in the literature on synthetic checkerboard data, and did not fare worse on data sets that did not require extensive exploration. Finally, we found that our algorithm is robust against high rates of classification noise.

Another measure of hypothesis change that we plan to evaluate is a variation of the measure d 1 from Section 3. This variation uses the symmetric difference of the subsets of S that h and h label as positive. I.e. if P (similarly P ) is the subset of S that h ( h ) labels positive, then we define which has range [  X  1 , +1] . This measure is similar to a spe-difference being a change in the normalization factor.
Our algorithm X  X  update of p via (2) is sensitive to the change measured between h and h via d .Asthesetofla-beled examples L grows, each new labeled example added to L has less impact on h . Thus d ( h, h ) could be small even if the new point was a valid exploration. We did not witness this phenomenon, but it is conceivable that if L is large, then p could be reduced even if an exploratory label-ing of x discovered a new region but was disregarded as noise since it was overwhelmed by the other examples in L . One way to remedy this would be to dynamically weight d by the size of L .I.e.when L is large, even small changes from h to h should result in a positive value of d ( h, h ) . The authors used the implementation of COMB from Ran El-Yaniv X  X  web site. This research was funded in part by NSF grant CCR-0092761 and a grant from the NU Founda-tion. It was also supported in part by NIH Grant Number RR-P20 RR17675 from the IDeA program of the National Center for Research Resources.

