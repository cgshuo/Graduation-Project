 Liang Sun sun.liang@asu.edu Shuiwang Ji shuiwang.ji@asu.edu Jieping Ye jieping.ye@asu.edu Canonical Correlation Analysis (CCA) (Hotelling, 1936) is commonly used for finding the correlations between two sets of multi-dimensional variables. It makes use of two views of the same set of objects and projects them into a lower-dimensional space in which they are maximally correlated. CCA has been applied successfully in various applications (Hardoon et al., 2004; Vert &amp; Kanehisa, 2003). One popular use of CCA is for supervised learning, in which one view is derived from the data and another view is derived from the class labels. In this setting, the data can be pro-jected into a lower-dimensional space directed by the label information. Such formulation is particularly ap-pealing in the context of dimensionality reduction for multi-label data (Yu et al., 2006).
 Multivariate linear regression (MLR) that minimizes the sum-of-squares cost function is a well-studied tech-nique for regression problems. It can also be applied for classification with an appropriate class indicator matrix (Bishop, 2006; Hastie et al., 2001). The so-lution to least squares problems can be obtained by solving a linear system of equations. A number of al-gorithms, including the conjugate gradient algorithm, can be applied to solve it efficiently (Golub &amp; Loan, 1996). Furthermore, the least squares formulation can be readily extended using the regularization technique. For example, 1-norm regularization can be incorpo-rated into the least squares formulation to control model complexity and improve sparseness (Tibshirani, 1996). Sparseness often leads to easy interpretation and a good generalization ability. It has been used successfully in PCA (d X  X spremont et al., 2004) and SVM (Zhu et al., 2003).
 In contrast to least squares, CCA involves a gener-alized eigenvalue problem, which is computationally more expensive to solve. Furthermore, it is challenging to derive sparse CCA, as it involves a difficult sparse generalized eigenvalue problem. Convex relaxation of sparse CCA has been studied in (Sriperumbudur et al., 2007), where the exact sparse CCA formulation has been relaxed in several steps. On the other hand, in-teresting connection between least squares and CCA has been established in the literature. In particular, CCA has been shown to be equivalent to Fisher Linear Discriminant Analysis (LDA) for binary-class prob-lems (Hastie et al., 1995). Meanwhile, it is well-known that LDA is equivalent to least squares in this case (Bishop, 2006; Hastie et al., 2001). Thus CCA can be formulated as a least squares problem for binary-class problems. In practice, the multi-class and multi-label problems are more prevalent. It is therefore tempting to investigate the relationship between least squares and CCA in the more general setting.
 In this paper, we study the relationship between CCA and least squares for multi-label problems. We show that, under a mild condition which tends to hold for high-dimensional data, CCA can be formulated as a least squares problem by constructing a specific class indictor matrix. Based on this equivalence relation-ship, we propose several CCA extensions including sparse CCA using the 1-norm regularization. Further-more, the entire solution path for sparse CCA can be readily computed by the Least Angle Regression algo-rithm (LARS) (Efron et al., 2004). We evaluate the es-tablished theoretical results using a collection of multi-label data sets. Our experiments confirm the equiva-lence relationship between these two models under the given assumption. Results also show that, even when the assumption does not hold, they achieve very sim-ilar performance. Our experiments also demonstrate the effectiveness of the proposed CCA extensions. Notations The number of training samples, the data dimensionality, and the number of labels are denoted by n , d , and k , respectively. x i  X  R d denotes the i th observation and y i  X  R k encodes the corresponding label information. Let X = [ x 1 , , x n ]  X  R d  X  n be the data matrix and Y = [ y 1 , , y n ]  X  R k  X  n be the class label matrix. We assume that both { x i } n 1 and { y i } n 1 are centered, i.e., P In this section we give a brief overview of CCA and least squares as well as several other related work. 2.1. Canonical Correlation Analysis In CCA two different representations of the same set of objects are given, and a projection is computed for each representation such that they are maximally correlated in the dimensionality-reduced space. For X  X  R d  X  n and Y  X  R k  X  n , CCA computes two pro-jection vectors, w x  X  R d and w y  X  R k , such that the following correlation coefficient: is maximized. Since  X  is invariant to the scaling of w x and w y , CCA can be formulated equivalently as subject to w T x XX T w x = 1 , w T y Y Y T w y = 1 . We assume that Y Y T is nonsingular. It can be shown that w x can be obtained by solving the following opti-mization problem: Both formulations in Eqs. (2) and (3) attempt to find the eigenvectors corresponding to top eigenvalues of the following generalized eigenvalue problem: where  X  is the eigenvalue corresponding to the eigen-vector w x . Multiple projection vectors under certain orthonormality constraints can be computed simulta-neously by solving the following optimization problem (Hardoon et al., 2004): where W  X  R d  X   X  is the projection matrix,  X  is the num-ber of projection vectors, and I is the identity matrix. The solution to the optimization problem in Eq. (5) consists of the top  X  eigenvectors of the generalized eigenvalue problem in Eq. (4).
 In regularized CCA (rCCA), a regularization term  X I with  X  &gt; 0 is added to XX T in Eq. (5) to prevent the overfitting and avoid the singularity of XX T (Bach &amp; Jordan, 2003). Specifically, rCCA solves the following generalized eigenvalue problem: 2.2. Least Squares for Regression and In regression, we are given a training set { ( x i , t i ) } n where x i  X  R d is the observation and t i  X  R k is the corresponding target. We assume that both the ob-servations and the targets are centered. Thus the bias term can be ignored. In this case, the projection ma-trix W can be computed by minimizing the following sum-of-squares cost function: where T = [ t 1 , , t n ]. It is well known that the opti-mal projection matrix is given by where the pseudo-inverse is used in case XX T is sin-gular. To improve the generality ability of the model, a penalty term based on 2-norm or 1-norm regulariza-tion is commonly applied (Hastie et al., 2001). Least squares is also commonly applied for classifica-tion. In the general multi-class case, we are given a data set consisting of n samples { ( x i , y i ) } n i =1 x i  X  R d , and y i  X  X  1 , 2 , , k } denotes the class la-bel of the i -th sample, and k &gt; 2. To apply the least squares formulation to the multi-class case, the 1-of-k binary coding scheme is usually employed to apply a vector-valued class code to each data point (Bishop, 2006). The solution to the least squares problem de-pends on the choice of class indicator matrix. Several class indicator matrices have been proposed in the lit-erature (Hastie et al., 2001). 2.3. Related Work The inherent relationship between least squares and several other models has been established in the past. In particular, LDA for binary-class problems can be formulated as a least squares problem (Duda et al., 2000; Bishop, 2006). Moreover, this equivalence rela-tionship can be extended to the multi-class case using a specific class indicator matrix (Ye, 2007). CCA has been shown to be equivalent to LDA for multi-class problems (Hastie et al., 1995). Thus, CCA is closely related to least squares in the multi-class case. We show in the next section that, under a mild condition, CCA can be formulated as a least squares problem for multi-label classifications when one of the views used in CCA is derived from the labels. In this section we investigate the relationship between CCA and least squares in the multi-label case. We first define four matrices essential for our derivation: Note that we assume n  X  k and rank( Y ) = k for multi-label problems. Thus, ( Y Y T )  X  1 2 is well-defined. It follows from the definition above that the solution to CCA can be expressed as the eigenvectors corre-sponding to top eigenvalues of the matrix C  X  XX C HH . 3.1. Basic Matrix Properties In this subsection, we study the basic properties of the matrices involved in the following discussion. Follow-ing the definition of H in Eq. (9), we have: Lemma 1. Let H be defined as in Eq. (9) and let { y i } n 1 be centered, i.e., P (1). H has orthonormal columns, i.e., H T H = I k ; (2). H T e = 0 .
 Given H  X  R n  X  k with orthonormal columns, there orthogonal matrix (Golub &amp; Loan, 1996), that is It follows that It can be verified from Eqs. (10), (11), and (13) that the matrices C XX , C HH , and C DD are all positive semidefinite.
 Let the Singular Value Decomposition (SVD) of X be where r = rank( X ), U and V are orthogonal matrices, V Since U 2 lies in the null space X T , we have: Lemma 2. Let H , X , U 2 , and D be defined as above. 3.2. Computing CCA via Eigendecomposition Recall that the solution to CCA consists of the top  X  eigenvectors of the matrix C  X  XX C HH . We next show how to compute the eigenvectors of C  X  XX C HH . Define the matrix A  X  R r  X  k by Let the SVD of A be A = P  X  A Q T , where P  X  R r  X  r and Q  X  R k  X  k are orthogonal, and  X  A  X  R r  X  k is di-agonal. Then The matrix C  X  XX C HH can be diagonalized as follows: C where the second equality follows since U is orthogo-shown in Lemma 2, and the last equality follows from Eq. (16). Thus the solution to CCA, which consists of the top  X  eigenvectors of matrix C  X  XX C HH , is given by where P  X  contains the first  X  columns of P . 3.3. Equivalence of CCA and Least Squares Recall from Eq. (8) that for a given class indicator matrix T , the solution to the least squares problem is given by We define the class indicator matrix  X  T as follows: In this case, the solution to the least squares problem is given by It is clear from Eqs. (17) and (19) that the difference between CCA and least squares lies in  X  A and Q T . We next show that all diagonal elements of  X  A are one under a mild condition, that is, rank( X ) = n  X  1 and rank( Y ) = k . Note that the first condition is equivalent to requiring that the original data points are linearly independent before centering, which tends to hold for high-dimensional data.
 Before presenting the main result summarized in The-orem 1 below, we have the following lemma: Lemma 3. Assume for some non-negative integer s . Then for the matrix  X   X 
A =  X  A  X  T A = diag ( a 1 , a 2 , , a r )  X  R r  X  r , we have where f = rank ( X  A ) .
 Proof. Define the matrix J  X  R d  X  d as follows: It follows from the definition of C XX , C HH , and C DD in Eqs. (10)-(12) that where b i = 1  X  a i , for i = 1 , , r . Note that since J is nonsingular, we have It follows from our assumption that Since both J T C HH J and J T C DD J are diagonal, there are a total of r + s nonzero elements in J T C HH J and J T C DD J . Note that f = rank( X  A ) = rank(  X   X  A ), thus a 1 &gt; &gt; a f &gt; 0 = a f +1 = = a r . It follows from Eq. (21) that a i + b i = 1 , for 1 6 i 6 r, b r &gt; &gt; b 1 &gt; 0 . (23) This implies that at least one of a i or b i is positive for 1 6 i 6 r . To satisfy the rank equality in Eq. (22), we therefore must have This completes the proof of the lemma.
 Theorem 1. Assume that rank ( X ) = n  X  1 and rank ( Y ) = k for multi-label problems. Then we have Thus s = 0 , where s is defined in Lemma 3, and This implies that all diagonal elements of  X  A are ones. Proof. Denote e T = [1 , 1 , , 1]  X  R 1  X  n , H = [ h 1 , , h k ], and D = [ h k +1 , , h n ]. Note that X is column centered, i.e., P n i =1 x i = 0. It follows from Lemma 1 that H T e = 0, that is, Since [ H, D ] is an orthogonal matrix, { h 1 , , h n } form a basis for R n . Thus we can represent e  X  R n as It follows from the orthogonality of [ H, D ] and Eq. (27) that e can be expressed as e = P n i = k +1  X  i h i , and 0 = Xe = X Since not all  X  i  X  X  are zero, the n  X  k columns of XD are linearly dependent, thus rank( XD ) 6 n  X  k  X  1. According to the property of matrix rank, we have Thus, rank( XD ) = n  X  k  X  1 holds.
 For matrix XH , we have On the other hand, since XH  X  R d  X  k , rank( XH ) 6 k . Thus we have rank( XH ) = k and It follows that s = 0. On the other hand, f = rank( A ) = rank( X   X  1 r U T 1 XH ) = rank( XH ) = k. Hence, and all diagonal elements of  X  A are ones. This com-pletes the proof of the theorem.
 Since rank( X  A ) = k , C  X  XX C HH contains k nonzero eigenvalues. If we choose  X  = k , then The only difference between W LS and W CCA lies in the orthogonal matrix Q T in W LS .
 In practice, we can use both W CCA and W LS to project the original data onto a lower-dimensional space before classification. For any classifiers based on Euclidean distance, the orthogonal transformation Q T will not affect the classification performance since the Eu-clidean distance is invariant of any orthogonal trans-formations. Some well-known algorithms with this property include the K-Nearest-Neighbor (KNN) algo-rithm (Duda et al., 2000) based on the Euclidean dis-tance and the linear Support Vector Machines (SVM) (Sch  X olkopf &amp; Smola, 2002). In the following, the least squares CCA formulation is called  X  X S-CCA X . Based on the equivalence relationship established in the last section, the classical CCA formulation can be extended using the regularization technique. Regularization is commonly used to control the com-plexity of the model and improve the generalization performance. Linear regression using the 2-norm reg-ularization, called ridge regression (Hoerl &amp; Ken-nard, 1970), minimizes the penalized sum-of-squares cost function. By using the class indicator matrix  X  T in Eq. (18), we obtain the 2-norm regularized least squares CCA formulation (called  X  X S-CCA 2  X ) by min-imizing the following objective function: where W = [ w 1 , , w k ], and  X  &gt; 0 is the regulariza-tion parameter.
 In mathematical programming, it is known that sparseness can often be achieved by penalizing the L -norm of the variables (Donoho, 2006; Tibshirani, 1996). It has been introduced into the least squares formulation and the resulting model is called lasso (Tibshirani, 1996). Based on the established equiv-alence relationship between CCA and least squares, we derive the 1-norm least squares CCA formulation (called  X  X S-CCA 1  X ) by minimizing the following ob-jective function: The optimal w  X  j , for 1  X  j  X  k , is given by which can be reformulated as: for some tuning parameter  X  &gt; 0 (Tibshirani, 1996). Furthermore, the solution can be readily computed by the Least Angle Regression algorithm (Efron et al., 2004). One key feature of LARS is that it computes the entire solution path for all values of  X  , with essen-tially the same computational cost as fitting the model with a single  X  value.
 If the value of  X  is large enough, the constraint in Eq. (33) is not effective, resulting in an unconstrained optimization problem. We can thus consider  X  from so that  X  =  X   X   X  with 0  X   X   X  1. The estimation of  X  is equivalent to the estimation of  X  . Cross-validation is commonly used to estimate the optimal value from denotes the size of S . If the value of  X  is sufficiently small, many of the coefficients in W will become ex-actly zero, which leads to a sparse CCA model. We thus call  X  the  X  X parseness coefficient X . We use a collection of multi-label data sets to ex-perimentally verify the equivalence relationship estab-lished in this paper. We also evaluate the performance of various CCA extensions. 5.1. Experimental Setup We use two types of data in the experiment. The gene expression pattern image data 1 describe the gene expression patterns of Drosophila during development (Tomancak &amp; et al. , 2002). Each image is annotated with a variable number of textual terms (labels) from a controlled vocabulary. We apply Gabor filters to ex-tract a 384-dimensional feature vector from each im-age. We use five data sets with different numbers of terms (class labels). We also use the scene data set (Boutell et al., 2004) which contains 2407 samples of 294-dimension and 6 labels. In all the experiments, ten random splittings of data into training and test sets are generated and the averaged performance is re-ported.
 In the experiment, five methods including CCA and its regularized version rCCA in Eq. (6), as well as LS-CCA and its regularization versions LS-CCA 2 and LS-CCA 1 are compared. These CCA methods are used to project the data into a lower-dimensional space in which a linear SVM is applied for classification for each label. The Receiver Operating Characteristic (ROC) score is computed for each label and the averaged per-formance over all labels is reported. 5.2. Gene Expression Pattern Image Data In this experiment we first evaluate the equivalence relationship between CCA and least squares. For all cases, we set the data dimensionality d larger than the sample size n , i.e., d/n &gt; 1. The condition in Theorem 1 holds in all cases. We observe that for all splittings of all of the five data sets, rank( C XX ) equals rank( C HH ) + rank( C DD )), and the ratio of the maxi-mal to the minimal diagonal element of  X  A is 1, which implies that all diagonal elements of  X  A are the same, i.e., ones. Our experimental evidences are consistent with the theoretical results presented in Section 3.3. 5.2.1. Performance Comparison In Table 1, we report the mean ROC scores over all terms and all splittings for each data set. The main ob-servations include: (1) CCA and LS-CCA achieve the same performance for all data sets, which is consistent with our theoretical results; (2) The regularized CCA extensions including rCCA, LS-CCA 2 , and LS-CCA 1 perform much better than their counterparts CCA and LS-CCA without the regularization; and (3) LS-CCA 2 is comparable to rCCA in all data sets, while LS-CCA 1 achieves the best performance in all cases. These fur-ther justify the use of the proposed least squares CCA formulations for multi-label classifications. 5.2.2. Sensitivity Study In this experiment, we investigate the performance of LS-CCA and its variants in comparison with CCA when the condition in Theorem 1 does not hold, which Mean ROC Score is the case in many applications. Specifically, we use a gene data set with the dimensionality fixed at d = 384, while the size of the training set varies from 100 to 900 with a step size about 100.
 The performance of different algorithms as the size of training set increases is presented in Figure 1 (left graph). We can observe that in general, the perfor-mance of all algorithms increases as the training size increases. When n is small, the condition in Theorem 1 holds, thus CCA and LS-CCA are equivalent, and they achieve the same performance. When n further increases, CCA and LS-CCA achieve different ROC scores, although the difference is always very small in our experiment. Similar to the last experiment, we can observe from the figure that the regularized meth-ods perform much better than CCA and LS-CCA, and LS-CCA 2 is comparable to rCCA. The sparse formu-lation LS-CCA 1 performs the best for this data set. 5.3. Scene Data Set We conduct a similar set of experiments on the scene data. As in the gene data set, the equivalence relation-ship holds when the condition in Theorem 1 holds. For the performance comparison and sensitivity study, we generate a sequence of training sets with the size n ranging from 100 to 900 with a step size around 100. The results are summarized in Figure 1 (right graph). Similar to the gene data set, CCA and LS-CCA achieve the same performance when n is small, and they differ slightly when n is large. We can also observe from the figure that the regularized algorithms including rCCA, and LS-CCA 2 , and LS-CCA 1 perform much better than CCA and LS-CCA without regular-ization, and LS-CCA 2 performs slightly better than others in this data set. 5.4. The Entire CCA Solution Path In this experiment, we investigate the sparse CCA model, i.e., LS-CCA 1 using the scene data set. Recall that the sparseness of the weight vectors w i  X  X  depends on the sparseness coefficient  X  between 0 and 1. Figure 2 shows the entire collection of solution paths for a subset of the coefficients from the first weight vector w 1 . The x -axis denotes the sparseness coeffi-cient  X  , and the y -axis denotes the value of the coef-ficients. The vertical lines denote (a subset of) the turning point of the path, as the solution path for each of the coefficients is piecewise linear (Efron et al., 2004). We can observe from Figure 2 that when  X  = 1, most of the coefficients are non-zero, i.e., the model is dense. When the value of the sparseness coefficient  X  decreases (from the right to the left side along the x -axis), more and more coefficients become exactly zero. All coefficients become zero when  X  = 0. In this paper we show that CCA for multi-label clas-sifications can be formulated as a least squares prob-lem under a mild condition, which tends to hold for high-dimensional data. Based on the equivalence rela-tionship established in this paper, we propose several CCA extensions including sparse CCA. We have con-ducted experiments on a collection of multi-label data sets to validate the proposed equivalence relationship. Our experimental results show that the performance of the proposed least squares formulation and CCA is very close even when the condition does not hold. Results also demonstrate the effectiveness of the pro-posed CCA extensions.
 The proposed least squares formulation facilitates the incorporation of the unlabeled data into the CCA framework through the graph Laplacian, which cap-tures the local geometry of the data (Belkin et al., 2006). We plan to examine the effectiveness of this semi-supervised CCA model for learning from both la-beled and unlabeled data. The proposed sparse CCA performs well for the gene data set. We plan to ana-lyze the biological relevance of the features extracted via the sparse CCA model.
 This research is sponsored in part by funds from the Arizona State University and the National Science Foundation under Grant No. IIS-0612069.

