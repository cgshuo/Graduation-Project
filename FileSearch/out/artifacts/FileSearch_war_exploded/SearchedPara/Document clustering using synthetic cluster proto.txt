 1. Introduction and a solution C ={ c j : j =1, ... , k } is searched that partitions the document into k disjoint clusters. complicated to select features in an unsupervised setting and it is usually achieved using heuristics [49 data into a feature space of much lower dimensionality.
 cluster representations that describe properly the underlying data classes of a dataset. An objective function start from a set of k cluster representations which are improved iteratively in a way that the medoid that is a real object which is representative for the cluster it belongs. have been proposed and applied for text collections [11  X  fast and gives better clusters than traditional k-means [12] .
 cluster structure in on-line high dimensional data streams [54,55] .
 documents constitute a typical example of data where such an adverse setting is met. methods.
 method. In Section 4 , comparative experimental results are reported and discussed, and remarks and future research directions. 2. Background 2.1. Document representation typical approach is to represent each input document as a bag-of-words [18] feature vector d scheme and document vectors are normalized to unit length with respect to Euclidean L modeled as: where tf ij is the frequency of j -th term in the i -th document and N computes the cosine of the angle between the two document vectors: 2.2. Properties of the representation space of documents is a speci fi c semantically narrow instance of a much more general document class. [27,28] .Forinstance,let d x , d y ,and d z three documents of the same class; it is possible for d different set of terms with d z whereas, at the same time, d similarity with K objects in the dataset (or a cluster), then the direct determination of its nearest K unavoidably make guesses.
 diversity even between documents of the same class, justi 2.3. Clustering using k-means family of methods objective function: 1. Reassignment step : each object is assigned to the cluster whose prototype is nearest to the object. optimizes the objective function.
Eq. (3) , where the centroids are computed as the arithmetic mean object that has the maximum average similarity to the objects of its cluster: complexity O ( n j 2 ) to determine a cluster medoid.
 normalized centroid u j = s j =  X  s j  X  2 , where s j =  X  fi and experimentally illustrated in this work.
 and swapping the cluster memberships for pairs of objects [42] . It is also a practical choice to re precomputed off-line. 2.4. Text document subspace clustering cluster during clustering.
 arbitrarily oriented subspaces. Their major dif fi culty is to deal with the in no intersection between the dimensions that span the different subspaces and hence, 2 approach adopted regarding the feature set a method starts to work with. Top iteratively try to determine narrow subspaces for each cluster. On the other hand, bottom subspaces and use a strategy similar to mining frequent item set to increase their dimensionality. beyond the scope of this work. In the rest of this section we will discuss the recent research on top document clustering problem.
 distance and any distance-based clustering method can produce the major issue is that all the N  X | V | parameters should be estimated during the process. means ( fwk-means ) [39] aims to minimize the following objective function: subject to where  X  j is the L 1 -normalized centroid of the j -th cluster and h computes the distance between the centroid  X  jl and a document d until convergence: 1. Object assignment to their nearest cluster using the computed centroids and feature weights. 2. Computation of the cluster centroids using the computed feature weights. 3. Computation of the feature weights for each cluster using the computed cluster centroids. cluster c j is computed by: where  X  is the average dispersion of the vocabulary measured off-line in a sample of N value because a feature weight is not computable if its dispersion in a cluster is zero. If we let mfv the l -th feature in the data sample then  X  is given by:
Moreover, their search strategy is more alike to fwk-means. A modi entropy e j =  X  V jj l =1 w jl log w jl corresponding to each cluster in order penalize the identi very few features. The objective function of ewk-means is: optimization is identical to that of fwk-means and differ only on the weight computation: where weight-adjusting principle was adopted in [34] , which at each step adds a allowing the incorporation of constraints derived from a labeled data subset. 3. The k-synthetic prototypes clustering method 3.1. Clustering using centroids and medoids clusters have low homogeneousity due to random initialization. More speci similarity between a normalized centroid u j and a document d in the respective cluster c c , or the documents of other clusters. In an extreme case, a document d number of objects per cluster in combination with high sparsity.
 thatthereisafeaturesubset f j + strongly related to eachdocumentclass j ,andausuallymuchlargersubset f | V |  X  | f j + | terms, then the learning process aims to fi nd a cluster prototype, i.e. a weight vector in class.Thismeansthatforeachclustertheclusteringalgorithmshouldtrytodeterminethe| f class and to estimate their relative weight distribution in the possible presence of | f of f
Feature over-aggregation appears at the initial iterations where very low object assignment to clusters and hence the problem is retained.

Both self-similarity and feature over-aggregation constrain the local search effect. However, as mentioned in Section 2.2 , since each document is a speci representative. 3.2. Synthetic cluster prototypes dynamic selection scheme implemented in the form of synthetic cluster prototypes , which are computed by formed clusters to retain the important features for each cluster. To compute a synthetic prototype we must de ii. feature selection on prototypes in order to select features from the reference cluster prototype. may not contain all the vocabulary terms.
 considered is the simple thresholding on the feature weights of a reference prototype to keep the P most signi much more detail (i.e. more objects and features) from the clusters to
A straightforward option for reference prototype is the Centroid documents of more than one class none of which is clearly dominant ( Fig. 2 b). number of nearest neighbors directly from the medoid object. Let values such that 0 b  X  i b  X  i +1 b ... b  X   X  =1. Starting with the medoid Y nearest to the centroid of subset Y i  X  1 . For a two-step example with c and then: i) we determine the  X  0.2 K  X  objects in c j nearest to the medoid and compute their centroid rp in c j nearest to rp 1 and compute rp 2 which is the fi nal MedoidKNN cluster c j , while for K =1 it is the cluster medoid. Typically, up to three steps ( the r NN-consistency of the whole dataset that can be similarly de cluster to become more class-discriminative. When considering the MedoidKNN objects included in the Y set for each cluster and again the feature selection on the MedoidKNN subset of core objects of a cluster.
 de fi mentioning difference is that we claim that after having concluded to a set of synthetic representatives de subspaces, then we may take into account the complete feature space to re be noted that k-sp cannot guarantee the monotonicity of convergence. In the case of Centroid as reference prototype that maximizes the cluster cohesion produced by the main k-sp procedure.

Algorithm 1 . k-synthetic prototypes clustering method function k_S_P ( k , p docs , p terms , ref_ fl ag ) input : the number of clusters k , two parameters p docs , p (see Algorithm 2 ), a fl ag ref_ fl ag that enables re fi output : the k clusters and the set of fi nal prototypes let : C , S , H , a partition, the synthetic cluster prototypes and the end let 1: { C , S }  X  InitializeClusters () 2: H  X  Cohesion ( C , S ) 3: repeat 5: C  X  AssignDocsToClusters () 6: S  X  ConstructS P ( C , p docs , p terms ) 7: H  X  Cohesion ( C , S ) 11: end if 12: if ref_ fl ag ==TRUE then 13: C  X  Re fi neSolution ( C ) 14: end if 15: return { C , S } 3.3. De fi nition of parameters
The k-sp parameters for computing the MedoidKNN ( s ) prototype can be de prototype of a cluster. Two parameters, both in [0, 1], must be speci
K is computed as:
Note that different numbers of neighbors are considered for each cluster c to fi nd the P j =  X  p terms | V j ( r ) |  X  terms of highest frequency in the reference prototype of rp contain a fraction p terms of the total feature weight sum i =1,..., P j , a function that indexes the selected features which represent the speci by:
The more uniform the weight distribution of rp j , the more features are selected to represent the c contained in one bin may be needed to be ordered and then to select the most informative subset out of them.
Algorithm 2 . Construct MedoidKNN synthetic prototype function ConstructSP (c, p docs ,p terms ,  X  ,  X  ) input : a cluster c , a threshold p docs  X  [0, 1] that determines the output : the synthetic prototype MedoidKNN ( s ) for cluster c let : n c the number of documents in cluster c end let 1: Yc  X  { Medoid ( c )} 2: rp  X  Medoid ( c ) 3: Kc  X   X  P docs n c  X  4: if Kc N 1 then 5: do for i =1, ... ,  X  6: Yc  X  NNDocs ( c , rp ,  X   X  i K c  X  ) 7: rp  X  Centroid ( Y c ) 8: end for 9: end if 10: sp  X  FSonRP ( rp , p terms ) 11: return { sp } 3.4. Re fi ning the solution of k-synthetic prototypes may be further re fi ned by considering the centroids of the obtained clusters as the initial prototypes for a coincides with the regular spk-means (this option is enabled by the k-sp phase, ii) assists in reducing the sensitivity of the k-sp to parameter de comparing the values of the objective function after the re
The experimentally observed improvement achieved by re fi clustering improvement achieved by k-sp re fi nement phase also con
Moreover, each respective cluster centroid would have a high weights which would lead to an improvement in its class-discrimination. 3.5. Selecting the k-sp parameters since it allows the selection of the best setting for parameters p candidate parameter values, the set S p summarized by the following steps: 1. The sets of values S p 2. Run k-sp with re fi nement ( Algorithm 2 ) several times for each combination of parameter values p corresponding parameter values.
 3.6. Implementation and complexity
In the present context, where document vectors and cluster centroids are normalized with respect to L u =  X  d i  X  c j d i =  X   X  d i  X  c j d i  X  2 the normalized centroid of cluster c
Hence we can determine the medoids of all clusters with linear cost O(N) to the size of the corpus. Thus, both randomly select any of them to construct our synthetic prototype.
 incremental (  X  =1) construction of a MedoidKNN ( r ) prototype costs O ( n medoid document: O ( n j ), ii) to locate medoid's K -1 nearest neighbors in the cluster: O ( K objects: O ( K | V |). The latter is the fi rststepoftheincrementalMedoidKNN by computing and then sorting the pairwise similarities between the n i =2,...,  X  .Thus,ifasubsetof K ( i ) cluster objects are used to construct the MedoidKNN 4. Experimental evaluation 4.1. Clustering methods also illustrated that ewk-means is not sensitive to the setting of
The spk-means [22] is the baseline approach, the same algorithm is also utilized to re [41] that try to spread the initial centroids away from each other. cosine similarity measure. The Laplacian matrix is computed as L = D let Z clustered using the standard k-means algorithm, assuming that i -th row of Z represents the i -th document.
MedoidK( p docs ) NN  X  P ( p terms ). 2 The set of values considered for p .80, .60, and .40}. In all cases, MedoidKNN ( r ) has been constructed incrementally in three steps ( 4.2. Datasets 4.2.1. Real data
Inordertoconductcontrolledexperimentswithrespecttothecorpussize,clustersizesandoverlap,bothrealandarti were used (see Table 1 ). We constructed a series of clustering problems from real collections, by
Newsgroups 3 collection usingasground truth theprovidedclass labelof each document. As anexample, M selected topics. Mini 20 4 contains100 documents from eachoneof thetwenty newsgroups,whileNG
K1 Jose Mercury newspaper articles that are distributed as part of the TREC collection (TIPSTER Vol. 3). class (CS) when considering only the vocabulary used by the class members (note that OS 4.2.2. Arti fi cial data the terms related to i -th topic, while an additional bag B de fi nedina k  X ( k +1)matrix W , whereeach element W ji is the probabilityof selecting the bag B of the j -th cluster. To sample a term from analready selected bag (step 2) we used the Zanette that has been proposed for generating a single long arti fi p already been used in the created sequence. This property of the process (called documents of the fi rst cluster then that of the second etc. The memory of the general bag B increasing cluster overlap (from A 4 (1) to A 4 (4) ), by lowering the probabilities W of selecting a term from the rest of the bags. The probability matrices W are presented in Fig. 5 (the vocabulary). The length of each document was randomly set by an exponential distribution with mean value parameter values that we used for the ZM process are  X  =0.3 and v =0.9. 4.3. Cluster evaluation measures clusters, c 1 , ... , c k , C ( L ) the grouping based on ground truth document labels c a dataset, N i the size of c i ( L ) , n j the size of c j denote the probabilities p ( c j )= n j / N , p ( c i ( L ) computed by dividing the MI by the maximum between the cluster and class entropy:
The Purity of a cluster can be interpreted as the classi fi dominant class. The clustering Purity is the weighted average of cluster-wise purity: each index over the runs on a dataset, while we also report (denoted as corresponding to the solution with the highest clustering objective function Moreover, in order to evaluate a method's behavior during iterations, we introduce the Q-index : where  X  ics ( t ) ( C ) is the intracluster similarity measure de cluster at iteration t : where n j the size of cluster c j . Initially, we assume that Q the clustering quality after one iteration.

Finally, the statistical t-test was applied to estimate the signi methods under comparison for each dataset, exc ept for HAC that is deterministic. Within a con degrees of freedom equal to 2  X  number_of_runs -2 we can test if our method is signi accepted. 4.4. Experimental results 4.4.1. Robust cluster representation of documents from the topics of Talk 3 dataset: a) a pure set of 300 documents from the shorter time. Fig. 4 indicates the weakness of centroid representation: it de documents should stay in that cluster. This constrains to a great extent the representation caseof Centroids ( r ) (e.g.withP(.4)), weobserve immediateclusteringimprovementinthe medoid for computing cluster representatives. 4.4.2. Clustering performance results two sets of values S p reported  X  re fi ned  X  solutions are obtained by k-sp re fi fi with the k-means++ heuristic (Spkm++), k-medoids (Medoid), the re using spk-means, and fi nally the spectral clustering method.
 and spectral clustering are quite similar. However, in a more confused setting, such as the A becomes more clear. Moreover, as the overlap between clusters increases, k-sp performs signi method produced much better results than spk-means. Using MedoidKNN with increasing size of clusters from small to large (datasets RS similarity and feature over-aggregation. The proposed re fi objects became available for a speci fi c problem, but the proposed k-sp remained the best among them. the other hand, as synthetic prototypes discard too much information prototypes for medium and large datasets (e.g. RS 4 ( M )
The information of the formed clusters can be further exploited by larger synthetic representatives in the re re fi nement turns out to be much smaller.

Table 4 summarizes the best and average performance of each method focusing on the re
Table 4 correspond to the set of experiments with the maximum average value of the re parameter setting may have produ ced a better solution. The column t-val presents the t -value of the signi t =1.999 ( p c =5%for p value). This means that if the computed t -value negative, k-sp performs worse than the compared method. In Table 4 the t -values with the k-means++ technique, k-medoids and HAC as well as their re
NMI and Purity. Spectral clustering seems to be clearly superior only for datasets M computational complexity of spectral clustering is O( N 3 that for all datasets the best solutions were provided by the k-sp method. 4.4.3. Discussion
As a general conclusion about the experimental study, it turns out that the re p noisy datasets where the clusters overlap in many dimensions. High values of p
However, we explained in Section 3.5 that the user speci fi can then be identi fi ed automatically by examining the values of the objective function of the re also remark that k-sp's feature selection on reference prototypes can ef fi xed and small number of features is considered (e.g. p terms then, in most cases, the quality of the clusters produced using MedoidK( p respective un fi ltered reference prototypes. As for the Centroid selection.

In both arti fi cial and real document datasets neither the sophisticated k-means++ initialization, nor the re way than Forgy random selection. The fact that spk-means++ and the re probability introduced by the former in order to select objects that are far from each other may not re distance , since it does not take into account the special properties of text feature space, such as sparsity. of fwk-means with respect to the average evaluation measures. At the same time for many datasets, e.g. A observation indicates that the feature weight entropy term e feature space. 5. Conclusions clustering.
 re fi
References
