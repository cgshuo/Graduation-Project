 Sinead A. Williamson sinead@cs.cmu.edu Avinava Dubey akdubey@cs.cmu.edu Eric P. Xing epxing@cs.cmu.edu In this document, we provide more in-depth proofs of the theorems and derive the Metropolis Hastings acceptance probabilities presented in the main paper. Theorem 1 (Auxiliary variable representation for the DPMM) . We can re-write the generative process for a DPMM as
D j  X  DP for j = 1 ,...,P and i = 1 ,...,N . The marginal dis-tribution over the x i remains the same.
 Proof. In the main paper, we proved the gen-eral result, that if  X   X  Dirichlet(  X  1 ,..., X  P ) and D j  X  DP(  X  j ,H j ), then D := P j  X  j D j  X  authors including Rao &amp; Teh (2009).
 Here, we provide an explicit proof that shows the re-sulting predictive distribution is that of the Dirichlet process.
 tributed according to G  X  DP (  X ,G 0 ). Then the con-ditional distribution of  X  n +1 given  X  1 ,..., X  n where G has been integrated is given by If D j  X  DP (  X /P,G 0 ),  X   X  Dir (  X  P ,...,  X  P ),  X  i  X   X  and  X  i  X  D  X  i then the conditional distribution of  X  n +1 given  X  1 ,..., X  n where D j ,  X  j and  X  have been inte-By superposition of gamma processes, Normalizing G 0 , we get as required by the HDP.
 Now, for m = 1 ,...,M and j = 1 ,...,P , let  X  mj  X   X (  X  j ) and D mj  X  DP(  X  j ,D 0 j ). This implies that Superposition of the gamma processes gives The total mass of G m is given by P P j =1  X  mj , so as required by the HDP.
 If we let  X  mj =  X  mj / P P k =1  X  mk , then we can rewrite Equation 5 as where (  X  m 1 ,..., X  mP )  X  Dirichlet(  X  1 ,..., X  P ). In both algorithms, the Metropolis Hastings proposal {  X  i } ), so we need only consider the likelihood ratios. so we get Equation 7. 2.2. Hierarchical Dirichlet processes For the HDP, the likelihood ratio is given by p ( { x mi }|{  X   X  mi  X ,  X   X  , X ,P ) p ( { x mi }|{  X  mi  X ,  X  , X ,P ) We consider a Chinese restaurant franchise represen-tation (Teh et al., 2006), where each data point is as-sociated with a table (corresponding to clustering in the lower-level DP), and each table is associated with a dish (corresponding to clustering in the upper-level DP).
 Let t j be the count vector for the top-level DP on pro-cessor j  X  in Chinese restaurant franchise terms, t jd is the number of tables on processor j serving dish d . Let n jm be the count vector for the m th bottom-level DP on processor j  X  in Chinese restaurant franchise terms, n jmk is the number of customers in the m th restau-rant sat at the k th table of the j th processor. Let T mj be the total number of occupied tables from the m th restaurant on processor j , and let U j be the total num-ber of unique dishes on processor j . Let a jmi be the total number of tables in restaurant m on processor j with exactly i customers, and b ji be the total number of dishes on processor j served at exactly i tables. We Since the Metropolis-Hastings step does not change the table and dish assignments of the data, the likeli-hood ratio in Eq. 8 can be re-written as: probabilities of the topic-and table-allocations in the local HDPs. This can be obtained by applying the Ewen X  X  sampling formula to both top-and bottom-level DPs. p ( { n jmk }|  X ,  X  ) =
Y and Rao, V. and Teh, Y.-W. Spatial normalized gamma processes. In NIPS , 2009.
 Teh, Y.-W., Jordan, M. I., Beal, M. J., and Blei,
D. M. Hierarchical Dirichlet processes. Journal of the American Statistical Association , 101(476):
