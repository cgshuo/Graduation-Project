 Robert Gwadera  X  Aristides Gionis  X  Heikki Mannila Abstract Sequence data are abundant in application areas such as computational biology, environmental sciences, and telecommunications. Many real-life sequences have a strong description of sequence segments using variable length Markov chains (VLMCs), also known as tree models. We discover the segment boundaries of a sequence and at the same time we compute a VLMC for each segment. We use the Bayesian information criterion (BIC) and Trofimov (KT) code length to select the number of segments of a sequence. On DNA data the method selects segments that closely correspond to the annotated regions of the genes. Keywords Sequence segmentation  X  MDL  X  DNA segmentation  X  Sequence data mining 1 Introduction We consider the problem of segmenting a sequence of symbols into contiguous homoge-neous segments. The segmentation problem has many applications in areas such as computa-it is known under the name change-point detection .

Many segmentation algorithms have been proposed in the data mining community, ranging approach), and from combinatorial to probabilistic, see, e.g., [ 14 , 12 , 23 ].
We consider the sequence segmentation problem as a model selection process where we Bayesian information criteria (BIC) and a variant of the minimum description length (MDL) VLMC for each segment; and ( ii ) determining the optimal number of segments to partition the sequence.

A d -order VLMC is a Markov chain (MC) whose contexts (memory) are allowed to be of VLMCs has the potential of capturing complex phenomena that are present in real-life sequences. VLMCs provide a sparse representation of a sequence by reducing the number of can fit high order models to segments to maximize the likelihood, without being penalized for an exponential increase in the number of parameters (as in the case of ordinary MCs).
The fundamental question is whether the increased modeling power of VLMCs with VLMCs can provide more accurate segmentations than MCs and are also capable of recog-nizing partition points in cases where MCs fail.

We fit an optimal VLMC for each segment of the sequence in order to discover segments of tree is a trade-off between maximizing the maximum likelihood of the segment and the tree complexity.

The challenges in our approach are the following: ( i ) fitting an optimal VLMC to data is many real sources have short segments and the algorithm has to fit VLMCs from sparse data; and ( iii ) the standard dynamic programming algorithm has a quadratic time complexity. BIC and KT criteria. We address ( ii ) by using VLMCs of variable order (maximum depth of techniques.

We conducted experiments on synthetic data, on a variety of DNA sequences and on natural-language text sources. The results show that the method selects gene segments that closely correspond to the currently known (annotated) gene regions. Our segmentation sys-web pages.

The rest of this paper is organized as follows. In Sect. 2 we introduce the notion of tree results. Section 5 reviews related work and Sect. 6 is a short conclusion. 2Treemodels 2.1 Basic definitions A string v is a suffix of s if there exists w such that s = wv .
 c  X  T . Each string c = c d of d edges labeled by symbols c d c d  X  1  X  X  X  c 1 .Weuse  X  to denote the empty context that corresponds to the root of the context tree.
 c ..., P ( a m | c ) ] ,where tree T and the probability assignments  X ( T ) by where c i is the longest suffix of s i 1 that belongs to T .
 we can compute the probability of observing the sequence s n 1 given the model using ( 1 ). [ 0 . P ( 0 | 00 ) = 0 . 1  X  0 . 1  X  0 . 7  X  0 . 1  X  0 . 1  X  0 . 3  X  0 . 5 = 105 10000000 .
In the case that the tree T is known, but the parameter vector  X ( T ) is unknown, one can compute the maximum likelihood estimator (MLE) of the parameter vector denoted  X   X ( T ) .In particular, the MLE for a conditional probability P ( a | c ) is: denote the maximum likelihood of s n 1 .
 depth d .
 In practice, given an input sequence s ,a d -VLMC is built using a two-stage process. Figure 1 shows an example of a 2-MC (top) and a 2-VLMC (bottom) for an alphabet of size node loses between 2 and m  X  1 children nodes as a result of pruning. The idea of virtual number of children.

We u s e M T ={ M ( T , X ( T )) :  X ( T )  X  ( T ) } to denote a set of all models sharing sequence has been generated by a VLMC, we denote by T 0 the generating tree. Accordingly,  X  is the generating parameter vector and d 0 is the depth of the generating tree. A k-segmentation of a sequence is a partition of the sequence in k consecutive segments. A d -VLMC k-segmentation is a segmentation by fitting a d -VLMC and having k segments. We also use the term d -VLMC segmentation to mean a segmentation that selects an optimal number of segments given a bound on the number of segments. We use the term BIC-or KT-segmentation with any of the above to specify the scoring function.

As an information criterion for model selection, we use a variant of the MDL principle that uses the KT code length. The MDL principle says that the best model of the process given the observed sequence is the one that gives the shortest description of the sequence, where the model itself is also a part of the description. For VLMCs, MDL has the following general form code length of the tree. 2.2 The Bayesian information criterion (BIC) and the Krichevski-Trofimov In this section we review the known criteria for selecting an optimal context tree, where an optimal context tree is a trade-off between maximizing the maximum likelihood of the segment and the tree complexity.
 BIC For a d -MC the BIC has the following form [ 7 ] For a d -VLMC the BIC has the following form [ 7 ] where and ML T correspond to the code length of the data given the model while the additional parameters. In statistical terms, BIC has an interpretation as a maximum likelihood method. penalty term equal to the number of free parameters, which prevents BIC from overfitting. KT of s n 1 over all possible parameter assignments p =  X   X  X  0 , 1 ] weighted by the Dirichlet [ 15 ] that the integral has an exact solution effect as pseudo-counts in ( 7 ).

For VLMCs, KT can be expressed using the fact that all symbols corresponding to the same s | c denotes a subsequence of s n 1 corresponding to context c . This leads to the following expression: KT is a minimizer of the worst case average redundancy R n ( T ) for the model class deter-mined by context tree T ,where R n ( T ) = | T | ( m  X  1 ) 2 log ( n ) [ 15 ].
In coding terms, KT corresponds to the mixture coding which consists of the encoding of s and the encoding of the tree [ 10 , 8 ]. Thus, the KT MDL estimator of an optimal context tree of depth up to D is defined as follows [ 28 ]: T given that describes a context tree T using L C ( T ) = m | T | X  1 m  X  1 bits.
We now are ready to define the problem of optimal sequence segmentation using tree models. 2.3 Definition of the problem of optimal sequence segmentation using tree models The problem of optimal sequence segmentation using tree models can be stated as follows. Given:  X  K : the maximal number of segments.  X  D : the maximal depth of the VLMC.  X  cost s j find a vector of partition points I =[ i 1 , i 2 ,..., i k ] ,1  X  k  X  K  X  1 such that where 1  X  k  X  K  X  1, i 0 = 1, i k + 1 = n and B is a border insertion penalty.
The cost function is either or depending on the corresponding tree model selection method used. In experiments we used 2.4 Pruning the tree local search methods such as the Context algorithm [ 21 ]andthe context tree maximization (CTM) algorithm [ 28 ] have to be used. The algorithms Context and CTM work in two stages. from the leaves and proceeding bottom-up.
 Algorithm Context Algorithm Context prunes a context tree as follows [ 5 ].

For every parent node w the algorithm considers every child node u w and marks it for pruning if N n ( u w) &lt; m or u w &lt; K ( n ) ,where is at least one non-marked child then the marked nodes are merged to create a virtual node, complexity is used in place of ( 12 ) as a pruning criterion.
 The context tree maximization (CTM) We now present the original version of the CTM algorithm [ 28 ].CTMfindsatreemaxi-CTM assigns two values: the maximum KT contribution to ( 8 ) of the contexts in the subtree maximizing tree. The algorithm proceeds bottom-up as follows: 1. if v is a leaf node then KT max (v) = KT 0 ( s n 1 | v) 2. if v is an internal node then 3. If KT 0 ( s n 1 | v) &lt; a  X  A KT max ( a v) then I max (v) = 1 else I max (v) = 0. After having visited all nodes, KT max ( r oot ) contains the maximized probability KT  X  T  X  T if I max (v) = 1 then the children are not pruned (they are a part of CTM has to visit all nodes in the tree. 3 TreeSegment: segmentation using tree models In this section we present the TreeSegment algorithm that solves the problem of optimal criteria. 3.1 Pruning according to Context algorithm to minimize the BIC of the tree of the form C log ( n ) ;herewegiveaderivationforthevalueof C in detail.

We start with an example, illustrated in Fig. 2 , which shows a tree rooted at a node w for A T w has been pruned to w and w becomes terminal. Thus, in the presented scenario, the size terminal node u has been pruned including a possible creation of a new virtual node. 1, which leads to and from ( 12 )wehave which finally gives us 3.2 Pruning according to CTM algorithm to minimize KT MDL of the tree We modify the CTM algorithm ( 13 ) to locally minimize the KT MDL score ( 9 ) as follows: tree. As a result of it ( 14 ) produces a sparser tree than ( 13 ).

We also implemented a refinement of criterion ( 14 ) that considers all valid subsets of versus none of the children. The criterion is as follows: is the virtual node; and a v is a child node.
 to s n 1 than by using criterion ( 14 ). 3.3 The segmentation algorithm In this section we present the details of the algorithm TreeSegment. The standard optimal segmentation algorithm can be expressed by the following dynamic programming equation, due to Bellman [ 2 ]:
However, Algorithm 1 achieves a linear speedup by computing W [ j , i ] in constant time T can be updated in constant time, since only one new context has to be added to it. After is ( n 2 ) . The space complexity of the algorithm is ( Kn ) .
 of a parameter . Using this modification, we obtain a suboptimal solution, but the running time of the Algorithm is (( n ) 2 ) . 3.4 The maximum depth of the tree Since we need to fit optimal trees to segments of varying length bounding the maximum ( ii ) it reduces the unnecessary computational complexity of estimating a deeper tree. The only problem with the bound is that it may increases the probability of underestimation by Algorithm 1 : Algorithm TreeSegment least m d to guarantee that on-average every context occurs at least once. 3.5 Border insertion penalties The border insertion penalty can be understood in terms of the Hidden Markov model (HMM) as a transition probability between hidden states of the generating source, where segments an additional parameter and penalized appropriately. Based on our extensive experiments we selected the following penalties for the BIC and the KT scoring methods: B BIC = ( K  X  1 ) 2 and B KT B
BIC follows from the BIC as a parameter penalty. B KT follows from MDL by observing roughly log 2 ( n ) bits for the first point, log 2 ( n 2 ) for the second and so on. 4 Experiments To evaluate results of segmentations obtained by TreeSegment we used the following distance measure D seg ( A , B ) : where on average. The distance is measured as a fraction of the total length of the sequence. So, the measure D ( A , B ) takes values between 0 and 1, where the value 0 means that the two segmentations A and B are identical, while the value 1 can be obtained only for segmen-tations with one segmentation points (and being at opposite ends). Thus, we compute the measures D seg ( BIC , SOURCE ) and D seg ( KT , SOURCE ) , where SOURCE, BIC, and KT segmentation, and the KT segmentation, respectively.
 We conducted our experiments using the following sources: synthetic data in Sect. 4.1 , DNA in Sect. 4.2 and text data in Sect. 4.3 . 4.1 Generated data In this section we use a synthetic sequence to show the advantage of VLMCs over MCs in segmentation. In short, VLMCs are advantages over MCs because if segments were gener-ated using VLMCs then MCs may miss the partition points. This follows from the fact that by using tree models we can fit high order models to segments to maximize the likelihood, without being penalized for an exponential increase in the number of parameters (as in the case of ordinary MCs).

As an illustration of such a case consider the following example synthetic sequence S 12 S of length n each generated as follows: The models generating S are presented in Fig. 3 .Thus,since M 1 and M 2 are identical in terms of 1-MC and 0-MC MC segmentation methods will select M 1 as the underlying model parameters corresponding to the non-existent contexts in the generating tree T 2 .ButaVLMC contexts and therefore it will properly select two segments with the partition point in the middle of S .

We now present figures that show the behavior of the optimal segmentation cost (3.3) for BIC 2-VLMC and KT 2-VLMC segmentation methods. Each figure in this section consists that when added up are equal to the quantity in the third subplot.

Figure 4 presents the BIC 2-MC case. We present this 2-segmentation case for comparison with the VLMC methods since we know that the algorithm selected 1-segmentation using 1-MC instead of the 2-segmentation. The plots of the BIC-penalty and ML are  X  X hoppy X  (2-MC, 1-MC and 0-MC) available to fit to both segments. Also, this figure reveals the fun-damental fact of the BIC segmentation namely the contribution of the penalty is not uniform x ( n  X  x ) is a square function of x with the maximum at x = n 2 .

Figure 5 presents the BIC 2-VLMC method. The area under the BIC-penalty curve is smaller for the 2-VLMC comparing to the 2-MC since here the BIC charges for only the relevant contexts. This example clearly shows the superiority of VLMCs over MCs in seg-mentation. Figure 6 show the results for the KT 2-VLMC method and in particular it compares the MDL KT score ( 11 ) with the ML. A comparison of Figs. 5 and 6 reveals that KT gives a more uniform penalty then BIC but at the expense of a flatter score characteristic. 4.2 DNA sequences In this section we present the results of experiments on DNA sequences. In particular, we consider DNA sequences containing genes and their flanking regions.

We distinguish the following structural regions in DNA [ 6 , 29 ]:  X  5 UTR and 3 UTR (untranslated regions)  X  CDS (coding region) on the directed and complementary strand  X  intron  X  5 flanking and 3 flanking regions, where CDSs and UTRs are part of an exon . Thus, we consider a total of seven functional regions to be segmented by TreeSegment.

In our experiments, we start with studying viral genomes in Sect. 4.2.1 , and then we con-ncbi.nlm.nih.gov and the eukaryotic gene sequences from http://www.ensembl.org . 4.2.1 Viral genomes genomes with the simplest possible structure. For this purpose we selected complete viral genomes from a subset of ssRNA positive-strand viruses that contain exactly three segments: UTR5 ,CDSandUTR3 and segmented them. Our results revealed the following facts: ( i ) for every genome TreeSegment delineates at most three segments, where for most of the the CDS segments have a common subtree of depth D = 1 consisting of contexts C and T . We now show detailed results for Wisteria vein mosaic virus genome (accession point shows segmentation results and Fig. 8 shows a tree built from the CDS region.
In the following experiment we segmented Bacteriophage lambda virus genome that has a long history of being used as a test sequence for demonstrating new segmentation techniques [ 4 , 16 ]. The sequence mostly contains overlapping CDS segments from both DNA strands. Figure 9 shows results for Bacteriophage lambda that are consistent with [ 4 , 16 ]. 4.2.2 Eukaryotic genes We now consider a more difficult task of segmenting eukaryotic genes for which the Tree-lowing experiment for many eukaryotic organisms. We first scanned the respective genomes context trees for those two kinds of sequences using algorithm Context. As an example we present results for Caenorhabditis elegans genome in Table 1 .

The results show that there is a structural difference between the intron and exon trees, where the intron tree is bigger ( | T |= 23 versus | T |= 3) and ( D = 3versus D = 2). Also the exon sequence has a higher CG content while the intron sequence has a higher AT content [ 9 ].
Given the discovered differences in tree structures, we segmented 10 example genes. The results are presented in Table 2 . By comparing the D seg distance measure for BIC and KT we can conclude that both methods perform comparably.

Below we present details of segmentations form Table 2 . To check whether TreeSeg-Drosophila melanogaster gene CG10045-RA, and then we segmented a sequence composed of that gene and flanking regions of length 1,000. The results are shown in Fig. 10 ,where GENE=[UTR5 , Intron12, CDS2, UTR3 ] and in Fig. 11 , where GENE=[Flanking5 ,UTR5 , Intron12, CDS2, UTR3 , Flnaking3 ]. Clearly, after adding the flanking regions the origins of the first exon and the second exon have been properly recognized by the BIC and KT methods. Figure 12 shows segmentation of Drosophila melanogaster gene CG5407-RA.
Figure 13 shows segmentation of Caenorhabditis elegans gene F33E11.3. The gene structure is as follows: GENE=[CDS1, Intron12, CDS2, Intron23, CDS3, Intron34, CDS4, Intron45 and CDS5]. The BIC method merged two regions: Intron23, CDS3 into one region while the KT method recognized all gene regions.

Figure 14 shows segmentation of Caenorhabditis elegans gene Y50D4C.3. The gene structure is as follows: GENE=[CDS1, Intron12, CDS2, Intron23, CDS3, Intron34, CDS4, Intron45, CDS5]. The BIC method merged seven consecutive regions starting from CDS2 while the KT method merged only four regions from CDS2 to Intron34. Also the KT method produced more segments than the annotated segmentation, while the BIC method produced fewer segments.

Figure 15 shows segmentation of Tetraodon nigroviridis gene GSTENT00014173001 for which the BIC method seems to have recognized all gene segments while the KT method merged three gene regions. Also unlike in the case of gene Y50D4C.3 here the BIC method produced more segments than the KT method.
 Figure 16 shows segmentation of Homo Sapiens gene ENST00000246662.

Figure 17 presents segmentation of intron7 of Pan troglodytes (chimpanzee) alpha-fetoprotein precursor (AFP) gene, where intron7 is know to contain distinct homogeneous segments [ 20 ]. The results are consistent with [ 20 ].
Concluding, the presented experiments on DNA sequences reveal two main properties of TreeSegment: ( i ) it tends to recognize boundaries exon X  X ntron in cases where the cor-criteria employed in TreeSegment favor a simpler model that spans a larger region instead the sequence. 4.3 Text In this section we experiment with natural language text corpus. In Sect. 4.3.1 we segment a multilingual text and in Sect. 4.3.2 we experiment with corrupted context separation. We converted the original texts to an alphabet of cardinality 27, which is a set consisting of the English alphabet and the space character. Every space character in the converted text corresponds to a sequence of white spaces between normal characters in the original text. 4.3.1 Multilingual text separation In this experiment we constructed a text sequence composed from three segments: German, English and French translations of Chapter 11 of Robur the Conqueror by Jules Verne. We obtained the text data from http://www.gutenberg.org . In order to convert the German and of the German text we converted the sharp s to  X  X s X  and the umlauts to  X  X e X ,  X  X e X  and  X  X e X , results of the segmentation that show a very accurate separation between the languages. As the figure shows both the BIC and the KT methods selected 1-VLMCs for each segment. As it turns out 0-MC can also separate the texts however not as accurately as 1-VLMC. 4.3.2 Corrupted context separation segment. The purpose of this experiment was to test whether the algorithm was able to dis-cover the corrupted context segment. This experiment presents the following two challenges for the algorithm: ( i ) all three segments have the same symbol composition such that the whole sequence is homogeneous in terms of 0-MC; and ( ii ) since the outer two segments are an almost perfect separation. 5 Related work Tree models and the algorithm Context were introduced by Rissanen in [ 21 ]. Consistency results for tree models were provided by Weinberger et al. in [ 26 ] and by B X hlmann and Wyner in [ 5 ] who also defined the term VLMC. The CTM was introduced by Wilems et al. in the context of coding and modeling. The Bayesian information criterion (BIC) was intro-KT as estimators of the optimal context tree.
 Segmentation algorithms have been central in the analysis of genomic sequences. In [ 17 ], Liu and Lawrence presented a Bayesian approach to DNA segmentation by assuming a 0-MC model and using the KT probability. The optimal number of segments was selected using Bayesian inference. Makeev et al. [ 18 ] studied a Bayesian approach to DNA segmentation boundaries.

Orlov et al. [ 19 ] presented a method for recognizing functional DNA sites and segment-ing genomes. They developed a program  X  X omplexity X  for computing a context tree of a program they analyzed DNA sequences of various functional classes (coding, non-coding and regulatory) and discovered that the DNA structure can be represented by trees.
The problem of DNA segmentation by model selection was posed by Li [ 16 ], where he considered a greedy top-down divide-and-conquer 0-MC segmentation approach to segment DNA by using the BIC and the Akaike information criterion but he did not consider gene segment DNA.
 has been based on the non-uniform codon usage in protein coding segments and has been modeled by non-uniform Markov models and HMMs [ 6 ]. Bernaola et al. [ 3 ] proposed using entropic segmentation for finding borders between coding and non-coding DNA regions. 6 Conclusions We presented a segmentation method that uses tree models to partition the input sequence The MDL principle is used to guide the segmentation process by deciding the optimal tree model in each segment and by deciding the overall number of segments. In our experiments we demonstrated that VLMCs can provide more accurate segmentations than MCs and are also capable of recognizing partition points in cases where MCs fail. In the experiments on DNA we showed usefulness of our method for gene segmentation.
 References Authors biography
