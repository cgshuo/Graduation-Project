 Information retrieval systems (e.g., web search engines) are criti-cal for overcoming information overload. A major deficiency of existing retrieval systems is that they generally lack user model-ing and are not adaptive to individual users, resulting in inherently non-optimal retrieval performance. For example, a tourist and a programmer may use the same word  X  X ava X  to search for different information, but the current search systems would return the same results. In this paper, we study how to infer a user X  X  interest from the user X  X  search context and use the inferred implicit user model for personalized search . We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. We develop an intelligent client-side web search agent (UCAIR) that can perform eager implicit feedback, e.g., query ex-pansion based on previous queries and immediate result reranking based on clickthrough information. Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models, Rel-evance feedback, Search Process Algorithms implicit feedback, personalized search, user model, interactive re-trieval
Although many information retrieval systems (e.g., web search engines and digital library systems) have been successfully deployed, the current retrieval systems are far from optimal. A major defi-ciency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users [17]. This in-herent non-optimality is seen clearly in the following two cases: Copyright 2005 ACM 1-59593-140-6/05/0010 ... $ 5.00. (1) Different users may use exactly the same query (e.g.,  X  X ava X ) to search for different information (e.g., the Java island in Indonesia or the Java programming language), but existing IR systems return the same results for these users. Without considering the actual user, it is impossible to know which sense  X  X ava X  refers to in a query. (2) A user X  X  information needs may change over time. The same user may use  X  X ava X  sometimes to mean the Java island in Indonesia and some other times to mean the programming language. With-out recognizing the search context, it would be again impossible to recognize the correct sense.

In order to optimize retrieval accuracy, we clearly need to model the user appropriately and personalize search according to each in-dividual user. The major goal of user modeling for information retrieval is to accurately model a user X  X  information need, which is, unfortunately, a very difficult task. Indeed, it is even hard for a user to precisely describe what his/her information need is.

What information is available for a system to infer a user X  X  infor-mation need? Obviously, the user X  X  query provides the most direct evidence. Indeed, most existing retrieval systems rely solely on the query to model a user X  X  information need. However, since a query is often extremely short, the user model constructed based on a keyword query is inevitably impoverished . An effective way to improve user modeling in information retrieval is to ask the user to explicitly specify which documents are relevant (i.e., useful for satisfying his/her information need), and then to improve user mod-eling based on such examples of relevant documents. This is called relevance feedback , which has been proved to be quite effective for improving retrieval accuracy [19, 20]. Unfortunately, in real world applications, users are usually reluctant to make the extra effort to provide relevant examples for feedback [11].

It is thus very interesting to study how to infer a user X  X  infor-mation need based on any implicit feedback information, which naturally exists through user interactions and thus does not require any extra user effort. Indeed, several previous studies have shown that implicit user modeling can improve retrieval accuracy. In [3], a web browser (Curious Browser) is developed to record a user X  X  explicit relevance ratings of web pages (relevance feedback) and browsing behavior when viewing a page, such as dwelling time, mouse click, mouse movement and scrolling (implicit feedback). It is shown that the dwelling time on a page, amount of scrolling on a page and the combination of time and scrolling have a strong correlation with explicit relevance ratings, which suggests that im-plicit feedback may be helpful for inferring user information need. In [10], user clickthrough data is collected as training data to learn a retrieval function, which is used to produce a customized ranking of search results that suits a group of users X  preferences. In [25], the clickthrough data collected over a long time period is exploited through query expansion to improve retrieval accuracy.
While a user may have general long term interests and prefer-ences for information, often he/she is searching for documents to satisfy an  X  X d hoc X  information need, which only lasts for a short period of time; once the information need is satisfied, the user would generally no longer be interested in such information. For example, a user may be looking for information about used cars in order to buy one, but once the user has bought a car, he/she is generally no longer interested in such information. In such cases, implicit feedback information collected over a long period of time is unlikely to be very useful, but the immediate search context and feedback information, such as which of the search results for the current information need are viewed, can be expected to be much more useful. Consider the query  X  X ava X  again. Any of the follow-ing immediate feedback information about the user could poten-tially help determine the intended meaning of  X  X ava X  in the query: (1) The previous query submitted by the user is  X  X ashtable X  (as op-posed to, e.g.,  X  X ravel Indonesia X ). (2) In the search results, the user viewed a page where words such as  X  X rogramming X ,  X  X oftware X , and  X  X pplet X  occur many times.

To the best of our knowledge, how to exploit such immediate and short-term search context to improve search has so far not been well addressed in the previous work. In this paper, we study how to construct and update a user model based on the immediate search context and implicit feedback information and use the model to im-prove the accuracy of ad hoc retrieval. In order to maximally ben-efit the user of a retrieval system through implicit user modeling, we propose to perform  X  X ager implicit feedback X . That is, as soon as we observe any new piece of evidence from the user, we would update the system X  X  belief about the user X  X  information need and respond with improved retrieval results based on the updated user model. We present a decision-theoretic framework for optimizing interactive information retrieval based on eager user model updat-ing, in which the system responds to every action of the user by choosing a system action to optimize a utility function. In a tradi-tional retrieval paradigm, the retrieval problem is to match a query with documents and rank documents according to their relevance values. As a result, the retrieval process is a simple independent cycle of  X  X uery X  and  X  X esult display X . In the proposed new retrieval paradigm, the user X  X  search context plays an important role and the inferred implicit user model is exploited immediately to benefit the user. The new retrieval paradigm is thus fundamentally different from the traditional paradigm, and is inherently more general.
We further propose specific techniques to capture and exploit two types of implicit feedback information: (1) identifying related im-mediately preceding query and using the query and the correspond-ing search results to select appropriate terms to expand the current query, and (2) exploiting the viewed document summaries to im-mediately rerank any documents that have not yet been seen by the user. Using these techniques, we develop a client-side web search agent UCAIR (User-Centered Adaptive Information Retrieval) on top of a popular search engine (Google). Experiments on web search show that our search agent can improve search accuracy over Google. Since the implicit information we exploit already naturally exists through user interactions, the user does not need to make any extra effort. Thus the developed search agent can improve existing web search performance without additional effort from the user.
The remaining sections are organized as follows. In Section 2, we discuss the related work. In Section 3, we present a decision-theoretic interactive retrieval framework for implicit user modeling. In Section 4, we present the design and implementation of an in-telligent client-side web search agent (UCAIR) that performs eager implicit feedback. In Section 5, we report our experiment results using the search agent. Section 6 concludes our work.
Implicit user modeling for personalized search has been stud-ied in previous work, but our work differs from all previous work in several aspects: (1) We emphasize the exploitation of immedi-ate search context such as the related immediately preceding query and the viewed documents in the same session, while most previous work relies on long-term collection of implicit feedback informa-tion [25]. (2) We perform eager feedback and bring the benefit of implicit user modeling as soon as any new implicit feedback infor-mation is available, while the previous work mostly exploits long-term implicit feedback [10]. (3) We propose a retrieval framework to integrate implicit user modeling with the interactive retrieval pro-cess, while the previous work either studies implicit user modeling separately from retrieval [3] or only studies specific retrieval mod-els for exploiting implicit feedback to better match a query with documents [23, 27, 22]. (4) We develop and evaluate a personal-ized Web search agent with online user studies, while most existing work evaluates algorithms offline without real user interactions.
Currently some search engines provide rudimentary personaliza-tion, such as Google Personalized web search [6], which allows users to explicitly describe their interests by selecting from pre-defined topics, so that those results that match their interests are brought to the top, and My Yahoo! search [16], which gives users the option to save web sites they like and block those they dis-like. In contrast, UCAIR personalizes web search through implicit user modeling without any additional user efforts. Furthermore, the personalization of UCAIR is provided on the client side. There are two remarkable advantages on this. First, the user does not need to worry about the privacy infringement, which is a big concern for personalized search [26]. Second, both the computation of person-alization and the storage of the user profile are done at the client side so that the server load is reduced dramatically [9].
There have been many works studying user query logs [1] or query dynamics [13]. UCAIR makes direct use of a user X  X  query history to benefit the same user immediately in the same search session. UCAIR first judges whether two neighboring queries be-long to the same information session and if so, it selects terms from the previous query to perform query expansion.

Our query expansion approach is similar to automatic query ex-pansion [28, 15, 5], but instead of using pseudo feedback to expand the query, we use user X  X  implicit feedback information to expand the current query. These two techniques may be combined.
In interactive IR, a user interacts with the retrieval system through an  X  X ction dialogue X , in which the system responds to each user ac-tion with some system action. For example, the user X  X  action may be submitting a query and the system X  X  response may be returning a list of 10 document summaries. In general, the space of user ac-tions and system responses and their granularities would depend on the interface of a particular retrieval system.

In principle, every action of the user can potentially provide new evidence to help the system better infer the user X  X  information need. Thus in order to respond optimally, the system should use all the evidence collected so far about the user when choosing a response. When viewed in this way, most existing search engines are clearly non-optimal. For example, if a user has viewed some documents on the first page of search results, when the user clicks on the  X  X ext X  link to fetch more results, an existing retrieval system would still return the next page of results retrieved based on the original query without considering the new evidence that a particular result has been viewed by the user.
We propose to optimize retrieval performance by adapting sys-tem responses based on every action that a user has taken, and cast the optimization problem as a decision task. Specifically, at any time, the system would attempt to do two tasks: (1) User model updating: Monitor any useful evidence from the user regarding his/her information need and update the user model as soon as such evidence is available; (2) Improving search results: Rerank imme-diately all the documents that the user has not yet seen, as soon as the user model is updated. We emphasize eager updating and reranking, which makes our work quite different from any existing work. Below we present a formal decision theoretic framework for optimizing retrieval performance through implicit user modeling in interactive information retrieval.
Let A be the set of all user actions and R ( a ) be the set of all possible system responses to a user action a  X  A . At any time, let A t = ( a 1 , ..., a t ) be the observed sequence of user actions so far (up to time point t ) and R t  X  1 = ( r 1 , ..., r t  X  1 ) the system has made responding to the user actions. The system X  X  goal is to choose an optimal response r t  X  R ( a t ) for the current user action a t .

Let M be the space of all possible user models. We further de-fine a loss function L ( a, r, m )  X &lt; , where a  X  A is a user action, r  X  R ( a ) is a system response, and m  X  M is a user model. L ( a, r, m ) encodes our decision preferences and assesses the op-timality of responding with r when the current user model is and the current user action is a . According to Bayesian decision theory, the optimal decision at time t is to choose a response that minimizes the Bayes risk, i.e., r t = argmin user model m t given all the observations about the user U made up to time t .

To simplify the computation of Equation 1, let us assume that the posterior probability mass P ( m t | U, D , A t , R t  X  1 centrated on the mode m  X  t = argmax m We can then approximate the integral with the value of the loss function at m  X  t . That is, where m  X  t = argmax m
Leaving aside how to define and estimate these probabilistic mod-els and the loss function, we can see that such a decision-theoretic formulation suggests that, in order to choose the optimal response to a t , the system should perform two tasks: (1) compute the cur-rent user model and obtain m  X  t based on all the useful informa-tion. (2) choose a response r t to minimize the loss function value L ( a t , r t , m  X  t ) . When a t does not affect our belief about first step can be omitted and we may reuse m  X  t  X  1 for m
Note that our framework is quite general since we can poten-tially model any kind of user actions and system responses. In most cases, as we may expect, the system X  X  response is some ranking of documents, i.e., for most actions a , R ( a ) consists of all the pos-sible rankings of the unseen documents, and the decision problem boils down to choosing the best ranking of unseen documents based on the most current user model. When a is the action of submitting a keyword query, such a response is exactly what a current retrieval system would do. However, we can easily imagine that a more in-telligent web search engine would respond to a user X  X  clicking of the  X  X ext X  link (to fetch more unseen results) with a more opti-mized ranking of documents based on any viewed documents in the current page of results. In fact, according to our eager updating strategy, we may even allow a system to respond to a user X  X  clicking of browser X  X   X  X ack X  button after viewing a document in the same way, so that the user can maximally benefit from implicit feedback. These are precisely what our UCAIR system does. A user model m  X  M represents what we know about the user U , so in principle, it can contain any information about the user that we wish to model. We now discuss two important components in a user model.

The first component is a component model of the user X  X  informa-tion need. Presumably, the most important factor affecting the opti-mality of the system X  X  response is how well the response addresses the user X  X  information need. Indeed, at any time, we may assume that the system has some  X  X elief X  about what the user is interested in, which we model through a term vector ~x = ( x 1 where V = { w 1 , ..., w | V | } is the set of all terms (i.e., vocabulary) and x i is the weight of term w i . Such a term vector is commonly used in information retrieval to represent both queries and docu-ments. For example, the vector-space model, assumes that both the query and the documents are represented as term vectors and the score of a document with respect to a query is computed based on the similarity between the query vector and the document vec-tor [21]. In a language modeling approach, we may also regard the query unigram language model [12, 29] or the relevance model [14] as a term vector representation of the user X  X  information need. Intuitively, ~x would assign high weights to terms that characterize the topics which the user is interested in.

The second component we may include in our user model is the documents that the user has already viewed. Obviously, even if a document is relevant, if the user has already seen the document, it would not be useful to present the same document again. We thus introduce another variable S  X  X  ( D is the whole set of documents in the collection) to denote the subset of documents in the search results that the user has already seen/viewed.

In general, at time t , we may represent a user model as m ( S, ~x, A t , R t  X  1 ) , where S is the seen documents,  X  X nderstanding X  of the user X  X  information need, and represents the user X  X  interaction history. Note that an even more general user model may also include other factors such as the user X  X  reading level and occupation.

If we assume that the uncertainty of a user model m t is solely due to the uncertainty of ~x , the computation of our current estimate of user model m  X  t will mainly involve computing our best estimate of ~x . That is, the system would choose a response according to sion mechanism implemented in the UCAIR system to be described later. In this system, we avoided specifying the probabilistic model P ( ~x | U, D , A t , R t  X  1 ) by computing ~x  X  directly with some existing feedback method.
The exact definition of loss function L depends on the responses, thus it is inevitably application-specific. We now briefly discuss some possibilities when the response is to rank all the unseen doc-uments and present the top k of them. Let r = ( d 1 , ..., d top k documents, S be the set of seen documents by the user, and ~x may simply define the loss associated with r as the negative sum of the probability that each of the d i is relevant, i.e.,  X  loss function, the optimal response r would contain the k ments with the highest probability of relevance, which is intuitively reasonable.

One deficiency of this  X  X op-k loss function X  is that it is not sensi-tive to the internal order of the selected top k documents, so switch-ing the ranking order of a non-relevant document and a relevant one would not affect the loss, which is unreasonable. To model rank-ing, we can introduce a factor of the user model  X  the probability of each of the k documents being viewed by the user, P ( view | d and define the following  X  X anking loss function X : Since in general, if d i is ranked above d j (i.e., i &lt; j P ( view | d j ) , this loss function would favor a decision to rank rel-evant documents above non-relevant ones, as otherwise, we could always switch d i with d j to reduce the loss value. Thus the sys-tem should simply perform a regular retrieval and rank documents according to the probability of relevance [18].

Depending on the user X  X  retrieval preferences, there can be many other possibilities. For example, if the user does not want to see redundant documents, the loss function should include some re-dundancy measure on r based on the already seen documents
Of course, when the response is not to choose a ranked list of documents, we would need a different loss function. We discuss one such example that is relevant to the search agent that we im-plement. When a user enters a query q t (current action), our search agent relies on some existing search engine to actually carry out search. In such a case, even though the search agent does not have control of the retrieval algorithm, it can still attempt to optimize the search results through refining the query sent to the search engine and/or reranking the results obtained from the search engine. The loss functions for reranking are already discussed above; we now take a look at the loss functions for query refinement.

Let f be the retrieval function of the search engine that our agent uses so that f ( q ) would give us the search results using query Given that the current action of the user is entering a query a = q t ), our response would be f ( q ) for some q . Since we have no choice of f , our decision is to choose a good q . Formally, which shows that our goal is to find q  X  = argmin q L ( q i.e., an optimal query that would give us the best f ( q ) query refinement strategy. In UCAIR, we heuristically compute by expanding q t with terms extracted from r t  X  1 whenever q have high similarity. Note that r t  X  1 and q t  X  1 are contained in m as part of the user X  X  interaction history.
Implicit user modeling is captured in our framework through the computation of ~x  X  = argmax ~x P ( ~x | U, D , A t , R system X  X  current belief of what the user X  X  information need is. Here again there may be many possibilities, leading to different algo-rithms for implicit user modeling. We now discuss a few of them.
First, when two consecutive queries are related, the previous query can be exploited to enrich the current query and provide more search context to help disambiguation. For this purpose, instead of performing query expansion as we did in the previous section, we could also compute an updated ~x  X  based on the previous query and retrieval results. The computed new user model can then be used to rank the documents with a standard information retrieval model.
Second, we can also infer a user X  X  interest based on the sum-maries of the viewed documents. When a user is presented with a list of summaries of top ranked documents, if the user chooses to skip the first n documents and to view the ( n +1) -th document, we may infer that the user is not interested in the displayed summaries for the first n documents, but is attracted by the displayed summary of the ( n + 1) -th document. We can thus use these summaries as negative and positive examples to learn a more accurate user model ~x ploited [19, 20]. Note that we should use the displayed summaries, as opposed to the actual contents of those documents, since it is possible that the displayed summary of the viewed document is relevant, but the document content is actually not. Similarly, a dis-played summary may mislead a user to skip a relevant document. Inferring user models based on such displayed information, rather than the actual content of a document is an important difference between UCAIR and some other similar systems.

In UCAIR, both of these strategies for inferring an implicit user model are implemented. In this section, we present a client-side web search agent called UCAIR, in which we implement some of the methods discussed in the previous section for performing personalized search through acts as a proxy for web search engines. Currently, it is only im-plemented for Internet Explorer and Google, but it is a matter of engineering to make it run on other web browsers and interact with other search engines.

The issue of privacy is a primary obstacle for deploying any real world applications involving serious user modeling, such as per-sonalized search. For this reason, UCAIR is strictly running as a client-side search agent, as opposed to a server-side application. This way, the captured user information always resides on the com-puter that the user is using, thus the user does not need to release any information to the outside. Client-side personalization also al-lows the system to easily observe a lot of user information that may not be easily available to a server. Furthermore, performing person-alized search on the client-side is more scalable than on the server-side, since the overhead of computation and storage is distributed among clients.

As shown in Figure 1, the UCAIR toolbar has 3 major compo-nents: (1) The (implicit) user modeling module captures a user X  X  search context and history information, including the submitted queries and any clicked search results and infers search session boundaries. (2) The query modification module selectively im-proves the query formulation according to the current user model. (3) The result re-ranking module immediately re-ranks any unseen search results whenever the user model is updated.

In UCAIR, we consider four basic user actions: (1) submitting a keyword query; (2) viewing a document; (3) clicking the  X  X ack X  button; (4) clicking the  X  X ext X  link on a result page. For each of these four actions, the system responds with, respectively, (1) generating a ranked list of results by sending a possibly expanded query to a search engine; (2) updating the information need model ~x ; (3) reranking the unseen results on the current result page based on the current model ~x ; and (4) reranking the unseen pages and generating the next page of results based on the current model
Behind these responses, there are three basic tasks: (1) Decide whether the previous query is related to the current query and if so expand the current query with useful terms from the previous query or the results of the previous query. (2) Update the information need model ~x based on a newly clicked document summary. (3) Rerank a set of unseen documents based on the current model Below we describe our algorithms for each of them.
To effectively exploit previous queries and their corresponding clickthrough information, UCAIR needs to judge whether two ad-jacent queries belong to the same search session (i.e., detect ses-sion boundaries). Existing work on session boundary detection is mostly in the context of web log analysis (e.g., [8]), and uses sta-tistical information rather than textual features. Since our client-side agent does not have access to server query logs, we make ses-sion boundary decisions based on textual similarity between two queries. Because related queries do not necessarily share the same words (e.g.,  X  X ava island X  and  X  X ravel Indonesia X ), it is insufficient to use only query text. Therefore we use the search results of the two queries to help decide whether they are topically related. For example, for the above queries  X  X ava island X  and  X  X ravel Indone-sia X  X , the words  X  X ava X ,  X  X ali X ,  X  X sland X ,  X  X ndonesia X  and  X  X ravel X  may occur frequently in both queries X  search results, yielding a high similarity score.

We only use the titles and summaries of the search results to cal-culate the similarity since they are available in the retrieved search result page and fetching the full text of every result page would sig-nificantly slow down the process. To compensate for the terseness of titles and summaries, we retrieve more results than a user would normally view for the purpose of detecting session boundaries (typ-ically 50 results).

The similarity between the previous query q 0 and the current query q is computed as follows. Let { s 0 1 , s 0 2 , . . . , s { s 1 , s 2 , . . . , s n } be the result sets for the two queries. We use the pivoted normalization TF-IDF weighting formula [24] to com-pute a term weight vector ~s i for each result s i . We define the av-erage result ~s avg to be the centroid of all the result vectors, i.e., ( ~s 1 + ~s 2 + . . . + ~s n ) /n . The cosine similarity between the two average results is calculated as If the similarity value exceeds a predefined threshold, the two queries will be considered to be in the same information session.
If the previous query and the current query are found to belong to the same search session, UCAIR would attempt to expand the current query with terms from the previous query and its search results. Specifically, for each term in the previous query or the corresponding search results, if its frequency in the results of the current query is greater than a preset threshold (e.g. 5 results out of 50), the term would be added to the current query to form an expanded query. In this case, UCAIR would send this expanded query rather than the original one to the search engine and return the results corresponding to the expanded query. Currently, UCAIR only uses the immediate preceding query for query expansion; in principle, we could exploit all related past queries.
Suppose at time t , we have observed that the user has viewed k documents whose summaries are s 1 , ..., s k . We update our user model by computing a new information need vector with a standard feedback method in information retrieval (i.e., Rocchio [19]). Ac-cording to the vector space retrieval model, each clicked summary s can be represented by a term weight vector ~s i with each term weighted by a TF-IDF weighting formula [21]. Rocchio computes the centroid vector of all the summaries and interpolates it with the original query vector to obtain an updated term vector. That is, where ~q is the query vector, k is the number of summaries the user clicks immediately following the current query and  X  is a parameter that controls the influence of the clicked summaries on the inferred information need model. In our experiments,  X  is set to 0 . 5 that we update the information need model whenever the user views a document.
In general, we want to rerank all the unseen results as soon as the user model is updated. Currently, UCAIR implements reranking in two cases, corresponding to the user clicking the  X  X ack X  button and  X  X ext X  link in the Internet Explorer. In both cases, the current (updated) user model would be used to rerank the unseen results so that the user would see improved search results immediately.
To rerank any unseen document summaries, UCAIR uses the standard vector space retrieval model and scores each summary based on the similarity of the result and the current user information need vector ~x [21]. Since implicit feedback is not completely reli-able, we bring up only a small number (e.g. 5) of highest reranked results to be followed by any originally high ranked results.
We now present some results on evaluating the two major UCAIR functions: selective query expansion and result reranking based on user clickthrough data.
The query expansion strategy implemented in UCAIR is inten-tionally conservative to avoid misinterpretation of implicit user mod-els. In practice, whenever it chooses to expand the query, the ex-pansion usually makes sense. In Table 1, we show how UCAIR can successfully distinguish two different search contexts for the query  X  X ava map X , corresponding to two different previous queries (i.e.,  X  X ravel Indonesia X  vs.  X  X ashtable X ). Due to implicit user modeling, UCAIR intelligently figures out to add  X  X ndonesia X  and  X  X lass X , respectively, to the user X  X  query  X  X ava map X , which would other-wise be ambiguous as shown in the original results from Google on March 21, 2005. UCAIR X  X  results are much more accurate than Google X  X  results and reflect personalization in search.

The eager implicit feedback component is designed to immedi-ately respond to a user X  X  activity such as viewing a document. In Figure 2, we show how UCAIR can successfully disambiguate an ambiguous query  X  X aguar X  by exploiting a viewed document sum-mary. In this case, the initial retrieval results using  X  X aguar X  (shown on the left side) contain two results about the Jaguar cars followed by two results about the Jaguar software. However, after the user views the web page content of the second result (about  X  X aguar car X ) and returns to the search result page by clicking  X  X ack X  but-ton, UCAIR automatically nominates two new search results about Jaguar cars (shown on the right side), while the original two results about Jaguar software are pushed down on the list (unseen from the picture).
To further evaluate UCAIR quantitatively, we conduct a user study on the effectiveness of the eager implicit feedback compo-nent. It is a challenge to quantitatively evaluate the potential per-formance improvement of our proposed model and UCAIR over Google in an unbiased way [7]. Here, we design a user study, in which participants would do normal web search and judge a randomly and anonymously mixed set of results from Google and UCAIR at the end of the search session; participants do not know whether a result comes from Google or UCAIR.

We recruited 6 graduate students for this user study, who have different backgrounds (3 computer science, 2 biology, and 1 chem-Figure 3: An example of TREC query topic, expressed in a form which might be given to a human assistant or librarian and TREC 2003 Web track [4] topic distillation task in the way to be described below.
 An example topic from TREC 2004 Terabyte track appears in Figure 3. The title is a short phrase and may be used as a query to the retrieval system. The description field provides a slightly longer statement of the topic requirement, usually expressed as a single complete sentence or question. Finally the narrative supplies additional information necessary to fully specify the requirement, expressed in the form of a short paragraph.
 Initially, each participant would browse 50 topics either from Terabyte track or Web track and pick 5 or 7 most interesting topics. For each picked topic, the participant would essentially do the nor-mal web search using UCAIR to find many relevant web pages by using the title of the query topic as the initial keyword query. Dur-ing this process, the participant may view the search results and possibly click on some interesting ones to view the web pages, just as in a normal web search. There is no requirement or restriction on how many queries the participant must submit or when the par-ticipant should stop the search for one topic. When the participant plans to change the search topic, he/she will simply press a button to evaluate the search results before actually switching to the next topic.
 At the time of evaluation, 30 top ranked results from Google and UCAIR (some are overlapping) are randomly mixed together so that the participant would not know whether a result comes from Google or UCAIR. The participant would then judge the relevance of these results. We measure precision at top n ( n = 5 , 10 , 20 , 30 documents of Google and UCAIR. We also evaluate precisions at different recall levels.

Altogether, 368 documents judged as relevant from Google search results and 429 documents judged as relevant from UCAIR by par-ticipants. Scatter plots of precision at top 10 and top 20 documents are shown in Figure 4 and Figure 5 respectively (The scatter plot of precision at top 30 documents is very similar to precision at top 20 documents). Each point of the scatter plots represents the preci-sions of Google and UCAIR on one query topic.

Table 2 shows the average precision at top n documents among 32 topics. From Figure 4, Figure 5 and Table 2, we see that the search results from UCAIR are consistently better than those from Google by all the measures. Moreover, the performance improve-ment is more dramatic for precision at top 20 documents than that at precision at top 10 documents. One explanation for this is that the more interaction the user has with the system, the more click-through data UCAIR can be expected to collect. Thus the retrieval system can build more precise implicit user models, which lead to better retrieval accuracy.
 Table 2: Table of average precision at top n documents for 32 query topics
The plot in Figure 6 shows the precision-recall curves for UCAIR and Google, where it is clearly seen that the performance of UCAIR Figure 4: Precision at top 10 documents of UCAIR and Google is consistently and considerably better than that of Google at all levels of recall.
In this paper, we studied how to exploit implicit user modeling to intelligently personalize information retrieval and improve search accuracy. Unlike most previous work, we emphasize the use of im-mediate search context and implicit feedback information as well as eager updating of search results to maximally benefit a user. We presented a decision-theoretic framework for optimizing interac-tive information retrieval based on eager user model updating, in which the system responds to every action of the user by choos-ing a system action to optimize a utility function. We further pro-pose specific techniques to capture and exploit two types of implicit feedback information: (1) identifying related immediately preced-ing query and using the query and the corresponding search results to select appropriate terms to expand the current query, and (2) exploiting the viewed document summaries to immediately rerank any documents that have not yet been seen by the user. Using these techniques, we develop a client-side web search agent (UCAIR) on top of a popular search engine (Google). Experiments on web search show that our search agent can improve search accuracy over Figure 5: Precision at top 20 documents of UCAIR and Google Figure 6: Precision at top 20 result of UCAIR and Google Google. Since the implicit information we exploit already naturally exists through user interactions, the user does not need to make any extra effort. The developed search agent thus can improve exist-ing web search performance without any additional effort from the user.
We thank the six participants of our evaluation experiments. This work was supported in part by the National Science Foundation grants IIS-0347933 and IIS-0428472. [1] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, [2] C. Clarke, N. Craswell, and I. Soboroff. Overview of the [3] M. Claypool, P. Le, M. Waseda, and D. Brown. Implicit [4] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.
 [5] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.
 [6] Google Personalized. http://labs.google.com/personalized. [7] D. Hawking, N. Craswell, P. B. Thistlewaite, and D. Harman. [8] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic [9] G. Jeh and J. Widom. Scaling personalized web search. In [10] T. Joachims. Optimizing search engines using clickthrough [11] D. Kelly and J. Teevan. Implicit feedback for inferring user [12] J. Lafferty and C. Zhai. Document language models, query [13] T. Lau and E. Horvitz. Patterns of search: Analyzing and [14] V. Lavrenko and B. Croft. Relevance-based language [15] M. Mitra, A. Singhal, and C. Buckley. Improving automatic [16] My Yahoo! http://mysearch.yahoo.com. [17] G. Nunberg. As google goes, so goes the nation. New York [18] S. E. Robertson. The probability ranking principle in  X  [19] J. J. Rocchio. Relevance feedback in information retrieval. In [20] G. Salton and C. Buckley. Improving retrieval performance [21] G. Salton and M. J. McGill. Introduction to Modern [22] X. Shen, B. Tan, and C. Zhai. Context-sensitive information [23] X. Shen and C. Zhai. Exploiting query history for document [24] A. Singhal. Modern information retrieval: A brief overview. [25] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web [26] E. Volokh. Personalization and privacy. Communications of [27] R. W. White, J. M. Jose, C. J. van Rijsbergen, and [28] J. Xu and W. B. Croft. Query expansion using local and [29] C. Zhai and J. Lafferty. Model-based feedback in KL
