 We derive a tractable and exact computation for the ex-pectation of F -measures. We also demonstrate the non-convexity of this expectation, and investigate errors of ap-proximating the expectation under different settings. Categories and Subject Descriptors: H.3.3 [ Informa-tion Systems ] :Information Search and Retrieval General Terms Algorithms, Measurement Keywords F -measures, Optimisation, Thresholding
F -measures [3], which satisfy several desirable measure-theoretic properties, are commonly used for measuring and comparing different text classification systems in experimen-tal information retrieval. F -measures combine precisions and recalls into single scores, where the relative weighting between the two is governed by a single parameter  X  . An F -measure with parameter  X  is commonly denoted as F  X  , with F 1 being the most commonly used.

Optimisation for F  X  is therefore common, and is normally achieved by converting document rankings to binary deci-sions via thresholds. The thresholds are usually chosen to maximise the F  X  on held-out validation sets, for example in [4]. Such thresholds are fixed independently of the test data statistics. This is contrary to the observation in [2] that  X  X o fixed threshold can be used to optimize F  X   X  because it does not satisfy the Probability Thresholding Principle[2]. [2] argued that we should instead choose a classification that maximise the expected value of F  X  . This conforms to decision theory if one considers F  X  to be utility functions. If the maximisation is done by tuning thresholds, this leads to dynamic thresholds dependent on the test data statistics. This strategy of setting thresholds has been used experimen-tally in [1], where, at least for rare categories, it is shown to perform better than using fixed thresholds.

To our knowledge, there is currently no tractable way of exactly computing this expectation, though [2] gives a tractable approximation. Such perceived intractability may be one reason why the expectation is seldom used in practi-cal thresholding strategies. To ameliorate, we provide in this paper a tractable algorithm for exact expectation computa-tion, and give some empirical observations of the properties of the expectation and its approximation.
 For J k (  X  ), considering the values of z k , we have
X = X = J k +1 (  X  ) + r k J k +1 (  X  + 1) .

In summary, in order to find  X  F  X   X  , we need to calculate recurrent equations and boundary conditions: This suggests a dynamic programming algorithm of space O ( N ) and time O ( N 2 ) for computing exactly  X  F  X   X  . We leave to future work to investigate the methods of difference equa-tions to find solutions or approximations to these equations. Assume that we can compute p i for each document i . Since  X  F  X   X  satisfies the Probability Ranking Principle[2], for optimisation, we should let the documents with the top n probabilities be considered relevant, and adjust n . Then, the expectation is considered a function of n for optimisation.
We perform some experiments for  X  F 1  X  by sampling proba-bilities p i from beta distributions parameterised by ( a,b ), i.e. P ( p i )  X  p a  X  1 i (1  X  p i ) b  X  1 . The sampling proceeds as follows: let x = log( a + b ) and y = a a + b ; for every ( a,b ) determined by a uniform 81  X  81 grid on the plane ( x,y )  X  [  X  2 , 2]  X  [0 . 1 , 0 . 9], 50 sets of 100 probabilities are sampled. In addition, all the experiments use C = 1 for the degenerate case.
Considering the non-degenerate  X  F 1  X  as a function of n , 9 of the 328,050 sampled sets of probabilities induce a non-convex function. These 9 sets are distributed in the lower right portion of the x-y plane. This suggests that if non-convexity should occur for a set of probabilities, it usually happens when the mean and variance of this set is low.
Since the expectation as a function of n is non-convex in general, in order to find the optimal it is required to calcu-late the expectation once for every n , taking time O ( N 3 ). However, excluding the degenerate case, it seems that the function is usually convex. One can therefore trade occa-sional non-global-optimality by using golden section search or similar strategies to find the local maxima within time O ( N 2 log N ). We leave to future work to investigate the conditions for convexity and the rarity of non-convexity.
We investigate the difference between exact and approxi-mate expectations at points of global maximas. We use the approximation from [2] and denote it by  X  X  X  X  X  X  X  .

For each set of probabilities, we compute n  X  and  X  n  X  given by arg max n  X  F 1  X  and arg max n  X  X  F 1  X  X  respectively. The root-mean-squared (rms) of the differences n  X   X   X  n  X  are calculated across the 50 sets. Figure 1(a) plots the contours of the rms values. Also in the figure, the small points indicate cases
