 The population of the Internet is increasing day by day. Now 50 billion webpages are years[1, 2]. This gigantic size of the Inte rnet creates some probl ems too. When people search in the Internet, they are now experiencing overwhelmed with lots of informa-Recommender systems suggest items or persons to a user based on the user's taste so that the user can quickly reach to the point of his interest without going through irre-levant information. Furthermore, a recommendation list itself may be large enough and usually the user considers only the top few items. So it is a challenging problem in recommender systems to rank top items accurately. 
Recommender systems are mainly divided into two categories: content-based fil-tering and collaborative filtering [3]. In recommender systems, an item is recom-mended to an active user based on 1) similarity of the item with other items in which active user, whom active user has similar preference on some set of items. In content-book recommendation, a book is recommended to a user based on the books profile similarity (book type, writer and so on) with other books that were already rated high-filtering uses users choice on items to derive similarity. Collaborative filtering (CF) is widely used in social network and also in online web [3-5]. As the size of the Internet is increasing day by day, scalability becomes an important issue and matrix factorisa-tion is the ultimate solution to handle this scalability problem [6]. 
In matrix factorisation (MF), data are placed into a matrix, which indicates user in-teraction over item. For example, R  X  R N X M is the rating matrix represents preferences on N users over M items. The major purpose of matrix factorisation in this context is to obtain some form of lower-rank approximation to R for understanding how the users relate to the items [7]. User preferen ces over item are modelled by user's latent factors and item preferences by user are modelled by item's latent factors. These fac-product equals to original rating matrix [8]. For example, the target matrix R  X  R N X M is approximated by two low rank matrix P  X  R N X K and Q  X  R M X K widely used for learning these latent factors. 
In this paper, we present a new algorithm for learning latent factors. Our main con-factorisation algorithm, namely GWMF, by using weights to update factor matrix in each steps to learn latent factors. We also introduce a regularisation parameter to re-duce overfitting. Result shows our algorithm achieves converge faster with higher accuracy than the ordinal gradient descent based matrix factorisation algorithm. thod in recommendation. A lot of research was done for further improvement of ac-curacy and reducing computational time. 
In [6] a probabilistic matrix frame (PMF) is proposed to approximate the user fea-ture vector and movie feature vector from user movie rating matrix. The work ad-dresses two crucial issues of recommender system: i) complexity in terms of observed rating and ii) sparsity of ratings. With a cost function by a probabilistic linear model with Gaussian noise, this method achieves 7% performance improvement over Netflix own system [17]. Authors in [9] identify drawback of their previous PMF models [6] as PMF is computationally complicated. To solve the problem, they propose Bayesian Probabilistic Matrix Factorisation (BPMF) where user and movie features have no zero mean which need to be modeled by some hyper-parameters. Therefore, the pre-dictive distribution of the rating is approximated by Monte Carlo algorithm. When the number of features is large, BPMF outperforms PMF in terms of time complexity. A hybrid approach is used in [10] to learn incomplete rating from a rating matrix. values of user and item matrix. These initial values are used by Weighted Non-negative Matrix Factorisations (WNMF). Here they use weights as indicator matrix. Authors in [11] extend WNMF in [10] to Graph Regularized Weighted Nonnegative Matrix Factorisation (GWNMF) to approximate low rank matrices of user and item. They use both user and item's internal information, external information and also information increase recommendation accuracy. 
Clustering is applied with Orthogonal Non-negative Matrix Factorisation (ONMTF) in [12]. ONMTF is applied to decompose a user-item rating matrix into a first similarity between the test user and item is calculated and similar user/item clus-ters are identified. Within the identified clusters, K most similar neighbours are found. The linear combination model based on user-based approach, item-based approach and ONMTF is applied to calculate the test user's rating on the item. Although expe-rimental result does not show any improvement, their method requires less computa-tional cost. 
An ensemble of collaborative filters is applied in [13] to predict ratings on Netflix dataset. As MF algorithm is sensitive to local minima, the author applies different MF algorithms with different initialisation parameters several times. And then the mean of their prediction is used to compute final ratings. 
Authors in [14] develop non-negative matrix factorisation algorithm where they put additional constraint that all elements of the factor matrices must be non-negative. They develop two algorithms by using multiplicative update steps, one attempts to vergence. Both of the algorithms guarantee locally optimal solutions and are of easy implementations. 
To the best of our knowledge, there is no research done by using user or item weights with latent factor vectors to approximate ratings. Our algorithm, GWMF latent factors not approximated the rating between the user and item well in the cur-rent iteration, so that in next gradient step the algorithm can focus on those items and users and give more update on them. We also introduce a regularisation parameter to reduce overfitting. Result shows that GWMF guaranteed lower training and test error and it achieves minimum RMSE faster than gradient descent. descent-based matrix factorisation algorithm (GMF) and our gradient weighted matrix factorisation algorithm (GWMF). Experimental settings and result of GWMF vs. GMF is presented in section 4. Finally we draw conclusions and future work. 3.1 Gradient Descent Algorithm for Matrix Factorisation Given a rating matrix R and mean square error (MSE) as optimisation function, Gra-which minimise the following objective. The algorithm is presented in Algorithm 1. tions steps, dimension of low rank matrix K. 
Output : Factor matrix P and Q. 1 Initialise P and Q randomly. 2 for s=1:steps do 3 for i=1:N do 4 for j=1:M do 5 if R(i,j)&gt;0 5.1 er  X  R(i,j)-(P(i,:)*Q(:,j)); 5.2 P(i,:)  X  P(i,:)+(  X  *er*2*Q(:,j)'-(  X  *  X  *P(i,:))); 5.3 Q(:,j)  X  Q(:,j)+(  X  *er*2*P(i,:)'-(  X  *  X  *Q(:,j))); end for end for end for 6 return P,Q 
As shown in Algorihtm 1, GMF treats each element equally, i.e. assign equal learn-ing rate for each element, in Q when update P and each equally in P when update Q . 3.2 Gradient Weighted Algorithm for Matrix Factorisation vectors P(i,:) and Q(:,j) . The approximation can be formulated as a learning problem for each row R(i,:) : instance, and P(i,:) is the linear regression model parameter. Similarly, for each col-umn R(:,j) , we have: where we can fix matrix P and solve the parameter vector Q(:, j) for a linear regres-sion model. In such a configuration, the matrix factorlisation is formulated as a set of linear regression models: one for each row of P and one for each column of Q . 
One of the common assumptions, underlying most linear and nonlinear least about the deterministic part of the total process variation. More specific, the standard deviation of the error term is constant over all values of the predictor or explanatory variables. However, this assumption does not hold, even approximately, in every modelling application. In such situations, weighted least squares can be used to maximise the efficiency of parameter estimation [18]. 
To this end, giving each instance its proper amount of influence over the parameter estimates may improve the performance of the approximation since a method that influence. 
Therefore, in GWMF, we use weights to optimise approximation of the target ma-more to the update than others. In GWMF we give more weight to instances that are not approximated well by previous iterations. We use two weight matrices 1) user weight WP  X  R N X M that indicates user approximation over item and item weight WQ  X  R N X M that indicates item approximation by user in current iterations. At initiali-changed for each observed user and item. We also enlarge this effect by exponential the same update formula. Furthermore, we also normalise the weight values by max-imum normalisation. world, there may be some outliers or noise in datasets. So this weight update formula weight update formula becomes: From Fig. 1 we see how weight values of WP and WQ changes according to equation 3 in each gradient steps. After the 1st iteration (Fig. 1(b)) the weight values of each user-item vary by a large amount. So more gradient update is performed to user-items that have high weight values than others. When the algorithm converges, user weight values for an item user item become 1. The GWMF algorithm is given in Algorithm 2. tions steps, dimension of low rank matrix K. 
Output : Factor matrix P and Q. To evaluate performance of GWMF, we use two datasets: (i) Small Dataset that consists Testing is done by those users-items rating from u1.test dataset. (ii) The Movielens Data-experiments separately on each five Movielens dataset and average the results. We com-pare the proposed GWMF with the standard GMF algorithm. For fair comparison, we need to make learning rates of these two algorithms equal. To achieve this, we set learn-learning rate of GWMF by initialisation weight values. Here initialisation weight values means if a user rated 5 items, all items receive equal weight values 1/5=0.2. This is be-weight. Here initialisation weight values remain the same for all iterations whereas other weight values (i.e multiplied WP and WQ ) change according to GWMF. In the settings weight value with learning rate of GMF. So this rate is also decreased and learning rate of GWMF and GMF are comparable. 
RMSE is used to measure performance. For the small dataset, as the size of our training set is smaller and thus more sparse, rating prediction is more challenging than that of the original Movielens dataset. 
To see how weights work on matrix factorisation, we perform two experiments on small dataset. In one experiment we use one weight matrix to examine how instances are approximated in previous iterations and give more weight to instances that are not approximated well. In other experiment, we use two weight matrices: the user weight for an item. The idea behind this experiment is: the user factor (item factor) is multip-iterations. RMSE of two weight matrices vs. one weight matrix on smaller version of Movielens dataset is given in Table 1. In Fig. 2, RMSE on test dataset is shown. 
From Fig. 2 we see that MF by two weight matrices performs better that one weight matrix. In this concern we use two weight matrices for the rest of our experi-achieve minimum RMSE for rbeta =0.5. Here in Fig. 3 we present performance of various rbeta values on GWMF with two weight matrices. Fig. 4 and 5 on the smaller dataset and Fig. 6 and 7 for the Movielens dataset. We set rbeta =1 for both dataset and learning rate=.0002 for smaller dataset and 0.01 for Mo-vielens dataset. 
For earlier iterations, it shows GWMF achieves lower RMSE and in 5000 itera-tions GWMF achieves its lowest RMSE. However, GMF reaches its lowest RMSE at 50000 iterations that is still larger than the lowest RMSE of GWMF. In Movielens dataset, GWMF also outperforms GMF (Fig. 6 and 7). On test set GWMF achieves the lowest RMSE in 100 iterations (0.75) whereas GMF achieves GWMF performs better than GMF in terms of accuracy and computational cost in both Movielens dataset. solve this problem. However, to the best of our knowledge, there is no research was done to optimise matrix factorisation by weighted gradient descent. GWMF is the first work in this direction, where we use weights directly in gradients update steps. As gradient descent ing. There is a risk to destroy gradient descents convergence property. But using an appro-priate cost function and regularisation parameter, we achieve this and our results are very encouraging. In future, we plan to apply this algorithm on larger datasets and other prob-lems where gradient descent algorithm is also applied. Acknowledgement. This project is funded by the Smart Services CRC under the Australian Government's Cooperative Research Centre program. 
