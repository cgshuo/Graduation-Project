 Word X  X entiment associations, commonly captured in sentiment lexicons, are useful in automatic sen-timent prediction (Pontiki et al., 2014; Rosenthal et al., 2014), stance detection (Mohammad et al., 2016a; Mohammad et al., 2016b), literary analysis (Hartner, 2013; Kleres, 2011; Mohammad, 2012), detecting personality traits (Grijalva et al., 2015; Mohammad and Kiritchenko, 2015), and other ap-plications. Manually created sentiment lexicons are especially useful because they tend to be more accu-rate than automatically generated lexicons; they can be used to automatically generate large-scale lexi-cons (Tang et al., 2014; Esuli and Sebastiani, 2006); they can be used to evaluate different methods of automatically creating sentiment lexicons; and they can be used for linguistic analyses such as examin-ing how sentiment is composed in phrases and sen-tences.

The sentiment of a phrase can differ significantly from the sentiment of its constituent words. Sen-timent composition is the determining of sentiment of a multi-word linguistic unit, such as a phrase or a sentence, from its constituents. Lexicons that in-clude sentiment associations for phrases as well as for their constituent words are useful in studying sentiment composition. We refer to them as senti-ment composition lexicons (SCLs) . We created SCLs for three domains, and all three were used in recent SemEval shared tasks. We refer to the lexicon cre-ated for the English Twitter domain as the SemEval-2015 English Twitter Sentiment Lexicon ; for the gen-eral English domain as the SemEval-2016 General English Sentiment Modifiers Lexicon ; and for the Arabic Twitter domain as the SemEval-2016 Ara-bic Twitter Sentiment Lexicon . Note that the English Twitter lexicon was first described in (Kiritchenko et al., 2014), whereas the other two are novel contribu-tions presented in this paper.

Most existing manually created sentiment lex-icons tend to provide only lists of positive and negative words with very coarse levels of senti-ment (Stone et al., 1966; Hu and Liu, 2004; Wil-son et al., 2005; Mohammad and Turney, 2013). The coarse-grained distinctions may be less use-ful in downstream applications than having access to fine-grained (real-valued) sentiment association scores. Only a small number of manual lexicons capture sentiment associations at a fine-grained level (Bradley and Lang, 1999; Warriner et al., 2013). This is not surprising because obtaining real-valued sentiment annotations has several challenges. Re-spondents are faced with a higher cognitive load when asked for real-valued sentiment scores for terms as opposed to simply classifying terms as ei-ther positive or negative. Besides, it is difficult for an annotator to remain consistent with his/her anno-tations. Further, the same sentiment association may map to different sentiment scores in the minds of dif-ferent annotators; for example, one annotator may assign a score of 0.6 and another 0.8 for the same degree of positive association. One could overcome these problems by providing annotators with pairs of terms and asking which is more positive (a compar-ative approach), however that requires a much larger set of annotations (order N 2 , where N is the num-ber of terms to be annotated). Best X  X orst Scaling (BWS) is an annotation technique, commonly used in marketing research (Louviere and Woodworth, 1990), that exploits the comparative approach to an-notation while keeping the number of required an-notations small.

In this work, we investigate the applicability and reliability of the Best X  X orst Scaling annotation technique in capturing word X  X entiment associations via crowdsourcing. Our main contributions are as follows: 1. We create fine-grained sentiment composition 2. We show that the annotations on all three do-3. We examine the relationship between  X  X ifferen-4. We calculate the minimum difference in sentime-The data and code created as part of this project (the lexicons, the annotation questionnaire, and the code We now describe how we created three lexicons, through manual annotation, that each provide real-valued sentiment association scores. 2.1 Best X  X orst Scaling Method of Annotation Best X  X orst Scaling (BWS), also sometimes referred to as Maximum Difference Scaling (MaxDiff), is an annotation scheme that exploits the comparative approach to annotation (Louviere and Woodworth, 1990; Cohen, 2003; Louviere et al., 2015). Annota-tors are given four items (4-tuple) and asked which item is the Best (highest in terms of the property of interest) and which is the Worst (least in terms of the property of interest). These annotations can then be easily converted into real-valued scores of asso-ciation between the items and the property, which eventually allows for creating a ranked list of items as per their association with the property of interest.
Given n terms to be annotated, the first step is to randomly sample this set (with replacement) to obtain sets of four terms each, 4-tuples , that satisfy the following criteria: 1. no two 4-tuples have the same four terms; 2. no two terms within a 4-tuple are identical; 3. each term in the term list appears approxi-4. each pair of terms appears approximately in the In practice, around 1 . 5  X  n to 2  X  n BWS questions, where n is the number of items, are sufficient to ob-tain reliable scores. We annotated terms for the three lexicons separately, and generated 2  X  n 4-tuples for each set.

Next, the sets of 4-tuples were annotated through a crowdsourcing platform, CrowdFlower. The annotators were presented with four terms at a time, and asked which term is the most positive (or least negative) and which is the most nega-tive (or least positive). Below is an example an-tated through a similar questionnaire in Arabic.) Each 4-tuple was annotated by ten respondents.
The responses were then translated into real-valued scores and also a ranking of terms by sen-timent for all the terms through a simple counting procedure: For each term, its score is calculated as the percentage of times the term was chosen as the most positive minus the percentage of times the term was chosen as the most negative (Orme, 2009; Flynn and Marley, 2014). The scores range from -1 (the most negative) to 1 (the most positive). 2.2 Lexicons Created With Best X  X orst Scaling S This lexicon is comprised of 1,515 high-frequency English single words and simple negated expres-sions commonly found in tweets. The set includes regular English words as well as some misspelled words (e.g., parlament ), creatively-spelled words (e.g., happeee ), hashtagged words (e.g., #loveu-mom ), and emoticons.
 S This lexicon was created in a similar manner as the English Twitter Lexicon but using Arabic words and negated expressions commonly found in Arabic tweets. It has 1,367 terms.
 S M D
EGREE A DVERBS (SCL-NMA): This lexicon consists of all 1,621 positive and negative single words from Osgood X  X  seminal study on word mean-ing (Osgood et al., 1957) available in General In-quirer (Stone et al., 1966). In addition, it includes 1,586 high-frequency phrases formed by the Osgood words in combination with simple negators such as no , don X  X  , and never , modals such as can , might , and should , or degree adverbs such as very and fairly . More details on the lexicon creation and an analysis of the effect of different modifiers on sentiment can be found in (Kiritchenko and Mohammad, 2016). Table 1 shows example entries from each lexicon. The complete lists of modifiers used in the three lex-these lexicons in SemEval shared tasks can be found in (Rosenthal et al., 2015; Kiritchenko et al., 2016). 3.1 Agreement and Reproducibility Let majority answer refer to the option chosen most often for a BWS question. The percentage of re-sponses that matched the majority answer were as follows: 82% for the English Twitter Lexicon, 80% for the Arabic Twitter Lexicon, and 80% for the General English Lexicon.
Annotations are reliable if similar results are ob-tained from repeated trials. To test the reliability of our annotations, we randomly divided the sets of ten responses to each question into two halves and com-pared the rankings obtained from these two groups of responses. The Spearman rank correlation coef-ficient between the two sets of rankings produced for each of the three lexicons was found to be at least 0.98. (The Pearson correlation coefficient be-tween the two sets of sentiment scores for each lex-icon was also at least 0.98.) Thus, even though an-notators might disagree about answers to individual questions, the aggregated scores produced by apply-ing the counting procedure on the BWS annotations are remarkably reliable at ranking terms.

Number of annotations needed: Even though we obtained ten annotations per BWS question, we wanted to determine the least number of annotations needed to obtain reliable sentiment scores. For ev-ery k (where k ranges from 1 to 10), we made the following calculations: for each BWS question, we randomly selected k annotations and calculated sen-timent scores based on the selected subset of annota-tions. We will refer to these sets of scores for the dif-ferent values of k as S 1 , S 2 , and so on until S 10 . This process was repeated ten times for each k . The aver-age Spearman rank correlation coefficient between S 1 and S 10 was 0.96, between S 2 and S 10 was 0.98, and S 3 and S 10 was 0.99. This shows that as few as two or three annotations per BWS question are suf-ficient to obtain reliable sentiment scores. Note that with 2  X  n BWS questions (for n terms), each term occurs in eight 4-tuples on average, and so even just one annotation per BWS question means that each term is assessed eight times. 3.2 Distribution of Sentiment Scores Figure 1 gives an overview of the sentiment scores in SCL-NMA. Each term in the lexicon is shown as a dot in the corresponding plot. The x-axis is the rank of each term in the lexicon when the terms are ordered from most positive to least positive. The y-axis is the real-valued sentiment score obtained from the BWS annotations. Observe that the lexicon has entries for the full range of sentiment scores (-1 to 1); that is, there are no significant gaps X  X anges of sentiment scores for which the lexicon does not in-clude any terms. The dashed red line indicates a uni-form spread of scores, i.e., the same number of terms are expected to fall into each same-size interval of scores. Observe that the lexicon has slightly fewer terms in the intervals with very high and very low sentiment scores. Similar figures (not shown here) were obtained for the other two lexicons. 3.3 Perception of Sentiment Difference The created lexicons capture sentiment associations at a fine level of granularity. Thus, these annota-tions can help answer key questions such as: (1) If native speakers of a language are given two terms and asked which is more positive, how does human agreement vary with respect to the amount of differ-ence in sentiment between the two focus terms? It is expected that the greater the difference in sentiment, the higher the agreement, but the exact shape of this increase in agreement has not been shown till now. (2) What least amount of difference in sentiment is perceptible to native speakers of a language?
Agreement vs. Sentiment Difference: For each word pair w 1 and w 2 such that score ( w 1 )  X  score ( w 2 )  X  0, we count the number of BWS an-notations from which we can infer that w 1 is more positive than w 2 and divide this number by the to-tal number of BWS annotations from which we can infer either that w 1 is more positive than w 2 or that w 2 is more positive than w 1 . (We can infer that w 1 is more positive than w 2 if in a 4-tuple that has both w 1 and w 2 the annotator selected w 1 as the most posi-tive or w 2 as the least positive. The case for w 2 being more positive than w 1 is similar.) This ratio is the human agreement for w 1 being more positive than w 2 , and we expect that it is correlated with the sen-timent difference d = score ( w 1 )  X  score ( w 2 ) . To get more reliable estimates, we average the human agreement for all pairs of terms whose sentiment dif-fers by d  X  0 . 01 . Figure 2 shows the resulting av-erage human agreement on SCL-NMA. Similar fig-ures (not shown here) were obtained for the English and Arabic Twitter data. Observe that the agreement grows rapidly with the increase in score differences. Given two terms with sentiment differences of 0.4 or higher, more than 90% of the annotators correctly identify the more positive term.

Least Difference in Sentiment that is Percep-tible to Native Speakers: In psychophysics, there is a notion of least perceptible difference (aka just-noticeable difference ) X  X he amount by which some-thing that can be measured (e.g., weight or sound intensity) needs to be changed in order for the differ-ence to be noticeable by a human (Fechner, 1966). Analogously, we can measure the least perceptible difference in sentiment. If two words have close to identical sentiment associations, then it is expected that native speakers will choose each of the words about the same number of times when forced to pick a word that is more positive. However, as the differ-ence in sentiment starts getting larger, the frequency with which the two terms are chosen as most posi-tive begins to diverge. At one point, the frequencies diverge so much that we can say with high confi-dence that the two terms do not have the same sen-timent associations. The average of this minimum difference in sentiment score is the least percepti-ble difference for sentiment. To determine the least perceptible difference, we first obtain the 99.9%-confidence lower bounds on the human agreement (see the thin blue line in Figure 2). The least per-ceptible difference is the point starting at which the lower bound consistently exceeds 50% thresh-old (i.e., the point starting at which we observe with 99.9% confidence that the human agreement is higher than chance). The least perceptible dif-ference when calculated from SCL-NMA is 0.069, from the English Twitter Lexicon is 0.080, and from the Arabic Twitter Lexicon is 0.087. Observe, that the estimates are very close to each other despite being calculated from three completely independent datasets. Kiritchenko and Mohammad (2016) use the least perceptible difference to determine whether a modifier significantly impacts the sentiment of the word it composes with. We obtained real-valued sentiment association scores for single words and multi-word phrases in three domains (general English, English Twit-ter, and Arabic Twitter) by manual annotation and Best X  X orst Scaling. Best X  X orst Scaling exploits the comparative approach to annotation while keep-ing the number of annotations small. Notably, we showed that the procedure when repeated produces remarkably consistent rankings of terms by senti-ment. This reliability allowed us to determine the value of the psycho-linguistic concept X  X east per-ceptible difference in sentiment. We hope these find-ings will encourage further use of Best X  X orst Scal-ing in linguistic annotation.
