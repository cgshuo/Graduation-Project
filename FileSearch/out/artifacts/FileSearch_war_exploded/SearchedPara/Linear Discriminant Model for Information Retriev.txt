 This paper presents a new discriminative model for information retrieval (IR), referred to as linear discriminant model (LDM), which provides a flexible framework to incorporate arbitrary fea-tures. LDM is different from most existing models in that it takes into account a variety of linguistic features that are derived from the component models of HMM that is widely used in language modeling approaches to IR. Therefore, LDM is a means of meld-ing discriminative and generative models for IR. We present two algorithms of parameter learning for LDM. One is to optimize the average precision (AP) directly using an iterative procedure. The other is a perceptron-based algorithm that minimizes the number of discordant document-pairs in a rank list. The effectiveness of our approach has been evaluated on the task of ad hoc retrieval using six English and Chinese TREC test sets. Results show that (1) in most test sets, LDM significantly outperforms the state-of-the-art language modeling approaches and the classical probabilis-tic retrieval model; (2) it is more appropriate to train LDM using a measure of AP rather than likelihood if the IR system is graded on AP; and (3) linguistic features (e.g. phrases and dependences) are effective for IR if they are incorporated properly . H.3.3 [ Information Storage and Retrieval ]: Retrieval models General Terms: Design, Algorithms, Theory, Experimentation Keywords: Language Model, Discriminative Training, Hidden Markov Model, Perceptron, Optimization Language modeling (LM) approaches to information retrieval (IR) assume that the relevance of a document, given a query, can be estimated as the generative probability of the query from the document [23]. One of the most appealing properties of this ap-proach is its ability to incorporate linguistic information of lan-guage such as phrase and dependences into the retrieval model in a systematic manner. For example, [9] assumes a two-step genera-tion process of a query from a document. First, a dependency structure is generated from a document. Then, a sequence of query terms is generated from the dependency structure. This model is conventionally called Hidden Markov Model (HMM), where the dependency structure can be viewed as the hidden variable. In theory, the hidden variable can represent any internal structures of language, which makes HMM a very powerful modeling frame-work in many natural language processing (NLP) tasks. applications for two reasons. The first is from a theoretical per-spective. The parameters of HMM have been traditionally trained using maximum likelihood estimation (MLE), usually with smoothing methods to deal with the sparse data problem. This approach is proved to be optimal in theory under two assumptions: the true distribution of data (documents and queries) on which HMM is based is known; and there are enough training data. Un-fortunately, these assumptions rarely hold in IR tasks. The second reason is from a practical perspective. The performance of IR systems is generally measures in terms of precision and recall (i.e. average precision, AP, in this study), which is loosely associated with the optimization criterion that MLE uses (i.e. to maximize the likelihood of training data). Therefore, the traditional HMM approach may not lead to an optimal solution in realistic IR sys-tems. outperform state-of-the-art HMM approaches in many NLP tasks [2]. The success is mainly attributed to the two properties which would eliminate to some degree the two aforementioned problems of the HMM approach. First, a discriminative model is based on a much weaker assumption that both training data (documents) and test data (queries) are generated from the same distribution but the form of the distribution is unknown. Second, unlike MLE that maximizes the function that is loosely associated with the per-formance measure of IR, discriminative training methods aim to directly optimize the precision and recall. So they potentially lead to a better solution. The performance of discriminative models depends to a large degree upon the selection of an optimal set of feature functions that can best distinguish desired results from undesired ones. However, there is no systematic selection method. Most previous work simply begins with a list of candidate features defined manually (e.g. by a domain expert), and then construct an optimal set empirically by trial and error. have their strength and weakness. The HMM approach assumes a generation process of data so that a sequence of models (e.g. each for a generation step) can be derived to capture linguistic informa-tion. The discriminative training methods, given a set of feature functions, tend to learn a model directly optimized for IR tasks. Our design strategy is to combine the strength of both approaches as follows: We first assume a query generation process, and derive a set of component models within the framework of HMM. We then derive a set of feature functions from the component models, and construct a linear discriminant model (LDM) for IR, where the linear interpolation weights are estimated using discriminative training methods so as to optimize the AP on the training set. The effectiveness of this design strategy will be demonstrated on the task of ad hoc retrieval on six English and Chinese TREC test sets. Results show that in most test sets, LDM outperforms signifi-cantly the state-of-the-art LM approaches and the classical prob-abilistic retrieval model. The robustness of the approach is also studied empirically in this paper. scribes a method of deriving linguistically-motivated feature func-tions within the HMM framework. Section 3 describes the LDM for IR and discusses in detail two discriminative training methods of estimating parameters. Section 4 presents experimental results. The discussion and related work are presented in Section 5. Fi-nally, the paper is concluded in Section 6. In LM approaches to IR, a Markov model is trained on each document d in the collection C to be searched. Then the docu-ments are ranked by the probability that a query q = { q would be generated from the respective document model P ( q | d ). Most state-of-the-art approaches assume a unigram Markov model, where P ( q | d )=  X  i =1... m P ( q i | d ) [30]. by introducing hidden variables. In the context of language mod-eling, the hidden variables can be used to represent any linguistic concepts that may improve the performance of IR but are  X  X id-den X  in the text. Some representative examples of such concepts are semantic chunks (e.g. named entities like person name, loca-tion name, etc.) and syntactic chunks (e.g. noun phrases, verb phrases, etc.). In HMM approaches to IR, documents are ranked by P ( q | d ). But unlike the case of Markov model, a two-stage gen-eration process is assumed when estimating P ( q | d ), as follows. name) to be queried, according to the probability distribution P ( c | d ); Then the user attempts to express each concept by c hoos-ing a sequence of terms, according to the probability distribution For efficiency, in practical systems we usually only consider the most likely c , which can be detected by parsing technologies, described in Section 4. We then end up with the basic form of HMM for IR in Equation (1). wards, and P ( c | d ) as document model . In our system we use one document model where we assume that each concept c ated depending on its preceding concept c i-1 , and each concept is represented by its headword h i which is detected using rules (e.g. the rightmost noun is the head of a NP, and so on). Therefore, the document model is a headword bigram model P ( h i | h i-1 a set of concept models, each of which models a different concept (i.e. estimates the generative probability of a term sequence given a certain type of concept), as shown in Table 1.

NE P ( q X  |NE)=P( h |NE) q  X  q X  P ( q | h , NE), where there are 
FT P ( q X  |FT)=1 if q X  can be parsed by FT grammar, 0 other-noun phrase; VP for verb phrase; NE for named en tities (i.e. person time.); q X  denotes the chunk of query terms, which represent a con-structed in different ways (e.g. person name models are n -gram models trained via MLE on the corpus of person name list whereas factoid models use derivation rules and have binary val-ues). The dynamic value ranges of different concept models can be so different that it is inappropriate to combine all models through simple multiplication as in Equation (1). one for each concept model, to effectively balance the contribu-tion of each com ponent model to the performance of IR. Intui-tively, we would assign a high weight to the component model which is either reliably trained (on enough training data) or repre-sents a salient concept of a query (e.g. proper noun). This moti-vates the use of the LDM framework, which will be described in the next section. Linear discriminant model in this study follows the general frame-work of linear discriminant functions widely used for pattern clas-sification [4], and has been recently intr oduced into NLP tasks in [2]. c , d ), for i = 0, ..., N . The features are arbitrary functions that map  X  X  X  N+1 , where f ( q , c , d ) = { f 0 ( q , c , d ), f parameters of the model are a vector of N + 1 parameters, each for one feature function,  X  = {  X  0 ,  X  1 , ...,  X  N }. The relevance score of a document d with respect to a given query q can be written as criminant functions in [4]. Our method is novel in that most of the feature functions in Equation (2) are derived from the component models in the HMM framework, as shown in Table 1. More spe-cifically, z f 0 ( . ) is called the base feature and is defined as the loga-z f 1 ( . ) is defined as the logarithm of the bigram probability, z f 2 ( . ) is defined as the logarithm of the document model to both traditional discriminative training methods and HMM methods. Most discriminative models use only binary-valued features, while the feature functions in LDM are much more  X  X n-formative X  because they are derived from probabilistic models. In HMM approaches, each com ponent model is optimized independ-ently according to a criterion loosely associated with AP, while all feature functions in the LDM framework can be jointly optimized directly toward the maximal AP on training data. In this study, our training set consists of a set of queries, each with a list of docu-ments whose relevance has been judged manually.  X  under the framework of gradient descent: an iterative procedure of adjusting the parameters  X  in the direction that optimizes the objective function. For each of the two algorithms, we will present in turn the objective function and the optimization algorithm. The first algorithm is to select an optimal parameter setting so as to directly maximize the average precision (AP) on training data. We call the algorithm maximum AP (MaxAP) training . AP is the most common performance measure in the IR research community. So MaxAP is intuitively appealing since it optimizes the perform-ance measure directly. We now give the formal definition of AP, and then describe the algorithm. be the query set in training data. Let Rank( d i , q ,  X  ) be the rank of the i  X  X h relevant document d i (for i = 1,...,| R document list of q , ordered according to the score computed by Equation (2), where  X  is the current parameter setting of the LDM. Then, the AP of a query q is defined as training data, and is defined in Equation (4). function optimization algorithm (e.g. [24]). Assume that we can maximize AP with respect to one parameter  X  using line search , which will be described below. The MaxAP algorithm works as follows: Take  X  0 ,  X  1 , ...,  X  N as a set of directions. Using line search, move along the first direction so that the objective function, as shown in Equation (4), is maximized; then move from there along the second direction to its maximum, and so on. Cycling through the whole set of directions as many times as necessary, until the object function stops increasing. that regular numeric line search methods [24] cannot be applied directly because the value of a parameter  X  versus the objective function AP(.) is not smooth and there are multiple local maxima. Therefore, we use the method proposed in [22]: Let  X  be the se-lected parameter. The line search is to find the optimal value of  X  so as to maximize the average precision. By adjusting  X  within a bracket (i.e. an interval which is known to contain acceptable points), we obtain for each query in training data an ordered se-quence of AP(.) values and a corresponding sequence of  X  inter-vals. By averaging AP(.) values over all queries in training data, we obtained a global sequence of AP(.) and the corresponding global sequence of  X  intervals. We can therefore find the optimal  X  as well as its corresponding AP(.) by traversing the sequence. converge on different maxima given different starting points. Fol-lowing [25], we attempt to perform the algorithm multiple times, each from a different, ra ndom starting point, and pick the parame-ter setting that achieves the maximal AP. This section first formulates the ranking problem under the frame-work of ordinal regression [14, 12]; then presents a loss function which is closely associated with AP, and a perceptron-based algo-rithm to optimize the parameter setting with respect to the loss function. Given a query q , let r *( q ) be the target (or optimal) document rank list, usually judged manually, and r ( q ,  X  ) the rank list gener-ated by the LDM of Equation (2) with the parameter setting  X  . Then, within the framework of ordinal regression,  X  is optimized in such a way that r ( q ,  X  ) is closest to r *( q ). As [14] suggested, the similarity between two ranks can be measured in terms of Kendall X  X   X  , which is defined as follows. A document pair d is concordant, if both r ( q ,  X  ) and r *( q ) agree on how they order d bers of concordant and discordant document pairs, respectively. Note that if there are M documents in the collection C to be Then, Kendall X  X   X  can be defined in Equation (5). See [14] for a full description. The optimization problem can be written as below, where Q is the query set in training data. Equation (6) shows that the optimal  X  is the one that leads to least number of discordant document pairs. It is worth noticing that as proved in [14], the number of inversions Y gives a lower bound on AP. So, optimizing  X  by minimizing Y is closely associated with increasing AP. C is judged by a binary value: 1 if the document is relevant, 0 otherwise. There is no order among relevant (or irrelevant) docu-ments. Therefore, Y in Equation (6) is reduced to the number of document pairs, where the irrelevant document is ranked higher than the relevant document. Using LDM, all documents are ranked by the score computed by Equation (2). Therefore, Equa-tion (6) can be rewritten as follows where I [  X  ]=1 if  X   X  0, and 0 otherwise; d i ment of q in C , and d j is any irrelevant document. tron-based algorithm to search for the optimal parameter setting. Please see [2] for the application of the perceptron algorithm in NLP tasks and the theoretical justifications (i.e. proofs of its con-vergence and bounded generalization errors). It is an incremental, error-correction training procedure. As shown in Figure 1, it starts with an initial parameter setting and adapts it each time a discor-dant pair is detected.

Input: training samples, {( d i , d j ) q ; d i , d j
Output: parameter setting  X  T 1. Initialization: set  X  0 = 1,  X  i = 0, for i = 1 ... N . 2. For t = 1 to T 3. For each training sample ( d i , d j ) q 4. If Score ( q , d j ,  X  t )&gt; Score ( q , d i ,  X  The perceptron algorithm is proved to be robust and guaranteed to converge when the training samples are separable (i.e., there is an ideal parameter setting which leads to a LDM that can achieve zero discordant pair). But in IR tasks, such an ideal parameter setting does not exist, and the training samples are not linearly separable. In theory, this may lead the perceptron algorithm un-stable, and the error-correction procedure can never cease. As [4] point out, if the correction process is determined at some arbitrary point, the parameter setting may or may not be in a good state. We used two methods to deal with this problem. First, to reduce the risk of obtaining a bad solution by accidentally choosing an unfor-tunate termination time, we average the parameter settings pro-duced by the correction rule in Step 6 of Figure 1 as follows: Let  X  t,m be the value for the n  X  X h parameter after the m  X  X h training sample has been processed in pass t over the training data. Then the average parameters are defined as where M and T are the number of training samples and the number of learning iterations, respectively. This variant of the perceptron algorithm is called the averaged perceptron algorithm, proposed in [2]. Second, we count the number of updates for each training sample. If the number is larger than a preset threshold (meaning that the sample cannot be correctly ordered after many trails and is likely to be a noisy sample), the sample will not be used for train-ing in the consequential iterations. As will be illustrated in Sec-tion 4, the two methods lead to a robust algorithm for IR. We evaluated the LDM approach to IR described in the previous sections using six different TREC test sets, including three Eng-lish test sets and three Chinese ones. Some statistics are shown in Table 2, where TX_cn denotes Chinese collections used in TREC-X. (Note that TREC-5 and 6 use the same collection). The English queries are TREC topics 201 to 250 (description field only) on TREC disks 2 and 3. Those topics are  X  X atural language X  queries consisting of one sentence each of length 10 to 15 words. Following [11], for the three English TREC collections, we re-move those queries that have no relevant document. The Chinese queries are TREC topics CH1 to CH79. We use long queries that contain the title, description and narrative fields. The average length of these queries is 120 characters. used for constructing LDM, we process the collections as follows. All Chinese texts have been word-segmented using the word seg-mentation system MSRSeg 2 [8]. The system also identifies fac-toids and named entities of various types. We then used an in-house HMM chunk parser to detect phrases such as NP and VP, as described in Table 1. Similarly, all English texts have been tokenized and chunked by an in-house HMM parser, which is trained on Penn Treebank. (i.e. the binary independent retrieval (BIR) model [15]) and some state-of-the-art language models proposed for IR in the literature. All models contain free parameters that must be estimated empiri-cally by trial and error. These parameters include feature weights in LDM, smoothing or interpolation parameters in language mod-els and weights or constants in the BIR model. Therefore, we have applied an experimental paradigm called two-fold cross validation. For each of the six query set, we divided it into two subsets, with one used for parameter training and the other for test. The re-trieval results reported on each TREC test set (as shown in Tables 3 and 4) combine two sets of results on two subsets of the query set, respectively. Each set of results on one subset is obtained using the parameter settings optimized on the other subset. recall pair. The main evaluation metric in this study is the non-interpolated average precision (AP). The significance tests are also conducted. Tables 3 and 4 present our main experimental results, where we compare LDM with four probabilistic retrieval models, including an implementation of the BIR model and three state-of-the-art language modeling approaches that are based on the framework of either Markov model or HMM. the most representative classical probabilistic retrieval models, and serves as one of the baseline models in our experiments. In particular, we used the Okapi system, which is the best-known implementation of BIR. Among the great number of term weight-ing functions provided by Okapi, we choose BM2500 for it has achieved good performance in previous experiments [27]. gram language model approach to IR proposed in [30]. It serves as the baseline LM approach in our experiments. Over all six TREC test sets, UGM achieves the performance similar to, or slightly worse than, that of BIR. It has been observed that in gen-eral the classical probabilistic retrieval model and the unigram language model approach perform very similarly if both have been fine-tuned. The slightly worse performance of UGM in our experiment might be due to our  X  X ver-tuned X  Okapi system, i.e. BM2500 has more weighting parameters tuned empirically. No-tice that unlike LDM, whose parameters can be trained using ap-propriate learning algorithms as described in Section 3, there is no systematic way of tuning free parameters of BIR and UGM (and other language models described below). So, we first use heuris-tics to find for each parameter a bracket, and perform exhaustive search. gram language model approach to IR. The query generation prob-ability is estimated by P ( q | d ) = P ( q 1 | d )  X  sumes that the query term only depends on its one preceding term. To deal with the sparse data problem, we used two smoothing methods. First, we linearly interpolated the bigram models trained on the document d and the entire collection C , respectively. Sec-ond, for both bigram models, the bigram probability was linearly interpolated with the unigram probability. All n -gram ( n = 1 or 2 in BGM) probabilities are estimated via MLE with a modified version of the absolute discount smoothing [10]. BGM can be viewed as a special case of HMM described in Section 2, where the concept sequence c is a sequence of adjacent word pairs. Re-sults show that BGM substantially outperforms UGM in all Eng-lish test sets, demonstrating that even the simplest c can benefit the IR performance. However, BGM does not outperform UGM on Chinese test sets. A possible reason is that MSRSeg already groups many adj acent short word sequences into long words, such as named entities and compound nouns. Therefore, BGM may not be able to capture additional useful local information compared to UGM. Our speculation has been justified in the pilot study: When using maximal matching to segment words by looking up a small dictionary, BGM outperforms UGM on Chinese test sets. tation of the dependence language model described in [9]. It serves in the comparison experiments as an example of the HMM approaches to IR. DLM uses a score function similar to Equation (1), where c is defined as a so-called linkage . The linkage is de-tected by a parser and is represented an acyclic, planar, undirected graph where two related query terms are connected by a graph edge. DLM assumes a two stage generation process as described in Section 2: First, the linkage is generated from the document according to the distribution P ( c | d ). Second, the query is gener-ated according to the distribution P ( q | c , d ) where each query term is generated depending on the associated term with which it is linked in the linkage. Therefore, DLM can be viewed as an exten-sion of BGM in that c is a set of word pairs where the two words are not necessarily adjacent. This advantage is however paid by the complexity of modeling. DLM uses a more sophisticated smoothing method than BGM does, and introduces more free parameters to be optimized heuristically. As shown in Table 3, although DLM outperforms BGM slightly but significantly on two out of three English test sets, considering the modeling cost, the limited performance gain may not be worthwhile in practical sys-tems. More importantly, it is almost impossible to integrate more sophisticated linguistic concepts through the hidden variables. scribed in Section 3. LDM(MaxAP) is the LDM trained using the MaxAP algorithm described in Section 3.1, and LDM(Percep) is the model trained using the perceptron-based algorithm described in Section 3.2. We see that both LDM methods achieve improve-ments over both BIR and UGM on five out of six query test sets, and also outperform other LM approaches on most of the test sets. As one of the reviewers point out that the comparison may be unfair for the LDM uses more features. We argue that this is the main advantage of LDM to incorporate arbitrary features. When the same set of features are used for both LDM and LMs, similar results were achieved. worse than other models. The reason is that while our parser is trained on news articles from People X  X  Daily, T9_cn is a collec-tion of news articles using Hong Kong Chinese (a local official language). Thus, the language gap between training and test texts leads to a bad result of concept extraction, and LDM cannot be generated properly. Another point worth noting is that FR query set is  X  X nbalanced X  in that a small number of queries have much more relevant documents than the rest. Therefore, the results of LDM are sensitive to the training/test split. We therefore ran-domly create 10 training/test splits. The results reported in this paper are the average among 10 tests. training algorithms. We find that MaxAP is robust across all test sets. It generally converges on both training and test sets after four or five iterations. We do not observe a severe overfitting problem. The perceptron-based algorithm, on the other hand, performed differently across different test sets. We can observe the learning curves of this algorithm on six test sets in Figure 2. Recall that the algorithm aims to minimize the number of discordant document pairs (i.e. Y in Equation (6)). It turns out that the algorithm achieves its goal quite successfully: In all six test sets, Y decreases with the increase of the number of iterations, though the overfit-ting problem can be observed in some test sets. For example, on FR, T5_cn and T9_cn, Y decreases in the beginning and then increases. We also observe a strong correlation between the reduction of Y and the improve-ment of AP in half of the test sets (WJS, AP and T6_cn). Globally, the perceptron algorithm is less robust than MaxAP. 
In summary, the experimental results show that (1) LDM significantly outperforms the state-of-the-art LM (2) It is more appropriate to train whatever IR model to op-(3) Linguistic features (e.g. phrases and dependences) are ef-The IR problem can be reformulated as a pattern classification problem via many ways, one of which is that, given a query, each document of the collection to be searched is classified into two classes: relevant and non-relevant. Probabilistic classifiers have been typically grouped into two categories: generative and dis-criminative models. The former learn a model of the joint prob-ability P ( x , y ) of the input x and the label y , and make predictions by using the Bayes rules to calculate P ( x | y ), and picking the most discriminative classifiers are preferred to generative ones due to several compelling reasons, one of which, as pointed out by Vap-nik [29], is that  X  X ne should solve a (classification) problem di-rectly and avoid solving a more general problem as an intermedi-ate step (such as modeling P ( x | y )). X  be viewed as generative models. For example, in the LM approach to IR [17, 30], assume each document is a unique class ( y ), and the task of IR model is to classify a query ( x ) into its most likely class as given by the posterior P ( y | x ). Then, language models make their prediction by using the Bayes rules to estimate P ( x | y ). Similarly, [19] shows that the classical probabilistic models in-cluding the BIR model [15] and its variant, the two-Poisson model [26], also belong to generative models. Although all these generative models achieve state-of-the-art performance in large scale IR experiments, there are several appealing reasons to ex-plore discriminative models for IR. (Readers can refer to [19] for a detailed discussion.) The first reason is the issue of the modeling assumption in generative models, as discussed in Section 1. Gen-erative models used in IR assume a multinomial distribution where query terms (or term pairs in bigram models) are generated independently by the document model. However, this independ-ence assumption is observed to be false in reality. Moreover, a single document cannot be regarded as a large training set for language model learning. Discriminative models, on the other hand, make very few assumptions on model form. They explore arbitrary features that can differentiate correct labeling versus wrong labeling. Even if a feature function (which for example is derived from a probabilistic model) is poorly estimated due to the sparse data problem, it can still bring some positive impact on the performance of the combined linear discriminative model, pro-vided that the weights are properly learned using an appropriate algorithm. arbitrary features. In generative models, as described earlier, the incorporation can be achieved in two ways (see e.g. [9, 20, 18, 28]). The first approach is to model those features as hidden vari-ables of HMM and integrate them into the generation process. The second approach is to model those features independently, and combine them as a mixture model. The problem of the first ap-proach is its complexity. The parameters of different component models are difficult to learn consistently via MLE. In the second approach, the interpolation weights can only be determined by empirical means and cannot guarantee the optimality. In discrimi-native models, the features can be arbitrary functions, while the feature weights are learned to optimize an objective function which is defined to be closely related to the evaluation measure of IR systems. viewed as a version of discriminative models. As discussed in [20], being a discriminative model, the parameters can be fit ei-ther to maximize the conditional likelihood on the training set, or to minimize training error. In LDM, the parameters  X  are learned either to maximize the AP directly, or to minimize the discordant document pairs in a rank list on the training set. In that sense, LDM belongs to the latter version of discriminative models, which is more truly in the  X  X pirit X  of discriminative learning. approach, most of them focus on parameter learning algorithms and do not explore thoroughly the problem of how to derive fea-tures. They either assume a set of pre-defined features or merge multiple preferences provided by multiple experts directly. In the latter case, each expert can be viewed as a feature function, solely on which a rank (i.e. preference) is based. In our approach, most feature functions are derived from the component models of HMM, and are expected to be more tractable and informative. Below we review some of the parameter learning methods that are closely related to ours. The ranking SVM proposed by Joachims [14] is related to the perceptron-based algorithm described in Section 3.2. If we de-compose the sum term in right-hand-side of Equation (7), and rewrite each decomposed term as a constraint of the form to that of a classification SVM on pairwise difference vector  X  ( f ( q , d )  X  f ( q , d j )), and can be solved using decomposition algorithms similar to those used for SVM classification. In our experiments, we have adapted the SVM light algorithm [13]. It achieves similar performance to that of the perceptron-based algorithm. special case of ordinal regression discussed in [12]. In ordinal regression, all objects are ranked on the same scale, while in IR documents need to be ranked with respect to one query. boosting algorithm to linearly combining multiple ranks provided by experts. If we consider each expert as a feature function, it appears to be equivalent to our problem. However, they do not consider explicitly the distribution over queries. In our pilot study that uses the two fold cross-validation method (see Section 4.1), we observe a serious overfitting problem of the boosting algo-rithm. The optimal parameter setting leaned on one query subset works poorly on the other subset. If we re-define the two fold cross-validation for each query, i.e. half documents for training and the other half for test, then the boosting algorithm works well. However, in realistic IR systems, we have to handle unseen que-ries which might be very different from previously seen queries. We leave the extension of the method to future work. Earlier work along this line can be found in [1]. by Crammer and Singer [3]. Unlike the perceptron-based algo-rithm described in Section 3.2, which reduces the total rank into a set of document pairs, the Pranking algorithm maintains a total ordered set via project, and adjusts the position of documents in the rank directly by adjusting model parameters. Thus, the algo-rithm is theoretically more efficient. It will be interesting to com-pare it in the LDM approach in large scale TREC experiments. We leave it to future work. ple example of the non-smooth optimization (NSO) algorithms [5]. Most parameter learning problem in NLP and IR tasks can be considered as a multi-dimensional function optimization problem. However, the objective function, such as the classification error rate of a given finite set of training samples, is usually a non-smooth function (i.e. a piecewise constant function) of the model parameters, and thus cannot be easily optimized using regular gradient-based numerical methods. Therefore, the line search methods of optimizing non-smooth function form a valuable re-search in IR. Our results show that the line search method (i.e. MaxAP), though simple, achieves even slightly better results than the perceptron-based method in some test sets. This is largely due to its property of directly optimizing the performance measure (i.e. AP). Similar observations have been reported in the experiments on machine translation [22, 25] and LM [7]. Though the method shows empirical benefits, the lack of theoretical underpinnings (such as optimality and stability) is the major concern. We leave it to further study. An alternative approach to the NSO problem is to use an approximated but smoothed objective function that can be easily optimized, such as the one suggested by Juang et al. [16]. The comparison of those NSO methods forms another area of future work. We have presented a discriminative model for IR, referred to as LDM. It provides a flexible framework to incorporate effectively a wide variety of features, including linguistically motivated fea-tures. We have also demonstrated that the feature functions that are derived from component model under the framework of HMM provide useful information for IR. When integrated in LDM, they achieve significant improvements over state-of-the-art language models and the classical probabilistic retrieval model on the task of ad hoc retrieval on six English and Chinese TREC test sets. Thus, our method demonstrates an interesting meld of discrimina-tive and generative models for IR. There are plenty of interesting problems left for future work, as described in Section 5. We are particularly interested in exploring the theoretical underpinnings of the learning algorithms presented in Section 3, such as robust-ness, scalability and stability, without which we cannot prove that our methods always work well. [2] Collins, Michael. 2002. Discriminative training met hods for [4] Duda, Richard O, Hart, Peter E. and Stork, David G. 2001. Pat-[5] Fletcher, R. 1987. Practical methods of optimization. John [8] Gao, Jianfeng, Mu Li, Andi Wu and Changning Huang. 2004. A [10] Gao, Jianfeng, Joshua Goodman and Jiangbo Miao. 2001. The [15] Jones, K. S., S. Walker and S. Robertson. 1998. A probabilistic [16] Juang, Biing-Hwang, Wu Chou and Chin-Hui Lee. 1997. Mini-[17] Lafferty, John and Chengxiang Zhai. 2001. Document la nguage [23] Ponte, J. and W. B. Croft. 1998. A la nguage modeling approach [28] Song, F. and Croft, B. 1999. A general la nguage model for in-[29] Vapnik, V. N. 1999. The nature of statistical learning theory. [30] Zhai, C., and J. Lafferty. 2002. Two-stage la nguage models for 
