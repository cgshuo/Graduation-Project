 Computer Engineering and Information Technology Department, Amirkabir University of Technology, Tehran, Iran 1. Introduction
A classification method partitions a dataset into a number of classes with respect to a priory known information about the labels of the patterns [14,39,55,57,68]. A wide range of methods are introduced to accomplish corresponding task. By combining different concepts [1,9,15,17,20,25,31,33,34,43,44,59, 60,63,65,67] or by applying various enhancements such as modifying the geometrical shape of patterns (e.g. kernel function [7,13,19,27,49]) or dimension reduction methods 1 (e.g. PCA [8,22,55], manifold learning [6,8,22,51,64,66] or coding [12,35,47,61]), the classification task may be performed in another space in which patterns might be separable with more accuracy or less complexity. However, without considering the enhancement which is applied to increase the accuracy of a classifier, there are limited basic concepts by which a method classifies patterns. These concepts can be divided into the following categories [4]. (1) Bi-classifier based methods: A bi-classifier is a decision hyperplane which is able to classify some (2) Mono-classifier (or center) based methods: In Bayes decision theory [14,49,55,57] or clustering (3) Dynamical system based: Memories (e.g. Hopfield) are the examples of dynamical system based
Since inner product is the main operator in traditional form of classifiers, the complexity of corre-sponding algorithm depends on the required number of inner products in feature space. As a result, the complexity of a classifier is usually more than/equal to the number of classes. By applying back propagation method [14,39,55,57,68] or tree (hierarchical) approaches [3,10,36,41], it is possible to reduce complexity (especially computational complexity) to a lower bound related to the number of classes. However, corresponding decrease could not reach the bound of  X  X nly one inner product X  in a complexity is required in practice. Adding more neurons to a multilayer layer perceptron [14,39,55, 68] or applying k -competition approach in hierarchical methods are such examples [3,36,41]. Reducing computational complexity is necessary in online applications including decision making and controlling aircrafts, web mining, medical instruments and etc.

In order to reduce computational complexity effectively, a new concept for multi-classification task is presented in [4] which is called Mapping to Optimal Regions . Compared with the concepts of bi-classifier and mono-classifier based methods, MOR is a Multi-classifier . As a result, it is a special pur-pose method for multi-class classification. 2 MOR applies only one simple mapping (an inner product) to classify patterns. In order to define such mapping, a code assignment process is applied which assigns to each cluster of patterns a unique code. Corresponding process enriches the mapping of multi-classifier by the topological information of feature space. These codes play the role of the labels with less effect on the problem called bad labeling. Since there is no need to assign a code to each disturbed pattern, corresponding strategy makes the process more robust to the noise.

For a given pattern, mapping is defined theoretically from feature space to the corresponding code; to map to the code exactly. As a result, it is necessary to define optimal regions around each code in which patterns with a same label/code are mapped effectively. It is the reason why the new method is called mapping to optimal regions. The optimal domain of the regions is estimated using a multi objective cost function [5,48] to increase the region size and the generalization ability [23,49,57] of the mapping and to reduce the mapping error. Estimation of optimal domain concerns the theories of numerical analysis [18, 53] and regularization [23,49,57].

By considering the advantages of MOR, it presents a different concept as a multi-classifier. It is shown that it classifies a considerable number of linearly separable classes (e.g. 39 classes) in high dimensional feature space using only one inner product [4]. The Hierarchical version of MOR (HMOR) is intro-duced for datasets with high number of classes or low dimensional feature spaces. It is shown that the computational and memory complexities of (H)MOR are significantly less than the other classifi-cation concepts [4]. However, applying a k -competition approach to obtain better accuracy in HMOR is not possible, since k -competition is the advantage of decision confidence in mono-classifier based approaches [3,28 X 30,36,41].

In order to eliminate the requirement to the hierarchy in HMOR and to obtain better accuracy, Mapping to Multidimensional Optimal Regions (M 2 OR) is proposed in this paper. In M 2 OR, an inner product is partitioned to a number of sub-mappings which are applied in l ower dimensional s paces. As a result, it is possible to learn more optimal regions using computational complexity which is approximately equal to one inner product in a high dimensional feature space. By taking the advantage of parallel computing, it is possible to share the partitioned mappings between different processors. Therefore, classification time is reduced considerably.

In this paper the Theorem of Solution Existence for MOR family (i.e. MOR and M 2 OR) states the sufficient condition for partitioning the feature space. Furthermore as a multi-classifier, MOR family generalizes the upper bound of Vapnik-Chervonenkis (V.C.) entropy and growth function [39,49,56,57]. Corresponding notions are updated proportionally for real functions. Additionally, it is showed that the value of V.C. dimension [39,49,56,57] of MOR family is controllable using the parameters of the model. The arrangement of the chapters is as follows: In Section 2, we will review the first version of MOR. In Section 3, M 2 OR and its theoretical advantages are presented. Some experimental results are prepared in Section 4 and finally, Section 5 contains conclusions and future works. 2. Mapping to optimal regions
The concept of MOR as a multi-classifier is presented in [4]. However, current section reviews corre-sponding concept and related advantages. 2.1. Challenges of constructing a multi-classifier In this section, we analyze the error produced by applying an inner product as a multiclass classifier. Corresponding mapping is defined by vector a , given by Eq. (1).
In the above relation x i is i th training sample which is augmented with one and normalized [68] and y i is corresponding label. R is the set of real numbers and l is the number of training samples. Superscript .

T stands for transpose operator. Using each pattern x i as the i th row of matrix X andbydefining Y = [ y 1 ,...,y l ] T , the estimation of a is given as: where . + is Moore-Penrose pseudo inverse operator [32,38,42,45,50,54].The two main challenges which Bad labeling occurs when close patterns do not have close labels. The second problem is due to the distribution of patterns in feature space. Such error is related directly to the radius of cluster sphere. 2.1.1. Bad labeling
With respect to Eqs (1) and (2), the estimated label of the pattern x i (i.e.  X  y i ), is given by:
As a result,  X  y i is a weighted summation of all training labels. In Ch. 6.4 of [68], Zurada restates that x version of X T .Since w = x T i .X + is obtained by the inner products between x i and the columns of X + to x i . Therefore, in Eq. (3), w emphasizes the labels of closer patterns. With respect to the role of w to emphasize the labels of closer patterns,  X  y i is affected by these labels. That means if there are patterns which are topologically close together; but they do not have close labels, then  X  y i will be a number between their labels. We call this problem bad labeling.

To demonstrate the effect of bad labeling, a simple example is performed for digit recognition problem using MNIST dataset [21]. In this dataset, the digits are first augmented with one and normalized [68]. The matrix X is formed by the first 3000 training digits. The i th value of w is called the  X  degree of affecting  X  for an estimated label (see Eq. (3)). Following the estimation of each label, the 10 greatest values of w are determined. The labels corresponding to these elements have more degree of affecting on the estimated one. In this experiment, the digit  X 9 X  is presented for label estimation. As shown in Fig. 1, it is noted that some of the most affecting digits are 7 s, which have the same slop as 9. The estimated weighted label for the given pattern is 8.1 (instead of 9), which is a number between the labels of 7 and 9.

As a result, the effect of bad labeling is occurred when most affecting patterns (which are topologically similar to the given one) does not have close labels. In Section 2.2, it is explained that the effect of bad labeling is reduced, by assigning close codes to closer patterns. 2.1.2. The effect of distribution of patterns
Even if close patterns have close labels, error is not avoidable using an inner product as a multiclass classifier. That is due to the distribution of patterns. As mentioned earlier, a clustering method can be employed for the classification task. In order to classify C classes, at least C clusters are required. By considering c j  X  R n +1 ( 1 j C ) as cluster centers and r j  X  R as radius of a sphere contains patterns belong to j th cluster,  X  y i is in the following bound: ( e In Eq. (5), by increasing the value of r ji or  X  a , the bound of error increases. Since this error depends on the distribution of patterns around the cluster center, error occurrence is unavoidable. By mapping to the optimal regions, it is shown that this kind of error can be corrected in many cases. 2.2. Raw codes and optimal codes In order to increase the probability of a ssigning appr opriate codes, HSOM is a pplied. In each level of HSOM nodes are expanded with a fixed branching factor ( bf ). The hierarchy is expanded until reaching a specific level. Patterns accepted by sibling sub-clusters, are all accepted by a unique parent. Therefore, these patterns are topologically close together. Using Eq. (6), closer codes are assigned to the sibling sub-clusters [4].
In Eq. (6), . is the ceil operator. The assigned codes to sub-clusters at the bottom layer of HSOM, the training patterns.

Before introducing the multi objective cost function to estimate optimal domain, the definition of mapping to optimal codes is required. As a result, in this section it is assumed that the optimal domain of the regions ( D o ) is known. D o is the distance between the center of a region and its border. The raw optimal code is calculated by multiplying an odd integer number in D o . When optimal codes and raw codes are arranged in increasing order, the correspondences between them are determined. The vector C Y ). At this step, the mapping from input space to optimal codes is estimated using Eq. (7),
By considering the effect of distribution of patterns, the optimal code of i th pattern is given by: 2.3. Estimation of optimal domain
In order to apply MOR, the optimal value of D o is required. Increasing the value of D o leads to an extended domain of mapping. Therefore, it seems that greater values of D o is more advantageously. On the other hand, smaller values of D o have another benefit, while generalization ability and the error of the mapping f ( x ) are proportional to D o .

The advantage of an extended domain is refereed to numerical analysis [18,53]. With respect to error correction process of MOR, when a pattern is mapped to a correct region, the error of corresponding mapping is correctable. As a result, a greater domain increases the numbers that a pattern can be mapped correctly to them.

On the other hand, smaller values of D o have another benefit, while generalization ability and the error its derivation with respect to x (i.e.  X  x f ) [54]. From Eq. (7), it is known that: more generalization ability. The dependency between the error of the mapping ( error o )and D o is given by:
In Eq. (10), error raw is the error of the mapping to the raw codes. For the approximation C o which satisfies Eqs (9) and (10), it has shown better performance if both raw and optimal codes have a balance distribution around zero. As explained earlier on the importance of the value D o , its value is determined by a multi objective minimization cost function formulated in Eq. (11),
The first term of Eq. (11) concerns increasing D o while the second term minimizes the error of the mapping f ( . ) . According to Eq. (10), corresponding error depends on error raw (which is a constant) and D o . As a result, minimizing E causes decreasing of D o , which results in decreasing  X  x f indirectly. With respect to the convexity of E in R + , it is proved that it has a unique solution in corresponding domain [5,48]. Another important aspect is as follows: since E is a multi-objective cost function, a weighed summation of objective terms is necessary to emphasize the importance of them. In general, the proper adjustment for such weighs in a multi objective cost function is a challenging task [23,48,49, 56,57]. In [4], without changing the main concept explained before, E is reformulated using other forms of D o and D o . error raw (i.e. E 2 and E 1 in Eqs (12) and (13)).
The solution D o in all of the formulations Eqs (11) X (13) correlates inversely with error raw .Bydeter-D o . As a result, D o is estimated using the general form presented in Eq. (14).

In Eq. (14),  X  is a free parameter adjusted according to the problem. With respect to the experimental results of [4], a set contains 4 members is enough to adjust corresponding value. The term D max is appeared in Eq. (14) to avoid occurrence of infinite values for D o . 2.4. The MOR algorithms and advantages
The required steps for training of MOR are as follows: 1-Assign raw codes by using HSOM. 2-Estimate D o by employing Eq. (14). 3-Assign optimal codes with respect to D o and C raw . 4-Estimate f ( . ) by applying Eq. (7). 5-For each training pattern find corresponding optimal code by using Eq. (8). 6-The label of each optimal region is the major lable of accepted patterns by corresponding optimal
An important note about MOR is that HSOM is applied to include the topological information of feature space into the multi-classifier. Therefore, there is no need to keep its information after training. In order to apply MOR to find the label of some test patterns in practice, following steps are required: 1-For each test pattern find corresponding optimal code using Eq. (8). 2-Retrieve the label of optimal region.

Using the first version of MOR, it is possible to classify a considerable number of linearly separable classes in high dimensional feature space using only one inner product (e.g. 39 classes in R 405 ). Although MOR reduces complexity in comparison to the traditional concepts of classifiers, it is not able to classify data sets with more number of classes (e.g. 57 classes) or low dimensional datasets. The problem stems from the fact that the normalized patterns which are on a hyper strip around  X  a , are mapped to the same region. Figure 2 shows such situation in R 3 . Corresponding probability increases for more number of classes or low dimensional datasets. In the case of patterns which are not normalized, hyper cubes with infinite length and width play the role of hyper strips.
In order to classify such patterns, a Hierarchical version of MOR (HMOR) is presented. In Training process of HMOR, each region that does not pass a specified threshold of accuracy, is expanded. As a result, the optimal domain and the mapping function for each new expansion is specially defined for the patterns accepted by that region. The process of expansion is recursively continued for every optimal region which is not passed the defined accuracy test. An example of applying HMOR to classify 39 linearly separable classes in R 255 is illustrated in Fig. 3. The computational complexity of applying HMOR for corresponding task is two mappings in R 255 .

To demonstrate the potential abilities of MOR, linearly separable datasets with the following specifi-cations are used. We use n  X  n _ set = {5, 55, 155, 205, 255, 305, 355, 405} as the dimension of patterns and C  X  C _ set = {3, 12, 21, 30, 39, 48, 57} as the number of classes. Classification is accomplished for each member of C _ set  X  n _ set . The maximum value of Computational Complexity ( CC )intermsof number of inner products is 3.03 for HMOR. In 61.9% of the cases, the value of CC is less than 2. As a result, the computational complexity of the proposed method is significantly lower than the number of classes. Maximum Likelihood (ML), Two Layer Perceptron (2LP) and (H)SOM are the other classifiers which are applied to compare the complexities. Since datasets are linearly separable, there is no require-ment to extra enhancements or combination with the state of the art methods. As a result, it is possible to compare only the abilities of the basic concepts of classifiers. Since (H)MOR takes the advantage of a multi-classifier, it has considerable low computational complexity which is not related to the number of classes. However, more enhancements are required to apply it in real datasets. 3. Mapping to multidimensional optimal regions
Although the complexity of HMOR is lower than traditional concepts of classifiers, its accuracy for complex datasets is not acceptable. Unfortunately, applying a k -competition approach to obtain better accuracy is not possible. That is the effect of applying a multi-classifier in contrast to mono-classifier based approaches in which each mono classifier has an individual decision confidence [14,28 X 30,39,49, 55,57]. However, it is possible to reduce computational complexity to one inner product with consider-able enhancement in accuracy using M 2 OR. In Section 2.4 it is explained that the normalized patterns which are on a hyper strip around  X  a , are mapped to a same region. By multiplying the original norm of normalized patterns in f ( x )= x,  X  a , the equation of the linear surfaces for un-normalized patterns can be calculated. For more easy comprehension, patterns are considered un-normalized from this point forward. As a result, MOR partitions the feature space using a number of hyper cubes and HMOR par-titions some of hyper cubes again using an inner product in R n . It is worth reminding that for each new expansion in a hierarchical method, a special subset of dimensions is more effective. Corresponding fact is the main idea behind M 2 OR which defines a set of sub-mappings on a partitioned feature space. 3.1. M 2 OR definition
M 2 OR proposes an enhanced version of MOR without imposing extra computational complexity. It partitions mapping f ( . ) to a number of sub-mappings which are applied in lower dimensional spaces. As a result, it does not modify the complexity of the one dimensional version considerably. However, since the hyper cubes (defied by the single mapping of MOR) are partitioned into detailed sub-cubes, accuracy increases significantly. Figure 4 illustrates mapping to two dimensional regions schematically. It is worth reminding that estimating and applying each sub-mapping is independent from others. As a result, training and testing process of M 2 OR can takes the advantage of parallel computing [11]. Definition 1: Mapping to m -dimensional optimal region is defined as f = ( f 1 ,...,f m ) such that
The sequence of  X  k determines which subset of the dimensions will be applied for the sub-mapping f are considered for the formation of M 2 OR. The condition n j =1  X  kj &gt; 1 guarantees that the dimension of each subspace is more than one. The training process of M 2 OR is summarized in Fig. 5. After de-termining the multidimensional optimal region of each training sample, corresponding region is labeled with respect to the major lable accepted by it.

In order to have better accuracy of M 2 OR during test phase, the probability of mapping to unlabeled regions should be considered. Since the neighbor regions accept topologically close patterns, it is prob-able that the label of an unlabeled region be equal to the major label of closest neighbors. K Nearest Neighbor (KNN) methods require to compute l number of distances (for l number of samples) to find the K nearest samples [14,55]. However, M 2 OR retrieves only the label of neighbors by modifying the phase. After labeling optimal regions, the sub-mappings are applied in the test phase as illustrated in Fig. 6. 3.2. V.C. entropy and growth function of multi-classifiers Denoted by bf and d as the branching factor and the deep of HSOM respectively (Section 2.2), number of optimal regions for each f k will be bf d . Therefore, m number of sub-mappings in M 2 OR is able to address bf d.m number of optimal regions. By increasing d , error propagation in HSOM increases. As a result, a limited value for d is considered in this paper. The number of optimal regions is controllable using bf and m . With respect to the flexibility in the number of optimal regions and by proper partitioning the feature space, M 2 OR is able to learn every sample set theoretically. In a special case, each optimal region can contain one sample. The ability of learning for a set of machines is measured according to the size of its hypothesis space which results in V.C. notions. The generalized upper bounds of V.C. entropy and growth function [49,56,57] for the set of multi-classifiers are given in Lemma 1 and Theorem 1. Definition 2: A set of multi-classifiers (called  X  )shatters l number of training samples in every dif-samples.
 Lemma 1: The maximum V.C. entropy for the set of multi-classifiers is l. log C ,where C is the number of classes and l is the number of training samples.
 Proof: For each sample, C different labels can be considered. As a result total different shattering ways of samples, H  X  l is defined as:
It is known that H  X  l is a distribution dependent property for a set of functions. Since growth func-tion is defined independent of distribution, it is a preferred alternative [49,56,57]. However, both of these options are non-constructive. Therefore, V.C. dimension which is a constructive and distribution independent property is applied in Theorem 1 to find a generalized upper bound for growth function. Definition 3: V.C. Dimension ( h ) is the cardinality of the largest subset of samples which can be shattered using a multi-classifier in every different way.
 Theorem 1: The generalized upper bound for growth function for a set of multi-classifiers is given in Eq. (17).
 Proof : With respect to Lemma 1, the proof for l h is clear. In case of l&gt;h , Eq. (18) is proved for bi-classifiers (indicator functions) [56]:
Since for a subset with i samples, there are C i number of shattering ways, Eq. (18) is updated for multi-classifiers as follows: Corollary 1: By considering the problem of quantized real function estimation as a multi-classification task, the upper bound of the growth function for corresponding estimation is given by Eq. (17), where C = ( B  X  A ) / X  (  X  is the quantization interval and A f ( . ) B ). Since the problem is defined in a quantized system, the hypothesis of working with bounded functions is true.
 According to Corollary 1, the multi-classification task is the sub-problem of real function estimation. In general, it is not possible to argue about the specifications of a problem with respect to its simplified sub-problem; except when it has been demonstrated that the simplified version converges to the original one within a determined condition. By considering  X  equal to the quantization interval of the quantized system, the sub-problem of multi-classification task is equivalent to the super one. However, by applying the set of bi-classifiers ( C =2 ) [49,56,57], it is not possible to argue about V.C. dimension, growth function and other related concepts for the set of real functions as a more general super problem. 3.3. V.C dimension and the potential of learning of M 2 OR Theorem 2: The upper bound of V.C. dimension for MOR family (mapping to one/multi dimensional optimal regions) is given in Eq. (20). Proof: Since it is possible to address bf d.m number of optimal regions, maximum number of h = bf d.m samples can be shattered using M 2 OR. m = 1 is applied for the first version of MOR and m&gt; 1 for M 2 OR.

Theorem 2 demonstrates maximum potential of learning in MOR family. However, Theorem 3 analy-ses whether there is a condition in which the proposed multi-classifiers fail to accomplish classification sented. By adding extra zero elements for each  X  kj =0 to  X  a k accordingly, an equivalent virtual mapping MOR family by applying a same notation in R n .
 Theorem 3 (Solution Existence): The sufficient condition for partitioning the feature space using MOR family (i.e. MOR and M 2 OR) is that training samples are not integrated at one point. Proof: According to the explanations in Sections 2 and 3, the concept of MOR (which includes vir-tual M 2 OR) fails in classification if exists no (virtual) mapping which partitions the feature space to clusterable patterns. Corresponding cond ition is defined mathematically as follows: where c is a constant. In other words, by considering any  X  a  X  R n , training samples are on a hypeperplane which is defined by parameters  X  a and c . Number of equations which are defined using Eq. (18) is infinite. By applying n number of linearly independent of  X  a s, for each 1 i l , a unique value of x i is calculable [63]. Since the set of equations for all 1 i l are equivalent, a unique value for all of x i is determined. As a result, if MOR family fails in partitioning the feature space, all of the samples are integrated at one point.
 3.4. Computational and memory complexities of M 2 OR
By considering total number of inner products in R n as Computational Complexity ( CC ), correspond-ing value to apply M 2 OR ( CC-test ) is given as follows:
In Eq. (22), m k =1 n k = n is total number of multiplies to apply sub-mappings and m is the number of divisions to find the index of optimal code. It is assumed here that the cost of division and multiply is the same. r is the cost of retrieving the label of optimal region. Retrieving each label requires m  X  1 number of multiplies which is a compiler task. As a result, r is not more than m  X  1 . Since the last terms of Eq. (22) is not considerable in comparison to n , CC is approximately equal to 1 for high dimensional datasets (i.e. CC _ test this paper. Experimental results confirm that such a low computational complexity is enough to fit on a complex sample set. It is worth reminding that by taking the advantage of parallel computing [11], total time required to compute Eq. (22) is reduced considerably (Fig. 6).

CC of the training phase ( CC-train )in R n is related to the complexities of HSOM ( O ( T.l ) ), inversion part of Moore-Penrose operator to estimate sub-mappings ( O ( n 2 ) ) 3 [65] and labeling the unlabeled regions ( O ( u.r.K/n ) ). For the presented CC s, T is total training steps of HSOM and u is the number of unlabeled regions which have at least one labeled neighbor. Depends on the parameter setting of M 2 OR, corresponding CC is polynomial and might be affected by the labeling part when the upper bound of V.C. dimension increases. All of the three major parts of M 2 OR algorithm can be shared in a parallel computing framework (Fig. 5).
 Memory Complexity ( MC ) which is the number of stored real vectors in R n is presented by Eq. (23). where m k =1 n k is the required memory for sub-mappings, bf d.m is the size of the table which stores the label of patterns and c is a constant equal to 1/8 to compute total real memory units. The latter is the affecting factor on MC . As a result, the memory complexity can be a considerable cost. However, with respect to the fast development in memories, time resources are more important than memories. In Section 4, the values of CC for more complex datasets are given. 4. Experimental results
The potential abilities of (H)MOR are presented i n [4], in which corresponding method is applied to classify C number of linearly separable classes ( 3 C 57 )in n dimensional feature space ( 5 n 405 ). In this paper, the ability of fitting the proposed method on more complex datasets is demonstrated. Although by applying various enhancements (e.g. kernel function), better accuracy can be expected, the aim of this paper is presenting the theoretical abilities of M 2 OR.
Table 1 presents the specification of the datasets which are applied in this paper. The dimensionality and the number of training and test samples are different in these datasets. MNIST [21] is the set of handwritten digits with 60000 training samples and 10000 test samples. Each digit have been size-normalized and centered in a fixed size (28  X  28) image. COIL100 [40] contains color images of 100 different objects which are turned by 5  X  . As a result, there are 72 images from different views for each object. In order to train M 2 OR, 18 images from each object (which are turned 20  X  ) are applied and the 54 remaining images are used to test.

Similar to [26], from each image of COIL100, 292 dimensional features are extracted. Each extracted feature contains 64  X  3 dimension for the histograms of Lab channels, 64 dimensional histogram of Discrete Cosine Transformation (DCT), 8 dimension for Hu moments in addition to the logarithm form of their absolute values, 10 dimensional shape information includes centroid, compactness, perimeter, eccentricity, circularity, aspect ratio, elongation, maximum and minimum diameters; in addition to the logarithm of their absolute values. Other datasets which are Forest Cover Type (Forest), Wall Following Robot (Robot) and Segmentation (Segment), are downloaded from UCI repository [69]. No feature extraction is applied on the UCI datasets and also on MNIST digits. Table 2 presents the parameter settings of M 2 OR for different datasets. 4.1. Partitioning the feature space Partitioning each f eature space is accomplished with respect to corresponding natural co rrelations. In case of MNIST, each two sequences of rows of the 28  X  28 image is regarded as the feature of each sub-mapping. For COIL100 dataset, original feature vector is partitioned into 8 sub-features as follows: the histogram of channels L, a, b and DCT, Hu moments, the logarithm of absolute values of Hu moments, the shape information and corresponding logarithm value. In case of other datasets, each feature is partitioned to lower dimensional sub-features sequentially. 4.2. The controllability of V.C. dimension
Figure 7 demonstrates the effect of parameters bf and d on the train and test error of COIL100 with m = 8and m I = 4. By increasing the value of bf , train error converges to 0; however, test error increases considerably. Corresponding result confirms Theorems 1 and 2. The best adjustment of the parameter bf is 2 for d = 3and3for d = 2 which leads to the upper bound of 2 24 and 3 16 for V.C. dimension respectively. The least percent of train and test errors in Fig. 7 are 3.7% and 19% for d = 3 and 1.3% and 19.9% for d = 2.
 Figure 8 presents the percent of unlabeled test patterns for m = 8 with and without considering the label of neighbors. Accurate label propagation using the neighbor regions verifies the expectation that M 2 OR preserves the topological information of the patterns. Additionally, Fig. 8 confirms that by increasing the upper bound of V.C. dimension, the probability of mapping to unlabeled regions increases. In case of d = 3, bf = 2and d = 2, bf = 3, corresponding probability is lower than 0.2. Corresponding computational complexity is equal to 1.05 (in terms of number of inner products in R 292 )(Table3).
The effect of optimal regions dimensionality ( m ) is shown in Fig. 9 for MNIST dataset using bf = 2, d = 2and m I = 3. Figure 9 confirms again the advantage of Theorems 1 and 2. That is by increasing the value of m , the bias of training error converges to zero. However, the test error has a decreasing order until reaching to m = 14 and after that an increasing order is pursued. The percent of unlabeled regions increases by increasing the upper bound of V.C. dimension. The train and test errors are 4.4% and 22.4% using bf = 2, d = 2, m = 14 and m I = 3. 4.3. Enhancements of M 2 OR
In this paper by considering different datasets with various specifications, better accuracy of M 2 OR in comparison to the original version (HMOR) is demonstrated. Table 3 presents the error rates of these methods in addition to their complexities. By applyi ng more enhancements (Section 5) better accuracy can be expected for both of them; however, confirming the theoretical advantages (Section 3) of M 2 OR in fitting on complex datasets using least computational complexity is studied in the presented work.
As explained before, the reason why the accuracy of HMOR is not considerable is in the requirement of hierarchical methods in applying k -competition approach. In the hierarchical methods, k -competition is a way to increase the accuracy of it. The value of k increases up to 8 which increases the computational complexity [41]. However, k -competitio n requires decision c onfidence for each clus ter/class of a mono-classifier based approach. According to Table 3, applying multidimensional optimal regions increases the accuracy of MOR compared with the hierarchical version.

In Table 3, the values of CC-test for M 2 OR are given using Eq. (22). Corresponding values for HMOR are the mean depth of the hierarchy for train and test samples. The values of MC for M 2 OR are given by Eq. (23). In the case of HMOR, total required number of mappings is considered as MC .Forlow dimensional datasets, divisions in Eqs (22) and (23) are accomplished with lower values of n .Asa result, the values of CC-test and MC are more than to the high dimensional one. It is worth reminding that computational complexity plays a major role in online applications. According to the explanations in Section 1 and Table 3, M 2 OR presents outstanding lower CC which does not depend on the number of classes. However, more enhancements in the accuracy of M 2 OR are required to make it comparable with the accuracy of the state of the art methods (see Section 5). 5. Conclusion and future works
In this paper, Mapping to Multidimensional Optimal Regions (M 2 OR) which proposes a major en-hancement on its original and hierarchical versions, is presented. Theoretical and experimental results confirm that M 2 OR by taking the advantage of multi-classifier, fits on complex data sets using the least expected computational complexity (approximately one inner product in a high dimensional feature space). By applying the benefits of parallel computing, the required time for classification can be more reduced.

Another advantage of M 2 OR is that since the topological information of sample set is included in the sub-mappings, the neighbor clusters of patterns map to the neighbor regions. It means that M 2 OR preserves the topological information of patterns. As a result, the label of an unlabeled region is the major label of its neighbors. However, unlike K-Nearest Neighbor methods, the index of the neighbor is easily determined with respect to index of unlabeled one. As a result, labeling the unlabeled regions is considered as an offline process. It is worth reminding that the proposed method does not guarantee a low memory complexity similar to its computational one; however, the computational cost is more important in comparison to the memory especially in online applications.

The upper bound of V.C. dimension of M 2 OR is controllable using the parameters of the model. With respect to the Theorem of Solution Existence, MOR family classifies every partitionable feature space. Other theorems generalize concepts related to V.C. entropy and growth function for multi-classifiers. Additionally, they update corresponding concepts for real functions.

Although M 2 OR increases the accuracy of HMOR considerably, its accuracy should be comparable with the accuracy of the state of the art of classifiers. Applying M 2 OR in Reproducing Kernel Hilbert Space (RKHS) [49] is one that remains as the proposing future enhancements. Enriching M 2 OR for online learning is another important respect. By taking the advantageous of manifold learning methods, the code assignment process of MOR family for clusters of patterns may be improved. In this paper, feature space is partitioned with respect to its natural correlations. It can be accomplished using more analytic solutions.
 Acknowledgments This paper is supported in part by Information and Communication Technology (ICT) under grant T-19259 X 500 and by National Elites of Foundation of Iran.
 References
