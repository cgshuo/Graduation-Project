 The KDD-Cup 2005 Competition was held in conjunction with the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. The task of the KDD-Cup 2005 competition was to classify 800,000 internet user search queries into 67 predefined categories. This task is easy to understand, but the lack of strai ghtforward training set, subjective user intents of queries, poor info rmation in short queries, and high noise level make the task very challenge. In this paper, we summarize the competition task, the evaluation method, and the results of the competition. Here we only highlight some key techniques used in submitted solutions. The technical details of the solutions from the three award winning teams are available in their papers separately in this issue of SIGKDD Explorations. At the end, we also share the results of a survey conducted with this year X  X  Cup participants. To facilitate research in this area, the task description, data, answer set, and related information of this KDD-Cup are published at the KDD-http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html. KDD-Cup, search, query, categorization, user intent, machine learning, data mining, text cla ssification, text mining, knowledge discovery Computer search including internet search has become a part of many people X  X  daily life and work. Given the exponential growth of information X  X  availability in electronic form, search becomes one of the most important and effective approaches to finding correct/relevant information to serve our needs. A user can type in key words in a search engine to find out where to buy a product certain medicine, he/she can find plenty related information including its usage and potential side-effects. Researchers can easily find the latest development of a research topic. To plan a hiking trip for a coming weekend, one can find the weather forecast simply through a search. These are just a few examples of how search can help a user X  X  daily life. Although researchers and industr y practitioners have achieved tremendous success in developing sm art search engines, we are Zijian Zheng was at Amazon.com during the time of KDD-Cup 2005. still facing many great challenges as current search engines are not very accurate. The difference is still quite big between what search engines can do and what we expect them to do. It is not uncommon that a search engine re turns irrelevant, misleading or, incorrect results after you type in a query. In another time, the relevance results are returned but down to the bottom of a long result list. Due to the nature that huge amount data is available from each search engine and many problems of search can be turned into mining techniques to contribute to the success of search. Since late 90 X  X , researchers and practitioners have been studying understanding search user intents [10][11][12], and providing personalized search [13][14][15][16]. A survey on search related research is available [9]. The other side of search being a difficult problem is that the information contained inside the data is often incomplete, fuzzy, subjective. They are sometimes ambiguous and dynamic. All of these present big challenges to the data mining community. In KDD-Cup 2005 [2], we presented one challenge problem: Search query categorization. Text classification and categori zation is a well-known topic in Information Retrieval and Te xt Mining fields. Manning and Scht X ze [5] discusses general me thodologies and applications of text categorization. Most work in this area has been focused on categorizing Web pages or longer te xt or corpus. However, search subjective user intents on the other hand. Therefore, how to automatically understand user sear ch intents given the search queries would be very intere sting to IR and text mining researchers. the participants in Section 2, including a discussion on why this task is challenging. In Section 3, we present the evaluation method. Then, we highlight the interesting techniques from the submissions in Section 4, and analyze the overall results as a whole in Section 5. The detailed presentations of techniques from the three winning teams are available as three separate papers in this issue of SIGKDD explorati ons. Finally, we summarize in Section 6. This year's competition is in the area of search query categorization. The task was to classify 800,000 search queries into 67 predefined categories. We provided: Participants are required to tag each of the 800,000 queries with up to 5 categories. This task desc ription and dataset are available http://www.acm.org/sigs/sig kdd/kdd2005/kddcup.html .
 The 67 categories are listed in Table 1. They are hierarchical with has several second level sub categor ies. They cover most of the areas in the internet information space. The purpose here is to provide a reasonable set of categories to make the competition meaningful in real life. We do not claim that this is the best category set in any sense. The 800,000 search queries were randomly selected from MSN [3] search logs with some preliminary filtering. The small set of 111 queries with tagged categories (up to five) was provided as a sample to demonstrate the mapping between query strings to categories. It is not intended to be used as a training set in the conventional tr aining/test sense. Of course, they like. The goal of this year X  X  KDD-Cup is to challenge the data mining community with this open res ource problem. Although every one who used search engine can unde rstand the task, it presents the following six challenges: To solve this problem, particip ants must develop/use multiple technologie and acquire extra information externally. They are opened to all the resources they can access. This simulates the real life search engine (or any related components) development well. In this year X  X  KDD-Cup, we used three evaluation criteria on each solution submitted by participants: Precision, F1, and creativity. Precision and F1 are defined below. They are commonly used measures in the area like information retrial and data mining [4][5]. Creativity was judged by reviewers based on creative ideas in the solution, its scalability, and the level of automation. Three awards were set with one for each of the three criteria. They are: In the remaining part of this s ection, we describe the submission requirements for cup participants , submission evaluation process, and award winner selection process. There were about 70 days between the task and dataset available online and the solution submission deadline. The required submission from each participant team consists of: Due to the fact that a system can be tuned differently for optimizing precision and F1 respectively, we allowed each team to submit one solution for the Precision Award and one solution for the Performance Award. In this case, the solution dedicated for the Precision Award is not qualified for the Performance Award even its F1 score is high. Similarly the solution dedicated for the Performance Award is not qualified for the Precision Award even its precision is high. Some teams raised an issue that they do not want to share their technique details since those are their companies X  confidential intellectual property. This is understandable, and we gave participants the option of not submitting solution descriptions. Any team that did not submit solu tion descriptions would not be considered for the Creativity Award. To evaluate a solution submitted from a participant, we need to know the correct or standard category labels of the 800,000 queries. One way to get this da ta is through human labeling. However, manually labeling a ll the 800,000 queries is too expensive and time consuming. The approach we took was: The participants were made aware of this evaluation process together with the task description, but they did not know which subset of the 800,000 queries will be used in the final evaluation. After the competition, we published the answer sets at the KDD-http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html Following this evaluation process, we obtained the overall precision and F1 score values of all submitted solutions. They are listed in Table 2. Each submitted solution was given a unique ID. These evaluation results are used to select award winning teams. For the Creativity Award, a usual paper review approach was used. We had four reviewers. Each of them reviewed the submitted technology/algorithm descriptions of all teams separately. The review focused on the creative ideas used for how easy the solution can be automated. Table 2. Overall precision and F1 score values of all submitted solutions For each of the three evaluation criteria, two teams were selected to receive the corresponding award and runner-up respectively based on their evaluation results on the criterion. Table 3. Qualified solutions for the Performance Award ranked on overall F1 Table 4. Candidate solutions for the Precision Award ranked on overall Precision We observed good techniques and interesting ideas from the submitted solution descriptions. They differ mostly in the approaches for designing the solutions and the learning methods used. Here, we highlight the ma jor techniques adopted by most participants. No solutions submitted are the sa me. We have seen reasonable differences in the approaches used by the contestants. We, however, abstract the solutions from the contestants as consisting of the following three high level components: There are several big benefits to preprocessing the queries: Although some preliminary filtering was performed by the organizers on the raw queries fo r removing obvious inappropriate content, the participants still have to handle some noisy and versatile forms of data, such as misspelling, foreign language words, acronyms, etc. It is interesting to see people in Data Mining and Information Retrieval fields to apply existing text mining preprocessing techniques and create new methodologies for processing search queries specifically. The common methods adopted by c ontestants are mostly standard text mining techniques: stop word s filtering, stemming, and term frequency filtering, etc. Some participants also applied more advanced techniques, such as, spelling correction, compound word breaking, abbreviation expansion, and named entity detection. Search queries can be treated as a semi-natural-language with implicit and subjective user intent s. On the other hand, a search query itself often only contains a few features in a very sparse feature space. Many queries only have one term in it. Sometimes it is hard to learn the meaning of a query solely based on the query itself. Many participants came up with different ways of gathering extra information to augment the query terms. Participants mostly utilize the unlimited knowledge base available on the World Wide Web and search engines. They used the following resources to build the knowledge base: search engines/Internet search result snippets, bag of words, search engine directories, search result titles, or search result web pages. Some of them also adopted tools like WordNet [6] to expand queries. Besides gathering extra information for augmenting search queries, some contestants also tried to build semantics for the categories. Tools like WordNet [6 ] and WikiPedia [7] were used to build category descriptions. This year X  X  competition task provided three major dimensions: search queries, words or phrases, and categories. Participants used the following two major modeling approaches to building the models based on these dimensions. Most of the participants adopted machine learning algorithms in their solutions. Among them, Na X v e Bayesian classifier, SVM, KNN, Neural Network, Logistic Regression are the popular ones. A few participants constructed multiple models and then combined them together to achieve better results. Some of them used distance/probability as criteri a to combine predictions. Some applied ad hoc rules in combining predictions. Others adopted ensemble learning (e.g. Boosting). More advanced methods were adopt ed to tune model parameters, such as using manually tagge d examples to tune model tuning/training. Other learning approaches taken by participants are transforming multi-class problem into multiple binary-class problems and Iterative learning. As we have mentioned earlier, search queries can be treated as a semi-natural-language with implic it and subjective intents. Simple preprocessing techniques, such as stemming and stop word filtering, are not sufficient in capturing the meaning of a search query. A lot of query processing techniques were considered by the competition contestants: Some people used substring/ partial matching and regular expression matching in text processing and inferences. Knowledge representation is also an important aspect. The way queries or other knowledge are re presented could dramatically affect the classification performance. Here are a few popular representation techniques we saw among submitted solutions: Performing a systematic study on preprocessing, representation, and mining the semantics and context of search queries will be a very interesting research topic. We hope to see more exciting research in this field using the data set we contributed to the community. At the time KDD-Cup submission d eadline, 142 teams registered. Among them, 32 teams submitted 37 solutions. As described in the previous section, a good number of very interesting and creative ideas and techniques we re developed and used. As a result of applying these ideas a nd techniques, some solutions achieved quite impression scores in terms of precision and F1. In this section, we first present the award winning teams, and then summarize the overall results from all the teams that made process, we will discuss whether the three labelers agree on the award winners. At the end of this section, we will share the results of a survey we conducted with registered participants. Here are the winning teams and their members of all the awards. It is worth mentioning that the Hong Kong University of Science and Technology team did a great job and won all the three awards. Among the 37 submitted solutions, the highest F1 is 0.44, the lowest F1 is 0.06, and the median is 0.23. Figure 1 Shows the F1 score distribution. We can see that the F1 scores are nicely normal distributed with a good number of solutions having reasonably high F1 scores. The range of the 37 precision scores is between 0.75 and 0.05, with a median of 0.24. Figure 2 shows the precision distribution, which is skewed toward low precision scores. It is worth mentioning that one solution has a very high precision score of 0.75. That is a result of heavy turning of the algorithm to make it generate high precision. This so lution actually created very few category predictions, one prediction for each query in many cases. However, because its F1 is very low: 0.21 (lower than the median), it was not qualified for the Precision Award. As specified before, the three answer sets used in the evaluation process were generated through manual labeling by three human editors separately. Therefore, each answer set reflects the understanding of the problem and the knowledge of the corresponding editor. This natura lly raises the following two questions: To answer the first question on the Performance Award, we list the top 10 submissions using their IDs ranked by each labeler as well as the overall score of all the three labelers on F1 in Table 5. We can see that all the three labelers agree on the first three answer is  X  X es X  to this question. Ranking 1 2 3 4 5 6 7 8 9 10 Overall 22 8 35 13 10 21 14 9 7 26 Labeler 1 22 8 35 13 10 21 14 7 9 26 Labeler 2 22 8 35 10 21 13 33 9 7 14 Labeler 3 22 8 35 13 10 21 14 9 7 26 Similarly, we list the top 10 s ubmissions using their IDs ranked by each labeler as well as the overall score of all the three labelers that submission 21 was ranked at the second place by Labeler 2 submissions 3 and 13 were ranked at the second place by Labeler 1 and Labeler 3 respectively, they were ranked at the fourth place or lower by the other labelers. Therefore, submission 21 should be given the second place although the three labelers had certain disagreement on this. From this analysis, we can see that the three labelers highly agreed on the winners of the awards although not completely Ranking 1 2 3 4 5 6 7 8 9 10 Overall 37 21 3 10 13 35 15 9 7 26 Labeler 1 37 3 21 10 13 35 7 9 15 26 Labeler 2 37 21 10 3 35 15 13 26 9 7 Labeler 3 37 13 21 3 10 35 15 9 7 26 Now, let us look at the question of how much these three labelers agree with each other. Table 7 shows the average precision and F1 score values of each labeler when evaluated against the other two labelers. Both precision and F1 scores of the three labelers disagreement. However, the lowest precision score in this table is higher than the highest precisi on score from the contestants highest F1 score from the contesta nts (Table 3). This indicates that the agreements among the three labelers are reasonably high for the purpose of judging the submitted solutions. We performed a survey at the end of the competition to collect information about the participan ts and their works. The purpose was to gain more understanding on this year X  X  KDD-Cup. We sent emails with a set of questions to all the registered participants. 30 of them responded, including teams who submitted solutions and teams who did not submit solutions . Since the sample size 30 is relatively small comparing to 142, please read the statistics with caution. As mentioned earlier, only 32 out of 142 teams submitted solutions. It is interesting to know why so many teams did not submit. From the survey, we lear ned that the major reason for not submitting was that the teams did not have enough time. Another interesting question is which countries/regions the participants were from. Figure 3 shows the participants X  participants covered 9 countri es/regions. Among them, United States, China, and Germany were the top three. Slovenia Hungary Australia Germany Figure 3. Participants' county/region distribution (unit: team) We knew that the KDD community is a good mixture of people from academy and industry. Figure 4 indicates a similar mixture for this year X  X  KDD-Cup. Although 52% teams were from universities and 39% teams were from companies, the 9% teams marked as individual were likely belong to industry. Therefore, the academy participant teams and industry participant teams were well balanced. We also f ound from the survey that students and industry professionals were the primary contributors (the team member spent most time in the team on the KDD-Cup task). As shown in Figure 5, each of them was 41%. From the survey responses, we found that while most common team size was 6 to 10 people (36 %), we had significant number of single person teams (27%). The team size distribution is shown in Figure 6. Due to the great challenge of this KDD-Cup task, many team spent tremendous effort. As Fi gure 7 shows, 40% teams spent more than 100 person-hours. The person-hour range for all the survey response teams was from 20 to 1000 with the median as 179.25. Figure 4. Organization types of the KDD-Cup participating teams Figure 5. Occupations of th e primary contributors in participating teams It is known that some people participated in several KDD-Cups. For this year, we can see many return participant teams as in Figure 8. However, the majority survey response teams are first time participants (56%), indicating a healthy dynamic community. competition The KDD-Cup 2005 task presented a real web search engine problem, categorizing search queries , to challenge the data mining community. Significant number of participants from both academy and industry and the long hours they spent indicated the great interests the community has in this area. In this paper, we described th e competition task, discussed the evaluation method, and presented the award winners. We briefly summarized the key techniques from the submitted solutions. Many of these techniques are technically significant and/or practical. Some of them achieved quite good results. To solve this task, participants developed a lot of interesting ideas. These techniques and ideas have potential to be used in practical search engines. However, given the pr ecision and F1 scores of the submitted solutions, there is still a significant room to improve further. To encourage the community to con tinue research in this area, we made all the materials of th is KDD-Cup including data and http://www.acm.org/sigs/ sigkdd/kdd2005/kddcup.html. We believe that more advanced and practical techniques for solving search problems will come from the data mining community. First we would like to thank everyone who participated in the KDD-Cup 2005 competition for their hard work and creativity. We also thank MSN Data Mining &amp; Research team for the great help on preparing the data and evaluating the results. They are Shuzhen Nong, Jeremy Tantrum, Teresa Mah, and Abhinai Srivastava. Finally we sin cerely thank KDD 2005 web master Michal Sabala for his support. [1] ACM SIGKDD 2005. [2] ACM SIGKDD-CUP 2005. [3] MSN Search. http://search.msn.com/ [4] C. J. van Rijsbergen, Information Retrieval (Second Edition). [5] C. D. Manning and H. Scht X ze. Foundations of Statistical [6] Wordnet. http://wordnet.princeton.edu/ [7] Wikipedia. http://www.wikipedia.org/ [8] C. Silverstein, M. Henzinger, H. Marais, and M. Moricz. [9] B. J. Jansen and U. Pooch. A review of web searching [10] U. Lee, Z. Liu, and J. Cho. Automatic identification of user [11] D. E. Rose and D. Levinson. Understanding user goals in [12] L. Wang, C. Wang, X. Xie, J. Forman, Y. Lu, W. Ma and Y. [13] J. Sun, H. Zeng, H. Liu, Y. Lu, and Z. Chen. CubeSVD: A [14] J. Teevan, S.T. Dumais and E. Horvitz. Personalizing search [15] Google personalized search. [16] My Yahoo! http://my.yahoo.com/?myhome . Ying Li is a Senior Director at MS N, Microsoft, in charge of Data Mining and Applied Research. Sh e manages a team of applied researchers and research analysts to solve algorithmic problems for the advertising platform at MS N. Before Microsoft, she was a Technical Director at Computer Research Institute of Montreal, Canada, leading various applied research projects. She holds a Ph.D. degree in Computer Scien ce from the University of British Columbia, Canada. Zijian Zheng is an active data mining and machine learning researcher and practitioner. He has a computer science PhD specialized in machine learning from the University of Sydney. After that he did three years re search in this area at Deakin University. In 1999, he moved to industry. Zijian has worked in Blue Martini Software, Microsof t, and Amazon.com. Where, he commercial business intelligence so ftware systems using data mining techniques to understand cust omers/users. Currently, he is in the MSN search group leading the effort of applying data mining and machine learning techniques. Honghua (Kathy) Dai is a Data Mining Research Analyst at MSN, Microsoft. She is also a computer science Ph. D. student in DePaul University. She is actively working on applying machine learning and data mining techni ques on Web / Search Data to understand Internet user behavior. 
