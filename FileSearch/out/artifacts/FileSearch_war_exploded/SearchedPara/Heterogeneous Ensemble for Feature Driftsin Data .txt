 With rapid technological advancement, many real-life applications, such as stock markets, online stores and sensor networks can produce massive datasets, or data streams . To discover knowledge from data streams, scientists have to confront the following challenges: (1) tremendous volumes of data; (2) dynamic changes of the discovered patterns, which is commonly referred to as concept drifts ;and(3) real-time response. Concept drifts are ca tegorized into two types: gradual drifts with moderate changes and sudden drifts with severe changes. Motivated by the above challenges, there are two common approaches of existing classification models for data streams: online incremental learning and ensemble learning.
Incremental learning train s a single classifier and updates it with newly arrived data. For example: Domingos and Hulten [5] proposed a very fast Hoeffding tree learner (VFDT) for data streams. The VFDT was later extended to CVFDT [12], which can handle the concept drifting streams by constructing alternative nodes and replacing them to the outdated nodes when concept drift s occur. Incremen-tal learning is quite efficient but it cannot adapt to sudden drifts. Ensemble learning, which aims to combine multiple classifiers for boosting classification accuracy, has attracted a lot of research due to its simplicity and good perfor-mance. It can manage concept drifts with the following adaptive approaches: (1) using dynamic combiner like a majority vote or weighting combination [18], (2) continuously updating the individual classifiers online [2,15], and (3) changing the ensemble structure by replacing outdated classifiers [16,21]. However, it has high computational complexity as there are many time-consuming processes, eg. generating new classifiers, and updating classifiers.

In this paper, we address the above problems by presenting a novel framework to integrate feature selection techniques and ensemble learning for data streams. To alleviate ensemble updating, we propose a new concept of  X  X eature drifts X  and use it to optimize the updating process. With a gradual drift, each classifier member is updated in a real-time manner. When a feature drift occurs, which represents a significant change in the underlying distribution of the dataset, we train a new classifier to replace an ou tdated classifier in the ensemble.
Moreover, feature selection helps to enhance ensemble learning. It not only im-proves the accuracy of classifier members by selecting the most relevant features and removing irrelevant and redundant features, but also reduces the complexity of the ensemble significantly as only a small subset of feature space is processed. Finally, we propose a heterogeneous ensemble where different types of classifier members are well selected to maximize the diversity of the ensemble [11,22]. Figure 1 gives an overview of our framework. The ensemble consists of many classifiers, each of which has its own feature subset. If there is a feature drift, the ensemble is updated with a new classifier together with a new feature subset; otherwise, each classifier is updated accordingly. To aggregate the classification results of classifier members, we assign ea ch classier a weight w.r.t its perfor-mance. This weighting method is proven to minimize the expected added error of the ensemble.

In summary, the following are contributions of our framework which integrates feature selection and ensemble learning techniques for data streams:  X  We enhance ensemble learning with feature selection which helps to lessen  X  We propose the definition of feature drifts and explore relationships between  X  We significantly increase the accuracy of the ensemble by designing a het-Ensemble learning, a process to constru ct accurate classifiers from an ensem-ble of weak classifiers, has attracted extensive research in the past decade. In general, these methods vary in the way of they construct various classifiers and combine their predictions. The first step of constructing a group of classifiers can be differentiated according to the dependencies among classifiers. The inde-pendent approach trains classifiers randomly and can be easily parallelized, for example, bagging [3], random subspace [19], and random forest [4]. The depen-dent approach constructs a new classifier while taking advantage of knowledge obtained during the construction of past classifiers, such as AdaBoost [8], Ad-aBoost.M2 [7], and Gradient boosting [9]. In the second step of combining the classifiers X  predictions, majority voting is one intuitive method to choose the dominant decision [3,4,19]. As majority voting cannot guarantee that the voting result will be better than the best individual one, the weighting method is intro-duced which assigns competent classifiers higher weights, such as performance weighting [7,8,9,21], Naive Bayes weighting [6], and entropy weighting [17].
Ensemble learning can also work well with data streams by effectively tackling the challenges of continuous incoming data and concept drifts [2,15,16,18,21]. In [15], Oza et al. employed the Poisson distribution to adapt the traditional bagging and AdaBoost techniques for data streams. Bifet et al. [2] proposed an ensemble of Adaptive-Size Hoeffding Tr ees (ASHT) and used a statistical test to detect concept drifts. Street and K im [18] proposed a streaming ensemble of decision trees using majority voting. Wang et al. [21] proposed a carefully weighted ensemble for concept-drifting data streams and proved that the en-semble is more accurate than a single classifier trained on the aggregated data of k sequential chunks. In [16], Sattar et al. adapted the traditional one-vs-all (OVA) classifiers for data streams where k individual concept-adapting very fast decision trees (CVFDT) [12] are learnt and each one is used to distinguish the instances of one class from the instances of all other classes. However, the above algorithms suffer from high complexity and the inability to adapt to different types of concept drifts.

Feature selection is anoth er important research issu e as data streams are usu-ally high-dimensional. Feature select ion techniques can be classified into three categories: filter, wrapper and embedded models [14]. The filter model evaluates a feature subset by using some independent measure, which only relies on the general characteristics of data. The wr apper model is attached to a learning al-gorithm and uses its performance to evaluate a feature subset. A hybrid model takes advantage of the above two models. As data streams require real-time re-sponses, we favor the filter approach due to its simplicity and independence to classification models. Moreover, in data streams, the definition of relevant fea-tures is dynamic and restricted to a certain period of time. Features that are previously informative may become irrelevant, and previously rejected features may become important features. Thus, dyn amic feature selection techniques are required to monitor the evolution of features. Unfortunately, to the best of our knowledge, there is limited research about the relationship between feature se-lection and ensemble learning, especially for data streams.

In this paper, we will address this gap between feature selection and ensemble learning and propose a novel framework for integrating feature selection and heterogeneous ensembles. Our framework not only adapts to different kinds of concept drifts properly, but also has low complexity due to its dynamic updating scheme and the support of feature selection techniques. In this section, we propose a general framework for ensemble learning for data streams. We assume infinite data streams X =[ x 1 , x 2 ,..., x t ,... ] as input in at time t . We assume the data streams have c different class labels. For any data vector x t , it has a class label y t  X  X  y 1 , y 2 ,..., y c } . Generally when the dimension p is large, there is often only a small set of key features that is critical for building accurate models for classification.

Data streams tend to evolve over time and so do the key features correspond-ingly. For ease of discussion, we will use the following definitions: Definition 1. A data source or a concept is defined as set of prior probabilities of the classes and class-conditional probability density function (pdf): Definition 2. Given data streams X , every instance x t is generated by a data source or a concept S t . If all the data is sampled from the same source, i.e. S 1 = S 2 = ... = S t = S , we say that the concept is stable. If for any two time points i and jS i = S j , we say that there is a concept drift .
 Definition 3. Given a feature space F ,attimepoint t , we can always select the most discriminative subset F t  X  X  . If for any two time points i and j F i = F j , we say that there is a feature drift .
 Next, we explore the relationship betw een concept drifts and feature drifts. Lemma 1. Concept drifts give rise to feature drifts.
 Proof. Assume that certain feature selectio n techniques evaluate the discrimi-nation of a feature subset F at time t by a function: And, there is a feature drift in [ t i ,t i +  X t ], We can always find a feature f a  X  X  and a class y b so that P ( f a | y b ,t ) = in the time interval [ t i ,t i +  X t ].
 Lemma 2. Concept drifts may not lead to feature drifts.
 Proof. For example, given data streams X within a time period [ t i ,t i +  X t ]we assume the prior probability of a class i and j , P ( y i )and P ( y j ), are changed; but their sum ( P ( y i )+ P ( y j )) and other probabilities remain the same. In this scenario, there is a concept drift but no feature drift.
 Combining Lemmas 1 and 2, we can conclude that: Theorem 1. Feature drifts occur at a slower rate than concept drifts. Feature drifts, which are observed in high dimensional data streams, occur no faster than concept drifts. As shown in t he overview of our framework Figure 1, we need to modify feature selection tec hniques to detect feature drifts. The key idea is using feature selection to a ccelerate ensemble learning and steer its updating process. Moreover, feature selection techniques not only remove irrelevant and redundant features, but also accelerate the learning process by reducing the dimensionality of the data. Thus, the overall performance of the ensemble is improved in terms of accuracy, time and space complexities.
First, we select online classifiers as cl assifier members as they can be incre-mentally updated with new data. Then, we construct a heterogeneous ensemble with a capability to adapt to both concept and feature drifts. With gradual drifts, we only need to update the classifier members. With feature drifts, we adjust the ensemble by replacing the outdated classifier with a new one. We further deploy a weighting technique to minimize the cumulative error of the heterogeneous ensemble. 3.1 Feature Selection Block Feature selection selects a subset of q features from the original p features ( q  X  p ) so that the feature space is optimally re duced. Generally, we expect the feature selection process to remove irrelevant and redundant features. We decide to use FCBF [13] as it is simple, fast and effective. FCBF is a multivariate feature selection method where the class relevance and the dependency between each feature pair are taken into account. Based on information theory, FCBF uses symmetrical uncertainty to calculate dependencies of features and the class rele-vance. Starting with the full feature set, FCBF heuristically applies a backward selection technique with a sequential search strategy to remove irrelevant and redundant features. The algorithm stops when there are no features left to elim-inate.

Symmetrical Uncertainty ( SU ) uses entropy and conditional entropy values to calculate dependencies of features. If X , Y are random variables, X receives value x i with probability P ( x i ), Y receives value y j with probability P ( y j ); the symmetrical uncertainty between X and Y is: where H ( X )and H ( Y ) are the entropies of X and Y respectively; I ( X,Y )is the mutual information between X and Y ; the higher the SU ( X,Y ) value, the more dependent X and Y are.

We choose to the sliding window version of FCBF so that it has low time and space complexities. Incoming data is stored in a buffer (window) with a predefined size. Next, the matrix of symme trical uncertainty values is computed to select the most relevant feature subs et. The process is performed in a sliding window fashion, and the selected feature subsets are monitored to detect feature drifts. When two consecutive subsets are different, we postulate that a feature drift has occurred. 3.2 Ensemble Block Heterogeneous Ensemble. When constructing an ensemble learner, the diver-sity among member classifiers is expect ed as the key contributor to the accuracy of the ensemble. Furthermore, a heterogeneous ensemble that consists of dif-ferent classifier types usually attains high diversity [11,23]. Motivated by this Algorithm 1. Ensemble Learning observation, we construct a small heterogeneous ensemble rather than a big ho-mogeneous ensemble with a large number of classifiers of the same type, which will compromise speed.

As mentioned, we aim to select online classifiers so that the ensemble can properly adapt to different types of co ncept drifts. Here, CVFDT [12] and On-line Naive Bayes (OnlineNB) are chosen as the basic classifier types, but the framework can work with any classification algorithm. The OnlineNB is an online version of the Naive Bayes classier. When a training instance ( x t ,y t ) comes, OnlineNB updates the corresponding prior and likelihood probabilities, to select the class having the maximum posterior probability as follows: Details of the heterogeneous ensemble X  s learning process are given in Algo-rithm1.Givenadatastream X and a predefined set M of different classifier Algorithm 2. Real Time Classification types, we initialize the ensemble with k classifiers of each type in M . Next, data streams are processed in a sliding window mode and data instances are grouped into predefined-size chunks. When a new chunk arrives, we apply a feature se-lection technique to find the most discriminative feature subset. If the subset is different from the previous one, there is a feature drift. We would then need to construct a new classifier with the selected feature subset. We add it to the ensemble, and remove the worst classifier if the ensemble is full; the new classifier will be of the same type as the best classifier member which has the smallest aggregated error (lines 7-12). Finally, we employ online bagging [15] for updating classifier members to reduce the variance of the ensemble (lines 13-18). Ensemble Classification. BasedontheresearchworkofTumer et. al [20] and Fumera et. al [10], the estimate error of the ensemble are: where  X  k and  X  k are the bias and the standard deviation of the estimate error  X  k of classifier C k ,  X  kl is the correlation coefficient of the errors  X  k and  X  l .
We assume that classifier members are unbiased and uncorrelated (i.e.  X  k = minimize the added error of the ensemble, the weights of classifier members are set as follows: When constructing the ensemble classifier, we estimate the added error of every classifier member, E k add , which is its accumulated error from its creation time to the current time. Moreover, to alleviate the extreme case of E m add  X  0, we modify the Equation 7 as follows: where  X  is a padding value which is empirically set to 0.001.

Details of the ensemble classification process are shown in Algorithm 2. Given the ensemble E where each member can classify a testing instance and output the result as a probability distribution vector, we use a weighting combination scheme to get the result. First, for each classifier C i we project the testing in-stance x t onto the subspace  X  i . Then, the classifier C i processes the projected instance and outputs a probability distribution vector. We attain the aggregated accuracy of C i from its creation time, and calculate its optimal weight according to Equation 8 to minimize the expected added error of the ensemble. Finally, the ensemble X  X  result is set as the norma lized sum of the weighted classification results of the members (lines 6-7). 4.1 Experimental Setup For our experiments, we use three synthetic datasets and three real life datasets. The three synthetic datasets, SEA generator (SEA), Rotating Hyperplane (HYP), and LED dataset (LED), are generated from the MOA framework [1]. Concept drifts are generated by moving 10 centroids at a speed of 0 . 001 per instance in the RBF dataset, and changing 10 attributes at a speed of 0 . 001 per instance in the HYP dataset. The three real life datasets are: network intrusion (KDD X 99 1 ), hand-written digit recognition (MNIST 2 ), and protein crystallography diffraction (CRYST 3 ) datasets. Table 1 shows the characteristics of the six datasets.
We compare our algorithm HEFT-Strea m with other prominent ensemble methods: AWE [21] and OnlineBagging [15]. The experiments were conducted on a Windows PC with a Pentium D 3GHz Intel processor and 2GB memory. To enable more meaningful comparisons, we try to use the same parameter values for all the algorithms. The number of classifier members is set to 10 for all the ensemble algorithms, and the chunk size is set to 1000. To simulate the data stream environment, we process all expe riments in a practical approach, called Interleaved-Chunk . In this approach, data instances are read to form a data chunk. Each new data chunk is first used to test the existing model. Then it is used to update the model and it is finally discarded to save memory. 4.2 Experimental Results As AWE and OnlineBagging are homogeneous ensembles and can only work with one classifier type, we set different classifier types for these ensembles accord-ingly. For AWE, we set classifier members as Naive Bayes and C4.5, which are recommended by the authors [21], and denoted as AWE(NB) and AWE(C4.5) . For OnlineBagging, we set its classifier members as OnlineNB and CVFDT, and denoted as Bagging(OnlineNB) and Bagging(CVFDT) respectively. We conduct the experiments ten times for each datas et and summarize their average accu-racy and running times in Table 2. Readers may visit our website 4 for algorithms X  implementation, more experimental res ults, and detailed theoretical proofs.
We observe that the AWE ensemble has the worst accuracy and the longest running time. This is because the AWE ensemble uses traditional classifiers as its members and trains them once; when th ere are concept drifts, these members become outdated and accuracy is degrad ed. Moreover, the AWE ensemble always trains a new classifier for every upcomin g chunk, and this increases processing time. The OnlineBagging has better performance but it largely depends on the classifier type. For example, Bagging(OnlineNB) is more accurate and faster than Bagging(CVFDT) for the LED dataset but Bagging(OnlineNB) become less precise and slower than Bagging(CVFDT) for the KDD X 99 dataset. It is also noteworthy that both AWE and OnlineBagging do not work well with high dimensional datasets, such as MNIST and CRYST.

Our approach, HEFT-Stream, addresses the above problems and achieves bet-ter performance than WCE and OnlineBagging. It achieves the best accuracy values and the lowest running time for most datasets. HEFT-Stream continu-ously updates classifier members with gradual drifts, and only trains new clas-sifiers whenever there are feature drifts or sudden drifts. This property not only enables HEFT-Stream to adapt to differ ent types of concept drifts but also conserve computatio nal resources. Furth ermore, HEFT-Stream can dynamically change the ratio among classifier types to adapt to different types of datasets; when a particular classifier type works well with a certain dataset, its ratio in the ensemble will be increased. Finally, with integrated feature selection capabil-ity, HEFT-Stream only works with the most informative feature subsets which improves accuracy and reduces processi ng time. The last column of the Table 1 shows the ratios of the selected features to the full feature sets for all datasets. We realize that feature selection techniques are very useful for high dimensional datasets. For example, the percent ages of the selected features are 3 . 95% for the MNIST dataset, and only 0 . 56% for the CRYST dataset. In this paper, we proposed a general fram ework to integrate fe ature selection and heterogeneous ensemble learning for stream data classification. Feature selection helps to extract the most informative feat ure subset which accelerates the learning process and increases accuracy. We first a pply feature selection techniques on the data streams in a sliding window manner and monitor the feature subset sequence to detect feature drifts which represent sudden concept drifts. The heterogeneous ensemble is constructed from well-chosen online classifiers. The ratios of classifier types are dynamically adjusted to increa se the ensemble  X  X  diversity, and allows the ensemble to work well with many kinds of datasets. Moreover, the ensemble adapts to the severity of concept drifts; we update the online classifier members for gradual drifts, and replace an outdated member by a new one for sudden drifts. We have conducted extensive experiments to show that our ensemble outperforms state-of-the-art ensemble learning algorithms for data streams.

In our future work, we will investigate more intelligent methods to adjust the ra-tios of classifier types as well as the ensemble size. We will continue to examine the relationship between concept and feature drifts and develop a metric to quantify concept drifts and use it to further adapt ensembles to achieve better accuracy.
