 Most cross-domain sentiment classification techniques consider a domain as a whole set of opinionated instances for training. However, many online shopping we bsites organize their data in terms of taxonomy. With multiple domains (or, nodes) organized in a tree-structured representation, we propose a general ensemble algorithm which takes into account: 1) the model application, 2) the model weight and 3) the stra tegies for selecting the most related models with respect to a target node. The traditional sentiment classification technique SVM and the transfer learning algorithm Spectral Features Alignment (SFA) were applied as our model applications. In addition, the model weight takes the tree information and the similarity between domains into account. Finally, two strategies, cosine function and taxonomy-based regression model (TBRM) are proposed to select the most related models with respect to a target node. Experimental results showed both (cosine function and TBRM) proposed strategies outperform two baselines on an Amazon dataset. Three tasks of the proposed methods surpass the gold standard generated by the in-domain classifiers trained on the labeled data from the target nodes. Good results from the three tasks enable this algorithm to shed some new light on eliminating the major difficulties in transfer learning research: the distribution gap. I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Ensemble Learning; Domain Adaptation; Sentiment Classification; Opinion Mining  X  X hat are other people X  X  opinions? X  is one of the frequently asked questions we may ask before purcha sing an item. In the past, we find answers to this question through word of mouth. Nowadays, access to other customers X  commen ts on some specific products is no longer a difficult task due to the fact that a huge amount of users share their experiences through various platforms on the Web. As a result, opinion mining research attracts considerable attention in recent years, such as sentiment analysis and opinion-oriented summarization [1]. Sentiment classification, one of the opinion mining techniques, is to extract a set of opinionated reviews in some specific domain and then predict the sentiment polarity on an unseen review using a trained classifier. However, in practice it often confronts wi th the domain dependency problem that the performance drops drastically if the feature-space distributions of the source and the target domains are different. In order to reduce the performance drop caused by the distribution gap, an achievable approach is to co-train a classifier based on the information extracted from the source and the target domain data and then transfer the knowledge c ontaining in the source domain to the target domain. The major challenging issues in cross-domain sentiment classification are two folds. On the one hand, since the customer reviews often carry plenty of domain-specific words, which make learning the sentiment classi fier from one domain to another a non-trivial task. For example,  X  X ensitive screen X  and  X  X harp X  are the domain-specific words in positive electronic product reviews, and is ha rd to find them in video game reviews. Some words even carry positive sentiment in one domain but negative sentiment in another domain. For example,  X  X ot X  in video game reviews are often carry positive polarity, indicating that a video game is passionately enthusiastic for the teenagers. In the electronic product domain, how ever, the same term  X  X ot X  appeared in  X  X hy does my smartphone X  X  battery get so hot? X  clearly carries a negative sentimen t. On the other hand, in many situations, there are often sufficien t amount of labeled data in the source domains, but have no or very limited amount of labeled data in the target domain. Until now, many techniques have been proposed to tackle the aforementioned cross-domain sentiment classification problem, such as Structural Correspondence Learning (SCL) [2]; graph-based approaches [3] X  X 5]; Spectral Feature Alignment (SFA) [6]; Co-Cluster-based Classification (CoCC) [7], sentiment thesaurus [8], TRIplex Transfer Learning (TriTL) [9], and so on. Almost all the techniques (SCL, SFA, CoCC, sentiment thesaurus and TriTL) consider a domain as a whole set of instances for training. However, in reality many onli ne shopping websites, such as Amazon 1 and eBay 2 , organize their data in terms of a tree-structured representation instead of a one-level structure. Take Amazon as an example: when a customer is searching for dog food, he/she may first click on the "pet supplies" category, and find "dog food" located in the second level of the Amazon taxonomy structure. He/she can rema in on this level to search for the desired product or move to a deeper level for a more specific category search such as "dry dog food" or "wet dog food". As there is more than one sub-division within a given category, this paper will firstly explore some in herent properties in the taxonomy through two preliminary experiments and through the preliminary http://www.amazon.com/ http://www.ebay.com/ results we propose an ensemble algorithm for the cross-domain sentiment classification proble m with a taxonomy structure. With many source nodes in a given domain, two preliminary experiments X  X n-domain and cr oss-domain X  X ust be conducted before designing the sc heme of the ensemble algorithm. The term in-domain in this paper means the source and the target nodes are in the same domain tree (all source/target nodes in one experimental task share the same root), while the cross-domain means the source nodes and the ta rget nodes belong to different domain trees (There are many domain trees which regard to different topics in the taxonomy data set, such as Electronics, Book or Kitchen domain trees). More deta iled experimental analysis in section 4 gives the preliminary e xperiments a whole picture of the accuracy bounds. Among the preliminary results, an important finding in cross-domain experiment is that some source models have good prediction performance, wh ich inspires us to utilize these good performance models and combine them with proper weights. Based on the observations, we propose an ensemble learning algorithm that takes into account: 1) the model app lications, based on different transfer learning/ machine learning algorithms have different efficiency on different datasets; 2) the model weight, for measuring the comparability of corpora (node) pairs and 3) the strategies for selecting the most related models with respect to the target node, based on our preliminary results. Although some ensemble algorithms for cross-dom ain text classification have been proposed in recent years [10] X  X 13], it must be emphasized that this is the first study that gives a thorough analysis on the taxonomy-based datasets for sentim ent classification. The model applications in this study ca n apply any existing single domain adaptation algorithm (e.g., Support Vector Machine, SCL or SFA), which can be seen as a learned hypothesis on the source domain data. As for 3), we studied two strategies in the ensemble algorithm, (1) similarity function: the idea behind similarity function lies in the observation that if two datasets are similar to each other, then their distribution gap will be smaller [14]. Whilst there are many similarity measures available in the information theory, such as Kullback-Leibler divergence, Jensen-Shannon divergence, Jaccard coe ffi cient and cosine similarity, we adopted the cosine similarity suggested in Corpus Linguistics for the first strategy. (2) Taxonomy-based Re gression Model (TBRM): this approach tries to find the best source node that carries minimum accuracy loss (the performance drop from in-node classification to cross-node classification). Ponoma reva and Thelwall introduced a regression model that predicts which node has the minimum accuracy loss from two domain characteristics: domain similarity and domain complexity variance [14]. Based on our preliminary analysis and inspiration from their regression model, we propose the TBRM, which integrates not only the domain similarity, the domain complexity but also the tree-structure information. The empirical validation expe riment demonstrates the effectiveness and robustness of our proposed ensemble algorithm on the multiple source adaptation tasks. The performances of three tasks of TBRM even surpass the upper bound generated by an in-domain classi fi er (trained on the labeled data from the target node), which implies that there is a chance to eliminate the distribution gap that previous studies cannot achieve. The remainder of this paper is organized as follows. Section 2 gives a survey on the related work. Section 3 defines the problem formally and describes the datase ts used in the experiments. Section 4 reports the prelimin ary in-domain and cross-domain sentiment analysis. The ensemble al gorithm is presented in section 5. Section 6 shows and discusses the experimental results. Section 7 concludes the remarks. The main objective of this paper is to explore the ensemble of models in taxonomy-based cross-domain sentiment classification. After one of the milestone works in transfer learning introduced in 2007 [2], there are two mainstream approaches at present: (1) cross-domain sentiment classifi cation on single domain. The single domain dataset is divided into different feature sets, and the component classifiers are traine d on those feature sets. One strategy is to adopt different n -gram features to train the source domain data [12], [15]. Another atte mpt is to split the source data into different part-of-speech (POS) groups to build different sentiment classifiers [10], [1 3]; (2) cross-domain sentiment classification on multiple domains, which build multiple classifiers from multiple source domain datasets, and then combine those trained classifiers with appropriate weights. Aue and Gamon propose an ensemble learner method that uses the meta-classifiers derived from a number of source domains [16]. Li and Zong provide two approaches to the multi-domain adaptation problem, i.e., the feature-level and the classifier-level fusion approaches [15]. The former approach uses a common set of terms to construct a uniform feature vector and then trains a classifier using all the source domain datase t. The latter approach builds m base classifiers for every source domain, and builds m development domains from the m base classifiers. Finally, the multiple classifier system i is built from the development domain i , for i =1, ..., m . Our study does not precisely belong to the multiple domain adaptation as the domain tree consists of multiple nodes in the taxonomy-based dataset. But if we extend the definition of multiple domain adaptation to tree-structured representation, our study belongs to it. As for model application, we explored Support Vector Machine (SVM) [17] and SFA [6] in our study. SVM is a robust pattern classification tool that has demonstrated its effectiveness in many sentiment analysis researches [1], [18] and other text-related mining tasks such as e-mail spam filtering [19], text categorization [20] and political opinion classi fication [21]. Given the labeled training dataset  X   X   X   X   X ,  X   X  ,...,  X   X   X   X ,  X   X   X  where each x vector containing features denoting the document i , and  X   X   X 1, X 1  X  is the sentiment of document i . The algorithm solves the problem of finding the largest margin between the hyperplane  X   X   X 0 X  X  X  (where w is the hypothesis vector and b is the bias term) and the dataset. The linear kernel function,  X  X  X   X   X   X  , is often adopted in many text classification problems due to the high dimensionality of the text corpus [20]. The SFA is one of the state-of-the -art transfer learning algorithms based on the structural correspondence learning [2]. The feature space V was firstly split into two disjoint sets V domain-independent and the domain-specific feature space) according to the mutual information in the corpus. The spectral feature clustering algorithm was then applied on the co-occurrence matrix built from V DI and V DS . Then the augmented new training instance with respect to a feature vector x is where  X   X  X  denotes a feature selection function for selecting domain-specific features, and  X  is the feature alignment mapping function learned from spectral featur e clustering algorithm. Finally, Pan et al. demonstrated the effectiveness of SFA on an Amazon dataset. domain, where  X   X  source distribution and  X   X  labels. Let  X   X   X  X  X  X   X  X  X  X   X  domain tar , where n is the number of data instances. Each data instance within a domain is gene rated under the same distribution of that domain and can be repres ented by a feature vector. Each domain tree consists of a collec tion of tree-structured nodes. The cross-domain sentiment classification can be defined as the task of learning a classifier using the labe led source domain data to make prediction on the unlabeled target domain data. We downloaded a collection 3 of product reviews from three different domain trees in Am azon, i.e., Electronics ( E ), Book ( B ) and Kitchen ( K ). We adopted a three-leveled tree structure for each domain. It is by no means restricted to the three-leveled structure and is easy to extend our method to the whole Amazon directory. The root is level one ( L1 ), with child nodes in level two ( L2 ), and the leaf nodes are in level three ( L3 ). Each domain tree is built bottom up. Each leaf node contains 1,000 positive reviews and 1,000 negative ones. The number of positive/negative reviews in each bottom node is not originally 1,000, but we randomly extract 1,000 positive and 1,000 negative reviews according to many previous studies [2], [6], [8], [12], [13], [15]. For each node in L2 , we randomly extract 1,000 positive and 1,000 negative reviews from its child nodes. Simila rly, we randomly fetch 1,000 positive and 1,000 negative reviews for the root from its leaf nodes. Each product review is rated a binary value. Reviews with 1-2 stars are regarded as negative while those with 4-5 stars are regarded as positive. Reviews with 3 star are considered as neutral comments and hence excluded fr om our experiment. The number of nodes and the number of reviews in each level of each domain tree is illustrated in Table 1. This section gives a preliminary analysis on the Amazon dataset to find out the inherent tree-structured in-domain and cross-domain sentiment classification characteri stics. We keep only unigrams with term frequency greater than or equal to 5 for the SVM classifier according our experiences. LIBSVM [22] is adopted as our SVM analysis classifier and its embedded parameter adjustment tool is used for parameter optimization. In the in-domain sentiment analysis, three domain trees ( E , B and K ) are used individually for each experiment. In the cross-domain sentiment analysis, the source and the target domain trees are distinct. http://nlg.csie.ntu.edu.tw/sentiment/ We explored two scenarios in the in-domain sentiment analysis. The in-node experiment conducts the 4-fold cross-validation on each target node, denoting the best case of each target node. The level-based experiment is performed to demonstrate the prediction ability between levels, thus all the source nodes are in the same level, and so do the target nodes. The predicted accuracies of all the nodes are then clustered to different groups according to their domain and level. The accuracies in the same cluster are averaged. Figure 1 shows the averaged upper bound accuracies of each cluster (each domain tree from L1 to L3 ). The y -axis represents the classification accuracy. The best case lies in the L3 of Electronics domain, reaching 86.72%. In each domain tree, the performance increases with the level (from L1 to L3 ). A reasonable explanation of this outcome is that reviewers in lower-level ( L3 ) nodes often use a narrow range of technical vocabulary while the upper-level ( L1 ) node reviews often contain a wide range of vocabulary, thus render the prediction more challenging. From the perspective of domain, as can be seen, the performance of Electronics consistently out performs other domains from level to level. The Book domain tr ee performs worst among these domain trees. The reason for this result is similar to the perspective of the level. Sinc e book reviews often use a wide range of vocabulary and discuss broad topics (such as, from the Harry Potter comments to the machine learning textbooks comments), making the Book domain learns worse than the Kitchen and Elec tronics domains. 
Figure 1. Averaged Upper Bound Accuracies of K, E and B To show the influence of data diversity and model generality within each domain tree, we perform the level-based sentiment experiment. In each task, all the source nodes are within the same level, and so are the target nodes. Below explains the prediction from one source level to another target level: if the source level is  X  using  X   X   X  to build an SVM mode l and make prediction on  X  final averaged target level accuracy of  X   X  is  X  source and the target level are the same (that is, the indicator function). Table 2 shows the prediction accuracies means the L1 of Kitchen domain. The performance drop from the in-node task to the level-based task of the same target level is the in-domain distribution gap . For example, the in-node prediction accuracy of L3 of Book is 82.65%, and the level-based prediction accuracies drop to 79.91%, 77.81% and 77.30% for L1 , L2 and L3 , respectively. From th e perspective of fixing the target level, when the source level is higher, th e performance is better in each domain tree, indicating the higher-level nodes do have better prediction ability and higher genera lity. And this is one of the reasons of the proposed weight adjustment strategy presented in section 5.3. Table 2. Level-based Sentiment Prediction Performance of K, To show a more comprehensive picture in the domain adaptation scenario, we conduct six cross-domain sentiment classification former use Kitchen as the source domain tree and the latter use Electronics as the target domain tree). In one cross-domain task, each node in the source domain tree builds an SVM model and makes prediction on each node in the target domain tree. There are 1,800 pairs in the K -E task, 2,500 in K -B task and 7,200 pairs in E -B task. Figure 2 shows the frequenc y distribution of accuracies over all the pairs in the cross-domain experiment. The overall performance of E -K and K -E is better than other tasks, implies the Kitchen and Electronics domains are similar to each other. When the Book is used as a source domain, it performs better than used than K -B . This finding implies that the Book is a broader and more diverse domain than the others. Figure 2. Frequency Distribution of Accuracies over all the Besides individual comparison, in general the portion of very good and very poor performances is relatively small. In B -E , over 35% of prediction accuracies lie between 71% and 73%, but the accuracies lie between 80% and 82% account for only 1.25%. Based on the preliminary findings, it is natural to design a scheme that select the nodes resulti ng in good performance and combine them with appropriate weights for cross-domain sentiment classification. In the next section, we will introduce a general ensemble learning scheme together with node selection strategies and model combination method. In this section, we begin with the ensemble algorithm in detail and then describe the model application, the model weight and strategies for selecting the models in the following subsections. The ensemble of models with multiple source domain trees is explained in the last subsection. From the preliminary analysis we realize that there are some models that perform well even when the source and the target nodes belong to distinct domain trees. And our proposed weighted ensemble model according to the preliminary analysis is: Where  X  is a model built from some source node and  X  is the test  X  X  X   X   X , X   X  selects k % of the source nodes that are most related to T .  X   X   X , X   X  denotes the sentiment score of one test instance x using  X , X  X  X  X   X   X , X  X  X  X  and  X   X   X  , the positive and negative probabilities of  X   X  predicted by  X  , respectively. Then, A formal description of the proposed algorithm is shown in Algorithm 1. Algorithm 1 Ensemble of models in taxonomy-based cross-domain sentiment classification Input: source domain tree nodes  X   X   X   X ,...,  X   X  , source domain tree Output: an ensemble of classifier  X  1: for  X   X  X   X 1 X  do 2:  X   X   X  learn model  X   X  from the source domain node  X  3:  X   X   X   X   X ,  X   X  calculate the distance between  X   X  and  X  4: end for 5: for  X   X  X   X 1 X  do 6: w  X   X   X   X ,  X   X  X   X   X   X   X ,  X  X  X   X  X  X   X   X  X ,  X  7: end for 8:  X  X  X   X   X , X   X   X  select k % of the source node models from 9: return  X  X  X  X   X   X  X   X   X   X   X , X   X   X , X  X  X   X   X   X  X  X  X  X  X  X   X   X , X   X  For each node in the dataset, the algorithm firstly learned a corresponding sentiment predic tion model using a machine learning algorithm. Then the line 3 of the algorithm measures the distance between the test node a nd other models. There are several ways to measure the distance (Euclidean distance, cosine similarity, Jaccard coefficient, etc.), and we selected the cosine similarity among them as expl ained in the introduction. The weight  X   X   X   X   X ,  X  is calculated using the distances measured in the previous step (more detailed e xplanation of the formula is in section 5.3). Finally, the model selection strategy in line 8 can be cosine function or TBRM in our implementation. There are two advantages of this algorithm: (1) flexibility. Since we can adopt different machine learning algorithms in line 2, different distance measurement method in line 3 and diffe rent strategies in line 8. (2) Model reusability. For different tasks, one just needs to assign appropriate weights to the corr esponding models and no need to train the source nodes again. Many existing sentiment classification algorithms are available for the sentiment score function. Fo r machine learning approaches such as Na X ve Bayes, maximum entropy and SVM [18], or transfer learning approaches such as SCL [2], RANK-SOCAL[4] and Sentiment Sensitive Thesaurus (SST) [8] are all applicable. In this paper, we train SVM cla ssifiers for each source node in  X   X   X   X ,...,  X   X  because of its effectiveness in many text classification studies [1], [18]. After that, the sentiment score of the test instance is later predicted using the trained classifiers (the first method). The second method in this study is to use the spectral feature alignment (SFA) algorithm introduced by Pan et al. [6]. For the experiment using SFA, we train a classifier from  X  X  X  on the augmented instances and use it to predict the sentim ent polarity of the target domain instances. More detailed steps of the SFA algorithm could be found in their paper [6]. In this study we applied the cosine function to measure the similarity between a s ource domain tree node  X  and a test node  X  . And the weight adjustment formula for node  X  corresponding model  X   X  , see Algorithm 1 for more detail) is: Where  X  X  X  X  X  X  X  X  X   X   X   X   X  represents the collection of nodes along the path of  X   X   X  X  parent node to the root. There are two advantages of the weight adjustment strategy: (1 ) since the outcome of the cosine similarity is between 0 and 1, af ter the re-weighting, higher level nodes will contain higher weights compared to its descendants. This is beneficial for the taxon omy-based dataset because higher level nodes contain higher diversities and higher generality than the lower level nodes. (2) If two source nodes,  X  same level have almost the same cosine values with respect to the same test node T , the node whose parent node is closer to  X  will have higher weight (if the childr en have very close similarity values, then let X  X  compare their parents X  similarity values). A simple and straightforward strategy is to sort all the weights  X   X   X   X   X ,  X  in a descending order, and then select the first k % of the source node models to form the set  X  X  X   X   X , X   X  . While there are multiple source nodes, an essential problem is how to select good source nodes for training. Ponomareva and Thelwall introduced two domain properties, domain similarity and domain complexity variance, to predict the accuracy loss on the target node [14]. The domain similarity measures the data distributions of different domains, and the domain complexity is the long tails (the percentage of rare words) of one domain. If a domain contains more rare-words, then it tends to be more complex for the machine-learning tasks. Ponomarev a and Thelwall argued that the domain similarity and domain co mplexity variance are both important domain properties. They conduct an experiment to show that these two properties have high correlation to the accuracy loss using a linear regression model. Inspired from their regression model and our preliminary analysis, we propose the taxonomy-based regression model , which integrates the domain similarity, domain complexity and the tree information . There are many relationships in a tree, such as parent-child, siblings, grandparent-grandchild, and so on. In order to preserve all the relationship among the nodes in the tree, we consider an encoding scheme that each node is represented as a bit sequence with length equal to the number of nodes in the tree. The method is as follows: starting from root, we use the breadth-first traversal to label the node number. Figure 3 illustrates one example tree after the labeling procedure. At first, all the bits are initialized to zero. For the bit sequence of each a node, we update the bits corresponding to itself, its ancestors and its descendants to 1. For example, Node 3 has ancestor Node 1 and descendants Node 6, Node 7 and Node 8, so we update its 1 st , 3 rd and 6 th ~8 th bits to 1. Below is the result after this process for the example tree: Node 1: 11111111 Node 2: 11011000 Node 3: 10100111 Node 4: 11010000 Node 5: 11001000 Node 6: 10100100 Node 7: 10100010 Node 8: 10100001 After the first step, every node will have its child and grandchild (if it has) nodes X  properties. Note all the bits in the Node 1 become 1, implies that Node 1 is the root. The encoding scheme transforms the associated relationships of nodes in the taxonomy into an appropriate feature representation that embeds each node X  X  ancestors X  and descendants X  properties into the bit sequence. For example, the bit sequences of Node 4 and Node 5 hold both the properties of Node 1 and Node 2 (the left part of Figure 4), indicating Node 4 and Node 5 are siblings. Comparatively, Node 4 and Node 6, which share the same root but distinct parents (i.e., they own the first cousins relationship), only have the 1 common (the right part of Figure 4). And we can indeed make the sibling relationship (e.g., Node 4 and Node 5) and the first cousins relationship (e.g., Node 4 and Node 6) distinguishable through this scheme. Figure 4. Sibling Re lationship and First Cousins Relationship. Finally, the multiple linear regr ession model (TBRM) is defined as follows.  X  X  X  X  X  X  X  X   X , X  is the accuracy drop from node i to node j ,  X  bias,  X   X  , ...,  X   X  X  X  are the unstandardized coefficients for the predictors, n is the total number of nodes in the taxonomy,  X  X  X  is the similarity between node i and node j . We use  X  the domain similarity measure.  X  X  X  X  X   X , X  is the difference between domain complexities of node i and node j. We adopt the percentage of rare words (words that occur less than or equal to 3 times in the node) as the domain complexity.  X  X  X  X   X , X  is the l -th bit in the encoded bit sequence of the source node i . The predicted accuracy drops are sorted in an ascending order, then TBRM select the first k % of the source models for  X  X  X   X   X , X   X  . The aforementioned ensemble mode l is applied for single source domain tree. For multiple source domains trees, we can simply insert a pseudo root to connect other domain trees X  root, and then the model combination approach is fully applicable. Figure 5 illustrates an example that a pseudo root is connected to the Book and Electronics domains. This section describes our expe rimental setup in detail and compares our method with othe r baseline methods. Parameter sensitivity test is discussed in Section 6.3. Section 6.4 shows the impact of transfer learning. The last subsection shows the experimental results of sing le source tree adaptation. The experimental dataset was de scribed in section 3, which contains three domains including Kitchen ( K ), Electronics ( E ), and Book ( B ).Although we only conduct our experiment on Amazon shopping website and not implement our algorithm on other websites, but we strongly believe that our algorithm can perform similar results in any pr oduct review related cross-domain sentiment classification datasets/websites such as eBay or TaoBao 4 . Note for multiple source adaptation, one domain tree is selected as the target tree, and the other two are unified into one source domain. A pseudo node is connected to the roots of the two source domain trees and becomes a unified source domain tree. The prediction results are classified according to the target domain and target level. We use (SourceDomains)  X  TargetDomain-TargetLevel to denote a sentiment classification task, e.g., ( K , E )  X  B -L3 means Kitchen and Electronics are the source domains and the nodes in L3 of Book is the target domain. We use TBMC-RS (Taxonomy-based Model Combin ation with Reweighted Sum) to denote our algorithm using cosine function and TBRM-RS (Taxonomy-based Regression Model with Reweighted Sum) to denote our algorithm using TBRM . The weight adjustment strategy of TBMC-RS and TBRM-RS was the method described in Section 5.3. The parameter k is set to 20 (20% of the source domain models). Other values of k are discussed in section 6.3. The SVM classifier is used in the sentiment score function in both TBMC-RS and TBRM-RS. We use classification accuracy as the evaluation metric, which is defined as the proportion of the correctly predicted test instances among the test node. To demonstrate the effectiveness of our approach, we use other four cross-domain methods to compare with our proposed TBMC-RS and TBRM-RS: Best-SVM, Best -SFA, Avg-SVM and Avg-SFA. Since there are more than one target node if the target level is L2 or L3 , the definition of one target node X  X  prediction accuracy is described as follows (assume the source dataset is  X   X   X   X   X ,...,  X   X   X  and the target node is T ): (1) Best-SVM : This method uses SVM classifier [18] for (2) Best-SFA : This method uses SFA algorithm [6] as the (3) Avg-SVM : This method uses SVM classifier [18] for (4) Avg-SFA : This method uses SFA algorithm [6] as the learning Besides the cross-domain methods , our proposed methods also compared to two in-domain classification methods. The definition of one target node X  X  prediction accuracy is described as follows: (5) InDomain : Every node in the domain tree builds an SVM http://www.taobao.com/ (6) TargetUpper : This is the 4-fold cross validation results using The gold standard is the in-dom ain method TargetUpper since it trained on the same node as it is tested. Methods (1), (2) and (5) can be regarded as our goals beca use InDomain was trained within the target domain and Best-SVM and Best-SFA select the best prediction accuracies. Avg-SVM and Avg-SFA are considered as the baselines. Table 3 shows a comprehensive picture of the aforementioned methods on the Amazon datase t. Each row denotes the classification accuraci es of different methods. Each column represents a cross-domain or in-domain sentiment classification task. The averaged accuracies are listed in the rightmost column. Bold-faced numbers are the best results among all the cross-domain classification methods. Fr om Table 3 we observe that TBMC-RS has best cross-domain prediction accuracies in has best cross-domain pred iction accuracies in ( E , B )  X  K -L1 , domain is Kitchen or Electronics, i.e., ( E , B )  X  { K -L1 , K -L2 , K -L3 } and ( K , B )  X  { E -L1 , E -L2 , E -L3 }, the per formance of our proposed TBMC-RS and TBRM-RS is better than InDomain, Best-SVM and Best-SFA in almost all tasks, and be able to compete with the TargeUpper. When the target domain tree is Book, the prediction results are poor in all methods. A logical explanation is that the Book domain carries more diverse terms and noisy text, thus to transfer knowledge from the Electro nics and Kitchen domains to the Book domain is more challengi ng. The results also indicate that the selection of the source domain tree is critical. All results of the TBMC-RS and TBRM-RS outperform the two baselines Avg-SVM and Avg-SFA, and are as good as Best-SVM and Best-SFA. The best overall performance in all cross-domain classification methods is TBRM-RS, surpasses the two goals Best-SVM and Best-SFA, and even better than the in-domain classification method InDomain. The best model among all cr oss-domain and in-domain classification methods is the go ld standard TargetUpper. The TargetUpper is always better than InDomain in each classification task, shows the fact that the distribution gap is still exists inside a domain tree. However, our proposed TBMC-RS and TBRM-RS ( K , B )  X  E -L1 . The results imply that there is a chance to eliminate the distribution gap using our ensemble algorithm under the taxonomy-based cross-domain scheme. The Avg-SFA is better than Avg-SVM in every task, indicating that in average case, the cross-domain sentiment classification algorithm SFA is better than the traditional machine learning algorithm SVM. In the previous experiment, the parameter k is fixed to 20 for TBRM-RS and TBMC-RS. Howeve r, different values of k will influence the prediction accuracies. In addition, different weighting scheme will influence the results too. As a result, for TBRM-RS, three weighting schemes are considered and compared in this section: (1) Sum : This experiment is conducted to demonstrate the (2) WeightedSum : This experiment demons trates the use of the (3) ReweightedSum : This is basically our proposed TBRM-RS, Figure 6 shows the comparisons of different weighting schemes Sum, WeightedSum a nd ReweightedSum on ( K , B )  X  E . The used percentage of source models was shown in the x-axis. The y -axis represents the classification accuracy. In Figure 6 we find that the performance is gradually decreasing when k is increasing from 10 to 100 in all strategies. Howeve r, the ReweightedSum, which adjusts the original cosine similarity weight, has relatively smoother trend. The preliminar y experiment of cross-domain classification gives us the intuition that not every model is suitable for knowledge transfer. Figure 6 further demonstrates the weight adjustment technique according to the tree structure is indeed beneficial for reducing the acc uracy loss caused by the poor Cross-domain 
Classification In-domain Classification source nodes. This subsection only shows the results of ( K , B )  X  E due to the space limitation, however the other two tasks, ( E , B )  X  K and ( K , E )  X  B , also show the similar trend. For the parameter sensitivity analysis on TBMC-RS, two weighting schemes are considered and compared: (1) Sum : This experiment demonstr ates the performance without (2) ReweightedSum: This is our proposed TBMC-RS, but only Figure 7 shows the results of the two weighting schemes Sum and ReweightedSum on ( K , B )  X  E . The performance is gradually decreasing when k is increasing from 10 to 100 in both strategies. The ReweightedSum, which re-weights the original cosine similarity, demonstrates relatively smoother trend. This results show the re-weighting technique base d on the tree structure is also beneficial on the TMBC. Th e other two tasks, i.e., ( E , B )  X  K and ( K , E )  X  B , also show the similar trends. 
Figure 7. Two Weighting Schemes Sum and ReweightedSum In order to analyze the influence of different model applications to the ensemble algorithm, we applied the SFA algorithm [6] on the sentiment score function  X , X  X  X   X   X  of TBRM-RS. The TBRM-SFA-RS uses the original TBRM-RS X  X  method, but the sentiment score function is replaced by the SFA algorithm. The sentiment classifiers are trained on the augmented instances, and then use the trained model to predict the sentiment polarity of the target domain instances. The parameter k is fixed to 20 according to the classification performance of TBRM-RS and TBRM-SFA-RS. TBRM-SFA-RS performs better on the tasks ( E , B )  X  K -L2 , ( K , E )  X  B -L3 , and ties with TBRM-RS on the task ( E , B )  X  K -L1 . The overall performance of TBRM-SFA-RS is slightly better than TBRM-RS. Student X  X  t -test is used to determine significant differences in the results from TBRM-SFA-RS to TBRM-RS. The p -value is 0.21315, showing no significance between these two methods. The result shows that the choice of learning models for the sentiment score function makes only little influences. Figure 6. Three Weighting Schemes Sum, WeightedSum and From section 6.1 to 6.4, we apply multiple source domain trees for training. For single source tree adaptation, the preliminary analysis in Figure 2 shows different sour ce domain to the same target domain has different accuracy distributions, e.g., B -K and E -K . In the single source domain tree adaption experiment, each task is performed on one source domain tree and one target domain tree. The prediction accuracies of different target levels are classified and averaged. The model parameter k is set to 100. Table 5 presents the classification accuracies of the single source domain tree adaptation of five methods (TBMC-RS, Best-SVM, Best-SFA, Avg-SVM and Avg-SFA) and the multiple source domain trees adaptation of TBMC-RS (at the bottom of Table 5). Because this is a cross-domain experiment, we use  X -- X  to indicate the in-domain cases. Numbers in bold-f ace are the best single domain adaptation method in the SourceDomain  X  TargetDomain-TargetLevel experiment. Table 5 shows that TBMC-RS performs K  X  { E -L1 , E -L2 , E -L3 } and E  X  B -L1 , and as good as Best-SVM and Best-SFA in other tasks. As for the baselines, TBMC-RS surpasses the Avg-SVM and Avg-SFA in every task, which shows the effectiveness of the TBMC-RS. The performances of the two baselines Avg-SFA and Avg-SVM are seriously affected by the choice of the source and target domains. If we compare the single and multiple source domain adaptation results of TBMC-RS, when the target domain tree is Kitchen, the performance of ( E , B )  X  { K -L1 , K -L2 ,K -L3 } is as good as E  X  { K -respectively. The performances of ( K , B )  X  { K -L1 , K -L2 , K -L3 } respectively, closer to K  X  { E -L1 , E -L2 , E -L3 }. The results show that the Book domain tree is not so suitable for training, which is consistent with the conclusion of the cross-domain preliminary experiment. To sum up, our proposed TBMC-RS utilize the weight adjustment techniques to diminish the influence of the poor models and enlarge the weights of good mode ls; as a result, the overall performance is more robust and effective than other methods. The native taxonomy structure of many online shopping websites is an important but rarely mentioned property for almost all the cross-domain sentiment classificatio n studies. In this paper, we explore the ensemble of models on the tree-structured Amazon dataset. The ensemble algorithm considers the model weight, the model application and strategies for selec ting the most related models with respect to the target node. In the multiple source adaptation experiment we have shown that both the two proposed strategies, TBMC-RS and TBRM-R S, outperforming the baselines including the SFA algorithm. Performances of three tasks are even better than the upper bound TargetUpper. We believe that to fully utilize the tree structure in the ensemble model is the key to eliminate the distribution gap, which is the major difficulty in the transfer learning research. Besi des the multiple source adaptation experiment, we also demonstrate th e influence of the size of used source nodes. Our proposed weight adjustment technique shows its empirical robustness in the parameter sensitivity experiment. To further understand the impact of different model application algorithm, experiment of using SFA shows little improvement in the overall performance. Finally, the single domain adaptation experiment shows the effectiveness and robustness of the proposed TBMC-RS algorithm. ( K , B )  X 
E -L2 Source TargetDomain-TargetLevel In the future we plan to study more characteristics of the proposed ensemble algorithm under the tree-st ructured dataset, such as to use different weighting strategies and apply different transfer learning algorithms. Our proposed ensemble algorithm is quite general, and other ensemble l earning methods such as bootstrap aggregating (bagging) or boosting for the cross-domain sentiment classification task are also an important research issue. Another research issue is to automatically construct the hierarchical tree structure of the dataset, becaus e currently many online shopping websites use manually constructed hi erarchical. We plan to adopt unsupervised algorithms for automatically constructing the hierarchical tree and compare with the human-built taxonomy. This research was partially supported by Ministry of Science and Technology, Taiwan, under the grants 101-2221-E-002-195-MY3 and 102-2221-E-002-103-MY3. [1] B. Pang and L. Lee,  X  X pinion mining and sentiment [2] J. Blitzer, M. Dredze, and F. Pereira,  X  X iographies, [3] Q. Wu, S. Tan, and X. Che ng,  X  X raph ranking for sentiment [4] N. Ponomareva and M. Thelwall,  X  X o neighbours help?: an [5] Q. Wu, S. Tan, H. Zhai, G. Zhang, M. Duan, and X. Cheng, [6] S. J. Pan, X. Ni, J.-T. S un, Q. Yang, and Z. Chen,  X  X ross-[7] W. Dai, G.-R. Xue, Q. Ya ng, and Y. Yu,  X  X o-clustering [8] D. Bollegala, D. Weir, and J. Carroll,  X  X ross-Domain [9] F. Zhuang, P. Luo, C. Du, Q. He, and Z. Shi,  X  X riplex [10] R. Xia and C. Zong,  X  X  POS-based Ensemble Model for [11] J. Gao, W. Fan, J. Jiang, and J. Han,  X  X nowledge transfer via [12] R. Samdani and W. Yih,  X  X  omain adaptation with ensemble [13] R. Xia, C. Zong, X. Hu, an d E. Cambria,  X  X eature Ensemble [14] N. Ponomareva and M. Thelwa ll,  X  X iographies or Blenders: [15] S. Li and C. Zong,  X  X ulti-domain sentiment classification, X  [16] A. Aue and M. Gamon,  X  X us tomizing sentiment classifiers to [17] B. E. Boser, I. M. Guyon, and V. N. Vapnik,  X  X  training [18] B. Pang, L. Lee, and S. Vaithyanathan,  X  X humbs up?: [19] H. Drucker, D. Wu, and V. N. Vapnik,  X  X upport vector [20] T. Joachims, Text categorization with support vector [21] M. Thomas, B. Pang, and L. Lee,  X  X et out the vote: [22] C.-C. Chang and C.-J. Lin,  X  X IBSVM: a library for support 
