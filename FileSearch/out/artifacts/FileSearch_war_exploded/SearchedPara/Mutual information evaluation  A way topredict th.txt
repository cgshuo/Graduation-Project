 Intelligent Data Ana lysis 17 (2013) 1001 X 1021 1001 DOI 10.3233/IDA-130617 IOS Press Abstract. Feature weighting is one of the popular and effective ways to improve clus tering quality. How to choose a proper weighting method for a data object is widely recognized as a difficult problem. Among majority of weighting schemes and combination weighting methods, the trad itional way is evaluating the performance of feature weightin g by measuring the number of times depends on the number of weighting schemes or the number of combination weighting iteration. To address the quality of clustering.
 Keywords: Mutual information, feature weighting, evaluation, clustering, information bottleneck  X  Corresponding author: Bo Ji, School of Information Engineering, Zhengzhou University, Zhengzhou 450052, Henan, China. Tel.: +86 136 0384 8440; E-mail: iebji@zzu.edu.cn. 1088-467X/13/$27.50 c 2013  X  IOS Press and the authors. All rights reserved 1002 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering 1004 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering  X  0 is the combination weight vector, 1006 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering i,j is.  X  n i,j is the number of times a given term t i appears in that document d j , 1008 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering 1010 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering 1012 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering
Dataset Corpus Newsgroup included Docs per group Total docs Number of Scale
NS2_1,2,3 20Newsgroup ta lk.politics.mideast
NS5_1,2,3 20Newsgroup Comp.graphics
NS10_1,2,3 20Newsgroup Alt.atheism RS2_1,2,3 Reuters Acq, earn 250 500 2000 small
RS5_1,2,3 Reuters Acq, crude, earn
RS8_1,2,3 Reuters Acq, crude, earn
US7_1,2,3 4Universities Course, department
RM2_1,2,3 Reuters Acq, earn 250 4000 2000 medium 1 The parameter option of rainbow software on the 20Newsgroups corpus is  X  X  X stext -avoid-uuencode  X  X kip-header -O 2 X  .It is  X -O 2 X  on the Reuters-21578 corpus and  X  X  X kip-html  X  X o-stoplist -O 2 X  on the 4Universities corpus.

Weighting Dataset MI AC PR RE Dataset MI AC PR RE scheme None NS2_1 2.9894 93.96 2 93.98 93.96 RS5_1 2.8161 78.40 80.22 78.40 Entropy 3.2589 93.76 93.80 93.76 3.1995 78.00 78.53 78.00 MI 3.3011 93.96 93.99 93.96 3.1960 83.80 85.85 83.80 MSD 2.2841 90.14 90.56 90.13 2.1098 81.40 82.75 81.40 TF/IDF 3.0408 91.75 91.77 91.75 3.8449 77.40 77.41 77.40 I_Entropy 2.9893 93.96 93.98 93.96 2.8160 78.40 80.22 78.40 I_MI 2.9892 93.96 93.98 93.96 2.8160 78.40 80.22 78.40 I_MSD 2.9904 93.96 93.98 93.96 2.8171 78.40 80.22 78.40 I_TF/IDF 2.9892 93.96 93.98 93.96 2.8160 78.40 80.22 78.40 None NS2_2 3.0375 92.79 92.80 92.79 RS5_2 2.8007 77.80 79.09 77.80 Entropy 3.3242 94.19 94.19 94.19 3.1684 76.40 78.18 76.40 MI 3.3522 93.19 93.19 93.19 3.1736 66.40 67.02 66.40 MSD 2.3499 92.38 92.40 92.38 2.0851 81.20 82.21 81.20 TF/IDF 3.0417 72.95 72.95 72.95 3.7149 84.60 84.71 84.60 I_Entropy 3.0374 92.79 92.80 92.79 2.8006 77.80 79.09 77.80 I_MI 3.0374 92.79 92.80 92.79 2.8006 77.80 79.09 77.80 I_MSD 3.0385 92.79 92.80 92.79 2.8018 76.60 78.52 76.60 I_TF/IDF 3.0374 92.79 92.80 92.79 2.8006 77.80 79.09 77.80 None NS2_3 2.9574 92.59 92.62 92.59 RS5_3 2.8262 79.80 81.10 79.80 Entropy 3.2397 92.59 92.60 92.59 3.1953 85.40 85.71 85.40 MI 3.2905 92.79 92.83 92.79 3.2063 84.20 84.89 84.20 MSD 2.2148 88.78 88.78 88.78 2.0495 79.20 80.44 79.20 TF/IDF 2.9420 87.37 87.39 87.38 3.6994 84.20 84.53 84.20 I_Entropy 2.9573 92.59 92.62 92.59 2.8261 79.80 81.10 79.80 I_MI 2.9572 92.59 92.62 92.59 2.8261 79.80 81.10 79.80 I_MSD 2.9585 92.59 92.62 92.59 2.8274 79.80 81.10 79.80 I_TF/IDF 2.9572 92.59 92.62 92.59 2.8261 79.80 81.10 79.80 None NS5_1 2.9826 91.94 91.98 91.93 RS8_1 2.9902 64.25 64.00 64.25 Entropy 3.2154 92.34 92.49 92.34 3.3231 67.50 67.92 67.50 MI 3.2589 93.35 93.39 93.35 3.3039 63.75 63.66 63.75 MSD 2.2159 83.87 83.94 83.86 2.3743 75.50 77.12 75.50 TF/IDF 1.9913 80.44 80.74 80.44 3.9108 57.50 56.33 57.50 I_Entropy 2.9825 91.94 91.98 91.93 2.9900 64.25 64.00 64.25 I_MI 2.9825 91.94 91.98 91.93 2.9900 64.25 64.00 64.25 I_MSD 2.9835 92.34 92.37 92.34 2.9910 64.25 64.00 64.25 I_TF/IDF 2.9825 91.94 91.98 91.93 2.9900 64.25 64.00 64.25 None NS5_2 2.9729 94.60 94.64 94.60 RS8_2 2.9198 66.00 65.88 66.00 Entropy 3.1906 95.20 95.23 95.20 3.2492 71.25 72.77 71.25 MI 3.2230 93.80 93.82 93.80 3.2360 60.50 62.07 60.50 MSD 2.2926 88.40 88.58 88.40 2.2861 78.50 80.15 78.50 TF/IDF 2.0727 83.00 83.23 83.00 3.8426 68.50 68.00 68.50 I_Entropy 2.9728 94.60 94.64 94.60 2.9196 66.00 65.88 66.00 I_MI 2.9728 94.60 94.64 94.60 2.9196 66.00 65.88 66.00 I_MSD 2.9736 94.60 94.65 94.60 2.9206 66.00 65.88 66.00 I_TF/IDF 2.9728 94.60 94.65 94.60 2.9196 66.00 65.88 66.00 None NS5_3 3.0448 95.98 96.05 95.98 RS8_3 2.9675 67.50 68.30 67.50 Entropy 3.2703 97.39 97.43 97.39 3.3165 63.75 63.55 63.75 MI 3.2888 95.98 96.00 95.98 3.3145 60.25 59.72 60.25 MSD 2.4350 90.76 90.98 90.77 2.2662 74.00 75.86 74.00 TF/IDF 3.2685 88.15 88.18 88.14 3.8910 55.75 56.25 55.75
I_Entropy 3.0447 95.98 96.05 95.98 2.9674 67.50 68.30 67.50 1014 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering
Weighting Dataset MI AC PR RE Dataset MI AC PR RE scheme I_MI 3.0447 95.98 96.05 95.98 2.9674 67.50 68.30 67.50 I_MSD 3.0455 95.98 96.05 95.98 2.9685 67.50 68.30 67.50 I_TF/IDF 3.0446 95.98 96.05 95.98 2.9674 67.50 68.30 67.50 None NS10_1 3.2262 64.66 63.50 64.62 US7_1 1.2286 41.19 40.66 41.19 Entropy 3.5367 66.06 64.85 65.98 1.3734 42.14 42.55 42.14 MI 3.4930 64.86 62.65 64.83 1.9749 47.38 44.84 47.38 MSD 2.7014 56.22 56.23 56.17 0.1396 23.57 23.22 23.57 TF/IDF 3.2336 37.55 37.30 37.52 0.1735 35.24 35.38 35.24 I_Entropy 3.2261 64.66 63.50 64.62 1.2286 41.19 40.66 41.19 I_MI 3.2261 64.66 63.50 64.62 1.2284 41.19 40.66 41.19 I_MSD 3.2268 65.86 64.99 65.82 1.2424 46.19 46.21 46.19 I_TF/IDF 3.2259 64.66 63.50 64.62 1.2300 41.19 40.66 41.19 None NS10_2 3.1252 66.67 65.14 66.64 US7_2 1.6078 43.33 43.71 43.33 Entropy 3.3707 61.65 61.23 61.53 2.3447 52.62 52.58 52.62 MI 3.3821 64.66 63.57 64.66 2.4143 48.10 46.15 48.10 MSD 2.4643 50.40 47.98 50.31 0.5064 34.29 35.34 34.29 TF/IDF 3.4014 46.39 45.91 46.35 2.7790 39.76 38.55 39.76 I_Entropy 3.1251 66.67 65.14 66.64 1.6077 43.33 43.71 43.33 I_MI 3.1251 66.67 65.14 66.64 1.6076 43.33 43.71 43.33 I_MSD 3.1259 66.27 64.54 66.25 1.6123 43.57 43.33 43.57 I_TF/IDF 3.1251 66.67 65.14 66.64 1.6077 43.33 43.71 43.33 None NS10_3 3.0377 69.82 68.91 69.70 US7_3 1.5606 45.00 45.56 45.00 Entropy 3.2338 72.64 71.59 72.51 2.1599 52.62 49.50 52.62 MI 3.2703 70.22 68.77 70.09 2.2931 46.90 47.24 46.90 MSD 2.4081 54.33 52.44 54.14 0.7406 26.43 27.30 26.43 TF/IDF 3.4273 43.26 42.64 43.15 0.1530 35.24 34.86 35.24 I_Entropy 3.0376 69.82 68.91 69.70 1.5605 45.00 45.56 45.00 I_MI 3.0376 69.82 68.91 69.70 1.5604 45.00 45.56 45.00 I_MSD 3.0384 69.62 68.95 69.49 1.5642 45.48 46.23 45.48 I_TF/IDF 3.0376 71.43 70.99 71.30 1.5559 45.00 45.56 45.00 None RS2_1 2.6055 91.00 92.25 91.00 RM2_1 2.6931 92.40 93.34 92.40 Entropy 3.3423 91.00 92.13 91.00 3.4997 90.85 92.22 90.85 MI 3.3929 91.00 92.13 91.00 3.5980 90.35 91.86 90.35 MSD 1.5940 95.80 96.13 95.80 1.5405 92.35 93.15 92.35 TF/IDF 3.7274 93.40 93.48 93.40 4.1228 93.35 93.47 93.35 I_Entropy 2.6052 91.00 92.25 91.00 2.6929 92.40 93.34 92.40 I_MI 2.6052 91.00 92.25 91.00 2.6929 92.40 93.34 92.40 I_MSD 2.6082 91.00 92.25 91.00 2.6958 92.40 93.34 92.40 I_TF/IDF 2.6053 91.00 92.25 91.00 2.6931 92.40 93.34 92.40 None RS2_2 2.6129 93.80 94.32 93.80 RM2_2 2.6811 92.17 93.21 92.18 Entropy 3.3270 94.20 94.72 94.20 3.4912 91.75 92.90 91.75 MI 3.3761 93.00 93.76 93.00 3.5905 91.55 92.76 91.55 MSD 1.6077 94.80 95.09 94.80 1.5117 93.58 94.11 93.58 TF/IDF 3.7652 92.80 92.98 92.80 4.0646 93.30 93.44 93.30 I_Entropy 2.6126 93.80 94.32 93.80 2.6809 92.17 93.21 92.18 I_MI 2.6125 93.80 94.32 93.80 2.6809 92.17 93.21 92.18 I_MSD 2.6154 93.80 94.32 93.80 2.6839 92.17 93.21 92.18 I_TF/IDF 2.6127 93.80 94.32 93.80 2.6811 92.17 93.21 92.18 None RS2_3 2.6286 94.40 94.96 94.40 RM2_3 2.6976 91.90 92.97 91.90 Entropy 3.3261 93.20 93.92 93.20 3.5050 91.72 92.86 91.72 MI 3.3730 90.80 92.10 90.80 3.6015 91.13 92.42 91.13
Weighting Dataset MI AC PR RE Dataset MI AC PR RE scheme MSD 1.6115 93.00 93.37 93.00 1.5472 94.25 94.72 94.25 TF/IDF 3.6896 93.40 93.41 93.40 4.1274 93.45 93.53 93.45 I_Entropy 2.6284 94.40 94.96 94.40 2.6973 91.90 92.97 91.90 I_MI 2.6283 94.40 94.96 94.40 2.6973 91.90 92.97 91.90 I_MSD 2.6312 94.40 94.96 94.40 2.7002 91.88 92.95 91.88
I_TF/IDF 2.6284 94.40 94.96 94.40 2.6975 91.90 92.97 91.90 2 Bold Font represents the best result, Italic Font represents the second best result. 3 For the convenience of comparison, we scale up the max value of MI to 50; The dot line represents the values of PR, the largest value. 1016 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering 1018 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering
Fig. 8. K-means (entropy and MSD), the dotted line curve is for PR and the solidline curve is for mutual information value. 1020 B. Ji et al. / Mutual information evaluation: A way to predict the performance of feature weighting on clustering [1] A.K. Jain, M.N. Murty and P.J. Flynn, Data clustering: A review, ACM Computing Surveys 31 (1999), 264 X 323. [3] A.K. Jain, Data clustering: 50 years beyond K-means, Pattern Recognition Letters 31 (2010), 651 X 666. [4] A. Kraskov, H. Stogbauer and R.G. Andrzejak, Hierarchical clustering using mutual information, Europhysics Letters 70 [5] B.G. Hu and Y. Wang, Evaluation criteria based on mutual information for classifications including rejected class, Acta [6] C.L. Hwang and K. Yoon, Multiple Attribute Decision Ma king: Methods and Applications , Springer, Berlin, 1981. [7] C. OkanSakar and O. Kursun, A method for combining mutual information and canonical correlation analysis, predictive [8] C.Y. Tsai and C.C. Chiu, Developing a feature weight self-adjustment mechanism for a K-means clustering algorithm, [10] F. Debole and F. Sebastiani, Supervised term weighting for automated text categorization, in: ACM Symposium on Ap-[11] F. Sombut, P. Ouen and A. Boonwat, Feature subset selection wrapper based on mutual information and rough sets, [12] H.C. Peng, F.H. Long and C. Ding, Feature selection based on mutual information: Criteria of max-dependency, max-[13] I.S. Dhillon, Information theoretic coclustering, in: KDD2003 , (2003). [14] J.Y. Zhang, L.Q. Peng and X.X. Zhao, Robust data clustering by learning multi-metric Lq-norm distances, Expert Systems [15] K. Torkkola, Feature extraction by non-parametric mutual information maximization, Journal of Machine Learning [16] L. Jing, M.L. Ng and J.Z. Huang, An entropy weighting k-means agorithm for subspace clustering of high-dimensional [17] M. Senst, G. Ascheid and H. Luumlders, Performance evaluation of the markov chain monte carlo MIMO detector based [18] N. Slonim, The information bottleneck: Theory and application, Ph.D. Dissertation, Hebrew University of Jerusalem, [19] O. Shamir, S. Sabato and N. Tishby, Learning and generalization with the information bottleneck, Theoretical Computer [21] P. Ido, M. Oded and B.G. Irad, Evaluation of gene-expression clustering via mutual information distance measure, Bmc [22] Q. Quan, C. Yu-hua and W. Yue, PID-based feature weight learning and its application in intrusion detection, CSIE 5 [23] R. Steuer, J. Kurths and C.O. Daub, The mutual information: Detecting and evaluating dependencies between variables, [24] S. Cost and S. Salzberg, A weighted nearest neighbor algorithm for learning with symbolic features, Machine Learning [25] S. Shankar and G. Karypis, A feature weight adjustment algorithm for document categorization, in: KDD2000, Boston, [27] T. Yang and C.C. Hung, Multiple-attribute decisi on making methods for pla nt layout design problem, Robotics and [28] X. Bai, G. Chen, Z. Lin, W. Yin and J. Dong, Improving image clustering: An unsupervised feature weight learning [29] X.J. Chen, Y.M. Ye and X.F. Xu, A feature group weighting method for subspace clustering of high-dimensional data, [30] Y.M. Wang and C. Parkand, Multiple attribute decision making based on fuzzy preference information on alternatives: [31] Y. Seldin, N. Slonim and N. Tishby, Information bottleneck for non co-occurrence data, in: Advances in Neural Informa-
