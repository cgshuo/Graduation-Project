 Language Models (LMs) are important for various natural language processing tasks [Ma and Zhao 2012; Wang et al. 2015a; Jia and Zhao 2014], such as speech recognition and Statistical Machine Translation (SMT) [Zhang and Zhao 2013; Zhang et al. 2014; Xu and Zhao 2012; Zhao et al. 2013]. There are mainly two ways to improve the performance of LMs in SMT. One is to estimate the probabilities of N -grams better and the other is to use a larger corpus for LM construction.

Traditionally, Back-off N -gram Language Models (BNLMs) [Chen and Goodman 1996, 1999; Stolcke 2002] are widely used for probability estimation. For better proba-bility estimation [Zhao et al. 2009a, 2009b, 2013; Zhao 2009], Continuous-Space Lan-guage Models (CSLMs), especially Neural Network based Language Models (NNLMs) [Bengio et al. 2003; Schwenk 2007; Le et al. 2011], have been used in SMT in recent years [Schwenk et al. 2006; Son et al. 2010; Schwenk et al. 2012; Son et al. 2012; Niehues and Waibel 2012]. These researches have shown that CSLMs can improve BLEU scores of SMT in comparison with BNLMs with the same sized training corpus. However, in practice, CSLMs have not been widely used in SMT due to a too high com-putational cost. Various methods have been proposed to tackle the training cost issues [Son et al. 2010; Schwenk et al. 2012; Mikolov et al. 2011], though there has been little progress on reducing the decoding cost, until recently. High time cost makes it difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs is a two-pass way: the first pass uses a BNLM in decoding to produce an n -best list, and in the second pass, a CSLM is used to rerank those n -best translations [Schwenk et al. 2006; Son et al. 2010; Schwenk et al. 2012; Son et al. 2012]. Another approach is to use Restricted Boltzmann Machines (RBMs) [Niehues and Waibel 2012] instead of using multilayer neural networks [Bengio et al. 2003; Schwenk 2007; Le et al. 2011]. Since the probability of an RBM can be calculated efficiently [Niehues and Waibel 2012], the RBM LM can be used in SMT decoding, but RBMs had only been used in a small-scale SMT task because of high training cost as well.

Vaswani et al. [2013] extend the Neural Probabilistic Language Model (NPLM) [Bengio et al. 2003] from two angles: (1) the rectified linear unit plays as the activa-tion function that computes much more efficiently than Sigmoid or hyperbolic tangent units [Nair and Hinton 2010], and (2) Noise-Contrastive Estimation (NCE) [Gutmann and Hyv  X  arinen 2010; Mnih and Hinton 2008] is adopted for model training, where the repeated summations over the vocabulary are not necessary. These two extensions enable building large-scale NPLMs efficiently. The main contribution of their work is to reduce the training cost of CSLM and apply the CSLM to a SMT decoder. However, they do not show any improvement in decoding speed in comparison with N -gram LMs. In this article, we propose a novel method to speed up decoding. The main idea of using NNLMs efficiently behind our method is to store the precalculated probabilities from CSLMs into N -gram format, which is quite different from that of Vaswani et al. [2013]. We will compare the decoding efficiency in Section 4.2.5. There are also some related researches that implement NNLMs or NN translation models for SMT [Auli et al. 2013; Kalchbrenner and Blunsom 2013; Liu et al. 2013; Zou et al. 2013; Devlin et al. 2014; Liu et al. 2014; Gao et al. 2014; Peng and Gildea 2014; Sundermeyer et al. 2014; Li et al. 2014; Cho et al. 2014; Lauly et al. 2014]. In particular, Devlin et al. [2014] propose a fast joint model of the LM and translation model using self-normalized NN and pre-computing the hidden layer. However, to integrate these techniques into SMT, almost all these existing methods more or less need modifications over the SMT decoder, while our method can be directly integrated into SMT without any modification.
In recent years, LMs in real application systems, such as Amazon Web Services, are increasing in size, because the performance (BLEU in SMT) of the LM is largely determined by the size of the corpus, and available corpora have become very large [Brants et al. 2007]. As BNLMs with cheaper computational cost can be trained from much larger corpora than CSLMs, to improve a BNLM by using a CSLM trained from a smaller corpus seems simpler and more realistic. Actually, a CSLM from a smaller corpus has been shown to enhance SMT [Papineni et al. 2002] reranking [Schwenk 2010; Huang et al. 2013]. In this article, we will demonstrate that a BNLM simulating a CSLM can improve BLEU scores in the first-pass decoding. The existing convert-ing approaches [Deoras et al. 2011; Arsoy et al. 2013, 2014] mainly focus on speech recognition. Deoras et al. [2011] use a Recurrent Neural Network Language Model (RNNLM) [Mikolov et al. 2010] to generate a large amount of texts, which is gener-ated by sampling words from the probability distributions calculated by RNNLM. They train a BNLM from the text using the interpolated Kneser-Ney smoothing. Arsoy et al. [2013] convert NNLMs into BNLMs using artificial higher-order N -grams constructed from lower-order N -grams. That is, all the words in the short-list are added to the tail of i -grams in the corpus in order to produce ( i + 1)-grams that may not be in the corpus. These ( i + 1)-grams are called artificial N-grams . However, most of the artificial N -grams do not have linguistic sense. For example, if a trigram is  X  would like to , X  then the extended 4-grams will be  X  wouldliketo* , X  where  X  *  X  stands for any word in the short-list. A lot of meaningless 4-grams, such as  X  wouldliketowould  X  X r X  would like to cat , X  will be generated. Although some of them can be pruned using a modified entropy-based method [Arsoy et al. 2014], a large part of meaningless 4-grams will still remain in the converted CSLM. In addition, a huge intermediate LM will be produced, which is commonly thousands of times larger than the original LM before pruning, 1 and the probabilities of its N -grams will have to be calculated using the CSLM. Consequently, the huge intermediate LM should be pruned into a specified size. Because the Arsoy method is very time and space consuming, it can only be applied to small corpora. For our proposed method, N -grams in the corpus are used as the input N -grams of CSLM. We therefore call them natural N-grams . Since no any intermediate LM will be constructed during converting, our method can be applied to very large corpora. Wang et al. [2014] and Wang et al. [2015b] propose a series of bilingual LM growing methods, which can be viewed as LM adaptation methods because they focus on special domain corpora.

To apply the proposed converting method to an extremely large corpora with billions of words, the time cost of CSLM converting is still an obstacle. Therefore, to prune LMs before converting LMs, without LM quality loss, is important for CSLM converting. A commonly used method for pruning LMs is cutoff , which simply discards N -grams whose frequencies are below a certain threshold. However, Heafield et al. [2013] have shown that the cutoff method may lead to significant performance loss in SMT. Another well-known method is entropy-based pruning [Stolcke 1998]. According to our best knowledge, this is the state-of-the-art pruning method. A shortcoming of this method is that it only uses monolingual information from the target language.

In a SMT system, every target language phrase is from a bilingual phrase table. So monolingual LM pruning methods may discard some useful N -grams, or keep some useless ones for SMT decoding. To overcome the deficiency of the existing pruning approaches, we propose a novel method that explicitly takes the bilingual phrase table into consideration for LM pruning. This Bilingual LM Pruning (BLMP) method is based on the following observation: the translation hypotheses of a phrase-based SMT system are a concatenation of the phrases from the phrase table. Suppose that two phrases  X  wouldliketolearn  X  X nd X  Chinese as second language  X  are in the phrase table and N -grams LM, respectively. In decoding, the two phrases may be connected together as  X  would like to learn Chinese as second language . X  However, the N -grams  X  would like to learn Chinese  X  X r X  learn Chinese as second language , X  which are concatenations of two phrases in decoding, may possibly be outside the N -gram LM. In other words, the connecting phrases may be defined to be the minimum set of N -grams that are necessarily used in decoding and ensure that all useful N -grams are retained.
The large LM constructed from an additional monolingual corpus can be pruned into a small LM without sacrificing its quality. The pruned LM can even outperform the large LM if the additional corpus introduces useful out-of-domain information. By using the proposed pruning and converting methods together, we can first prune the BNLM built from the large corpus, and then only convert the pruned LM. This will save a lot of computing time, and make it possible to use a very huge corpus in a reasonable time.

The remainder of this article is organized as follows. Section 2 describes BNLM and CSLM, and reviews CSLM converting methods using artificial N -grams. Section 3 proposes a novel method for converting a CSLM into a BNLM with efficient bilingual LM pruning. Section 4 reports experiments and analyzes the results. We summarize this article in Section 5. This section will introduce the structure and probability estimation of a BNLM and CSLM, and the existing CSLM conversion methods using artificial N -grams. A BNLM predicts the probability of a word w i given its preceding ( n  X  1) words history, h the training data. So an estimation by backing-off to models with smaller histories is necessary. In the case of the interpolated Kneser-Ney smoothing [Chen and Goodman 1996, 1999], the probability of w i given h i under a BNLM, P b ( w i | h i ), is defined as where  X  P b ( w i | h i ) is a discounted probability and  X  ( h i ) is the back-off weight. A four-layer feedforward neural network based CSLM works in the following way: input layer projects all words in the context h i onto projection layer (the first hidden layer); the second hidden layer and output layer achieve nonlinear probability estimation and calculate LM probability P ( w i | h i ) for a given context [Schwenk 2007].
The CSLM calculates the probabilities of all words in the vocabulary of corpus given the context at once. However, due to the high computational complexity of calculating the probabilities of all words, the CSLM can only be used to calculate the probabilities of a small subset of the entire vocabulary. This subset is called a short-list ,which consists of the most frequent words in the vocabulary. The CSLM also calculates the sum of the probabilities of all words inside and outside the short-list by assigning a neuron for that purpose. The probabilities of other words outside the short-list are still obtained from a BNLM [Schwenk 2007, 2010].

Let w i and h i be the current word and history, respectively. The CSLM with a BNLM calculates the probability of w i given h i , P ( w i | h i ), as follows: where V 0 is the short-list, P c (  X  ) is the probability calculated by the CSLM, w  X  V 0 P c ( w | h i ) is the sum of probabilities over all the words in the short-list in the output layer, P b (  X  ) is the probability calculated by the BNLM as in Equation (1), and
We can also understand that the CSLM redistributes the probability mass of all words in the short-list. This probability mass is calculated by using the BNLM. Since the CSLM can calculate the probabilities of N -grams outside the training corpus, one way to construct a large Converted CSLM (CONV) is to generate N -grams based on the training corpus. The existing methods use the short-list to construct those new N -grams.

The method in Arsoy et al. [2013] and Arsoy et al. [2014] adds all the words in the short-list after the tail word of the i -grams, to construct the ( i + 1)-grams. Then the probabilities of the ( i + 1)-grams are calculated using the ( i + 1)-CSLM. 2 As a result, a very large converted ( i + 1)-gram LM will be generated, and then this large LM needs to be pruned into a smaller size using entropy-based LM pruning [Stolcke 1998]. 3 The ( i + 2)-grams are grown using ( i + 1)-grams, recursively. At last the CONV is interpolated with the original BNLM. Figure 1 illustrates the Arsoy method. Since CSLM outperforms BNLM in probability estimation accuracy and BNLM out-performs CSLM in computational time, our key idea of converting CSLM into BNLM is to use the probabilities of N -grams calculated by CSLM to rewrite the probabilities of N -grams of BNLM. That is, N -grams from BNLM are used as the input of CSLM, and the output probabilities of CSLM together with the corresponding N -grams of BNLM constitute CONV.

The N -grams in the CONV can be viewed as a subset of the CSLM. In other words, this CONV can only represent part of the CSLM. To construct a larger CONV to simulate the CSLM and eventually improve the LM, the N -grams outside the corpus must be considered. 4 The N -grams can be divided into artificial ones and natural ones. The converting method using artificial N -grams has been reviewed at Section 2.3.
In short, our proposed converting method uses natural N -grams that actually occur in corpus, while the Arsoy method uses artificial N -grams. For a small corpus such as 42M words/1M sentences, which is commonly used for SMT, the Arsoy method may perform well, because a large CONV can be generated without an extra corpus. However, the Arsoy method is hard to scale up, because the computational cost will increase dramatically as facing a very large corpus. In contrast, our method can work on an extremely large corpus with billions of words by selecting the most useful N -grams in a large LM for CSLM converting. That is, we can use our bilingual pruning method specially designed for SMT at first, and then apply our CSLM converting method. A drawback of the Arsoy method is overgeneration of poor-quality artificial N -grams, such as  X  would like to would , X   X  wouldliketocat , X  and so on. Although part of these N -grams can be pruned by using an entropy-based strategy [Arsoy et al. 2014], a lot of useless N -grams will still remain in the converted CSLM. Instead of using artificial N -grams, we put forward generating natural N -grams from a larger in-domain corpus as the original training corpus.

The pipeline of NNGC is given in Figure 2. Our new approach can be summarized as the following four steps: (1) Split the BNLM into different order N -grams ( n = 2,3,4,5), and use the unigrams (2) Take N -grams ( n = 2,3,4,5) in the BNLM as input of corresponding order ( n = (3) Replace the probabilities of N -grams ( n = 2,3,4,5) in the BNLM with the prob-(4) Renormalize the probabilities and back-off weights  X  ( h i ) in accordance with Equa-
This converting method is hereafter referred to as Natural N-gram based Converting (NNGC) . Though larger and larger LMs can be constructed with rapid development of computing resource, it is still very time consuming for those extremely large LM constructing tasks. Therefore, to develop an efficient LM pruning method without performance loss is still necessary and very important.

To overcome the drawback of the existing monolingual pruning approaches, we pro-pose a method that explicitly takes the bilingual phrase table into consideration to identify the minimum set of N -grams that are potentially to be used in decoding. 3.3.1. The Essence in Large LMs. LM from a larger monolingual in-domain corpus 6 can improve both PPL and BLEU [Brants et al. 2007]. Our question is, which detailed factor yields the improvement of a large LM?
There are two main hypotheses about the improvement: (1) the probabilities in a larger LM are better estimated than those in a smaller LM; (2) a larger LM has more possibly useful N -grams, because a larger LM has more N -grams than a smaller LM. To testify the first hypothesis, we first made three back-off 5-gram LMs from three NTCIR English corpora consisting of 42M, 746M, and 5B words, respectively. We call the corresponding LMs KN42M , KN746M ,and KN5B . Both the 746M and 5B word corpora included the 42M word corpus. We used the 42M word parallel corpus in the Chinese-to-English SMT experiments. Detailed settings will be further described in Section 4. All of the LMs were made with the interpolated Kneser-Ney (KN) smoothing method without cutoff by using the vocabulary of KN42M.
 We extracted the same sets of N -grams as contained in KN42M from KN746M and KN5B. The Extracted LMs were renormalized to produce EKN746M and EKN5B.
 These LMs (KN42M and EKN746M/5B) have the same N -grams with different proba-bilities and back-off weights. We conducted SMT experiments using these LMs under the same settings of other models. Their PPL and BLEU on test data are shown in Table I. In this table,  X  X ize X  means the sizes in gigabytes (G) and  X  N -gram X  means the number of N -grams in millions (M) of uncompressed N -gram files. Table I shows that the extracted LMs obtain better PPL, but similar BLEU in comparison with KN42M. The results of Table I demonstrate that higher PPL does not lead to much better BLEU, which suggests that the BLEU improvement with a larger LM may not directly come from the better probability estimation for the same N -grams in the smaller LM, but from more N -grams. 3.3.2. Connecting Phrase-Based Pruning. Because the translation hypotheses of a phrase-based SMT system are various concatenations of phrases from the phrase table, only the N -grams in the hypotheses are used in SMT decoding. Namely, only the phrases in the phrase table and the connecting phrases can be generated and used in the decoding, and these N -grams are considered as useful . N -grams other than these phrases are not used in decoding and they are considered as useless .

Although some N -grams in LMs constructed from a large monolingual corpus have linguistic senses, such as  X  X ixed polarity compound X  or  X  X onventional lithographic etch process X  in KN5B of Table I, they are neither in the phrase table constructed from a small bilingual corpus, nor connecting phrases. So these N -grams will never occur in SMT decoding.

Thus, we propose a new LM pruning method based on the phrase table. An N -gram that appears in a translation output satisfies either one of the following two conditions: (1) it is already included in a phrase in the phrase table or (2) it is the result of concatenating two or more phrases in the phrase table.
 According to the preceding discussion, we design the following procedure:
Let w b a be a target-language phrase starting from the a -th word and ending with the b -th word, and  X w b a  X  be a phrase including w b a as a part of it, where  X  and  X  represent any word sequence or none. An i -gram phrase w k 1 w i k + 1 (1  X  k  X  i  X  1) is a connecting phrase, 7 if (1) w k 1 is the right (rear) part of one phrase  X w k 1 in the phrase table, and (2) w i k + 1 is the left (front) part of one phrase w i k + 1  X  in the phrase table.
For example, let  X  abcd  X  be a 4-gram phrase; it is a connecting phrase if at least one of the following conditions holds: (1)  X   X   X  X nd X  bcd  X   X  are in the phrase table, or (2)  X   X  ab  X  X nd X  cd  X   X  are in the phrase table, or (3)  X   X  abc  X  X nd X  d  X   X  are in the phrase table.

Therefore, the N -grams in the phrase table from Step 1 and the connecting phrases from Step 2 are kept, and then their probabilities are renormalized. 3.3.3. Tuning the Size of the Pruned LM. Using connecting phrases, a large LM can be pruned into a smaller one. During this pruning process, we may determine the size of the pruned LM. Thus, the more useful connecting phrases should be selected by ranking the occurrence probabilities of the connecting phrases in SMT decoding.
Suppose that P ( e | f ) is the translation probability from f (source phrase) to e (target phrase), which can be calculated using bilingual parallel training data. In decoding, the probability of a target phrase e occurring in SMT can be estimated as where P source ( f ) is the occurrence probability of a source phrase, which can be cal-culated using source LMs in the training data. This P target ( e ) will absorb bilingual information rather than only using monolingual target LMs. 8 If a target phrase e is long, its aligned source phrase f will also be long and has a low occurrence probability P source ( f ). According to Equation (5), P target ( e ) will also be low.
After all of the connecting phrases are generated, their occurrence probabilities are calculated to determine which connecting phrases should be discarded. For an i -gram connecting phrase w k 1 w i k + 1 , its probability can be roughly estimated as 9 phrases whose occurrence probabilities are higher than the threshold are retained. In this way, the sizes of the pruned LMs are tuned to smaller proper ones.

The thresholds for each order N -grams are different. The distribution ratios of dif-ferent order N -grams ( n = 2, 3, 4, 5) in pruned LMs follow the small LM. It should be noted that the unigrams are not pruned because we use the vocabulary of small corpus for all LMs. In practice, the N -grams in different orders are ranked independently. The top N -grams in different orders of large LMs, such as KN5B, are selected with the same distribution ratio of small LMs, such as around 46: 148: 244: 295 of KN42M in NTCIR task (please refer to Section 4.1 for setting up LMs). 3.4.1. Computational Complexity of CSLM Converting Methods. The time complexity of the CSLM is given as [Schwenk 2007] where n is the order of N -grams, P is the size of the projection layer, H is the size of the hidden layer, and N is the size of the output layer. The original N is equal to the size of the vocabulary ( | V | ). To reduce the time complexity, the short-list V 0 in Equation (2), which is a subset of the whole vocabulary, is used as the output layer N . The probabilities of other words outside the short-list in the output layer will be calculated using the background BNLM.

Arsoy et al. have applied their method to 4-gram LM for speech recognition [Arsoy et al. 2013, 2014]. The Arsoy method needs to add every word in the short-list after the tail word of i -gram to construct the ( i + 1)-gram. The short-list usually includes thousands of words, so the generated ( i + 1)-grams will be thousands larger than the i -grams before they are pruned. Because the higher-order N -grams commonly contain more N -grams, computational cost significantly grows as the order of N -gram increases. The limited size of vocabulary and low-order N -gram make the Arsoy method applicable to speech recognition. However, SMT needs a higher-order BNLM (5-gram LM as a common setting for SMT, compared with 4-gram LM for speech recognition). Thus, the Arsoy method is not feasible for large BNLMs because the converting time for construction is basically prohibitive.

Since we rewrite the N -grams from the original BNLM, we only need to calculate the same number of N -grams as the original BNLM, instead of thousands more times of the original BNLM as that in Arsoy et al. [2013] and Arsoy et al. [2014]. The NNGC method thus requires a much lower computational cost, which makes NNGC have good scalability on large corpora. 3.4.2. Computational Complexity of CSLM Methods in Decoding. To improve the efficiency of CSLM in SMT decoding, NPLMs [Vaswani et al. 2013] use rectified linear units [Nair and Hinton 2010], where its activations are cheaper to compute in comparison with sigmoid or hyperbolic tangent units. They also use NCE [Gutmann and Hyv  X  arinen 2010; Mnih and Hinton 2008] in the output layer, instead of softmax, which requires a summation over all the units in the output layer in training, to produce  X  X pproximately normalized probabilities X  and  X  X imply ignore normalization X  in decoding [Vaswani et al. 2013]. The Arsoy converting method and the proposed NNGC method both store the N -grams and their precalculated probabilities from CSLM into N -gram LM format. So computation complexity is similar with the N -gram LM. In summary, Table II shows the computation complexity of all the methods mentioned previously. 10 3.4.3. Computational Complexity of LM Pruning Methods. For the time complexity, the entropy-based pruning method needs to calculate all the related probabilities and back-off weights for the N -grams with the same history h and back-off history h .Our proposed BLMP only takes the N -grams themselves into account, which allows parallel computation speedup. 11
For the space complexity, the entropy-based pruning method needs to load all the related N -grams of the LM into the memory, which is hard to implement if a very huge LM (such as larger than 1T) needs to be pruned. In contrast, BLMP only needs to take the N -grams used for pruning, rather than loading any other N -gram.

Therefore, BLMP has advantages in both time and space complexities. In this section, comparison experiments between our proposed methods and the exist-ing approaches were conducted and the results were analyzed. Our implementation of the proposed NNGC CSLM converting method has been available online. 12 We used the patent bilingual data for the Chinese-to-English patent translation sub-task from the NTCIR-9 patent translation task [Goto et al. 2011]. The parallel training, development, and test data consisted of 1M, 2,000, and 2,000 sentences, respectively.
The same settings of the NTCIR-9 Chinese-to-English translation baseline system [Goto et al. 2011] were followed besides the part of LMs for the purpose of compari-son. We used the Moses phrase-based SMT system [Koehn et al. 2003], together with GIZA++ [Och and Ney 2003] for alignment and MERT [Och 2003] for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores. 13 4.1.1. Corpus. Three corpora with different sizes were used: (1) Corpus42M: The English side of the 1M parallel sentence training data with 42M (2) Corpus746M: The English text from the 2005 U.S. patent data distributed in the (3) Corpus5B: English texts extracted from U.S. patent data (from 1993 to 2005) with
It should be noted that the Corpus42M was added to Corpus746M and Corpus5B. 4.1.2. BNLM. All the BNLMs were trained using SRILM [Stolcke 2002; Stolcke et al. 2011] and the same vocabulary of Corpus42M was used. As the baseline BNLM, we trained a 5-gram BNLM with interpolated Kneser-Ney smoothing using Corpus42M. We did not discard any N -grams in training this model. That is, we did not use count cutoffs. This BNLM was called KN42M .

We also trained a larger 5-gram BNLM with interpolated Kneser-Ney smoothing by using Corpus746M, discarding 3,4,5-grams that occurred only once when we created KN746. This BNLM was called KN746M(cutoff) .

The 5-gram LMs were trained with the interpolated KN and Good-Turing (GT) smoothing methods, 14 on the Corpus5B without cutoff. These LMs were called KN5B and GT5B , respectively. The KN/GT5B would be pruned into smaller ones in different sizes with different pruning methods. 4.1.3. CSLM. A 5-gram CSLM was trained on the same 1M training sentences (Cor-pus42M) using the CSLM toolkit [Schwenk 2010]. The settings for the CSLM were as follows: projection layer of dimension 256 for each word, hidden layer of dimension 384, and output layer (short-list) of dimension 8,192, which were recommended in the CSLM toolkit and Wang et al. [2013]. This CSLM was called CSLM42M, in which KN42M is used as the background BNLM.

Arsoy et al. [2013] and Arsoy et al. [2014] used around 55M words as the corpus, including 84K words as vocabulary, and 20K words as the short-list. They only con-structed a 4-gram LM. In this article, we used around 42M words as the corpus, in-cluding 456K words as vocabulary, and 8K words, which covers 92.89% of words in the training corpus, 15 as the short-list for both NNGC and the Arsoy method. We adopted a similar size setting for the corpus and short-list as [Arsoy et al. 2013] and [Arsoy et al. 2014]. Our vocabulary was much larger than theirs, because the whole vocabu-lary must be used for decoding in SMT, compared with only a small vocabulary used in speech recognition. The ratio of | V 0 | / | V | affects how much time cost will reduced for the output layer. In Arsoy X  X  experiments, | V | was small (84K), and nearly 75% time was saved by using 20K as the short-list. For NNGC, | V | was 456K and the short-list was 8K. Therefore, nearly 98% time cost was reduced.

It should be noted that every setting of the CSLM for all of the methods was the same for fair comparison in this article. 4.1.4. SMT Models. Fourteen standard SMT feature scores are used: four translation model scores, one phrase pair number penalty score, one word penalty score, seven distortion scores, and one LM score.

All the models were trained on Corpus42M, except LMs. The weights between the 14 standard SMT features were tuned using MERT independently.

All of the experiments were conducted using the same computer with 2.70GHz CPUs. 4.2.1. Setup for Proposed NNGC Method. The CSLM can only predict the probability of the same order i -grams as they are. That is, the bigram CSLM can only calculate the probabilities of bigrams. So different order CSLMs must be constructed if we want to convert a 5 -gram LM.
 The 2,3,4,5-CSLMs were trained with the same setting for the CSLM described in Section 4.1.3 on Corpus42M using the CSLM toolkit [Schwenk 2010], and KN42M as the background BNLM. These CSLMs were called 2,3,4,5-CSLM42Ms, respectively. By using the method described in Section 4.2, we rewrote KN42M with CSLM42Ms. This rewritten BNLM was interpolated with the original KN42M. The interpolation weight was determined by the grid search. That is, we changed the interpolation weight to 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolated BNLM. Then, we used that interpolated BNLM in the SMT system to tune the weight parameters on the first half (1,000 sentences) of the development data. Subsequently, we selected the interpolation weight that obtained the highest BLEU score on the second half of the development data. As the interpolation weights were determined, we applied MERT again to the whole 2,000 sentence development data to tune the weight parameters. 16 This converted BNLM was called CONV-KN42M. 4.2.2. Setup for Compared Methods. The same CSLM42Ms were used for the Arsoy method. All the words in short-list were added after the tail word of i -grams, 17 to construct the ( i + 1)-grams. Then, the probabilities of ( i + 1)-grams were calculated using the ( i + 1)-CSLM. So a large converted ( i + 1)-gram LM will be grown, and then it needs to be pruned into the same size as the original ( i + 1)-grams using entropy-based LM pruning. The ( i +2)-grams were grown using ( i + 1)-grams recursively. At last, the CONV was interpolated with the original BNLM 18 using the same methods in Section 4.2.1. 4.2.3. Comparison between Converting Artificial N-grams and Natural N-grams. The KN42M was the BNLM trained on Corpus42M using the interpolated Kneser-Ney smoothing. The Arsoy42M was the converted CSLM42M using the Arsoy method, which used artificial N -grams. The converted CSLM and the original BNLM were interpolated in Arsoy et al. [2013] and Arsoy et al. [2014]. Since the N -grams in the converted CSLM and BNLM were quite different, the interpolated LM was larger than both of them. Consequently, the interpolated LM was pruned into the same size as the original BNLM. The CONV-KN42M was the converted CSLM42M using NNGC, which used natural N -grams from KN42M, and CONV-KN42M (interpolated) was the CONV-KN42M interpolated with KN42M. All the preceding LMs were trained on Corpus42M.
The comparisons of the Arsoy method and ours are shown in Table III. We performed the paired bootstrap resampling test [Koehn 2004], 19 and sampled 2,000 samples for each significance test. Table IV showed statistical significance test.
From the results in Tables III and IV, we can obtain the following observations: (1) Both the Arsoy method and NNGC improved the PPL compared with the origi-(2) The BLEU score of CONV-KN42M was better than Arsoy42M (pruned) by around 4.2.4. Comparison for Converting Efficiency. In this subsection, we showed the converting efficiency of the Arsoy method and NNGC. 1M trigrams were used as the input for the CSLM42M for both methods. In converting, the Arsoy method will generate many more N -grams and then a large part of these N -grams should be pruned. 20 The converting time is shown in Table V.

Table V shows that Arosy X  X  converting method took much more time (nearly 40 times) than ours. The Arsoy method generated all the possible N -grams ending with the words in the short-list (usually thousands of times of the original one) and then pruned them into smaller ones. Although the probabilities of all the tail words in the short-list can be calculated at the same time using neurons in the output layer in the CSLM, the Arsoy method still took much more time than ours. For our NNGC method, the same N -grams with the N -grams in the BNLM were converted. 4.2.5. Comparison for Decoding Efficiency. Vaswani et al. [2013] applied several tech-niques to speeding up NPLM in SMT; meanwhile, the Arsoy method and NNGC method stored the probabilities of the CSLM into the BNLM format. In this subsection, we com-pare the decoding efficiency of BNLM, Arsoy, CONV, CSLM, and NPLM. The numbers of hidden layers were set the same as the CSLM and other settings followed the default setting of the NPLM toolkit [Vaswani et al. 2013], with all the same settings as in SMT decoding. The 2,000 sentences of the NTCIR-9 test data were used as the evaluation data. The decoding time and SMT performance are shown in Table VI.
From the results of Table VI, we reach the following conclusions: (1) The decoding times using CONV-KN42M and BNLM (KN42M) were nearly all the (2) The decoding time using CSLM was much slower than BNLM, and the decoding (3) In comparison with the LMs with similar decoding time, the BLEU of CONV was (4) In comparison with NNLMs directly used in decoding, which were much slower In this subsection, we compare the performance of pruned LMs using different methods. The experiments on LM pruning were divided into four groups: the original, cutoff pruning, entropy-based, and BLMP methods. The experiment results are listed in Tables VII and VIII.

For the cutoff LM pruning, the default setting 1-1-2-2-2 in SRILM was applied, which means we did not cut off unigrams and bigrams, but cut off N -grams whose frequency was less than two for the higher-order N -grams. They are represented by KN/GT5B-Cutoff in Tables VII and VIII. KN/GT5B-Connect-* (where * stands for any ID) were LMs tuned into different sizes using the BLMP method. The sizes of the entropy-based pruned LMs, KN/GT5B-Entropy-*, were tuned to be similar to those of BLMP KN/GT5B-Connect-* pruned LMs by setting thresholds appropriately.
 The corresponding (in similar size) entropy-based pruned LMs and BLMP pruned LMs, such as KN5B-Connect-1 and KN5B-Entropy-1, were interpolated with the pa-rameter 0.5 into KN5B-Inter-1 using SRILM. The IDs after the pruned LMs (1/2/3/4) indicate the different sizes (in decreasing order) of LMs for the same pruning method.
We also performed the paired bootstrap resampling test. Two thousand samples were used for each significance test. The marks at the upper right of the BLEU score indicated whether the LMs were significantly better/worse than the entropy-based pruned LMs with the same IDs ( X  ++ /  X  X  X   X : significantly better/worse at  X  = 0 . 01;  X  + /  X   X  :  X  = 0 . 05; no mark: not significantly better/worse at  X  = 0 . 05).

From Tables VII and VIII, we made the following observations: (1) KN5B-Connect-1 and GT5B-Connect-1 were the pruned LMs with all the connect-(2) For the BLMP method, as LMs became smaller, the PPL increased and BLEU de-(3) For the entropy-based pruning method, as LMs became smaller, PPL did not always (4) For KN smoothing, the entropy-based pruned LMs obtained better PPL on the LMs (5) For GT smoothing, the entropy-based method also obtained better PPL on the LMs (6) Some interpolated LMs obtained comparable BLEU, with comparable 1/4 of the (7) By comparing N -grams in the entropy-based pruned LMs and BLMP pruned LMs, 4.3.1. Discussions. The preceding results showed that BLMP performed well in SMT. In this subsection, which kind of N -grams were useful during decoding are investigated. We calculated the log-probabilities of every reference sentence of test data to simulate the performance of 5-gram LMs in SMT decoding, and counted the ratio of different order N -grams used for each pruned LM. Then, the Average Length of the N -grams Hit (ALNH) in SMT decoding for different LMs was calculated as follows: where P i-gram means the ratio of the i -grams hit in SMT decoding. The results were illustrated in Figure 3.

As shown in Figure 3, ALNH in SMT decoding of BLMP pruned LMs was longer than that of the entropy-based pruned LMs for KN smoothing, and ALNH of BLMP pruned LMs and the entropy-based pruned LMs were similar for GT smoothing. ALNH of interpolated LMs was longer than both of the two methods for KN and GT smoothing.
As we know, LM refers to the probabilities of ( i  X  1)-grams together with the adjusted weights using back-off, if no corresponding i -grams are hit. The preceding statistics indicated that more high-order N -grams were hit for BLMP pruned LMs in SMT compared with the entropy-based pruned LMs. In other words, less back-off was applied with the BLMP pruned LMs in SMT decoding. 22 There was also a positive correlation between ALNH in Figure 3 and BLEU in Tables VII and VIII. Most of the LMs with larger ALNH also obtained higher BLEU in SMT. This indicated that useful N -grams in SMT were not only the N -grams used in the decoding, but also those useful N -grams that were long . Although our current method only focuses on probability, the results suggested that we should also focus on lengths of the N -grams for pruning in the future work. As mentioned in Section 1, a large additional corpus may let the LMs better. In this subsection, experiments on converting large LMs were conducted on both NTCIR and NIST datasets. 4.4.1. Effects of Corpus Size. In Section 4.2, the results showed that the proposed NNGC CONV outperformed the original BNLM and Arsoy method on a small corpus contain-ing 42M words. In this subsection, we show the results of the NNGC method on a large corpus containing around 1B words.
 Both the Corpus42M and the Corpus746M were used for this experiment. The KN42M and CONV-KN42M were the same as in Table III. We obtained the CONV-KN746M by rewriting the KN746M (cutoff) with CSLM42M in the same way as CONV-KN42M.

Table IX showed BLEU scores on the test data. The figures in the  X 1st Pass X  column of Table IX show the BLEU scores in the first pass decoding when we changed LMs. The figures in the  X  X eranking X  column show BLEU scores when we applied CSLM42M to rerank the 100-best lists for different LMs. 23 When we applied CSLM42M for rerank-ing, the CSLM42M score was added as the additional 15th feature. The weight param-eters were tuned by using Z-MERT [Zaidan 2009].

The statistical significance test is listed in Table X.
From the results of Tables IX and X, we can observe: (1) Reranking by applying the CSLM42 increased the BLEU scores for all the first-(2) The CONV42M was better than the KN42M for both first pass and reranking. (3) The first pass of CONV42M and CONV746M (33.07 and 33.59) were comparable (4) The last column in Table IX indicates the tuned SMT feature weights for different
The preceding results indicate that NNGC can scale up well on large corpora. 4.4.2. Conversion of Pruned LMs. Although NNGC using natural N -grams worked well on large corpora, it is time consuming to convert a BNLM trained from an even larger corpus (such as one containing around 5B words). Since the BLMP method can prune the useless N -grams from large LMs, without sacrificing their SMT performance sig-nificantly, instead of converting the huge BNLM, we can first make the pruning before converting it.

The smallest pruned LMs (KN/GT5B-Entropy-4 and KN/GT5B-Connect-4) that were nearly 1/20 the size of the original LM (KN/GT5B), and KN/GT5B-Connect-3 that were nearly 1/10 the size of the original LM (KN/GT5B) in Tables VII and VIII, were selected for conducted converting experiments. We took them as the input BNLM instead of the original one. The same setting for natural N -grams converting experiments were set here with the pruned LMs as the input. We also conducted significance tests to show whether the BLEU of CONV was significantly better than the corresponding BNLM, and the results are shown in Table XI.

From the results of Table XI, we can observe the following: (1) The converted pruned CONVs outperformed the pruned BNLM significantly. This (2) As the size of CONV increased, both PPL and BLEU were further improved. These (3) The BLEU of CONV-GT* were significantly better than the corresponding GT*.
In addition, the proposed BLMP could also be applied to the Arsoy method. That is, all the words in the short-list were added to the tails of i -grams in the corpus in order to produce ( i + 1)-grams called artificial N-grams . BLMP was applied to these artificial N -grams to select connecting phrases. These connecting phrases ( N -grams) were used as input of CSLM and their probabilities were obtained as output. These generated N -grams were interpolated with the original BNLM to form the larger converted LM consequently.
 Table XII showed that we used BLMP to prune both Arsoy X  X  converted LMs and NNGC converted LMs. (1) Arsoy X  X  converting and the proposed BLMP can work well together (Arsoy-Connect-(2) The NNGC with BLMP (CONV-KN5B-Connect-3/4) outperformed Arsoy X  X  convert-4.4.3. Experiments on NIST Dataset. We also evaluated the performance of our proposed methods on the NIST Chinese-to-English dataset. OpenMT06 in the NIST open ma-chine translation 2006 Evaluation 24 was used. The parallel corpus followed the setting of Wang et al. [2014], and it mainly consists of news and blog texts. The training set consists of 430K sentences and 12M words. The English corpus from the United Na-tions dataset 25 was used as the large monolingual corpus, 26 which included around 7M out-of-domain sentences and 206M words as the parallel corpus. The datasets of NIST Eval 2002 to 2005 was used as development data for MERT tuning. The NIST Eval 2006 was used as evaluation data. The same settings of Section 4.1 for SMT were followed.
 We compared the performances of small/large BNLM (NIST-KN12/206M), the Arsoy method (NIST-Arsoy12M), the proposed NNGC method (CONV-NIST-KN12M), the BLMP method (NIST-KN206M-connect-1/2), and their converted LMs (CONV-NIST-KN206M-connect-1/2).

From the results of Table XIII, we make the following observations: (1) Because the large extra monolingual corpus and parallel corpus were in different (2) The proposed NNGC method worked well and performed slightly better than the In this article, we have proposed a NNGC CSLM converting method for better proba-bility estimation, and a BLMP pruning approach for enhancing LMs for SMT decoding and improving the efficiency of CSLM converting. Our methods have the following two attractive features: (1) the NNGC and BLMP can select the N -grams with linguis-tic sense, which are potentially to be used in decoding, and (2) the proposed pruning and converting methods have lower computational complexity in comparison with the existing methods.

A series of experiments on various datasets have been conducted to evaluate the per-formance and efficiency of our proposed methods. The experimental results show that both the converting and pruning methods outperform the existing approaches for SMT. We have also shown that the NNGC converting method and the BLMP pruning method can work well together to convert a huge LM trained from 5B words efficiently. The experiments on NIST data also suggest that BLMP can be applied to LM adaptation.
In this article, the proposed NNGC converting method can be regarded as an opti-mization method of the probabilities of the items in an N -gram LM that requires a small computational cost, using an NNLM that has a high generalization ability and requires a large computational cost. We would also like to use the distributed semantic representation in this optimization method as our future work.

