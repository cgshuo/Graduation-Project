 In this paper, we demonstrate how combining coarse-grained and fine-grained supervision bene-fits sentence-level sentiment analysis  X  an important task in the field of opinion classification and retrieval (Pang and Lee, 2008). Typical supervised learning ap-proaches to sentence-level sentiment analysis rely on sentence-level supervision. While such fine-grained supervision rarely exist naturally, and thus requires labor intensive manual annotation effort (Wiebe et al., 2005), coarse-grained supervision is naturally abundant in the form of online review ratings. This coarse-grained supervision is, of course, less infor-mative compared to fine-grained supervision, how-ever, by combining a small amount of sentence-level supervision with a large amount of document-level supervision, we are able to substantially improve on the sentence-level classification task. Our work com-bines two strands of research: models for sentiment analysis that take document structure into account; and models that use latent variables to learn unob-served phenomena from that which can be observed.
Exploiting document structure for sentiment anal-ysis has attracted research attention since the early work of Pang and Lee (2004), who performed min-imal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarse-grained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated gen-erative content structure models with discriminative models for multi-aspect sentiment summarization and ranking. These approaches all rely on the avail-ability of fine-grained annotations, but T  X  ackstr  X  om and McDonald (2011) showed that latent variables can be used to learn fine-grained sentiment using only coarse-grained supervision. While this model was shown to beat a set of natural baselines with quite a wide margin, it has its shortcomings. Most notably, due to the loose constraints provided by the coarse supervision, it tends to only predict the two dominant fine-grained sentiment categories well for each docu-ment sentiment category, so that almost all sentences in positive documents are deemed positive or neutral, and vice versa for negative documents. As a way of overcoming these shortcomings, we propose to fuse a coarsely supervised model with a fully supervised model.

Below, we describe two ways of achieving such a combined model in the framework of structured conditional latent variable models. Contrary to (gen-erative) topic models (Mei et al., 2007; Titov and McDonald, 2008; Lin and He, 2009), structured con-ditional models can handle rich and overlapping fea-tures and allow for exact inference and simple gradi-ent based estimation. The former models are largely orthogonal to the one we propose in this work and combining their merits might be fruitful. As shown by Sauper et al. (2010), it is possible to fuse gener-ative document structure models and task specific structured conditional models. While we do model document structure in terms of sentiment transitions, we do not model topical structure. An interesting avenue for future work would be to extend the model of Sauper et al. (2010) to take coarse-grained task-specific supervision into account, while modeling fine-grained task-specific aspects with latent vari-ables.

Note also that the proposed approach is orthogonal to semi-supervised and unsupervised induction of context independent (prior polarity) lexicons (Turney, 2002; Kim and Hovy, 2004; Esuli and Sebastiani, 2009; Rao and Ravichandran, 2009; Velikovich et al., 2010). The output of such models could readily be incorporated as features in the proposed model. 1.1 Preliminaries Let d be a document consisting of n sentences, s = ( s ) n i =1 , with a document X  X entence-sequence pair de-noted d = ( d, s ) . Let y d = ( y d , y s ) denote random variables 1  X  the document level sentiment, y d , and the sequence of sentence level sentiment, y s = ( y s i ) n i =1 In what follows, we assume that we have access to two training sets: a small set of fully labeled in-coarsely labeled instances D C = { ( d j ,y d j ) } m f + m Furthermore, we assume that y d and all y s i take val-ues in { POS , NEG , NEU } .

We focus on structured conditional models in the exponential family, with the standard parametrization p  X  ( y where  X   X &lt; n is a parameter vector,  X  (  X  )  X &lt; n is a vector valued feature function that factors according to the graph structure outlined in Figure 1, and A  X  is the log -partition function. This class of models is known as conditional random fields ( CRF s) (Lafferty et al., 2001), when all variables are observed, and as hidden conditional random fields ( HCRF s) (Quattoni et al., 2007), when only a subset of the variables are observed. 1.2 The fully supervised fine-to-coarse model McDonald et al. (2007) introduced a fully super-vised model in which predictions of coarse-grained (document) and fine-grained (sentence) sentiment are learned and inferred jointly. They showed that learn-ing both levels jointly improved performance at both levels, compared to learning each level individually, as well as to using a cascaded model in which the predictions at one level are used as input to the other.
Figure 1a outlines the factor graph of the corre-sponding conditional random field. 2 The parameters,  X 
F , of this model can be estimated from the set of fully labeled data, D F , by maximizing the joint con-ditional likelihood function where  X  2 F is the variance of the Normal (0 , X  2 F ) prior. Note that L F is a concave function and consequently its unique maximum can be found by gradient based optimization techniques. 1.3 Latent variables for coarse supervision Recently, T  X  ackstr  X  om and McDonald (2011) showed that fine-grained sentiment can be learned from coarse-grained supervision alone. Specifically, they used a HCRF model with the same structure as that in Figure 1a, but with sentence labels treated as la-tent variables. The factor graph corresponding to this model is outlined in Figure 1b.

The fully supervised model might benefit from fac-tors that directly connect the document variable, y d , with the inputs s . However, as argued by T  X  ackstr  X  om and McDonald (2011), when only document-level supervision is available, the document variable, y d , should be independent of the input, s , conditioned on the latent variables, y s . This prohibits the model from bypassing the latent variables, which is crucial, since we seek to improve the sentence-level predic-tions, rather than the document-level predictions.
The parameters,  X  C , of this model can be esti-mated from the set of coarsely labeled data, D C , by maximizing the marginalized conditional likelihood function L
C (  X  C ) = where the marginalization is over all possible se-quences of latent sentence label assignments y s .
Due to the introduction of latent variables, the marginal likelihood function is non-concave and thus there are no guarantees of global optimality, how-ever, we can still use a gradient based optimization technique to find a local maximum. The fully supervised and the partially supervised models both have their merits. The former requires an expensive and laborious process of manual an-notation, while the latter can be used with readily available document labels, such as review star rat-ings. The latter, however, has its shortcomings in that the coarse-grained sentiment signal is less infor-mative compared to a fine-grained signal. Thus, in order to get the best of both worlds, we would like to combine the merits of both of these models. 2.1 A cascaded model A straightforward way of fusing the two models is by means of a cascaded model in which the predic-tions of the partially supervised model, trained by maximizing L C (  X  C ) are used to derive additional features for the fully supervised model, trained by maximizing L F (  X  F ) .

Although more complex representations are pos-sible, we generate meta-features for each sentence based solely on operations on the estimated distribu-tions, p  X  lowing probability distributions as discrete features by uniform bucketing, with bucket width 0 . 1 : the joint distribution, p  X  ment distribution, p  X  tence distribution, p  X  argmax of these distributions, as well as the pair-wise combinations of the derived features.

The upshot of this cascaded approach is that it is very simple to implement and efficient to train. The downside is that only the partially supervised model influences the fully supervised model; there is no reciprocal influence between the models. Given the non-concavity of L C (  X  C ) , such influence could be beneficial. 2.2 Interpolating likelihood functions A more flexible way of fusing the two models is to interpolate their likelihood functions, thereby allow-ing for both coarse and joint supervision of the same model. Such a combination can be achieved by con-straining the parameters so that  X  I =  X  F =  X  C and taking the mean of the likelihood functions L F and L
C , appropriately weighted by a hyper-parameter  X  . The result is the interpolated likelihood function A simple, yet efficient, way of optimizing this ob-jective function is to use stochastic gradient ascent with learning rate  X  . At each step we select a fully labeled instance, ( d j , y d j )  X  X  F , with probability  X  and a coarsely labeled instance, ( d j ,y d j )  X  X  C , with probability (1  X   X  ) . We then update the parameters,  X  , according to the gradients  X  X  F and  X  X  C , respec-tively. In principle we could use different learning rates  X  F and  X  C as well as different prior variances  X 
F and  X 
Since we are interpolating conditional models, we need at least partial observations of each instance. Methods for blending discriminative and generative models (Lasserre et al., 2006; Suzuki et al., 2007; Agarwal and Daum  X  e, 2009; Sauper et al., 2010), would enable incorporation of completely unlabeled data as well. It is straightforward to extend the pro-posed model along these lines, however, in practice coarsely labeled sentiment data is so abundant on the web (e.g., rated consumer reviews) that incorpo-rating completely unlabeled data seems superfluous. Furthermore, using conditional models with shared parameters throughout allows for rich overlapping features, while maintaining simple and efficient in-ference and estimation. For the following experiments, we used the same data set and a comparable experimental setup to that of T  X  ackstr  X  om and McDonald (2011). 3 We compare the two proposed hybrid models (Cascaded and Interpo-lated) to the fully supervised model of McDonald et al. (2007) (FineToCoarse) as well as to the soft vari-ant of the coarsely supervised model of T  X  ackstr  X  om and McDonald (2011) (Coarse).

The learning rate was fixed to  X  = 0 . 001 , while we tuned the prior variances,  X  2 , and the number of epochs for each model. When sampling according to  X  during optimization of L I (  X  I ) , we cycle through D
F and D C deterministically, but shuffle these sets between epochs. Due to time constraints, we fixed the interpolation factor to  X  = 0 . 1 , but tuning this could potentially improve the results of the interpolated model. For the same reason we allowed a maximum of 30 epochs, for all models, while T  X  ackstr  X  om and McDonald (2011) report a maximum of 75 epochs.
To assess the impact of fully labeled versus coarsely labeled data, we took stratified samples with-out replacement, of sizes 60, 120, and 240 reviews, from the fully labeled folds and of sizes 15,000 and 143,580 reviews from the coarsely labeled data. On average each review consists of ten sentences. We performed 5-fold stratified cross-validation over the labeled data, while using stratified samples for the coarsely labeled data. Statistical significance was as-sessed by a hierachical bootstrap of 95% confidence intervals, using the technique described by Davison and Hinkley (1997). 3.1 Results and analysis Table 1 lists sentence-level accuracy along with 95% confidence interval for all tested models. We first note that the interpolated model dominates all other models in terms of accuracy. While the cascaded model requires both large amounts of fully labeled and coarsely labeled data, the interpolated model is able to take advantage of both types of data on its own and jointly. Still, by comparing the fully supervised and the coarsely supervised models, the superior impact of fully labeled over coarsely labeled data is evident. As can be seen in Figure 2, when all data is used, the cascaded model outperforms the interpolated model for some recall values, and vice versa, while both models dominate the supervised approach for the full range of recall values.
As discussed earlier, and confirmed by Table 2, the coarse-grained model only performs well on the predominant sentence-level categories for each docu-ment category. The supervised model handles nega-tive and neutral sentences well, but performs poorly on positive sentences even in positive documents. The interpolated model, while still better at capturing the predominant category, does a better job overall.
These results are with a maximum of 30 training iterations. Preliminary experiments with a maximum of 75 iterations indicate that all models gain from more iterations; this seems to be especially true for the supervised model and for the cascaded model with less amount of course-grained data. Learning fine-grained classification tasks in a fully su-pervised manner does not scale well due to the lack of naturally occurring supervision. We instead proposed to combine coarse-grained supervision, which is natu-rally abundant but less informative, with fine-grained supervision, which is scarce but more informative. To this end, we introduced two simple, yet effective, methods of combining fully labeled and coarsely labeled data for sentence-level sentiment analysis. First, a cascaded approach where a coarsely super-vised model is used to generate features for a fully supervised model. Second, an interpolated model that directly optimizes a combination of joint and marginal likelihood functions. Both proposed mod-els are structured conditional models that allow for rich overlapping features, while maintaining highly efficient exact inference and robust estimation prop-erties. Empirically, the interpolated model is superior to the other investigated models, but with sufficient amounts of coarsely labeled and fully labeled data, the cascaded approach is competitive.
 The first author acknowledges the support of the Swedish National Graduate School of Language Technology (GSLT). The authors would also like to thank Fernando Pereira and Bob Carpenter for early discussions on using HCRFs in sentiment analysis.
