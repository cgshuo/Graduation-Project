 In this paper, we present our work of humor recognition on Twitter, which will facilitate affect and sentimental analysis in the social network. The central question of what makes a tweet (Twitter post) humorous drives us to design humor-related features, which are derived from influential humor theories, linguistic norms, and affectiv e dimensions. Using machine learning techniques, we are able to recognize humorous tweets with high accuracy and F-measure. More importantly, we single out features that contribute to distinguishing non-humorous tweets from humorous tweets, and humor ous tweets from other short humorous texts (non-tweets). This proves that humorous tweets possess discernible characteristics that are neither found in plain tweets nor in humorous non-tweets. We believe our novel findings will inform and inspire the burgeoning field of computational humor research in the social media. I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Text analysis. Algorithms, Experimentation. computational humor; Twitter; humor recognition; machine learning Humor pervades our life, both phy sical and virtual, as a highly intelligent communicative activity that defines us as human beings. Humor can take both nonverbal (e.g., visual) or verbal forms. Verbal humor, or verba lly expressed humor (Ritchie, 2004), has been intensively studied from a linguistic perspective in the past decades (Raskin, 2012). Important theories such as the Semantic Theory of Verbal Humor (Raskin, 1985) have shaped a new offshoot of humor study  X  computational humor. A still young area of study, computational humor is concerned with the automatic generation, recogniti on, and understanding of humor (Binsted et al., 2006). In this paper, we are interested in verbal humor and its automatic recognition in the Twittersphere. Automatic humor recognition has many applications, such as computational advertising, a nd education. Nevertheless, automatically recognizing a piece of text to be humorous is a challenge even for humans. In th e past, humor recognition usually leverages fixed patterns of partic ular humor species (Taylor and Mazlack, 2004) or a selected set of linguistic features in a machine learning scheme (Mihalcea and Strapparava, 2006). There are two inadequacies in the previous work. 1) No systematic study of humor-inducing characteristics is 2) Most researches use static and stereotyped humor instances. To overcome such inadequacies , we focus on humor recognition humorous or not. We believe our work will facilitate sentiment analysis and user behavior anal ysis on Twitter. Technically, our contributions are listed in the following.  X  We systematically derive humor-related aspects and  X  We identify humor-inducing characteristics on Twitter that  X  We show how to harvest humor data from Twitter with the In the remainder of the paper, we first introduce in Section 2 previous researches in verbal humor and computational humor method of collecting the data. The principled feature set design is detailed in Section 4. Then we re port the experimental results and important findings in Section 5. Section 6 concludes the paper and points to future venues. Traditionally, the humor study is rooted in sociological, Superiority theories (Gruner, 1997), Release/Relief theories (Rutter, 1997), and Incongruity theories (Suls, 1972). The recent decades witness the bur geoning of linguistics theories of humor by Victor Raskin and his followers  X  the Semantic Script Theory of Humor (SSTH; Raskin, 1985), the General Theory of Verbal Humor (GTVH; Attardo and Raskin, 1991), and the Ontological Semantic Theory of Humor (OSTH; Raskin et al., 2009). (Taylor and Raskin, 2012), so computational humor develops with the inflow of artificial intelligen ce and the linguistic approaches to verbal humor, especially the SSTH-GTVH-OSTH theories (Raskin, 2012). As a deep unders tanding of the humor mechanism is very difficult, the bulk of co mputational humor research is devoted to humor generation and humor recognition. Many reported humor generators produce special kinds of verbal humor using templates and langua ge resources. For example, Stock and Strapparava (2006) ge nerate humorous acronyms with WordNet Domains and other lexical resources.  X zbal and Strapparava (2012) create humor ous neologism using WordNet and ConceptNet. Some generation work draws on humor theories to account for humor-related aspect s, such as SSTH (Labutov and Lipson, 2012) and structural am biguity (Oaks, 2012). With the rise of big data technology, massi ve datasets (Valitutti et al., 2013) and unsupervised methods (Petrovi c and Matthews, 2013) are also successfully used. Humor recognition is arguably mo re challenging and sometimes the fun cannot even be detected by a human reader. Some representative works focus on special kinds of humor with clearly identifiable characteristics, including Knock Knock jokes (Taylor and Mazlack, 2004), double entendre jokes (Kiddon and Brun, 2011), and one-liners (Mihalcea and Strapparava, 2006). The mainstream humor recognition method utilizes humor-related features to train a classifica tion model that distinguishes humorous text from non-humorous text (Mihalcea and Strapparava, 2006; Mihalcea and Pulman, 2007; Reyes, 2012). An exception is Friedland and Allan X  X  (2008) information retrieval-based joke recognition model. In most of those works, the humor-related features are not syst ematically derived from humor theories. The majority of humor recognition studies use static or stock instances of humor found in literary compilations or online repositories. Although humor is fre quently used on the social media platform of Twitter and has practical values (Holton and Lewis, 2011), there is little work on recognizing humor on Twitter. Raz X  X  (2012) work outlines Twitter humor classification, i.e., recognizing the types of known humor on Twitter. We assume that humorous tweet s should be distinguished from both non-humorous tweets and humorous non-tweets. So we prepared three datasets accordingly. The first dataset consists of 1000 humorous tweets that are collected semi-automatically. Following the suggestion of Raz (2012), we target humorous lists ( X  X omedy X ) and comedian accounts (such as  X  X unTweets. com X  and  X  X unny Tweets X ). Moreover, we look at tweets with the humor hashtag ( X #humor X ), which are supposed to be user-annotated humorous tweets. One may be tempted to think that those collected tweets are already humorous and can be directly used in the dataset. In fact, the source labels and user annotations are only approximate. Information on Twitter, a popular social media service, is notorious for misuse and noise. We have found many non-humorous tweets posted by the comedian accounts and many plain tweets with #humor, such as Example (1). (1) People with good sense of #humor are the most #emotional In many other cases, a #humor tweet is only a link to an amusing picture or video, i.e., nonverbal humor, such as Example (2). (2) Egyptian wig http://t.co/fQiQ4flv6e #humor To obtain the truly humorous ve rbal humor instances, manual verification is needed. Therefore, we first automatically build a dataset of 2000  X  X ossibly humorous X  tweets using the aforementioned pointers with Twitter X  X  REST API. Then a native English speaker is employed to r ead all the tweets and identify the truly humorous ones. It takes him two weeks to finish the job and 1267 distinct humorous tweets as well as 589 distinct non-humorous tweets 1 are verified. We then randomly select 1000 of the 1267 humorous tweets to make the humorous tweet dataset. The following is an example. The second dataset of 1000 non-hum orous tweets seems easier to build as most tweets are presumably unamusing. But we require the dataset to be pure and accurate  X  not containing any unlabeled humorous tweet. For this purpose, manual verification is also needed. We first use Twitter X  X  streaming API to collect 1500 tweets, and then employ the same reader to filter out any humorous tweets. Fortunately, all those tweets are found non-humorous. Next we mix them with the 733 humo r-labeled non-humorous tweets before randomly selecting 1000 of them to make the non-humorous tweet dataset. We assume that the existence of some suspiciously humorous tweets in this dataset will make the recognition task more challenging and truly discriminative humor features will stand out. The following is an example of non-humorous tweet. We also build the third dataset of 1000 humorous non-tweets, which are usually stock and sta tic humorous texts. For this purpose, we make use of the verbal humor resource on textfiles.com . They don X  X  add up to 2000 because duplicated or nearly duplicated tweets are filtered. The humorous texts are electronically archived and thus can be automatically collected. But there are two constraints. First, we want to make sure that the hum orous non-tweets are as long as the humorous tweets. Since a tweet is limited to be no more than 140 characters long, we select humor ous texts of approximately the same length. Second, we need to filter out duplicated or nearly duplicated ones because many archived humorous texts are highly stereotyped (e.g., bulb changing joke) and parody is frequently encountered. Including highly duplicated humor texts in the dataset reduces the credibility of the experimental result. Therefore we use simple text comparison techniques to filter out the duplicates and near duplicates. Two texts are decided to be duplicate if their Jaccard similarity &gt; 0.65. What results is a dataset of 1000 distinct short joke s. One example is the following. (5) Did you here about the Ethiopian who fell into the alligator In this work, we are committed to designing humor-related features based on humor theories and linguistic norms, and then empirically determine the most discriminative ones. As Obrst (2012) observes, the lingui stic theories of verbal humor are best represented by the incongruity theories, SSTH, and GTVH, which affect all the levels posited by linguistics. The incongruity theories (Shultz, 1976; Suls, 1972) claim that an initial understanding of the text is incongruous with the ending (punchline) so that a different understanding is attempted to resolve the incongruity. SSTH (Raskin, 1985) elaborates the incongruity-resolution mechanism into script overlapping and script opposition as the necessary and sufficient conditions for a text to be funny. GTVH (Attardo and Raskin, 1991) is a generalization of SSTH, which includes 6 knowledge sources: script opposition, logical mechanism, situation, target, narrative strategy, and language. Obviously incongruity or script opposition is the basic tenet of the humor theories. Linguistically, th ey are realized on different levels: phonetics, morphology, syntax , semantics, and pragmatics, as is found in incongruous sounds, words, structures, sentences, and discourses. In the light of the theories and the short textual nature of Twitter humor (so that a sentence-based sy ntactic analysis is unnecessary), we design 4 categories of humor-related features.  X  Phonetic features  X  Morpho-syntactic features  X  Lexico-semantic features  X  Pragmatic features In addition, due to humor X  X  phy siological function (Rutter, 1997), corroborated by previous research on polarity and emotional status (Mihalcea and Pulman, 2007; Reyes et al., 2012). Accordingly, we also include the fifth category:  X  Affective features Many humorous texts play w ith sounds, creating incongruous sounds or words, as in the following example. (6) Some people come here to sit and think I come here to shit We design 4 phonetic features based on two well understood pronunciation dictionary 2 is used to convert word strings to phone strings. Two or more words beginning w ith the same phones make an alliteration chain. Multiple alliteration chains are possible in a text. Since both the number of chains and the chain length are indicative of the use of alliteration, we extract 2 features: F1-1: Number of alliteration chains A chain consists of at least two words and all the words in a chain should begin with the same phone. F1-2: Maximum length of alliteration chains It is the length of the longest chain in the text. We normalize it to the range of (0, 1) by the total amount of phones. Words that end with the same syllable are in a rhyme relationship. Like for alliteration, we extract 2 similar features for rhyme. F1-3: Number of rhyme chains F1-4: Maximum length of rhyme chains The only difference is that we discover rhyme words by the last syllable, not phone. We approxima te syllable identification by looking for vowels, which are marked as 0, 1, and 2 in the CMU dictionary. Incongruous structures underlie some humorous texts, which are usually realized as morpho-syntactic patterns, as is shown in the following example. (7) I came, I saw. I cleared the browser history. Since our target texts are very short, which usually consist of one or two sentences, sentence-level sy ntactic parsing is not suitable. Therefore we resort to word-level syntax, i.e., POS sequence. We design 8 morpho-syntactic features of two subcategories: ratios of major POS X  X  and POS pattern chains. Because of Twitter X  X  non-standard and noisy nature, a POS tagger trained on conventional text is not suitable for Twitter POS tagging. We use a state-of-the-a rt Twitter POS tagger (Owoputi et al., 2013) to POS-tag the tweets. To the non-tweet texts we apply a state-of-the-art standard PO S tagger (Toutanova and Manning, 2000). After POS tagging, we calculate the ratios of nouns, pronouns, verbs, and modifiers. Note that the Twitter POS tagger We use the NLTK package, http://www.nltk.org/data.html, which includes the data. uses a Twitter-specific tag set, wh ich needs to match the standard tag set. The details are given below. F2-1: Ratio of common nouns In both the Twitter-specific tag set and the standard tag set, common nouns are tagged  X  X  X . F2-2: Ratio of proper nouns Proper nouns are tagged  X  X  X  or  X  X  X  in the Twitter-specific tag set, and  X  X N X  in the standard tag set. F2-3: Ratio of pronouns Pronouns are tagged  X  X  X  or  X  X  X  in the Twitter-specific tag set, and  X  X R X  in the standard tag set. F2-4: Ratio of verbs Verbs are tagged  X  X  X  in both tag sets. F2-5: Ratio of modifiers Modifiers include adjectives and adverbs. Adjective are tagged Adverbs are tagged  X  X  X  in the Twitter-specific tag set but  X  X B X  in the standard tag set. Because individual POS statistics are not enough to capture the structural patterns, we also l ook at POS patterns like  X  X oun-verb-adjective X . The number and length of such patterns point to some structural characteristics. To get all the POS patterns of all lengths, we treat POS tags as characters and apply a simple string search algorithm. There are 3 features in this subcategory. F2-6: Number of POS pattern chains A chain consists of at least two instances of a POS pattern. F2-7: Maximum length of POS pattern chains Like F1-2, the length is normalized by dividing the total number of POS tags. F2-8: Maximum length of POS patterns Different from F2-7, pattern length is the number of POS tags in a pattern. For example, the length of  X  X -V-A X  is 3 and that of  X  X -N-V-A X  is 4. Semantic ambiguity often leads to incongruous sentences, which are often found in some humorous texts such as puns and punning riddles. For example, two interp retations of the word  X  X ight X  create humor in Example (8). (8) when nothing goes right... go left For short texts, we focus on wo rd-level semantics and design 8 lexcico-semantic features of 3 subcategories: Twitter-specific symbols, humor themes , and lexical chains. Some tweets are semantically loaded with special symbols, including # (hashtag prefix), @ (a mention of some Twitter account), urls (quote of a source), and emoticons (mood indicator). The extraction of # a nd @ is straightforward, and the Twitter POS tagger is used to identify urls ( X  X  X ) and emoticons ( X  X  X ). We use 4 binary features indicating the occurrence of those symbols. F3-1: Occurrence of # F3-2: Occurrence of @ F3-3: Occurrence of urls F3-4: Occurrence of emoticons One of GTVH X  X  Knowledge Res ources is target, i.e., the particular person or thing to be laughed at (Davies, 1990). Adult or sexual jokes are also a major species of humor. We focus on two major humor themes: sexuality and politics. To identify words related to the themes, we use the domain knowledge from WordNet Domains (Bentivogli et al., 2004). There are 2 features about the ratios of those humor themes. F3-5: Ratio of words from the sexuality domain F3-6: Ratio of words from the politics domain Humor that plays with ambiguous meaning to create incongruity often contains semantically rela ted words, or lexical chains. A lexical chain (Morris and Hirst, 1991) consists of words that each stands in one relationship (s ynonym, antonym, hypernym, hyponym, holonym, meronym) with at least one other word in the chain. We use 2 lexical chain-related features. F3-7: Number of lexical chains A chain consists of at least two words. F3-8: Maximum length of lexical chains The length is normalized by dividing the total number of words. A humorous text communicates f un to its intended reader. On Twitter, the communicative nature and discourse characteristics are more pronounced. Some pragmatic aspects are thus useful for recognizing humor. For example, the rhetorical question  X  X hy can X  X  I X  in Example (9) triggers the fun. (9) If teachers have substitutes why can't I? So we design a total of 10 pragmatic features of 3 subcategories: deictics, discourse markers, and speech act cues. important for recognizing humor with reference to a contextualized person, place, or time. So we use 3 binary features to indicate the occurrence of those deictics. F4-1: Occurrence of person deictics Person deictics are essentially English pronouns, such as they , your , himself , which make up a closed group. F4-2: Occurrence of place deictics Place deictics include this , that , here , and there . F4-3: Occurrence of time deictics and later . Discourse markers are words or phrases that serve the discourse whereas ), etc. Collections of typi cal English discourse markers are available online 3 . We count up to 3-word discourse markers and come up with 3 binary features. F4-4: Occurrence of 1-gram discourse markers F4-5: Occurrence of 2-gram discourse markers F4-6: Occurrence of 3-gram discourse markers Another important pragmatic asp ect is speech act (Austin, 1962), which indicates whether Twitter users are sharing information, asking questions, making suggestions , expressing sentiments, etc. Zhang et al. (2013) shows that speech acts can be automatically recognized with the help of speech act cues and other textual features. To simplify, in this work we use their speech act cues to extract features for 4 major speech act types: comment, question, statement, and suggestion according to (Zhang et al., 2013). F4-7: Occurrence of comment cues F4-8: Occurrence of question cues F4-9: Occurrence of statement cues F4-10: Occurrence of suggestion cues Aside from popular humor theori es and linguistic concepts, humor is essentially associated with sentiment and affect, i.e., the text is likely to be humorous if some sentimentally loaded words are found, like  X  X diot X  in the following example. (10) Yahoo is buying Tumblr for $1. 1 Billion. Idiot, we download To identify word-level sentiment and affect, we use the open APIs of SenticNet (Cambria and Hussain, 2012), which provide rich affective information trained on large corpus. This resource enables us to design 20 features of two subcategories: polarity and sentics. Polarity indicates whether a word is associated with positive or negative sentiment. Positive values indicate positive sentiments; negative values indicate negativ e sentiments. Large absolute values represent intensified pol arity. Accordingly, we use 4 features for a text X  X  polarity. F5-1: Total polarity score It is the sum of polarity scores of all the words. For example, http://english.edusites.co.uk/article/improving-writing-discourse-markers-a-teachers-guide-and-toolkit/ F5-2: Average polarity score It is equal to F5-1 divided by the number of all words. F5-3: Total absolute polarity score Different from F5-1, this feature makes sure that positive and negative polarity scores don X  X  cancel out. F5-4: Average absolute polarity score It is equal to F5-3 divided by the number of all words. Invented by Cambria and Hussain (2012), sentics are the basic units of sentiment, which include 4 major dimensions according to their Hourglass Model  X  sensitiv ity, attention, pleasantness, and type, we design 4 features analogous to the polarity features. The whole list is in the following. F5-5: Total sensitivity score F5-6: Average sensitivity score F5-7: Total absolute sensitivity score F5-8: Average absolute sensitivity score F5-9: Total attention score F5-10: Average attention score F5-11: Total absolute attention score F5-12: Average absolute attention score F5-13: Total pleasantness score F5-14: Average pleasantness score F5-15: Total absolute pleasantness score F5-16: Average absolute pleasantness score F5-17: Total aptitude score F5-18: Average aptitude score F5-19: Total absolute aptitude score F5-20: Average absolute aptitude score We now evaluate the features by observing how they are useful in recognizing humor on Twitter. After introducing the evaluation method and scheme, we report the result on the three datasets. Humor recognition is usually formul ated as a binary classification task (Mihalcea and Strapparava, 2006). That is, we try to classify a text as either humorous or not. For our problem, two classification tasks are needed : separating humorous tweets from non-humorous tweets, and humorous tweets from humorous non-tweets. To get impartial result, we remove the #humor hashtag from any tweet because its existen ce will affect feature design and feature analysis. The classification algorithm we choose is Gradient Boosted Regression Trees or GBRT (Friedman, 1999), which is a powerful boosting method based on decision tr ees as base learners. GBRT builds a model additively: where F m ( x ) is the mth-step model function, h m ( x ) is the mth-step base learner, and  X  m is the mth step length. At each step, h chosen to minimize a loss function L , which is the negative binomial log-likelihood loss function for our binary classification problem. GBRT approximates the optimization by taking the negative gradient of L as the steep descent: and We choose GBRT for three reasons . First, the decision tree base learners can naturally handle hete rogeneous features, which befit the nature of our task. Second, the classifier is highly accurate. In a preliminary experiment, it outperforms an SVM classifier and a Na X ve Bayes classifier, which are often used by previous work on humor recognition. Third, thanks to boosting and decision tree mechanism, feature importance can be calculated by averaging trees. In implementation, we adopt the sklearn toolkit 4 and use the default settings. We are mainly concerned with the humor-related feature. First, we compare the 5 categories of features as evaluated by how useful each of them is for Twitter humor recognition. Then we analyze the importance of indivi dual features, using the GBRT output. Finally, we compare varian ts of our model, a full model important features, and a unigram model for reference. The unigram model uses all the unigrams in the datasets as features, which is often the practice of many text classification practices. The classification met hod for this model is the same. Since all our datasets are of equal size (1000 texts), we deal with a balanced number of in-class and non-class instances. So the standard evaluation metric of accuracy and F1 measure both apply. So for each classification scenario, we report the accuracy and F1 measure from stratified 10-fold cross-validation. Statistical significance is measured by th e paired two-tailed t-test. We now report the evaluation result on the 3 datasets: humorous tweets, non-humorous tweets, and humorous non-tweets. Using one of the 5 feature categories each time to train the model, we arrive at the result shown in Figure 1 and Figure 2. http://scikit-learn.org/ Figure 1. Feature Set Comparis on (Humorous Tweets vs Non-As Figure 1 shows, lexico-seman tic features alone can recognize humorous tweets among all the tweets with an accuracy of 84%, followed by morpho-syntactic features. Pragmatic, phonetic, and significantly lower than the other features. This shows that in Twitter, word meaning and POS structure are the most indicative of humor. Figure 2. Feature Set Compar ison (Humorous Tweets vs As Figure 2 shows, the compar ison is similar for recognizing humorous tweets among all the humorous texts although separating humorous tweets fro m humorous non-tweets is less successful than separating humorous tweets from non-humorous tweets. But this time morpho-syntac tic features prove to be the most useful, even though the second-best lexico-semantic features include Twitter-specific symbols. Their F1 measure is significantly higher than that of a ll the other features. This shows that humorous tweets have some unique POS patterns. It is noteworthy that phonetic and affective features are quite useless as they are not much better than the 50% baseline. Our explanation is that funny tweets do not possess special sound patterns and affect is almost uni versal in all kinds of humor. In the next stage, we focus on individual features and report their importance, a score that equals the frequency of acting as the split point in the base decision trees of GBRT. Since we have 50 features, the average score is 1/50 = 0.02. In Figures 3 and 4, we only show the features with above-average scores. The features are coded with the labels specified in Section 4, prefixed by a short description. They are also color coded to indicate which category they belong to, according to the following table. 
Figure 3. Important Features (Humorous Tweets vs Non-Among the most important featur es for distinguishing humorous tweets from non-humorous tweets, le xico-semantic features and 5 morpho-syntactic features take the largest share, accounting for 5 each. 3 Twitter-specific features ar e important: url, #, and @. Manual inspection confirms that those symbols usually occur in non-humorous tweets (recall that #humor is removed). Not surprisingly, lexical chain features are important for fun creation, even in short texts. All the majo r POS X  X  are important, especially pronouns and verbs. Of all the affective features, only polarity features stand out. The second-level sentics features are not very useful due to their limited coverage of words. Of the phonetic features, the chain length is more important than th e chain number as longer chains indicate real phone patterns that give rise to humor. Only person and place deictics are the important pragmatic features, but their contribution is not significant. discriminative for separating humorous tweets from humorous non-tweets. Most of the important features are from the morpho-syntactic category. The two most prominent POS X  X  are proper nouns and verbs. Manual inspecti on confirms that proper nouns in humorous tweets, whereas verb ra tio is higher in tweets than in non-tweets. There are 3 pragmatic features: the speech acts of comment and question, and the 1-gram discourse marker. This finding confirms that Twitter messages are more communicative and dynamic than stock texts. Comment s and questions, which are rare in stereotyped literary jokes, are often found in humorous tweets. The important lexico-s emantic features are similar to those found in Figure 3. Note that although @ and # are unique to Twitter, they rank only 4th and 5th among all the features. Polarity features are also important, indicating a link between sentiment polarity and humor as well as social media. out that traditional species of humor play with sounds more frequently. Finally we compare the model performance by using different feature sets. In the following figures,  X  X ll Features X  represents the model that uses all the 50 features,  X  X ptimal Features X  is the model that uses only the important features as shown in Figures 3 and 4 (17 features for humorous tweets vs non-humorous tweets, and 19 features for humorous tweets vs humorous non-tweets).  X  X nigram Features X  is the mode l built with all the unigrams. There are 9413 unigram features for humorous tweets vs non-humorous tweets and 8313 unigram features for humorous tweets vs humorous non-tweets. The base line model is the one that blindly recognizes all instances as humorous tweets. 
Figure 5. Model Comparison (Humorous Tweets vs Non-Adopting only 34% of all the featur es slightly improves the model performance, instead of degrading it. This fact shows that the humorous effect can be captured by a small number of features and that it is worthwhile to develop high-value features. The models developed with our pr oposed features outperform the linguistically agnostic  X  X nigram F eatures X  model with statistical significance (p &lt; 0.01), which proves the value of applying humor theories to feature design. It is impossible to directly compare our method with previous methods, due to different data a nd classification algorithms. Just for reference, a similar study by Mihalcea and Strapparava (2006) on recognizing one-liners achieves the accuracies of 0.970 vs newspaper titles (of a very different genre), 0.792 vs general text, 0.848 vs proverbs, and 0.824 vs co mmonsense assertions by using both stylistic and content-based features. To compare models for r ecognizing humorous tweets vs humorous non-tweets, we add tw o models by removing the Twitter-specific features.  X  X on-Tw itter Features X  is the model with all but the 4 Twitter-specific features (F3-1 to F3-4).  X  X on-Twitter Optimal Features X  is a model with all but the Twitter-specific important features (F3-1 and F3-2 according to Figure 4). We believe removing the Twitter-sp ecific features will make the problem more challenging. The result is shown in Figure 6. Humorous tweets are clearly more difficult to distinguish from humorous non-tweets than from non-humorous tweets, probably because humor characteristics are better represented than Twitter characteristics by our features. Nevertheless, our features only 38 percent of all the features achieves almost the same performance. As expected, removing the Twitter-specific features hurts the model, but the perform ance drop is not statistically significant (p &gt; 0.05) compared with the top models. Therefore our features successfully capture some characteristics unique to the social media text. This work is motivated by probi ng to what degree humor theories enlighten computational humor studies. Although a precise definition of humor is beyond the scope of our work, we zoom in on a particular problem  X  recognizing verbal humor on Twitter  X  by systematically developing and analyzing features on a theoretical ground. The problem of Twitter humor recognition is necessary and important because on the one hand, user-generated tags such as #humor are often spurious or inadequate, and on the other, automatically recognizing humor promotes sentim ent analysis, opinion mining, and user study on social media at large. Drawing on humor theories, li nguistic norms, and affective dimensions, we propose an initial set of 50 humor-related features of 5 categories: phonetic features, morpho-syntactic features, lexico-semantic features, pragmatic features, and affective recognizing humorous tweets vs non-humorous tweets and recognizing humorous tweets vs humorous non-tweets. Despite the difficulty of distinguishing two kinds of structurally and stylistically similar text, our best features achieve an accuracy of 85% in the former and 82% in the latter. Among the five categories of f eatures, lexico-semantic and morpho-syntactic features are the most useful for both tasks, indicating that in short texts on so cial media, word-level syntax and semantics contribute the most to the amusing effect. Examined individually, Twitter-specific symbols, ratios of contents word, and word polarities are the most important features for distinguishing humorous tw eets from non-humorous tweets, whereas ratios of content words, speech act types, Twitter-specific symbols and sound patterns are the most important for distinguishing humorous tweets from humorous non-tweets. Such results show that Twitter humor possesses some unique aspects that are not found in plain Twitter text or static humor, which can be captured by carefully designed features. humor on Twitter with an emphasis on theory-motivated feature design and analysis. We believe our findings will shed light on automatic generation of humor, se ntiment word recognition, and user mood prediction on the social media, among many other interesting research topics and application scenarios. Those are all promising venues we can pursue in the future. This work is partially funded by a Shanghai Social Sciences Fund (2013BYY003) for the project  X  X  Humor Perspective to the Changing Patterns in the Microblog Language X . [1] Attardo, S. and Raskin, V. 1991. Script theory revisited: Joke [2] Austin, J. 1962. How to Do Things with Words . Oxford: [3] Bentivogli, L., Forner, P., Magnini, B., and Pianta, E. 2004. [4] Binsted, K., Bergen, B., Coulson, S., Nijholt, A., Stock, O., [5] Cambria, E. and Hussain, A. 2012. Sentic Computing: [6] Davies, C. 1990. Ethnic Humor Around the World . [7] Friedland, L. and Allan, J. 2008. Joke retrieval: recognizing [8] Friedman, J. H. 1999. Stochastic gradient boosting , [9] Gruner, C. R. 1997. The Game of Humor: A Comprehensive [10] Holton, A. E. and Lewis, S. C. 2011. Journalists, Social [11] Kiddon, C. and Brun Y. 2011. That X  X  What She Said: Double [12] Labutov, I. and Lipson H. 2012. Humor as Circuits in [13] Mihalcea, R. and Pulman, S. 2007. Characterizing humour: [14] Mihalcea, R. and Strapparava, C. 2006. Learning to laugh [15] Morris, J. and Hirst, G. 1991. Lexical Cohesion Computed [16] Oaks, D. D. 2012. On a Possible Generative Approach to [17] Obrst, L. 2012. A Spectrum of Linguistic Humor: Humor as [18] Owoputi, O., O X  X onnor, B., Dyer , C., Gimpel, K., Schneider, [19]  X zbal, G. and Strapparava C. 2012. Computational Humour [20] Petrovic S. and Matthews D. 2013. Unsupervised joke [21] Raskin, V. 1985. Semantic Mechanisms of Humor . [22] Raskin, V. 2012. A Little Metatheory : Thoughts on What a [23] Raskin, V., Hempelman, C. F ., and Taylor, J. M. 2009. How [24] Raz Y. 2012. Automatic Humor Classification on Twitter. In [25] Reyes, A. 2012. Linguistic-based Patterns for Figurative [26] Reyes, A., Rosso, P. and Buscaldi D. 2012. From humor [27] Ritchie, G. 2004. The Linguistic Analysis of Jokes . London: [28] Rutter, J. F. 1997. Stand up as Interaction: Performance and [29] Shultz, T. R. 1976. A Cognitive-Developmental Analysis of [30] Stock, O., and Strapparava, C. 2006. Automatic Production [31] Suls, J. 1972. A two-stage model for the appreciation of [32] Taylor, J. and Mazlack, L. 2004. Computationally [33] Taylor, J. and Raskin, V. 2012. On the Transdisciplinary [34] Toutanova, K. and Manning, C. D. 2000. Enriching the [35] Valitutti, A., Toivonen, H., Doucet, A. and Toivanen, J. [36] Zhang, R., Li, W., Gao, D., and Ouyang, Y. 2013. Automatic 
