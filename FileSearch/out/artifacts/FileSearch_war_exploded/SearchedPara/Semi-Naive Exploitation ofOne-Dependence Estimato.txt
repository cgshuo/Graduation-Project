 @lamda.nju.edu.cn
In principle, the optimal Baye sian classifier is the optimal way for supervised classification [9], which is, however, a theoretical model that requires infinite number of samples for true joint probabilities. Hence, in practice, restricted Bayesian models are used to approximate the optimal Bayesian model. The restricted models are ideally designed to be flexible sufficiently for capturing the ground-truth attribute dependencies as well as succinct sufficiently for estimating probabilities effectively. There is, however, a dilemma; that is, the model should only allow lower-order attribute dependencies for obtaining an effective estimate of probabilities from limited samples, yet for capturing the ground-truth attribute dependencies, the model should consider higher-order dependencies. It is noteworthy that learning the optimal dependencies has been proven to be NP-hard [5], [6].

Consequently, a spectrum of Bayesian learning ap-proaches has been developed, which takes different balances between the complexity of attribute dependencies and the effectiveness of probability estimation. The two extremes of the spectrum are the naive Bayes [20] and Bayesian network [23], [16]; the former totally ignores the attribute dependencies while the latter takes the maximum flexibility for approximating the attribute dependencies. Between the two extremes on the spectrum are semi-naive Bayesian clas-sifiers which try to find proper tradeoffs and have achieved successes [26], [25], [12], [11], [17], [4], [33], [36], [29], [32].

Among the numerous Bayesian learning approaches, semi-naive Bayesian classifiers which utilize one-dependence estimators (ODEs) have achieved remarkable performance. By allowing one-order dependencies, the probability estimation in ODEs is effective, while the model still has some flexibility for capturing the attribute dependencies. Representative approaches include TAN (tree augmented naive Bayes) [12], AODE (averaged one-dependence estimators) [29], HNB (hidden navie Bayes) [32], etc. Note that in previous studies, ODEs were exploited directly in a simple way, e.g., simple average of ODEs. It will be interesting to study whether a better performance can be achieved by exploiting the ODEs in other ways such that the resulting model inherits the effectiveness of ODEs while gains more flexibility for modelling higher-order dependencies.

In this paper, we propose a framework of semi-naive exploitation of ODEs (SNODE), where the ODEs are em-ployed to capture high-order attribute dependencies. SNODE approximates the joint probabilities by using functions of ODEs that are fitted according to maximum likelihood estimation. For implementation, generalized additive model (GAM) [15] is employed in this paper, and the function fitting problem is reduced to a constrained concave op-timization problem whose global optimal solution can be efficiently obtained. Experiments show that the performance of SNODE is superior to many state-of-the-art Bayesian classifiers. Bias-variance decomposition shows that, the suc-cess of SNODE mainly owes to its very low bias with slightly increased variance, which validates our proposal of semi-naive exploitation of ODEs.

The rest of the paper is organized as follows. In Section II, we introduce the background of Bayesian classifier learning and give a brief review on related work. In Section III, we propose the SNODE approach. Then we report on our experiments in Section IV. Finally, we conclude the paper in Section V.

Denote  X  =(  X  1 , X  2 ,  X  X  X  X  , X   X  ) as an instance, where  X  the value of the  X  -th attribute,  X   X  X   X  1 , X  2 ,  X  X  X  X  , X   X  where  X  is the number of classes, and  X  (  X  ) denotes the class of an instance  X  .

The optimal Bayesian classifier provides an optimal way to decide the class of an instance  X  as which yields the minimum possible classification error [9]. However, it is intractable in practical applications, since real-world training data is usually insufficient for a reliable estimate of the joint distribution  X  (  X  , X  ) . Consequently, approximating  X  (  X  , X  ) becomes the central problem in Bayesian classifier learning.

The modification of the optimal Bayesian classifier to-wards practical applicability forms a spectrum, along which different Bayesian learning approaches utilizing the attribute dependencies in different ways.

One extreme is naive Bayes [20]. It simply ignores all the attribute dependencies by t aking the conditional indepen-dence assumption, i.e., the attributes are independent given the class, thus it follows that Ignoring the dependencies makes the naive Bayes approach efficient, yet the missing of important dependence infor-mation sometimes hampers the classification performance seriously.

Another extreme is Bayesian network [23], [16], which has a strong flexibility for representing and exploiting various attribute dependencies. Suppose that the Bayesian network structure of attributes is known, Bayesian network can encode the joint probability distribution as where  X   X  denotes the values of the parents of the  X  -th attribute excluding the class value  X  for the instance  X  . However, learning the optimal structure, even on restricted structures, is NP-hard [5], [6]. Moreover, the data become overly sparse when the dependency order goes high, which causes both the ineffectiveness of probability estimation and inaccurate evaluations in the search of a good structure. Between the two extremes of the spectrum are semi-naive Bayesian classifiers [33]. Different from totally ignoring the attribute dependencies as naive Bayes or taking maximum flexibility for modelling dependencies as Bayesian network, semi-naive Bayesian classifiers try to exploit attribute de-pendencies in moderate orders. For example,  X  -DE (  X  -dependence estimator) [25] allows each attribute to depend on at most  X  other attributes in addition to the class.
Among numerous semi-naive Bayesian classifiers, ap-proaches which utilize ODEs have demonstrated remarkable performance. Representative examples include TAN and SP-TAN [12], [18], AODE and its variants [29], [34], [35], [31], HNB [32], etc. TAN restricts that each attribute can only depend on one parent in addition to the class, and thus it follows that where  X  X  X  (  X   X  ) denotes  X   X   X  X  dependent attribute, and is de-termined in the learning process. In AODE, an ensemble of ODEs is learned and the prediction is produced by aggregating the predictions of all qualified ODEs. In HNB, a hidden parent is created for each attribute which combines the influences from all other attributes.

Some other approaches implicitly restrict the dependen-cies by localizing naive Bayes classifiers, such as NBTree [19] which embeds a naive Bayes classifier in the leaves of a pre-trained decision tree, and LBR (lazy Bayesian rule) [36] which trains a naive Bayes classifier under a local rule. The success of semi-naive Bayesian classifiers using ODEs suggests that ODEs are on well balance between the ground-truth dependencies approximation and the ef-fectiveness of probability estimation. However, in previous approaches, ODEs were used directly in simple ways for classification. In next section, we will present the SNODE method which exploits ODEs in a semi-naive way.
 A. The Framework
Suppose that the ground-truth dependency structure of the attributes is known. The joint probability distribution can be denoted as where  X   X  is the attribute values on which the  X  -th attribute depends. Based on Eq. 1, the problem of estimating  X  (  X  , X  ) can be reduced to estimating the probabilities  X  (  X  ) and  X  (  X   X   X   X   X  , X  )  X  X . Since  X  (  X  ) can be easily estimated from the data, the key is to estimate  X  (  X   X   X   X   X  , X  )  X  X . Obviously,  X  (  X   X   X   X   X  , X  )  X  X  also encode the dependencies between at-tributes.

To maintain the effectiveness of probability estimation, we use only ODEs. Consequently, we want to approximate model the underlying relationship between  X  (  X   X   X   X   X  , X  )  X  (  X   X   X   X   X  , X  )  X  X  for each different  X  .

For this purpose, we can search in some function space for a function  X   X  by optimizing a certain criterion, such that the inputs of  X   X  are  X  (  X   X   X   X   X  , X  )  X  X  and the output is The log-likelihood and conditiona l likelihood are commonly used optimization criterions, thus can be used here.
Let  X   X  =[  X  (  X   X   X   X  1 , X  ) ,  X  X  X  X  , X  (  X   X   X   X   X  , X  )]  X  tribution can be written as where  X   X   X  X  are functions to be found. Given a training data set  X  , the log-likelihood of the model is where  X  (  X  ) is the class of an instance  X  ,and  X  = {  X  1 ,  X  X  X  X  , X  is the set of functions to be determined.

Taking log-likelihood as the optimization criterion, the function optimization pr oblem can be formalized as where  X  is the feasible function space from which the  X   X  are chosen.

Obviously, in Eq. 5, the objective is linear. Thus, if all  X   X   X  X  are independent, solving Eq. 5 is equivalent to solving a series of problems in Eq. 6 for different  X   X  X  where  X  X  X   X  (  X   X  ) is defined in Eq. 4.

Once the functions  X   X   X  X  are obtained by solving the optimization problem in Eq. 6, the joint distribution  X  (  X  , X  ) can be computed according to Eq. 2, and the class of instance  X  can be decided.

In SNODE, the functions of ODEs are fitted to gain the model flexibility as that by modelling higher-order attribute dependencies. Though maximum likelihood criterion is em-ployed here to fit the functions, other criterions such as minimum square error or conditional likelihood can also be employed.
 B. Modelling Higer-Order Dependencies with ODEs
Generalized additive model (GAM) [15] is a statistical model that permits the response probability distribution to be any member of the exponential family of distributions, which is also the probability distribution represented by Bayesian networks [16]. Here, we employ GAM for the implementation of our proposed SNODE framework. Note that it is also possible to use other models, which is an interesting future work. 1) Formulation: GAM relates the variables  X   X   X  X  to the objective variable  X  through the link function  X  (  X  ) and the smoothing functions  X   X  (  X  )  X  X  as where  X   X  X  X   X  X  are model parameters.

Employing GAM to approximate  X  (  X   X   X   X   X  , X  )  X  X  from  X  (  X   X   X   X   X  , X  )  X  X  , it becomes where  X  (  X   X   X   X   X  , X  ) is the objective variable, and  X  parameters. For simplicity, we use link function  X  (  X  )= log(  X  ) and smoothing function  X   X  (  X  )=log(  X  ) here. As the consequence, Eq. 7 can be rewritten as Considering normalization, the probability distribution can be written as where  X   X  (  X   X  )= Now, Eq. 8 relates  X  (  X   X   X   X   X  , X  ) and  X  (  X   X   X   X   X  , X  ) [  X  1 ,  X  X  X  X  ,  X   X  ] is the parameter with the constraints 2) Optimization Problem: Substituting Eq. 8 into the optimization problem in Eq. 6, we get the optimization problem to be solved as where  X   X  and  X   X  (  X   X  ) are defined in Eq. 10 and 11, respec-tively.
 Proposition 1 The optimization problem defined in Eq. 12 is a constrained concave optimization problem.  X  Training Process  X  Testing Process Proof: With a simple derivation we can get Thus,  X   X  (  X   X  ) is a concave function with respect to  X  Moreover, the feasible region is convex.

So, the problem in Eq. 12 is a constrained concave optimization problem.  X 
With the above proposition, the problem in Eq. 12 has one unique global optimal solution, and it can be solved effi-ciently by standard optimization algorithms such as interior-point methods [2]. 3) Algorithm: The pseudo-code is shown in Figure 1.
At the training time, SNODE forms the tables of the joint variable values and class frequencies from which the complexity of the tables is  X  number of classes,  X  is the average number of values of every attribute, and  X  is the number of attributes. Deriva-tion of the frequencies required to populate these tables is of time complexity  X  (  X  X  X  2 ) ,where  X  is the number of training examples. The concave optimization problem is critical to the training process, and its time complexity is  X  (  X  3 ) [22]. Therefore, the overall training time required is  X  (  X  2 (  X  +  X  2 )) .

At the testing time, classifying an instance requires the calculation of Eq. 8, and it is of time complexity  X  (  X  X  X  2 ) C. Discussion
It is easy to see that our proposed SNODE is a flexible framework. In the current paper, GAM and log-likelihood are employed as the model and optimization criterion, re-spectively. Obviously, other models and criterions can also be utilized.

By using logistic functions, we can get a special case of SNODE which is similar to the well-studied log-linear model [7]. This suggests that, it might be possible to derive more insight by connecting the log-linear model to semi-naive Bayesian classifiers.

Discriminative learning of Bayesian classifiers has at-tached much attention. Instead of optimizing the log-likelihood, discriminative approaches try to maximize the conditional likelihood and may produce better class prob-ability estimates [14], [24], [3]. A discriminant version of SNODE can be developed by maximizing the conditional likelihood; however, the corresponding optimization prob-lem may be more difficult than the current one, since the conditional likelihood does not decompose into separate terms for each function to be learned.
 A. Settings Thirty-five UCI data sets are used in our experiments. These data sets span a broad range of real domains, with sizes ranging from 76 to 20,000. Some statistics of the data sets are summarized in Table I, where #Inst , #Attr and #Class mean the number of instances, attributes and classes, respectively.

Considering that many Bayesian classifiers could not handle missing values and real values directly, for simplicity, we filled in the missing values by the mode or mean of the corresponding variable, and discretized all the data using the MDL discretization [10].

We compare SNODE with several Bayesian classifiers, including the simplest one, naive Bayes (NB), a general Bayesian network learning algorithm, K2 [8], and state-of-the-art semi-naive Bayesian methods including TAN, AODE and HNB. SNODE was implemented in W EKA [30], using the M ATLAB optimization toolbox [27] for optimization. The implementations of NB, K2, TAN, AODE and HNB in W EKA were used here. Laplacian correction was used in probability estimations.

Bias-variance decomposition [13] is helpful for analyzing performance of learning algorithms. It breaks the error into bias and variance . Bias describes the error of the learner in expectation, while va riance reflects the sensitivity of the learner to variations in the training samples. We performed bias-variance decomposition using the repeated cross-validation approach [28], which is the default method in W EKA .

In our experiments, fifty runs of two-fold cross-validation were executed and divided i nto 5 groups each contained 10 runs. Bias-variance decomposition was performed on each group, and thus the averaged predictive error and a pair of bias/variance were obtained for each group. The mean and standard deviations were recorded, and pairwise  X  -tests with 95% significance level were conducted.
 B. Results
The error, bias and variance averaged across all data sets for the compared classifiers are presented in Table II. The results of pairwise  X  -tests are summarized in Table III, where win/tie/loss means that SNODE wins, ties and loses on #win , #tie and #loss number of data sets against the corresponding comparison algorithm, according to pairwise t -tests. Detailed information over all data sets can be found in Tables IV, V and VI, respectively.

Figures 2, 3 and 4 depict the error, bias and variance relative to NB, respectively, where the  X  -axis shows error (bias, variance) of SNODE divided by that of NB, while the  X  -axis shows that of the compared classifier. Each point in the figures corresponds to one data set. The vertical and horizonal lines (i.e.,  X  =1 and  X  =1 ) are used to highlight the performance of NB, and the diagonal lines (i.e.,  X  =  X  ) indicate equal performance of the compared approaches. If the error (bias, variance) of SNODE is better than the compared method on a data set, the corresponding point will appear above the diagonal line. 1) Comparison with State-of-the-art Methods : It can be observed from Tables II and IV that SNODE achieved the lowest error on much more data sets than the compared algorithms, and it achieved the lowest average error. Con-cerning pairwise t -tests results in Table III, SNODE always has larger counts of wins than losses.

In Figure 2, most points appear to the left of the vertical line  X  =1 , indicating that SNODE performs better than NB. Also, the number of points appear above the diagonal line are apparently more than that below the diagonal line, indicating that the error of SNODE error is lower than that of the compared algorithms.

If we divide these algorithms into two groups, i.e., extreme group consisting of NB and K2, and moderate group con-sisting of SNODE, AODE, HNB and TAN, it can be found that classifiers in the moderate group performs better than those in the extreme group, which supports the motivation of developing Bayesian classifiers with the exploitation of moderate attribute dependencies.

Within the moderate group, it can be found that TAN sometimes has relatively poor performance, such as on house-votes-84 , nursery and kr-vs-kp , while SNODE, AODE and HNB, which utilize ODEs, have relatively close perfor-mance on most data sets. 2) Bias-Variance Analysis: It has been shown that the maximum dependency determines the upper bound of rep-resentation ability of a Bayesian classifier [21]; this implies that, from the viewpoint of version space, the maximum dependency determines the upper bound of the range of the function space. It is known that the larger the version space, the lower the bias, and the larger the variance; this suggests a connection between bias/variance and the flexibility for dependencies in Bayesian classifiers.

From the comparison of average bias and variance shown in Table II, it can be found that NB was with the highest bias but the lowest variance, K2 exhibited the lowest bias but the highest variance, and TAN, AODE and HNB achieved a tradeoff with bias and varian ce. Similar behaviors can also be observed in detailed results shown in Tables V and VI. This observation can be explained from the fact that NB ignores all dependencies, K2 tries to exploit all possible dependencies, while TAN, AODE and HNB try to utilize dependencies only in moderate orders.
 In Figures 3 and 4 it can be found that, compared with K2, SNODE achieved comparable bias yet much lower variance; compared with TAN, AODE and HNB, SNODE exhibited lower bias yet higher variance. This is also verified by pairwise t -tests shown in Table III. Hence, we can conclude that the success of SNODE ow es much to its low bias.
In Table II it can be seen that SNODE achieved the lowest bias while its variance is still lower than K2. This may suggest that, the utilization of many ODEs is sufficient to obtain a bias as low as that can be obtained by exploiting a full Bayesian structure. Furthermore, it is usually feasible to estimate one-dependent estimat ors accurately in real appli-cations. This validates our purpose of gaining flexibility as that by modelling higher-order probabilities based on one-dependent estimators.

In contrast to many previous Bayesian learning methods which utilize ODEs directly for classification in a simple way, in this paper, we present the SNODE framework, a semi-naive exploration of ODEs. In SNODE, functions of ODEs are employed to gain the flexibility by modelling higher-order attribute dependencies. As a special case, gen-eralized additive model is employed for the implementation, and the function optimization problem is then reduced to an optimization problem whose global optimum can be solved efficiently.

Experiment results show that the proposed SNODE achieves better performance than many state-of-the-art Bayeian classifiers. Bias-variance decomposition discloses that, SNODE achieves very low bias; this validates our proposal of gaining the flexibility as that by modelling higher-order attribute dependencies based on ODEs. More-over, the bias-variance decomposition suggests that there is a notable space for reducing the variance of SNODE. It is an interesting future work to improve SNODE by exploring regularization in the maximum likelihood estimation [1].
This work was supported by the National Science Foun-dation of China (60635030, 60721002), the Jiangsu Science Foundation (BK2008018) and the Jiangsu 333 High-Level Talent Cultivation Program.

