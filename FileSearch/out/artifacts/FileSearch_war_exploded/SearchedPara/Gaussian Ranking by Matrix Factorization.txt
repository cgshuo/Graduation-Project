 The ranking quality at the top of the list is crucial in many real-world applications of recommender systems. In this pa-per, we present a novel framework that allows for pointwise as well as listwise training with respect to various ranking metrics. This is based on a training objective function where we assume that, for given a user, the recommender sys-tem predicts scores for all items that follow approximately a Gaussian distribution. We motivate this assumption from the properties of implicit feedback data. As a model, we use matrix factorization and extend it by non-linear acti-vation functions, as customary in the literature of artificial neural networks. In particular, we use non-linear activa-tion functions derived from our Gaussian assumption. Our preliminary experimental results show that this approach is competitive with state-of-the-art methods with respect to optimizing the Area under the ROC curve, while it is par-ticularly effective in optimizing the head of the ranked list. H.2.8 [ Database Management ]: Database Applications X  Data Mining Recommender Systems; Collaborative Filtering; Matrix Fac-torization; Learning to Rank
In many applications, the recommendation quality at the head of the ranked list of items is crucial. There has been a large body of work on training collaborative filtering ap-proaches as to maximize various ranking metrics, see eg [5, 8, 17, 2, 18, 10, 20, 21, 14, 13, 12]. This is a difficult op-timization problem, as recommender systems typically pre-dict a continuous-valued score for each item, based on which the items are then ranked for a given user. The mapping from scores to ranks is not smooth, however, which makes it hard to optimize ranking metrics directly during training. For this reason, most approaches make smooth approxima-tions to the ranking metric as to enable gradient descent for computationally efficient training. In many cases, a spe-cific training method has been developed for each particular ranking metric, eg [18, 10, 20, 14, 13, 12].

In this paper, we develop an approach for training a rec-ommender system toward various ranking metrics, as cho-sen by the researcher. We illustrate its flexibility in terms of ranking metrics by optimizing normalized Discounted Cu-mulative Gain (nDCG), which emphasizes the top of the ranked list, as well as the Area Under the ROC Curve (AUC), which gives equal importance to the ranking of items across the entire list. Moreover, we develop a pointwise variant as well as a listwise one. While the listwise variant requires a more involved training algorithm, we find in our experiments that it yields improved results when optimizing toward the head of the ranked list, while the required computation time is comparable to the one of the pointwise approach.
In our approach, we take advantage of two well-known properties of implicit feedback data, namely that it is typi-cally binary and sparse . Binary implicit feedback data are ubiquitous in many applications of recommender systems, like clicks on links, purchases of goods, or plays of songs and movies. Moreover, the data are typically extremely sparse, as each user provides feedback on only a small number of items, and no feedback on the vast majority. Usually, the density of the data is around 1% for many publicly avail-able data sets, while it is often orders of magnitudes lower in industrial settings.

Moreover, we specifically focus on the matrix factoriza-tion (MF) model (see Section 2). MF has been found to be one of the best single models for collaborative filtering in the literature, see eg [6]. In this paper, we take advantage of the fact that it predicts the score for an item by sum-ming over numerous latent dimensions. This fact, combined with the central limit theorem in statistics, as well as with the properties of implicit feedback data mentioned above, motivated us to approximate the distribution of the pre-dicted scores by a Gaussian distribution in this paper (see Section 3). Furthermore, we present several approximations to this Gaussian assumption, which results in different ac-tivation functions  X  X ome of which are also commonly used in artificial neural network models (see Section 4.2). We also present a novel activation function that is particularly efficient and simple, as it is only slightly different from the root mean squared error (RMSE), which is perhaps the most popular accuracy metric in the recommender literature. We use these activation functions as to map between the scores of the items as predicted by the MF model, and the corre-sponding ranks of the items. The obtained ranks can then be used in various ranking metrics X  X e use nDCG and AUC in this paper (see Section 4.1). The resulting training ob-jective function and algorithm are outlined in Section 5. In Section 6, we compare our approach to related work in the literature, and point out its advantages. In our experiments on a publicly available data set as well as on a proprietary one (see Section 7), we find that the presented approach is competitive with state-of-the-art ranking methods regarding AUC, while it is particularly effective in optimizing the head to the ranked list.

In summary, the contributions of this paper are as follows:
Matrix factorization (MF) has proven to be one of the most accurate approaches to collaborative filtering in the literature of recommender systems, eg see [6] and references therein. For a given user u , the score s u,i of an item i is determined by the dot product of the latent item and user vectors, ~p i and ~v u , respectively: where k = { 1 ,...,K } is the index over the latent dimensions. In matrix notation, this reads ~s u = P~v u with score vector ~s = [ s u, 1 ,...,s u,N ] &gt; and matrix P = [ ~p 1 ,...,~p all items i  X  X  1 ,...,N } .

Concerning implicit feedback data, one typically distin-guishes between positive and negative items. Positive items are defined as the ones for which the user provided (positive) implicit feedback. In contrast, all other items are typically considered negative items. Note that the items in the nega-tive class may be further subdivided into items that the user truly does not like, items that the user may like but did not have time yet to provide feedback on, etc. Even though a single negative class is a simplification, it makes practical approaches feasible, and has been adopted as the common approach in the literature, eg [5, 8, 10, 17]. In the remainder of this paper, the number of positive and negative items of a given user u is denoted by N (+) u and N (  X  ) u = N  X  N respectively.

In this paper, we use the asymmetric matrix factoriza-tion model (AMF) introduced by [9] and later extended to SVD++ in [6]: the latent user-vector is defined as the sum of the latent item-vectors ~q j of the positive items j of the user:
Figure 1: AMF viewed as a NN (see Section 2). where I ( u ) is the set of positive items in the feedback his-tory of user u ; we define matrix Q = [ ~q 1 ,...,~q N ] as well as the user X  X  feedback vector ~ h u , where h u,j = 1 / j  X  I ( u ), and 0 otherwise. Compared to an unconstrained latent user vector that has to be learned for each individual user in the basic MF model, the constraint in Eq. 2 results in a considerable reduction in the number of model param-eters. Consequently, the parameters underlying the latent user vector ~v u can be estimated more accurately from the sparse data, where N (+) u N for a typical user u . An addi-tional advantage is the simplicity of the fold-in, ie computing ~v u from the user X  X  feedback history ~ h u .

As to render notation in this paper independent of a par-ticular MF model, we use  X  as to refer to any model pa-rameter, eg the elements in the matrices P and Q . More specifically, ~  X  u refers to the vector of all parameters that de-termine the latent user vector ~v u , ie all q j,k where j  X  I ( u ); and ~  X  i = ~p i refers to the parameters of item i .
In the remainder of this paper we will regularly refer to ideas from artificial neural networks (NN). Fig. 1 illustrates that AMF can be understood in terms of a NN: the user X  X  feedback vector ~ h u corresponds to the input layer of the NN. The matrix Q maps ~ h u to the hidden layer in the NN, which corresponds to the user vector ~v u in AMF. Multipli-cation with matrix P yields the values of the output layer ~s , which are the scores of the items in AMF. Finally, as we will discuss in the remainder of this paper, a (non-linear) activation function is applied to the scores as to obtain the final output ~r u of the NN. Note that the data points are in-dexed by the users u , while the observed dimensions within each data point are indexed by the items i .
This section provides a formal definition of the ranking problem considered in this paper. While we use AMF in this paper, note that the presented learning-to-rank approach is also applicable to other MF models. Our objective is to minimize a rank loss function, like AUC or nDCG, denoted in general by where the losses L are summed over all users u (we use an unweighted sum for brevity of notation). The class label for negative and positive items is denoted by y u,i  X  X  0 , 1 } , and the corresponding vector regarding all items as ~y u ; ~s vector of predicted scores, as defined above. As to train the MF model, in the remainder of this paper we derive L as a function of the model parameters  X  , based on the following assumptions: 1. binary labels: there are only positive items i (with 2. sparsity: for each user, the number of positive items is 3. for each user u , the distribution of the predicted scores
The rank loss L is typically defined as a function of the ranks r u,i , which themselves are determined by the scores s u,i ; 1 and s u,i are a function of the model parameters  X  . As to minimize L by stochastic gradient descent (SGD), its partial derivative w.r.t. each model parameter  X  is required. Using the chain rule, its derivative decomposes accordingly (where indices are omitted for brevity): where we have additionally inserted the normalized score s ( n ) . Comments on each of the four terms are in order: 1. L as a function of the ranks rather than the scores is 2. an item X  X  rank r u,i depends on its normalized score
Ranks are determined by sorting in descending order, ie the highest score results in the first rank.
 Figure 2: Assuming a Gaussian distribution of the scores, the Gaussian CDF (black line) provides the mapping from score s to rank r via the  X  X ormalized reverse X  rank  X  . Also shown are approximations by the logistic function (blue dash-dotted line) and the piecewise quadratic function of Eq. 13 (red dashed). 3. the normalized score depends on the (unnormalized) 4. the score s u,i depends on the model parameters ~  X  ; eg
The first two terms of the chain rule in Eq. 4 are inter-esting and will be discussed in the following sections.
This section discusses the rank loss L as a function of the ranks r u,i , regarding the first term in Eq. 4. In the following, we consider the loss for a given user, and hence drop the index u in the notation. Note that the loss L is minimized by minimizing the negative of the following rank metrics. Here we outline the equations for AUC and nDCG, which X  X ike many rank metrics X  X epend explicitly only on the ranks r (+) of the positive items, while the negative items enter only indirectly by affecting the ranks of the positives. Also other rank-loss functions may be used in this approach as long as they are differentiable with respect to the rank, when the rank is considered a continuous variable. Loss functions that cannot be used in this framework, are for instance, recall@ n , precision@ n etc, as they are not smooth at rank n .
The Area Under the ROC Curve (AUC) is commonly used for evaluating the ranking of the entire list of items (rather than the head of the ranked list). It can be defined as with the indicator function II(  X  ) = 1 if its argument is true, and 0 otherwise; N (+) and N (  X  ) are the number of positive and negative items, respectively, and N (+) + N (  X  ) AUC determines the fraction of times when the score s (+) of a positive item i is correctly predicted to be larger than the score s (  X  ) j of a negative item j . AUC is equivalent to the Wilcoxon-Mann-Whitney statistic and to the fraction of concordant pairs. Replacing the indicator function by a smooth function immediately leads to a popular training method, Bayesian Personalized Ranking (BPR) [10].

Instead of evaluating N (+)  X  N (  X  ) pairwise comparisons, however, it can be computationally more efficient to sort the N scores in time O ( N log N ), and then use the ranks r i of the positive items: AUC = 1 It is easy to see the equivalence of Eqs. 6 and 7, eg see [3]. AUC is hence linear in the ranks r (+) i ; the derivative of Eq. 7 is hence a constant for each positive item i ,
The normalized Discounted Cumulative Gain (nDCG) em-phasizes the head of the list. It is defined as where DCG (opt) is determined by the optimal ranking. While DCG can deal with graded relevance, it reads in the binary setting if negative items have zero relevance. The derivative of Eq. 9, which is needed in Eq. 4, reads  X  X 
This section is concerned with the second term in Eq. 4, ie  X  X / X  X  ( n ) . Eq. 5 provides the link between scores and ranks for a general CDF. The two most important special cases are: (1) if the scores follow a uniform distribution (on a finite interval), then the ranks are a linear function of the scores. This justifies the hinge loss of the scores as to optimize AUC, as discussed in more detail in [16]. In case (2), the scores s u,i (approximately) follow a Gaussian distribution. This was motivated in the third item in Section 3. In this case, the probit function provides the link between the scores and the ranks. It is well know that the probit is well approximated by the logistic or sigmoid function with appropriate scaling: probit( s )  X   X  ( s p 8 / X  ), see eg [1]. Moreover, a computationally efficient approximation can be obtained by a piecewise quadratic function, when using the scaling: probit( s )  X  quad( s/ approximations are illustrated in Fig. 2.

An even simpler approximation that we found to work well in practice, is obtained when we use the fact that among the positive items only those with a positive score matter (ie toward the top of the ranked list), while the (few) ones with negative scores can be ignored (ie toward the bottom of the ranked list): it is the rank squared error where [ x ] + = x if x &gt; 0, and 0 otherwise. It is easy to see that quad( x ) = rankSE( x ) for the relevant values, ie for x &gt; 0.
In this section, we consider listwise ranking, ie we take into account properties of the entire list of items for a given user. First, this allows us to overcome the assumption that the distribution of the scores has a fixed standard deviation std u = const, independent of the user. Now we allow for different values of std u for different users u . This can effi-ciently be estimated from the empirical distribution of the scores s u,i for a given user u .

Second, as discussed in Section 4.1, many rank losses de-pend explicitly on the ranks of only the positive items. Positive items typically have large scores and hence lie in the tail of the distribution of all the scores (of positive and negative items). Due to the sparsity in the tail, the map-ping by means of the activation function may result in noisy estimates of their ranks as a function of their scores. In addition, the scores cannot be expected to follow exactly a Gaussian distribution in practice (as it is a simplifying as-sumption after all). For these two reasons, in the listwise variant, we simply sort all the items by their scores as to directly obtain their exact ranks, which are denoted by  X  r Then, in Eq. 4, the derivative of the rank loss  X  X  ( r ) / X  X  can be evaluated immediately at r =  X  r u,i . The second term in Eq. 4, ie  X  X / X  X  ( n ) , can be approximated by mapping the rank  X  r u,i back to the corresponding score  X  s ( n ) u,i lytically tractable if we assume a logistic activation function  X  : given  X  r u,i , we first compute the normalized reverse rank  X   X  u,i = 1  X   X  r u,i /N , see Fig 2. Then, given that  X   X  u,i are linked via  X   X  u,i =  X  ( X  s ( n ) u,i ), see Fig. 2, one obtains where the indices are omitted for brevity. Note that this is computationally efficient as no logistic or exponential func-tion has to be computed explicitly.

This is illustrated in Figure 3 when combined with AUC and nDCG: for the choice of AUC, one obtains Note that, for the top half of the ranked list (  X   X  &gt; 0 . 5), the gradient L 0 (  X   X  ) monotonically decreases toward the top of the ranked list (ie toward  X   X  = 1). In contrast, as illustrated by the other two curves in Figure 3, which show the behavior for nDCG, the derivative as a function of  X   X  reads L as obtained by combining Eqs. 15 and 11. Note that (1  X   X   X  ) N =  X  r  X  { 1 ,...,N } . As shown in Figure 3, L increases with growing  X   X  .

Between these two opposite behaviors (ie decreasing vs in-creasing derivatives for AUC vs nDCG), the constant gradi-ent is a trivial but interesting case. It hence can be expected to optimize the ranking with emphasis that is partially on the head and partially on the entire list. Obviously the corresponding function has to be linear in the scores. The hinge loss and the linear rectifier are two functions that are linear unless they are zero. While the former is used as a loss in support vector machines, the latter is applied as an activation function in deep NN.

There is also another way of obtaining a nearly constant gradient, resulting in a partially head-heavy ranking loss: in our pointwise variant, one may choose an activation function with an associated standard deviation std activ that is con-siderably larger than std u of the actual distribution of the scores. Then the resulting ranking loss is increasingly head-heavy as std activ becomes larger compared to std u reason is that, for std activ std u , the gradient decreases more slowly than the one resulting from the proper stan-dard deviation (only if std activ = std u , the mapping from the scores to the ranks is correct in the sense of Eq. 5). In fact, we find this idea to work quite effectively in our ex-periments in Section 7: we use AUC as the loss function, but choose an activation function with std activ std to render the resulting loss head-heavy. This is indicated by probit ( w ) , logistic ( w ) and rankSE ( w ) in Tables 1 and 2, which show that the resulting rankings are competitive toward the head of the ranked list, for the price of being degraded with respect to AUC.
Having discussed the rank loss L and several of its variants above, we now can add the regularization terms as to obtain the final training objective function to be minimized: X The details are as follows. First, in the sum over the set I ( u ) of positive items, we have the loss function L , which commonly is a function of the positive items only (see Sec-tion 4.1), as well as the usual ` 2 -regularization terms for the model parameters ~  X  u and ~  X  i concerning the user u and items i , ie the parameters p i,k and q i,k with i  X  I ( u ) in AMF. The ` -regularization may be viewed as the logarithm of a Gaus-sian prior distribution with zero mean and variance 1 / (2  X  ).
Second, the sum over all items j  X  I in Eq. 18 enforces the (approximately Gaussian) distribution of the scores s about zero. This reflects our main assumption, as outlined in Section 3, namely that the scores follow approximately Figure 3: The derivative of the loss  X  X   X  X   X  X  first two terms in Eq. 4, as a function of the  X  X or-malized reverse X  rank  X  : as  X  approaches the top of the list (  X   X  1 ), the gradient decreases for L =  X  AUC (solid line), while it increases for L =  X  nDCG , for different lengths N of the ranked list, N = 10 , 000 (dashed line) and 10 million (dash-dotted line). The latter two curves are re-scaled along the y-axis. See Sections 4.3 and 4.1 for details. the posterior distribution s u,j  X  N (0 , std 2 u ). As to obtain such a posterior distribution with the desired variance std one may use a prior distribution s u,j  X  X  (0 , 1 / (2  X  )), where  X  is chosen appropriately. The logarithm of this prior im-mediately yields the additional term  X   X  s 2 u,j . For simplicity, we have chosen  X  to be independent of user u here.

The derivative of Eq. 18 may be interpreted as the force that pulls an item X  X  score toward the target values 0 or 1 during training. This interpretation immediately originates from the perspective of physics when Eq. 18 is interpreted as an energy. In this picture,  X  may be viewed as the degree with which an item X  X  score is pulled toward zero . During training with the feedback data, this is counter-balanced by having the scores of the positive items being pulled toward a positive target value (like 1) due to the rank loss L . A proper choice of  X  is hence crucial for the obtained results; the value of  X  can be determined such that the desired rank metric on a validation set is optimized.

Note that there is no term in Eq. 18 that explicitly tries to pull scores toward a negative target value. For this reason, it is not necessary for the  X  X rior X  to pull negative scores toward zero. In fact, it is beneficial to modify the last term in Eq. 18 from  X   X  s 2 u,i to  X   X  ([ s u,i ] + ) 2 , where [ x ] 0 otherwise; ie, an item X  X  score s u,i is pulled toward zero only if it is positive. We found this modification to yield improved ranking results in our experiments.

Eq. 18 can efficiently be minimized by stochastic gradient descent (SGD). The necessary derivatives of the different variants of the rank loss are outlined in the previous sections. For computational efficiency, the sum over all items I in Eq. 18, and hence the iteration at line 11 in Algorithm 1, are sub-sampled, like eg in [8, 17]. Moreover, note that line 16 of Algorithm 1 refers to the negative items, while the last term in Eq. 18 refers to both positive and negative items X  this difference is negligible in practice as N (+) N (  X  ) iterative training algorithm for AMF has the same structure as the one for training SVD++ [6]: for each user, one first Algorithm 1 SGD for AMF to minimize pointwise loss L . 1: Input: : learning rate  X  , parameters  X , X  , training data. 2: Output : latent vectors ~p i , ~q i for all items i . 3: do : 4:  X  i : ~p i  X  random initialization 5:  X  i : ~q i  X  random initialization 6: repeat 7: for user u do 8:  X   X  0 9: ~  X  0 11: for all items i  X  I ( u ) do 12: if i  X  I ( u ) then 13: err  X  X  X  L 0 ( ~p &gt; i ~v u ) 14:  X  i  X   X  15:  X   X   X  +  X  16: else 17: err  X  X  X   X  [ ~p &gt; i ~v u ] + 18:  X  i  X  0 19: ~  X  ~ + err  X  ~p i / 20: ~p i  X  ~p i +  X   X  ( err  X  ~v u  X   X  i  X  ~p i ) 21: for i  X  I ( u ) do 22: ~q i  X  ~q i +  X   X  ( ~  X   X   X  ~q i ) 23: until convergence iterates through all the items i as to update the parameters ~p , while the updates for the parameters ~q i are aggregated in the temporary vector ~ and in the regularization value  X . After that, in a second iteration at line 21 in Algorithm 1, the parameters ~q i get updated based on the temporary vector. Algorithm 1 provides the pseudo code for pointwise ranking with AMF, using the derivative of the loss L see also Eq. 4. The fourth derivative-term in Eq. 4 is included in lines 19 and 20 in Algorithm 1.

In the listwise training (see Section 4.3), there is an addi-tional iteration over the items as to compute their scores, and then their ranks and the scores X  standard deviation. Only after that, the SGD updates in line 11 and following are made analogous to the pointwise approach.

In addition, we use dropout learning [15]. It combines the random subspace method [4] with parameter sharing across the models in the (implicit) ensemble, which results in a reduced generalization error.
In the recommender literature, many learning-to-rank ap-proaches are pairwise methods. The most well-known ap-proach is perhaps BPR [10], which optimizes AUC. As to op-timize head-heavy rank metrics, custom-tailored approaches to optimize mean-reciprocal-rank (MRR) and mean-average-precision (MAP) were presented in [13, 12]. Moreover, weight-ing schemes [19, 20] and sampling schemes for the negative items [21] were proposed, and may be viewed as extensions to BPR.

In contrast, the presented approach considers all items concerning a user u at once, rather than each user-item pair ( u,i ) separately. If we assume that the number of sam-pled negative items is the same as the number of positive items, ie N (+) u for user u , then all the SGD updates for a user u can be computed by first sorting the scores in time O ( N (+) u log( N (+) u )) in the listwise approach (see Section 4.3). Note that this accounts for all pairwise comparisons between positive and negative items. In the pairwise approach, this would require much more time, ie O (( N (+) u ) 2 ).
While minimizing RMSE on the observed ratings is as-sociated with the  X  X ating prediction X  problem in the recom-mender literature, minimizing the squared error with proper sampling and weighting of negatives [5, 8, 17] was also suc-cessfully used for ranking problems, and outperformed other ranking methods, like eg [2]. The presented approach essen-tially replaces the RMSE objective with a rank loss.
A listwise approach to training recommender systems has been proposed in [14]. It was motivated from top-1 recom-mendations, and was experimentally shown to outperform Cofi-rank [18] regarding nDCG for the top items. This ap-proach uses the softmax activation function and the cross entropy as loss function. Interestingly, the same approach is also popular for training artificial neural networks. We included this approach as an additional baseline in our ex-periments. This section outlines our results on the 10 million Movie-Lens data [7] and our proprietary Netflix data.
In the 10 million MovieLens data [7], 69,878 users have rated 10,677 movies. As to simulate binary  X  X lay X  data for use in our approach, we binarized the MovieLens data by retaining all ratings of 3 or higher as positives (which may indicate some user satisfaction), while discarding the lower ratings. This resulted in a data set of 8.2 million plays, with a sparsity of about 1%. All other user-movie pairs are considered as  X  X egative X , as defined in Section 2. We split this data set into a training set (95%) and a disjoint test set (5%). We repeated this random split five times, and report mean and standard deviation. Given that the disjoint test set does not contain any movies played by the user in the training set, those movies are filtered out from the user X  X  recommendations. We used AMF with K = 50 latent dimensions.

We evaluated the ranked list of items with respect to AUC and nDCG. Besides nDCG, there are various ranking metrics that summarize the ranking quality with a certain emphasis on the top of the list in a single number, like mean aver-age precision (MAP), mean reciprocal rank (MRR), etc. In-stead, we provide results w.r.t. recall@ n for several n , which provides detailed insights in the ranking quality at different positions in the ranked list. In the interest of space, we do not report precision@ n , as it is proportional to recall@ n for the same test data and the same value n for each user, see eg [17].

When training the AMF model, we re-scaled the inputs of the activation functions such that their widths agreed with rankSE in Eq. 14 (rather than with the probit function) X  this allowed as to use the target values 1 (instead of and 0 for the positive and negative items in the pointwise approach, respectively. In the training objective function in Eq. 18 we used the parameters  X  = 10  X  7 and  X  = 0 . 3 for this data set for the pointwise variants, as determined by a grid search. The step size in SGD was  X  = 0 . 001. Choosing Table 1: Ranking results on the MovieLens data [7]. Based on five repetitions of the experiments, the result are highlighted. that large a value for  X  leads to an increased emphasis of the ranking quality at the head of the ranked list, as discussed at the end of Section 4.3. For this reason, we use probit logistic (w) and rankSE (w) in Table 1, as to indicate that the AUC rank loss is combined with a w ide activation function. Table 1 reflects this fact clearly, as  X  X UC X  with all three wide activation functions obtains high values with respect to recall toward the top as well as regarding nDCG. Moreover, these three variants of wide activation functions lead to very similar results, as expected from Figure 2.

The listwise variants of the proposed approach adapt the activation function to the width of the distribution of the scores of each user, ie std activ = std u . Consequently, the chosen rank loss, nDCG or AUC, determines whether the ranking is optimized regarding the head of the ranked list or concerning the entire list. Table 1 illustrates this flexibility of the presented approach, as either recall @ 10 and 30 is optimized, or AUC, depending on the chosen rank loss. In-terestingly, Table 1 also shows that the presented approach trained with nDCG loss is much more head-heavy than ex-pected: while it achieves high recall @ 10 and 30, the rank-ing quality deteriorates very quickly below the head of the ranked list, reflected by an extremely low AUC. This also degrades nDCG on the entire list. While not fully clear from the theoretical side, this behavior is actually desirable in real-world applications of recommender systems, where only a very limited number of items can be shown to the user due to space constraints.
Table 1 also shows the results of three competing ap-proaches for comparison, whose hyper-paramters were also optimized by grid search. 2 As to optimize the root mean squared error (RMSE) 2 on binary data we followed the ideas outlined in [5, 8, 17], ie we randomly sampled negative items in addition to using the positive items in the implicit feed-back data. This is a crucial change to the popular approach used for  X  X ating prediction X , eg [11, 6]. As a result, this ap-proach is geared toward optimizing the ranking, and was found to outperform other ranking methods, like eg [2]. Ta-ble 1 shows that optimizing RMSE with sampled negative items yields solid ranking results.

BPR 2 [10] optimizes AUC in Table 1, as expected. Not surprisingly, as the head X  X  contribution to the ranking of the entire list is small, the ranking at the top is rather unim-
For fair comparisons, we also used the AMF (instead of the standard MF) model for all competing approaches. portant, and consequently the head-heavy metrics are lower than for RMSE optimization.

As already outlined in Section 6, the approach that op-timizes the cross-entropy and uses the softmax activation function 2 in Table 1 was motivated in [14] to optimize top-1 recommendations, and it was also shown to outperform Cofi-rank [18] concerning nDCG at the top. In Table 1 re-garding the top of the ranked list, however, it only improves over the variants optimzied toward AUC and RMSE, while it is outperformed by all our pointwise and listwise variants that optimize the head.
The results on our proprietary data agree with the in-sights from the MovieLens data above. We considered all movies and TV shows available in our US catalog (for sim-plicity, we ignored videos available on Netflix outside the US, as a global model that can deal with different availabil-ities of videos in different countries is beyond the scope of this paper). The split into disjoint training and test data was performed based on the time-stamp when a video was played, as to simulate the situation of the real-world system: we randomly picked a test-day in 2014, and the preceding 6 months as the training period. We used binary data that captured whether a user played a video or not during the training or test periods, respectively. If a user played the same video in the training and the test data, we removed it from the test data. We sampled a random subset of about 500,000 anonymized users. This was repeated five times, and we report the mean and standard deviation (std) of the test-metrics.

Table 2 depicts the results, where we focus on the very top of the recommended list, as this is crucial in the real-world application. We also provide AUC regarding the entire list as additional information. The hyper-parameters in the training objective were again determined by grid search for each model. The results are relative to the baseline model, for which we picked the one that optimizes RMSE regarding positive items combined with randomly sampled negative items, as described in the previous section. The proposed pointwise variants with a wide activation function, as well as the listwise optimization of nDCG again perform well at the top of the list. Regarding the AUC metric, BPR as well as our listwise optimization of AUC (and the RMSE baseline model itself) achieve the highest AUC (within the noise level). Table 2: Ranking results on our proprietary Net-flix data set, as a relative improvement over train-ing toward RMSE with proper sampling of negative items. 2 The uncertainty (std) is 2 % for recall and 1% for AUC (percentage points). Values within std of the best result are highlighted. loss activ. recall @ fct. fct. 1 5 10 30 AUC  X  X UC X  logistic (w) 30 % 15 % 7 % -1% -13% nDCG logistic 32 % 17 % 9 % 2 % -5% AUC logistic 24% 9% 2% -3% 0 % RMSE linear  X   X   X   X   X   X   X   X   X   X  X ent. softmax 28% 14% 8 % 0 % -1%
In this paper, we presented a novel learning-to-rank ap-proach that takes advantage of two typical properties of im-plicit feedback data, namely that they are sparse and binary. This motivated us to make the approximation that the scores of the items, as predicted by a matrix factorization model, follow approximately a Gaussian distribution. Based on this assumption, we derived several non-linear activation func-tions. This enabled us to optimize toward different ranking metrics efficiently. In our experiments, we found that the presented approach is competitive with respect to the Area under the Curve, while it is particularly effective for opti-mizing the head of the ranked list.

Given that the used asymmetric matrix factorization model can be viewed as a shallow neural network, future work in-cludes the extension of the presented learning-to-rank ap-proach to deep neural networks. I would like to thank Caitlin Smallwood, Carlos Gomez-Uribe and Roelof van Zwol for their encouragement and sup-port of this work. [1] C. Bishop. Neural networks for pattern recognition . [2] P. Cremonesi, Y. Koren, and R. Turrin. Performance [3] D. J. Hand and R. J. Till. A simple generalization of [4] T. Ho. The random subspace method for constructing [5] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [6] Y. Koren. Factorization meets the neighborhood: a [7] MovieLens. Univ. of Minnesota, homepage: [8] R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose, [9] A. Paterek. Improving regularized singular value [10] S. Rendle, C. Freudenthaler, Z. Gantner, and [11] R. Salakhutdinov and A. Mnih. Probabilistic matrix [12] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [13] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [14] Y. Shi, M. Larson, and A. Hanjalic. List-wise learning [15] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, [16] H. Steck. Hinge rank loss and the area under the ROC [17] H. Steck. Training and testing of recommender [18] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola. [19] J. Weston, H. Yee, and R. Weiss. WSABIE: Scaling [20] J. Weston, H. Yee, and R. Weiss. Learning to rank [21] W. Zhang, T. Chen, J. Wang, and Y. Yu. Optimizing
