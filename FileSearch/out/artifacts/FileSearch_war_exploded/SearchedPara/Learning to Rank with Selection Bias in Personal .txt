 Click-through data has proven to be a critical resource for improving search ranking quality. Though a large amount of click data can be easily collected by search engines, var-ious biases make it difficult to fully leverage this type of data. In the past, many click models have been proposed and successfully used to estimate the relevance for individual query-document pairs in the context of web search. These click models typically require a large quantity of clicks for each individual pair and this makes them difficult to apply in systems where click data is highly sparse due to personalized corpora and information needs, e.g., personal search. In this paper, we study the problem of how to leverage sparse click data in personal search and introduce a novel selection bias problem and address it in the learning-to-rank framework. This paper proposes a few bias estimation methods, includ-ing a novel query-dependent one that captures queries with similar results and can successfully deal with sparse data. We empirically demonstrate that learning-to-rank that ac-counts for query-dependent selection bias yields significant improvements in search effectiveness through online exper-iments with one of the world X  X  largest personal search en-gines.
 Personal Search; Selection Bias; Learning-to-Rank
In the past several years, click-through data has become an indispensable resource for online information retrieval services. It provides a natural, abundant and continuously renewable source of user feedback. However, despite its tremendous value, click-through data is inherently biased and very noisy. Previous research shows that in order to reliably leverage click-through data one has to account for multiple sources of bias including: position bias [22], presen-tation bias [33], and trust bias [28]. Therefore, directly using click-through data may result in noisy and biased training data, which will negatively impact the downstream applica-tions [21]. As a result, there has been a great deal of research on extracting reliable signals from click-through data [10].
Previous work has typically focused on click modeling to estimate relevance for individual query-document pairs. For instance, Craswell et al. [11] proposed the Cascade model, in which the conditional probability of a click on a document x given position i is predicated on the marginal probability of the document x being relevant to the query and the marginal probabilities of documents at positions 1 ,...,i  X  1 being non-relevant to the query. In order to estimate these marginals, the click models often assume access to large quantities of click data for each document x given the query [8, 11, 15]. Such models have generally proven to be successful in the context of web search, where this assumption holds. How-ever, it is less clear how they can be applied in search sce-narios where click data is highly sparse.

One such scenario that is the focus of this paper is per-sonal search . Personal search is an important and well stud-ied information retrieval task with applications such as email search [6], desktop search [13], and, most recently, on-device search [23]. One important difference between personal and web search is that in the personal search scenario each user has access only to their own private document corpus (e.g., emails, files, or mobile application data). Therefore, the vast majority of the existing click models that learn click prob-abilities from large quantities of clicks for individual query-document pairs are not applicable in the personal search scenario.

Another important challenge in the context of personal search is the collection of explicit relevance judgments. Col-lection of TREC-like document relevance judgments by third party raters (that are commonly used in other information retrieval tasks) such as LETOR data set [29] are difficult to obtain due to privacy restrictions. In addition, since each user will have their own unique set of information needs and documents that evolve over time (e.g., new emails arrive ev-ery day), explicit relevance judgments may be prohibitively costly to maintain. Therefore, development of ranking mod-els in general, and specifically learning-to-rank models [26] that utilize click-through data as a noisy and biased source of relevance data becomes essential for personal search.
In this paper, we study the the problem of learning-to-rank from click data in personal search. Different from the majority of past click modeling work whose focus is on es-timating the relevance for individual query-document pairs, we propose a novel selection bias problem in the context of the learning-to-rank from click data. The basic idea of the selection bias problem is that queries are under-sampled to different extents and thus biased when click data is collected to learn ranking functions. We propose several methods to estimate this bias. We begin with a global bias model and refine it to a segmented bias model. We show that such a segmented bias model gives rise to a general framework that defines a query-dependent bias, where every query (and its associated result set) can be associated with a potentially different bias model. This general query-dependent frame-work is especially powerful in the personal search scenario as it allows accurate bias estimation without explicit access to a large number of clicks for any given query-document pair. To the best of our knowledge, this is the first study that both proposes a theoretical framework for eliminating selection bias in personal search and provides an extensive empirical evaluation using large-scale live experiments.
Our primary contributions of this paper can be summa-rized as follows: The remainder of the paper is organized as follows. In Section 2, we review previous related work. Our problem is formally defined in Section 3. Different methods of quantify-ing selection bias are described in Section 4. We present our extensive experimental study and our evaluation methodol-ogy in Section 5. Finally, we conclude and discuss future work in Section 6.
There is an abundance of prior work on interpreting clicks as implicit feedback from users. One of the seminal papers in the field by Joachims et al. [22] evaluates the reliabil-ity of click-through signals via a user study. The overall conclusion of the study is that clicks are indeed useful for implicit feedback interpretation as long as certain biases are accounted for, including  X  X rust bias X  (commonly referred to as  X  X osition bias X  in later work) that leads to more clicks on higher-ranked results, and  X  X uality bias X  in which the click behavior is influenced by the overall quality of the ranked list.

Joachims et al. [22] also proposed five simple strategies to eliminate these biases, including the Click &gt; Skip Above strategy, which gave rise to the well-known Cascade model [11]. Later work also introduced a variety of click mod-eling techniques including, among many others, a dynamic Bayesian network click model [8], click chain model [17], ses-sion utility model [14], whole page click model [9], multiple browsing model [15], and a general click model [36]. A re-cent survey by Chuklin et al. [10] provides a good overview of the latest advances in the field.

While both click models in prior work and selection bias estimation presented in this paper focus on deriving useful implicit feedback from click-through data, there are several important differences. First, while the majority of the past click modeling work focuses on estimating the degree of rele-vance between a query and a document, the main goal of this paper is to study the selection bias problem in which click data used in learning-to-rank may drift away from the true underlying distribution. Second, the existing click models assume that a sufficient amount of click data is available for each query-document pair in each position to reliably train the model parameters. While this assumption holds in the web search setting, it is not feasible in domains like desk-top search, email search or enterprise search, where each user might have access to a different set of documents, and it is impossible to leverage the  X  X isdom of the crowds X  to aggregate clicks across users. Third, the click models are generally evaluated using the perplexity between the esti-mated and observed clicks [9, 15]. In contrast, we directly evaluate the ranking effectiveness of our methods through offline evaluation and online live experiments.

Click data is extensively used in sponsored search where the main goal is to predict the click-through rate for ads (e.g., [30, 36]). Noticeably, our selection bias estimation methods are related to the position bias model in [30]. The main difference is that we use the estimated bias to address the selection bias problem, while the bias was used as the expected ads impressions when computing the click-through rate of ads using data from multiple positions. Furthermore, we also propose more advanced query-dependent bias models that are more tractable even in scenarios where training data can be scarce due to small sample sizes, low search volume or personal document collections.

Our bias estimation models rely on the randomized ex-perimental data. Order randomization removes the position biases that are inherent to click data, and therefore one can view the proposed models as propensity score [31] estimates. Furthermore, randomized data is the basis for our proposed unbiased offline evaluator. Similar evaluation methodologies were proposed by Li et al. in prior work [24, 25]. The dif-ference is that we also use the randomized data for selection bias estimation to improve ranking functions, which is not the case in the past work.

Order randomization also eliminates, to a certain degree, the selection bias inherent in many information retrieval ap-plications that employ pooling of top retrieved results [27]. Previous work [5] proposed methods to avoid the selection bias in TREC-style evaluation settings. However, such ap-proaches do not easily extend to the online evaluation case, which is addressed in this work.

The methods proposed in this paper can also be viewed as a novel extension of sample selection bias correction meth-ods, which are well-studied in the context of regression and classification [20, 34, 32], in the online learning-to-rank set-ting. In contrast to previously proposed learning-to-rank models that make explicit assumptions about user behavior [19] or use heuristic-based method for document selection [1], we learn the selection bias directly from experimental data.
In this section, we introduce the selection bias problem for learning-to-rank in personal search scenarios. We begin by briefly reviewing the general setting of learning-to-rank.
Let Q = ( q, { x 1 ,..., x n } ) denote a query string q and its set of result documents. We write x  X  Q to indicate that x is in the result set of Q . Let P ( Q ) denote the probability of observing query Q , based on the underlying distribution of queries in the universe Q of all possible queries that users can issue together with all possible result combinations. The goal of learning-to-rank is to find a scoring function f ( x ) that can minimize the loss function defined as: where l ( Q,f ) is the incurred loss of scoring function f ap-plied to query Q . Let x i Q x j denote all pairs x i , x result documents in Q for which x i is more relevant than x . An example of a pair-wise loss function used in [35] is defined as: The intuition behind this loss function is to penalize the out-of-order pairs when ranked by f .

In practice, the distribution of queries in Q is is unknown and the empirical loss defined over a uniformly random sam-ple U = { Q  X  Q : Q  X  P ( Q ) } is used as the objective function for learning.
 Most learning-to-rank algorithms differ in how the loss func-tion l ( Q,f ) is defined [26]. Generally, the state-of-the-art loss functions are pair-wise or list-wise. Practically, pair-wise loss functions tend to be more efficient for training and have been widely adopted by large search engines [4, 35]. Thus, in the rest of the paper, we make the assumption that a pair-wise loss function (e.g., Eq 2) is being used. However, it is important to point out that the methods described in this paper are general enough to be applied to list-wise loss functions as well.
The data set U in Eq 3 is the training data used to learn the scoring function f ( x ). There are two commonly used approaches to obtain relevance estimates for U . One way is to sample a set of queries and ask human raters to ex-plicitly judge the relevance of the retrieved documents. The other way is to collect implicit relevance judgments such as click-through data. The click-through data approach has at-tracted the attention of the research community [21], as it is much cheaper to obtain than human-judged data, especially for major search engines. However, as we mentioned before, click data is biased and very noisy. For example, because of position bias , simple click counts can not be used directly to estimate relevance. A great deal of previous work (see Section 2) focuses on overcoming such bias to infer actual (or unbiased) relevance. Our focus is on the more general selection bias problem that arises when using click-through data to train learning-to-rank models.
 Observation 1 When using click-through data for learning-to-rank, queries without clicks provide no useful information when optimizing pair-wise loss functions. Figure 1: An illustration of selection bias in click data. The shaded documents are the relevant ones.
 A check mark means the document is clicked.

For example, consider Eq 2. When there are no clicks for query Q , the set x i Q x j is empty since there is no way to derive preferences between any pairs of documents. Such an observation can be generalized to list-wise loss functions as well. In the following, we focus on the collection of queries with clicks and use S to denote this collection.
 Observation 2 The collection of queries S is biased. For-mally, let  X  P ( Q ) denote the probability mass of query Q in S , then  X  P ( Q ) 6 = P ( Q ) .
 We use an example to better explain this observation. In Figure 1, we have two queries Q 1 and Q 2 that both have equal probability of being issued by users, i.e., P ( Q P ( Q 2 ) as they have equal probability in U . The relevant document for Q 1 is at position 1 and is clicked every time the query is issued. On the contrary, the relevant document for Q 2 is at position 2 and is clicked half of the time when the query is issued. Thus,  X  P ( Q 2 ) = 1 2  X  P ( Q 1 ) in S , which helps illustrate how selection bias may arise in click data. The problem illustrated in this example is rooted at the com-monly known position bias and confirmed by eye tracking studies [22, 30] as well, which found that the users are less likely to see, and hence click on, lower-ranked documents.
Selection bias is a widely known problem in many other scientific communities, such as the health care field in which the problem arises in clinical trial studies [31]. Many meth-ods such as propensity matching, inverse propensity weight-ing, and doubly robust estimation have been applied in on-line settings [7] with the goal of comparing the effect of a treatment vs. its control (e.g., showing vs. not showing the ad). Some methods, including propensity matching, are specifically designed for comparison: given any individual in the treatment group, match it with another individual in the control group in the sense of their propensity scores being equal; the effect is obtained as the difference between the average of the treatment group and the matched individuals from the control group. It is not immediately clear how to adapt these methods to our use-case.

On the other hand, the inverse propensity weighting ap-proach can easily be adopted to help overcome selection bias for learning-to-rank. With inverse propensity weight-ing,  X  P ( Q ) is known as the propensity score of Q . Let w P ( Q ) /  X  P ( Q ), i.e. the ratio between the probability of Q ap-pearing in U and the probability that Q actually appears in S . Then the empirical loss function becomes: To the best of our knowledge, our work is the first to gen-erally study selection bias to improve the effectiveness of learning-to-rank models. The problem of selection bias is es-pecially important in the scenario of personal search where the personalized nature of information needs strongly biases the available training data.

To apply selection bias in practice, the primary challenge becomes estimating the inverse propensity weights w Q . An open question is also whether such a weighting approach will have a significant impact on the effectiveness of learning-to-rank models. We address this challenge and answer this question in the following sections.
Before we describe different methods of inverse propensity weighting estimation, we briefly describe our application sce-nario and the data set used to quantify the bias.

Application Scenario. Our application is a search en-gine for one of the world X  X  largest commercial email and cloud file storage services. Given a query, the search en-gine provides instant results (i.e., the results refresh as the user types). These instant results provide an efficient way for the user to examine the results. There are up to n in-stant results with no pagination. The results are retrieved from a personal corpus (e.g. emails or cloud storage files) and therefore are generally unique to the user. Once the user detects a relevant result and clicks on it, the clicked document is immediately opened in the browser.

Our click data is obtained exclusively from the instant results. Therefore, for each issued query, there will be either no click or exactly one click. In the rest of the paper, we study the selection bias problem in this setting. While the methods presented here can easily be extended to the web search setting, it is beyond the scope of the current study.
Result Randomization. In order to quantify the posi-tion bias, which will be used for inverse propensity weighting estimation, we employ result randomization and collect user click data on the randomized result sets. Specifically, given a ranked result list of n documents returned for some query, instead of showing the original list, we permute the results uniformly at random and present the shuffled list to a small fraction of end users. We denote the collected randomized data by R . As a special case, when n = 2, the randomization reduces to the previously proposed FairPairs algorithm [11, 33].

In the rest of this section, we present different methods to quantify the selection bias using the collected randomized data.
The global bias model can be viewed as the standard po-sition bias model [11, 30]. It assumes that the bias is a func-tion of the position within the ranked list itself. Formally, let c Q x i denote the probability of receiving a click when a document x is shown at position i for query Q , r Q x be the probability of relevance of x to Q , and b i be the bias at posi-tion i (meaning how likely a user is to examine the document Figure 2: The position bias propensity scores for user emails and cloud storage files. at this position), then it follows that: Our goal is to estimate b i for 1  X  i  X  n . Given query Q , P ( x | Q,i ) denotes the probability of showing result x  X  Q at position i . In the randomized data, the probability of showing a given result x  X  Q is the same for all positions, i.e., P ( x | Q,i 1 ) = P ( x | Q,i 2 ) for all 1  X  i given Q , for all 1  X  i 1 ,i 2  X  n : Hence, b = which is the total number of clicks at position i in the ran-domized data R . In our application, every query in S has a single click. Let i be the clicked position for Q . Then b is proportional to the ratio between the probability of Q appearing in S and the probability of Q in the uniformly sampled collection U , i.e.,  X  P ( Q ) /P ( Q ), and thus
We now show some empirical data for the global bias model. We ran our randomization experiments for two doc-ument corpora (user emails and cloud storage files) and col-lected their data. We set n = 4, normalize all b i P i b i = 1, and plot the normalized b i values in Figure 2. As the results show, there are clearly position biases in both the email corpus and the cloud file storage corpus. For exam-ple, b 1 in the email corpus is about 0 . 40 but b 4 is about 0 . 15, confirming the strong bias of clicking top positions. More interestingly, Figure 2 also shows that the emails and the cloud storage files have very different bias values: the bias for files is much more flat than that of the emails.
The global bias model quantifies the position bias solely based on the clicked position, which is a rather coarse esti-mate. Most of past work does not go beyond this. However, motivated by the bias difference shown in Figure 2, it is possible that even within a single document corpus (e.g., email), different segments of queries have different position biases. We thus propose a more fine-grained model called the segmented bias model .

The basic idea of the segmented bias model is to parti-tion queries into a few segments and then apply the global position bias model separately within each segment. Thus, we have a specific bias model for each segment. There may be multiple application-specific ways of segmenting queries such as using a query classifier. In our application, we focus on the email corpus from now on, and rely on the categories or labels assigned to each email. There are several such la-bels available in our corpus such as Promotional or Social . Each email can be associated with multiple labels 1 . There are multiple emails in the result list for each query and our goal is to select a single label and treat it as the segment for the query.

Inspired by the inverse document frequency (IDF) metric, we compute the inverse query frequency (IQF) for each label. For a label t , where t  X  Q means that label t is attached to some email retrieved for query Q and |{ Q : t  X  Q }| is the total number of queries that have label t attached in the randomized data. Then the label t ( Q ) of a query Q is: A by-product is that this creates segments with more bal-anced size. Given all the queries labelled by t , we can then estimate the position bias b t i for this segment. Thus, for query Q , its inverse propensity weighting becomes: where i is the clicked position of Q . Though simple, we find that this method is quite effective in our experiments.
The segmented model goes a step further to model the bias in a more fine-grained manner. A natural question is how to generalize this even further. For example, is it possible to have a query-dependent bias model? In other words, each query can potentially have different position bi-ases. Due to the large number of unique queries, such a formulation seems intractable. However, as we show in this section, result randomization makes it possible to formulate a generalized query-dependent bias model.

Specifically, to estimate the position bias for query Q , sup-pose that we can present a randomly shuffled list of docu-ments every time the query is issued and that user clicks are independent. A similar approach to the global bias model can be applied on the randomized data specifically for query Q . However, this is not practically feasible for the following reasons. First, to be able to accurately estimate the position bias for the query Q , hundreds or thousands of data points
The labeling algorithm itself is out of the scope of the pa-per, but the reader may refer to Grbovic et al. [16] or Bekker-man [2] for some prior work on the subject. are needed. This means most queries will be filtered out because of data sparseness. Second, in the private search scenario, as discussed here, documents are unique to an in-dividual user. Even for the same query string, the retrieved documents will differ across users. In order to collect suffi-cient data for a query, we need to show different randomized results for the same user from time to time. This will not only annoy the users, but also miss the purpose of data ran-domization since the data independence assumption will be violated. Thus, the challenge is how to tackle the following prediction problem.
 Definition 1 (Position Bias Prediction) Given a query Q = ( q, { x 1 ,..., x n } ) , the problem of Position Bias Predic-tion is to estimate the click probability at each position i (1  X  i  X  n ) if we show the set of documents in a uniformly random order, specifically for Q .

Recall that we have a set of queries in the randomized data. To solve the problem above, we propose a learning-based approach using multi-class logistic regression. We have n positions and thus n classes in the regression. For each query, we seek to estimate the probability of the query belonging to each class. Thus, we construct the training data from the randomized data and describe our approach as follows:
We call the above the generalized bias model . Indeed, both global and segmented models are special cases in this generalized bias model.
 Proposition 1 When the feature vector for each query has a single constant element 1 , the generalized bias model is reduced to the global bias model. Proof Sketch . Let c i denote the total number of clicks on position i and C = P i c i . When the feature vector for each query has a single constant element 1, b Q i only depends on i and the log likelihood of b i in Eq 4 is which is maximized when b i = c i /C .
 Proposition 2 The segmented bias model becomes a spe-cial case of the generalized bias model when we construct the feature vectors as follows. We create a binary feature for each segment, and a query has a value 1 in the feature corresponding to its segment and 0 elsewhere.
 Proof Sketch . The feature vector defined for the segmented model can partition the queries in the same way as the seg-mented model. The log likelihood of the whole data set can be separated into a few individual components, with each corresponding to a segment. Each component will be maxi-mized similarly to Proposition 1 and thus the yielded biases are the same as the segmented bias model.

As noted before, the generalized bias model is flexible to take any types of features such as query-specific or user-specific features. In our experiments we use a simple yet effective set of query length and query segment features de-scribed in Table 1. In Section 5, we will report some empiri-cal results on the effectiveness of the generalized bias model based on these features.
In this section, we conduct experiments to compare differ-ent position bias prediction methods. All the methods that we compare are summarized in Table 2. In this table, No-Correction means learning a scoring function without taking selection bias into account. It serves as the baseline to com-pare the other methods against. In the following, we first describe the experimental design and then present the ex-perimental results.
We use two data sets in this paper: regular click data and randomized click data. Feature Type Description
Query length Binary indicator based on the buck-
Segment Binary indicator based on category seg-Name Method Description
NoCorrection No bias correction is applied. This
Global The bias is estimated for each position
Segmented The bias is estimated for each position
Generalized The bias is estimated for each position
Table 2: List of position bias prediction methods.
Our learning-to-rank algorithm is an adaptive one, in which we build a new model on top of the existing score. This is different from the standard approach, which learns a scor-ing function using the entire set of features. Instead, in the adaptive approach, we aim to train the adjustment  X  ( x ) over the base score s ( x ). The final scoring function becomes: We use the following ranking features to learn the adjust-ment  X  ( x ): This yields tens of ranking features. Although this may seem like a small number of features, the base score s ( x ) is already highly optimized and includes hundreds of different features. The email category and user interaction features considered here add some additional information that is somewhat or-thogonal to those used to compute the base score. For the NoCorrection baseline (see Table 2), we train  X  ( x ) with-out applying any bias correction (i.e. w Q = 1), and apply the respective selection bias weights during training for the Global, Segmented and Generalized models.

The additive nature in our adaptive model naturally fits the Multiple Additive Regression Trees (MART) learning algorithm [18]. In every iteration, MART trains a new tree to be added to the existing list of trees. In our setting, we start with our base score s ( x ) and then train additive trees over it.
In this section, we evaluate the position bias prediction models in a couple of different settings. Among them, the
Mean 4.0 3.7360 3.7337 3.7336 95% CI /  X  0.0202  X  0.0201  X  0.0197 Table 3: Perplexity on the randomized data with 95% Confidence Interval. online experiments serve as the ultimate ground truth, but are expensive because they need to run against live traffic. We thus explore cheap offline evaluation methodologies and discuss their strengths and weaknesses.
The position bias prediction problem can be treated as a standard prediction problem and thus can be evaluated using techniques like cross-validation. We split the random-ized data into 10 folds and use the leave-one-out strategy to evaluate the different prediction methods. For each query, a prediction method gives a distribution over all the positions. We can thus use perplexity as the evaluation metric, which is defined as: where N is the total number of observations in the test data and p o is the probability of observation o as predicted by the model to be evaluated. Perplexity measures how well a distribution predicts samples and is often used to evaluate or compare language models [3]. It is also used in recent work to compare click models [15]. In our case, each sample corresponds to a click at a position in the test data, and p is the predicted bias probability for that sample. A lower perplexity score means the model is better at predicting the observations.

In Table 3, we show the perplexity score and the 95% con-fidence intervals based on the cross-validation. In this table, Uniform is a non-informative baseline and corresponds to the uniform prediction that gives a 25% for each of the 4 positions. From this table, we can see that all the position bias methods outperform Uniform significantly. For exam-ple, the 95% confidence interval of Global is [3 . 7158 , 3 . 7562]. The Uniform perplexity 4 . 0 is outside of this range and thus the difference is significant. Comparing across all the bias prediction methods, we can see that Generalized achieves the best score and Segmented is very close to Generalized. However, the difference among all the 3 methods showed in the table are not significant.

Perplexity is an intrinsic measure of a given method X  X  pre-diction accuracy. However, from the practical perspective we are much more interested in extrinsic evaluations of how models trained using each approach perform in terms of re-trieval effectiveness. In the following, we examine different options for directly evaluating search quality.
The obvious way of evaluating the ranking quality offline is to apply our learnt scoring function to a held-out test data set. However, as we will show, such a method is not well-grounded. Let us consider the evaluation metric Mean Reciprocal Rank (MRR), which is defined as follows: Table 4: Offline evaluation on the regular data based on the weighted MRR. The number in this table is normalized by the smallest MRR. where rank Q is the rank of the first clicked document of query Q . In the context of selection bias, we need to incor-porate the bias correction w Q into the metric as well and thus get the weighted MRR:
Different position bias prediction models give rise to dif-ferent w Q values in Eq 7 and thus the MRR metric is not comparable across different w Q weights. This means that even for the same data set and the same scoring function, we may get different MRR values. To illustrate this, Table 4 reports the weighted MRR on the test data set of the regu-lar data. We normalize the raw MRR values by the smallest value in the table. The row denoted by NoCorrection corre-sponds to the evaluation results on the same data set with the same scoring function. Clearly, the table shows differ-ent values when different w Q are used in the MRR metric. Though we can compute the relative improvement of differ-ent position bias correction models over the NoCorrection model, using the MRR with the corresponding w Q , such improvement numbers are still not grounded as a reliable comparison metric. By changing the weight w Q , one can ar-tificially manipulate the improvement. For example, a high weight w Q can be given to queries where the new model has a higher MRR.
In this section, we address the challenge of offline evalu-ation by proposing a novel unbiased offline evaluator based on the randomized data. Such a method directly evaluates the ranking quality and thus is more practically useful than perplexity. Furthermore, it is theoretically sound, unbiased, and can overcome the comparability issues that arise when using the regular data for offline evaluations.

Unbiased offline evaluators have been studied extensively in the setting of contextual-bandit problems [12, 25]. We adapt this strategy to our problem setting. Our proposed algorithm is detailed in Algorithm 1. This algorithm goes through every query in the randomized data set R and se-lects a matched subset R s based on the provided scoring function f ( x ). The matching condition is that the ranking recorded in our log data R is the same as that ranked by f ( x ) for the top k documents. The metric value is then computed on the selected subset R s .
 Theorem 1 Given uniformly randomized data R , Algorithm 1 gives an unbiased estimate of any metric M for any scoring function f ( x ) .
 Proof : This is a simplified version of Theorem 1 in [25] in that we have a static scoring function. We only prove the Algorithm 1 Offline Evaluator Input : scoring function f ; randomized data R ; evaluation metric M on top k : M k .
 Output : evaluation value of f . 1: Set matched data collection R s : =  X  2: for Q = ( q,  X  x 1 ,..., x n  X  ) in R do 3: Let  X  x j 1 ,..., x j n  X  be  X  x 1 ,..., x n  X  re-ranked by f 4: if  X  j 1 ,...,j k  X  =  X  1 ,...,k  X  then 5: R s : = R s  X  Q 6: end if 7: end for 8: return M k ( R s ) case when k = n . Our goal is to show that R s is an unbi-ased sample of events if we use f ( x ) as the scoring function. Since all the rankings in R s are the same as ranked by f ( x ), we only need to prove that the marginal probability on the query string itself, P ( q ), in R s is unbiased.
 where q  X  Q means q is the query string of Q . This is the case because P ( q ) in R is unbiased since we collect all the data and the probability of entering the if condition in Algorithm 1 is 1 n ! for all the queries.

One caveat of the above algorithm is that we only use queries with exactly n results. For queries with j &lt; n re-sults, we can weigh them by j ! n ! when estimating the metric.
The full procedure for evaluating a position bias prediction method is as follows:
Using NoCorrection as the baseline, we report the relative improvement of different bias prediction models in Table 5. In this table, we show the MRR in Eq 6 evaluated on the top k results along with the size of R s . The results show that all position bias prediction methods outperform the NoCor-rection baseline. For the position bias prediction methods, both Segmented and Generalized are better than Global, and Segmented is slightly better than Generalized, demon-strating the potential utility of advanced position bias mod-els. However, the differences are not statistically significant, due to the high variance incurred by the small evaluation data set. For k , the expected size of R s is about ( n  X  k )! the size of R . A larger data set could be used to increase the statistical power of our proposed methods.
Online experiments are the ultimate litmus tests to eval-uate different scoring functions. Our online experiments are in the form of A/B testing: we allocate a fraction of live traffic for each experiment. One half of the fraction is used Table 5: Comparison of different position bias pre-diction methods using the unbiased offline evalua-tor. We report the relative improvement over the NoCorrection baseline.
 Table 6: Comparison of different bias prediction methods using online experiments. We report the relative improvement over the NoCorrection base-line. Notation *, ** and *** means the difference is significant at level 0.1, 0.05 and 0.01 respectively. as control (i.e., the NoCorrection model, as described in Sec-tion 5.1.2) and the other half is used as treatment (i.e., the one of the bias prediction methods). We create an online experiment for each of the methods listed in Table 2. We ran all four online experiments for a period of one week, col-lecting millions of clicks per experiment. Based on the click data, we compare the treatment and the control by com-puting the relative improvement in terms of our evaluation metric MRR (mean reciprocal rank of a click) in Eq 6. We also compute the relative improvement between two treat-ment methods using NoCorrection as the calibrator.
 Table 6 summarizes the results of our online experiments. In this table, we report the relative improvement in MRR compared with the NoCorrection and Global baselines. For the NoCorrection baseline, we can see that all our position bias methods yield statistically significant improvements at the 0 . 01 level. This confirms that selection bias in click data is significant and overcoming it can lead to significant quality improvement. From Global baseline, we can see that more fine-grained position bias models are capable of further improving our metric. For example, Segmented outperforms Global significantly at 0 . 1 level.

For the online experiments, we can also report the click-through rate (CTR). The CTR metric reflects how attractive the result section is as a whole. We also report the relative improvement in terms of CTR in Table 6. Our observations for the CTR metric parallel those for the MRR metric  X  the Global baseline significantly increases CTR, and Seg-mented and Generalized methods provide further improve-ments. Furthermore, Segmented model achieves significant improvement over Global at 0 . 05 level.

The results of the online experiments do not indicate a significant difference between Segmented and Generalized models, even though both outperform the Global baseline. As we showed before, Segmented is a special case of General-Figure 3: The importance of query length varied with positions. ized when only the segmented features are used, suggesting the usefulness of the category segment feature, and the im-portance of feature engineering for further improvement of the generalized bias model.
The generalized bias model not only provides a flexible way for position bias prediction, but also enables us to un-derstand the impact of different features in terms of their usefulness in modeling position bias. In this section, we an-alyze the features to distill additional insights.

We have two types of features in the regression models: segment features and query length features. The question is which group of features is more predictive. To answer this question, we compare 3 generalized models with dif-ferent sets of features and observe the following changes in perplexity (as defined in Eq 5): segment and query length features (3 . 7336), segment features only (3 . 7337), and query length features only (3 . 7358). Clearly we can see that seg-ment features are more useful than query length features in reducing the perplexity.

Furthermore, we can observe the impact of different lengths of query features. Our query lengths are bucketed as in Ta-ble 1 with larger bucket IDs corresponding to longer queries. For each position i and each query length bucket j , we have  X  ij corresponding to the coefficient in the logistic regres-length j to the odd of click on position i : b i / (1  X  b other words, the odds of a click at position 1 decrease when the query is longer. In contrast, for position i = 4, e  X  becomes larger when j becomes larger, which means the odds of a click at position 4 increase as the query becomes longer. This makes sense intuitively, since when queries are longer, users have more refined needs and the position bias becomes flatter. This means that the users are more willing to examine the lower-ranked documents.
In this paper, we studied the problem of learning-to-rank with selection bias for personal search. We discussed the infeasibility of using existing click models in personal search and proposed a novel approach to overcome the inherent se-lection bias for this application. We proposed a few methods to estimate the selection bias and addressed it using inverse propensity weighting. In addition, we study offline and on-line evaluation methodologies and also propose a novel unbi-ased offline evaluator. Through extensive offline and online experiments, we show that the proposed methods for mod-eling selection bias can significantly improve the quality of learning-to-rank models that use click data for training.
There are a few interesting lines of future work. (1) We evaluate our methods in the context of personal search, but it would be interesting to see how applicable they are to web search. (2) Our experiments use queries with a single click. It would be interesting to extend the framework to the search scenarios that allow multiple clicks per query. (3) Given a different application, such as cloud storage files, what are the effective features in bias estimation? This could inspire lots of interesting feature engineering work in the research community. (4) The expensive part of our method is the dependency on randomized data. How to collect random-ized data in a cheaper, less-intrusive manner is also worth studying. Furthermore, how to adapt our offline evaluator to improve its data utilization is also an interesting research problem. [1] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and [2] R. Bekkerman. Automatic categorization of email into [3] P. F. Brown, V. J. D. Pietra, R. L. Mercer, S. A. D. [4] C. J. Burges. From RankNet to LambdaRank to [5] S. B  X  uttcher, C. L. A. Clarke, P. C. K. Yeung, and [6] D. Carmel, G. Halawi, L. Lewin-Eytan, Y. Maarek, [7] D. Chan, R. Ge, O. Gershony, T. Hesterberg, and [8] O. Chapelle and Y. Zhang. A dynamic bayesian [9] W. Chen, Z. Ji, S. Shen, and Q. Yang. A whole page [10] A. Chuklin, I. Markov, and M. de Rijke. Click Models [11] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [12] M. Dud  X  X k, J. Langford, and L. Li. Doubly robust [13] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, [14] G. Dupret and C. Liao. A model to estimate intrinsic [15] G. E. Dupret and B. Piwowarski. A user browsing [16] M. Grbovic, G. Halawi, Z. Karnin, and Y. Maarek. [17] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, [18] T. Hastie, R. Tibshirani, and J. Friedman. The [19] K. Hofmann, S. Whiteson, and M. de Rijke. Balancing [20] J. Huang, A. Gretton, K. M. Borgwardt, B. Sch  X  olkopf, [21] T. Joachims. Optimizing search engines using [22] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [23] M. Kamvar, M. Kellar, R. Patel, and Y. Xu.
 [24] L. Li, S. Chen, J. Kleban, and A. Gupta.
 [25] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased [26] T.-Y. Liu. Learning to rank for information retrieval. [27] T. Minka and S. Robertson. Selection bias in the [28] M. O X  X rien and M. T. Keane. Modeling result X  X ist [29] T. Qin, T.-Y. Liu, J. Xu, and H. Li. Letor: A [30] M. Richardson, E. Dominowska, and R. Ragno.
 [31] P. R. Rosenbaum and D. B. Rubin. The central role of [32] A. Swaminathan and T. Joachims. Counterfactual risk [33] Y. Yue, R. Patel, and H. Roehrig. Beyond position [34] B. Zadrozny. Learning and evaluating classifiers under [35] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression [36] Z. A. Zhu, W. Chen, T. Minka, C. Zhu, and Z. Chen.
