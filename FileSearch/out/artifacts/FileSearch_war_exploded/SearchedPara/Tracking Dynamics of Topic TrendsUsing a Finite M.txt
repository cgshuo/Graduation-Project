 In a wide range of business areas dealing with text data streams, including CRM, knowledge management, and Web monitoring services, it is an important issue to discover topic trends and analyze their dynamics in real-time .Specifi-cally we consider the following three tasks in topic trend analysis: 1) Topic Structure Identification ; identifying what kinds of main topics exist and how important they are, 2) Topic Emergence Detection ; detecting the emergence of a new topic and recognizing how it grows, 3) Topic Characteri-zation ; identifying the characteristics for each of main topics. For real topic analysis systems, we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way, and be dealt with in a single framework. This paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically .In this framework we pro-pose the usage of a time-stamp based discounting learning algorithm in order to realize real-time topic structure iden-tification .This enables tracking the topic structure adap-tively by forgetting out-of-date statistics .Further we apply the theory of dynamic model selection to detecting changes of main components in the finite mixture model in order to realize topic emergence detection .We demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamics of topic trends in a timely fashion.
 H.2.8 [ Database Management ]: Database Applications -Data Mining topic analysis, model selection, CRM, text mining
In a wide range of business areas dealing with text streams, including CRM, knowledge management, and Web monitor-ing services, it is an important issue to discover topic trends and analyze their dynamics in real-time .For ex amp le, it is desired in the CRM area to grasp a new trend of topics in customers X  claims every day and to track a new topic as soon as it emerges .A topic is here defined as a seminal event or activity .Specifically we consider the following three tasks in topic analysis: 1) Topic Structure Identification ;learninga topic struc-ture in a text stream, in other words, identifying what kinds of main topics exist and how important they are. 2) Topic Emergence Detection ; detecting the emergence of a new topic and recognizing how rapidly it grows, similarly, detecting the disappearance of an existing topic. 3) Topic Characterization ; identifying the characteristics for each of main topics.

For real topic analysis systems, we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way, and be dealt with in a single framework.
The main purpose of this paper is to propose a new topic analysis framework that satisfies the requirement as above, and to demonstrate its effectiveness through its experimen-tal evaluations for real data sets.

Our framework is designed from a unifying viewpoint that a topic structure in a text stream is modeled using a finite mixture model (a model of the form of a weighted average of a number of probabilistic models) and that any change of a topic trend is tracked by learning the finite mixture model dynamically .Here each topic corresponds to a single mixture component in the model.

All of the tasks 1)-3) are formalized in terms of a finite mixture model as follows: As for the task 1), the topic struc-ture is identified by statistical parameters of a finite mix-ture model .They are learned using our original time-stamp based discounting learning algorithm , which incrementally and adaptively estimates statistical parameters of the model by gradually forgetting out-of-date statistics, making use of time-stamps of data .This makes the learning procedure adaptive to changes of the nature of text streams.

As for the task 2), any change of a topic structure is rec-ognized by tracking the change of main components in a mixture model .We apply the theory of dynamic model se-lection [7] to detecting changes of the optimal number of main components and their organization in the finite mix-ture model .We may recognize that a new topic has emerged if a new mixture component is detected in the model and re-mains for a while .Unlike conventional approaches to statis-tical model selection under the stationary environment, dy-namic model selection is performed under the non-stationary one in which the optimal model may change over time .Fur-ther note that we deal with a complicated situation where the dimension of input data, i.e., the number of features of a text vector, may increase as time goes by.

As for the task 3), we classify every text into the cluster for which the posterior probability is largest, and then we characterize each topic using feature terms characterizing texts classified into its corresponding cluster .These feature terms are extracted as those of highest information gain, which are computed in real-time.

We demonstrate the validity of the topic trend analysis framework, by showing experimental results on its applica-tions to real domains .Specifically we emphasize that it is really effective for discovering trends in questions at a help desk.
The technologies similar to 1)-3) have extensively been ex-plored in the area of topic detection and tracking (TDT) (see [1]) .Actually 1) and 2) are closely related to the subprob-lems in TDT called topic tracking and new event detection , respectively .Here topic tracking is to classify texts into one of topics specified by a user, while new event detection, for-merly called first story detection , is to identify texts that discuss a topic that has not already been reported in earlier texts .The latter problem is also related to work on topic-conditioned novelty detection by Yang et.al.[16]. In most of related TDT works, however, topic tracking or new event detection is conducted without identifying main topics or a topic structure, hence the tasks 1)-3) cannot be unified within a conventional TDT framework .Further topic time-line analysis has not been addressed in it.

Swan and Allen [12] addressed the issue of how to auto-matically overview timelines of a set of news stories .They used the  X  2 -method to identify at each time a burst of fea-ture terms that more frequently appear than at other times. Similar issues are addressed in the visualization commu-nity [3] .However, all of the methods proposed there are not designed to perform in an on-line fashion.

Kleinberg [4] proposed a formal model of  X  X ursts of ac-tivity X  using an infinite-state automaton .This is closely related to topic emergence detection in our framework .A burst has a somewhat different meaning from a topic in the sense that the former is a series of texts including a specific feature, while the latter is a cluster of categorized texts .Hence topic structure identification and characteriza-tion cannot be dealt with in his model .Further note that Kleinberg X  X  model is not designed for real-time analysis but for retrospective one.
 Related to our statistical modeling of a topic structure, Liu et.al. [2] and Li and Yamanishi [6] also proposed meth-ods for topic analysis using a finite mixture model .Specif-ically, Liu et.al. considered the problem of selecting the optimal number of mixture components in the context of text clustering .In their approach a single model is selected as an optimal model under the assumption that the opti-mal model does not change over time .Meanwhile, in our approach, a sequence of optimal models is selected dynam-ically under the assumption that the optimal model may change over time.

Related to topic emergence detection, Matsunaga and Ya-manishi [7] proposed a basic method of dynamic model se-lection, by which one can dynamically track the change of number of components in the mixture model .However, any of all of these technologies cannot straightforwardly be ap-plied to real-time topic analysis in which the dimension of data may increase as time goes by.

Related to topic structure identification, an on-line dis-counting learning algorithm for estimating parameters in a finite mixture model has been proposed by Yamanishi et. al .[14] . The main difference between our algorithm and theirs is that the former makes use of time-stamps in order to make the topic structure affected by a timeline of top-ics while the latter considers only the time-order of data ignoring their time-stamps.

The rest of this paper is organized as follows: Section 2 describes a basic model of topic structure .Section 3 gives a method for topic structure identification .Section 4 gives a method for topic emergence detection .Section 5 gives a method for topic characterization .Section 6 gives experi-mental results .Section 7 gives concluding remarks .
We employ a probabilistic model called a finite mixture model for the representation of topic generation in a text stream .Let W = { w 1 ,  X  X  X  ,w d } be the complete vocabulary set of the document corpus after the stop-words removal and words stemming operations .For a given document x , let tf ( w i ) be the term frequency of word w i in x .Let idf ( w betheidfvalueof w i ,i.e., idf ( w i )=log( N/df ( w i )) where N is the total number of texts for reference and df ( w the frequency of texts in which w i appears .Let tf -idf ( w be the tf-idf value of w i in x ,i.e., tf -idf ( w i log( N/df ( w i )) . We may represent a text x of the form: or We may use either type of the representation forms.
Let K be a given positive integer representing the num-ber of different topics .We suppose that a text has only one topic and a text having the i -th topic is distributed according to the probability distribution with a density: p ( x |  X  i )( i =1 , 2 ,  X  X  X  ,K ), where  X  i is a real-valued parame-ter vector .We suppose here that x is distributed according to a finite mixture distribution (see e.g., [8]) with K compo-nents given by where  X  i &gt; 0( i =1 ,  X  X  X  ,K )and  X  =(  X  1 ,  X  X  X  , X  K  X  1 , X  1 ,  X  X  X  , X  K ) .Here  X  i denotes the degree to what the i -th topic is likely to appear in a text stream. Note that each component in the mixture defines a single cluster in the sense of soft-clustering.

Throughout this paper we suppose that each p i ( x |  X  i )takes a form of a Gaussian density: Letting d be the dimension of each datum, p ( x |  X  i )=  X  i ( x |  X  i ,  X  i )(2) where  X  i is a d -dimensional real-valued vector,  X  i is a d dimensional matrix, and we set  X  i =(  X  i ,  X  i ) .In this case (1) is so-called a Gaussian mixture .Note that a Gaussian density may be replaced with any other form of probability distributions, such as a multinomial distribution.

In terms of a finite mixture model, a topic structure is identified by A) the number of components K (how many topics exist), B) the weight vector (  X  1 ,  X  X  X  , X  K ) indicating how likely each topic appears, and C) the parameter values  X  ( i =1 ,  X  X  X  ,K ) indicating how each topic is distributed .A topic structure in a text stream must be learned in an on-line fashion .Topic emergence detection is conducted by track-ing the change of main components in the mixture model. Topic characterization is conducted by classifying each text into the component for which the posterior is largest and then by extracting feature terms characterizing the classi-fied texts .Topic drift may be detected by tracking changes of a parameter value  X  i for each topic i .These tasks will be described in details in the sessions to follow.
 The overall flow of the tasks is illustrated in Figure 1. A text is sequentially input to the system .We prepare a number of finite mixture models, for each of which we learn statistical parameters using the time-stamp based learning algorithm to perform topic identification .These tasks are performed in parallel .On the basis of the input data and learned models, we conduct dynamic model selection for choosing the optimal finite mixture model .We then com-pare the new optimal model with the last one to conduct topic emergence detection .Finally for each component of the optimal model, we conduct topic characterization.
In this section we propose an algorithm for learning a topic structure, which we call an time-stamp based discounting topic learning algorithm .

The algorithm is basically designed as a variant of the in-cremental EM algorithm for learning a finite mixture model (see, e.g., Neal and Hinton [9]). Our proposed one is distinguished from existing ones with regards to the following three main features: 1) Adaptive to the change of the topic structure. The pa-rameters are updated by forgetting out-of-date statistics as time goes on .This is realized by putting a larger weight to the statistics for a more recent data. 2) Making use of time stamps for texts. Not only the time order of texts but also their time stamps are utilized to make the topic structure depend on the timeline .For example, for two text data x t 1 ,x t 2 ( t 1 &lt;t 2 ), if the length t thetopicstructurelearnedattime t 2 will be less affected by that at time t 1 . 3) Normalizing data of different dimensions. We consider the on-line situation where the dimension of a datum may increase as time goes by .This situation actually occurs be-cause new words may possibly be added to the list of words every time a new text is input .Hence it is needed for nor-malizing data of different dimensions.

We suppose that text data x 1 ,x 2 , ... are given in this order, and each has a time-stamp indicating when it ap-peared .Here is a description of the algorithm, in which  X  is a discounting parameter,  X  i denotes the posterior density of the i th component, and m is introduced for calculation of weights for old statistics.
 Time-stamp Based Discounting Learning Algorithm Initialization: Set initial values of  X  (0) i , X  (0) i ,  X  (0) i ,m (0) ( i =1 ,  X &gt; 0 , 0 &lt; X &lt; 1begiven.
 Iteration: For t =1 , 2 ,.. do the following procedure.
 For th e t -th data be x t and its time stamp be t new .Let the time stamp of the ( t  X  1)-th data be t old .
 For i =1 ,  X  X  X  ,k , update the parameters according to the following rules: p ( i | x t ):= where WA denotes the operation such that
Generally, we set the initial value  X  (0) i =1 /K, m (0) =0,a each other .This algorithm updates  X  i ,  X  i ,and X  i as the weighted average of the latest parameter value and the new statistics .The weight ratio is m ( t  X  1) :  X   X  ( t new
Note that Yamanishi et.al. X  X  sequentially discounting learn-ing algorithm [14] can be thought of as a special case of this algorithm in which the time interval t l +1  X  t l is independent of l .In that case if we further let  X  = 1, the algorithm becomes an ordinary incremental EM algorithm.

In real implementation, we supposed that  X  i is a diagonal matrix for the sake of computational complexity issues .The scalability issue for dealing with a general matrix  X  i remains for future study.
In this section we are concerned with the issue of topic emergence detection , i.e., tracking the emergence of a new topic .We reduce here this issue to that of selecting the optimal components in the mixture model dynamically .We call this statistical issue dynamic model selection (see also [7]).

The key idea of dynamic model selection is to first learn a finite mixture model with a relatively large number of com-ponents, then to select main components dynamically from among them on the basis of Rissanen X  X  predictive stochastic complexity [10].

The procedure of dynamic model selection is described as follows: Initialization: Let K max (maximum number of mixture components) and W (window size) be given positive integers.
 Iteration: For t =1 , 2 ,  X  X  X  , do the following procedure 1 to 4: 1. Model Class Construction: Let G t i be the window average of the posterior probability (  X  procedure: Let 1 ,  X  X  X  , k be the indices of k highest scores such that G model with k components: For s = t  X  W,  X  X  X  ,t, where U is a uniform distribution over the domain. 2. Predictive Stochastic Complexity Calculation: When the t -th input data x t with dimension d t is given, compute 3. Model Selection: Select k  X  t minimizing S ( t ) ( k ) .Let p j ( x |  X  ( t be main components at time t ,whichwewriteas { C ( t ) 1 , 4. Estimation of Parameters: Learn a finite mixture model with K max components using the time-stamp based discounting learning algorithm .Let the estimated parameter be (  X  ( t ) 1 ,  X  X  X  , X  ( t ) K Note that the S ( t ) ( k ) can be thought of as a variant of Rissanen X  X  predictive stochastic complexity [10] normalized by the dimension for each datum, which can be interpreted as the total code length required for encoding a data stream x  X  W , ..., x t into a binary string sequentially.

Once main components C ( t ) 1 ,  X  X  X  ,C ( t ) k  X  gence of a new topic or the disappearance of an existing topic in the following way .If a new component is selected at some point and remains for a longer time than a specified threshold, we may determine that a new topic has emerged. Specifically, if the optimal number k  X  t of components be-comes larger than k  X  t  X  1 , we can recognize that a new topic has emerged .Similarly, if an existing component is not se-lected at some time and does not appear any longer, then we may determine that the topic has disappeared.
Once the optimal finite mixture model is obtained, we are concerned with the issue of how to characterize each topic .We address this issue by extracting terms character-izing each topic and by observing the growth or decay of each topic component .Details are shown below .

A) Extracting terms characterizing each topic. We at-tempt to characterize each topic by extracting characteris-tic words for it .We perform this task by computing the information gain of possible words.

In the time-stamp based discounting topic learning algo-rithm, the posterior probability distribution over the set of clusters is estimated every time a text data is input .Ac-cording to that posterior distribution an input text will be categorized into the component for which the posterior prob-ability is largest .This clustering task can be performed in an on-line fashion.
 After observing the t -th datum x t ,for i =1 ,  X  X  X  ,k ,let S ( i )bethesetoftextsin x t = x 1 , .., x t classified into the i -th component and let t i be the size of S t ( i ) .Let
Below we show the method for computing the information gain of a term w for each topic component .For any term w ,let S ( w )beasetofvectorsin S t such that the frequency of w is larger than a given threshold, and let m w be the size of
S ( w ) .Let S (  X  w )beasetofvectorsin S t such that the frequency of w is not larger than the threshold, and m  X  thesizeof S (  X  w ).

For a specified topic component, say, the i -th component, let m + w be the number of vectors in S ( w )thatarealsoin-cluded in S t ( i ) .Let m +  X  w be the number of vectors in that are also included in S t ( i ).

Then we define the information gain of w for the i -th topic component as follows: where I ( x, y ) is the information measure such as stochastic complexity [10], extended stochastic complexity [13][5] .The stochastic complexity [10] is given as follows: where H ( x )=  X  x log x  X  (1  X  x )log(1  X  x )log(1  X  x )isthe binary entropy function, and log X  X  base is 2 .A special case of extended stochastic complexity is given as follows [13][5]: where c is a constant.

We select a specified number of terms w s of largest in-formation gains .We can think of them the set of terms characterizing the i -th topic component.

The statistics: m w ,m + w ,m  X  w ,m +  X  w needed for computing information gain can be calculated in an on-line fashion. Hence topic characterization task is conducted in real-time.
B) Observing the growth or decay of each cluster. Let G ( be the window average of the posterior probability of the i -th topic component, that is, G t i =(  X  t  X  W i + ,  X  X  X  G i increases when texts corresponding to the i -th topic is input, and decreases when the other is input .We can see how rapidly this topic grows by observing G ( t ) i as t goes by.
We conducted an experiment on real data: contact data of a help desk for an internal e-mail service .An example of the data record is presented in Table 1 .It has the field of contact date/time, question/request, answered date/time, answer, and so on .The number of the records is 1202 .The date of the first/last contact is Feb 21 2004/May 20 respectively.
We input contact dates as the time-stamps, and ques-tions/requests as the text data to our system .We set K max to 50 and  X  to 0 . 99 .Our system ran on an NEC Express5800 with 1GHz Pentium III and a 1GB memory .The system was implemented using C, and OS was Windows 2000 Server. Processing 1202 records of data took about five minute.
Figure 2 shows the number of components k  X  t selected by our system as main topics .The number increases at the beginning of March, and has a peak in the middle of April. Since a fiscal year begins at April in Japan, we can suppose that the number of topics at the help desk is increasing around the first day of April.

Let us look into a few of the components, because we do not have enough space for all of the components .Here, we observe Component 27 and 42 in detail .Figure 3 shows the window averages G 27 ,G 42 of the posterior probabilities and the periods where the components are selected as main top-ics. G 27 increases in the beginning of April and has the first peak at April 12 .Then it repeats increase and decrease until the middle of May .The corresponding component is selected as main from the first week of April, and remains as main until the middle of May (with short discontinuances). G 42 is positive during April, and also the corresponding topic is main during April .The lines of G i s indicate how important the corresponding topics are in each time .Moreover, we can observe how the emerged topics grows and disappears from the figure .The topic corresponding to Component 42 emerges at the beginning of April, grows for two weeks, is attenuated, then drops out from the main topics at the end of April.

Term  X  X ransfer X  was extracted as a characteristic word for Component 27 .Texts classified into this component are questions like  X  X s it possible to use Service XXX after I am transfered to YYY? X  .That kind of questions may increase around the beginning of a fiscal year . X  X ervice ZZZ X  and  X  X ailure X  were extracted as chracteristic words for Compo-nent 42 .Actually, Service ZZZ failed in the beginning of April, then, the topic consists of related complaints and questions.

In this way we can recognize the emergence, growth, and decay of each topic from the system .Through this example it has turned out that our framework for topic trend analysis are very effective for tracking dynamics of topic trends in contact data at a help desk.
In this paper we have proposed a framework for tracking dynamics of topic trends using a finite mixture model .In this framework the three main tasks: topic structure identi-fication, topic emergence detection, and topic characteriza-tion are unified within a single framework .Topic structure identification has been realized by our unique time-stamp based learning algorithm .It enables tracking topic struc-tures adaptively by forgetting out-of-date statistics .Topic emergence detection has been realized on the basis of the theory of dynamic model selection .It enables detecting changes of the optimal number of components in the finite mixture model to check whether a new topic has appeared or not .Topic characterization has been realized by on-line text clustering and feature extraction based on information gain .Through the experiments using real data collected at a help desk, it is demonstrated that our framework works well in the sense that dynamics of topic trends can be tracked in a timely fashion.
 The following issues remain open for future study:
Context-based topic trend analysis: In this paper we have proposed an approach to word-based topic trend analysis. However, we need to further analyze contexts , i.e., relations among words, in order to more deeply analyze the semantics of topics.

Multi-topics analysis: We supposed that one text comes from a single mixture component corresponding to a single topic .It is our future study how to deal with texts having multi topics. [1] J.Allen, R.Papka, and V.Lavrenko: On-line new [2] X.Liu, Y.Gong, W.Xu, and S.Zhu: Document [3] S.Harve, B.Hetzler, and L.Norwell: ThemeRiver: [4] J.Kleiberg: Bursty and hierarchical structure in [5] H.Li and K.Yamanishi: Text classification using [6] H.Li and K.Yamanishi: Topic analysis using a finite [7] Y.Matsunaga and K.Yamanishi: An [8] G.McLahlan and D.Peel: Finite Mixture Models , [9] R.M.Neal and G.E.Hinton: A view of the EM [10] J.Rissanen: Universal coding, information, and [11] R.Swan and J.Allen: Extracting significant [12] R.Swan and J.Allen: Automatic generation of [13] K.Yamanishi: A Decision-theoretic Extension of [14] K.Yamanishi, J.Takeuchi, G.Williams, and P.Milne: [15] Y.Yang, T.Pierce, J.G.Carbonell: A study on [16] Y.Yang, J.Zang, J.Carbonell, and C.Jin:
