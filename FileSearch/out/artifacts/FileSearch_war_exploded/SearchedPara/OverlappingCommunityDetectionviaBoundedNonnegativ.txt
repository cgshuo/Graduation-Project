 Complex networks are ub iquitous in our daily life, with the World Wide Web, social networks, and academic citation networks being some of the commo n examples. It is well und erstood that modeling a nd und erstand ing the network structure is of crucial importance to revealing the network functions. One important problem, known as commun ity de-tection, i s to detect and extract the comm un ity structure of networks. More re cently, the focus in this research topic has been switched to the detection of overlapp ing commun ities. In this paper, based on the matrix factorization app roach, we propose a method called bound ed nonn egative matrix tri-facto rizat ion (BNMTF). Using three facto rs in the fac-torization, we can explicitly model and learn the commun ity membership of each n ode as well as the interaction among commun ities. Based on a un ified formulation for both d i-rected and und irected networks, the optimization p roblem und erlying BNMTF can u se either the squared loss or the generalized KL-divergence as its loss function. In add ition, to a dd ress the sparsity problem as a result of missing edges, we also propose another setting in which the loss function is defined only on the observed ed ges. We report some experi-ments on real-world d atasets to demonstrate the superiority of BNMTF over other related matrix factorization methods. I.2.6 [ Artificial Intelli gence ]: Learning; H.2.8 [ D atabase Manag ement ]: Database App lications X  Data mining Algorithms Network Analysis, Commun ity Dete ction, NMF, BNMTF
Complex networks are ub iquitous in our daily life. For example, the World Wide Web, social networks, and aca-demic ci tation n etworks are some of the commonly encoun -tered ones. In these networks, links or edges connecting en-tities represent relations between them, such as hyperlinks between webpag es, friend relations between people, and ci-tations in academic pub licat ions. It is well und erstoo d that modeling a nd und erstand ing the network structure is of cru-cial importance to revealing the network functions. One important problem in und erstand ing the structure of a net-work is to detect and extract its modu lar structure con-sisting o f comm un ities, which is often known as comm un ity detection [7]. Although the problem is intuitively easy to un -derstand , what constitutes a commun ity in a network does not yet have a well-acc epted definition. In this paper, sim-ilar to the notion u sed in [7], we regard commun ities in a network as densely conn ected sub sets of vertices with a rel-atively high ratio o f the nu mber of intra-commun ity edges to the nu mber of inter-comm un ity edges.

Over the past decade, research on commun ity detection has mostly adopted the assumption that each n ode or ver-tex in a network belongs to one and only one commun ity. We refer to this as the non-overlapp ing commun ity detection problem [7, 13], which is similar in many ways to clustering in a lot of data mining a pp lications. Some of the popu -lar commun ity detection methods in this category make use of a quality measure called modu larity [4, 2] to evaluate the quality of the commun ity structure found , possibly by directly maximizing a n objective function which is based on the modu larity measure. This assumption is app arently too restrictive. In many real-world networks such as social networks, it is not un common to find entities belonging to multiple groups or commun ities. In other words, the com-mun ities are not disjoint bu t do o verlap. By relax ing the as-sumption, the commun ity detection p roblem becomes more general and we refer to it as the overlapp ing commun ity de-tection p roblem. The first method for this problem, called the clique percolation method (CPM), was proposed in [16 ]. CPM is based on the assumption that each commun ity is a un ion of adjacent k -cliques. It is an influ ential method with many extensions proposed sub sequently, e.g., [ 6, 17 ]. More recently, several methods have been developed based on matrix factorization. For example, nonn egative matrix facto rization (NMF) [10 ] has been app lied to develop some methods which d eliver promising performance [18, 19].
Even though the NMF-based methods in [18, 19 ] exhibit goo d p erformance on some overla pp ing comm un ity detec-tion appli cations, they do have drawbacks. For example, the method in [18] uses the conventional NMF model an d h ence the ph ysical meaning of the parameters is not very clear for the overlapp ing comm un ity detection p roblem, and the method in [19] has to use different formulations for directed and und irected n etworks. Moreover, methods in [18, 19 ] on-ly consider one loss function, i. e., generalized KL-divergence in [18] and squared loss in [19 ], bu t in app lications we still have no idea which loss function is a more suitable choice for the task at hand . In this paper, our goa l i s to overcome these drawbacks within the matrix factorization app roach. We propose a method called bound ed nonnegative matrix tri-facto rizat ion (BNMTF). Sp ecifically, from the matrix facto rization p erspective, we use tri-factorization, i. e., three-facto r facto rizat ion, i n the form UBU T , where U repre-sents the membership of each n ode in each comm un ity and B represents the interaction among a ll commun ities. Each entry in U corresp ond s to the probability that a node be-longs to a comm un ity and h ence its value is restricted to the range [0 , 1], i.e., U is bound ed. Obviously, the ph ysi-cal meaning o f U and B is very clear. We also note that the same tri-factorization form can b e used for both direct-ed and und irected networks. The only difference lies in B , which is required to be symmetric for und irected networks. Unlike the previous methods [18, 19], we consider both the squared loss and generalized KL-divergence as loss function-s. Moreover, we note that t here is intrinsic ambiguity when an edge is missing between two vertices. While the two ver-tices involved may ind eed be un related, it is also possible that t he relation b etween them has not been observed. In the same spirit as many methods for colla borative filtering, we also consider another setting in which the loss function is defined on the edges with nonzero weights only. We will report some experiments on real-world d atasets to demon-strate the superiority of BNMTF over existing NMF-based methods.
 Notations . Throughout t he paper, we use lowercase let-ters for scalars, bold lowercase lette rs for vectors, and b old upp ercase lette rs for matrices. We use tr( M ) to denote the trace of a square matrix M . For vecto r and matrix norms, we use k M k 1 to denote the l 1 norm of a matrix M , which is equal to the sum of the absolute values of all the elem ents of M , k M k F to denote the Frobenius norm of M , and k a k and k a k 2 to denote the l 1 and l 2 norms, respectively, of a vector a . To characterize a matrix, we write M  X  0 to mean that all elements in M are nonn egative, and M 1  X  (  X  ) M to denote e lementwise inequality between M 1 and M 2 . We use 0 and 1 to denote a vector or matrix consisting o f all zeros and ones, respectively, of the app ropriate size. The identity matrix of an app ropriate size is denoted by I and the Hadamard or elementwise produ ct is denoted by  X  .
In this section, we present the BNMTF model and pu t it in the context of related methods for overlapp ing commun ity detection. Details on h ow to solve the optimization problem for parameter learning will be presented in the next section.
Let us denote a network by N = ( V , E ), where V is a set of n vertices and E is a set of m edges with each of them conn ecting a pair of vertices in V . The adjacency matrix is a nonn egative matrix G  X  R n  X  n + whose ( i, j )th entry g represents the e dge weight between the i t h and j th vertices.
Depending o n whether the network is directed or und irect-ed, G is asymmetric or symmetric accordingly. Moreover, the network is usually far from being a fully conn ected net-work, implying that G is sparse and h ence m  X  n ( n  X  1) / 2.
We assume that the maximum nu mber of possible com-mun ities, denoted by k , is given. In our opinion, a maximum nu mber is much easier to set than the exact nu mber. We use a matrix U  X  R n  X  k + to denote the commun ity member-ship of the n vertices in V and B  X  R k  X  k + to denote the commun ity interaction matrix. For each entry u ij in U , we interpret it as the probabili ty that the i t h vertex belongs to the j th comm un ity. The higher the value of u ij , the more active is the i t h entity in the j th comm un ity. We thu s have the following bound ed constraint on each element of U :
The matrix B represents the relations between comm un i-ties. For example, the comm un ities (or interest group s)  X  X -conomics X  and  X  X olitics X  are strongly rela ted and h ence we expect a large value for the corresp ond ing entry in B ; on the contrary, the commun ities  X  X ovies X  and  X  X olitics X  are only weakly related and h ence the correspond ing entry in B is li kely to be small. The produ ct form UBU T repre-sents the relation b etween any two vertices in terms of the commun ity structure. We want t o use it to a pp roximate the adjacency matrix G . In other words, we app roximate G by a nonn egative matrix tri-factorization UBU T with bound ed U : Two loss functions, namely, squared loss and generalized KL-divergence, are used to measure the app roximation er-ror. Sp ecifically, they are defined as where  X  g ij is the ( i, j )th entry of  X  G .

As discussed above, G is usually sparse. In previous com-mun ity detection methods, the zero entries in G are often interpreted as having no edges between the corresp ond ing vertices. However, it is also possible that some otherwise nonzero entries have not been observed du ring the data col-lection p rocess. If a loss function is defined on the whole matrix G , then even the second case will be treated as the first case, bringing a dd itional noise to the learning process. Here we propose another loss function which is defined only on the edges with g ij &gt; 0:
In real networks, usually each vertex does not participate in too many com mun ities and so U is sparse. To enhance the sparsity of U , we use the l 1 norm to regularize it, i. e., k U k 1 = 1 T U1 du e to the nonn egativity of U . Moreover, by utilizing the l 1 norm of U , we can reduce the e ffective nu m-ber of free parameters in U and h ence control the complexity of the model.

Combining the several considerations above, the optimiza-tion p roblem und erlying BNMTF can be formulated as
In the objective function, the loss function L ( G , U , B ) can be any of the loss functions defi ned in Eqs. (2) to (5) and the regularization p arameter  X  &gt; 0 balances the tradeoff between the app roximation error and the complexity of U .
As discussed above, two NMF-based commun ity detection methods have been developed. In this sub section, we give a more in-depth review of these two methods to pu t the BNMTF model in perspective.

Psorakis et al. [18] used the conventional NMF model which h as two factors. Sp ecifically, using the generalized KL-divergence as in Eq. (3), the model approximates G us-ing XY T for directed networks and XX T for und irected networks. One disadvantag e of this formulation is that the ph ysical meaning o f X and Y is not very clear, because X and Y cann ot be interpreted directly as representing com-mun ity membership since different entries are of different s-cales. From the perspective of BNMTF, X (or Y ) may take the form UV where V is related to B . Thus, X (or Y ) cap-tures both U and B , making its ph ysical meaning un clear. For und irected n etworks, using a positive semidefinite (PSD) matrix XX T to a pp roximate the symmetric matrix G is not very suitable in some sense. For example, the diago nal ele-ments of XX T are usually positive bu t t hose of G are almost zero. Moreover, the e igenvalues of XX T are all nonn egative bu t G can h ave negative eigenvalues. For directed n etworks, the nu mber of parameters (2 nk ) is much larger than that of the und irected case ( nk ) and also that of BNMTF ( nk + k As a result, i ts model complexity is also higher acc ordingly. Moreover, since XY T = ( XD )( YD  X  1 ) T where D can b e any diago nal matrix with p ositive diago nal elem ents, this facto rization also faces the nonidentifiabil ity problem and hence slows down the convergence of the algo rithm.
In [19], the authors treated d irected and und irected net-works differently. For und irected n etworks, like in [18], the facto rization form XX T is used bu t with the squared loss as in Eq. (2). For directed networks, however, a different for-m XAX T is used. On the contrary, BNMTF has a un ified formulation for both d irected and und irected n etworks, with the only difference being that B is s ymmetric or asymmetric acc ording to whether the network is und irected or directed. With a un ified formulation, we can ga in a bette r und erstand -ing o f our model und er different settings. Moreover, from the implemen tation p oint of vi ew, having a un ified form ulation is also favorable du e to the high d egree of code reusability. For und irected networks, because the model in [19 ] is sim-ilar to that in [18], it has the same drawbacks as discussed above. For directed networks, even though their formulation app ears to be similar to o urs, there exist some crucial dif-feren ces. For example, their method implicitly assumes that each row of X represents the probability distribution that the corresponding vertex belongs to each of the commun ities because a postprocessing step makes the probabili ties sum to 1 . They do not impose constraints on X directly because doing so would make the optimization p roblem even more difficult du e to the nonexistence of an analytical solution. However, in order to model overlapp ing comm un ities, we believe the probabilities of each vertex belonging to differ-ent comm un ities should n ot be constrained as in [19 ]. For instance, a vertex may belong to the comm un ity  X  X olitics X  with p robability 0.9 a nd the commun ity  X  X conomics X  with probability 0.8, du e to the strong relation between these two commun ities. An entity can b e very active in multiple commun ities, bu t this scenario cann ot be modeled well if we impose the constraint that t he row sum be e qual to 1 .
Besides, the network sparsity problem has not been ad-dressed by both methods in [18, 19].

In summary, compared with [18, 19 ], BNMTF has some app ealing a dvantag es: 1) The use of a tri-factorization form in BNMTF gives 2) A un ified formulation is used for both directed and 3) BNMTF addresses the network sparsity problem ex-
We note that nonn egative matrix tri-factorization h as been investiga ted b y some other researchers. For example, Ding et al. [5] proposed an orthogo nal nonn egative matrix tri-facto rization method for clustering problems by placing a n orthogo nal constraint on U . However, their method is not suitable for overlapp ing commun ity detection b ecause it as-sumes that each vertex can only belong to o ne comm un i-ty. To the best of our knowledge, there does not exist any nonn egative matrix tri-factorization method with bound ed constraints imposed.
In this s ection, we discuss how to solve the optimization problem (6) effici ently.

In general, three types of optimization methods have been used for solving NMF-based methods, namely, auxiliary func-tion methods [11], which are similar to the majorizat ion-minimization app roach [9] in statistics, projected gradient methods [12 ], and the newly emerg ing coo rdinate descent methods [3, 8]. Amo ng these three app roaches, auxiliary function and coo rdinate descent methods are very popu lar du e to their efficiency and the existence of analytical solu-tions. In what follows, we will develop coo rdinate descent and auxiliary function methods for the BNMTF model.
We first discuss how to solve problem (6) when the loss function is the squared loss as in Eq. (2) or (4). For conve-nience, let us un ify Eqs. (2) and (4) to the following form: where I is an index set of the available entries in G . More specifically, for the squared loss defined in Eq. (2), I denotes the set of all i nd ices in G for directed networks and the set of all i nd ices in the upp er-triangular portion of G for und irected n etworks. For the squared loss defined in Eq. (4), I denotes the set of indices of all the nonzero entries of G for directed networks and the set of ind ices of all nonzero entries in the upp er-triangular portion of G for und irected networks.

Here we use the coo rdinate descent method to solve prob-lem (6) with the un ified squared loss function as in Eq. (7). method. However, d ue to the existence of l 1 regulariza-tion and the bound ed constraint on U , we have not been able to find a goo d auxiliary function, as an upp er bound of the objective function of problem (6), with an analytical solution.
For the matrix U , the co ordinate descent method consid-ers a one-variable upd ate for problem (6) as where O pq nk denotes an n  X  k matrix with all entries equal to 0 except t he ( p, q )th entry which is equal to 1 , and u denotes the ( p, q )th entry of U . By introdu cing a weight matrix W whose ( i, j )th entry equals 1 when ( i, j )  X  I and 0 o therwise, we rewrite the un ified squared loss as and then simplify f U pg ( t ) t o where Z 0 = UBU T  X  G , Z 1 = O pq nk BU T + UB ( O pq nk Z 2 = O pq nk B ( O pq nk ) T , a = tr(( W  X  Z 2 )( W  X  Z 2 2tr(( W  X  Z 1 )( W  X  Z 2 ) T ), c = 2tr(( W  X  Z 0 )( W  X  Z tr(( W  X  Z 1 )( W  X  Z 1 ) T ), d = 2tr(( W  X  Z 0 )( W  X  Z and e = tr(( W  X  Z 0 )( W  X  Z 0 ) T ) +  X  1 T U1 . We can further simpli fy a , b , c and d by noting the following: where the first equation follows from a property of the Hadamard produ ct that tr( A T ( C  X  D )) = tr(( A  X  C ) and the second equation follows from another property of the Hadamard produ ct t hat A  X  C  X  D = A  X  D  X  C and a property of W that W  X  W = W because W  X  { 0 , 1 } n  X  n is a binary matrix. If there is no constraint in problem (8), we can set the derivative of f U pg ( t ) with respect to t to 0 a nd get which amounts to solving a root find ing problem. It is easy to show that a, b  X  0. Since different values of a , b , c and d affect the res ult of the root find ing problem, we discuss it separately for the following cases: 1) a &gt; 0 2) a = 0 , b &gt; 0 3) a = 0 , b = 0 , c 6 = 0 4) a = 0 , b = 0 , c = 0 , d 6 = 0.
 The solutions for these four cases are depicted in Tables 1 to 4 separately.

For the matrix B , we consider two situations depending on whether the network is directed. When it is directed, B is a g eneral nonn egative matrix and the objective function for each step of the coo rdinate descent method is formulated as Table 1: Algorithm for solving pro blem (8) when a &gt; 0 . where b pq is the ( p, q )th entry of B . We simplify f where P 0 = UO pq kk U T and P 1 = UBU T  X  G . We note that f B pq ( t ) is a quadratic function of t . It is easy to show that t he solution to problem (11 ) can b e calculated as Table 2: Algorithm for solving pro blem (8) when a = 0 , b &gt; 0 . Table 3: Algorithm for solving pro blem (8) when a = 0 , b = 0 , c 6 = 0 . Table 4: Algorithm for solving pro blem (8) when a = 0 , b = 0 , c = 0 , d 6 = 0 .
When the network is u nd irected, B is symm etric and the objective function for a non-diago nal entry b pq ( p 6 = q ) is given by where the objective function can b e simplified to with P 2 defined as P 2 = U ( O pq kk + O qp kk ) U T . Then the optimal t can b e calculated as
For each diago nal entry b pp , the upd ate rule is identical to that for directed n etworks and we omit the derivation h ere.
For the upd ate of U , we need to efficiently compu te the coeffici ents in Eq. (9) or, equivalently, the matrices Z and Z 2 . Acc ording to the definition of Z 2 , it is a zero ma-trix with only one nonzero element ind exed by ( p, p ) as b Z 0 measures the app roximation residu al and we can keep track of Z 0 and upd ate it whenever each entry in U and B changes. Since Z 1 is related to UB and BU T , similar to Z , we also keep track of UB and BU T . By taking a dvan-tag e of the extreme sparsity of the matrices involved, we can compute the coefficients in Eq. (9) effici ently.
 For the upd ate of B , we need to compu te P 0 , P 1 and P . Note that P 1 is equal to Z 0 which is kept as a g lobal variable and n eeds no a dd itional compu tation. For P 0 is easy to show that P 0 = u p u T q acc ording to its definition, where u p is the p t h column of U , and h ence we can compute P 0 more efficiently. Similar to P 0 , P 2 can b e computed as P 2 = u p u T q + u q u T p in an effici ent way.
Similar to the squared loss, we first give a un ified form of the generalized KL-divergence as follows
According to [3, 8], the coo rdinate descent method h as no closed-form solution and so we res ort to a n auxiliary function method. We first give the definition of an auxiliary function.
De finiti on 1. G ( h , h 0 ) is an auxiliary function for a func-tion F ( h ) if the conditions are satisfied.

For the minimization of F ( h ), we can minimize G ( h , h instead b ased on the following lemma from [9, 11 ].
Le mm a 1. For an estimate h 0 of F ( h ) , we can upda te it to h 1 with F ( h 1 )  X  F ( h 0 ) where h 1 satisfies
With B fixed, the objective function for U is formulated as where [ A ] ij denotes the ( i, j )th elem ent of A . Then we define the auxiliary function for F ( U ) in the following the-orem.
 The orem 1.
 where  X  is a po sitive c onstant,  X  u ij is the ( i, j ) th element of  X  F ( U ) .

In Theorem 1, we ad d  X  to each  X  u rs to increase nu mer-ical stability because  X  u rs may be equal or very close to 0. Then we need to minimize G ( U ,  X  U ) with respect to U where  X  U is the current estimate of U . For each entry of U , the optimization p roblem is formulated by rewri ting G ( U , where
If a = 0 and b = 0, f ( u pq ) degenerates to f ( u pq ) =  X  c ln u pq . It is easy to show that the optimal solution is u pq = 1 because c  X  0.

If a = 0 a nd b &gt; 0, we set the derivative of f ( u pq respect t o u pq to 0 a nd get the solution as u pq, 1 we can get u pq, 1  X  [0 , 1] an d since the second -order derivative c &gt; b , we can get f  X  ( u pq ) = b  X  c u implying that f ( u pq ) is nonincreasing and h ence 1 is the optimal solution.

If a = 0 a nd b &lt; 0, f  X  ( u pq ) = b  X  c u implies that f ( u pq ) is nonincreasing a nd h ence u pq optimal solution.

When a &gt; 0, 2 the situation is more complicated. If there is no constraint in problem (14 ), we can set the derivative of f ( u pq ) with respect to u pq to 0 a nd obtain the solutions for u pq as
It is easy to show that u pq, 1  X  0 and u pq, 2  X  0 since a &gt; 0 and c  X  0. When u pq, 1  X  [0 , 1], we have f  X  X  ( u pq ) = a + c 0, implying that u pq, 1 is the optimal solution of problem (1 4). When u pq, 1 /  X  [0 , 1] or equivalently u pq, 1 &gt; 1 since u pq, 1  X  0, the derivative of f ( ) denoted by f  X  ( u pq tive over [0 , 1] since f  X  ( u pq ) = a u d ue to u pq, 1 &gt; 1 a nd u pq, 2  X  0. This implies that f ( u nonincreasing o ver [0 , 1] and h ence u pq = 1 is the optimal solution to problem (14).

When U is fixed, the objective function for B is formu-lated as
For the auxiliary function of F ( B ), we have the following result. The orem 2.
 where  X  b rs is the ( r , s ) th element of  X  B and  X  ij is the auxiliary function of F ( B ) .

Then we need to minimize G ( B ,  X  B ), where  X  B is the cur-rent estimate of B , with the objective function for each entry b pq of B formulated as
When the network is directed, B is asymm etric and we set the derivative of f ( b pq ) with respect to b pq to 0 to get the solution as which is nonnegative an d h ence satisfies the constraint. When the network is und irected, B is symmetric. We set the derivative of f ( b pq ) with respect to b pq and b qp get the solution as
Even though the derivation above ap pears complicated, it can actually be summarized as an elegant matrix for-mulation which can b e implemented effici ently using some matrix-based software such as MATLAB.

For the upd ate of U , we need to compu te the coefficients in the objective function of problem (14). We stack the coeffici ents for all { u pq } in three matrices, namely, M and M c , which are all of size n  X  k . Then we can compu te them as where E nk denotes an n  X  k matrix of all ones, an d M 1 M matrices M 1 an d M 2 denotes elementwise division. When we utilize all the e lements in G , W becomes E nn and we can further simplify the formulation. After getting M a , M and M c , we can easily obtain the estimate of U .
For the upd ate of B , we un ify the upd ate rule for directed and und irected networks as where  X  B is the u pd ated solution for B . In this section, we report the empirical performance of BNMTF by comparing it with th e two related NMF-based methods mentioned b efore, denoted b y NMF sq [19], which uses the squared loss, and b y NMF kl [18], which u ses the generalized KL-divergence. The two variants of BNMTF are denoted by BNMTF sq and BNMTF kl which u se the squared loss and generalized KL-divergence, respectively. The implemen tation of our method can b e downloaded from http://www.cse.ust.hk/~dyyeung/code/BNMTF.zip .

The first performance measure is modu larity which was proposed by Newman and Girvan in [14]. The modu larity measure Q is defined as
Q ( Y ) = 1 where d ( i ) denotes the degree of node i , y il is the ( i, l )th element of the membership matrix Y , an d X is a matrix the mod ularity, the bette r the performance. Since modu lar-ity was originally designed for non-overla pp ing comm un ity detection, only one e ntry in each row of Y is 1 while all other entries are 0. So a ll the methods compared need a postprocessing step to obtain Y as where U is the membership matrix returned b y any method compared and u it is an element of U . The modu larity mea-sure may be viewed as a criterion to measure the confid ence of the commun ity structure returned by each method. Be-sides modu larity, we also propose another measure which is the area und er curve (AU C) score based on modu larity. AUC score is to measure the accuracy of the find ing o f mul-tiple commun ities by each method. We first scale the e ntries in each column of the membership matrix U to [0 , 1] to make the mem bership value of the most active node in one com-mun ity as 1.We then vary a threshold from 0 to 1 a nd set all those entries in U that exceed the threshold to 1 a nd 0 o th-erwise. Finally we compu te the modu larity Q ( U ) and also the AUC score. We use several benchmark datasets 3 (see Table 5) from real-world app lications for the experiments.
We first compare BNMTF with NMF sq and NMF kl based on the modu larity and AU C score. The results are summa-rized in Tables 6 a nd 7. Using the generalized KL-divergence, BNMTF kl is either comparable to o r better than NMF kl on every dataset with respect t o both p erformance mea-sures. The res ult is si milar when the squared loss is used. We also compare the two variants of BNMTF. Out of 12 benchmark datasets, BNMTF kl outperforms BNMTF sq on eight in terms of modu larity measure while for AU C score, BNMTF kl outperforms BNMTF sq on seven datasets. The result seems to sugg est that both loss functions are compa-rable in performance.
The methods BNMTF kl and BNMTF sq utilize the whole graph . Here we also consider another setting in which the loss function is defined only on the e dges with n onzero weight-s, i.e., Eqs. (4) and (5). The corresp ond ing methods are denoted by sBNMTF sq and sBNMTF kl . The res ults are also shown in Tables 6 a nd 7. We note that sBNMTF kl and sBNMTF sq are generally bette r than BNMTF kl and BNMTF sq in terms of the AUC score bu t worse in terms of modu larity. This sugg ests that sBNMTF kl and sBNMTF sq seem to be bette r for identifying the commun ity structure bu t worse for find ing the most confid ent comm un ities. More-over, the performance of sBNMTF kl and sBNMTF sq is un -satisfacto ry in some cases, e.g., on the  X  X ord adjacencies X  dataset. One possible reason for this is that t he network is so sparse that t here are only very few edges with n onzero weights.
The free parameters in BNMTF includ e the rank param-eter k , the regularization p arameter  X  , and  X  which is add ed to the denominator in BNMTF kl . Here we condu ct some ex-periments on the  X  X olph ins X  and  X  X merican college football X  datasets to stud y the sensitivity of these parameters.
We vary the rank parameter k from 3 to 100 a nd the results are shown in Figures 1(a) and 1(b). When k does not exceed 10 , the performance change is small. The per-formance deteriorates very fast as k increases. This is not surprising because the total nu mber of commun ities is gen-erally very small.

For the regularization p arameter  X  , we vary it from 0.01 to 100 . Figures 2(a) and 2(b) show the res ults. We find that t he performance is very stable except with some slight degradation as  X  app roaches 100 . Because the performance is not sensitive to  X  , setting its value is quite easy.
Figures 3(a) and 3(b) show the res ults when  X  varies from 0 to 1 . The performance increases s ignificantly as it increases from 0 to 0 .2, showing that a nonzero value can h elp to enhance the nu merical stabili ty of BNMTF. Beyond 0.2, the performance is generally insensitive to its value and h ence its value is also easy to set.
By using matrix tri-factorization, BNMTF explicitly mod-els the comm un ity mem bership of each n ode and the in-teraction among commun ities, making it outperform other NMF-based methods in our empirical comparative stud y. sBNMTF kl NMF sq BNMTF sq sBNMTF sq sBNMTF kl NMF sq BNMTF sq sBNMTF sq datasets.

Currently, BNMTF requires the maximum nu mber of com-mun ities to be set in advance. One possible extension in the future is to a llow the actual nu mber of comm un ities to be determined automatically. For example, we may pu rsue a probabilistic reformulation of BNMTF so that methods such as automatic relevance determination [1] can b e in-corporated to learn the nu mber of commun ities. Moreover, the current paper focuses on the static commun ity detection problem. Another interesting direction is to extend BNMTF to dynamic c ommun ity detection [15 ] by modeling dynamic network evolution over time.
 This research h as been supp orted by General Research Fund 621310 from the Research Grants Coun cil of Hong Kong. [1] C. M. Bishop. Pattern Rec ognition and Machine [2] V. D. Blond el, J.-L. Guillaume, R. Lambiotte, and [3] A. Cichocki and A.-H. Phan. Fast local algorithms for [4] A. Clauset, M. E. J. Newm an, and C. Moo re. Find ing [5] C. Ding, T. Li, W. Peng, and H. Park. Orthogo nal foo tball X  datasets. [6] I. J. Farkas, D.  X  Abel, G. Palla, and T. Vicsek. [7] M. Girvan and M. E. J. Newm an. Comm un ity [8] C.-J. Hsieh and I. S. Dhillon. Fast coo rdinate descent [9] K. Lange, D. R. Hun ter, and I. Yang. Optimization [10] D. D. Lee and H. S. Seun g. Learning the parts of [11] D. D. Lee and H. S. Seun g. Algorithms for [12] C.-J. Lin. Projected gradient methods for nonn egative [13] M. E. J. Newm an. Fast algorithm for detecting [14] M. E. J. Newm an and M. Girvan. Finding a nd [15] G. Palla, A.-L. Barab  X asi, and T. Vicsek. Quantifying [16] G. Palla, I. Der  X enyi, I. Farkas, and T. Vicsek. [17] G. Palla, I. J. Farkas, P. Pollner, I. Der  X enyi, and [18] I. Psorakis, S. Roberts, M. Ebd en, and B. Sh eldon. [19] F. Wang, T. Li, X. Wang, S. Zhu, and C. H. Q. Ding.
