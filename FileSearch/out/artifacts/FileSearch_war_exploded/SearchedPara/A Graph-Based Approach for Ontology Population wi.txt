 Automatically populating ontology with named entities ex-tracted from the unstructured text has become a key issue for Semantic Web and knowledge management techniques. This issue naturally consists of two subtasks: (1) for the entity mention whose mapping entity does not exist in the ontology, attach it to the right category in the ontology (i.e., fine-grained named entity classification), and (2) for the en-tity mention whose mapping entity is contained in the ontol-ogy, link it with its mapping real world entity in the ontology (i.e., entity linking). Previous studies only focus on one of the two subtasks and cannot solve this task of populating ontology with named entities integrally. This paper pro-poses APOLLO, a grA ph-based aP proach for pO puL ating ontoLO gy with named entities. APOLLO leverages the rich semantic knowledge embedded in the Wikipedia to resolve this task via random walks on graphs. Meanwhile, APOLLO can be directly applied to either of the two subtasks with minimal revision. We have conducted a thorough experi-mental study to evaluate the performance of APOLLO. The experimental results show that APOLLO achieves signifi-cant accuracy improvement for the task of ontology pop-ulation with named entities, and outperforms the baseline methods for both subtasks.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Algorithms, Experimentation Ontology population, Named entity classification, Entity link-ing, Label propagation
The trend to advance the traditional keyword-based search to the semantic entity-based search has attracted a lot of at-tention in recent years. A critical step to achieve this goal is to construct a comprehensive machine-understanding ontol-ogy about the world X  X  entities, their semantic categories and their mutual relationships. Despite there exist some ontolo-gies such as WordNet [11] which are constructed manually, they have limited coverage in various regions. Furthermore, as world evolves, new facts come into existence and are dig-itally expressed on the Web. Therefore, populating and en-riching the existing ontology with the newly extracted facts become more and more important. Manually populating on-tology requires substantial human effort and is usually time consuming. This has motivated the research on the auto-matic ontology population techniques.

The development of the information extraction techniques makes the automatic ontology population techniques possi-ble. Recently, information extraction from large document collections has received a lot of attention, and a variety of information extraction problems have been considered such as named entity recognition [17, 10], named entity classifica-tion [12, 21] and relation extraction [3, 25]. Integrating the newly extracted knowledge derived from the information ex-traction systems with the existing ontology requires to deal with the task of populating ontology with named entities.
Ontology population with named entities is the task to lo-cate the right place of the detected named entity in the on-tology. Given a named entity mention detected from the un-structured text, if the mapping entity of the entity mention is not contained in the ontology, we should find the right cat-egory node to which the entity mention should be attached in the ontology, which is known as the task of fine-grained named entity classification. Otherwise, if the mapping en-tity of the entity mention exists in the ontology, the aim of this task is to link this detected entity mention with its cor-responding real world entity in the ontology, which is known as the entity linking task. For example, we assume that the ontology contains the entity of NBA player named  X  X ichael Jordan X , and there is only one entity named  X  X ichael Jor-dan X  in the ontology. In the text  X  X rofessor Michael Jordan has a talk on machine learning. X , the mapping entity of the entity mention  X  X ichael Jordan X  is the Berkeley professor whose name is also  X  X ichael Jordan X . Since the ontology does not contain such an entity, we should perform the fine-grained named entity classification task and obtain the cat-egory (i.e., Professor ) to which the mapping entity of this entity mention belongs. Then we create a new node for this entity mention  X  X ichael Jordan X  and attach this node to the category node Professor in the ontology. While for the entity mention appearing in the text  X  X ichael Jordan wins NBA champion. X , we should map this mention of  X  X ichael Jordan X  to the entity of NBA player existing in the ontology, which is called the entity linking task.

Ontology population with named entities has received much attention recently, and several solutions to this task have been proposed in research [15, 14, 28, 6, 8, 7]. However, these state-of-the-art systems only focus on one of the two subtasks (i.e., fine-grained named entity classification and entity linking). It calls for a unified framework to resolve the task of populating ontology with named entities inte-grally.

The ontology-based fine-grained named entity classifica-tion problem has been addressed by many researchers [15, 14, 28, 6, 13]. These systems classify the named entities detected from the text into a large number of categories specified by an ontology or a multi-level taxonomy. In most of these studies, they suppose that the entity disambigua-tion/linking process has been completed, and all identified entity mentions have been mapped to their unique represen-tations. Besides, some of them consider that the entity men-tions identified in the data set are not ambiguous, and thus ignore the ambiguity problem of the named entity. How-ever, in realistic scenario, the mention form of the named entity is highly ambiguous. For example, the entity men-tion of  X  X ichael Jordan X  can refer to the famous basketball player, the computer science professor or some other per-sons. Henceforth, in APOLLO, we do not ignore the ambi-guity problem of the named entity, and resolve the task of populating ontology with named entities integrally.
The solutions proposed in [4, 7, 8, 24] address the entity linking task, and they all aim to link the textual mention form of the named entity with the corresponding real world entity in the existing ontology. If the matching entity of certain entity mention does not exist in the ontology, they just return NIL (denoting an unlinkable entity mention) for this mention form, and cannot attach this unlinkable entity mention to the right category in the ontology.

In this paper, we propose APOLLO, a graph-based weakly supervised framework to resolve the task of automatic ontol-ogy population with named entities integrally. Meanwhile, our proposed framework APOLLO can be directly applied to either of the two subtasks with minimal revision. APOLLO is based on the assumption that if the contexts where two named entities appear are semantically similar, they are likelytobelongtothesamecategory,whichistheexten-sion of the distributional hypothesis [16]. The only training data for APOLLO is an initial ontology, in which there are a list of labeled named entities whose categories are known to us beforehand. Therefore, APOLLO is weakly super-vised and needs minimal human involvements. Given each entity mention/named entity and its associated document context, we firstly recognize all the Wikipedia concepts ap-pearing in this context, and we consider the set of these de-tected Wikipedia concepts as the semantic signature of this entity mention/named entity. Then we construct a graph consisting of the nodes coming from all the entity mentions which need to be populated into the ontology, the named entities contained in the ontology, and the Wikipedia con-cepts existing in their corresponding semantic signatures . We weight the edges between the Wikipedia concept nodes in the graph, by leveraging the rich semantic knowledge em-bedded in the link structure of the Wikipedia articles. The nodes of the named entities contained in the ontology are annotated with their category labels, and other unlabeled entity mention nodes are required to be classified. Subse-quently, the Adsorption label propagation algorithm [2] is applied to this constructed graph to produce a probability distribution over categories for each unlabeled entity men-tion node, based on the rich graph structure. Finally, for each entity mention, we have to validate whether there ex-ists a named entity in the ontology we could link this entity mention with. Otherwise, we attach this entity mention to the category that has the largest distribution. It is noted that a very preliminary two-page version of this paper [22] has been published in WWW X 12. In this paper, we make further enhancements, and give a complete and in-depth de-scription of our proposed APOLLO framework.

To summarize, we make the following contributions.
The remainder of this paper is organized as follows. Sec-tion 2 formulates the problem and presents the APOLLO framework. Next, the three modules of APOLLO (i.e., Graph Creation, Label Propagation and Linking Validation) are re-spectively introduced in Section 3, Section 4 and Section 5. Section 6 presents our experiments and Section 7 discusses the related work. We conclude this paper in Section 8.
In this section, we will study the problem of automatically populating ontology with named entities extracted from the large text corpus. For this purpose, we will firstly give some notations and formulate the problem of ontology population with named entities in Section 2.1. Subsequently, the overall framework of APOLLO will be introduced in Section 2.2.
The only input of our framework APOLLO is a collection of documents and an initial ontology. Let D be the collec-tion of the input documents and  X  be the initial ontology. Let  X  be the set of all entity mentions recognized from the document set D , and each entity mention s  X   X  needs to be populated into the ontology  X . Suppose that there are a list of labeled named entities whose categories are known within the initial ontology  X . Let N  X  denote the set of all named entities contained in the ontology  X , and C  X  be the set of all categories in the taxonomy of  X .
 Entity mention and mapping entity :Anentitymen-tion s  X   X  is a token sequence in the text document which refers to some named entity. Let n s denote the correspond-ing real world named entity the entity mention s refers to. We should differentiate between the entity itself and its vari-ous entity mentions. In reality, an entity may have multiple entity mentions. For example, the entity Hewlett-Packard has its abbreviation  X  X P X . On the contrary, one entity men-tion may also refer to several different real world entities. For instance, the entity mention of  X  X ichael Jordan X  can refer to the famous basketball player, the computer science professor or some other persons. Henceforth, the mapping entity n s of the entity mention s depends on the context where the entity mention s occurs.
 Document context : We define the document context  X  s of the entity mention s  X   X  as a window of words around the occurrence of the entity mention s . Assume that the entity mention s of length | s | words appears in a document d at po-sition p .Thesize-k document context  X  s of entity mention s with respect to d is the window w p  X  k ,...,w p  X  1 ,w p w + | s | + k  X  1 of words around the occurrence of s ( w i sents the word at position i ). For instance, the entity men-tion of  X  X ichael Jordan X  occurs in a document containing such a sentence,  X  X n the NBA Final of 1991, Michael Jordan shot 12 free throws. X  When the size k is set to 5, the size-k document context  X  s is  X  X he NBA Final of 1991 shot 12 free throws X . On the other hand, for each named entity n  X  N  X  we define the document context  X  n of the named entity n as the description context for n in the ontology. As both the entity mention s  X   X  and the named entity n  X  N  X  have document contexts, we use  X  to denote the document con-text corresponding to an entity mention or a named entity. Semantic signature : To capture the semantic informa-tion existing in the document context  X  ,werecognizeall the Wikipedia concepts  X  appearing in  X  , and consider the set of these detected Wikipedia concepts as the semantic signature  X  . Here, Wikipedia concept means the concept which has its corresponding descriptive article in Wikipedia, and each Wikipedia concept is represented by the title of its Wikipedia article. For the general textual document, we utilize the open source toolkit Wikipedia-Miner 1 to de-tect the Wikipedia concepts appearing in the context. The Wikipedia-Miner toolkit takes the general unstructured text as input and uses the machine learning approach to detect the Wikipedia concepts in the input document [20]. For the document context in the example mentioned above, this Wikipedia-Miner toolkit returns two Wikipedia concepts, i.e., NBA Final and Free throw . Therefore, it can be seen that these detected Wikipedia concepts are highly seman-tically related to the NBA player Michael Jordan ,andwe can leverage this semantic information contained in this se-mantic signature to populate this entity mention  X  X ichael Jordan X  into the ontology  X  effectively. As we know, the document from the Wikipedia has its special layout to orga-http://wikipedia-miner.cms.waikato.ac.nz/ nize its content, i.e., Wiki markup 2 . The references to other Wikipedia concepts in the Wikipedia document are within pairs of double square brackets. Henceforth, for a Wikipedia document, we can identify the Wikipedia concepts appearing in it directly and accurately by leveraging the characteristic of Wiki markup .

Now we can formulate the problem of ontology population with named entities.
 Ontology population with named entities: Given a collection of documents D , an initial ontology  X  and a set of entity mentions  X  detected from D , the task of ontology population with named entities is to locate the right place for each entity mention s  X   X  in the ontology  X  .Foreach s  X   X  , if the mapping entity n s /  X  N  X  , the entity mention s has to be attached to the proper category c s  X  C  X  ;Ifthe mapping entity n s  X  N  X  , the goal of this task is to return this mapping entity n s .
Based on the problem definition, we propose a framework called APOLLO, to address the task of ontology population with named entities using three modules as follows: In the following sections, we will introduce those three mod-ules in details.
To represent all the available information in a unified form, we need a representation capable of encoding effi-ciently all the complicated relationships between the entity http://en.wikipedia.org/wiki/Wiki markup mentions  X  and the named entities N  X  . To achieve this goal, we select the graph as the representation, since the graph can encode different types of objects (i.e., entity mentions, named entities and Wikipedia concepts) as the nodes in the graph, and represent various relationships between these ob-jects as the edges between these nodes. Furthermore, the graph makes the potential label propagation paths explicit, and the label information can be propagated along these con-necting paths from the labeled named entity nodes to the unlabeled entity mention nodes. For example, if the seman-tic signatures of the entity mention X  X ichael Jordan X  X nd the named entity Yao Ming both contain the Wikipedia concept NBA Final , then this can be treated as an evidence that this entity mention  X  X ichael Jordan X  may have the same cate-gory as the named entity Yao Ming .Henceforth,thepath connecting the two nodes (i.e.,  X  X ichael Jordan X  and Yao Ming ) via the Wikipedia concept node (i.e., NBA Final ) may help to forward the label information of the labeled named entity node Yao Ming to the unlabeled entity men-tion node  X  X ichael Jordan X .

Specifically, we construct a single graph G =( V,E,W )to represent all the information available for this task, where V denotes the set of nodes, E is the set of edges and W : E  X  R is the weight function which gives positive weight for each edge in E . It is noted that we define the graph G as an undirected graph. The node set V consists of the nodes which come from all the entity mentions  X  , the named entities N  X  and the Wikipedia concepts in their semantic signatures  X  . Specifically, for each entity mention s  X   X  ,we pair it with each Wikipedia concept  X   X   X  s where  X  s denotes the semantic signature of s , to create the triple ( s,  X , w ), and the weight w could be 1 /dist ( s,  X  )where dist ( s,  X  )is the distance between the positions of s and  X  in the context. In the experiment, we set this weight w to 1.0 for the purpose of simplicity. For each triple ( s,  X , w ), s and  X  are added to V and the edge ( s,  X  ) is added to E ,with W ( s,  X  )= w .And for each named entity n  X  N  X  , we also pair it with each Wikipedia concept  X   X   X  n where  X  n denotes the semantic signature of n , to create the triple ( n,  X , w ), where the weight w could be the degree of importance for  X  in the description context of entity n . The degree of importance for  X  could be calculated as its average semantic relatedness to all other Wikipedia concepts in  X  n . However, we set this weight w to 1.0 in the experiment for simplicity as well. For each triple ( n,  X , w ), n and  X  are added to V and the edge ( n,  X  ) is added to E ,with W ( n,  X  )= w .Afterthetriple( s,  X , w ) for each entity mention s  X   X  and the triple ( n,  X , w )for each named entity n  X  N  X  are all added into the graph G , two nodes of the entity mention or the named entity in the graph G are just connected via the Wikipedia concept nodes which co-occur in both of their semantic signatures . Therefore, we define the current status of the graph G as G co , denoting that this graph just contains the co-occurrence information between the entity mentions or named entities and the Wikipedia concepts in their semantic signatures .
To forward the label information over the graph more ef-fectively, the semantically related Wikipedia concept nodes should be connected by some edges to enrich the informa-tion propagation paths. For instance, if the semantic sig-nature of the entity mention  X  X ichael Jordan X  contains the Wikipedia concept Free throw ,andthe semantic signature of the named entity Yao Ming contains the Wikipedia con-cept Technical foul , this entity mention  X  X ichael Jordan X  is expected to have the same category as the named entity Yao Ming , since the two Wikipedia concepts are highly semanti-cally related. Thus, we incorporate the semantic relatedness between the Wikipedia concepts into the graph.

Since the link structure of the Wikipedia articles expresses the rich semantic relations, two Wikipedia concepts are con-sidered to be semantically related if there are many Wikipedia concepts that link to both. In order to measure the strength of the semantic relatedness, we adopt the Wikipedia Link-based Measure (WLM) described in [19] to calculate the se-mantic relatedness between Wikipedia concepts. The WLM modeled from the Normalized Google Distance [5] is based on the Wikipedia X  X  hyperlink structure. Given two Wikipedia concepts u 1 and u 2 , we define the semantic relatedness be-tween them as follows:
SR ( u 1 ,u 2 )=1  X  log ( max ( where U 1 and U 2 are the sets of Wikipedia concepts that link to u 1 and u 2 respectively, and WP is the set of all concepts in Wikipedia. This definition gives higher value to more related concept pair and the value of SR ( u 1 ,u is varied from 0.0 to 1.0. For each pair of Wikipedia con-cept nodes (  X  1 , X  2 ) in the graph, if the semantic relatedness SR (  X  1 , X  2 ) is greater than some threshold  X  , we add an edge (  X  1 , X  2 )to E ,with W (  X  1 , X  2 )= SR (  X  1 , X  2 ).
Figure 1 shows an example of the created graph, in which there are one entity mention node (i.e.,  X  X ichael Jordan X ) and one named entity node (i.e., Yao Ming ). We assume that the semantic signature of the entity mention  X  X ichael Jordan X  has four Wikipedia concepts (i.e., NBA Most Valu-able Player Award , NBA Final , Chicago Bulls and Free throw ), while the semantic signature of the named entity Yao Ming has three Wikipedia concepts (i.e, NBA Final , Technical foul and Shanghai Sharks ). In Figure 1, we add a real line between each entity mention/named entity node and each Wikipedia concept node in its semantic signature .FromFig-ure 1, we can see that the Wikipedia concept node NBA Fi-nal is connected with both the entity mention node X  X ichael Jordan X  and the named entity node Yao Ming . The dash lines added between the semantically related Wikipedia con-cept nodes in Figure 1 make the paths connecting the entity mention node  X  X ichael Jordan X  and the named entity node Yao Ming more abundant, which also demonstrate that the entity mention  X  X ichael Jordan X  is likely to have the same category as the named entity Yao Ming .

It is noted that in our framework, the semantic relatedness can be computed by other methods such as the type hier-archy based similarity and distributional context similarity introduced in [23] or combination of them, which gives flex-ibility to APOLLO in an efficient and simple way.
The aim of this section is to assign each entity mention s  X   X  to the proper category c s  X  C  X  . Firstly, we annotate each named entity node n  X  N  X  with its corresponding cat-egory label in the graph G . In this paper, the named entity category is used as the label for the node, and we assume that each named entity just belongs to one category for the purpose of simplicity. The remaining question is how to propagate the category labels present on the labeled named entity nodes to the unlabeled entity mention nodes in the graph. To solve this problem, we apply the Adsorption la-bel propagation algorithm introduced in [2] to the graph G , to produce the predicted category c s  X  C  X  for each unla-beled entity mention node s  X   X  based on the rich graph structure. Furthermore, the Adsorption algorithm supports incremental updates and can be easily parallelized, which are important for large scale ontology population task. The Adsorption algorithm works on the graph G , and ultimately produces for each unlabeled entity mention node s  X   X  a label distribution L s , representing which category labels are appropriate for the unlabeled entity mention s .Ourframe-work APOLLO assumes that named entities that occur in semantically similar contexts belong to the same category. Specifically, we consider that named entities that co-occur with semantically related Wikipedia concepts may have the same category. Therefore, the label propagation algorithm is to forward the category label between the related named entity nodes and entity mention nodes.

The Adsorption algorithm has three different but equiv-alent interpretations, whose details are introduced in [2]. However, in this paper, we use two interpretations to clas-sify the unlabeled entity mention into the proper category. Adsorption via Averaging: In this view of the algo-rithm, the labels are propagated from one node to all its neighbors. Thus each node in the graph has two roles, for-warding labels and collecting labels, and each node keeps track of the history of all labels it receives. For the sake of presentation, we preprocess the original graph G to gen-erate the augmented graph G =( V ,E ,W )intheway that, for each labeled named entity node n  X  N  X  ,wecre-ate a  X  X hadow X  node  X  n which has just one neighbor n in G , with an edge (  X  n, n ) connecting them with W (  X  n, n )=1. Let  X  N  X  denote the set of  X  X hadow X  nodes,  X  N  X  = {  X  n | n  X   X  N
 X   X  V .Thus, V = V  X  N  X  , E = E { (  X  n, n ) | n  X  N  X  } and W (  X  n, n )=1for n  X  N  X  , W ( v 1 ,v 2 )= W ( v 1 ,v v ,v 2  X  V . Meanwhile, we give the label distribution L n of each n  X  N  X  to its  X  X hadow X  node  X  n in G ,andleave n in graph G with no label distribution. We define  X  as the label which represents lack of information about the actual labels. Then, at the beginning of the algorithm, we define the initial label distribution I v for all v  X  V . Specifically, for each  X  X hadow X  node  X  n  X   X  N  X  , I  X  n = L n , and for all other nodes v  X  V ,v /  X   X  N  X  , I v = L  X  where L  X  represents that we have no information about the label distribution of the node v . Subsequently, the algorithm proceeds as follows: for each node v  X  V , we compute the label distribution as the weighted average of the label distributions of all its neigh-bors, i.e., L v = u W ( u, v ) L u .
 Adsorption via Random Walks: This view takes ran-dom walks over the edge-reversed version of the graph G to find the label distribution for each node, which has been proved to be equivalent with the Averaging view, as de-scribed in [2]. As the graph G is undirected, its edge-reversed version of G is the same as itself. Therefore, to estimate the label distribution L v for each node v  X  V ,we perform a random walk on graph G starting from node v . When the random walk reaches a node t , there are three choices: (a) continue the random walk to the neighbors of t ; (b) abandon the random walk; (c) stop the random walk and inject the initial label distribution I t . We assume the probabilities of these three events are P c ( t ), P a ( t )and P respectively. Finally, L v is set to be the expectation of all labels injected from random walks starting from node v . Algorithm 1 Adsorption Algorithm Input: G =( V ,E ,W ), { I v | v  X  V } .
 Output: { L v | v  X  V } . 1: for all v  X  V do 2: L v = I v 3: end for 4: repeat 5: for all v  X  V do 6: M v = X  u W ( u, v ) L u 7: end for 8: Normalize M v to have unit M 1 norm 9: for all v  X  V do 10: L v = P c ( v )  X  M v + P i ( v )  X  I v + P a ( v )  X  11: end for 12: until convergence
We combine these two interpretations of the Adsorption algorithm to generate the label distribution L v for each node v  X  V with Algorithm 1 like [27]. Algorithm 1 firstly ini-tializes the label distributions for all nodes in the graph (line 1-line 3). Then, for each node v  X  V , the algorithm itera-tively computes the weighted average of the label distribu-tions of all its neighbors (line 5-line 7), and normalizes the computed label distribution M v to have unit norm (line 8). Next, we use the random walk probabilities to estimate the new label distribution L v for each node v  X  V (line 9-line 11). Until convergence, each node v  X  V carries a label dis-tribution and outputs L v as the final results. Convergence occurs if the label distributions of all nodes do not change in a round. However, in practice, we run the algorithm for a fixed number of iterations alternatively. In Algorithm 1, via using the variable M v in line 10, we compute the label distribution for node v in the i th iteration entirely based on its neighbors X  label distributions from the ( i  X  1) th iteration. Therefore, Algorithm 1 has the memoryless property and can be easily parallelized, which is beneficial for large scale ontology population task.

To set the random walk probabilities, we used the follow-ing heuristics from [27]. Let c v = log X  log (  X  + exp ( H ( v )=  X   X  u p uv  X  log ( p uv )with p uv = W ( u,v ) fore, if node v has many neighbors, c v is low. In the experi-ment, we set  X  =2. Ifnode v  X   X  N  X  ,weset i v =(1  X  c v
H ( v ); otherwise, i v =0. Thenlet z v = max ( c v + i v Lastly, we computed the random walk probabilities for each node v  X  V as follows:
According to Formula 2, we can see that for the high-degree node, the continue probability P c ( v )islow.Thuswe can decrease the probability o f the random walk running into the unrelated regions in the graph, and make the random walk stay relatively close to its source node.
For each entity mention s  X   X  , we obtain the label distri-bution L s over the categories C  X  in the Label Propagation module. We consider the category which has the largest dis-tribution in L s as the predicted category c s for the entity mention s . According to the task definition of ontology pop-ulation with named entities, if the mapping entity n s of the entity mention s exists in the ontology  X , we have to link this entity mention s with its mapping entity n s .Hence-forth, we add this module to validate whether its mapping entity n s  X  N  X  .

As stated in Section 2.1, one entity mention may refer to several different real world entities. Thus, given an entity mention s , we firstly retrieve the set of entities that may be referred by this entity mention s , and we denote this set of entities as the candidate entity set CN s for s .Intuitively, the candidate entities in CN s should have the name of the entity mention of s . To solve this problem, we need to build a dictionary DT that contains vast amount of information about various mention forms of the named entities, like name variations, abbreviations, confusable names, spelling varia-tions, nicknames, etc. In our paper, the dictionary DT is a &lt; key, value &gt; mapping, where the column of the key K is a list of entity mentions and the column of the mapping value K.value is the set of named entities which are referred by the key K . We construct the dictionary DT by leveraging the following four structures of Wikipedia: Entity page , Redirect page , Disambiguation page and Hyperlink in Wikipedia article . The detailed construction method is introduced in [24, 23]. A part of the dictionary DT is shown in Table 1.

For each entity mention s  X   X  , we look up the dictio-nary DT andsearchfor s in the column of the key K .If a hit is found, i.e., s  X  K , we add the set of the mapping entities s.value to the candidate entity set CN s . Suppose that any two entities belonging to the same category do not have the same name. For example, there is only one entity named  X  X ichael Jordan X  belonging to the category of NBA basketball player. Therefore, if two entity instances having the same name belong to the same category, we can predict that these two entity instances are the instances of the same entity. Thus, if there exists some entity n  X  CN s whose cat-egory is also c s , the same category as the predicted category for the entity mention s , then we can predict that this entity n is the mapping entity n s of the entity mention s ,andwe should link this entity mention s with this entity n ;other-wise, we can predict that the mapping entity of the entity mention s does not exist in the ontology  X , that is to say, n
To evaluate the effectiveness of APOLLO, we conducted a thorough experimental study in this section. Firstly, we tested APOLLO over both of the two subtasks, i.e., fine-grained named entity classification task (described in Sec-tion 6.1) and the entity linking task (introduced in Section 6.2), respectively. Subsequently, we demonstrate the exper-imental results of APOLLO for the task of ontology popu-lation with named entities in Section 6.3.
To the best of our knowledge, there is no publicly avail-able data set for the fine-grained named entity classification task, and the page for accessing the data set provided in [9] is also unavailable. Thus, we constructed the data set for the fine-grained named entity classification task, from the May 2011 version of Wikipedia and YAGO(1) 3 of version 2009-w10-5. To generate the training and test data, we chose 20 categories which are the subclasses of the person category from YAGO. Since the numbers of the instances belonging to different categories vary much, we randomly selected at most 200 instances for each selected category by querying the YAGO ontology, and the created data set DS NEC con-sists of 3304 distinct instances belonging to the 20 categories in total. Since person names are more ambiguous and the categories of person entities are more diverse, the person name classification is much more challenging. Moreover, the experiments of previous methods [12, 28, 15, 14, 13] are all carried out with person names for the fine-grained named entity classification task.

We compared our framework APOLLO with the classifi-cation based approach proposed in [13], which significantly outperforms the single-context rule-based extractor similar to several state-of-the-art techniques for the task of fine-grained named entity classification [13]. We refer to this baseline method as Ganti-KDD . The approach Ganti-KDD considers two types of features, which are text n-gram fea-ture and the list-membership feature. Since these features are all extracted from the multi-context , the union of all contexts across multiple documents within which the entity occurs, they assume that each entity identified in the corpus has been converted to its canonical representation. How-ever, in the real application, the general documents corpus cannot satisfy this assumption. http://www.mpi-inf.mpg.de/yago-naga/yago/
According to the original experimental setting in [13], we used the support vector machine model, libsvm 4 , as the un-derlying classifier. To generate the multi-context features, we employed 3.5 million Wikipedia pages as the document corpus, where each entity in the Wikitext has been converted to its canonical representation, and this characteristic of the Wikipedia document corpus satisfies the assumption of Ganti-KDD . To compute the n-gram features, we extracted all n-grams in the size-4 document context for each occur-rence of an entity, using the presence/absence of the 10K most frequent n-grams among them as features [13]. For the list-membership features, we used the entire document an entity occurs in as the aggregate context of this entity, using a 10% sample of the entities in the training data for each category as the list corpus [13].

To evaluate the performance of APOLLO over the fine-grained named entity classification task, we just eliminated the Linking Validation module in APOLLO, and let the La-bel Propagation module output the final label distribution for each test entity. To generate the semantic signature for each entity in DS NEC , we used its corresponding entire en-tity page in Wikipedia as the document context. In the experiment, we set the edge weight between a named en-tity and each Wikipedia concept in its semantic signature to 1.0 for the purpose of simplicity. In this experiment, the threshold  X  is experimentally set to 0.34, which yields the best performance. The created graph G co contains 82,833 nodes and 155,450 edges, and after we added edges between the semantically related Wikipedia concept nodes into the graph, the final graph G consists of about 2.5 million edges. We refer to the results of our framework APOLLO applied to the graph G co as APOLLO CO , and denote the results of APOLLO applied to G as APOLLO SR . In the Label Prop-agation module, the number of iterations for the Adsorption algorithm was set to 10. In this subsection, we present the evaluation results of APOLLO for the fine-grained named entity classification task. As the output of our framework is a label distribu-tion, we computed the Mean Reciprocal Rank (MRR) of the test entity with respect to the gold standard target cat-egory. In addition, since the baseline method Ganti-KDD employs the SVM model which produces the predicted cat-egory for each test entity, we used the usual metric of Ac-curacy (Accu.) on the classification results to evaluate the performance of Ganti-KDD . Meanwhile, to give a fair com-parison, the accuracy of the predicted category which has the largest distribution in the label distribution of each test entity is also computed for APOLLO. To demonstrate the performance of these approaches with different numbers of training entities, we made the parameter  X  denote the pro-http://www.csie.ntu.edu.tw/  X  cjlin/libsvm/ portion of the training entities in the data set DS NEC .To split the data set DS NEC into the training and test data set with respect to  X  , for each selected category in DS NEC we randomly selected the corresponding number of entities belonging to this category as the training entities, and the remaining entities are regarded as the test entities.
Table 2 shows the experimental results of these approaches over the DS NEC data set under the different settings of the parameter  X  , varying from 0.5 to 0.9. From the results, we can see that the baseline method Ganti-KDD and the ap-proach APOLLO CO have the similar accuracy when  X  is set from 0.5 to 0.8. But when  X  equals to 0.9, the accuracy achieved by APOLLO CO is much higher than what can be achieved by Ganti-KDD . However, it is noticed that the fea-tures of the baseline method Ganti-KDD are all extracted from the multi-context , the union of all contexts across multiple documents within which the entity occurs, while the approach APOLLO CO only leverages the co-occurrence information of the Wikipedia concepts in the semantic signa-ture extracted from the single context . When the features of the baseline method Ganti-KDD are extracted from the single context , the accuracy achieved by Ganti-KDD de-creases greatly, which will be confirmed in Section 6.3.2. By leveraging the semantic knowledge embedded in Wikipedia, the approach APOLLO SR significantly outperforms the base-line method Ganti-KDD in terms of accuracy and the ap-proach APOLLO CO in terms of both MRR and accuracy. Overall, the experimental results indicate that the Wikipedia concepts extracted from the document context and the se-mantic relations between them are quite useful for the task of fine-grained named entity classification.

The detailed results for each category of these three ap-proaches are shown in Table 3 when the training entity pro-portion  X  equals to 0.8. It can be seen from the results in Table 3 that some of these categories are relatively easy to distinguish (e.g., Presidents of the United States and Chi-nese emperors), while some categories (e.g., American so-cialists and American revolutionaries) are very difficult to be classified accurately. In Table 3, for each row (cate-gory), the best accuracy is in bold, and the results show that APOLLO SR obtains the highest accuracy for 12 cate-gories, while the approaches Ganti-KDD and APOLLO CO get the highest accuracy for 7 and 8 categories respectively.
Entity linking is initiated as a task in the track of Knowl-edge Base Population (KBP) at the Text Analysis Confer-ence (TAC). The data set for TAC-KBP track in 2009 5 is available for us, so we used it as the test data set for APOLLO over the entity linking task. According to the problem formulation of ontology population with named en-http://apl.jhu.edu/  X  paulmac/kbp.html tities, our entity linking subtask focuses on the entity men-tion whose mapping entity exists in the ontology, which is called the linkable entity mention. In this TAC-KBP data set, there are total 1675 linkable entity mentions, which re-quire to be linked with the ontology.

To perform the entity linking task, APOLLO firstly pro-duces the candidate entity set CN s for each entity mention s using the dictionary DT introduced in Section 5. To gen-erate the semantic signature for each entity mention in the TAC-KBP data set, we used the entire document where the entity mention occurs as the document context. Next, for each entity mention s , APOLLO creates a graph consist-ing of the nodes coming from the entity mention s itself, the Wikiepdia concepts in its semantic signature  X  s and the candidate entities in CN s .Intheexperiment,wealso set the edge weight between the entity mention s and each Wikipedia concept in  X  s to 1.0. Since all candidate enti-ties in CN s are Wikipedia concepts, so we computed the semantic relatedness between each candidate entity in CN s and each Wikipedia concept in  X  s according to Formula 1. If the semantic relatedness is greater than the threshold  X  , APOLLO creates an edge between them and weights the edge using the relatedness measure. In this experiment, the threshold  X  is experimentally set to 0.086, which yields the best performance. Next, APOLLO annotates each candi-date entity node in the graph with a unique label, and af-ter running the Adsorption algorithm, the candidate entity whose corresponding label has the largest distribution in L is regarded as the mapping entity for the entity mention s . In addition, due to many spelling errors existing in the set of entity mentions, we also tried to correct them using the query spelling correction supplied by Google. To evaluate the performance of APOLLO over the TAC-KBP data set, we adopted the evaluation measure Accuracy , which is used in most work about entity linking. The accu-racy is calculated as the number of correctly linked entity mentions divided by the total number of all entity mentions. The experimental results of APOLLO over the TAC-KBP data set are shown in Table 4. The results of the top 4 systems which perform best over the set of linkable entity mentions in TAC-KBP track of 2009 [18] are also shown in Table 4, for the purpose of comparison. Besides the ac-curacy, we also show the number of correctly linked entity mentions. The results in Table 4 show that APOLLO out-performs the best systems in TAC-KBP track of 2009, which demonstrates the effectiveness of APOLLO over the entity linking task.
To evaluate the performance of APOLLO over the task of ontology population with named entities, the test data set should contain both unlinkable entity mention that requires to be attached to the proper category, and linkable entity mention that should be linked with the entity existing in the ontology. We denote the DS NEC data set used in Section 6.1.1 when the parameter  X  equals to 0.8 as DS NEC X  =0 . 8 which consists of 2643 training entities and 661 test enti-ties. In the following experiments, we regard the training entities in the DS NEC X  =0 . 8 data set as the set of named en-tities contained in the ontology. In addition, besides the test entity mentions in DS NEC X  =0 . 8 which are all unlink-able, we added some new test entity mentions which can be linked with the named entities existing in the ontology. To make the newly added test entity mentions linkable, we randomly sampled 20% of the named entities for each cat-egory existing in the ontology, and regarded the names of these sampled entities as the set of newly added test entity mentions. For each newly added test entity mention, we obtained its document context via querying its name with Google. The top ranked page which is not from Wikipedia is regarded as its candidate document context, because the corresponding Wikipedia page has been regarded as the doc-ument context for its corresponding mapping entity in the ontology. Then we verified whether the entity described in the candidate document context is the same as the corre-sponding mapping entity by human judgments. If so, we added this test entity mention and its candidate document context to the test data set; Otherwise, we removed it. Fi-nally, we obtained the data set for the ontology population task (which we refer to DS OP ), in which the set of named entities contained in the ontology is the same as the train-ingdatasetof DS NEC X  =0 . 8 , and the test data set consists of 661 unlinkable test entity mentions from the original test data set of DS NEC X  =0 . 8 and the newly added 372 linkable test entity mentions.

We added the Linking Validation module of APOLLO to the baseline method Ganti-KDD introduced in Section 6.1.1 to create the baseline method BASELINE OP for the ontol-ogy population task. The features for the named entities existing in the original DS NEC X  =0 . 8 data set are extracted from the multi-context in the same way as in Section 6.1.1. Whereas for the newly added linkable test entity mention whose document context is not Wikitext , we are only able to extract the features existing in this single context, rather than across multiple documents, since we cannot obtain the canonical representation of the entity mention. It is fairly common for one of the mention forms of an entity in a doc-ument to be a long and typical mention form of that entity (e.g.,  X  X ichael Jordan X ), while the other mention forms of the same entity are shorter mention forms (e.g.,  X  X ordan X ). To generate richer features for each linkable test entity men-tion for the BASELINE OP method, we used a simple in-document coreference resolution method which is to map shorter mention forms to the long and typical entity men-tion form in the same document.

We applied APOLLO to the DS OP data set to evaluate the performance of APOLLO for the task of ontology population with named entities. We used the same setting for APOLLO as described in Section 6.1.1. The final graph G contains 99,609 nodes and about 3.1 million edges.
To evaluate the performance of APOLLO and BASELINE OP over the DS OP data set, we also adopted the evaluation mea-sure Accuracy (Accu.), which is used in both of the two sub-tasks. For the unlinkable test entity mention, if it is attached to the gold standard category, it is regarded as correct. And for the linkable test entity mention, if it is linked with the correct entity, we consider it as correct. The overall accu-racy is calculated as the number of all correctly assigned entity mentions divided by the total number of the entity mentions.

The experimental results of APOLLO and BASELINE OP over the DS OP data set are shown in Table 5. We show the accuracy and the number of correctly assigned entity mentions for both APOLLO and BASELINE OP , according to the different types of the test entity mentions (i.e., all, unlinkable and linkable). From the results in Table 5 we can see that APOLLO achieves significantly higher accuracy compared with the baseline method BASELINE OP in all as-pects. For the set of linkable test entity mentions, the accu-racy achieved by BASELINE OP (23.66%) is much lower than the accuracy achieved by APOLLO (81.72%) which lever-ages the rich semantic information derived from Wikipedia. Furthermore, from the experimental results shown in Ta-ble 5 and Table 2, we can see that for the same set of the unlinkable test entity mentions, APOLLO obtains higher ac-curacy (75.34%) over the DS OP data set in comparison with the accuracy (74.89%) achieved over the DS NEC X  =0 . 8 data set. The main reason is that via adding some linkable test entity mentions and the Wikipedia concepts existing in their semantic signatures , the graph created from the DS OP data set is more beneficial for the process of label propagation. Therefore, it can be seen that APOLLO can obtain better performance over the richer graph structure, which demon-strates the scalability of APOLLO.
The classic named entity classification task is confined to classify named entities into coarse-grained categories, such as person, location and organization. It has been investi-gated for several years by means of supervised approaches, which require a large number of manually tagged texts as the training data. As ontology generally contains hundreds of entity categories, supervised methods are not directly ap-plicable for ontology-based fine-grained named entity classi-fication, because the amount of the training data they need is too large, and the process of manually creating such anno-tated data requires too much human effort. Recently, many weakly supervised systems have emerged to address the task of fine-grained named entity classification [6, 28, 15, 14, 13].
Cimiano and V  X  olker [6] addressed the fine-grained classifi-cation of named entities based on the Harris X  distributional hypothesis as well as the vector space model. This method assigns a named entity to the contextually most similar con-cept from the ontology. The empirical results show that the pseudo-syntactic dependencies are an interesting alternative to the word window-based approaches.

Tanev and Magnini [28] proposed a weakly supervised ap-proach to automatically populating a part of their ontology with named entities from text. For each category in the ontology, the algorithm learns a feature vector exploiting the lexico-syntactic information extracted from the contexts where the entities belonging to this category occur. They assumed that the named entities in the test data set are not ambiguous and they did not consider the problem of entity ambiguity. However, this assumption does not remain true in the real data sets.

Giuliano and Gliozzo [15] presented an instance-based learn-ing algorithm for fine-grained named entity classification against the People ontology, an excerpt of the WordNet on-tology. This proposed approach is based on a lexical substi-tution technique, and the plausibility of the generated sen-tence is estimated using the Web data. In a similar setting, Giuliano [14] proposed a kernel-based method that implic-itly maps entities, represented by aggregating all contexts in which they occur, into a latent semantic space derived from Wikipedia. However, in both of these two approaches, to collect sufficient contextual information for each named en-tity, these systems query the search engine with the name of the entity, and consider all snippets retrieved by the search engine referring to the same entity. Therefore, they ignored the problem of ambiguity of proper names.

The approach presented in [13] is the baseline method in-troduced in Section 6.1.1. The experimental results in [13] show that this method significantly outperforms the single-context rule-based extractor similar to several state-of-the-art techniques for the task of fine-grained named entity clas-sification. In this study, the authors assumed that each en-tity identified in the corpus has been converted to its canon-ical representation. However, in the real application, the general documents corpus cannot satisfy this assumption. As more and more knowledge bases like DBpedia [1] and YAGO [26] are available publicly, the entity linking task has attracted great interests of many researchers starting from Bunescu and Pasca [4], who used the bag of words model to measure the cosine similarity between the context of the en-tity mention and the text of the Wikipedia article. Cucerzan [7] proposed a solution which is the first system to recognize the global document-level topical coherence of the entities. The system addresses the entity linking problem through a process of maximizing the agreement between the context of the entity mention and the contextual information extracted from the Wikipedia, as well as the agreement among the cat-egories associated with the candidate entities. The learning basedsolutionin[8]focusesontheclassificationframework to resolve entity linking. It develops a comprehensive fea-ture set based on the entity mention, the contextual docu-ment and the knowledge base entry, and then uses a SVM ranker to score each candidate entity. Our previous work [24] proposed LINDEN to deal with the entity linking task. LINDEN is a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet, by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base. Over-all, the essential step of the entity linking task is to define a similarity measure between the text around the entity men-tion and the document associated with the entity.
In this paper, we have studied the problem of ontology population with named entities. We propose APOLLO, a novel unified framework to resolve the task of automatic on-tology population with named entities integrally via random walks on graphs. APOLLO is a weakly supervised frame-work and can be easily parallelized. To evaluate the effec-tiveness of APOLLO, a thorough experimental study was conducted, and the experimental results demonstrate that APOLLO achieves significantly higher accuracy for the on-tology population task compared with the baseline method, by leveraging the rich semantic knowledge embedded in the Wikipedia. Furthermore, we extensively evaluated the per-formance of APOLLO over both subtasks, and the experi-mental results show that APOLLO outperforms the baseline methods for both subtasks.
This work was supported in part by National Natural Sci-ence Foundati on of China under Grant No . 60833003, Na-tional Basic Research Program of China (973 Program) un-der Grant No. 2011CB302206, and an HP Labs Innovation Research Program award.
