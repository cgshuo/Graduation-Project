 This paper proposes a framework for cost-sensitive classification under a generalized cost function. By combining decision trees with sequential binary programming, we can handle unequal misclassification costs, constrained classification, and complex objective functions that other methods cannot. Our approach has two main contributions. First, it provides a new method for cost-sensitive classification that outperforms a traditional, accuracy-based method and some current cost-sensitive approaches. Second, and more important, our approach can handle a generalized cost function, instead of the simpler misclassification cost matrix to which other approaches are limited. This paper introduces a framework for cost-sensitive classification under a generalized cost structure. The limitation of traditional classification methods in instances with unequal misclassification costs is well known [5]: in such cases, these methods maximize accuracy instead of maximizing profits (or minimizing costs). As a result, approaches have been developed that are sensitive to these unequal costs. While newer methods do consider simple, unbalanced misclassification costs, they do not, however, consider more complex cost structures that may be present in the real world. classification that can handle any cost structure. By using mathematical programming techniques, we can in fact handle any objective function, freeing us from the constraints of a cost matrix. The remainder of the paper is organized as follows. In section 2 we review prior work on cost-sensitive classification. In sections 3-5 we discuss limitations of current approaches, followed in sections 6-7 by a presentation of our method that addresses these limitations. Section 8 tests our approach on a public example data set and sections 9-11 conclude with a discussion of future work we are currently pursuing. The goal of classification is to learn from past data how to predict the class label of yet unseen examples. Past data are partitioned on attributes that will be known for future observations, and each partition is assigned a class label for use in prediction. Partitions are considered worthwhile if they successfully predict the class labels of future observations. Existing methods for partitioning the feature space typically focus on purity or expected error rate and include methods such as C4.5 [14] and CART [2]. traditional methods typically do not perform well since they maximize accuracy instead of maximizing profits. with non-symmetric costs of error. For Gaussian processes and generalized cost of error functions, he showed that a bias term should be added to obtain what he called cost-optimal predictions. Usage, however, focused on contexts slightly different than classification in which error sizes and cost of error functions were continuous valued. More recently, the field of cost-sensitive learning has produced methods that address this problem of unequal error costs in the context of classification [5]. Cost-sensitive methods typically represent misclassification costs in a cost matrix. Table 1 shows such a cost matrix for a direct marketing example. Prediction Cost-sensitive decision tree methods generally fall in one of three areas insensitive classifier, but afterward the predicted class labels are revised if doing so lowers the expected cost [5]. Each partition X  X  estimated class probability is used to determine the expected cost under each class label, and the label with minimum expected cost is selected. insensitive classifier [22]. However, instead of relabeling the tree ex post, these methods ex ante alter the training set X  X  ratio of positive/negative classes relative to their misclassification costs. As such, the tree X  X  majority-based (50%) leaf-labeling rule on the reweighed set should now correspond to a cost-optimal labeling threshold (different than 50%) on the un-weighted data. For example, using the matrix of Figure 1, the training set X  X  number of  X  X uyers X  could be increased four-fold (or number of  X  X on-buyers X  reduced by 75%) and then the altered data inputted to a cost-insensitive classifier. This method also presents challenges: a special sampling method such as Costing may be needed to properly reweigh the data, and application to multi-class problems may not be possible [22]. insensitive classifiers and address costs (or profits) in pre-or post-processing. In this way, the objective of the classifier and the real objective of the user have become disjoint. In contrast, a direct approach integrates the two. Turney [17] and Kwedlo et al. [10] incorporate costs in a genetic algorithm X  X  fitness function. Ling et al. [11] use total cost (misclassification and testing) as a decision tree X  X  splitting criterion. Tan [15] modifies the splitting criteria to consider each feature's ratio of information gain to measurement cost, but the focus is more on reducing measurement costs than unequal misclassification costs. Our method also directly incorporates costs in the learning phase. In addition, however, it allows for any objective function (not just the simpler cost matrices used by current reclassification and re-weighing methods). Several other approaches have been designed to handle  X  X eneral X  cost functions, and their limitations will be discussed in section 10. classification problem. Assume a data set of tuples : . We henceforth use the term generalized partitioning optimization problem (GPOP) for problems in which we maximize a function F ( g (  X  )). The objective function F depends on our classifications, and the decision variables are the partitions and labels induced by classifier g As such, traditional classification is just the special case in which the objective function represents accuracy. Similarly, the traditional cost-sensitive classification problem above is the case in which F is linear and represents net benefits (or negative costs). algorithms, even after being modified to be cost-sensitive, are not suited for many GPOPs. the word optimization . In classification this term is often associated with Support Vector Machines, which attempt to maximize predictive accuracy by minimizing error. However, our use of the term refers to the objective (profit/cost) function F above. We next illustrate the limitation of traditional classifiers (cost-sensitive or not) to address GPOPs. Consider the example below in which we use C4.5 with cost-sensitive leaf re-classification to classify buyers and non-buyers. Figures 1 and 2 show a set of customers with two continuous attributes. In this model, correctly predicting a buyer yields $15 profit, whereas misclassifying a non-buyer yields a $4 loss. The figures show that an entropy-based method, even with the cost-optimal leaf labeling, may create suboptimal partitions: information gain is greater with C4.5 X  X  split in Figure 1, but profits are higher with the split in Figure 2. This difference suggests a purity-based split criteria may not always be adequate. cannot be applied to more complex GPOPs. Consider the archetypal golf data used to show the power of C4.5 [8] [14]. vector describes each day X  X  weather conditions and the class label is a recommendation whether or not to play golf. The golfer would like to know, given the upcoming day X  X  weather conditions, whether or not he should pay greens fees to play that day. Use of C4.5 produces a tree with 100% accuracy on the training data (Figure 3). complex objective. This golfer is practicing for a pro tour in southern Florida, where it is rarely windy. Thus he wishes to practice as much as he can, but he does not want to get used to compensating for wind. He is able to quantify the utility of each practice session according to Table 3. Furthermore, he considers it worth an extra $200 if less than 30% of the days he plays on are windy. U Total =  X  U Daily + $200 if windy days &lt; 30% single decision tree model with either the reclassification or reweighing approaches. Using reclassification, C4.5 builds a 100% accurate tree and then relabels one of its leaves from Don X  X  Play to Play (Figure 4). C4.5 with reclassification yields training profit of $230. However, growing a tree that accounts for our true objective during the learning phase (as discussed in section 6), generates training profit of $360 (section 8 illustrates that such improvements often hold on test data as well). The use of profit (cost) as a splitting criterion is present in direct cost-sensitive learning work such as Ling et al. [11]. However, these direct approaches only consider GPOPs whose costs can be represented in matrix form or have constant example-specific costs. The main contribution of our method is its ability to accommodate any objective function, such as the modified golf example above. The method we have developed is motivated by traditional reclassification. Leaf reclassification can actually be viewed as solving a binary program: Assume the following:  X  We have a training set S ={ t  X  We have divided set S into n partitions (leaves), with  X  Each partition Y  X  The objective (profit) function, F , is dependent on  X  Decision variables can be formulated as: x We may then formulate the following binary program: ( ) Here, a ~ b represents an equivalence relationship between a and b , meaning they are in the same partition of S : r Y b a  X  , [1] purity related metric and then, in essence, solves this binary program once to determine the profit (cost) optimal classifications for each partition. We propose a novel method entitled Sequential Binary Programming (SBP). Using SBP to solve GPOPs begins with the original set S and then iteratively solves this binary program for each available 1-dimensional split on each objective function value is then accepted, and further partitioning evaluated until additional partitions fail to improve the objective function or a stopping metric is reached. Examples of a stopping metric are maximum tree depth or minimum leaf size. With each iteration, SBP allows us to select the partition that yields the optimal reclassification. In this way, the true objective function is used throughout the growing process. sensitive methods is its ability to accommodate any objective function, not just a cost matrix. In addition, we may incorporate other ideas from mathematical programming in data mining. For instance, in the golf example above, consider adding the constraint: no more than 10% of days the golfer plays can be sunny. In such a case, we would simply add the following constraint to the binary programming sub-problem: cannot, of course, be guaranteed to hold on out-of-sample data, in some cases we may obtain reasonable adherence, while in other cases, being at least close to the constraint may be better than disregarding it entirely. While it may seem prohibitively expensive to solve a series of integer programs for each iteration it is important to note that the binary program above has some desirable characteristics. First, the binary programs in each iteration are simple relaxations of one or more preceding iterations, differing only by several x i,q constraints. The reductions in these constraints arise from acceptance of a new partition at the end of an iteration, and resulting set of equivalence relationships. This immediately gives us a bound on the primal for branch-and-bound solution methods. substitution. Generically, we may replace collections of individual row decision variables with variables representing whole partitions, as well as drop one set of classification variables in favor of non-binary-constrained slack. Together these operations transform an m x p problem into an nx ( p -1) problem and eliminate several constraints: Transforming the objective function appropriately, we In the case where the objective function is linear (this applies with the use of cost matrices or certain example specific costs), we may take great advantage of these properties. Consider the above problem with a linear objective function. Without loss of generality, assume that the partition accepted at the end of the previous iteration has split previously existing partition into Y n-1 and Y n . We may then write our program: max This is indeed a separable program, the first section of which has already been solved as part of a previous iteration. This leaves us with the following two simple programs (after further separation) needing solution: These programs can be solved by selecting the classification which corresponds to the highest objective function coefficient, for each of partition Y n-1 and Y For a generalized non-linear objective function (  X  ), we may reduce computation time, finding the approximate solution to each SBP sub-problem, by substituting  X  into the equations above. We call this method Sequential Binary Programming with Linearization (SBPL). Additionally, for increased speed, we may model constrained classification problems through a combination of penalty/multiplier methods and SBPL. The SBP method (with a maximum tree depth of 3 levels) was compared to C4.5 with leaf re-classification and rejection sampling [22] on a public data set. In addition, Na X ve Bayes, Curtailment [20], and an advanced form of logistic regression called TR_IRLS_CGDEV [9] were tested with cost-sensitive reclassification applied. demographic and insurance purchase data for a large customer base and was used in CoIL Challenge 2000 data mining competition [19]. One interesting aspect of this set is that less than 6% of observations were buyers. The objective of the CoIL competition was to correctly classify an unknown test set as buyers or non-buyers of mobile home insurance policies. A group of our students created a version of the Caravan problem using the following profit matrix: 
Table 4. Profit matrix for Caravan insurance-Prediction For evaluation purposes, 30% of the Caravan data set was used as holdout data, while the rest was used for model training. Table 5 shows the results of SBP along with the aforementioned methods. Table 5. Results of classification methods on reclassification 
C4.5 w/ rejection C4.5 w/ Curtailment $29,028 
TR_IRLS_CGDEV $28,645 Table 5 shows the propensity for an entropy-based method corrected for cost-sensitive learning to over-fit a training set where one class occurs at a relatively low frequency relative to the other. More recent methods, such as Curtailment and TR_IRLS_CGDEV show more stable results. However, use of SBP demonstrates increased profits over all of these methods when derived rules are held over onto the test set. We have demonstrated that for cost-based classification problems, our approach outperforms a traditional accuracy-based method as well as several cost-sensitive methods. We have only begun to benchmark results against the most recent advances in decision tree learning, such as smoothing and curtailment [20], but plan to do more so in future work and across a larger number of data sets. A possible limitation we are investigating is overfitting when costs are example-specific and in particular when they have high variance. We plan to test this case in future work as well design methods that may alleviate such an issue if present. On interpreting the above results, it is imperative to stress the following: regardless of which method performs better in these comparisons, the bigger contribution of the SBP method is its ability to handle generalized cost functions and constrained classification. Thus benchmarking is appropriate for simpler cost-based problems. However, with more complex problems, we cannot perform benchmarking because our method is the first we know of that accommodates such a wide range of objective functions in the classifier 2 . We may now, for example, study medical treatment examples with economies of scale, bulk discounts on drug costs, and shipping constraints. The traditional view of a cost matrix or simple example-specific costs no longer limits the formulation. The term  X  X eneral cost function X  has been used previously in classification literature. However, methods designed to deal with such  X  X eneral costs X  are built to handle generalized cost matrices which incorporate example specific costs (such as the cost of credit card fraud being transaction specific). While both SBP and SBPL handle these types of generalizations, they also directly handle non-linear costs functions such as those arising from bulk discounts. SBP treats the optimization of each sub-problem as a black box. Thus any non-linear programming solver may be plugged into the SBP algorithm to best suit our objective function. Among the other methods and frameworks that are designed to handle  X  X eneral X  cost functions are the following:  X  ICET [17]:  X  X eneral X  cost-sensitive learning may be  X  MetaCost [4]: MetaCost obtains example-specific The benefits of the GPOP formulation and SBP method are twofold. First, we can perform decision tree-based cost-sensitive classification under a generalized objective function by incorporating mathematical programming in the tree-growing phase. Second, the use of profit as a growing metric is likely to seem less abstract than notions such as entropy and Bayesian classification to those in the business community. in commerce, we have developed this method into an Excel add-in for business analysts. This add-in, as part of our ongoing research, is an attempt to transform the management perspective of data mining as a highly technical process, for which firms must often hire a consultant, to something as understandable and ubiquitous as Excel Solver. modeling of SBP sub-problems as generalized assignment problems [13]. Essentially, we may consider the classes as workers, each of which we must assign to one or more available facilities (partitions). This formulation opens the door for a large family of solution algorithms, letting us quickly solve GPOPs with quadratic objective functions and SBP sub-problems with a large functionality of genetic algorithms, simulated annealing, and speed-increasing heuristics for solving SBPs. 
