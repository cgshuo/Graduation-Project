 Many recent and highly effective retrieval models for long queries use query reformulation methods that jointly op-timize term weights and term selection. These methods learn using word context and global context but typically fail to capture query context. In this paper, we present a novel term ranking algorithm, PhRank, that extends work on Markov chain frameworks for query expansion to select compact and focused terms from within a query itself. This focuses queries so that one to five terms in an unweighted model achieve better retrieval effectiveness than weighted term selection models that use up to 30 terms. PhRank terms are also typically compact and contain 1-2 words com-pared to competing models that use query subsets up to 7 words long. PhRank captures query context with an affin-ity graph constructed using word co-occurrence in pseudo-relevant documents. A random walk of the graph is used for term ranking in combination with discrimination weights. Empirical evaluation using newswire and web collections demonstrates that performance of reformulated queries is significantly improved for long queries and at least as good for short, keyword queries compared to highly competitive information retrieval (IR) models.
 H.3.3 [ Information Search and Retrieval ]: Query For-mulation Random Walk; Verbose Queries; Query Reformulation
Query reformulation is a rich area of information retrieval (IR) research, including techniques for query expansion, de-pendency analysis, query segmentation and term selection. For short queries, IR effectiveness is improved by smooth-ing and query expansion using techniques such as pseudo relevance feedback [14, 23, 34]. Conversely, long or verbose queries contain words that are peripheral or shared across many topics so expansion is prone to query drift. Reformula-tion instead focuses on term weighting [18, 5], term selection [3, 2] and query reduction [15]. The selection of informative terms, defined as one or many-word units, becomes critical as the number of potentially noisy terms increases.
Techniques for term selection and term weighting auto-matically emphasize the  X  X ssence X  of a query. Several suc-cessful techniques jointly optimize weights and term selec-tion using both global statistics and local syntactic features [3, 33]. However, these features can fail to detect or differ-entiate informative terms, where an informative term repre-sents the essential aspects of query meaning given a collec-tion of documents. Global statistics are strong indicators of term importance [4] but do not reflect local query context. There is also evidence that they do not lead to significant improvement in query effectiveness [20]. Syntactic features precisely identify word relations but do not identify all the informative relations [22]. The ubiquity of global statistics and syntactic features in current methods for term selection suggests a continuing need for improved understanding of alternatives ways to estimate term informativeness [20].
In this paper, we present PhRank (phrase rank), an al-gorithm that uses pseudo relevance feedback for in-query term selection rather than expansion to terms not in a query. Compact and focused terms are selected from a list of can-didates by ranking terms using a Markov chain framework and picking the top-ranked candidates. Candidate terms are all combinations of 1-3 words in a query that are not stop-words. Term scores are computed using the average word score for words in a term, combined with global discrimi-nation weights. Word scores are computed using a random walk of a word co-occurrence graph constructed from pseudo relevant documents, combined with word salience weights for the query and global contexts. This approach selects terms that achieve significant gains in both recall and pre-cision compared to the most effective techniques for query reformulation that do not use term weighting, expansion, or stratified dependencies [4]. This is achieved by focusing on a limited number of word relationships with a core concept. PhRank has three advantages compared to previous work. First, to our knowledge PhRank is the first method to use pseudo relevance feedback for in-query term selection. Feed-back was applied initially to estimate weights for indepen-dent query words without selection [8] and is predominantly used for query expansion. Previous approaches to in-query term selection use highly localized word context ,intheform of syntactic relations and co-occurrence, and global context in the retrieval collection. They do not consider query con-text , such as a general query topic identified from pseudo relevant documents. The intuition behind a random walk of a query context graph is that it reinforces words that capture query  X  X ssence X  more strongly than words that are periph-eral to query meaning. For this reason, informative terms are more readily apparent if query context is considered.
Second, PhRank achieves significant performance gains with a small number of compact terms while retaining the flexibility to select more and longer terms if required. Other approaches use a robust, but less effective, distribution over many imprecise but approximately relevant terms. Alterna-tively, they take a relatively inflexible, high-risk approach that prefers a few exact terms and is prone to mistakes. For example, Table 1 shows the terms selected for TREC topic #756 by three top performing IR models. The sequential dependence (SD) model is straightforward and robust [24]. The key concept (KC) model [3] aims at a highly succinct representation but is hampered by a requirement that terms are predefined syntactic units (noun phrase length). The subset distribution (SDist) model [33] optimizes over many term and weight variables and is highly effective but is bi-ased towards longer terms of 3-6 words. PhRank demon-strates that for a majority of queries, a few precise terms, in addition to a standard query likelihood representation, are more effective than term distributions. They also result in queries that have up to 90% fewer terms, and these terms are typically only 1-2 words long.

Finally, an affinity graph captures aspects of both syn-tactic and non-syntactic word associations in an integrated manner. A co-occurrence affinity graph shares the same structure as a global dependency graph in which edges are defined by linguistic relations. Specifically, the most con-nected vertices are high frequency functional words and less frequent content-bearing words tend towards the edges of the graph [10, 11]. By consequence, the semantic significance of a word is correlated with the degree of the corresponding vertex. We infer that the shared structure of dependency and affinity graphs captures aspects of both syntactic and non-syntactic word associations. Moreover, an affinity graph can be used to estimate the semantic significance of words. To summarize, unlike existing models of term selection, PhRank integrates three characteristics that we believe are important to accurately identify the most informative terms: query context, compactness, and integration of syntactic and semantic knowledge. We show that consolidating these characteristics delivers up to 14% performance improvement compared to highly competitive methods for TREC descrip-tion topics and is comparable to the state-of-the-art for TREC keyword (title) queries.

The rest of this paper is organized as follows. In Section 2 we review related work and its connection to PhRank. Sec-tion 3 defines the problem of term selection and its key char-acteristics. In Section 4 we formally describe the PhRank algorithm. Section 5 presents the evaluation framework. In Section 6 we discuss the results of empirical experiments, and Section 7 concludes the paper.
Markov chain frameworks and spreading activation net-works for a network of words are well-studied in IR with origins in associative word networks [7]. They include re-search on webpage authority [26], e.g. PageRank, as well as query expansion [17, 6, 23, 14]. However, they are novel for unexpanded term selection.

The Markov chain framework uses the stationary distri-bution of a random walk over an affinity graph G to esti-mate the importance of vertices in the graph. Vertices can represent words, in which case edges represent word asso-ciations. If the random walk is ergodic, affinity scores at vertices converge to a stationary distribution that can be used to establish a ranking, e.g. over words.

A random walk describes a succession of random or semi-random steps between vertices v i and v j in G .Let ij be the transition probability (or edge weight) between v i and v The path of the walk is determined by a square probability matrix H =( h ij ) with size n ,where n is the number of unique vertices in G . The probability h ij = ij if v i and v are connected, and h ij = 0 otherwise. Affinity scores are computed recursively. Let  X  t j be the affinity score associated with v j at time t .Then  X  t +1 j is the sum of scores for each v connected to v j , weighted by the possibility of choosing v as the next step on the path from v i :
It is usual to introduce some minimal likelihood that a path from v i at time t will randomly step to some v j at time t + 1 that may be unconnected to v i . Otherwise, clusters of vertices interfere with the propagation of weight through the graph. This likelihood is often defined to be the uniform probability vector u =1 /n , although any other vector can be chosen [14]. A corresponding factor reflects the likeli-hood that a path will follow the structure of edges in G .A damping factor  X  controls the balance between them:
The Markov chain framework has has been used in a prin-cipled way to smooth and expand queries in a language mod-eling framework [34], but application in query reformulation has been limited to selection of individual words that do not appear in the original query. By contrast, PhRank ranks terms containing one or more words that do appear in the original query. Moreover, while expansion techniques can exacerbate problems with unrelated terms, PhRank reduces the problem of query drift through improved term selection.
Markovchainprocesseshavealsobeenappliedintext summarization for keyphrase extraction. This is a task sim-ilar to term detection for automated indexing. TextRank [25], SingleRank [32] and ExpandRank [32] use a random walk to identify salient sequences of nouns and adjectives. They improve over earlier unsupervised methods for this task but achieve only 30-40% task accuracy and may be outperformed by a tf.idf metric [13]. ExpandRank supple-ments text with pseudo relevant documents but does not im-prove performance compared to SingleRank [13]. PhRank is similar to these algorithms but is more flexible and better suited to IR. It uses multiple sources of co-occurrence evi-dence and the discriminative ability of terms in a collection. It also produces an unbiased ranking over terms of mixed lengths, does not rely on syntactic word categories such as nouns, and permits terms to contain words with long dis-tance dependencies.

Other related work focuses on techniques for identification of dependent terms [28], key concepts [3], or sub-queries [16, 33]. This includes techniques for the removal of stop structure [15]; reduction of narrative queries to word se-quences associated with part of speech blocks [19]; selection of candidate sub-queries using noun phrases [3]; query term ranking using dependency tree relations [27]; and optimized ranking over possible subqueries [33]. There is also a sig-nificant body of work on learning individual term weights [18, 5]. Much of this work incorporates syntactic and statis-tical features in machine learning.
We hypothesize that the following principles define word and term informativeness. These principles motivate the PhRank algorithm detailed in the next Section.
 An informative word : 1. Is informative relative to a query: An informa-2. Is related to other informative words: The Asso-An informative term: 3. Contains informative words: Consider a base case 4. Is discriminative in the retrieval collection: A
PhRank captures query context with an affinity graph constructed from stopped, stemmed pseudo-relevant doc-uments. Vertices in the graph represent unique stemmed words (or simply, stems ). Edges connect stems that are ad-jacent in the processed pseudo relevant set. Graph transition probabilities (edge weights) are computed using a weighted linear combination of stem co-occurrence, the certainty that the document in which they co-occur is relevant, and the salience of sequential bigram factors in the pseudo relevant set. The edge weights thus represent the tendency for two stemmed words w i and w j = i to appear in close proximity in documents that that reflect a query topic.

Stems in the affinity graph are scored using a random walk algorithm. Following convergence, stem scores are weighted by a tf.idf style weight that further captures salience in the pseudo relevant set. This aims to compensate for potential undesirable properties of the random walk. Finally, term scores are computed using the average score for stemmed wordsinaterm,weightedbytermsalienceintheretrieval collection. The m highest scoring terms are employed to reformulate Q . Pseudo code for the algorithm is shown in Figure 1. The rest of this section describes the algorithm in more detail, including three heuristic weights (factors r , s and z ). A number of choices for these factors could have been made and specific choices are analyzed in Section 6.1. 1) Graph construction (principle 1):
Let a query Q = { w 1 , ...w n } and C be a document col-lection. The top k documents retrieved from C using Q are assumed to describe a similar topic to Q . We define C to be the retrieval collection plus English Wikipedia. We use Wikipedia since it improves IR results for query expansion using a random walk [6], but also explore the effectiveness of using the retrieval collection alone. The top k documents in C , together with Q itself encoded as a short document d ,comprise neighboring documents in the neighborhood set N = { d 0 , ....d k } .

Documents in N are stopped using a minimal list of 18 words [21] and stemmed using the Krovetz stemmer. This improves co-occurrence counts for content-bearing stems and reduces the size of an affinity graph G constructed from the processed documents. Stoplisting with a longer list hurt IR effectiveness. Edges in G connect stemmed words i and j at vertices v i and v j if i and j are adjacent in N .Documents in N with only one word (e.g. some queries) are discarded to ensure that all vertices have at least one connecting edge. 2) Edge weights (principle 1):
Transition probabilities (edge weights) ij are based on a weighted linear combination of the number of times i and
Figure 1: Pseudocode for the PhRank algorithm. j co-occur in windows W of size 2 and 10. This is moti-vated by the idea that different degrees of proximity provide rich evidence for word relatedness in IR [25, 32, 24]. Edge weights are defined by: where p ( d k | Q ) is the probability of the document in which the stems i and j co-occur given Q ,and c ij W 2 and c ij W the counts of stem co-occurrence in windows of size 2 and 10 in N .  X  is set to 0.6. We set the relevance of d 0 to Q to be high but reasonable (-4 for Indri log likelihood scores). The exact setting has very little effect on term ranking.
Factor r is a tf.idf style weight that confirms the impor-tance of a connection between i and j in N . G includes many stemmed words, so unweighted affinity scores can be influ-enced by co-occurrences with highly frequent, but possibly uninformative, stems such as  X  make  X . Factor r minimizes this effect. Since the tf component is already accounted for by  X c ij W 2 +(1  X   X  ) c ij W 10 , we reduce r to the idf component: 3) Random Walk (principle 2):
A random walk of G follows the standard Markov chain framework presented in Section 2. Edge weights are normal-ized to sum to one and  X  j is the affinity score of the stem associated with v j .  X  j indicates the importance of a stem in the query context. Iteration of the walk ceases when the dif-ference in score at any vertex does not exceed 0.0001. This translates to around 15 iterations but may be optimized for efficiency. The damping factor  X  =0 . 85 is equivalent to a walk along five connected edges in G before the algorithm randomly skips to a possibly unrelated vertex. The aver-age sentence length in English is around 11-15 words so this equates to skipping at or near the boundary of a sentence around one half of the time. 4) Vertex weights (principle 3):
Following the random walk, stemmed words in G are fur-ther weighted to capture both the exhaustiveness with which they represent a query, and their global saliency in the col-lection [30]. Exhaustivity indicates whether a word w 1 is a sufficient representation of the query. If w 1 appears many times in N then it is less likely that a term x containing w will benefit from additional words w 2 ...w n . For example, the term geysers quite exhaustively represents the TREC query #840,  X  Give the definition, locations, or characteristics of geysers  X . A term containing additional words, e.g. defini-tion geysers , is not more informative. However, common stems, such as  X  make  X , tend to have high affinity scores be-cause they co-occur with many words.

Factor s balances exhaustivity with global saliency to iden-tify stems that are poor discriminators been relevant and non-relevant documents. Specifically, s w n = w n f avg  X  where w n f avg is the frequency of a word w n in N , averaged over k + 1 documents (the average frequency) and normal-ized by the maximum average frequency of any term in N . As usual, idf w n is the inverse document frequency of w n cabulary of stemmed words in the collection C ,and df w n the number of documents in C containing w n .

An advantage of factor s is that it enables PhRank to be independent of an IR model. A model may treat the com-ponent words of terms as independent or dependent. Factor s helps to ensure that the selected terms are informative irrespective of this representation. 5) Term ranking (principles 3, 4):
To avoid a bias towards longer terms, a term x is scored by averaging the affinity scores for its component words { w 1 , [ ...w n ] } . Term rank is determined by the average score multiplied by a factor z x that represents the degree to which the term is discriminative in a collection: Let x e be a proximity expression such that the component words of x appear in an unordered window of size W =4per word. Thus, a term with two words appears in an 8-word window, and a term with three words appears in a 12-word window. The frequency of x e in C is f x e and idf x e is defined analogously to idf w n above. l x is an exponential weighting factor proposed for the normalization of ngram frequencies during query segmentation [12]. This factor favors longer ngrams that tend to occur less frequently in text. Multipli-cation of ngram counts by l x enables comparison of counts for terms of varying length. Let | x | be the number of words in x ,then l x = | x | | x | .

In summary, the PhRank algorithm describes how infor-mative a term x is for Q compared to other terms. This is computed using the function:
PhRank often assigns a high rank to multi-word terms that contain only one highly informative word. This is due to use of an average word affinity score, and is desirable be-cause informative terms can contain uninformative words. For example, given a query about  X  the destruction of Pan Am Flight 103 over Lockerbie, Scotland  X  (TREC #409), the term  X  pan flight 103  X  is informative even if the polysemous word  X  pan  X  is uninformative by itself. However, this can re-sult in low diversity of top ranked terms used in query refor-mulation. To increase diversity, we apply a simple, heuristic filtering technique with top-down constraints.

Given a ranked list, all terms with a score of zero are discarded. The lowest ranked term, x n , is checked against the list of terms with a better rank x m&lt;n .Let A be the set of component words in x n and B be the set of component words in any single term x m&lt;n .If  X  A : A  X  B  X  A  X  B then we discard x n iff every component word of x n is contained in at least one x m = n that is in the retained list of ranked words at the time B is evaluated. For example, if x n = X  birth rate  X  and we find some x m&lt;n = X  birth rate china  X  X henwediscard x n on the assumption that the longer term better represents the information need. If x n = X  declining birth rate  X  X ndwe find some x m&lt;n = X  birth rate  X  X ndsome x m&lt;n = X  declining birth  X  X henwediscard x n on the assumption that the shorter terms better represent the information need and the longer term is redundant. Note that the top-ranked term is always retained. This process is adequate to increase diversity in the ranked list and ensures that no vital information is lost, but clearly presents an opportunity for further improvement.
This section describes comparative models and query re-formulations used to assess the degree to which PhRank queries are robust, precise and succinct, and represent word dependency. The main point of comparison is a robust and highly effective IR model (SD) that uses term selection and is employed as a baseline in related work [3, 28, 33]. We also compare against the model with the highest mean average precision of which we are aware that is relevant to a discus-sion of term selection with query expansion (sDist). Finally, since compact queries are a feature of PhRank, we compare against a succinct yet competitive model that selects only two terms (KC). We note that superior IR effectiveness is possible with term weighting, but we focus on results using unweighted terms to more clearly demonstrate the effect of term selection alone. We also report results for query like-lihood (QL) for reference even though this model uses no term selection. This is because the other models reported include a query likelihood component. We do not compare against models that use pseudo relevance feedback for ex-pansion. Pseudo relevance feedback without expansion is a novel feature of our work that contributes to PhRank per-formance.
Evaluation across three TREC collections using both de-scription topics and title queries requires a strong, robust baseline. We use a sequential dependence (SD) variant of the Markov random field (MRF) model [24]. SD uses a lin-ear combination of three cliques of terms, where each clique is prioritized by a weight  X  c . The first clique contains in-dividual words (query likelihood QL ),  X  1 =0 . 85. The sec-ond clique contains query bigrams that match document bi-grams in 2-word ordered windows ( X  #1  X ),  X  2 =0 . 1. The third clique uses the same query bigrams as clique 2 with an 8-word unordered window ( X  #uw8  X ),  X  3 =0 . 05. For ex-ample, the query  X  new york city  X  in Indri 1 query language is:
Because it is very simple to generate SD queries, this model is regularly used as a baseline. Highly effective weighted variants have also been developed [4, 33, 28]. We com-pare SD with a PhRank model (PR-.F) that uses the same query format, except the second and third cliques contain PhRank terms instead of query bigrams. In addition, be-cause PhRank terms may be 1-3 words long, we adjust the unordered window operator in the manner proposed for the full dependence variant of the MRF model [24]. Namely, the window size is 4 multiplied by the number of words in a term. Note that for a term with only one word i ,theoper-ators #1( i ) and #uw8( i ) equate to a search for the word i in a document. So, if two terms  X  york  X  X nd X  new york city  X  are selected by PhRank, the PR-.F model has the form:
PR-.F uses five terms for description topics and feature analysis experiments, and three terms for title queries (or less, if the required number of terms is not available after rank filtering).
Highly competitive performance compared to SD can be achieved by jointly optimizing possible subqueries and sub-query weights using syntactic and statistical features. Among the models of which we are aware, the subset distribution model (sDist) [33] achieves the highest mean average preci-sion on long queries using term selection with no higher order dependencies [4]. However, it is not entirely fair to compare sDist with PR-.F since sDist uses heavily optimized weights for ten subqueries. A subquery in sDist is a linear combina-tion of a standard SD query and one selected term treated as a bag-of-words. This compares with the flat  X  weights used in SD and PR-.F. Despite this, sDist is the most effec-tive model we can use for stringent comparison that ensures real progress has been made. We therefore include sDist in our evaluation even though queries for Robust04 are not available from the authors.
Queries formulated with PhRank have few terms and a maximum of three words per term. To evaluate highly suc-cinct queries we compare against Key Concepts (KC) [3]. KC is another succinct weighted linear feature model that combines two cliques. The first clique (  X  1 =0 . 8) contains a bag-of-words query representation of the original query, and the second clique (  X  2 =0 . 2) combines a weighted bag-of-words representation for each of two selected terms. The top terms are selected from the set of query noun phrases using a decision tree with frequency-based features [3]. The model reduces to a weighted representation of the original query with word independence. If  X  city  X  X nd X  new york  X  X re the top two terms, it takes the following form, where  X  is the decision tree confidence score associated with a term:
To compare against KC, we present a model (PR-zF2) that takes the same form but does not benefit from term weights  X  .WeusethetwotoptermsselectedbyPhRank.
Assumptions of word dependence are an important is-sue in IR. To clarify the dependence assumptions made by PhRankwerefertofourmodelsof phrase belief presented by [9] (Figure 2, a-d). These models show how belief in a document d c  X  C flows to belief in a query Q in an inference network, and thus how words and terms can be dependent. In PhRank, we do not perform inference, but by analogy these models aid interpretation of PhRank features.
Of the four models in Figure 2, the more general depen-dence assumption (d) is used by PhRank to score words, and term ranks are computed using an independence assumption (b). Even if component words of terms are not connected in G , weight is propagated through the graph such that word dependencies affect evidence for a term. PhRank factors z , s and r reflect Figure 2 models a, b and c respectively.
We speculate that optimal term selection occurs when 1) a high rank is assigned to terms that are important under all four interpretations of phrase belief according to evidence in N , 2) the rank of terms that have less evidence under one or more interpretation decreases gracefully, and 3) the ranking meets the principles of term selection proposed in Section 3. We examine the performance of PhRank in three ways. First, we compare versions of the algorithm in which we omit specific features. Second, we compare performance of queries reformulated using PhRank top ranked terms against highly effective models for both TREC description topics and title queries. Third, we compare on a query by query basis the robustness and performance error for PhRank ver-sus a distributed approach to term selection (SD).
We evaluate on three TREC collections using version 4.12 of Indri with Dirichlet smoothing,  X  = 2500. The Robust04, WT10G and GOV2 newswire and open web text collections have queries that vary substantially in length and known dif-ficulty. Together they provide a diverse platform for exper-iments (Table 2). Topic 672 is excluded from the Robust04 evaluation as the collection contains no relevant documents. All collections and queries are stopped and stemmed using the INQUERY stoplist and Krovetz stemmer. Queries are further stopped to exclude 18 TREC stopwords such as  X  de-scribe  X  [1]. Candidate terms are all units of 1-3 words in the power set P ( q ) of content-bearing words in Q .IRmod-els are defined in Section 5. Pseudo relevant documents are retrieved using a sequential dependence model. Mod-rTsTzT 26 . 65 30 . 05 0 . 00 0 . 00 28 . 83 34 . 55 zF 27 . 32 30 . 32 23 . 68 26 . 71 28 . 64 34 . 13 sF 26 . 03 29 . 61 21 . 00 25 . 10 27 . 93 33 . 67 rF 26 . 67 30 . 02 22 . 44 25 . 70 28 . 93 34 . 65 rTsTzT 24 . 87 29 . 04 21 . 78 25 . 73 31 . 49 37 . 26 zF 26 . 14 30 . 13 20 . 85 24 . 72 30 . 73 36 . 26 sF 25 . 90 30 . 03 20 . 72 24 . 30 31 . 30 36 . 91 rF 26 . 32 30 . 25 21 . 81 25 . 70 31 . 59 37 . 42 Table 3: Feature analysis results. Description topics perform best with omission of the global term weight z (zF). Title queries perform best with the omission of bigram salience weight r (rF). els  X .F X  exclude the feature represented by  X . X  and models  X .T X  include the feature. Thus, model rTsTzT includes all features.
In this section we explore the impact of PhRank feature removal on IR effectiveness assessed using model PR-.F. Re-sults in Table 3 show that PhRank is highly effective in se-lecting informative terms for a query. However, not all the features proposed consistently improve term ranking for IR. Description topics are most effective when factor z is omit-ted, and title queries are most effective when r is omitted. 1) Factor r : words dependent on term
Factor r imperfectly captures belief in component words dependent on belief in a term (Figure 2c). It uses global bi-gram statistics to scale edge weights in G . During a random walk, this affects the affinity scores for individual stemmed words. However, bigram statistics are only an approximate measure of term unity. More problematically, r relies on words in a term being co-occurrent in N . Highly informative terms are likely to have their component words connected in G , but this is not guaranteed. For terms with more than two words, edge weights in G also must be factored. Perhaps due to these limitations, r had minimal impact on IR effec-tiveness for title queries and could be omitted to improve algorithm efficiency.
 However, we note that r is useful for description topics. We speculate that this is because the query words for de-scription topics may be peripheral to the core information need. Spurious adjacent word dependencies in Q tend to ap-pear in the pseudo relevant set because bigrams feature in the IR model employed for initial retrieval. Thus, if word co-occurrence in Q reflects query meaning, as typically occurs with title queries, the edges and weights used to initialise G are likely to be adequate. If word co-occurrence is spurious, the initialisation may be suboptimal. Factor r ameliorates misleading initial edge weights for description topics. as a hybrid with features of all these models (e) for term x = 2) Factor s : word independence
Factor s contributes to belief in a term dependent on be-lief in individual words (Figure 2b). It weights each vertex in an affinity graph by its salience in the query context N balanced by its salience in the document collection. Omis-sion of s substantially hurt IR effectiveness. Among all the features tested it had the most impact on overall perfor-mance, perhaps because independent belief in words is the most important factor in IR effectiveness [24]. In addition, work with random walk algorithms for query expansion has found that words co-occurring with high frequency are of low value if they are not semantically close to the query [6]. We suggest that salience in N as captured by s represents se-mantic closeness to the query, and salience in the collection helps to identify high frequency co-occurrent words. 3) Factor z : term as elemental unit
Factor z represents belief in a term independent of belief in its component words (Figure 2a). It resembles a standard tf.idf weight and reflects the principle that a term should be discriminative in the retrieval collection. Given the es-tablished effectiveness of tf.idf weighting, it is surprising that omission of z improves IR effectiveness for description topics. However, it is based on observations of a term in an unordered proximity window in the retrieval collection. The way such observations are made implies a dependence assumption that may not provide an accurate estimate of term salience. In addition, it has recently been suggested that global statistics rarely improve retrieval performance and that local, document level evidence is sufficient [20].
We also note that both r and z account for the discrimina-tion ability of multi-word units in the collection: r applies to bigrams and z applies to words in unordered windows. This encoding is partially redundant, so description queries may not require z because they use r , and title queries may re-quire z because they do not use r .Weremove z for our final runs for description queries, and retain it for title queries. 4) Factor k : pseudo relevant documents
Results in Table 4 show that the most improvement in IR effectiveness is achieved with 2 to 5 pseudo relevant docu-ments. Higher k decreases effectiveness due to the introduc-tion of non-relevant information. However, PhRank is quite robust to variation in k due to the weighting of co-occurrence relations by document relevance. Even with construction  X 
PRF 26 . 44 29 . 59 21 . 88 25 . 36 27 . 85 33 . 28 k2 26 . 86 30 . 05 22 . 76 25 . 42 28 . 81 34 . 38 k5 27 . 32 30 . 32 23 . 68 26 . 71 28 . 64 34 . 13 k10 27 . 29 30 . 05 22 . 33 25 . 02 28 . 64 34 . 16 k50 27 . 09 30 . 11 23 . 04 26 . 21 28 . 82 34 . 27 k100 26 . 80 29 . 82 22 . 78 26 . 11 28 . 34 33 . 91 Table 4: IR effectiveness for description topics using k pseudo relevant documents. Best IR effectiveness is achieved using the top few documents only.
 Figure 3: IR effectiveness with feature analysis and variable threshold. In many cases PhRank achieves performance gains with two terms, and is robust to variance in the number of terms selected. of the affinity graph from the original query only (  X  PRF), PhRank performs better than sDist and comparably with SD. This suggests that most important information is re-tained by the term selection process.
We present our best results for runs using description top-ics and title queries on three TREC collections.
For description topics, the results in Table 5 show highly significant or significant improvement in mean average preci-sion (MAP) and R-precision compared to the SD baseline for QL 25 . 25 28 . 69 19 . 55 22 . 77 25 . 77 31 . 26
SD 26 . 57 30 . 02 20 . 63 24 . 31 28 . 00 33 . 30 sDist  X   X  21 . 14 24 . 93 27 . 64 33 . 50
PR-zF 27 . 32 30 . 32 23 . 68  X  26 . 71  X  28 . 64  X  34 . 13 PR  X  W 27 . 19  X  30 . 12 22 . 90  X  26 . 57 28 . 18 33 . 77 KC 25 . 62 28 . 89 20 . 15 22 . 58 26 . 88 32 . 73 PR-zF2 25 . 91 28 . 92 22 . 02  X  25 . 69  X  27 . 04 32 . 75
PR  X  W2 25 . 76 28 . 33 21 . 43 25 . 40  X  26 . 05 31 . 75 QL 24 . 37 28 . 52 19 . 48 23 . 08 28 . 55 34 . 41 SD 26 . 16 30 . 25 20 . 97 23 . 75 31 . 25 36 . 88 PR-rF 26 . 32 30 . 25 21 . 81  X  25 . 70  X  31 . 59 37 . 42 PR  X  W 26 . 44 30 . 40 21 . 76  X  25 . 57  X  31 . 50 37 . 14 Table 5: Retrieval results for description topics and title queries. PhRank significantly outperforms a highly effective baseline for description topics and is strongly competitive for title queries.  X  shows sig-nificant ( p&lt;. 05 )and  X  highly significant ( p&lt;. 01 ) results compared to SD and KC respectively as de-termined by a sign test.
 GOV2 and WT10G. Substantial improvements in precision on Robust04 are just short of significance. For title queries, improvement is highly significant for WT10G and compara-ble to the baseline for other collections. Increased precision occurs for top ranked documents (top 5 and 10) as well as being a general trend in the results. Exclusion of Wikipedia has a small negative effect as shown by PR  X  WandPR  X  W2 corresponding to PR-.F and PR-zF2 respectively.

To assess the quality of the ranked list of terms without a measure of ground truth for term informativeness, we ex-plore the impact of varying the number of terms included in query reformulations. The results in Figure 3 show that the quality of top terms output by PhRank are stable as more terms are selected. Further, a large part of the gain in precision is attributed to the top two terms.

To investigate performance further, for each collection we manually reviewed the ranked term lists for queries that per-form significantly better or worse than SD ( &gt; 100% change in MAP), and 10 queries with comparable performance. Across all queries observed, there is a strong tendency for PhRank to single out one word, or a pair of words, as the main concept of the query, and rank all terms that contain the main concept highly. Remaining terms are ranked accord-ing to the contributions of their additional words. This high risk, high reward strategy negatively affects the robustness of PhRank on a query by query basis as shown in Figure 4 for description topics. Title queries exhibit similar behavior.
For example, one of the best performing queries for GOV2 is #756 as shown in Table 1. For this query, identification of  X  volcano  X  as the main concept greatly helped IR. On the other hand, the same strategy for query #780, one of the worst performing queries for GOV2 (see Table 6), selected Figure 4: MAP difference compared to SD base-line per query for description topics. PhRank does slightly better on harder queries. The strategy of focus around one concept usually helps, but can sig-nificantly hurt some queries.  X  earth  X  as the main concept that should be included in all top terms. This resulted in terms that were representative of the query, but not well distributed.

Nevertheless, Figure 4 shows consistent improvement for queries that are known to be harder (Robust04 HARD track) or easier (high baseline MAP). It is more likely that PhRank selects an appropriate main concept for easy queries because the pseudo relevant documents are of high quality. Difficult queries are less clearly defined and often benefit from the strong directional focus provided by PhRank terms.
In comparison, models like SD and sDist, take a more ro-bust approach to term selection with a distribution of pos-sibly relevant terms. This presents a very different term selection strategy, so one potential avenue for improvement is interpolation of PhRank term selection with bigrams in SD. However, the robustness of a distributed term selection approach can come with a tradeoff in overall effectiveness. Initial interpolation experiments with a weighted linear com-bination of SD and PhRank terms did not appear to yield any benefit over PhRank terms alone.

Alternatively, the properties of G may be turned to advan-tage. It has been observed that a Markov field framework selects more general and robust query expansion terms than competing methods [6]. A combination of query expansion and term selection using a Markov field framework may bal-ance complementary high reward and robust query reformu-lation strategies and result in significant overall gains.
Results in Table 5 show significant improvement in MAP and R-precision for PhRank compared to sDist for both GOV2 and WT10G. PhRank terms are significantly more precise on average than the highest precision models for un-weighted term selection. Unfortunately, the focus on one aspect of query meaning has unpredictable effects and some queries are significantly hurt by a high precision strategy. There are two potential causes for negative results. First, PhRank may be picking a suboptimal concept. This does occur, particularly in the presence of polysemous or highly co-occurrent words in the query, or irrelevant documents in N . This is demonstrated with the high rank for  X  earth  X  X n query #780 (Table 6). In the case of highly co-occurrent Table 6: TREC query #780: poor performance for PhRank compared to SD.
 Table 7: Percentage of PhRank and KC terms with various lengths. words, these have a higher in-degree in G so they tend to accumulate weight during a random walk. A reduction in the number of iterations may help address this problem.
Irrelevant documents in G also hurt performance. The ad-equacy of an affinity graph G constructed using N is highly reliant on the quality of the initial query, the precision of the document similarity metric, and the adequacy of the collec-tion being searched. If non-relevant documents occur in N there will be reduced connectivity in G , and this has an un-desirable impact on the balance of word affinity scores. One solution to this problem may be to merge ranked lists com-puted by PhRank using different resources. The mistakes made by different instances of PhRank for the same query are likely to be less consistent than accurate assessments of term informativeness.

Second, more than one focus can occur, particularly in long queries. For example, there are two focal concepts of query #336:  X  A relevant document would discuss the fre-quency of vicious black bear attacks worldwide and the pos-sible causes for this savage behavior  X . The two core concepts are  X  black bear  X  X nd X  savage behavior  X  but PhRank largely misses the importance of black bears. Instead, its top ranked terms for this query are { savage, savage behavior, bear sav-age, vicious savage, attacks savage } . This has a negative effect on IR effectiveness.
Results show that the performance of the top two PhRank terms in the same query structure as KC but with no term weighting performs comparably to KC with term weighting . The length of the terms is similar in both models, with around 75% of terms having a length of 1-2 words. This suggests that improved performance of unweighted PR-zF2 queries is more likely to be due to differences in the strategy for term selection than differences in term length. Note that KC shares the distributed approach to term selection with SD and sDist. KC selects two distinct concepts, whereas the top two terms selected by PhRank typically overlap.
More generally, it is observed that the succinct terms se-lected by PhRank are also novel. Table 8 shows that al-though PhRank and KC have the same number of 1-2 word terms overall, they display less than half of their poten-tial overlap (we account for fewer terms in KC in this fig-ure). Moreover, around 50% of PhRank terms contain two words, but only around half of them are also selected by SD. Table 8: Percentage of PhRank terms selected by other models. Low figures show that PhRank is de-tecting novel terms with long-range dependencies. Terms that are three words long dominate sDist (69% of all terms) yet less than half of the terms with three words in PhRank are also found in sDist queries. One likely expla-nation for these findings is that PhRank is not limited by syntactic or adjacency relations that are used in the other models. It detects distant word dependencies because repeat co-occurrences of word combinations reflect the associations in which they take part.

We hypothesize that distant associations may be present in queries because users condense information by relying on the ability of a search engine to infer links between words. The frequency of such textual economy was assessed in a sample of 100 queries randomly selected from Robust04 and GOV2. We assume that title queries capture a succinct in-formation need and have informative associations between query words. By aligning description and title vocabulary, we discovered that 22% of description topics contain at least one informative word association that cannot be detected us-ing any form of syntax or word adjacency, and a further 11% of topics contain at least one association that can only be detected using dependency relations.
We have presented PhRank, a novel term ranking algo-rithm that extends work on Markov chain frameworks for query expansion to select focused and succinct terms from within a query. PhRank captures query context with an affinity graph constructed using word co-occurrence in pseudo-relevant documents. A random walk of the graph is used for term ranking in combination with discrimination weights.
We showed that PhRank focuses on a limited number of words associated with a core query concept. Overall, this is more effective for both description topics and title queries than a distributed approach to term selection, and can gen-erate queries with up to 90% fewer terms. However, this term selection strategy is risky and less robust than com-peting methods. For all collections, around 26% of queries have more than 5% decrease in MAP compared to SD (sig-nificant change is around 3-6%).

The two main issues affecting robustness are the handling queries with multiple concepts, and variation in the quality of pseudo relevance feedback. The first issue may be ad-dressed by a diversity constraint on top ranked terms that adjusts the number of selected terms permitted to include the highest scoring query word. Improved sensitivity of the ranking algorithm may also improve results. For example, the present implementation does not consider the degree of connection between two words in the affinity graph when scoring terms. A third approach might apply a non-linear interpolation of SD and PhRank that backs off to distributed terms where required. Adaptive methods for the selection of k can address challenges with the depth of coverage in a collection or occasions when evidence for multiple concepts is widely dispersed.
Finally, the high precision strategy of term selection might be combined with the more conservative and robust expan-sion terms generated with a Markov chain approach to query expansion. On this point, we note that although weighted variants of an affinity graph have been proposed before, our concrete suggestion for a vertex weight s based on word salience in pseudo-relevant documents improves the informa-tiveness of affinity scores and may benefit other techniques that use a Markov chain framework.

More generally, the work in this paper may be applicable to lexical feature selection methods for other areas of IR, in-cluding text-based image and multimedia retrieval or match-ing of search advertisements. Efficiency considerations sur-rounding the time to construct an affinity graph may be ameliorated by off-line indexing to precompute a language model for each document in a collection.
This work was supported in part by the Center for In-telligent Information Retrie val. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.

