 1. Introduction
Arabic is a morphologically rich language. Any Arabic word consists of a sequence of morphemes in the moves a few common affixes) allows as good or better retrieval than a more complicated morphological example to emphasize this fact: (Engineer) are: .
 After applying the Arabic light stemmer on the previous Arabic words, they will all converted to the
Arabic word (Engineer). It is clear from the previous example that the not stemmed Arabic words may cause confusion and may decrease the translation pair extraction system performance.
Arabic text is far behind on the Web  X  s exponential growth curve, Arabic text (as opposed to images) did not really start emerging on the Web until the release of Microsoft Windows 98TM, which provided Arabic support in its version of Internet Explorer. Unlike some language pairs such as English X  X apanese where you can find thousands of parallel articles, English X  X rabic parallel documents are not easy to find in the interest in researches and experiments conducted for Arabic language. Some researches were related to stemming and segmentation of Arabic words ( Chen &amp; Gey, 2002; Lee, Papineni, Roukos, Emam, &amp;
Hassan, 2003; Rogati, McCarley, &amp; Yang, 2003; Xu, Fraser, &amp; Weischede, 2001 ). Some others dealt 2002 ).
 corpora, through the Internet or via distribution agencies providing newspapers articles in different lan-guages, has led researchers to develop methods to extract bilingual lexicons from such corpora, in order ural language processing ( Resnik &amp; Smithy, 2003 ). The content of the Internet is changing from being mainly in the English language to being multi-lingual. The number of non-English speaking Internet users used to create translation pairs.

There were some efforts to construct English X  X rabic parallel corpus from the Internet archive. Resnik and Smithy used a technique called STRAND to collect English X  X rabic parallel text from the internet ar-chive regardless the document types where Chen and Gey (2001, 2002) obtained a collection of documents from the United Nations that included translation-equivalent document pairs in English and Arabic.
In this paper we propose a system that automatically creates English/Arabic bitexts from the internet archive. This system also, makes English/Arabic sentences alignment. Two different algorithms rely on sta-bined the two algorithms to have better accuracy and recall. Finally, Arabic light stemmer is used to have better accuracy and recall.
The paper is organized as follows; Section 2 presents related works, Section 3 describes the approach to make them suitable for bilingual dictionary construction. Section 5 explains the system and the used algorithms to construct the dictionary and the results. Section 6 introduces the conclusion of this work and the possible future work. 2. Related works
Some researchers tried to construct bilingual dictionary from the Internet Web documents. TANAKA and UMEMURA used a third language to construct a bilingual dictionary ( Tanaka &amp; Umemura, 1994 ).
McEwan1 ( McEwanl, Ounis, &amp; Ruthven, 2002 ) tried to collect English/Spanish parallel texts. They col-prepared texts, and 72 out of 423 for the automatic prepared. Their approach to translating English to
Spanish terms based on statistical co-occurrence techniques. The dictionary list of 1687 English terms was generated from the 35 aligned files of the manual corpus. A list of 1047 English terms was generated from the 72 aligned files of the automatic corpus. In the first version a total of 1687 English terms were collected from the manual corpus. For each of these terms, 2 Spanish terms were collected resulting in a total of 3394 Spanish words. A total of 612 English terms had a correct translation in the list of Spanish terms (18%). The result from automatic corpus was a little bit less. This system dealt with word to word translation only, whereas our new system deals not only with word to word translation, but also word to phrase as well. Also the precision and the total number of extracted translation pairs are higher for our system.

The algorithms that were implemented by Kay and Roscheisen (1993) and Stanley, did not produce as comprehensive a Word-Alignment model as hoped. And although there were various threshold parameters could be tuned to produce more words, the accuracy would then degrade considerably. In addition, the dictionary) ( Julapalli &amp; Dhond, 2003 ). However, our new system is able to generate large bilingual dictionary.
 and source word s have two drawbacks: an example, one such model contained an average of 39 French translations for each English word.
For word alignment, Cherry and Lin created a model that is similar to IBM  X  s Model 1, in that they both take into account only the word types that participate in a given link but Cherry and model allows easy word types into account, and the results are promising.

Many statistical translation models ( Brown et al., 1993; Niessen, Vogel, Ney, &amp; Tillmann, 1998; Till-dences between source and target words. The model is often further restricted that each source word is assigned exactly one target word. These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is j i = aj from source position j to target between groups of words. As experiments have shown it is difficult to handle different word order and the translation of compound nouns ( Niessen et al., 1998; Och, Josef, Tillmann, &amp; Ney, 1999; Tillmann et al., 1997; Vogel et al., 1996 ). Our system does not has this problem.

The parameters of the statistical alignment models are optimized with respect to a maximum-likelihood criterion, which is not necessarily directly related to alignment quality. Such an approach, however, re-quire any manually defined alignments.
 Xu and Weischedel used a statistical machine translation toolkit called WEAVER developed by John
WEAVER has a component to automatically derive word translations based on the sentence-alignment ( Xu &amp; Weischedel, 2000 ). Ashish Venugopal presented a technique that begins with improved IBM models allel corpus, so we extract bitexts automatically form the Internet archive.

Also non-aligned comparable corpora were used to extract bilingual lexicon using different tools like: morphological analyzers, EDR bilingual dictionary, NTCIR-2, and SMART information retrieval system ( Sadat, Yoshikawa, &amp; Uemura, 2003 ).
 paper will introduce a new approach for detecting multi-word translations. 3. Extracting parallel documents from the Internet archive
When presenting the same content in two different languages, authors exhibit a very strong tendency to use the same document structure. Resnik, basing on this idea, used a technique called STRAND to collect in the Internet are not of the same structure. We have avoided this problem when we extracted English/Ara-bic bitexts from the internet archive as follows:
Three steps are required to find parallel documents: (1) Locating the pages that might contain parallel documents, (2) Generating the document pairs that might be translation of each other, (3) Filtering-out of non-translation candidate pairs.

We simply sent queries to the different Internet search engines. These queries contained some words like: download the pages that might contain English/Arabic parallel documents. Then we collected two types of pages: (a) A parent page is one that contains hypertext links to different-language versions of a document. (b) A sibling page is a page in one language that itself contains a link to a version of the same page in
In each page, we download all the files in the English page and all the files in the Arabic page as well. According to the documents name (for example an Arabic file called exp _ a38 , is most probable to that are probably parallel. Then if the following relation:
Verified for the English X  X rabic document pair, we expect that this pair is parallel and we remove the doc-were specified by using 50 English X  X rabic document pair that were created manually as training data. We have collected 4582 document pairs that are expected to be translation of each other. We filtered manually 523 document pairs that are actually translation of each other. These 523 document pairs contain 21734 sentence pairs. 4. Preprocessing (A) Sentence alignment for the 21734 sentence pairs, based on the sentence length. (B) Removing the English and Arabic stop word lists from the English and Arabic documents respec-(C) Preprocessing step for English and Arabic documents to make them suitable for the next step, for (D) Convert all plural English words to singular. 5. The system 5.1. System block diagram
Fig. 1 illustrates the block diagram of the system. The system consists of the following blocks: (1) Queries sent to the Internet archive to pick parent and sibling pages. (2) Searching for the document pairs that expected to be translation of each others. (3) Filtering to get the actual parallel documents. (4) Preprocessing and alignment to extract sentence pairs. (5) The stemming step and the two algorithms combination to output the bilingual dictionary as follow: 5.2. The first algorithm
The algorithm uses a similarity metric S(a, e) between words in Arabic language (A) and words in Eng-lish language (E) based on statistical co-occurrence and the frequency of each Arabic and English word. threshold, the translation pairs whose association score exceeds this threshold become the entries in the translation lexicon. The algorithm detail is as follows:
The first step in the algorithm is to make a table that contains the word and sentence numbers that the word appeared in and the frequency of each word in English and Arabic documents. Then proceed with the rest of the algorithm as follows: Set i  X  j  X  1 Test : If  X  mi ; j &gt;  X  x ani  X  &amp;&amp;  X  y enj  X  &lt; ani &lt;  X  z enj  X  j  X  j  X  1 If j &lt;  X  NE i  X  i  X  1 &amp; j  X  1 If i &lt;  X  NA words in the table. NA = the total number of Arabic words in the table.

The final step of the algorithm is to use an online dictionary (Babylon) to specify the correct and the incorrect translations in the final file.

Some Arabic words have more than one translation and some English words also. So we should not take proportional to the precision of the output dictionary and inversely proportional to the dictionary size. From the training 3000 sentence pairs, we found that the nominal value for x is 0.87.
The use of this alignment model raises major problems as it fails to capture dependencies between groups of words. As experiments will show, it is difficult to handle the translation of compound nouns. On the co-occurrence frequency increases in a big parallel corpus which increase the precision. 5.2.1. Examples to illustrate the above algorithm
Assume the following English sentences: (1) Swimming is a popular sport. (2) Basketball was considered as the popular game in USA.

The Arabic translations are: (1) . (2) .

Specifying the frequency of each word in the Arabic and English sentences, we found that each word mentioned once except the English word (popular) and the Arabic word ( ) mentioned twice. Com-paring the sentence numbers where each word mentioned in. We find that the English word (popular) was mentioned in the first and the second sentences, and the Arabic word ( ) was mentioned in the first and the second sentences too. So the two words are most probable to be translation of each other.
 5.2.2. Experimental results for algorithm 1 tional to the recall.
 call ( F -measure) for x = 0.87. Here we define the terms precision, recall and F -measure as follows:
As shown in Table 1 , we can achieve high precision with low recall. However the precision, recall and F -measure are low in general since the used corpus is small and the algorithm fails to capture dependencies between groups of words which exist extensively in English/Arabic bi-texts. Hence we propose the second algorithm which avoids the first algorithm drawback. 5.3. The second algorithm
To avoid algorithm 1 disadvantages, we propose the following algorithm which is based on statistical co-occurrence of translation pairs too: Set n=1, &amp; m=n+1.
 Test: Compare Arabic _ sentence (n) with Arabic _ sentence (m) m=m+1 GOTO Test n=n+1 &amp; m=n+1 If m &lt; =N GOTO Test End Exchange Arabic by English, and then repeat the previous pseudo code.
 For i=1; i &lt; =Na _ word; i++ End
Since N = total number of English/Arabic sentences. Na _ word = the total number of Arabic words in the final table R  X  e  X  X  f  X  e  X  English word  X  X  e  X  X  and Ne = the total number of the English words associated with the same Arabic word. When exchanging arabic by English in the previous algorithm, we use the repetition percentage of the Arabic word  X  X  a  X  X  which is R  X  a  X  X  f  X  a  X  Repeat the previous pseudo code for data types in b and c.
 The final step is to remove the final extracted words (in the final file) from the original English and
Arabic documents then repeat the previous algorithm Several times until no significant words are file.

Unlike algorithm 1 and the previous published works, this algorithm can extract translation pairs from two sentence pairs only. This algorithm can capture dependencies between groups of words to get word/ phrase translation pair which was the problem of many statistical approaches like algorithm 1. 5.3.1. Examples to illustrate the above algorithm
Assume the following English sentences: 1. I can play football . 2. Football is a popular sport. 3. Basketball was considered as the popular game in USA.

The Arabic translations are: (1) . (2) . (3) .

Comparing the first two English sentences, we can extract only one word which is  X  X  X ootball X  X . Then com-paring the first two Arabic sentences, and extract all common words which are  X  X   X  X . Now we expect that the Arabic phrase  X  X   X  X  is the Arabic translation of the English word  X  X  X ootball X  X . If the same Ara-are translation of each others or not according to the repetition percentage.

If the system specified that these words are translation of each other, then remove these words from the first two Arabic and English sentences. After removing the words, the English sentences will be: (1) I can play. (2) is a popular sport. (3) Basketball was considered as the popular game in USA.

And the Arabic sentences will be: (1) . (2) . (3) .

Compare the first English sentence with the third one, and do the same with the Arabic sentences. We do not find any common words.

Compare the second English sentence with the third English sentence, and do the same with Arabic sen-tences. Then extract all common words. Only the word  X  X  X opular X  X  is common between the two English sen-tences and the word  X  X   X  X  is common between the two Arabic sentences. We can expect that the English word  X  X  X opular X  X  is the translation of the Arabic word  X  X   X  X . 5.3.2. Experimental results
The precision and recall of translation pairs of the output dictionary resulted from applying the previous sely proportional to (Th) . We have applied the algorithm on 700 sentence pairs. Table 2 shows the results for different values of (Th) .
 Also for (Th) = 0.99 and word frequency more than 4, we could reach 100% precision and 44% recall. In Table 2 , the precision and recall associated with Th = 0.8 and 0.99 are almost the same. Hence we can use Th = 0.8 as a nominal value.

Applying the algorithm on the whole 21734 sentence pairs, taking Th = 0.8, for several trials after 84.0%.

This algorithm could extract a dictionary that defines more than one translations for a certain word for example the Arabic word  X  X   X  X  is to be translated to the English words (of, from). Also it could extract
From Tables 1 and 3 , the results of algorithm 2 are better than that of algorithm 1 from precision and recall point of view. However, we can combine both algorithms together to have better precision and recall.
 5.4. Algorithms 1 and 2 combination
As mentioned before, each algorithm has advantages and disadvantages. Using algorithm 1, we can lation of compound nouns. Moreover the precision and recall are higher than that of algorithm 1. However combination of algorithm 1 and algorithm 2 to gain the advantages of both of them and avoid the disad-vantages as much as possible.
 cision. Then we remove the resulted translation pairs form the English X  X rabic sentence pairs. Then run the second algorithm on the document pairs using Th = 0.8. Using Algorithm 1 and 2 combination, we achieved 88.6% precision, 81.5% recall and 84.9% F -measure. 5.5. The effect of stemming
Previous stemming research has shown that even in a highly inflected language like Arabic, a very light stemmer (one that removes a few common affixes) allows as good or better retrieval than a more compli-cated morphological analyzer. Hence, applying light stemmer before the two algorithms combination, may improve the total system performance.

We have applied the  X  X  X itao Chen and Fredric Gey X  X  Arabic light stemmer on the Arabic documents. The suffixes in the pre-defined set of suffixes in the following sequence: 1. If the word is at least five-character long, remove the first three characters if they are one of the following: . 2. If the word is at least four-character long, remove the first two characters if they are one of the following: . 3. If the word is at least four-character long and begins with , remove the initial letter . 4. If the word is at least four-character long and begins with either or , remove or only if, after removing the initial character, the resultant word is present in the Arabic document collection. four characters long before removing a suffix: . three-character long before removing a suffix: .

Applying the two algorithms combination on the whole 21734 sentence pairs (but this time after apply-ing the stemming step), the precision improved to 89.9% but the recall decreased to 78.6% and the F -measure = 83.8%.

After stemming, the system accuracy increased but the total recall decreased. The accuracy increased be-ming. The recall decreased due to that many Arabic words have been reduced to one word after stemming.
On the other hand the accuracy did not increase too much after stemming because the formation of bro-ken Arabic plurals is complex and often irregular. The following example is an incorrect translation pair that was obtained as a part of the output dictionary: (tool = ). The original English word was  X  X  X ools X  X ; the system before stemming translated  X  X  X ools X  X  to  X  X   X  X  which is correct. But after changing the plural English word  X  X  X ools X  X  to singular  X  X  X ool X  X  and after stemming of the Arabic word  X  X   X  X  to be  X  X   X  X , the translation become (tool = ) which is not correct since the singular word of  X  X   X  X  is  X  X   X  X  not  X  X   X  X  be-cause the Arabic plural word  X  X   X  X  is irregular. To avoid this problem, the system checks the Arabic word after stemming. If the word does not exist in the vocabulary obtained from the gathered documents, then use the word without stemming otherwise stem the word as shown in Fig. 1 .

After using the above approach, we achieved 91.6% precision, 82.6% recall and 86.8% F -mneasure for a final dictionary size of 2342 translation pairs.
 Table 4 is a part of the resulted dictionary. The Arabic words are on the left column, the translation
English words are on the right column, and then another column contains 1 if the translation is correct and 0 if the translation is incorrect. 6. Conclusions system that is able to extract English X  X rabic bilingual dictionary from the Internet archive documents using several approaches. The first algorithm achieved high precision with low recall for high frequency words and its required processing time is small. However it failed to handle the translation of compound nouns. Algorithm 2 can handle the translation of compound nouns. Moreover the precision and recall are high. However the processing time required for algorithm 2 is higher than that of algorithm 1. Both algo-the new translation pairs that are not exist in the available commercial dictionaries. We could gain both advantages of the two algorithms by combining them. Stemming as a preprocessing step has increased to that many Arabic words have been reduced to one word after stemming.
 In the future work, we will use this system in cross language information retrieval (CLIR). Acknowledgements
This research was partially supported by the Ministry of Education, Science, Sports and Culture, Grant-in-Aid for Scientific Research (B), 14380166 and 17300065, Exploratory Research 17656128 in 2005, Inter-national Communications Foundation (ICF). The authors also wish to acknowledge the anonymous re-viewer  X  s insightful comments and suggestions.
 References
