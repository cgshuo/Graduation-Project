 Bootstrapping (Yarowsky, 1995; Abney, 2004) is a technique frequently used in natural language pro-cessing to expand limited resources with minimal supervision. Given a small amount of sample data ( seeds ) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns character-izing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected in-stances are added to the seed set, and the process is iterated until sufficient labeled data are acquired.
Many bootstrapping algorithms have been pro-posed for a variety of tasks: word sense disambigua-tion (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), and statistical parsing (Steedman et al., 2003; Mc-Closky et al., 2006).

Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift : as the iteration proceeds, the algorithms tend to select in-stances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances { New York City , Maldives Islands } , bootstrapping might learn, at one point of the iteration, patterns like  X  X ictures of X X  and  X  X hotos of X, X  which also co-occur with many irrelevant instances. In this case, a later iteration would likely acquire frequent words co-occurring with these generic patterns, such as Michael Jack-son .

Previous work has tried to reduce the effect of se-mantic drift by making the stop list of instances that must not be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these ap-proaches require expert knowledge.

In this paper, we propose a graph-based approach to seed selection and stop list creation for the state-of-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this ap-proach is that it requires zero or minimal super-vision. The idea is to use the hubness score of instances and patterns computed from the point-wise mutual information matrix with the HITS al-gorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espresso has the same root as topic drift (Bharat and Henzinger, 1998) observed with HITS, noting the algorithmic similarity between them. While Komachi et al. pro-posed to use algorithms different from Espresso to avoid semantic drift, in this paper we take advantage of this similarity to make better use of Espresso.
We demonstrate the effectiveness of our approach on a word sense disambiguation task. In this section, we review related work on seed se-lection and stop list construction. We also briefly in-troduce the Espresso bootstrapping algorithm (Pan-tel and Pennacchiotti, 2006) for which we build our seed selection and stop list construction methods. 2.1 Seed Selection The performance of bootstrapping can be greatly in-fluenced by a number of factors such as the size of the seed set, the composition of the seed set and the coherence of the concept being expanded (Vyas et al., 2009). Vyas et al. (2009) studied the impact of the composition of the seed sets on the expansion performance, confirming that seed set composition has a significant impact on the quality of expansions. They also found that the seeds chosen by non-expert editors are often worse than randomly chosen ones. A similar observation was made by McIntosh and Curran (2009), who reported that randomly chosen seeds from the gold-standard set often outperformed seeds chosen by domain experts. These results sug-gest that even for humans, selecting good seeds is a non-trivial task. 2.2 Stop Lists Yangarber et al. (2002) proposed to run multiple bootstrapping sessions in parallel, with each session trying to extract one of several mutually exclusive semantic classes. Thus, the instances harvested in one bootstrapping session can be used as the stop list of the other sessions. Curran et al. (2007) pur-sued a similar idea in their Mutual Exclusion Boot-strapping , which uses multiple semantic classes in addition to hand-crafted stop lists. While multi-class bootstrapping is a clever way to reduce human su-pervision in stop list construction, it is not generally applicable to bootstrapping for a single class. To ap-ply the idea of multi-class bootstrapping to single-class bootstrapping, one has to first find appropri-ate competing semantic classes and good seeds for them, which is in itself a difficult problem. Along this line of research, McIntosh (2010) recently used Algorithm 1 Espresso algorithm clustering to find competing semantic classes (nega-tive categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch  X  utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all pos-sible patterns. We denote all pmi values as an n  X  p instance-pattern matrix A , with the ( i , j ) element of A holding the value of pmi between the i th instance and the j th pattern. Let A T denote the matrix trans-pose of A .
 Algorithm 1 shows the pseudocode of Espresso. The input vector i 0 (called seed vector ) is an n -dimensional binary vector with 1 at the i th com-ponent for every seed instance i , and 0 elsewhere. The algorithm outputs an n -dimensional vector i and an p -dimensional vector p , respectively representing the final scores of instances and patterns. Note that for brevity, the pseudocode assumes fixed numbers ( k and m ) of components in i and p are carried over to the subsequent iteration, but the original Espresso allows them to gradually increase with the number of iterations. 3.1 Espresso and HITS Komachi et al. (2008) pointed out the similarity between Espresso and Kleinberg X  X  HITS web page ranking algorithm (Kleinberg, 1999). Indeed, if we remove the pattern/instance selection steps of Algo-rithm 1 (lines 13 and 16), the algorithm essentially reduces to HITS. In this case, the outputs i and p match respectively the hubness and authority score vectors of HITS, computed on the bipartite graph of instances and patterns induced by matrix A .
An implication of this algorithmic similarity is that the outputs of Espresso are inherently biased towards the HITS vectors, which is likely to be the cause of semantic drift. Even though the pat-tern/instance selection steps in Espresso reduce such a bias to some extent, the bias still persists, as em-pirically verified by Komachi et al. (2008). In other words, the expansion process does not drift in ran-dom directions, but tend towards the set of instances and patterns with the highest HITS scores, regard-less of the target semantic class. We exploit this ob-servation in seed selection and stop list construction for Espresso, in order to reduce semantic drift. 3.2 The Procedure Our strategy is extremely simple, and can be sum-marized as follows. 1. First, compute the HITS ranking of instances 2. Next, check the top instances in the HITS rank-3. The third step depends on the outcome of the 4. Run Espresso with the seeds or stop list found We evaluate our methods on a variant of the lexi-cal sample word sense disambiguation task. In the lexical sample task, a small pre-selected set of a tar-get word is given, along with an inventory of senses for each word (Jurafsky and Martin, 2008). Each word comes with a number of instances (context sentences) in which the target word occur, and some of these sentences are manually labeled with the cor-rect sense of the target word in each context. The goal of the task is to classify unlabeled context sen-tences by the sense of the target word in each con-text, using the set of labeled sentences.

To apply Espresso for this task, we reformulate the task to be that of seed set expansion, and not classification. That is, the hand-labeled sentences having the same sense label are used as the seed set, and it is expanded over all the remaining (unlabeled) sentences.

The reason we use the lexical sample task is that every sentence (instance) belongs to one of the pre-defined senses (classes), and we can expect the most frequent sense in the corpus to form the highest HITS ranking instances. This allows us to com-pletely automate our experiments, without the need to manually check the HITS ranking in Step 2 of Section 3.2. That is, for the most frequent sense (majority sense), we take Step 3a and use the highest ranked instances as seeds; for the rest of the senses (minority senses), we take Step 3b and use them as the stop list. 4.1 Datasets We used the seven most frequent polysemous nouns ( arm, bank, degree, difference, paper, party and shelter ) in the S ENSEVAL -3 dataset, and line (Lea-cock et al., 1993) and interest (Bruce and Wiebe, 1994) datasets 1 for our experiments. We lowercased words in the sentence and pre-processed them with the Porter stemmer (Porter, 1980) to get the stems of words.

Following (Komachi et al., 2008), we used two types of features extracted from neighboring con-texts: collocational features and bag-of-words fea-tures. For collocational features, we set a window of three words to the right and left of the target word. 4.2 Evaluation methodology We run Espresso on the above datasets using differ-ent seed selection methods (for majority sense of tar-get words), and with or without stop lists created by our method (for minority senses of target words).
We evaluate the performance of the systems ac-cording to the following evaluation metrics: mean average precision (MAP), area under the ROC curve (AUC), R-precision, and precision@ n (P@ n ) (Man-ning et al., 2008). The output of Espresso may con-tain seed instances input to the system, but seeds are excluded from the evaluation. 5.1 Effect of Seed Selection We first evaluate the performance of our seed se-lection method for the majority sense of the nine polysemous nouns. Table 1 shows the performance of Espresso with the seeds chosen by the proposed HITS-based seed selection method (HITS), and with the seed sets randomly chosen from the gold stan-dard sets (Random; baseline). The results for Ran-dom were averaged over 1000 runs. We set the num-ber of seeds n seed = 7 and number of iterations  X  = 5 in this experiment.

As shown in the table, HITS outperforms the baseline systems except degree . Especially, the MAP reported in Table 1 shows that our approach achieved improvements of 10 percentage points on bank , 6 . 1 points on party , 27 . 7 points on line , and 10 . 4 points on interest over the baseline, respec-tively. AUC and R-precision mostly exhibit a trend similar to MAP, except R-precision in arm and shel-ter , for which the baseline is better. It can be seen from the P@ n (P@30, P@50 and P@100) reported in Table 1 that our approach performed considerably better than baseline, e.g., around 17 X 20 points above the baseline on bank and 25 X 37 points on line . 5.2 Effect of Stop List Table 2 shows the performance of Espresso using the stop list built with our proposed method (HITS), compared with the vanilla Espresso not using any stop list (NoStop).

In this case, the size of the stop list is set to n stop = 10, and the number of seeds n seed = 10 and iterations  X  = 20. For both HITS and NoStop, the seeds are selected at random from the gold standard data, and the reported results were averaged over 50 runs of each system. Due to lack of space, only the results for the second most frequent sense for each word are reported; i.e., the results for more minor senses are not in the table. However, they also showed a similar trend.

As shown in the table, our method (HITS) outper-forms the baseline not using a stop list (NoStop), in all evaluation metrics. In particular, the P@ n listed in Table 2 shows that our method provides about 11 percentage points absolute improvement over the baseline on interest , for all n = 10, 20, and 30. We have proposed a HITS-based method for allevi-ating semantic drift in the bootstrapping algorithm Espresso. Our idea is built around the concept of hubs in the sense of Kleinberg X  X  HITS algorithm, as well as the algorithmic similarity between Espresso and HITS. Hub instances are influential and hence make good seeds if they are of the target seman-tic class, but otherwise, they may trigger semantic drift. We have demonstrated that our method works effectively on lexical sample tasks. We are currently evaluating our method on other bootstrapping tasks, including named entity extraction.
 We thank Masayuki Asahara and Kazuo Hara for helpful discussions and the anonymous reviewers for valuable comments. MS was partially supported by Kakenhi Grant-in-Aid for Scientific Research C 21500141.
