 Principal Components Analysis (PCA) is a standard linear technique for dimensionality reduction. Given a matrix X  X  R n  X  l of l centered, n -dimensional observations, PCA performs an eigende-composition of the covariance matrix Q := XX &gt; . The r  X  n matrix W whose rows are the eigenvectors of Q associated with the r  X  n largest eigenvalues minimizes the least-squares recon-struction error where || X || F is the Frobenius norm.
 As it takes O ( n 2 l ) time to compute Q and up to O ( n 3 ) time to eigendecompose it, PCA can be prohibitively expensive for large amounts of high-dimensional data. Iterative methods exist that do not compute Q explicitly and thereby reduce the computational cost to O ( rn ) per iteration. One such method is Sanger X  X  [1] Generalized Hebbian Algorithm (GHA), which updates W as tend to converge to the principal component solution as t  X   X  ; though its global convergence is not proven [2].
 One can do better than PCA in minimizing the reconstruction error (1) by allowing nonlinear pro-jections of the data into r dimensions. Unfortunately such approaches often pose difficult nonlinear optimization problems. Kernel methods [3] provide a way to incorporate nonlinearity without un-duly complicating the optimization problem. Kernel PCA [4] performs an eigendecomposition on complexity, Kim et al. [2] introduced the Kernel Hebbian Algorithm (KHA) kernelizing GHA. Both GHA and KHA are examples of stochastic approximation algorithms, whose iterative updates employ individual observations in place of  X  but, in the limit, approximating  X  statistical proper-ties of the entire data. By interleaving their updates with the passage through the data, stochastic approximation algorithms can greatly outperform conventional methods on large, redundant data sets, even though their convergence is comparatively slow.
 Both the GHA and KHA updates incorporate a scalar gain parameter  X  t , which is either held fixed or annealed according to some predefined schedule. Robbins and Monro [5] established conditions on the sequence of  X  t that guarantee the convergence of many stochastic approximation algorithms; Here we propose the inclusion of a gain vector in the KHA, which provides each estimated eigen-vector with its individual gain parameter. We present two methods for setting these gains: In the KHA/et algorithm, the gain of an eigenvector is reciprocal to its estimated eigenvalue as well as the iteration number t [6]. Our second method, KHA-SMD, additionally employs Schraudolph X  X  [7] Stochastic Meta-Descent (SMD) technique for adaptively controlling a gain vector for stochastic gradient descent, derived and applied here in Reproducing Kernel Hilbert Space (RKHS), cf. [8]. The following section summarizes Kim et al. X  X  [2] KHA. Sections 3 and 4 describe our KHA/et and KHA-SMD algorithms, respectively. We report our experiments with these algorithms in Section 5 before concluding with a discussion. Kim et al. [2] apply Sanger X  X  [1] GHA to data mapped into a reproducing kernel Hilbert space (RKHS) H via the function  X  : R n  X  H . H and  X  are implicitly defined via the kernel k : R n  X  R n  X  X  with the property  X  x , x 0  X  R n : k ( x , x 0 ) =  X   X ( x ) ,  X ( x 0 )  X  the inner product in H . Let  X  denote the transposed mapped data: This assumes a fixed set of l observations whereas GHA relies on an infinite sequence of observa-tions for convergence. Following Kim et al. [2], we use an indexing function p : N  X  Z l which concatenates random permutations of Z l to reconcile this discrepancy.
 PCA, GHA, and hence KHA all assume that the data is centered. Since the mapping into feature space performed by kernel methods does not necessarily preserve such centering, we must re-center the mapped data: where M denotes the l  X  l matrix with entries all equal to 1 /l . This is achieved by replacing the kernel matrix K :=  X  X  &gt; ( i.e., [ K ] ij := k ( x i , x j ) ) by its centered version Since all rows of MK are identical (as are all elements of MKM ) we can precalculate that row in The kernel centered on the training data is also used when testing the trained system on new data. From Kernel PCA [4] it is known that the principal components must lie in the span of the centered mapped data; we can therefore express the GHA weight matrix as W t = A t  X  0 , where A is an r  X  l matrix of expansion coefficients, and r the number of principal components. The GHA weight update (2) thus becomes where e coefficients as Introducing the update coefficient matrix we obtain the compact update rule In their experiments, Kim et al. [2] employed the KHA update (8) with a constant scalar gain,  X  = const . They also proposed letting the gain decay as  X  t = 1 /t for stationary data. (2). At the desired solution, the rows of W t contain the principal components, i.e., the leading eigenvectors of Q = XX &gt; . The elements of y t thus scale with the associated eigenvalues of Q . Wide spreads of eigenvalues can therefore lead to ill-conditioning , hence slow convergence, of the GHA; the same holds for the KHA.
 In our KHA/et algorithm, we counteract this problem by furnishing KHA with a gain vector  X  t that provides each eigenvector estimate with its individual gain parameter. The update rule (10) thus becomes where diag(  X  ) turns a vector into a diagonal matrix. To condition KHA, we set the gain parameters proportional to the reciprocal of both the iteration number t and the current estimated eigenvalue; a similar apporach was used by Chen and Chang [6] for neural network feature selection. Let  X  t be the vector of eigenvalues associated with the current estimate (as stored in A t ) of the first r eigenvectors. KHA/et sets the i th element of  X  t to where  X  0 is a free scalar parameter, and l the size of the data set. This conditions the KHA update eigenvalues.
 effect comparable to an adaptive  X  X earch then converge X  gain schedule [9] without introducing any tuning parameters.
 As the goal of KHA is to find the eigenvectors in the first place, we don X  X  know the true eigen-values while running the algorithm. Instead we use the eigenvalues associated with KHA X  X  current eigenvector estimate, computed as where [ A t ] i  X  denotes the i -th row of A t . This can be stated compactly as where the division and square root operation are performed element-wise, and diag(  X  ) (when applied to a matrix) extracts the vector of elements along the matrix diagonal.
 Note that naive computation of AK 0 is quite expensive: O ( rl 2 ) . Since the eigenvalues evolve gradually, it suffices to re-estimate them only occasionally; we determine  X  t and  X  t once for each pass through the training data set, i.e., every l iterations. Below we derive a way to maintain AK 0 incrementally in an affordable O ( rl ) via Equations (17) and (18). While KHA/et makes reasonable assumptions about how the gains of a KHA update should be scaled, it is by no means clear how close the resulting gains are to being optimal. To explore this question, we now derive and implement the Stochastic Meta-Descent (SMD [7]) algorithm for KHA/et. SMD controls gains adaptively in response to the observed history of parameter updates so as to optimize convergence. Here we focus on the specifics of applying SMD to KHA/et; please refer to [7, 8] for more general derivations and discussion of SMD.
 Using the KHA/et gains as a starting point, the KHA-SMD update is where the log-gain vector  X  t is adjusted by SMD. (Note that the exponential of a diagonal matrix is obtained simply by exponentiating the individual diagonal entries.) In an RKHS, SMD adapts a scalar log-gain whose update is driven by the inner product between the gradient and a differential of the system parameters, all in the RKHS [8]. Note that  X  t  X  0 can be interpreted as the gradient in the RKHS of the (unknown) merit function maximized by KHA, and that (15) can be viewed as r coupled updates in RKHS, one for each row of A t , each associated with a scalar gain. SMD-KHA X  X  adaptation of the log-gain vector is therefore driven by the diagonal SMD X  X  differential parameters: that (9) implies where the r  X  l matrix A t K 0 can be stored and updated incrementally via (15): A 1 suitably sparse.
 Finally, we apply SMD X  X  standard update of the differential parameters: where the decay factor 0  X   X   X  1 is another scalar tuning parameter. The differential d  X  t of the gradient is easily computed by routine application of the rules of calculus: Inserting (9) and (20) into (19) yields the update rule In summary, the application of SMD to KHA/et comprises Equations (16), (21), and (15), in that order. The complete KHA-SMD algorithm is given as Algorithm 1. We initialize A 1 to an isotropic normal density with suitably small variance, B 1 to all zeroes, and  X  0 to all ones. The worst-case a time complexity of O ( rl ) or less.
 Algorithm 1 KHA-SMD We compared our KHA/et and KHA-SMD algorithms with KHA using either a fixed gain (  X  t =  X  0 ) Performing kernel PCA and spectral clustering on the well-known USPS dataset [10], replicating an image denoising experiment of Kim et al. [2], and denoising human motion capture data. In all experiments the Kernel Hebian Algorithm (KHA) and our enhanced variants are used to find the first r eigenvectors of the centered Kernel matrix K 0 . To assess the quality of the result, we reconstruct the Kernel matrix from the found eigenvectors and measure the reconstruction error where || X || F is the Frobenius norm. The minimal reconstruction error from r eigenvectors, E min := min A E ( A ) , can be calculated by an eigendecomposition. This allows us to report reconstruction errors as excess errors relative to the optimal reconstruction, i.e., E ( A ) / E min  X  1 . To compare algorithms we plot the excess reconstruction error on a logarithmic scale after each pass through the entire data set. This is a fair comparison since the overhead for KHA/et and KHA-SMD is negligible compared to the time required by the KHA base algorithm. The most expensive operation, the calculation of a row of the Kernel matrix, is shared by all algorithms. We manually tuned  X  0 for KHA, KHA/t, and KHA/et; for KHA-SMD we hand-tuned  X  , used the same  X  0 as KHA/et, and the value  X  = 0 . 99 (set a priori ) throughout. Thus a comparable amount of tuning effort went into each algorithm. Parameters were tuned by a local search over values in the set { a  X  10 b : a  X  X  1 , 2 , 5 } , b  X  Z } . 5.1 USPS Digits Our first set of experiments was performed on a subset of the well-known USPS dataset [10], namely the first 100 samples of each digit in the USPS training data. KHA with both a dot-product kernel and a Gaussian kernel with  X  = 8 1 was used to extract the first 16 eigenvectors. The results are shown in Figure 1. KHA/et clearly outperforms KHA/t for both kernels, and KHA-SMD is able to increase the convergence speed even further. Figure 1: Excess relative reconstruction error for kernel PCA (16 eigenvectors) on USPS data, using a dot-product (left) vs. Gaussian kernel with  X  = 8 (right). 5.2 Multipatch Image PCA For our second set of experiments we replicated the image de-noising problem used by Kim et al. most of the noise. The image considered here is the famous Lena picture [12] which was divided in four sub-images. From each sub-image 11  X  11 pixel windows were sampled on a grid with two-pixel spacing to produce 3844 vectors of 121 pixel intensity values each. The KHA with Gaussian kernel (  X  = 1 ) was used to find the 20 best eigenvectors for each sub-image. Results averaged over employed by Kim et al. [2] for comparison.
 After 50 passes through the training data, KHA/et achieves an excess reconstruction error two orders of magnitude better than conventional KHA; KHA-SMD yields an additional order of magnitude improvement. KHA/t, while superior to a constant gain, is comparatively ineffective here. Kim et al. [2] performed 800 passes through the training data. Replicating this approach we obtain a reconstruction error of 5.64%, significantly worse than KHA/et and KHA-SMD after 50 passes. The KHA/et achieves comparable performance much faster, reaching an SNR of 13.49 in 50 passes. 5.3 Spectral Clustering where 10 kernel PCs were obtained by KHA. We used the spectral clustering method presented in Figure 2: Excess relative reconstruction error (left) for multipatch image PCA on a noisy Lena image (center), using a Gaussian kernel with  X  = 1 ; denoised image obtained by KHA-SMD (right). of information (right) for spectral clustering of the USPS data with a Gaussian kernel (  X  = 8 ). [13], and evaluate our results via the Variation of Information (VI) metric [14], which compares the of 4.54 corresponds to random performance, while clustering in perfect accordance with the class labels would give a VI of zero.
 Our results are shown in Figure 3. Again KHA-SMD dominates KHA/et in both convergence speed and quality of reconstruction (left); KHA/et in turn outperforms KHA/t. The quality of the resulting clustering (right) reflects the quality of reconstruction. KHA/et and KHA-SMD produce a cluster-ing as good as that obtained from a (computationally expensive) full kernel PCA within 10 passes through the data; KHA/t after more than 30 passes. 5.4 Human motion denoising In our final set of experiments we employed KHA to denoise a human walking motion trajectory from the CMU motion capture database ( http://mocap.cs.cmu.edu ), converted to Cartesian coordinates via Neil Lawrence X  X  Matlab Motion Capture Toolbox ( http://www.dcs.shef. ac.uk/  X  neil/mocap/ ). The experimental setup was similar to that of Tangkuampien and Suter [15]: Gaussian noise was added to the frames of the original motion, then KHA with 25 PCs was used to denoise them. The results are shown in Figure 4.
 As in the other experiments, KHA-SMD clearly outperformed KHA/et, which in turn was better than KHA/t. KHA-SMD managed to reduce the mean-squared error by 87.5%; it is hard to visually Figure 4: From left to right: Excess relative reconstruction error on human motion capture data with Gaussian kernel (  X  = noisy data, and a superposition of the original and reconstructed (denoised) data. detect a difference between the denoised frames and the original ones  X  see Figure 4 (right) for an example. We include movies of the original, noisy, and denoised walk in the supporting material. We modified Kim et al. X  X  [2] Kernel Hebbian Algorithm (KHA) by providing a separate gain for each eigenvector estimate. We then presented two methods, KHA/et and KHA-SMD, to set those gains. KHA/et sets them inversely proportional to the estimated eigenvalues and iteration number; KHA-SMD enhances that further by applying Stochastic Meta-Descent (SMD [7]) to perform gain adaptation in RKHS [8]. In four different experimental settings both methods were compared to a conventional gain decay schedule. As measured by relative reconstruction error, KHA-SMD clearly outperformed KHA/et, which in turn outperformed the scheduled decay, in all our experiments. Acknowledgments National ICT Australia is funded by the Australian Government X  X  Department of Communications, Information Technology and the Arts and the Australian Research Council through Backing Aus-tralia X  X  Ability and the ICT Center of Excellence program. This work is supported by the IST Program of the European Community, under the Pascal Network of Excellence, IST-2002-506778. [1] T. D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward network. [2] K. I. Kim, M. O. Franz, and B. Sch  X  olkopf. Iterative kernel principal component analysis for [3] B. Sch  X  olkopf and A. Smola. Learning with Kernels . MIT Press, Cambridge, MA, 2002. [5] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical [6] L.-H. Chen and S. Chang. An adaptive learning algorithm for principal component analysis. [7] N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. [8] S. V. N. Vishwanathan, N. N. Schraudolph, and A. J. Smola. Step size adaptation in reproduc-[9] C. Darken and J. E. Moody. Towards faster stochastic gradient search. In J. E. Moody, S. J. [10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. J. Jackel. [12] D. J. Munson. A note on Lena. IEEE Trans. Image Processing , 5(1), 1996. [13] A. Ng, M. Jordan, and Y. Weiss. Spectral clustering: Analysis and an algorithm (with ap-[14] M. Meila. Comparing clusterings: an axiomatic view. In ICML  X 05: Proceedings of the 22nd [15] T. Tangkuampien and D. Suter. Human motion de-noising via greedy kernel principal compo-
