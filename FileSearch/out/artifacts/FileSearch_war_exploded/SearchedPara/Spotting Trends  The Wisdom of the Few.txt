 Social media sites have used recommender systems to suggest items users might like but are not already familiar with. These items are typically movies, books, pictures, or songs. Here we consider an alternative class of items -pictures posted by design-conscious in-dividuals. We do so in the context of a mobile application in which users find  X  X ool X  items in the real world, take pictures of them, and share those pictures online. In this context, temporal dynam-ics matter, and users would greatly profit from ways of identifying the latest design trends . We propose a new way of recommend-ing trending pictures to users, which unfolds in three steps. First, two types of users are identified -those who are good at uploading trends (trend makers) and those who are experienced in discover-ing trends (trend spotters). Second, based on what those  X  X pecial few X  have uploaded and rated, trends are identified early on. Third, trends are recommended using existing algorithms. Upon the com-plete longitudinal dataset of the mobile application, we compare our approach X  X  performance to a traditional recommender system X  X . H.4 [ Information Systems Applications ]: Miscellaneous Experimentation Mobile, Social Media, Trend Detection Good user experience is what makes online services attractive. To be outstanding, many services not only try to provide easy ac-cess to the content of what users are looking for, but also attempt to help them discover new information which they might be inter-ested in. Mostly, they use recommender systems to give person-alized suggestions to each user. Depending on the context of the applications, recommended items are of all kinds [2, 6, 9, 10, 20, 21, 25, 32, 33, 34].

We here consider a social mobile application called iCoolhunt Its users are encouraged to take pictures of objects they think  X  X ool X  and share these pictures with friends online. The idea of  X  X unt-ing cool X  items has attracted a very special community -users are mainly technology-savvy and design-conscious individuals. In such a context, helping users to discover trends becomes impor-tant. As we shall see in Section 2, in the literature, there have been studies on what trends are and who creates them. But no study has been yet conducted to show how such knowledge can be leveraged to make personalized recommendation on trends.

To fill the gap, we propose a trend-aware recommender system (Section 4). We build upon the insight offered by Amatriain et al. [2]: that is, the recommendation process should not always rely on many (crowd) ratings but might also benefit from few expert rat-ings. By exploiting this insight, we make two main contributions:
We evaluate the effectiveness of our trend-aware recommender system at helping users discover trends. To this end, we compare our approach to a traditional item-based recommender system (Sec-tion 5). We then conclude by discussing how such a trends-aware recommender approach could be applied to a real-life working sys-tem (Section 6).
This paper builds upon existing studies in two main research ar-eas: personalized recommendations, and analyses of trends in so-cial networks. www.icoolhunt.com Personalized Recommendations. Recommender systems are used in different online services. Traditionally, studies have been fo-cused on recommending books, CDs [21], movies [2, 34], songs [18, 32], news [9], and videos [10]. With the advent of mobile applica-tions, many applications have been able to know where users are, and some services have thus started to recommend location-based events [25], activities, and POIs (Points of Interests) [33]. Adding the users X  social connections to their geographic information has been found to improve the quality of recommendations [15, 19]. Also, new social connections have themselves become  X  X tems to recommend X  [6, 24]. There has been a lot of work on algorithms over the last few years (a useful categorization of them can be found in Adamic et al.  X  X  work [1]), and effective techniques such as ma-trix factorization have emerged [14, 15, 33].
 Analyses of trends in social networks.  X  X rends X  are items (e.g., pictures, videos) that receive abrupt attention and are of two types: endogenous and exogenous depending on the sources that triggered them. Endogenous trends are triggered by collective activities within the user community, while exogenous ones result from activities outside the community. In Youtube, researchers have been able to distinguish between endogenous and exogenous trends by simply looking at temporal patterns (more specifically, at the user response rates) [8]. In Twitter, instead, researchers identified trends based on a richer set of features -content, user interactions and social net-works [23]. In addition to identify trends, researchers have also looked at the human beings behind trends and have tried to answer the question of who creates them. In his popular book  X  X he Tip-ping Point X , Malcolm Gladwell argued that the creators belong to the  X  X pecial few X . Those are often called  X  X nfluentials X  [12] and are believed to be characterized by large social networks [17] and disproportionally high social standing [27, 29, 31]. By contrast, Watts argues that trends are created by accidental activities, that is, by adopting the right item at the right time [28]. More recently, researchers are coming round to the idea that trends in a networked environment are actually generated by a combination of accidental activities and presence of influential individuals [3, 13]. These two lines of research (i.e., recommender systems and trend analysis) are here brought together to study whether one could build simple ways of facilitating discovery and recommendation of trends. We experimentally do so in the context of a mobile social-networking application with which users discover, organize and share pictures of  X  X ool X  design items. iCoolhunt users are encouraged to take pictures of objects that they think  X  X ool X , upload them and share them with friends online. If their devices are GPS-enabled, pictures are automatically tagged with (longitude, latitude) points corresponding to where pictures are uploaded. Uploaders are asked to tag each of their pictures with one of the predefined categories, which include technology, lifestyle, music, design and fashion. They must also add a brief tex-tual description to each of their pictures. By following each other, users can then vote others X  pictures using  X  X ike X  and  X  X islike X  but-tons and can comment on them. Users are automatically assigned to different levels of expertise depending on the quality of their uploads and votes, and the number of followers they attract. Our anonymized dataset is complete, in that, it covers all user activities from February, 2010 (its launch) to August 2010 (before the time when iCoolhunt launched their web application). Figure 1: Empirical CDF of 1(a) #uploads and #votes per user; 1(b) #likes, #dislikes and #votes per user.
Our dataset includes 9,316 iCoolhunt mobile application users, 6,395 photos, and 21,252 votes. To filter away inactive users and lurkers (which, based on our data, are indistinguishable), we focus on users who have uploaded or voted at least once.
 Uploads and Votes. In the mobile application, uploading and vot-ing pictures are two main user activities. From the distributions of uploads and votes per user in Figure 1(a), we see that, as one ex-pects, most of the users (92%) have uploaded only once, or voted (85%) only once. The remaining active minority have contributed 83% of pictures and 94% of votes. This results in data sparsity: the fraction of non-zero values in the user-item matrix is around 1% . Users can vote others X  pictures using  X  X ike X  and  X  X islike X  buttons. Figure 1(b) shows the distribution of the number of  X  X ikes X ,  X  X is-likes X  and the total number of votes per user. One can see that the distribution of number of  X  X ikes X  is very similar to the total number of votes per user, suggesting that users tend to frequently express their opinions.
 Social Connectivities. Users can also create social contacts and follow each other. We depict the distribution of the number of fol-lowers/followees per user in Figure 2, and find that only a very small portion of users follow others, suggesting that using social ties to recommend content would not work in this specific applica-tion.
 From this brief analysis, two main points emerge: 1) there is a core group of users who contributed most of the content; and 2) relying only on the social graph to recommend trends would not be bene-ficial. For these two reasons, we propose a way of recommending trends based on the idea of identifying the  X  X pecial few X .
To recommend trends, we perform three steps (Figure 3): 1. Identify trend makers and trend spotters (Section 4.1); 2. Identify trends (Section 4.2); Figure 2: Empirical CDF of #followers and #followees per user. 3. Recommend the previously identified trends (Section 4.3).
In Section 6 on  X  X iscussion X , we will explain why it is not a good idea to identify trends directly and, instead, it is beneficial to identify trend spotters (makers) first and then trends later on.
In every social application, there are large behavioral differences among users [22]: some are able to identify trends early on, and some are leisure laggards. To identify the former type, we focus on two user categories  X  trend makers and trend spotters.

Trend makers are those who tend to upload items that then be-come trends, and trend spotters are those who tend to vote items that then become trends early on. To quantitatively identify those users, we consider the following features: Activity. The main activities on the application are two and are Content. Users vary in how diverse their interests are: one could Social Network. Since information might partly propagate along Geography. We finally consider: 1) how much and how often one
Having these features, we now perform three steps. For each user, we: Step 1 Compute the user X  X  spotter score and maker score; Step 2 Discretize the user X  X  scores.
 Step 3 Predict the user X  X  discretized scores on input of the previous Let us now spell out each of the steps.
 Step 1. To begin with, we introduce two metrics that reflect the extent to which a trend maker (spotter) u is successfully uploading (spotting) trends. User u  X  X  makerScore ( u ) is: where I u is the set of trends that u has uploaded, and I ( iisatrend ) is an indication function which equals to 1, if i is a trend; otherwise, it is 0. To establish whether an item is a trend or not, we use a met-ric similar to the one proposed in [23]. That is, for each time unit t , each item i is assigned with a trendScore ( i, t ) computed as: where |  X  i,t | is the number of votes item i has received within time unit t ,  X  i is the mean number of votes it received per time unit, and  X  is its standard deviation. A high trend score tells that the item have received more attention than expected within the time unit. In each time unit, items are sorted according to their trend scores, and top-N items are extracted and identified as trends. From our analysis, we found that the temporal resolution (one week or two weeks) and the length of the recommended list do not significantly change the scores. In Figure 4, one observes that the trend spotter score does not change as the list length (top-10 vs. top-50) changes.
To add the spotter score to the maker score, we observe that the ability of spotting trends is largely determined by three factors  X  how many, how early, and how popular one X  X  spotted trends be-come. To incorporate the factor of how early and how popular trends become, for each trend i that u has spotted (voted), we com-pute the following gain g u,i score: Figure 4: Trend spotter score (log). We split trend spotters into three classes using a proportional 3-interval discretization, as the two vertical lines show. in which  X  i is the total votes i received, p u,i captures that u is the p th user who spotted i (the lower p , the better), and  X  is a decay factor. Combining a user X  X  gains all together, we obtain a cumulative spotterScore for user u (which is normalized by user u  X  X  total number of votes): Step 2. Based on their maker scores and trend scores, we are able to cluster users into K classes, which indicate their ability of upload-ing (spotting) trends. To do so, we apply a proportional k -interval discretization [30] over the whole range of maker (spotter) scores and assign each user to one of the three classes (with k =3 ). We chose three because the distribution of trend spotter(maker) scores in Figure 4 shows three distinct increments in score and thus lends itself to the identification of three classes of users. Step 3. Based on a user X  X  (activity, content, social network, and ge-ographic) features, machine learning models predict to which class the user belongs. We tried a variety of models -Naive Bayesian, linear regression, and SVM -and found that SVM works best (Sec-tion 5).
Trend makers and trend spotters are the source of trends, but not all items uploaded and voted by those users become trends -there is a certain probability that they will be so. More generally, an item is likely to become a trend depending on: the extent to which the item X  X  uploader is a trend maker; and the extent to which the item X  X  voters are trend spotters. We model these insights in the following logistic regression: where Pr ( y i =1) is the probability of an item y i is a trend ( Pr ( y i =1) ), and X i are a set of predictors, which are the up-loader X  X  trend maker class and the voters X  trend spotter classes. Re-sults are again presented in Section 5.
Having identified items that are likely to be trends, we are now able to build a trend-aware preference matrix P ,inwhich p 1 or 0 depending on whether u liked item t that has then become a trend. Upon this matrix, we apply two existing recommender sys-tems algorithms: Implicit SVD [14] and item-based collaborative filtering [21]. We compare how these algorithms perform on input of the trend-aware matrix and on input of a traditional preference matrix P ,inwhich p u,t is 1 or 0 depending, again, on whether u voted on item t that has then become a trend. The difference be-tween the two preference matrices is that the trend-aware one is less sparse because, at the columns, it does not have all items but only those that we have predicted to be trends.
We now evaluate the effectiveness of each of the three steps we have just introduced.
 Classifying users into trend spotter(maker) classes. We first evaluate the extent to which SVM is able to classify each user into one of the three maker/spotter classes on input of the user X  X  features (introduced in Section 4.1). To this end, upon our dataset that has 209 unique trends, 50 trend makers, and 531 trend spotters, we run a 10-fold cross validation and test three algorithms: linear regres-sion, Naive Bayesian, and SVM . The best accuracies are returned by SVM : 83.80% of trend spotters and 60.7% of trend makers are correctly identified.
 Determining whether an item is a trend or not. After ascer-taining that SVM is able to reasonably identify trend spotters and trend makers, we now need to test whether the logistic regression in Section 4.2 is able to identify trends based on information about uploaders and voters. The regression predicts whether an item is a trend or not based on four features: the uploader X  X  trend maker class (first feature), and the number of votes from users who be-long to: the low spotter class (second feature), the medium spotter class (third feature), and high spotter class (fourth feature). To test the logistic regression, we build a balanced dataset that contains our 209 trends plus 209 (randomly extracted) non-trends and ob-tain the results in Table 1. The statistically significant coefficients suggest that an item is more likely to become a trend, if its uploader is a good trend maker and its voters are in the upper (trend spotter) class.

To avoid overfitting in Equation 5, we add a Tikhonov regular-ization term. The problem of learning  X  now translates into the following optimization problem: We split the dataset of trends into two subsets: the first subset con-sists of 80% of the entire dataset and is used to train the model, while the remaining 20% is used to test the model. With a 10-fold cross validation, we first fix the value of  X  and then fit the model with the training set. To measure the accuracy of the regularized logistical regression model, we apply the trained model to the test set. We obtain the ROC curve plot that reflects both the model X  X  TPR (true positive rate) and FPR (false positive rate). In Figure 5, one sees that the regression returns accurate results, whereby the point (0,1) corresponds to the best classification performance (the diagonal reflects the baseline of random guess).
 Recommending trends. At this point, we have ascertained our ability to identify trends. Now the question is: if we were to build a user-by-trend matrix out of the predicted trends, what would be the performance of an existing collaborative filtering algorithm? To answer that question, we need to establish three things: 1. We need to select which existing algorithm to use. For now, we Predictors Coefficient
Uploader X  X  trend maker class 6.21  X  X  X  X  #Voters from low trend spotter class -1.30 #Voters from medium trend spotter class -1.17  X  #Voters from high trend spotter class 0.64  X  X  X  X  Table 1: Coefficients of the logistic regression (a correlation coefficient within 2 standard errors is statistically significant. The significance levels are marked with  X   X  X : p&lt; 0 . 001 ( p&lt; 0 . 01 (  X  X  X  ), p&lt; 0 . 05 (  X  ))) Figure 5: ROC curve for the logistic regression that predicts whether an item is a trend or not. 2. We need to determine which metrics reflect performance. To 3. Finally, we need to determine the baseline against which our
Figure 6 shows precision and recall for the traditional and trend-aware item-based collaborative filtering as a function of the rec-ommended list size (top-N recommendations). For both systems, precision and recall improve linearly as N increases, but the trend-aware X  X  increase is faster, suggesting that a traditional item-based recommender system would not be able to recommend trends (pre-cision/recall is only 0.05 for top-10 recommendations), while a trend-aware system would (precision/recall is 0.2). These results also suggest that, in the presence of data sparsity, relying on few expert ratings is an effective way of recommending trends.
So far, we have analyzed how an item-based collaborative fil-tering algorithm would perform. Next, we test whether a matrix Figure 6: Precision and Recall. Results for trend-aware recom-mender vs. item-based recommender. The size of the recom-mended list is N . factorization approach -Implicit SVD -would improve the perfor-mance. Figure 7 shows this to be the case. As the size of the rec-ommended list increases, Implicit SVD consistently returns better precision and recall than item-based X  X .

Interestingly, if one were to recommend only popular trends, pre-cision and recall would be worst. This suggests that even for trends -items that one expects to be non-long tail -personalization makes sense. But up to a point -precision and recall results are limited, and that is largely because of the very nature of trends.
To sum up, the results on precision and recall should be inter-preted in comparative terms, while the other results answer a more fundamental question -whether trend detection helps the recom-mendation process; and the answer is a definite  X  X es X . We now discuss some open questions.
 Why not detecting activity bursts directly? Since a burst detec-tion algorithm could be easily applied to identify trends (in a way similar to expression (2)), one might wonder why we are going to the trouble of having the intermediate step of identifying trend Figure 7: Precision and Recall. Results for two trend-aware recommenders (item-based and Implicit SVD) and for recom-mendations of most popular trends. spotters (makers) and not, instead, identifying trends directly. The main reasons for this choice are efficiency and time: 1. Efficiency. Monitoring a limited number of users who are 2. Time. This is the most important reason and comes from Figure 8: Number of days an item ( a )vs. atrend( b ) receives votes for. Online Updating. In our analysis, we have not registered the fre-quent emergence of new trend spotters and trend makers. How-ever, in a system with a larger user base, that might be the case, and ways of updating the pool of key users -trend spotters (mak-ers) -would be needed. To decide when and how to run such updates, we are currently exploring the use of controllers that au-tomatically and accurately estimate frequency of updates. These techniques have been recently introduced with the idea of ensuring stable and high-quality recommendations in dynamically evolving environments [16].
We have shown that, upon activity, network, and geographic at-tributes, a machine learning approach can identify key users -trend spotters and trend makers. A simple logistic regression can then reliably infer whether an item (a picture in this case) will be a trend or not based on whether the item has been uploaded by a success-ful trend maker and voted by trend spotters. We have then seen that existing recommender systems can profit from this ability of identifying these special users. One promising future research di-rection is to simultaneously model the two processes here treated separately -collaborative filtering and trend detection. This could be done by, for example, combing Amatriain et al.  X  X  work [2] with existing models of temporal dynamics.
We thank Neal Lathia for his comments on earlier drafts of the paper. The work was funded by the French National Research Agency (ANR) under the PROSE project (ANR-09-VERS-007), by an industrial scholarship from PlayAdz 3 , and by RCUK through the Horizon Digital Economy Research grant (EP/G065802/1). [1] G. Adomavicius and A. Tuzhilin. Toward the Next [2] X. Amatriain, N. Lathia, J. Pujol, H. Kwak, and N. Oliver. [3] S. Aral and D. Walker. Identifying Influential and Susceptible [4] D. Boyd. The Future of Privacy: How Privacy Norms Can [5] M. Cha, H. Haddadi, F. Benevenuto, and K. Gummadi.
 [6] J. Chen, W. Geyer, C. Dugan, M. Muller, and I. Guy. Make [7] Z. Cheng, J. Caverlee, K. Lee, and D. Sui. Exploring [8] R. Crane and D. Sornette. Robust Dynamic Classes Revealed [9] A. Das, M. Datar, A. Garg, and S. Rajaram. Google News [10] J. Davidson, B. Liebald, J. Liu, P. Nandy, T. Van Vleet, [11] C. Droge, M. Stanko, and W. Pollitte. Lead Users and Early [12] M. Gladwell. The Tipping Point: How Little Things Can http://www.playadz.com [13] S. Gonzalez-Bailon, J. Borge-Holthoefer, A. Rivero, and [14] Y. Hu, Y. Koren, and C. Volinsky. Collaborative Filtering for [15] M. Jamali and M. Ester. A Matrix Factorization Technique [16] T. Jambor, J. Wang, and N. Lathia. Using Control Theory for [17] D. Kempe, J. Kleinberg, and E. Tardos. Influential Nodes in a [18] N. Koenigstein, G. Dror, and Y. Koren. Yahoo! Music [19] I. Konstas, V. Stathopoulos, and J. Jose. On Social Networks [20] H. Li, Y. Wang, D. Zhang, M. Zhang, and E. Chang. Pfp: [21] G. Linden, B. Smith, and J. York. Amazon.com [22] M. Muller, N. S. Shami, D. R. Millen, and J. Feinberg. We [23] M. Naaman, H. Becker, and L. Gravano. Hip and Trendy: [24] D. Quercia and L. Capra. FriendSensing: Recommending [25] D. Quercia, N. Lathia, F. Calabrese, G. Di Lorenzo, and [26] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Item-based [27] J. Tang, J. Sun, C. Wang, and Z. Yang. Social Influence [28] D. Watts and S. Strogatz. Collective Dynamics of [29] J. Weng, E.-P. Lim, J. Jiang, and Q. He. TwitterRank: [30] I. H. Witten and E. Frank. Data Mining: Practical Machine [31] L. Yu, S. Asur, and B. A. Huberman. What Trends in [32] Y. Zhang, D. S X aghdha, D. Quercia, and T. Jambor. Auralist: [33] V. Zheng, Y. Zheng, X. Xie, and Q. Yang. Collaborative [34] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale
