 Handling high-dimensional data represents one of the most challenging prob-lems for learning. Given numbers of features, it is observed that the inclusion of irrelevant and redundant features will deteriorate the performance of learning algorithms both in speed and prediction a ccuracy. Feature selection is an effec-tive way for dimensionality reduction in knowledge discovery and data mining applications. It aims to select a subset of the most useful features that produces the same results as the original entire set of features [2,6,7]. As a result, the data set quality and data analysis may improve through feature selection.

According to whether combining a machine learning algorithm to evaluate the feature subset, the feature selectio n methods can be divided into three cate-gories: wrapper, filter and embedded methods [6]. The embedded method mainly combine the advantages of wrapper method and filter method. Compared with the wrapper method, the filter method has much lower computational complex-ity. Thus we mainly focus on the filter method. It is noteworthy that among the filter methods, rough set-based feature se lection provides a sy stematic theoretic framework [1, 3], which does not attempt to maximize the class separateness but rather attempts to retain the discernible ability of original feature set for the object set. The main advantage of rough set theory is that it requires no human subjective input other than the given information themselves.

Most existing rough set-based featur e selection approaches rely on the com-plete data, i.e., the data sets without missing data. In fact, it is common to meet with the data sets with missing feature values in real-world tasks [3-5, 7-9]. Here we introduce some representative feature selection algorithms in the context of incomplete data. Meng [4] studied the relationships of several types of feature selection from a th eoretical framework in the i nconsistent incomplete data. From the viewpoint of discernibility matrix, Qian [5] presented two approx-imation feature selections to obtain relative feature subsets in an inconsistent incomplete data. However, the construction of discernibility matrices is time-consuming. From the viewpoint of information entropy, Sun [7] developed a rough entropy-based feature selection algorithm in the incomplete data. Dai [3, 8] provided the conditional entropy measure to evaluate the uncertainty in the incomplete data, and they pr esented the conditional entropy-based feature se-lection to select a feature subset. Howev er, some possible rules can be extracted from the incomplete data after feature selection. Above all, to improve the ef-ficiency of feature selection from the incomplete data sets, we will develop an efficient consistency-based feature selection algorithm in incomplete data. The key step in the consistency-based feature selection methods is to compute the positive region.

The remainder of this paper is organized as follows. In Section 2, some pre-liminary concepts are briefly reviewed. In Section 3, an effective method of com-puting the positive region is provided. In Section 4, a consistency-based feature selection algorithm in incomplete data is developed. In Section 5, the experi-ments are conducted to demonstrate the efficiency of the proposed algorithm. Finally, Section 6 concludes the paper. In this section, we briefly review the basic concepts about rough set theory, which can be found in [1, 3-5, 7-9]. In addition, some definitions involved in this paper are introduced.

Rough set-based data analysis starts from a data table, also called information system. If condition features and decision features in an information system are distinguished, it is ca lled a decision system. Let S =( U,C,D,V,f )bean incomplete decision system, where U is a non-empty set of objects, C is a non-empty set of condition features and D is a decision feature set with C  X  D =  X  ; V = V C  X  V D ,where V C is the set of condition feature values and V D is the set of decision feature values; f is a mapping function from U  X  ( C  X  D )to V . For P  X  C , a tolerance relation T P is defined as follows: T P = { ( x,y ) | X  a  X  P,f ( x,a )= f ( y,a )  X  f ( x,a )=  X  X  X  f ( y,a )=  X  X  .For  X  x  X  U , T P ( x ) can denote the maximal set of objects which are possibly indiscernible by P with object x . i.e., T P ( x )= { y  X  U | T P ( x,y ) } . The tolerance classes induced by P is defined as U//T P = { T P ( x ) | x  X  U } .For X  X  U , the lower and upper approximations of X with respect to P are defined as follows, respectively. T P ( X )= { x | T P ( x )  X  X } and T P ( X )= { x | T P ( x )  X  X =  X  X  . The lower approximation is called the positive region, that is POS P ( X )= T P ( X ). The objects in T P ( X )belongto X certainly. Definition 1. Let S =( U,C,D,V,f ) be an incomplete decision system, for any P  X  C , a  X  C  X  P , the significance measure of feature a is defined as sig ( a,P,D )= | POS P ( D )  X  POS P  X  X  a } ( D ) | .

From Definition 1, the feature with most significance measure can be selected in the process of feature selection.
 Definition 2. Given an incomplete decision system S =( U,C,D,V,f ) , for any P  X  C ,if POS P ( D )= POS C ( D ) and POS P ( D ) = POS P ( D ) for any P  X  P ,then P is a selected feature subset of the incomplete decision system. incomplete decision system unchanged. In what follows, we give the definition involved in the design process of the algorithm for computing the positive region in incomplete data.
 Definition 3. Let S =( U,C,D,V,f ) be an incomplete decision system, for P = { a 1 ,a 2 ,  X  X  X  ,a s } X  C , suppose the missing feature value of feature a s is a special known feature value different from other feature values in the system, a where E P i =[ x ] P = { y  X  U | X  a  X  P  X  X  a s } ,f ( x,a )= f ( y,a )  X  f ( x,a s )= f ( y,a s ) =  X  X  (1  X  i  X  k ) ,and E P j =[ x ] P = { y  X  U | X  a  X  P  X  X  a s } ,f ( x,a )= f ( y,a )  X  f ( x,a s )= f ( y,a s )=  X  X  ( k +1  X  i  X  k + l ) . In this section, we develop an efficient algorithm for computing the positive region in incomplete data.

In Algorithm 1, the key step of computing positive region is to compute the tolerance classes. if the feature value of an object is missing, then the tolerance class of the object is equal to the previous tolerance class, which does not need to recompute. In such case, we can use the label technique to find out the previous results such that the time is saved. In addition, if the feature value of an object is not missing, the tolerance classes of the object can be expressed as the union set of some equivalence classes. In such case, we can use the radix sorting technique to compute directly such that the computation is simple.
 Time Complexity Analysis. The time complexity of Steps 2-9 is O ( | U | ), the time complexity of Steps 10-20 is O ( | t | )+ O ( m | T P | ), where t is the number of represented items, m is the maximal cardinality of the tolerance class generated by the missing objects,where | m || U | . | T P | is the cardinality of the generated tolerance class under feature set P ,intheworstcase, | T P | is equal to | U | .The time complexity of Steps 21-26 is O ( m | U | ), the time complexity of Steps 27-32 is Algorithm 1. Computing the positive region in incomplete data O ( | U | + | t | ). The time complexity of Steps 35-38 is O ( n where t is the number of represented items. Therefore, the time complexity of computing the positive region is O ( m | U || C | ). In this section, a consistency-based dimensionality reduction algorithm is devel-oped by the forward greedy search strategy in incomplete data. At first, some indispensable features that cannot be deleted from the whole feature set are selected, and then a forward greedy fea ture selection approach takes a feature with the most significance into the feature subset in each loop until this subset satisfies the stopping condition, finally, some possible redundant features can be deleted from the whole feature subset.
 Algorithm 2. A consistency-based dimensionality reduction algorithm Time Complexity Analysis. Step 2 is to compute the positive region,the time complexity is O ( m | U || C | ). Steps 3-6 are to add some indispensable features, the time complexity is O ( m | U || C | 2 ). Step 7 is to construct a descending sequence of the remaining features, the time complexity is O ( m | U || C  X  P | ). Step 9-12 are to add the most significance features to the selected feature subset, the time complexity is O ( m | U || C  X  P | ), Step 13-16 are to delete some redundant features from the selected feature subset, the time complexity is O ( m | U || P | ). Therefore, the time complexity of Algorithm 2 is O ( m | U || C | 2 ). where m is the maximal cardinality of the tolerance class generated by the missing objects. In this section, to test and verify the effici ency of the proposed feature selection algorithm, four real-life data sets are downloaded from UCI Database [10]. The data sets are described in Table 1. All th edatasetsaretheincompletedatasets. For the numerical features, we employ the data tool Rosetta [11] to transform them into categorical ones. All the experiments are run on a PC with Windows XP, Core2, CPU E7400 and 2GB memory. Algorithms are coded in C++ and the software being used is Microsoft Visual 2008.

In what follows, we compare the proposed feature selection algorithm with other three algorithms in terms of the size of feature subset and computational time. For convenience, we denote the pro posed feature selection algorithm as PFS, the discernibility function-based feature selection algorithm mentioned in [5] as DFS, the feature selection algorithm with the traditional method of com-puting positive region as TFS and a rough entropy-based feature selection al-gorithm in [7] as RFS. Table 2 records the comparative results of the feature subset size.

From Table 2, we can see that the size of feature subset of Algorithm PFS is smaller or equal than the other three algorithms in most of the data sets. Take the Soybean-large data set for exam ple, PFS selects 10 features, which is the same as that of TFS. DFS and RFS obtain 12 and 11 features, respectively. Obviously, PFS selects few er features than both DFS and RFS. The possible reason is that PFS deletes the redundant features from the selected result, but DFS and RFS do not have this behavior. The results indicate that PFS can effectively reduce the dimensionality in incomplete data.
To distinguish the computational time, each data set is divided into 10 parts of equal size. The first part is viewed as the first data set, the combination of the first part and the second part is viewed as the second data set, the combination of the second part and the third part is viewed as the third data set and so on. Fig.1 displays the computational time of the four algorithms versus the size of data sets. The x -coordinate pertains to the size of the data sets, while the y -coordinate gives the computational time, which is expressed as seconds.
It is clear from Fig.1 that the curves of the four feature selection algorithms increases with the increase of the size of the data set. However, this relationship is not strictly monotonic. Take the Mammographic data set as an example, the computational time of the RFS algorithm decreases from the fifth data set to the sixth data set, and the computational time of the TFS algorithm decreases from the ninth data set to the tenth data set. The same phenomenon can be observed in other data sets. The underlying reason is that different numbers of features are selected. In additon, there is an obvious difference that PFS runs faster than the other three algorithms. And the difference becomes larger and larger when the size of the data set increases. Take the Mushroom data set as an example, the computational time of PFS is about 94s at the tenth data set, while the computational time of DFS, TFS and RFS is about 259s, 197s and 175s. The reason is that our proposed algorithm for computing the tolerance classes is faster than the traditional method, thus PFS has less time than TFS for feature selection. Also, we can observe that the worst algorithm is DFS, the possible reason is that the Boolean operation for the discernibility matrix is time-consuming. Comparing PFS and RFS, PFS can select a feature subset in much shorter time than RFS. The computation of the tolerance classes takes less time in PFS than that in RFS, and the computation of rough entropy in RFS takes more time than that of positive region in PFS.

From the experimental results, it should be stressed that the proposed algo-rithm is more efficient than the existing feature selection algorithms for dimen-sionality reduction, especially for the incomplete data. In real-world data sets, it often happened that the feature values of an object set are missing. Although there are some feature selection algorithms to deal with such data sets, they are usually not computationally costless. The main reason is that computing the tolerance classes is time-consuming. In this paper, we firstly provided an effective method of computing the tolerance classes and posi-tive region, and then we developed a consistency-based dimensionality reduction algorithm, the experiments are conducted on UCI data sets. The results demon-strate the proposed algorithm has less computational time for dimensionality reduction comparing the state-of-the-art methods.
 Acknowledgments. This work was supported by the Innovation Funds of Ex-cellence Doctor of Beijing Jiaotong University (No.2014YJS040), the Natural Science Foundation of Jiangx i Province(No.20132BAB201045).

