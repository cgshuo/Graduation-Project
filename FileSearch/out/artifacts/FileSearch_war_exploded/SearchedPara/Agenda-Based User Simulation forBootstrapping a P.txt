 1.1 Bootstrapping Statistical Dialogue Managers One of the key advantages of a statistical approach to Dia-logue Manager (DM) design is the ability to formalise de-sign criteria as objective reward functions and to learn an optimal dialogue policy from real dialogue data. In cases where a system is designed from scratch, however, it is often the case that no suitable in-domain data is available for training the DM. Collecting dialogue data without a working prototype is problematic, leaving the developer with a classic chicken-and-egg problem.

Wizard-of-Oz (WoZ) experiments can be carried out to record dialogues, but they are often time-consuming and the recorded data may show characteristics of human-human conversation rather than typical human-computer dialogue. Alternatively, human-computer dialogues can be recorded with a handcrafted DM prototype but neither of these two methods enables the system designer to test the implementation of the statistical DM and the learn-ing algorithm. Moreover, the size of the recorded corpus (typically 10 3 dialogues) usually falls short of the re-quirements for training a statistical DM (typically 10 4 dialogues). 1.2 User Simulation-Based Training In recent years, a number of research groups have inves-tigated the use of a two-stage simulation-based setup. A statistical user model is first trained on a limited amount of dialogue data and the model is then used to simulate dialogues with the interactively learning DM (see Schatz-mann et al. (2006) for a literature review).

The simulation-based approach assumes the presence of a small corpus of suitably annotated in-domain dia-logues or out-of-domain dialogues with a matching dia-logue format (Lemon et al., 2006). In cases when no such data is available, handcrafted values can be assigned to the model parameters given that the model is sufficiently simple (Levin et al., 2000; Pietquin and Dutoit, 2005) but the performance of dialogue policies learned this way has not been evaluated using real users. 1.3 Paper Outline This paper presents a new probabilistic method for simu-lating user behaviour based on a compact representation of the user goal and a stack-like user agenda . The model provides an elegant way of encoding the relevant dialogue history from a user X  X  point of view and has a very small parameter set so that manually chosen priors can be used to bootstrap the DM training and testing process.
In experiments presented in this paper, the agenda-based simulator was used to train a statistical POMDP-based (Young, 2006; Young et al., 2007) DM. Even with-out any training of its model parameters, the agenda-based simulator was able to produce dialogue behaviour realistic enough to train a competitive dialogue policy. An extensive study 1 with 40 human subjects showed that task completion with the learned policy was above 90% despite a mix of native and non-native speakers. 2.1 User Simulation at a Semantic Level Human-machine dialogue can be formalised on a seman-tic level as a sequence of state transitions and dialogue acts 2 . At any time t , the user is in a state S , takes ac-tion a u , transitions into the intermediate state S , receives machine action a m , and transitions into the next state S where the cycle restarts.
 Assuming a Markovian state representation, user be-haviour can be decomposed into three models: P ( a u | S ) for action selection, P ( S | a u ,S ) for the state transition into S , and P ( S | a m ,S ) for the transition into S . 2.2 Goal-and Agenda-Based State Representation Inspired by agenda-based methods to dialogue manage-ment (Wei and Rudnicky, 1999) the approach described here factors the user state into an agenda A and a goal G During the course of the dialogue, the goal G ensures that the user behaves in a consistent, goal-directed manner. G consists of constraints C which specify the required venue, eg. a centrally located bar serving beer, and re-quests R which specify the desired pieces of information, eg. the name, address and phone number (cf. Fig. 1).
The user agenda A is a stack-like structure containing the pending user dialogue acts that are needed to elicit the information specified in the goal. At the start of the dia-logue a new goal is randomly generated using the system database and the agenda is initially populated by convert-ing all goal constraints into inform acts and all goal re-quests into request acts. A bye act is added at the bottom of the agenda to close the dialogue.

As the dialogue progresses the agenda and goal are dy-namically updated and acts are selected from the top of the agenda to form user acts a u . In response to incom-ing machine acts a m , new user acts are pushed onto the agenda and no longer relevant ones are removed. The agenda thus serves as a convenient way of tracking the progress of the dialogue as well as encoding the rele-vant dialogue history. Acts can also be temporarily stored when actions of higher priority need to be issued first, hence providing the simulator with a simple model of user memory 3 . 2.3 User Act Selection At any time during the dialogue, the updated agenda of length N contains all dialogue acts the user intends to convey to the system. Since the agenda is ordered ac-cording to priority, with A [ N ] denoting the top and A [1] denoting the bottom item, selecting the next user act sim-plifies to popping n items off the top of the stack. Hence, letting a u [ i ] denote the i th item in the user act a u a u [ i ]:= A [ N  X  n + i ]  X  i  X  [1 ..n ] , 1  X  n  X  N. (3)
Using A [ N  X  n +1 ..N ] as a Matlab-like shorthand nota-tion for the top n items on A , the action selection model becomes a Dirac delta function
P ( a u | S )= P ( a u | A, G )=  X  ( a u ,A [ N  X  n +1 ..N ]) where the random variable n corresponds to the level of initiative taken by the simulated user. In a statistical model the probability distribution over integer values for n should be conditioned on A and learned from dialogue data. For the purposes of bootstrapping the system, n can be assumed independent of A and any distribution P ( n ) that places the majority of its probability mass on small values of n can be used. 2.4 State Transition Model The factorisation of S into A and G can now be ap-plied to the state transition models P ( S | a u ,S ) and P ( S | a m ,S ) . Letting A denote the agenda after select-ing a u (as explained in the previous subsection) and using N = N  X  n to denote the size of A ,wehave Using this definition of A and assuming that the goal remains constant when the user executes a u , the first state transition depending on a u simplifies to Using S =( A, G ) , the chain rule of probability, and rea-sonable conditional independence assumptions, the sec-ond state transition based on a m can be decomposed into goal update and agenda update modules: When no restrictions are placed on A and G , the space of possible state transitions is vast. The model parame-ter set is too large to be handcrafted and even substantial amounts of training data would be insufficient to obtain reliable estimates. It can however be assumed that A is derived from A and that G is derived from G and that in each case the transition entails only a limited number of well-defined atomic operations. 2.5 Agenda Update Model for System Acts The agenda transition from A to A can be viewed as a sequence of push-operations in which dialogue acts are added to the top of the agenda. In a second  X  X lean-up X  step, duplicate dialogue acts, null() acts, and unnecessary request() acts for already filled goal request slots must be removed but this is a deterministic procedure so that it can be excluded in the following derivation for simplicity. Considering only the push-operations, the items 1 to N at the bottom of the agenda remain fixed and the update model can be rewritten as follows: The first term on the RHS of Eq. 9 can now be further simplified by assuming that every dialogue act item in a m triggers one push-operation. This assumption can be made without loss of generality, because it is possible to push a null() act (which is later removed) or to push an act with more than one item. The advantage of this as-sumption is that the known number M of items in a m now determines the number of push-operations. Hence N = N + M and The expression in Eq. 11 shows that each item a m [ i ] in the system act triggers one push operation, and that this operation is conditioned on the goal. This model is now simple enough to be handcrafted using heuristics. For ex-ample, the model parameters can be set so that when the item x=y in a m [ i ] violates the constraints in G , one of the following is pushed onto A : negate() , inform(x=z) , deny(x=y, x=z) , etc. 2.6 Goal Update Model for System Acts The goal update model P ( G | a m ,G ) describes how the user constraints C and requests R change with a given machine action a m . Assuming that R is conditionally independent of C given C it can be shown that
To restrict the space of transitions from R to R it can be assumed that the request slots are independent and each slot (eg. addr,phone,etc. ) is either filled using infor-mation in a m or left unchanged. Using R [ k ] to denote the k  X  X h request slot, we approximate that the value of R [ k ] only depends on its value at the previous time step, the value provided by a m , and M ( a m ,C ) which indicates a match or mismatch between the information given in a m and the goal constraints.

To simplify P ( C | a m ,R ,C ) we assume that C is derived from C by either adding a new constraint, set-ting an existing constraint slot to a different value (eg. drinks=dontcare ), or by simply changing nothing. The choice of transition does not need to be conditioned on the full space of possible a m , R and C . Instead it can be conditioned on simple boolean flags such as  X  X oes a m ask for a slot in the constraint set? X ,  X  X oes a m signal that no item in the database matches the given constraints? X , etc. The model parameter set is then sufficiently small for handcrafted values to be assigned to the probabilities. 3.1 Training the HIS Dialogue Manager The Hidden Information State (HIS) model is the first trainable and scalable implementation of a statistical spoken dialog system based on the Partially-Observable Markov Decision Process (POMDP) model of dialogue (Young, 2006; Young et al., 2007; Williams and Young, 2007). POMDPs extend the standard Markov-Decision-Process model by maintaining a belief space , i.e. a proba-bility distribution over dialogue states, and hence provide an explicit model of the uncertainty present in human-machine communication.

The HIS model uses a grid-based discretisation of the continuous state space and online -greedy policy iter-ation. Fig. 2 shows a typical training run over 60,000 simulated dialogues, starting with a random policy. User goals are randomly generated and an (arbitrary) reward function assigning 20 points for successful completion and -1 for every dialogue turn is used. As can be seen, di-alogue performance (defined as the average reward over 1000 dialogues) converges after roughly 25,000 iterations and asymptotes to a value of approx. 14 points. 3.2 Experimental Evaluation and Results A prototype HIS dialogue system with a learned policy was built for the Tourist Information Domain and exten-sively evaluated with 40 human subjects including native and non-native speakers. A total of 160 dialogues with 21667 words was recorded and the average Word-Error-Rate was 29.8%. Task scenarios involved finding a spe-cific bar, hotel or restaurant in a fictitious town (eg. the address of a cheap, Chinese restaurant in the west).
The performance of the system was measured based on the recommendation of a correct venue, i.e. a venue matching all constraints specified in the given task (all tasks were designed to have a solution). Based on this definition, 145 out of 160 dialogues (90.6%) were com-pleted successfully, and the average number of turns to completion was 5.59 (if no correct venue was offered the full number of turns was counted). This paper has investigated a new agenda-based user sim-ulation technique for bootstrapping a statistical dialogue manager without access to training data. Evaluation re-sults show that, even with manually set model parame-ters, the simulator produces dialogue behaviour realistic enough for training and testing a prototype system. While the results demonstrate that the learned policy works well for real users, it is not necessarily optimal. The next step is hence to use the recorded data to train the simulator, and to then retrain the DM policy.

