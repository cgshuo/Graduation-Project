 The technique of latent semantic indexing is used in a wide variety of commercial applications . In these applications, the processing time and RAM required for SVD computation, and the processing time and RAM requi red during LSI retrieval operations are all roughly linear in the number of dimensions, k, chosen for the LSI representation space. In large-scale commercial LSI applications, re ducing k values could be of significant value in reducing server costs. This paper explores the effects of varying dimensionality. The approach taken here focuses on term comparisons. Pairs of terms are considered which have st rong real-world associations. The proximities of members of these pairs in the LSI space are million document collection, a value of k  X  400 provides the best performance. The results suggest that there is something of an  X  X sland of this range without incurring significan t distortions in at least some term-term correlations. H.3.4 [Information Storage and Retrieval] Systems and Software Performance evaluation Algorithms, Performance, Experimentation LSI, Dimensionality, Latent Semantic Indexing Latent semantic indexing (LSI) is a machine learning technique that takes as input a collecti on of documents and produces as output a vector space representation of the documents and terms of the collection. The central feat ure of LSI is the use of singular dimensionality reduction for the problem addressed. The technique of latent seman tic indexing consists of the following primary steps: [1] 1. A matrix is formed, wherein each row corresponds to a term that appears in the documents of interest, and each column corresponds to a documen t. Each element ( m,n ) in the matrix corresponds to the number of times that the term m occurs in document n . 2. Local and global term weighting is applied to the entries in the term-document matrix. 3. Singular value composition (SVD) is used to reduce this matrix to a product of three matrices, one of which has non-zero values (the singular values) only on the diagonal. 4. Dimensionality is reduced by deleting all but the k largest values on this diagonal, together with the corresponding columns in the other two matrices. This truncation process is used to generate a k -dimensional vector space. Both terms and documents are represented by k-dimensional vectors in this vector space. 5. The relatedness of any two objects represented in the space is reflected by the proximity of their representation vectors, generally using a cosine measure. LSI provides an elegant solution for a wide range of information Historically, use of this technique has been constrained by its relatively high requirements for computational power and fast memory, as compared to alternative techniques. Today, however, advances in the throughput of mi croprocessors and decreases in the cost of RAM have greatly redu ced these constraints. LSI now routinely is being applied to coll ections of millions to tens of millions of documents. In commercial applications, the choice of the parameter k is of performance on a variety of tasks was heavily dependent upon the value of k. Choosing too low a value for k did not provide sufficient fidelity of representation of relationships among terms and documents. Choosing too large a value introduced too much noise. Several experiments s howed that a value of k = 300 produced optimal results for specific tasks. Based on these investigations, k = 300 has, over time become something of a of experience in working with this value for k in commercial practice. Across a wide range of applications, this choice of dimensionality has been shown to produce highly useful applications, term proximity * in LSI spaces with k = 300 has been shown to correlate well with hum an judgment of proximity of those terms in a semantic sense. In large-scale commercial applications, the choice of k has a significant impact on required hardware resources. The processing time and RAM required for SVD co mputation, and the processing time and RAM required during LSI retrieval operations are all roughly linear in the number of dime nsions chosen. In large-scale applications, a reduction in k can translate directly into reductions in server costs and improvements in system performance. This paper represents an initial investigation of the required value of k for large (multi-million document) text collections. The question of appropriate choice of dimensionality was addressed in early work on LSI. Probably the most influential work in this area was published by Landauer and Dumais in 1997 [3]. They used 30,473 articles from an encyclopedia to form an LSI space. They then employed vectors from that space in similarity comparisons for words from the Test of English as a Foreign Language (TOEFL). They found that the degree of match between cosine measures in the LSI space and human judgment was strongly dependent on k, with a maximum for k = 300. Since that time, many other aut hors have carried out studies of LSI performance variation as a function of k. A list of such tests is presented in Table 1 on the following page. Figure 2, also on the following page. There is great variability in results, even for the experiments that used standard test collections. For example, 12 of the points in Figure 2 correspond to tests carried out using the MED collection [4]. In these 12 optimum k, varying from 40 to 200. In some cases, differences seen for various collections reflect use of different evaluation metrics by different investigators. In experimental parameters, such as use of stemming, stopwords employed, and differing methods fo r local and global weighting. Overall, as a result of these f actors, it is difficult to draw conclusions based on the aggregate of published results. In general, however, there is a sugge stion that larger values for optimum k are associated with larger test collections. These previous studies have dea lt with small test collections (typically from several hundreds to a few tens of thousands of documents). In such collections, chance co-occurrence of terms in a few documents can have a significant impact on term-term relationships. This raises a ques tion as to the extent to which indications of optimum dimensionality from those studies can be In these LSI spaces, the cosine between the representation vectors corresponding to the term s typically is used as the proximity measure. generalized to much larger document collections. One of the primary goals of the present study was to examine dimensionality dependencies for collections more representative of modern LSI applications (millions of documents). There has been theoretical work on techniques for estimating optimum dimensionality for LSI. However, no computational method of determining optimum values for k has come into general use in LSI applications [5]. Skillicorn describes four technique s for determining appropriate choices for k: [6]  X  identifying sharp changes in a scree plot of the singular  X  evaluating the entropy of the collection of singular values;  X  the technique of Zhu and Ghodsi [7], which treats singular  X  choosing k such that the residual matrix corresponding to the Other approaches employ heuristics applied to the singular values. For example, for small collections, some approaches keep all of the singular values greater than th e average of the singular values. Ding has proposed a probabilistic model allowing determination of optimum k that exploits th e dual relationship between terms and documents in LSI spaces [8]. Efron has proposed a technique employing amended parallel analysis, which has performed well for small collections [9]. Examining a scree plot of the singular values produced by the SVD computation has been popular . When these values are plotted in descending order, ther e sometimes is a point where the values drop significantly. The k singular values up to the point where this drop occurs are then kept. Unfortunately, however, 
Singular Value
Figure 1. Distribution of singul ar values for the five million Figure 1 shows a plot of the first 500 singular values for the five million document test collection used here. The distribution of singular values is very smooth, precluding use of this approach. 1990 Deerwester et al [10] 1,033 100 1994 Hull [11] 1,399 200 1994 Young [12] 18,895 113-115 1996 Syu, Lang, and Deo [13] 82-1997 Landauer &amp; Dumais [3] 30,473 300 1998 Wu, Yang, and Soo [14] 670 10 1998 Yang et al [15] 1,134 200 1998 Zha [16] 1,033 50-150 1999 Jiang et al [17] 10,000 900 1999 Lerman [18] 1,000 6 1999 Wiemer-Hastings et al [19] 8,100 200-300 2000 Jiang &amp; Littman [20] 1,400 300 2000 Kanerva, et al [21] 37,600 250 2000 Wiemer-Hastings [22] 184 200 2000 Ye [23] 1,033 133 2001 Caron [24] 979 350 2001 Husbands et al [25] 1,033 200 2001 Jessup &amp; Martin [26] 424-2001 Lizza &amp; Sartoretto [27] 423 -2001 Torrkola [28] 6,535 513 2002 Buckeridge and 2002 Cheng [30] 425-2002 Olde et al [31] 416-2003 Dumais [32] 1,033 90 2003 Gee [33] 2,893 200 2003 Kim et al [34] 79,919 200 2003 Lin and Gunopulos [35] 10,377 400 2003 Price [36] 2,572 90 2003 Singh, Hull, and 2003 Turney and Littman [38] 37,600 250 2004 Dobsa and Basic [39] 1,033-2004 Dupret [40] 82-2004 He et al [41] 7,800 246 2004 Pincombe [42] 364 150 2004 Shima et al [43] 7,063 316 2005 Efron [9] 392-2005 Elsas [44] 2,000 10 2005 Moldovan, Bot , &amp; 2005 Moravec [46] 16,889 100 2005 Tang et al [47] 505-2005 Yu, Yu, and Tresp [48] 1,600-2006 Geis [49] 169 100 2006 Kontostathis &amp; 2006 Kumar and Srinivas [51] 1,033-2007 Budiu, Royer, and 2007 Dumais [53] 1,033 90 2007 Haley et al [54] 667 80 2007 Kontostathis [55] 1,033-2007 Li and Shawe-Taylor [56] 1,000 200-400 In an LSI space, one may compare terms with terms, terms with documents, or documents with documents. In many contemporary applications of LSI, relations between terms are of primary importance. Examples include ontology construction [57][58], electronic data discovery [59][60][61], and intelligence the degree of relatedness between entities such as names of people, locations, and organizati ons. Most studies of optimum dimensionality have focused on co mparisons between documents. Emphasis on mean square error or precision/recall measures for documents may mask large varia tions in relationships between specific term pairs. Because of this concern, this study focuses on relationships between terms. A ccordingly, the results will be most applicable to LSI applications where term comparisons are of particular importance. As noted by Bast and Mumjar, no one choice of dimensionality will work well for all related term pairs [64]. What is sought here is a value for k that will provide optimum average performance and avoid exponential divergence of term ranks. In this study, three LSI spaces were created, based on indexing test collections of one, two, and five million documents, respectively. The documents in each collection are a subset of the documents in the next larger coll ection. The documents are news articles, covering the time frame 1998 to 2006. News articles were considered to be particularly useful for this investigation for the following reasons:  X  The articles cover a wide range of topics, improving the  X  There are a large number of occurrences of terms of  X  Prior work has shown that when news articles are used as a The characteristics of the test collections used are shown in Table 2. For each collection, only terms that occurred at least twice and in at least two different documen ts were indexed. This is a common convention when creating LSI representation spaces. This constraint eliminates larg e numbers of  X  X oi se X  terms. 1M Doc Set 4/2004  X  9/2006 1,000,000 714,855 2M Doc Set 10/2003  X  9/2006 2,000,000 1,044,338 5M Doc Set 12/1998  X  9/2006 5,000,000 1,809,597 Contemporary applications of LSI principally exploit the ability of the technique to discern relati onships among terms. Clearly, to be of utility, an LSI representation space must imply close relationships between items th at have close real-world associations. In this study, pairs of terms we re examined that have close associations. The 250 pairs employed included:  X  country-capital pairs, (e.g., Denmark-Copenhagen, Hungary- X  common synonyms (e.g., accelerate-quicken, doctor- X  person-country pairs (e.g., Cast ro-Cuba, Musharraf-Pakistan)  X  person-occupation pairs (e.g., Einstein-physicist, Picasso- X  words and their plural forms (e.g., bacterium-bacteria, tooth- X  nouns and their adjectival fo rms (e.g., America-American,  X  noun-verb pairs (e.g. discuss-discussion, attack-attacker)  X  verb declensions (e.g., swim-swam, run-running)  X  words and their contractions (e.g., automobile-auto,  X  contextually related terms (e.g., astronomy-telescope, In the experiment, for each term pair of interest, one term was treated as a query in the space. Vectors for other terms were ranked in relationship to this que ry, based on the cosine measure between those vectors and the query vector. The rank of the paired term in this list was us ed as the measure of relatedness between the two terms. Rank was used to allow comparisons across different collections and across different values of k. Rankings were determined for valu es of k= 10, 25, 50, 75, 100, 150, 200, 250, 300, 400, 500, 600, 700, 800, 900, and 1000. Cosine values, per se, could not be used as a measure for two reasons. First, cosine values are not comparable across collections. Second, and more im portantly, cosine measures vary with dimension within a collection. For LSI spaces in general, higher dimensionality yields lower mean cosine values. Figure 3 s hows the average cosine between document collection used here. Average Cosine Although the average cosines comprise a smooth, monotonically decreasing curve, individual cosines vary widely in their behavior. Figure 4 shows exampl es of cosine variation with dimension for three cases in the 5 million document set which differ significantly from the average. The cosine values for the Musharraf-Pakistan test pair decrease very rapidly with increasing relatively constant across the dimensions tested. The curve for the Finland-Helsinki term pair increases significantly with increasing dimension.
 For each term pair considered, test results are shown here as the rank of the second term with resp ect to the first term, plotted versus dimension. Several types of behavior are noted. For some term pairs, the plot of experimental results is nearly flat across the dimensions tested. More commonl y, there is divergence of the rank values at low dimensionality. This is most pronounced at the lowest dimensions tested, k = 10 and k = 25. In some cases, divergence at these low dimensi ons is exponential (e.g., a term that has rank = 2 at k = 300 will have rank &gt; 100,000 at k = 10). There generally is some divergence in rank at the highest dimensions tested, particularly at k = 1000. In some cases, the divergence at the high dimensions is exponential. Typically there is some divergence at both low and high dimensionality, with the divergence at low k values being much greater than that at high values. The varying patterns of rank divergence complicate the issue of determining an effective k value for a given application. Most evaluations of optimum rank have focused on average behavior. In some applications, this would be quite appropriate. However, in other applications, the exponential divergence for some term pairs must be avoided, to the greatest extent possible. Figure 5 shows the average rank for 250 pairs of associated terms The average rank of the term pairs is comparable over the range k = 300 to k = 500, with no major differences. Above 500 and below 300 dimensions, however, the ranks diverge relatively rapidly. The results indicate that the best average performance might be obtained with k  X  400. There are some term pairs that demonstrate rapid divergence at low or high dimensionality (or both). Figure 6 shows examples of dimensionality. In some important applications, us ers want to gain insight into questions such as:  X  What people are most closely associated with a given  X  What people are most closely associated with each other?  X  What locations are most closely associated with a given As Figure 6 shows, use of k values less than 300 here will result in some terms being dropped from the upper ranks of lists generated phenomenon will vary with the appli cation. However, in general it is clear that there will be a grow ing degradation of representational fidelity as the dimensionality is reduced below k = 300. Depending upon the application, such behavior may preclude use of dimensionality less than 300. In all of the cases investigated here, divergences such as those presented in Figure 6 represented a genuine departure from real-million document test set at selected values of k. 
Table 3. Top 10 ranked terms for selected k values for query k = 10 k = 50 k = 100 k = 500 intermix mathematician malignanc y compass Exploratorium cosmology Bohr unmoving cosmic Bohr physics nature X  X  Zato-based Bohr X  X  mathematician terms vary only slightly. All are intuitively reasonable. At dimensions less than 300, the t op 10 terms start to become less query term. At this low dimensionality, the rank of the term physicist with respect to the query term Einstein is &gt;400,000. At that dimensionality, the term typist is much closer to the term Einstein than is the term physicist . Clearly such instances mark a representing real-world associations. The same type of divergence is also noted in the one and two million document collections, as shown in Figures 7 and 8. The divergences shown in these figures are not the result of idiosyncrasies of term distri butions in a small number of documents. Every term used in the examples presented here appears at least one hundred times in the test collections. The median number of occurrences of the example terms in the 5 million document test set is nearly 25,000. There are roughly proportionate numbers of occurrences for these terms in the 1 and 2 million document test sets. For some pairs of terms, there is an exponential divergence of divergence.  X 
Figure 9. Large divergence at high k values in the three test This figure vividly demonstrat es the strong effect of LSI dimensionality. For k varying from 25 to 300, the terms Russia and Russian are very closely associated, which matches our real-This is true in both an absolu te sense (rank) and a relative sense (relative to the number of documen ts or the number of terms in the respective collections). world expectations. For the two million and five million the one million document data set, at k = 300, Russian is the sets ranges from more than 10,000 to more than 100,000. In this divergence. This divergence is not a uni que aspect of the term Russian as the 10 top-ranked terms all appear to be quite reasonable from the point of view of what would be expected of real-world associations. For example, in the five million document test set, terms. Moscow is the next closest. T hus, at this dimensionality, it would appear that the LSI representation vector for Russia does a good job of representing the general context expected of that term. At the higher k values, however, the terms appear more specialized. At k = 1000, the entire top 10 terms from the k=300 case have fallen below rank = 10. They have been replaced by terms that imply more specific contexts. For example, one is the name of a Russian television progr am. Another is the name of a Russian television personality. As in the previous examples, this behavior is not an artifact of chance term distribution in a sm all number of documents. The terms Russia and Russian appear hundreds of thousand to millions of times in these collecti ons, as shown in Table 4. 
Table 4. Occurrences of the terms Russia and Russian in the As Figure 9 shows, for terms such as this, use of k values greater than 400 will result in some terms being dropped from the upper described above. The precise impact of this will vary with the there will be a growing degradati on of representational fidelity as the dimensionality is increased beyond 400. Depending upon the application, such behavior may preclude use of dimensionality greater than 400. For some term pairs, the degree of divergence at both higher and the query term pair judge -court . Once again, the greatest difference corresponds to the largest test set. the 10 top-ranked terms differ only s lightly. All are general terms related to judges and courts. At lower and higher values of k, specific names of police inspectors, judges, and defendants appear in the top 10 list. It is apparent that these represent more specific associations emphasized at individual choices of dimensionality. At k = 10, the common terms that prevail in the 150 to 400 range are almost completely replaced by specific names. At this dimensionality, the rank of the term court with respect to the letterhead is much closer to the term judge than is the term court. term judge is &gt;3,000. At that dimensionality, the term video-makers is much closer to the term judge than is the term court. Clearly, at both low and high dimens ionality, there is a substantial degradation in the fidelity of the LSI space in representing real-world associations. Figure 10. Large divergence at both low and high k values for As with the other examples presented here, the number of occurrences of these terms is large enough to ensure that the observed pattern is genuinely an effect of dimensionality. Table 5 shows the number of occurrences of the terms judge and court in the test sets. Table 5. Occurrences of the terms judge and court in the test sets rank divergence. For example, Figure 11 shows the case for the term pairs fire-bullet and fire-extinguished in the five million document test set. At k = 10, the terms bullet and extinguished are comparable in rank. From there through k=300, the term bullet is more highly ranked than the word extinguished . At k=300, the two terms are of nearly equal rank. Above that point, the curves diverge. Extinguished is the most highly ranked term with respect to fire at k=500. It remains at rank 1 through k=1000. In contrast, above k=500, the rank of the term bullet diverges rapidly. Overall, in this collection, the term bullet occurs much more often more or less comparable fashion. This type of divergence has been seen for other pairs of related terms where one or both of the terms is polysemous. It is apparent that polysemy is an im portant factor affecting relative ranking of many term pairs. 
Figure 11. Opposite divergence patterns for two senses of a The word fire is particularly polysemous. No single value of k related to its various meanings. 
Table 6. Occurrences of the terms from Figure 11 in the five Term # of The experimental results reporte d here provide further evidence that the question of representati onal fidelity of LSI spaces as a function of dimension is a complex issue. Of the k values tested here, k=400 provided the best resu lts. This is somewhat higher than the value of k=300 often cited in previous work. This likely In 1991, Susan Dumais, one of the inventors of LSI, reported that k=100 worked well for the test collections available at that time (  X  1,000 documents). At that tim e she stated:  X  X he number of dimensions needed to adequately capture the structure in other collections will probably depend on their breadth X  [66]. Since that time, numerous other researchers have speculated that the broader the conceptual content of a collection, the larger the value of k would need to be in order to adequately represent it. The results presented here are consis tent with that perspective. As noted earlier, a value of 300 appeared to be a good choice for k when text collections under examination were in the range of reported here deals with collec tions two orders of magnitude larger. However, the optimum value obtained of k  X  400 is only modestly larger. If more dimensi ons are, indeed, required to deal with larger collections, the grow th in k appears to be quite modest. This would be in acco rd with the Johnson-Lindenstrauss Lemma [67] which would suggest that such growth need be no more than logarithmic.  X  This previously has been suggested as a possible guiding principle for selection of k by Skillicorn [68]. In the test results presented here there appears to be something of an  X  X sland of stability X  in the k = 300 to 500 range. Exponential outside of this range without incu rring significant distortions in at least some term-term correlations. For the data dealt with here, k could be reduced to 250 with so me degradation. Reducing k below this value would produce major distortions for at least some pairs of terms. Conversely, increasing k much beyond 500 also would have a significant adverse effect on term-term correlations. More work remains to be done in order to develop an adequate understanding of the effects of choice of dimensionality for large LSI applications. Term-term corre lations need to be investigated evaluations need to be carried out that emphasize relations between entities; especially persons , locations, and organizations. The rankings of terms used here include counts of many terms that have little application-rela ted significance. Future work needs to emphasize rankings among entities, not among all terms. For example a useful metric might be: what is the rank of a given person name among the person names most closely associated with a given entity? [1] Deerwester, S. et al 1988. Improving information retrieval [2] Dumais, S. 2004. Latent Semantic Analysis. In ARIST [3] Landauer, T. and Dumais, S. 1997. A solution to Plato X  X  [4] Medline standard IR test collection available at [5] Landauer, T. et al, eds., 2007. Handbook of Latent Semantic [6] Skillicorn, D. 2007. Unders tanding Complex Datasets. [7] Zhu, M. and Ghodsi, A. 2006. Automatic dimensionality The Johnson-Lindenstrauss lemma shows that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space  X  where k is logarithmic in n and independent of d  X  so that all pa irwise distances are maintained within an arbitrarily small factor. [8] Ding, C. 1999. A dual probabilistic model for latent semantic [9] Efron, M. 2005. Eigenvalue-ba sed model selection during [10] Deerwester, S., et al. 1990. Indexing by latent semantic [11] Hull, D. 1994. Improving text retrieval for the routing [12] Young, P. 1994. Cross-Langua ge Information Retrieval [13] Syu, I., Lang, S., and Deo, N. 1996. Incorporating latent [14] Wu, S., Yang, P., and Soo, V. 1998. An assessment of [15] Yang, Y. et al 1998. Translingual information retrieval: [16] Zha, H. 1998. A Subspace-Based Model for Information [17] Jiang, F. et al, 1999. Efficient Singular Value [18] Lerman, K. 1999. Unpublished paper available at: [19] Wiemer-Hastings, P. et al. 1999. Improving an intelligent [20] Jiang, F. and Littman, M. 2000. Approximate dimension [21] Kanerva, P., Kristoferson, J ., and Holst, A. 2000. Random [22] Wiemer-Hastings, P. 2000. Addi ng syntactic information to [23] Ye, Y. 2000. Comparing Matrix Methods in Text-based [24] Caron, J. 2001. Experiments w ith LSA scoring: optimal rank [25] Husbands, P., Simon, H., and Ding, C. 2001. On the use of [26] Jessup, E. and Martin, J. 2001. Taking a new look at the [27] Lizza, M. and Sartoretto, F. 2001. A comparative analysis of [28] Torkkola, K. 2001. Linear discriminant analysis in document [29] Buckeridge, A. and Sutcliffe , R. 2002. Disambiguating noun [30] Cheng, B. 2002. Towards Unde rstanding Latent Semantic [31] Olde, B. et al 2002. The right stuff: do you need to sanitize [32] Dumais, S. 2003. Data-driven approaches to information [33] Gee, K. 2003. Using latent semantic indexing to filter spam. [34] Kim, Y-S., Chang, J-H., and Zhang, B-T. 2003. An [35] Lin, J., and Gunopulos, D. 2003. Dimensionality reduction [36] Price, R., 2003. Personal communication. [37] Singh, S., Hull, D., and Fluder, E. 2003. Text influenced [38] Turney, P., and Littman, M. 2003. Measuring praise and [39] Dobsa, J., and Basic, B. 2004. Comparison of information [40] Dupret, G. 2004. Latent semantic indexing with a variable [41] He, X. et al. 2004. Locality preserving indexing for [42] Pincombe, B. 2004. Comparis on of Human and Latent [43] Shima, K., Todoriki, M., a nd Suzuki, A. 2004. SVM-based [44] Elsas, J. 2005. An Evaluation of Projection Techniques for [45] Moldovan, A., Bot, R., and Wa nka, G. 2005. Latent semantic [46] Moravec, P. 2005. Testing di mension reduction methods for [47] Tang, B. et al 2005. Compar ing and combining dimension [48] Yu, K., Yu, S., and Tresp, V. 2005. Multi-label informed [49] Geis, J. 2006. Latent Seman tic Indexing and Information [50] Kontostathis, A., and Pottenge r, W. 2006. A framework for [51] Kumar, C. and Srinivas, S. 2006. Latent semantic indexing [52] Budiu, R., Royer, C., and Pirolli, P. 2007. Modeling [53] Dumais, S. 2007. LSA and information retrieval: getting [54] Haley, D. et al 2007. Tuni ng an LSA-based assessment [55] Kontostathis, A. 2007. Essen tial dimensions of latent [56] Li, Y., Shawe-Taylor, J. 2007. Advanced learning algorithms [57] Fortune, B., Mladenic, D., and Grobelnik, M. 2005. Semi-[58] Hoenkamp, E. 1998. Spotting ontological lacunae through [59] Lodder, A., and Oskamp, A. 2006. Information Technology [60] Edlund, S. et al. 2008. Supervision and discovery of [61] Waterman, K. 2006. Knowledge Discovery in Corporate [62] Skillicorn, D. 2004. Applying Matrix Decompositions to [63] Bradford, R. 2006. Application of latent semantic indexing [64] Bast, H., and Mumjar, D. 2005. Why spectral retrieval [65] Lindsey, R. et al. 2007. Be wary of what your computer [66] Dumais, S. 1991. Improving the retrieval of information [67] Johnson, W., and Lindenstrauss, J., 1984. Extensions of [68] Skillicorn, D., McConnell, S., and Soong, E. 2003. 
