 Sanjoy Dasgupta dasgupta@cs.ucsd.edu Daniel Hsu djhsu@cs.ucsd.edu 9500 Gilman Drive, La Jolla, CA 92093-0404 The active learning model is motivated by scenarios in which it is easy to amass vast quantities of unlabeled data (images and videos off the web, speech signals from microphone recordings, and so on) but costly to obtain their labels. It shares elements with both su-pervised and unsupervised learning. Like supervised learning, the goal is ultimately to learn a classifier. But like unsupervised learning, the data come unla-beled. More precisely, the labels are hidden, and each of them can be revealed only at a cost. The idea is to query the labels of just a few points that are especially informative about the decision boundary, and thereby to obtain an accurate classifier at significantly lower cost than regular supervised learning. Indeed, there are canonical examples in which active learning prov-ably yields exponentially lower label complexity than supervised learning (Cohn et al., 1994; Freund et al., 1997; Dasgupta, 2005; Balcan et al., 2006; Balcan et al., 2007; Castro &amp; Nowak, 2007; Hanneke, 2007; Dasgupta et al., 2007). However, these examples are highly specific, and the wider efficacy of active learning remains to be characterized.
 Sampling bias. A typical active learning heuristic might start by querying a few randomly-chosen points, to get a very rough idea of the decision boundary. It might then query points that are increasingly closer to its current estimate of the boundary, with the hope of rapidly honing in. Such heuristics immediately bring to the forefront the unique difficulty of active learn-ing, the fundamental characteristic that separates it from other learning models: sampling bias . As train-ing proceeds, and points are queried based on increas-ingly confident assessments of their informativeness, the training set quickly diverges from the underlying data distribution. It consists of an unusual subset of points, hardly a representative subsample; why should a classifier trained on these strange points do well on the overall distribution? In section 2, we make this in-tuition concrete, and show how ill-managed sampling bias causes many active learning heuristics to not be consistent: even with infinitely many labels, they fail to converge to a good hypothesis.
 The two faces of active learning. The recent liter-ature offers two distinct narratives for explaining when active learning is helpful. The first has to do with effi-cient search through the hypothesis space . Each time a new label is seen, the set of plausible classifiers (those roughly consistent with the labels seen so far) shrinks somewhat. Using active learning, one can explicitly select points whose labels will shrink this set as fast as possible. Most theoretical work in active learning attempts to formalize this intuition.
 The second argument for active learning has to do with exploiting cluster structure in data . Suppose, for in-stance, that the unlabeled points form five nice clus-ters; with luck, these clusters will be  X  X ure X  and only five labels will be necessary! Of course, this is hope-lessly optimistic. In general, there may be no nice clus-ters, or there may be viable clusterings at many differ-ent resolutions. The clusters themselves may only be mostly-pure, or they may not be aligned with labels at all. In this paper, we present a scheme for cluster-based active learning that is statistically consistent and never has worse label complexity than supervised learning. In cases where there exists cluster structure (at whatever resolution) that is loosely aligned with class labels, the scheme detects and exploits it. Our model. We start with a hierarchical clustering of the unlabeled points. This should be constructed so that some pruning of it is weakly informative of the class labels. We describe an active learning strategy with good statistical properties, that will discover and exploit any informative pruning of the cluster tree. For instance, suppose it is possible to prune the cluster tree to m leaves ( m unknown) that are fairly pure in the labels of their constituent points. Then, after querying just O ( m ) labels, our learner will have a fairly accurate estimate of the labels of the entire data set. These can then be used as is, or as input to a supervised learner. Thus, our scheme can be used in conjunction with any hypothesis class, no matter how complex. Many active learning heuristics start by choosing a few unlabeled points at random and querying their labels. They then repeatedly do something like this: fit a classifier h  X  H to the labels seen so far; and query the label of the unlabeled point closest to the decision boundary of h (or the one on which h is most uncertain, or something similar). Such schemes make intuitive sense, but do not correctly manage the bias introduced by adaptive sampling. Consider this 1-d example: Here the data lie in four groups on the line, and are (say) distributed uniformly within each group. Filled blocks have a + label, while clear blocks have a  X  la-bel. Most of the data lies in the two extremal groups, so an initial random sample has a good chance of com-ing entirely from these. Suppose the hypothesis class consists of thresholds on the line: H = { h w : w  X  R } where h w ( x ) = 1 ( x  X  w ). Then the initial bound-ary will lie somewhere in the center group, and the first query point will lie in this group. So will every subsequent query point, forever. As active learning proceeds, the algorithm will gradually converge to the classifier shown as w . But this has 5% error, whereas classifier w  X  has only 2.5% error. Thus the learner is not consistent: even with infinitely many labels, it returns a suboptimal classifier.
 The problem is that the second group from the left gets overlooked. It is not part of the initial random sample, and later on, the learner is mistakenly confident that the entire group has a  X  label. And this is just in one dimension; in high dimension, the problem can be expected to be worse, since there are more places for this troublesome group to be hiding out. For a discussion of this problem in text classification, see the recent paper of Schutze et al. (2006).
 Sampling bias is the most fundamental challenge posed by active learning. This paper presents a broad frame-work for managing this bias that is provably sound. Our active learner starts with a hierarchical clustering of the data. Figure 1 shows how this might look for the example of the previous section.
 Here only the top few nodes of the hierarchy are shown; their numbering is immaterial. At any given time, the learner works with a particular partition of the data set, given by a pruning of the tree. Initially, this is just { 1 } , a single cluster containing everything. Ran-dom points are drawn from this cluster and their la-bels are queried. Suppose one of these points, x , lies in the rightmost group. Then it is a random sample from node 1, but also from nodes 3 and 9. Based on these random samples, each node of the tree maintains statistics about the relative numbers of positive and negative instances seen. A few samples reveal that the top node 1 is very mixed while nodes 2 and 3 are sub-stantially more pure. Once this transpires, the parti-tion { 1 } will be replaced by { 2 , 3 } . Subsequent random samples will be chosen from either 2 or 3, according to a sampling strategy favoring the less-pure node. A few more queries down the line, the pruning will likely be refined to { 2 , 4 , 9 } . This is when the benefits of the partitioning scheme become most obvious; based on the samples seen, it can be concluded that cluster 9 is (almost) pure, and thus (almost) no more queries will be made from it until the rest of the space has been partitioned into regions that are similarly pure. The querying can be stopped at any stage; then, each cluster in the current partition gets assigned the ma-jority label of the points queried from it. In this way, the entire data set gets labeled, and the number of erroneous labels induced is kept to a minimum. If de-sired, these labels can be used for a subsequent round of supervised learning, with any learning algorithm and any hypothesis class. 3.1. Preliminary Definitions The cost of a pruning. Say there are n unlabeled points, and we have a hierarchical clustering repre-sented by a binary tree T with n leaves. For any node v of the tree, denote by T v both the subtree rooted at v and also the data points contained in this subtree (at its leaves). A pruning of the tree is a subset of nodes { v 1 , . . . , v m } such that the T v together cover all the data. At any given stage, the active learner will work with a partition of the data set given by a pruning of T . In the analysis, we will also deal with a partial pruning : a subset of a pruning. A weight of a node v  X  T is the proportion of the data set in T v : w v = (number of leaves of T v ) /n . Likewise, the weight of a partial pruning is the fraction of the data set that it covers, w ( P ) = ing has weight 1.
 Suppose there are k possible labels, and that their pro-portions in T v are p v,l for l = 1 , . . . , k . Then the error introduced by assigning all points in T v their majority label is  X  v = 1  X  max l p v,l . Consequently, the error induced by a particular pruning (or partial pruning) P  X  X hat is, the fraction of incorrect labels when each cluster of P is assigned its majority label X  X s In pruning the tree, it always helps to go as far down as possible, provided we can accurately estimate the majority labels in those nodes.
 Empirical estimates for individual nodes. Due of limited sampling, we will only have labels from some of the nodes, and even for those, we may not be able to correctly determine the majority label. If we as-sign label l to all the points in T v , the induced er-ror is  X  v,l = 1  X  p v,l . Likewise, when each cluster v of pruning (or partial pruning) P is assigned label L ( v )  X  X  1 , 2 , . . . , k } , the error induced is We will at any given time have only very imperfect estimates of the p v,l  X  X  and thus of these various er-ror probabilities. Fix any node v , and suppose that at time t , we have queried n v ( t ) random points contained in that node. This gives us estimates of its class prob-abilities, p v,l ( t ). Correspondingly, our estimate of  X  will be  X  v,l ( t ) = 1  X  p v,l ( t ).
 The quality of these estimates can be assessed us-ing generalization bounds. At any given time t , we can associate with each node v and label l a con-fidence interval [ p LB v,l , p UB v,l ] within which we expect the true probability p v,l to lie. One possibility is to use [max( p v,l ( t )  X   X  v,l ( t ) , 0) , min( p v,l ( t )+ X   X  will give a precise value for  X  v,l ( t ) for which we are able to assert that (with high probability) every p v,l is always within this interval. However, there are other ways of constructing confidence intervals as well. The most accurate is simply to use the binomial (or hyper-geometric) distribution directly.
 When are we confident about the majority label of a subtree? As mentioned above, it is advantageous to descend as far as possible in the tree, provided we are confident about our estimate of the majority label. To this end, define A A v,l asserts that l is an admissible label for node v , in the weak sense that it incurs at most  X  times as much error as any other label. To see this, notice that label l gets at most 1  X  p LB v,l ( t ) fraction of the points wrong, whereas l  X  gets at least 1  X  p UB v,l  X  ( t ) fraction of the points wrong. In our experiments, we use  X  = 2, in which case For any given v, t , several different labels l might sat-isfy this criterion, for instance if p LB v,l ( t ) = p UB 1 /k for all labels l . When there are only two possible We will maintain a set of ( v, l ) pairs for which the condition A v,l ( t ) is either true or was true sometime in the past: A ( t ) is the set of admissible ( v, l ) pairs at time t . We use it to stop ourselves from descending too far down tree T when only a few samples have been drawn. Specifically, we say pruning P and labeling L are ad-missible in tree T at time t if:  X  L ( v ) is defined for P and ancestors of P in T .  X  ( v, L ( v ))  X  A ( t ) for any node v that is a strict  X  For any node v  X  P , there are two options: This final condition implies that if a node in P is not admissible (with any label), then it is forced to take on an admissible label of its parent.
 Empirical estimate of the error of a pruning. For any node v , the empirical estimate of the er-ror induced when all of subtree T v is labeled l is  X  v,l ( t ) = 1  X  p v,l ( t ). This extends to a pruning (or partial pruning) P and a labeling L : This can be a bad estimate when some of the nodes in P have been inadequately sampled. Thus we use a more conservative adjusted estimate: with e  X  ( P, L, t ) = (1 /w ( P )) various definitions are summarized in Table 1. Picking a good pruning. It will be convenient to talk about prunings not just of the entire tree T but also of subtrees T v . To this end, define the score of v at time t  X  X enoted s ( v, t ) X  X o be the adjusted empirical error of the best admissible pruning and labeling ( P, L ) of T v . More precisely, s ( v, t ) is min { e  X  ( P, L, t ) : ( P, L ) admissible in T v at time t } . Written recursively, s ( v, t ) is the minimum of  X  e  X  v,l ( t ), for all l ; Algorithm 1 Cluster-adaptive active learning
Input: Hierarchical clustering of n unlabeled points; batch size B P  X  X  root } (current pruning of tree)
L (root)  X  1 (arbitrary starting label for root) for time t = 1 , 2 , . . . until the budget runs out do end for for each cluster v  X  P do end for Starting from the empirical estimates p v,l ( t ) , p LB v,l it is possible to update the set A ( t ) and to compute bottom-up pass through the tree. 3.2. The Algorithm Algorithm 1 contains the active learning strategy. It remains to specify the the manner in which the hier-archical clustering is built and the procedure select . Regardless of how these decisions are made , the al-gorithm is statistically sound in that the confidence intervals p v,l  X   X  v,l ( t ) are valid, and these in turn vali-date the guarantees for admissible prunings/labelings. This leaves a lot of flexibility to explore different clus-tering and sampling strategies.
 The select procedure. This controls the selective sampling. Some options: (1) Choose v  X  P with probability  X  w v . This is similar to random sampling. (2) Choose v with probability  X  w v (1  X  p UB v,L ( v ) ( t )). This is an active learning rule that reduces sampling in regions of the space that have already been observed to be fairly pure in their labels. (3) For each subtree ( T z , z  X  P ), find the observed majority label, and assign this label to all points in the subtree; fit a classifier h to this data; and choose v  X  P with probability  X  min {|{ x  X  T v : h ( x ) = +1 }| , |{ x  X  T v : h ( x ) =  X  1 }|} . This biases sampling towards regions close to the current decision boundary. Building a hierarchical clustering. The scheme works best when there is a pruning P of the tree such that | P | is small and a significant fraction of its con-stituent clusters are almost-pure. One option is to run a standard hierarchical clustering algorithm, like average linkage, perhaps with a domain-specific dis-tance function (or one generated from a neighborhood graph). Another option is to use a bit of labeled data to guide the construction of the hierarchy. 3.3. Naive Sampling First consider the naive sampling strategy in which a node v  X  P is selected in proportion to its weight w . We X  X l show that if there is an almost-pure pruning with m nodes, then only O ( m ) labels are needed before the entire data is labeled almost-perfectly. Proofs are deferred to the full version of the paper.
 Theorem 1 Pick any  X ,  X  &gt; 0 and any pruning Q with  X  ( Q )  X   X  . With probability at least 1  X   X  , the learner induces a labeling (of the data set) with error  X  (  X  + 1)  X  ( Q ) +  X  when the number of labels seen is Recall that  X  is used in the definition of an admissible label (equation (1)); we use  X  = 2 in our experiments. The number of prunings with m nodes is about 4 m ; and these correspond to roughly (4 k ) m possible clas-sifications (each of the m clusters can take on one of k labels). Thus this result is what one would expect if one of these classifiers were chosen by supervised learning. In our scheme, we do not evaluate such clas-sifiers directly, but instead evaluate the subregions of which they are composed. We start our analysis with confidence intervals for p v,l and n v .
 Lemma 1 Pick any  X  &gt; 0 . With probability at least 1  X   X  , the following holds for all nodes v  X  T , all labels l , and all times t . (a) | p v,l  X  p v,l ( t ) | X   X  v,l  X   X  v,l ( t ) , where  X  v,l ( t ) = for  X   X  =  X / ( kBt 2 d 2 v ) . (b) n v ( t )  X  Btw v / 2 if Btw v  X  8 log( t 2 2 2 d v / X  ) . Our empirical assessment of the quality of a pruning P is a blend of sampling estimates p v,l ( t ) and perfectly known values w v . Next, we examine the rate of con-vergence of  X  ( P, L, t ) to the true value  X  ( P, L ). Lemma 2 Assume the bounds of Lemma 1 hold.
 There is a constant c such that for all prunings (or partial prunings) P  X  T , all labelings L , and all t , w ( P ) |  X  ( P, L, t )  X   X  ( P, L ) |  X  c | P |
Bt Lemma 2 gives useful bounds on  X  ( P, L, t ). Our algo-rithm uses the more conservative estimate e  X  ( P, L, t ), which is identical to  X  ( P, L, t ) except that it automati-cally assigns an error of 1 to any ( v, L ( v )) 6 X  X  ( t ), that is to say, any (node, label) pair for which insufficiently many samples have been seen. We need to argue that for nodes v of reasonable weight, and their majority labels L  X  ( v ), we will have ( v, L  X  ( v ))  X  X  ( t ). Lemma 3 There is a constant c  X  such that ( v, l )  X  A ( t ) for any node v with majority label l and w v  X  max The purpose of the set A ( t ) is to stop the algorithm from descending too far in the tree. We now quantify this. Suppose there is a good pruning that contains a node q whose majority label is L  X  ( q ). However, our al-gorithm descends far below q , to some pruning P (and associated labeling L ) of T q . By the definition of ad-missible pruning, this can only happen if ( q, L ( q )) lies in A ( t ). Under such circumstances, it can be proved that ( P, L ) is not too much worse than ( q, L  X  ( q )). Lemma 4 For any node q , let ( P, L ) be the admissible pruning and labeling of T q found by our algorithm at time t . If ( q, L ( q ))  X  X  ( t ) , then  X  ( P, L )  X  (  X  + 1)  X  Proof sketch of Theorem 1. Let Q, t be as in the theo-rem statement, and let L  X  denote the optimal labeling (by majority label) of each node. Define V to be the set of all nodes v with weight exceeding the bound in Lemma 3. As a result, ( v, L  X  ( v ))  X  X  ( t ) for all v  X  V . Suppose that at time t , the learning scheme is using some pruning P with labeling L . We will decompose P and Q into three groups of nodes each: (i) P a  X  P are strict ancestors of Q a  X  Q ; (ii) P d  X  P are strict descendants of Q d  X  Q ; and (iii) the remaining nodes are common to P and Q .
 Since nodes of P a were never expanded to Q a , we w ( Q a \ V ). Meanwhile, from Lemma 4 we have Putting it all together, we get  X  ( P, L )  X   X  ( Q, L  X  )  X   X  + (  X  + 1)  X  ( Q ), under the conditions on t . 3.4. Active Sampling Suppose our current pruning and labeling are ( P, L ). So far we have only discussed the naive strategy of choosing query nodes u  X  P with probability propor-tional to w u . For active learning, a more intelligent and adaptive strategy is needed. A natural choice is to pick u with probability proportional to w u  X  UB u,L ( u ) associated with node u . This takes advantage of large, pure clusters: as soon as their purity becomes evident, querying is directed elsewhere.
 Fallback analysis. Can the adaptive strategy per-form worse than naive random sampling? There is one problematic case. Suppose there are only two labels, and that the current pruning P consists of two nodes (clusters), each with 50% probability mass; however, cluster A has impurity (minority label probability) 5% while B has impurity 50%. Under our adaptive strat-egy, we will query 10 times more from B than from A . But suppose B cannot be improved: any attempts to further refine it lead to subclusters which are also 50% impure. Meanwhile, it might be possible to get the error in A down to zero by splitting it further. In this case, random sampling, which weighs A equally to B , does better than the active learning scheme. Such cases can only occur if the best pruning has high impurity, and thus active learning still yields a pruning that is not much worse than optimal. To see this, pick any good pruning Q (with optimal labeling L  X  ), and let X  X  see how adaptive sampling fares with respect to Q . Suppose our scheme is currently working with a pruning P and labeling L . Divide P into two regions: P 0 = { p  X  P : p  X  T v for some v  X  Q } and P 1 = P \ P . The danger is that we will sample too much from P , where no further improvement is needed (relative to Q ), and not enough from P 1 . But it can be shown that either the active strategy samples from P 1 at least half as often as the random strategy would, or the current pruning is already pretty good, in that  X  ( P, L )  X  2  X  ( Q, L  X  )+terms involving sampling error . Benefits of active learning. Active sampling is sure to help when the hierarchical clustering has some large, fairly-pure clusters near the top of the tree. These will be quickly identified, and very few queries will subsequently be made in those regions. Consider an idealized example in which there are only two possi-ble labels and each node in the tree is either pure or (1 / 3 , 2 / 3)-impure. Specifically: (i) each node has two children, with equal probability mass; and (ii) each impure node has a pure child and an impure child. In this case, active sampling can be seen to yield a convergence rate 1 /n 2 in contrast to the 1 /n rate of random sampling.
 The example is set up so that the selected pruning P (with labeling L ) always consists of pure nodes { a pure node b (at depth d ). These nodes have weights w pure node causes the error of the best pruning to be node b to cut this error in half (say, because the target error is  X / 2). This can be achieved with a constant number of queries from node b , since this is enough to render the majority label of its pure child admissible and thus offer a superior pruning.
 If we were to completely ignore the pure nodes, then the next several queries could all be made in node b ; we thus halve the error with only a constant number of queries. Continuing this way leads to an exponen-tial improvement in convergence rate. Such a policy of neglect is fine in our present example, but this would be imprudent in general: after all, the nodes we ignore may actually turn out impure, and only further sam-pling would reveal them as such. We instead select a node u with probability proportional to w u  X  UB u,L ( u ) and thus still select a pure node a i with probability roughly proportional to w a some cautionary exploration while still affording an improved convergence rate.
 The chance of selecting the impure node b is w The inequality follows (with high probability) because the error bound for b is always at least the true error  X  (up to constants), while another argument shows that We need to argue that the pure nodes do not get queried too much. Well, if they have been queried at least of selecting b is  X ( with active sampling suffice to land a constant num-ber in node b  X  X ust enough to cut the error in half. Overall, the number of queries needed is then O ( p required of random sampling. How many label queries can we save by exploiting cluster structure with active learning? Our analysis suggests that the savings is tied to how well the clus-ter structure aligns with the actual labels . To evaluate how accommodating real world data is in this sense, we studied the performance of our active learner on several natural classification tasks. 4.1. Classification Tasks When used for classification, our active learning frame-work decomposes into three parts: (1) unsupervised hi-erarchical clustering of the unlabeled data, (2) cluster-adaptive sampling (Algorithm 1, with the second vari-ant of select ), and (3) supervised learning on the re-sulting fully labeled data. We used standard statistical procedures, Ward X  X  average linkage clustering and lo-gistic regression, for the unsupervised and supervised components, respectively, in order to assess just the role of the cluster-adaptive sampling method. We compared the performance of our active learner to two baseline active learning methods, random sam-pling and margin-based sampling, that only train a classifier on the subset of queried labeled data. Ran-dom sampling chooses points to label at random, and margin-based sampling chooses to label the points closest to the decision boundary of the current classi-fier (as described in Section 2). Again, we used logistic regression with both of these methods.
 A few details: We ran each active learning method 10 times for each classification task, allowing the budget of labels to grow in small increments. For each bud-get size, we evaluated the resulting classifier on a test set, computed its misclassification error, and averaged this error over the repeated trials. Finally, we used  X  -regularization with logistic regression, choosing the trade-off parameter with 10-fold cross validation. OCR digit images. We first considered multi-class classification of the MNIST handwritten digit images. 1 We used 10000 training images and 2000 test images. The tree produced by Ward X  X  hierarchical cluster-ing method was especially accommodating for cluster-adaptive sampling. Figure 2 (left) depicts this quanti-tatively; it shows the error of the best k -pruning of the tree for several values of k . For example, the tree had a pruning of 50 nodes with about 12% error. Our active learner found such a pruning using just 400 labels. Figure 2 (right) plots the test errors of the three active learning methods on the multi-class classification task. Margin-based sampling and cluster-adaptive sampling both outperformed random sampling, with margin-based sampling taking over a little after 2000 label queries. The initial advantage of cluster-adaptive sam-pling reflects its ability to discover and subsequently ignore relatively pure clusters at the onset of sampling. Later on, it is left sampling from clusters of easily con-fused digits (e.g. 3 X  X , 5 X  X , and 8 X  X ).
 The test error of the margin-based method appeared to actually dip below the test error of classifier trained using all of the training data (with the correct labels). This appears to be a case of fortunate sampling bias. In contrast, cluster-adaptive sampling avoids this issue by concentrating on converging to the same result as if it had all of the correct training labels. Newsgroup text. We also considered four pairwise binary classification tasks with the 20 Newsgroups data set. Following Schohn and Cohn (2000), we chose four pairs of newsgroups that varied in difficulty. We used a version of the data set that removes duplicates and some newsgroup-identifying headers, but other-wise represents each document as a simple word count vector. 2 Each newsgroup had about 1000 documents, and the data for each pair were partitioned into train-ing and test sets at a 2:1 ratio. We length-normalized the count vectors before training the logistic regression models in order to speed up the training and improve classification performance.
 The initial word count representation of the newsgroup documents yielded poor quality clusterings, so we tried various techniques for preprocessing text data before clustering with Ward X  X  method: (1) normalize each document vector to unit length; (2) apply TF/IDF and length normalization to each document vector; and (3) infer a posterior topic mixture for each document us-ing a Latent Dirichlet Allocation model trained on the same data (Blei et al., 2003). For the last technique, we used Kullback-Leibler divergence as the notion of distance between the topic mixture representations. Figure 3 (top) plots the errors of the best prunings. Indeed, the various changes-of-representation and spe-cialized notions of distance help build clusterings of greater utility for cluster-adaptive active learning. In all four pairwise tasks, both margin-based sampling and cluster-adaptive sampling outperformed random sampling. Figure 3 (bottom) shows the test errors on two of these newsgroup pairs. We observed the same effects regarding cluster-adaptive sampling and margin-based sampling as in the OCR digits data. 4.2. Rare Category Detection To demonstrate its versatility, we applied our cluster-adaptive sampling method to a rare category detec-tion task. We used the Statlog Shuttle data, a set of 43500 examples from seven different classes; the small-est class comprises a mere 0 . 014% of the whole. To discover at least one example from each class, ran-dom sampling needed over 8000 queries (averaged over several trials). In contrast, cluster-adaptive sampling needed just 880 queries; it sensibly avoided sampling much from clusters confidently identified as pure, and instead focused on clusters with more potential. Acknowledgements. Support provided by the En-gineering Institute at Los Alamos National Labora-tory and the NSF under grants IIS-0347646 and IIS-0713540.
 Balcan, M.-F., Beygelzimer, A., &amp; Langford, J. (2006). Agnostic active learning. ICML .
 Balcan, M.-F., Broder, A., &amp; Zhang, T. (2007). Mar-gin based active learning. COLT .
 Blei, D., Ng, A., &amp; Jordan, M. (2003). Latent dirichlet allocation. JMLR , 3 , 993 X 1022.
 Castro, R., &amp; Nowak, R. (2007). Minimax bounds for active learning. COLT .
 Cohn, D., Atlas, L., &amp; Ladner, R. (1994). Improving generalization with active learning. Machine Learn-ing , 15 , 201 X 221.
 Dasgupta, S. (2005). Coarse sample complexity bounds for active learning. NIPS .
 Dasgupta, S., Hsu, D., &amp; Monteleoni, C. (2007). A general agnostic active learning algorithm. Neural Information Processing Systems .
 Freund, Y., Seung, H., Shamir, E., &amp; Tishby, N. (1997). Selective sampling using the query by com-mittee algorithm. Machine Learning , 28 , 133 X 168. Hanneke, S. (2007). A bound on the label complexity of agnostic active learning. ICML .
 Schohn, G., &amp; Cohn, D. (2000). Less is more: active learning with support vector machines. ICML . Schutze, H., Velipasaoglu, E., &amp; Pedersen, J. (2006).
Performance thresholding in practical text classifica-tion. ACM International Conference on Information
