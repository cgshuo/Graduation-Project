 With the development of Web 2.0, people have become interested in expressing their consumer product. In practice, people would like to focus on the summary of opinion expressions with their own preference to make decision [1]. For instance, users would like to give a query,  X  X hat are the opinions on X? X  to express their preference of X, summarization(TOS) to meet the user X  X  personal preference. 
In Example 1, there are three opinion expressions tagged in bold in sentence (a). If a query is given about a computer game , traditional summarization regards only topical relevance, such as  X  game  X ,  X  operation  X ,  X  screen  X  to be the information need. For TOS, however, it is supposed to take both topic-specific information and we define user X  X  information need of TOS as topic-specific opinionated information (TOI), i.e. the opinion expressions about the user X  X  query. 
One of the fundamental problems in TOS is how to effectively represent the user X  X  information need so as to evaluate and summarize salient opinion expressions. 
Previous methods using KL-divergence [2] or feedback-style learning [3] have the limitation that TOI is represented by one single word. In practice, one single word can hardly represent both topic-specific inform ation and opinionated information at the same time, especially for those domain-independent sentiment words that barely express topic-specific information and opi nionated information by topic-specific words and sentiment words, respectively. However, they regarded the document as bag-of-word , and neglected the contextual information, which means word-based representation cannot hold the associative information between topic-specific information and opinionated information in individual opinion expression and lead to a mismatch. In Example 1, although topi c-specific information and opinionated associative information between them is lost due to lack of contextual information. In the salient opinion expression. 
Li et al. proposed to adopt word pair to represent TOI [6]. A word pair is constructed by a sentiment word together with its corresponding topic-specific word. The sentiment word represents opinionated information, i.e. the opinion, and the corresponding topic-specific word represents topic-specific information, i.e. target (also refer to the topic-specific word in this paper). With the help of pairwise representation, the contextual information between the opinion and its corresponding associations between the sentiment word and the topic-specific word within different word  X  small  X  which associated with different topic-specific words ( X  game  X  in (a) and  X  screen  X  in (b)). Intuitively,  X  small  X  is a domain-independent sentiment word, and it should be dynamically assigned different weights when modifying different targets. pair to describe the distinct association. 
In this paper, we also follow the TOI representation by word pair. According to the above analysis of word pair, we first propose an effective weighting scheme from the perspective of information gain by selecting the word pair to measure both topic-specific words and sentiment words, and then provide an individual associative score measured. Finally, we integrate word pair into a random walk model for sentence ranking and adopt maximal marginal relevance (MMR) method to generate the topic-specific opinion summary. To investigate the effectiveness of our approach, experiments were made based on the TAC2008 and OpQA benchmark datasets. Significant improvements over the best run in TAC 2008 and those models with word-based representation were shown in this paper. 
In the remainder of this paper, we first present pairwise representation of topic-specific opinionated information together wi th a new weighting scheme in Section 2. We then integrate word pair into a random walk model for sentence ranking and generate opinion summary by using MMR method in Section 3. In Section 4, we will show the experimental results. We review the related work in Section 5. Finally, this paper is concluded and the future work is suggested in Section 6. Topic-specific opinion summarization (TOS) was first proposed in the Text Analysis Conference (TAC) 2008, and the objective is to extract an informative summary of opinion expressions about a given query, as found in a document collection [7]. contain the following attributes: opinion (i.e. opinionated information), holder, target (i.e. topic-specific information) and polarity [8]. In this section, we will first describe how to express TOI by pairwise representation, word pair. Then a new weighting scheme is introduced for measuring the pairwise representation. 2.1 Pairwise Representation aims at extracting an informative summary of  X  X  (  X  X  X  X  ) with opinion expressions from  X  about the  X  . expression together with the associations between these attributes. We also utilize topic-sentiment word pair [6] in this paper. The notion of word pair was first proposed for opinion retrieval to capture the contextual information between the opinion and its the blogosphere, we do not take opinion holder attribute into consideration. lexicon which is used to express opinions. We maintain the semantic information between the topic-specific word and the se ntiment word by pairwise representing. Thus, TOI of an opinion expression is represented by a word pair. We assume that for one query the candidate targets and opinions are in  X   X  , and  X   X  , respectively. The total number of the word pair is  X  X   X  X  X   X  X  X  X  X  X  , (  X |  X   X  X  X  , |  X   X  |  X  X  ) . 
In practice, the weight of a sentiment word may differ from variant targets. In the following section, we will describe a weighting scheme to measure word pair from the perspective of information gain, and introduce a method by normalizing Point-wise Mutual Information (PMI) between the sentiment word and the corresponding expression. 2.2 Weighting Scheme for Word Pair Previous weighting schema would like to capture both topic-specific information and [9], or integrates the sentiment weight into query word, e.g., computes the Point-wise Mutual Information (PMI) score between sentiment word and the target combined information and opinionated information of an opinion expression to be represented by topic-specific word and sentiment word of a word pair, respectively. Therefore, we measure both topic-specific words and sentim ent words by computing the information gain in selecting the word pair. 
In TOS, both sentiment words and topi c-specific words are considered as informative content words, and described as  X  term  X  (denoted by  X  ). Additionally, we would like to concentrate on the granularity of sentence rather than document. 
We can compute term weight of  X  ,  X  X  X  X  X  X  by Equation (1): Obviously  X   X   X  X   X   X  . 
We measure each term according to its distributions between the sentence set it occurs and the whole sentence set. In other words, we weigh sentiment word by computing the gain in selecting a sentence containing the sentiment word. 
Recall Example 1, one sentiment word (resp. target) may be assigned different weights when associated with different targets (resp. sentiment words). Therefore, there is a must to embody the different associations between sentiment words and topic-specific words. 
Previous works [4, 5, 6] focus on using a unified parameter to express variant combinations between topic-specific informa tion and opinionated information. It is inadequate to express the variant of associations even to one specific domain. We paper) for each individual association between the sentiment word and the topic-specific word. 
Inspired by the fact that Mutual Information is a measurement to assess how two mutual information to estimate the trade-off parameter for both sentiment words and the target words. the probabilities of observing  X   X  and  X   X  independently. The mutual information between words  X   X  and  X   X  are calculated as follows: absent. 
The parameters are estimated as follows: and  X   X  , respectively,  X  X  X   X   X   X  , X 1  X   X   X 1 X  is the number of sentences that contain both and  X   X  , and  X  is the total number of sentences in the collection. We then normalize weight of  X   X  when associated with  X   X  : more frequently. 
After estimating the associative score between the two elements of a word pair, we can assign the weight to individual word pair  X   X  X  X  . from Equation (1). 
As to those word pairs with a negation operator around, an alternative would be to the polarity of the word pair, we assign  X  X   X  X  X  with the same weight as  X   X  X  X  . sentence ranking. Then, we utilize MMR method to generate a summary. 3.1 PageRank Based on Word Pair In Section 2, we introduce a weighting scheme for measuring individual word pair. pair representing salient opinion expression should be assigned a relatively high weight. We, therefore, consider the global information of word pair for sentence ranking by the recursive procedure in the random walk model, PageRank. 
One of our objectives is to investigate the effectiveness of our proposed weighting scheme for word pair, so we compute the similarity between sentences according to the weighted word pair. Moreover, in our approach, we do not explicit divide sentiment words into domain-dependent and domain-independent, but use the independent sentiment words. In order to correct the opinionated information of a domain-independent sentiment of a word pair, we utilize synonym dictionary SentiWordNet [13]. We choose the sentiment word with the highest PMI score over a topic-specific word in Equation (2) as the cue word and consider all synonyms of the cue word together with the corresponding target to be the same word pair. 
We define a PageRank model that has sentences to be summarized as nodes and edges placed between two sentences that are similar to each other. 
We can then score all the sentences based on the expected probability of a random  X 1 X  . The jumping probability from node  X   X  to node  X   X  is given by: the word pair they contain. similar score and scores of all other sentences linked with it as follows: where  X   X  query, we choose a number of sentences with weights higher than a threshold as candidate set  X  for TOS. 3.2 Summary Generation In order to generate a summary, we adopt maximal marginal relevance (MMR) method and incrementally add the top ranked sentences from  X  into the answer set. relative importance given to  X  X elevance X  versus redundancy. As different users with can particularly generate summaries according to a user X  X  need. In the experiment, we set  X 0.5 X  to balance the novelty and the relevance. 4.1 Experiment Setting 4.1.1 Benchmark Datasets Our experiments are based on two benchmark datasets for topic-specific opinion summarization, TAC2008 and OpQA. 
TAC2008 dataset is the benchmark data set for the topic-specific opinion summarization track in the Text Analysis Conference 2008 (TAC2008), which also provided. Summarizations for all queries must be retrieved from the TREC Blog06 collection [15], which consists of review and blog data. The top 50 documents were retrieved for each query. 
The Opinion Question Answering (OpQA) corpus consists of 98 documents appeared in the world press between June 2001 and May 2002. The documents covered four general topics, and 30 questions were given. [16] 4.1.2 Sentimental Lexicon and Topic Collection In our experiment, we use SentiWordNet as the sentiment lexicon. SentiWordNet is a sentiment words and 2290 positive sentiment words. For each sentiment word, SentiWordNet also provides its synonyms. 
In order to acquire the collection of topic terms, we adopt two expansion methods, dictionary-based method and pseudo relevance feedback method [6]. 4.1.3 TOS Approaches for Comparison To demonstrate the effectiveness of pairwise representation for TOS, we compared it with the following models: (1) Baseline 1: This model [18] achieved the best run in TAC2008 opinion summarization task. We treated it as Baseline 1 in the experiment . (2) OPM-1: This model was proposed for opinion question and answering, which achieved 2% improvement over the best run in TAC2008 Opinion QA track [19]. (3) OPM-2: This model was similar with OPM-1, but use PageRank model for sentence ranking instead. (4) GOSM: This model was originally designed for opinion retrieval, and it adopted pairwise representation of TOI. GOSM adopted  X  relevance  X  measurement for sentiment word and utilized a uniform parameter to balance topic-specific information and opinionated information. We re-designed GOSM to deal with TOS by using Pair-based HITS model so that we could compare the effectiveness of different weighting schema for word pair [6]. (5) PPM: our proposed approaches. Additionally, in our experiments, we will also investigate the performance of sentence retrieval with different probability models. We used the metrics in the Text Retrieval Conference (TREC), which are average precision (AvPr), R-precision (R-Pre) and precision at 10 sentences (P@10). 4.2 Performance Evaluation 4.2.1 Parameter Tuning parameter according to the specific need. In our experiment, we set the parameter  X 0.5 X  to balance the novelty and the relevance. 
We studied how the parameter  X  (in Equation (6)) influenced the performance of sentence ranking in both TAC2008 and OpQA datasets. The results are given in Fig. 1. Best F value was achieved, when  X  was set around 0.8 in both TAC2008 and OpQA datasets. Therefore, in the following experiments, we set  X  X 0.8 . 4.2.2 Comparisons on Sentence Ranking In our evaluation, we also tested the performance of sentence ranking of other probability models, including tf-idf model and Bose-Einstein model. We used the metrics in the Text Retrieval Conference 10 (TREC), which are average precision experiment, we created the judgment through pooling method. The experimental results based on these metrics are shown in Table 1. 
Table 1 showed that Bose-Einstein model achieved best R-Prec and P@10 on OpQA and TAC2008 datasets. Thus, we chose Bose-Einstein model for further evaluation. 4.2.3 Comparisons on TOS We were also interested in the performance comparison with the other models for TOS. 
Table 2 showed that PPM achieved around 6% and 5% improvement in F value compared with Baseline 1 in OpQA and TAC2008, respectively. Research on opinion summarization started mostly on review-type data, and much progress has been made in automatic sentiment summarization in the review domain [20]. These summarizations referred to as feature-based summarization or aspect summarizations are extracted from a collec tion of reviews on some specific product. Benefited from the limited topics and fixed sentiment words in specific domain, technologies such as LDA, LSA, pLSA, have been utilized and they achieved good performance in product review [21, 22, 23, 24]. In this paper, we focus on TOS, which is about general domain summarization, and the above works are out of the scope of TOS due to the constraints of limited targets and fixed sentiment words. 
For TOS, lots of work concentrates on term weighting to improve the precision of sentence ranking. A weighted sentiment dictionary was generated from previous Text Retrieval Conference (TREC) relevance data [11]. This dictionary was submitted as a retrieved documents. Similarly, a pseudo opinionated word composed of all opinion [3]. This method was shown to be very effective in TREC evaluations. the weights of the terms in the sentiment word dictionary were biased towards the terms with high values. Experimental results showed that this method had detrimental effect on the performance. [9] followed the KL divergence measurement and made a positive experimental result by taking term frequency into consideration. corresponding target could be uniformly represented. However, [6] didn X  X  give an and document instead, which is in accordance with  X  relevance  X . work, we present a weighting scheme, which regards both topic-specific words and sentiment words as informative content to represent topic-specific information and opinionated information. Moreover, regarding the specialty of TOS, we propose a method to estimate individual associative score for each word pair to measure the association of topic-specific information and opinionated information, and take negation into consideration and integrate it into word pair for TOS. by the representation of word pair. Based on word pair, we further propose a weighting scheme so that both topic-specifi c words and sentiment words are weighed. We also provide a method by normalizing PMI between sentiment word and topic-well expressed and measured. We integrate word pair into the PageRank model for sentence ranking and adopt maximal marginal relevance method to extract salient sentences as the result of TOS. (1) Deeper NLP techniques e.g., discourse analysis [26], dependency parser, (2) Opinion holder is another important attribute of TOI [27]. It would be (3) Since the new weighting scheme and the trade-off parameter indicate topic-Acknowledgments. This work is partially supported by National 863 program of China (Grant No. 2009AA01Z150), the Innovation and Technology Fund of Hong Kong SAR (Project No. GHP/036/09SZ) and 2010/11 CUHK Direct Grants (Project No. EE09743). 
