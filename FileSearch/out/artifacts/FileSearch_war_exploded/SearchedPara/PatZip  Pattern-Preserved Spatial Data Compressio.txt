 structure of spatial data. A common approach to such challenges is to perform some [7]. General requirements for a quality compression include:  X 
Minimal information loss. Information needed by the mining methods should be preserved in the compressed data while the size of the data is significantly reduced. 
To minimize the information loss, there should be a mapping between the when necessary.  X 
Efficient. For data mining purposes, a compression becomes worthwhile only if it can make the mining process more efficient. In most cases, compression methods cannot be slower than mining methods. Otherwise, the efficiency of the whole process is decreased.  X  general compression method should not make assumptions on data features but let the data speak for themselves. 
This paper addresses spatial data compression problem from the above requirements. PatZip extends the idea of GraphZip [14] that merges closest data iteratively based on a nearest neighbor graph by adding an automatic termination merging grows out of proportion. While most existing compression methods require user-specified parameters on the size of compressed data, PatZip can detect when compression should stop so that overcomp ression or undercompression caused by wrong inputs can be avoided. The termination detection is based on a comparison between predicted distortion and actual distortion as the compression continues. Since patterns, if the actual distortion is greater than the predicted value, some patterns have been wrongly merged together and compression stops before such a merging happens. This paper studies and discovers the properties of PatZip with comprehensive experimental studies. The discovered properties imply that PatZip can be used to compress source data for pattern mining. PatZip has been applied to assist FA X ADE [13], a clustering method, to discover spatial patterns effectively. 
The rest of this paper is organized as follows. Section 2 reviews related work. The process of PatZip is described in Section 3. Section 4 studies the properties of PatZip through experiments. Section 5 concludes the paper. and sampling [4]. The representative summary construction methods include micro-clustering and vector quantization. K -means [11] and Birch [16] are two representative micro-clustering methods while LBG algorithm [10] and PNN algorithm [5] are two classic vector quantization methods. Sampling approaches include random sampling versions [1, 6, 9, 12] of these methods proposed in recent years. 2.1 Sampling Random sampling may be the most popular size reduction method used in data sampling has advantages on efficiency and generality. Its disadvantages, however, are apparent: the inaccuracy introduced by sampling variance, the missing of small data, so the original data are usually recovered with a nearest neighbor classification, which is computational expensive when the sample is big. 
Palmer and Faloutsos [12] propose a density-biased sampling method to solve the small-cluster-missing problem caused by random/uniform sampling. The heart of their density-biased sampling is more accurate for cluster detection than uniform sampling, guided process when lacking knowledge about the given data. 2.2 Micro-clustering and Birch [16]. The disadvantages of k -means include: low efficiency when k is big, propose an approach to perform k -means clustering more effectively by finding good still not efficient when k is big. 
BIRCH [16] is an efficient clustering algorithm for large databases, which is sensitive to data input order. Several concepts like radius, diameter, and centroid are used in Birch to describe the distance proper ties of a cluster, which leads to inaccurate compression for non-sphere regions. GraphZip [14] is a recently published graph-based micro-clustering method. PatZip and GraphZip use a similar way generating representative points of the original data. PatZip, however, can detect the termination that does not require user input on size of compressed data. 2.3 Vector Quantization of compression. 
The most popular code book generation method is the LBG algorithm [10]. Its idea on representative point generation is exactly the same as that of k -means while their usages are different. While k -means is for unsupervised learning, LBG is grouping will remain unchanged. This is also the main difference between micro-clustering and vector quantization methods. PNN (Pairwise Nearest Neighbor) quantization. In each step of PNN, two closest vectors are merged and the process is repeated until the desired size of the codebook is reached. To minimize the average distortion, PNN requires O(n 3 ) time for n data points to complete the generation of the codebook [6]. 
PatZip is distinguished from the aforementioned approaches for several desirable properties. First of all, PatZip is data-driven and completely automatic. Both summary patterns, which not only increases burden on users but also causes overcompression or undercompression. Summary construction methods usually contain additional parameters for the models they use, which are also hard to set without prior knowledge. Secondly, PatZip is fast. Its requires only O(nlogn) time for n data points. recovered from compressed data. original data set with a 1 -neareset neighbor graph and takes the graph as the input of substituting the points produced in the previous running cycle. Each running cycle is contribution of this paper. 3.1 Compression Before presenting the process of PatZip, let us first introduce the concept of k -nearest neighbor graph briefly. Generally, each vertex of a k -nearest neighborhood graph represents a data item. For each pair of data items, if either of them is among the k -most similar data items of the other, there exists an edge between the two corresponding vertices. In a spatial database, the data points is usually measured by the Euclidean distance between them. 
As shown in Fig. 3.1, PatZip is an iterative process, which accepts the output of the 
The time complexity of PatZip can be decided in a similar way to that of GraphZip connected components. Constructing a 1 -nearest neighbor graph can be considered as O(nlogn) time algorithm for the all-nearest-nei ghbors problem for an arbitrarily fixed dimension d was given by Clarkson [3], using randomization. Vaidya [15] solves the problem deterministically, in O(nlogn) time. Vaidya X  X  algorithm can be implemented in the algebraic computation tree model and is, therefore, optimal. To find the O(nlogn)+O(n+n)=O(nlogn) . Now let us analyze how many iterations PatZip needs to run before reaching the final representative point. Theorem 3.1. PatZip requires O(log(n)) iterations to compress the size of a given data set from n to 1 . Proof. Assume that after the first iteration, there are X 1 connected components in the 1 -nearest graph of the given data set. According to the definition of k -nearest neighbor graph, every node has at least k edges connected. Since k  X  1 , there is no isolated node variable t leads to t  X  log(n) . 
Theorem 3.1 indicates that the total time complexity of PatZip is the sum of all of the log(n) iteration steps: =O((n+n/2+ ... +1)log(n)) =O(2nlogn)=O(nlogn) . 
The algorithm depicted in Fig. 3.1 can be easily extended to accept a user input to specify the termination condition when necessary. The condition could be a lower bound of the size of the compressed data or a maximum allowable compression ratio. can terminate under user X  X  control. The better way, however, is to let PatZip terminate automatically through estimating the compression distortion, as described in the following section. 3.2 Termination Condition merged until the size of the data reaches 1 , i.e., the final representative point. In most pattern mining applications, however, it is desirable to stop before reaching the final clustering methods should be applied to a set of compressed data that still contains the condition of a compression before losing spatial patterns. square sum used by many previous approaches [5, 6, 16]: let G denote a set of n data radius r of G : We compute a distortion value at every compression stage and check the value increment. If the value increases out of proportion, then we think one or more spatial patterns have been destroyed, compression should stop. To justify this method, let us patterns are destroyed in continuous compressions. According to the definition, representative point that represents the data points from different clusters would have point is in the same cluster and merging happens only inside the cluster. As cluster if the compression continues. Second, due to various shapes of clusters, as the compression continues, a point representing part of cluster A may be nearer to a point two sets of points are merged together and distortion value does not increase sharply, the two sets must belong to the same cluster, or neither of them represents a affecting the true patterns. In summary, a compression stage whose distortion values increase disproportionally is considered a termination stage before which compression should stop. threshold: predicting the distortion value of each compression stage with the reducing number of data points at the stage. The predicted value is computed by assuming that total area is fixed. Then we have A points, i.e.: where  X  is the half of the average interval between two neighbor points of the original data set, as shown in Fig. 3.2. Based on Formulas (2) and (3), we obtain the approximate definition of predicted distortion as follows. Definition 3.1. (Predicted Distortion) The predicted distortion at compression stage A i is: compression stage and  X  is the radius of the area of this stage. Finally, we have: Definition 3.2. (Termination Stage, Termination Condition) termination condition satisfies:  X  i&lt;t , D i  X  P i and D t &gt; P t . D will be mainly affected by inter-cluster distances, which are typically much larger than the average, i.e., P t . 
In summary, compression will terminate before the stage whose actual distortion is bigger than the predicted value. The self-terminated version of PatZip is described in Fig. 3.3. The effect of the approximation will be evaluated in Section 4. This section will investigate the properties of PatZip. Section 4.1 will demonstrate the compression results while Section 4.2 will evaluate the termination scheme. 4.1 Experimental Results 100,000 points. DS2 is a benchmark used by CHAMELEON [8], which contains clusters of different shapes and 10,000 data points. DS3 is our synthetic data set that densities and 1 million data points. 
The compressed results for the 3 data sets after applying PatZip are shown in Fig. 4.1 (b). To make the visual examination easier, the results shown in Fig. 4.1(b) should not contain too few points, so we stop PatZip at compression ratio of about 50 before different spatial pattern mining algorithms for improving efficiency. 4.2 Accurate Termination This section will evaluate the termination condition described in Section 3.2. Fig. 4.2 compares the predicted distortion and the actual distortion for DS1~3 at different compression stages. It clearly shows the termination stage when the actual distortion exceeds the predicted distortion. Table 4.1 lists the sizes of the remaining data when compression stops and the actual number of clusters in the corresponding data sets. We expect each compressed data set to have a size close to but not smaller than the actual number of pattern mining methods. If the approximation is correct, compression will stop at the stage before patterns are mixed up. As mentioned in Section 3.2, two conditions can cause compression stop: first, a whole cluster has been merged into only one point. If the compression continues, this cluster has to be merged with others according to the merged into 26 points, i.e., 74 clusters will be merged into others. This would cause a sharp increase in distortion and compression will stop. The second possibility is: even representative points of different clusters could still be merged together due to various but the compression cannot proceed. This is because for non-spherical clusters, points points and should stop. Generally, the closeness between the actual number of clusters cases, the size of the remaining data, i.e., the approximated number of clusters, would be closer to the actual number of clusters. This paper presents a simple but effective data compression method, called PatZip, to any prior knowledge about the data. The compression can terminate automatically of pattern discovery methods. In the future we will investigate the impact of different data on prediction of termination stages and how to optimize PatZip for more pattern mining algorithms. 
