 It is common to develop and validate classi fi ers through a process of repeated testing, with nested training and/or test sets of increas-ing size. We demonstrate in this paper that such repeated testing leads to biased estimates of classi fi er effectiveness. Experiments on a range of text classi fi cation tasks under three sequential testing frameworks show all three lead to optimistic estimates of effective-ness. We calculate empirical adjustments to unbias estimates on our data set, and identify directions for research that could lead to general techniques for avoiding bias while reducing labeling costs. H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation .
 Performance Supervised learning, text categorization, evaluation, F-measure
Classi fi ers are frequently produced by supervised learning, with the classi fi er trained on one set of annotated examples, and then tested on another set selected by random sampling. When this test-ing is done a single time, textbook techniques of point estimation, con fi dence interval estimation, and hypothesis testing on effective-ness are directly applicable.

In many practical circumstances, however, effectiveness is tested multiple times. The developer may repeatedly add examples to the training data, retrain classi fi ers, and test until estimated effec-tiveness reaches a target level. Or the classi fi er may already be constructed, and the developer wishes to validate its effectiveness while minimizing the number of test examples that need to be la-beled. Or the developer may grow both training and tests sets at the same time, seeking both to improve and to certify effectiveness.
Unfortunately, as we observe in this paper, repeated testing of a classi fi er X  X  effectiveness introduces statistical bias into estimates. Statistical inference is based on the recognition that any particu-lar measurement has an element of randomness to it. We may by chance draw a sample that gives an optimistic estimate, leading us to put into use a classi fi er less good than it appears. Conversely, we may reject a classi fi er incorrectly based on a sample that gives too pessimistic an estimate. But by drawing a sample randomly we can quantify these risks, and reduce them through larger samples.
When we test repeatedly, however, the two dangers are not sym-metric. An optimistic estimate will lead us to put a classi fi er into use and congratulate ourselves on having annotated little data. A pessimistic result will lead us to do more training, tuning and/or testing. We are therefore more likely to accept overestimates than underestimates in sequential testing, leading to an optimistic bias.
In this paper, we explore the behavior of sequential testing for the three scenarios outlined above ( fi xed test and variable training; fi xed training and variable test; and both test and training variable). Two estimates of effectiveness are considered: a point estimate of the F 1 score, and the lower bound of a one-sided 95% con fi dence interval on F 1 . We empirically quantify the bias introduced by sequential testing across a range of text classi fi cation tasks, provide guidance for practitioners, and point to a range of open questions.
We use SVM perf (V. 3.0) with a linear kernel [Joachims, 2006] to train text classi fi ers optimized for F 1 ,using fl ags  X -c 1000 -l 1 -w 3 X . Our data set is the RCV1-v2 collection of Reuters newswire stories [Lewis et al., 2004]. Feature vectors were constructed using the RCV1-v2 stopped, stemmed token fi les (On-Line Appendix 12 of Lewis et al. [2004]). Feature values were log TF x IDF weights, with IDF equal to the natural log of the number of documents in the collection divided by the number of documents the word occurs in.
We de fi ned29binaryclassi fi cation tasks corresponding to the 29 Topics categories with at least 25 , 000 positive examples in the 804 , 414 -document collection. The use of high frequency cate-gories enabled studying several orders of magnitude of variation in sample sizes. Our effectiveness measure is F 1 ;thatis,thehar-monic mean of precision and recall. We de fi ne F 1 to equal 1 . 0 if a classi fi er makes no positive predictions on a data set with no positive examples [Lewis, 1995].

Our experiments compare F 1 estimates with true population F We derive a highly accurate estimate of the latter from a randomly sampled  X  X ruth set X  of 700 , 000 of the RCV1-v2 documents. The mean width of the two-sided con fi dence interval on that estimate is 0 . 0059  X  X inuscule compared to the variations we see in estimates from our smaller experimental test sets.
The test and training sets in our experiments are drawn from these 700 , 000 documents and from the remaining 104 , 414 doc-uments respectively. In each run, the training and/or test set was grown in a nested fashion, by adding 20 randomly selected anno-tated documents, retraining the classi fi er on the training set if nec-essary, and estimating F 1 based on the test set. The 20 documents were either all added to test (Section 3.1), all added to training (Sec-tion 3.2), or half added to training and half to test (Section 3.3). We compute two estimates of F 1 from each test set. First, the F 1 score on the test set is taken as a point estimate, F . Second, we fi nd the lower bound,  X  , of a lower one-sided 95% con fi dence interval on F 1 . We compute this con fi dence interval estimate by taking the 5th percentile of 40 , 000 Monte Carlo draws on beta-binomial posteriors on the classi fi er X  X  true positive and false positive rates, using the method described by Webber [2013] (see Goutte and Gaussier [2005] for a similar binomial posterior on F (Similar sequential biases would arise for other interval methods, or other effectiveness measures such as precision or recall.)
Webber [2013] shows that this Monte Carlo procedure gives cov-erage probabilities very close to the nominal 95% level for two-tailed intervals on recall, in contrast to commonly used normal ap-proximations. Nevertheless, we checked the accuracy of the method for one-tailed intervals on F 1 . We calculated an observed coverage ; namely, the proportion of testing points at which the lower bound of the con fi dence interval is at or below the true F 1 score. An edge case occurs when the classi fi er achieves no true positives on the test set, either because the classi fi er is poor or the test set has no positive examples. In this case, the lower bound of the con fi dence interval is 0 . 0 , guaranteeing 100% coverage. To avoid crediting the algorithm for this trivial case, we computed coverage only on cases with at least 100 training and 100 test examples. With this restric-tion, we fi nd that the observed coverage of Webber X  X  algorithm is very near 95% in all experiments.

As will be seen, however, applying sequential stopping rules to nominal 95% con fi dence intervals leads to stopping conditions with actual coverage lower than 95% in all three scenarios studied. One way to adjust for this might be to compute the con fi dence interval at a nominal level higher than 95%, but assert only a 95% level for the resulting interval. For each protocol, we calculate what the nominal level must be to achieve 95% coverage with our data set.
We present results for three conditions: variable test, variable training, and variable both test and training.
First, we consider the case where a classi fi er has been produced using a fi xed training set. A developer wishes to certify, at a given con fi dence level, that the classi fi er exceeds a target value of effec-tiveness. We assume that a testing budget of 10 , 000 annotations is available, but that the developer would prefer to pay for fewer annotations if possible. This is the standard setting in sequential sampling theory [Wetherill and Glazebrook, 1986]. We assume that they iteratively annotate randomly selected examples, add them to the test data, and compute the two estimates of F 1 (point estimate and lower bound of a one-sided lower 95% con fi dence interval). The developer decides to accept the classi fi er (and stop testing) if at some point the lower con fi dence limit exceeds the target value of F . They reason (erroneously) that they will accept an inadequate classi fi er at most 5% of the time. They reject the classi fi er if the budget of 10 , 000 annotations is used up without accepting. Figure 1 shows the results of one such classi fi er evaluation run. The target value of F 1 is 0 . 5 , while the true effectiveness (unbe-
F Figure 1: Variable test, fi xed training. The dashed line is true effectiveness ( F 1 ); the dotted line is the target F 1 ; the solid line is estimated F 1 , for increasing test set sizes; and the bold line is the lower bound of the 95% one-sided con fi dence interval on F . Results are shown for a single run for Topic C31. knownst to the developer) is 0 . 42 . The point estimate  X  to almost always be above its true value, and for many test set sizes above the substantially higher target value. The lower bound of the con fi dence interval is more conservative, but wanders above the true value at several points, and above the developer X  X  target for several test sets in the range of 100 to 200 documents. The devel-oper would have mistakenly halted testing and accepted the classi-fi er upon reaching the test set of size 120 . For large test sets, both the point estimate and the lower bound of the con fi dence interval approach the true value of effectiveness, but our developer would never reach these test set sizes. Figure 1 is only for a single run, and perhaps (from a reliable evaluation perspective) an unlucky one. A 95% con fi dence interval is expected to fail roughly 5% of the time. We therefore performed the same experiment across all 29 topics, and made 20 runs for each topic. In each case the target effec-tiveness was set in fi nitesimally above the true effectiveness of the classi fi er, so that the classi fi er should be rejected in every run; not doing so counts as a failed evaluation. Unsurprisingly, the point es-timate exceeded the target value at some point on almost every run. So using the point estimate would fail almost 100% of the time. But even the lower bound of the con fi dence interval exceeded the target value 31 . 55% of the time (far more than the nominal 5% the developer might expect), causing the classi fi er to be erroneously accepted. The observed coverage of the con fi dence intervals was 95 . 68% , making clear that the problem is the developer X  X  sequen-tial stopping rule, not the (single-test) con fi dence interval estimate.
While the bias introduced by over fi tting has long been a sub-ject of study in machine learning [Toussaint, 1974], the bias intro-duced by sequential testing appears to have been ignored. Sequen-tial stopping rules, and techniques for unbiasing estimates when us-ing them, have been studied in statistics and quality control [Sieg-mund, 1985, Wetherill and Glazebrook, 1986]. The theory is well developed (if not simple) for cases such as the binomial propor-tion and the normal mean. Unfortunately, F 1 , as the ratio of two unknown quantities, is substantially more dif fi cult to address ana-lytically. We can, however, observe an empirical adjustment on our dataset. To achieve actual 95% con fi dence, assuming that lower-bound effectiveness is tested every 20 documents, we should set a nominal con fi dence level of 99.5%.
F Figure 2: Fixed test, variable training. True classi fi er effective-ness (measured by F 1 ) as the training set is increased is shown as a dashed green line; the estimated effectiveness, based on a test set of 1 , 067 , is the solid blue line; and the 95% lower-bound con fi dence on that effectiveness is the bold gold line.
Next we examine a common use of sequential testing in classi fi er construction. The developer of the classi fi er has a fi xed, randomly selected test set. Training examples are selected, annotated, and used to improve the classi fi er. At regular intervals, the classi fi er is tested on the test set. We assume that training and testing continue until the estimated effectiveness of the classi fi er reaches some tar-get value, or when a total of 15 , 000 documents have been labeled. We fi x the test set size at 1 , 067 documents (a commonly used value that gives a maximum two-sided con fi dence interval width of  X  3% on a binomial proportion). This corresponds to training budgets be-tween 13 and 13 , 933 , with increments of 20 documents. However, the choice of a particular test set size is not critical to our results.
Figure 2 shows an example of this scenario. The developer has set a target value of 0 . 7 on F 1 . Thinking themselves conservative, they decide to stop training when the lower con fi dence limit on F exceeds 0 . 7 . They (erroneously) reason they will accept an inad-equate classi fi er at most 5% of the time they use this procedure. They reject the classi fi er if the combined budget of 15 , 000 annota-tions is used up without accepting.

The true effectiveness of the classi fi er rises fairly smoothly with the addition of training data, as we would expect. The two estimates of effectiveness are, however, far more erratic. Both the point esti-mate (at 333 training examples) and, surprisingly, the lower bound of the con fi dence interval (at 473 examples) exceed the target value well before the true effectiveness does (at 660 examples).
The erratic behavior evinced in Figure 2 seems at fi rst surprising given a fi xed test set and steady growth in true effectiveness. How-ever, while the true effectiveness of successive classi fi ers increases fairly steadily as the training set grows in size, their behavior on a particular test set of modest size can fl uctuate greatly from classi fi er to classi fi er. It is not the case that training improvements smoothly and monotonically convert test set errors into test set successes. Thus, even though no additional sampling is being performed, ran-dom fl uctuations in estimated effectiveness occur, and we are more likelytostopwhenthose fl uctuations lead to overstating effective-ness than when they lead to understating it.

Does Figure 2 simply show an unlucky case? We again ran 29 topics and 20 runs per topic. In each run, we set the target value for effectiveness to be 90% of the highest true effectiveness achieved (usually at the largest training set size) during the run. The effec-tiveness of the classi fi er was estimated after each addition of 20 annotated examples to the training set. We examined two stop-ping rules: stopping when the point estimate fi rst hits this target, and stopping when the lower con fi dence interval bound fi rst hits this target. For unbiased point estimates, we might expect the fi rst rule to stop early 50% of the time (the point estimate is as likely to be above as below the true effectiveness), and the latter rule to stop early only 5% of the time. In practice, across 29 topics and 20 runs per topic, we found that the point-estimate rule stopped early 53.58% of the time. The lower-bound stopping rule stops early 8.13% of the time. The latter of these values is signi fi cantly different from the expected proportion ( p&lt; 0 . 01 under an exact bi-nomial test with 29  X  20 = 580 observations); the former is not sig-ni fi cantly different, but the jaggedness of the estimate curve means that there inevitably is an optimistic bias, though our number of cat-egories and runs was too small to demonstrate this at the p =0 . 05 level. The observed con fi dence interval coverage for this scenario is 95.26%, so again it is the stopping rules that are the problem.
This bias is smaller than in Section 3.1, but also dif fi cult to at-tack analytically. The theory of sequential testing has focused on estimating fi xed distributions, not time varying ones. Computa-tional learning theory has found the analysis of learning curves challenging even in the generalization framework (corresponding to our comparatively smooth curve of true effectiveness) [Haussler et al., 1996] X  X e do not know of a treatment of learning curves for fi nite test sets. Lacking analytical guidance, we observe that a nominal con fi dence level of 97% is required to achieve an actual con fi dence of 95%.
Finally, we consider the situation in which both test and training set are increased over time. Though not a widely used approach, it has the advantage of avoiding committing to too small or too large a testing or training set size at the outset. Many adaptive policies are possible; we consider the simple one of allocating as many doc-uments to testing as we do to training (for our experiments, 10 to training and 10 to test at each increment, then performing the test).
Figure 3 shows what happens for a single run on a single topic under this regime. We assume the developer has set a target value of 0 . 6 on F 1 , and decides to stop training and testing when the lower con fi dence limit exceeds this target. They reject the classi fi er if the combined budget of 15 , 000 annotations is used up fi rst.
Again, the increase in true effectiveness with increasing train-ing set sizes is relatively smooth. And again, the point estimate on effectiveness is more erratic, here from a combination of both changing classi fi ers (as the training set is increased) and the chang-ing content of the test set. The lower bound once more varies with the point estimate, shrinking the gap gradually, since half of the annotations go to testing. Both the point estimate of F 1 lower bound of the con fi dence interval hit the target of 0.6 before true effectiveness reaches it: the developer would stop too early.
We once more estimate the degree of bias by observing behavior across all 29 topics and 20 runs. Setting the target effectiveness at 90% of maximum actual effectiveness, we fi nd that the point esti-mate crosses this threshold prematurely 68.38% of the time, while the lower bound crosses it prematurely 9.40% of the time. The observed coverage for this scenario is 95.39%, showing again that stopping rules are at fault. Here, a nominal con fi dence level of 98% is required to achieve an actual con fi dence of 95%.
F Figure 3: Variable test, variable training. The dashed green line gives true effectiveness ( F 1 ) of the classi fi er at each train-ing set size. The solid blue line gives a point estimate on effec-tiveness of a test set as large as the training set at each point. The bold gold line is the lower bound of a 95% lower one-sided con fi dence interval estimate.
To generalize our observations on the nominal con fi dence of 95%, we tried a range of con fi dence levels between 50% and 99%. Figure 4 shows the actual con fi dence of the three scenarios studied in this paper. It is, in all cases, lower than the nominal con fi dence (as shown by the dashed line), and most markedly so for the vari-able test, fi xed training case. These curves can serve as an indicator to correct the bias resulting from sequential sampling, though it is unclear how well they generalize to other conditions.
We have examined the bias involved in three sequential testing methods: fi xed training and variable test; fi xed test and variable training; and both test and training variable. The bias in fi xed train-ing and variable test is severe: a nominal 95% con fi dence interval provides only 68% coverage in practice, for the scenario considered here. Variable training and fi xed test also introduces a signi fi cant bias to estimates, with a nominal 95% giving 92% coverage. The combination of variable training and variable test has only slightly greater bias than variable training with fi xed test; a nominal cover-age interval of 95% gives actual coverage of 91%. These are not due to problems in the Monte Carlo method of calculating inter-vals on F 1 , which gives coverage very close to nominal levels for all three conditions. The bias lies solely in the use of sequential testing.

For the case of fi xed training (and fi xed classi fi ers in general) and variable test a sequential sampling analysis of the F-measure would be of great practical value, enabling valid estimates while saving labeling costs. Both traditional analytical approaches (e.g. Brownian motion approximations [Siegmund, 1985]) and simula-tion techniques are worth considering. For the cases in which the training set varies, estimation must track the moving target of the learning curve. Analytical solutions are likely impossible, and even simulation techniques will require confronting issues of classi fi er stability [Kearns and Ron, 1999].

Actual con fi dence level (%) Figure 4: Actual con fi dence corresponding to nominal con fi -dence level.

In this paper, we have provided empirical adjustments to con-fi dence limits, but these apply only to the particular scenarios and dataset explored here. For other cases, they may provide some rule-of-thumb guidance for developers, but they cannot stand in place of a statistically well grounded validation. For now, a trustworthy val-idation requires testing the fi nal classi fi er once on a set of annotated instances that is used for no other purpose.

