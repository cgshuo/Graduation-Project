 abdeslam.boularias@tuebingen.mpg.de Modern robots are designed to perform complicated planning and control tasks, such as manipulat-ing objects, navigating in outdoor environments, and driving in urban settings. Unfortunately, man-ually programming these tasks is almost infeasible in practice due to their high number of states. Markov Decision Processes (MDPs) provide an efficient tool for handling such tasks with a little help from an expert. The expert X  X  help consists in simply specifying a reward function. However, in demonstrate examples of a desired behavior than to define a reward function (Ng &amp; Russell, 2000). Learning policies from demonstration, a.k.a. apprenticeship learning, is a technique that has been widely used in robotics. An efficient approach to apprenticeship learning, known as Inverse Re-inforcement Learning (IRL) (Ng &amp; Russell, 2000; Abbeel &amp; Ng, 2004), consists in recovering a reward function under which the policy demonstrated by an expert is near-optimal, rather than di-rectly mimicking the expert X  X  actions. The learned reward is then used for finding an optimal policy. Consequently, the expert X  X  actions can be predicted in states that have not been encountered during the demonstration. Unfortunately, as already pointed by Abbeel &amp; Ng (2004), recovering a reward of reward functions. Most of the work on apprenticeship learning via IRL focused on solving this 2006; Ramachandran &amp; Amir, 2007; Syed &amp; Schapire, 2008; Syed et al., 2008).
 In this paper, we focus on another important problem occurring in IRL. IRL-based algorithms rely on the assumption that the reward function is a linear combination of state-action features. Therefore, the value function of any policy is a linear combination of the expected discounted frequency (count) the demonstration (the trajectories). In practice, this method works efficiently only if the number deterministic. For the tasks related to systems with a stochastic dynamics and a limited number of available examples, we propose an alternative method for approximating the expected frequencies partially demonstrated policy is near-optimal, and generalizes the expert X  X  policy beyond the states that appeared in the demonstration. We show that this technique can be efficiently used to improve the performance of two known IRL algorithms, namely Maximum Margin Planning (MMP) (Ratliff et al., 2006), and Linear Programming Apprenticeship Learning (LPAL) (Syed et al., 2008). is a set of states, A is a set of actions, T a is a transition matrix defined as  X  s,s 0  X  S ,a  X  A : T factor. We denote by MDP \ R a Markov Decision Process without a reward function, i.e. a tuple The value V (  X  ) of a policy  X  is the expected sum of rewards that will be received if policy  X  will constraints, are necessary and sufficient for defining an occupancy measure of a policy: {  X   X  ( s ) =  X  ( s ) +  X  X A policy  X  is well-defined by its occupancy measure  X   X  , one can interchangeably use  X  and  X   X  to denote a policy. The set of feasible occupancy measures is denoted by G . The frequency of a the value of a policy is completely determined by the frequencies (or counts) of the features f i . 3.1 Overview to recover the expert X  X  complete policy. However, the problem of learning a reward function given an all constant functions for instance, may lead to the same optimal policy. To overcome this problem, is calculated by using the worst-case reward function. This property is derived from the fact that when the frequencies of the features under two policies match, the cumulative rewards of the two subsections, we briefly describe two algorithms for apprenticeship learning via IRL. The first one, known as Maximum Margin Planning (MMP) (Ratliff et al., 2006), is a robust algorithm based on learning a reward function under which the expert X  X  demonstrated actions are optimal. The second one, known as Linear Programming Apprenticeship Learning (LPAL) Syed et al. (2008), is a fast algorithm that directly returns a policy with a bounded loss in the value. 3.2 Maximum Margin Planning Maximum Margin Planning (MMP) returns a vector of reward weights w , such that the value of the This criterion is explicitly specified in the cost function minimized by the algorithm: is given for the actions that are different from those of the expert. This algorithm minimizes the difference between the value divergence w T F X   X  E  X  w T F X  and the policy divergence l X  . minimized by using a subgradient method. For a given reward w , a subgradient g q w is given by: where  X  + = arg max  X   X  X  ( w T F + l )  X  , and  X  w  X   X  E =  X  +  X   X   X  E . 3.3 Linear Programming Apprenticeship Learning Linear Programming Apprenticeship Learning (LPAL) is based on the following observation: if the margin is found by solving the following linear program: The last three constraints in this linear program correspond to the Bellman-flow constraints (Equa-tion (1)) defining G , the feasible set of  X   X  . The learned policy  X  is given by: 3.4 Approximating feature frequencies Notice that both MMP and LPAL require the knowledge of the frequencies v i, X  These frequencies can be analytically calculated (using Bellman-flow constraints) only if  X  E is com-the frequencies v i, X  E are estimated as: There are nevertheless many problems related to this approximation. First, the estimated frequencies  X  v i, X  E can be very different from the true ones when the demonstration trajectories are scarce. Sec-In practice, these two values are too different and cannot be compared as done in these cost func-the empirical estimation of v i, X  E does not take advantage of the known transition probabilities. To show the effect of the error in the estimated feature frequencies on the quality of the learned rewards, we present an analysis of the distance be-tween the vector of reward weights  X  w returned by MMP with estimated frequencies  X  v  X  E = F  X   X   X  E , calculated from the examples by using Equa-tion (5), and the vector w E returned by MMP with accurate frequencies v  X  E = F X   X  E , calculated by using Equations (1) with the full policy  X  E .
 We adopt the following notations:  X  v  X  =  X  v  X  E  X  v  X  E ,  X  w =  X  w  X  w E , and V l ( w ) = max  X   X  X  ( w T F + l )  X  , and we consider q = 1 . The fol-lowing proposition shows how the re-ward error  X  w is related to the fre-quency error  X  v  X  . Due to the fact tween  X  w and  X  v  X  . However, we show that for any  X  w  X  R k , there is a monotonically decreasing function f such that for any  X  R + , if k  X  v  X  k 2 &lt; f ( ) then k  X  w k 2 6 .
 Proposition 1 Let  X  R + , if  X  w  X  R k , such that k w  X   X  w k 2 = , if the following condition is verified: then k  X  w k 2 6 .
 Proof The condition stated in the proposition implies: ( w k w E  X   X  w k 2 6 and thus k  X  w k 2 6 .
 Consequently, the reward loss k  X  w k 2 approaches zero as the error of the estimated feature fre-heuristics of V l .
 Corollary : Let V l and V l be respectively a lower and an upper bound on V l , then Proposition (1) holds if V l ( w )  X  V l (  X  w ) is replaced by V l ( w )  X  V l (  X  w ) .
 Figure (1) illustrates the divergence from the optimal reward weight w E when approximate fre-quencies are used. The error is not a continuous function of  X  v  X  when the cost function is not regularized, because the vector returned by MMP is always a fringe point. Informally, the error is proportional to the maximum subgradient of the function V l  X  v  X  E at the fringe point w E . The feature frequency error  X  v  X  can be significantly reduced by using the known transition func-tion for calculating  X  v  X  E and solving the flow Equations (1), instead of the Monte Carlo estimator (Equation (5)). However, this cannot be done unless the complete expert X  X  policy  X  E is provided. Assuming that the expert X  X  policy  X  E is optimal and deterministic, the value w T F X   X  E in Equa-tion (2) can be replaced by max  X   X  X  the demonstration. The cost function of the bootstrapped Maximum Margin Planning becomes: where G  X  E is the set of vectors  X   X  , subject to the following modified Bellman-flow constraints:  X   X  ( s ) =  X  ( s ) +  X  X S e is the set of states encountered in the demonstrations, where the expert X  X  policy is known. sponds to a margin between two convex functions: the value of the bootstrapped expert X  X  policy optimal solution of this modified cost function can be found by using the same subgradient as in Equation (3), and replacing  X   X  E by arg max  X   X  X  perimental analysis, the solution returned by the bootstrapped MMP outperforms the solution of MMP where the expert X  X  frequency is calculated without taking into account the known transition probabilities. This improvement is particularly pronounced in highly stochastic environments. The computational cost of minimizing this modified cost function is twice the one of MMP, since two optimal policies are found at each iteration.
 In the remainder of this section, we provide a theoretical analysis of the cost function given by Equation (6). For the sake of simplicity, we consider q = 1 and  X  = 0 .
 Proof If q = 1 and  X  = 0 , then the cost c q ( w ) corresponds to a distance between the convex and piecewise linear functions max  X   X  X  ( w T F + l )  X  and max  X   X  X  w most equal to the number of optimal vectors  X  in G  X  E , which is upper bounded by the number of deterministic policies defined on S \S e , i.e. by |A| |S| X  X S e | .
 Consequently, the number of different local minima of the function c q decreases as the number of states covered by the demonstration increases. Ultimately, the function c q becomes convex when the demonstration covers all the possible states.
 Theorem 1 If there exists a reward weight vector w  X   X  R k , such that the expert X  X  policy  X  E is the of the function c q , defined in Equation (6).
 sponds to vectors F X  0  X  F X  00 , with  X  0  X  arg max  X   X  X  ( w T F + l )  X  and  X  00  X  arg max  X   X  X   X   X  0  X  arg max  X   X  X  ( w T F + l )  X ,  X   X  00  X  arg max  X   X  X  therefore,  X   X  E  X  arg max  X   X  X  (  X w  X  T F + l )  X  . As with MMP, the feature frequencies in LPAL can be analytically calculated only when a complete assuming  X  E is deterministic (In LPAL,  X  E is not necessarily an optimal policy). Instead of enumer-v The maximal margin is found by solving the following linear program: where the values v E i are found by solving k separate optimization problems ( k is the number of weights w defined as: w i = 1 and w j = 0 ,  X  j 6 = i . To validate our approach, we experimented on two simulated navigation problems: a gridworld and two racetrack domains, taken from (Boularias &amp; Chaib-draa, 2010). While these are not meant to be challenging tasks, they allow us to compare our approach to other methods of apprenticeship learn-ing, namely MMP and LPAL with Monte Carlo estimation, and a simple classification algorithm at least one known state is encountered. The distance between two states corresponds to the shortest path between them with a positive probability. 7.1 Gridworld We consider 16  X  16 and 24  X  24 gridworlds. The state corresponds to the location of the agent on the grid. The agent has four actions for moving in one of the four directions of the compass. The actions succeed with probability 0 . 9 . The gridworld is divided into non-overlapping regions, and to the optimal deterministic policy found by value iteration. In all our experiments on gridworlds, we used only 10 demonstration trajectories, which is a significantly small number compared to other methods ( Neu &amp; Szepesvri (2007) for example). The duration of the trajectories is 50 time-steps. of the same duration as the demonstration trajectories. Our first observation is that Bootstrapped MMP learned policies just as good as the expert X  X  policy, while both MMP and LPAL using Monte Carlo (MC) estimator remarkably failed to collect any reward. This is due to the fact that we used a very small number of demonstrations ( 10  X  50 time-steps) compared to the size of these problems. Note that this problem is not specific to MMP or LPAL. In fact, any other algorithm using the same approximation method would produce similar results. The second observation is that the values of the policies learned by bootstrapped LPAL were between the values of LPAL with Monte Carlo and the optimal ones. In fact, the policy learned by the bootstrapped LPAL is one that minimizes the difference between the expected frequency of a feature using this policy and the maximal one among all the policies that resemble to the expert X  X  policy. Therefore, the learned policy maximizes the frequency of a feature that is not necessary a good one (with a high reward weight). We also notice that the performance of all the tested algorithms was low when 576 features were used. In this case, every feature takes a non null weight in one state only. Therefore, the demonstrations did not provide enough information about the rewards of the states that were not visited by the expert. Finally, we remark that k -NN performed as an expert in this experiment. In fact, since there are no obstacles on the grid, neighboring states often have similar optimal actions. 7.2 Racetrack We implemented a simplified car race simulator, a detailed description of the corresponding race-position, and the duration of each demonstration trajectory is 20 time-steps. For racetrack (2), the car starts at a random position, and the length of each trajectory is 40 time-steps. A high reward is given for reaching the finish line, a low cost is associated to each movement, and high cost is associated to driving off-road (or hitting an obstacle). Figure 2 (a-f) shows the average reward per step of the learned policies, the average proportion of off-road steps, and the average number of steps before reaching the finish line, as a function of the number of trajectories in the demonstra-off-road on both the cumulated reward and the velocity of the car. In this context, neighbor states do not necessarily share the same optimal action. Contrary to the gridworld experiments, MMP with Monte Carlo achieved good performances on racetrack (1). In fact, by fixing the initial state, the demonstration covers most of the reachable states, and the feature frequencies are accurately estimated from the demonstration. On racetrack (2) however, MMP with MC was unable to learn a good policy because all the states were reachable from the initial distribution. Similarly, LPAL with both MC and bootstrapping failed to achieve good results on racetracks (1) and (2). This is due to the fact that LPAL tries to maximize the frequency of features that are not necessary associated to a high reward, such as hitting obstacles. Finally, we notice the nearly optimal performance of the bootstrapped MMP, on both racetracks (1) and (2). The main question of apprenticeship learning is how to generalize the expert X  X  policy to states that have not been encountered during the demonstration. Inverse Reinforcement Learning (IRL) pro-behavior, and then using it for the generalization. A strong assumption considered in IRL-based al-features can be estimated from a few demonstrations even if these demonstrations cover only a small tic systems. We also showed that this problem can be solved by modifying the cost function so that the value of the learned policy is compared to the exact value of a generalized expert X  X  policy. We also provided theoretical insights on the modified cost function, showing that it admits the expert X  X  true reward as a locally optimal solution, under mild conditions. The empirical analysis confirmed the outperformance of Bootstrapped MMP in particular. These promising results push us to further investigate the theoretical properties of the modified cost function.
 As a future work, we mainly target to compare this approach with the one proposed by Ratliff et al. (2007), where the base features are boosted by using a classifier. Abbeel, Pieter and Ng, Andrew Y. Apprenticeship Learning via Inverse Reinforcement Learning. In
Proceedings of the Twenty-first International Conference on Machine Learning (ICML X 04) , pp. 1 X 8, 2004.
 Boularias, Abdeslam and Chaib-draa, Brahim. Apprenticeship Learning via Soft Local Homomor-phisms. In Proceedings of 2010 IEEE International Conference on Robotics and Automation (ICRA X 10) , pp. 2971 X 2976, 2010.
 Neu, Gergely and Szepesvri, Csaba. Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods. In Conference on Uncertainty in Artificial Intelligence (UAI X 07) , pp. 295 X  302, 2007.
 Ng, Andrew and Russell, Stuart. Algorithms for Inverse Reinforcement Learning. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML X 00) , pp. 663 X 670, 2000. Ramachandran, Deepak and Amir, Eyal. Bayesian Inverse Reinforcement Learning. In Proceedings of The twentieth International Joint Conference on Artificial Intelligence (IJCAI X 07) , pp. 2586 X  2591, 2007.
 Twenty-third International Conference on Machine Learning (ICML X 06) , pp. 729 X 736, 2006.
Prediction for Imitation Learning. In Advances in Neural Information Processing Systems 19 (NIPS X 07) , pp. 1153 X 1160, 2007.
 Syed, Umar and Schapire, Robert. A Game-Theoretic Approach to Apprenticeship Learning. In Advances in Neural Information Processing Systems 20 (NIPS X 08) , pp. 1449 X 1456, 2008. Syed, Umar, Bowling, Michael, and Schapire, Robert E. Apprenticeship Learning using Linear
Programming. In Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML X 08) , pp. 1032 X 1039, 2008.
