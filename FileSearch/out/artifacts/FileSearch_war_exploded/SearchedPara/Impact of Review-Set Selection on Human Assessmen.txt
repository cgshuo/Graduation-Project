 In a laboratory study, human assessors were significantly more likely to judge the same documents as relevant when they were presented for assessment within the context of documents selected using random or uncertainty sampling, as compared to relevance sampling. The effect is substantial and significant [0.54 vs. 0.42, p &lt; 0.0002] across a population of documents including both relevant and non-relevant doc-uments, for several definitions of ground truth. This result is in accord with Smucker and Jethani X  X  SIGIR 2010 finding that documents were more likely to be judged relevant when assessed within low-precision versus high-precision ranked lists. Our study supports the notion that relevance is mal-leable, and that one should take care in assuming any la-beling to be ground truth, whether for training, tuning, or evaluating text classifiers.
In supervised learning for text classification, each of a set of training documents is labeled as relevant or non-relevant by a human assessor. The training documents and their la-bels are used to induce a classifier to predict the relevance or non-relevance of the remaining documents in the popula-tion from which the training documents were drawn. Train-ing documents may be selected using random sampling, or using active learning methods such as uncertainty sampling or relevance sampling [8, 7]. Simulation studies comparing these approaches typically rely on the assumption that the sampling strategy does not influence how the assessor will label a particular document. We show, in a study involv-ing 36 paid assessors recruited from a university community, that the same documents are much more likely to be judged relevant when embedded in a set selected by random sam-pling or uncertainty sampling, than one selected by relevance sampling.

The influence of sampling strategy on labeling has im-pact beyond the selection of training sets for inducing clas-sifiers. In technology-assisted review, where every document with a positive classification is assessed and labeled, a su-perior (higher precision) classifier might yield fewer labeled-relevant documents than an inferior (lower precision) classi-fier. In the Cranfield approach to IR evaluation ( see [13]), a set of relevance assessments based on a pool of likely rel-evant documents might yield substantially different results from one based a sample of the whole. Our result calls into question the common practice of estimating measures like re-call and precision from statistical samples, especially those employing non-uniform inclusion probabilities.

A number of studies [11, 12, 6, 10, 5] have shown that the proportion, as well as the order of presentation of relevant and non-relevant documents, can affect user assessment be-haviour. Taken together, the results suggest that assessors are less likely to label documents relevant once they have seen a number of relevant documents, either due to presen-tation order or due to the overall proportion of relevant doc-uments. This observation forms the basis of the question we addressed: Since relevance sampling produces a higher pro-portion of relevant documents than uncertainty sampling or random sampling, does it suppress the assessor X  X  propensity to judge a document relevant? We further addressed the question of whether this effect was conditioned on ground truth, for several definitions of ground truth.
Our design specified that assessors would review batches of 100 documents for relevance to several topics, where the batches for each topic contained the same 12 known docu-ments, and 88 documents selected by one of random sam-pling, uncertainty sampling, or relevance sampling. 1 There were 9 topics, and hence 36 batches in total; each batch was assessed by three different assessors.

Table 1 provides a glossary of terms specific to our exper-imental design.
Documents were selected from the TREC-6 Ad Hoc col-lection, which has been the subject of previous relevance assessment studies [14, 9, 4], and has two independent sets of relevance assessments: the official NIST binary relevance assessments ( X  X elevant, X  and  X  X ot relevant X ) created using the pooling method, and a set of graded relevance assess-
Due to an error in our setup that went undetected until the assessments were complete, for some topics, the batches contained only 10 or 11 of the same known documents. The corresponding shortfall reduced the statistical power of our experiment, but does not affect its validity.
Batch 100 documents presented to assessors
Known docu-ments
Context The manner in which the documents
NIST assess-ments
Waterloo as-sessments
Rel The relevance class of a document, as
CAL Context in which documents are se-
SAL Context in which documents are se-
SPL Context in which documents are se-Table 1: Glossary of terms used throughout this work. ments ( X  X elevant, X   X  X ffy, X  and  X  X ot relevant X ) constructed by the University of Waterloo using interactive search and judg-ing [1]. We augmented each set of assessments with an ad-ditional category  X  X njudged X  for documents that were ex-cluded from the pool. The net effect is that each document has one of 12 combinations of three NIST and four Waterloo assessment categories. We chose at random one document with each combination of assessments as the 12 known doc-uments for each of 9 topics.

Our selection of topics was predicated on the fact that, of the 50 topics in the collection, only the nine we chose had at least one document labeled with each of the 12 combinations of assessment categories. Using each sampling method, as discussed below, we selected a list of 90 documents as de-tailed below, and inserted the known documents at fixed positions, chosen at random. Surplus documents (at posi-tions beyond 100) were discarded.

For random sampling, the list of 100 documents was a uniform random sample of the 560,000-document TREC cor-pus, in random order. Following Cormack and Grossman [2], we label this protocol, simple passive learning ( X  X PL X ). For uncertainty sampling and relevance sampling, we employed the simple active learning ( X  X AL X ) and continuous active learning ( X  X AL X ) methods [2, 3], training a classifier to re-trieve 10 documents, adding those documents to the train-ing set, and repeating the process nine times. Sofia-ML was used as the base classifier, configured to minimize logis-tic loss, and applied to a tf-idf representation of the docu-ments. The initial training set consisted of a positively la-beled pseudo-document consisting of the topic description, plus 100 negatively labeled documents selected at random without regard to their relevance. The training set was used to train the classifier, which was used to compute the like-lihood of relevance for each document in the collection. For SAL, the 10 documents with likelihood closest to 0 . 5 were selected and added to the training set; for CAL, the 10 docu-ments with greatest likelihood were selected. Training labels were derived from the Waterloo assessments:  X  X elevant X  and  X  X ffy X  were labeled positive;  X  X on-relevant X  and  X  X njudged X  were labeled negative.

Our list of 90 context documents consisted of the doc-uments retrieved by these nine iterations, in the order re-trieved. Table 2 depicts the prevalence of positive (Waterloo  X  X elevant X  or  X  X ffy X ) documents in the corpus, as well as the number in each batch, including known documents.
 Table 2: Corpus prevalence and number of positive documents for each context, where Waterloo  X  X ele-vant X  and  X  X ffy X  assessments are considered positive. The counts for each batch include known documents.
Following approval from the ethics review panel, we re-cruited 36 participants at large from University of Waterloo, including undergraduate students, graduate students, and faculty. At the outset, a participant was assigned one of the methods and topics at random, and was told that they would be remunerated $20 for reviewing all 100 documents. If the participant took 2 hours or less to judge the documents and achieved at least 25% recall and 25% precision with respect to the NIST assessments for the 100 documents, they were paid a bonus of $10 and offered the opportunity to assess up to two additional batches. For each subsequent batch, a new context and topic were selected at random, such that no participant saw the same context or the same topic more than once. Participants whose assessments did not meet the criteria were paid $20 and not invited to continue. The criteria and bonus were used to encourage participants to perform to the best of their ability. The recall and precision cutoffs were chosen with the intention of ensuring quality but not forcing users to be NIST-quality. 19 participants assessed 3 batches, 7 participants completed 2 batches, and 10 participants completed 1 batch. In total, three batches representing each context and each topic were assessed. The the continuation criterion was intended to limit the impact of poor assessments, while the random context and topic as-signment without repetition was intended to mitigate learn-ing effects. Participants conducted their assessments using a full-screen HTML interface, that displayed panels containing:
The outcome of general interest is the probability Pr[User that an assessor will render a positive judgement. The pri-mary predictor variable is the context within which the doc-ument is assessed. Accordingly, we wish to measure the conditional probability Pr[User + | Context] in order to test the hypothesis that Pr[User + | CAL] &lt; Pr[User + | SAL  X  SPL]. Assuming this hypothesis to be supported, we wish to test whether, individually, Pr[User + | CAL] &lt; Pr[User + | SAL] and Pr[User + | CAL] &lt; Pr[User + | SPL].

A second predictor variable is the relevance class Rel of the document, as determined by some combination of Water-loo and NIST assessments. To preserve the statistical power of our experiment, we restrict our consideration to the class W-RI, and its complement W-NU, where W-RI denotes any combination of assessments for which the Waterloo assess-ment is either  X  X elevant X  or  X  X ffy. X  We assumed that the hy-pothesis Pr[User + | W-RI] &gt; Pr[User + | W-NU] was extremely unlikely to be rejected, and concerned ourselves instead with the two hypotheses: (1)Pr[User + | CAL  X  W-RI] &lt; Pr[User + | (SAL  X  SPL)  X  W-RI] (2)Pr[User + | CAL  X  W-NU] &lt; Pr[User + | (SAL  X  SPL)  X  W-NU]
To estimate the conditional probabilities, we computed the fraction of positive assessments for documents satisfy-ing the specified predictors. To evaluate the significance of hypothesized differences, we applied a paired binomial test, where possible, to corresponding batches. There are an equal number of batches for each context, and for each of the relevance classes W-RI and W-NU. Across these sets, batches may be matched by topic and by the specific combi-nation of relevance assessments, leaving us to match within corresponding triples of batches, each reviewed by a differ-ent assessor. We matched the members of these triples by the assessor X  X  experience: As far as possible, the first batch reviewed by one assessor was matched to the first batch re-viewed by another assessor; the second batch reviewed by one assessor was matched to the second match reviewed by another assessor; and so on.
 Table 3: Probability of a study participant making a positive assessment, with 95% confidence intervals, for the primary predictors. For context, p-values were computed using a two-tailed paired binomial test; for relevance, p-values were computed using a z-test for difference in proportions. the CAL con-text.
 Table 4: Probability of a study participant making a positive assessment, with 95% confidence intervals, for combined predictors. p-values were computed relative to CAL, using a two-tailed paired binomial test.
 Figure 1: Probability of positive assessment given a context and elementary relevance class. Relevance classes are denoted xy where x  X  R, I, N, U denotes Waterloo relevant, iffy, non-relevant and unjudged, and x  X  R, N, U denotes NIST relevant, non-relevant, and unjudged.
Table 3 shows the results of our primary hypotheses, that context and relevance class separately influence the probabil-ity of a positive assessment. Separately, SAL and SPL both yield a substantially and significantly higher probability of positive assessment than CAL; W-RI yields a substantially and significantly higher probability of positive assessment than W-NU.

Table 4 shows the combined effect of context and rele-vance class. With respect to the W-NU relevance class, SAL and SPL separately yield a substantially and signifi-cantly higher probability of positive assessment than CAL. With respect to W-RI, the difference appears to be substan-tive, but only the difference between CAL and SAL appears to be significant, and even so would not be significant under Bonferroni correction for multiple hypothesis testing. The difference Pr[User + | CAL  X  W-RI]  X  Pr[User + | (SAL  X  SPL)  X  W  X  R )] is significant ( p 0 . 0288), by the following argu-ment: for the null hypothesis to be true, it would be neces-sary that Pr[User + | CAL  X  W-RI]  X  Pr[User + | SAL  X  W-RI)] and Pr[User + | CAL  X  W-RI]  X  Pr[User + | SPL  X  W-RI)]. The probability of both of these occurring by chance cannot ex-ceed the probability of either one, which implies Figure 2: Average time for assessment given a con-text and elementary relevance class. Relevance classes are denoted xy where x  X  R, I, N, U denotes Waterloo relevant, iffy, non-relevant and unjudged, and x  X  R, N, U denotes NIST relevant, non-relevant, and unjudged p &lt; min(0 . 0288 , 0 . 1214) = 0 . 0288. For each combination of Waterloo and NIST assessments, Figure 1 plots the prob-ability of a positive user assessment. Consistent with our statistical findings, the curves for SAL and SPL are gener-ally superior to the curve for CAL. It appears that for cases where one of the Waterloo or NIST assessments is  X  X ele-vant X  and is not discordant with the other ( i.e. , the other is  X  X elevant X  or  X  X njudged X ) there may be an insubstantial difference between CAL and the other contexts. Whether this observation reflects chance or an effect is a subject for future research.
We collected timing information in the course of imple-menting our participant retention criteria. Table 5 indi-cates that neither the context nor the relevance class has a substantial or significant effect on the time taken by par-ticipants to review documents. Figure 2, which plots as-sessment time taken against elementary relevance classes, suggests that non-relevant documents that were included in both the Waterloo and NIST judging pools may take longer to review. An interesting avenue of research would be to in-vestigate whether this observation is a manifestation of the observation by Smucker and Jethani [12], who found that as-sessors took longer to make incorrect assessments. Namely, we are interested in answering the following question: Are the long prediction times observed for the NN relevance class the result of false positives?
Our results show clearly that assessors are less likely to judge documents relevant when they are presented within the context of documents selected using relevance sampling, than when they are presented within the context of docu-ments selected using uncertainty sampling or random sam-pling. The effect holds for both relevant and non-relevant Table 5: Average time, in seconds, taken to assess documents under each condition with 95% confi-dence intervals for both all documents and known documents only. For context, p-values are with re-spect to a paired two-tailed t-test against the CAL predictor. For relevance, p-values are from Welch X  X  t-test. documents, as determined by archived assessments rendered by the University of Waterloo. Whether this effect applies to all relevant documents, or only to marginally relevant documents, remains an open question.

Our results call into question the practice of deeming the assessments of one individual to be authoritative [15], or of assuming that validation based on sampling is equivalent to validation using the pooling method.
