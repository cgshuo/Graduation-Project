 know it can be decomposed as where L the row/column space) of L recover the column-space of the low-rank matrix L C , exactly and efficiently? column space of this low-rank approximation.
 space  X  and hence the corresponding matrix L the identities of the outliers. This is precisely our proble m. R matrix can be decomposed as Here L Value Decomposition (SVD) Thus it is clear that the columns of U space. Also note that at most (1  X   X  ) n of the columns of L the outliers). C columns of C With this notation, out intent is to exactly recover the column space of L to impose a few additional assumptions. We develop these in S ection 2.1 below. We are also interested in the noisy case, where tion of both the true subspace and the outliers. 2.1 Incoherence: When does exact recovery make sense? To make the problem meaningful, we need to impose that the low -rank matrix L column-sparse as well. This is done via the following incoherence condition . is said to be column-incoherent with parameter if where { e i } are the coordinate unit vectors.
 then = 1 .
 if the points are generated by a uniform distribution over a bounded set, then =  X (1) . A small incoherence parameter essentially enforces that the matrix L up to date [4, 15 X 17].
 column of C known a priori . They only arise in the analysis of our algorithm X  X  performa nce. accordingly, A U , is denoted by P P P which U and V we are using. The complementary operators, P for any matrix A that satisfies P  X  norm of the columns, and k A k  X  base vector. The SVD of L i.e., the fraction of outliers. While we do not recover the matrix L under very weak assumptions  X  exactly recover both the column space of L space the uncorrupted points lie on) and the column support o f C from M . If there is additional noise corrupting the data matrix, i. e. if we have M = L a natural variant of our approach finds a good approximation. 3.1 Algorithm is Algorithm 1 Outlier Pursuit Find (  X  L,  X  C ) , the optimum of the following convex optimization program. Compute SVD  X  L = U Output the set of non-zero columns of  X  C , i.e.  X  I = { j :  X  c algorithm, we have the following variant for the noisy case. torial and intractable) first approach to the recovery probl em: where k k 3.2 Performance the low-rank matrix L statement appears below.
 Theorem 1 (Noiseless Case). Suppose we observe M = L herence parameter . Suppose further that C points,  X  , satisfies where c indeed it holds for any  X  in a specific range which we provide below. we have the following result.
 Theorem 2 (Noisy Case). Suppose we observe  X  M = M + N = L with c  X  L,  X  the rank of the matrix L 3.3 Related Work applications such as finance, bio-informatics, and more.
 solved in polynomial time.
 convex optimization cannot recover L well as different analysis techniques on which we elaborate below. appear in a full version available online [26]. The proof fol lows three main steps ery of L For any matrix X , define P onto matrices that share the same column space or row space wi th L  X  . That is, column H  X  P there exists a Q such that unique optimum.
 Note that here k k is the spectral norm (i.e. largest singular value) and k k i.e.  X  particular, recall the SVD of the true L onto the space of all matrices with column space contained in U for the column support I when all the columns in I c Note that U optimum of (2) to satisfy P subspace. Similarly, having  X  C satisfy P Oracle Problem: Minimize: k L k  X  +  X  k C k 1 , 2 Subject to: matrix V  X  R r  X  n such that  X  U  X  V  X  = U easy to show that P obeys P  X  i  X  I 0 ,  X  H i =  X  C i / k  X  C i k 2 .
 Define matrix G  X  R r  X  r as and constant c , k G k . Further define matrices  X   X  Proposition 2. If c &lt; 1 ,  X  U the column space of L V , we automatically satisfy the condition P that P For the general, non-orthogonal case, however, we require t he matrices  X  ideas, we then quickly obtain the proof for the noisy case. time than interior point methods.
 identical copy of a random Gaussian vector. Outlier Pursuit succeeds if  X  C  X  P observations. We scale each outlier so that the  X  with an  X  Outlier Pursuit correctly identifies the outliers.
 probability (independently). Letting  X  be the set of observed entries, we solve understand theoretical guarantee of (9) in the incomplete o bservation case. is not possible. Hence, we use the  X  the outliers: a larger  X  we apply thresholding after C is obtained. Figure 3(a) shows the  X  are identified. (a) Complete Observation (b) Partial Obs. (one run) (c) Part ial Obs. (average) Acknowledgements H. Xu would like to acknowledge support from DTRA grant HDTRA 1-08-edge support from the NSF CAREER program, Grant 0954059.
