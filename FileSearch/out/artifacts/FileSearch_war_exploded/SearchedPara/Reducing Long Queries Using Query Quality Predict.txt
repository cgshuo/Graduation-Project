 Long queries frequently contain many extraneous terms that hinder retrieval of relevant documents. We present tech-niques to reduce long queries to more effective shorter ones that lack those extraneous terms. Our work is motivated by the observation that perfectly reducing long TREC descrip-tion queries can lead to an average improvement of 30% in mean average precision. Our approach involves transform-ing the reduction problem into a problem of learning to rank all sub-sets of the original query (sub-queries) based on th eir predicted quality, and selecting the top sub-query. We use various measures of query quality described in the literatu re as features to represent sub-queries, and train a classifier . Replacing the original long query with the top-ranked sub-query chosen by the ranker results in a statistically signifi -cant average improvement of 8% on our test sets. Analysis of the results shows that query reduction is well-suited for moderately-performing long queries, and a small set of quer y quality predictors are well-suited for the task of ranking s ub-queries.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation Algorithms, Experimentation, Performance Long Queries, Verbose Queries, Query Quality, Query Re-duction
While most queries presented to search engines vary be-tween one to three terms in length, a gradual increase in the average length of queries has been observed 1 . These longer queries are typically used to convey more sophisti-cated information needs. Unfortunately, the performance of most commercial and academic search engines deterio-rates while handling longer queries. For example, a query such as ideas for breakfast menu for a morning staff meet-ing , while conveying the true information need of the user, is better handled by most search engines when posed as break-fast meeting menu ideas . This task of reducing the original query to a shorter one in the course of a search session is usually left to the user.

In this paper we present a way to automatically reduce long queries to shorter, more effective ones. Long queries lend themselves to such reduction as they invariably contai n extraneous terms ( for , morning , and staff in our example) that serve more to confuse the search engine than support it in its task. In Section 2, we analyze and quantify the po-tential improvement in mean average precision (MAP, Sec-tion 4) that can be obtained by selecting ideal terms (or rejecting unnecessary terms) from long TREC description queries.
 This potential for improvement motivated past work like Bendersky and Croft [1] and Lease et al. [18] on automatic techniques for query reduction. The former X  X  approach in-volved learning to identify key concepts in long queries usi ng a variety of features while the latter focused on a regressio n-based approach to re-weight all the terms in long queries. An interactive technique that involved completely droppin g unnecessary terms from long queries was successfully demon -strated by Kumaran and Allan [17]. However, the interac-tion burdened the user with additional cognitive and physi-cal effort.

Our technique for automatic query reduction involves ana-lyzing all the subsets of terms from the original query (sub-queries), and identifying the most promising sub-query to replace the original long query. The technique involves rep -resenting each sub-query by a set of query quality predictor s, and then learning effective ranking functions based on this representation.

To find the predictors to represent (the quality of) each sub-query, we draw on the large body of previous work on query quality prediction. These include predicting the quality of queries using either pre-retrieval indicators l ike Query Scope [10], or post-retrieval indicators like Query Clarity [7]. The ability to predict query performance finds use in applications as diverse as resource selection in fede r-ated search [26], determining whether to invoke user intera c-http://blogs.zdnet.com/micro-markets/index.php?p=27 tion [16], and learning when to advertise [2]. We used query-quality predictors as features to describe each sub-query. In Section 3 we describe the entire set of query quality predic-tors that we used.

Our approach to ranking sub-queries is reminiscent of work related to learning to rank documents [13, 4, 22]. Learning to rank documents involves representing docu-ments using features computed from properties associated with them, and the query. A classifier is then trained to rank relevant documents higher. In the paper we apply this framework to the problem of learning to rank sub-queries, with the specific goal of surfacing the best sub-query to replace the original long query. The particular algorithm we used is outlined in Section 4.2.

We performed query reduction experiments on several combinations of TREC collections (Section 4), and observed significant improvements in performance (Section 5). These observations validated the utility of query reduction. The analysis of the results, which is presented in Section 6, revealed that query reduction is most useful for long querie s that originally exhibited moderate performance as measure d by MAP. Further, from among the thirty one features that we used, we observed that a small subset of well-known query quality predictors like Query Clarity provided most o f the predictive power in the models we learned for different collections. All this points to query reduction as a promisi ng avenue for future work, with emphasis on developing better query quality predictors as well as effective strategies and algorithms for ranking queries.
In this section, we provide an illustrative example showing the utility of query reduction, and demonstrate the improve -ment in performance that can be realized through the tech-nique. We further show how the problem of query reduction can be transformed into a ranking problem.
TREC topics consist of a title , description , and narrative , of progressively increasing length. While the title is usua lly between one and four terms in length, the description is longer, ranging from three to thirty terms in length. We use the TREC description queries as surrogates for long queries in our experiments.

Table 1 provides insight into the utility of query reduc-tion for the description portion of TREC topic 333 from the Robust 2004 data collection. The table contains the ac-tual description query in the header, along with sample sub-queries and their associated performance metrics in the firs t two columns (Sub-query and Performance Metrics). We can observe that some sub-queries achieve significantly better performance compared to the original query. These queries are the candidate targets for query reduction.

Table 2 shows the summary upper-bound performance that can be achieved for a set of 200 TREC description queries from the Robust 2004 track used for training (Sec-tion 4).  X  X aseline X  refers to a query-likelihood retrieval model [20] run using the Indri search engine [21] using the original long query.  X  X racle X  refers to the situation when the best sub-query was selected to replace the original long query. This gives us an upper bound on the performance that can be realized through query reduction for this set of queries. It is this statistically significant improvement i n Table 2: The utility of query reduction for 200 train-ing queries from the TREC 2004 Robust collection. A value in bold face implies statistically significant improvement over the baseline. Statistical signifi-cance was measured using the Wilcoxon matched-pairs signed-ranks test, with  X  set to 0.05. Figure 1: MAP at various ranks when sub-queries are ranked using query quality predictors. The sub-queries are from 172 Robust 2004 Track long de-scription queries. performance through query reduction that we target in our work.
We now focus our attention on the third column of Ta-ble 1 (Query Quality Predictors). This column contains the values of some popular query quality predictors for each of the sub-queries: Query Clarity [7], average inverse collec -tion term frequency (AvICTF) [10], average inverse docu-ment frequency (AvIDF), and query scope (QS) [10]. Also included in the final row are the correlation coefficient value s of each of these query predictors with the AP of sub-queries. The values imply a weak X  X ositive to positive correlation be -tween the predictors and AP. This means that if we were to use these query quality predictors to rank the sub-queries, then there is a reasonable chance that we can identify a better-performing sub-query.

In Figure 1 we illustrate the effect of ranking sub-queries of 172 long training 2 queries from the Robust 2004 track us-ing individual query quality predictors. The figure shows th e MAP at ranks one to ten when the sub-queries are ranked
We selected training queries that had ten or more sub-queries to generate this graph. Our sub-query pruning pro-cedure (Section 4.3) resulted in some queries having less th en ten sub-queries. using each of the query quality predictors, as well as the upper bound ranking for the 172 queries. We notice that none of the query quality predictors is able to achieve the upper bound performance, or even the baseline performance got by using the long queries without any modifications. We hypothesize that by using a combination of the query qual-ity predictors we can train a ranker that beats the baseline performance.

This is the main idea of the paper: using query quality predictors as features to learn a ranking function to target the upper-bound ranking of sub-queries. Once the ranking is completed, we can use the top sub-query in place of the cor-responding long query. In the next section, we will describe the set of features we considered for each sub-query, and the machine learning algorithms we used to learn a ranking function to bias better performing sub-queries higher up th e ranked list.
For each of the O (2 n ) sub-queries that a query of length n can have, we calculated the following query quality pre-dictors to serve as descriptive features. While some of thes e features such as mutual information (MI) are stand-alone, i.e. they are calculated for the sub-query as a whole, other features such as Average IDF (AvIDF) are aggregated from term level statistics. Some of these features are pre-retri eval, i.e. they are derived directly from query and corpus statis-tics. Others like Query Clarity are post-retrieval, i.e. th ey involve performing an initial retrieval and hence are more expensive to compute. We now describe the set of query quality predictors we used, and include references to their sources. This feature was adopted from the work by Kumaran and Allan [15] that was based on the observation by van Rijs-bergen [23]. We represented each of the O (2 n ) sub-queries as a graph constructed with the constituent terms as ver-tices, and the mutual information [5] between the terms as edge weights. The mutual information was calculated using Equation 1 [5]. The maximum spanning tree [6] was then identified on each graph, and its average weight was used as a predictor of the quality of the corresponding sub-query. where n ( x, y ) is the number of times terms x and y occurred within a term window of 100 terms across the corpus, n ( x ) and n ( y ) are the frequencies of x and y in the collection and T is the number of term occurrences in the collection.
Drawing on work by He and Ounis [10] and the observa-tion by Kumaran and Allan [17] that the best sub-queries have lengths between two and six, we included SQLen as a query quality predictor. SQLen for a sub-query is defined as the number of terms in it.
Developed by Cronen-Townsend et al. [7] this post-retrieval predictor is the Kullback-Leibler divergence of the query model from the collection model. The query model is estimated from the top-ranked documents retrieved by the original query. QC is computed as where P ( w | Q ) is the probability of the occurrence of the word w in the query model, and P C ( w ) is the probability of the occurrence of w in the collection.
To avoid the expensive computation of query clarity, He and Ounis [10] proposed simplified clarity score as a com-parable pre-retrieval performance predictor. It is calcul ated as where P ml ( w | Q ) is the probability of the occurrence of the word w in the query. We calculated the IDF of each query term w as where N w is the document frequency of w and N is the number of documents in the collection.

For each sub-query we calculated the (a) sum (b) standard deviation [10] (c) maximum/minimum [10] (d) maximum (e) arithmetic mean (f) geometric mean (g) harmonic mean and (h) coefficient of variation of the IDFs of constituent terms. These values served as additional query quality predictors for each sub-query.
Query scope [10, 19] is a measure of the size of the re-trieved document set relative to the size of the collection. We can expect that high values of query scope are predic-tive of poor-quality queries as they retrieve far too many documents.
 where n Q is the number of documents containing at least one query term.
Proposed by Zhao et al. [28], this query quality predictor is based on the hypothesis that queries that have higher sim-ilarity to the collection as a whole will be of higher quality . For each term w in the query
Based on the SCQ values of each term, we calculated ag-gregate values similar to those for IDF (Section 3.5) as sub-query quality predictors.
Inverse collection term frequency of a term w is defined as
Using the ICTF values, we calculated aggregate statistics similar to those for IDF (Section 3.5).
Guided by the notion that the reduced query should still reflect the original query X  X  information need, we calculate d the cosine similarity between the TF-IDF vectors represent -ing each sub-query and the original long query. We hypoth-esized that a sub-query that was not radically different from the original query was preferable to one that had drifted away completely from the original query X  X  intent.
In summary, we represented each sub-query with a set of thirty one features. In the following sections we will descr ibe the experimental setup as well as the results of learning to rank sub-queries using these features.
We used the indexing and retrieval capabilities of ver-sion 2.6 of the Indri search engine, developed as part of the Lemur 3 project. Our retrieval model was the query-likelihood variant of statistical language modeling [20]. We used Dirichlet smoothing [27] with  X  set to 1000. For ex-perimentation, we used three collections 4 . The first, Robust 2004, was the 2004 Robust track collection that has 250 5 queries and contains around half a million documents from the Financial Times, the Federal Register, the LA Times, and FBIS. The second collection, TREC123, was created by combining existing TREC collections. It contained docu-ments from TREC disks 1 and 2, and the 150 TREC top-ics 51 to 200 as queries. We took advantage of the fact that some TREC collections used the same sets of docu-ments, and created this new collection with a larger num-ber of queries. This enabled the creation of good train-ing/test splits required for training classifiers. Each set of queries was broken down into an 80%/20% split of train and test queries. The Robust 2004 collection is known to con-tain difficult queries, and thus provided a challenging data set to test the utility of query reduction for hard queries. TREC123 offered a collection of moderate difficulty. To fur-ther test the ability to learn a ranking across collections, we combined the training (200 + 120) and test (46 + 30) queries from Robust 2004 and TREC123 to create a new collection called Robust 2004 + TREC123.

All collections were stemmed using the Krovetz stem-mer [14] provided as part of Indri. We used an extended set of 418 stop words, also referred to as the INQUERY stop http://www.lemurproject.org
We refer to a set of documents and associated queries as a collection.
We had to drop four queries that either did not have any relevant documents in the collection, or were too long to be handled by our system. word list [3]. To identify named entities in the queries, we used the Stanford Named-Entity Recognizer [8], which can identify Person, Location, and Organization entities.
As performance measures, we report precision at five documents (P@5), precision at ten documents (P@10), normalized discounted cumulative gain at 15 documents (NDCG@15, as defined in [24]), and mean average precision (MAP). P@5 and P@10 refer to the fraction of relevant documents in the top five and ten documents retrieved re-spectively. NDCG@15 is a measure similar to precision that includes rank-based discounting. This means that systems that return relevant documents higher up a ranked list will receive higher scores compared to those that return them lower. Average precision (AP) is a single value obtained by averaging the precision values at each new relevant document observed. MAP is the arithmetic mean of the average precisions of a set of queries.
Given as input a set of predictors for each sub-query, our goal was to combine these inputs to produce an ef-fective ranking function. To accomplish this we used RankSVM [13], a learning-to-rank algorithm based on the same framework as the well-established Support Vector Machines (SVM) classification algorithm.

For each original query Q i of length n , the set of all possi-ble O (2 n ) sub-queries SQ i = { sq i 1 , sq i 2 , ..., sq erated. Each sub-query was represented by its AP value y ij and associated vector of k query quality predictors sq ij [ x ij 1 , x ij 2 , ..., x ijk ]. RankSVM works by utilizing a pair-wise preference ranking framework in which instead of tak-ing each sub-query in isolation, all possible sub-query pai rs (with different AP values) are used as instances in the learn-ing process.
 We used the RankSVM implementation available in the SV M Light [12] package. Both Linear and RBF kernels were considered in our experiments. The regulariza-tion parameter C (trade-off between training error and margin) as well as the gamma parameter of the RBF kernel were selected from a search within the discrete set 10  X  4 , 10  X  3 , 10  X  2 , 10  X  1 , 10 0 over the validation set the selection of parameter C , the default SVMLight value was also considered.

Although the differences were not substantial, exper-iments with the best RBF kernel parameters performed slightly better than the best linear kernel parameters for the majority of the validation experiments. Unless other-wise noted, all results henceforth were obtained using an RBF kernel, with gamma set to 0.001.
An exponential number ( O (2 n )) of sub-queries can be obtained from a query of length n . Further, training the ranking classifier involved generation of pairwise prefere nce constraints from this exponential number. This 2 n followed by n 2 combinatorial explosion resulted in an extremely large training set that was very expensive to train using SV M Light . This necessitated the pruning of candidate sub-queries. We used the following strategies to reduce the number of candidate sub-queries. 20% of the queries in the training set were randomly se-lected and used to create a validation set.
These simple pruning strategies resulted in a more man-ageable set of candidate sub-queries. We acknowledge that such pruning could hurt performance, but as we will show in the next section, we still obtained significant improvement s in performance on our test queries.
Table 3 contains the results of ranking the sub-queries of the test queries for each of the collections we considered. W e can observe that our ranking technique selects query reduc-tions that result in a statistically significant improvemen t over the baseline for all collections. Also included in the t a-ble are the upper bound values for the performance metrics, i.e. the performance when a hypothetical ranker achieves perfect ranking of the test sub-queries. This provides an indication of the scope for further improvement.

We notice that the rankers we have trained achieve 15  X  30% of the net gain in MAP possible for the three collec-tions. The percentage improvement is greater for TREC123 than for Robust 2004, possibly indicative of the moderate difficulty of the TREC123 queries. The improvement due to query reduction for Robust 2004 + TREC123 is encouraging as it implies that the ranking procedure is robust enough to handle training and test instances containing feature valu es computed from different collections.
Query Performance: Query reduction resulted in sta-tistically significant improvements in AP for all the test co l-lections. To understand the effect of query reduction on long queries, we now analyze the results introduced above. To make the task of analysis easier, we define three types of long queries. The first, with an baseline AP between 0 and 0.1, are considered poorly-performing queries. Queries wi th a baseline AP between 0.1 and 0.4 are labeled moderately-performing queries. Finally, queries with a baseline AP greater than 0.4 are referred to as well-performing queries . The distribution of queries of different types, as well as the observations we make, are specific to the collections we have experimented with. Additional collections need to be ana-lyzed to verify whether the same trends generalize further.
Figure 2 is a set of three scatter plots depicting the util-ity of query reduction on all three collections. To provide clarity, incomplete grid lines have been drawn to isolate th e areas corresponding to the queries of different types. The improvements in P @ 5, P @ 10, and NDCG @ 15 follow the trends for MAP. line y = x is also included to convey whether a particular query was improved by query reduction or not. A point above the line corresponds to a query that was improved by query reduction, while a point below the line refers to a query that was hurt by query reduction.
 Figure 2(a) is a scatter plot for the 30 test queries in TREC123. The bottom left box contains queries that are poorly-performing, the middle one contains queries that are moderately-performing, and the rightmost one is occupied by high-performing queries. Query reduction for poorly-performing queries almost never results in signific ant improvements in performance. An approximately equal number of queries appear to be hurt and improved. In the box for high-performing queries, where there are fewer queries, we estimate that query reduction leads to much larger gains or losses in performance. The bulk of the overall improvement in performance that we see appears to come from moderately-performing queries. The improve-ments for this category are neither as pronounced as the high-performing queries nor are they as minuscule as for the poorly-performing ones. We notice similar trends in all three query sets.

Another interesting point to note is that the queries ap-pear to be  X  X oxed-in X , i.e. very rarely do we see a query that is drastically hurt or drastically improved to the extent th at is ends up outside the boxes. The size of the boxes increases progressively from poorly-performing to high-performing queries. This means that the effect of query reduction is more pronounced as the quality of the original query improves.

Robust 2004 + TREC123 (Figure 2(c)) contains a mix of relatively easy TREC123 queries and harder Robust 2004 queries. We observe that the trends observed in Figures 2(a) and 2(b) carry over to this collection as well, i.e. the ranke r trained on Robust 2004 + TREC123 performs better on moderately performing TREC123 queries (marked with symbol  X  X  X ) than on moderately performing Robust 2004 queries (marked with symbol  X  X  X ). Thus, the quality of the original query plays a very important role in the impact of query reduction. Increasing the amount of training data, as in the case of Robust 2004 + TREC123, doesn X  X  seem to result in a ranker more capable of improving poor and moderately performing long queries.

Top Features: In Table 4 we present the list of five top-ranked features for each collection. To identify these features we used RankSVM with linear kernels since the rel-ative feature weights are not available for models with RBF kernels 7 . The table shows that the set of top features is quite consistent across collections though their relative im-portance depends on the collection. Clarity and features based on term-coherence (MI), IDF and ICTF are clearly the most important features.
Past work by Kumaran and Allan [15] set the stage for interactive versions of query reduction. Using a single fea -ture namely mutual information (MI) they selected a set of ten top-ranked sub-queries and presented them to the user to choose from. By analyzing supplemental information in the form of snippets of text from the top-ranked document corresponding to each sub-query, users were able to select good sub-queries. However, such selection required addi-tional physical and cognitive effort from users, motivating the need for automatic query reduction techniques.
Bendersky and Croft [1] approached the problem of query reduction as a problem of finding key concepts in long queries for preferential weighting. They used a number of query and corpus-dependent as well as corpus-independent features to learn to identify the key terms in long queries. Our work differs from theirs as we did not assume that
In all collections, the linear kernel ranker performed slig htly worse than the RBF one, but the difference was not statis-tically significant. Figure 2: Scatter plots of baseline long queries AP versus AP of corresponding reduced queries. finding key concepts will capitalize on the full potential fo r query reduction. With the exception of named entities, we treat all terms as equally useful following the observation by Kumaran and Allan [15] that the best sub-queries sometimes contained terms that could be considered ordinary, but help with retrieval in more complex ways than imagined.
More recently, Lease et al. [18] proposed a regression framework to re-weight the terms in long queries. They introduced a set of secondary features that correlated with the term weights, and then applied different types of regres-sion techniques to learn appropriate feature-based rankin g functions. In contrast with our approach, Lease et al. did not try to reduce long query terms directly, but instead choose to re-weight them.

Numerous efforts have been made towards improving tech-niques for predicting query quality. Cronen-Townsend et al. [7] developed the Query Clarity measure, which was the top-ranking feature in all our models, to serve as a predic-tive measure for tracking AP. He and Ounis [10] explored a number of pre-retrieval features to determine query effec-tiveness. Zhao et al. [28] explored pre-retrieval predicto rs that were based on the similarity between a query and the document collection as well as the variability in query term distribution across documents. Features built on the query -collection similarity-based predictors ranked high in the list of important features identified by our rankers.

Hauff et al. [9] conducted a survey of 22 pre-retrieval query quality predictors. They concluded that there wasn X  X  one single predictor that performed best on all collections, an d the utility of different predictors was related to the collec tion in question. In contrast, our experiments used both pre and post retrieval query quality predictors, and results revea led that Query Clarity was consistently the best feature for all test collections.

Unlike MI, our query-term coherence measure, He et al. experimented with the use of measures related to the co-herence of documents in the ranked list [11]. While they reported those measures as being correlated to MAP, we choose not to use them because of the complexity involved in calculating the measures X  values.
We have presented a new way of approaching the problem of query reduction, an effective technique that is quite hard to realize. By casting the query reduction problem as a sub-query ranking problem we have been able to draw on work in the areas of query quality prediction and learning to rank . We have shown statistically significant improvements on all our test collections, validating the utility of query reduc tion. Our analysis of the results revealed some interesting prop-erties of long queries such as the dependence of the utility of query reduction on the quality of the original long query. By analyzing the top features in the learned ranking models we have identified a set of features well-suited for ranking sub-queries.

Our choice of query quality predictors is by no means ex-haustive. However, our choice of predictors was deliberate -we avoided complex features like document and ranked list perturbation [29, 25] that would have been too expensive to compute for the exponential number of sub-queries. By showing significant improvement in performance using easil y computed features, we have paved the way for easy adoption of query reduction.
The quality of a ranker is intimately connected to the quality of the features used to represent training and test instances. As future work we plan to work on developing and incorporating more effective query quality predictors. We plan to extend this work on TREC collections to queries submited to web search engines. We also plan to optimize the ranking of sub-queries to target other performance mea-sures like P@5, P@10, and NDCG.
 We wish to thank the anonymous reviewers of this paper for their helpful comments.
