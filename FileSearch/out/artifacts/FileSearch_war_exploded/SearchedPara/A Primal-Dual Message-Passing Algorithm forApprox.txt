 Unlike standard supervised learning problems which involve simple scalar outputs, structured pre-one would want to make joint predictions on the structured labels instead of simply predicting each element independently, as this additionally accounts for the statistical correlations between label elements, as well as between training examples and their labels. These properties make structured prediction appealing for a wide range of applications such as image segmentation, image denoising, sequence labeling and natural language parsing.
 Several structured prediction models have been recently proposed, including log-likelihood models such as conditional random fields (CRFs, [10]), and structured support vector machines (structured SVMs) such as maximum-margin Markov networks (M3Ns [21]). For CRFs, learning is done by minimizing a convex function composed of a negative log-likelihood loss and a regularization term. Learning structured SVMs is done by minimizing the convex regularized structured hinge loss. Despite the convexity of the objective functions, finding the optimal parameters of these models can be computationally expensive since it involves exponentially many labels. When the label structure corresponds to a tree, learning can be done efficiently by using belief propagation as a subroutine; The sum-product algorithm is typically used in CRFs and the max-product algorithm in structured SVMs. In general, when the label structure corresponds to a general graph, one cannot compute the objective nor the gradient exactly, except for some special cases in structured SVMs, such as matching and sub-modular functions [22]. Therefore, one usually resorts to approximate inference algorithms, cf. [2] for structured SVMs and [20, 12] for CRFs. However, the approximate inference algorithms are computationally too expensive to be used as a subroutine of the learning algorithm, therefore they cannot be applied efficiently for large scale structured prediction problems. Also, it is not clear how to define a stopping criteria for these approaches as the objective does not monotoni-cally decrease since the objective and the gradient are both approximated. This might result in poor approximations.
 In this paper we propose an approximated structured prediction framework for large scale graphical models and derive message-passing algorithms for learning their parameters efficiently. We relate CRFs and structured SVMs, and show that in CRFs a variant of the log-partition function, known as soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an intuitive approximation for the structured prediction problem, using duality, based on a local entropy approximation and derive an efficient message-passing algorithm that is guaranteed to converge. very large number of parameters. We demonstrate the effectiveness of our approach in an image denoising task. This task was previously solved by sharing parameters across cliques. In contrast, our algorithm is able to efficiently learn large number of parameters resulting in orders of magnitude better prediction.
 In the remaining of the paper, we first relate CRFs and structured SVMs in Section 3, show our approximate prediction framework in Section 4, derive a message-passing algorithm to solve the approximated problem efficiently in Section 5, and show our experimental evaluation. Consider a supervised learning setting with objects x  X  X and labels y  X  X  . In structured prediction the labels may be sequences, trees, grids, or other high-dimensional objects with internal structure. Consider a function  X  : X  X Y  X  R d that maps ( x,y ) pairs to feature vectors. Our goal is to construct a linear prediction rule with parameters  X   X  R d , such that y  X  ( x ) is a good approximation to the true label of x . Intuitively one would like to minimize the loss ` ( y,y  X  ) incurred by using  X  to predict the label of x , given that the true label is y . However, since the prediction is norm-insensitive this method can lead to over fitting. Therefore the parameters  X  are typically learned by minimizing the norm-dependent loss we focus on structured SVMs and CRFs which are the most common structured prediction models. The first definition of structured SVMs used the structured hinge loss [21] The structured hinge loss upper bounds the true loss function, and corresponds to a maximum-margin approach that explicitly penalizes training examples ( x,y ) for which  X  &gt;  X ( x,y ) &lt; ` ( y,y  X  ( x )) +  X  &gt;  X ( x,y  X  ( x )) .
 The second loss function that we consider is based on log-linear models, and is commonly used in CRFs [10]. Let the conditional distribution be p ( X  y | x,y ;  X  ) = is then the negative log-likelihood under the parameters  X  In structured SVMs and CRFs a convex loss function and a convex regularization are minimized. In CRFs one aims to minimize the regularized negative log-likelihood of the conditional distribution problem of minimizing the regularized loss in (1) with the loss function  X  ` log can be written as where ( x,y )  X  S ranges over training pairs and d = P ( x,y )  X  X   X ( x,y ) is the vector of empirical means.
 Structured SVMs aim at minimizing the regularized hinge loss  X  ` hinge (  X  ,x,y ) , which measures the Since y  X  ( x ) is independent of the training label y , the structured SVM program takes the form: (structured SVM) min where ( x,y )  X  X  ranges over the training pairs, and d is the vector of empirical means. In the following we deal with both structured prediction tasks (i.e., structured SVMs and CRFs) as two instances of the same framework, by extending the partition function to norms, namely ing over  X  y  X  Y . Using the norm formulation we move from the partition function, for = 1 , to the maximum over the exponential function for = 0 . Equivalently, we relate the log-partition and the max-function by the soft-max function For = 1 the soft-max function reduces to the log-partition function, and for = 0 it reduces to the max-function. Moreover, when  X  0 the soft-max function is a smooth approximation of the max-function, in the same way the ` 1 / -norm is a smooth approximation of the `  X  -norm. This smooth approximation of the max-function is used in different areas of research [8]. We thus define the structured prediction problem as which is a one-parameter extension of CRFs and structured SVMs, i.e., = 1 and = 0 respec-tively. Similarly to CRFs and structured SVMs [11, 16], one can use gradient methods to optimize structured prediction. The gradient of  X  r takes the form where is a probability distribution over the possible labels  X  y  X  X  . When  X  0 this probability distribution gets concentrated around its maximal values, since all its elements are raised to the power of a very large number (i.e., 1 / ). Therefore for = 0 we get a structured SVM subgradient.
 exponentially many labels in Y . The feature maps usually describe relations between subsets of label elements y  X   X  X  y 1 ,...,y n } , and local interactions on single label elements y v , namely Each feature  X  r ( x,  X  y ) can be described by its factor graph G r,x , a bipartite graph with one set of nodes corresponding to V r,x and the other set corresponds to E r,x . An edge connects a single label node v  X  V r,x with a subset of label nodes  X   X  E r,x if and only if y v  X  y  X  . In the following we consider the factor graph G =  X  r G r which is the union of all factor graphs. We denote by N ( v ) and N (  X  ) the set of neighbors of v and  X  respectively, in the factor graph G . For clarity in the naturally extends to any graphical model representing the interactions ` ( y,  X  y ) . nentially many labels have to be considered. This is in general computationally prohibitive, and thus one has to rely on inference and message-passing algorithms. When the factor graph has no cycles inference can be efficiently computed using belief propagation, but in the presence of cycles inference can only be approximated [25, 26, 7, 5, 13]. There are two main problems when deal-ing with graphs with cycles and approximate inference: efficiency and accuracy. For graphs with cycles there are no guarantees on the number of steps the message-passing algorithm requires till convergence, therefore it is computationally costly to run it as a subroutine. Moreover, as these message-passing algorithms have no guarantees on the quality of their solution, the gradient and the objective function can only be approximated, and one cannot know if the update rule decreased or increased the structured prediction objective. In contrast, in this work we propose to approximate the structured prediction problem and to efficiently solve the approximated problem exactly using message-passing. Intuitively, we suggest a principled way to run the approximate inference updates for few steps, while re-using the messages of previous steps to extract intermediate beliefs. These beliefs are used to update  X  r , although the intermediate beliefs may not agree on their marginal probabilities. This allows us to efficiently learn graphical models with large number of parameters. The structured prediction objective in (3) and its gradients defined in (4) cannot be computed ef-ficiently for general graphs since both involve computing the soft-max function, ln Z ( x,y ) , and many elements  X  y  X  Y . In the following we suggest an intuitive approximation for structured pre-diction, based on its dual formulation.
 Since the dual of the soft-max is the entropy barrier, it follows that the dual program for structured prediction is governed by the entropy function of the probabilities p x,y ( X  y ) . The following duality formulation is known for CRFs when = 1 with ` 2 2 regularization, and for structured SVM when = 0 with ` 2 2 regularization, [11, 21, 1]. Here we derive the dual program for every and every ` p p regularization using conjugate duality: Claim 1 The dual program of the structured prediction program in (3) takes the form Proof: In [6] When = 1 the CRF dual program reduces to the well-known duality relation between the log-likelihood and the entropy. When = 0 we obtain the dual formulation of structured SVM which emphasizes the duality relation between the max-function and the probability simplex. In general, Claim 1 describes the relation between the soft-max function and the entropy barrier over the prob-ability simplex.
 The dual program in Claim 1 considers the probabilities p x,y ( X  y ) over exponentially many labels  X  y  X  Y , as well as their entropies H ( p x,y ) . However, when we take into account the graphical model imposed by the features, G r,x , we observe that the linear terms in the dual formulation con-probabilities with their corresponding beliefs, and to replace the entropy term by the local entropies P  X  c  X  H ( b x,y, X  ) + P v c v H ( b x,y,v ) over the beliefs. Whenever ,c v ,c  X   X  0 , the approximated dual is concave and it corresponds to a convex dual program. By deriving its dual we obtain our approximated structured prediction, for which we construct an efficient algorithm in Section 5. Figure 1: Gaussian and bimodal noise : Comparison of our approach to loopy belief propaga-tion and mean field approximations when optimizing using BFGS, SGD and SMD. Note that our approach significantly outperforms all the baselines. MF-SMD did not work for Bimodal noise. Theorem 1 The approximation of the structured prediction program in (3) takes the form Proof: In [6] In the following we describe a block coordinate descent algorithm for the approximated structured prediction program of Theorem 1. Coordinate descent methods are appealing as they optimize a small number of variables while holding the rest fixed, therefore they are efficient and can be easily parallelized. Since the primal program is lower bounded by the dual program, the primal objective function is guaranteed to converge. We begin by describing how to find the optimal set of variables every ( x,y )  X  X  .
 Lemma 1 Given a vertex v in the graphical model, the optimal  X  x,y,v  X   X  ( X  y v ) for every  X   X  N ( v ) ,  X  y v  X  X  v , ( x,y )  X  X  in the approximated program of Theorem 1 satisfies  X   X  are zero then  X  x,y, X   X  v corresponds to the `  X  norm and can be computed by the max-function. Moreover, if either and/or c  X  are zero in the objective, then the optimal  X  x,y,v  X   X  can be computed for any arbitrary c  X  &gt; 0 , and similarly for c v &gt; 0 .
 Proof: In [6] It is computationally appealing to find the optimal  X  x,y,v  X   X  ( X  y v ) . When the optimal value cannot be found, one usually takes a step in the direction of the negative gradient and the objective function needs to be computed to ensure that the chosen step size reduces the objective. Obviously, com-puting the objective function at every iteration significantly slows the algorithm. When the optimal  X  x,y,v  X   X  ( X  y v ) can be found, the block coordinate descent algorithm can be executed efficiently in distributed manner, since every  X  x,y,v  X   X  ( X  y v ) can be computed independently. The only interactions occur when computing the normalization step c x,y,v  X   X  . This allows for easy computation in GPUs. We now turn to describe how to change  X  in order to improve the approximated structured prediction. Since we cannot find the optimal  X  r while holding the rest fixed, we perform a step in the direction of the negative gradient, when ,c  X  ,c i are positive, or in the direction of the subgradient otherwise. We choose the step size  X  to guarantee a descent on the objective.
 Lemma 2 The gradient of the approximated structured prediction program in Theorem 1 with re-spect to  X  r equals to where However, if either and/or c  X  equal zero, then the beliefs b x,y, X  ( X  y  X  ) can be taken from the  X  y whenever and/or c v equal zero.
 Proof: In [6] Lemmas 1 and 2 describe the coordinate descent algorithm for the approximated structured predic-tion in Theorem 1. We refer the reader to [6] for a summary of our algorithm.
 The coordinate descent algorithm is guaranteed to converge, as it monotonically decreases the ap-proximated structured prediction objective in Theorem 1, which is lower bounded by its dual pro-gram. However, convergence to the global minimum cannot be guaranteed in all cases. In particular, for = 0 the coordinate descent on the approximated structured SVMs is not guaranteed to converge to its global minimum, unless one uses subgradient methods which are not monotonically decreas-ing. Moreover, even when we are guaranteed to converge to the global minimum, i.e., ,c  X  ,c v &gt; 0 , the sequence of variables  X  x,y,v  X   X  ( X  y v ) generated by the algorithm is not guaranteed to converge to an optimal solution, nor to be bounded. As a trivial example, adding an arbitrary constant to the variables,  X  x,y,v  X   X  ( X  y v ) + c , does not change the objective value, hence the algorithm can generate non-decreasing unbounded sequences. However, the beliefs generated by the algorithm are bounded and guaranteed to converge to the solution of the dual approximated structured prediction problem. Claim 2 The block coordinate descent algorithm in lemmas 1 and 2 monotonically reduces the approximated structured prediction objective in Theorem 1, therefore the value of its objective is guaranteed to converge. Moreover, if ,c  X  ,c v &gt; 0 , the objective is guaranteed to converge to the global minimum, and its sequence of beliefs are guaranteed to converge to the unique solution of the approximated structured prediction dual.
 Proof: In [6] The convergence result has a practical implication, describing the ways we can estimate the con-vergence of the algorithm, either by the primal objective, the dual objective or the beliefs. The approximated structured prediction can also be used for non-concave entropy approximations, such stationary points correspond to the stationary points of the approximated structured prediction and its dual. Intuitively, this statement holds since the coordinate descent algorithm iterates over points  X  x,y,v  X   X  ( X  y v ) , X  r with vanishing gradients. Equivalently the algorithm iterates over saddle points  X  ever the dual program is concave these saddle points are optimal points of the convex primal, but for non-concave dual the algorithm iterates over saddle points. This is summarized in the claim below: Claim 3 Whenever the approximated structured prediction is non convex, i.e., ,c  X  &gt; 0 and c v &lt; 0 , the algorithm in lemmas 1 and 2 is not guaranteed to converge, but whenever it converges it reaches a stationary point of the primal and dual approximated structured prediction programs.
 Proof: In [6] We performed experiments on 2D grids since they are widely used to represent images, and have many cycles. We first investigate the role of in the accuracy and running time of our algorithm, for fixed c  X  ,c v = 1 . We used a 10  X  10 binary image and randomly generated 10 corrupted samples flipping every bit with 0 . 2 probability. We trained the model using CRF, structured-SVM and our approach for = { 1 , 0 . 5 , 0 . 01 , 0 } , ranging from approximated CRFs ( = 1) to approximated structured SVM ( = 0 ) and its smooth version ( = 0 . 01 ). The runtime for CRF and structured-SVM is order of magnitudes larger than our method since they require exact inference for every training example and every iteration of the algorithm. For the approximated structured prediction, the runtime slightly increases, but it decreases for = 0 since the `  X  norm is computed efficiently using the max function. However, = 0 is less accurate than = 0 . 01 ; When the approximated structured SVM converges, the gap between the primal and dual objectives was 1 . 3 , and only 10  X  5 for &gt; 0 . This is to be expected since the approximated structured SVM is non-smooth (Claim 2), and we did not used subgradient methods to ensure convergence to the optimal solution.
 We generated test images in a similar fashion while using the same for training and testing. In this setting both CRF and structured-SVM performed well, with 2 misclassifications. For the ap-proximated structured prediction, we obtained 2 misclassifications for &gt; 0 . We also evaluated the quality of the solution using different values of for training and inference [24]. When predicting with smaller than the one used for learning the results are marginally worse than when predicting with the same . However, when predicting with larger , the results get significantly worse, e.g., learning with = 0 . 01 and predicting with = 1 results in 10 errors, and only 2 when = 0 . 01 . The main advantage of our algorithm is that it can efficiently learn many parameters. We now com-pared in a 5  X  5 dataset a model learned with different parameters for every edge and vertex (  X  300 parameters) and a model learned with parameters shared among the vertices and edges (2 parameters for edges and 2 for vertices) [9]. Using large number of parameters increases performance: sharing parameters resulted in 16 misclassifications, while optimizing over the 300 parameters resulted in 2 errors. Our algorithm avoids overfitting in this case, we conjecture it is due to the regularization. We now compare our approach to state-of-the-art CRF solvers on the binary image dataset of [9] that consists of 4 different 64  X  64 base images. Each base image was corrupted 50 times with each type of noise. Following [23], we trained different models to denoise each individual image, using 40 examples for training and 10 for test. We compare our approach to approximating the conditional likelihood using loopy belief propagation (LBP) and mean field approximation (MF). For each of these approximations, we use stochastic gradient descent (SGD), stochastic meta-descent (SMD) and BFGS to learn the parameters. We do not report pseudolikelihood (PL) results since it did not work. The same behavior of PL was noticed by [23]. To reduce the computational complexity and the chances of convergence, [9, 23] forced their parameters to be shared across all nodes such that the full flexibility of the graph and learn more than 10 , 000 parameters. This is computationally prohibitive with the baselines. We use the pixel values as node potentials and an Ising model with only bias for the edge potentials, i.e.,  X  i,j = [1 ,  X  1;  X  1 , 1] . For all experiments we use = 1 , and p = 2 . For the baselines, we use the code, features and optimal parameters of [23].
 Under the first noise model, each pixel was corrupted via i.i.d. Gaussian noise with mean 0 and stan-Note that our approach outperforms considerably the loopy belief propagation and mean field ap-proximations for all optimization criteria (BFGS, SGD, SMD). For example, for the first base image the error of our approach is 0 . 0488% , which is equivalent to a 2 pixels error on average. In contrast the best baseline gets 112 pixels wrong on average. Fig. 2 (left) depicts test examples as well as our denoising results. Note that our approach is able to cope with large amounts of noise.
 Under the second noise model, each pixel was corrupted with an independent mixture of Gaussians. For each class, a mixture of 2 Gaussians with equal mixing weights was used, yielding the Bimodal deviation b . Fig. 1 depicts test error in (%) for the different base images. As before, our approach outperforms all the baselines. We do not report MF-SMD results since it did not work. Denoised images are shown in Fig. 2 (right). We now show how our algorithm converges in a few iterations. Fig. 3 depicts the primal and dual training errors as a function of the number of iterations. Note that our algorithm converges, and the dual and primal values are very tight after a few iterations. For the special case of CRFs, the idea of approximating the entropy function with local entropies appears in [24, 3]. In particular, [24] proved that using a concave entropy approximation gives robust prediction. [3] optimized the non-concave Bethe entropy c  X  = 1 ,c v = 1  X  X  N ( v ) | , by repeatedly maximizing its concave approximation, thus converging in few concave iterations. Our work differs approximated program ( c  X  ,c v &gt; 0 ) and our framework and algorithm include structured SVMs, as well as their smooth approximation when  X  0 .
 Some forms of approximated structured prediction were investigated for the special cases of CRFs. regularization, i.e., C = 0 . As a result the local log-partition functions are unrelated, and efficient counting algorithm can be used for learning. In [3] a different approximated program was derived for c  X  = 1 ,c v = 0 which was solved by the BFGS convex solver. Our work is different as it considers efficient algorithms for approximated structured prediction which take advantage of the graphical model by sending messages along its edges. We show in the experiments that this significantly improves the run-time of the algorithm. Also, our approximated structured prediction includes as special cases approximated CRF, for = 1 , and approximated structured SVM, for = 0 . More-over, we describe how to smoothly approximate the structured SVMs to avoid the shortcomings of subgradient methods, by simply setting  X  0 .
 Some forms of approximated structured SVMs were dealt in [19] with the structured SMO algo-rithm. Independently, [14] presented an approximated structured SVMs program and a message passing algorithm, which reduce to Theorem 1 and Lemma 1 with = 0 and c  X  = 1 ,c v = 1 . However, in this algorithm the messages are not guaranteed to be bounded. They main difference of [14] from our work is that they lack the dual formulation, which we use to prove that the structured SVM smooth approximation, with  X  0 , is guaranteed to converge to optimum and that the dual variables, i.e. the beliefs, are guaranteed to converge to the optimal beliefs. The relation between the margin and the soft-max is similar to the one used in [17]. Independently, [4, 15] described the connection between structured SVMs loss and CRFs loss. [15] also presented the one-parameter extension of CRFs and structured SVMs described in (3). In this paper we have related CRFs and structured SVMs and shown that the soft-max, a variant of the log-partition function, approximates smoothly the structured SVM hinge loss. We have also pro-posed an approximation for structured prediction problems based on local entropy approximations and derived an efficient message-passing algorithm that is guaranteed to converge, even for general graphs. We have demonstrated the effectiveness of our approach to learn graphs with large number of parameters.We plan to investigate other domains of application such as image segmentation.
