 The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine trans-lation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004).

Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural lan-guage processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when ap-plied to HMMs for unsupervised part of speech tag-ging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which in-volves finding a segmentation of the input sentences into phrasal units, it is particularly important to con-trol the tendency of EM to choose longer phrases, which explain the training data well but are unlikely to generalize.

However, most state-of-the-art machine transla-tion systems today are built on the basis of word-level alignments of the type generated by GIZA++ from the IBM Models and the HMM. Overfitting is also a problem in this context, and improving these word alignment systems could be of broad utility in machine translation research.

Moore (2004) discusses details of how EM over-fits the data when training IBM Model 1. He dis-covers that the EM algorithm is particularly suscep-tible to overfitting in the case of rare words, due to the  X  X arbage collection X  phenomenon. Suppose a sentence contains an English word e 1 that occurs nowhere else in the data, and its French transla-tion f 1 . Suppose that same sentence also contains a word e 2 which occurs frequently in the overall data but whose translation in this sentence, f 2 , co-occurs with it infrequently. If the translation t ( f 2 | e 2 ) oc-curs with probability 0 . 1 , then the sentence will have a higher probability if EM assigns the rare word and its actual translation a probability of t ( f 1 | e 1 ) = 0 . 5 , and assigns the rare word X  X  translation to f 2 a prob-ability of t ( f 2 | e 1 ) = 0 . 5 , than if it assigns a proba-bility of 1 to the correct translation t ( f 1 | e 1 ) . Moore suggests a number of solutions to this issue, includ-ing add-n smoothing and initializing the probabili-ties based on a heuristic rather than choosing uni-form probabilities. When combined, his solutions cause a significant decrease in alignment error rate (AER). More recently, Mermer and Saraclar (2011) have added a Bayesian prior to IBM Model 1 us-ing Gibbs sampling for inference, showing improve-ments in BLEU scores.

In this paper, we describe the results of incorpo-rating variational Bayes (VB) into the widely used GIZA++ software for word alignment. We use VB both because it converges more quickly than Gibbs sampling, and because it can be applied in a fairly straightforward manner to all of the models imple-mented by GIZA++. In Section 2, we describe VB in more detail. In Section 3, we present results for VB for the various models, in terms of perplexity of held-out test data, alignment error rate (AER), and the BLEU scores which result from using our ver-sion of GIZA++ in the end-to-end phrase-based ma-chine translation system Moses. Beal (2003) gives a detailed derivation of a varia-tional Bayesian algorithm for HMMs. The result is a very slight change to the M step of the original EM algorithm. During the M step of the original al-gorithm, the expected counts collected in the E step are normalized to give the new values of the param-eters: The variational Bayesian M step performs an inexact normalization, where the resulting parameters will add up to less than one. It does this by passing the expected counts collected in the E step through the function f ( v ) = exp(  X  ( v )) , where  X  is the digamma function, and  X  is the hyperparameter of the Dirichlet prior (Johnson, 2007): This modified M step can be applied to any model which uses a multinomial distribution; for this rea-son, it works for the IBM Models as well as HMMs, and is thus what we use for GIZA++.

In practice, the digamma function has the effect of subtracting 0 . 5 from its argument. When  X  is set to a low value, this results in  X  X nti-smoothing X . For the translation probabilities, because about 0 . 5 is subtracted from the expected counts, small counts corresponding to rare co-occurrences of words will be penalized heavily, while larger counts will not be affected very much. Thus, low values of  X  cause the algorithm to favor words which co-occur fre-quently and to distrust words that co-occur rarely. In this way, VB controls the overfitting that would otherwise occur with rare words. On the other hand, higher values of  X  can be chosen if smoothing is de-sired, for instance in the case of the alignment prob-abilities, which state how likely a word in position i of the English sentence is to align to a word in po-sition j of the French sentence. For these probabili-ties, smoothing is important because we do not want to rule out any alignment altogether, no matter how infrequently it occurs in the data.

We implemented VB for the translation probabil-ities as well as for the position alignment probabili-ties of IBM Model 2. We discovered that adding VB for the translation probabilities improved the perfor-mance of the system. However, including VB for the alignment probabilities had relatively little ef-fect, because the alignment table in its original form does some smoothing during normalization by inter-polating the counts with a uniform distribution. Be-cause VB can itself be a form of smoothing, the two versions of the code behave similarly. We did not experiment with VB for the distortion probabilities of the HMM or Models 3 and 4, as these distribu-tions have fewer parameters and are likely to have reliable counts during EM. Thus, in Section 3, we present the results of using VB for the translation probabilities only. First, we ran our modified version of GIZA++ on a simple test case designed to be similar to the exam-ple from Moore (2004) discussed in Section 1. Our test case, shown in Table 1, had three different sen-tence pairs; we included nine instances of the first, two instances of the second, and one of the third.
Human intuition tells us that f 2 should translate to e 2 and f 1 should translate to e 1 . However, the EM algorithm without VB prefers e 1 as the translation of f 2 , due to the  X  X arbage collection X  phenomenon described above. The EM algorithm with VB does not overfit this data and prefers e 2 as f 2  X  X  translation.
For our experiments with bilingual data, we used three language pairs: French and English, Chi-nese and English, and German and English. We used Canadian Hansard data for French-English, Europarl data for German-English, and newswire data for Chinese-English. For measuring align-ment error rate, we used 447 French-English sen-tences provided by Hermann Ney and Franz Och containing both sure and possible alignments, while for German-English we used 220 sentences pro-vided by Chris Callison-Burch with sure alignments only, and for Chinese-English we used the first 400 sentences of the data provided by Yang Liu, also with sure alignments only. For computing BLEU scores, we used single reference datasets for French-English and German-English, and four references for Chinese-English. For minimum error rate train-ing, we used 1000 sentences for French-English, 2000 sentences for German-English, and 1274 sen-tences for Chinese-English. Our test sets con-tained 1000 sentences each for French-English and German-English, and 686 sentences for Chinese-English. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words.

We ran our code on ten thousand sentence pairs to determine the best value of  X  for the transla-tion probabilities t ( f | e ) . For our training, we ran GIZA++ for five iterations each of Model 1, the HMM, Model 3, and Model 4. Variational Bayes was only used for Model 1. Figure 1 shows how VB, and different values of  X  in particular, affect the per-formance of GIZA++ in terms of AER. We discover that, after all training is complete, VB improves the performance of the overall system, lowering AER (Figure 1) for all three language pairs. We find that low values of  X  cause the most consistent improve-ments, and so we use  X  = 0 for the translation prob-abilities in the remaining experiments. Note that, while a value of  X  = 0 does not define a proba-bilistically valid Dirichlet prior, it does not cause any practical problems in the update equation for VB.
Figure 2 shows the test perplexity after GIZA++ has been run for twenty-five iterations of Model 1: without VB, the test perplexity increases as training continues, but it remains stable when VB is used. Thus, VB eliminates the need for the early stopping that is often employed with GIZA++.

After choosing 0 as the best value of  X  for the translation probabilities, we reran the test above (five iterations each of Models 1, HMM, 3, and 4, with VB turned on for Model 1) on different amounts of data. We found that the results for larger data sizes were comparable to the results for ten thousand sentence pairs, both with and without VB (Figure 3).

We then tested whether VB should be used for the later models. In all of these experiments, we ran Models 1, HMM, 3, and 4 for five iterations each, training on the same ten thousand sentence pairs that we used in the previous experiments. In Table 2, we show the performance of the system when no VB is used, when it is used for each of the four models in-dividually, and when it is used for all four models simultaneously. We saw the most overall improve-ment when VB was used only for Model 1; using VB for all four models simultaneously caused the most improvement to the test perplexity, but at the cost of the AER.
 For the MT experiments, we ran GIZA++ through Moses, training Model 1, the HMM, and Model 4 on 100,000 sentence pairs from each language pair. We ran three experiments, one with VB turned on for all models, one with VB turned on for Model 1 only, and one (the baseline) with VB turned off for all models. When VB was turned on, we ran GIZA++ for five iterations per model as in our earlier tests, but when VB was turned off, we ran GIZA++ for only four iterations per model, having determined that this was the optimal number of iterations for baseline system. VB was used for the translation probabilities only, with  X  set to 0 .

As can be seen in Table 3, using VB increases the BLEU score for all three language pairs. For French, the best results were achieved when VB was used for Model 1 only; for Chinese and German, on the other hand, using VB for all models caused the most improvements. For French, the BLEU score increased by 0.20; for German, it increased by 0.82; for Chinese, it increased by 1.05. Overall, VB seems to have the greatest impact on the language pairs that are most difficult to align and translate to begin with. We find that applying variational Bayes with a Dirichlet prior to the translation models imple-mented in GIZA++ improves alignments, both in terms of AER and the BLEU score of an end-to-end translation system. Variational Bayes is especially beneficial for IBM Model 1, because its lack of fer-tility and position information makes it particularly susceptible to the garbage collection phenomenon. Applying VB to Model 1 alone tends to improve the performance of later models in the training se-quence. Model 1 is an essential stepping stone in avoiding local minima when training the following models, and improvements to Model 1 lead to im-provements in the end-to-end system.
