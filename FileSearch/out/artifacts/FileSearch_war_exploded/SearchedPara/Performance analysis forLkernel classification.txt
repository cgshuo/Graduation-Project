 the form parameter.
 estimation described in [4, 5].
 gathered in an appendix. label. From decision theory, the optimal classifier has the form of densities X  d  X  ( x ) := f + ( x )  X   X f  X  ( x ) .
 The class-conditional densities are modelled using the Gaussian kernel as with constraints  X  = (  X  1 , . . . ,  X  n )  X  A where The Gaussian kernel is defined as The ISE associated with  X  is Now, b  X  is defined as and the final classifier will be 2.1 Estimation of H (  X  ) densities can be expressed as Then, where where N + = | I + | , N  X  = | I  X  | . Then, the estimate of H (  X  ) is H n (  X  ) = 2.2 Optimization term H n (  X  ) in (3) is linear in  X  and can be expressed as program (QP) variant of the Sequential Minimal Optimization (SMO) algorithm [3]. N  X  as deterministic variables n + and n  X  such that n +  X  X  X  and n  X   X  X  X  as n  X  X  X  . 3.1 Concentration inequality for H n (  X  ) Lemma 1. Conditioned on X i , b h i is an unbiased estimator of h ( X i ) , i.e, Furthermore, for any  X  &gt; 0 , where c = 2 3.2 Oracle Inequality best possible kernel classifier.
 Theorem 1. Let  X  &gt; 0 and set  X  =  X  (  X  ) = 2 n 2  X   X  Proof. From Lemma 1, with probability at least 1  X   X  for all  X   X  A , we have 3.3 ISE consistency Next, we have a theorem stating that ISE ( b  X  ) converges to zero in probability. due to space limitations. 3.4 Bayes Error Consistency the probability of error.
 f error converges to the Bayes error L  X  in probability as n  X  X  X  . The proof is given in Appendix A.2. 3.5 Application to density estimation kernel density estimate of f ( x ) is defined as with b  X  i  X  X  optimized such that corollary.
 H then in probability. consistency for  X  not necessarily tending to zero. A.1 Proof of Lemma 1 For i  X  I + , By Hoeffding X  X  inequality [16], the first term in (7) is P The second term in (7) is Therefore, In a similar way, it can be shown that for i  X  I  X  , Then, P
 X   X  P = P  X  P = P = P  X   X  n + = n A.2 Proof of Theorem 3 From Theorem 3 in [17], it suffices to show that in probability. Since from the triangle inequality vergence of  X  to  X   X  can be easily shown from the strong law of large numbers. event D =  X  ( n + , n  X  ) proves the theorem.

