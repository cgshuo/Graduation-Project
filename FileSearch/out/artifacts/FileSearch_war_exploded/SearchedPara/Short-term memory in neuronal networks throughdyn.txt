
Interdisciplinary Center for Neural Computation, Hebrew University, Jerusalem 91904, Israel How neuronal networks can store a memory trace for recent sequences of stimuli is a central question in theoretical neuroscience. The influential idea of attractor dynamics [1], suggests how single recurrent networks. But, such simple fixed points are incapable of storing sequences. More recent proposals [2, 3, 4] suggest that recurrent networks could store temporal sequences of inputs in their ongoing, transient activity, even if they do not have nontrivial fixed points. In principle, past inputs could be read out from the instantaneous activity of the network. However, the theoretical principles underlying the ability of recurrent networks to store temporal sequences in their transient dynamics are poorly understood. For example, how long can memory traces last in such networks, and how does memory capacity depend on parameters like network size, connectivity, or input statistics? Several recent theoretical studies have made progress on these issues in the case of linear neuronal networks and gaussian input statistics. Even in this simple setting, the relationship between the memory properties of a neural network and its connectivity is nonlinear, and so understanding this relationship poses an interesting challenge. Jaeger [4] proved a rigorous sum-rule (reviewed in more detail below) which showed that even in the absence of noise, no recurrent network can remember inputs for an amount of time that exceeds the number of neurons (in units of the neuronal time constant) in the network. White et al. [5] showed that in the presence of noise, a special class of  X  X rthogonal X  networks, but not generic recurrent networks, could have memory that scales with network size. And finally, Ganguli et. al. [6] used the theory of Fisher information to show that the memory of a recurrent network cannot exceed that of an equivalent feedforward network, at least for times up to the network size, in units of the neuronal time constant.
 A key reason theoretical progress was possible in these works was that even though the optimal estimate of past inputs was a nonlinear function of the network connectivity, it was still a linear function of the current network state, due to the gaussianity of the signal (and possible noise) and the linearity of the dynamics. It is not clear for example, how these results would generalize to nongaussian signals, whose reconstruction from the current network state would require nonlinear operations. Here we report theoretical progress on understanding the memory capacity of linear recurrent networks for an important class of nongaussian signals, namely sparse signals. Indeed a wide variety of temporal signals of interest are sparse in some basis, for example human speech in a wavelet basis. We use ideas from compressed sensing (CS) to define memory curves which capture the decay of memory traces in neural networks for sparse signals, and provide methods to compute these curves analytically. We find strikingly different properties of memory curves in the sparse setting compared to the gaussian setting. Although motivated by the problem of memory, we also contribute new results to the field of CS itself, by introducing and analyzing new classes of CS measurement matrices derived from dynamical systems. Our main results are summarized in the discussion section. In the next section, we begin by reviewing more quantitatively the problem of short-term memory in neuronal networks, compressed sensing, and the relation between the two. Consider a discrete time network dynamics given by Here a scalar, time dependent signal s 0 ( n ) drives a recurrent network of N neurons. x ( n )  X  R N is the network state at time n , W is an N  X  N recurrent connectivity matrix, and v is a vector of feedforward connections from the signal into the network. We choose v to have norm 1 , and we demand that the dynamics be stable so that if  X  is the squared magnitude of the largest eigenvalue of W , then  X  &lt; 1 . If we think of the signal history { s 0 ( n  X  k ) | k  X  0 } as an infinite dimensional temporal vector s 0 whose k  X  X h component s 0 k is s ( n  X  k ) , then the current network state x is linearly related to s through the effective N by  X  measurement matrix A , i.e. x = As 0 , where the matrix elements which the dynamical system in (1) can remember the past can then be quantified by how well one with covariance  X  s 0 k s 0 l  X  =  X  k,l , the optimal, minimum mean squared error estimate  X  s of the signal history is given by  X  s = A T ( AA T )  X  1 x . The correlation between the estimate  X  s k and the true signal s whose decay as k increases quantifies the decay of memory for past inputs in (1). Jaeger proved an important sum-rule for M ( k ) : P  X  k =0 M ( k ) = N for any recurrent connectivity W and feedforward connectivity v . Given that M ( k ) cannot exceed 1 for any k , an important consequence of this sum-rule is that it is not possible to recover an input signal k timesteps into the past when k is much larger than N in the sense that  X  s k will be at most weakly correlated with s 0 k .
 Generically, one may not hope to remember sequences lasting longer than N timesteps with only N neurons, but in the case of temporally sparse inputs, the field of compressed sensing (CS) suggests this may be possible. CS [7, 8] shows how to recover a sparse T dimensional signal s 0 , in which only a fraction f of the elements are nonzero, from a set of N linear measurements x = As 0 where A is an N by T measurement matrix with N &lt; T . One approach to recovering an estimate  X  s of s 0 from x involves L 1 minimization, which finds the sparsest signal, as measured by smallest L 1 norm, consistent with the measurement constraints. Much of the seminal work in CS [9, 10, 11] has focused on sufficient conditions on A such that (3) is guaranteed to perfectly recover the true signal, so that  X  s = s 0 . However, many large random measurement matrices A which violate sufficient conditions proven in the literature still nevertheless typically yield perfect signal recovery. Alternate work [12, 13, 14, 15] which analyzes the asymptotic performance of large random measurement matrices in which each matrix element is drawn i.i.d. from a gaussian distribution, has revealed a phase transition in performance as a function the signal sparsity f and the degree of subsampling  X  = N/T . In the  X  -f plane, there is a critical phase boundary  X  c ( f ) such that if  X  &gt;  X  c ( f ) then CS will typically yield perfect signal reconstruction, whereas if  X  &lt;  X  c ( f ) , CS will yield errors.
 Motivated by the above work in CS, we propose here that a neural network, or more generally any dynamical system as in (1), could in principle perform compressed sensing of its past inputs, and that a long but sparse signal history s 0 could potentially be recovered from the instantaneous network state x . We quantify the memory capabilities of a neural network for sparse signals, by assessing our ability to reconstruct the past signal using L 1 minimization. Given a network state x arising from a signal history s 0 through (1), we can obtain an estimate  X  s of the past using (3), where the measurement matrix A is given by (2). We then define a memory curve namely the average reconstruction error of a signal k timesteps in the past averaged over the statistics of s 0 . The rise of this error as k increases captures the decay of memory traces in (1). The central goal of this paper is to obtain a deeper understanding of the memory properties of neural networks for sparse signals by studying the memory curve E ( k ) and especially its dependence on W . In particular, we are interested in classes of network connectivities W and input statistics for which E ( k ) can remain small even for k N . Such networks can essentially perform compressed sensing of their past inputs.
 From the perspective of CS, measurement matrices A of the form in (2), henceforth referred to as dynamical CS matrices, possess several new features not considered in the existing CS literature, features which could pose severe challenges for a recurrent network W to achieve good CS per-formance. First, A is an N by  X  matrix, and so from the perspective of the phase diagram for CS reviewed above, it is likely that A is in the error phase; thus perfect reconstruction of the true signal, even for recent inputs will not be possible. Second, because we demand stable dynamics in (1), the columns of A decay as k increases: || W k v || 2 &lt;  X  k where again  X  &lt; 1 is the squared magnitude of the largest eigenvalue of W . Such decay can compound errors. Third, the different columns of A can be correlated; if one thinks of W k v as the state of the network k timesteps after a single unit input pulse, it is clear that temporal correlations in the evolving network response to this pulse are equivalent to correlations in the columns of A in (2). Such correlations could potentially adversely affect the performance of CS based on A , as well as complicate the theoretical analysis of CS performance. Nevertheless, despite all these seeming difficulties, in the following we show that a special class of network connectivities can indeed achieve good CS performance in which errors are controlled and memory traces can last longer than the number of neurons. In this section, we work towards an analytic understanding of the memory curve E ( k ) defined in (4). This curve depends on W , v and the statistics of s 0 . We would like to understand its prop-erties for ensembles of large random networks W , just as the asymptotic performance of CS was analyzed for large random measurement matrices A [12, 13, 14, 15]. However, in the dynamical setting, even if W is drawn from a simple random matrix ensemble, A in (2) will have correlations across its columns, making an analytical treatment of the memory curve difficult. Here we consider an ensemble of measurement matrices A which approximate dynamical CS matrices and can be treated analytically. We consider matrices in which each element A  X k is drawn i.i.d from a zero mean gaussian distribution with variance  X  k . Since we are interested in memory that lasts O ( N ) timesteps, we choose  X  = e  X  1 / X N , with  X  O (1) . This so called annealed approximation (AA) to a dynamical CS matrix captures two of the salient properties of dynamical CS matrices, their infinite temporal extent and the decay of successive columns, but neglects the analytically intractable corre-lations across columns. Such annealed CS matrices can be thought of as arising from  X  X maginary X  dynamical systems in which network activity patterns over time in response to a pulse decay, but are somehow temporally uncorrelated.  X  can be thought of as the effective integration time of this dynamical system, in units of the number of neurons. Finally, to fully specify E ( k ) , we must choose the statistics of s 0 . We assume s 0 has a probability f of being nonzero at any given time, and if nonzero, this nonzero value is drawn from a distribution P ( s ) which for now we take to be arbitrary. To theoretically compute the memory curve E ( k ) , we define an energy function where u  X  s  X  s 0 is the residual, and we consider the Gibbs distribution P G ( s ) = 1 Z e  X   X E ( s ) . We will later take  X   X   X  so that the quadratic part of the energy function enforces the constraint As = As 0 , and then take the low temperature  X   X   X  limit so that P G ( s ) concentrates onto the global minimum of (3). In this limit, we can extract the memory curve E ( k ) as the average of ( s k  X  s 0 k ) 2 over P G and the statistics of s 0 . Although P G depends on A , for large N , the properties of P
G , including the memory curve E ( k ) , do not depend on the detailed realization of A , but only on its statistics. Indeed we can compute all properties of P G for any typical realization of A by averaging over both A and s 0 . This is done using the replica method [16] in our supplementary material. The replica method has been used recently in several works to analyze CS for the traditional case of uniform random gaussian measurement matrices [14, 17, 15]. We find that the statistics of each component s k in P G ( s ) , conditioned on the true value s 0 k is well described by a mean field effective Hamiltonian where z is a random variable with a standard normal distribution. Thus the mean field approximation to the marginal distribution of a reconstruction component s k is where D z = dz e  X  1 2 z 2 is a Gaussian measure. The order parameters Q 0 and  X  Q  X  Q 1  X  Q 0 obey spect to a Gibbs distribution with Hamiltonian given by (6), and the double angular average  X  X  X  X  X  z refers to integrating over the Gaussian distribution of z . Q 1 and Q 0 have simple interpretations in terms of the original Gibbs distribution P G defined above: Q 1 = 1 N P  X  k =1  X  k  X  u 2 k  X  P Q can be understood as self-consistency conditions for the definition of Q 0 and  X  Q in the mean field approximation to P G . In this approximation, the complicated constraints coupling s k for various k are replaced with a random gaussian force z in (6) which tends to prevent the marginal s k from as-suming the true value s 0 k . This force is what remains of the measurement constraints after averaging over A , and its statistics are in turn a function of Q 0 and Q 1 , as determined by the replica method. Now to compute the memory curve E ( k ) , we must take the limits  X , X ,N  X   X  and complete the average over s 0 k . The  X   X   X  limit can be taken immediately in (6) and  X  disappears from the problem. Now as  X   X   X  , self consistent solutions to (8) and (9) can be found when Q 0  X  q 0 and  X  Q  X   X  q/ X  , where q 0 and  X  q are O (1) . This limit is similar to that taken in a replica analysis of CS for random gaussian matrices in the error regime [15]. Taking this limit, (6) becomes Since the entire Hamiltonian is proportional to  X  , in the large  X  limit, the statistics of s k are domi-nated by the global minimum of (10). In particular, we have where is a soft thresholding function which also arises in message passing approaches [18] to solving the CS problem in (3), and ( y ) + = y if y &gt; 0 and is otherwise 0 . The optimization in (12) can be understood intuitively as follows: suppose one measures a scalar value x which is a true signal s 0 corrupted by additive gaussian noise with variance  X  . Under a Laplace prior e  X  X  s 0 | on the true signal,  X  ( x, X  ) is simply the MAP estimate of s 0 given the data x , which basically chooses the estimate s = 0 unless the data exceeds the noise level  X  . Thus we see that in (10),  X   X  k  X  q plays the role of an effective noise level which increases with time k . Also, the variance of s at large  X  is where and  X ( x ) is a step function at 0 . Inserting (11) and (13) and the ansatz  X  Q  X   X  q/ X  into (8) and (9) then removes  X  from the problem. But before making these substitutions, we first take N  X  X  X  at fixed  X  and f of O (1) by taking a continuum approximation for time, t = k/N ,  X  k  X  e  X  t/ X  , become, where the double angular average reflects an integral over the gaussian distribution of z and the full distribution of s 0 , i.e. F ( z,s 0 ) z,s 0  X  (1  X  f ) R D z F ( z, 0) + f R D z ds 0 P ( s 0 ) F ( z,s 0 ) . Finally the memory curve E ( t ) is simply the continuum limit of the averaged squared residual Equations (15),(16), and (17) now depend only on  X  , f and P ( s 0 ) , and their theoretical predictions can now be compared with numerical experiments. In this work we focus on a simple class of plus-minus (PM) signals in which P ( s 0 ) = 1 / 2  X  ( s 0  X  1) + 1 / 2  X  ( s 0 + 1) . Fig. 1A shows an example of a PM signal s 0 with f = 0 . 01 , while Fig. 1B shows an example of a reconstruction of  X  s using L 1 minimization in (3) where the data x used in (3) was obtained from s 0 using a random annealed measurement matrix with  X  = 1 . Clearly there are errors in the reconstruction, but remarkably, despite the decay in the columns of A , the reconstruction is well correlated with the true signal for a time up to 4 times the number of measurements. We can derive theoretical memory curves for any given f and  X  by numerically solving for q 0 and  X  q in (15),(16), and inserting the results into (17). Examples of the agreement between theory and simulations are shown in Fig. 1C-E.
 As t  X   X  , L 1 minimization always yields a zero signal estimate, so the memory curve asymptoti-cally approaches f for large t . A convenient measure of memory capacity is the time T 1 / 2 at which the memory curve reaches half its asymptotic error value, i.e. E ( T 1 / 2 ) = f/ 2 . A principle feature Figure 1: Memory in the annealed approximation. (A) A PM signal s 0 with f = 0 . 01 that lasts T = 10 N timesteps where N = 500 . (B) A reconstruction of s 0 from the output of an annealed measurement matrix with N = 500 , X  = 1 . (C,D,E) Example memory curves for f = 0 . 01 , and  X  = 1 (C), 2 (D), 3 (E). (F) T 1 / 2 as a function of  X  . The 4 curves from top to bottom are for of f . The 3 curves from bottom to top are for  X  = 1 , 2 , 3 . For (C-H), red curves are theoretical predictions while blue curves and points are from numerical simulations of L 1 minimization with N = 100 averaged over 300 trials. The width of the blue curves reflects standard error. of this family of memory curves is that for any given f there is an optimal  X  which maximizes T 1 / 2 (Fig. 1F) . The presence of this optimum arises due to a competition between decay and interference. If  X  is too small, signal measurements decay too quickly, thereby preventing large memory capacity. However, if  X  is too large, signals from the distant past do not decay away, thereby interfering with the measurements of more recent signals, and again degrading memory. As f decreases, long time signal interference is reduced, thereby allowing larger values of  X  to be chosen without degrading memory for more recent signals. For any given f , we can compute T 1 / 2 ( f ) optimized over  X  (Fig. 1G). This memory capacity, again measured in units of the number of neurons, already exceeds 1 at modest values of f = 0 . 1 , and diverges as f  X  0 , as does the optimal value of  X  . By analyzing (15) and (16) in the limit f  X  0 and  X   X   X  , we find that  X  q is O (1) while q 0  X  0 . Furthermore, as The smallest error occurs at t = 0 and it is natural to ask how this error E (0) behaves as a function of f for small f to see how well the most recent input can be reconstructed in the limit of sparse signals. We analyze (15) and (16) in the limit f  X  0 and  X  of O (1) , and find that E (0) is O ( f 2 ) as confirmed in Fig. 1F. Furthermore, E (0) monotonically increases with  X  for fixed f as more signals from the past interfere with the most recent input. We have seen in the previous section that annealed CS matrices have remarkable memory properties, but our main interest was to exhibit a dynamical CS matrix as in (2) capable of good compressed sensing, and therefore short-term memory, performance. Here we show that a special class of net-work connectivity in which W = norm vector possesses memory properties remarkably close to that of the annealed matrix ensemble. Fig. 2A-F presents results identical to that of Fig. 1C-H except for the crucial change that all simu-lation results in Fig. 2 were obtained using dynamical CS matrices of the form A  X k = (  X  k/ 2 O k v )  X  , rather than annealed CS matrices. All red curves in Fig. 2A-F are identical to those in Fig. 1 and reflect the theory of annealed CS matrices derived in the previous section.
 For small  X  , we see small discrepancies between memory curves for orthogonal neural networks and the annealed theory (Fig. 2A-B), but as  X  increases, this discrepancy decreases (Fig. 2C). remarkable match between the optimal memory capacity of orthogonal neural networks and that predicted by the annealed theory (see Fig. 2E). And there is good match in the initial error even at small  X  (Fig. 2F). Figure 2: Memory in orthogonal neuronal networks. Panels (A-F) are identical to panels (C-H) in Fig. 1 except now the blue curves and points are obtained from simulations of L 1 minimization using measurement matrices derived from an orthogonal neuronal network. (G) The mean and standard deviation of  X  f for 5 annealed (red) and 5 orthogonal matrices (blue) with N=200 and T=3000. The key difference between the annealed and the dynamical CS matrices is that the former neglects correlations across columns that can arise in the latter. How strong are these correlations for the case of orthogonal matrices? Motivated by the restricted isometry property [11], we consider the following probe of the strength of correlations across columns of A . Consider an N by fT matrix B obtained by randomly subsampling the columns of an N by T measurement matrix A . Let  X  f be the maximal eigenvalue of the matrix B T B of inner products of columns of B .  X  f is a measure of the strength of correlations across the fT sampled columns of A . We can estimate the mean and standard deviation of  X  f due to the random choice of fT columns of A and plot the results as function of f . To separate the issue of correlations from decay, we do this analysis for  X  = 1 and finite T (similar results are obtained for large T and  X  &lt; 1 ). Results are shown in Fig 2 for 5 instances of annealed (red) and dynamical (blue) CS matrices. We see strikingly different behavior in the two ensembles. Correlations are much stronger in the dynamical ensemble, and fluctuate from instance to instance, while they are weaker in the annealed ensemble, and do not fluctuate (the 5 red curves are on top of each other). Given the very different statistical properties of the two ensembles, the level of agreement between the simulated memory properties of orthogonal neural networks, and the theory of annealed CS matrices is remarkable.
 Why do orthogonal neural networks perform so well, and can more generic networks have similar performance? The key to understanding the memory, and CS, capabilities of orthogonal neural networks lies in the eigenvalue spectrum of an orthogonal matrix. The eigenvalues of W = when O is a large random orthogonal matrix, are uniformly distributed on a circle of radius in the complex plane. Thus when  X  = e  X  1 / X N , the sequence of vectors W k v explore the full N dimensional space of network activity patterns for O (  X N ) time steps before decaying away. In contrast, a generic random Gaussian matrix W with elements drawn i.i.d from a zero mean gaussian with variance  X /N has eigenvalues uniformly distributed on a solid disk of radius plane. Thus the sequence of vectors W k v no longer explore a high dimensional space of activity patterns; components of v in the direction of eigenmodes of W with small eigenvalues will rapidly decay away, and so the sequence will rapidly become confined to a low dimensional space. Good compressed sensing matrices often have columns that are random and uncorrelated. From the above considerations, it is clear that dynamical CS matrices derived from orthogonal neural networks can come close to this ideal, while those derived from generic gaussian networks cannot. In this work we have made progress on the theory of short-term memory for nongaussian, sparse, temporal sequences stored in the transient dynamics of neuronal networks. We used the framework of compressed sensing, specifically L 1 minimization, to reconstruct the history of the past input sig-nal from the current network activity state. The reconstruction error as a function of time into the past then yields a well-defined memory curve that reflects the memory capabilities of the network. We studied the properties of this memory curve and its dependence on network connectivity, and found results that were qualitatively different from prior theoretical studies devoted to short-term memory in the setting of gaussian input statistics. In particular we found that orthogonal neural networks, but importantly, not generic random gaussian networks, are capable of remembering inputs for a time that exceeds the number of neurons in the network, thereby circumventing a theorem proven in [4], which limits the memory capacity of any network to be less than the number of neurons in the gaussian signal setting. Also, recurrent connectivity plays an essential role in allowing a network to have a memory capacity that exceeds the number of neurons. Thus purely feedforward networks, which always outperform recurrent networks (for times less than the network size) in the scenario of gaussian signals and noise [6] are no longer optimal for sparse input statistics. Finally, we exploited powerful tools from statistical mechanics to analytically compute memory curves as a function of signal sparsity and network integration time. Our theoretically computed curves matched reasonably well simulations of orthogonal neural networks. To our knowledge, these results represent the first theoretical calculations of short-term memory curves for sparse signals in neuronal networks. We emphasize that we are not suggesting that biological neural systems use L 1 minimization to reconstruct past inputs. Instead we use L 1 minimization in this work simply as a theoretical tool to probe the memory capabilities of neural networks. However, neural implementations of L 1 mini-mization exist [19, 20], so if stimulus reconstruction were the goal of a neural system, reconstruction performance similar to what is reported here could be obtained in a neurally plausible manner. Also, we found that orthogonal neural networks, because of their eigenvalue spectrum, display remark-able memory properties, similar to that of an annealed approximation. Such special connectivity is essential for memory performance, as random gaussian networks cannot have memory similar to the annealed approximation. Orthogonal connectivity could be implemented in a biologically plausible manner using antisymmetric networks with inhibition operating in continuous time. When exponentiated, such connectivities yield the orthogonal networks considered here in discrete time. Our results are relevant not only to the field of short-term memory, but also to the field of compressed sensing (CS). We have introduced two new ensembles of random CS measurement matrices. The first of these, dynamical CS matrices, are the effective measurements a dynamical system makes on a continuous temporal stream of input. Dynamical CS matrices have three properties not considered in the existing CS literature: they are infinite in temporal extent, have columns that decay over time and exhibit correlations between columns. We also introduce annealed CS matrices, that are also infinite in extent and have decaying columns, but no correlations across columns. We show how to analytically calculate the time course of reconstruction error in the annealed ensemble and compare it to the dynamical ensemble for orthogonal dynamical systems. Our results show that orthogonal dynamical systems can perform CS even while operating with errors.
 This work suggests several extensions. Given the importance of signal statistics in determining memory capacity, it would be interesting to study memory for sparse nonnegative signals. The inequality constraints on the space of allowed signals arising from nonnegativity can have important effects in CS; they shift the phase boundary between perfect and error-prone reconstruction [12, 13, 15], and they allow the existence of a new phase in which signal reconstruction is possible even without L 1 minimization [15]. We have found, through simulations, dramatic improvements in memory capacity in this case, and are extending the theory to explain these effects. Also, we have used a simple model for sparseness, in which a fraction of signal elements are nonzero. But our theory is general for any signal distribution, and could be used to analyze other models of sparsity, i.e. signals drawn from L p priors. Also, we have worked in the high SNR limit. However our theory can be extended to analyze memory in the presence of noise by working at finite  X  . But most importantly, a deeper understanding of the relationship between dynamical CS matrices and their annealed counterparts would desirable. The effects of temporal correlations in the network activity patterns of orthogonal dynamical systems is central to this problem. For example, we have seen that these temporal correlations introduce strong correlations between the columns of the corresponding dynamical CS matrix (Fig. 2G), yet the memory properties of these matrices agree well with our annealed theory (Fig. 2E-F), which neglects these correlations. We leave this observation as an intriguing puzzle for the fields of short-term memory, dynamical systems, and compressed sensing. Acknowledgments S. G. and H. S. thank the Swartz Foundation, Burroughs Wellcome Fund, and the Israeli Science Foundation for support, and Daniel Lee for useful discussions. [1] J.J. Hopfield. Neural networks and physical systems with emergent collective computational [2] W. Maass, T. Natschlager, and H. Markram. Real-time computing without stable states: A new [3] H. Jaeger and H. Haas. Harnessing nonlinearity: Predicting chaotic systems and saving energy [4] H. Jaeger. Short term memory in echo state networks. GMD Report 152 German National [5] O.L. White, D.D. Lee, and H. Sompolinsky. Short-term memory in orthogonal neural net-[6] S. Ganguli, D. Huh, and H. Sompolinsky. Memory traces in dynamical systems. Proc. Natl. [7] A.M. Bruckstein, D.L. Donoho, and M. Elad. From sparse solutions of systems of equations [8] E. Candes and M. Wakin. An introduction to compressive sampling. IEEE Sig. Proc. Mag. , [9] D.L. Donoho and M. Elad. Optimally sparse representation in general (non-orthogonal) dic-[10] E. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction [11] E. Candes and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theory , 51:4203 X  [12] D.L. Donoho and J. Tanner. Sparse nonnegative solution of underdetermined linear equations [13] D.L. Donoho and J. Tanner. Neighborliness of randomly projected simplices in high dimen-[14] Y. Kabashima, T. Wadayama, and T. Tanaka. A typical reconstruction limit for compressed [15] S. Ganguli and H. Sompolinsky. Statistical mechanics of compressed sensing. Phys. Rev. Lett. , [16] M. Mezard, G. Parisi, and M.A. Virasoro. Spin glass theory and beyond . World scientific [17] S. Rangan, A.K. Fletcher, and Goyal V.K. Asymptotic analysis of map estimation via the [18] D.L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed [19] Y. Xia and M.S. Kamel. A cooperative recurrent neural network for solving l 1 estimation [20] C.J. Rozell, D.H. Johnson, R.G. Baraniuk, and B.A. Olshausen. Sparse coding via thresholding
