 1. Introduction
Relevance has always been an equivocal concept in information retrieval (IR) evaluation, and thus it has given rise to many studies and discussions. Relevance is defined multidimensional ( Barry, 1994 ; Cuadra &amp;
Katter, 1967 ; Saracevic, 1975 ; Schamber, 1994 ), and different aspects of relevance have been recognised where the goal is to measure IR system or method performance reliably, the basis of measurement is of great interest. Typically, system evaluation is based on topical interpretation of relevance ( Cooper, 1971 ; Saracevic, 1996 ) leaving the other aspects aside. This is practical from the laboratory point-of-view since test collections are used and the relevance assessment setting is always somewhat arti-2002 ).

Given topicality, relevance may be understood as a continuous variable; thus, if an item is relevant it surement, the variable is divided into categories. The division of the scale into appropriate number of cate-gories has also been discussed ( Tang, Shaw, &amp; Vevea, 1999 ): should the number be two, three, ... or seven?
There seems to be no obvious answer, and perhaps because of this difficulty, the use of graded relevance in information retrieval (IR, for short) evaluation is not common. Binary division into relevant and irrelevant categories is prevalent in IR studies. But it is worth noting that, if compared to a scale with three or more categories, binary scale is just one option of categorization of the variable.

However, the choice of binary or graded relevance is not without consequences: these two options model different view on documents and information needs. The former claims that all relevant documents are equally valuable for the user, and the systems can be evaluated on the basis of how well they find and rank these documents in the result set; the latter claims that the value of the documents differ, and systems should be evaluated on the basis of how they find and rank documents according documents  X  degree of relevance.

The differences between binary and graded relevance could be understood analogously with differences in relevance assessments by different assessors. The effects of different relevance assessment sets on IR per-formance have been investigated in several studies, e.g. by Lesk and Salton (1968) , Burgin (1992) , Voorhees (1998) . Lesk and Salton (1968) propose two hypotheses regarding the effects of variations in relevance assessments by different assessors: the strong hypothesis claims that the absolute performance (measured as precision and recall) of any system will change only negligibly when different relevance assessments are used; the weak hypothesis claims that the ranking of the systems will not change when the relevance assessments are made by different assessors. (See also Burgin, 1992 .) Voorhees (1998) studied the effects of different assessment sets on the rankings of IR systems in Text
REtrieval Conference (TREC). She concludes that there is some variation in the order of the systems but overall the correlation between rankings is high. The results become more stable when the number of requests (topics) used in testing gets larger. She states that, in case of TREC 6, with 25 topics the results of system comparisons are reliable.
 cumulated gain, discounted cumulated gain, and their normalised versions. Voorhees (2001) compared
TREC web track results using graded relevance and binary relevance. She states that the correlation be-tween system ranking by binary relevance on the one hand, and by the graded relevance on the other is lower than the correlation between rankings based on different assessment sets by different assessors.
In 2002 Sormunen reported results of reassessment of documents from TREC-7 and -8 document pools using non-binary scale. The scale had four categories: irrelevant, marginally relevant, fairly relevant and highly relevant. Sormunen states that 50% of the relevant documents in his study were marginally relevant and only 16% were highly relevant. He suspects that taking the degree of relevance of documents into ac-count might change the IR evaluation results ( Sormunen, 2002 ).

In this paper we shall further investigate how graded relevance affects the results of IR evaluation. Espe-cially, what are the effects of different relevance weighting schemes on system ranking. By a relevance weighting scheme we refer to weights given to relevant documents at different levels of relevance. Further, we shall investigate the properties of the cumulated gain based measures. We shall compare the rankings of the IR systems produced by binary and non-binary relevance in TREC 7 and 8 data. In Section 2 our test setting will be introduced: the relevance scale, assessments and measures used for graded relevance. In Sec-2. Data and methods 2.1. Graded relevance assessments
In this study the reassessed TREC documents from Sormunen  X  s study (2002) are used. Data include 21 topics from TREC 7 and 20 topics from TREC 8 ad hoc tracks. obtained by rejudging documents judged relevant by NIST assessors and about 5% of irrelevant documents for each topic. The selection of topics was based on the size of recall bases, i.e. each topic should have more than 30 relevant documents but the size of the pool to be reassessed should not exceed 200 documents (see
Sormunen, 2002 ). Six Master  X  s students of Information Studies made the new assessments. They all are flu-ent in English though not native speakers. The relevant and irrelevant documents were pooled, and the assessors did not know the number of documents judged previously relevant or irrelevant in the pool.
The assumption about relevance in the reassessment process was topicality. This agrees with the TREC assessments for the ad hoc track: documents are judged one by one; general information with limitations given in the topic  X  s narrative is searched, not details in sense of question answering. New assessments were done on a four-point scale ( Sormunen, 2002 ): (0) Irrelevant document . The document does not contain any information about the topic. (1) Marginally relevant document . The document only points to the topic. It does not contain more or (2) Fairly relevant document . The document contains more information than the topic description but the (3) Highly relevant document . The document discusses the themes of the topic exhaustively. In case of
In Table 1 the results of reassessment (UTA relevance data) are shown with respect to the original TREC assessments (TREC relevance data). Altogether 6119 documents were reassessed. Almost all originally irrel-evant documents were also assessed irrelevant in reassessment (93.9%). Of the TREC relevant documents about 76% were judged relevant at some level, and 24% irrelevant. This seems to indicate that the re-asses-sors have been somewhat stricter than the original judges, which tendency has been observed also earlier ( Voorhees, 1998 ).

In UTA recall bases there were on average 28 documents of relevance level 1, 18 documents of relevance level 2, and 11 documents of relevance level 3 per topic, that is on average 57 relevant documents of some relevance level per topic. 2.2. Cumulated gain-based measures
The use of graded relevance assessments is not common in IR studies. Typically relevance is conflated into two categories at the analysis phase because of the calculation of precision and recall. There are, nev-ertheless, some attempts to define a measure that could be used for graded relevance. The sliding ratio mea-pose. In the present study we will use precision and recall for binary relevance, and cumulated gain (CG), discounted cumulated gain (DCG) and normalised discounted cumulated gain (nDCG) for binary and graded relevance. Traditional measures are selected to make the comparisons between the effects of
TREC and new assessments possible. Cumulated gain-based measures are selected because they support 2002 ). Further, these measures allow researchers to test different weighting schemes for relevant documents, which reflect different user scenarios. 2.2.1. Cumulated gain
Cumulated gain seeks to take into account the ranked order of the documents in the result list of IR and the degree to which the document is relevant with respect to the request. In cumulated gain, the degree of relevance of each document is used as a gained value measure for its ranked position in the result list, and the gain is summed progressively from position 1 to n . Thus the ranked document lists (of some determined length) are turned to gained value lists by replacing document IDs by their relevance values. Assume that the relevance categories 0 X 3 are used (3 denoting high value, 0 no value). Turning document lists up to rank 200 to corresponding value lists gives vectors of 200 components each having the value 0, 1, 2 or 3. For example:
The cumulated gain at ranked position i is computed by summing from position 1 to i when i ranges from 1 to 200. Formally, let us denote position i in the gain vector G by G[ i ]. Now the cumulated gain vector CG is defined recursively as the vector CG where: 2.2.2. Discounted cumulated gain
The greater the ranked position of a relevant document, the less valuable it is for the user, because the less likely it is that the user will ever examine the document due to effort, or cumulated information from documents already seen. Thus a discount factor is added to calculation: the greater the rank, the smaller share of the document score is added to the cumulated gain. A simple way of discounting with this require-ment is to divide the document score by the log of its rank. By selecting the base of the logarithm, sharper or smoother discounts can be computed to model varying user behaviour. Formally, if b denotes the base of the logarithm, the cumulated gain vector with discount DCG is defined recursively as the vector DCG where:
Note that we must not apply the logarithm-based discount at rank 1 because nen, 2002 ).

For example, let b =2.FromG 0 given in the preceding section we obtain DCG 7.28,7.99,8.66,9.61,9.61, ...  X  . By averaging over a set of test queries, the average performance of a partic-ular IR method can be analysed. Averaged vectors have the same length as the individual ones and each component i gives the average of the i th component in the individual vectors. The averaged vectors can directly be visualised as gain-by-rank-graphs. ( Ja  X  rvelin &amp; Keka  X  la  X  inen, 2002 .) 2.2.3. Ideal vectors and normalised DCG measure
The actual CG and DCG vectors by a particular IR method may also be compared to the theoretically best possible. The latter vectors are constructed by ordering the values of the most relevant documents in first positions, then adding the values of the second best documents, etc. The ideal vector is based on the recall base of the search topic.
 Continuing our example, a sample ideal gain vector could be: Based on the sample ideal gain vector I 0 , we obtain the ideal DCG ( b = 2) vector:
The DCG curves are not relative to an ideal. Therefore it is difficult to assess the magnitude of the difference of two DCG curves. The DCG vectors for each IR technique can be normalised by dividing them by the corresponding ideal DCG vectors, component by component. In this way, for any vector position, the nor-malised value 1 represents ideal performance, and values in the range [0,1) the share of ideal performance cumulated by each technique. This holds for CG-curves which can be normalised as well. However, we only use nDCG-measure in this study because it includes the discount effect we believe is important.
Given a DCG vector V =  X  v 1 , v 2 , ... , v k  X  of an IR technique, and the ideal DCG vector I =  X  i the normalised performance vector nDCG is obtained by the function:
For example, based on DCG 0 and DCG 0 I from above, we obtain the normalised DCG vector nDCG 0 = norm-vect (DCG 0 , DCG 0 I  X  X 
Normalised DCG vectors for two or more IR techniques also have a normalised difference. These can be compared in the same way as P X  X  curves for IR techniques. The average of a DCG vector (or its norma-lised variation), up to a given ranked position, summarises the vector (or performance) and is analogous to the non-interpolated average precision of a DCV curve up to the same given ranked position. ( Ja  X  rvelin &amp;
Keka  X  la  X  inen, 2002 .) 2.3. Comparison of results based on binary and graded relevance In this study we want to compare rankings of IR methods based on binary and graded relevance data.
We used topics from TREC 7 and 8 ad hoc track, which had two sub-tracks: automatic, which means deriv-ing a query from a topic statement without manual intervention; and manual, which is anything else. ( Voorhees &amp; Harman, 1999 ). In principle all result lists X  X epresenting different methods or runs X  X ere in-volved in our comparison, however, some lists had to be discarded because of technical problems. Also, if two runs from one participant had identical results the other was discarded. The number of runs are given in Table 2 . For comparison runs of the manual and automatic sub-tracks were pooled together.
Kendall  X  s tau is a measure of rank correlation suitable for comparing different rankings. It simply gives the degree of agreement or concordance between two rankings: If concordance is perfect, the correlation is +1, if ordering in two rankings is reverse, the correlation is 1, value 0 denotes no correlation ( Conover, 1980, p. 256 ). Voorhees (2001) notes that Kendall  X  s tau is not a measure of significance and does not have a critical value, which could show the importance of differences. She points out that all systems strive to re-trieve good documents, thus, correlations much higher than 0 are to be expected. According to her corre-lations greater than 0.9 should be considered equivalent and those  X  less than 0.8 generally reflect noticeable
As a justification for this she refers to her study ( Voorhees, 2000 ) comparing rankings based on relevance assessments by different assessors ( Voorhees, 2001 ).

There are four different questions we want to answer with four different comparisons. First, we shall find out if the rankings of systems differ when they are based on TREC judgments on the one hand, and on
UTA judgements on the other. We shall calculate Kendall correlations for the rankings of runs based on original binary TREC relevance data, and UTA relevance data, which was conflated to binary values, in order to find out does the change of assessors as such cause notable reordering in ranking. In this com-parison we rank systems according to precision.

Second, we want to investigate how do the rankings based on binary precision and different cumulated gain based measures correlate. We shall compare rankings based on UTA precision and UTA cumulated gain measures with different weighting schemes. These schemes indicate how different relevance grades are weighted. This comparison is between the different UTA relevance based measurements.

Third, we shall compare different cumulated gain based measures with different weighting schemes in order to see to what extent they correlate. That is, are the rankings based on different measures utilizing graded relevance similar or do they differ?
Fourth, we shall find out what is the effect of different weighting schemes within each cumulated gain based measure. This comparison clarifies how sensitive each measure is for weighting of the relevance grades; in other words, does a measure generate different rankings when the weighting scheme is changed. Last we shall illustrate the different measures and the changes in rank order as curves.

Answering these questions will give us insight into properties of different measures: do they agree on the ranking of systems? Also, and more importantly, this will clarify how sensitive the results concerning the ranking of systems are for different relevance scales and different modelling of user  X  s gain. 3. Results 3.1. Correlations between TREC and UTA binary relevance results
We ranked the runs on the basis of their average precision at document cut-off value 10 (dcv 10), 100 (dcv 100), and 1000 (dcv 1000). The lowest cut-off value shows the user-oriented high precision end, while the cut-off 1000 is the high recall end and last reliable measuring point, since the size of TREC result sub-missions is 1000 documents for each topic. Corresponding correlations are given in Table 3 . The correla-thus assume no notable difference between rankings based on UTA and TREC binary relevance. 3.2. Correlations between UTA precision and cumulated gain measures
Four different weighting schemes were applied to cumulated gain based measures, i.e. documents at dif-ferent relevance levels were given equal weight, or the weight ascended according to the relevance level. The different weighting schemes reflect the values a user is supposed to give to documents of different relevance levels. First, in binary weighting all relevant documents at any relevance level were given weight 1 (weight-ing scheme w1, for short); second, highly relevant documents were given weight 3, fairly relevant ones were given weight 2, and marginally relevant ones kept weight 1 (w3, for short); third, highly relevant documents were given weight 10, fairly relevant ones weight 5, and marginally relevant ones weight 1 (w10, for short); last, the weight of highly relevant documents was 100, the weight of fairly relevant ones was 10, and again the weight of marginally relevant ones was 1 (w100, for short). For discounting we tested to logarithm bases, 2 and 10, but the results regarding system performance differences did not differ notably. Thus, the results with the base 2 are reported here. 2
CG, DCG and nDCG measures were calculated for document cut-off values 10 and 100. These were se-lected to illustrate a user-oriented view with a small result set (dcv 10), and a more stable cut-off sibility to retrieve the most relevant documents (dcv 100) since the number of all relevant documents was lower than 100 in 35 of all 41 topics. 4
In Table 4 the correlations between (n)(D)CG measures and precision are given. CG combined to binary weighting scheme (CG-w1) gives the same ordering as precision because the latter is cumulated binary gain, or the number of relevant documents, divided by the rank, which is an order preserving transformation.
The other correlations vary being lowest between w100 and precision. Cumulated gain with binary weight-ing yields the most similar ranking with precision. Discounted cumulated gain and its normalised version are less similar with precision. Thus, discounting and normalising affect ranking along with weighting. In all, the results obtained with cumulated gain measures and non-binary weighting seem to deviate from those obtained with binary precision, markedly when (n)DCG is combined with the weighting schemes w10 and w100. 3.3. Correlations between cumulated gain based measures
Do the cumulated gain measures rank the systems similarly? We calculated Kendall correlations at doc-ument cut-off values 10 and 100, but because both cut-offs provide very similar results, we report the results of dcv 100. Table 5 gives Kendall correlations between CG, DCG and nDCG for TREC 7 and 8 data.
CG and DCG correlate better than CG and nDCG, and correlations between DCG and nDCG are again fairly high. Same weighting schemes tend to have highest correlation with each other, but there are exceptions. The lowest correlation is usually at the opposite end of the weighting scale, e.g. w1 and w3 have the lowest correlation with w10 or w100; w10 and w100 have the lowest correlation with w1 or w3. But again, there are exceptions like CG-w10 which correlates least with nDCG-w100. In TREC 8 data an exception is the nDCG-w10 combination which has lowest correlations with (D)CG-w100. In all, nDCG with weighting scheme w100 deviates most from the others.
 3.4. Correlations between different weighting schemes
Table 6 gives Kendall correlations between the four weighting schemes within different cumulated gain measures for TREC 7 and 8. Also here, only the results at dcv 100 are given for the same reason as above.
Only the rankings obtained with the weighting scheme w100 seem to differ from the rankings obtained with weighting scheme w1 X  X nd in some cases w3 X  X o some extent, and the difference is greatest within nDCG.
Further, the difference is greater in TREC 8 the than in TREC 7. 3.5. Changes in ranks illustrated
Kendall correlations give an overall picture of the similarity of the rankings based on different measures with binary and graded relevance. To give a visualisation of the change in rankings, precision, CG, DCG and nDCG curves of five TREC 7 automatic title-based runs are given in Fig. 1 a X  X  (see Appendix A for six
TREC 8 runs). Fig. 1 a X  X  show the relativity of CG and DCG measures: the scale of the ordinate axis differs according to the weighting scheme. They also illustrate the effects of discounting combined with weighting scheme w100, as the curve of run D ascends. Interestingly, run B is uncontested first in all measurements until the last Fig. 1 g, where it is threatened by run D . An interesting question is how can the normalisation change the ranking. The performance of runs varies from topic to topic, i.e. runs close in average perfor-mance typically outperform each other in different topics. Normalising tends to diminish differences in per-formance, and then averaging may result in reversed rank order, although rank order is not changed for individual topics.

Similar types of changes in curves of TREC 8 runs are illustrated in Appendix A . For example, changes occur between runs a and c in Fig. 2 d X  X , and between runs e and f in Fig. 2 b X  X . The similarity in rankings between binary PR curves and CG w1 curves is notable. 4. Discussion
We tested performance of different TREC runs with binary and graded relevance assessments, using 21 topics from TREC 7 and 20 topics from TREC 8. For the reliability of IR evaluation the number of topics is a crucial matter. In Buckley  X  sand Voorhees  X  test (2000) an error rate was around 10% when 20 topics were used for measuring precision at dcv 10. The error rate sinks to 5% when the number of topics increased to 50 with the same measure. If the number of topics cannot be raised, other measures, e.g. with higher cut-off values, are more stable. The number of topics in this study is lower than in the
TREC evaluation. This is due to the available data set with graded relevance assessments. However, we repeated the test with two data sets and the results obtained are similar. Further, several cut-off values (from one to 200) were tested, of which dcv 10 and/or dcv 100 are reported here. These two represent low and high cut-off values and the results at these are in line with other cut-off values in low and high ends.

The number of relevant documents in each relevance level affects the results. Typically, the number of marginally relevant documents is high whereas there are only few highly relevant documents per topic (see Table 1 ). When the few highly relevant documents are given a strong emphasis by weighting, or regarded as only relevant documents, the evaluation becomes less stable, i.e. loosing or gaining one rel-evant document affects the evaluation measurement value strongly. However, if the highly relevant doc-uments are sparse but wanted, they should be counted for in evaluation. We have shown that this is possible.

It is obvious that the different measures emphasise different properties of IR in evaluation. Measures based on binary relevance have the simplistic view that all relevant documents are equally valuable. The cumulated gain takes degrees of relevance into account, but treats all relevant documents of the same rel-evance level equally irrespective of their position. Discounted cumulated gain takes users effort into account by discounting the relevance gain of any document according to its position in the result list. Cumulated gain is intuitive, yet CG and DCG are dependent on the weighting schemes X  X hey are not comparable irre-spective relevance scale and weighting scheme. Normalising (discounted) cumulated gain makes the values comparable in different settings (scales and weighting schemes). Discounting and normalising affect the ranking order of systems, as can be seen in Fig. 1 c versus 1g, and 2c versus 2g.

There cannot be a natural categorization or scale for relevance. By different weighting schemes it is possible to model different user perspectives on desirable ranking. We have tested what kind of weight-ing yields differences in system rankings. In our case there were four relevance levels: highly, fairly, mar-ginally and non-relevant documents. When compared to traditional binary precision, the results obtained by cumulated gain differ somewhat when the fairly and highly relevant documents are given 10 times the value of the previous level. When the gain is discounted in accordance of documents  X  posi-tion in result list, weighting scheme w100 (giving weight 1, 10, 100 to relevance levels marginal, fair and high, respectively) brings about a difference compared to binary relevance. When besides discounting the gain measure is also normalised, even slighter weighting is enough to change ranking slightly (w10; see Table 4 ).

Weighting schemes form a continuum: Cumulated gain with binary or  X  slight  X  weighting correlates to binary precision, but deviates from it with  X  heavier  X  weighting for highly relevant documents. DCG and nDCG measures correlate less with binary precision. Yet, all cumulated gain measures correlate quite strongly with each other, given the same weighting scheme (see Table 5 ), and even different weighting schemes within a measure correlate fairly well (see Table 6 ). Nevertheless, ranking of systems based on weighting scheme w100, which gives a strong emphasis to highly relevant documents, seems very often devi-ate from ranking by binary precision or binary cumulated gain.

What kind of measure and weighting are desirable? The evaluation process involves setting of evaluation criteria, e.g. valuing benefits, costs and drawbacks, which is a situation dependent process. The measures presented here are usable in laboratory framework, i.e. relevant documents are assumed to be known beforehand. The categorization of relevance and valuing of relevance levels have to be decided by the evaluator. We believe that cumulated gain with discounting and normalisation is a useful and practical mea-sure. Use of document cut-off values in cumulated gain measures as in precision/recall gives a user-oriented view in evaluation ( Keen, 1992 ). Yet, it has limitations because of the different numbers of relevant docu-ments per topic, as noted by Hull (1993) and Sakai (2003) . This could partly be overcome by using the num-ber of relevant documents as cut-off point for each topic, i.e. different cut-offs for each topic; a similar method as in the measure known as R precision in TREC.

On the basis of these results we do not claim the ranking of the best systems in TREC would change with graded relevance assessments, nor do we know what kind of features in systems favour retrieval of highly relevant documents. These findings, which are in line with Voorhees  X  (2001) results, show that some systems retrieve more highly relevant documents or rank them higher than others. 5. Conclusion
We have tested the performance of several IR systems with binary and graded relevance assessments to find out the effects of these two different scales on the rank order of the systems. We calculated Kendall correlations between measurements based on binary and graded relevance, and found out that when fairly and highly relevant documents are given more weight, the correlation diminishes, in other words, ranking order of the systems changes.

The traditional approach to IR evaluation is to calculate precision values based on binary relevance assessments. Precision may be measured at different cut-off points of the result set or over standard recall points. The former is a user-oriented and the latter system-oriented way to consider performance. Precision combined with binary relevance is an  X  easy choice  X  for an evaluator: it is established and straightforward. However, it does not take into account that degree of relevance may vary in documents.

Measures based on the idea of cumulating gain offer a user-oriented perspective in laboratory frame-work. The controllability is based on the use of recall bases, that is, calculation of cumulated gain is based on known relevant documents. These measures support the idea of graded relevance, and are customisable to the environment to be evaluated. Yet, the evaluator may have difficulties in deciding how to categorize relevance or in choosing a weighting scheme for different relevance levels, i.e. in setting the parameters. Nevertheless, the effort might be worthwhile because of the gained realism.

Cumulated gain is an intuitive measure showing the gain X  X s sum of relevance weights X  X  user gets at a given cut-off point of the result set. Combined with binary weighting, i.e. binary relevance, it gives the same ranking for systems as binary precision. Combined with graded relevance cumulated gain starts to deviate from ranking of precision, depending of the chosen weighting scheme: the more emphasis is put on highly relevant documents the more the ranking deviates.

Besides the weighting, also other properties of the measures tested affect ranking order: discounting and normalising the cumulated gain values. Discounting is needed to penalise the late coming relevant docu-ments (discounted cumulated gain), normalising is needed to make the figures relative to optimal perfor-mance, and thus easier to compare (normalised cumulated gain). Cumulated gain, discounted cumulated gain, and normalised discounted cumulated gain yield different rankings for systems to be evaluated be-cause they have different criteria. The selection of the measure depends on what user  X  s preferences the evaluator wants to study.
 Acknowledgment
This study was funded by Academy of Finland under the grant number 52894. We thank Erkka Leppa  X  -nen for his work with data analysis, Kalervo Ja  X  rvelin for help and encouragement, and the anonymous ref-erees for their thoughtful comments.
Appendix A References
