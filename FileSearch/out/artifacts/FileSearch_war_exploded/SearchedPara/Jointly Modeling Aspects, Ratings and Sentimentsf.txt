 Recommendation and review sites offer a wealth of infor-mation beyond ratings. For instance, on IMDb users leave reviews, commenting on different aspects of a movie (e.g. actors, plot, visual effects), and expressing their sentiments (positive or negative) on these aspects in their reviews. This suggests that uncovering aspects and sentiments will allow us to gain a better understanding of users, movies, and the process involved in generating ratings.

The ability to answer questions such as  X  X oes this user care more about the plot or about the special effects? X  or  X  X hat is the quality of the movie in terms of acting? X  helps us to understand why certain ratings are generated. This can be used to provide more meaningful recommendations.
In this work we propose a probabilistic model based on collaborative filtering and topic modeling. It allows us to capture the interest distribution of users and the content distribution for movies; it provides a link between inter-est and relevance on a per-aspect basis and it allows us to differentiate between positive and negative sentiments on a per-aspect basis. Unlike prior work our approach is entirely unsupervised and does not require knowledge of the aspect specific ratings or genres for inference.
 We evaluate our model on a live copy crawled from IMDb. Our model offers superior performance by joint modeling. Moreover, we are able to address the cold start problem  X  by utilizing the information inherent in reviews our model demonstrates improvement for new users and movies. Collaborative Filtering; Topic Models; Integrated Modeling; Sentiment Analysis E qual contribution
Collaborative filtering is a staple to many business in the internet economy. Data to build good content recommender systems essentially comes in three guises: interactions, rat-ings, and reviews. First and foremost there is information whether a recommended item was consumed (i.e. viewed, clicked-on, purchased). This is the key source of informa-tion in search, ranking and advertising systems [4]. A com-m on approach to processing this data is to try to estimate the probability that a user will interact with a given item, using past interactions as training data. Second, there is rating information regarding whether the user enjoyed the recommended item. This is the traditional domain of collab-orative filtering. Its use was popularized through the Netflix contest [ 3] and it aims to reconstruct a choice set of matrix e ntries [ 17] or the entire matrix altogether [ 5]. Third, there a re reviews , as provided by the users. This is arguably the most valuable user generated content since in it users not only rate items but they also explain why they liked or dis-liked an item. Hence, a system capable of extracting this information automatically should be able to generate more relevant information, and, as a side effect, also allow us to obtain meaningful profiles of the users and objects involved [ 12]. Our system belongs to the family of i ntegrated models that use ratings and reviews to extract a wealth of informa-tion. We provide a statistical model and we demonstrate in experiments that our approach excels at recommending movies while simultaneously providing meaningful analysis of the interests and aspects relevant for users and movies.
We begin with an example of the type of analysis we are able to obtain for reviews. In it, positive sentiments are a nnotated as green, negative ones as red, and blue terms a re movie-specific . Below we omit information regarding t he specific aspect for visualization purposes (see Table 6). I enjoyed this DVD from the library very much. Daniel C raig plays a believable James Bond. There are some of the o lder 007 action scenes and similar gimmicks with updates t hanks to the younger Quartermaster. Eve plays well with g rit and feminism including a surprise revelation at the end. It X  X  touching as well with the final scenes in the mansion a nd the old Caretaker. Adele X  X  award for best song is well d eserved. But the plot was pretty weak and the film dragged on and on and on, probably being 30 minutes too long. The w ife and I found this boring, something you can X  X  usually l evel against a Bond film.
T he motivation for our work arises from the task of serving the right items to users. This involves a number of challenges ranging from designing an effective user interface to user per-sonalization to solving the cold-start challenge of initializing a recommendation system with meaningful content. Given the wealth of information inherent in review sites, such as IMDb, Netflix and Amazon Prime, it is tempting to extract more than just ratings from the data. After all, we want to understand why a user liked a particular movie, what his preferences are when it comes to selecting a movie (visual effects, plot, choice of subject matter).

Conventionally, in factorization approaches to recommen-dation [ 9] one uses exclusively the information inherent in t he ratings. Consequently the latent factors have a certain degree of ambiguity  X  for instance, if we capture user and movie attributes with vectors v u and v m to predict a score r um  X  h v u , v m i , then the parameters are invariant under ro-tation. That is, replacing v u with U v u and v m with U v for some rotation U will leave the outcome unchanged, yet it may considerably alter the interpretation of coordinate-wise attributes in v u and v m . This is undesirable in several respects: It leads to hard-to-understand factors; The fac-tors may change considerably while leaving the underlying statistical model unchanged.

However, these factors represented as a vector of numbers are usually hard to interpret. For instance, did the movie have good acting but bad story, did the user prefer the di-rector but dislike the genre? At the same time many re-view sites have textual content in addition to the numerical scores. For instance, IMDb is primarily a review site, Net-flix allows for comments, YouTube is comments-only, Yelp contains comments and reviews but is lacking in terms of recommendation, and Zagat primarily focuses on curated content. This allows us to solve the above problem. For example, the word  X  X redictable X  in a movie review tells us that the user is talking about the  X  X lot X  aspect with a  X  X eg-ative sentiment X ; likewise, a word  X  X ilarious X  tell us that the movie is a comedy and that the user probably likes it.
It is therefore tempting to try extracting additional mean-ing from textual data. This is valuable, e.g. when building a search and retrieval system since it allows us to identify attractive (and undesirable) aspects. For example, if a user always likes to write reviews talking about the special ef-fects, we should recommend movies with great special ef-fects to her if we can identify these movies. Moreover, we can learn from aspect related specialization which terms are associated with aspect specific sentiments.
Our model provides a principled extension of the factor-ization models commonly used for recommendation. That is, we retain the notion that reviews are generated by incor-porating user and movie specific features. However, unlike simple vectorial rating models, we use a structured represen-tation to capture the interaction between movie and user. In this sense our model borrows from the tensorial factorization approach of [ 16] and it extends it from scalars to documents.
Mo re specifically, we assume that each user (and each movie) has an aspect distribution of interest. Reviews are generated by drawing from the product of movie and user aspects. For instance, a review text will likely contain de-tails about special effects, but only if the user is actually interested in them and if the movie has special effects worth discussing. Hence, reviews inform both about the content of a movie and also about the interests of a user.

This differentiation allows us to attribute partial scores to interests, i.e. we assume that the review scores arise from the process of combining partial scores associated with different aspects of the movie. Not only does this improve rating accuracy, it also allows us to attach sentiments to aspects. In other words, we can model which terms associate with positive, negative, and neutral aspect specific words within an aspect. We model the following five groups of words: Background That is, words uniformly distributed in every Movie-specific Words such as the name of the charac-Aspect These are words associated with specific aspects. Aspect-Sentiment These words usually come with a spe-General sentiment For example, words such as  X  X reat X ,
The key contribution of our model is that it integrates all available data sources, that is, it provides a joint model of user activity, movie content, ratings, reviews, and a detailed language model of the reviews. We show the following:
In summary, this is the first model tackling the problem set as a whole rather than piecemeal. We begin with an overview of related work in Section 2. This is followed by a de scription of the model in Section 3. Inference algorithms a re provided in Section 4. We then present experimental re sults in Section 5 and a conclusion in Section 7.
Collaborative filtering is a fertile area of research and there exists a multitude of techniques which can readily be applied to subsets of the problem that we tackle. See e.g. [ 18, 9] for a re view. Specifically, probabilistic matrix factorization meth-ods [ 15, 17] have proven successful in real world problems [3, 8, 11, 25, 22].

H owever, probabilistic matrix factorization techniques strug-gle to generalize to new items, i.e. they fail at the cold-start problem. Regression based latent factor models (RLFM) [ 1] us e attribute features to solve this problem by incorporat-ing observable features into latent factors. Recent research [22, 16] incorporates Latent Dirichlet Allocation (LDA) and us es the topic as features, e.g. for recommending scientific articles. In terms of ratings, [ 19] use a statistically more a ppropriate model for capturing the discrete nature of the reviews by formulating an exponential families approach.
Moreover, there is rich literature analyzing reviews, e.g. using LDA [ 14, 26, 23] to uncover topics and sentiments. T hese works provide a more fine-grained analysis of review texts by separating sentiment words from neutral aspect words. In light of this, we build a language model component in our integrated model to capture aspects and sentiments in reviews. Different from a semi-supervised component or opinion lexicon used in [ 14, 26], our sentiments are learnt by bui lding a linkage between user ratings and sentiments.
A recent line of work aims to model multi-aspect ratings from reviews, e.g. [20, 13, 10, 13, 12]. However, it often re lies on having aspects readily available, often with aspect-specific ratings. The work of [ 20] uses LDA based model to i dentify  X  X opics X  that are correlated with user ratings. Sim-ilarly, [ 13] uses multi-aspect ratings to infer sentiments for pre defined aspects. With respect to the importance of min-ing ratable aspects that contribute to user ratings, as shown in these works, our model also seeks to profile a user X  X  aspect preference when it comes to selecting a movie.

The key difference in our model is that it provides an inte-grated approach to this broad range of problems. Probably Hidden Factors as Topics (HFT) [ 12] is the closest to our w ork. HFT jointly models review texts and user ratings by associating each topic dimension with a hidden factor. How-ever, unlike HFT we do not have such constraint. This in-creases the range of applicability. As shown in experiments, our model discovers a more meaningful low-rank represen-tations of aspects, sentiments and movies, and a better rec-ommendation results.
Our task is to predict for a given user u and a movie m both the observed rating r um and also the review w um , as given by a collection of words w umi . In contrast to previous work we model both aspects jointly, using a multitude of observed and latent variables.
 The most intuitive way of understanding the model of Figure 1 is to consider how a review is written. Users are a ssumed to have a given interest distribution  X  u in terms of aspects they write and care about. Moreover, they are also assumed to have biases b u regarding what can be consid-ered to be a reasonable baseline with regard to their choice. Likewise, movies contain a number of aspects, as indicated by  X  m and a bias b m .

Whether a user likes a particular movie depends on a num-ber of things. First, it helps if the movie contains aspects the user cares about. Secondly, it is also important that the user X  X  expectations v u match the movie X  X  properties v when viewed under the angle of a specific aspect, as cap-tured by M a . These aspect-specific ratings of a movie by a user r uma are then aggregated, based on the user X  X  priorities to obtain an aggregate rating r um . Figure 1: Factorized rating and review model. Note t he symmetry between users and movies. The model contains four major plates: aspects, users, movies, and the words within a given review. They are nested and partially overlapping. For convenience we represent the aspect plate as two separate plates (language model and aspect review model are con-tained in the same plate).

As for the actual review text, we assume the following: re-views contain words drawn from a baseline language model of words typically occurring in reviews  X  0 . Moreover, there are positive and negative sentiment words, as indexed by  X  , where s  X  { positive , negative } . Finally, we assume that there are aspect specific word distributions  X  a , again colored by sentiment s , i.e.  X  as . Depending on whether a user appre-ciates a particular aspect of a movie, as indicated by r uma he will generate positive or negative sentiment words (or simply neutral ones). Finally, there are also movie-specific words, such as the name of the main protagonists, the ti-tle, and other named entities that are bound to occur in a review, regardless of the user. This approach of mixing between five different components summarizes our strategy. Probably most closely related is the model of [ 2] who use a similar switch construction to distinguish between posi-tive and negative sentiment. The key difference is that we do not have any explicit information regarding attitude and aspects. Instead, we need to extract this from the reviews.
We employ a conventional bag of words representation, paired with a Dirichlet-Multinomial to capture the word dis-tribution of the reviews. Aspect-specific ratings are gener-ated by matrix factorization, i.e. a Gaussian inner-product model. The twist here is that we capture aspect specific pre ferences via a scaling matrix M a . This is a strict gen-eralization of regular factorization approaches. Finally, the mixing between these aspects occurs by an exponential lin-ear model which also governs review combination.
As is common in collaborative filtering, only a tiny frac-tion of matrix entries are present  X  our dataset contained less than 0 . 03% observed entries. To infer the missing entries collaborative filtering relies on the assumption that the un-derlying matrix has fairly low rank and thus, a small number of terms suffice to determine the remainder of the results.
One may argue that this is only part of a solution, since the relative values of the entries matter more than their ab-solute value [ 25, 24]. That said, for the purpose of compar-i son to existing results we adopt the strategy of measuring the least mean square deviation. The matching probabilis-tic model is that of additive noise relative to an estimated relevance score. We build on the probabilistic matrix fac-torization (PMF) approach of [ 17].

A s in PMF, we assume that users u and movies m are characterized by latent factor vectors v u and v m respectively, that are drawn from zero-mean spherical Gaussian priors The hyperparameters  X  2 u and  X  2 m are user-related and movie-related variances, respectively. In a conventional recom-mender model one would then assume that That is, given biases b u and b m , we observe a noisy variant of the compatibility. Different from PMF [ 15], we assume a n aspect-specific rating of movie m by user u . Here b u and b m are biases for users and movies respectively and b 0 is a common bias. The idea is that while v u and v m encode the general profile, the matrix M a emphasizes the aspect specific properties. That is, while movies may be overall good, they may or may not excel quite as much in specific aspects.

We assume Gaussian priors with fixed mean and precision on real-valued parameters. Specifically, we assume that each element of M a , v u , v m , b u , b m follows a Gaussian distribu-tion with zero mean and a fixed precision.
One of the challenges in combining user and movie at-tributes is in the task to fuse the respective attributes into a joint model. Our approach borrows from [ 7] by designing a n exponential additive model in terms of  X  u and  X  m . The latter are user and movie specific aspect parameters which jointly generate the aspect distribution of a review. Our assumptions are as follows: Moreover, the joint aspect distribution is given by  X  um  X  exp(  X  u +  X  m ) i.e. p ( a |  X  u ,  X  m ) = We also make the (slightly controversial) modeling assump-tion that the extent of discussion in a review and the relative importance of a aspect coincide. That is, aspects that are discussed at twice the length will contribute twice as much to the aggregate score for a review. This yields Here  X  r um is the predicted review rating, and the observed rating r um is generated using N ( X  r um ,  X   X  2 ). This is a strict generalization of the PMF model. The key difference is that the aspect weighting for a given (user,movie) combination is dependent on the aspects they excel in. In other words, the metric is variable in accordance with the content of the movie and the interest of the user. The idea is that, if a movie is a SciFi movie with correspondingly high value of  X  m, SciFi , then the user X  X  review of the movie will likely con-tain SciFi-related content and moreover, the SciFi quality of the movie will matter in terms of the overall rating. That is,  X  um, SciFi will likely be large.

A few comments regarding M a are in order. First and foremost, it does not increase the total number of param-eters dramatically, since we only require k terms for each diagonal matrix. In turn it allows us retain one joint latent attribute model in v u and v m while simultaneously being able to identify individual aspects as needed via v  X  u M
A key in our reasoning is the integration between ratings and reviews. We already established the link between gen-eral attributes, aspect-specific ratings and posited a model for the aspect distribution of the a review.

As shown in the sample review in the introduction, when writing a movie review, the user will express his opinions through a set of sentiment words, such as best , weak or bor-ing . Close examination also shows that the user has different opinions on different aspects of the movie. For instance, the user might like the music of a movie but dislike the plot . This motivates us to model an aspect-specific sentiment for a movie. Overall, we assume that the review language model is given by a convex combination of five components. Crucial to the mixture between these models is the use of a switch variable which chooses between the above types. We accomplish this via  X  , the switching distribution. From it we draw the selector variable y umi for each word and depending on its value we pick one of the above five components. We now go through each of the terms in detail: Switching distribution  X  u m : We draw it from a Dirichlet Aspect z umi : Whenever we draw an aspect-specific word, Aspect sentiment s umi : When s umi is an aspect-specific Aggregate sentiment s umi : When s umi is an general sen-Language models  X  0 ,  X  s ,  X  a ,  X  as ,  X  m : Each of the language Emission model: The final piece in our approach is to By default we choose Gaussian priors for real-valued pa-rameters and Dirichlet conjugate priors for the multinomial distributions. This completes the model specification.
Before we delve into details of the inference algorithm, a brief discussion of some properties of the model is in order. The coupling between aspect specific sentiments and ratings allows us to infer such terms without the need for detailed reviews. In fact, it overcomes the problem arising in [ 19]: t here the recommender model could not take advantage of aspect specific ratings to obtain a more refined user model. Moreover, it overcomes the limitation of having only a small number of aspects, such as in [ 12] since it does not require an e xplicit formulation of categories. To the best of our knowl-edge, this is the first integrated model for recommendation.
As byproduct we obtain aspect preferences for both movie and user. Furthermore, we are able to extract movie-specific terms via  X  m . This is useful for search and retrieval.
Our goal is to learn the hidden factor vectors, aspects, and sentiments of the textual content to accurately model user ratings and maximize the probability of generating the textual content. Hence our objective is the negative log posterior, defined as where R , W denote the ratings and words respectively and  X  and  X  are the Gaussian and Dirichlet hyperparameters.
Unfortunately, inference in this problem is intractable in its direct formulation. Instead, we resort to a hybrid in-ference procedure combining sampling and variational op-timization. That is, we use Gibbs-EM [ 21], an inference m ethod that alternates between collapsed Gibbs sampling [ 6] a nd gradient descent, to estimate parameters in the model. After collapsing out the parameters pertaining to the lan-guage model, terms cease to be conditionally exchangeable, hence we cannot decompose L further. That said, all rele-vant terms decompose for the purpose of the inference algo-rithm and we have:
L  X = X  X The first term denotes the prediction error on user ratings. The second term denotes the probability of observing the text conditioned on priors. Note that this is not a formal equality since each review and score depends on its anno-tation and, indirectly, on the annotations of all remaining documents. This is simply to convey the intuition of the inference approach that we will pursue.

In the E-step, we perform Gibbs sampling to learn the hidden variables by fixing the values of  X  um and { r uma In the M-step, we perform gradient descent to learn hidden factor vectors by fixing the values of { y, z, s } umi .
In the E-step, we perform Gibbs sampling to learn the hid-den variables { y, z, s } umi by fixing the values of  X  um { r uma } A a =1 updated in the gradient descent step. Dirichlet-Multinomial conjugacy allows Gibbs sampling to work by sampling on the individual tuple of { y, z, s } umi , collapsing out all the language models  X  . As this is a conventional step, we omit the detailed derivations and present the de-rived Gibbs sampling update rules. Interested readers are referred to [ 6] for more details.

F or the word in the i -th position of the review written by user u for movie m , we jointly sample its switching vari-able y umi , topic z umi and sentiment s umi , conditioned on its Markov blanket. Let w = w umi and d denote the set of variables { u mi } .  X  C pled as a movie-specific word in movie m excluding the cur-rent word assignment; all the other C s are defined in the same way. I (  X  ) is a indicator function that returns 1 if the statement is true and 0 otherwise. In other words, we effec-tively have a big switch statement distinguishing 5 cases.
Note that when y = 3 , the word is an aspect word, and we need to sample an aspect label from  X  um , which is a deterministic softmax transformation of the sum of  X  u and  X  m given by ( 4). The aspect sentiment probability p ( s | r is based on ( 10) and the aggregate sentiment p ( s d = s |  X  r uses an analogous logistic function for the predicted general rating  X  r um of the movie.
In this step, we use gradient descent to learn the set of pa-rameters  X  = [ { v u , b u ,  X  u } U u =1 , { v m , b m ,  X  by fixing the values of { y, z, s } umi . In this case, our objec-tive function is further modified as follows: The first term remains unchanged from ( 12). The second g oal is to maximize the likelihood of generating all the ob-served { y, z, s, w } u,m variables obtained from Gibbs sam-pling. The final term is the Gaussian prior of all the pa-rameters. We then seek to minimize the following objective function, decomposed from ( 14).

L et L  X  um be the objective for a single rating and review texts, i.e.: L  X  = P r likelihood contribution of a given (user,movie) pair L  X  um follows: L where N y =1 u,m,s is the number of times general sentiment s appears in user u  X  X  review in movie m , and N y =2 u,m,a,s number of times the aspect sentiment s appears under aspect a , and N y =3 u,m,a is the number of times aspect a appears in the review. We then compute the first derivatives of L  X  with respect to the variables. We optimize L  X  using L-BFGS.
We perform 500 runs of Gibbs EM. In each run, we run one iteration for the Gibbs sampling stage and another 10 iterations of gradient descent. We fixed the number of topics and the dimension of the latent factors. For our models and competing baseline models, we use grid search on a devel-opment set to select the model hyperparameters. For grid search, we choose latent factor size from { 5 , 10 } . As our data is sparse, a fairly low rank of factor vectors is sufficient; we also choose a relatively small aspect size from { 5 , 10 , 20 } , so as to leave space for the model to learn a much larger number of movie words. In the following experiments, the regression parameter  X   X  2 is set to be 5 . 0. Aspect distribu-tions  X  u ,  X  m have Gaussian priors, with variances being 0 . 1 and 1 . 0 respectively. To reflect the fact that more sentiment words should be adjectives, adverbs, or verbs,  X  0 ,  X  movie  X  aspect is 0 . 001 on adjectives, adverbs, and verbs, and 0 . 01 for other words. On the other hand,  X  sentiment is 0 . 01 on adjectives, adverbs, and verbs, and 0 . 001 for other words.
Having defined our model mathematically we now proceed to evaluating it. We begin with a quantitative evaluation in the present section. A qualitative discussion of the results follows. Our experiments show that:
We use a dataset compiled from IMDb. We randomly se-lect 50k movies and crawl all their reviews. We only keep those reviews with user ratings (scaled from 0 to 10). We remove users who have less than two reviews and then re-move movies with less than two reviews. Note that despite this simple cleaning, our data is much more sparse with only 0 . 03% entries present, than, say Netflix [ 3] or the datasets s tudied in HFT [ 12]. Table 1 displays some statistics. T able 1: IMDb data set. Unigrams containing stop words or punctuations, as well as infrequent uni-grams that appear less than five times in the corpus are removed during pruning.

We present histograms over different numbers of reviews for movies and user in Figure 2. Clearly, the majority of us ers only write a small number of reviews and the major-ity of movies only receive a few reviews. This is not too surprising, given that IMDb aims to catalogue all movies, including obscure works dating from the 19th century. This Figure 2: Histograms for reviews for movie and user. s parsity underscores the importance of a method that can handle  X  X old-start X  for users or movies with few reviews.
We randomly split our data set into training, validation and test sets. Similar to [ 12], we use 80% of our dataset a s training data, 10% for validation, and 10% for testing. We evaluate the following competing models for comparison: offset only, two state-of-the-art methods, and our model. Offset only Predict the rating as the average of past rat-PMF Probabilistic matrix factorization [ 15]. This model is HFT Hidden factors with topics [ 12]. This work also mod-JMARS Jointly modeling aspects, ratings and sentiments. We analyze the perplexity of all the competing models. Perplexity is a standard measure to evaluate the quality of probabilistic models. The performance in terms of perplex-ity shows the prediction power of the model on unseen re-views, where a lower perplexity means a better performance.
Since PMF does not use review texts, it is not consid-ered in this evaluation. For HFT and our model, we define perplexity as follows: log PPX( D test ) =  X  1 Here p ( w umi ) denotes the likelihood of generating the i -th word in the review written by user u for movie m in D test and N w is the total number of words in the test data. In the following formulas, we use w and y to refer to w umi and y umi whenever indices are obvious.

In HFT, the word likelihood p ( w umi ) is defined as: where  X   X  a,w is the estimated word distribution of topic a , and  X   X  m,a is the estimated topic distribution of the movie m .
In our model, p ( w umi ) is defined as: p ( w ) = p ( y = 0 |  X  um )  X   X  w 0 + p ( y = 1 |  X  um ) Here we use the word distributions  X  , user parameters { v movie parameters { v m , b m ,  X  m } and M a learned in the train-ing step. In this case, we can calculate all terms in Eqn. 17 except  X  u m . Then we run Gibbs sampling on the testing data for 50 iterations to estimate  X  um .

We vary aspect size and latent factor size to test model performance. Note that HFT enforces topics and latent fac-tors with the same dimension, but our model allows them to have different dimensions. To evaluate the sensitivity of model performance in terms of aspect size, we vary aspect size for each latent factor size. Results are shown in Table 2. Table 2: Comparison of models in terms of perplex-ity on held-out data in terms of different topic and latent factor size.
 Consistently, our model achieves better performance than HFT in terms of different factor size. The main difference between our model and HFT lies in the way of modeling review texts, where our model uncovers underlying rich in-formation, e.g.: aspect, sentiment and movie-specific con-tents. This shows a carefully designed language model for review texts could have better predictive power for unseen data. Furthermore, our model X  X  performance varies more in terms of different aspect size A instead of factor size K . This shows that the latent factor dimension in probabilistic ma-trix factorization has minor effect, compared to the aspect dimension in topic modeling.
Factor size Offset PMF HFT JMARS A=20 A=10 A=5 5 7 .07 5.99 5 . 21  X  4.97  X  5.11 5.23 10 5.92 5 . 14  X  5.05  X  5.18 5.28 Table 3: Comparison of models in terms of MSE on held-out data.  X  and  X  mean the result is better than the method in the previous columns at 1% and 0.1% significance level, measured by McNemar X  X  test.

We compare our model with baseline models on the movie recommendation task, measured by Mean-Square-Error (MSE) on the held-out test data. Results are shown in Table 3. Sim-i larly we vary topic size and latent factor size to test model performance. Our observations as follows:
Making recommendations for new users or items which do not have enough rating data is a common issue in rec-ommendation systems. For our model and HFT, although the training data for an item is scarce, the review associated with it can still provide important textual information. HFT clusters the review words into topics, which are tied with item factor vector. Our model identifies aspect distribution and aspect sentiment within the review, and associates the sentiment words with matrix factorization. Therefore, both models can potentially help to better deal with  X  X old-start X  users and items.

We compare the performance of our model with HFT in terms of relatively improvement over PMF. Performance is evaluated on movies/users with different amount of reviews in training data, as shown in Figure 3. Our findings are as fo llows: Figure 3: Improvement in MSE compared to PMF f or  X  X old-start X  movies and users.
 Aspect Director History War Life Character Rating 9.36 8.55 8.51 9.20 9.50
P rob 0.12 0.10 0.09 0.09 0.08 . . . what an excellent piece of cinema . . . the actors are great a nd directing incredible . . . in 300 , Gerard Butler dominates the screen . . . battle scenes are incredible . . . T able 4: The learnt aspect-specific ratings and latent sentiment identified by our model for a review.

To evaluate whether our model is capable of interpreting the reviews correctly, we examine the learned aspect ratings of our model. We present one review in our training set along with the learned aspect ratings and sentiments of the top 5 aspects in the table above. As we can see, the high aspect probability in  X  X irector X  aspect reflects the fact that positive sentiment has been expressed towards the director, e.g.:  X  X irecting incredible X  in the review. Commonly one would assume that the War topic would dominate in any-thing written about the movie 300 , whereas here we are able to infer that it is the directing that is being reviewed.
Background-word and sentiment-word distributions,  X  0 and  X  S , are presented in Table 5.

N ot surprisingly, the top three background words are  X  X ilm X ,  X  X tory X , and  X  X haracter X , all of which provide little informa-tion about aspects or sentiments. Positive sentiment words such as  X  X reat X  and  X  X ood X , and negative sentiment words such as  X  X ad X  and  X  X oring X , are all sentiment words which are not aspect-specific. Note that we do not handle nega-tion, hence  X  X ot good X  will be split into  X  X ot X  and  X  X ood X , which makes  X  X ood X  appear in negative word distribution. Type Words
Background film, story, character, films, characters,
Positive great, good, love, acting, fun,
Negative bad, good, pretty, acting, plot, Table 5: Top background words from  X  0 and senti-ment words from  X  s .
Aspect words and aspect-sentiment words from three pop-ular aspects are shown in Table 6. These words are easily in-t erpretable. For example, for the aspect  X  X dventure X , the top words are  X  X arth, X  X  X uman X  and  X  X pace X . Aspect-sentiments contain sentiment words specific to aspects, e.g.  X  X pectac-ular X  of  X  X dventure X  aspect,  X  X harp X  of  X  X ocial X  aspect, and  X  X asty X  of  X  X iolence X  aspect. These words emphasize the importance of discriminating sentiment words for different aspects. Note that the word  X  X asty X  is classified as both positive and negative in the context of  X  X iolence X . In our opinion, this is not a mistake, as the word  X  X asty X  can in-deed convey positive or negative connotations for different users at the same time.
We present movie-specific words in Table 7. These are w ords that do not convey sentiment or genre information and are particular to the movie. They typically correspond to names of places, actors, and other entities. For example, character names like  X  X ond X  and  X  X ames X  pertaining to the movie  X  X asino Royale X  and words like  X  X eo X  to the movie  X  X he Matrix Reloaded X  . These words also provide a list of interpretable keywords specific to the movies.
 Movie Words Casino Royale bond, craig, james, casino, royale B atman Begins batman, bruce, wayne, bale, begins The Matrix Reloaded matrix, neo, reloaded, action, flight American Beauty beauty, american, spacey, lester, kevin
In summary, our model performs well at distinguishing different types of words: background, aspect, sentiment, aspect-sentiment and movie specific words. The resulting word distributions provide a low-rank representations of as-pects, sentiments and movies, which give a great insight to understand them.
After examining the cases which have higher prediction error rates, we find that one source of errors is the inconsis-tency of ratings and review words in reviews.
 The reviewer expresses clearly positive opinions in the re-view yet gives a low rating. This is an observation that most systems would like to rule out since it may harm the whole system. One possible solution is to perform database cleaning by examining the inconsistency between sentiment words and ratings and rule out such cases. Our system can detect this case by observing the inconsistency between word probability and rating accuracy. This technique can then be applied to anomaly detection or database cleaning, which removes reviews with less meaningful information.
In this paper we proposed JMARS which provides supe-rior recommendations by exploiting all the available data sources. Towards this end, we involve information from re-view and ratings. In fact our model is able to capture the sentiment in each aspect of a review, and predict partial scores under different aspects. Additionally the user inter-ests and movie topics can also be inferred with the integrated model. We showed that our model outperforms state-of-the-art systems in terms of prediction accuracy and the language model for reviews is accurate. Future work includes captur-ing the hierarchical nature of movie topics and incorporat-ing non-parametric models to increase flexibility. Moreover, a fast inference algorithm is required to further increase the scalability of this model.
This research is supported by the Singapore National Re-search Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office, Media Development Authority (MDA). [1] D. Agarwal and B.-C. Chen. Regression-based latent [2] A. Ahmed and E. P. Xing. Staying informed: [3] R. M. Bell and Y. Koren. Lessons from the Netflix [4] A. Z. Broder. Computational advertising and [5] J.-F. Cai, E. J. Cand  X es, and Z. Shen. A singular value [6] T. Griffiths and M. Steyvers. Finding scientific topics. [7] L. Hong, A. Ahmed, S. Gurumurthy, A. Smola, and earth effects effects murder nudity violence social attempts ultimately s pace cgi adventure killer female genre society sense dramatic w orld sci-fi exciting crime pace solid point material intelligent a lien lack sets case gratuitous sinister act trite complexity s cience cg epic death naked inspired nature ugly sympathetic s ave giant cool thriller brutal macabre men grotesque compelling labels (adventure, violence, social) are manually assigned. [8] Y. Koren. Collaborative filtering with temporal [9] Y. Koren, R. Bell, and C. Volinsky. Matrix [10] A. Lazaridou, I. Titov, and C. Sporleder. A bayesian [11] H. Ma, H. Yang, M. R. Lyu, and I. King. SoRec: [12] J. McAuley and J. Leskovec. Hidden Factors and [13] J. J. McAuley, J. Leskovec, and D. Jurafsky. Learning [14] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. [15] A. Mnih and R. Salakhutdinov. Probabilistic matrix [16] I. Porteous, E. Bart, and M. Welling. Multi-HDP: A [17] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [18] X. Su and T. M. Khoshgoftaar. A survey of [19] C. Tan, E. H. Chi, D. Huffaker, G. Kossinets, and [20] I. Titov and R. Mcdonald. A Joint Model of Text and [21] H. M. Wallach. Topic Modeling: Beyond [22] C. Wang and D. M. Blei. Collaborative Topic [23] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating [24] M. Weimer, A. Karatzoglou, Q. Le, and A. J. Smola. [25] S.-H. Yang, B. Long, A. Smola, H. Zha, and Z. Zheng. [26] X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly modeling
