
In real-world applications of machine learning, labeled data may be scarce and expensive or time-consuming to obtain while unlabeled data is abundant and relatively easy to collect. Learning classifiers on a small set of labeled training data may not produce good performance. There-fore, various algorithms, such as active learning and semi-supervised learning, have been proposed to effectively utilize the unlabeled data.

Many semi-supervised learning methods have been pro-posed in the past decade such as s elf-training, co-training, semi-supervised support vector machines, graph-based meth-ods, and so on [1] [2] [3]. The general idea for self-training [4] and co-training [ 5] is to select some unlabeled UCI datasets have also been used in other papers [11] X  X 13]. In [1], eight benchmark datasets are introduced and used for performance comparison of different semi-supervised learning methods. Although much work has been done, it is not quite clear about the performance comparison among several commonly used standard semi-supervised learning algorithms, when using state-of-the-art non-naive Bayesian classifiers that may obtain better performance than NB in the supervised learning setting.

In addition, during our attempt to apply different Bayesian classifiers in semi-supervised learning on some UCI datasets, semi-supervised learning generally could not outperform the base classifiers built only on the labeled data. In fact, researchers have been interested in whether unlabeled data can always help semi-supervised learning or under what circumstances unlabeled data will be more helpful. In [14], Cozman and Cohen pointed out that unlabeled data may degrade classification performance, not only in somewhat extreme conditions but also under common assumptions when the model assumptions are incorrect. Their study is based on the assumption that the labeled data and unlabeled data come from the same distribution. In [15], Chawla and Karakoulas conducted an empirical study to examine the effect of labeled data and unlabeled data in four meth-ods (co-training, Reweighting, ASSEMBLE and Common-component mixture with EM) when each dataset has an extremely imbalanced class distribution and the distributions of data points in labeled and unlab eled datasets are different. AUC was adopted as the performance metric. Some main conclusions drawn in their paper are: (1) Semi-supervised learning techniques may not consistently provide an im-provement over supervised learning for all different ratios of labeled data to unlabeled data; (2) the amount of labeled and unlabeled data is domain dependant and no consistent conclusions can be drawn from the experiments; and (3) with sample-selection bias, more unlabeled data may help when a better representation of the data can be induced. However, their conclusions only show the final comparison results. No detailed analysis of performance during the learning process was given in their work. Moreover, their work only used Naive Bayes as the base classifier for the four methods, and C4.5 as another base classifier in the co-training experiments. It is not clear that whether using other non-naive Bayesian classifiers will have similar problems.
Despite all the work that has been done on semi-supervised learning methods, three questions listed as fol-lows are not quite clear: (1) It is not clear whether standard semi-supervised learning methods can always outperform the classifiers built only on the small amount of labeled data, especially when different non-naive Bayesian classifiers are used as the base classifier, respectively. (2) It has not been indicated whether selecting unl abeled instances with efforts to enlarge the training set can have better performance than does randomly expanding the training set. (3) No detailed each classifier uses all its unlabeled data instead of a percent-age of the unlabeled data. TSVM [7] utilizes unlabeled data to regularize the decision boundary. LLGC [8] is a graph-based semi-supervised learning method, which designs a sufficiently smooth function over the graph represented by labeled and unlabeled data.

Generally, co-training can result in a good performance, assuming two things about th e splitting of the feature set are hold true [5]: each sub-view is sufficient to build a good classifier; and the two sub-views are conditionally independent of each other. However, these two assumptions may not be satisfied in real-world applications. In [6], it states that, co-training still helps when the feature set is split randomly, although not as much as when the features are split sufficiently and independently. Hence, in our exper-iment, we randomly split the attributes into two subsets in co-training and co-EM.

When using confidence of prediction as selection crite-rion, during the selection process of self-training and co-training, the unlabeled exam ples are ranked and selected according to the prediction confidence of classifiers and the class distribution. For example, if the positive-to-negative ratio in a binary class is 3:1, after the unlabeled data are classified by the classifier, 3  X  X ositive X  examples and 1  X  X egative X  example with the highest predicted posterior probabilities will be selected in each iteration. However, it is not clear whether selecting unlab eled instances with efforts can outperform simple random selection. For comparison, we also conducted experiments that randomly select unla-beled instances instead of selecting the most confident ones. For simplicity, in this paper, these two selection methods are denoted as  X  X onfident selection X  and  X  X andom selection X , respectively.

Distinguished by prediction goals, there are two kinds of semi-supervised learning settings: inductive learning, where the goal is to predict unseen data; and transductive learning, where the goal is to predict the unlabeled data. Self-training, co-training and co-EM can be used for both inductive learning and transductive learning, while TSVM and LLGC are used for transductive learning in nature.

The performances of self-training, co-training and co-EM are compared among using 6 different base classifiers: NB, NBTree [16], TAN [17], HGC [18], DNB [19], and HNB [20]. NBTree is a hybrid method combing decision tree and Naive Bayes. TAN is an improved ChowLiu algorithm for learning tree-augmented Naive Bayes, where each non-class node can have one augmenting edge that points to it from an-other non-class node. HGC is a Bayesian network structure learning algorithm using hill climbing as the search method. HNB is an improved Bayesian model that each attribute has a hidden parent created by the average of weighted one-dependence estimators. DNB is an efficient learning algorithm that learns parameters of Bayesian networks by discriminatively computing frequencies from data. It inte-C. Experimental settings
We implemented the standard self-training, co-training and co-EM methods in WEKA 2 . All experiments in this study utilize the implementations of NB, NBTree, TAN, HGC and HNB from WEKA release 3.7.0 [21]. In our ex-periments, similar to [19], the maximum number of parents for each node is set to 2 in HGC, and NB is used as the classifier in DNB. The maximum number of iterations in self-training or co-training, represented by  X  , is set to 80. The maximum number of iterations in co-EM is set to 30. The size of data pool in co-training is set to 50% of  X  . For TSVM and LLGC, default parameters as given in their implementation are used in our experiments. Only binary-class datasets will be used for TSVM.

The performance measure used is accuracy. In order to examine whether unlabeled data really help improve the classification accuracy, for sel f-training, co-training and co-EM, performance of corresponding base classifiers which are learned from only the original labeled data are shown the significant level of 95%. Each entry  X   X  /  X  /  X   X  means that the semi-supervised learning method in the corresponding column wins on  X  datasets (marked by  X  X  X ), ties on  X  datasets, and loses on  X  datasets (marked by  X * X ) against corresponding  X  X ase X .

From the results in Table III and Table IV, it can be observed that, on most UCI datasets, the standard self-training algorithm cannot impr ove classification accuracy by utilizing unlabeled data. The performance of using NB in self-training improves only on 1 dataset but degrades on 11 datasets. When using TAN or HNB in self-training, it wins slightly on more datasets over  X  X ase X  than do using NB. When using HGC or DNB in self-training, it loses on less datasets over  X  X ase X  than that of using NB. By observing the performance of co-training, it is found that, the standard co-training algorithm does not im prove classification accuracy on most UCI datasets, neither. In the standard co-training, using TAN or HGC or HNB improves only on 1 or 2 datasets, slightly better over  X  X ase X  than do using the 3 other classifiers. For standard co-EM, the performance is even worse on these UCI datasets.

To compare performance among using different classifiers in each of the three semi-supervised learning method, Ta-ble V displays the  X  X /t/l X  counts of t-test for using each other classifier against using NB, respectively. The first row, results of self-training, indi cates that, on the UCI datasets, using the other 5 Bayesian classifiers generally perform a bit better than using NB. The second row and the third row show that, for co-training and co-EM, using the other 5 Bayesian classifiers is closer to using NB.
 In Table VI, the comparison results of LLGC against 1-NN are shown. Results on  X  X etter X  and  X  X ushroom X  are not available when running the LLGC implementation, which are omitted in the table. The t-test results show that LLGC improves classification accuracy on 2 UCI datasets but degrades it on 17 UCI datasets. In Table VII, the average results of TSVM and SVM on 18 binary-class UCI datasets are displayed. Results indicat e that TSVM improves classi-fication accuracy on 10 UCI datasets and degrades it on 7 datasets.

It is interesting to find that these methods hardly improve classification accuracy on most of the 26 UCI datasets, a finding that contradicts the core idea of semi-supervised learning.
 self-training and co-trainin g can only improve the accuracy on 1 dataset. TSVM improves average accuracy on about 50% of the datasets, which is similar to its performance on the 26 UCI datasets.

Hence, to summarize, these semi-supervised learning methods (except TSVM) generally do not work well on the 26 UCI datasets and the 6 benchmark datasets, although the classifiers used in these frameworks have good performance in supervised learning. And TSVM improves the perfor-mance with around 50:50 chance. Some analysis will be conducted in Section V.

Little work has ever been done to examine the effect of randomly selecting unlabeled instances. In this section, performance of  X  X andom selection X  against  X  X onfident se-lection X  in self-training and co-training are discussed.
Table VIII shows the average results of using  X  X andom se-lection X  in self-training and co-training against  X  X ase X  on 26 UCI datasets when  X  X  X  is 10%. Figures of average accuracies are not listed due to space limitation. The  X  X /t/l X  counts of t-test results for  X  X andom selection X  against  X  X onfident selection X  are summarized in Table IX. For self-training, it is observed that  X  X andom selection X  even outperforms ultimately affected in a significant way. A similar situation occurs on colic dataset as displayed in Figure 3.
Generally, the standard self-training and co-training should improve the performance if the selected unlabeled instances are labeled correc tly (which is justified in Sub-section V-B later). However, this was not the case in our experiments on mushroom dataset.

A random running on mushroom in Figure 4 is strange: the accuracy on testing and the remaining unlabeled data decrease although all the selected unlabeled instances were correctly labeled. To eliminate any possible influence caused by semi-supervised learning, we performed some experi-ments on the whole mushroom dataset: two small sets of instances were manually kept from the whole dataset as the initial training data and testing data; then a few instances were manually picked from the remaining data and added into the training data. The accu racies on the testing data were compared with each other when more instances were added into the training data. The results show that, on mushroom data, when given more training data, accuracy is sometimes reduced. This is interesting becau se, intuitively, adding more instances should help improve the performance or should at least retain a similar performance -but the opposite situation occurred on the mushroom dataset here.

The curves of a random running on mushroom ,when  X  is set to 3500, are shown in Figure 5 to examine the performance when more unlabel ed instances are used. More numbers of incorrect labeling occur when more unlabeled data are used. It therefore seems that the initial labeled data may not be a good representative of the whole training data, and the new classifier tends to select more and more similar instances around the labeled data. performance by increasing ins tances to the labeled part, especially when  X  X  X  is smaller than 25%.
 C. Does the amount of labeled data affect the performance of semi-supervised learning?
Table XIII shows the results of the two-tailed  X  -test when  X  X  X  changes from 5% to 100%. Each entry  X  /  X  /  X  shows how self-training with  X  X  X  in the corresponding column wins in  X  datasets, ties in  X  datasets, and loses in  X  datasets, compared to self-training with  X  X  X  in the corresponding row. It is evident that, the overall performance of self-training on the 26 UCI datasets generally increases when the number of instances in original  X  increases.

However, with a given  X  X  X  , self-training still cannot win corresponding  X  X ase X  on most of the 26 datasets. Moreover, data may help increase the performance even if some newly selected instances are not correctly labeled, for example, the first and the fourth sub-graph in Figure 1. There are also some situations that more unlabeled data degrades the performance such as the curve shown in Figure 4.
This paper details experiments we conducted on 26 UCI datasets and 6 benchmark datasets using the standard self-training, co-training and co-EM with six different Bayesian classifiers, TSVM, and LLGC. Two kinds of selection cri-teria were also compared in the experiments. The results show that, some of these semi-supervised learning methods generally do not outperform the baseline performance of classifiers learned only on the labeled data; and particularly selecting unlabeled instances with some efforts during the learning process in self-training and co-training is not nec-essarily superior to randomly selecting unlabeled instances. To further explore these results, learning curves were drawn on several datasets, as well as for the correctness rate of labeling selected unlabeled instances at each iteration. The results illustrate a diversity of reasons that the standard self-training does not perform as well as may be ideally expected. The performance may be affected by the classifier used, or the dataset itself. An interesting observation on mushroom dataset is that, the accuracy on the testing set can still decrease even when all the selected unlabeled instances at each iteration are correctly labeled. Further improvement will be presented according to the findings.

