 MicrosoftResearch UniversityofCaliforniaBerkeley StanfordUniversity that a semantic argument frame is a joint structure, with strong dependencies among the arguments. We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases. The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments.
 cope with parser noise and uncertainty. 1. Introduction
Since the release of the FrameNet (Baker, Fillmore, and Lowe 1998) and Propbank (Palmer, Gildea, and Kingsbury 2005) corpora, there has been a large amount of work onstatisticalmodelsforsemanticrolelabeling.Mostofthisworkreliesheavilyon local classifiers: ones that decide the semantic role of each phrase independently of the roles ofotherphrases.
 ture, with strong dependencies between arguments. For instance, in the sentence [ Final-hour trading ] THEME accelerated [ to 108.1 million shares ]
If we did not consider the rest of the sentence, it would look more like an AGENT argu-ment,butwhenwerealizethatthereisnoothergoodcandidatefora THEME argument, because to 108.1 million shares mustbea TARGET and yesterday ismostlikely ARGM -TMP , wecancorrectly labelit THEME .
 parse tree nodes (see Section 2), many important phenomena have not been modeled.
Thekeypropertiesneededtomodelthisjointstructureare:(1)nofiniteMarkovhorizon assumption for dependencies among node labels, (2) features looking at the labels of multiple argument nodesand internal features ofthesenodes,and(3)astatistical model capable of incorporating these long-distance dependencies and generalizing well. We show how to build a joint model of argument frames, incorporating novel features into a discriminative log-linear model. This system achieves an error reduction of 24 . 1% on ALL arguments and 36 . 8 %on CORE arguments over a state-of-the-art independent classifier forgold-standard parsetreesonPropbank.
 ing modifiers), there are at least three types of information to be captured. The most basic is to limit occurrences of each kind of argument. For instance, there is usually at most one argument of a verb that is an ARG 0 (agent), and although some modifier rolessuchas ARGM -TMP can fairly easily be repeated, others such as ARGM -MNR also generallyoccuratmostonce. 1 Theremainingtwotypesofinformationapplymainlyto core arguments (the strongly selected arguments of a verb: ARG 0 X  ARG 5 in Propbank), which in most linguistic theories are modeled as belonging together in an argument frame (set of arguments). The information is only marginally useful for adjuncts (the choices notincluded intheargument frameofaverb.
 has shown that these are strongly correlated with the word sense of the verb (Roland and Jurafsky 2002). If verbs were disambiguated for sense, the semantic roles of phrases would be closer to independent given the sense of the verb. However, be-cause in almost all semantic role labeling work (including ours), the word sense is unknown and the model conditions only on the lemma, there is much joint informa-tion between arguments when conditioning only on the verb lemma. For example, compare: Inthefirstcasethenounphraseafter passed isan ARG 1,whereasinthesecondcaseitisa
ARGM -LOC ,withthechoicegovernedbythesenseoftheverb pass .Secondly,evenwith samesenseofaverb,differentpatternsofargumentrealizationleadtojointinformation betweenarguments. Consider: 162
Despite both examples having an identical surface syntax, knowing that the ARG 1of cook is expressed by the initial noun meal in the second example gives evidence that the children isthe ARG 2 (beneficiary),notthe ARG 1 inthiscase.
 the parse tree t , representing the labels of the nodes and the dependencies between them. In order for a model over these variables to capture, for example, the statistical tendencyofsomesemanticrolestooccuratmostonce(e.g.,thatthereisusuallyatmost one constituent labeled AGENT ), there must be a dependency link between any two variables. To estimate the probability that a certain node gets the role AGENT , we need toknowifanyoftheothernodeswerelabeledwiththisrole.
 globally conditioned on the observation (the parse tree). a Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). However, note that in practice this term has previously been used almost exclusively to describe the restricted case of linear chain Conditional Markov Random Fields (sequence mod-els) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least models that have strong Markov properties, which allow efficient dynamic programming al-gorithms (Cohn and Blunsom 2005). Instead, we consider a densely connected CRF structure,withnoMarkovproperties,anduseapproximateinferencebyre-rankingthe n -bestsolutionsofasimplermodelwithstrongerindependenceassumptions(forwhich exactinferenceispossible).
 dangers X  X ne is that the computational complexity of training the model and search-ing for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-fit the training data and will not generalize well. We propose a model which circumvents these two dangers and achieves significant performance gains over a similar local model that does not add any dependency arcs among the random variables. To tackle the efficiency problem, we adopt dynamic programming and re-ranking algorithms. To avoid overfitting we encode only a small set of linguistically motivated dependencies in features over sets of the random variables. Our re-ranking approach, like the ap-proachtoparsere-rankingofCollins(2000),employsasimplermodel X  X localsemantic role labeling algorithm X  X s a first pass to generate a set of n likely complete assign-ments of labels to all parse tree nodes. The joint model is restricted to these n assign-ments and does not have to search the exponentially large space of all possible joint labelings. 2. Related Work
There has been a substantial amount of work on automatic semantic role labeling, starting with the statistical model of Gildea and Jurafsky (2002). Researchers have workedondefiningnewusefulfeatures,anddifferentsystemarchitecturesandmodels.
Here we review the work most closely related to ours, concentrating on methods for incorporating jointinformation andforincreasing robustness toparser error. 2.1 Methods forIncorporating JointInformation
Gildea and Jurafsky (2002) propose a method to model global dependencies by includ-ing a probability distribution over multi-sets of semantic role labels given a predicate.
In this way the model can consider the assignment of all nodes in the parse tree and evaluatewhetherthesetofrealizedsemanticrolesislikely.Ifanecessaryroleismissing or if an unusual set of arguments is assigned by the local model, this additional factor can correct some of the mistakes. The distribution over label multi-sets is estimated using interpolation of a relative frequency and a back-off distribution. The back-off distribution assumes each argument label is present or absent independently of the otherlabels,namely,itassumes aBernoulli NaiveBayesmodel.
 proximately using re-scoring of the top k = 10 assignments according to a local model, which does not include dependencies among arguments. Using this model improves the performance of the system in F-measure from 59 . 2to62 . 85. This shows that adding global information improves the performance of a role labeling system considerably. However, the type of global information in this model is limited to label multi-sets.
We will show that much larger gains are possible from joint modeling, adding richer sources ofjointinformation usingamoreflexiblestatistical model.
 on Support Vector Machines, and incorporating a large set of structural and lexical features. At the heart of the model lies a local classifier, which labels each parse tree node with one of the possible argument labels or NONE . Joint information is integrated intothemodelintwoways: classifying the current node. This is similar to the Conditional Markov Models (CMM) often used in information extraction (McCallum, Freitag, and Pereira 2000). Notice that here the previous two nodes classified are not in general the previous two nodes assigned non-NONE labels. If a linear order on all nodes is imposed, then the previous twonodes classifiedmostlikelybearthelabel NONE .
 language model over semantic role label sequences. The target predicate is also part of thesequence.
 line system using only the features of Gildea and Jurafsky (2002). The performance gain due to joint information over a system using all features was not reported. The joint information captured by this model is limited by the n -gram Markov assumption of the language model over labels. In our work, we improve the modeling of joint dependencies by looking at longer-distance context, by defining richer features over the sequence of labels and input features, and by estimating the model parameters discriminatively.
 et al. (2004) and Punyakanok, Roth, and Yih (2005). The idea is to build a semantic role labeling system that is based on local classifiers but also uses a global component that ensures that several linguistically motivated global constraints on argument frames are satisfied. The constraints are categorical and specified by hand. For example, one global constraint is that the argument phrases cannot overlap X  X hat is, if a node is labeled with a non-NONE label, all of its descendants have to be labeled NONE .The proposed framework is integer linear programming (ILP), which makes it possible to find the most likely assignment of labels to all nodes of the parse tree subject to 164 specified constraints. Solving the ILP problem is NP-hard but it is very fast in practice (Punyakanok et al. 2004). The authors report substantial gains in performance due to these global consistency constraints. This method was applied to improve the performance both of a system based on labeling syntactic chunks and one based on labeling parse tree nodes. Our work differs from that work in that our constraints are not categorical (either satisfied or not), but are rather statistical preferences, and that they are learned automatically based on features specified by the knowledge engineer.
On the other hand, we solve the search/estimation problem through re-ranking and n -bestsearch onlyapproximately, notexactly.
 systemsthatonlyuseshallowsyntacticinformationhavealsobeenpresented(Hacioglu 2004;Punyakanoketal.2004);usingfullsyntacticparseinformationwasnotallowedin theCoNLL2004sharedtaskonSemanticRoleLabelinganddescriptionofsuchsystems can be found in (Carreras and M ` arquez 2004). Most systems which use only shallow syntactic information represent the input sentence as a sequence of tokens (words or phrases), which they label with a BIO tagging representation (beginning, inside, and outsideargumentlabels)(Hacioglu2004).Limitedjointinformationisusedbysuchsys-tems, provided as a fixed size context of tags on previous tokens; for example, a length fivewindowisusedinthechunk-based systemin(Pradhan,Haciogluetal.2005).
 and Blunsom (2005). It uses a tree-structured CRF, where the statistical dependency structure is exactly defined by the edges in the syntactic parse tree. The only depen-dencies captured are between the label of a node and the label of each of its chil-dren. However, the arguments of a predicate can be arbitrarily far from each other in the syntactic parse tree and therefore a tree-CRF model is limited in its ability to modeldependenciesamongdifferentarguments.Forinstance,thedependencybetween the meal and the children for the sentence in example (4) will not be captured because thesephrases arenotinthesame localtreeaccording toPennTreebanksyntax. 2.2 Increasing Robustness to Parser Error
There have been multiple approaches to reducing the sensitivity of semantic role labeling systems to syntactic parser error. Promising approaches have been to consider multiple syntactic analyses X  X he top k parses from a single or multiple full parsers (Punyakanok, Roth, and Yih 2005), or a shallow parse and a full parse (M ` arquez et al. 2005; Pradhan et al. 2005), or several types of full syntactic parses (Pradhan, Ward et al. 2005). Such techniques are important for achieving good performance: The top four systems in the CoNLL 2005 shared task competition all used multiple syntactic analyses (Carreras andM ` arquez2005).
 sions obtained using different syntactic annotation. The method of Punyakanok, Roth, and Yih (2005) uses ILP to derive a consistent set of arguments, each of which could be derived using a different parse tree. Pradhan, Ward et al. (2005) use stacking to trainaclassifierwhichcombinesdecisionsbasedondifferentannotations,andM ` arquez etal.(2005)usespecial-purposefilteringandinferencestageswhichcombinearguments proposedbysystems usingshallow andfullanalyses.
 and is a simple general method to factor in the uncertainty of the parser by apply-ing Bayesian inference. It is most closely related to the method described in Finkel, Manning, andNg(2006)andcanbeseenasanapproximation ofthatmethod.
 labeling models in Section 4, and later building on them to define joint models in Sec-tion5.Beforewestartpresentingmodels,wedescribethedataandevaluationmeasures used in Section 3. Readers can skip the next section and continue on to Section 4 if they arenotinterestedinthedetailsoftheevaluation. 3. Data and Evaluation Measures 3.1 Data
For most of our experiments we used the February 2004 release of Propbank. We also report results on the CoNLL 2005 shared task data (Propbank I) in Section 6.2. For the latter, we used the standard CoNLL evaluation measures, and we refer readers to the description of that task for details of the evaluation (Carreras and M ` arquez 2005). In this section we describe the data and evaluation measures we used for the
February 2004 data. We use our own set of measures on the February 2004 data for three reasons. Firstly, we wish to present a richer set of measures, which can better illustrate the performance of the system on core arguments as against adjuncts and the performance on identifying versus classifying arguments. Secondly, we technically could not use the CoNLL measure on the February 2004 data, because this earlier data was not available in a format which specifies which arguments should have the additional R -ARGX labels used in the CoNLL evaluation. 3 better for comparison with early papers, because most research before 2005 did not distinguish referring arguments. We describe our argument-based measures in detail here in case researchers are interested in replicating our results for the February 2004 data.
 andtestsets X  X heannotationsfromsections02 X 21formedthetrainingset,section24the development, and section 23 the test set. The set of argument labels considered is the setofcoreargumentlabels( ARG 0 through ARG 5 )plusthemodifierlabels(seeFigure1).
The training set contained 85,392 propositions, the test set 4,615, and the development set2,626.
 trees produced by Charniak X  X  automatic parser (Charniak 2000). For gold-standard parse trees, we preprocess the trees to discard empty constituents and strip functional tags. Using the trace information provided by empty constituents is very useful for improving performance (Palmer, Gildea, and Kingsbury 2005; Pradhan, Ward et al. 2005), but we have not used this information so that we can compare our results to previous workandsince automaticsystems thatrecover itarenotwidelyavailable. 3.2 Evaluation Measures
Since 2004, there has been a precise, standard evaluation measure for semantic role la-beling,formulatedbytheorganizersoftheCoNLLsharedtasks(CarrerasandM ` arquez 166 2004,2005).Anevaluationscriptisalsodistributedaspartoftheprovidedsoftwarefor thesharedtaskandcanbeusedtoevaluate systems onPropbankIdata.
 uation measures for semantic role labeling that make it difficult to compare results obtained by different researchers, because researchers use their own implementations of evaluation measures, without making all the exact details clear in their papers. The first issue is the existence of arguments consisting of multiple constituents. In this case it is not clear whether partial credit is to be given for guessing only some of the con-stituentscomprisingtheargumentcorrectly.Thesecondissueiswhetherthebracketing of constituents should be required to be recovered correctly, in other words, whether pairs of labelings, such as [ the ] ARG 0 [ man ] ARG 0 and [ the man ] same or not. If they are considered the same, there are multiple labelings of nodes in a parse tree that are equivalent. The third issue is that when using automatic parsers, someoftheconstituentsthatarefillersofsemanticrolesarenotrecoveredbytheparser.
Inthiscaseitisnotclearhowvariousresearchgroupshavescoredtheirsystems(using headword match, ignoring these arguments altogether, or using exact match). If we vary the choice taken for these three issues, we can come up with many (at least eight) differentevaluationmeasures,andthesedetailsareimportant,becausedifferentchoices canleadtorather largedifferencesinreportedperformance.
 2004 data reported in this article. The measures are similar to the CoNLL evaluation measure, but report a richer set of statistics; the exact differences are discussed at the endofthissection.
 whichwecall argument-basedevaluation .Todescribetheevaluationmeasure,wewill useasanexamplethecorrectandguessedsemanticrolelabelingsshowninFigures2(a) and C -ARGX .Thelabel C -ARGX isusedtorepresentmulti-constituentarguments.Acon-stituentlabeled C -ARGX isassumedtobeacontinuationoftheclosestconstituenttothe leftlabeled ARGX .Oursemanticrolelabelingsystemproduceslabelingsofthisformand thegoldstandardPropbankannotationsareconvertedtothisformaswell. tioniscarriedoutindividuallyforeachpredicateanditsassociatedargumentframe.Ifa sentencecontainsseveralclauses,theseveralargumentframesareevaluatedseparately. constituting an argument is correct, there is no need to know how this set is broken into constituents) and do not give partial credit for labeling correctly only some of several constituents in a multi-constituent argument. They are illustrated in Figure 2.
Forthesemeasures,asemanticrolelabelingofasentenceisviewedasalabelingonsets ofwords.Thesesetscanencompassseveralnon-contiguousspans.Figure2(c)givesthe representationofthecorrectandguessedlabelingsshowninFigures2(a)and2(b),inthe firstandsecondrowsofthetable,respectively.Toconvertalabelingonparsetreenodes to this form, we create a labeled set for each possibly multi-constituent argument. All remaining sets of words are implicitly labeled with NONE . We can see that, in this way, exact bracketing is not necessary and also no partial credit is given when only some of several constituents inamulti-constituent argument arelabeledcorrectly. a guessed set of labeled spans to a correct set of labeled spans. We briefly define the various measures of comparison used herein, using the example guessed and correct 168 labelings shown in Figure 2(c). All spans not listed explicitly are assumed to have label racy (Acc.) X  X cross nine different conditions. When the sets of labeled spans are com-pareddirectly,weobtainthecompletetaskmeasures,correspondingtotheI D &amp;C LS row and ALL columninFigure2(d).Wealsodefineseveralothermeasurestounderstandthe performanceofthesystemondifferenttypesoflabels.Wemeasuretheperformanceon identification(I D ),classification(C LS ),andthecompletetask(I D &amp;C LS ),whenconsider-ingonlythecorearguments ( CORE ),allarguments butwithasingle ARGM labelforthe whichwenowdescribe.Foreachofthem,wecomputetheWholeFrameAccuracyand F-Measure asfollows: there is an exact match between the proposed and correct labelings. For example, the whole frame accuracy for I D &amp;C LS and ALL is 0, because the correct and guessed sets of labeled spans shown in Figure 2(c) do not match exactly. In the figures,  X  X cc. X  is always an abbreviation for this whole frame accuracy. Even though this measure has notbeenusedextensivelyinpreviouswork,wefinditusefultotrack.Mostimportantly, potential applications of role labeling may require correct labeling of all (or at least the core)argumentsinasentenceinordertobeeffective,andpartiallycorrectlabelingsmay not be very useful. Moreover, a joint model for semantic role labeling optimizes Whole FrameAccuracy moredirectlythanalocalmodeldoes.
 inthismulti-classsetting,wedefineithere.F-Measureisdefinedastheharmonicmean tivespansinagivenguessedlabeling. True positive isthenumberofspanswhosecorrect labelisoneofthecoreormodifierargumentlabels(not NONE )andwhoseguessedlabel is the same as the correct label. False positive is the number of spans whose guessed label is non-NONE and whose correct label is different from the guessed label (possibly NONE ). False negative isthenumberofspanswhosecorrectlabelisnon-guessed label is not the same as the correct one (possibly NONE ). In the figures in this paper we show F-Measure multiplied by 100 so that it is in the same range as Whole FrameAccuracy.
 arguments only, without regard to modifier arguments. They can be obtained by first mappingallnon-core argument labelsintheguessed andcorrect labelings to NONE . know a given span has a modifier role, without knowledge of the specific role label. In addition,decidingexactmodifierargumentlabelswasoneofthedecisionswithhighest disagreement among annotators (Palmer, Gildea, and Kingsbury 2005). To estimate performance under this setting, we relabel all ARGM -X arguments to ARGM in the proposed and correct labeling. Such a performance measure was also used by Xue and
Palmer (2004). Notethat these measures donot exclude thecore arguments butinstead
ALL we count { 0 } as a true positive span, { 1,2 } , { 3,4 and { 1,2,3,4 } and { 7,8,9 } asfalsenegative.
 distinction.Forthepurposesofthisevaluation,allspanslabeledwithanon-NONE label are considered to have the generic label ARG . For example, to compute CORE I D ,we compare thefollowingsetsoflabeledspans: TheF-Measure is1.0andtheWholeFrame Accuracy is100%.
 were also guessed to be argument spans (but possibly the exact label was wrong). In otherwords,thesemeasuresignorethe ARG vs. NONE confusions.Theyignoreallspans, which were incorrectly labeled NONE , or incorrectly labeled with an argument label, when the correct label was NONE . This is different from  X  X lassification accuracy X  used in previous work to mean the accuracy of the system in classifying spans when the correct set of argument spans is given. To compute C LS measures, we remove all spans from S guessed and S correct that do not occur in both sets, and compare the resulting sets.
Forexample,tocomputethe ALL C LS measures,weneedtocomparethefollowingsets oflabeledspans: according to one of the labelings and non-NONE according to the other. The F-Measure is.50andtheWholeFrame Accuracy is0%.
 predicate in the sentence separately. It is possible for a sentence to contain several propositions X  X nnotations of predicates occurring in the sentence. For example, in the fortheverbs faces and explore .Theseare:
Ourevaluationmeasurescomparetheguessedandcorrectsetoflabeledspansforeach proposition. 3.3 Relation to the CoNLL Evaluation Measure
The CoNLL evaluation measure (Carreras and M ` arquez 2004, 2005) is almost the same as our argument-based measure. The only difference is that the CoNLL measure intro-ducesanadditionallabeltypeforarguments,oftheform R -ARGX ,usedforreferringex-170 pressions.ThePropbankdistributioncontainsaspecificationofwhichmulti-constituent arguments are in a coreference chain. The CoNLL evaluation script considers these multi-constituent arguments as several separate arguments having different labels, whereoneargumenthasan ARGX labelandtheothershave R -ARGX labels.Thedecision of which constituents were to be labeled with referring labels was made using a set of rules expressed with regular expressions. 5 A script that converts Propbank annotations toCoNLLformatisavailable aspartofthesharedtasksoftware.
 arguments of began asfollows: distinguish coreferential versus non-coreferential split arguments. According to our argument-based evaluation, theannotation ofthearguments oftheverb began is: measureissuchthatwecannotsaythatthevalueofoneisalwayshigherthanthevalue of the other. Either measure could be higher depending on the kinds of errors made. For example, if the guessed labeling is: [ The deregulation ] [ began ] PRED enabled shippers to bargain for transportation , the CoNLL script would count the argument that as correct and report precision and recall of .5, whereas our argument-based measure would not count any argument correct and report precision and recall of 0. On the other hand, if the guessed labeling is [ The deregulation ] measurewouldreportaprecisionandrecallof0,whereasourargument-basedmeasure would report precision and recall of 1. If the guessed labeling is [ The deregulation ] measures would report precision and recall of 1. (For our argument-based measure it does not make sense to propose R -ARGX labels and we assume such labels would be converted to C -ARGX labels if they are after the phrase they refer to.) Nevertheless, overallweexpectthetwomeasures toyieldverysimilar results. 4. Local Classifiers
Aclassifieris local ifitassignsaprobability(orscore)tothelabelofanindividualparse treenode n i independently ofthelabelsofothernodes.
 labeling into identification and classification phases. Formally, let L denote a mapping of the nodes in a tree t to a label set of semantic roles (including NONE ) with respect to a predicate v .Let Id ( L ) be the mapping which collapses L  X  X  non-NONE values into ARG .
Then, like the Gildea and Jurafsky (2002) system, we decompose the probability of a labeling L intoprobabilitiesaccordingtoanidentificationmodel P model P CLS .

This decomposition does not encode any independence assumptions, but is a use-ful way of thinking about the problem. Our local models for semantic role labeling use this decomposition. We use the same features for local identification and classi-fication models, but use the decomposition for efficiency of training. The identifica-tion models are trained to classify each node in a parse tree as ARG or NONE ,and the classification models are trained to label each argument node in the training set with its specific label. In this way the training set for the classification models is smaller. Note that we do not do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation(1).
 they produce probability distributions, identification and classification models can be chained in a principled way, as in Equation (1). The baseline features we used for the localidentificationandclassificationmodelsareoutlinedinFigure3.Thesefeaturesare a subset of the features used in previous work. The standard features at the top of the figure were defined by Gildea and Jurafsky (2002), and the rest are other useful lexical and structural features identified in more recent work (Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004). We also incorporated several novel features which wedescribe next.
 172 4.1 Additional Features for Displaced Constituents
We found that a large source of errors for ARG 0 and ARG 1 stemmed from cases such as those illustrated in Figure 4, where arguments were dislocated by raising or control verbs. Here, the predicate, expected , does not have a subject in the typical position X  indicatedbytheemptyNP X  X ecausetheauxiliary is hasraisedthesubjecttoitscurrent
S UBJECT , indicating whether the predicate is  X  X issing X  its subject, and use this feature in conjunction with the P ATH feature, so that we learn typical paths to raised subjects conditionedontheabsenceofthesubject initstypicalposition. quite far from its predicate. The predicate widen shares the phrase the trade gap with expect as an ARG 1 argument. However, as expect is a raising verb, widen  X  X  subject is not in its typical position either, and we should expect to find it in the same position as expected  X  X  subject. This indicates it may be useful to use the path relative to expected to find arguments for widen . In general, to identify certain arguments of predicates embedded in auxiliary and infinitival VPs we expect it to be helpful to take the path from the maximum extended projection of the predicate X  X he highest VP in the chain of VPs dominating the predicate. We introduce a new path feature, P ROJECTED P ATH , which takes the path from the maximal extended projection to an argument node. This feature applies only when the argument is not dominated by the maximal projection (e.g., direct objects). These features also handle other cases of discontinuous and non-local dependencies, such as those arising due to control verbs. The performance gain from these new features was notable, especially in identification. The performance on the additional features as well, are shown in Figure 5. For these results, the constraint that argument phrases do not overlap was enforced using the algorithm presented in
Section4.2. 4.2 Enforcing the Non-Overlapping Constraint
The most direct way to use trained local identification and classification models in testing is to select a labeling L of the parse tree that maximizes the product of the probabilities according to the two models, as in Equation (1). Because these models are local,thisisequivalenttoindependentlymaximizingtheproductoftheprobabilitiesof thetwomodelsforthelabel l i ofeachparsetreenode n i asshownbelowinEquation(2).
Aproblemwiththisapproachisthatamaximizinglabelingofthenodescouldpossibly violate the constraint that argument nodes should not overlap with each other. There-fore,toproduceaconsistentsetofargumentswithlocalclassifiers,wemusthaveaway ofenforcingthenon-overlapping constraint.
 to find a non-overlapping assignment, or the general-purpose ILP approach of
Punyakanok et al. (2004). For labeling chunks an exact algorithm based on shortest paths was proposed in Punyakanok and Roth (2001). Its complexity is quadratic in the lengthofthesentence.
 likely non-overlapping (consistent) labeling of all nodes in the parse tree, according to a product of probabilities from local models, as in Equation (2). For simplicity, we describethedynamicprogramforthecasewhereonlytwoclassesarepossible: ARG and is similar to the Viterbi algorithm for context-free grammars, because we can describe descendants.
 bilities rather than the product of local probabilities, which is equivalent. The dynamic program works from the leaves of the tree up and finds a best assignment for each subtree, using already computed assignments for its children. Suppose we want the most likely consistent assignment for subtree t with child trees t the most likely consistent assignment of its nodes, as well as the log-probability of the assignment for t istheonethatcorresponds tothemaximumof: 174 of assigning the root node of t to NONE plus the sum of the log-probabilities of the likelynon-overlappingassignment.Byslightlymodifyingthisprocedure,weobtainthe most likely assignment according to a product of local identification and classification models. We use the local models in conjunction with this search procedure to select a most-likelylabelingintesting.
 which is usually much less than the square of the number of words in the sentence ( l thecomplexityofthePunyakanokandRoth(2001)algorithm.Forexample,forabinary-branching parse tree, the number of nodes is approximately 2 l . The speedup is due to the fact that when we label parse tree nodes, we make use of the bracketing constraints imposed by the parse tree. The shortest path algorithm proposed by Punyakanok and Rothcanalsobeadaptedtoachieve thislowercomputational complexity.
 gainsinperformance.TheresultsinFigure5arefrommodelsthatusethedynamicpro-gramforselectingnon-overlappingarguments.Toevaluatethegainfromenforcingthe constraint, Figure 6 shows the performance of the same local model using all features, when the dynamic program is used versus when a most likely possibly overlapping assignment ischosen intesting.
 in re-ranking. The non-overlapping constraint is enforced using the dynamic program.
Thisisastate-of-the-artmodel.ItsF-Measureon ALL argumentsis88.4accordingtoour argument-based scoring measure. This is very similar to the best reported results (as of 2004)usinggold-standardparsetreeswithoutnullconstituentsandfunctionaltags:89.4 F-Measure reportedforthePradhan etal.(2004)model. 7 ure 7(a), and the two confusion matrices in Figures 7(b) and 7(c), which display the number of errors of each type that the model made. The first confusion matrix con-centrates on CORE arguments and merges all modifying argument labels into a single errorsareconfusionsofargumentlabelswith NONE .Thenumberofconfusionsbetween pairsofcoreargumentsislow,asisthenumberofconfusionsbetweencoreandmodifier labels. If we ignore the column and row corresponding to NONE in Figure 7(b), the number of off-diagonal entries is very small. This corresponds to the high F-Measures
The number of confusions of argument labels with NONE , shown in the NONE column, is larger than the number of confusions of NONE with argument labels, shown in the experimented with the precision X  X ecall tradeoff but this did not result in an increase in F-Measure.
 between modifier argument labels is higher than the number of confusions between core argument labels. This corresponds to the ALL C LS F-Measure of 95.7 versus the performance on some very frequent modifier labels is in the low sixties or seventies. Theconfusions betweenmodifierlabels and NONE arequitenumerous.
 withoutloweringprecision.Inparticular,whenthemodelisuncertainwhichofseveral likely CORE labels to assign, we need to find additional sources of evidence to improve its confidence. To improve the performance on modifier arguments, we also need to lower the confusions among different modifier arguments. We will see that our joint model improves the overall performance mainly by improving the performance on 176 context. 4.3On Split Constituents
As discussed in Section 3, multiple constituents can be part of the same semantic argument as specified by Propbank. An automatic system that has to recover such informationneedstohaveawayofindicatingwhenmultipleconstituentslabeledwith thesamesemanticroleareapartofthesameargument.Someresearchers(Pradhanetal. 2004; Punyakanok et al. 2004) have chosen to make labels of the form C -ARGX distinct argument labels that become additional classes in a multi-class constituent classifier.
These C -ARGX are used to indicate continuing arguments as illustrated in the two trees inFigure2.Wechosetonotintroduceadditionallabelsofthisform,becausetheymight unnecessarily fragment the training data. Our automatic classifiers label constituents with one of the core or modifier semantic role labels, and a simple post-processing rule is applied to the output of the system to determine which constituents that are labeled the same are to be merged as the same argument. The post-processing rule is the following: For every constituent that bears a core argument label ARGX , if there is a preceding constituent with the same label, re-label the current constituent C -ARGX .
Therefore, according to our algorithm, all constituents having the same core argument labelarepartofthesameargument,andallconstituentshavingthesamemodifierlabels areseparateargumentsbythemselves.Thisruleisfairlyaccurateforcoreargumentsbut is not always correct; it fails more often on modifier arguments. An evaluation of this rule using the CoNLL data set and evaluation measure shows that our upper bound in performance because ofthisruleisapproximately 99.0F-Measure on ALL arguments. 5. JointClassifiers
Weproceedtodescribeourmodelsincorporatingdependenciesbetweenlabelsofnodes in the parse tree. As we discussed briefly before, the dependencies we would like to model are highly non-local. A factorized sequence model that assumes a finite Markov horizon, such as a chain CRF (Lafferty, McCallum, and Pereira 2001), would not be able to encode such dependencies. We define a CRF with a much richer dependency structure. 5.1 Form of the Joint Classifiers
Motivation for Re-Ranking. For argument identification, the number of possible as-signments for a parse tree with n nodes is 2 n . This number can run into the hundreds of billions for a normal-sized tree. For argument labeling, the number of possible assignments is  X  20 m ,if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments. Training a model which has such a huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.
Therefore, in order to be able to incorporate long-range dependencies in our models, we chose to adopt a re-ranking approach (Collins 2000), which selects from likely as-signmentsgeneratedbyamodelwhichmakesstrongerindependenceassumptions.We utilize the top n assignments of our local semantic role labeling model P likelyassignments.AscanbeseenfromFigure8(a),forrelativelysmallvaluesof n ,our re-ranking approach does not present a serious bottleneck to performance. We used a value of n = 10 for training. In Figure 8(a) we can see that if we could pick, using an oracle, the best assignment out of the top 10 assignments according to the local model, wewouldachieveanF-Measureof97 . 3onallarguments.Increasingthenumberof n to 30 results in a very small gain in the upper bound on performance and a large increase inmemoryrequirements. Wethereforeselected n = 10asagood compromise.

Generation of top n Most Likely Joint Assignments. We generate the top n most likely non-overlapping joint assignments of labels to nodes in a parse tree according to a local model P SRL , using an exact dynamic programming algorithm, which is a direct generalization of the algorithm for finding the top non-overlapping assignment describedinSection4.2.

ParametricModels. Welearnlog-linearre-rankingmodelsforjointsemanticrolelabel-ing,whichusefeaturemapsfromaparsetreeandlabelsequencetoavectorspace.The form of the models is as follows. Let  X  ( t , v , L )  X  R target verb v , and joint assignment L of the nodes of the tree, to the vector space
L , L 2 ,  X  X  X  , L N denote the top N possible joint assignments. We learn a log-linear model with a parameter vector W , with one weight for each of the s dimensions of the feature vector.Theprobability(orscore)ofanassignment L accordingtothisre-rankingmodel isdefinedas
Thescoreofanassignment L notinthetop n iszero.Wetrainthemodeltomaximizethe sum of log-likelihoods of the best assignments minus a quadratic regularization term.
Inthisframework,wecandefinearbitraryfeaturesoflabeledtreesthatcapturegeneral properties ofpredicate X  X rgument structure. 5.2 Joint Model Features
Wewillintroducethefeaturesofthejointre-rankingmodelinthecontextoftheexample parse tree shown in Figure 9. We model dependencies not only between the label of a 178 node and the labels of other nodes, but also dependencies between the label of a node and input features of other argument nodes. The features are specified by instantiation oftemplatesandthevalueofafeatureisthenumberoftimesaparticularpatternoccurs inthelabeledtree.
 define the candidate argument sequence as the sequence of non-NONE labeled nodes [ n 1 , l 1 , ... , v PRED , ... , n m , l m ]( l i isthelabelofnode n sequence usually contains very few of the nodes in the tree X  X bout 2 to 7 X  X s this is the typical number of arguments for a verb. To make it more convenient to express our feature templates, we include the predicate node v in the sequence. This sequence of labeled nodes is defined with respect to the left-to-right order of constituents in the parsetree.Becausenon-NONE labelednodesdonotoverlap,thereisastrictleft-to-right order among these nodes. The candidate argument sequence that corresponds to the correctassignment inFigure 9isthen:
FeaturesfromLocalModels. Allfeaturesincludedinthelocalmodelsarealsoincluded in our joint models. In particular, each template for local features is included as a joint template that concatenates the local template and the node label. For example, for the local feature PATH , we define a joint feature template that extracts PATH from every node in the candidate argument sequence and concatenates it with the label of the node. Both a feature with the specific argument label and a feature with the generic back-off ARG label are created. This is similar to adding features from identification and classification models. In the case of the example candidate argument sequence provided, forthenodeNP 1 wehavethefeatures: templates in the two models. If these were the only features that a joint model used, we would expect its performance to be roughly the same as the performance of a local model. This is because the two models will in fact be in the same parametric family but will only differ slightly in the way the parameters are estimated. In particular, the likelihood of an assignment according to the joint model with local features will differ from the likelihood of the same assignment according to the local model only in the denominator (the partition function). The joint model sums over a few likely assignments in the denominator, whereas the local model sums over all assignments; also, the joint model does not treat the decomposition into identification andclassification models inexactlythesamewayasthelocalmodel.

Whole Label Sequence Features. As observed in previous work (Gildea and Jurafsky 2002; Pradhan et al. 2004), including information about the set or sequence of labels assigned to argument nodes should be very helpful for disambiguation. For example, including such information will make the model less likely to pick multiple nodes to fill the same role or to come up with a labeling that does not contain an obligatory argument. We added a whole label sequence feature template that extracts the labels of all argument nodes, and preserves information about the position of the predicate.
Two templates for whole label sequences were added: one having the predicate voice only, and another also including the predicate lemma. These templates are instantiated asfollowsfortheexamplecandidate argument sequence: specificlabelsforthearguments.Thesefeaturetemplateshavetheeffectofcountingthe numberofargumentstotheleftandrightofthepredicate,whichprovidesusefulglobal information about argument structure. A local model is not able to represent the count of arguments since the label of each node is decided independently. This feature can very directly and succinctly encode preferences for required arguments and expected number ofarguments.
 sequencefeaturesisnothelpful.Thiscorrespondstothestandardlinguisticunderstand-ing that there are no prevalent constraints on the position or presence of adjuncts in an argument frame, and was confirmed in our experiments. We redefined the whole label sequence featurestoexclude modifyingarguments.
 the independence assumptions of the local model. Because these features look at the sequence of labels of all arguments, they capture joint information. There is no limit on the length of the label sequence and thus there is no n -gram Markov order independence assumption (in practice the candidate argument sequences in the top n complete assignments are rarely more than 7 nodes long). Additionally, the nodes in the candidate argument sequences are in general not in the same local tree in the syntactic analysis and a tree-CRF model (Cohn and Blunsom 2005) would not be able toencode thesedependencies.

Joint Syntactic X  X emantic Features. This class of features is similar to the whole label sequence features, but in addition to labels of argument nodes, it includes syntactic features of the nodes. These features can capture the joint mapping from the syntactic realizationofthepredicate X  X argumentstoitssemanticframe.Theideaofthesefeatures is to capture knowledge about the label of a constituent given the syntactic realization and labels of all other arguments of the verb. This is helpful in capturing syntactic alternations,suchasthedativealternation.Forexample,considerthesentence( i )[ Shaw Publishing ] ARG0 [ offered ] PRED [ Mr. Smith ] ARG2 [ a reimbursement ] alization( ii )[ Shaw Publishing ] ARG0 [ offered ] PRED [ a reimbursement ] 180
When classifying the NP in object position, it is useful to know whether the following argument is a PP. If it is, the NP will more likely be an ARG 1, and if not, it will more likely be an ARG 2. A feature template that captures such information extracts, for each candidate argument node, its phrase type and label. For example, the instantiations of such templates in ( ii ), including only the predicate voice or also the predicate lemma, wouldbe: node and found that the phrase type and the head of a directly dominating PP X  X f one exists X  X eremosthelpful.
 additiontofeaturesofthepredicate.Theycannottakeintoaccountthefeaturesofother argument nodes, because they are only given the input (parse tree), and the identity of theargumentnodesisunknown.Itisconceivablethatalocalmodelcouldconditionon the features of all nodes in the tree but the number of parameters (features) would be extremely large. The joint syntactic X  X emantic features proposed here encode important dependencies usingaverysmallnumberofparameters, aswewillshowinSection5.4. syntactic frame , which often captures similar information. The important difference is that their template extracts contextual information from noun phrases surrounding the predicate, rather than from the sequence of argument nodes. Because we use a joint model, we are able to use information about other argument nodes when labeling a node.

Repetition Features. We also add features that detect repetitions of the same label in a candidate argument sequence, together with the phrase types of the nodes labeled with that label. For example, ( NP-ARG 0, WHNP-ARG 0 ) is a common pattern of this form.
Variantsofthisfeaturetemplatealsoindicatewhetherallrepeatedargumentsaresisters intheparsetree,orwhetherallrepeatedargumentsareadjacentintermsofwordspans.
These features can provide robustness to parser errors, making it more likely to assign thesamelabeltoadjacentphrasesthatmayhavebeenincorrectlysplitbytheparser.In
Section 5.4 we report results from the joint model and an ablation study to determine thecontribution ofeachofthetypesofjointfeatures. 5.3 Applying Joint Models inTesting
Here we describe the application in testing of a joint model for semantic role labeling, usingalocalmodel P SRL andajointre-rankingmodel P r SRL .Thelocalmodel P togenerate N non-overlapping jointassignments L 1 , ... , L score from the local model. In our experiments, we noticed that for larger values of N , the performance of our re-ranking model P r SRL decreased. This was probably due to the fact that at test time the local classifier produces very poor argument frames near the bottom of the top n for large n . Because the re-ranking model is trained on relatively few good argument frames, it cannot easily rule out very bad frames. It makes sense thentoincorporate thelocal modelintoourfinalscore. Ourfinalscore isgivenby: where  X  isatunableparameterdeterminingtheamountofinfluencethelocalscorehas onthefinalscore(wefound  X  = 1 . 0toworkbest).Suchinterpolationwithascorefrom afirst-passmodelwasalsousedforparsere-rankingin(Collins2000).Giventhisscore, attesttimewechoose amongthetop n localassignments L 1 , ... , L 5.4 JointModel Results
We compare the performance of joint re-ranking models and local models. We used n = 10 joint assignments for training re-ranking models, and n = 15 for testing. The weight  X  of the local model was set to 1. Using different numbers of joint assignments in training and testing is in general not ideal, but due to memory requirements, we couldnotexperiment withlarger valuesof n fortraining.
 from earlier figures, a joint model using only local features ( J using local + whole label sequence features ( L ABEL S EQ described types of features ( A LL J OINT ). The evaluation is on gold-standard parse trees.
In addition to performance measures, the figure shows the number of binary features included in the model. The number of features is a measure of the complexity of the hypothesis space oftheparametricmodel.
 by .5 points of F-Measure. The joint model using local features estimates the feature weightsonlyusingthetop n consistentassignments,thusmakingthelabelsofdifferent nodes non-independent according to the estimation procedure, which may be a cause of the improved performance. Another factor could be that the model J combination of two models as specified in Equation (4), which may lead to gains (as is usual forclassifier combination).
 jump in F-Measure on all arguments. An additional .8 gain results from the inclusion of syntactic X  X emantic and repetition features. The error reduction of model A 182 over thelocal model is 36.8 %in CORE arguments F-Measure, 33.3 %in CORE arguments wholeframeaccuracy,24.1%in ALL argumentsF-Measure,and21.7%in ALL arguments whole frame accuracy. All differences in ALL arguments F-Measure are statistically significant according to a paired Wilcoxon signed rank test. J better than L OCAL (p &lt; .001), L ABEL S EQ is significantly better than J Wilcoxonsignedranktestonper-proposition ALL argumentsF-Measureforallmodels. duetothefactthatthelocalmodelhasseenmanymorenegativeexamplesandtherefore more unique features. The joint features are not very numerous compared to the local features in the joint models. The A LL J OINT model has around 30 %more features than pecially on CORE arguments, increasing the F-Measure on these arguments by two points when added to the J OINT L OCAL model. This shows that even though the local model is optimized to use a large set of features and achieve state-of-the-art performance, it is still advantageous to model the joint information in the sequence of labels in a predicate X  X  argument frame. Additionally, the joint syntactic X  X emantic features improved performance further, showing that when predicting the label of an argument, it is useful to condition on the features of other arguments, in addition to theirlabels.
 is given in Figure 11(a) (Summary results), and the two confusion matrices in Fig-ures 11(b) and 11(c), which display the number of errors of each type that the model made. The first confusion matrix concentrates on CORE arguments and merges all modifying argument labels into a single ARGM label. The second confusion matrix concentrates on confusions among modifying arguments. This figure can be compared to Figure 7, which summarizes the results for the local model in the same form. The biggest differences are in the performance on CORE arguments, which can be seen by comparing the confusion matrices in Figures 7(b) and 11(b). The F-Measure on each of the core argument labels has increased by at least three points: the F-Measure on core argument labels with NONE have gone down significantly, and also there is a largedecreaseintheconfusionsof NONE with ARG 1 .Thereisgenerallyaslightincrease in F-Measure on modifier labels as well, but the performance on some of the modifier labels has gone down. This makes sense because our joint features are targeted at capturing the dependencies among core arguments. There may be useful regularities for modifier arguments as well, but capturing them may require different joint feature templates.
 84.1 %of the propositions, the re-ranking model chose the same assignment that the modelwaspromotedtofirst8.6%ofthetime.Thefigureshowsstatisticsforthetopten assignments only. The rest of the assignments, ranked 11 through 15, were chosen as bestbythere-rankingmodel foratotalof0.3%ofthepropositions.
 fixed by the joint models. The local classifier labeled the first argument in the tree as position. 6. Semantic Role Labeling of Automatic Parses
Wenowevaluateourmodelswhentrainedandtestedusingautomaticparsesproduced by Charniak X  X  parser. The Propbank training set Sections 2 X 21 is also the training set of theparser.Theperformanceoftheparseristhereforebetteronthetrainingset.Whenthe constituentsofanargumentdonothavecorrespondingconstituentsinanautomatically produced parse tree, it will be very hard for a model to get the semantic role labeling correct. However, this is not impossible and systems which are more robust to parser errorhavebeenproposed(Pradhanetal.2005;M ` arquezetal.2005).Oursystemcanalso theoretically guess the correct set of words by labeling a set of constituents that cover 184 theargumentwords,butwefoundthatthisrarelyhappensinpractice.Figure13shows the percentage of argument constituents that are missing in the automatic parse trees produced by Charniak X  X  parser. We can see that the percentage of missing constituents isquitehigh.
 for gold-standard parses, we test on all arguments regardless of whether they corre-spondtoconstituentsthathavebeenrecoveredbytheparserandusethesamemeasures detailed in Section 3.2. We also compare the confusion matrices for the local and joint arguments F-Measure and8.3%in ALL arguments F-Measure. 6.1 Using MultipleAutomatic Parse Guesses
Semantic role labeling is very sensitive to the correctness of the given parse tree, as the results show. If an argument does not correspond to any constituent in a parse tree, or a constituent exists but is not attached or labeled correctly, our model will have a very hardtimeguessing thecorrectlabeling.
 tic role labeling system. The theoretically correct way to propagate the uncertainty of thesyntacticparseristoconsider(sumover)multiplepossibleparsetrees,weightedby their likelihood. In Finkel, Manning, and Ng (2006), this is approximated by sampling parsetrees.Weimplementthisideabyanargmaxapproximation,usingthetop k parse treesfromtheparserofCharniak(2000).
 s withprobabilities P ( t i | s )givenbytheparser.Thenforafixedpredicate v ,let L the best joint labeling of tree t i , with score score SRL model.Thenwechoose thelabeling L whichmaximizes intheuncertaintyoftheparsertosomeextent.However,accordingtothismethod(due totheargmaxoperation)wearechoosingasingleparseandacompletesemanticframe derived from that parse. Other methods are able to derive different arguments of the semanticframefromdifferentsyntacticannotationswhichmaymakethemmorerobust (M ` arquezetal.2005;Pradhan,Wardetal.2005;Punyakanok, Roth,andYih2005). thejointmodel.Theweightingparameterfortheparserprobabilitieswas  X  = 1.Wedid notexperimentextensivelywithdifferentvaluesof  X  .Preliminaryexperimentsshowed thatconsidering 15parses wasabitbetter,andconsidering thetop20wasabitworse. 6.2 Evaluation on the CoNLL 2005 Shared Task
The CoNLL 2005 data is derived from Propbank version I, which is the first official releasein2005,whereastheresultswehavebeenreportingintheprevioussectionsused the pre-final February 2004 data. Using the CoNLL 2005 evaluation standard ensures that results obtained by different groups are evaluated in exactly the same way. In 186
Propbank I, there have been several changes in the annotation conventions, as well as error fixes and addition of new propositions. There was also a change in the way PP arguments are annotated: In the February 2004 data some PP arguments are annotated attheheadNPchild,butinPropbankIallPPargumentsareannotatedatthePPnodes.

In order to achieve maximal performance with respect to these annotations, it would probably be best tochange the feature definitions toaccount for thechanges. However, wedidnoadaptation ofthefeatures.
 is from the Brown corpus (Test Brown). The CoNLL annotations distinguish referring arguments, oftheform R -ARGX ,asdiscussed inSection3.
 identically labeled constituents are part of the same argument was to label constituents with only the set of argument labels and NONE and then map some of these labels into referring or continuation labels. We converted an ARGX into a R -ARGX if and only if the label of the constituent began with  X  X H X . The rule for deciding when to add continuation labels was the same as for our systems for the February 2004 data described in Section 4.3: A constituent label becomes continuing if and only if it is a coreargumentlabelandthereisanotherconstituentwiththesamecoreargumentlabel to the left. Therefore, for the CoNLL 2005 shared task we employ the same semantic role labeling system, just using a different post-processing rule to map to CoNLL-style labelings ofsetsofwords.
 following way: Take the gold-standard CoNLL annotations for the development set (includingreferringandcontinuinglabels),convertthesetobasicargumentlabelsofthe form ARGX ,then convert theresulting labeling toCoNLL-style labeling usingour rules torecover thereferringandcontinuingannotations. TheF-Measure obtainedwas99.0. test sets X  X est WSJ (Section 23) X  X hen using gold-standard parse trees. Performance on gold-standard parse trees was not measured in the CoNLL 2005 shared task, but we reportitheretoprovide abasisforcomparison withtheresultsofotherresearchers. and two test sets. We present results for the local and joint models using the max-scoring Charniak parse tree. Additionally, we report results for the joint model using the top five Charniak parse trees according to the algorithm described in Section 6.1.
The performance measures reported here are higher than the results of our submission in the CoNLL 2005 shared task (Haghighi, Toutanova, and Manning 2005), because of two changes. One was changing the rule that produces continuing arguments to only add continuation labels to core argument labels; in the previous version the rule added continuation labels to all repeated labels. Another was fixing a bug in the way the sentences were passed in as input to Charniak X  X  parser, leading to incorrect analyses offorwardquotes. 8 part of the CoNLL 2005 data (and having wrong forward quotes) in Figure 18. We then report results from the same local and joint model, and the joint model using the top five Charniak parses, where the parses have correct representation of the forward quotes in Figure 19. For these results we used the version of the Charniak parser from 4 May 2005. The results were very similar to the results we obtained with the version from18March2005.Wedidnotexperimentwiththenewre-rankingmodelofCharniak andJohnson(2005),eventhoughitimproves uponCharniak(2000)significantly.
 78.45 on the WSJ Test set. The winning system (Punyakanok, Roth, and Yih 2005) had anF-Measureof79.44andourcurrentsystemhasanF-Measureof80.32.FortheBrown
Test set, our submitted version had an F-Measure of 67.71, the winning system had 67.75,andourcurrent systemhas68.81.
 Charniak parse trees on the Test WSJ test set. The columns show the Precision, Recall,
F-Measure, andthetotalnumber ofarguments foreachlabel. 188 7. Conclusions
In accord with standard linguistic assumptions, we have shown that there are sub-stantial gains to be had by jointly modeling the argument frames of verbs. This is especially true when we model the dependencies with discriminative models capable of incorporating non-local features. We incorporated joint information by using two types of features: features of the complete sequence of argument labels and features modelingdependenciesbetweenthelabelsofargumentsandsyntacticfeaturesofother arguments.Weshowedthatbothtypesoffeaturesyieldedsignificantperformancegains overastate-of-the-artlocalmodel.
 see at least three promising avenues for improvement. First, one could improve the identificationofargumentnodes,bybetterhandlingoflong-distancedependencies;for example,byincorporatingmodelswhichrecoverthetraceandnullelementinformation inPennTreebankparsetrees,asinLevyandManning(2004).Second,itmaybepossible to improve the accuracy on modifier labels, by enhancing the knowledge about the semantic characteristics of specific words and phrases, such as by improving lexical statistics; for instance, our performance on ARGM -TMP roles is rather worse than that of some other groups. Finally, it is worth exploring alternative handling of multi-constituent arguments; our current model uses a simple rule in a post-processing step to decide which constituents given the same label are part of the same argument. This couldbedonemoreintelligentlybythemachine learningmodel.
 performance of current semantic role labeling systems is syntactic parser performance, the more important question is how to improve performance in the presence of parser errors. We explored a simple approach of choosing from among the top k parses from
Charniak X  X  parser, which resulted in an improvement. Other methods have also been proposed, as we discussed in Section 2 (M ` arquez et al. 2005; Pradhan, Ward et al. 2005; Punyakanok, Roth, and Yih 2005; Yi and Palmer 2005; Finkel, Manning, and Ng 2006). Thisisaverypromisinglineofresearch.
 Acknowledgments References 190
