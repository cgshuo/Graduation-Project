 In this paper we propose Regularised Cross-Modal Hashing (RCMH) a new cross-modal hashing model that projects annotation and visual feature descriptors into a common Hamming space. RCMH optimises the hashcode similarity of related data-points in the annotation modality using an iterative three-step hashing algorithm: in the first step each training image is assigned a K-bit hashcode based on hyper-planes learnt at the previous iteration; in the second step the binary bits are smoothed by a formulation of graph regular-isation so that similar data-points have similar bits; in the third step a set of binary classifiers are trained to predict the regularised bits with maximum margin. Visual descriptors are projected into the annotation Hamming space by a set of binary classifiers learnt using the bits of the corresponding annotations as labels. RCMH is shown to consistently im-prove retrieval effectiveness over state-of-the-art baselines. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Locality Sensitive Hashing; Cross-Modal Retrieval
Hashing-based approximate nearest neighbour (ANN) search has emerged as an effective technique for efficiently find-ing nearest neighbours in large multimedia data collections. Data-points are transformed into compact binary codes via projection [6] and quantisation [7] operations that ensure similar data-points are assigned hashcodes with low Ham-ming distance. Hashcodes of length K are generated by learning K hyperplanes within the data-space. Depending on which side of a hyperplane a data-point falls its hashcode is appended with a 1 or 0. Each subspace formed by the K hyperplanes constitutes the bucket of a hashtable. Similar-ity preserving hashcodes can therefore be used as the indices into the buckets of a hashtable for constant time search: a given query need only be compared to data-points falling within the same bucket vastly cutting down the search space.
Most previous hashing research has focused on generating binary codes for data-points within the same modality, for example, a text query executed against a database consisting of textual documents. However it is frequently the case that similar data-points exist in different modalities, for exam-ple a Wikipedia page discussing Einstein and an associated image of the scientist. An interesting research question is whether an effective hashing scheme can be constructed to learn hashcodes that are also similar across disparate modal-ities -in this case the Einstein Wikipedia article will ideally be assigned a similar hashcode to the relevant embedded image. Hashing methods that effectively bridge the cross-modal gap will enable the efficiency of ANN search to be expanded to cross-modal data.

In this paper we propose Regularised Cross-Modal Hash-ing (RCMH), an extension of the unimodal hashing model Graph Regularised Hashing (GRH) [6]. RCMH employs a three-step iterative scheme to learn a set of K hash func-tions: in the first step hashcodes are assigned to the training images using K learnt hyperplanes from the last iteration; in the second step a formulation of graph regularisation [2] re-fines the distribution of annotation binary bits so that anno-tations from neighbouring images are assigned similar bits. In the third step K binary classifiers are trained to predict the regularised bits with maximum margin. RCMH forms a cross-modal bridge by subsequently projecting visual de-scriptors into the learnt annotation Hamming space: this is achieved by learning K binary classifiers in visual space with the bits of the associated annotations as labels.
Cross-modal hashing research has received increased inter-est over the past several years due to the recent emergence of large freely available cross-modal datasets from sources such as Flickr. Existing cross-modal hashing schemes seek to jointly preserve the within-modality and between-modality similarities of related data-points in a shared Hamming space. This requirement is frequently solved by learning two sets of K hyperplanes that partition each space into buckets in a manner that yields similar hashcodes for similar data-points both within and across modalities.

Cross-Modal Semi-Supervised Hashing (CMSSH) [1] inte-grates eigendecomposition and boosting to learn a common multi-modal space for hashing. Cross-View Hashing (CVH) [5] employs Canonical Correlation Analysis (CCA) [4] to learn a shared latent space from two modalities. The au-thors of [10] proposed Co-Regularised Hashing (CRH) that learns hash functions for each bit by solving DC (difference-of-convex function) programs, with multiple bits learnt via boosting. Inter-Media Hashing (IMH) [9] minimises a loss function consisting of graph Laplacian terms for intra-modal similarity, a trace minimisation term for inter-modal simi-larity and linear regression for out-of-sample prediction. The closest related work to RCMH is Predictable View Hashing (PDH) [8]. PDH employs an iterative scheme for hashcode learning: the annotation bits are used to learn K hyperplanes for the image modality, while the image bits are used to learn K hyperplanes for the annotation modal-ity. RCMH is different to PDH in several aspects, including our novel method of bridging the modalities, our integration of graph regularisation [2] and the lack of an eigendecompo-sition step. We show that RCMH is much more effective.
Let D = { ( a i , v i ) : i = 1 . . .N } denote a collection of N annotated images. Each image is represented by two com-ponents: the annotation a i , and the visual descriptor v The annotation a i is a vector over textual features. Visual descriptor v i is a vector of real-valued visual features. Our goal is to learn a pair of hash functions F, G that map an-notations and visual descriptors into binary hashcodes con-sisting of K bits. We impose two constraints on our hash functions: (i) the annotation hashcode F ( a i ) should be sim-ilar to the visual hashcode G ( v i ) of the same image; and (ii) the annotation hashcodes F ( a i ) and F ( a j ) should be simi-lar whenever images i and j are considered neighbours . The neighbourhood structure for the collection is dictated by an affinity matrix S , where S ij = 1 indicates that i and j are neighbours, and S ij = 0 indicates they are not. Our approach is based on a unimodal method GRH [6]. GRH is restricted to a single modality, while we propose a method that learns a pair of hash functions across two sep-arate modalities: text annotations a i and visual descriptors v . The hash functions F, G are based on K hyperplanes each: f 1 . . . f K for the space of words and g 1 . . . g space of visual features. The hyperplane f j is used to assign the j  X  X h bit in the annotation hashcode, while g j determines the j  X  X h bit in the visual hashcode. We initialise all hyper-planes randomly, and iteratively perform the following steps: (1) hashing , where the hyperplanes f 1 . . . f K are used to as-sign hashcodes b 1 . . . b N to the training images, (2) regu-larisation , where the hashcodes b 1 . . . b N are made more consistent with the affinity matrix S and (3) partitioning , where we adjust the hyperplanes f j , g j to be consistent with the j  X  X h bit of the hashcodes from step (2).
We start by assigning a K -bit binary hashcode b i to each training image i . Each of the K bits in b i is based on a dot product between the image annotation a i and one of the hyperplanes f 1 . . . f K : Here H ( x ) is the Heaviside step function that returns 0 for negative x and 1 otherwise. At test time, hashcodes of visual features G ( v i ) can be computed in the same manner, but using the visual hyperplanes g 1 . . . g K .
The aim of this step is to make the hashcodes we obtained in step 1 more consistent with the affinity matrix S . Specif-ically, whenever images i and j are neighbours, we would like the hashcodes b i and b j to be similar in terms of their Hamming distance. We achieve this by interpolating the hashcode of image i with the hashcodes of all neighbouring images j for which S ij = 1. Our approach is similar to the score regularisation method of [2]. Formally, we regularise the hashcodes via the following equation: Here S is the affinity matrix and D is a diagonal matrix containing the number of neighbours for each image. The matrix B  X  X  0 , 1 } N  X  K represents the hashcodes assigned to every image in step 1 of the algorithm and  X  is a free parameter that specifies how aggressively we regularise the bits. We show our approach intuitively in Figure 1. In the left side we show 5 images a. . .e with their initial hashcodes ( K =2 bits for this example). The lines between images re-flect the neighbourhood structure encoded in the affinity matrix S . Image d has a hashcode 01, but its neighbours b, c, e have hashcodes 00, 11 and 10 respectively. The right side of Figure 1 shows the effect of equation (2) for image d : its hashcode changes to 10, which is more consistent with neighbouring hashcodes (on average).
In the final step of the algorithm, we re-estimate the hy-perplanes f 1 . . . f K and g 1 . . . g K to make them consistent with the regularised hashcodes from step 2 of the algorithm. For each bit j = 1 . . .K , we treat the values b 1 j . . .b Nj training labels. Specifically, if b ij = 1 then the annotation vector a i constitutes a positive example for the hyperplane f , and the visual vector v i is a positive example for g j b ij = 0 then a i and v i are negative examples for f j and g Each hyperplane is learned using liblinear [3] to maximise the margin between positive and negative examples.
The approach is illustrated in Figure 2. We show five im-ages a. . .e in two sets of coordinates: the word space on the left and the visual feature-space on the right. Each image is associated with a 2-bit hashcode, and each bit is used to learn a maximum-margin hyperplane that bisects the corre-sponding space. For example, the first bit has value 0 for images a, b and value 1 for images c, d, e , giving rise to hy-perplanes f 1 and g 1 , shown as dark lines on the left and the right parts of Figure 2. Note that f 1 and g 1 look very different, because they are defined over two completely dif-ferent modalities: words on the left and visual features on the right. Similarly, the second bit results in the hyperplanes f and g 2 , shown in lighter colour.
We repeat steps 1-3 above for a small number of itera-tions M . We briefly describe how the steps enforce the two constraints we imposed on our hash functions in section 3.1. Constraint (i) is enforced in step 3 of the algorithm, when we use the same bit values b ij as targets for the word hy-perplanes f j and visual hyperplanes g j . Any image i will either be a positive example for both hyperplanes, or it will yield the same bit value as v  X  i g j . Constraint (ii) is enforced in step 2 of our procedure, where the hashcode for image i is moved towards the centroid hashcode of its neighbours. The centroid (before it is binarised) is a point that minimizes ag-gregate Euclidean distance to the neighbours, so after step 2 hashcodes b 1 . . . b N are expected to be more consistent with the neighbourhood structure S .
We evaluate RCMH on two publicly available benchmark 2,866 Wikipedia articles. Each article is described with text and an associated image. The visual modality is formed from 128-bit SIFT descriptors, while the annotation modal-ity is represented as 10-dimensional probability distribution over LDA topics. NUS-WIDE is a web image dataset con-sisting of 269,648 images downloaded from Flickr. We keep the image-text pairs associated with the most frequent 10 classes and perform PCA on the tag co-occurrence features to form a 1,000-dimensional annotation feature set [10] . Each image is represented as a 500-D bag-of-words vector derived from SIFT descriptors. The ground truth nearest neighbours are based on the semantic labels supplied with the datasets, that is, if two images share a class in common they are regarded as true neighbours [10, 9]. Following pre-vious work [10, 9] we randomly select 20% (Wiki) and 1% (NUS-WIDE) of the data-points as queries with the remain-der forming the database over which our retrieval experi-ments are performed. We randomly sample 20% (Wiki) and 1% (NUS-WIDE) of the data-points from the database to form the training dataset ( T ) to learn the hash functions. CVH [5], CMSSH [1], CRH [10], IMH [9] and PDH [8].
We evaluate RCMH based on two cross-modal retrieval tasks: 1) Image query vs. text database: an image is used to http://www.svcl.ucsd.edu/proj ects/crossmodal/ http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm
Image Query v s.
 Text Database
Text Query v s.
 Image Database Table 1: mAP scores for Wiki ( T = 574).  X  indicates statis-tical significance vs. PDH (Wilcoxon: p-value &lt; 0.01). retrieve the most related text in the text database; 2) Text query vs. image database: a text query is used to retrieve the most similar images from the image database. Retrieval accuracy is measured using Hamming ranking [10, 9]: binary codes are generated for both the query and the database items and the database items are then ranked in ascending order of the Hamming distance. We evaluate using mean average precision (mAP). Results are the average over 10 random query/database partitions.
RCMH has three meta-parameters: the number of iter-ations M , the amount of regularisation  X  and the flexibil-ity of margin C . We optimise all meta-parameters via grid search on the validation dataset. Holding the margin pa-rameter constant at C = 1, we perform a grid search over and  X  constant at their optimised values, and sweep C  X  and 1). The IMH  X  and  X  parameters are set via grid search over the range  X  10  X  6 , 10  X  3 , 1 , 10 3 , 10 6  X  . The CRH  X   X  y parameters are tuned over the range { 0 . 001 , 0 . 01 , 0 . 1 , 1 } in an identical manner to RCMH.
Our cross-modal retrieval results are presented in Tables 1-2. We observe that RCMH outperforms the baseline sys-tems on both datasets and across all hashcode lengths. For example, for image-text retrieval, RCMH outperforms PDH by a substantial 16% relative mAP at 24 bits on the Wiki dataset and 9% on the NUS-WIDE dataset. We test the statistical significance of the gain in mAP vs. PDH using a Wilcoxon signed rank test on the mAP scores resulting from each random query/database partition: the difference is statistically significant for p &lt; 0 . 01. This is an encourag-ing result: RCMH is devoid of a computationally expensive eigendecomposition step as most baselines [9, 8, 5], relying instead on graph regularisation to maintain the neighbour-hood structure.
In this paper we introduced Regularised Cross-Modal Hash-ing (RCMH). RCMH employs an iterative three-step scheme
Image Query v s.
 Text Database
Text Query v s.
 Image Database Table 2: mAP scores for NUS-WIDE ( T = 1866).  X  indicates statistical significance vs. PDH (Wilcoxon: p-value &lt; 0.01). to learn a shared multi-modal Hamming space: in the first step hashcodes are assigned to images based on learnt hyper-planes; in the second step hashcodes within the annotation space are refined by updating a node X  X  hashcode to be the average of the hashcodes of its nearest neighbours. In the third step RCMH learns a set of hyperplanes to partition the annotation space into buckets using the bits from the previ-ous step as labels. Visual descriptors are projected into the annotation Hamming space by learning a set of hyperplanes in the visual space using the associated annotation bits as labels. RCMH outperforms a set of strong cross-modal hash-ing baselines. [1] M. M. Bronstein, A. M. Bronstein, F. Michel, and [2] F. Diaz. Regularizing query-based retrieval scores. In [3] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [4] D. R. Hardoon, S. R. Szedmak, and J. R.
 [5] S. Kumar and R. Udupa. Learning hash functions for [6] S. Moran and V. Lavrenko. Graph regularised [7] S. Moran, V. Lavrenko, and M. Osborne.
 [8] M. Rastegari, J. Choi, S. Fakhraei, H. D. III, and L. S. [9] J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen. [10] Y. Zhen and D. yan Yeung. Co-regularized hashing for
