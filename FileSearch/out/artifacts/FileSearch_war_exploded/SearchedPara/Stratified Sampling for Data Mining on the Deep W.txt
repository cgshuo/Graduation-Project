 }
In recent years, one mode of data dissemination has become extremely popular, which is the deep web . Deep web is the term coined to describe the contents that are stored in the databases and can be retrieved over the internet by querying through HTML forms, which are also called query interfaces . Deep web data sources are playing an important role in the modern society, impacting practically every Internet user. An early study on the deep web, conducted in year 2000, estimated that the public information in the deep web is 500 times larger than the surface web, with 7,500 Terabytes of data, across 200,000 deep web sites [5].

Deep web data sources have a unique characteristic, which is that data can only be accessed through the query interface they support. These interfaces are based on input attribute(s) , and a user query involves specifying value(s) for these attributes. In response to such a query, dynamically generated HTML pages returned as the output, comprising output attributes . The deep web has received much attention lately [7], [9]. A number of recent efforts have also been building deep web querying systems[7], [9], trying to provide mediator-like support. However, one issue that has not been considered much is mining data available in one or more deep web data sources. Clearly, like any other data sources, data mining on the deep web can produce important insights or summary of results. For example, there may be interest in analyzing data available from two different travel web-sites, and summarizing the cases for which one can offer lower fares than the other.

Data mining on the deep web is challenging because the databases cannot be accessed directly. Thus, data mining must be performed based on sampling of the datasets. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs. Though sampling for efficient data mining has been widely studied [23], [21], [11], [6], these methods are not directly applicable, as samples can only be obtained by issuing queries with specific inputs on the deep web. At the same time, there have been recent efforts on sampling the deep web [3], [13], [15]. However, none of these have been in the context of mining the deep web. Web mining is another widely studied topic [20], but refers to analysis of the surface web. The deep web involves a distinct set of challenges.

In this paper, we target two related data mining problems, and develop sampling methods to efficiently mine deep web data sources. The first is frequent itemset mining [2], [26], one of the most widely studied problems in the data mining community. The second is a somewhat similar problem, differential rule mining . The goal in differential rule mining is to obtain a summary of the comparison of values of attributes between two data sources. A differential rule is of the form X  X  D 1 ( t ) &gt;D 2 ( t ) is a frequent itemset composed of identical attributes , and differential attribute in the two data sources. This rule indicates that given the occurrence of the itemset X , the value of attribute t from the data source D 1 is significantly larger than that from the data source D 2 . Here, identical attributes are attributes whose values are same in these two data sources, whereas differential attributes are the attributes whose values are different.
Sampling for frequent itemset mining has been studied by several researchers [23], [21], [11], [6], but the current work has been in the context of relational databases or streaming data, where data records are visible and samples are easy to obtain. An intuitive method for the deep web will be to identify rules from a pilot random sample and further obtain samples to verify these rules. This method seems similar to the algorithm proposed by Toivonen [23]. However, since the data distribution in the backend database is unknown and the data records are not directly accessible, the sampling procedure is quite challenging. For example, to verify a discovered association rule A = a  X  Y more data records containing A = a are required. If A is an output attribute from the deep web, obtaining such data records is quite hard, as the deep web data source will only accept values of input attributes in the queries. Randomly issuing more queries may yield desired data records, but it may need a large and unknown number of queries to collect a certain number of desired data records. Acquiring data from the deep web is especially time consuming, since deep web queries are executed over a wide area network. A recent study from a deep web integration system shows that nearly 80% of the execution time is spent on data delivery between the server and the clients [24]. Thus, to reduce the computation cost and the network connection time, sampling methods must be efficient and effective.

In this paper, we introduce the stratified sampling method to support association rule mining and differential rule mining on the deep web. Our approach includes novel methods for both stratification and sample allocation . As stated earlier, we first pick a pilot random sample from the deep web for identifying interesting rules. Then, the data distribution and relation between input attributes and output attributes are learnt from the pilot random sample. After that, stratification and sample allocation is conducted on the deep web.

Traditional stratified sampling methods [12], [22], [8] aim at minimizing variance of estimation, which is an indication of estimation accuracy. However, in order to conduct stratified sampling in the context of deep web, efficiency or sampling cost , i.e., the number of distinct queries that need to be issued, is also an important consideration. We define an integrated cost that takes into account both the variance of estimation and sampling cost. We also allow users to specify different weights to these two factors.

Stratification , which aims at dividing the entire population into sub-populations, is conducted on the query space of deep web data sources based on the knowledge obtained from the pilot random sample. The query space is recursively stratified in order to minimize the integrated cost in a greedy way. At each step, the input attribute that reduces the integrated cost maximally is chosen and the query space is divided. For sample allocation , i.e., for deciding how many data records should be sampled from each stratum, we propose an algorithm which optimally minimizes the integrated cost.

Our experimental results using two real datasets show the following. First, compared with simple random sampling on the deep web, our algorithm has a higher sampling accuracy and a lower sampling cost. Second, our algorithm efficiently and consistently reduces sampling cost by trading off a certain fraction of estimation error, compared to approaches based on adapting the traditional stratified sampling methods for the deep web.

The rest of the paper is organized as follows. Section II gives a brief overview of association rule mining and differential rule mining. Section III formulates our sampling problem, and then gives background on stratified sampling and the well-known Neyman allocation method. Section IV describes our proposed methods for stratification and sample allocation. Section V reports the results from our detailed evaluation study. We compare our work with related research efforts in Section VI and conclude in Section VII.

In this section, we describe the background knowledge about association rule mining and differential rule mining problems.
The goal of association rule mining algorithms is to discover the co-occurrence patterns for items. The key step is finding frequent itemsets X whose support is larger than a predefined threshold . Based on frequent itemset mining, association rule mining discovers rules in the form of X  X  Y where X  X  Y is a frequent itemset and the confidence , support ( X  X  Y than another specified threshold .

Somewhat related to association mining, differential rule min-ing is proposed to discover the patterns in the differences between the values for the same entity provided by different deep web data sources. Summarizing such a difference is clearly important for the users of these data sources. For example, users will benefit from a summary information stating that one website routinely offers rooms at hotels of certain type, and/or in cities within a certain geographical region, at a lower price. This can be viewed as a variation of contrast set mining [4], [16], with emphasis on differential values of numerical attributes.

More specifically, in differential rule mining, the goal is generating a high-level summary of different values of target attributes , by comparing values for the same data entities across different sources, under different combinations of conditions. We consider two types of attributes when comparing data objects. Identical attributes are the attributes whose values are the same when considering the same data object obtained from different data sources. These attributes help identify the same data objects. Differential attributes or target attributes are the attributes whose values could be different for the same data object. For example, in finding differential rules across travel-related deep web data sources, hotel name and location are identical attributes, whereas price is a differential or target attribute.

A differential rule is of the form X  X  D 1 ( t ) &gt;D 2 ( t ) and shows the difference in the values of the target attribute t , across the data sources D 1 and D 2 , under the condition which is captured by the values of identical attributes. In order to identify differential rules, there are two main steps: 1) Frequent itmesets mining which discovers the frequent itemsets composed of identical attributes 2) Having identified the frequent itemsets, a hypothesis test, paired Z-test , is conducted to find significant differential pattern of the values for each target attribute. The patterns of values for the target attribute is considered to be significantly differential if the mean of the difference between its values in the two data sources is significantly larger or smaller than 0.

Besides analysis of travel-related websites, differential rule mining has several other applications. For example, it can be used for comparing the values of attributes for the same entity under different circumstances, i.e. sales amount of the same product in different years, or comparing the values of different attributes for the same entity, i.e. incomes of husband and wife in the same household.
In this section, we formulate the sampling problem we are addressing in this paper, and then describe the basic idea of applying stratified sampling [17] in data mining problems, as well as one of the well known sampling allocation method, Neyman Allocation [12].
 A. Problem Formulation
For mining any deep web source, the sampling process will involve two steps:  X  A pilot sample is randomly sampled from the deep web, and
While applying association rule mining and differential rule mining on the deep web, an item is represented as A = a , where A is an attribute that is assigned the value a . Both input and output attributes can be involved. Identified association rules are of the form X  X  Y , and identified differential rules are of the form X  X  D 1 ( t ) &gt;D 2 ( t ) , where X is a frequent itemset. To verify an identified rule, which could be an association rule or a differential rule, additional data records are required under the space of X .If X only contains input attributes, the task of sampling is easy. Queries with values of items in X can be randomly selected and submitted to the query interface. However, if
X involves output attributes, the sampling problem becomes more complicated, because of the limited interfaces supported by deep web data sources. These interfaces only accept queries with values of input attributes, and do not have mechanism to find records with specific values of output attributes. Our goal is to achieve high estimation accuracy with a low sampling cost, and random sampling is not a satisfactory solution.

To simplify the presentation of our approach, we focus on the rule whose left hand side only contains a single output attribute, i.e. X is { A = a } , where A is an output attribute. However, our approach is general, and can consider multiple different output and/or input attributes in the rule. Our method tries to learn the relation between output attributes and input attributes, and conduct stratified sampling in the query subspaces composed of input attributes. If there are multiple different output and/or input attributes in the rule, which can be considered as a composite output attribute, our method is still able to learn the relation between the composite output attribute and input attributes except those included in the composite output attribute, and then conduct stratified sampling as before.

While considering association rule mining, identified associa-tion rules are in the form of A = a  X  Y , with support s ( which is the probability of Y given A = a in the pilot sample, being larger than a threshold. Our goal is to obtain more data records under the space of A = a to more accurately estimate the methods that can achieve high accuracy of estimation, with small sampling cost, or while acquiring as few samples as possible.
If our goal is differential rule mining, the problem is quite similar. We focus on the differential rules in the form of a  X  D 1 ( t ) &gt;D 2 ( t ) , where A is an output attribute of the deep web data source, and an identical attribute for our formulation. is a differential attribute, whose value we are comparing across the two data sources, D 1 and D 2 . The differential rule indicates the behavior of differential attribute t given A = a . Thus, our goal is to design sampling plans for accurately estimating the mean value for D 1 ( t )  X  D 2 ( t ) ,given A = a .
 B. Stratified Sampling and Neyman Allocation
Before presenting our approach, we show how the stratified sampling is applied in data mining problems. Unlike simple random sampling , which draws a sample from the population in entirety, stratified sampling [17] picks separate samples from H groups, which are also called strata or sub-populations . When data varies considerably across sub-populations, and the variance within each sub-population is small, it is advantageous to sample each sub-population (stratum) independently.

There are two considerations in reducing the variance in stratified sampling: stratification , which is the process of dividing the entire population into sub-populations, and sample allocation , which determines the size of sample drawn from the i th stratum. A well known method for sample allocation is the Neyman Allocation [12]. Neyman allocation is based on the observation that the variance of the stratified estimation is minimized when sample size for the i th stratum is proportional to the size of the stratum and to the variance of the target values in the i Target values have different meanings for different data mining problems.
 Now, let us consider the application of stratified sampling and Neyman allocation to differential rule mining and association mining. For differential rule mining in the form of A = a  X  D 1 ( t ) &gt;D 2 ( t ) , the goal is to estimate the mean value for D 1 ( t )  X  D 2 ( t ) given A = a over the entire population. Let denote the number of data records sampled across the strata, and n denote the number of data records drawn from the i th stratum. The size of population containing tuples under the space of is denoted by N i for the i th stratum. Now, let the expression denote D 1 ( t )  X  D 2 ( t ) given A = a and  X  y i denote the sample average of the i th stratum. Then, using stratified sampling, the mean value of y is estimated as It turns out that  X  y s is an unbiased estimation , which implies that E (  X  y s )= E ( y ) .If  X  2 i is the variance of the variance for  X  y s is: In the stratum whose variance is unknown,  X  2 i is estimated by sample variance, i.e, Here, y ij is a sampled record from the i th stratum.
These expressions are also applied to association rule mining in the form of A = a  X  Y , with the goal being to estimate the the expression y to denote whether Y is contained in the data record under the space of A = a : if the data record under space of
A = a contains Y , let y =1 ; otherwise, y =0 . The confidence for the association rule is calculated by estimating the mean value of y based on Expression 1, and the variance of estimation for the confidence over the entire population is still calculated by Expression 2.

Formally, with Neyman allocation, the sample allocation can be stated as: where  X  i are calculated as described above.

While Neyman Allocation efficiently reduces the variance of estimation, it does not consider the sampling cost in the deep web. Specifically, sampling cost is defined as the number of queries submitted to deep web data sources in order to obtain a fixed number of data records under the particular space. We assume that for each query submitted to the deep web, a single data record is obtained. In the process of sampling data records corresponding to A = a over the deep web, the probability of A = a in each stratum is not considered in Neyman allocation (or other existing methods). On a deep web source, it is quite possible that a large sample might have to be drawn from a stratum where the probability of a data element satisfying A = a is small. This will result in a high sampling cost.
 The following example shows the inefficiency of Neyman Allocation in the case of differential rule mining on the deep web. For a rule A = a  X  D 1 ( t ) &gt;D 2 ( t ) , we assume there are three strata. The columns one through four of the Table I show the stratum ID, population size ( N i ),  X  2 i , which is the variance of D 1 ( t )  X  D 2 ( t ) , and  X  i , which enotes the probability of being true in any given record in this stratum.

By assuming that the total sample size is 600, the sample size n according to Neyman Allocation for each stratum is shown in the fifth column of Table I. According to the Expression 2, the resulting estimation is 66.67, and the estimated sampling cost is 3361.1. In this example, the sampling cost is very high, because a large sample is allocated to the stratum 3, which happens to have a low probability of A = a . If we could allocate a smaller sample to the stratum 3 and a larger sample to the strata 1 and 2, the sampling costs would be decreased, though possibly with some increase in the sampling variance.
 C. Our Approach
As we stated in the previous section, randomly constructing and submitting queries can not ensure that the output results contain data records within the space of A = a . Thus, efficiently acquiring samples under the space of output attribute A = a and accurately estimating the parameters (confidence or mean value) of target attributes under the space of A = a both become challenging problems.

Since sampling on the deep web can only be performed by constructing and submitting queries, stratification needs to be performed on the query space, i.e., involving input attributes only. In order to efficiently acquire samples under the space of a specific value of an output attribute (i.e. A = a ), the relation between input attributes and output attributes is important. In other words, we need to know which input queries are more likely to yield output web pages containing tuples under the space of A = a . Furthermore, in order to accurately estimate the parameters (confidence or mean value) of the target attributes under the space of A = a , similar values of target attributes under A = a are required to be in the same group. For this purpose, we are proposing a greedy stratification method. Our method considers both efficiency and accuracy, so that input queries that are more likely to yield output web pages containing tuples under the space of A = a are identified, and similar values of target attributes under A = a are grouped in the same sub-population. In our approach, the relation of input attribute and output attribute as well as the data distribution of target attributes are learnt from the pilot sample.

The second component of our approach is a novel sample allocation method . To efficiently acquire samples under the space of output attribute A = a , large sample should be drawn from the query subspace that is more likely to yield output web pages containing tuples under the space of A = a . On the other side, to accurately estimate the parameters (confidence or mean value) of target attributes under the space of A = a , a large sample should be drawn from the query subspace that has a larger variance of the target value under the space of A = a . We have developed an optimized sample allocation algorithm which integrates the two aspects (efficiency and accuracy).

In this section, we introduce our stratified sampling approach in the context of association rule mining and differential rule mining on deep web data sources. As we stated earlier, our approach comprises of a stratification method and a sample allocation method. These are described in the next two subsections. Towards the end of this section, we describe how differential rule mining can be performed using these two methods.
 A. A Greedy Stratification Method
Stratification refers to partitioning the population being sam-pled. Traditional stratification methods such as Recursive strat-ified sampling [22], [8] aim at grouping similar values into the Algorithm 1 Stratification( N,Q,PS,s,Lf ) same subpopulation , in order to maximally reduce variance of estimation. As we have stated previously, two related issues make the sampling for a deep web data source different. The first is because of the limited access to the database, i.e., samples can only be obtained by querying through a simple interface. Thus, obtaining additional samples meeting A = a is non-trivial. Second, efficiency of sampling is an important consideration, since each deep web query is slowed down due to the data delivery delay between the server and the client.

We now describe our greedy stratification method which par-titions the population of the deep web recursively. Stratification is performed on the query space composed of input attributes in the query interface. In other words, the population of a deep web source can be considered to correspond to the entire query space, and the subpopulations correspond to query subspaces. More precisely, a subpopulation comprises of the data records that can be obtained by submitting queries from the corresponding subspace.

To establish this correspondence, the relationship between input attributes and output attributes is learnt from the pilot sample. In this fashion, the subpopulations or strata that are more likely to contain tuples where A = a is true are identified. We define sampling cost to be the estimated number of data records that will need to be drawn from the deep web, in order to obtain a specified number of records where A = a is true. Let  X  i represents the probability of finding a data record with A = a in the stratum i . This value can be estimated based on the pilot sample. Thus, to obtain n i data records where A = a is true in i th stratum, the estimated number of data records drawn from i th stratum is n i  X  i . Overall, in order to obtain our desired records where A = a is true from the entire population, the estimated sampling cost will be:
While focusing on the sample cost and using the strata that are more likely to contain tuples where A = a is clearly important, variance of estimation is another important issue, since it corresponds to the error of estimation. Thus, we define the integrated cost by taking into account the variance of estimation s and the sampling cost where  X  s and  X  v are weights for sampling cost and estimate variance, respectively, such that their sum is 1. Users could set the two weights based on the importance they attach to these two factors. For example, if response to a query is needed quickly, sampling cost should have a higher weightage.

The relation between input attributes and output attributes, as well as data distribution, are modeled by a tree built on the query space recursively. Each node in the tree represents a query subspace, and is associated with a query that comprises specific values for a subset of input attributes, the sample size, as well as the potential splitting attributes. The sample size shows how many data records would be drawn from the subpopulation of the node.

Algorithm 1 shows the process of splitting a node N , which as we stated above, is associated with the query Q , the sample size s , and a list of potential splitting attributes, PS . The inputs of the algorithm also include set Lf , which represents leaf nodes of the tree. At the beginning, the entire query space is represented by the root node. The corresponding query of the root node is null , the sample size n is the size of the sample that is to be drawn from the entire population, and the potential splitting attributes list is the complete set of input attributes of this data source. The initial set of leaf nodes is empty.

The main goal of the algorithm is to split the query tree in a greedy way. For each potential splitting attribute, the integrated cost after splitting is computed according to the Expression 5. The input attribute which brings the most reduction in the integrated cost is selected. More specifically, first, the integrated cost for the subpopulation of the node N is calculated. If this value is smaller than a predefined threshold  X  , node N is set to be a leaf node (Lines 2-5). Otherwise, node N will be partitioned by considering the set of potential splitting attributes PS (Lines 6-21). For each input attribute IA  X  PS with domain D IA , there would be | D IA | strata under the space of node N . The integrated cost on | D IA | strata is computed. Let M  X  PS with domain denote the input attribute with the minimum integrated cost after the split, and let the set of allocated sample sizes, computed by the sample allocation method explained later, be AS M . Then, | D M | children are generated for the node M . For each child CH i ,i =1 ,..., | D M | , the associated query is Q N  X  X  M = m sample size is as i  X  AS M , and potential splitting attributes is PS N  X  M . The process of splitting is then recursively applied to children of node N .

In the process of calculating costs during the stratification process, we need to perform sample allocation , i.e., divide the parent node X  X  sample size among the potential children nodes. This is required for calculating the integrated cost for the potential split. This is based on our sample allocation method, which we describe next in Section IV-B. Furthermore, for calculating the integrated cost, the variance of target value  X  2 i and probability of output attribute A = a ,  X  i , for each stratum is computed based on the pilot sample.

Initially, the stratification process on the query space begins by calling Stratification ( R, null, FIA, n, null ) , where R root node. The process of stratification would stop if there is no leaf node with integrated cost larger than a predefined threshold  X  . Each leaf node in the tree is a final stratum for sampling, and the associated sample size denotes the number of data records drawn from the subpopulation of the stratum.
 B. An Optimized Sample Allocation Method
Now, we introduce our optimized algorithm for sample allo-cation which integrates variance reduction and sampling cost. As introduced in section IV-A, integrated cost is defined by taking into account of variance of estimation and sampling cost. The goal of our sample allocation algorithm is to minimize the integrated cost by choosing the sample size, n i , for each stratum. In our algorithm, we adjust the value of SampCost and  X  2 that their influences on the integrated cost are in the same unit: where SmpCost ( r )= n  X  r is the expected sampling cost of the entire population, and  X  r denotes the probability of A = a true for the entire population. where  X  2 r =  X  2 n denotes the variance of estimation of the target value on the entire population.

The key constraint on the values of the sample sizes for each strata is that their sum should be equal to the total sample size. A vector n = { n 1 ,n 2 ,...,n H } is used to represent the sample sizes, where the i th element, n i , is the sample size for the stratum.

By including sampling cost and variance of estimation into the integrated cost, our sample size determination task leads to the following optimization problem: where N i denotes the population size of data records under the space of A = a in i th stratum. Note that this value may not be known if A is an output attribute. However, the total number of records in the i th stratum is typically known, and can be denoted as DN i . Then N i can be estimated by  X  i  X  DN i population size of A = a on the entire population is estimated by N = i  X  i  X  DN i .

For finding the minimum of integrated cost, we utilize La-grange multipliers , a well know optimization method. Lagrange multipliers aims at finding the extrema of a function object to constraints. Using this approach, a new variable  X  called a Lagrange multiplier is introduced and defined by: If n is a minimum solution for the original constrained problem, then there exists a  X  such that ( n,  X  ) is a stationary point for the Lagrange function. Stationary points are those points where the partial derivatives of  X ( n,  X  ) are zero: In our problem, by conducting partial derivatives on Formula 7, a group of equations are yielded as follows: where the solution n leads to the minimum value of integrated cost in Formula 5.
 However, it is difficult to solve the group of equations directly. Thus, we use numerical analysis to approximate the real solu-tion. Newton X  X  method is utilized for finding successively better approximations to the zeroes (or roots) of a real-valued function. Given an equation f ( x )=0 with f ( x ) which denotes the deriva-tive of function f ( x ) , and x 0 , a first guess of the root. Newton X  X  method iteratively provides, x t +1 , a better approximation of the root, based on x t , the previous approximation of root according to the following formula: The iteration is repeated until a sufficiently accurate value is reached, i.g. | f ( x t ) | is smaller than a predefined threshold. In our problem of Formula 8, there are H +1 equations F ( x t )= { f 1 ( x t ) , ..., f H +1 ( x t ) } on H +1 variables { n 1 , .., n H , X  } , where the equations in F ( x t ) are as follows: The Newton X  X  method is also applied iteratively via the system of linear equations: where J F ( x t ) is the ( H +1)  X  ( H +1) Jacobian matrix of the equation system F ( x t ) by assigning variables x with values of vector x t . The entry J F ( i, j ) is calculated by d ( f i f ( x ) is the i th equation of F ( x t ) and x j is the j th element of x .

From the Expression 9, a better approximation x t +1 is obtained based on previous approximation x t . The iterative procedure would be stopped if  X  i | f i ( x t ) | is smaller than a predefined threshold, and then the sample size n i is allocated for each stratum so that the integrated cost is minimized. In reality, we are required to pick an integral number of records from each stratum during the sampling step. Thus, we round down each to its nearest integer, n i +0 . 5 .

In the example shown in Table I, suppose we set both weights  X  v and  X  s to be 0.5. Further, assume the variance of the entire population,  X  2 r , to be 80000. The probability of A = a entire population  X  r is 0.242. By using the proposed optimized sample allocation method, the sample sizes for the three strata are 162, 299, and 139, respectively. In this case, the variance of estimation according to the Expression 2 is 93.66, and the estimated cost is 1943.7. We can see that, compared with Neyman allocation, the sampling cost is decreased by 42.1%, but results some increase in variance. Overall, this example shows that we can achieve lower sampling cost by trading off some accuracy. C. Overall Sampling Process Algorithm 2 DiffRuleSampling( DW 1 ,DW 2 ,FIA,t,S t )
Algorithm 2 shows the overall sampling process for differential rule mining on two deep web data sources, DW 1 and DW 2 , and with differential attribute t . The inputs of the algorithm also contain the full set of input attributes FIA as well as the sample size S t . The algorithm starts with a pilot sample PS , from which the differential rules are identified. For the rule R : X  X  DW 1( t ) &gt;DW 2( t ) only containing input attributes, S t data records are directly drawn from the space of X (Lines 5-6). However, for the rule R : X  X  DW 1( t ) &gt;DW 2( t ) containing output attributes, query spaces of DW 1 and DW 2 are stratified and sample is recursively allocated to each stratum with corresponding query subspace (Line 10). For each leaf node of the tree built by stratification, a sample is drawn according to its sample size (Line 13). The mean value of DW 1( t )  X  DW 2( t ) is updated by the further sample (Line 16). The sampling process for association rule mining is very similar and not shown here.
We evaluate our sampling methods for association mining and differential rule mining on the deep web using two datasets, described below.
 US Census data set: This is a 9-attribute real-life data set obtained from the 2008 US Census on the income of US households. This data set contains 40,000 data records with 7 categorical attributes about the race, age, and education level of the husband and wife of each household and 2 numerical attributes about the incomes of husband and wife.
 Yahoo! data set: The Yahoo! data set, which consists of the data crawled from a subset of a real-world hidden database at http://autos.yahoo.com/. Particularly, we download the data on used cars located within 50 miles of a zipcode address. This yields 30,000 data records. The data consists of 7-attribute with 6 categorical attributes about the age, mileage, brand, etc, of the cars and one numerical attribute, which is the price of the car. Variance of Estimation is estimated for the target value (i.e. mean value in differential rule mining, and confidence in associ-ation rule mining). In our experiments, the variance of estimation is calculated according to the Expression 2. Since variance of estimation reveals the variation of the estimated value from the true value, smaller variance suggests better estimation. Sampling Cost is estimated by the number of queries submitted to data sources in order to acquire a certain number of data records containing target output attributes, i.e. A = a . Larger sample size implies higher sampling costs in a deep web setting, where the queries are executed over the internet.
 Estimate accuracy is estimated by Absolute Error Rate (AER) . Small AER value indicates higher accuracy. For an estimator on variable Y with true value y and estimated value  X  y , the AER of the estimator is calculated by AER (  X  y )= | y  X   X  y y | A. Association Rule Mining
In this section, we present the results of our method for association rule mining. Using our overall approach, we have created four different versions, which correspond to four different sets of weights assigned to variance of estimation and sampling costs. 1) Full Var only considers variance of estimation by setting the weight  X  v =1 . 0 and  X  s =0 . 0 ,2) Var7 is created by setting the weight  X  v =0 . 7 and  X  s =0 . 3 ,3) Var5 is created by weights  X  v =0 . 5 and  X  s =0 . 5 , and 4) Var3 is the version with the the weights  X  v =0 . 3 and  X  s =0 . 7 . In addition, we also compare these approaches with a simple random sampling method, which is denoted by Random .

We focus on the queries in the form of A = a  X  B = b , where A and B are output categorical attributes. Other categorical attributes in the data set are considered as input attributes. association rules are randomly selected from the datasets. Each of the 50 association rules are re-processed 100 times using 100 different (pilot sample, sample iterations) combinations. Thus, the result is the average result for 5000 executions.

In all charts reported in this section, the X-axis is k , which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k  X  x , where x is a fixed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size.

Figure 1 shows the result from our stratified sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment, the fixed value x is set to be 300, which means the smallest sample size at k =1 is 300, and the largest sample size at 10 is 3000. Figure 1 a) shows the variance of estimation for the five sampling procedures. Figure 1 b) shows the sampling cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of estimation and sampling cost for the sampling procedures var7 , var5 , var3 , and rand are normalized by the corresponding values of Full Var . Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c) shows the accuracy for the five sampling procedures.

From Figure 1 a) and b), we can see that, as expected, com-pared with sampling procedures Var7 , Var5 and Var3 , Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7 , Var5 , and Var3 , we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k =10 , the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40%, compared with sampling procedure Full Var . The experiment shows that our method decreases the sampling cost efficiently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k =10 , we can see that the AER of sampling procedure Var3 is increased by 20% compared with sampling procedure Full Var . However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy.
 In addition, compared with sampling procedure of Full Var , Var7 , Var5 , and Var3 , sampling procedure Random , has higher estimation of variance, sampling cost and lower estimation accu-racy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web.

Figure 2 shows the experiment result of our proposed stratified sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the fixed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However, although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var , the sampling cost of Random is 2% smaller than Full Var . This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low  X  , which denotes the probability of containing data records under the space of
A = a , resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7 , Var5 , Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random . Furthermore, Random has smaller sampling accuracy than Full Var , Var7 and Var5 , but has larger sampling accuracy than Var3 . This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent.

To summarize, our results shows that our proposed stratified sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to trade-off variance of estimation and sampling accuracy to some extent, while achieving a large reduction in sampling costs.
 B. Differential Rule Mining
In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a  X  D 1 ( t )  X  D 2 ( t ) , where A is an output categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes.

In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var , Var7 , Var5 , Var3 and Random , have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are re-processed 100 times using 100 different (pilot sample, sample iterations) combinations. Thus, the result is the average result for 5000 runs.

First, we evaluated the performance of these procedures on the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment, the fixed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method.
 We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the fixed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments.

We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web.
 Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules, which are then further verified on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining. FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized count-ing algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets.

Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges.
 Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation (APA) [18] was proposed to estimate aggregation queries over categorical data utilizing precomputed statistics about the dataset. Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads.

More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al. [19] considered the problem of estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata.

Our methods are clearly distinct for these approaches. First, strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method.
 Hidden Web Sampling : There is recent research work [3], [13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al. [13], [15] proposed HDSampler, a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database. Bar-Yossef et al. [3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a specific task, instead of simple random sampling.

In this paper, we have proposed stratification based sampling methods for data mining on the deep web, particularly con-sidering association rule mining and differential rule mining. Components of our approach include: 1) A tree model to capture the relation between input attributes and output attributes of the deep web data source, 2) A recursive stratification method to maximally reduce an integrated cost metric that combines esti-mation variance and sampling cost, and 3) An optimized sample allocation method that takes into account both the estimation error and the sampling costs.

Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error.
