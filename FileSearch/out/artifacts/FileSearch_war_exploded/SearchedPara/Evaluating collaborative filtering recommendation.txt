 1. Introduction
Collaborative filtering (CF) algorithms are techniques used by recommender systems to predict the utility of items for users based on the similarity among their preferences (votes, ratings, etc.) and the preferences of other users. Such tech-niques are largely widespread in several domains such as movies ( Dahlen et al., 1998 ), songs ( Shardanand, 1994 ) and books recommendations, and have been successfully applied to build e-commerce recommender systems ( Sarwar, Karypis, Kon-to express their preferences about the items (be them digital files, products or other kind of items). The basic idea behind CF is that users belonging to some specific community should express their likes and dislikes about the items, thus creating a large database of preferences (evaluative information) that can be further used to build models for prediction or to calculate similarities among users (or items). The recommendations are then made using such models, or based in the assumption that
Zhang, 2009 ). According to Breese, Heckerman, and Kadie (1998) these two main different approaches of CF algorithms  X  are classified as: Model-based and Memory-based. The former normally uses the entire database in order to discover pat-terns and build models of the user ratings that are further used to make predictions. Examples of model-based algorithms for CF are Bayesian Networks, Clustering, Rule-based and Regression-based approaches. The Memory-based class of CF uses a database of user-item information and applies statistical techniques to discover those users (also named as neighbors) that
Thus, in order to memory-based CF algorithms properly work; it is key the computation of similarity among users. Some of the most known techniques applied to the computation of similarity are Correlation-based similarity (Pearson correlation, sible to develop memory-based CF algorithms first considering only the relationship (similarity) among items to further rec-ommend them to similar users (the so-called item-based CF).

The enormous growth of learning objects on the internet and the availability of preferences of usage by the community of users in the existing learning object repositories (LORs) have opened the possibility of testing the efficiency of CF algorithms on recommending learning materials to the users of these communities. It is known that LOR represent a special kind of digital collection, in which the preferences about items can be considered to be related to contingent learning needs. In consequence, seeking learning resources can be hypothesized to be substantially different from selecting products for purchase or informa-tion resources. Particularly, it seems apparent that the criteria for quality of learning objects are complex. For example, the Learning Object Review Instrument (LORI) evaluates the quality using nine criteria: Content Quality, Learning Goal Alignment, Feedback and Adaptation, Motivation, Presentation Design, Interaction Usability, Accessibility, Reusability, and Standards standards compliance might be an issue of interest only to some users), and for algorithms that take into account the peda-gogical value of the recommendations ( Tiffany &amp; Gordon, 2009 ) where the recommended resources should meet the cognitive goals of the students and help them to increase their knowledge. Also, issues such as the prior knowledge of the learner and the most suitable learning strategies for them are also considered to be important information that could interfere in the suc-cess of learning objects recommender systems ( Drachsler, Hummel, &amp; Koper, 2008 ). Moreover, in educational environments learners can interact multiple times with the same resources (or problems) along the time, and the information coming from in some LOR both instructors and learners are users, so that it might be possible that mixing preferences of both kinds would produce poor results when used with existing CF algorithms that do not consider different types of users.

Although the community involved with research on recommenders in TEL may agree that LOR represent a different kind of digital collection and that the recommendation of materials using CF algorithms should take into account such specific-ities, the field of recommenders in TEL currently faces a major lack of sharable datasets that can be used for testing solutions in a way that they can be generalized ( Drachsler et al., 2010; Khribi, Jemni, &amp; Nasraoui, 2009 ). According to Manouselis,
Drachsler, Vuorikari, Hummel, and Koper (2011) , few researchers have attempted to test and validate their recommender systems based on data captured from real-life data settings. In fact, researches on the field are mainly concentrated on small-scale experiments that are conducted in a way that users and teachers are asked to provide ratings in a controlled environment ( Verbert et al., 2011 ). These problems have led part of the research community involved with recommender systems in TEL to recently launch a challenge called  X  X  X ataTEL Challenge X  X , which invites research groups to submit existing datasets from TEL applications so that other researchers can use them to test their solutions ( Manouselis, Drachsler, Verbert, view of the performance of different TEL recommender systems as a way of comparing results of these systems according to standardized evaluation criteria, i.e., as we face difficulties to generalize results to other TEL contexts (due to the lack of benchmarking datasets), new studies reporting their results over real-life data settings are very welcome to the area. More-over, Mulwa, Lawless, O X  X eeffe, Sharp, and Wade (2011) have recently developed a hybrid framework to recommend eval-uation approaches to software developers of personalized e-learning systems and reinforced that evaluators should report their used approaches and methodologies to increase the empirical value of their studies and increase the chances of repli-cation of their experiments. We believe that finding responses to the specificities required by recommenders in LORs depend first on implementing CF algorithms inside LORs as  X  X  X hey are X  X , thus allowing future contrast between these results and oth-ers that consider those specific aspects.

The main goal of this paper is to evaluate recommendations of learning resources generated by different well known memory-based CF algorithms using a database gathered from the popular MERLOT repository. It is known that recommender system evaluation can be approached from different perspectives ( Herlocker, Konstan, Terveen, &amp; Riedl, 2004 ), including accuracy, coverage, usefulness or novelty among others. Some of them can be performed offline by using datasets of ratings, which has the benefit of providing a first assessment before evaluations with users is approached. This initial study is in-tended as a first step prior to perform a more comprehensive evaluation combining several criteria. In fact, this study is the continuation of a previous work developed by Sicilia, Garc X a-Barriocanal, S X nchez-Alonso, and Cechinel (2010) where the authors evaluated recommendations generated by two memory-based algorithms (using Pearson correlation and Euclid-ean distance similarities) and initially contrasted the results with existing endorsement mechanisms of the MERLOT repos-itory. In that work the authors have found that the average errors results were high in the context of the kind of digital repositories analyzed and that recommendations are somewhat related to other endorsement mechanisms of MERLOT. In here, we extended the mentioned work in a number of ways. First, the previous work generated recommendations for just one dataset gathered from the ratings (explicit preferences) given by the users of the repository, whereas in this work we also generated recommendations for a dataset based on the implicit preferences of the users. Second, in here we have tested two new similarity metrics that were not tested in the initial work (LogLikelihood similarity metric for the explicit prefer-ences dataset and Tanimoto coefficient for the implicit preferences dataset). Third, the previous work set up the parameters of the algorithms to run with just 30% of the available dataset, while in the present work we have used the entire datasets.
Moreover, in here we have contrasted the results of the recommendations generated by the algorithms with several existing endorsement mechanisms of the repository in a more in-depth way in order to evaluate possible relations among them. Fi-nally, we have also compared the results generated by the different algorithms and have evaluated whether or not they over-lap in the generated recommendations. During the present study we try to answer the following research questions:
RQ1  X  For the context of the MERLOT ratings dataset, do different algorithms (similarity metrics) present similar average errors?
RQ2  X  How do the parameters (minimum similarity and neighborhood size) influence the results of the algorithms for the studied datasets? RQ3  X  Are the results of the implementations of the present study distinct from other results found in recommenders in TEL related literature?
RQ4  X  Which are the algorithms able to generate more recommendations and which are the ones that present the highest coverages?
RQ5  X  Are there relations between the recommendations and the other endorsement mechanisms of the MERLOT repository?
RQ6  X  Are there differences between the recommendations based on the ratings dataset and the Personal Collections dataset?
The rest of this paper is structured as follows. Section 2 describes related research and the motivation for the present study. Section 3 describes the collected datasets and the evaluation of CF algorithms for the different parameters. Section 4 analyzes the recommendations generated by the algorithms regarding their coverage and their relations with other quality measures of the repository. Finally, Section 5 provides answers to the research questions proposed above and Section 6 pre-sents the conclusions and limitations of this study, as well as some open possibilities for future work. 2. Recommender systems for e-learning
LOR provide a platform for the sharing of educational resources on the Web, and most of them provide some mechanisms for building community dynamics around their resources. The community dimension and its social dynamics have been found to be an important aspect for the success of these repositories. For example, Brosnan (2005) provided a conceptual-ization for that importance based on social capital theory, and Monge, Ovelar, and Azpeitia (2008) analyzed the potential impact of Web 2.0 strategies to foster social dynamics and participation in repositories. In a similar direction, Han, Korte-meyer, Kr X mer, and von Pr X mmer (2008) reported an empirical study on the LON-CAPA repository in which a non-explicit community model was identified on the basis of co-contribution of resources to the same courses by popular authors. Sev-eral repositories have started to provide services by which members can share their personal collections of favorite resources and comment or review other X  X  resources. Concretely in the context of information retrieval, Vuorikari and Koper (2009) have found that search methods that use such kind of social information provide more relevant learning resources with less effort from the users when compared to text-based only searching mechanisms. Social information in these sites is normally openly available and represents an objective account of usage and expression of preference that can be used by CF algorithms in order to produce personalized recommendations ( Vuorikari, Manouselis, &amp; Duval, 2008 ).

Different authors have approached the problem of recommending resources for e-learning through the use of CF tech-niques, most of them involving some hybrid solution. For instance, Anderson et al. (2003) have developed a system called
RACOFI (Rule-Applying Collaborative Filtering) that recommends audio Learning Objects by using a combination of CF algo-rithms and a rule-based tool. Tang and McCalla (2005) proposed a web-based learning system that is able to adapt and rec-ommend content (research papers) based on clusters of users generated by the observation of the learners X  behavior (interaction with the system) and on the explicit ratings given by them to the materials. Khribi et al. (2009) proposed a framework for personalized recommendations that combines mining processes (for building learner and content models) together with content-based and memory-based CF techniques. Klasnja-Milicevic, Vesin, Ivanovic, and Budimac (2011) developed a system that recognizes different patterns of learning styles and interests of learners and provides personalized recommendations for learners using a CF algorithm that calculates the similarity among them by using the Pearson corre-lation similarity metric. Even though the existence of these (and other) proposals regarding the use of CF focused on the rec-ommendation of learning resources, most of these works are context focused and often related to a specific community of users inside some Learning Management System. So far, and from our knowledge, the current and most popular existing LOR are still not using CF algorithms to recommend materials, and there is a considerable lack in the literature of actual results about the use of such techniques in real, large and open e-learning datasets.
In this direction, Recker, Walker, and Lawless (2003) reported three experiments of a collaborative filtering system, with 15, 63 and 375 participants respectively, but provided only user experience measures, not an actual evaluation of the accu-racy of the collaborative filtering algorithm. Manouselis, Vuorikari, and Van Assche (2007) reported on a dataset of 2000+ multi-attribute learning object evaluations from the CELEBRATE portal. The study contrasted variations of three options of similarity calculation (Pearson, Euclidean and Vector/cosine) and two neighborhood selection criteria (correlation weight threshold and maximum number of neighbors). Their results found several algorithms performing reasonably well and with acceptable performance figures with Mean Average Errors around 0.57 and 0.67 for coverage higher than 65%.

This paper aims at complementing these existing studies with the evaluation of several recommender variants evaluated against two datasets of users preferences extracted from the MERLOT learning object repository. also us to contrast the recommended items with other quality or endorsement mechanisms in the repository, aiming to explore potential relations between them. 3. Materials and methods
This section gives a brief overview about the MERLOT repository, describes the data collection and the methodology ap-plied in the analysis. 3.1. MERLOT repository and users preferences
The Multimedia Educational Resource for Learning and Online Teaching (MERLOT) is an international recognized initia-tive focused on sharing online learning materials, where resources are catalogued by registered users according to some dis-cipline (Arts, Business, Humanities, Mathematics and Statistics, Social Sciences and Science and Technology) and according to their type (pedagogical goal, granularity and technical format). The MERLOT possesses a robust system of quality assess-ment which is based on several kinds of evaluative metadata (see Fig. 1 ), and which is used to rank resources every time users perform a search inside the repository. The first of these noteworthy strategies of quality assessment adopted by MER-
LOT is the post publication peer-review model ( Cafolla, 2006 ), where experts of some field rate resources accordingly to tion to the peer-review model, MERLOT also allows registered members to provide comments and ratings about the re-sources. This is an important complementation of MERLOT X  X  strategy of evaluation given that the number of experts is restricted and not enough to review all resources, and that the number of users of the repository is rapidly growing ( Cechinel over, MERLOT annually awards outstanding peer-reviewed materials according to a program criterion of the disciplines, the so-called MERLOT Classic Awards. At last, MERLOT also allows users to bookmark their preferred materials in Personal Collec-tions as a way of providing personalization according to users X  individual interests ( Sicilia, S X nchez-Alonso, Garc X a-Barrioca-indicators of quality inside the MERLOT repository ( Cechinel, S X nchez-Alonso, &amp; Garc X a-Barriocanal, 2011; Garc X a-Barrioca-nal &amp; Sicilia, 2009 ).

For the context of our study, it is possible to identify two potential datasets that contain user X  X  preferences and that are suitable for the implementation of collaborative filtering recommendations: (1) the ratings given by users, and (2) the pres-ence of resources in users Personal Collections . Both datasets are completely different regarding their inherent features; for whereas the second dataset contains implicit ratings, i.e., the preference is inferred from the user behavior. Moreover, the preferences of the first dataset operate in a scale of 5-point, whereas the preferences of the second dataset are unary Accord-former imposes a cognitive cost on the evaluator, the latter does not. The author also highlights some possible problems underlying the acquisition of explicit ratings (or preferences) such as the use of appropriate scales, motivation and incentives for evaluators, and biased evaluators; and encourages the pursuit of systems able to collect implicit preferences (called by him as implicit ratings).

The differences between these two datasets make of them good baseline candidates for the comparison of collaborative filtering recommendations inside the repository. 3.2. Data collection and description
A database from the MERLOT repository was gathered on May 2009 by using a web crawler that systematically traversed the web pages of the repository. Information of a total of 20,601 resources and 63,999 users were extracted, of which 1660 were also recognized as resource authors. 372 of these authors had no declared organization, and the rest of the frequencies of occurrence of individuals from the same organization were below 7, which allows us to discard a possible bias coming from a dominant institution behind the community of users. The fact that there is a significantly higher number of contrib-utors than authors is relevant as it points out that MERLOT is more a community of contributors than of authors. This can be a result of MERLOT being a repository only storing metadata and not the contents themselves (also denominated by Ochoa and Duval (2009) as a referatory) as occurs in other systems as Connexions. users Personal Collections . 3.2.1. Users ratings dataset
In total 5171 ratings about 2754 distinct learning resources in MERLOT and coming from 1677 users were extracted from
As it can be seen in Fig. 2 (left side), few users rated several resources and a long tail of users rated just a few of them (5 resources or less). The same pattern can be observed in the distribution of ratings per resource, where few resources are rated by several users, and a long tail of resources are rated by just a few of them. Both distributions appear to follow a typ-ical power law distribution, which is consistent with existing studies on the unequal distribution of contributions in learning object repositories ( Ochoa &amp; Duval, 2009 ).
 3.2.2. Personal Collections dataset
The Personal Collections dataset is approximately five times bigger than the ratings dataset. In total 5954 users book-marked 8905 distinct learning resources to their Personal Collections , totaling 32,698 bookmarks. Such huge difference in the amount of these two datasets shows a strong preference of the community of users towards adding favorite resources lections per user (left side) and the distribution of Personal Collections per learning resource (right side). lections per learning resource (right side) also appear to follow typical power-law distributions. 3.3. Evaluating parameters for collaborative filtering algorithms The method followed for this study was the evaluation of distinct user-based collaborative filtering algorithms using the
Apache Mahout version 3.0. 5 Evaluations for both datasets were computed using a split of the data taking 90% of it for com-puting the predicted recommendations and 10% for their evaluation. Previous work done by Sicilia et al. (2010) has used just 30% of the data sample thus achieving very different results than those achieved here. As both data samples vary significantly regarding their intrinsic features, the metrics used for the evaluation were distinct for each dataset.

For the ratings dataset, the evaluation was done using three distinct similarity metrics: Pearson, Euclidean and LogLike-lihood; and was based on two predictive accuracy metrics: Average Error (AE) and the Root Mean Squared Error (RMS). The computation of the errors was done repeatedly (100 times) per each algorithm configuration and the average of the error of the 100 runs was used for the analysis.

As the Personal Collections dataset does not have ratings; i.e., users express generic  X  X  X es X  X  preferences for a resource, we evaluated recommendations by using three classification accuracy metrics (Precision at 10, Recall and F-Measure) for the
Tanimoto coefficient similarity, that according to Mild and Reutterer (2003) is suitable for the case of asymmetric distributed and binary datasets.

For both datasets the algorithm variants tested were dependant on the following parameters: (1) Neighborhood size  X  vari-ations from 1 to 20 users were experimented and; (2) Minimum similarity  X  variations from zero to one in increments of 0.1 were considered. Table 1 summarizes the evaluation metrics performed, the Mahout Apache classes used, and the parame-ters applied for each dataset. 3.3.1. Evaluating parameters for the ratings dataset
The first evaluation was the influence of the parameters in the quality of predictions for the ratings dataset. As mentioned before Average Errors (AEs) and Root Mean Squared Errors (RMSs) were calculated for different variations of Neighborhood size and Minimum similarity. According to Herlocker et al. (2004) , Average Error (AE) measures the average deviation between the predicted rating the actual rating given by the user, and the RMS makes a similar calculation but emphasizing (or penal-izing) on large errors. Table 2 shows the different correlations found between errors and parameters for each one of the three similarity metrics.

In Table 2 , the column P stands for the p -value obtained in the analysis, and the Spearman rank correlation coefficient ( r indicates the strength of the association between the error and the parameter, varying from 1 to 1 (where 0 means no asso-ciation). As it can seen from the table, with exception of minimum similarity and RMS for the LogLikelihood metric, all parameters are correlated to errors for all similarity metrics at a 95% level of significance. In general, minimum similarity is positively correlated to AE and RMS, and neighborhood size is negatively correlated to the AE and RMS. However, the cor-relations are not always too strong. The strongest correlations are found between: (1) minimum similarity and AE for the
Pearson correlation similarity metric ( r s = 0.688); (2) minimum similarity and AE for the Euclidean distance similarity metric ( r = 0.559); and (3) neighborhood size and RMS for the LogLikelihood similarity metric ( r Figs. 4 X 6 help us to better visualize these encountered correlations. In the figures bellow, AE and RMS are showed for
Pearson, Euclidean and LogLikelihood similarity metrics respectively, and considering different neighborhood sizes (left side of the figures) and minimum similarities (right side of the figures). As it can be seen, the ranges of both AE and RMS for all three metrics are relatively similar, varying approximately from 0.5 to 0.8 for the AE, and from 0.8 to 1 for the RMS.
For Pearson Correlation similarity metric, it is possible to notice a very smooth tendency towards the decrease of the er-observe a strong tendency of increasing the errors as the minimum similarity increases (right side of Fig. 4 ). For Euclidean distance similarity metric ( Fig. 5  X  left side), the same behavior is observed for the case of the neighborhood size parameter.
On the other hand, the tendency of the errors for the minimum similarity is a little bit different. In there, the errors seem to decrease as the minimum similarity increases up to approximately 0.7, and then they start to increase after this value ( Fig. 5  X  right side). This suggests that the best minimum similarity for Euclidean is around 0.7. For both, Pearson and Euclidean that errors tend to maintain the same for different minimum similarity values at the right side of Fig. 6 .
 Table 3 shows the values of the parameters for the top and the bottom five AE found for each one of the similarity metrics.
Table 3 reinforces that the minimum errors are found when low minimum similarities are applied for the case of Pearson, and that the best results for Euclidean are achieved with minimum similarity equals to 0.7. Moreover, the table also shows that even though some negative correlations between the size of the neighbors and the errors exist for Pearson and Euclid-ean, they are not as strong as the correlations found between minimum similarity parameter and the errors, for instance, some of the bottom five error results have high neighborhood sizes. At last, table shows that LogLikelihood presents the best results when the size of the neighbors is higher or equal to 13 and the worst results when the neighborhood size is equal to 1. collaborative recommendation of e-learning resources, more specifically in Manouselis et al. (2007, 2010) .

The parameters which achieved the best results were further used to generate the recommendations for the ratings data-set for each similarity metric as follows: Pearson correlation similarity: neighborhood size = 20; minimum similarity = 0.5.
 Euclidean distance similarity: neighborhood size = 14; minimum similarity = 0.7.
 LogLikelihood similarity: neighborhood size = 14; minimum similarity = 0.5.
 3.3.2. Evaluating parameters for the Personal Collections dataset
The evaluation for the Personal Collections dataset was made through the calculation of the following three classification accuracy measures for the Tanimoto Coefficient (or Jaccard) similarity metric: Precision (at 10), Recall and F0.25-Measure.
Precision is defined as the percentage of recommended items that are relevant to the user (i.e., the number of items recom-mended that were relevant divided by the total number of recommended items), while Recall represents the probability that a relevant item will be recommended (i.e., the number of items recommended that were relevant divided by the total num-ber of relevant items in the entire dataset) ( Herlocker et al., 2004 ). Both Precision and Recall should always be measured together, once that it is possible to increase one at the expense of the other. For that, it is normally used a combined measure called F1-Measure that gives equal weights for Precision and Recall. Table 4 shows the correlations between neighborhood sized and minimum similarity for Precision and Recall.

As it is possible to see from Table 3 , there are no correlations between Precision and Recall and the size of neighbors. On the other hand, both Precision and Recall present strong correlations with the minimum similarity parameter. For this parameter it is possible to observe a positive correlation of 0.8902 for the case of Precision, and a negative correlation of 0.9547 for the case of Recall. Fig. 7 better illustrates these correlations.
 The left side of Fig. 7 reinforces that the neighborhood size does not influence on the percentages of Precision and Recall.
On the other hand, the right side of the figure clearly shows the influence of the minimum similarity parameter on both Pre-cision and Recall. It is possible to observe that Precision achieves very high percentages (varying from 20% to 100%), while Recall has the maximum value of only 18% for minimum similarity equals to 0.1. The figure also shows that Precision and
Recall have an inverse dependency, i.e., it is possible to achieve very high Precision percentages at the cost of decreasing Re-mender systems should normally choose one at the expense of the other depending on the context of the application. It is important to highlight that a previous study carried out by Verbert et al. (2011) found that the performance of the results of user-based CF increases as we increase the number of neighbors, which is different from the findings of the present study.
As mentioned before, the most common performance measure for evaluating Precision and Recall together is the F1-mea-sure which weights both as equals and is given by the following formula:
For our study, the highest value of Recall possible to achieve is too low, therefore, the cost X  X enefit of decreasing Precision to increase Recall is not worthy. Because of that, we have used an alternative to the F1-measure, which is assigning more importance to Precision at the cost of Recall. Concretely, we have weighted Precision four times more than Recall by using the value of b as 0.25. This means to say that, for the context of our study, the users of our system would prefer four times more to be presented to very few but good recommendations, rather than to be presented to more resources at the cost of their relevance. Table 5 shows the parameters for the top five results of the F0.25Measure calculation.

The parameters which achieved the best results were further used to generate the recommendations for the Personal Col-lections dataset with Tanimoto, as follows:
Neighborhood size = 2; minimum similarity = 0.4. 4. Analysis of recommended Items The same algorithm variants (Pearson, Euclidean and LogLikelihood for the ratings dataset; and Tanimoto for the Personal
Collections dataset) optimized according to the above described analysis were used to systematically generate recommen-dations for all the users. Concretely, they were asked to generate at most 10 recommendations for each user.
For the ratings dataset, the Pearson algorithm generated 1285 recommendations of 394 learning objects for 185 users; while the Euclidean algorithm generated 1935 recommendations of 492 learning objects for 280 users. The LogLikelihood algorithm was able to reach approximately 4 times more users and resources than Pearson and Euclidean, generating 8607 recommendations of 1208 resources for 1068 users. The distributions of the number of recommendations per user and of the number of recommendations per learning object for Pearson, Euclidean and LogLikelihood are depicted in Figs. 8 X 10 respectively.

As it can be seen from the figures above (left sides), the distribution of the number of recommendations per user for Pear-son and Euclidean are relatively similar, i.e. divided in three segments as follows: approximately 50% of the users received 10 recommendations (the maximum number permitted), 20% of the users received between 4 and 9 recommendations and 30% of the users received from 1 to 3 recommendations. The distribution of recommendations per user for Likelihood is slightly different, with approximately 65% of the users receiving 10 recommendations, 22% of the users receiving between 4 and 9 recommendations and only 13% receiving just from 1 to 3 recommendations. The distributions of recommendations per learning objects (right side of the figures) for the three algorithms seem to be very similar, with few resources being recom-mended several times, and a long tail of resources being recommended few times. However, it is also possible to see that for the case of LogLikelihood algorithm, the number of times the resources are recommended is much higher than for the case of Pearson and Euclidean.

For the Personal Collections dataset, the Tanimoto algorithm generated 18,422 recommendations of 4975 learning objects for 3294 users. Fig. 11 presents the distributions of the number of recommendations per users and the number of recommen-dations per resources.

As it can be seen in the left side of the figure, approximately 30% of the users received 10 recommendations, while other 30% received between 4 and 9 recommendations and 40% received from 1 to 3 recommendations. As a consequence of the dataset size, the number of recommendations generated using Tanimoto was significantly bigger than those previously de-scribed for the ratings dataset. The right side of the figure shows that the distribution of recommendations per learning re-sources is similar to the distributions described for the ratings dataset. 4.1. Coverage of the recommendations
According to Ge, Battenfeld, and Jannach (2010) coverage deals with the degree to which resources can be recommended to all potential users and the percentage of items that are effectively recommended to a user. Shani and Gunawardana (2011) argue that even though some algorithms may provide high quality recommendations; sometimes they do not cover all items and users, leaving a big portion of the dataset uncovered. Thus, coverage is an important metric that has to be observed when choosing among algorithms for recommender systems. In here, we measured coverage from two distinct perspectives: (1) the item-space coverage  X  defined as the number of distinct recommended items divided by the number of distinct items in the dataset; and (2) the user-space coverage  X  defined as the number of distinct users to whom items were recommended divided by the number o distinct users in the dataset. Table 6 shows the calculated coverage for each one of the implemented algorithms and datasets.

As it can be seen in Table 6 , it is evident the differences of both coverage values among the algorithms. For the ratings dataset, the item-space coverage of LogLikelihood is approximately 2.5 times better than the coverage of Pearson and Euclid-ean, and the user-space coverage is approximately five times better. For the Personal Collections dataset, the Tanimoto rec-ommendations achieve almost the same coverage for both item-space and user-space, around 55%. 4.2. Recommendations versus other quality measures of MERLOT
One important aspect of the use of CF algorithms is to what extent recommendation algorithms complement existing endorsement mechanisms in repositories. In the case of MERLOT, peer reviews and ratings are used  X  but not as personalized rankings, together with some labeling as  X  X  X ERLOT Classics X  X  or the like. An interesting contrast is analyzing if the algorithms generate recommendations to items that are already favored by these existing endorsement mechanisms, or if there are some sort of bias of such mechanisms towards the recommendations. In this direction, we have evaluated the percentages of presence of such quality measures in the original datasets (overall sample, ratings dataset and Personal Collections data-set) and in the recommendations dataset (Pearson, Euclidean, LogLikelihood and Tanimoto recommendations).

Table 7 presents the amount and percentage of resources peer-reviewed, bookmarked to Personal Collections and that received the MERLOT Classic awards regarding the overall sample, the ratings dataset, and the recommendation datasets generated with Pearson, Euclidean and LogLikelihood algorithms.

In Table 7 , the Distinct Values Only ( DVO ) refer to non-repeated learning resources inside the datasets, i.e., only distinct sets, i.e., as more than one rating is given to learning resources, and as some learning resources are often recommended to users more than once, there is a considerable overlapping of resources inside these datasets.

From the 20,601 distinct learning resources collected in the overall sample, 2065 (12.65%) were peer-reviewed, 8897 (43.19%) were included in at least one Personal Collection and only 85 (0.41%) had the MERLOT Classic Awards label. As it is possible to see in the table, the percentages of such quality measures have increased inside all other datasets when com-pared to the overall sample.
 Moreover, for the case of DVO , all percentages related to all quality aspects (Peer-reviews, Personal Collections and MER-
LOT Classics) increase from the overall dataset to the ratings dataset and from the ratings dataset to the recommendation datasets. Regarding the recommendations dataset for the AVI , the increase is more evident in the LogLikelihood recommen-dations dataset, curiously, the one which was able to generate the highest number of recommendations. For instance, while in the ratings dataset 28.83% of the resources were peer-reviewed, in the Pearson and Euclidean recommendations dataset this percentage was of 35.37% and 37.07% respectively, and in the LogLikelihood recommendations dataset the percentage achieved 40.05%. It is also possible to see that from the total of 66 resources labeled with the MERLOT Classic Awards in the ratings dataset, 45 were recommended by the LogLikelihood algorithm, while only 13 and 20 were recommended by Pearson and Euclidean respectively. At last, the percentage of resources attached to Personal Collections increases approximately 10% from the ratings dataset to the LogLikelihood recommendations dataset (from 68.29% to 78.03%). The increase in the percent-age of resources included in Personal Collections is reinforced by the evaluation of the average of occurrences in Personal
Collections for each dataset. In here, the ratings dataset has an average of 4.89 Personal Collections per resource, while Pear-son, Euclidean and LogLikelihood datasets have averages of 6.05, 7.67 and 7.46 respectively. As personal collections are items included in many personal collections are more likely to have high ratings and to be recommended.
 All percentages related to all quality aspects also increase from the Overall sample to the ratings dataset for the case of the
DVO , thus meaning that resources endorsed by these other quality measures of the repository tend to be rated by users more times than other resources. Differently from the DVO , in here there is no increase in the percentages from the ratings dataset to the Pearson and Euclidean recommendations dataset. In fact, the only increase of percentages observed is for the case of the LogLikelihood dataset, more specifically in the percentages of Peer-reviewed resources (from 40.92% to 48.52%) and re-sources attached to Personal Collections (from 75.19% to 80.31%). Such behavior suggests that, as the number of recommen-dations the algorithm is able to generate increases, the bias of the quality aspects towards such recommendations also increases.

We have run Mann X  X hitney and Kolmogorov X  X mirnov tests in order to contrast the difference between medians and the distributions of Personal Collections and Peer-Reviews among the ratings datasets and the recommendation datasets for the
DVO and for the AVI. With just one exception (the median of Personal Collections between ratings dataset and Pearson rec-ommendations dataset for the DVO ), all recommendation datasets presented different medians and distributions for all qual-ity measures at a 95% significance level.

It is also possible to make the same kind of analysis for the context of the Personal Collections dataset. Table 8 shows the amount and percentage of resources peer-reviewed, rated by users, and that received the MERLOT Classic awards inside the dataset of recommendations generated by Tanimoto algorithm.

As it can be appreciated in Table 8 , all percentages related to quality aspects inside the datasets increased from the overall sample to the other datasets. Note in the table (in the DVO ) that all 85 resources awarded with the MERLOT Classics were included in at least one Personal Collection (and thus are part of the Personal Collections dataset) and only 4 of them were not recommended by the Tanimoto algorithm. Moreover, from the 2605 peer-reviewed resources in the overall sample, 2123 were included in the Personal Collections dataset, and 1584 (more than a half of them) were recommended by the Tanimoto algorithm. At last, from the 2511 user-rated resources in the overall sample, 1566 were included in the Personal Collections dataset, and 1114 were recommended by Tanimoto.

The increase in the percentages for the case of the AVI is not as high as for the DVO. In here, the percentages of the user-rated and peer-reviewed resources for the Tanimoto dataset have increased when compared with the Personal Collections dataset, while the percentage of resources labeled with the Merlot Classics has decreased from 11.10% to 10.30%.
We have also run Mann X  X hitney and Kolmogorov X  X mirnov tests in order to contrast the difference between medians and the distributions of Users ratings and Peer-Reviewers ratings among the Personal Collections datasets and the Tanimoto recommendation datasets for the DVO and for the AVI. In here, with just one exception (the median of Users ratings between
Personal Collections dataset and Tanimoto recommendations dataset for the AVI ), all contrasts presented significant differ-ences at a 95% level. 4.2.1. Intersecting recommendations of the different datasets
We intersected the datasets generated by the different algorithms in order to evaluate whether they presented some overlap among them. Table 9 shows the amount of recommendations that are the same between the different combinations of the recommendations datasets, and the percentage that these intersections represent in each one of the datasets.
As it can be seen from Table 9 , the two recommendations datasets with the highest number of equal recommendations between them were Pearson and Euclidean, with 1052 equal recommendations. These recommendations also represented the highest percentage inside the datasets, 81.86% for the case of Pearson and 54.36% for Euclidean. The second highest amount of equal recommendations is observed between Euclidean and LogLikelihood, followed by Pearson and LogLikeli-hood. It is important to note the huge difference between the amounts of identical recommendations which occur when the intersections involve the Tanimoto dataset. For these cases, the maximum amount of equal recommendations observed is 12 for the intersection between LogLikelihood and Tanimoto, followed by 6 for the intersection between Euclidean and
Tanimoto. This large disparity between the number of intersections involving the Tanimoto dataset strongly reflect the dif-ferences between the two original datasets used (ratings and Personal Collections) and the differences of the generated rec-ommendations based on them. 5. Research findings
The present work has evaluated standard user-based CF algorithms with two datasets extracted from MERLOT repository in May 2009. Precisely, recommendations have been explored by using Pearson, Euclidean and LogLikelihood similarity met-rics for the dataset formed by the ratings given by users of the repository, and by using Tanimoto coefficient similarity metric for the dataset composed by the Personal Collections of the users. For all similarity metrics, algorithm variants with different neighborhood size and minimum similarity parameters were tested. During the present work we were able to find answers for the main research questions we established on Section 1.

RQ1  X  For the context of the MERLOT ratings dataset, do different algorithms (similarity metrics) present similar average errors? Yes, they did. It was possible to observe that the best Average Errors (AEs) for the three similarity metrics tested (Pearson, Euclidean and LogLikelihood) were very similar and around 0.55.

RQ2  X  How do the parameters (minimum similarity and neighborhood size) influence the results of the algorithms for the studied datasets?
For the ratings dataset, in general, minimum similarity is positively correlated with AE and RMS, and neighborhood size is negatively correlated to the AE and RMS. Regarding this, it is important to highlight that sometimes, the influence of one parameter imposes its strength over the other parameter. For instance, the negative correlation found between the size of the neighbors and the errors for Pearson and Euclidean is weaker than the minimum similarity influence (even though both parameters are correlated to the errors). This is an important finding if we consider that the number of evaluated neighbors influences on the time required for computing the recommendations, thus its importance could be diminished in specific scenarios.

The evaluation of Tanimoto coefficient similarity metric for the Personal Collections dataset has shown that the neighbor-hood size does not influence the percentages of Precision and Recall, and that the minimum similarity parameter strongly influences both measures.
 RQ3  X  Are the results of the implementations of the present study distinct from other results found in recommenders in TEL related literature?
For the ratings dataset, no, they are very similar. The AE around 0.55 are consistent with the results encountered in re-lated literature in the field of collaborative recommendation of e-learning resources, more specifically the ones found by
Manouselis et al. (2007, 2010) . On the other hand, such results are significantly better than the results achieved in the pre-vious work developed by Sicilia et al. (2010) where AE was around 1.0, but this is certainly a consequence of changing the parameters related to the percentage of the dataset from 30% to 100%.

For the Tanimoto algorithm in the context of the Personal Collections dataset, it was also possible to observe that both classification measures (Precision and Recall) were inversely correlated, which is consistent with the existing literature ( Manning et al., 2008 ). However, as we mentioned before, for this dataset the neighborhood size does not present association with the percentages of Precision and Recall, which is distinct from a previous study of Verbert et al. (2011) that found the performance results of user-based CF increases as we increase the number of neighbors.

RQ4  X  Which are the algorithms able to generate more recommendations and which are the ones that present the highest coverages?
In terms of the number of recommendations, the LogLikelihood algorithm was able to generate approximately four times more recommendations than Pearson and Euclidean, and the Tanimoto algorithm was able to generate two times more rec-ommendations than the LogLikelihood algorithm. This last difference is probably a reflection of the differences between the two datasets used (ratings dataset and Personal Collections dataset), since they are substantially distinct in their amount of information.

Moreover, item-space and user-space coverage have been measured for each one of the algorithms, also presenting very distinct values. For the ratings dataset the LogLikelihood algorithm achieved an item-space coverage approximately 2.5 times better than the coverage of Pearson and Euclidean (around 44%) and a user-space coverage approximately five times better (around 63%). For the Personal Collections dataset, the Tanimoto algorithm has also achieved high values of coverage, precisely a 55.85% of item-space coverage and a 55.32% of user-space coverage. In terms of the number of recommendations and coverages, the results for the Personal Collections dataset outperform the results for the ratings dataset.
RQ5  X  Are there relations between the recommendations and the other endorsement mechanisms of the MERLOT repository? Yes there are. We have found that recommendations are somewhat related to other endorsement mechanisms in MER-
LOT. It was possible to observe that the percentage of resources endorsed with quality mechanisms of the repository nor-mally increases from the original datasets to the recommendations datasets. Such behavior deserves further in-depth exploration, but initially suggests that recommendations suffer some sort of bias coming from these mechanisms.
RQ6  X  Are there differences between the recommendations based on the ratings dataset and the Personal Collections dataset?
Yes, there is an enormously difference between the recommendations generated using the two distinct datasets. Intersec-tions among the recommendations generated by the four algorithms (Pearson, Euclidean, LogLikelihood and Tanimoto) have shown several equal recommendations between Pearson and Euclidean, Pearson and LogLikelihood and Euclidean and Log-
Likelihood, and very few similar recommendations when the Tanimoto dataset was considered. This reinforces the initial idea that these two datasets (ratings dataset and Personal Collections dataset) are representing very distinct information about the preferences of the users, and thus the recommendations generated through the use of them will also be highly different. 6. Final remarks
One important limitation of the present work is the lack of inclusion of experiments with model-based algorithms. Pre-vious works ( Breese et al., 1998; Candillier, Meyer, &amp; Boull, 2007 ) have already highlighted the importance of model-based approaches for large-scale recommendation scenarios where memory-based approaches face efficiency problems. Although scalability issues were not under the scope of this work, they certainly should be considered before the final implementation of a recommender system and future work must take experimentations on this direction into consideration. Other authors ( Su &amp; Khoshgoftaar, 2009 ) also point out the advantages of using model-based algorithms for addressing sparsity problems, for improving prediction performance and for giving a more intuitive and logical based explanation for recommendations.
Moreover, considering that the answers for some of our research questions are directly affected by the scope of the algo-rithms investigated, the inclusion of experiments comparing the performances of memory-based versus some of the classical model-based CF algorithms would significantly enrich the findings and help to ground the basis for further real CF imple-mentation inside learning object repositories.

In order to better evaluate the effectiveness of the recommendations generated by the studied algorithms, future work will have to involve users X  opinions in the process. For instance, it would be interesting to conduct a survey as in Tiffany and Gordon (2009) , where among other things, the authors contrasted if recommendations for a given user fall in the dis-ciplinary area of that user or are crossing disciplines, as well as evaluated if the users are already familiar with the recom-mended resources, and if they would recommend such resources to their fellows. Moreover, considering that the number of users contained in the evaluated data samples is relatively smaller than the number of items, it is possible the algorithms can build better correlations among items than among users ( Herlocker et al., 2004 ). Future work could then focus on testing an item-based approach, as well as some other similarity metrics such as Spearman and Uncentered Cosine, for instance. An-other possibility would be of repeating the present experiment with a more diversified setting of different number of train-ing users and different number of items that are generated by the algorithms.

It is important to mention here that CF techniques are normally applied over datasets composed of several thousands of ratings and usage preferences. Just to give an example, the Netflix Prize (an open competition for the best CF algorithm to recommend movies) provided a training dataset of approximately 100 millions of ratings given by 480 thousands of users. As pointed by Drachsler et al. (2008) , when insufficient data is available, the recommendation strategy should combine more than one technique in order to provide the most applicable recommendations for the given scenario. This could be the case here, given that the available datasets are sparse and with huge differences in the distribution of the preferences per users and per resources. Considering that the context of the present study is exclusively focused on the use of social evaluations (ratings and personal collections) of the repository, future work could also include other parameters in our crawler (e.g., types of material, categories of discipline, target audience) in order to test and evaluate different model-based CF algorithms.
Another interesting possibility for future work would be to test and develop a hybrid recommender that works with data about the semantic relatedness of the resources metadata (as it was done by Shelton, Duffin, Wang, and Ball (2010) which developed a content-based system that recommends related resources based on the similarity of their metadata) combined with usage data about the resources of the repository (as done by Kanellopoulos and Kotsiantis (2012) which developed a tool for recommending greek newspaper websites based on traffic rank information and user profile models).

The employment of real-life data settings about LORs usage (as the one we present here) is essential for the definition of a baseline evaluation framework of recommenders in TEL that still does not exist at this point. Once the community of the field is able to formulate such framework, it would be possible to adjust recommender systems to the specific case of learning.
Evaluation of recommenders in learning can be considered a completely different task from other domains given that the evaluation of whether the learner has improved due to the recommendations takes much more time and effort than the clas-sical evaluation metrics used for the application of CF on the usual domains (e-commerce, movies, etc.). The present study attempts to diminish the observed lack of studies with real-life data settings of learning. Unfortunately, the dataset used here the field of recommenders in TEL faces, which is related to the shortage of available datasets that can be used as benchmark-ing for contrasting the performance of the different recommender proposed solutions ( Drachsler et al., 2010 ). Interesting and existing learning object repositories could benefit from the results generated by these experiments. Moreover, there is a clear need of defining how these datasets should/could be put available, i.e., as the solutions proposed by the community is ing  X  user  X  resource; user  X  resource) are not sufficient to allow experimentation for all these proposals. It would be inter-esting to include on the ongoing studies that are defining formats to exchange data sets, room for information related to LO metadata (topic, level, difficulty, type of material, among others) in order to allow testing model-based algorithms more eas-ily, as well as room for information derived from users activities in order to promote more studies based on Contextualized Attention Metadata (CAM) such as the one conducted by Broisin, Brut, Butoianu, Sedes, and Vidal (2010) .

The present work offers a first attempt towards the evaluation of user-based CF techniques applied in real large LOR data-sets that can be used as a basis for further and more in-depth explorations.
 References
