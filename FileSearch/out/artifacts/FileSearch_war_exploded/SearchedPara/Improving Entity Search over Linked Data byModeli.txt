 Entity ranking has become increasingly important, both for retrieving structured entities and for use in general web search applications. The most common format for linked data, RDF graphs, provide extensive semantic structure via predicate links. While the semantic information is poten-tially valuable for effective search, the resulting adjacency matrices are often sparse, which introduces challenges for representation and ranking. In this paper, we propose a principled and scalable approach for integrating of latent semantic information into a learning-to-rank model, by com-bining compact representation of semantic similarity, achieved by using a modified algorithm for tensor factorization, with explicit entity information. Our experiments show that the resulting ranking model scales well to the graphs with mil-lions of entities, and outperforms the state-of-the-art base-line on realistic Yahoo! SemSearch Challenge data sets. H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval X  Retrieval models Entity Search, Learning to Rank, Tensor Factorization
Providing entity-oriented search tools is one of the com-mon trends of the past few years in the search engine indus-try. Some examples of existing products are Google Knowl-edge Graph, Facebook Graph Search, Bing Snapshot, Wol-framAlpha, and Yandex Islands. They aim to resolve entity-centric queries and show the search results (or only the best one) in the user convenient form. The analysis of real query  X  Work done while visiting Emory University Figure 1: A semantic graph of entities that are rel-evant to a query richmond virginia logs justifies importance of the task. For example, a recent empirical study [8] of the representative Yahoo! query log sample discovered that about 58% queries have the clear user intent of soliciting information about certain entities, i.e., goods, people, organizations, locations, events and others. In this paper, we address the problem of accurate modeling of the underlying query intent, which is key for enabling rich search experience.

In entity search over RDF graphs [8], we consider an en-tity description as an RDF subgraph that comprises RDF triples including the entity URI. The subgraph may contain literals along with pairs of predicates and URIs of different entities from the graph. For example, Figure 1 shows an entity graph for a city Richmond, VA. Given a query rich-mond virginia , we may expect the following search result list: fb:Richmond, Virginia , dbr:Richmond Virginia , and fb: East End, Richmond Virginia . The first two search results would be of excellent relevance level, and the last one is considered as of fair relevance.

Along with the growing commercial interest of the leading search companies, there is active research into methods in academia. Most previous works adapt the standard IR ap-proach to the task. Given an entity, its literals and, possibly, literals of other entities associated with the given entity are folded into a pseudo document with multiple fields. Then, one may apply BM25F [1, 2] or language model based [6] ranking functions. In [9], string similarity scores and the ex-plicit semantics of owl:sameAs and DBpedia:redirect prop-erties have been used for enhancing the search context and shown as remarkable improvement factors. The authors of [5] have improved entity ranking considering n-gram statis-tics along with a PageRank adaptation.

Unlike previous works, our method takes advantage of the supervised learning-to-rank paradigm and first integrates in-formation about the semantic graph into a ranking model in a compact representation. It represents queries, entities and query-entity pairs with a set of features that fall into two categories: term-based features and structural features . The features of the first category are derived from basic word statistics of queries and entities. The features of the sec-ond category capture the latent semantics of relations in the entity graph, inferring the distribution over latent factors for entities. To model this kind of semantics, our method relies on a tensor factorization based representation of ini-tial relational data. Finally, a machine learning algorithm (in our case, Gradient Boosted Regression Trees) discovers some patterns of feature values and optimizes those regu-larities with respect to the one of the standard evaluation measures.

In summary, key contributions of our work include:
In the next three sections, we describe the different com-ponents of our ranker: representing entity descriptions (Sec-tion 2), representing link information (Section 3), and com-bining them together using a learning to rank approach (Sec-tion 4).
In this section, we describe our modification of a previous approach [6] to model entity descriptions. Then, we derive a set of term-based features for ranking.

We follow the entity multi-fielded document paradigm. In particular, our model distinguishes three groups of predicate values that correspond to the document fields:
To model relevance between a query and entity descrip-tions, we come up with a set of four features:
Mixture of language models (MLM). A distinct prob-abilistic multinomial language model P ( t |  X  f ) with its own Dirichlet prior  X  f is built for each field. Finally, a mixture of language models P ( t |  X  ) = P f w f P ( t |  X  f ) is applied, and the probability P ( q |  X  ) = Q t  X  q P ( t |  X  ) for the entity, given a query, is used as a ranking score. The usage of this function as a feature is motivated by its superior performance in the entity search scenario [6].

Bigram relevance models. Most entity search queries are about named entities. Proper names are often fixed phrases, in which the words cannot be freely swapped or omit without losing the context. The queries south dakota state university or brooklyn bridge are good examples. That X  X  why we suggest using a plain bigram relevance model that considers frequency of consequences of length two from a given query. Since occurrences in the name field more likely lead to relevant entities, we consider different scores per each field.
 Additionally, we introduce two query related features.
Query features . The entity independent features do not model the relevance directly, however, they may boost some of considered features. For example, query length may be helpful for a learning model to increase weights for bigram relevance scores on long queries. Besides, we exploit query clarity [3], i.e., a measure of query ambiguity. This feature aims to distinguish clear queries from vague queries that also occur in the test data, e.g. carolina . We expect that the MLM score has to perform better on queries of that kind.
In this section, first, we introduce a model to represent entity links in a latent space. Second, we describe structural features to evaluate query-entity relevance using the latent representation.

A factorization of the data into a lower dimensional space introduces a way to represent the original data in a concise manner preserving their most characteristic features along with reducing the inherent noise. In this work, we rely on the recent algorithm for tensor factorization  X  RESCAL, which has shown a decent performance in link prediction scenario [7]. Next, we describe how entity graph can be modelled as a tensor and entities as vectors.

Entity graph as a tensor . We introduce a tensor X of the size n  X  n  X  m , where X ijk = 1, if there is k -th predicate between i -th entity and j -th entity, and X ijk otherwise. Thus, each k -th frontal tensor slice X k adjacency matrix for the k -th predicate, that is usually very sparse. Given r is the number of latent factors, we factorize each X k into the matrix product: where A is a dense n  X  r matrix, a matrix of latent em-beddings for entities, and R k is an r  X  r matrix of latent factors.

The matrices A and R k are computed as a result of the optimization problem. The objective function f ( A,R ) is min
The second summand is a regularization term with a pa-rameter  X  . The first is the sum over the squared Frobenius norms of the discrepancies between the given tensor slice and fitting matrix. To solve the optimization problem, [7] presents an iterative alternating least squares algorithm that updates A and R k matrices until a convergence criterion is met. As a result, i -th row of the matrix A represents i -th en-tity in the latent space. Finally, we apply sum normalization for resulting matrix rows.

Top-k entity similarity . We exploit the resulting en-tity vectors in the following way: given a query, we retrieve top-3 entities with respect to a baseline score and compute a distance between the closest top-3 entity vector and a given entity vector. Specifically, we experimented with cosine sim-ilarity, Euclidean distance, and heat kernel 1 as distances.
A learning-to-rank framework is a well-developed technol-ogy to combine multiple rankers. In our case, it is partic-ularly beneficial, because it is challenging to come up with a hand-crafted formula for such a diverse feature set (see Table 1). In this section, we describe our procedure to in-corporate the features into a learning-to-rank model.
As a preprocessing step, values of all the features except #1 and #2 are normalized by subtracting the minimum value and dividing by difference between maximum and min-imum values per each query. We treat relevance labels as continuous dependent values.

In principle, our approach is agnostic to the actual learn-ing model. However, we tend to expect that performance of structural features may vary greatly depending on the avail-able information of entity links, and they have to be supple-mentary to term-based features. Therefore, we use Gradient Boosted Regression Trees (GBRT) [4] as a learning model, which is proven to be very successful in similar scenarios. We optimize its hyperparameters (number of trees, tree depth, learning rate, and subsample size) with respect to NDCG. Applying the trained model for all entities in the index is usually impractical. So, we retrieve top-1000 entities with respect to MLM scores beforehand, apply the trained model, and order results by its predicted values.
The heat kernel function k ( x,y ) = e  X  k x  X  y k nalize very dissimilar vectors harder than Euclidean dis-tance; we fix  X  = 10  X  3 in our experiments We combine queries and labeled data from Yahoo! Sem-Search Challenge (YSC) 2010 2 and YSC 2011 3 evaluation campaigns into a single set. In total, the query set contains 142 queries. Before applying the searching methods we ex-perimented with, we fixed misspellings in 9 queries using a  X  X earch instead for X  feature of Google Search. To form the training data, we use extended versions of the YSC assess-ment files with labels acquired after our additional evalu-ation campaign on Amazon Mechanical Turk (AMT) 4 . In total, we have 14 048 labeled examples.

The Billion Triple Challenge (BTC) 2009 RDF data set 5 is used as a data collection. The data set is diverse and covers various domains including academic publications, geograph-ical data, music, biomedicine and many others. The largest data sources include DBpedia, LiveJournal, GeoNames, and DBLP.

To make our tensor factorization experiments faster, we sample from the whole entity graph in the following way: given a query set, we take top 1000 relevant entities with re-spect to MLM scores along with the labeled entities, and re-trieve all adjacent entities considering only 10 most promis-ing predicates from the entity graph (Table 2). Thus, the tensor eventually has 144 020  X  144 020  X  10 entries. Seek-ing the balance between expressiveness of the latent space and running time, we have set the following parameter con-figuration: the number of latent factors r = 100, regulariza-tion constant  X  = 0, and convergence threshold  X  = 10  X  7 Our experiments do not show that varying the parameters is sensitive for search results. The algorithm has converged within 19 iterations and taken 23 minutes on a powerful ma-chine with 24  X  2.3Hz CPU and 256GB RAM. Although, the calculation has required no more than 2GB RAM run-time. The tensor factorization is implemented in the fork of the RESCAL project 6 in Python. Our implementation is memory efficient, relies on the SciPy Sparse Matrix mod-ule, and scales well for graphs with millions of nodes on the affordable hardware. http://km.aifb.kit.edu/ws/semsearch10/ http://km.aifb.kit.edu/ws/semsearch11/ http://github.com/nzhiltsov/YSC-relevance-data http://km.aifb.kit.edu/projects/btc-2009/ http://github.com/nzhiltsov/Ext-RESCAL 0.276 (+ 4.2%) 0.561 (+ 4.1%)  X 
For the learning-to-rank task, we use the implementation of GBRT in scikit-learn 7 .
To measure the utility of the structural features, we have performed 10-fold cross validation for two subsets (Table 3). We used a set of term-based features as a strong baseline, de-signed to capture the main ideas of a state-of-the-art entity ranking method presented in [6]. We performed extensive experiments optimizing the baseline, achieving the perfor-mance comparable to the results reported in [6]. Statisti-cal significance (marked  X * X ) is measured by the Wilcoxon signed-rank test with p &lt; 0 . 05.

According to our analysis of boosting scores, the most instructive structural distance is heat kernel (Table 1, fea-ture # 9). Adding the structural features has significantly improved NDCG and P@10, and demonstrates better per-formance with respect to MAP. This finding supports our hypothesis that the semantic link information does extend the effective search context. For example, our method has the largest improvement on NDCG as well as MAP on a complex query shobana masala that likely asks about an In-dian actress Shobana Chandrakumar who starred in movies of the Masala genre. Clearly, the query is not well treated by n-gram features and also is hard for MLM, because it favors entities who are named Shobana too. On contrary, since the correct entity is included in top-3, the structural features boost entities that are identical or very close to it. The similar explanations are applicable for the other best performing queries in comparison: motorola bluetooth hs850 , sagemont church houston tx , philadelphia neufchatel cheese , bounce city humble tx . All these queries name a primary entity and contain refining terms. At the same time, top-3 entities of these queries have rich link information (primar-ily, the ones from DBpedia).

After analyzing the worst performing queries, we have found two main reasons of failures. The first cause is the poor performance of the baseline on retrieving the top-3 sim-ilar entities, needed for structural features (e.g. for metropark clothing , sedona hiking trails ) that leads to drifting to other entities. The second cause is query ambiguity (e.g. emery ). The baseline correctly favors entities with single term names as in the case of  X  X he Emery X , a rock band. The structural features promote entities representing people with similar names, e.g. Emery Barns or Sid Emery, which assessors tend to label as irrelevant. http://scikit-learn.org
In this paper, we presented a novel, principled, and scal-able approach for incorporating structural and term-based evidence for entity ranking. In particular, we have intro-duced a scalable application of tensor factorization to entity search, and developed new and effective features for entity ranking. Our method outperforms the previous state of the art on a large-scale evaluation over a standard benchmark data set. We complemented our experimental results with thorough error analysis and discussion. In the future, we plan to explore extending the entity structure representation by incorporating term information into the latent space, be-cause it will enable us to infer a distribution of latent factors for entities with limited link information. It could be done by enhancing the tensor structure with the entity-term matrix. Yet another prospective research direction is an application of the method in the entity list search scenario.
