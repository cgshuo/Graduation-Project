 1. Introduction
With the rapid growth of information, sentence categorization and novelty mining (NM) have become one of the key ically defined in literature as the opposite of redundancy ( Ng, Tsai, Goh, &amp; Chen, 2007; Tang &amp; Tsai, 2009 ).
The first step in the novelty mining is preprocessing by removing stop words, performing word stemming, etc. In the next step, each incoming sentence is categorized into the relevant topic bin. Finally, within each topic bin, novelty mining searches through the time sequence of relevant sentences and retrieves only those with enough  X  X  X ovel X  information.
The first attempt in the sentence categorization is to calculate the similarity between the sentences and the query from feedback in the target collections and the external resources, such as Wikipedia and search engines, ( Diaz &amp; Metzler, 2006 ) Moreover, many machine learning algorithms have been applied to sentence categorization ( Lavrenko &amp; Croft,
Novelty mining has been proposed to retrieve novel yet relevant information, based on the specific topic defined by a sentence-level novelty mining is related to the TREC Novelty Track for finding novel sentences given a chronologically or-tion in one document, especially when the domain of the document is new. Therefore, we focus on sentence-level novelty mining in our study.

Previous studies for novelty mining have been mainly conducted on the English language. Novelty mining on the Malay that have discussed the differences in preprocessing issues between English and Chinese, such as word segmentation and reported on discussing and comparing the multilingual sentence categorization and novelty mining performance on Far East-ern languages, such as Malay and Chinese. This combination of languages is especially useful in southeast Asian countries such as Malaysia and Singapore, where documents may have combinations of English, Malay, and Chinese sentences. Our paper will discuss the preprocessing and categorization for retrieving the relevant English, Malay, and Chinese sentences. Then, we apply sentence-level novelty mining on Malay and Chinese datasets and compare their novelty mining perfor-mance. Our studies provide the foundation for an integrative novelty mining system that can categorize and detect novel sentences in multilingual documents.

The rest of this paper is organized as follows. Section 2 gives a brief overview of related work on multilingual sentence categorization and novelty mining. Section 3 introduces the details of preprocessing steps for English, Malay and Chinese. work. 2. Related work
Traditional sentence categorization methods use queries from topic information to evaluate the similarity between an provide some limited information. Later works have emphasized on how to expand the query so as to optimize the retrieval tion, achieving better categorization results ( Lavrenko &amp; Croft, 2008; Wei &amp; Croft, 2006 ).

In the novelty mining task, many previous works on the sentence-level novelty mining (NM) tend to apply some prom-2002 )
For the Malay language, Kwee et al. (2009) described sentence-level novelty mining. In their work, a language-specific preprocessing consisting of Malay stop words removal, Malay word stemming has been integrated into the NM system. Experimental results on TREC 2003 and TREC 2004 Novelty Track data showed that the sentence-level NM algorithm de-signed for English can also be applied on Malay. NM studies on the Chinese language have been done on topic detection research works in the multilingual information retrieval were conducted on how to merge a unique result list that includes 2000 ). There are some works that mainly investigate the multilingual sentence categorization in certain languages. To the elty mining performance. That is the main focus and contribution of this paper. 3. Preprocessing for different languages
Before performing categorization and novelty mining, we must first preprocess the sentences. As preprocessing steps are different for English, Malay, and Chinese, we will describe the different steps for each language separately. 3.1. English
The entire preprocessing steps in English can be seen in Fig. 1 . 3.1.1. Stop words removal
The first step for English preprocessing is removing all stop words from sentences. Stop words are words that are too common to be informative, and should be removed, otherwise they will negatively influence the novelty prediction of sen-words in the English language. 3.1.2. English words stemming
After stop words removal, the remaining words are then stemmed. Stemming, by definition, is a process of reducing the inflected (or sometimes derived) words to their root forms. For example, words  X  X pens X  and  X  X pened X  become  X  X pen X  after being stemmed.

This paper used the Porter stemming algorithm ( Porter, 1997 ) for English word stemming. This algorithm removes the commoner morphological and inflexional endings from the words in English. 3.2. Malay
Malay shares the same alphabetical characters with English and words are also separated by white spaces. Therefore, pre-processing steps in the Malay language are similar as those in the English language. The differences between them are the dissimilar stop words list and stemming algorithm. Both stop words and stemming rules are language-dependent, so every language has its own set of stop words and its own rules of word stemming ( Kwee et al., 2009 ).
  X  X hus X  becomes  X  X aka X , and  X  X efore X  becomes  X  X ebelum X . Some others were gathered from Malay documents, such as  X  X yuh X ,  X  X mboi X , and  X  X lamak X .

For the Malay stemming algorithm, this paper adopted the stemming algorithm used in Kwee et al. (2009) . In the Malay pear together in a word (e.g. X  mem belanja kan  X  means  X  X o spend X ). Therefore, stemming Malay words are more complicated than that of English words because it involves removing prefixes and suffixes. Moreover, because the Malay stemming algo-rithm heavily depends on the dictionary of Malay root words, a more complete Malay dictionary is needed ( Kwee et al., 2009 ). Currently, the dictionary in our experiments consists of 5300 Malay root words. Those words were taken from Bha-not X  X  Malay X  X nglish dictionary ( Bhanot, 2008 ). 3.3. Chinese
Preprocessing for Chinese is more complex than English or Malay. In the Chinese language, the word is the smallest inde-pendent meaningful element. However, the character is the basic written unit in Chinese. There is no obvious boundary be-tween words so that Chinese lexical analysis, such as Chinese word segmentation, is the prerequisite for Chinese categorization and novelty mining.
 they do not consistently lead to the same conclusions. Moreover, since there is no white space between Chinese words or expressions, we need to identify the words from the whole strings of Chinese characters. There are many ambiguities in the Chinese language, such as: (means  X  X ertainly X  in English) might be (means  X  X ertainly X  in English) (means  X  X ut X  in English) or (means  X  X f X  in English) (means  X  X ertainly X  in English). This ambiguity is a great chal-stemming is not applicable.

In order to reduce the noise brought by Chinese word segmentation and get a better word list for the sentence, we apply such as nouns and verbs so that we can get the main information by extracting these meaningful Chinese words. Moreover, it will decrease the impact of the errors in Chinese word segmentation on categorization and novelty mining because only meaningful words are considered. Fig. 2 shows the preprocessing steps on Chinese text for sentence categorization and nov-elty mining.

ICTCLAS ( Zhang, Liu, Cheng, Zhang, &amp; Yu, 2003 ) was used when performing word segmentation and POS tagging in our experiments. It is an open source project and achieves a better precision in Chinese word segmentation and POS tagging word segmentation and 95.63% accuracy in POS tagging (24 tags) ( Zhang et al., 2003 ) when the tasks were performed sep-arately. First, we apply word segmentation on Chinese sentences. Word segmentation includes atom segmentation, N -short-est path based rough segmentation and unknown words recognition. Atom segmentation is an initial step of the Chinese language segmentation process, where an atom is defined to be the minimal unit that cannot be split further. The atom mentation. Next, we detect some unknown words such as person name, location name so as to optimize the segmentation mance than just removing some non-meaningful words (like stop words) ( Zhang &amp; Tsai, 2009b ). Therefore, we POS tag the words and only keep nouns, verbs, adjectives, and adverbs in the word list. 4. Categorization
From the output of the preprocessing steps on English, Malay, and Chinese languages, we obtain bags of English, Malay and Chinese words. The corresponding term sentence matrix (TSM) can be constructed by counting the term frequency (TF) of each word. Therefore, each sentence can be conveniently represented by a vector where the TF value of each word is con-sentations of the topic and the sentences. The famous Rocchio algorithm ( Rocchio, 1971 ) is adopted to categorize the sentences to their topics.

Rocchio X  X  relevance feedback technique is a query modification process that has been extensively investigated in the lit-mum query, which can maximize the similarity with relevant sentences while minimizing similarity with non-relevant sen-tences. If R  X  R  X  is the set of relevant(non-relevant) sentences, then we wish to find a query, Q opt , such that: we obtain: where the optimum query is a vector defined by the difference of the centroids of the relevant and non-relevant sentences. ilarity is: q . 5. Novelty mining ing, compared to complex language modeling techniques such as Interpolated Aggregate Smoothing, Dirichlet Smoothing,
Shrinkage Smoothing, and Sentence Core Mixture Model. Hence, we use cosine similarity to judge whether an incoming sen-tence is similar to a set of sentences.

Cosine similarity is a symmetric measure related to the angle between two vectors. If we represent a sentence d as a vec-tor d  X  X  w 1  X  d  X  ; w 2  X  d  X  ; ... ; w n  X  d  X  T , the definition of cosine similarity is: where w k  X  d  X  is the weight of k th element in the sentence vector d .
 define novelty as the opposite of redundancy, the novelty score can be defined as one minus the similarity score. For the of its history sentences individually, where the minimum novelty score among them will be chosen as the novelty score of novel. The novelty score is defined as follows: where d i represents one of the most recent system delivered t novel sentences appearing before the incoming sentence d t each dimension. 6. Experiments and results 6.1. Dataset sentence categorization and novelty mining. TREC 2004 Novelty Track data is developed from the AQUAINT collection. In this tistics of the dataset is summarized in Table 1 . 6.2. Evaluation measures recall on a certain topic are defined as: where R  X  ; R ; N  X  ; N correspond to the number of sentences that fall into each category (see Table 2 ).
Based on all the topics X  categorization/novelty mining precision and categorization/novelty mining recall, we could get F Score ( F ) is obtained by the harmonic average of the average precision and average recall. 6.3. Translation and preprocessing
Since the original dataset is in English, we first translated them into Malay and Chinese. During this process, we inves-tigated issues on machine translation vs. manually corrected translation.
 For Malay, we conducted tests on a subset of topics translated using two methods. First, we used only machine translation Example-Based Machine Translation (EBMT). 1 Next, we manually corrected the machine translation. We discovered that the Therefore, we used machine translation for the remaining sentences.

For Chinese, we also compared the automatically translated result using Google Translate API 2 with the manually cor-Chinese had little impact on our actual results.

Then, for Malay, we performed stop word removal and word stemming, which have been discussed in Section 3 . For Chi-nese, we POS tagged each sentence and only kept nouns, verbs, adjectives, and adverbs as the candidate words to calculate the relevance and novelty scores. 6.4. Categorization We conducted experiments to compare the categorization performance on Malay and Chinese with that of English on TREC 2004 Novelty Track. The topic information from TREC 2004 Novelty Track data is extracted by title, description and narrative. We used topic title and topic description to construct the initial query. Each sentence will be compared with to this topic. To obtain the PR curve of the Rocchio algorithm, we used various fixed thresholds h The grey dashed lines show contours at intervals of 0.1 points of F Score.

From Fig. 3 , we observe that the categorization performance using Rocchio on Malay is similar with that on English and the performance on Chinese is lower than that on English. Because Malay shares the same alphabetical characters with Eng-errors mainly come from Chinese word segmentation. As our tasks were performed sequentially (word segmentation fol-lowed by POS tagging), the imprecise results of segmentation will decrease the performance of Chinese POS tagging. Li and Taylor (2003) also mentioned that the results of Chinese text categorization for small categories were much worse than cessing of Chinese text, may lead to better results.
 6.5. Novelty mining
Assuming that all the relevant sentences are retrieved by our system, we performed novelty mining on the three lan-evant sentences. Whether an incoming sentence is novel is predicted by comparing with the 1000 most recent system of novelty score thresholds, varying from 0.05 to 0.95 with a step of 0.1.
 novelty mining performance on Malay and Chinese relevant sentences is similar with that on English, which indicates that corresponding preprocessing steps, we expect to achieve the similar novelty mining result as on any single language. More-over, we noticed that the influence of preprocessing quality on categorization seems larger than that on novelty mining. Therefore, the preprocessing performance is of vital importance to the success of multilingual categorization. Assuming we could obtain the correct category for each sentence, the novelty mining performance should achieve similar performance on English, Malay and Chinese.
 We also benchmarked the novelty mining results without categorization, as shown in the dashed lines in the same Fig. 4 . ing. It is learnt that the novelty mining performance without categorization is much lower than novelty mining of relevant rithm for multilingual novelty mining. 7. Conclusion and future research
This paper studied the problem of categorization and novelty mining of multilingual sentences, which, to the best of our Malay and Chinese, and then performed categorization on TREC 2004 Novelty Track for the different languages. Finally, we conducted experiments to evaluate the sentence-level novelty mining performance for each language. Experimental results outperform Chinese. This could be due to the higher preprocessing error rate in Chinese, especially word segmentation er-rors, as Chinese words do not share the same character set as English and Malay.
 However, the results of sentence-level novelty mining show that similar results were observed in all three languages. egorization. Although in this paper we only experimented on English, Malay, and Chinese sentences, we expect that, given mining. Thus, categorization is necessary for the successful performance of novelty mining.

In our future work, we will use the results of our studies to develop an integrated novelty mining system that can cate-gorize and detect novelty for different languages. The combination of English, Malay, and Chinese is especially useful in southeast Asian countries such as Malaysia and Singapore, where documents may have combinations of all three languages. References
