 Syslog monitoring technologies have recen tly receiv ed vast atten tions in the areas of net work managemen t and net work monitoring. They are used to address a wide range of im-portan t issues including net work failure symptom detection and event correlation disco very. Syslogs are intrinsically dy-namic in the sense that they form a time series and that their beha vior may change over time. This pap er prop oses a new metho dology of dynamic syslo g mining in order to detect failure symptoms with higher con dence and to dis-cover sequen tial alarm patterns among computer devices. The key ideas of dynamic syslog mining are 1) to represen t syslog beha vior using a mixture of Hidden Mark ov Mo dels, 2) to adaptiv ely learn the mo del using an on-line discoun ting learning algorithm in com bination with dynamic selection of the optimal num ber of mixture comp onen ts, and 3) to give anomaly scores using univ ersal test statistics with a dynami-cally optimized threshold. Using real syslog data we demon-strate the validit y of our metho dology in the scenarios of failure symptom detection, emerging pattern iden ti cation, and correlation disco very.
 I.2 [ AR TIFICIAL INTELLIGENCE ]: Learning Exp erimen tation, Theory Syslog mining, Failure detection, Correlation analysis, Prob-abilistic mo deling, Mo del selection The curren t address is Nom ura Securities, 2-2-2 Otemac hi, Chiy oda-ku, Toky o 100-8130, Japan. E-mail: y1-maruy ama@frc.nom ura.co.jp
Syslogs are a sequence of events whic h are collected using the BSD syslog proto col [8]. Syslog monitoring technologies have recen tly been taking vast atten tions in the areas of net work risk managemen t, net work monitoring, autonomic computing, etc. They are used to address a wide range of imp ortan t issues including net work failure symptom detec-tion and event correlation disco very.

We are mainly concerned with the follo wing three issues of syslog mining: 1) Failur e symptom dete ction: It is to detect anomalous events whic h can be though t of as symptoms of failures as early as possible. 2) Emer ging pattern identi c ation: It is to iden tify a syslog pattern whic h has emerged when an unkno wn type of net-work/computer failure has occurred. 3) Dynamic correlation disc overy: It is to detect event corre-lations among syslogs for di eren t devices. It is imp ortan t to recognize how computer devices are dynamically correlated when anomalous events occur.
 We require that 1) and 2) be pro cessed in real-time.
The notion of dynamics is critical to addressing these three issues from the follo wing two reasons. One reason is that syslogs form a time series. Hence we must consider time cor-relation of events, whic h we call syslo g behavior throughout this pap er. This implies that syslog beha vior must be repre-sen ted using a dynamical mo del and be learned in an on-line fashion. The other reason is that patterns of syslog beha vior may dynamically change over time, because computer envi-ronmen ts are intrinsically non-stationary . This implies that syslog beha vior must be learned under the non-stationarit y assumption for syslog sources.

The purp ose of this pap er is to introduce a new metho d-ology of dynamic syslo g mining to address the three issues 1)-3) as above and to demonstrate its e ectiv eness through exp erimen ts using real data. The key ideas of our dynamic syslog mining approac h are summarized as follo ws: I) A dynamic syslog beha vior is represen ted using a nite mixture of HMMs (Hidden Mark ov Mo dels), whic h we ab-breviate as an HMM mixtur e .
 II) The parameters in the HMM mixture are dynamically learned using the on-line disc ounting learning algorithm (see [21],[20]), whic h learns the mo del by gradually forgetting out-of-date statistics. This mak es the mo del adaptiv e to the non-stationary environmen t.
 III) The optimal num ber of mixture comp onen ts in the HMM mixture is dynamically selected on the basis of the theory of dynamic model sele ction (see [10]).
 IV) An anomaly score is calculated for a series of messages on the basis of universal test statistics and an alert is raised when the anomaly score exceeds a threshold, whic h is dy-namically optimized over time.
 Functions II)-IV) are implemen ted in an on-line fashion.
We exp ect the follo wing e ects of the dynamic syslog min-ing for the three issues 1)-3) as above: As for the issue 1), it enables us to detect symptoms for failures with less false alarms than any static metho d. As for the issue 2), it en-ables us to detect the emergence of a new syslog beha vior pattern by dynamically trac king a new comp onen t in the HMM mixture. As for the issue 3), it enables us to disco ver a dynamic correlation among anomalous events for a num-ber of computer devices, e.g., \ev ent message X for device A ! event message Y for device B" where X and Y are both related to anomalous events. We empirically demonstrate these e ects using data sets collected for a real net work.
The technologies of analyzing event log les, including syslog mining, have extensiv ely been explored in the areas of dep endable computing, net work managemen t, net work monitoring, etc. Vaarandi [17] addressed the issue of failure detection using a clustering technique. It rst constructs clusters by grouping event logs on the basis of their mes-sage characters and then detects failures by trac king anoma-lous events whic h do not belong to any existing cluster. Sm yth [14] has dev elop ed Mark ov monitoring metho d for mining sequen tial patterns to apply it to deep space net-work monitoring and failure detection.

The issue of event correlation disco very has been addressed in the scenario of fault localization. Con ventional approac hes to it include alarm correlation metho d using mo del-based reasoning systems [4], fault propagation mo deling suc h as belief net work or Bayesian net works [16], code-based tech-niques [22], and AI techniques suc h as exp ert systems (see the review pap er [15]). Other related works include temp o-ral correlation mining, i.e., episo de rule induction [6],[9], and historical event log mining, i.e., perio dic pattern/similarit y pattern disco very and its visualization [3]. Perng et.al. [12] took a hybrid approac h of data-driv en and kno wledge based metho ds to event relationship net work analysis. A num ber of log le monitoring tools for rule-based correlation analysis have also been dev elop ed (see e.g., [5],[18]).

It has been pointed out in [3] that the technique of sequen-tial pattern mining [1] could be applied to failure detection and time correlation disco very. However, little of previous works addressed the issue of adaptiv ely trac king dynamics of syslog beha vior in the non-stationary environmen t, i.e., how to adaptiv ely trac k syslog beha vior patterns even when they change over time. Hence it is our primary challenge to build a new metho dology of syslog mining in terms of not only \dynamic mo deling" but also of \adaptiv e trac king of the dynamics," with applications to failure symptom detection, emerging pattern iden ti cation and correlation disco very.
The rest of this pap er is organized as follo ws: Section 2 introduces our metho dology of dynamic syslog mining. Sec-tion 3 sho ws exp erimen tal results on its applications to fail-ure symptom detection and emerging pattern iden ti cation. Section 4 sho ws its application to dynamic correlation dis-covery. Section 5 yields concluding remarks.
Our approac h to dynamic syslog mining consists of the follo wing key comp onen ts: I) probabilistic mo deling using an HMM mixture, II) on-line discoun ting learning of pa-rameters in the mo del, III) dynamic mo del selection for de-termining optimal mixture comp onen ts, and IV) scoring us-ing univ ersal test statistics with a dynamically optimized threshold. We describ e their details below.
Syslogs form a time series of events. Table 1 sho ws an ex-ample of syslogs. We call eac h row in Table 1 an event . Here \Ev ent Sev erit y" indicates the sev erit y level of the message, \Att1" and \Att2" are elds for pro cesses that generated the message, and \Message" con tains free-form text whic h gives detailed information of the event [8]. In this pap er we ignore the time stamp and keep the ordering of events only . Although eac h event may be multi-dimensional, we consider it as a sym bol belonging to a nite alphab et.

In mo deling syslogs, we divide syslogs into a num ber of sessions to get a session stream where eac h session is a subse-quence of events forming a time series. We emplo y an HMM mixture to represen t a probabilistic mo del of session gener-ation. An HMM mixture is a linear com bination of hidden Mark ov mo dels where eac h HMM comp onen t corresp onds to a syslog beha vior pattern and a mixture of K comp o-nen ts represen ts that K di eren t patterns exist. Details are describ ed below.

Let S be a nite set of states suc h that jSj = N 1 and Y be a nite set of di eren t event sym bols suc h that jYj = N where N 1 and N 2 are given positiv e integers. Let f y 1 denote a session stream consisting of M sessions where y j ( y T ( j = 1 ; ; M ). An elemen t y t denotes the t -th observ ed event in the j -th session.

For a given positiv e integer K , we assume that eac h ses-sion is indep enden tly dra wn according to an HMM mixtur e with K comp onen ts of the follo wing form: where k is a mixture coecien t satisfying k &gt; 0 and P k =1 k = 1 ( k = 1 ; ; K ) ; and P k ( j k ) denotes an HMM corresp onding to the k -th comp onen t speci ed by the parameter k . We set = ( 1 ; : : : ; K ; 1 ; : : : ; K
Let n be a given positiv e integer. A dynamic structure of eac h session is represen ted using an n -th-or der HMM of the be a vector of hidden states , we set where the summation in (2) is tak en over the set of all pos-probabilit y distribution of a tuple of hidden states ( s 2S n , and all of a k ( j )s and b k ( j )s are transition probabil-ities. We set k = ( k ( ) ; a k ( j ) ; b k ( j )).
Naive Bayes model (NB) is reduced to the special case where n = 1, K = 1, N 1 = 1 and S consists of a single state f s g : Then (2) is written as
The overall ow of tasks in dynamic syslog mining is il-lustrated in Figure 1. Events in syslogs are sequen tially input to the system. We prepare a num ber of HMM mix-tures, for eac h of whic h we learn statistical parameters us-ing the on-line discoun ting learning algorithm. These tasks are performed in parallel. On the basis of the input data and learned mo dels, we conduct dynamic mo del selection for choosing the optimal HMM mixture. We then conduct emerging pattern iden ti cation using the optimal mo del. We then conduct anomalous session detection, whic h leads to failure symptom detection. All of the tasks are implemen ted in an on-line fashion.
We introduce here an algorithm for learning an HMM mixture in an on-line fashion, where the num ber of comp o-nen ts in the HMM mixture is xed. This algorithm can be though t of as a hybrid of Baum-W elch algorithm for learning HMMs [2] and an on-line disc ounting type of EM algorithm for learning mixtur es (see [11],[20]). Belo w we describ e a new varian t of the latter for learning HMM mixtures. Every time a session y j is given, this algorithm conducts E-step and M-step only once for k = 1 ; : : : ; K to output estimates of the parameters = ( k ( ) ; k ( ) ; a k ( j ) ; b The details of the algorithm are sho wn in Figure 2. The E-step updates the memb ership probability c jk ; whic h is de-ned as the probabilit y that the j -th session is generated according to the the k -th comp onen t's distribution P k The M-step updates the parameter . For both steps a dis-counting parameter 0 &lt; r &lt; 1 is introduced in order to mak e the e ect of past statistics gradually deca y at every iteration. A larger value of r indicates that it has a smaller in uence of past examples. In the M-step k; 1 , a k; 1 , and b k; 1 , whic h are sucien t statistics of HMMs, are updated as a weigh ted average of new statistics (with weigh t r ) and old sucien t statistics (with weigh t 1 r ). The state probabil-from t n to t is s 1 ; : : : ; s n +1 while 0 k;s;t is the probabilit y that a state at t is s . They are calculated by Baum-W elch algorithm [2]. In the E-step a parameter is further intro-duced in order to impro ve the stabilit y of the estimates of c tak en only in the case of y t = y:
The total computation time for the on-line discoun ting learning algorithm for a 1-order HMM mixture with K com-ponen ts for sample size M is O ( KM ( N 2 1 + N 1 N 2 )) where N 1 is the num ber of states and N 2 is the num ber of di eren t messages.
We are concerned with the issue of detecting the emer-gence of a new syslog beha vior pattern. We reduce here this issue to that of dynamically selecting the optimal num-ber of comp onen ts for an HMM mixture and trac king its change. We realize this function on the basis of the theory of dynamic mo del selection (for short, DMS) [10].
According to [10], DMS is classi ed into two types: Se-quential DMS and batch DMS. The former sequen tially out-puts the optimal num ber of mixture comp onen ts everytime a session is input. Hence it suites the on-line setting as in this pap er. Mean while the latter outputs a sequence of optimal num bers of mixture comp onen ts after seeing all the sessions. It rather suites a retrosp ectiv e varian t of our mining setting. Belo w we sho w both types of DMS.
The key idea of sequen tial DMS is to conduct the fol-lowing pro cess every time a session is input: Sequen tially learn a num ber of HMM mixtures with di eren t num bers of comp onen ts in parallel, then select the one with the optimal num ber of comp onen ts from among them on the basis of the information-theoretic mo del selection criterion called Rissa-nen's predictiv e stochastic complexit y [13], sho wn below.
For a given num ber K of comp onen ts, for a given session stream y j = y 1 ; ; y j , we de ne the predictive stochastic complexity of the stream relativ e to the mixture mo del P ( j ) with K comp onen ts as Figure 2: On-line Discoun ting Learning Algorithm where ( j 0 1) is the parameter value estimated by the on-line discoun ting learning algorithm from a sequence of past ses-sions: y 1 ; : : : ; y j 0 1 . From the information-theoretic view-point, (3) is interpreted as the total code-length required for enco ding the stream y 1 ; ; y j into a binary sequence in a predictiv e way. At eac h time j , we select K j = K minimizing I ( y j : K ) over various values of K .
If the optimal num ber K j of comp onen ts has become larger than K j 1 at some point j , then we can recognize that a new pattern has emerged at that time. We then chec k a new comp onen t to iden tify what beha vior pattern has emerged. Similarly , if K j has become smaller than K at some time j , then we can recognize that some beha vior pattern has disapp eared at that time.
Batc h DMS de nes a map from a session sequence y M = y ; ; y M to a mo del sequence K M = K 1 ; ; K M for any data size M . Here eac h mo del indicates the num ber of comp onen ts in the HMM mixture. We denote an HMM mixture with K comp onen ts and parameter value as P ( y : ; K ).

Supp ose that K j is determined by K j 1 = K 1 K j 1 for all j . We introduce a model transition probability P j ( K K j K j 1 : ) as the probabilit y of K j = K given K j 1 speci ed by the parameter where is unkno wn.

Initially choose K 0 arbitrarily . For a session sequence y
M and a mo del sequence K M , we de ne an information-theoretic criterion for batc h DMS as follo ws: where ( j 1) is the parameter value estimated by the on-line discoun ting learning algorithm from a past session sequence y j 1 and ( j 1) is the parameter value estimated from a past mo del sequence K j 1 . Here the rst term in the righ t-hand side of (4) is the predictiv e stochastic complexit y of y
M relativ e to K M and the second term is the predictiv e stochastic complexit y of K M itself. Hence (4) is considered as the total codelength required for enco ding y M and K M into a binary sequence in a predictiv e way. Batc h DMS pro-cedure tak es y M as input and outputs K 1 K M = K M minimizing the criterion (4).

Note that the result for batc h DMS di ers from that for sequen tial DMS in general. Batc h DMS works better than sequen tial DMS in the sense that the value of criterion (4) for batc h DMS tends to be smaller than the total code-length for sequen tial DMS (see [10]). Mean while, sequen tial DMS can pro cess a session stream in an on-line manner while batc h DMS pro cesses it in a retrosp ectiv e manner only .
We mak e an assumption that a mo del can only transit to the neigh bouring states at eac h time, i.e., the transiton probabilities are given as follo ws: where 0 &lt; &lt; 1 is unkno wn and K max is the maxim um value of K .

We emplo y the batc h DMS algorithm prop osed in [10] for ecien tly computing the mo del sequence minimizing the criterion (4). The details are sho wn in Figure 3. In this algorithm as in (5) is estimated using the Kric hevsky and Tro mo v metho d [7], while the optimal path is calculated using the dynamic programming metho d like the Viterbi algorithm [19]. The computation time is ( K max M 2 ) :
Once the mo del sequence is obtained using batc h DMS, we can detect change points in the sequence to chec k the emergence or disapp earance of syslog beha vior patterns, as with sequen tial DMS. Giv en: K max : maxim um num ber of K M : data size For eac h K , (0) : initial parameter value Initialization: j = 1 For eac h K , S ( K; 0 ; 1) = log K max log P ( y 1 j (0) : K ), K ( K; 0 ; 1) = ( K ).
 Pro cedure: N
K;j : num ber of change points in K 1 ; : : : ; K j = K . ^ P ( K j K 0 ; ( N K 0 ;j 1 )): probabilit y value obtained by sub-stituting ( N K 0 ;j 1 ) for of (5).
 Mo del Selection Estimation of Mo del Transition Probabilities Output the Optimal Path: j = M ( K Output ( K 1 ; : : : ; K M ) = K ( K M ; N K
We give an anomaly score to eac h session where a higher score indicates a higher possibilit y that the session is anoma-lous and thus migh t be related to failure. Hence the de-tection of failures or their symptoms from syslogs can be reduced to the issue of anomalous session detection. The details of our scoring metho d are describ ed below.
For the j -th observ ed session y j with length T j , for the learned HMM mixture P for whic h the num ber of comp o-nen ts is determined by dynamic mo del selection, we de ne its anomaly score by Score( y j ) = 1 where ( j 1) is the parameter value estimated by our algo-rithm from a sequence of past sessions: y 1 ; : : : ; y j 1 function compress( y ) denotes a compression rate of y for a noiseless univ ersal data compression scheme suc h as Lemp el-Ziv algorithm [23]. The quan tity (6) is kno wn to be a uni-versal test statistics dev elop ed by Ziv [24]. In this scoring, Giv en: N
H : total num ber of cells : parameter for threshold
H : estimation parameter r : discoun ting parameter M : data size Initiallization: Let f q (1) 1 ( h ) g (a weigh ted sucien t statistics) be a uniform distribution. for j = 1 ; :::; M 1, h = 1 ; :::; N H , Alarm Output: For the j -th session, mak e an alarm if and only if Score( y j ) ( j ).
 Up dating Rule: if Score( y j ) falls into the cell indexed by h q Threshold Optimization: Let ( j + 1) be the least index suc h that P if any two sessions tak e the same values for the rst term (Shannon information), then the one with higher regularit y, eventually with a smaller compression rate, would result in a larger anomaly score.

We set a threshold for anomaly scores, then we deter-mine that any session is anomalous if its anomaly score ex-ceeds the threshold. Here the threshold must be optimized adaptiv ely to the score distribution, whic h may dynamically change over time.

The fundamen tal idea of threshold optimization is as fol-lows: We use a 1-dimensional histogram for the represen ta-tion of the score distribution. We learn it in an on-line and discoun ting way as with the technique of Section 2.2, then, for a speci ed value , to determine the threshold to be the largest score value suc h that the tail probabilit y beyond the value does not exceed .

We sequen tially learn a histogram of anomaly score values every time a session is input. Let N H be a given positiv e dimensional histogram with N H bins where h is an index of bins, with a smaller index indicating a bin having a smaller score. For given a; b suc h that a &lt; b , N H bins in the his-togram are set as: ( 1 ; a ) ; [ a + f ( b a ) = ( N H 2) g `; a + Let f q ( j ) ( h ) g be a histogram updated after seeing the j -th session y j : The pro cedures of updating the histogram and threshold optimization are given in Figure 4.

We usually set N H = 20 ; = 0 : 05, H = 0 : 5 ; and r H = 0 : 001.
We used four syslog data sets eac h of whic h was collected for a serv er in an ATM (Async hronous Transfer Mo de) net-work from Nov. 2001 to Jan. 2002. Event sev erit y of all of the events were under 4. Note that sev erit y score ranges from 0 to 7, with a lower score indicating higher sev erit y [8]. The num bers of events for these sets were 17859, 14533, 15273, and 26147, resp ectiv ely.

Eac h data set took the form as in Table 1. As a prepro-cessing step, we deleted all of the numerical information in messages and iden ti ed eac h message by its character infor-mation only . An event represen ted by (Ev ent Sev erit y, Att1, Att2, Message) was transformed into one sym bol. Thus ev-ery event was dealt with as a sym bol ranging over a nite alphab et. We utilized time stamps only for the purp ose of constructing sessions, as sho wn below.

For eac h data set, we constructed a session stream by grouping events of iden tical occurrence time in tens of sec-onds (hh:mm:s) into one session. For example, the event whic h occurred at time 10:12:40 and the one at time 10:12:48 were group ed together into one session. The num bers of events for the four data sets were 17859, 14533, 15273, and 26147, resp ectiv ely, while the num bers of sessions for them were 4334, 4111, 5040, and 2215, resp ectiv ely. The total num ber of di eren t events ( N 2 ) for the four data sets were 34, 22, 23, and 36, resp ectiv ely.
We applied our metho dology to failure symptom detection for the data set as above. Message \Ethernet Slot 2L1/1 Lock-up" as sho wn in Table 1, whic h we abbreviate as \lock-up," is a critical failure message in syslogs. As for our data sets, the \lock-up" was the failure whic h a net work operator taking care of the net work system though t most critical and was mainly concerned with. The num bers of \lock-up" event series in the four data sets were 3, 1, 1, and 5, resp ectiv ely.
We are interested in detecting anomalous sessions related to \lock-up" earlier than it actually occurs. We call suc h anomalous sessions failur e symptoms . Note that it is prac-tically hard to decide whether any alarm of an anomalous session is truly related to the failure or not. Mean while, in the real net work managemen t system, an operator must chec k the net work status every time an alarm is raised re-gardless of whether it is truly a symptom or not. Hence in the evaluation of this exp erimen t we formally de ne a failure symptom as any alarm that is raised within one week before \lock-up." This de nition seems reasonable from the stand-point of net work operation. We are thus concerned with the issue of how early our metho d is able to detect failure symptoms and how man y alarms it gives in total. Earlier detection with less alarms would be a better solution to this issue.

We emplo yed an HMM mixture as a dynamic probabilistic mo del of syslog beha vior patterns. The parameter values were set as follo ws: N 1 = 3, r = 0 : 1, = 0 : 5, n = 1, N 20, = 0 : 05 ; H = 0 : 5 ; and r H = 0 : 001. As for a dynamic mo del selection pro cedure, we emplo yed sequen tial DMS. For the sak e of comparison, we also emplo yed a Naiv e Bayes mo del, whic h we abbreviate as NB, as a static probabilistic mo del. For NB, the num ber of mixture comp onen ts was set 1 and r = 1 =j . Hence this varian t performs neither discoun ting learning nor dynamic mo del selection. In both mo dels the rst 10 sessions were fed to learning algorithms without scoring.
 As a post-pro cessing step, we applied the follo wing rule: Once an alarm is raised, ignore all of the alarms that were raised within 60 min utes after that time.

Table 2 sho ws the results on failure symptom detection of our metho d for HMM mixture and NB. \Lo ck-up" time is the time when \lock-up" failure actually occurred. Lead time is the earliest time when any alarm was raised within one week before the lock-up time. The sym bol \ " sho ws that the metho d was not able to detect anything. Alarm/total (ev ent) means the ratio of the total num ber of event alarms over the total num ber of events. Notice here that one session consists of a num ber of events, and that an alarm was made per session. Hence for an alarmed session, all of the events included in the session were considered as alarmed events. Computation time was measured for PC with Pentium IV 2GHz, 768MB.

We observ e from Table 2 that as for the lead time HMM mixture is almost comparable to NB except for two cases (serv ers A and C). For example, for serv er A, both meth-ods detected failure symptoms 30 min 2 days earlier than \lock-up." Most of the lead times were early enough for operators. However, alarm/total for HMM mixture is sig-ni can tly smaller than NB. For example, alarm/total for HMM mixture is around 1 = 100 while that for NB is around 1 = 50 1 = 10. This implies that the dynamic approac h mak es more reliable alarms than the static one.
Next we are concerned with the issue of iden tifying emerg-ing beha vior patterns related to \lock-up" failures. This is conducted by looking into a new emerging comp onen t in the HMM mixture when the optimal num ber of comp onen ts has changed.

A comp onen t in an HMM mixture is not necessarily easy to understand the syslog beha vior pattern of messages be-cause it includes a num ber of hidden variables. Hence we transform an HMM into a Markovian expr ession in order to mak e it more readable. This is constructed in the follo wing way: We classify eac h data into a cluster in the mixture with the largest posterior probabilit y. Then for eac h cluster we learn a Mark ov mo del from a data set assigned to it, to obtain a Mark ovian expression, i.e., a list of transitions in a descenden t order with resp ect to their probabilities.
Figure 5 sho ws how the num ber of mixture comp onen ts (clusters) changed over time. The horizon tal axis sho ws the session num ber while the vertical one sho ws the opti-mal num ber of comp onen ts in the HMM mixture calculated on the basis of dynamic mo del selection, as in Section 2.3.
We observ e that the change occurred at the 4211-st ses-sion (time Jan.15th 15:04:10), immediately after \lock-up" failure. At that time the num ber of mixture comp onen ts increased from 2 to 3. Figure 5 sho ws Mark ovian represen-tations of the three comp onen ts in the HMM mixture where the occurrence probabilities for the comp onen ts were 0.90, 0.091, and 0.009, resp ectiv ely. A correlation rule with the highest transition probabilit y is sho wn for eac h cluster.
The comp onen t with the smallest occurrence probabilit y turned out to be a pattern whic h newly emerged at the change point. Figure 5 further gives a Mark ovian represen-tation of this comp onen t. In comparison with other clusters, we found that this comp onen t was characterized by the fol-lowing patterns: \ERR:bridge:!bridgursrv: q ! ERR:bridge:!!bridgursrv: q" \ERR:gated:krt ifread:in ! ERR:bridge:!!bridgursrv: q." \ WARN:k ern:!LEC:Multicast ! WARN:k ern:!LEC:Con trol D The rst pattern means the rep etition of the message that \queue is full." The second one means that \interface error propagates to queueing error." The third one means that \con trol direct VCC down follo ws multicast forw ard VCC down."
These patterns actually characterize the syslog beha vior asso ciated with \lock-up" failure for this serv er. Thus we were able to successfully iden tify the syslog beha vior pat-terns related to a critical failure. This led to kno wledge disco very for the net work system of concern.
Supp ose that a num ber of computer devices are connected one another within a net work, where syslogs are observ ed for eac h computer device. We are interested in the issue of dis-covering how these devices are dynamically correlated when anomalous events whic h migh t be related to failures occur. A straigh tforw ard solution to it is rst to sync hronize and merge log les for all of the computers to obtain a total log le, and then to disco ver correlations suc h as belief net works from among them. However, in suc h a solution we may suf-fer from the problem that man y redundan t rules whic h are not necessarily related to failures are pro duced, and thus sig-ni can t failure-related information is buried in them. Belo w we give another solution to overcome this problem. The basic ow of our approac h is illustrated in Figure 6. The key idea is to conduct two stage learning of syslog dy-namics; one is for syslo g quantization for individual syslogs while the other is for correlation disc overy for merged sys-logs. Speci cally syslog quan tization is an imp ortan t step because it mak es our atten tion focus on the correlations of anomalous events. The fundamen tal steps of the two stage learning are summarized as follo ws: 1st Step: Quan tization. For eac h computer device, we dynamically learn an HMM mixture from syslogs and then conduct anomalous session detection using the techniques as in Sections 2.2-2.4. We then quantize sessions included in original syslogs by classifying them into anomalous ones and normal ones where any session is determined to be an anomalous one if and only if its score exceeds the threshold with = 0 : 05, otherwise a normal one. Here original event messages are preserv ed only for anomalous sessions, while those for normal ones are ignored and are uniformly trans-formed into a single message \others." 2nd Step: Correlation Disco very. We sync hronize and merge the quan tized sessions for all the serv ers to get a to-tal log le. We dynamically learn from it an HMM mix-ture again using the techniques as in Sections 2.2 and 2.3. Comp onen ts in the resulting HMM mixture sho w dynamic correlations among di eren t computer devices.
We applied the above metho dology to conduct correlation analysis using syslogs for 21 computer devices within a net-work. The total num ber of events included for the merged session stream was 181363. For eac h device a session stream was constructed in the same way as in Section 3.1 while ses-sions in the merged sequence were constructed by sliding a windo w of length 10. The total num ber of sessions in the merged session stream was 181354. The total num ber of dif-feren t event messages whic h app eared in the merged syslogs was 132. We emplo yed the same parameter values for learn-ing HMM mixtures as in Section 3.2 and sequen tial DMS for a dynamic mo del selection pro cedure.
 Table 3 sho ws examples of disco vered correlation rules. For example, the rst line sho ws that when anomalous ses-sions emerge, Message \W ARN:k ern:!LEC: Called Pa" for serv er B1 app ears, then Message \W ARN:k ern:!LEC: UNIT=0 commaE" follo ws for serv er A10 with probabilit y 4.03999e-01.
Figure 7 sho ws a macro correlation map whic h we ob-tained. Serv ers speci ed by an iden tical alphab et were lo-cated at the same node of ATM net work. All of the serv ers whic h were not correlated with other serv ers were dropp ed o from this gure. A width of a connection line is prop or-tional to the transition probabilit y, i.e., a wider line indi-cates a higher correlation. The numerical value asso ciated with eac h line sho ws the occurrence num ber of transitions between the serv ers connected by the line.

We may observ e from Figure 7 that there were strong connections between C1 and C9 and between C3 and C4. This fact is not surprising because serv ers located at the same node may usually be correlated. Mean while, it should be noticed that there were found some strong correlations between di eren t nodes: e.g., between A10 and B1 and be-tween B9 and D8. This seems quite interesting because it indicates that di eren t nodes were highly correlated eac h other when anomalous sessions occurred. It suggests that for example, any failure may propagate from the node A to the node B with high probabilit y. This correlation disco v-ery has actually been appreciated by operators and net work designers taking care of this net work managemen t system.
We have introduced a new metho dology of dynamic syslog mining for net work failure monitoring. This includes four key techniques; I) probabilistic mo deling using an HMM mixture, II) on-line discoun ting learning of parameters in the mo del, III) dynamic mo del selection for determining the optimal num ber of mixture comp onen ts, and IV) scoring sessions using univ ersal test statistics with a dynamically optimized threshold. We have demonstrated the validit y of our metho dology using real syslog data in the scenarios of failure symptom detection, emerging pattern iden ti cation and dynamic correlation disco very. Our metho dology can be straigh tforw ardly applied to the analysis of a wide range of event log les, including system calls, command lines, Web access logs, etc. In this pap er we have focused on mining sym bolic data. It is left for future study how to extend our work in order to disco ver richer kno wledge from syslogs by mining numeric data in com bination with sym bolic data. [1] R. Agra wal and R. Srik ant. Mining sequen tial [2] L. E. Baum and T. Petrie and G. Soules and N. Weiss. [3] L. Burns and J. L. Hellerstein and S. Ma and [4] G. Jak obson and M. D. Weissman. Alarm correlation. [5] S. E. Hansen and E. T. Atkins. Automated system [6] M. Klemettinen and H. Mannila and H. Toivonen. [7] R. E. Kric hevsky and V. K. Tro mo v. The [8] C. Lon vick. The BSD syslog proto col, RF C, 3164, [9] H. Mannila and H. Toivonen and A. I. Vernamo. [10] Y. Maruy ama and K. Yamanishi. Dynamic mo del [11] R. M. Neal and G. E. Hin ton. A view of the EM [12] C-S. Perng and D. Tho enen and G. Grabarnik and [13] J. Rissanen. Univ ersal coding, information, prediction, [14] P. Sm yth. Mark ov monitoring with unkno wn states. [15] M. Steinder and A. Sethi. The presen t and future of [16] M. Steinder and A. Sethi. Probabilistic fault [17] R. Vaarandi. A data clustering algorithm for mining [18] R. Vaarandi. Sec -a ligh tweigh t event correlation tool. [19] A. J. Viterbi. Error bounds for con volutional codes [20] K. Yamanishi and J. Takeuc hi and G. Williams and [21] K. Yamanishi and J. Takeuc hi. A unifying framew ork [22] S. A. Yemini and S. Kliger and E. Mozes and [23] J. Ziv and A. Lemp el. Compression of individual [24] J. Ziv. On classi cation with empirically observ ed
