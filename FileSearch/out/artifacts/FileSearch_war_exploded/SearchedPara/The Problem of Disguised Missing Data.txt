 Missing data is a well-recognized problem in large datasets, widely discussed in the statistics and data analysis liter-ature. Many programming environments provide explicit codes for missing data, but these are not standardized and are not always used. This lack of standardization is one of the leading causes of the subtle problem of disguised missing data , in which unknown, inapplicable, or otherwise nonspec-ified responses are encoded as valid data values. Following a brief overview of the problem of explicitly coded missing data, this paper discusses sources, consequences, and detec-tion of disguised missing data, including two real-world ex-amples. As the first of these examples illustrates, the conse-quences of disguised missing data can be quite serious. The key to its detection lies in first, recognizing disguised miss-ing data as a possibility and second, finding a sufficiently informative view of the data to reveal its presence. Missing data is a common problem with a variety of causes, several of which are discussed briefly in subsequent sections of this paper. In the specific case of survey sampling, this problem has been studied fairly extensively [12; 13; 14; 17; 22] and can arise from poorly designed questionnaires (e.g., inapplicable or ambiguously worded questions), errors made by the interviewer (e.g., omitted questions), or nonresponse by the interview subject (e.g., subject can X  X  remember or refuses to answer). Problems of missing data are especially prevalent in large datasets assembled from several sources. There, missing data arises either because these sources ex-hibit different degrees of completeness in collecting the same type of data, or because they collect different types of data, causing missing values to occur in blocks X  X ometimes quite large ones[17, p. 7] X  X hen the combined dataset is formed. Because it severely complicates some types of data analy-sis, there is a large literature dealing with the treatment of missing data [8; 9; 10; 11; 12; 13; 14; 15; 17; 18; 19; 21; 22]. The U.S. Food and Drug Administration X  X  Adverse Event Reporting System (AERS) [23] documents reports of adverse reactions to prescription drugs. This database is assembled from many sources, including drug manufacturers, health-care professionals, and consumers. It consists of multiple files, organized by Individual Safety Reports (ISR X  X ) that notes that cyclosporine (CSA) and FK506 measurements are complimentary: if one value is present, the other is missing. Further, CSA level can be measured in four different ways, each corresponding to a separate variable. Hence, 5 of the 34 real-valued fields in the short-term follow-up dataset con-stitute a mutually exclusive set and, although some of them are almost completely missing individually, the aggregate of the five is only about 16% missing. The point of the two examples just presented is not to criti-cize these databases X  X ndeed, missing data fractions as high as 60% have been reported in some studies [13] X  X ut rather to illustrate the character of the missing data problem com-monly encountered in practice. As to its consequences, Hor-ton and Lipsitz [11] list three important problems caused by missing data. One is the fact that many procedures cannot handle explicitly coded missing data, forcing us to modify our analysis as discussed in Sec. 1.3. Generally, these modi-fications fall into one of three classes: omission of incomplete records, imputation of missing data values, or computational modifications to explicitly deal with missing data values. Omission of incomplete records effectively reduces our sam-ple size, leading to a loss of statistical efficiency, one of the other two problems discussed by Horton and Lipsitz. For example, note that most univariate data characterizations exhibit variances that decay inversely with the sample size. Thus, reducing the effective sample size correspondingly re-duces the precision of our data characterizations. In cases where the missing data values differ systematically from the non-missing data values, substantial biases in our analysis results can arise, the third problem noted by Horton and Lipsitz. As a specific example, Mistiaen and Ravallion show that reported incomes from the Current Population Survey for the United States are more likely to be missing at higher incomes, causing the average income to be underestimated [19]. This phenomenon is referred to as non-ignorable miss-ing data and is discussed further in Sec. 3.5. There are at least four different ways of dealing with explic-itly coded missing data: deletion, single imputation, multi-ple imputation, and iterative procedures. Deletion strate-gies simply omit some or all of the missing data records, depending on the details of the analysis considered. For example, Little and Rubin [17] distinguish between com-plete case analysis , based only on complete data records, and available case analysis , based on all records that are sufficiently complete for the analysis under consideration to be undertaken. The difference between these analysis strategies can be important in datasets with many fields per record since available case characterizations involving fewer variables (e.g., univariate characterizations like means and standard deviations) will generally be based on larger data subsets than those involving more variables (e.g., multiple regression analysis). For small fractions of missing data, these deletion strategies are used quite extensively. For larger fractions of missing data, or in other cases where deletion strategies are deemed undesirable, one common al-ternative is imputation , where missing data values are esti-mated on the basis of those that are available [11; 17; 21; 22]. Single imputation strategies provide a single estimate for each missing data value. Popular examples are hot deck Table 1: The eight clinical predictor variables included in the Pima Indians diabetes dataset. and they have been adopted widely in the machine learn-ing community as benchmarks for comparing methods. The Pima Indians diabetes dataset contains records for 768 fe-male members of the Pima Indian tribe, each giving values for the eight variables listed in Table 1 together with the patient X  X  diagnosis as diabetic or nondiabetic.
 Although the metadata for this dataset indicates that there are no missing data values, Breault [3] notes that five of the variables listed in Table 1 exhibit biologically implausi-ble zero values, suggesting that this metadata is incorrect. For example, Fig. 1 shows a plot of the recorded diastolic blood pressure values for the 768 patients included in the dataset, with 35 zero values represented as solid circles. It is clear in retrospect that these values cannot be correct and must therefore be treated as missing, but Breualt notes that many published analyses have overlooked this point and have simply used the data values as recorded. Indeed, he briefly summarizes the results of approximately 70 pre-vious analyses, most of which treated the dataset as though it were complete. This oversight is extremely serious since some of the missing data fractions are quite high: triceps skin fold thickness, a measure of obesity, is approximately 29 . 6% missing while serum insulin concentration is approxi-mately 48 . 7% missing. Since 500 of the 768 patients included in this dataset are non-diabetic, simply classifying everyone as non-diabetic achieves a classification accuracy of 65 . 1%, and several of the examples discussed by Breault exhibit classification accuracies barely greater than this (the low-est reported accuracy from his list of published examples is 67 . 6%). Not surprisingly, Breault was able to obtain gener-ally better results by omitting the disguised missing values, even though this complete case analysis reduced the effective sample size from 768 patients to 392. Further illustrations of the consequences of these disguised missing data values on various other analyses are given in Sec. 3. Disguised missing data has a variety of different causes. De-liberate fraud is one obvious possibility, but other less ob-vious causes occur more commonly in practice. Ironically, one source of disguised missing data is the use of form-based electronic data entry systems with rigid edit checks, included to prevent data entry errors. A specific example described by Adriaans and Zantige [1, p. 84] illustrates the problem: Even within a single data file, multiple codes are commonly seen for missing data. For example, Little and Rubin note that in coding survey data separate codes might be used for different types of non-response (e.g.,  X  X on X  X  know X  vs.  X  X efused to answer X  vs.  X  X ut of legitimate range X ) [17, p. 3]. Another example of special coding for different types of missing data is provided by vegetation index described on the following website: http://islscp2.sesda.com/ISLSCP2_1/html_pages There, nominal data values are non-negative with negative values used to indicate three distinct types of missing data:  X  99 for measurements made over bodies of water,  X  88 for missing vegetation data over land areas, and  X  77 for mea-surements made over regions of permanent ice. Multiple representations for missing data can arise even when there is only one type of missing data. For example, missing gen-der values in the AERS database are coded as either  X  X S X  (not specified),  X  X NK X  (unknown), or  X   X  (blank). The key point of this discussion is that since there is no universally accepted way of representing or handling missing data values, disguised missing data can easily arise when the person or organization responsible for originally generating a dataset adopts a specific representation for missing data, but this representation is not communicated clearly to other individuals or organizations involved in the analysis of the dataset. The Pima Indians diabetes dataset provides a clear illustration of this point: an  X  X bvious X  (in retrospect) coding of missing data appears not to have been recognized by a number of researchers who analyzed it. The likelihood of such a breakdown in commnuication increases significantly as the distance X  X hysical, organizational, or both X  X etween the collection and the analysis of the data increases. Indeed, the prevalence of disguised missing data can be expected to increase as more completely automated procedures are used to collect and analyze larger and larger datasets. For example, Myllymaki [20] recently described an XML-based tool for automatically extracting Web data, noting that: This point is revisited briefly in Sec. 4.1. The primary effect of disguised missing data is often the in-troduction of significant biases in our analysis results. The following subsections provide simple illustrations of this point. An important characteristic of the Pima Indians diabetes dataset is that all of the missing values are encoded with the same anomalous value, a situation that occurs frequently in practice. This situation corresponds to point contamination , which causes the sample mean to shift toward the anomalous value (here, zero) and which can cause the sample standard deviation to either increase or decrease [21, p. 72]. This point is illustrated in Table 2, which gives, for each of the eight clinical variables in the dataset, the number of zero Figure 2: Relationship between two obesity measures: body mass index (BMI) and triceps skinfold thickness (TSF). Note the prominent grouping of zero values for TSF at the left end of the plot. The dashed line represents the least squares regression line fit to the original dataset and the solid line represents the corrersponding fit to the dataset with zero values of TSF removed. is widely used in quantifying the association between vari-ables, it is intimately related to regression modeling, and it forms the basis for a useful dissimilarity measure in clus-ter analysis. Unfortunately, as the following example illus-trates, disguised missing data can seriously distort correla-tion estimates. Fig. 2 plots the recorded body mass index (BMI) against the recorded triceps skinfold thickness (TSF) for the 768 patients in the Pima Indians diabetes dataset. Since both variables are obesity measures, we expect them to be positively associated, exhibiting a positive correlation coefficient. The correlation coefficient computed from the recorded data values is 0 . 393, suggestive of a weak positive association, but removing the records with zero TSF values yields a substantially larger correlation coefficient of 0 . 632. Fig. 2 also presents two regression lines, each fit by the method of ordinary least squares to the BMI/TSF variable pairs. The dashed line was fit to the complete dataset and the solid line was fit to the dataset with the TSF zeros re-moved. Since the slopes of these lines are simply the cor-relation coefficients discussed above, these results represent another way of viewing the influence of disguised missing data on the correlation results. In particular, note that the dashed line, obtained from the unmodified dataset, has the smaller slope and does not reflect the tendency for large BMI values to be associated with large TSF values as well as the solid line with the larger slope does. To provide an explicit multivariable example, consider the problem of constructing a classification tree to predict the diabetic status of the patients in the Pima Indians dataset from the eight explanatory variables listed in Table 1. Fig. 3 shows results obtained under three different treatments of the zeros appearing in the dataset. The left-most tree was constructed from the unmodified dataset, without recogniz-ing the zeros as missing data values. Specifically, this tree was generated using the classification tree procedure tree() Figure 4: Histograms of the best cross-validation tree sizes, for 100 bootstrap samples of the Pima Indians diabetes dataset, comparing three different treatments of zeros. determined by cross-validation from 100 bootstrap samples drawn from the Pima Indians diabetes dataset. The differ-ences between the results obtained from the raw data and those obtained by the factor method are relatively minor, al-though the factor method does lead to slightly smaller trees, on average (i.e., median tree size of 8 . 5 vs. 9). In contrast, the results obtained by omitting incomplete records give a significantly smaller median tree size of 6 and a generally narrower distribution of tree sizes.
 The histograms shown in Fig. 5 illustrate that the dif-ferences in variables defining the splits in the three trees shown in Fig. 3 are representative. In particular, the top three histograms in Fig. 5 show that serum insulin concen-tration (INS) rarely appears in trees constructed from the unmodified dataset, it appears slightly more often in trees built using the factor method, and it appears much more frequently in trees built using complete case analysis. In contrast, the bottom three plots show exactly the opposite trend for body mass index (BMI), which is most likely to be included in trees constructed from the unmodified dataset and least likely to be included in trees constructed using complete case analysis. In dealing with missing data, it is often assumed that the missing values are distributed randomly through the dataset. This assumption corresponds to the missing completely at random (MCAR) missing data model [17, p. 12] and it is the simplest case to deal with, representing a  X  X est be-haved X  missing data scenario. Unfortunately, this assump-tion frequently fails in practice as the probability that an observation is missing commonly depends either on other observed data values, giving rise to the less restrictive miss-ing at random (MAR) missing data model, or on the missing data values themselves, leading to the not missing at ran-dom (NMAR) missing data model [17, p. 12]. An example of this last case is the reported income data considered by Mistiaen and Ravallion [19] discussed in Sec. 1.2. An important practical issue in dealing with missing data that occurs systematically rather than randomly is that sim-Figure 6: Differences in estimated age distributions between Pima Indian diabetes records with missing insulin data val-ues and those with non-missing insulin data values. knowledge (he is an MD) and a preliminary examination of the ranges of the data values. While such domain-specific validations can in principle be included in highly automated data collection or analysis systems, often they are not be-cause the developers are not domain experts. Myllymaki terms these validations  X  X emantic checks, X  notes they are  X  X omain-specific but very powerful, X  and describes an ex-ample for stock market data, noting that stock prices seldom exceed $ 1000 per share [20].
 Conversely, even very limited partial domain knowledge can sometimes be extremely useful in uncovering disguised miss-ing data. That is, even if we do not have precise upper or lower bounds on data variables like blood pressures or stock prices, the knowledge that they are necessarily posi-tive means that zero or negative values are infeasible and can be identified as anomalies, possibly encoding missing data. An outlier may be defined [2, p. 4] as: If the values selected to encode missing data are sufficiently far outside the range of the nominal data to appear as out-liers, we can apply standard outlier detection procedures to look for disguised missing data. Conversely, it is important to note three points. First, not all disguised missing data values will necessarily be detected as outliers. In fact, this situation holds for the Pima Indians diabetes dataset: while space limitations do not permit a detailed discussion of the results here, the zero values in this dataset are generally not extreme enough relative to the valid data values to be detectable as outliers. Second, even if these values are all de-tected as outliers, additional outliers may also be detected, requiring us to examine the results further to find the dis-guised missing values. Finally, it is important to note that a variety of procedures for univariate outlier detection exist and they generally find different sets of outliers in the same dataset [21, Ch. 3].
 Figure 8: Normal Q-Q plot for the triceps skinfold thickness data. The flat lower tail in this plot gives a strong indication that something is unusual in this data sample. the single point in the upper right corner of the plot repre-sents an isolated outlier that may be seen clearly in Fig. 7. The first of these features X  X he pronounced horizontal lower tail X  X rovides clear evidence of the distributional anomaly caused by the repeated zero values in the dataset. In fact, this anomalous lower tail behavior is an extremely useful indicator of the presence X  X r at least the possibility X  of disguised missing data in all of the clinical variables in-cluded in the Pima Indians diabetes dataset. This point is illustrated in Fig. 9, which shows the resulting Q-Q plots for four of these eight clinical variables. The upper left plot shows the normal Q-Q plot for the diastolic blood pressure values shown in Fig. 1. As with the plot in Fig. 8 for the triceps skinfold thickness, the flat lower tail in this plot cor-responds to the repeated zeros in the dataset, leading us to immediately focus on these disguised missing data values. The same observation holds for the serum insulin concen-tration (INS) Q-Q plot shown in the upper right in Fig. 9; the primary difference is the greater width of this lower tail, reflecting the much greater number of zero values in the INS data sample. The lower left Q-Q plot is that for the diabetes pedigree function, which has no recorded values of zero and which therefore lacks the flat lower tail seen in the upper two plots. Finally, the lower right plot is the normal Q-Q plot for the number of times pregnant (NPG), which is a difficult case since, while this data record does contain a significant number of zeros, this value is plausible for NPG. Also, since NPG assumes only integer values, every portion of this Q-Q plot is flat, indicating repeated occurrances of these integer values. Hence, the flatness of the lower tail is not indicative of a data anomaly for this variable, but the width of this tail does raise the question of whether the zero value is over-represented for NPG. Without knowing how this variable should be distributed, we cannot say whether this is the case or not, but the shape of the Q-Q plot does lead us to raise the question. The problem of outliers in data is well-known and widely discussed in the literature [2; 21]. Less well-known is the Figure 10: Fraction of data records with specified month-level latency values, from L = 0 to L = 120 (10 years), com-puted from first quarter 2002 AERS data. The dashed line represents the results computed from the complete dataset, while the solid line represents the results obtained after re-moving all records with an Event Date of  X  X anuary 1. X  and has no impact on any of the other secondary peaks. Similarly, removal of all records with Event Date of  X  X anuary 1, 2000 X  causes the peak at a latency value of 27 months to disappear, and analogous behavior is observed for the other, smaller secondary peaks as  X  X anuary 1 X  dates from earlier years are removed from the dataset. This behavior suggests that the data anomaly is associated with the recorded Event Date  X  X anuary 1, X  in any given year.
 Further support for this view is provided in Fig. 11, which plots the fraction of recorded event dates as a function of the day for the months January, February, March, and April. In the absence of any association between day of the month and adverse event, we expect an approximately uniform dis-tribution of day values, indicated by the horizontal dashed lines in Fig. 11. It is clear that most of the observed results conform reasonably well to this expectation, except for the first day of each month, which always appears much more frequently than expected, but especially for January. Overall, these results strongly suggest that  X  X anuary 1 X  is commonly used as a surrogate for  X  X ate unknown X  in en-tering Event Date data into the AERS system. These re-sults also suggest that the first day of other months is used this way, but less frequently than  X  X anuary 1. X  It is possi-ble that, excluding January, the first of the month is used as a surrogate for a known month but an unknown day (e.g.,  X  X pril 1 X  for  X  X ometime in April X ), corresponding to the phenomenon of heaping [9; 10]. In contrast to missing data values, which may be regarded as completely unknown, heaped data values may be regarded as coarsely quantized and thus imprecise but partially known, similar to censored data encountered in survival analysis, where lower bounds on survival times are known for patients who were still alive at the end of a study. All of these forms of imprecision may be regarded as special cases of coarsened data [9; 10], which refers to data values that have been imprecisely observed to varying degrees by a variety of mechanisms.
 the dataset. Specific techniques include comparing the data values with known  X  X easonableness limits, X  either on the ba-sis of detailed domain-specific knowledge (e.g., Myllymaki X  X  $1000 upper limit for stock prices [20]) or on the basis of par-tial knowledge (e.g., that variables are necessarily positive), the use of automated outlier detection procedures followed by careful analysis of the anomalous data observations de-tected, or the use of other characterization methods like the quantile-quantile plots discussed in Sec. 4.3.
 In more subtle cases, like those involving inliers, the detec-tion of disguised missing data may require the use of more application-specific analyses where the expected outcome is known in advance. This point is illustrated in Sec. 4.4, where an analysis of AERS Event Date latency data mostly gave the expected result (i.e., a large main peak in the distri-bution of latency values) but also showed unexpected aux-illiary peaks. Subsequent investigation revealed that these peaks were due to the use of the date  X  X anuary 1 X  as a surro-gate for  X  X ate unknown. X  The key objective of this example was to illustrate three points: first, that inlying disguised missing data values sometimes can be detected; second, that the key to this detection lies in performing simple analyses where the general form of the expected result is known at the outset; and third, that even when we can detect strong evidence for disguised missing data, we may not be able to tell which specific records exhibit this problem (e.g., which  X  X anuary 1 X  entries are legitimate).
 Finally, it is important to recognize that there may be cases where we cannot be certain whether disguised missing data is present or not, as in the case of the variable NPG (num-ber of times pregnant) in the Pima Indians diabetes dataset. There, since the zero value used to code missing observa-tions in other variables in this dataset is plausible for NPG but possibly over-represented, we cannot say with certainty whether it is used to code missing NPG data or not. [1] P. Adriaans and D. Zantinge. Data Mining . Addison-[2] V. Barnett and T. Lewis. Outliers in Statistical Data . [3] J. Breault. Data mining diabetic databases: Are rough [4] L. Breiman. Bagging predictors. Machine Learning , [5] L. Breiman. Heuristics of instability and stabilization [6] C. Date. An Introduction to Database Systems . [7] D. DesJardins. Outliers, inliers, and just plain liars X  [8] A. Feelders. Handling missing data in trees: surrogate
