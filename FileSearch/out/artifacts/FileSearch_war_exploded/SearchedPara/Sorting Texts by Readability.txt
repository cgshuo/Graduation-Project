
Graduate School of Information Science and Technology, University of Tokyo training data annotated with two reading levels. The proposed method is compared with regres-called Terrace, which retrieves texts with readability similar to that of a given input text. 1. Introduction
Readability assessment is an important NLP issue with much application in the domain of language education. The capability to automatically judge the readability of a text would greatly help language teachers and learners, who currently spend a great deal of time skimming through texts looking for a text at an appropriate reading level. suming that these reflect the text reading level. For example, Kincaid, Fishburne, and
Rodgers (1975) assumed that the lengths of words and sentences represent their re-difficult to apply to languages other than English, because some features, such as word length, are specific to alphabetic writing. Such methods, however, do not compete with recent methods based on more sophisticated handling of language statistics. Collins-
Thompson and Callan (2004) proposed a classification model by constructing different language models for different school grades (Si and Callan 2001), and Schwarm and
Ostendorf (2005) applied a support vector machine (SVM). Both of these methods outperform classical methods and are less language-dependent.
 tempted for multiple languages: the lack of training corpora. Large amounts of training data annotated with 12 school grades have not been at all easy to obtain on a reasonable scale. Another possibility might have been to manually construct such training data, but humans are generally unable to precisely judge the level of a given text among 12 arbitrary levels. The corpora therefore have to be constructed from academic texts used in schools. The amount of such data, however, is limited, and its use is usually strictly limited by copyrights. Thus, it is crucial to devise a new method or approach that allows readability assessment by using only generally available corpora, such as newspapers. 12 levels, but given two texts, there should be a better chance of judging which of them is more difficult. This intuition led to the new model presented in this article. Our idea is based on sorting , which is implemented in two stages:
The first step requires a training corpus, but because the comparator only judges which of two texts is more difficult, the texts of a corpus need only be labeled according to two different levels. Two sets of texts X  X ne difficult, the other easy X  X re far easier to obtain than a training corpus annotated for 12 different levels. Overall, our model of readability thus differs from previous regression or classification models. system that retrieves a text with readability similar to that of a given input text. Terrace was originally motivated by a faculty request made by teachers of multiple foreign languages. The system currently works for English and Japanese, and the languages will be extended to include Chinese and French.
 ods. Although our method does compete well with previous methods, the classification approach used in any given scenario should remain the most natural, relevant method.
The intention of this article is simply to propose an alternative way of handling read-ability assessment, especially when adequate training corpora annotated with multiple levels are not available. 2. Related Work
Readability, in general, describes the ease with which a text document can be read and understood. Readability is studied in at least two different domains, those of coherence (Barzilay and Lapata 2008) and language learning. Readability in this article signifies the latter, for both a mother tongue and a second language.
 DuBay 2004a, 2004b). DuBay (2004a) writes that:
Every method of readability assessment extracts some features from a text and maps the feature space to some readability norm. There are the two viewpoints regarding features and the mapping of feature values to readability, and correspondingly there are two kinds of work in this domain. 204
Recently, Pitler and Nenkova (2008) presented an impressive verification of the effects of each kind of feature and found that vocabulary and discourse relations are prominent, although other features are not negligible.
 use the same features throughout the article, as explained further in Section 3.1. Rather, the focus of this article is on mapping the extracted feature values to a readability norm. So far, two models have been used for this: regression and classification.
 of feature values. Early methods, from the Wannetka formula (Washburne and Vogel 1928), to the recent methods of Flesch X  X incaid (Kincaid, Fishburne, and Rodgers 1975) and Dale X  X hall (Chall and Dale 1995), are of this kind. Elaboration of such regression methods in a more modern context could proceed through a generalized linear model based on estimation of the weights by machine learning, although we have not found such an approach within the literature of readability assessment for language learning. Our proposal is compared with such an enhanced version of regression in Section 8. is conducted as a classification task. The first is implemented by means of statistical classification modeling, as reported in Collins-Thompson and Callan (2004) and Si and Callan (2001). The authors used a language model (unigrams) and a naive Bayes classifier by presuming different language models for each reading level. A language model M i is constructed for each level of readability i by using different corpora for each level. The readability of a given text T is assessed using the formula L ( M
 X  w  X  T C ( w )logPr( w of w ,andPr( w | M i ) denotes the probability of w under M
SVM (Schwarm and Ostendorf 2005) and the authors also studied the effect of statistical features, such as n -grams and syntactic features.
 classes of readability. That is, given a single text, the system assigns a value correspond-ing to a school grade. The result is easy to understand, and various applications have been constructed with this type of scoring. This solution only works, however, when a sufficient amount of training data with annotations regarding multiple levels is pro-vided. Usually, the availability of training data in readability assessment is limited, even for school grading. This is due to the inherent difficulty of classifying the readability of a text into 12 grades, making it difficult to uniformly construct a large set of training data.
Moreover, the copyright issue is more serious for academic texts. when readability assessment is modeled by regression or classification, a research team wanting to apply these previous methods faces the problem of assembling training data, as we did for over a year.
 two texts, a comparator judges which is more difficult. By applying this comparator, a set of texts is sorted. The readability of a text is assessed by searching for its position within the sorted texts. The norm is thus considered as the location of a text among an ordered set of texts. Our approach linguistically enhances assessment of the readability of
Inui and Yamamoto (2001), the readability of sentences for deaf people is judged by a comparator generated by an SVM. In addition, Pitler and Nenkova (2008) presented a comparison of texts in terms of difficulty by using an SVM. Similarly to what we present in Section 3.1, those authors propose constructing a comparator by using an SVM to compare two sentences or texts with multiple features. However, neither further applied this approach to obtain readability assessment based on sorting. Our contribution in this study is therefore that we show how a machine learning method can be used as a comparator and applied to sort texts.
 ordered training data. Various methods have been proposed so far. In one of the earliest attempts, Cohen, Schapire, and Singer (1998) obtain a function that scores the probabil-ity that an element is ranked higher than another, and rank all elements by maximizing the sum of the pairwise probabilities. In another, Joachims (2002) applies an SVM to rank elements, by devising the input vector by subtraction of feature values. In more recent studies, such as Xia et al. (2008), an attempt is made to directly obtain the ranking function for the whole ordered training data, not as a composition of pairwise function application between elements. Among these methods, our proposal is unique in two ranking based on sorting. Second and most importantly, all previous methods assume the existence of fully ordered training data. In contrast, as emphasized by our problem described in this section, such training data are difficult to acquire in the readability domain, and we have to devise a method which works even when very limited training using a learning-to-rank method even when learning data are only partially available.
Such an approach can be further considered for learning-to-rank methods in general in future work. 3. The Method
In our method, a comparator of the level of difficulty of two texts is generated by using machine learning and then the comparator is applied to sort a set of texts. The method has two parts:
These tasks are explained in the following sections. 3.1 Readability Comparator
The readability comparator is constructed by applying machine learning. Given texts a , b  X  S , where S is a set of texts, feature vectors V an operator  X  , V ab is constructed as V a  X  V b . When V comparator outputs 1 when a &gt; b (i.e., a is more difficult) and  X 1 when a &lt; b . Because 206 the output is binary, we use an SVM to construct the comparator (see Figure 1). Note that V ab and V ba are not the same. Reversibility of a feature vector is thus not obvious in our work, but ideally, when the judgment for V ab is 1, that for V been proposed (Klare 1963; DuBay 2004a, 2004b; Schwarm and Ostendorf 2005; Pitler and Nenkova 2008). In this work, we only utilize the most basic features of vocabulary the focus of this article is not to study the set of features, it is best to set the feature issue aside and use only the most fundamental features. Even then, there are many viewpoints to be verified, as will be seen in Sections 7 X 9. Furthermore, we have to take into account the pros and cons of various features, because naive features only capture coarse, default trends and could degrade performance. For example, the text length in our data tends to be longer for more difficult texts, thus having a bad influence on short, higher-grade texts and long, lower-grade texts. Therefore, in this article, we only consider simple features regarding vocabulary. Second, previous work has argued for the fundamental nature of vocabulary as a factor in readability (Alderson 1984; Laufer 1991; Pitler and Nenkova 2008). Third, some features other than word frequencies are language-dependent in terms of the writing system, corpus availability, and perfor-mance of NLP analysis systems. For example, average word length, used in the Flesch X 
Kincaid approach, cannot be applied to Japanese. Thus, we have focused on features that are available for any language, so that the performance becomes comparable across languages.
 and global factors. The local factor is what words are used and how frequently they ap-pear within a text, whereas the global factor indicates the degree of readability of these words among the overall vocabulary. Both are considered within this article, as follows:
Local: the frequency of each word divided by the frequency of the number of words in Global: the log frequency of words obtained from a large corpus.

Local frequency is the most basic statistic used in machine learning methods. Relative frequency is used to avoid the influence of text length, as mentioned above. a level of familiarity that is fairly commonly understood among people. For exam-ple, the verb meet is more familiar than the verb encounter . Such levels of familiarity (1995), the authors counted the number of words not appearing in a list of the most basic 3,000 words, in order to judge vocabulary difficulty, but the plausibility of such a list is difficult to evaluate and such lists might not exist for languages other than English.
An alternative is to use word lists with familiarity scores, such as the MRC list in English (Database 2006) or the Amano list in Japanese (Amano and Kondo 2000), but such lists are costly to generate and the word coverage is limited. Instead, we decided to use the log frequency obtained from several terabytes of corpora, because the psychological familiarity score is known to correlate strongly with the log frequency obtained from corpora, as reported in Tanaka-Ishii and Terada (2009) and Amano and Kondo (1995). Tanaka-Ishii and Terada show that log frequency correlates better for larger corpora.
English and 2 terabytes for Japanese X  X ossibly the largest corpora available for the two languages X  X ere used to obtain the log frequencies.
 be compared. The final vector is generated by applying an operator
V  X  V
Concatenation: concatenate V b  X  X  elements after the elements of V
Conjunction: produce a new value for the i th element of V
The two possible vectors for V ab are explained by the illustration in Figure 2. The upper half of the figure shows V ab generated by concatenation. Because the comparator is 208 constructed by machine learning, the dimension of the vectors is four times the number of word tokens that appear in the training data. Each kind of word has a location in a vector, which appears four times within V ab : twice in each V global features. The first half denotes V a , and the second half denotes V of V a , the vector values are the relative frequencies for words appearing in text a ,orzero otherwise. For the second half of V a , the values are log frequencies measured in a large corpus for the words appearing in a , or zero otherwise. V manner, and the two vectors are concatenated.
 the number of word tokens appearing in the training data, as illustrated in the lower half of Figure 2. The i th dimension of V ab is calculated as a function of the i th values of
V a and V b . Pitler and Nenkova (2008) are likely to have used subtraction within their experiment. 2 For this reason, among various other possible functions for the conjunction of two vector values, we also used subtraction.
 the difficulty between two texts, the training data contains two sets of texts, a set L on a combination of texts taken from L e and L d . The training data can combine up to 2  X | L could amount to 720,000. Whether learning can fully exploit such combined training data is an interesting issue, which is examined in Section 8.
 inputting V a b into the SVM. In this phase, if a new word which was not in the training corpora appears in either a or b , the word is ignored for the purposes of this work. 3.2 Sorting and Searching Texts
With a comparator thus constructed, the texts of S are sorted. Further, readability assessment is performed by searching for the position of a text within the sorted texts. already established within computer science. There are further requirements, however, as follows:
Given the first two requirements, we chose binary insertion sort and binary sorting from among various sorting and searching algorithms, because it provides one of the fastest search methods. comparisons at one time, as explained here. In binary insertion sort, given a new text, the insertion position is searched for among the previously sorted texts (from the easiest text to the most difficult text), and the text is inserted between two successive texts i and i + 1, where the text is more difficult than the i th easiest text and it is easier than the i + 1th text. Through repetition of this operation, all texts are sorted. Because our comparator is generated using an SVM, one sole comparison could lead to an erroneous result. Moreover, one comparison error could lead to disastrous sorting results with a binary sort, because binary search changes the insertion position significantly, especially at the beginning of the search procedure.
 multiple comparisons with texts located at the proximity of each position. The number of multiple comparisons is called the width and is denoted as M = 2 K + 1, with K being a positive natural number, as illustrated in Figure 3. M is dynamically changed during the search procedure by narrowing it down to indicate the exact position. The procedure starts with a given set of texts S and an empty set SS . At each judgment, a text to be inserted is removed from S and inserted into SS . The procedure proceeds as follows: 1. Obtain a text x from S .If SS is empty, the text x is put into SS and another 2. The texts of SS are already sorted and are indexed by i = 1 ... n , where 3. If M &gt; ( top  X  bottom ) / 2 , then K  X  ( top  X  bottom ) / 2 . A change in K also 4. Compare x with texts at i = middle  X  K ... middle + K . If more than K texts 5. If top &lt; bottom , then insert x before the bottom . Otherwise, go to step 3. 210 6. Insert x before bottom . 7. If | S | &gt; 1, then go to step 1. Otherwise, the procedure is complete. The complexity for sorting a set | S | amounts to O ( C | of M . Because the time complexity required for insertion is log n , it might seem that binary insertion requires the whole data structure to be implemented by an array, which requires copying of the whole SS at every insertion. Note that the sorting speed at this point does not affect the usability of our application, since the text collection is sorted offline (as explained in Section 5).
 the readability of a text is assessed by searching for its insertion position. The searching is done by binary search, through the same procedure in steps 2 to 6. The computational complexity is O ( C log | SS | ), where C is the maximum value of M . 4. Pros and Cons of the Proposed Method
Now that we have explained our method, we compare it with the previous models of regression and classification mentioned in Section 2. The comparison is summarized in Table 1.
 score or class, which typically has been indicated in terms of school grades (third row).
In contrast, readability in our method is presented as a position among texts, indicating nature of the output of assessment, the regression method is continuous, in that feature values are mapped to scores within a continuous range, whereas classification and our method are both discrete, in that the former gives a class and the latter gives a rank (fourth row).
 of our method is that it facilitates the construction of training data (fifth row), because it requires only two sets of typically difficult and easy texts. Here, what kinds of corpora the regression method requires in the modern machine learning context has not been clarified because of the lack of previous work in machine learning regression. However, easy training data will not be sufficient, and machine learning regression requires texts labeled with scores for different levels.
 only outputs a relative ranking, applications must be re-designed differently from those in the previous work (sixth row). It could be said that when a &gt; b for documents a and b , then a contains more unfamiliar words as tokens. Even if two texts are next to each other, however, their readability could be very different. For example, texts of grades 1 and 11 could be next to each other in a collection if it lacks texts from grade 2 to 10. An application system generated using our model must cope with this new problem caused by this lack of absoluteness for the readability norm. In the next section, we show how this problem is dealt with in Terrace through the use of graphical representation. position of texts within the readability norm. On the other hand, because our method methods require only O (1) time for searching, whereas our method requires a certain amount of time before a response is obtained. Therefore, it must be verified that an ap-plication works within a reasonable response time when handling a large text collection.
In Section 9, we show that the response time is indeed within a reasonable time. 5. Terrace: An Application
As mentioned in Section 1, creation of this system was motivated by a request from faculty members who teach multiple foreign languages. These teachers must look for up-to-date reading materials with appropriate reading levels every day. This requires scanning through newspapers and Web resources. The teachers X  request was that we construct a system that would facilitate this text search task. More precisely, given a sample text used within the classroom, the system should return texts of similar levels of readability from among newspaper articles and other on-line archives.
 and Collins-Thompson and Callan (2004). Because the request covered multiple lan-guages, though, we faced the corpus construction problem in different languages. This
English were unavailable. We were forced to look for another path towards building the requested system without annotated corpora having academic grades.
 intermediating a score/class, because both the input and output are texts. Thus, chang-framework presented so far.
 returns texts of similar readability from among a collection of texts usable as teaching materials, which are crawled for and obtained from the Web. Currently, Terrace works for English and Japanese. The number of languages is currently being increased. displayed.
 collection is 14,877 and each is taken from CNN (CNN 2008). The ranking in this example is 8,174th from the easiest text. A horizontal bar is shown below, indicating the location among the 14,877 texts, with triangular indicators showing the locations of texts with annotation in terms of grades. These indicators are meant to help users 212 shows where a text with an annotation of grade one is located within the bar. Note that such indicators are easily generated, because the number of annotated texts does not need to be large. listed. Further down, more difficult texts are listed. By clicking on any of these texts, the user can obtain texts with the desired readability.
 ing and for crawling, as shown in Figure 6. The crawler collects texts from news sites and other related archive sites every day, and the module incrementally sorts the collected documents. These texts are searched upon a user request. 6. Data for Evaluation In the rest of this article, the proposed method and the Terrace system are evaluated.
The key question to be considered through the evaluation is whether the comparator can discern slight differences in the readability levels of test data from only two sets of training data that are roughly different.
 in Table 2, where the upper block corresponds to English and the lower to Japanese. For 214 both languages, there were training data and test data. The test data consisted of two sorts: TD1: A collection of texts taken from the same kind of data as the training data.
TD2: A collection of texts unrelated to the training data and originally assigned levels
TD2 further consisted of two kinds of data in each language: data for learners of their mother tongue (i.e., children and students) and data for learners of a foreign language. These are labeled as TD2-M and TD2-F, respectively.
 (TimeForKids 2008). We downloaded 600 articles (that is, | were used as TD1-E. The total number of different words in TD1-E is 22,736, which is the dimension of the feature vector of a text when TD2 is used as the text data. using subtraction as  X  , the dimension is doubled (for local and global), and when using concatenation, the dimension is four times this value. TD2-M-E consisted of the data set called AtoZ, which can be purchased (ReadingA-Z.com 2008). Each of the texts in this data set is labeled by 27 levels and graded by 5 levels. TD2-F-E consisted of the English textbooks used in Japanese junior high and high schools (Morizumi 2007; Yoneyama 2007). These texts are classified into five grades and also linearly ordered; that is, the texts become more difficult in their order of appearance in the textbooks. These levels and orders originally attached to the data were used as the gold standard in this study.
For the global frequency, we used the log frequency of each word as measured from almost 6 terabytes of Web data in English, scanned in the autumn of 2006 (Tanaka-Ishii and Terada 2009).
 (AsahiNewspaper 2008; KodomoAsahi 2008). Six hundred (600) articles were acquired, and 100 of these were used as TD1-J. The total number of different words in this training data is 48,762. 4 TD2-M-J consisted of Japanese junior high and high school textbooks with six grades, which also appeared in a linear order (Miyaji 2008). TD2-F-J consisted of the texts used in the Japanese language proficiency test (JEES 2008). The texts in this data set are classified into four levels and not linearly ordered. For the global frequency, we used the log frequency of each word as measured from almost 2 terabytes of Web data in Japanese, scanned in the autumn of 2006.
 used LIBSVM (Chang and Lin 2001). The SVM training was done using the parameters of cost = 0.1 and gamma = 0.00001 with a Gaussian kernel. The value of K used in robust sorting (Section 3.2) was K = 2. 7. Evaluation of the Comparator
The basic performance was first tested using TD1. Because were chosen and paired randomly, and 2  X  500 = 1,000 pairs were used for training. The factor of 2 is necessary, because a pair can be used twice by exchanging the comparison order V ed and V de . The remaining 100 texts were randomly paired and tested. The results presented here were produced through six-fold cross-validation, and each fold was further repeated five times by changing the pairing of training and testing (i.e., total execution was done 30 times to obtain one performance value).

The rows of a block represent the different operators explained in Section 3.1. Overall, the scores were above 90%.
 training data amount was set to 2  X  600. The accuracy was measured for all pairs of texts with different levels. For TD2-M-E, all five levels were considered, whereas for the linearly ordered TD2-F-E and TD2-M-J, the levels were considered by grade (that is, five levels for TD2-F-E and six levels for TD2-M-J). The accuracy reported here is the average of execution done five times by changing the random pairing for the training data. The performance was, in general, lower than that for TD1, but still stayed close to 90%. For the operator  X  , whether concatenation or subtraction was used made no difference. Therefore, from here on, the operator  X  is set to concatenation. two-class combination was determined for TD2-M-E in English, as shown in Table 5.
Because TD2-M-E has five grades, the columns indicate the 1st to 4th grades, whereas the rows indicate the 2nd to 5th grades. The evaluation thus forms a 4 each cell indicates the accuracy of distinguishing the two X  X lass pairs for the row and column.
 216 the levels to be distinguished became closer to each other. The results reflect this ten-dency, with lower values for cells closer to the diagonal. In particular, the performance between the 1st/2nd and 2nd/3rd grades was more successful than that between the 3rd/4th and 4th/5th grades, since the lower the grades the easier it is to discern two given successive school levels.
 mal our generated comparator was. Ideally, we want a complete ordering of the set, and for this the comparator must obey certain laws in the sense of mathematical sets. A comparator is considered abnormal if it does not obey two laws: Transitivity: If a &lt; b and b &lt; c , then a &lt; c .

Especially for transitivity, in an ordered set this law is the primary requirement that must be fulfilled among ordered elements. If transitivity does not hold in many triples, we have to introduce partial ordering instead of the total ordering considered thus far. by changing the random pairing of test data (TD1) five times. For reversibility,
Such strong results were obtained because the training was done by reversing the order of the pairs (thus, the SVM learned both V ed as +1 and V transitivity,
Such results show how rarely these anomalies occur in our method. Therefore, our choice of total ordering seems relevant. 8. Evaluation of Sorting
Since the basic results have been clarified thus far, we will now report the results for concatenation for a more global evaluation of sorting and searching.
 tion with the correct order was investigated. Three methods were used for comparison:
In these three methods, the readability level is obtained as a value, whereas our method presents an order. Therefore, the results of the three methods were sorted according to the values. The resulting orders for the three methods and for ours were then compared with the correct order in the test data. We used the finest annotation for correct ordering; TD2-M-J, and 4 levels for TD2-F-J.
 the texts of Time / Asahi for children as  X 1.0, and then using the 600 texts for training. The LIBSVM package was used with the same kernel and parameter settings given in Section 6.
 tion formula is where n is the number of texts, and d i is the difference in ranking between the correct multiple elements having the same ranking. Given x and y as ordered sequences with the same ranking, the correlation is given as follows: where
Here, n x and n y are the numbers of the rankings for equivalently ranked elements in x and y , respectively, and t i , t j denote the number of elements with the same ranking [1,2,3,3,4], n x = 1, because only 3 had the same ranking, t are two 3s.
 of TD2, and the vertical axis represents the correlation value. Note that for the Japanese 218 data, there are only two bars, because Flesch X  X incaid and Dale X  X hall are inapplicable.
Moreover, note that the vertical heights are comparable within a data set but not among data sets, because the number of levels for each data set is different.
 a correlation of more than 0.8 for all cases. Flesch X  X incaid performed quite well, having effective, with the correlation being lower than that with our method. This shows that the performance of the regression method is limited, even with a machine learning method, when training data for two levels only are available. In contrast, our method shows that even with rough two-level training data, high correlation is achievable. Such performance is enabled by comparison among texts even in the middle range between difficult and easy texts.
 cult to say. Above all, our method cannot be fairly compared with previous classification methods from the viewpoint of classification, because in order to transform our sorted results into classes, we would have to give the number of texts in each class. Because this information is not provided to the classification methods, the comparison would be unfair, thus favoring our method. Therefore, the comparison must be made by means of correlation. Another problem is that because we do not possess the training data used in previous work, we could only test with what we have listed in Table 2.
 the amount of training data was small, the number of classes considered here was five.
Slightly less than half of the data (67 texts) was taken from each of five different levels, and the remaining texts (which differed in number at each level) were used for testing: a sufficient amount of test data is needed, too, since our evaluation is done through correlation measured on sorted results. By exchanging the halves, the result reported here is the average of two-fold cross-validation.
 1. A classification method using part of TD2-E-M as training data. 2. The sorting method using the same part of TD2-E-M as training data. 3. The sorting method with TD1-E as training data.

The first classification method followed that of Schwarm and Ostendorf (2005). The amount of training data used to build a classifier for each class was 67 for +1 and 4 for  X 1. The parameters used for the SVM were the same as those used for our method.
For the second method, the training data consisted of 1,340 (2 classes]  X  67 texts) random pairs of two successive levels. Each fold of two-fold cross-validation was done five times by changing the pairs randomly. For the third method,
TD1-E was used as in the previous evaluation, but this time the verification was done with five levels (whereas the first block in Figure 7 was evaluated with 27 levels). The amount of training data was the same as TD1 (i.e., 1,200) and the experiment was done five times by changing the pairs randomly.
 responding to each of the three methods. For classification, the correlation was 0.925, whereas the correlations of the second and third bars were 0.946, 0.941, respectively. Our methods thus slightly outperformed the classification method. Moreover, the difference between the second and third methods showed that two-level training data could perform similarly to multiple-level training data. This shows the strength of our method 220 in that it can complement a limited amount of training data through relative comparison within the set.
 tion method based only on this experiment. Classification has a much better chance of achieving better performance with large-scale training data, especially when features especially when sufficient amounts of corpus data annotated in multiple classes are unavailable.

Section 3.1. Given a set of relatively difficult texts L maximum number of training pairs will amount to 2  X | L d | X | the amounts of | L d | and | L e | .Welet N = | L d | = | constructed by randomly sampling N documents from L d and L and then constructing V de and V ed for each pair.
 whose maximum number is 2  X | L d | X | L e | for a given N . When N = 600, this amounts to 720,000 training pairs, which is too large in terms of the time required for training.
Therefore, by fixing N = 100, we tested the learning effect for numbers of training pairs up to 2  X  100  X { 1, 5, 10, 50, 100 } .
 amount of data and the second graph shows the effect of combination. The horizontal axes represent the amount of training data (namely, 2 N for the first graph and 2 { 1, 5, 10, 50, 100 } for the second graph), and the vertical axes represent the correlation.
In the second graph, the horizontal axis is in log scale. Each graph has four lines, each corresponding to a subset of data from TD2. Every plot was obtained by averaging five repetitions of the random pairing of learning data.
 increase in performance, which is almost invisible. In the second graph, the increase in combination did not lead to higher performance, but rather to drastically decreased performance for the English data. This decrease was especially prominent with TD2-F-
E, the English textbook for Japanese students. This must have been due to the different natures of the training and test data. On the other hand, the texts of the Asahi newspaper articles and TD2-{ M,F } -J are controlled under a similar standard (in terms of vocabu-English.
 amounts of training data by combination, this would not lead to higher performance.
Moreover, the graphs suggest the importance of obtaining a sufficient amount of train-ing data from two levels which match the target domain. 9. Evaluation of the Terrace System 9.1 Evaluation of Searching
The search performance using the algorithm presented in Section 3.2 was evaluated via method was compared with that of the three methods presented at the beginning of
Section 8. Because the sorting performance already differed among the methods, we evaluated the performance on correctly sorted texts. One text was removed from the correctly sorted texts, its position was searched for with each method, and the difference 222 between the resulting and correct positions was measured. This procedure was repeated for all texts of TD2, and the average difference was obtained. The finest levels for TD2 were used as the correct annotation. The execution was again performed five times by changing the random pairing within the training data.
 data, and the vertical axis represents the average locational error of searching divided by the total number of levels of each data. Unlike the correlation figures seen so far, the smaller the value of each result, the better the precision of the search results. As was the case in Figure 7, the heights of bars are comparable within a data block, whereas bars across blocks are not comparable because of the different numbers of levels. performance in sorting had smaller errors. Overall, our method had the smallest errors among all methods.
 of Flesch X  X incaid or Dale X  X hall, it was presented to several language teachers who originated the Terrace project (as mentioned in Section 5). They reacted positively to
Terrace, even though the system does not show any absolute readability scores. There are two main reasons for the favorable response. First, as mentioned, because they need texts rather than scores, teachers liked the fact that Terrace returns texts directly without outputting the values. With previous systems, teachers themselves had to input eign language teachers, especially of languages other than English, scores are not well standardized, and teachers are often puzzled by some readability scores. The teaching levels do not necessarily correspond to standard school grades for natives. Therefore, they prefer that a system outputs texts directly as in Terrace, because this means they found that the graphical indicators are helpful for locating themselves among numerous collections. 9.2 Response Time
In Section 4, qualitative analysis showed that a disadvantage of our model is the greater computational complexity in readability assessment. This section reports on the response time when using Terrace.
 The time needed for feature vector construction is linear with respect to the text length. collection is then searched for. Because comparison is performed multiple times in a search, this factor dominates the response time. As mentioned in Section 3.2, this time constant.
 collection in English and TD2-M-E is a larger data set than TD-F-E. As noted in Section 5, this collection amounts to 14,877 texts taken from CNN (CNN 2008). The input texts were all texts of TD2-M-E. Because K = 2, which makes M = 2 width of comparison was 5), the total number of comparisons was about 70 for one search. The computational environment was as follows: CPU: Intel(R) Core(TM)2 Quad CPU Q9550 (2.83 GHz); main memory: 3.5 GB; OS: Debian GNU/Linux 4.0.
 the length of a text, the horizontal axis represents the text length and the vertical axis represents the response time. Each plot corresponds to one text in TD2-M-E. The longer corresponds to texts which were judged as more difficult and those judged as easier, respectively, at the first comparison within the binary search. Because the Terrace system stores its feature vectors in a MySQL database, when the number of non-zero features (which is how it is implemented in MySQL) becomes longer, and the overall procedure takes a longer time. In any case, for all texts the response time was less than a second.
Therefore, although our method does have a complexity disadvantage, the results of this evaluation suggest this is not a serious problem. 10. Conclusion
We have described a new model of readability assessment that uses sorting. Our 224 of the text. In our method, a comparator is constructed using an SVM, which judges which of two texts is more difficult. This comparator is then used to sort the texts of a given set using a binary insertion sort algorithm. Our method differs from traditional readability assessment methods, most of which are based on linear regression, and it also differs from recent methods of statistical readability assessment, which are based on multi-value classification. The advantage of our model is that it solves the problem of assembling training data annotated in multiple classes, because it only requires two classes of training data: easy and difficult. At the same time, our model has the disadvantages of the resulting norm being somewhat incomprehensible as a ranking and greater computational complexity compared to previous methods.
 of our method with those of other methods. Our method achieved a higher correlation with the gold standard, more than 0.8, in sorting text collections in both English and
Japanese; this was higher than both traditional methods and an SVR trained with two-class corpora. Another comparison with a state-of-the-art classification method confirmed the potential of our method.
 text of similar readability from a text collection when given an input text. We showed that Terrace can overcome the disadvantages of the proposed model, by introducing verifying that the response time is always less than a second.
 are investigating the effects of introducing other features. References 226
