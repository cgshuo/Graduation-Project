 User engagement evaluation task in social networks has re-cently attracted considerable attention due to its applica-tions in recommender systems. In this task, the posts con-taining users X  opinions about items, e.g., the tweets contain-ing the users X  ratings about movies in the IMDb website, are studied. In this paper, we try to make use of tweets from different web applications to improve the user engage-ment evaluation performance. To this aim, we propose an adaptive method based on multi-task learning. Since in this paper we study the problem of detecting tweets with posi-tive engagement which is a highly imbalanced classification problem, we modify the loss function of multi-task learning algorithms to cope with the imbalanced data. Our evalu-ations over a dataset including the tweets of four diverse and popular data sources, i.e., IMDb, YouTube, Goodreads, and Pandora, demonstrate the effectiveness of the proposed method. Our findings suggest that transferring knowledge between data sources can improve the user engagement eval-uation performance.
 H.2.8 [ Database Management ]: Data Mining; J.4 [ Com-puter Applications ]: Social and Behavioral Sciences User engagement; transfer learning; multi-task learning; ada-ptive model
Micro-blogging platforms such as Twitter have become tremendously popular in the past few years. These plat-forms let people express their opinions and thoughts as fast as possible and this fact makes them a rich source of infor-mation about people X  X  everyday lives. Due to the high speed of information diffusion, several web applications have been integrated with Twitter to let people share their opinions about items (e.g., movies in IMDb) [7, 11].

Tweets containing the opinions of users about items (or products) could be used to improve the performance of rec-c ommender systems. It is shown that the total number of users X  interactions on tweets can represent the satisfaction of users. In other words, the value of achieved engagements is correlated with the interest of users in the received mes-sages [7]. In addition, Uysal and Croft [10] have investigated an important usage of engagement evaluation, which is de-signing personalized content filters. It is notable that the problem of engagement prediction or online participation has been extensively studied in news websites, social net-works, and discussion forums. Petrovic et al. [8] have tried to predict the number of retweets or to detect the tweets that will be retweeted. Note that the aforementioned meth-ods have investigated tweets with arbitrary content, while we are interested in predicting the engagement of tweets with predefined content. This characteristic of tweets eliminates the influence of textual features in our task.

According to the importance of user engagement eval-uation and its influences on recommender systems, ACM Recommender Systems Challenge 2014 1 focused on rank-ing IMDb tweets of each user based on their engagements. Similar to this challenge, in this paper the  X  engagement  X  is computed as the total number of retweets and favorites that a tweet has gained.

In this paper, we try to answer two crucial research ques-tions in user engagement evaluation. (1) Do tweets of vari-ous web applications differ with each other in terms of being engaging? (2) Is there any commonality among tweets of different web applications, which could be used to improve engagement evaluation? And how to benefit from these com-monalities? To answer these questions, we focus on the prob-lem of detecting the tweets with positive engagement . We first create a dataset containing tweets about items of four popular web applications in completely different domains: IMDb (movies), YouTube (Video clips), Goodreads (books), and Pandora (musics). By analyzing these datasets as well as performing in-domain and cross-domain experiments, we investigate the first question. We further propose an adap-tive method based on multi-task learning, a transfer learn-ing technique, to share knowledge between these domains. Since the data is highly imbalanced, we modify the loss func-tion of multi-task learning algorithms by adding an instance weighting matrix to the loss function formulation. To the best of our knowledge, this is the first work that has focused on adaptive models for user engagement evaluation. The experiments demonstrate that the proposed method outper-forms the baselines and in most cases, the improvements are statistically significant. h ttp://2014.recsyschallenge.com/
In this section, we first briefly introduce the employed multi-task learning algorithms and describe how we deal with the imbalanced data in multi-task scenarios. Then, we introduce our features for user engagement evaluation.
The problem of dealing with different distributions be-tween training and test data has been extensively studied in the literature [2, 3, 4]. Various transfer learning approaches have been so far proposed to share knowledge between data with different distributions. In this paper, we use multi-task learning (MTL) [3], an inductive transfer learning technique whose goal is to improve the generalization performance by leveraging the domain-specific signals of related tasks.
We first introduce our notation. Let T be a set of t tasks and each task i has n i training instances { ( x i 1 , y i 1 y i ) } in which y ij  X  R and x ij  X  R the label and the features vector, where d is the number of features. The input features and labels can be stacked together as X i = { x i 1 ,  X   X   X  , x in i } and Y i = { y respectively. The weight of features are represented in ma-trix W  X  R d  X  t . We consider the logistic loss function which is widely used for linear classification. The logistic loss func-tion in multi-task learning algorithms is defined as follows:
L ( W, X, Y ) = where c i is the bias term for task i . Since the distribution of data in our problem is highly skewed, we assign higher weights to the instances from the minority class and vice versa. To this aim, for each task i we define an instance weighting matrix  X  i  X  R n i  X  1 whose elements correspond to the weight of task i training instances. The elements of  X  is computed as: where n ( j ) i denotes the number of training instances in task i with label y ij . A similar idea for coping with imbalanced data in single-task classification problems has been previ-ously proposed in [1]. In Equation (1), the weights of in-stances in each task are normalized to make the influence of all tasks equal in the total loss function. We redefine the logistic loss function for MTL as follows:
L ( W, X, Y ) =
There are a number of MTL methods with different learn-ing strategies and different assumptions. In this paper, we consider two linear MTL algorithms as follows:
MTL Trace [4]. This MTL method assumes that all tasks are related to each other and it tries to transfer knowl-edge between all tasks. The objective function in MTL Trace considers the trace-norm of matrix W for regularization.
MTL CASO [5]. This MTL method uses a convex relaxed alternative structured optimization (CASO) in its computations which decomposes the model of each task into two components: task-specific and task-shared feature map-pings. It has been shown that there is an equivalence rela-tionship between CASO and clustered MTL (CMTL) which
Items type movie video clip book music # of tweets 100,206 239,751 65,445 98,212 # of users 6,852 6,480 3,813 3,312 # of items 13,502 154,041 31,558 32,321
Avg eng. 0.1097 0.4737 0.1632 0.0778 % of tweets w ith eng &gt; 0 4.139 14.193 6.931 6.285 assumes that tasks have group structure and thus, all tasks i n each cluster are related to each other.
In this task, each tweet is tweeted by a user about an item (e.g., movie). For each tweet, we extract 23 features partitioned into three categories: user-based, item-based, and tweet-based. Since the contents of tweets are predefined by the web applications and users usually do not edit tweets contents, we do not consider textual features. More details about the exact definition of features can be found in [11].
User-based features. Number of followers, Number of followees, Number of tweets, Number of tweets about domain X  X  items, Number of liked tweets, Number of lists, Tweeting frequency, Attracting followers frequency, Follow-ing frequency, Like frequency, Followers/Followees, Followers-Followees.
 Item-based features. Number of tweets about the item.
Tweet-based features. Mention count, Number of hash-tags, Tweet age, Membership age at the tweeting time, Hour of tweet, Day of tweet, Time of tweet, Holidays or not, Same language or not, English or not.
In this paper, we investigate the tweets of four diverse and popular web applications (hereafter, called domains): IMDb, YouTube, Goodreads, and Pandora which contain movies, video clips, books, and musics, respectively. These applications let people express their opinions about items in Twitter. Tweets contents are predefined, but can be edited by users.

Recently, Dooms et al. [6] created a similar dataset by col-lecting recent tweets. Since their dataset is very sparse, we expand their approach by also gathering all related tweets of new users. Therefore, our created dataset is more realis-tic. Statistics of our created dataset are reported in Table 1. The dataset is freely available for research purposes. 3
To decrease the effect of having considerably different amo-unts of training data in different domains and to equally weight the contribution of each domain in the learning pro-cess, we train all models using the same number of instances from each domain. Hence, we select 65 , 445 instances (size
A few number of features employed in [11] which cannot be applied for all the domains are ignored. Note that the baseline results with and without using these features are approximately equal. Designing an adaptive model to han-dle different features in each domain will be considered in the future.
Available at http://ece.ut.ac.ir/node/100770 Table 2: F1-measure and BA achieved by in-domain and c ross-domain experiments.
 T rain
IMDb F1 B A 0.1206 0 .5948* 0.3696 0 .6679 0.2033 0 .5851 You-T ube
Good-re ads
Pan-d ora of the smallest domain) for each domain and cut it in half to c reate training and test sets. We repeat this process 30 times by shuffling the data of each domain randomly. Average of the results obtained on 30 shuffles are reported.
As expected and shown in Table 1, percentage of data with positive engagement is by far lower than percentage of those with zero engagement. This makes our data highly imbal-anced and thus, accuracy could not be a proper evaluation metric for this task. We consider two widely used evalua-tion metrics for classification in imbalanced environments: F1-measure and balanced accuracy (BA) [9]. 4
In all experiments, the instance weighting technique is applied for both baselines and the proposed method. 5 For hyper-parameter optimization in all methods, we perform stratified k-fold cross validation over training data and find the optimum parameters using grid search. Moreover, statis-tical t-test with 95% confidence is used to find the significant differences between results. The experiments are done using Scikit-learn 6 and MALSAR 7 packages.
In this section, we try to experimentally answer the two research questions mentioned in Section 1.
The statistics in Table 1 demonstrate several differences among different domains. According to this table, average engagement of tweets varies a lot between different domains. For instance, average engagement in YouTube tweets is six times higher than that in Pandora tweets. These differences might be related to different functionalities of these appli-cations and also their popularity. Similar differences can be captured by considering number of tweets with positive engagement in the domains.

In addition to the different nature of the domains ex-plained above, we design a set of experiments to investigate differences between the distributions of data in various do-mains. To this end, we train an SVM classifier with linear kernel on the training data of each domain and test it on the test data of all domains. The results in terms of F1-measure and BA are reported in Table 2. In each target domain (each column), the result which is significantly higher than its cor-responding values is marked by  X * X . Table 2 shows that in all target domains, BA obtained by in-domain (training and
B A is computed as the mean of accuracy in each class.
For the sake of space, the results without instance weight-ing which are far below the reported results are not reported. http://scikit-learn.org/ https://github.com/jiayuzhou/MALSAR test data from the same domain) experiment is significantly higher than all the cross-domain experiments (training and test data are from different domains). Except for IMDb, there is a similar pattern for F1-measure, i.e., in-domain re-sult is higher than cross-domain results in each test set. In IMDb, all the cross-domain experiments achieve higher F1-measure compared to the in-domain experiment, although the improvements are not significant. The reason could be that IMDb is the most sparse dataset among other domains (see Table 1) and because of this highly imbalanced situa-tion, the learning models cannot create a proper model using the IMDb training data and the learned model is biased to-wards the majority class. That is why F1-measure (which considers the precision and recall of the positive class) for cross-domain experiments are higher, but BA (which con-siders both classes equally) are not.
In the second set of experiments, to evaluate the perfor-mance of MTL methods, we compare them with two base-lines: (1) STL: a single-task classifier which is trained on in-domain data, and (2) STL-Pooling: a single-task classi-fier which is trained on the data of all domains. We employ SVM as a single-task classifier in our experiments, which has been shown to be highly effective in different tasks. Since the mentioned MTL methods are linear, we also consider linear kernel for SVM to have a fair comparison.

To evaluate the methods with different amounts of train-ing data, we consider 10 subsets of the created training set for each domain in each random shuffle with different sizes ranging from 10% to 100% of the instances. Figure 1 shows the learning curves on the four target domains in terms of F1-measure and BA.

According to Figure 1, the results of MTL methods in each domain are very close to each other. The reason is that all domains are related and the group structured assumption in MTL CASO does not affect the results, significantly. In the following, we analyze the results for each target domain separately and then, provide a general analysis.

IMDb. The results show that STL always performs bet-ter than STL-Pooling in terms of BA, but there is not a sim-ilar observation in terms of F1-measure; the reason could be that based on Table 1, IMDb domain is the most sparse do-main and with the limited amount of training data, STL can-not create a proper model for detecting positive instances. Therefore, pooling the domains can help to improve the re-sults in terms of F1-measure which combines the precision and recall of the positive class. Note that since the distribu-tions of data in different domains vary a lot, by increasing the number of instances the performance of STL is also in-creased, but the STL-Pooling X  X  performance is not. In other words, distribution of all the training data together differs with the distribution of the IMDb test data. The results also show that MTL methods outperform the baselines, in particular when the portion of training data is more than 50%. Statistical test shows that these improvements are statistically significant.
 YouTube. Since other domains are more sparse than YouTube domain, pooling the domains can be effective in detecting the tweets with zero engagement. That is why the BAs achieved by STL-Pooling in beginning of the learning curve are higher than those obtained by the other methods. As expected, STL-pooling could not perform well in terms of F1-measure; because, pooling with more sparse data re-duces the accuracy of SVM to classify positive instances. By increasing the amounts of data, MTL methods outperform both STL methods. In addition, since YouTube domain is more balanced than the other domains, the results obtained on YouTube are higher than the others.

Goodreads and Pandora. The results of these two do-mains show that MTL methods outperform the baselines. The BA improvements are always significant, but the F1-measure improvements usually are not. This shows that transferring the knowledge helps more to correctly classify the zero engagement instances in these two domains. Be-cause of the differences between training data in different domains, STL-Pooling could not perform well.

Considering the results of all domains, by increasing the number of training instances from 10% to 100%, the results of MTL methods vary from 4% to 6% in terms of F1-measure and from 3% to 5% in terms of BA. The learning curves in terms of BA become stable earlier than that of F1-measure. To wrap it up, when enough amount of training data is avail-able, STL in general performs better than STL-Pooling and MTL methods perform better than STL. Conversely, if lim-ited amount of training data is available, STL-Pooling may perform better than STL and again, MTL methods usually perform better than the baselines. Therefore, we can con-clude that multi-task learning can transfer knowledge be-tween these domains to improve the performance.
In this paper, we proposed an adaptive method based on multi-task learning technique for user engagement evalua-tion. To be able to cope with imbalanced data, we modified the logistic loss function in the mutli-task learning methods by adding an instance weighting matrix to its formulation. We considered four popular web applications in our experi-ments: IMDb, YouTube, GoodReads, and Pandora. The ex-perimental results showed that distributions of data in these domains are different and multi-task learning methods can transfer knowledge between the domains to improve user engagement evaluation performance. A direction for future work is to recognize which domains can affect a given target domain. Other user engagement evaluation problems, such as ranking the tweets based on their engagements, could also be studied in the future. [1] R. Akbani, S. Kwek, and N. Japkowicz. Applying [2] J. G. C. de Souza, H. Zamani, M. Negri, M. Turchi, [3] R. Caruana. Multitask learning. Machine Learning , [4] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse [5] J. Chen, L. Tang, J. Liu, and J. Ye. A convex [6] S. Dooms, T. De Pessemier, and L. Martens. Mining [7] D. Loiacono, A. Lommatzsch, and R. Turrin. An [8] S. Petrovic, M. Osborne, and V. Lavrenko. Rt to win! [9] D. Powers. Evaluation: From precision, recall and [10] I. Uysal and W. B. Croft. User oriented tweet ranking: [11] H. Zamani, A. Shakery, and P. Moradi. Regression
