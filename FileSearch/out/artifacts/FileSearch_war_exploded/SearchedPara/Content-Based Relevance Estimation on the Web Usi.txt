 I n adversarial and noisy search settings as the Web, the document-query surface level similarity can be a highly mis-leading relevance signal. Thus, devising content-based rele-vance estimation (ranking) approaches becomes highly chal-lenging. We address this challenge using two methods that utilize inter-document similarities in an initially retrieved list. The first removes documents from the list that ex-hibit high query similarity, but for which there is insuffi-cient additional support for relevance that is based on inter-document similarities. The method is based on a prob-abilistic model that decouples document-query similarities from relevance estimation. The second method re-ranks the list by  X  X ewarding X  documents that exhibit high similarity both to the query and to other documents in the list. Both methods incorporate, in addition, at the model level, query-independent document quality estimates. Extensive empir-ical evaluation demonstrates the merits of our methods.
I n many retrieval methods, the surface-level similarity of a document to a query is an important source of information in estimating the relevance of the document to the under-lying information need. Over the Web, this  X  X elevance indi-cator X  can be highly misleading due to the adversarial and noisy nature of the retrieval setting; e.g., search engine op-timization (SEO) efforts that employ content manipulation [6] can significantly bias relevance estimates that are based on surface level document-query similarities.

We present a novel probabilistic content-based relevance estimation model that accounts for the fact that document-query similarity can be an ineffective, and even (maliciously) misleading, indicator. The key idea is decoupling document-query similarities from relevance estimation; the latter is based on using inter-document-similarities in an initially r etrieved list. The method that we devise, based on this model, removes documents from the initial list if they ex-hibit high surface-level query similarity that is not indicative of the ability to satisfy the information need  X  i.e., there is lack of sufficient additional content-based relevance evidence induced from inter-document similarities. A second method that we present re-ranks the initial list based on this rel-evance evidence. Both methods incorporate, at the model level, query-independent document quality estimates.
The Web-retrieval performance of our methods is substan-tially better than that of using only document-query simi-larities. The performance of our re-ranking method also transcends that of state-of-the-art methods that utilize (i) term proximity information [10], (ii) hyperlinks-based infor-mation, and (iii) content-based (query-independent) docu-ment quality measures [1]. In addition, we demonstrate the merits of integrating our methods with spam detection [4].
It is important to note that the content-based relevance estimates that we focus on should be viewed as one type of  X  X eature X  among the many used by Web search methods. The importance of this  X  X eature X  is that it directly touches on the definition of relevance; i.e., whether the document content satisfies the information need.
O ur first method differs from classical retrieval paradigms by the virtue of decoupling document-query similarities from relevance estimation at the model level . This helps to ac-count for the potential misleading nature of these similari-ties which might be due to adversarial effects. This is also an important difference between our model and previous work on using inter-document similarities. (See [5, 7] for surveys). Another difference between our two proposed methods and a previous suite of methods that utilize inter-document sim-ilarities [7] is treating inter-document similarities and query-independent document-quality estimates, at the model level , as complementary types of information rather than as sub-stitutable [7]. Thus, our second (re-ranking) method essen-tially generalizes past approaches [7].
S uppose that some search algorithm is employed over a corpus of documents D in response to query q to satisfy the presumed information need I that q expresses. As a result, an initial ranking of D is induced. We use D [ k ] q t o denote the result list of the k most highly ranked documents.
We assume that the initial ranking is based on, among other criteria, the surface-level similarity between q and doc-u ments. Naturally, then, the result list, D [ k ] q documents not relevant to I that exhibit high surface-level similarity to q . In adversarial retrieval setting such as the Web, some of these documents might have been manipulated (e.g., keyword stuffed) to include substantial presence of q  X  X  terms. Others might simply bear very little textual content to begin with and/or provide only partial textual cover to I ; e.g., a document containing a table that is composed only of a few keywords, some of which are q  X  X  terms [1].
T he goal of our first approach is filtering out from D d ocuments that exhibit high surface-level similarity to q , but which do not pertain to the information need I .
Let S be the set of documents in the corpus that contain  X  X uspiciously X  high presence of q  X  X  terms; that is, presence that does not necessarily attest to the information needs that the document content can satisfy. (Refer to the key-word stuffing and table examples from above.) Let NR be the set of non-relevant documents in the corpus. Our goal is estimating for each document d in D [ k ] q t he probability that it does not satisfy I (i.e., is not relevant), and has a  X  X uspiciously X  high presence of q  X  X  terms:
More formally, let q and I denote random variables that take as values queries and information needs, respectively. Then, Equation 1 amounts to p ( d 2 S ,d 2 NR | q = q, I = I ) . Part (a) of Figure 1 presents the dependencies between random variables and events that we utilize. It is impor-tant to observe the following. Once q is set to the query q , S is uniquely determined; yet, S can correspond to various queries. Furthermore, by definition, the binary event d 2S is independent of I , as it depends on the connection between d and q and properties of d itself. Another observation is that once we fix I to the specific information need I , the set of non-relevant documents, NR , is uniquely determined. Moreover, this set does not depend on q as relevance is de-termined only with respect to the information need. Then, one of our tasks is to estimate the probability that the bi-nary event that represents non-relevance, d 2 NR , happens. As is common, to facilitate notation we omit random vari-ables from the formulation and use their values. Using the dependencies from part (a) of Figure 1 we get:
The separate treatment of (non-)relevance and surface-level document-query similarity at the model level , which is manifested in Equation 2, is a fundamental aspect by which our approach departs from classical retrieval paradigms. This separation helps target non-relevant documents that exhibit high surface-level query similarity. In an adversarial re-trieval setting, some of these documents, for example, might have been stuffed with q  X  X  terms. Classical retrieval ap-proaches, on the other hand, are often based, in spirit, on estimating p ( d 2 R | I ), the probability that d is in the set R of documents relevant to I . (Refer to part (b) of Figure 1 that we further discuss below.) Then, q is coupled with I , and p ( d 2 R | q ), which is often estimated based on the similarity between d and q , is used for ranking. The sec-ond aspect that differentiates our approach from classical retrieval methods is the actual estimates of (non-)relevance explored next. Figure 1: The dependencies used by the (a) Filter, and (b) ReRank methods.
T o derive a document scoring method from Equation 2, we first estimate p ( d 2S| q ). Since q is fixed, and consequently so is S , we get The probability p ( q | d 2 S ) can be interpreted as follows. Given document d , which is among those having a  X  X uspi-ciously X  high presence of terms from some query, what is the probability that the query at hand is q ? (Recall that S can correspond to different queries.) A natural estimate is based on the similarity between q and d (we use two different similarity measures in Section 4.1): Here and after,  X  p ( x ) denotes an estimate of p ( x ).
The next step is estimating p ( d 2 S ) in Equation 3 . As q is not conditioned upon, and S can correspond to various queries, p ( d 2 S ) can be thought of as the prior probabil-ity of d having  X  X uspiciously X  high presence of terms  X  i.e., terms with presence not necessarily reflecting the informa-tion needs that d  X  X  content can satisfy. The probability that d is spam can serve, for example, for an estimate, as spam documents, by definition, do not satisfy any information needs. Other document-quality measures [7, 1] can serve to the same end and be integrated as we show in Section 4.2. One such measure, which we found to be quite effective in our experiments, is the entropy of the term distribution of d [7]. The term distribution is expected to have relatively high peaks, rather than be relatively flat, in case a document is  X  X ow on content X , and has a non-representative occurrence of terms; in this case, the entropy is low. Formally, if p i s the probability assigned to term w by a language model induced from d (details in Section 4.1), then d  X  X  entropy is defined as H ( d ) d ef =  X  tion 2 that we have to estimate is p ( d 2 NR | I ), the proba-bility that d is not relevant to I . To that end, we leverage in-sights gained in work on re-ranking search results in  X  X lean X  settings (e.g., newswire collections) [5, 7]; specifically, that similarity to documents in a short list retrieved in response to a query can imply relevance [5, 7]. The reason, following the cluster hypothesis [12], is that similarity between rel-evant documents is potentially stronger than that between non-relevant documents, and that between relevant and non-relevant documents.

Accordingly, we assume that low similarity with docu-ments in D [ k ] q i ndicates non-relevance. However, in contrast t o work on utilizing inter-document-similarities in  X  X lean X  (e.g., newswire) settings [5, 7], which are free of content manipulation, and which often contain documents of high quality, here we compute inter-document similarities while disregarding query terms. The motivation is to account for the fact that query-terms occurrence is not necessarily in-dicative of the document content, a situation which is exac-erbated in adversarial (and noisy) settings such as the Web. (Empirical exploration  X  actual numbers are omitted due to space considerations  X  supported the merits of disregarding query terms.) Formally, let sim  X  q (  X  ,  X  ) be an inter-document similarity measure disregarding q  X  X  terms (details in Section 4.1), then
Using the estimates from above to instantiate Equation 2 yields our Filter method that scores d by 1
T he final corpus ranking is based on the initial ranking with the exception that the m documents d in D [ k ] q w ith the highest S F ilter ( d ; q ) are removed; m is a free parameter. A n alternative approach that we consider is re-ranking D q b ased on presumed relevance. Documents not in D [ k ] i .e., those initially positioned at ranks lower than k , main-tain their original ranks. Formally, for each d in D estimate p ( d 2 R | I ), the probability that d is relevant to the information need I ; R is the set of relevant documents in the corpus, which is uniquely determined once I is fixed. (Refer to part (b) of Figure 1.) To that end, we can use the
We use the document term distribution entropy for the estimate of the prior of d  X  X  relevance, that is,  X  p ( d 2 R ). As noted above, high entropy might imply to a breadth of content. The set R can contain documents that also satisfy information needs other than I and which are represented by queries different than q ; and, I itself can be represented by queries other than q . Thus, we use the following evidence combination for the estimate  X  p ( I | d 2 R ) of the probabil-ity that I is an information need satisfied by d . First, we measure the similarity between d and all other documents in D [ k ] q . These documents can be considered  X  X seudo rele-vant X  to I as they were retrieved in response to q which is a  X  X ignal X  about I . Second, we scale the similarities just men-tioned with the document-query surface level similarity [7]. This is intended to  X  X alance X  the effect that aspects not re-lated to I have on the inter-document-similarities measured. Accordingly, our ReRank method scores d by: S
A ll the estimates in Filter can be turned to probability distributions by normalization with respect to documents. Note that such normalization does not affect ranking.
I n what follows we use language models for several pur-poses. Let p D ir [  X  ] z (  X  ) denote the Dirichlet-smoothed unigram language model induced from text z , with smoothing param-eter  X  [13]; unless otherwise stated,  X  = 1000 [13]. Setting  X  = 0 results in a non-smoothed maximum likelihood esti-mate that is used for (i) the corpus unigram language model, (ii) computing document entropy, and (iii) measuring inter-document similarities and language-model-based document-query similarity, as described below.

To measure inter-document similarities when disregard-ing q  X  X  terms, we use the cross entropy (CE) measure [8, language models induced only after q  X  X  terms were removed from all documents in the corpus. The computational over-head incurred is not significant, as similarities are computed for at most a few hundreds top-retrieved documents.
To create an initial ranking (henceforth init. rank. ) of the corpus in response to query q , we employ two methods. The first, denoted LM , is using sim ( q,d ) d ef = dard KL retrieval approach [8]; this document-query simi-larity measure is used in our methods when employed over the LM initial ranking. The second method, denoted MRF , is the Markov Random Field approach [10] employed with the sequential dependence model (SDM), which is a state-of-the-art content-based retrieval approach [10, 1]. Our meth-ods use sim ( q,d ) d ef = p ( d ,q ), which is induced using SDM, when operating on the MRF initial ranking. The free pa-rameters of SDM,  X  T ,  X  O a nd  X  U a re set to 0 . 85, 0 . 1, and 0 . 05, respectively, as these values are known to yield effec-tive performance [10, 1]. For the experiments with MRF, the document language model smoothing parameter,  X  , was set to 2500 following previous recommendations [1]. For evaluation, we use the ClueWeb collection (category B) [3] that contains about 50 million documents. We use two query sets: 1-50 from TREC 2009 ( ClueWeb-09 ), and, 51-100 from TREC 2010 ( ClueWeb-10 ).

The Indri/Lemur toolkit (www.lemurproject.org) was used for experiments. Porter stemming was applied to both docu-ments and queries. Stopwords appearing in a short list com-posed of 35 words [9] were removed only from queries. We use MAP (@1000), precision of the top 5 documents (p@5), NDCG (@20), and ERR (expected reciprocal rank) (@20) [2] for evaluation. Statistically significant performance dif-ferences are determined using the two-tailed paired t-test at a 95% confidence level.

Our Filter and ReRank methods operate on D [ k ] q , the re-sult list of k initially highest ranked documents. We set k to a value in { 50 , 100 , 200 ,..., 1000 } ; recall that Filter removes m documents from D [ k ] q a nd uses the initial ranking for the residual corpus; m is set to values in { 1 , 5 , 10 , 15 , 20 , 25 } . The values of k and m , as those of the free parameters of the reference comparisons we use below, are set using 10-fold cross validation performed over queries; MAP serves for the optimization criterion in the training phase. M ain result. I n Table 1 we present our main result for ClueWeb. Recall that the MRF initial ranking was cre-ated by setting free parameters to some effective values. Hence, as a reference comparison we use MRF X  X  sequen-tial dependence model (SDM) for re-ranking the top 1000 documents retrieved by MRF and by LM, independently, where the free-parameter values are set using 10-fold cross validation; specifically,  X  T ,  X  O a nd  X  U a re set to values in { 0 , 0 . 05 , 0 . 1 ,..., 1 } where  X  T +  X  O +  X  U = 1. OptMRF de-notes the resultant retrieval model as free-parameter values are optimized (with respect to MAP) in the training phase.
We see in Table 1 that none of the LM and MRF ini-tial rankings posts performance that dominates that of the other across all relevant comparisons (query set  X  evaluation measure). Thus, we have initial rankings of different effec-tiveness with respect to the different evaluation measures. We also see that OptMRF outperforms in a vast majority of the relevant comparisons the LM and MRF initial rank-ings for both ClueWeb-09 and ClueWeb-10; for ClueWeb-09, many of the improvements are statistically significant.
We also see in Table 1 that Filter outperforms the ini-tial ranking for all relevant comparisons; statistically signif-icantly so in most cases. Our ReRank method is the best-performing in Table 1. It always outperforms the initial ranking, which is based only on document-query similarities, in a substantial and statistically significant manner; ReRank also outperforms OptMRF  X  often statistically significantly so and by quite a large margin; and, it outperforms Fil-ter in all relevant comparisons. Further exploration (actual numbers are omitted due to space considerations) reveals that of the three information types used by ReRank, inter-document-similarities is the most effective one.
 A pplying spam detection. W e now study the effectiveness of integrating our methods with spam detection. To that end, we first use as a reference comparison a common spam removal method denoted here SpamRm [4, 1]. Specifically, we apply Waterloo X  X  spam detector [4] upon the initial rank-ing, top to bottom, and remove documents d with a non-spam score, NonSpam ( d ), below 50 until we accumulate 1000 documents; NonSpam ( d ) is in [0 , 100] and reflects the presumed percentage of documents in the entire ClueWeb English collection (category A, which includes around 500 million documents) that are  X  X pammier X  than d .

To integrate spam detection in Filter, we scale document d  X  X  score by 1 N onSpam ( d )+1 , which amounts to using  X  p ( d 2 is, the prior probability that d has a  X  X uspiciously X  high presence of terms that do not reflect the information needs that can be satisfied by d is now estimated based on both its term distribution entropy and the probability that d is spam. We use Filter+SpRm to denote the resultant retrieval model. Analogously, the score assigned to d by ReRank is scaled by NonSpam ( d ) + 1, which amounts to using both the document entropy and the probability that it is not spam as a prior for relevance. The resultant retrieval model is denoted ReRank+SpRm . Both Filter+SpRm and ReRank+SpRm operate on D [ k ] q , as is the case for Fil-ter and ReRank.

We see in Table 2 that applying suspected-spam removal (SpamRm) upon the initial ranking improves performance (except for MAP for ClueWeb-09). This finding is in line with previous reports [1]. We also see that ReRank improves over SpamRm in a majority of the relevant comparisons; for MAP, the improvements are often statistically significant. In the few cases ReRank is outperformed by SpamRm, the differences are not statistically significant. Filter, on the other hand, is outperformed by SpamRm in many cases.
We also see in Table 2 that integrating spam detection in our methods helps to somewhat improve their perfor-mance in many cases. (Compare  X  X +SpRm X  with  X  X  X  rows.) Consequently, ReRank+SpRm is the most effective method in most relevant comparisons. Specifically, ReRank+SpRm outperforms SpamRm in a vast majority of the relevant com-parisons, with quite a few of the improvements being statis-tically significant. These findings attest to the merits of integrating our methods with spam detection.
 document entropy for a query-independent document qual-ity measure [7, 1]. The fraction of stopwords in the corpus that appear on the page, and the ratio of stopwords to non-stopwords on the page, were shown to be even more effective as document-quality measures for Web retrieval [1]; and, the most effective quality measures among those explored [1]. (Here, a term is considered a stopword if it is among the 100 most frequent alphanumeric terms in the corpus [1].)
We use ENT+SW to denote a retrieval method that uses, in addition to query-document similarity, the entropy and the two stopwords-based features just described as doc-ument quality measures. In the language modeling case (LM), we use the document-quality values for scaling the document-query similarity, thereby treating the product of these values as a document relevance prior. In the MRF case, we follow recent work [1]. That is, we interpolate the quality values (after normalizing each to a [0 , 1] range)  X  having a different weight parameter assigned to each  X  with the value assigned to the document by the SDM model. The range of values for each such parameter is { 0 , 0 . 05 ,..., 1 } with the constraint that the parameters X  values, and those of the three SDM parameters mentioned above (  X  T ,  X  O , and  X  ) , sum to 1. Using the same approach, we study a method that integrates a document X  X  PageRank score, which is a non-content-based quality measure, with the document-query similarity [1]. PageRank is computed upon the hy-perlink graph of the English ClueWeb category A. (In the tively. Boldface marks the best result in a column. MRF case, the interpolation parameter for the PageRank score is set to a value in { 0 . 05 , 0 . 1 ,..., 1 } .) The ENT+SW and PageRank methods re-rank the 1000 initially highest ranked documents as recently proposed [1].
 In Table 3 we compare the performance of PageRank, ENT+SW, ReRank, and, ReRank+SW  X  a variant of ReRank that uses both the entropy and the stopwords-based features as a relevance prior. (We use the product of the val-ues; see Equation 7.) We can see that using PageRank as a document-quality measure helps improve performance over that of the initial ranking in the MRF case, but not in the LM case; these findings echo those from previous reports [11, 1]. In both cases, however, the performance of using PageRank is substantially worse than that of ReRank.
Table 3 shows that ReRank outperforms ENT+SW. Both methods utilize document-query similarity and document entropy. ENT+SW, in addition, uses the stopwords-based document quality values, while ReRank utilizes inter-document similarities. Thus, we see that inter-document similarities can be a more effective source of information than the stopwords-based document-quality measures, which are the most ef-fective among those recently explored for Web search [1]. The fact that ReRank+SW improves over ReRank in many cases, except for MRF over ClueWeb-10, leads to the con-clusion that query-independent document quality measures, and inter-document similarities, can be complementary sources of information for relevance estimation.
W e presented two methods that address the challenge of content-based relevance estimation in adversarial (and noisy) retrieval setting as the Web, wherein document-query surface-level similarities can be a misleading relevance indi-cator. The methods utilize information induced from inter-document similarities in an initially retrieved list. Empirical evaluation demonstrated the merits of the methods.
 Acknowledgments We thank the reviewers for their com-ments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. Any opinions, findings and conclusions or recom-mendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
