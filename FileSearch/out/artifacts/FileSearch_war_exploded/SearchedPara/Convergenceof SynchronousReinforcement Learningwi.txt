 Artur Merk e artur.merke@udo.edu Ralf Schoknec ht ralf.schoknecht@ilkd.uni-karlsr uhe.de Reinforcemen t Learning (RL) is concerned with learn-ing optimal policies for the interaction of an agent with its environmen t. This problem is form ulated as a dy-namic optimisation problem and modelled as a Mark ov Decision Process (MDP). This MDP corresp onds to the following learning situation. An agent interacts with the environmen t by selecting an action a from the available  X nite action set A and receiving feedbac k about the resulting immediate reward r . As a conse-quence of the action the environmen t makes a transi-tion from a state s to a state s 0 . Accum ulated over time the obtained rewards yield an evaluation of every state concerning its long-term desirabilit y. The objec-tive is to  X nd an optimal policy that corresp onds to an optimal value function. One algorithm to compute such an optimal policy is policy iteration (Bertsek as &amp; Tsitsiklis, 1996). This algorithm consists of two steps that are executed in turn. The policy evaluation step determines the value function for a  X xed policy. From this value function an impro ved policy is deriv ed in the policy improvement step. In this paper we only consider policy evaluation.
 As long as a tabular represen tation for the value func-tion is used, RL algorithms like TD(  X  ) (Sutton, 1988) or the residual gradien t algorithm (Baird, 1995) are known to converge to the optimal solution of the policy evaluation task. For large problems, however, a tab-ular represen tation of the value function is no longer feasible with respect to time and memory considera-tions. Therefore, linear feature-based function appro x-imation is often used. Such function appro ximation makes the issue of convergence more complicated. On the one hand algorithms like TD(  X  ) may not converge at all (Baird, 1995). And on the other hand, even if they converge, di X eren t algorithms may converge to di X eren t solutions (Schoknec ht, 2003).
 For the TD(  X  ) algorithm with general linear function appro ximation convergence with probabilit y one can be proved under the crucial condition that the states are sampled according to the steady state distribution of the underlying Mark ov chain (Tsitsiklis &amp; Van Roy, 1997). This requiremen t may be disadv antageous for policy impro vemen t as shown in (Koller &amp; Parr, 2000) because it may lead to bad action choices in rarely visited parts of the state space. Moreo ver, in practical reinforcemen t learning it is desirable to take transi-tion data from arbitrary sources, e.g. from on-line be-haviour, archived data or from observing the system while under the control of some other policy. In this case a certain sampling distribution cannot be assured which may prevent convergence. Therefore, we need a convergence analysis for RL algorithms with linear function appro ximation when the transition data is ar-bitrary .
 In (Schoknec ht &amp; Merk e, 2003) a uni X ed framew ork for synchronous RL algorithms with linear function appro ximation was considered. It was shown that the update rules of synchronous RL algorithms can be written as inhomogeneous iterations with a special common structure. Moreo ver, su X cien t conditions of convergence for this class of iterations were deriv ed. Our main theorem in this paper extends this result in two importan t ways. First, we consider a more general iteration that contains the above iterations as a special case. And second, we give a necessary and su X cien t condition for the convergence of this general iteration. We apply our new theorem to prove the convergence of the synchronous residual gradien t algorithm for a  X xed set of arbitrary multiple transitions.
 In (Merk e &amp; Schoknec ht, 2002) the uniform RL al-gorithm in combination with interpolating grid based function appro ximators was introduced. In contrast to the synchronous TD(0) algorithm the uniform RL algorithm converges for arbitrary single transitions. However, it was an open question if the uniform RL algorithm also converges for arbitrary multiple tran-sitions. Using the new theorem we construct a coun-terexample with two transitions that violates the nec-essary condition of convergence. This shows that the uniform RL algorithm diverges in the general case. We consider the general inhomogeneous matrix itera-tion of the form were A 2 K n  X  n , x ( k ), b 2 K n and K 2f R ; C g denotes the  X eld of real or complex numbers respectiv ely. The iteration (1) can equiv alently be written as with the a X ne mapping  X  A;b ( x ) := Ax + b . The k -th elemen t in the sequence is then given by x ( k ) =
A;b ( x (0)). There are four possible behaviours for the equiv alent iterations (1) and (2) Convergence
Conv ( A;b ) : ()8 x 2 K n : lim Conditional convergence (the limit depends on the start value x ) CondConv ( A;b ) : ()8 x 2 K n : lim Boundedness (also known as Lagrange stabilit y) Bounded ( A;b ) : ()8 x 2 K n : lim sup Divergence Diverge ( A;b ) : ()9 x 2 K n : lim sup Obviously , the following relations between the di X eren t prop erties of the iterations hold: Conv ( A;b ) ) CondConv ( A;b ) ) Bounded ( A;b ) and Bounded ( A;b ) , : Diverge ( A;b ). The objectiv e is to state necessary and su X cien t conditions for the above prop erties that depend on A and b . For prop erty Conv ( A;b ) there exist standard numerical results which provide such necessary and su X cien t conditions (Green baum, 1997). For the special case b = 0 conditions for the prop erties CondConv ( A; 0) and Bounded ( A; 0) are given in (Ludyk, 1985) (see prop osition 1 below). However, there exist no nec-essary and su X cien t conditions for the prop erties CondConv ( A;b ) and Bounded ( A;b ) with an arbitrary translation vector b . In this paper we state such conditions and prove that they are both necessary and su X cien t. This result is especially importan t for appro ximate reinforcemen t learning (RL) where questions about convergence or boundedness of RL algorithms in conjunction with linear function appro ximators naturally arise (Bertsek as &amp; Tsitsiklis, 1996; Schoknec ht &amp; Merk e, 2003).
 Our analysis will rely on spectral prop erties of the matrix A . In the following  X  ( A ) denotes the spec-trum of A , i.e. the set of its eigen values. The eigen-value with maximal absolute value de X nes the spec-tral radius  X  ( A ) := max fj  X  j j  X  2  X  ( A ) g of A . The eigenspace of A corresp onding to the eigen value  X  is denoted by E A  X  = f x j Ax =  X  X  g , and the generalised eigenspace by H A  X  = f x j9 k : (  X  X   X  A ) k x = 0 g . From the de X nitions it follows that E A  X   X H A  X  , but in general  X  6 = H A  X  . The case E A  X  = H A  X  means that if a vector is annihilated by (  X  X   X  A ) k for some k , then it is already annihilated for k = 1 (cf. (Goh berg et al., 1986)). In the following we de X ne two prop erties of a matrix A with eigen values  X  2  X  ( A ) that will be useful to state our results: The  X rst condition ensures that 1 is the only eigen-value with modulus one. The second condition ensures that eigenspaces of eigen values with modulus one are identical to the corresp onding generalised eigenspaces. The following prop osition reviews the results for a ho-mogeneous iteration Prop osition 1 For the homo geneous iteration with A 2 K n  X  n and x ( k ) 2 K n the following holds CondConv ( A; 0) ()  X  ( A )  X  1 ^ CondE ( A ) ^ Cond1 ( A ) Bounded ( A; 0) ()  X  ( A )  X  1 ^ CondE ( A ) : Proof: The proof uses the Jordan canonic al form of the matrix A and can be adapte d from (Ludyk, 1985; Elaydi, 1999). } Thus, in order to obtain conditional convergence from boundedness the only eigen value of the iteration ma-trix with modulus one may be  X  = 1. In the follow-ing we presen t our main theorem. It completely clar-i X es the behaviour of the inhomogeneous iteration (1) and states the most general conditions for its conver-gence or boundedness. The idea behind the proof is to reduce the inhomogeneous case (1) via homogenisa-tion to the homogeneous iteration ~ x ( k + 1) = ~ A ~ x ( k ). Then, we will use prop osition 1 to obtain conditions for the di X eren t behaviours of the iteration. How-ever, CondE ( A ) , CondE ( ~ A ) generally does not hold. Therefore, we will need an additional condition b 2 Im f I  X  A g , where Im f A g = f Ax j x 2 K n g denotes the range of the linear mapping A .
 Theorem 1 Consider the inhomo geneous matrix iter-ation (1) . Then the following equivalenc es hold Case 1: 1 = 2  X  ( A ) Case 2: 1 2  X  ( A ) Proof: Equivalenc e (3) is a standar d numeric al result (see (Greenbaum, 1997)), and was include d for com-pleteness. The iteration (1) can also be written in the homo geneous form Let p (  X  ) and ~ p (  X  ) be the the characteristic polynomials of A and ~ A respectively. Then, ~ p (  X  ) = p (  X  )(1  X   X  ) holds and we obtain Moreover, due to the block structur e of ~ A the vector ( x 1 ;:::;x n ) &gt; is a (gener alised) eigenve ctor of A corre-sponding to eigenvalue  X  if and only if ( x 1 ;:::;x n ; 0) &gt; is a (gener alised) eigenve ctor of ~ A corresponding to the same eigenvalue  X  . Thus, As iteration (8) is just the equivalent homo genise d form of iteration (1) the relations CondConv ( A;b ) , CondConv ( ~ A; 0) and Bounded ( A;b ) , Bounded ( ~ A; 0) hold.
 Let us  X rst consider the case 1 = 2  X  ( A ) . Together with (9)  X  = 1 is a simple eigenvalue of ~ A . Ther e-fore, the corresponding eigensp ace must at least be one-dimensional. On the other hand, the gener alised eigensp ace can be at most one-dimensional. Thus E 1 = H CondE ( ~ A ) . From (9) we directly obtain  X  ( A )  X  1 ,  X  ( ~ A )  X  1 . Thus, we have  X  ( A )  X  1 ^ CondE ( A ) ()  X  ( ~
A )  X  1 ^ CondE ( ~ A ) . Proposition 1 states that this equivalent to Bounded ( ~ A; 0) which is equivalent to Bounded ( A;b ) . This yields (5) . Due to (9) it holds that Cond1 ( A ) , Cond1 ( ~ A ) . Together with (5) this gives (4) .
 We now consider the more interesting case 1 2  X  ( A ) . Assume that CondE ( ~ A ) holds. With (9) we directly ob-tain E A 1 = H A 1 . And with (10) this yields CondE ( A ) . Moreover, the new eigenvalue 1 of ~ A yields a new eigenve ctor ~ x = ( x;x n +1 ) &gt; with x n +1 6 = 0 . Without loss of gener ality assume that x n +1 = 1 . It holds that Ther efore, CondE ( ~ A ) ) CondE ( A ) ^ b 2 Im f I  X  A g . On the other hand let CondE ( A ) ^ b 2 Im f I  X  A g be valid. According to (11) the latter condition implies that ~ A has an eigenve ctor of the form ~ x = ( x;x n +1 with x n +1 6 = 0 . Thus, dim E ~ A 1 = dim E A 1 + 1 . From CondE ( A ) it follows that E A 1 = H A 1 . And due to E this yields CondE ( ~ A ) ( CondE ( A ) ^ b 2 Im f I  X  A g . Henc e we have shown the equivalenc e CondE ( ~ A ) , CondE ( A ) ^ b 2 Im f I  X  A g . Together with proposi-tion 1 this yields (7) . With the same argument as above equivalenc e (6) follows from identity (9) and (7) . To get a better intuition for the conditions in theo-rem 1 we consider a simple example. The translation x ( k + 1) = Ix ( k ) + b for b 6 = 0 clearly diverges be-cause of x ( k ) = kb . This can also be seen by observing that  X  ( I ) = f 1 g and b = 2 Im f I  X  I g = f 0 g which is a violation of equiv alence (7) in theorem 1. The framew ork for synchronous RL algorithms in the policy evaluation case was set up in (Schoknec ht &amp; Merk e, 2003). For a Mark ov decision process (MDP) with n states S = f s 1 ;:::;s n g , action space A , state transition probabilities p ( s i j a ; s j ) and reward function r : ( S;A ) ! R policy evaluation is concerned with solving the Bellman equation for a  X xed policy  X  : S ! A . V  X  i denotes the value discoun t factor.
 In practice due to the large number of states, the value function (vector) V is appro ximated by a lin-ear combination of basis functions f  X  1 ;:::  X  F g which can be written in matrix form as V =  X  w with  X  = [ X  1 j ::: j  X  F ] 2 R n  X  F . We can also write  X  &gt; = ture vector of state s i (see also (Bertsek as &amp; Tsitsiklis, 1996)).
 In (Schoknec ht &amp; Merk e, 2003) it was shown that for one transition x ! z with reward  X  the TD(0) algo-rithm for the solution of (12) can be written as with c = ' ( x ) and d =  X ' ( z )  X  ' ( x ).
 This can be generalised for a set T = f ( x i ;z i ; X  i ) j i = 1 ;:::;m g of m transitions where in the case of the synchronous TD(0) algorithm r = (  X  1 ;:::; X  m ) &gt; , C = [ ' ( x 1 ) j ::: j ' ( x [  X ' ( z 1 )  X  ' ( x 1 ) j ::: j  X ' ( z m )  X  ' ( x m )]. 3.1. Residual Gradien t Algorithm As shown in (Baird, 1995) the synchronous TD(0) algorithm can diverge. And therefore, the residual gradien t (RG) algorithm was prop osed as a conver-gent alternativ e. According to (Schoknec ht &amp; Merk e, 2003) this algorithm can be represen ted with appro-priately chosen matrices C and D , where C =  X  D . The su X cien t condition of convergence in (Schoknec ht &amp; Merk e, 2003) was tailored to iterations of the form (14). As (14) is a special case of the more general iter-ation (1) the new theorem 1 can also be used to obtain a concise convergence proof for the RG algorithm. Corollary 1 For an arbitr ary matrix C 2 K n  X  m and w ( k ) 2 R n , r 2 R m there exist a range of positive  X  's such that the iteration conver ges for every initial value w (0) (the limit de-pends on w (0) if CC &gt; is singular).
 Proof: We write A  X  := I  X   X CC &gt; . The matrix CC &gt; is positive semide X nite having real and nonne gative eigenvalues. If CC &gt; is nonsingular then there obvi-ously exists a positive  X  such that  X  ( A  X  ) &lt; 1 and we are done. In the singular case again because of the nonne gativeness of the eigenvalues we can choose an  X  such that the conditions  X  ( A  X  )  X  1 and Cond1 ( A  X  ) hold. As A  X  is symmetric and therefore diagonalisable, it follows that E A  X  = H A  X  , which establishes property CondE ( A  X  ) . It remains to show that Cr 2 Im f I  X  A  X  or equivalently that Cr = CC &gt; x for some x 2 R n . Us-ing the pseudo inverse ( C &gt; ) y = C of C &gt; (cf. (Bj X orck, ful X ls the above requirement. } 3.2. Uniform RL Algorithm In (Schoknec ht &amp; Merk e, 2003) it was shown that a whole class of synchronous RL algorithms is gener-ated by replacing C in (14) with some other matrix which depends on the x i and z i . This class contains the Kaczmarcz, RG, nearest neigh bour and the uni-form RL algorithm. In (Merk e &amp; Schoknec ht, 2002) it was shown that only the RG algorithm and the uni-form RL algorithm converge for single transitions. It has been an open question if the uniform RL al-gorithm would also converge in the case of multiple synchronous transitions. In this section we will use theorem 1 to generate coun terexamples which violate a necessary condition of convergence. This shows that the uniform RL algorithm generally diverges for more than one transition.
 The uniform RL algorithm is applicable for linear grid based function appro ximators. This kind of function appro ximator assumes a d -dimensional grid and a tri-angulation into simplices given. In  X gure 1 we see an example of a two-dimensional grid with a Kuhn tri-angulation. The represen table functions are linear on the simplices and continuous on the simplex bound-aries. They can be describ ed as a linear combination of generalised hat functions.
 In the example in  X gure 1 the supp ort of a two-dimensional hat function is shaded. The black dot is the top of the hat function, where it attains the value 1. At the boundaries of the supp ort the hat function vanishes, and on each simplex it is linear. These conditions uniquely de X ne the hat function cen-tred on the black dot. The set of all hat functions f ' 1 ;:::;' F g corresp onding to the F grid nodes is a basis of the space of represen table functions, i.e. equiv alent to the value of the i -th hat function at point x , determines the weight that is given to grid node i . The vector ' ( x ) = ( ' 1 ( x ) ;:::;' F ) &gt; contains the barycen tric coordinates of x . It satis X es 0  X  ' i ( x )  X  1 and P F i =1 ' i ( x ) = 1.
 If we now consider iteration (13) then we see that the magnitude of the change of the comp onen ts i of w ( k ) depends on the feature ' i ( x ). If x is near the centre of the i -th hat function, then the change will be greater than if x is near the boundary of the supp ort of the hat function. For x outside the supp ort of the i -th hat function ' i ( x ) will be zero, which re X  X cts the local character of grid appro ximators. The idea behind the uniform update rule is to replace c = ' ( x ) in (13) by a vector which equally weights all hat functions which have x in its supp ort. Formally we de X ne where p is the number of nonzero entries in ' ( x ). Be-cause of ' i ( x ) 6 = 0 for at least one i this is well de X ned. For a single transition ( x;z; X  ) we now obtain the uni-form RL algorithm w ( k + 1) = ( I +  X  X  ( x )(  X ' ( z )  X  ' ( x )) &gt; ) w ( k ) by replacing c = ' ( x ) in (13) with c =  X  ( x ). It was shown in (Merk e &amp; Schoknec ht, 2002) that the iter-ation (17) converges for suitable choices of  X  . As we will show in the following, this is no longer true in the case of multiple transition updates. Our searc h for a divergen t coun terexample with two transitions will be guided by theorem 1. The objectiv e is to  X nd two transitions such that a necessary condition of conver-gence is violated. In this case the iteration diverges. For the sake of simplicit y we consider the case of a one-dimensional simplex with two transitions as de-picted in  X gure 2 The grid consists of just one simplex, namely the interval [0 ; 1]. For each x 2 [0 ; 1] the fea-ture vector contains the barycen tric coordinates of x with respect the boundary points 0 and 1, and there-fore ' ( x ) = (1  X  x;x ) &gt; .
 The iteration (17) generalises in the same way to the case of arbitrary multiple transitions as iteration (14) was deriv ed from (13). Thus, a two transitions iter-ations for the uniform RL algorithm can be written as with and According to iteration (18) the matrix A and the vec-tor b in theorem 1 are given by A = I +  X CD &gt; and b = Cr . Let us assume that x 1 ;x 2 ;z 1 ;z 2 2 [0 ; 1] as depicted in  X gure 2. In the following we con-struct a coun terexample that violates the necessary condition of convergence b 2 Im f I  X  A g . We set x 1 = z 1 = 0, x 2 =  X  X  2 and z 2 2 (0 ; 1). Then we have because only one entry of ' ( x 1 ) is nonzero. With and Thus, Im f I  X  A g = Im f CD &gt; g = [(3 ; 1) &gt; ], where [ v ] denotes the linear subspace spanned by the vec-tor v . We must now choose r = (  X  1 ; X  2 ) such that lent to  X  1 + 1 2  X  2 6 = 3 2  X  2 which is equiv alent to  X  Thus for an arbitrary choice of  X  1 and  X  2 with  X  1 6 =  X  condition b 2 Im f I  X  A g is violated and the iteration (18) diverges for all initial values w (0). We have proved necessary and su X cien t conditions of convergence for a general matrix iteration. The update rules in a uni X ed framew ork for synchronous reinforce-ment learning (RL) algorithms with linear function ap-proximation can be seen as a special case of this itera-tion. Therefore, our results can be used to prove either convergence or divergence of the di X eren t algorithms in this framew ork. We have applied the new theo-rem to prove convergence of the synchronous residual gradien t algorithm (Baird, 1995). Moreo ver, we have addressed the unresolv ed problem if the uniform RL algorithm (Merk e &amp; Schoknec ht, 2002) converges for multiple arbitrary transitions. Our theorem allows to construct a coun terexample for which this algorithm diverges. Therefore, the residual gradien t algorithm remains the only RL algorithm for which convergence has been shown in the case of multiple transitions and linear function appro ximation.
 Baird, L. C. (1995). Residual algorithms: Reinforce-ment learning with function appro ximation. ICML . Bertsek as, D. P., &amp; Tsitsiklis, J. N. (1996). Neur o-dynamic programming . Athena Scien ti X c.
 Bj X orck,  X  A. (1996). Numeric al metho ds for least squar es problems . SIAM.
 Elaydi, S. N. (1999). An introduction to di X er ence equations . Springer. 2 edition.
 Gohberg, I., Lancaster, P., &amp; Rodman, L. (1986). In-variant subsp aces of matric es with applic ations . A Wiley-In terscience publication. Wiley .
 Green baum, A. (1997). Iterative metho ds for solving linear systems , vol. 17 of Frontiers in Applie d Math-ematics . SIAM.
 Koller, D., &amp; Parr, R. (2000). Policy iteration for factored mdps. UAI .
 Ludyk, G. (1985). Stability of time-variant discr ete-time systems . View eg.
 Merk e, A., &amp; Schoknec ht, R. (2002). A necessary con-dition of convergence for reinforcemen t learning with function appro ximation. ICML (pp. 411{418).
 Schoknec ht, R. (2003). Optimalit y of reinforcemen t learning algorithms with linear function appro xima-tion. NIPS 15 (pp. 1555{1562). MIT Press.
 Schoknec ht, R., &amp; Merk e, A. (2003). Convergen t com-binations of reinforcemen t learning with linear func-tion appro ximation. NIPS 15 (pp. 1579{1586). MIT Press.
 Sutton, R. S. (1988). Learning to predict by the meth-ods of temp oral di X erences. Machine Learning , 3 . Tsitsiklis, J. N., &amp; Van Roy, B. (1997). An analysis of temp oral-di X erence learning with function appro xi-
