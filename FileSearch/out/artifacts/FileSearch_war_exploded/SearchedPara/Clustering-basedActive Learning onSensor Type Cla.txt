 Commercial and industrial buildings account for a consid-erable portion of all energy consumed in the U.S., and thus reducing this energy consumption is a national grand chal-lenge. Based on the large deployment of sensors in modern commercial buildings, many organizations are applying data analytic solutions to the thousands of sensing and control points to detect wasteful and incorrect operations for energy savings. Scaling this approach is challenging, however, be-cause the metadata about these sensing and control points is inconsistent between buildings, or even missing altogether. Moreover, normalizing the metadata requires signi cant in-tegration effort.

In this work, we demonstrate a rst step towards an auto-matic metadata normalization solution that requires mini-mal human intervention. We propose a clustering-based ac-tive learning algorithm to differentiate sensors in buildings by type, e.g., temperature v.s. humidity. Our algorithm exploits data clustering structure and propagates labels to their nearby unlabeled neighbors to accelerate the learning process. We perform a comprehensive study on metadata collected from over 20 different sensor types and 2,500 sen-sor streams in three commercial buildings. Our approach is able to achieve more than 92% accuracy for type classi -cation with much less labeled examples than baselines. As a proof-of-concept, we also demonstrate a typical analytic application enabled by the normalized metadata.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Performance, Experimentation Active learning, Clustering, Metadata normalization c  X 
According to recent reports from U.S. Department of En-ergy [27, 17], commercial and industrial buildings in the U.S. account for almost 20 percent of the country's total energy use and a good 30 percent of that energy is used\inefficiently or unnecessarily." Reducing this energy usage is a national grand challenge: in 2011, the U.S. government launched the Better Buildings Challenge to make these buildings at least 20 percent more efficient by 2020 [10]. To achieve this goal, many organizations are applying data analytics to the thou-sands of sensing and control points in a typical commercial building to detect wasteful and incorrect operations.
The analytic-based approach is highly effective but very difficult to scale because the metadata about the type, lo-cation, and relationships between the sensing and control units are inconsistent between buildings, or even missing altogether. Generally, the sensing and control points in a building are intended to be used for manual inspection or by handcrafted control loops in a building automation sys-tem (BAS); they are not designed for automatic extraction and consumption by external software. As a result, map-ping the sensor streams to the inputs of a data analytic engine requires signi cant integration effort and anecdotally takes about a week or longer for each commercial building. For large organizations that are applying this approach to hundreds or more buildings, such as Microsoft's 88 Acres project [28], this process can take years. Even if this highly manual process is performed once, the need for additional mapping is not necessarily eliminated. New types of meta-data will be required as the building is modi ed or reno-vated, as the equipment is upgraded, or as new conditions and algorithms are added to the analytics engine. T able 1: Example point names for temperature sensors from three different buildings.

In modern commercial buildings, a sensing or control\point" is a sensor measurement, a controller, or a software value, e.g., a temperature sensor installed in an office room. The metadata about the point indicates the physical location, the type of sensor or controller, how the sensor or con-tro ller relates to the mechanical systems, and other impor-tant contextual information. Most of the time, the meta-data is encoded as a \point name", which is usually a short text string with several concatenated abbreviations. Ta-ble 1 lists a few point names of sensors in three differ-ent building management systems contracted by Trane 1 , Siemens 2 and Barrington Controls 3 . For example, the point name SDH_SF1_R282_RMT is constructed as a concatenation of the name of the building ( SDH ), the supply fan unit iden-ti er ( SF1 ), the room number ( R282 ) and the sensor type ( RMT , room temperature). As the name indicates, this sensor stream measures the temperature in a speci c room; and it also reveals the control unit that can affect the temperature in this room. Unfortunately, different naming conventions are used in most buildings due to different equipment, ven-dors, manufacturers, and contractors being used. As shown in the table, the notion of room temperature is encoded with different abbreviations in these three buildings: Temp , RMT and ART . Similar variation exists between buildings on a sin-gle commercial campus and even between buildings with the same contractor.

We envision a system that will allow an advanced analytic engine to quickly connect to and analyze the data from a commercial building. It would extract or infer metadata val-ues about sensing and control points to a normalized meta-data standard. Such a tool would not only save time but also allow building managers to experiment with many different kinds of analytic engines. Nevertheless, there is limited work on this topic. Bhattarcharya et al. [4] exploit a programming language based solution, where they derive a set of regular expressions from a handful of labeled examples to normalize the point name of sensors. This approach assumes a consis-tent format for all point names and one single pattern for each type, which is not the case in practice, as shown in Table 1. Schumann et al. [22] develop a probabilistic frame-work to classify sensor types based on the similarity of a raw point name to the entries in a manually constructed dictio-nary. However, the performance of this method is limited by the coverage and diversity of entries in the dictionary, and the dictionary size becomes intractable when there exist a lot of variations of the same type, or con icting de nitions of a dictionary entry in different buildings.

In this work, we demonstrate a rst step towards an au-tomatic metadata normalization solution that requires min-imal human intervention. We focus on a key category of metadata: the type of sensor associated with a sensing point. In contrast to the aforementioned methods, where the an-notation process is isolated from model training, we develop a novel active learning based solution to integrate these two processes so as to minimize the manual labeling effort throughout the metadata normalization process. Beyond the traditional active learning solutions [24, 6, 5], we further accelerate the learning process by exploiting the clustering structure of point names. We should note that although the naming schemata vary signi cantly across buildings, there will be de nite variants within the same building, given there are only nite types of sensors deployed in a build-ing. As a result, during active learning, unlabeled examples can be clustered; those in the same cluster are more likely ht tp://www.trane.com/ http://www.siemens.com/
The company is no longer in business. to share the same label and hence do not need to be queried repeatedly. Moreover, the acquired labels can be further propagated to the unlabeled neighbors in the same cluster to expedite classi er training. In our solution, the examples selected for labeling are chosen based on both their repre-sentativeness in the cluster and the informativeness of the cluster itself. A Gaussian Mixture Model with Dirichlet Pro-cess Prior [20] is used to cluster the instances on the y to accommodate the dynamic nature of active learning. Label propagation is performed with respect to the connectivity of labeled examples' neighborhood in an adaptive manner.
To investigate the effectiveness of the proposed solution for sensor type classi cation, we performed extensive ex-perimental comparisons against the state-of-the-art active learning algorithms on a large collection of real sensor stream data, which includes over 20 different sensor types and 2,500 sensors in three different commercial buildings. Our method achieved increased classi cation performance with reduced amount of manual labels.

Our main contributions in this paper can be summarized as follows:
To the best of our knowledge, there is few existing work addressing the sensor metadata normalization problem stud-ied in this paper. Besides [4, 22] discussed before, there are two major bodies of related work to ours, i.e., active learning in machine learning and schema matching in database.
The main idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the data from which it learns [24]. Therefore, the key research ques-tion in active learning is evaluating the informativeness of unlabeled instances for querying. Various solutions have been proposed from different perspectives, including uncer-tainty reduction [6], query by committee [12], and expected error reduction [2]. In particular, Nguyen and Smeulders also incorporated clustering idea into active learning to se-lect the most informative examples [16]. Their proposed query strategy gives priority to instances that are close to both classi cation boundary and cluster centroid. Dasgupta and Hsu utilized a hierarchical clustering structure to allevi-ate sampling bias and improve learning efficiency [7]. They present an algorithm that is statistically consistent and guar-antees to have better label complexity than supervised learn-ing. However, in those two methods clustering is performed in an ad-hoc manner, e.g., with prede ned cluster size; and neither of them considers the label proximity between adja-ce nt examples or propagates labels to reduce the amount of labels required for model training.

Automatic schema matching [19] is a classical problem in the database community where correspondences between el-ements of two schemata are identi ed as part of the data integration process. Many techniques have been proposed to achieve a partial automation of the match operation for speci c application domains. Doan et al. ask a user to pro-vide semantic mappings for a small set of data sources and then train a set of learners with existing machine learning approaches to nd the mappings for new data sources [9]. Dhamankar et al. extend [9] to a semi-supervised setting, where domain-speci c knowledge is introduced for complex expressions learning [8]. Madhavan et al. exploit a large col-lection of schemata with known mappings to learn a prior distribution of the elements and their properties [14]. The learned prior distribution is then used as constraints to help a suite of base learners to complete the matching. Though adapting learning techniques, these works mostly focus on offline supervised settings and do not emphasize the effi-ciency of learning methods, i.e., to reduce manual efforts throughout the learning procedure.
In this work, we develop a novel active learning based solution to perform automated sensor type classi cation in commercial buildings. Because in practice one can get direct access to all point names in a target building, our approach adopts the pool-based sampling setting, which selects the most informative instances from the entire collection of unla-beled data points. The data clustering structure is exploited to aid manual labeling of instances and to avoid repeated se-lection of similar instances. In addition, the acquired labels are propagated to their unlabeled neighbors to accelerate the training process. To accommodate the dynamic nature of active learning, we appeal to a non-parametric Bayesian approach to identify the clustering structure, and perform label propagation in an adaptive manner.
Formally, the problem of using active learning for auto-mated sensor type classi cation can be described as fol-lows. For a given collection of unlabeled point names D = f x 1 ; x 2 ; : : : ; x n g , in which the text string of point name x is represented as a d dimensional feature vector, we aim at learning a classi er f : f ( x ) ! y , with respect to a set of labeled instances D l = f ( x 1 ; y 1 ) ; ( x 2 ; y 2 ) ; : : : ; ( x quired during classi er training. In particular, y i is the true sensor type for the point name x i and takes value from a set of prede ned labels. Our goal of learning is to maximize f ( x )'s classi cation accuracy on the future testing data while minimizing the size of D l . We should note that our proposed solution has no assumption about the classi er f ( x ); any su-pervised multi-class classi er can be used in our method.
Conventional active learning methods mostly focus on effi-cient search through the hypothesis space. Each time a new label is added to D l , the set of hypothesis space shrinks, e.g., lters the classi ers that are inconsistent with the la-bels seen so far. Great research attention has been devoted onto designing effective query strategies for label acquisi-tion. Typical solutions include uncertainty-based sampling [6], query by committee [12], and expected model change [2].
However, such solutions implicitly assume unlabeled in-stances are independent, and thus fail to exploit any infor-mation conveyed in the density of data, i.e., the marginal dis-tribution of p ( x ) is assumed to be uniform. However, if the unlabeled instances in D form clusters, i.e., their clustering membership is consistent with the underlying class labels, one will only need one label from each cluster to estimate a perfect classi er. Although this example is overly optimistic, the clustering structure can still unveil the representative-ness of instances with respect to their neighborhood. Hence special emphasis should be given to those most represen-tative instances while labeling them. In addition, because instances in the same cluster are more likely to share the same label [31], one can accelerate active learning by avoid-ing labeling instances from the same cluster and propagating labels to the nearby unlabeled instances.

In our solution, the density of unlabeled instances, i.e., p ( x ), is exploited via its clustering structure. In particular, we use a probabilistic mixture model to identify the latent clusters, e.g., p ( x ) = ter label. The detailed instantiation of this mixture model, e.g., how to decide the cluster size and the speci cation of cluster conditional likelihood, will be discussed in Section 3.3. With the identi ed clustering structure at hand, we devise a divide-and-conquer strategy for selecting the query instances: 1) it should come from the most informative clus-ter; and 2) it should be representative in that cluster. This query strategy focuses on the entire input space; comparing to those individual instance based selection methods, it can thus help to avoid querying outliers or repeatedly querying similar instances.

To quantify the \informativeness" of a cluster, we exploit an information theoretic metric -class entropy H ( c ). For every cluster c , we apply f ( x ) to predict the labels for all the unlabeled instances in it. Then based on these predicted labels, we compute the class entropy for this cluster as, where Y c i is the set of unique labels in cluster c i predicted by classi er f ( x ).

A cluster with larger class entropy indicates increased dis-crepancy between the current classi er's prediction and clus-tering structure inferred from the density of unlabeled in-stances. Therefore instances from such a cluster are consid-ered potentially more helpful in introducing new informa-tion to classi er training. In addition, the size of a cluster is also an important criterion for measuring its informative-ness. When multiple clusters have similar class entropy, a larger cluster will indicate more uncertainty of class labels in it. Hence acquiring a label for such a cluster can mostly reduce the classi er's uncertainty on a larger portion of in-stances. Combining these two aspects, we locate the cluster of choice by the product of cluster proportion p ( c ) and class entropy H ( c ) as follows,
Once we locate the candidate cluster ^ c , we need to choose the most \representative" instance from it for labeling. We use the conditional likelihood p ( x j ^ c ) over all the unlabeled instances to select such an instance. The intuition behind this choice is that the instance with the maximum condi-ti onal likelihood best captures the homogeneity of instances in the selected cluster, such that knowing its label will pro-vide a substantial boost for the classi er to predict the class labels within this cluster.

In addition, we also view high class entropy in the selected cluster as an indication of low resolution of clustering results in that local region: classi cation boundary goes inside the cluster. To reduce this divergence between predicted classes and clustering results, we need to further separate this clus-ter into ner clusters. The bene t of this sub-clustering is that the class distribution estimated by the classi er is intro-duced into clustering; this helps generate more homogeneous clusters for later instance selection.
The basic assumption in our clustering-based active learn-ing solution is that instances in the same cluster tend to share the same class label. The query strategy described in the previous section exploits this assumption to avoid re-peated selection of similar instances. In this section, we will further capitalize on this assumption to propagate the ac-quired labels to their nearby unlabeled neighbors to reinforce our current knowledge about the underlying class distribu-tion.

The idea of label propagation is popularized in trans-ductive learning [31, 30, 3], where class labels are propa-gated from the labeled instances to their unlabeled neigh-bors based on the structure of data manifold. In practice, the data manifold is approximated by the nearest neighbors of each instance derived from data features. To empirically validate the feasibility of applying label propagation in our problem, we randomly selected a small portion of labeled sensor point names in our evaluation corpus, computed the Euclidean distance between all pairs of instances based on their feature vectors, and grouped them into pairs from the same class and different classes. We plot the cumulative distributions of those three types of distances in Figure 1. F igure 1: Distribution of pairwise distance between in-stances from the same class and different classes.
As we can clearly notice from the results, there is a clear gap between the cumulative distribution of distances be-tween instances from the same class and those from different classes. This result also con rms our assumption in the pro-posed clustering-based active learning that nearby instances tend to share the same class label. Therefore, with proper choice in constructing the data manifold, we can con dently propagate the labels to their closest unlabeled neighbors and use them to update the classi er. This label propagation will amplify the importance of acquired labels and better avoid repeatedly querying of similar instances.

A typical approach to construct the data manifold for la-bel propagation in transductive learning is to look for the k nearest neighbors of each instance. However, in our solution it is challenging to nd a universal setting of k . To accommo-date the clustering structure, label propagation should only be performed within the same cluster. However, since the resulting clusters vary and change across iterations, a xed setting of k might lead to inconsistency between the data manifold and clustering results. To address this issue, we introduce a distance threshold r to de ne the data manifold for label propagation in our active learning process. When the label y is acquired for instance x , all the unlabeled in-stances located within the distance r to x will be assigned the same label y . Those instances will be then removed from D and added to a collection of propagated label set D p for later classi er training.

The optimal threshold r should be the minimum inter-class distance between any pair of instances. However, it is impossible to calculate without knowing the true class labels. In our solution, we keep an estimation of r from all the labeled instances in D l during active learning, d ( x i ; x j ) is the Euclidean distance between x i and x
We divide the minimum distance by 2 to avoid possible overlapping of label propagation between two labeled in-stances. The adaptive estimation of distance threshold de-ned in Eq (3) will shrink during active learning and there-fore re nes the propagated labels. Accordingly, we need to correct previously propagated labels where the estimation of r was less accurate. In particular, when we have a new estimation of r , we will remove instances from D p that fall outside the region de ned by the latest r to any closest la-beled neighbors with the same label, and put them back into the unlabeled set D . This correction happens before the new round of classi er training.
In our proposed clustering-based active learning, data den-sity is exploited via its latent clustering structure. Since we assume instances in the same cluster tend to share the same class label, we choose Gaussian Mixture Model (GMM) [32], a partitional clustering algorithm, to perform the clustering.
In GMM, the cluster label for every instance is treated as a latent variable, which is drawn from a multinomial distri-bution p ( c ), i.e., p ( c ) / c , where 8 c; c 0 and In any given cluster c , the conditional data likelihood of an instance x is speci ed by a multivariate Gaussian distribu-tion. To reduce the number of parameters to be estimated, we choose the isotropic Gaussian in our solution, where the variance 2 is shared by all the clusters. f c ; and are considered as model parameters in GMM.

However, in GMM, we need to manually specify the num-ber of clusters for a given input data set; and the cluster-ing result of GMM is very sensitive to such setting. More im portantly, in our solution, we also need to perform sub-clustering whenever we encounter a cluster contradicting with the current classi er's prediction. It is impossible for us to prede ne the size of those sub-clusters during active learning. To make clustering feasible in our solution, we ap-peal to a non-parametric Bayesian solution: we assume the model parameters ( ; ) in each cluster are random vari-ables, which are drawn from a Dirichlet Process prior [11]. A Dirichlet Process DP ( G 0 ; ) with a base distribution G 0 and a scaling parameter is a distribution over distri-butions [11]. The base distribution G 0 speci es the prior distribution of model parameters, e.g., mean parameter in each cluster, and the scaling parameter speci es the con-centration of samples drawn from DP, e.g., cluster propor-tion p ( c ). An important property of the DP is that though the draws from a DP have countably in nite size, they are discrete with probability one, which leads to a probability distribution on partitions of data. The number of unique draws, i.e.,the number of clusters, varies with respect to the data and therefore is random, instead of being pre-speci ed.
As a result, with the introduced DP ( G 0 ; ) prior, data density in a given collection of instances can be expressed using a stick-breaking representation [23]: where = 1 c =1 Stick ( ) represents the proportion of clusters in the whole collection. The stick-breaking process Stick ( ) for the cluster proportion parameter is de ned as: c Beta (1 ; ) ; c = 2 is xed in all clusters, we use a conjugate prior for in G , i.e., for 8 c; ci N ( a; b ), with the assumption that each dimension in c is independently drawn from a univariate Gaussian. This will greatly simplify the later on inference procedure.

Because the data density distribution de ned in Eq (5) only has nite support at the points of f c ; c g k c =1 calculate the posterior distribution of latent cluster labels in each unlabeled instance to discover the clustering structure for active learning. Following the sampling scheme proposed in [15], we appeal to a Gibbs sampling method to infer the posterior of cluster membership. Detailed speci cations of this sampling algorithm can be found in [15]. In particular, we use the same hyper-parameter setting of ( a; b ) in G 0 for initial clustering and subsequent sub-clustering during our cluster-based active learning process.

Putting it all together: Algorithm 1 summarizes our clustering-based active learning solution for the sensor type classi cation problem. In each iteration, we select the most \informative" cluster measured by the class entropy of pre-dicted labels by the classi er. We acquire a label for the instance centered in the selected cluster, and propagate the newly obtained label to its unlabeled neighbors within an estimated distance. At the end of this iteration, we per-form sub-clustering on the selected cluster to re ne its local clustering structure for later instance selection.
Comparing to the existing solutions for the sensor type classi cation, our proposed algorithm addresses the prob-lem from a totally different perspective. As we discussed before, because existing solutions [4, 22] isolated the anno-tation process from model training, it likely leads to wasted A lgorithm 1: Clustering-based Active Learning
Inp ut : point names D = f x 1 ; x 2 ; : : : ; x n g , and label budget B Output : predicted labels of the point names Y
Initialize: Generate clusters with DP ( G 0 ; ) on D , reset the labeled instance set D l and propagated label set D p to empty while B &gt; 0 do end effo rt in data annotation. With active learning, we can re-strict our effort to a smaller set of instances that are the most helpful for classi er training. A practical bene t of this learning approach is that we can rapidly bootstrap the building analytic software across different sites. Instead of going back and forth for several rounds of blind annotation, a building manager only needs to interact with the system for several iterations on the y to achieve immediate satis-factory results.

Comparing to the other active learning algorithms, our proposed method focuses on exploiting information conveyed in the data clustering structure. Based on the assumption that nearby instances are more likely to share the same class label, we select the instances for labeling based on the iden-ti ed latent clustering structure of instances and propagate the labels to their adjacent unlabeled neighbors. This helps us avoid repeatedly querying similar instances and enhance the importance of labeled instances. In addition, the clus-tering structure and label propagation strategy are adapted in an online fashion. In Nguyen and Smeulders's work [16], clustering is also exploited for active learning; but their clus-tering structure is static and no label propagation is per-formed. In Dasgupta and Hsu's work [7], hierarchical clus-tering is used to provide a more exible and dynamic cluster-ing structure, but no label propagation is performed as well. We should note this proposed active learning algorithm is general, it only assumes the proximity of label distribution in nearby instances. Therefore, this algorithm can also be applied in a broader context, e.g., document categorization [26] and image retrieval [25].
To demonstrate the effectiveness of our proposed solu-tion in addressing sensor type classi cation, we evaluate our clustering-based active learning algorithm on point names collected from three distinct buildings. Extensive experi-mental comparisons con rm that our method achieved the F igure 2: A typical HVAC system consisting of an air han-dler unit (AHU), several variable air volume boxes (VAV), water-based heating/cooling pipes and air circulation ducts. (Figure used with permission from the authors of [1].) same classi cation accuracy with much fewer labeled exam-ples than all the baseline methods. We also demonstrate how metadata normalization can enable meaningful analytic ap-plications with the raw sensor streams under the context of commercial building energy control and comfort assessment.
Figure 2 illustrates a typical heating, ventilation, and air conditioning (HVAC) system deployed in modern commer-cial buildings. An HVAC system usually uses a combination of hot and cold water pipes in conjunction with air han-dler units (AHU) to maintain the appropriate thermal en-vironment within the building. An HVAC system usually consists of several AHUs and each AHU is responsible for a physical zone in the building. An AHU consists of variable speed drives that supply cold air (cooled by the supplied cold water) using ducts to VAV boxes distributed through-out the building. The hot water loop is also connected to these VAV boxes using separate pipes. Each VAV box con-trols the amount of air to be let into an HVAC zone using dampers, whose opening angle can be programmed. A re-heat coil, which uses supplied hot water, is used to heat the air to meet the appropriate HVAC settings for each zone.
Our evaluation data set is collected from sensor streams from over 2,500 sensors of more than 20 different types deployed in three commercial buildings. Specially, Build-ing A uses a building management system contracted with Trane and building B comprises deployment by KETI 4 and Siemens, while building C uses an archaic system by Bar-rington Controls.

Table 2 summarizes all the types of sensors evaluated in these three buildings and the number of sensors of each type. For example,\room temperature"measures the temperature in room and for a better understanding, all the other temper-ature measurements on water circulation and air ventilation are illustrated in Figure 2. For setpoints, we assign only one general type which includes all set points for every actuator con gured in the building. ht tp://www.keti.re.kr/e-keti/ T able 2: Number of points by type for the 3 test buildings. \Temp" stands for \temperature", \HW" for \hot water" and \CW" for \cold water".
The sensor point names are the input of our active learning algorithm. As shown in our motivating example in Table 1, the point names are short text strings with several con-catenated abbreviations. To represent the primitive point names as feature vectors for classi er training, we rst con-vert all point names to lower cases and trim out the numeri-cal characters, resulting in a series of words, e.g., Zone Temp 2 RMI204 becomes {zone, temp, rmi} . To capture possi-ble variants of abbreviations in point names, e.g., \tmp" and \temp" for temperature, we adopt k-mers [13] as our fea-tures. The term k-mer refers to all the possible substrings of length k, which are contained in a string. This feature is popularly used in protein and gene sequence analysis in bioinformatics. And it helps measure sequence similarity without alignment. In our case, we limit the k-mers com-putation only within a word boundary. In general, having too small a k will increase the chance of overlapping k-mers, making the points less differentiable. Therefore, we compute k-mers of length 3 and 4 for all point names. For exam-ple, {zone, temp, rmi} will yield a set {zon, one, tem, emp, rmi} with k=3. A dictionary of k-mers is constructed with all the k-mers generated from each point name. Each point name is then converted into a feature vector based on the frequency of k-mers in it. For example, a set of k-mers {zon, tem, emp, zon} will be transformed into a vec-tor of (2,0,1,1,0) with the dictionary {zon, one, tem, emp, rmi} . This feature representation will be used in our later evaluations.
To evaluate the performance of our proposed algorithm, we adopt four active learning algorithms as baselines. achieve better accuracy on all buildings with less labeled examples.
Random (RAND): this method selects an example at ran-dom uniformly from the unlabeled set in each iteration.
Least Margin (LM) [21]: this method adopts the a simple yet effective sampling strategy, which queries the instances for labeling with least con dence measured by the difference of posterior probability of the rst and second most probable class labels predicted by the classi er: where ^ y 1 and ^ y 2 are the rst and second most probable pre-dicted class labels, respectively. We compute the class pre-diction probability by SVM based on the method proposed in [18].

Pre-clustering (PC) [16]: this method pre-clusters the in-stances and selects the example for querying satisfying two criteria: 1) locate at the classi cation boundary and 2) rep-resentative of dense clusters. The clusters are constructed by the same Gaussian Mixture Model with Dirichlet Process prior as used in our method.

Hierarchical Clustering (HC) [7]: this method leverages a hierarchical clustering structure, which is represented as a binary tree, and estimates the purity of labels for each clus-ter node. The algorithm iteratively selects examples from a subtree whose size is signi cantly large or the impurity of labels is high.

In all baselines, and our method, a linear SVM model is used as the classi er.

We measure the overall classi cation accuracy of each ac-tive learning algorithm with different amount of manual la-bels, and examine how many examples are needed by each method to reach a required accuracy level. Besides classi-cation accuracy, we also compute the weighted macro F1 score of each method given different labeling budgets (e.g., 5%, 10%, etc of the entire unlabeled set) in each building.
In our experiments, to reduce possible bias introduced by training/testing split, we perform 10-fold cross validation for each active learning method, and repeat it 10 times with different random seeds. The average performance of 10 runs from each method is reported.
In Figure 3, we illustrate the comparison results of sen-sor type classi cation accuracy from all three buildings over all methods. In all three buildings, our method performed the best against all the baselines. In particular, our method consistently requires the least amount of manual labels to achieve a satisfactory and converged accuracy. In building A, to achieve the accuracy of 90%, our clustering based ac-tive learning method (denoted as AL+CP) requires 105 la-beled examples while HC needs 110, which is the best among all baselines. LM takes 127 labeled examples and the other two baselines take a hundred more labeled examples to ob-tain the same accuracy. In building B, both our method and LM reach the accuracy of 99% with 26 examples, while it takes 51, 61 and more than 200 examples, respectively, for the HC, PC and RAND baselines. In building C, our method achieves 99% accuracy with 35 labels while HC and LM requires 101 and 135 labels, respectively, for the same accuracy. Again, PC and RAND require much more labeled examples to get the same performance.

On all three buildings, RAND performs reasonably on ini-tial iterations (e.g., before 20) but becomes the worst later on. This is because of imbalanced class distribution in our three data sets. RAND selects more examples from larger classes at the beginning and quickly gets a fraction of the major classes correctly classi ed. However, in later itera-tions, RAND has a lower chance in picking examples from smaller classes and converges to a local optimal quickly.
The PC baseline, which also exploits data clustering struc-ture, performs signi cantly worse than other baselines for Building A. PC gives priority to the examples that are: 1) close to the decision boundary, and 2) representative in a cluster. Such criteria are very similar to those in our method. Further inspection reveals that examples selected by the PC baseline were neither informative nor representative enough to distinguish different types of instances. This stems from the fact that for building A, the initial clusters from GMM fairly overlap with each other. This biases the PC baseline to select more examples with relatively high uncertainty but less representative in a cluster.

The HC baseline is quite effective in avoiding querying similar examples from dense areas. However, because HC does not perform label propagation, after all clusters become roughly equally pure with sufficient sampling, it will start repeatedly querying previously sampled areas, which results in a long plateau for building B and C. For building A, the larger classes contain a few variations in their point names, therefore when HC avoids sampling from the same dense area at rst, it misses important informative examples and is only able to pick them up in later iterations.
Moreover, we notice that, for all methods, the conver-gence rate on building B and building C is much better than building A, despite the fact that there are more points in those two buildings. The reason is that the point names in these two buildings share stronger regularities -the last seg-ment always indicates the sensor type. Recall the examples shown in Table 1: SDH_SF1_R282_RMT from building B and SODA1R410B_ART from building C. Both of them adhere to the same order of segment categories -building name -air handler/supply fan identifier -room number-sensor type , only with a slightly different set of delimiters. Our string feature captures such regularity and helps the learning algo-rithm converge faster. However, the same rule doesn't apply to building A because there the type information can be en-coded in any segment in a point name. For instance, a tem-perature measurement can be named in many conventions, such as averageSpaceTemperature 1st Floor Area1 , East Space Temperature scc2UIP33 or RM511A Zone Temp . With such variety in the naming conventions, the learning algo-rithms need to explore all possible variants of point names for convergence in building A.

On such imbalanced data sets, investigation of accuracy only is not enough. We also measure the weighted macro F1 score of classi cation for each method with different label-ing budgets, i.e., different percentage of total instances that can be labeled. We examine the F1 score at three different labeling budgets, 1%, 5% and 10%. The weighted macro F1 score is an altered version of macro F1 score [29], which calculates the F1 score for each class, where \one-versus-all" binary classi cation is performed, and weight the resulting F1 of each class by support (the number of true instances for each label). Paired two sample t-test is performed to validate the statistical signi cance of improvement from our method over the best-performing baseline. The results are shown in Table 3. In general, more labeled examples leads to better classi cation performance over all classes; and our method performed the best in all cases. As we discussed ear-lier, all methods converge much faster in building B and C than in building A. In those two buildings, our method can achieve an 100% classi cation accuracy with slightly more than 1% of total examples being labeled.

In addition, to investigate the contributions of data clus-tering and label propagation in our proposed active learning algorithm, we also conduct experiments by disabling data clustering and label propagation in our method. From the results in Figure 4, we can observe that with clustering only, the performance of our method is no better than margin-based active learning; and beyond a certain point, the per-formance plateaus because the classi cation boundary con-verges to the clustering structure, and no more new exam-ples could be selected for labeling. However, we do observe some improvement from clustering only over margin-based active learning for building B and building C in early itera-tions. This is because the same type of points in these two buildings contain few variations and these initial examples at the center of relatively pure clusters are more represen-tative for a dense region. Interestingly, after adding label propagation, we clearly see a boost in the performance than performing clustering only. We attribute the improvement of our method to label propagation which considerably am-pli es the amount of available labeled examples for classi er training, and therefore helps to estimate a better classi er.
Besides, we also examine how the label propagation alone can help active learning in general. We adopt the same distance threshold estimation method as de ned in Eq (3) to introduce propagated labels in the margin-based baseline and denote it as LM+P in Figure 4. On building A, the inclusion of label propagation makes LM even worse, be-cause in early stage the estimated threshold r in LM+P is not accurate enough, and it mistakenly propagates the la-b els. Only when more labels are acquired in later stages, the estimation of distance threshold r improves and there-fore LM receives propagated labels with improved quality. In our method, because of data clustering, the labeled exam-ples in the early stage are more representative; it in turns helps better estimate the distance threshold r . As a con-clusion, data clustering and label propagation complement with each other in our proposed method, and help active learning acquire more informative examples quickly.
As a rst step towards automated metadata normaliza-tion, our algorithm is able to classify and transform the type information in the primitive metadata into a normalized name space. It provides a better opportunity for running uniform analysis on heterogeneous metadata across build-ings with different management systems. As a proof-of-concept, we demonstrate an illustrative example, in which we search over the normalized metadata for different types of sensors that are in the same room under the application context of building comfort assessment and energy control.
Ideally, with the occupancy information of spaces consid-ered, a properly con gured building should automatically stop conditioning unoccupied rooms and areas. However, one typical problem plaguing buildings that incurs energy waste is unoccupied room being conditioned. For a build-ing manager to inspect the building and locate such spots for better scheduling, he should be able to run simple key-word based searches over the metadata to look for different types of streams, such as room temperature and occupancy streams. With the desired types of streams returned, he can match different types of streams by room location to perform further examination, e.g., which room is still com-fortable during unoccupied periods.

To accomplish the task, we rst classify the sensor streams by type with both our method and the best baseline (LM). Then based on the predicted sensor type for each stream, we normalize each type of sensors into a common name space, e.g., all the streams predicted as room temperature are named as \room temperature". With the type informa-tion normalized, we can simply retrieve all the streams of a certain type with one keyword, e.g, using \temperature" to search for temperature streams. The next step is to match different types of sensors by room location. In particular, we derive a few regular expressions to nd the segment in-dicating room location in the point names of each sensor.
In this experiment, we search for four types of sensors -occupancy, temperature, humidity and CO 2 -by simply using these type names as the keywords. Then we match the room location of these four groups with regular expres-sions on their original primitive point names. Speci cally, we examine the accuracy of three searches: 1) occupancy and temperature in the same room; 2) occupancy, temper-ature and humidity in the same room; and 3) occupancy, temperature, humidity and CO 2 in the same room, given these combinations are usually used to assess the comfort of a room or identify potential waste in unoccupied rooms.
Table 4 illustrates the performance of basic searches on building A for occupancy (O), temperature (T), humidity (H) and CO 2 (C). Our method can return better results with both higher recall and precision because of better pre-dictions on type class for the streams. Given the four re-turned groups of streams, we further run regular expression T able 4: The performance of searches over the normalized metadata with our method (AL+CP) and the best base-line (margin based active learning, LM). We search for four speci c types of streams: occupancy (O), temperature (T), humidity (H) and CO 2 (C).
 T able 5: The performance of searches for different types of streams that are in the same room. We consider the pair of occupancy and temperature (O+T), the combination of occupancy, temperature and humidity (O+T+H), and also the combination of occupancy, temperature, humidity along with CO 2 (O+T+H+C). based matching on the primitive metadata to nd the pairs in the same room. For instance, we rst identify the room location of each returned occupancy stream with regular ex-pressions, then we search for the temperature stream with the same room number as each of these occupancy streams. Similarly, we search for humidity and CO 2 streams in the same room. With these different types of streams grouped into the same room, a building manager can compare the actual data against some standards to decide if a room is problematic or not. We summarize the search results in Ta-ble 5. In general, the normalized metadata by our method produces search results with higher recall and slightly lower precision. Such results are expected considering the prac-tical demand from building managers -it is acceptable to return some unexpected results (false positives) while not missing the expected ones (false negatives), where he can manually examine a much smaller group of candidates and lter out the incorrect ones. We conclude that with nor-malized metadata for a building, people such as a building manager can more easily retrieve expected streams and con-duct meaningful analysis to identify problems in a building.
In this paper, as a rst step towards automated sensor metadata normalization, we investigate the problem of build-ing sensor type classi cation and introduce a novel, effective yet general active learning method to address the problem. Following the assumption that similar instances are more likely to share the same class label, our solution exploits the data clustering structure and propagates the labels to their nearby unlabeled examples to accelerate the learning process. Extensive experimental comparisons are performed against several state-of-the-art active learning algorithms with over 20 different sensor types and 2,500 sensor streams collected from three buildings. Our proposed solution is able to achieve satisfactory classi cation accuracy with much less labeled examples than the baseline algorithms. In addition, we also demonstrate that the normalized metadata can po-tentially enable meaningful analytic applications with the raw sensor streams under the context of commercial build-ing comfort assessment and energy control.

Our proposed active learning algorithm is general, and therefore it is applicable in a broader context, e.g., docu-ment categorization [26] and image retrieval [25]. In our current metadata normalization process, only the text fea-tures from point names are utilized. However, another im-portant aspect of the sensor streams is the actual data from the sensor readings. Features constructed from such raw signals can also be introduced to characterize the stream. Such feature becomes vital when the point names are miss-ing or corrupted. In addition, our current problem setting is limited to one building; it is necessary for us to solve the normalization problem across buildings, e.g., classi cation model learned in one building can be used to bootstrap the learning in another building.
