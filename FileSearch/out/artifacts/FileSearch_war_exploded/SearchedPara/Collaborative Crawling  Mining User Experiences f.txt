
The rapid growth of the world wide web had made the prob-lem of topic specific resource discovery an important one in recent years. In this problem, it is desired to find web pages which satisfy a predicate specified by the user. Such a pred-icate could be a keyword query, a topical query, or some ar-bitrary contraint. Several techniques such as focussed crawl-ing and intelligent crawling have recently been proposed for topic specific resource discovery. All these crawlers are link-age based, since they use the hyperlink behavior in order to perform resource discovery. Recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process. In this paper, we will approach the problem of resource discovery from an entirely different perspective; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate. This user be-havior can be mined from the freely available traces of large public domain proxies on the world wide web. We refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources. 
Such a strategy is extremely effective because the topical consistency in world wide web browsing patterns turns out to very reliable. In addition, the user-centered crawling sys-tem can be combined with linkage based systems to create an overall system which works more effectively than a sys-tem based purely on either user behavior or hyperlinks. 
With the rapid growth of the world wide web, the problem of resource collection on the world wide web has become very relevant in the past few years. Users may often wish to search or index collections of documents based on topical or keyword queries. Consequently, a number of search engine technologies such as Lycos and AltaVista have flourished in recent years. A significant method proposed recently for automated re-personal or classroom use is granted without fee provided that copies are permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. source discovery is the technique of focused crawling [4]. The essential idea in focused crawling is that there is a short range topical locality on the web. This locality may be used in order to design effective techniques for resource discov-ery by starting at a few well chosen points and maintain-ing the crawler within the ranges of these known topics. A recent paper discusses the application of learning method-ologies [3] in order to improve the effectiveness of the crawl. 
While the focussed crawling technique is designed for find-ing web pages belonging to one of the classes from a hierar-chically arranged topical collection, the intelligent crawling technique is capable of more flexible and arbitrary predicates such as a combination of different kinds of topical queries, keyword queries or other constraints on the content or meta-information about the web page such as its URL domain. 
Both the focussed crawling and intelligent crawling meth-ods [3, 4] use only linkage based information in order to find topical resources. In the former case, the linkage based in-formation is used directly under the assumption that web pages show topical locality, whereas in the latter case, a learning process is used to identify the most suitable web pages. It is this learning process of the intelligent crawling technique which provides it with the flexibility and robust-ness necessary to collect web pages belonging to arbitrary predicates. 
In many cases, links are quite noisy in terms of topical locality. Such links may correspond to banners, advertise-ments and other content which do not carry specific in-formation about resource discovery. Often advertisements, banners and such content dominate the resource structure of many web sites even though they are only occasionally browsed by users. In addition, since the web continues to grow rapidly in a unstructured fashion through the efforts of millions of non-collaborating users, it is difficult for a sin-gle linkage model to completely capture the complex and unstructured variation in hyperlink patterns. 
This paper will therefore diverge significantly from linkage-based methodologies, and instead concentrate on utilizing the browsing patterns of users on the web in order to mine significant patterns. We will achieve this goal by utilizing the logs of user access behavior on public domain proxies. An example of such a public-domain proxy is the Squid [5] proxy cache, which is a set of large hierarchical caches on the web which is capable of improving the latency perfor-mances of different geographical regions. The access pat-terns of these systems are representative of a large sample of world wide web clients. 
The utilization of user behavior turns out to be quite use-
This paper is organized as follows. In the next section, 
The collaborative crawler assumes that multiple users browse likely to belong to the predicate. Specifically the character-of the users are learned by the collaborative crawler in order to estimate their likelihood of satisfying the predicate. 
An important aspect of most web logs is that they often do not contain individual user information but simply the domain names of the accesses. Since some of the domain names are actually proxy accesses from large groups of users rather than individuals, this reduces the level of granularity of the available data. Some simple processing can however improve the quality of the trace:  X  All source IP addresses which occurred in a larger num-ber of accesses than a given threshold 1 were removed. This filtered out a large fraction of group accesses.  X  The above criterion was also used for shorter periods of time such as a 1 minute interval. If there were more than a certain num-ber 2 of accesses per unit of time from the same source IP address, then this was indicative of group behavior. The above preprocessing steps would still result in many 
IP addresses corresponding to groups of users rather than individuals. However, our primary aim is to discover re-sources rather than user behavior, and as long as there was considerable correlation between the source IP address and 1We picked a threshold depending upon the number of days that it represented. Our threshold was 2000 accesses per day. The assumption was that only. large groups of people could make so many accesses in a given day. 2We used 100 accesses per minute as the threshold. The reason for this high threshold is that multiple access entries in the trace may actually correspond to the different parts (images, frames and html) of the same user access. access content, we did not consider this to be a serious lim-itation. For the rest of the paper, we will still refer to a source IP address as a user for the purpose of conceptual abstraction. 
The crawler is implemented as an iterative search algo-rithm which always maintains a set of candidate web pages together with corresponding priorities. These priorities are determined by relating the statistical behavior of the crawled web pages to the users that have accessed them. Each time a web page is crawled, it is determined whether or not it satisfies the predicate. The statistical behavior of the users that have accessed this web page is utilized to update the priorities of the candidate web pages. The crawler keeps track of the nodes which it has already visited, as well as a potential list of candidates. A web page is said to be a can-didate when it has not yet been crawled, but some other web page which has been accessed by the same user has already been crawled. The access behavior of this user is incorpo-rated into the statistical model used to decide whether that candidate is likely to satisfy the predicate. This information may be used by the crawler to decide the order in which it visits web pages. 
Thus, at each point the crawler maintains candidate nodes which it is likely to crawl and keeps calculating the priorities of the nodes using the information about the user access patterns of different web pages. This statistical information which relates the user behavior to the predicate may be used to learn the nature of the relationship between web pages and users. 
In this section, we will discuss the statistical model which is used to keep track of the crawling behavior. Since the crawler system uses the gateway logs for resource discovery, it is an interesting question as to how the user behavior may be converted into priorities for visiting web pages. It is clear that we need to build a statistical model which connects the user behavior to the predicate satisfaction probability of the candidate web pages. 
We assume that the set of features used in the learning process are stored in the set /C. This set /C maintains the set of probabilities which indicate the user behavior during the crawling process. We note that several kinds of informa-tion about the user behavior may be relevant in determining whether a web page is relevant to the crawl:  X  The access frequency behavior of the users for different web pages: Users that have accessed web pages belonging to a given predicate are more likely to access other web pages belonging to the predicate.  X  The access frequency behavior of the users with the help of signature features: A signature feature is described as any characteristic of a web page such as content, vocabulary, or any other characteristic of a web page, In many cases, the access behavior of users in terms of signature topics yields information which cannot be revealed by their access behavior from individual web pages. This is because users may often access web pages belonging to combinations of topics rather than individual combinations of web pages.  X  The temporal patterns of behavior of the dif-ferent users across different web pages: This means that users that have just accessed web pages belonging to the predicate are more likely to access the same in the im-mediate future. Thus, by taking the order of accesses into account, considerable amount of useful information can be mined. 
In this section, we discuss the probabilistic model for uti-lizing the access frequencies of users in computing priorities. In order to calculate the priorities, we compute the likeli-hood that the frequency distribution of user accesses makes it more likely for a candidate web page to satisfy the predi-cate. In order to understand this point a little better, let us consider the following case. Suppose that we are searching for web pages on online malls. Let us assume that only 0.1% of the pages on the web correspond to this particular pred-icate. However, it may happen that the percentage of web pages belonging to online malls accessed by a user is over 10%. In such a case, it is clear that the user is favorably disposed to accessing web pages on this topic. If a given candidate web page has been accessed by many such users that are favorably disposed to the topic of online malls, then it may be useful to crawl the corresponding web page. We would like to quantify the level of favorable disposition of the candidate page by a value that we will refer to as the 
In order to develop the machinery necessary for the model, we will introduce some notations and terminology. Let N be total number of web pages crawled so far. Let U be the event that a crawled web page satisfies the user defined predicate. For a candidate page which is about to be crawled, the value of 
P(U) is the probability that the web page will indeed satisfy the user-defined predicate. The value of P(U) can be estimated by the fraction of web pages already crawled which satisfy the user defined predicate. 
We will estimate the probability that a web page belongs to a given predicate U, given the fact that the web page has been crawled by user i. We shall denote the event that the person i has accessed the web page by Ra. Therefore, the predicate satisfaction probability is given by P(UIRi) = P(U N Ri)/P(Ri). We note that when the person i is topi-cally inclined towards accessing web pages that belong to the predicate, then the value of P(UIRa) is greater than Correspondingly, we define the interest ratio of predicate satisfaction as follows: We note that an interest ratio larger than one indicates that the person i is significantly more interested in the predicate than the average interest level of users in the predicate. The higher the interest ratio, the greater the topical affinity of the user i to the predicate. Similarly, an interest ratio less than one indicates a negative propensity of the user for the predicate. Now, let us consider a web page which has been accessed by the users il ... ik. Then, a simple definition of the cumulative interest ratio I(UIRi1,... Ri~) is the prod-uct of the individual interest ratios for each of the users. Therefore, we have: The above definition treats all users in a uniform way in the computation of the interest ratio. However, not all interest ratios are equally valuable in determining the value of a user to the crawling process. This is because we need a way to filter out those users whose access behavior varies from average behavior only because of random variations. In order to measure the significance of an interest ratio, we use the following computation: We note that the denominator of the above expression is the standard deviation of the average of N independent identi-cally distributed bernoulli random variables, each with suc-cess probability P(U). The numerator is the difference be-tween the conditional probability of satisfaction and the un-conditional probability. The higher this value, the greater the likelihood that the event Rij is indeed relevant to the predicate. We note that this value of T(U, Ri~) is the sig-nificance factor which indicates the number of standard de-viations by which the predicate satisfaction of U is larger than the average if the user ij has browsed that web page. In the computation of the interest ratio of a candidate page, we use only those users ij for which T(U, R~) &gt; t threshold s t. 
We note that the nature of proxy traces is inherently sparse. As a result, in many cases, a single user may not access too many documents in a single trace. Therefore, a considerable amount of information in the trace can be bro-ken up into signatures which are particular characteristics of different web pages. Such signatures may be chosen across the entire vocabulary of words (content), topical categories based on scans across the world wide web, or other relevant characteristics of web pages. The use of such characteristics is of tremendous value if the signatures are highly correlated with the predicate. For example, if the document vocabu-lary is used as the relevant signature, then even though there many be billions of documents across the world wide web, the number 4 of relevant words is only of the order of a hun-dred thousand or so. Therefore, it is easier to find sufficient overlap of signatures across users in the crawling process. This overlap helps in reducing the feature space sufficiently, so that it is possible to determine interesting patterns of user behavior which cannot be discerned only by using the patterns in terms of the individual web pages. 
In order to formalize the above concept, we will assume that Lij is the event that the signature j has been accessed by a given user i. Thus, for example, when the event j corresponds to the word j being accessed, it means that the document corresponding to word j is accessed at least once by the user i. We define V~ as the event that a document belonging to the predicate has been accessed at least once by the user i. We note that the ratio P(I/~ILij)/P(P~) the interest ratio that the access of feature j is interesting for the user i. However, we are not just interested in the behavior of a single feature, but that of all the features. Therefore, we denote the signature specific interest ratio of the user i by SFi. Here E[.] denotes the expected value over all the different ZFor the purpose of this paper, we will use a threshold of t = 2 standard deviations in order to make this determination. 4This assumes that the documents are in English and stop-words/rare words have been removed. features. As in the previous case, let us assume that a given candidate page has been browsed by the users it ... ik. The corresponding events are denoted by /ht...R~. We de-note the event that the web page satisfies the predicate by U. Then, the cumulative interest ratio of predicate satis-faction, given that any of the signature characteristics in-side it have been browsed by users il...i~, is denoted by I$f(U[R4t... R4k) and is defined by the following expres-sion: Not all users may show a high propensity for or against a particular predicate. Consequently, we would like to filter out the noise from the above expression. Let ns be the total number of users for which signature specific interest ratios have been computed. We define the mean par and standard deviation a sy of the feature-specific interest propensities: Therefore, we define the significance of each user i as follows: Once the significance of each user i has been determined, we can now use only those users whose significance is above a pre-deflned threshold t for the purpose of crawling. This reduces the noise effects in the computation of the inter-est ratio and ensures that only users that show an access behavior which is significantly related to the predicate are used. 
As discussed above, a wide variety of signature character-istics can be used in order to facilitate the crawling process. One example of such a set of characteristics is the use of the topical categories available in the Yahoo! taxonomy. The characteristics in a web page are defined as the dominant classes in the taxonomy which are related to it. In order to achieve this goal, we implemented a classifier system similar to that described in [2]. This classifier system was capable of predicting the classes most related to a given web page. We found the km,,~ = 5 most closely related classes to the page using the nearest neighbor classifier of [2]. 
The web pages accessed by a given user often show con-siderable temporal locality. This is because the browing behavior of a user in a given session is not just random but is highly correlated in terms of the topical subject matter. Often users that browse web pages belonging to a particular topic are likely to browse similar topics in the near future. This information can be leveraged in order to improve the quality of the crawl. 
In order to model this behavior, we will define the concept of temporal locality region of a predicate U by Tll(U). do so, we will first define the temporal locality of each web page access A. The temporal locality of a web page access A is denoted by TLR(A) and is the n pages accessed either strictly before or strictly after A by the same user, but not including A. Now let us say that A1...Am be the set of accesses which are known to belong to the predicate U. Then the temporal locality of the predicate U which is denoted by TI:(U) is defined as follows: Let fl be the fraction of web pages belonging to which also satisfy the predicate. Furthermore, let f2 be the fraction of web pages outside T X (U) which satisfy the predicate. Then, the overall interest ratio for a web page belonging to T X (U), is given by: Similarly, the Interest Ratio for a web page which does not belong to the temporal locality of U is given by: We note that in most cases, the value of .fl is larger than P(U), whereas the value of .f2 is smaller than P(U). respondingly, the interest ratios are larger and smaller than one respectively. For a given web page, we check whether or not it belongs to the temporal locality of a web page which satisfies the predicate. If it does, then the corresponding interest ratio is incorporated in the computation of the im-portance of that predicate. 
The different factors discussed above can be utilized in order to create a composite interest ratio which measures the value of the different factors in the learning process. We define the composite interest ratio as the product of the in-terest ratios contributed by the different factors. Therefore, the combined interest ratio IC(u) for a web page which has been accessed by users il ... ik is given by: This composite interest ratio reflects the overall predicate satisfaction probability of a web page based on its charac-teristics. 
The overall sketch of the collaborative crawling algorithm is discussed in Figure I. The method maintains a candidate list which is denoted by Cand which keeps track of the web addresses that are to be crawled. In each iteration, the high-est priority page FF of the candidate list Cand is crawled, while the learning information and priorities are updated based on the user access behavior of F. These updated pri-orities are used in order to expand the list of potential can-didates. The candidate page F is deleted from Cand after it has been crawled. The process of calculating the priorities is denoted by CalculatePriorities, whereas that of expanding the candidate list is denoted by ExpandList. This iterative process continues until the list Cand becomes empty. In addition~ it is assumed that at least rnin-cand web pages be crawled before the process terminates. If min.cand web pages have not yet been crawled and Cand is empty, then the algorithm randomly samples the trace for web pages which are further added to the candidate list. This ensures 
Learning Statistics: K); Find all the users 'P(F) than have accessed F; Compute the priorities of all web pages .Af accessed by 
P(F) using the computation discussed in section 2; return(A/'); { Add those candidates in .iV to Cand which have priority above a user-defined threshold; } 
Set priority of each element in Cand to 1; while Cand is not empty do begin Sort Cand in order of decreasing priorities; Pick the first page F from Cand; 
Issue a get request for UI:tL F; EzpandList( Cand, Af); 
Delete F from Cand; if Cand is empty and at least min-cand end; 
Figure 1: The Collaborative Crawler Algorithm 
In the actual implementation, two hash tables were main-temporal locality of U. (10) Number of documents s~ not belonging to the temporal locality of U which satisfy the predicate. 
The quantities (1), (2), (7), (8), (9) and (10) are globally maintained in individual variables, whereas the quantities (3), (4), (5), and (6) are maintained for each user in the hash table. This information are used to estimate the key statistical parameters and probabilities as follows: (3) SFi = ~hll Features j (r~/rlj)/(Number of Features) 
We note that the above set of estimates suffice to com-pute all the interest ratios discussed in the previous section. However, it is time-consuming to calculate all the interest-ratios after each web page is crawled. In order to achieve better efficiency, the process of updating is executed in a lazy fashion by CalculatePriorities. The preference values are updated only for those users which have accessed the web page that was last crawled. Then, the interest ratios for the candidate web pages accessed by these users are up-dated. Even though this results in the interest ratios of a few web pages being stale, the web pages which are updated are the ones which have the maximum change in their priority values because of the last web page being crawled. In addi-tion, a periodic process of updating the priorities is applied every few iterations in order to update these values. The procedure CalculatePriorities returns a list Af of candidate URLs whose interest ratios were updated. All those interest ratios which are larger than 1 are added to the candidate list. This is achieved by the procedure ExpandList. After expansion of the list, the candidate page F is removed from Cand. This procedure is repeated until at least rnin-cand web pages have been crawled and Cand becomes empty. We note that one of the inputs to the algorithm is the starting seed set S. If topic specific resources in the trace are known, then these can be used as the starting seeds; alternatively the use of S = {} results in the random sampling of min-cand pages from the trace as the default starting point. 
We note that even though this paper is tailored towards the problem of using user relevance in the crawling process, it can be combined with a linkage based system in order to find predicate-specific pages which are less popular. In or-der to achieve this, the system can find all the web pages which are linked to by a predicate-satisfying web page and added to the candidate list. As a result, the list Cand be-comes a combination of candidates for which we may have either web log access information or linkage information. In addition, we need to make changes to the process of calcu-lation of priorities. In [3], we discussed the computation of interest ratios which are analogous to those discussed in this paper using purely linkage based information. Let IL(U) be the interest ratios computed for a candidate page using the method in [3]. Let I (U) be the corresponding user-based interest ratio. As discussed earlier, not all web pages have both linkage based and user-based information associated with them. Therefore, when such information is not avail-able, the corresponding value for In(U) or Iv(U) is set to one. Since the URL for a candidate page is discovered either 
In Figures 2 and 3, we have illustrated an examples of lift 
In this paper, we discussed a method for leveraging user Experiences for Topical Resource Discovery. IBM Research Report, 2002. of using supervised clustering for building categorization systems. KDD Conference, 1999. Crawling on the World Wide Web with Arbitrary Predicates. WWW Conference, 2001. Crawling: A New Approach to Topic Specific Resource Discovery. WWW Conference, 1999. Proxies. http://www.cs.ndsu.nodak.edu/rousskov/ 
