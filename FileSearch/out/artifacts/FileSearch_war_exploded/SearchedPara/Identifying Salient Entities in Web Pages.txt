 We propose a system that determines the salience of entities within web documents. Many recent advances in commer-cial search engines leverage the identification of entities in web pages. However, for many pages, only a small subset of entities are central to the document, which can lead to degraded relevance for entity triggered experiences. We ad-dress this problem by devising a system that scores each entity on a web page according to its centrality to the page content. We propose salience classification functions that in-corporate various cues from document content, web search logs, and a large web graph. To cost-effectively train the models, we introduce a soft labeling methodology that gen-erates a set of annotations based on user behaviors observed in web search logs. We evaluate several variations of our model via a large-scale empirical study conducted over a test set, which we release publicly to the research commu-nity. We demonstrate that our methods significantly out-perform competitive baselines and the previous state of the art, while keeping the human annotation cost to a minimum. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Document aboutness, entity salience, content analysis.
The concept of Salience or Aboutness has been investi-gated in many fields of research, from linguistics to semi-otics, and from sociology to psychology. While dictionary definitions look deceptively simple ( X  X ost noticeable or im-portant X  (OED),  X  X tate or condition of being prominent X  (Wikipedia)), the notion of salience is very hard to pin down in practice. A number of observations around salience might be uncontroversial, however: (1) Salience and rele-vance/importance are not the same. An entity or notion A in a text can be highly salient, yet unimportant to the reader. (2) Salience is a function of the structure of a text, and indirectly a function of the intention of the author, as opposed to a function of the reader X  X  intent or needs. Salience also has very practical implications on the Web: People, entities, and content are increasingy linked in a  X  X eb of Things X  paradigm [5]. However, in our samples we found that fewer than 5% of entities on a page are salient to the web page, making it very important to be able to distinguish them from the remaining non-salient entities.

We propose scalable weakly-supervised models for learn-ing to score entities according to their salience to a docu-ment. We leverage web search logs to automatically acquire soft labels as a supervision signal for our training data. We train our models on a large number of web pages, leveraging features from document content, page classifiers, and a web graph. Finally we show empirical evidence, on data repre-senting the HEAD and TAIL distributions of the web, that our methods significantly outperform the previous state of the art on various ranking and classification metrics. As this is the first dataset created of its kind, we release it publicly to the research community.

The major contributions of this paper are:
Understanding the meaning or aboutness of a document has received attention from both a theoretical [23, 12, 2] and practical perspective. In the latter approaches, driven by application-specific demands, computational models have decomposed aboutness and focused on detecting aspects of aboutness such as key terms [30, 13, 21], latent semantic spaces/topics [18, 1], and summaries [25, 17, 10].

Most related to our work is the current state of the art described in Paranjpe [21] where the focus is on the detection of key terms in web pages. There are three key differences between our proposed method and theirs. First, our notion of document aboutness is entity-centric, i.e., we consider the identification of salient entities, as opposed to salient terms of any kind. Second, our soft labeling method is different from theirs and we demonstrate that it outperforms it and is more robust to effects of popularity and presentation order of URLs in the search results page (SERP). Finally, our feature set is a significant extension of the set of features Paranjpe utilizes.

The keyword extraction task can be seen as related to en-tity salience, where keywords and key phrases are a super-set of salient entities in a document. Keyword extraction is often addressed in the context of various document under-standing tasks, most often in extraction based summariza-tion or abstract generation [11, 20, 10], and more recently in (online) contextual advertisement or keyword appraisal [30]. Linguistic cues, including syntactic, semantic and discourse information for keyword extraction are investigated in [11], [20], [12], [8] and [2], for example.

Term frequency statistics and term weighting schemes are also commonly used to score the specificity/importance of a term in information retrieval [19, Chapter 7]. The same idea motivates the vector space model and its application in ad hoc document retrieval, indexing and key phrase generation [27, 26].

We use supervised machine learning to build our models of entity salience, a method that has been used widely for vari-ous tasks in web document processing. Machine learning of-fers a principled way to calibrate signals from heterogeneous sources, which is crucial when incorporating diverse (e.g. document content, term-weighting, web graph) insights into one system. A wide variety of tactics are employed in the literature to overcome the bottleneck of acquiring supervi-sion data, for a theoretical perspective on these approaches see [31]. One well studied approach to obtain relevance-related supervision for web document training data is the use of web search logs: the click behavior that is recorded in these logs can serve as implicit user feedback and hence indi-cate relevance of a document to a user. This signal has been exploited for relevance annotation in document retrieval sys-tems [14, 15, 9, 24], for other web document tasks beyond retrieval, i.e., [29], [16], [22], and [13]. In our system, we ex-ploit web search logs by designing a soft labeling function for entity salience that is based on user behavior information.
What constitutes an entity has been cause of many philo-sophical debates. For our purposes, we consider something an entity if it is of a type that has or reasonably could have a Wikipedia page associated with it. This would include people, places, companies, as well as events, concepts, and famous dates.

We consider the following working assumptions in building our entity salience ranking system:
Entity salience is distinct from two other aspects of about-ness: entity importance and entity relevance . The impor-tance of an entity refers to its influence or substantiveness outside of the scope of the document. For example, although Barack Obama is a very important entity, he can be periph-eral to some news stories. On the other hand, the relevance of an entity is inherently subjective to the reader X  X  perspec-tive and intent.

Although local scoping suggests that the evidence for en-tity salience can be derived most effectively from the docu-ment content, it is important to note that extra-document information such as incoming anchor links and user click-through data provide important signal, and will be leveraged by our models. Also, by assuming the source of salience to be local to a document, we limit the search space to those entities in the document.
We conducted a small manual inspection of web pages in order to get a first perspective at the difficulty and scope of our problem. We sampled 50 documents, randomly chosen from a traffic-weighted sample of documents from a com-mercial web search index. We examined the content of the pages in a web browser and made a list of all entities and their salience.

On average, fewer than 5% of the entities in each docu-ment were deemed salient. We observed certain cues when identifying salience. Unsurprisingly, salient entities tend to be mentioned in the title, headings, and/or first paragraph, and are frequently mentioned.

By our local scoping assumption, any salient entity is con-tained in its document. Hence, a system that is capable of identifying each entity in a document would serve as a can-didate generator for a salience ranking system. We ran a proprietary state-of-the-art NER system, trained using the perceptron algorithm [4], on the content of the web docu-ments. We then compared for each page the set of automat-ically identified entities to the human annotation.
We found that in 91% of the documents, at least one of the salient entities is in the candidate entity set identified by our NER tagger. For over 90% of these pages, all the human annotated salient entities are captured by our NER engine. Therefore it is reasonable to use the NER system as a candidate generator.

We next examined whether simple cues for entity salience are so straightforward that a heuristic would suffice to iden-tify them. We observed many cases where cues were not reliable or conflicted with each other, making heuristic de-sign a difficult proposition. For example, the presence of an entity in a title string is often a good indicator for salience. However, being included in the title (or in the first para-graph) is neither a necessary nor a sufficient condition for salience. Based on these observations, we believe that a machine learned model that can combine evidence from a multitude of signals is a better approach than developing simple heuristics.
Let D and E be the sets of all documents and entities on the web, respectively. Let E d  X  E be the set of entities mentioned in d  X  D . We formally define the aboutness task as learning the function: where  X  ( d,e ) reflects the salience of e in d 1 . We denote the ranking of E d according to  X  as: where pairs of entities with tied scores are ordered randomly. We define the ranking function such that R  X  ( d,e ) equals the rank of e in R S d 2 .
Instead of manually labeled data we rely on a soft labeling approach that uses behavioral signals from web users as a proxy for salience annotation. Individual clicks in a web search log from a commercial search engine indicate a user X  X  interest in a URL based on their entity query, i.e., they indicate the relevance of the entity in the URL to the user. In aggregate, the combined interests for an entity/URL pair will correlate with the entity being salient, since users are less likely to search for an entity and then examine a page that is not about that entity. This  X  X oft label X  is available for pages that receive enough traffic to derive reliable user click statistics, but the learned model uses features that are independent of user behavior, hence it can generalize to the tail of the distribution.

A simple click measure is Clickthrough Rate (CTR), i.e. the rate at which users click on a URL given a query. Paran-jpe [21] points out that CTR is very much biased towards the top-ranked result on the SERP which tends to receive the bulk of user clicks. Instead, they propose to use Click At-tractivity (CA) as a search log based metric that correlates with salience. CA for a term t and document d is defined as: where clicks ( t,d ) is the number of times users clicked on d for a query containing t , and skips ( t,d ) is the number of times users clicked on another document d 0 that is ranked at a lower position than d , where d is in the top-5 results. Both clicks and skips are aggregated over all queries that
We fix  X  ( d,e ) = 0 for all e /  X  E d .
R  X  ( d,e ) is not defined for e /  X  E d . contain t and lead to at least 32 instances where document d is displayed in the SERP. In its original setting, CA was used for any term t in a document. For this paper, only terms that are entities are considered, i.e., t = e .
CA has the following problems, though: First, recency can trump salience. Assume that entity e is involved in some re-cent gossip news. The user will be most interested in the lat-est gossip about e (which provides a good signal for salience) but will hardly ever click on the IMDB or Wikipedia page for e , although on these pages e is very salient. Second, popularity can trump salience. Within a set of URLs that are equally about e , some of the sites might be more pop-ular than others (e.g., a celebrity home page will be more popular than a page about her maintained by a fan.) This will distort the CA score. Finally, CA is subject to position bias similarly to CTR. If the user is generally more likely to click on a URL in the top position, this also means that she is less likely to skip that top position and hence that CA is also influenced by position bias.

We propose a different soft labeling function that aggre-gates over only the queries that lead to clicks on a URL without taking the number of views (CTR) or the number of skips (CA) into account. We define Entity Query Ratio (EQR) for entity e and document d by looking at all queries that lead to a click on d . Within that set of queries, we calcu-late the ratio of the number of clicks from queries containing e to the number of clicks from all queries. We define con-taining here as an exact match between an entity string and a query. In our experiments, this strict matching definition performed better than a substring-based definition. where Q is the set of all queries and clicks ( e,d ) is rede-fined as the number of times users clicked on d for a query matching e .
We represent each entity/document pair  X  e,d  X  as a vec-tor of features. At the highest level, there are three distinct classes of features: (1) those that are computed from proper-ties of e and the whole document collection D , labeled F (2) those that are solely computed from properties of d , la-beled F d ; and (3) those that are computed from properties of e in d , labeled F e,d . Document features, F d , further sub-divide into categorical features representing the page classi-fication of d , features of the document URL, and length fea-tures. Entity/document features, F e,d , are subcategorized into structural features that relate e to the structure of d , web graph features that indicate the frequency of e in inlinks and outlinks, position features that capture the location of e in d , and finally features that capture the frequency of e in 17 different page segments that are automatically identi-fied based on visual properties (see [3] for more details and [28] for other work that used visual blocks as input). Novel features considered in this paper include: Page classification and segmentation features, more detailed position features, and corpus features based on an offline corpus of documents in the top domain of d .
 We use regression and ranking learning to model CA and EQR. We employ boosted decision trees [6] as our learning algorithm. The hyperparameters are the number of itera-tions, learning rate, minimum instances in leaf nodes, and the number of leaves. The parameter tuning procedure is described in Section 6.1.
Let  X  be a graded relevance scoring function for a docu-ment d and entity e : where for  X  ( d,e ): We define a test set T = {  X ,  X  } where  X  = { ( d, E d ) : d  X  D , E d  X  E } is a collection of pairs of web pages and entities for which we have a gold standard  X  .

We start by constructing a universe of web pages by min-ing all the shared URLs on the full firehose of Twitter.com during May 2012 (to ensure that we focus on URLs that are actively shared and discussed). This set was narrowed down by eliminating: (1) any URL that redirected to a query on a search engine, (2) YouTube.com links (since salient entities here are often trivial to identify), and (3) URLs that re-ceived fewer than three clicks within six months. The final set consists of over half a million URLs, for which we have access to a full crawl of the content.

From this set of web pages, we produce 2414 manually annotated test cases for our experiments, spanning two test sets outlined below. Each test set consists of randomly sam-pled web pages such that each page contains fifty or fewer en-tities to facilitate manual annotation. The first set, labeled HEAD , consists of a traffic-weighted random sample of web pages from our universe of URLs, where the traffic weights are estimated using the number of clicks each URL received during a six month period. This set represents the head dis-tribution of our URLs. The second test set, TAIL , consists of a uniform random sample of web pages from our universe of URLs. This set represents the long tail of the web.
For each web page in our test sets, we built the set of entity mentions by running the Named Entity Recognizer, described in Section 3.2, on the content of each page. There are 1228 candidate entities in the HEAD set and 1186 in the TAIL set. To complete HEAD and TAIL , we construct gold standard relevance assessments,  X  , for each entity-document pair. We used a crowdsourcing tool to collect relevance judg-ments ( MS , LS , or NS ) from non-expert paid judges. For each entity-document pair, we requested five judgments. We removed all judgments from bad judges, which were identi-fied as those whose mean judgment score was further than two standard deviations from the mean of all judges. This resulted in the removal of four judges for HEAD and seven for TAIL . The task had fair agreement for both test sets, with a Fleiss X   X  score of 0.29 on HEAD and 0.25 on TAIL . Three expert judges then adjudicated the majority vote for each entity-document pair.

The HEAD and TAIL test sets along with their gold standard annotations are available at http://research.microsoft.com /research/downloads/details/5a2ddfde-83f7-4962-9ad7-d80 cd5098f38/details.aspx.
To assess the quality of a salience function  X  on a test set T , we compute the aggregate performance against the salience judgements given by the human judges. We consider two types of applications. First, rank-sensitive applications, such as those deriving relevance features for a search ranking function, require the top-K most salient entities. For these, classic IR metrics such as nDCG (normalized discounted cumulative gain) and MAP (mean average precision) are applicable [19]. Second, in class-sensitive applications, such as highlighting the salient entities on a document, we require all the salient entities on the page. For this class of applica-tions, Precision , Recall , and F1 metrics are applicable.
Below we define nDCG and MAP with respect to a sys-tem  X  , its corresponding ranking function R  X  (Eq. 2), and test set T . where  X  tri ( d,e r ) maps the relevance score of e r real-valued score ( MS  X  1 . 0, LS  X  0 . 5, NS  X  0) and IDCG ( d, E d ) is the ideal DCG if E d was perfectly ranked. where R  X  d [ 1 , r ] = { e 1 ,...,e r | e i  X  R  X  d } ,  X  the entity at rank r is salient or not in d , and: Recall and F1 follow trivially. We first ran our NER system on the content in our Web Page Data, discarding those pages in our HEAD and TAIL test sets, and associated with these pages all queries from the US English market of Bing.com that led to a click on the pages during a six month period. We computed the CA and EQR scores for each entity. Many entity-URL pairs receive a zero score because no query mentioning the entity leads to any click on the URL. Although such an entity-URL pair could in fact be salient (even with six months of web search log data, there is sparsity in the tail), in most cases the pair is non-salient. In our experiments, we tried configura-tions that included all zero-scoring entity-URL pairs, none of them, and balancing the number of zero-scoring pairs to be equal to the number of non-zero-scoring pairs via random sampling. The balanced configurations consistently and by a large margin outperformed the others, and hereon we con-sider only balanced configurations. For the EQR soft label, our final training set contains 66,055 entity-URL pairs; for the CA soft label the number of entity-URL pairs in the training set is 48,759 3 .
This discrepancy in number of training cases is due to the fact that we only compute the CA label for documents in the top 5 displayed search results, to keep the CA signal sufficiently reliable.
For each soft-labeled entity-URL pair, we computed the features described in Section 4.3. We used the Bing search engine to compute features that require web graph data or page classification. To set the hyperparameters of our re-gression and ranking models from Section 4.3, we perform a sweep of 144 combinations of parameter settings on a three-fold cross validation, for each system configuration.
Each system that we train and evaluate consists of three choices: soft labeling method (CA vs. EQR), feature set, and model type (regression and ranking).

We consider the following five baselines against which to test our systems: We report our results on the following system configurations:
Table 1 lists the performance of our baseline and sys-tem configurations on both the HEAD and TAIL datasets. We report nDCG and MAP scores (at 1 and 5) and F1. EQR ALL and EQR ALL RANK , our best configura-tions, significantly outperform the soft labeling baselines, on both HEAD and TAIL , by 37% and 51% on F1, respec-tively. On TAIL , we improve on the previous state of the art, CA PJP , significantly on all metrics, by 16% on F1. On HEAD , we show significant improvement over CA PJP in the first position on both nDCG and MAP.

In general, the HEAD is  X  X asier X  than the TAIL : Absolute metrics are higher, and the choice of feature sets and soft labeling function matters less. This is not surprising for two reasons: (1) the soft label signal is reliable only in the head but it is extremely sparse in the tail; and (2) the head is represented dominantly in the training data. As [21] points out, the strategy behind learning a salience model from a soft label is to learn from the cases where we have a good super-vision signal and to generalize to the cases in the tail. Given this argument, our expectation was to see gains mostly in the tail for our proposed soft labeling function and feature set EQR ALL . The positive gains on the HEAD were unex-pected. The choice of the soft labeling function is important: On TAIL , EQR outperforms CA overall as a training signal. On HEAD , the soft labeling technique matters less; using our full feature set, both techniques yield similar performance except on F1 where EQR outweighs CA. Using our rank models, we observe par performance against the regression models on HEAD . On TAIL , the rank models outperform re-gression in the first position on nDCG and MAP.

Examining the precision/recall characteristics of the sys-tems, we found that the TFIDF features underperform com-pared to the other feature sets in all settings. The PJP fea-ture set improves precision/recall in all cases against TFIDF, but in a more pronounced fashion when used with the EQR soft labels. The best precision/recall curves are obtained from EQR ALL . The best system produces precision/recall gains especially in the region where precision is greater than 0.7. At recall  X  0 . 6 the precision gain on HEAD is nearly 7.5 points, on tail it is nearly 10 points at  X  0 . 5 recall.
Examination of the feature weights in EQR ALL reveals that the strongest salience cues are the position and the fre-quency of the entity in the document and anchor text. In the model, 174 features receive non-zero weights. The top five features are: the frequency of e in the anchor text, document and title, and the df of e and offset of e in the document. The next series of 37 features in order of feature weight is a mix of page classification, position, URL, structural and page segmentation features with no discernible prominence of any of these families. The binary features representing top level domains and page categories occur in the lower weight area of the feature list, with the exception of the feature in-dicating that the top level domain is Wikipedia -this feature ranks 11 th which is not surprising given the frequency and highly specific structure of this domain. We also performed feature ablation on EQR ALL to see how well a system that does not have access to information that requires ei-ther a sizeable web crawl or components that are typically part of a commercial search engine would do. On HEAD , the difference is minimal and not statistically significant, except for F1 where EQR ALL outperforms EQR DOC .
 In TAIL , however, EQR ALL achieves better results, with significant gains in nDCG@1 and nDCG@5.
This paper formalizes and addresses the task of scoring entity-URL pairs according to the salience of the entity in the document. We propose a system that is cost-effective to build and improves upon the state of the art. We propose weakly-supervised learned models combined with a novel method for automatically labeling large quantities of train-ing data by leveraging usage behaviors found in web search logs. This, along with an extensive feature set leads to sig-nificant improvements over the current state of the art on both head and tail distributions of the web. As no public data exists to date to evaluate this task, we design and re-lease to the research community a gold standard data set with salience annotations, representing the head and tail distributions of pages on the web.

For further details, we refer the reader to [7]. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] P. D. Bruza, D. W. Song, and K. F. Wong. Aboutness [3] D. Cai, S. Yu, J. Wen, and W. Ma. Extracting content [4] M. Collins. Discriminative training methods for [5] N. N. Dalvi, R. Kumar, B. Pang, R. Ramakrishnan, [6] J. H. Friedman. Greedy function approximation: A [7] M. Gamon, T. Yano, X. Song, J. Apacible, and [8] B. Hj X rland. Towards a theory of aboutness, subject, [9] S. Holland, M. Ester, and W. Kie X ling. Preference [10] E. Hovy and C. Y. Lin. Automated text [11] A. Hulth. Improved automatic keyword extraction [12] W. Hutchins. On the problem of  X  X boutness X  in [13] U. Irmak, V. V. Brzeski, and R. Kraft. Contextual [14] T. Joachims. Optimizing search engines using [15] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and [16] M. Komachi and H. Suzuki. Minimally supervised [17] J. Kupiec, J. O. Pedersen, and F. Chen. A trainable [18] T. Landauer and S. Dumais. A solution to Plato X  X  [19] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [20] D. Marcu. From discourse structures to text [21] D. Paranjpe. Learning document aboutness from [22] M. Pa  X sca and B. V. Durme. What you seek is what [23] H. Putnam. Formalization of the concept  X  X bout X . [24] F. Radlinski and T. Joachims. Query Chains: [25] G. Salton, J. Allan, and C. Buckley. Approaches to [26] G. Salton and C. Buckley. Term-weighting approaches [27] G. Salton, A. Wong, and C. S. Yang. A vector space [28] R. Song, H. Liu, J. Wen, and W. Ma. Learning block [29] G. Xu, S. Yang, and H. Li. Named entity mining from [30] W. Yih, J. Goodman, and V. Carvalho. Finding [31] X. Zhu. Semi-Supervised Learning Literature Survey.
