 Continuing advances in data storage and communication technolo-gies have led to an explosive growth in digital music collections. To cope with their increasing scale, we need effective Music In-formation Retrieval (MIR) capabilities like tagging, concept search and clustering. Integral to MIR is a framework for modelling mu-sic documents and generating discriminative signatures for them. In this paper, we introduce a multimodal, layered learning frame-work called DM CM . Distinguished from the existing approaches that encode music as an ensemble of order-less feature vectors, our framework extracts from each music document a variety of acoustic features, and translates them into low-level encodings over the tem-poral dimension. From them, DM CM elucidates the concept dy-namics in the music document, representing them with a novel mu-sic signature scheme called S tochastic M usic C oncept H istogram ( SM CH ) that captures the probability distribution over all the concepts. Experiment results with two large music collections con-firm the advantages of the proposed framework over existing meth-ods on various MIR tasks.
 H.3.3 [ Information Search and Retrieval ]: Query formulation, Search process; H.5.5 [ Sound and Music Computing ]: Systems Algorithms, Design, Experimentation, Human Factors Music Information Retrieval, Similarity Measure, Music Concepts
The past decade has witnessed a tremendous growth in the avail-ability of digital music on various application platforms. At the same time, the pervasiveness of social media and affordability of home media servers are bringing about a fundamental change in the way people enjoy and share music. Indeed, online music dis-tribution has overtaken physical media like compact discs [2, 6, 7, 13], becoming the dominant distribution channel with consumers. These trends call for Music Information Retrieval (MIR) capabil-ities for tagging (for example to enable browsing and faceted re-trieval), searching by concepts (in a similar manner as text search), and clustering/classification (to automatically organize the music library). Underpinning these capabilities is an effective framework for modeling rich musical content, so as to generate signatures that capture the distinct characteristics of individual music documents.
While music modeling has been a long-standing research topic, substantial scope remains for further advances. To illustrate the challenges involved, consider the song  X  X ohemian Rhapsody X  by Queen -a British rock band. There are six sections in the song: in-tro, balla, guitar solo, opera, hard rock and outro. At various points, the song is performed by different singers and features different musical instruments like piano and guitar. Moreover, its tempo speeds up mid-way through. In characterizing the song, at the ac-coustic level the importance of the timbral, spectral and rhythmic features would vary over time. On a semantic level, the concepts associated with segments of the song, like  X  X ender X ,  X  X omforting X  and  X  X omantic X , also vary as shown in Figure 1 1 .

Due to those challenges, music modeling goes beyond extraction of acoustic features, to learning their inter-play and, from there, deducing semantic concepts. However, existing approaches in [28, 34, 35, 33, 17, 25, 23] simply represent a music document as a bag of audio features and directly apply machine learning techniques on the features. Surprisingly, less attention has been paid on modeling the interaction or association among musical features, subpieces and dependency between different levels of concept. Moreover, the free-patch representation ignores information about temporal dynamics and order, which has been proven to be very important for accurate music search and analysis.

In this paper, we postulate three principles for an effective mu-sic modeling framework. First, the framework should capture both low-level acoustic features and high-level concepts. Second, the acoustic features and concepts may exhibit non-linear dependen-cies. Effectively modeling the complex associations between dif-ferent concepts can be very helpful in improving system perfor-mance. Third, the characteristics of a music document are likely to vary over the temporal dimension. Building on the principles, we introduce a new D ynamic M usical C oncept M ixture ( DM CM ) framework to facilitate comprehensive music modelling. To ac-count for temporal variations, the framework splits each music doc-
The wave form is generated using Audacity. ument by time into multiple segments, and derives a two-layer model for each segment:
Thus, the music document is translated into a set of low-level audio words, and a corresponding set of high-level SM CH s. To the best of our knowledge, no prior work has adopted a similar approach as ours. To validate the effectiveness of the proposed framework, we have carried out a comprehensive set of experiment studies with two large music collections. A comparative analysis involving existing state-of-the-art methods confirms that our frame-work achieves substantial improvement in accuracy and robustness for various MIR tasks.

The rest of the paper is structured as follows: In Section 2, we review and analyze related work in the area of music signature gen-eration. Their assumptions, limitations and application domains are discussed in detail. In Section 3, we introduce our feature extrac-tion scheme and audio word generation process. Following that, Section 4 presents the proposed DCM C framework. Section 5 re-ports our experiment configuration, including test collection, eval-uation metrics and evaluation methodology. The experiment results are reported in Section 6. Finally, the article is concluded in Section 7.
Computing effective music signatures is an important but chal-lenging problem. Various research communities have proposed approaches that built upon different musical features. They in-clude textual labels for the title, performers, composers, style and symbolic representations of melody (e.g., MIDI and digital music scores). Due to space limitation, we focus on existing studies based on acoustic (content-based) features, which are the most relevant to our paper.

Music descriptor generation aims to derive effective content rep-resentation for information management or analysis applications (e.g., search, classification or tagging). While many systems exist for content-based speech recognition, there is much less effort on descriptive music feature extraction or modeling. Most of them di-rectly treat low level spectral features as music signatures. Typical examples include the system developed by Nam and Berger [24], which applies three low-level acoustic features (spectral centroid, short time energy, and zero crossing rate) for automatic music genre classification. Another example is the work by Daudet [9] which applies pruned wavelet trees to model transients inside music sig-nals. In [19], Lu et al. apply nine different audio features for audio classification. The feature set used in this study consists of MFCCs, zero crossing rates (ZCR), short time energy(STE), sub-band power distribution, brightness, bandwidth, spectrum flux (SF), band peri-odicity (BP) and noise frame ratio (NFR). Moreover, a support vec-tor machine (SVM) is used for statistical classification. Tzanetakis et al. develop MARSYAS system -an advanced infrastructure for characterizing different acoustic properties of music signals [37]. The musical features extracted by MARSYAS include timbral tex-ture, pitch content and rhythm. The features are combined linearly, and input to a SVM classifier. The study achieved a 61% classifi-cation accuracy on a small test collection.
 More recently, Li et al. propose a DWCHs scheme to calculate Daubechies wavelet coefficients of music signal [16]. The key con-jecture underlying the approach is that compared to conventional spectral analysis, a wavelet histogram technique is better able to capture both local and global temporal information inside the music signal. Their empirical results obtained on large scale test collec-tions demonstrate that due to the wavelets X  accurate summarization of the probability distribution over time and frequency domains, DWCHs outperforms MARSYAS with different machine learning classifiers for music genre classification.

The techniques described above rely on either one type of low level feature extracted using spectral analysis, or a linear concate-nation of multiple acoustic features. It has been proven that such approaches generally do not produce a comprehensive and discrim-inative representation of music sequences. This is because the hu-man auditory system senses and analyzes music sequences by in-tegrating multiple characteristics in a nonlinear fashion. Conse-quently, any single type of acoustic feature is unlikely to contain sufficient information to represent the music effectively. Further, the acoustic features extracted from a music sequence are not weighted equally in human music perception and comprehension; in other words, the human hearing system gives different responses/weights to timber, rhythm and pitch. Thus, systems that assume a linear combination of acoustic features are inherently handicapped. There is also an emerging stream of literature exemplified by Sheh et.al [27], Turnbull et.al [32] and Zhang et.al [39] that adopts statistical modeling to generate discriminative music descriptors from raw acoustic features. Shen et.al [30] develop the InMAF scheme, with a hybrid architecture that incorporates principal com-ponent analysis (PCA) and a multilayer perceptron neural network, to combine multiple musical properties in a nonlinear fashion. Ex-periment results obtained with two small data sets show its effec-tiveness and robustness against various kinds of audio alteration. Song et.al [31] propose a semi-supervised distance-based learn-ing framework to integrate music features for genre classification. Turnbull et.al [34, 35] propose a supervised multi-class labeling (SML) probabilistic scheme to generate semantic descriptors for large scale music retrieval; the framework builds upon GMMs. One of the common disadvantages suffered by the aforementioned methods is that feature vectors extracted from a music sequence are assumed to be independent and identically distributed (i.i.d.), and the music sequence is encoded as an ensemble of orderless feature vectors. Consequently, temporal dynamics in the music sequence are not accounted for properly. Motivated by the concerns, Coviello et.al [8] develop a dynamic texture model [10] to facilitate auto-matic music annotation and retrieval. It effectively accounts for not only temporal dynamics but also the timbral content in music. Ex-periment results obtained with the CAL500 dataset shows that the model improves the performance of music search and annotation significantly. In this study, we represent audio features as a set of  X  X udio words X . To compute audio words to describe music sequences, raw input signal is firstly partitioned into several segments of equal length. From each segment, we extract five different kinds of features to form the low level music representation. They include timber, spec-trum, rhythm, pitch and time as explained below.
The k -mean algorithm is applied to cluster each of the feature spaces to form audio words [11]. The values of k are preconfigured to be 40 for timbral feature, 30 for spectral features, 35 for rhyth-mic features, 35 for wavelet features and 10 for time features. The whole process can be treated as a special transformation : where c kf denotes the centroid of cluster kf after k -means clus-tering on acoustic feature f . The audio word aw is defined as a sequence of audio feature cluster centroids and its representation is given as, where c ti , c r , c sp , c w , and c t denote the cluster generated using timber feature, rhythm feature, spectral feature, wavelet feature and time feature respectively. Similar to keywords in text documents, we can construct an audio codebook CB containing R audio words, where aw r denotes audio word r in codebook CB . In general, the music segments represented by the same set of audio words share certain levels of closeness and contain similar semantic con-cepts. Because huge variations are commonly encountered in raw music signal in practice, no existing scheme is able to group all the segments into one cluster based only on the temporal or (and) acoustic appearance. Thus, we employ the k -mean algorithm due to its simplicity and efficiency.
This section introduces a novel scheme to generate effective mu-sic descriptor for searching large scale music data collections. As depicted in Figure 2, the proposed system ( DM CM ) consists of two major components: 1) music preprocessing layer and 2) music concept modeling layer. In following sections, details of the system architecture and related algorithms are presented.
The key functionality of the music preprocessing layer is to cal-culate multiple acoustic features and project them onto an audio stochastic music concept histograms ( SM CH s). word from the codebook. Upon receiving the raw music signal, our system first decomposes it into multiple short segments with equal duration for the purpose of feature extraction. The number of segments S is preconfigured as a system parameter. Then, for each of the segments, based on four different kinds of features ex-tracted, we find the most similar entry in the codebook and replace the index accordingly.

After this process, each input music is transformed into a se-quence of indices or one-dimensional codes of audio words. The music document is now similar to a text document, which is a list of keywords. Under this paradigm, each music can be re-constructed by using the codebook of audio words.
The second layer of our system contains multiple music concept profiling modules developed based on generative learning princi-ple. Each module corresponds to one music segment and outputs a stochastic music concept histogram -SM CH , describing the  X  X robabilistic association X  between the music and concepts at dif-ferent levels. The set of SM CH s generated from this layer serves as signature of the input music for different MIR or content analy-sis applications. Before reviewing the architecture of this layer, we introduce the SM CH music signature scheme.
 Figur e 3: The structure of stochastic music concept histogram -SM CH
Music concept representation is a crucial component of many music data management applications, including indexing, brows-ing, mining and retrieval. Here, we propose a unified scheme -stochastic music concept histogram ( SM CH ) as probabilistic rep-resentation of music documents. As demonstrated in Figure 3, it models the probability distribution of a music or a music segment over a set of predefined music concepts represented by audio words, where related to the music sequences, and p c represents the probability of a given music object assigned to concept c . Since each SM CH is a real-valued vector in metric feature space  X  C , the distance between two music sequences ms i and ms j can be calculated using the normalized linear metric: where msim ( S M CH i , SM CH j )  X  [0 , 1] . The distance func-tion for SM CH enjoys the following properties,
For the properties shown above, the feature space based on SM CH is a C -dimensional metric space  X  C , and each SM CH is a nu-meric vector in this space.
The main goal of the multiclass probability estimation module is to derive a probabilistic distribution between a set of predefined Algorithm 1 Generative Process to Learn Concept Mapping M . Description: 1: Obtain prior about matching via sampling an n-to-1 mapping 2: for each matched edge (n, c) in graph G , where n = 1 , ..., N 3: Sample latent concept: z c  X  N (0 , I d ) , min{ d A 4: Sample features for audio words 5:  X  A ( aw n )  X  N ( W A z c +  X  A , A ) 6: Sample features for textual words 7:  X  T ( t c )  X  N ( W T z c +  X  T , T ) 8: for each unmatched audio word n : 9:  X  A ( av n )  X  N (0 ,  X  2 I d A ) 10: for each unmatched textual word c : 11:  X  T ( t c )  X  N (0 ,  X  2 I d T ) concepts (keywords) and a given music encoded with the audio codebook. In this study, we develop a generative model to facil-itate the mapping process M based on canonical correlation analy-sis (CCA) [15].

As illustrated in Figure 4, the mappings M are treated as a weighted bipartite graph G = ( A, T, M, P ) . A and T are the feature vec-tors to describe audio words and textual keywords. For each au-dio word, we calculate the contextual feature -the co-occurrence counts of the word in neighboring music segments for audio words. Thus, we can generate a set of audio word feature vectors: A = ( av 1 , ..., av n ) , where av n  X  R d A for all n . To capture the text feature, the co-occurrence of different textual words is counted over the corpus. Hence we obtain a set of feature vectors T = ( t , ..., t C ) for C different music concept keywords. P is a set of the related probabilities (weights) estimated using the generative model. Given T and A , the learning process aims to derive an opti-mal mapping to translate words from the audio domain to a textual keyword (concept). It inputs feature matrices of the audio and tex-tual words. Algorithm 1 describes each step of the whole training process and the Figure 4 illustrates the abstract of the projection.
Different from the approach presented in [14], our algorithm samples an n-to-1 matching M from the prior mappings. We also restrict each audio word to occur only once but each text word can appear in multiple mapping pairs (line 1). The priors are assumed to be uniformly distributed. The approach underlying the learn-ing algorithm is based on the probabilistic interpretation of CCA. It can be proved that the canonical correlation must be largest for the maximum likelihood estimates. Hence, a latent concept z N (0 , I d ) can be generated for each matched edge ( n, c ) in the mapping M via sampling (line 3). I d is a d  X  d identity matrix. Given the latent concept, samples of the audio words X  feature vec-tors  X  A ( aw ) are drawn from a multidimensional Gaussian model with mean W A z c +  X  A and covariance A (line 4 and line 5). W is a d A  X  d matrix projecting concept z c to the feature vector of audio words. A similar process is applied to the textual words (line 6 and line 7). A and T are covariance matrices to measure vari-ations in two different domains. For unmatched words, we assume that the background distribution of audio and textual words can be used for mapping (line 8 -line 11).
Our training algorithm is developed on the principle of expectation-maximization (EM). It aims to find the maximum likelihood esti-mate via an iterative process. Given the statistical model introduced above, the log-likelihood of the training data is, where  X  = ( W A , W T , A , T ) is a set of model parameters. The learning algorithm consists of two main steps, M-step: The goal of M-step is to optimize the values of  X  so as to maximize the log-likelihood of all the mapping words from two different domains,
This learning goal is equivalent to maximizing the likelihood of the probabilistic KCCA model [3]. For two sets of samples, CCA seeks to find basis vectors which can (i) map the elements of the samples into the multidimensional space and (ii) maximize the cor-relation between sets of the projections. Because CCA is effective only for linear relationships, kCCA is applied to first project each data point into a multidimensional space with higher dimensional-ity, and CCA analysis is then carried out in the new feature space. Similar to kernel PCA [26], two kernels K A and K T are defined over A and T . With the kernels, the related function that we need to optimize is given by,
In order to avoid trivial learning, the process is regularized by introducing controlling the flexibility of the project. The partial least squares (PLS) is applied to penalized the norms of associated weights. Based on [4], a standard eigen problem can be obtained E-step aims to calculate the expected value (posterior) over all the possible matching pairs in the graph G . It is easy to prove that the related computing process is P-complete [38]. Since the hard EM only needs to compute the b best mapping under the current model, we have: where  X  is current model parameter estimated for kCCA. The optimization process is casted as a maximum weighted bipartite matching problem. The goal is to project the vectors in two do-mains (audio and text) onto the latent space. Our system uses the Euclidean distance function to quantify similarity -p a,t the audio word a and textual word t . Meanwhile, p a,t is inter-preted as the matching probability or edge weight in the graph G for different pairs ( a , t ). sented by textual words.
 Algorithm 2 SM CH s Computation Description: 1: Di vide the input song to S segments 2: Feature extraction over each music segment 3: Encode music with codebook 4: Calculate SM CH for each segment using music concept mod-5: Output a set of SM CH s
Using the equation, we calculate the matching probabilities for all possible mappings and find the best b pairs. Once the process is completed, we add the results into the kCCA learning examples and re-run the M-step. Our EM training procedure is similar to bootstrapping. Through iterations, the number of edges in graph G increases gradually. When the EM process stops, the matching probabilities obtained are used to compute SM CH .
The goal of DM CM is to derive a set of SM CH s to serve as signature of a given music. The SM CH s can be applied in different MIR tasks. Using the system architecture introduced in the previous sections, we start with a training database where the codebook of audio words is generated. In the training stage, we process the songs in the database, extract features for each music and calculate the audio word codebook. Then, using Algorithm 1 and the EM training procedure introduced above, we construct the concept dynamics modeling layer in our framework.

After the training phase, we proceed to compute the music sig-natures. The basic procedure is shown in Algorithm 2 and consists of five main steps. For a given music item, the system initially partitions the input song into S segments (line 1). After that, the feature extraction procedure generates different kinds of features using the techniques described in Section 3 (line 2). Next, we ap-ply the features to encode the input music sequence (line 3). A set of SM CH s are calculated using the music concept modeling module in the second layer, one per generative model: where SM CH s is a set of the probabilities (weights) -P learned with the generative model for segment s .
This section describes in detail our experiment configuration to facilitate large scale performance evaluation and comparison. First, an introduction of the evaluation metrics and methodology is given in Section 5.1. Next, Section 5.2 presents details of two differ-ent testbeds used in the empirical study. After that, the competing methods that are included in the performance comparison are intro-duced in Section 5.3. All the music descriptor generation schemes evaluated here have been fully implemented and tested on a Pen-tium (R) D, 3.20GHz, 1.98 GB RAM PC running the Windows XP operating system. The number of music segments in DCM C is set to 20.
Music signature generation is one of the most fundamental com-ponents in various kinds of MIR applications. In order to conduct a comprehensive performance comparison of different schemes, our proposed system and the competitors are tested and compared on three MIR related tasks. They are,
Two evaluation metrics are used for the music annotation task (Task I). They are mean per-tag precision and recall. The top 5, 10, 15, 20 and 25 tags are generated by the various systems for comparison. The per-tag recall and per-tag precision are formally given by where | t GT | is the number of songs annotated with the tags in the human-generated "ground truth" annotation and t T P is the number of music annotated correctly with the tags. A detailed explanation on how the "ground truth" information is generated will be given shortly.

To measure the performance of the various systems for music search (Task II), the mean average precision (MeanAP) and the area under the receiver operating characteristic curve (AROC) are adopted as assessment metrics. For comparing the performance of various clustering methods (Task III), the metric we used is the F-Measure F . The formula is given by where, T P is the true positive ratio, F P is the false positive ratio, and F N is the number of false negatives. In addition, to ensure result qual-ity, tenfold cross validation is used to calculate the classification and clustering accuracy. This means the whole dataset is divided into ten disjoint subsets of (approximately) equal size. For testing, we train the algorithms on nine of the ten subsets and the remaining one is used for testing, each time leaving out a different subset. The process above is repeated for each system tested in our experiment study.
The test collections play a very important role in the empirical comparison study for MIR research. To ensure accuracy and fair-ness of the empirical results, we carefully develop or select two different test collections. Their details are as follows.
Both TSI and TSII are used for testing the performance of differ-ent systems. TSII is applied for studying the performance of var-ious signature generation schemes on music clustering and music search. For the purpose of acoustic feature extraction, we extract the audio tracks and convert them to 22050Hz, 16-bit, mono MP3 audio documents to ensure recording quality. The average length of the music clips is 120 seconds, the maximum length is 200 seconds and the shortest is about 100 seconds.
For task I (music tagging), we compare the performance of our system against three state-of-the-art approaches including MMTag-ger [29], Autotagger [12, 5] and MSML [35, 34]. Acoustic fea-ture considered by MSML is Mel-frequency cepstral coefficient (MFCC). Autotagger is evaluated based on three feature sets in-cluding MFCC delta, afeats and bfeats 2 . For MMTagger, we con-sider five low level feature configurations (timber features denoted by TF, rhythm features denoted by RF, spectral features denoted by SF, melody features denoted by MF and timber features+rhythm features+spectral features+melody features denoted by ALL.). Au-totagger(MFCC delta), Autotagger(afeats) and Autotagger(bfeats) denote Autotagger with MFCC delta, afeats and bfeats respectively. MMTagger(TF), MMTagger(SF), MMTagger(MF), MMTagger(RF), MMTagger(ALL) denote our proposed model with timbral features, spectral features, rhythmic features, melody features and the com-bination of all four musical features.

To demonstrate the effectiveness of the music annotation gener-ated by our system in search and clustering (Task II and Task III), we examine a wide range of methods for generating music descrip-tors, including CBIF 3 , InMAF [30], DWCHs [16] and MARSYAS (denoted by MAR) [36].
The music signatures generated by DM CM are applicable to a wide range of MIR applications. This section presents a set of experiment studies to test and compare the performance of different systems on three MIR tasks including music tagging, music search and music clustering. How the systems are able to perform in a noisy environment is also evaluated.
In the first study, we test the performance of various tagging sys-tems and DM CM on the music tagging task. Table 2 and 3 report the empirical results of the systems with different feature configu-rations for the two testbeds. In this study, we test MMTagger and Autotagger with five and three feature settings respectively. The size of the tag set generated is configured to 10. The first row in both tables indicate how MSML performs on the tagging task us-ing MFCC. Among the four systems tested, MSML demonstrates the lowest effectiveness. We believe there are two main reasons for the poor accuracy. Firstly, since music signals contain a rich set of acoustic features, effective music classification or annotation [35] gives a detailed description of the feature sets.
CBIF denotes the method proposed in [31]. cannot be achieved by considering only a single low-level acous-tic feature. Moreover, the system does not consider temporal dy-namics inside the music signal. Compared to MSML, Autotag-ger demonstrates a performance improvement though the accuracy gain is very limited. It is interesting to observe that MMTagger(SF) and MMTagger(MF) based on our learning framework could suf-fer from lower accuracies than Autotagger with certain acoustic feature configurations. This suggests the importance of music sig-nature quality. On the other hand, the performance of MMTag-ger(ALL) is better than Autotagger using any feature combinations. A significant effectiveness gain ranging from 5% to 15% can be observed as different features are gradually integrated. The results clearly demonstrate that it is critical to combine features properly to achieve tagging effectiveness. The results obtained with both datasets clearly show that for the music tagging task, DM CM significantly outperforms all the existing systems. For example, Table 2 shows that in comparison to MMTagger(ALL), our system achieves a precision gain of 0.351 to 0.413 for the CAL500 dataset, and 0.327 to 0.352 for the TSII collection. We also obtain similar improvements with the other two evaluation metrics.
 T able 2: Comparison of tagging accuracy on test collection CAL500(TSI).

Effective music retrieval is important in coping with the fast growth of online music data. The next study is to evaluate the performance of DM CM and the competitors on the music search task. For a given music query, we use different methods to ex-tract descriptors (feature vectors) and the system being evaluated retrieves a list of music pieces from the database that are most simi-lar; the similarity is quantified by the distance between two descrip-tors with the Euclidean ( l 2 ) distance function. We use MeanAP and MeanAROC as metrics to compare the rank list.

One of our key hypotheses for this study is that the more discrim-inative the information that is packed into the music signature, the more accurate the retrieval results will be. The experiments here are intended to validate the hypothesis. Table 4 summarizes the experiment results with TSII. It is clearly shown that MARSYAS which combines the feature vectors linearly demonstrates the worst performance on both metrics. Meanwhile, because DWCHs cap-tures only low level acoustic musical properties, the related perfor-mance improvement is very marginal. Compared to DWCHs, both InMAF and CBIF take into account multiple kinds of acoustic fea-tures, which brings about substantial improvement in search effec-tiveness. Specifically, we observe an increase of at least 20.1% in MeanAP and 23.4% in MeanAROC. The results also show clearly that DM CM leads to much better performance than the competi-tors, delivering around 10.9% and 23.3% improvement in MeanAP over CBIF and InMAF. The findings suggest that it is impossible to achieve accurate MIR without effectively combining musical fea-tures.
 T able 4: Music retrieval accuracy comparison on test collection TSII.
 T able 5: Music clustering accuracy comparison on test collec-tion TSII (Task -Artist based clustering).
 T able 6: Music retrieval robustness comparison on test collec-tion TSII. Noise type -50% volume amplification.
Accurate music clustering process has become increasingly im-portant for different MIR applications. Our main objective in the third study is to examine the accuracy of clustering based on the music descriptors generated by DM CM versus the other approaches. We perform the performance comparison with artist based music clustering.

Table 5 shows how the various systems perform on the cluster-ing task. Clearly, our proposed DM CM significantly outperforms all the other approaches in effectiveness. In particular, the results reveal that DM CM enjoys an 21% increase in F-Measure over T able 7: Music retrieval robustness comparison on test collec-tion TSII. Noise type -50% volume deamplification.
 T able 8: Music retrieval robustness comparison on test collec-tion TSII. Noise type -10 second cropping.
 T able 9: Music retrieval robustness comparison on test collec-tion TSII. Noise type -35dB SNR mean background noise. T able 10: Music retrieval robustness comparison on test collec-tion TSII. Noise type -35dB SNR white background noise. CBIF. The gain of DM CM over InMAF is even more pronounced, averaging around 24%. We believe the performance gains arise be-cause DM CM generates an effective combination of information on different acoustic features and musical dynamics, resulting in discriminative and informative signatures for the music signals.
The human hearing system possesses the robust capability to sense and identify music even in a noisy environment. This ca-pability is very useful for real life MIR applications, where the music signals may be mixed with noise signals. Typical examples include music recorded at live concerts or at other outdoor environ-ments. However, very few existing schemes are designed to deal with inputs that are accompanied by distortions. Thus it is impor-tant for us to evaluate the robustness of the various systems against noises. We also wish to examine how different types of audio noise may influence the music retrieval process supported by DM CM and other music signature generation methods. For this study, we pollute each query music with different kinds of audio distortions. Then we carry out experiments to test the corresponding retrieval performance of our system and the competitors. We run same set of tests in Section 6.2 on TSII. In this study, we consider five different distortion cases including 50% volume amplification, 50% volume deamplification, 10 second cropping, 35dB SNR mean background noise and 35dB SNR white background noise 4 .

Tables 6-10 summarize the search accuracy of the five systems for the various audio distortion cases. CL and NO denote the search accuracies obtained under clean input and noisy input, respectively. DR is the search accuracy drop ratio between clean music input and input containing noise. From the results, we observe that gen-erally all the systems suffer certain levels of accuracy loss with noisy input. However, DM CM performs more robustly than the competitors over all the noise cases. For example, DM CM  X  X  MeanAP experiences a 10.2% drop when the music inputs are pol-luted with 50% volume amplification. In comparison, the MeanAP ratio of InMAF and CBIF decrease about 14.2% and 13.5% re-spectively. Moreover, in the case of 35dB SNR mean background noise, DM CM only loses 10.1% in MeanAROC whereas perfor-mance degradation of 15.7% and 14.2% are observed for InMAF and CBIF. For MARSYAS and DWCHs, the performance gaps are even much wider. Thus, we conclude that DM CM emerges as the more robust scheme against noise and acoustic distortion.
As an enabling technology for large scale MIR, music signature generation has received a lot of attention in recent years, with many different proposed approaches. Notwithstanding that, the technol-ogy is still in its infancy as the reported effectiveness are existing schemes are generally poor. The main reasons for this stagnation include 1) the lack of advanced techniques to intelligently combine multimodal and temporal information and 2) the unavailability of a comprehensive classification scheme to systematically bridge the gap between different levels of semantics. In this paper, we report a novel framework called DM CM that incorporates an advanced feature extraction scheme and a composite system architecture for comprehensive music signature generation. Our system architec-ture contains two basic modules -1) music preprocessing module and 2) concept dynamics modelling module in the form of an ad-vanced two-layer classification scheme. A comprehensive empiri-
W e use SN R dB = 10 log 10 Si N o to calculate the signal-to-noise ratio SN R dB , where Si denotes the signal power, and N o denotes the noise power in dB. cal study involving two large music collections has been conducted to compare our framework with competing methods on various MIR tasks. The experiment results show that our framework leads to significant improvements in accuracy, robustness and scalability on the MIR tasks.

The current study can be extended in several directions for fur-ther investigation: One of biggest advangtages enjoyed by DM CM is the use of multiple feature about both acoustic characteristics and temporal dynamics from music. This results in more compre-hensive statistical models and hence a better modelling effective-ness. One question naturally raised is how much each of these fac-tors contributes towards accuracy and robustness improvement. In the future, we plan to have a detailed study. Further, applying the method on data from other application domains would be also very interesting.
