 Semistructured data provides the users of a community-based information system with the flexibility to store in-formation without having to adhere to any predefined, rigid schema. However, such flexibility needs to be used with cau-tion as it can lead to a very heterogeneous data structure and is therefore not feasible in terms of unified data access and search functionality. We present an approach which avoids such proliferation of substructures and provides the inser t-ing user with recommendations, which are responsible for the creation of a commonly used structure. The presented recommendation algorithm adapts the recommendations to the stored information and its structure created by the com-munity.
 H.3.5 [ Storage and Retrieval ]: Online Information Ser-vices; H.4.m [ Information Systems ]: Miscellaneous Algorithms, Design, Human Factors, Experimentation Semistructured Data, Recommendations, Structure, Human Interaction, RDF
Storing information in an online, collaborative system is often realized by wiki systems. These systems provide means to easily add, modify and delete information, which does not have to adhere to any predefined schema or structure. In contrast, traditional (relational) databases are stric tly-structured. They would not be able to cope with such a large amount of collaboratively created information with very heterogeneous structures and schemata like for exampl e Wikipedia does. Nevertheless, strictly-structured datab ases provide the big advantage of structured access, which en-ables complex query capabilities. Traditional wiki system s only support full-text search which is not feasible for com-plex queries such as  X  X hich Austrian cities have more than 10.000 inhabitants and have a female mayor who has a doc-toral degree? X . Weikum et. al. [10] observed that informa-tion systems have to be able to support both structured and unstructured data to combine the advantages of both worlds and be able to answer such questions.
 Semistructured data provides mechanisms for the combina-tion of both unstructured and structured storage of data. Such semistructured data features a structure without hav-ing to specify a fixed schema. The most popular example is RDF, which consists of triples containing a subject , a prop-erty and a value . Infoboxes within Wikipedia articles are perfect examples of such semistructured data, which can als o be extracted as RDF triples. These infoboxes are manually created, tabular aggregations of the most important facts within an article and consist of multiple properties and ac-cording values. For example an infobox about New York City contains the property-value pairs area metropolis: 468.9 sq mi and elevation: 33 ft . These property-value pairs  X  together with the article URL itself  X  constitute RDF-triples. Such triples are machine-processable, thus a l-lowing for structured access and structured queries. The problem of collaborative semistructured information s ys-tems without any restrictions is the proliferation of subst ruc-tures and schemata. Every single user has his own view of structuring information and uses his own terminology. Fur-nas et. al. [3] showed that two people would spontaneously choose the same word for an object with a probability of less than 20%. The proliferation of structures, schemata and vocabulary results in a very heterogeneous schema and therefore impedes a common schema, which is essential for structured access and complex queries. Even Wikipedia has to cope with this problem. According to Boulain et. al. , only 35% of all edits within Wikipedia are related to con-tent, whereas all other edits are concerned with the structu re of articles to avoid proliferation. Moreover, Wu and Weld [12] showed that even schemata of template-based infoboxes  X  which are supervised and enforced by the community  X  are divergent and noisy. Wu and Weld evaluated Wikipedia infoboxes and found that 25% of all templates have less than five instances, 11% have only one instance. Addition-ally, only 46% of all attributes are used by at least 15% of template-instances. These facts imply that even with the support of a huge commited community, the proliferation of schemata within a multi-user system cannot be prevented. We present the recommendation algorithm of SnoopyDB , a novel information system, which is based on semistructured data and is able to avoid the proliferation of schemata by providing structure recommendations to the user during the insertion process.
 The remainder of the paper is organized as follows. Section 2 describes the basic Snoopy concept. Subsequently, the al-gorithm for the computation recommendations is described in Section 3. Section 4 covers the conducted evaluations. Section 5 describes related work and Section 6 concludes this paper with an outlook.
The Snoopy concept [4] offers the same flexibility as wiki systems but at the same time provides the possibility to structure information like (relational) databases. In the Snoopy concept, information about a certain subject is stor ed as a collection , similar to a wiki page. A collection consists of an arbitrary number of property-value pairs, which can be specified by the user without restrictions of any kind. To cope with the mentioned proliferation of structures in such semistructured data, the Snoopy concept pushes the part of alignment of data and schemata to the inserting user and supports the user during the insertion and alignment proces s using a self-adapting schema system and recommendations. The Snoopy concept  X  X noops X  as much information as possi-ble from the user (structure, relations, semantics, etc.), who has extensive knowledge about the subject and can therefore resolve many issues on the fly during the insertion process. One of the main parts of the concept is the structure rec-ommendation algorithm, which adapts itself to the already stored information in the system and guides the user in the alignment process to a common, homogeneous schemata.
The goal of structure recommendations is to provide the user with appropriate property recommendations when addin g property-value pairs to a certain subject. Structure rec-ommendations guide the user to apply an implicit, com-mon schema and therefore enable a homogeneous schema throughout the whole system. For the computation of these recommendations all previously entered properties are tak en into account and common substructures, which consist of multiple properties which are frequently used together on the same subject, are detected. Consider the example of a user entering information about cities. The property-valu e pairs describing the name of the city and its mayor have already been entered. Multiple other users, who already entered information about cities, also used these properti es and additionally specified the population and elevation of the respective city. Therefore, the recommendation mech-anism provides the user with recommendations concerning further properties, in this case e.g. the population and el-evation of the city. Such a recommendation system points the user to frequently used properties and  X  given the accep-tance of such recommendations  X  leads to a common schema within the subjects.
Recommendations are based on all already stored informa-and S = { S 1 , S 2 , S 3 , ...S m } the set of all subjects defined within the system. For each subject S i  X  S , the set of properties occurring within this specific subject is P S i where S S add a certain property to a certain subject can be seen as a collaborative filtering system, where for each subject S recommendations of properties are computed based on all subjects stored within the system.
The computation of recommendations is based on all pairs ( p a , p b ) where p a , p b  X  X  and both p a and p b occur within the same subject S i . These pairs are stored as rules , which have the form r = ( p a , p b , c ) and are contained in the set R . Such a rule denotes that property p a and property p b co-occur on c subjects. This way of storing pairs as rules eliminates du-plicate pairs and provides a compact and fast storage facili ty. Based on these rules, the recommended properties for a cer-tain subject S i are computed by selecting a subset C  X  P of property recommendation candidates by determining all rules which feature properties occurring on the subject S as shown in Algorithm 1. It is important to note that the computation of recommendations is an iterative process  X  as soon as a new property is added to the subject, rules are created or updated based on this new set of properties. Algorithm 1: Rec. Candidate Computation Input : P S i , R Output : set C of all recommendation candidates for S i C  X   X 
T  X   X  foreach p i  X  X  S i do end foreach ( p a , p b , c i )  X  X  do end return C
The execution of the recommendation candidate compu-tation algorithm listed in the previous section results in a set of recommendation candidates C . These candidates are basically pairs ( p b , c ) where p b is the recommended property and c is the number of co-occurrences with properties in the same subject. This probably large set C cannot be shown to the user in its entirety. Due to this fact, the use of a subset of C of about 10 properties has proven to be reasonable as it can be perceived completely by the user. To present the 10 most appropriate properties of C , a ranking has to be computed for all elements of C . Basically, the count value c which is computed for all recommendation candidates (see Algorithm 1), is used to rank each candidate. The higher the count value, the higher the frequency of occurrence of the rule and therefore the co-occurrences of properties p and p b and therefore the higher the rank of the property.
The presented recommendation algorithm is evaluated us-ing a test set based on Wikipedia infoboxes. The SnoopyDB system uses semistructured information and guides the in-serting user by recommendations. However, as SnoopyDB cannot provide a large data set yet, we used the Wikipedia infobox set, as it is very large and features similarities to data sets of a semistructured and guided information system . Wikipedia does not impose any restrictions on structure or schemata, the better part of the data adhere to homogeneous schemata which are enforced by a committed community. The schemata, which have been grown with the Wikipedia system, can be compared to recommendation based evolved schemata. However, these schemata are not completely ho-mogeneous [12] and feature a noisy collaborative style and therefore are best suited for our experiments.
 For the evaluation of this approach, we used a DBpedia dump (2010-03-16) [2] containing all 4,000,000 infobox in-stances (about 41 million triples) of the English Wikipedia page. We randomly chose 150,000 instances (10%) out of about 1.5 million instances which have at least 6 distinct properties for our test set. Based on the remaining set of 3,850,000 instances (38 million triples), we computed abou t 486 million rule instances as described in Section 3.2. The reduced rule set of 7.8 million distinct rules (about 250 MB disk space including the primary index, MySQL 5.0 Inn-oDB engine) provided a basis for all further recommenda-tion computations. We conducted a leave-one-out test on the 150,000 infobox instances by randomly choosing three properties within every infobox and removing all other prop -erties from the infobox. Based on these three properties, Algorithm 2 was applied to each subject of our test set. Algorithm 2: Test Algorithm (user simulation) P cur  X  X  p 1 , p 2 , p 3 } //remaining three properties P rem  X  X  p 4 , p 5 , . . . , p n } //removed properties
P rec  X  top 10 rec ( P cur ) //get top 10 rec. properties while (( P rec  X  X  rem ) 6 =  X  ) do end
The test algorithm computes the top-10 ranked property recommendations based on the three remaining properties. If the top-10 recommendations contain a previously removed property, the recommendation is accepted and the property is added to the set of current properties, which is then the basis for further recommendation computations. This step is repeated until the top-10 recommendation set does not contain removed properties anymore or the subject is fully reconstructed with respect to the original infobox. Proper -ties occurring on less than five subjects  X  the so-called long tail of the property distribution  X  are neither considered f or recommendations, nor for the evaluation. Based on this al-gorithm, we evaluated the following metrics on the test set. Figure 1: Reconstruction of Subjects (top-n, n =10) Reconstruction denotes to which extent an infobox instance can be reconstructed using Algorithm 2 (see Figure 1). Out of the 150,000 subjects, 16,565 subjects were fully recon-structed. Overall, 90,630 subjects were reconstructed to more than 50%, which amounts to 60% of all subjects. Precision and Recall were calculated using the first top-10 recommendation set, which is based on the remaining three properties (denoted by first ). Furthermore, the aver-age precision and average recall of all iteratively compute d top-10 recommendation sets (denoted by all ) was calcu-lated. Precision and recall are defined as precision ( P rec where P rem are the previously removed properties and P rec is the set of top-10 recommendations. We conducted our evaluations using the top-n recommendations with n = 5, 10, 15, 20 and 25 on all 150,000 subjects. The resulting re-construction , precision (first and all) and recall (first and all) values are shown in Figure 2. The results show that the re-construction rate cannot be increased by setting the number of recommendations to a value higher than 10. The higher the number of recommendations, the more rises the recall (first) value and the more decreases the precision (first). The overall recall and precision stay constant, which can be led back to the higher precision of recommendations based on subjects, which have already been reconstructed for the most part.
 Our experiments and results show that only three properties inserted by the user are sufficient to guide the user to a com-mon schema by recommendations. Even in a very large and collaboratively created data set, like our Wikipedia datas et, the algorithm is stable and features a precision of over 40% for all 800,000 top-10 recommendation sets in the test run. This means that at least 4 out of 10 recommendations are appropriate, the remaining 6 can be inappropriate or also adequate but are note used in the respective Wikipedia ar-ticle.

The presented experiments evaluate the automatic recom-mendation of structure without any user interaction. How-ever, the Snoopy concept provides additional recommenda-tions while the user is typing. Consider a user who specifies the first character of a property, e.g.  X  X  X . This informa-tion can cut down the number of suitable recommendations dramatically. Furthermore, by using a thesaurus, synonyms can be matched and a more commonly used synonym can be recommended to the user. Consider a user who enters
Figure 2: Precision, Recall and Reconstruction citizens as a property. The system recommends the usage of population and by accepting this recommendation, the user contributes to a more homogeneous schema. All these user input-based recommendations heavily increase the re-call and precision values but cannot be tested within the presented test scenario, as real interaction of a human user is required. Some preliminary results of such user-based tests can be found in [4] and show the acceptance of such recommendations and a first prototype of the concept.
Various approaches by the recommendation, data min-ing and semantic web communities are relevant for our ap-proach.
 The detection of frequent patterns within data has been studied extensively [6, 5]. The detection of substructures within semistructured information has been studied in [1, 7 ]. Such common substructures can be used for unified access. However, the structure of data is not changed or improved in any way. The computation of CF-based recommendations for web personalization based on Association Rules has been facilitated in [8]. However, our approach does not compute Association Rules based on an already existing data basis, the rules are stored during the insertion process. There are approaches aiming at the storage of semantic in-formation within wiki systems. Semantic Wikipedia [9] ex-tends the wiki markup language and introduces annotated links and attributes within wikis to increase the machine-readability of data. However, the user is not supported dur-ing the specification of this additional information in any way. The  X  X ntelligence in Wikipedia project X  [11] aims at extending infoboxes in Wikipedia. Missing attributes with in infoboxes or even missing infoboxes are detected and af-ter this step, the system attempts to complete this miss-ing information. Therefore, the project features Kylin, a self-supervised information extraction system which gath ers information from various sources. This information is then verified by explicit community feedback.
 To the best of our knowledge, SnoopyDB is the only ap-proach which has tried to contribute to a common schema within triple-like data by using recommendations already during the insertion process.
We presented a self-adapting recommendation algorithm to ensure common and homogeneous structures within col-laborative semistructured information systems. The recom -mendations are provided to the user during the insertion process and guide the user to a common schema without restricting the user or the structure of information. These recommendations are computed by detecting common sub-structures in already collaboratively created informatio n. The evaluation on a very large dataset of 4 million sub-jects showed that at least 4 out of 10 recommendations are appropriate and the algorithm is able to guide the user to a common schema after having inserted only three properties. The resulting homogeneous data contributes to a common data access and therefore enables complex queries on collab -oratively created information.
