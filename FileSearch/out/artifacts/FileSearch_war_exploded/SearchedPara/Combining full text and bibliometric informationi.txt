 1. Introduction
In an earlier paper by Glenisson, Gla  X  nzel, and Persson (2005) full text analysis and traditional biblio-metric methods were combined to improve the efficiency of the individual methods. This methodology was applied to a special issue of Scientometrics, particularly to the dedicated Scientometrics issue of selected papers presented at the 9th International Conference on Scientometrics and Informetrics held in
Beijing (China) on August 25 X 29 2003. The study was based on 19 selected papers that could readily be assigned to five categories, namely to the category of Mathematical models , the category of Advances in Scientometrics devoted to methodological papers, the category of Policy relevant contributions , the category
Webometrics/Informetrics and finally the category of contributions devoted to bibliometric studies of Col-laboration in science . The latter was created to represent the material of the 4th COLLNET Meeting organ-ised as post-event of the Beijing Conference. This category proved X  X s expected X  X ather heterogeneous.
The described subject classification was used by the guest-editors of the special issue to organise, structure and present the otherwise broad and heterogeneous material of the conference in an adequate manner, but it was also used for validation purposes in the above-mentioned study. The outcomes have shown that such hybrid methodology can be applied to both research evaluation and information retrieval. Because of the limited number of papers underlying the study, the paper, however, has to be considered a pilot study that is to be extended in the present study on the basis of a larger set of papers. For this extension, the authors have chosen the complete publication year 2003, i.e., vols. 56 X 58 of the journal Scientometrics. The cate-gory scheme used for the Chinese proceedings issue was tailor-made to represent the scope of the confer-ence. The 2003 issues of Scientometrics, however, represent a research profile that somewhat deviates from that special issue. Among these issues of vols. 56 X 58, we find, for instance, part of the proceedings of the 7th
International Conference on Science and Technology Indicators held in Karlsruhe (Germany) in 2002 the special issue on the 4th Triple Helix Conference in Copenhagen (Denmark) in 2002 not represent the mainstream of bibliometric research. This was the reason why we have adopted a modified lowing methodological section.

The idea of studying the full text of scientific literature by means of mathematical statistics, and combin-ing these tools with bibliometric methods is not new. Mullins, Snizek and Oehler (1988; Snizek, Oehler, &amp;
Mullins, 1991) began studying structural and textual characteristics of a scientific paper already fifteen years ago. Also the idea of combining bibliometric methods with the full text analysis of a scientific paper is not new. It has its roots in modern co-word analysis developed by Callon for purposes of evaluating re-search (e.g., Callon, Courtial, &amp; Laville, 1991 ). Braam, Moed, and Van Raan (1991) suggested combining co-citation with word analysis in the context of evaluative bibliometrics to improve efficiency of co-citation clustering. The word analysis by Braam et al. used publication  X  X  X ord-profiles X  X  that were based on indexing terms and classification codes. Not much later, Noyons and Van Raan (1994) and Zitt and Bassecoulard (1994) demonstrated the appeal of plunging into contents by using keywords from both patent X  X nd scien-tific literature to characterise the science-technology linkage. Many of these early studies were based on descriptors such as indexing terms, subject headings or keywords extracted from titles and/or abstracts.
The full text analysis applied in the study by Glenisson et al. (2005) was also supplemented by the above-mentioned traditional approach based on indexing terms extracted from titles and abstracts. The authors obtained X  X o to speak X  X s a by-product a direct comparison between the power of full text and title/ab-stract based text analysis.

The bibliometric part of the earlier study was restricted to simple statistical functions obtained from the papers  X  reference lists, particularly the mean reference age and the share of references to serial literature.
Taking the outcomes of the previous papers and the opportunities offered by a medium-sized set of under-lying papers into account, the following research questions will be answered:
What structures in bibliometric research as represented by the journal Scientometrics can be revealed with the help of text-mining methods?
Are titles and abstracts descriptive enough to allow acceptable analyses based on terms or is full text to be preferred?
Can bibliometric measures be assumed to reflect formal characteristics of documented scientific commu-nication that might supplement results obtained from content-based analyses?
In how far can text-mining extend, improve and explain structures found on basis of bibliometric methods?
In particular, this study aims at analysing in how far the cognitive structure of contemporary bibliomet-rics and informetrics is reflected by coherent clusters found in a representative selection of papers, in how far these clusters might have adequate bibliometric characteristics and in how far the two methods can sup-plement each other. 2. Methods 2.1. Text representation
We adopted the common vector space model to encode a document in a k -dimensional term space where each component w ij represents the weight of term t j in document d is called a vocabulary or thesaurus.

The TF X  X DF term weighting scheme is defined as follows: total number of documents and n j is the number of documents containing term t arithm is called inverse document frequency (IDF).

We express similarity between pairs of documents d i 1 and d responding vector representations as introduced by Salton and McGill (1986) :
The underlying hypothesis states that high similarity equals strong relevance (see Baeza-Yates &amp; Ribe-iro-Neto, 1999 ). We can rewrite the TF-IDF scheme as a transformation of the n  X  m document-term ma-trix A containing the raw counts: A = PA tf idf Q , where P is a n  X  n diagonal matrix with normalization constants for each document and Q an m  X  m diagonal matrix holding the inverse IDF  X  s for each term. A is placed on the left side of the equation to draw the analogy with the LSI formulation described below.
 To find the major associative patterns of word usage in the document collection and to get rid of the
 X  noise variability  X  in it, we used a transformation of A based on Latent Semantic Indexing (LSI). Here we assume that there is some underlying or latent structure in the word usage that is partially obscured by variability in word choice. By means of a Singular Value Decomposition (SVD) we compose another matrix A k that is an  X  optimal  X  approximation of A , but with rank k much lower than the column or row dimension of A ( Berry, Dumais, &amp; O  X  Brien, 1995; Deerwester, Dumais, Furnas, Landauer, &amp; Harshman, 1990 ). More specifically, the SVD of the n  X  m document-term matrix is written as where n  X  n and m  X  m matrices respectively. The best rank k approximation of A is defined by A
U and V k the first k columns of U and V , and R k the k  X  k diagonal matrix containing the k largest singular values of A .

Choosing a rank k from a screeplot (see Fig. 1 ) that models the semantic structure of a collection in an optimal way remains an open question and is governed mostly by empirical testing ( Berry et al., 1995 ). The interesting effect of Latent Semantic Indexing is the fact that synonyms or different term combinations describing the same concept will be mapped onto the same factor based on the common context in which they generally appear X  X ven for terms that do not co-occur in any document. Besides the implicit relating of synonyms, also the problem of polysemy is partly addressed by LSI. Other concept indexing methods exist, but are outside the scope of this investigation.

The indexing process itself is carried out using an in-house adaptation of the Jakarta Lucene indexing platform, which is a high-performance, open source, full-featured text search engine library written entirely in Java. 5 2.2. Preprocessing
As in most data mining endeavours, data preprocessing, acquisition, and cleansing jointly represent up to 80% of the overall effort distribution. Preprocessing steps include the removal of stopwords and author names, stemming and the detection of phrases. Stopword removal is the process of eliminating words that have little or no semantic value, such as articles and prepositions. Author names were extracted from the references and removed from the vocabulary so to eliminate author co-citation influences in the clustering process. Stemming involves the removal of word suffixes such as plurals, verb tenses and deflections, and the replacement by their canonized equivalent. A popular stemmer for English is the Porter stemmer ( Por-ter, 1980 ), which uses a simple rule-based scheme to process the most common English words. The dimen-sionality of the vector space is hereby reduced, but a disadvantage is the lesser value of stems for interpretation purposes. Bigrams, or phrases composed of two words, were detected using the Dunning likelihood ratio test ( Dunning, 1993; Manning &amp; Schutze, 2000 ). The 500 bigrams that scored best accord-ing to this test were selected, and subsequently verified manually. After elimination of words that occur only in a single document, we finally combined the withheld words and phrases into a final thesaurus by means of which all documents were indexed. Salton  X  s cosine on the subsequent LSI matrix was used as a basis for distance calculation. 2.3. Overall framework
An overview of the subsequent text-based and bibliometric analysis is presented in Fig. 2 : we cluster the documents under consideration using a hierarchical method and compare these results with the expert category assignments as well as with a bibliometric analysis. To aid the extraction of underlying themes from the resulting clusters, we perform a co-word analysis on top-scoring terms from each cluster.
The resulting term networks are mainly intended to provide a qualitative rather than quantitative way of ing out and displaying interactive representations of biological graphs, is used for visualizing the networks.
Two different types of edge weights exist in our term network. An edge from the large central node (red) to a term is weighted according to the importance of the term in the cluster. Intuitively, this weighting scheme forms imaginary concentric circles around the central node with more important terms appearing on a cir-cle with smaller radius. When two terms in a network are linked by an edge, they both co-occur in the same document of the corresponding cluster, but within a given lexical distance, set to 8.

Due to the lack of ground truth and the difficulties to define a crisp categorization, we will provide an in-depth analysis how bibliometric, text-based and expert category information provide different views on the thematic structure of the document collection. Moreover, we compare the outcome of using full text infor-mation to similar approaches based on Title and Abstract, and term-based information from the Refe-rences. We mention again that author names were systematically excluded from the thesauri used in our analyses. 3. Materials
As already stated in the outset, the complete publication year 2003 of the journal Scientometrics has been selected. This publication year comprises vols. 56 X 58 with three issues each. Only papers have been selected the organisation of which is basically in line with that of a research article. Letter to the editor, items on individuals, news items, editorial material and reviews have therefore been omitted. Our assign-ment is not perfectly in keeping with that used by the Institute for Scientific Information (ISI X  X homson
Scientific, Philadelphia, PA, USA), as in three cases the assignment differs. Altogether 85 papers could thus be selected.

We have already mentioned in the introduction that we have adopted a modified version of the classi-fication used by Schoepflin and Gla  X  nzel (2001) . In particular, the classification scheme used in previous work by Glenisson et al. (2005) proved imperfect since, above all, the heterogeneous category Collaboration in science , used because of the COLLNET workshop, proved unnecessary for the 2003 data. The study by
Schoepflin and Gla  X  nzel aimed at monitoring and characterising structural changes in the research profile in bibliometrics in the period 1980 X 1997. The authors created five categories, Mathematical models/informetric laws, Case studies, Advances in Scientometrics, Indicator engineering, Sociological approaches and Policy rele-vant issues . The term Webometrics did not yet appear in this scheme since at that time it was not yet esta-blished as a sub-discipline of scientometrics/informetrics. Allowing for structural changes we came up with the scheme in Table 1 .
 Besides the introduction of the new category I the only change in the scheme suggested by Schoepflin and
Gla  X  nzel combines the former categories Empirical papers/Case studies and Indicator engineering/Data pre-sentation to the new one denoted by E. According to the study by Schoepflin and Gla  X  nzel mainly the share of empirical and methodological papers increased between 1980 and 1997. In particular, the share of cate-gory A grew from roughly 1/5 to 1/3, whereas category E increased from about 1/4 to 1/2. This trend is contrasted by the decrease of political issues (P). The explanation for these trends is very simple. The dis-cussion over how bibliometric indicators could best be used as tools for science policy and research man-agement was to a large extent replaced by concrete bibliometric studies on politically relevant questions. The categories M and S remained somewhat in the periphery of bibliometric mainstream research. In the mirror of the above results we had expected a large share of papers belonging to categories A and
E. This is only in part reflected by the situation in 2003 (cfr. Table 1 ). Although the categories are clearly predominant as they make up about 2/3 of all papers, the share of P-class papers is somewhat unexpected. The question arises of whether there is a new structural change ongoing in our field. The fact that the Triple
Helix special issue does not cover bibliometric mainstream research does only partially answer this ques-tion. The answer is much more complex. The formerly by and large clear borderlines between indicator re-search, sociology of science, informetric laws and science policy become more and more fuzzy, and are gradually fading away. The case of the Lamirel paper ( Lamirel, Francois, Al Shehabi, &amp; Hoffmann, 2004 ) discussed in our pilot study might just serve as an example for this link between Informetrics/webo-metrics and mathematics. Furthermore, in several papers tailor-made methods are developed and immedi-ately applied to concrete questions in a policy-relevant context. Mathematical or statistical models are established in methodological studies. On the other hand, phenomena on the web or in information science have stimulated mathematical research. An increasing number of papers requires double or even triple assignment. In several cases a simultaneous assignment to the categories E, A and P would be most appro-priate. The category assignment remains thus imperfect. We expect a positive effect of the combination of bibliometric and text-mining methods on monitoring, describing and understanding the structure of our field. 4. Clustering of full text articles 4.1. Latent semantics
We aim at laying out research themes covered by the journal Scientometrics in 2003 by constructing a full text -based document map, from which term networks are subsequently derived. As outlined in the
Methods section, we process and index each of 85 articles  X  title, abstract and full text body X  X ith exemption of the reference list. The resulting 85  X  3589 document-by-term matrix A is transformed to a reduced rank 6 matrix A 6 by a Singular Value Decomposition (SVD). Rank 6 was chosen through an  X  educated guess  X  neighbouring values may be appropriate as well. We recall that this is inherent to Latent Semantic Index-ing. A 6 has the same dimensions as A but is no longer sparse due to this transformation. This property in-duces non-zero weights on terms that are related to a document, but not occurring in it. For example, top terms for the Braun, Szabadi-Peresztegi, and Kova  X  cs-Ne  X  methl (2003) paper, and medicine, 1901 X 2001  X  X , include  X  award  X  ,  X  nobel prize  X  and  X  laureate  X  in the original matrix A , whereas A reads:
 X  Rewards and recognition in scientific research can take several important aspects  X  , and confirms that it is related to the Matthew effect in a scientometric context, although Matthew effect and mocr are absent in the full text of the article. 4.2. Exploratory document map
From A 6 we create a 85  X  85 distance matrix using the Salton cosine measure. To visualise the interre-latedness between all documents, we plot a three-dimensional MDS map and overlay the assigned category information in Fig. 3 .

The Informetrics group (I) is reasonably distinguishable. The Methodological Advances (A) and Empir-ical case studies (E) are scattered throughout the map and give a first indication of heterogenity. The two deep-mathematical contributions (M) seem to be closely related, whereas the three sociological studies (S) are further apart. Finally, some of the policy-relevant (P) contributions are close, whereas others are, again, scattered over the cloud. Although the map constitutes a low dimensional approximation of mutual dis-tances, we already can see that the underlying topics are likely to be intertwined or, at least closely con-nected, and that the subject classification is, in part, insufficient. To gain more insight in the underlying topic structure, we proceed with a detailed cluster and co-word analysis. 4.3. Document cluster analysis
Based on the computed distances we cluster the data using Ward  X  s hierarchical clustering and cut the dendrogram at k = 6. To determine a statistically optimal number of clusters we used the Stability method as proposed in Ben-Hur, Elisseeff, and Guyon (2002) and used in Glenisson, Mathijs, Moreau, and De
Moor (2003) . In this method the optimal number of clusters k is determined by inspection of a stability diagram as in Fig. 4 . The plot essentially shows a cumulative distribution of overlaps between two, recur-rently computed, cluster solutions. More specifically we measure for 1000 times the overlap between clustering solutions of pairs of random subsamples from the data matrix (i.e., sampled rows from A subsample comprises 72 documents, corresponding to 85% of 85 datapoints. We thus adopt the parameter settings f = 0.85 and k =10in Ben-Hur et al. (2002) . The overlap, or correlation between the pair of solu-tions, is quantified by the Jaccard coefficient. The figure shows that for higher k the distances between the curves decrease and form a band. Typically one looks for a transition curve to this wider set of distributions.

For small k , there are several interesting observations to make. First of all the solutions are not mono-tonic over k , which supports the observation in the MDS map that the underlying structure is non-trivial.
The cases k = 2,3,4 appear to produce solutions that are less stable than those of very fine-grained segmen-tations (higher k ) in over 60 X 70% of the subsampling runs. Based on these observations we chose k =6as the most stable solution.

Silhouette values constitute the basis for checking the quality of a clustering solution when resorting so-measure that describes the ratio between cluster coherence and cluster separation for each point: tance of i to the points in the nearest cluster. We can summarize the silhouette values for all points in a cluster by taking an average. Likewise we can compute a score for an entire solution by subsequently aver-aging out over all clusters.

In Fig. 5 we illustrate the Silhouette profiles for the solution k = 6. For each cluster k it shows a tilted histogram of s ik for all its members. We observe homogeneous structure in clusters 1, 3, 4 and 5. Cluster 2 appears to cover a broader set of topics, whereas in Cluster 6 we can expect themes that strongly incline to other clusters as several negative Silhouette values occur. These observations will be elaborated in detail in the next subsection.

To relate the clustering outcome to the expert categorization we use the Rand index ( Jain &amp; Dubes, 1988 ), engineered to quantify a clustering outcome with a  X  gold standard  X  partitioning. The Rand index measures the correspondence between a cluster solution and an external partitioning by examining all pairs of objects: pairs that end up in the same cluster for both the computed and the expert solution are consid-ered an agreement. The same goes for pairs that are allocated to different clusters in both outcomes. All other pairs are considered a disagreement. The statistic takes on values in the interval [0,1] where 1 indicates perfect correspondence. For the proposed clustering solution, we report a value of 0.1127 for the Rand index, which is quite low, but still significant ( p -value &lt; 10
To understand the relatively low value of the Rand index, we show the confusion table, on which the computation of the Rand index is based, in Table 2 . We see classes S, M, I and P, admittedly all of smaller size, moderately to well conserved in the text-based cluster structure. Conversely, papers assigned to the larger classes A and E are heavily shifted around the text clusters. To better understand this discrepancy, we proceed with looking into the content of each cluster by:
Examining each cluster  X  s ranked list of documents with respect to the cluster  X  s medoid (i.e., representa-tive element). This is printed in Table 3 .
 Analyzing the content structure of the document clusters through a co-word analysis as described in the Method section.
 5. Scientometrics in 2003 through the eyes of text mining The results of the co-word analysis on all documents in each cluster are shown in Figs. 6 X 11 . The map in
Fig. 6 represents the content structure of cluster 1 with altogether 9 papers. This cluster represents publications that are concerned with methodological questions related to bibliometric indicators.
Indicator-related terms such as indicator names and terms relevant in the context of measuring publication activity and citation impact are close to the centre, and strongly interlinked. The acronym  X  mec  X  located in the south of the map stands for  X  Matthew Effect for Countries  X  . One could consider this cluster representing methodol-ogical indicator research .

Cluster 2 is dominated by empirical papers and case studies (cf. Table 3 ). The content structure in this cluster is very dense. The terms in this map are presented in Fig. 7 and relate above all to national and insti-tutional aspects as well as to science fields. This is the cluster of case studies and traditional bibliometric applications .

Cluster 3 is a second theoretical/methodological cluster. Unlike the first one, this cluster relates to more advanced methodological techniques, such as informetric laws, frequency distributions and multivariate statistics. This cluster could be characterised as theoretical and mathematical issues in bibliometrics . The term structure is presented in Fig. 8 .

Cluster 4 presented in Fig. 9 clearly represents webometrics and network-related issues. All terms are strongly interlinked. This cluster corresponds by and large to the category of Webometrics/Informetrics .
Cluster 5 with 3 papers is the smallest one. Co-citation analysis and the analysis of other citation statis-these studies. This cluster covers specific applications of statistical methods .

The last cluster with 30 papers (see Fig. 11 ) is by far the largest one. It comprises technology and inno-vation related studies, the science-technology interface and almost the complete Triple Helix issue can be found here (cf. Table 3 ). Also the sociological approaches are covered by this cluster. This cluster can be considered a borderland of classical scientometrics, namely the interdisciplinary approaches such as sociological, policy relevant and technology related issues .

The comparison of the topic structure based on articles given in Table 3 as well as the content structure presented in Figs. 6 X 11 with the category assignment in Table 1 shows only a partial accordance. The two large categories A and E covering 65% of all papers proved heterogeneous. Category A has (jointly with category M) three sub-clusters, namely, Cluster 1, 3 and 6, whereas Category E falls apart into three other sub-clusters: Cluster 2, 5 and 6. Policy relevant issues are also covered by clusters 2 and 6. Only Category I is represented by a corresponding co-word cluster, namely cluster 4. The full text analysis substantiates that both methodological and empirical research have nowadays at least two different main focuses each, one is based on scientometric standard techniques such as classical indicators, the other ones are clearly broaden-ing the scope of traditional bibliometrics.
 The phenomena discussed here is also in line with the situation visualised in Fig. 3 where only the
Webometrics/Informetrics group is clearly distinguishable. Especially the groups A and E appear as heterogeneous clouds which seem X  X ointly with the P-papers X  X o fall apart. Both approaches thus substan-tiate that there are additional evolutions within the empirical and methodological research in biblio-metrics. 6. Added value of full text with respect to Title and Abstract and reference-based term information
We extended our text analysis by comparing the outcome using full text to similar analyses using infor-mation from Title and Abstract on the one hand, and keywords present in the article  X  s Reference section on the other hand. Table 4 shows a comparison of the Silhouette values per cluster and the Silhouette values for the entire solution in the case of k = 6. Note that there is no direct correspondence between the clusters across these three solutions; i.e., cluster 1 from full text does not necessarily correspond to cluster 1 from Title and Abstract, etc.

We see that the information captured in the reference titles gives rise to a less pronounced cluster struc-ture (0.1447) than Title and Abstract, which, in turn, fares worse (0.2105) than full-text (0.5321). These trends are confirmed when assessing correspondence to the expert categories with the Rand index printed below in Table 4 .

We remark, however, that it is unlikely that the structure of the data in the three cases is such that a parameterization k = 6 is equally probable. Hence to ensure the consistency of the observed trend over other parameterizations in each of the cases, we averaged out the overall Silhouette and Rand values over all values k = 2 X 10 in Table 5 . We see that full text still produces, on average, better cluster struc-tures (0.4206) than Title and Abstract (0.2870) and Reference info (0.160), whereas the advantage in favour of full text is again less pronounced when considering the Rand index over various k (0.6817 versus 0.6237).

The question remains whether there are hidden mechanisms that drive this advantage. It could well be that it is the adopted clustering method, and not the information nuggets, that accounts for the observed effect over the three experiments. To test this hypothesis, a sample of clustering methods should be applied to the three methods. Three more hierarchical methods (single-link, average-link and complete-link), and one divisive method (K-medoids) were additionally applied to the three information sources. We kept the parameterization k = 6 fixed for each of them and show the results in Table 6 . We do not discuss the difference in performance between the clustering methods as it is outside the scope of this paper. Despite the low sample size, we tested as alternative hypothesis H and Abstract, and Reference based sources differs significantly with respect to the full text approach. The second observation in Table 6 (single-link clustering) prevents a rejection of the null hypothesis can consider it an outlier making the p -value drop to 0.0097 in favour of our research hypothesis. Never-theless, too few samples (i.e., the four clustering methods) are now included for rigorous inference, so we cannot strictly rule out the possibility that the observed differences are due to artefacts rather than the structure of the data sources.

However, as mentioned in the discussion of Table 3 , the expert classification scheme is not fully geared at classifying documents according to the scientific realms in which they reside (e.g., the category  X  case studies (E)  X  covers quite a broad range of topics X  X ee supra). Also, when manually comparing co-word maps across the three data structures, we found that the use of full text included more relevant phrases for interpretation (results not shown). As extra supportive evidence we provide the complete clustering solutions with k =6 for Title and Abstract and Reference text in Tables A1 and A2 .

Although our hypotheses might seem straightforward, it was not known a priori how informative key-words from references were for mapping purposes. Likewise, we expected that the benefits of full text infor-mation would be invisible due to the inclusion of excessive noise. The results on this data set point in the opposite direction: given appropriate preprocessing, the use of full text in the presented framework can be a valuable asset in mapping disciplines and subfields. 7. Combined text mining and bibliometrics
The statistical analysis based on the full text provided a relational chart of the structure represented by the documents under study. As already used in our pilot study ( Glenisson et al., 2005 ), the mean reference ences and social sciences. In the following we will check whether these indicators can be used to characterise the plot of Mean Reference Age vs. Share of Serials on basis of the categories. This can be considered a traditional bibliometric approach although the classification of articles was peer based.

As already seen in the pilot study, Webometrics is characterised by low reference age and medium X  X igh low share of serials. Nevertheless, there is a group of papers with clearly higher share, too. This confirms the results of the full text analysis, namely that this category practically forms two sub-clusters. The cat-egory Advances in Scientometrics proves strikingly homogeneous with several outliers only. Most of the
A-class papers have, however, a mean reference age ranging between 5 and 15 years, with medium X  X igh share of serials ranging between 50% and 90%. The empirical groups proved heterogeneous, indeed.
Regarding the share of serials this class forms two distinct sub-classes, particularly, one with low share ( 6 55%) and one with relatively high share ( P 67%). The class with lower share has similar characteristics as the policy relevant class. The classes and subclasses are visualised by ellipses (see Fig. 12 ). The other two groups M and S are very small as compared with the other classes, and do not show any character-istic patterns.

In what follows, we combine the bibliometric approach with results of the full text analysis. For this pur-pose the clusters in Table 3 are used. They are represented by their medoids (i.e., representative elements).
Fig. 13 presents the plot of Mean Reference Age vs. Share of Serials on basis of the co-word clusters. Similar bibliometric characteristics of clusters are much less pronounced than in the category-based approach. The number of outliers is also somewhat larger. We have indicated the two special issues (Triple Helix Confer-ence and S&amp;T Indicators Conference) by ellipses. These issues form surprisingly homogeneous groups.
Moreover, cluster 2 is characterised by medium Mean Reference Age (MRA). Cluster 3 can be subdivided into two sub-clusters with regard to the MRA using a threshold of about 12 years. Those with older MRA have a more theoretical/informetrical focus.

On the other hand, cluster 6 consists of two sub-clusters with regard to the share of serials. Here a share of 60% forms an appropriate threshold. This threshold seems to separate more methodologically oriented scientometrics/technometrics papers and policy relevant applications. Papers with similar  X  X  X ontent X  X  might thus have different bibliometric characteristics depending on the target readership and the field of application.
 8. Conclusion
The results of this study have much deepened the findings of the pilot study. The pilot study was based on a small selection of papers presented at an International Conference on Scientometrics and
Informetrics. Besides quality, representative coverage was the main criterion for selection. Due to limited space in the journal, less than 20 papers could be selected, a small set of papers that showed the picture of a clearly structured discipline. Unlike this special issue, the 2003 publications in Scientometrics repre-sent almost the complete and heterogeneous spectrum of scientometric, informetric and technometric re-search activity also covering topics beyond the mainstream in the field. Nevertheless, the combination of text-mining and bibliometric techniques proved an appropriate tool to answer the questions addressed in the outset.

Text-mining provided reliable results in representing structural aspects of bibliometric research if meth-could be identified. The term structure of each cluster gave a clear picture about the research profile of the sub-disciplines represented by the cluster.
 The restriction to (non-author) reference terms, on the other hand, proved to yield the weakest results.
Whether to prefer full text over Title and Abstract remains, however, an issue of discussion. We found that the cohesion of clusters, their interpretability towards science mapping and correspondence to the expert classification do not deteriorate when implying full text in our mining approach. Given the limited scope of our investigation, we remain cautiously optimistic about the traceable benefits with respect to Title and Abstract information.

The clusters found through application of text mining provided additional information that can be used to extend, improve and explain structures found on basis of bibliometric methods. The full text analysis has thus shown that within the categories, such as methodological or empirical research, substantial differences in profile and orientation can occur. The question how bibliometric measures can, in turn, be assumed to reflect formal characteristics of documented scientific communication that might supplement results ob-tained from content-based analyses could also be answered in a positive way. Reference-based citation mea-sures can help to fine-structure clusters determined on basis of co-word analysis. Among others, bibliometric indicators can provide information how  X  X  X heoretical X  X  or  X  X  X pplied X  X , how  X  X  X ard X  X  or  X  X  X oft X  X  re-search within the same topic is. Hybrid methodologies combining data-mining techniques and bibliometric methods will therefore probably prove valuable tools to facilitate endeavours in mapping fields of science in the future.
 Acknowledgements The authors acknowledge support from the Flemish Government (Steunpunt O&amp;O Statistieken), Research Council K.U. Leuven (GOA-Mefisto-666, GOA-Ambiorics, IDO), the Fonds voor Wetenschappelijk Onderzoek-Vlaanderen (G.0115.01, G.0240.99, G.0407.02, G.0413.03, G.0388.03, G.0229.03, G.0241.04), the Instituut voor de aanmoediging van Innovatie door Wetenschap en Technologie Vlaanderen (STWW-Genprom, GBOU-McKnow, GBOU-SQUAD, GBOU-ANA), the Belgian Federal Science Policy Office (IUAP V-22), and the European Union (FP5 CAGE, ERNSI, FP6 NoE Biopattern, NoE Etumours). The authors would like to thank the reviewers for their useful sugg-estions. Appendix A Tables A1 and A2.
 References
