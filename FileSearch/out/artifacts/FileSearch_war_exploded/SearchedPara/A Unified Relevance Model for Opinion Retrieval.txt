
Representing the information need is the greatest challenge for opinion retrieval. Typical queries for opinion retrieval are com-posed of either just content words, or content words with a small number of cue  X  X pinion" words. Both are inadequate for retrieving opinionated documents. In this paper, we develop a general formal framework X  X he opinion relevance model  X  X o represent an infor-mation need for opinion retrieval. We explore a series of methods to automatically identify the most appropriate opinion words for query expansion, including using query independent sentiment re-sources. We also propose a relevance feedback-based approach to extract opinion words. Both query-independent and query-dependent methods can also be integrated into a more e ff ective mixture rele-vance model. Finally, opinion retrieval experiments are presented for the Blog06 and COAE08 text collections. The results show that, significant improvements can always be obtained by this opinion relevance model whether sentiment resources are available or not.
H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Algorithms, Experimentation, Theory
Opinion retrieval, sentiment analysis, language model, opinion relevance model, relevance feedback, query expansion
People have become increasingly interested in sharing their per-sonal opinions and reviews about consumer products, commercial services, and even po litics with others through online media. Their opinions can not only help other people to make decisions, but also help business and government agencies to collect valuable feed-back. To support these activities, th ere is clearly a strong need for systems that can retrieve and analyze online opinions precisely and e ffi ciently [21].

There has already been a considerable amount of research on sentiment analysis, including semantic orientation analysis of opin-ion words [24], sentiment classification of documents [20], subjec-tivity categorization of sentences [6], online opinion extraction [11] and opinion summarization [19]. In this paper, we will focus on opinion retrieval, which aims at automatically finding attitudes or opinions about specific ta rgets, such as name d entities, consumer products, or public events.

Opinion retrieval is very di ff erent to traditional topic-based re-trieval. Firstly, relevant documents should not only be relevant to the targets, but also contain subjective opinions about them. Sec-ondly, the text collections are more informal  X  X ord of mouth X  web data. Two typical sources are blogs that generally reflect personal opinions and forums that present group opinions. Thirdly, although web retrieval pays more attention to precision, opinion retrieval at-taches extra importance to recall, since further sentiment mining relies heavily on the coverage of the opinion collection.
Finally, the greatest challenge for opinion retrieval lies in the dif-ficulty in representing the user X  X  information need. For topic-based retrieval, the information need is usually defined by a user query consisting of a small number of keywords. Similarly, for opinion retrieval, typical queries are composed of either just content words, or content words as well as some cue words, for example,  X  Steve jobs  X ,  X  Find opinions about NASA  X . Unfortunately, both of these are poor representation for opinion retrieval. It has been shown that if these types of queries are used directly, the average pre-cision of opinion retrieval is much lower than that of topic-based retrieval [13]. In addition, only a small portion of relevant doc-uments (about 25% in Blog06, a major text collection for opinion retrieval) contain cue words such as  X  X pinion X ,  X  X ttitude X ,  X  X eview X  and  X  X entiment X . Simply submitting these words in queries to the retrieval system will hurt both recall and precision.

Currently, the majority of previous work in opinion retrieval treats this task as a two-stage process. In the first stage, documents are ranked by topical relevance only. In the second stage, candidate relevant documents are re-ranked by their opinion scores [16, 13]. The opinion scores can be acquired by either machine learning-based sentiment classifiers, such as SVM [29], external sentiment dictionaries with weighted scores from training documents [5, 1, 15], exhaustively computed query term X  X pinion word proximity scores [25, 31], or external toolkits such as OpinionFinder [5]. Al-though this two-stage process has been shown to be quite e the overall performance strongly d epends on the in itial topic-based retrieval and has high computational overheads, even if the opinion scores can be obtained during indexing [17].

In this paper, we will develop a general formal framework X  the opinion relevance model  X  X o directly represent the information need for opinion retrieval. According to this model, query terms are expanded with a small number of automatically chosen opinion words to represent the information need. We then explore a se-ries of methods to extract the most appropriate sentiment words automatically: we will first study how to e ffi ciently use query-independent sentiment resources such as opinionated seed words, annotated sentiment corpora, and relevance data to improve opin-ion retrieval; next we will propose a relevance feedback approach within the language modeling framework to combine the informa-tion about relevant or pseudo-relevant documents into the ranking algorithm. Both query-independent and query-dependent methods are also integrated into a more e ff ective mixture relevance model.
The novelty and e ff ectiveness of the proposed approach lie in: 1) the two-stage process of topic retrieval and sentiment classi-fication can be converted to a unified opinion retrieval procedure through query expansion; 2) when relevance data are available, the improvement of this approach over the topic retrieval baseline is considerable and comparable to the best previous TREC results. When sentiment resources are not available, significant improve-ments can still be achieved; 3) this approach is not only suitable for English data, but also shown to be e ff ective for a Chinese bench-mark collection; 4) although this framework is proposed for opin-ion retrieval, it can be extended to other retrieval tasks where user queries are inadequate to express the information need, such as ge-ographical information retrieval and music retrieval.

The rest of this paper is structured as follows. We first review re-lated work in the next section. The formal framework of the opinion relevance model is presented in Section 3. After that, we propose a series of methods to automatically extract the most appropriate opinion words to expand the original queries. Section 5 describes the two benchmark collections, as well as some English and Chi-nese sentiment resources. Section 6 will present the experimental results in detail. The final part is the concluding remarks.
Related work can be found in three areas. The first is the re-cent developments in language model techniques in information retrieval, in particular, the relevance model. Our opinion relevance model is actually an extension of the relevance model for topic-based retrieval. The second includes the lexicon-based opinion finding techniques, which are used to get the opinion score of doc-uments in two-stage opinion retrieval. Third, there has been some research on unified sentiment retrieval models. The use of language models in information retrieval started with Ponte and Croft, who described a retrieval model based on multiple-Bernoulli language models [22]. In a language model, documents in a collection are viewed as models, and a query is regarded as a term sequence randomly sampled from these models. Then the documents are ranked by the probability that the query is sampled from the models of these documents.

Zhai and La ff erty described smoothing techniques for language modeling in information retrieval [9]. Among them, Dirichlet smooth-inghasprovedtobeverye ff ective.

The basic language model approach represents the information need with query terms. Although it has a number of advantages, it is limited in terms of combining information about relevant doc-uments into the ranking. Therefore, the relevance model was pro-posed to represent the topic covered by relevant documents, and then both queries and relevant documents are regarded as samples of text generated from the relevance model [10].
Research on opinion retrieval has been significantly advanced by the TREC Blog track, where an opinion finding task was introduced to find public sentiment for given targets. TREC evaluations during last three years have shown that sentiment lexicon-based methods lead to good performance in two-stage opinion retrieval [17].
A lightweight lexicon-based statistical approach was proposed in [5]. In this method, the distribution of terms in relevant opinion-ated documents was compared to their distribution in relevant fact-based documents to calculate an opinion weight. These weights were used to compute opinion scores for every retrieved document. A weighted dictionary was generated from previous TREC rele-vance data in [1]. This dictionary was submitted as a query to a search engine to get an initial query-independent opinion score of all retrieved documents. Similarly, a pseudo opinionated word composed of all opinion words was first created, and then used to estimate the opinion score of a document in [15]. This method was showntobeverye ff ective in TREC evaluations.

The query-independent sentiment expansion described here also requires an external lexicon. However, only the most frequent words are used for expansion, instead of all the opinion words in the lexi-con. Therefore, heavy computational overheads are avoided.
There has been some limited research on unified sentiment re-trieval models.

Eguchi and Lavrenko proposed a sentiment retrieval model in the framework of generative language modeling [4]. They modeled a collection of natural language documents or statements, each of which consisted of some topic-bearing and some sentiment-bearing words. The sentiment was either represented by a group of prede-fined seed words, or extracted from a training sentiment corpus. This model was shown to be e ff ective on the MPQA corpus.
Mei and Zhai tried to build a fine-grained opinion retrieval sys-tem for consumer products [14]. The opinion score for a product was a mixture of several facets. Due to the di ffi culty in associating sentiment with products and facets, the experiment was also tested in small scale text collections.

Zhang and Ye proposed a generative model to unify topic rele-vance and opinion generation [28]. This model led to satisfactory performance, but an intensive computation load was inevitable dur-ing retrieval, since for each possible candidate document, a opinion score was summed up from the gene rative probab ility of t housands of sentiment words.

Similar to these retrieval models, the proposed opinion relevance model also tries to unify topic and opinion relevance. Unlike them, our approach does not model document generation, but the infor-mation need instead, which is more straightforward and e ffi
As we have mentioned, modeling the information need is the greatest challenge for opinion retrieval. In this section, we will extend the relevance model approach to incorporate the particular information needs required for opinion retrieval.
 Suppose we can obtain an opinion relevance model from a query. Then we can compare this query directly with the documents and rank documents according to the KL-divergence between the two probability distributions of the opinion relevance model and doc-ument model [8]. Let R denote the opinion relevance model for a query, D denote the document model, and V denote the vocabulary, then the KL-divergence between these two models is defined as:  X  V P ( w | R ) logP ( w | R ) is identical for all documents. Then docu-ments can be scored by the reverse order of P ( w | D ) can be estimated e ff ectively from the interpolation of the maximum likelihood estimation a nd the occurrence probability in the entire document collection, for example, Dirichlet smoothing [27]: Where, freq D ( w ) is the occurrence frequency of w in D , C collection frequency of w , | C | and | D | are the number of word occurs in document D and collection C respectively, and  X  is an empirical parameter (typically, 2,500).

Now the problem turns to the estimation for P ( w | R ), the proba-bility of word w given the model R . The basic language model sim-ply estimates P ( w | R ) from the occurrence of w within the query Q . Lavrenko and Croft X  X  relevance model tries to estimate the prob-ability of words from a group of documents relevant to the user query [10]. This model is proposed for topic-based retrieval. Here we will extend the relevance model to improve opinion retrieval, and call this new model the opinion relevance model .

Content words and opinion words contribute di ff erently to opin-ion retrieval. Topical relevance is determined by the matching of content words and the user query, and the sentiment and subjectiv-ity of a document is decided by the opinion words. So we divide the vocabulary V into two disjoint subsets: a content word vocabulary CV , and an opinion word vocabulary OV .Thenwehave:
Definition 1. An opinion relevance model is a unified model of both topic relevance and sentiment. In this model, Score ( D ), the score of a document is defined as: The parameter  X  is introduced to balance two relevance scores. The first part on the right hand side of the equation is just the same as the relevance model for topic-based retrieval. CV can be as-signed as the set of original query terms, or obtained by any query expansion technique, which is outside the scope of this research. The key issue in our research is the selection of OV and the estima-tion of P ( w | R )for w  X  OV , which is actually a special query expan-sion procedure. This procedure is called sentiment expansion ,since only opinion words are used to expand an original query instead of content words.
Since the relevance scores should be computed as quickly as pos-sible during retrieval, a smaller vocabulary is preferred. In the fol-lowing subsections, we will explore several sentiment expansion methods to build the opinion word vocabulary with the most ap-propriate words. These methods can be divided into three cate-gories. For query-independent sentiment expansion, we will make use of several kinds of sentiment resources. These resources in-clude seed words, opinionated or general text corpora, and rel-evance data. For query-dependent sentiment expansion, we pro-pose a relevance feedback-based approach. The query-independent and query-dependent methods can also be combined into a mixture model.
This method restricts OV to some predefined seed words, such as those recommended by Turney [24]:
Positive seeds : good, nice, excellent, positive, fortunate, correct, superior.

Negative seeds : bad, nasty, poor, negative, unfortunate, wrong, inferior.

Seed words like  X  X ood X  and  X  X ad X  are also adopted by Eguchi and Lavrenko X  X  sentiment retrieval model [4].

In this case, the expanded query is composed of the original query and some additional opinionated seed words. The advantage of this method lies in that these seed words nearly always express strong sentiment. The disadvantage is that some of them are infre-quent in the text collections.
Instead of predefined seed words, we can obtain opinion words from lexical resources, such as General Inquirer 1 , OpinionFinder X  X 
There are always thousands of entries in a lexicon, so only the most frequent opinion words are selected to expand the original queries. That is to say, we can select opinion words according to their occurrence probabilities.

The occurrence probability can be estimated by the maximum likelihood method from any corpus, such as the text collection it-self or the entire web. However, occurrences in a general corpus may be misleading. For example,  X  X omplete X  is the second most frequent opinion word in the Blog06 collection, but it occurs more frequently in fact-based documents than in opinionated documents. In fact, it means  X  X inish X  or  X  X inished X  in most fact-based docu-ments, and then is non-opinionated. Therefore, an opinionated cor-pus will be more reliable, such as the Cornell movie review datasets [20] and MPQA Corpus [26].

In the following experiments, the original queries are expanded with the most frequent opinion words in several opinionated or gen-eral corpora. We also find an appropriate number of opinion words in sentiment expansion.
We now discuss the estimation of P ( w | R ), or equivalently for ranking, the weight of w . For topic-based retrieval, a simple max-imum likelihood estimate is often used in practice, based on the frequency in the query text ( freq Q ( w )) and the number of words in the query( | Q | )[3]:
Since opinion words usually do not appear in the query text, this estimation is not applicable in the above methods. Therefore, the 1 http: // www.wjh.harvard.edu /  X  inquirer http: // www.cs.pitt.edu / mpqa probability is assumed to be uniform for the seed word or corpora-based sentiment expansion  X  that is to say, all opinion words are regarded as equally important.

Recently,  X  X earning to rank X  techniques have gained attention from both the information retrieval and machine learning commu-nities. The goal is to automatically learn a function from training data to rank documents [7]. Query-independent features have been shown to be useful for ranking [2, 23].

Many opinion words, especially the most frequent opinion words, are also query-independent. For example,  X  X ood X ,  X  X ad X  can be used to modify almost any target, and  X  X ven X ,  X  X oo X  can be used to modify almost any opinionated adjective. Therefore, we can make use of a simple machine learning technique to find the most valu-able opinion words for sentiment expansion. Moreover, the weights of opinion words can also be obtained by learning.

Given a set of query relevance judgments, we can define the in-dividual contribution to opinion retrieval for an opinion word:
Definition 2. If w is an opinion word, then the contribution of w means the maximum increase in the mean average precision (MAP) of the expanded queries over a set of original queries, where w is used to expand every original query.

More formally, let Q i be the i -th query in the training set, Q be the i -th expanded query, weight ( w ) is the weight of w , AP ( Q is the average precision of the retrieved documents for Q AP ( Q i  X  w , weight ( w )) is the average precision of the retrieved documents for the expanded query while the weight of w is set as weight ( w ), then, Contribution ( w ) = max And also,
The contribution of an opinion word can be used to assess to what extent it can improve the performance of opinion retrieval. After we learn the individual contribution for every opinion word, those words with the highest contribution will be used for sentiment expansion.
In the above methods, the selection of an opinion word is as-sumed to be independent of individual targets. On the other hand, a target is always associated with some particular opinion words. For example,  X  X ozart X , the Austrian musician, is always regarded as a  X  X enius X  and  X  X amous X . Therefore, we can condition the prob-ability of w on a query to incorporate the dependency between the target and the opinion word:
Where, q 1 , q 2 ,..., q n are query terms in Q . In order to esti-mate the conditional probability, w e propose a relevance feedback method to extract opinion words from a set of user-provided rele-vant opinionated documents. First, we have: P ( q 1 , q 2 ,..., q n ) can be obtained through marginalization:
Then the joint probability of P ( w , q 1 , q 2 ,..., q n the relevant document set of C : The prior probability P ( D ) is assumed to be uniform, while P ( w | D ) is also estimated by Dirichlet smoothing according to Equa-tion (1). Assuming q i is conditionally independent to q j D and w are observed, then we have P ( q i | D , w ) is then estimated by the co-occurrence of q D :
The relevance feedback approach can still be used when there are no user-provided relevant documents. Under these circumstances, the relevant document set C can be acquired through pseudo-relevance feedback. We first rank documents using query likelihood scores, then select some top ranked documents to get the pseudo relevant set of C .
We have proposed two types of sentiment expansion approaches, query-independent and query-dependent. For query-independent approaches, the most valuable opinion words are always general words and can be used to express opinions about any target. For query-dependent approaches, those words most likely to co-occur with the terms in the original query are used for expansion. These words are used to express opinions about particular targets. It is quite natural to integrate query-independent and query-dependent sentiment expansion into a mixture relevance model to cover both types of opinion words.

Definition 3. In a mixture relevance model , the final score of a document is defined as the interpolation of the scores assigned by original query, query-independent sentiment expansion, and query-dependent sentiment expansion:
Where OV 1 and OV 2 are the sets of query-independent and query-dependent opinion words respectively, and  X , X   X  [0 , 1].
The proposed methods will be verified on two benchmark collec-tions,  X  X log06 X  and  X  X OAE08 X . Blog06 was created by the Uni-versity of Glasgow for the blog retrieval track of TREC [12, 17]. This track has continued from 2006 to 2008, and 50 new queries are provided for evaluation every year. The 50 queries as well as Table 1: Details for the Blog06 and COAE08 collections. The English translation of the Chinese topic is placed in parenthe-ses.
 the relevant opinionated documents in 2006 are used for training, while the other 100 queries are used for testing.

COAE08 is the benchmark data set of the opinion retrieval track of the first Chinese Opinion Analysis Evaluation (COAE), which was created by the Institute of Computing Technology, Chinese Academy of Sciences [30]. Since COAE has been held only once, training queries are not available.

Both evaluations aim at locating documents that express an opin-ion about a given target. The targ et can be not only a named entity, but also a concept, a product name, or an event. The relevance judg-ments were created by the pooling method, where documents are ranked at di ff erent levels: irrelevant, relevant but without opinion, and relevant with opinion. Table 1 shows some details for these two collections. External sentiment lexicons provide the source of opinion words. Currently there are several online English sentiment lexicons: Gen-eral Inquirer lists about 3,600 opinion words, and OpinionFinder X  X  Subjectivity Lexicon lists more than 5,600 words.

Whether a word is opinionated or not is still debatable. For ex-ample,  X  X ome X  and  X  X ust X  are among the most frequent opinion words in General Inquirer , but are not listed in OpinionFinder , while  X  X o X  is listed in OpinionFinder , but not in General Inquirer .
Our opinion relevance model does not depend on the coverage of the sentiment lexicon, since an original query is expanded with only a small number of opinion words. In order to reduce the variability causedbydi ff erent lexical resources, the intersection of these two lexicons is used instead.

Three English opinionated corpora are used for query-independent sentiment expansion: the Cornell movie review datasets ,the MPQA Corpus , and the  X  X log06(op) X  opinionated data set, which is com-posed of the opinionated documents relevant to the 50 training queries.

Two general English collections are also used. One is the Blog06 collection itself. The other is  X  X eb X , which gives the Google hits for all opinion words.

Table 2 shows the most frequent 5 opinion words from these col-lections. From this table, we can see that this set varies with the cor-pus. Two opinionated corpora of  X  Movie Reviews  X  and  X  X log06(op) X  share a lot similar opinion words, while  X  MPQA  X  X ontains some distinct words. In fact, di ff erent to other corpora, the MPQA cor-pus belongs to the political genre a nd then contains a lot of formal opinion on political, economic, a nd governmental issues.
Table 3 shows some statistics about the most frequent opinion words from the  X  X log06(op) X  collection.  X  X verage TF X  is the av-erage term frequency,  X  X F X  is the document frequency, and  X  X ov-erage X  is the percentage of documents in which the opinion word occurs. All of these opinion words occur several times in more than Table 2: Most frequent 5 English opinion words from 3 opin-ionated and 2 general collections.
 Documents 2000 535 11523 3.2M 14B Table 3: Statistics for the most frequent 5 opinion words in the  X  X log06(op) X  collection.
 half of the opinionated documents. In fact, only about 10% opin-ionated documents contain none of them. This helps to explain why query-independent sentiment expansion can improve opinion retrieval significantly.
For Chinese opinion retrieval, HowNet Sentiment Vocabulary is used as the sentiment lexicon 3 . It consists of about 7,000 opinion words. Two Chinese opinionated document sets are used in the ex-periments:  X  Product  X  is the data set of the opinion extraction track of the first COAE evaluation, which was created by Institute of Automation, Chinese Academy of Sciences and Fudan University [30]. This data set is composed of reviews of consumer products. The other is  X  Hotel  X , which is composed of reviews of hotels and eral text corpora of COAE08 itself and the web are also provided. Since Google does not support Chinese segmentation, the web hits are provided by the Sogou Lab 5 .

Table 4 shows the most frequent 5 opinion words from these col-lections. It can be found that almost all of them are single-character words with more than 10 ambiguous meanings (Sogou does not provide hits for single-character words). For example,  X   X  X c-curs frequently, and it has two opinionated meanings of  X  X ind X  and  X  X eace X , but it means  X  X nd X  most of the time!
Table 5 and 6 summarize the evaluation results for the Blog06 and COAE08 collections using our opinion relevance model.
The left columns in these tables show the sentiment expansion approaches. The results of the baseline system is first given, which is implemented with Indri search engine 6 . The baseline uses the basic relevance model as well as the Dirichlet smoothing technique 3 http: // www.keenage.com 4 http: // www.searchforum.org.cn / tansongbo / corpus-senti.htm 5 http: // www.sogou.com / labs / http: // www.lemurproject.org only. CV , the content word vocabulary, is assigned as the set of original query terms, and the document priors are set to be uniform.
All the sentiment expansion approaches are divided into three categories: query-independent, query-dependent and mixture rel-evance model. Each category is further divided into several sub-categories, which will be explained in the following subsections.
The right columns show the evaluation results. The mean aver-age precision (MAP) is the primary evaluation metric in both TREC Blog and COAE evaluations. Other metrics in these evaluations in-clude R-precision (R-prec), binary P reference (bPref) and Precision at 10 documents (P@10).
Query-independent sentiment expansion is further categorized by the sentiment resources, including the seed words, the opin-ionated corpus, the general corpus, and the relevance data. The seed words-based approach is not applicable for the COAE08 col-lection, since there are no generally accepted Chinese seed words. Relevance data-based sentiment expansion is also not available for COAE08. There are two runs based on the seed words for the Blog06 collection. In  X  X eed-1 X , the final query is represented with the original query as well as a single pair of seed words of  X  X ood X  and  X  X ad X . In  X  X eed-7 X , seven pairs of seed words are given, which are the same as [24]. The top 5 opinion words are used for senti-ment expansion in other query-independent runs. Here, all corpus-based query-independent runs are named by the associated corpus, and  X  X D X  is the relevance data-based run.

From these tables we can see that the seed word approach is not always helpful. In fact, its e ff ectiveness is dependent on the selec-tion of seed words. Although these seed words are typical opin-ion words with strong and unambiguous sentiment, only a small portion of them frequently appear in the opinionated documents. Among the most frequent 50 English opinion words, only  X  X ood X ,  X  X ad X ,  X  X ice X  and  X  X oor X  are chosen as seed words.

Previous studies show that sentiment dictionary-based methods lead to good performance in two-stage opinion retrieval, especially when statistical information obtained from relevance data is avail-able [17]. Our experiments also verify this. When such a corpus is not available, other opinionated corpora are also helpful: signifi-cant improvement over the baseline approach can be achieved using the  X  Movie Review  X  and the  X  X otel X  corpus. Here, the Wilcoxon signed-rank test is used to test the di ff erences between runs at a sig-nificance level of 0.05. If the annotated corpus is absent, marginal improvements may still be achieved with the help from a general text collection. When the most frequent opinion words in the Blog06 collection are used to expand the original queries, the improvement of MAP is still significant.

On the other hand, corpus based-sentiment expansion is sensi-tive to the resources. For example, when MPQA is used, MAP decreases. In fact, this corpus is very di ff erent to the Blog06 col-lection. A similar phenomenon happens in the  X  X roduct X  run. Table 7: Highest-contribution opinion words for the Blog06 and COAE08 collection.

Given the relevance data for 50 queries from 2006, we can es-timate the individual contribution of all English opinion words for the Blog06 collection. We first calculate the MAP using the origi-nal queries. Then each opinion word is used to expand the original queries with a group of predefined weights, and the MAP using these expanded queries is also obtained. The most significant im-provement is assigned as the contribution. The contributions of Chinese words for the COAE08 collection are also estimated from the overall relevance judgments just for comparison. Those opinion words that contribute the most to sentiment expansion are shown in Tabl e 7.

It is interesting to notice that those 5 English opinion words with the highest contribution are the most frequent words in the Blog06(op) corpus but in a di ff erent order. Because the machine learning approach can assign weights for the opinion words more accurately, RD performs significantly better than Blog06(op).
We can also find that the contributions of Chinese words are much lower than those of English words. This is the major rea-son why the performance improvement on the COAE08 collection is not as significant as the Blog06 collection. Other reasons include the semantic ambiguity of Chinese opinion words, and the lack of a training corpus similar to COAE08.
The query-dependent approach is based on pseudo relevance feed-back. For PRB, the pseudo relevance feedback run, 20 query-dependent opinion words were extracted from 5 top-ranked doc-uments. The mixture models are combined with PRB and the best performing query-independent runs using three di ff erent sentiment resources, which are Blog06(op), Blog06 and RD.

Table 5 and 6 show that pseudo relevance feedback does signif-icantly improve opinion retrieval. Table 8 gives some examples of the opinion words with the highest conditional probability given the original queries. Some of them are still general terms, but a lot of opinion words are now strongly associated with the original queries  X  we can extract  X  X enius X  and  X  X amous X  for the musician  X  X ozart X , as well as  X  X rotective X  and  X  X idelity X  for the organization  X  X llianz X .

The mixture relevance model e ff ectively integrates the query-Table 5: Comparison of opinion-finding MAP, R-prec, bPref, P@10 for di Baseline 0.2655 0.3252 0.2974 0.4770 Seed-7 0.2650 0.325 0.3058 0.4690 Blog06(op) 0.3097 0.3530 0.3395 0.5570 Web 0.2733 0.3313 0.3055 0.5100 Table 6: Comparison of opinion-finding MAP, R-prec, bPref, P@10 for di collection. The best in each column is highlighted.
 Baseline 0.3565 0.4046 0.3874 0.7700 Sougo 0.3571 0.4139 0.3880 0.7700 estimated by the PRB run from the top 5 documents.
 (believe) (rise up) Figure 1: Results of varying the number of opinion words for the Blog06 collections using both query-independent and query-dependent sentiment expansion. Three typical query-independent runs of Blog06(op), Blog06, and RD, as well as the query-dependent runs of PRB are compared. The X-axis shows the number of opinion words used in sentiment expansion. The Y-axis gives the corresponding MAP. independent and query-dependent approaches. The mean average precision of each mixture run is significantly better than both com-ponents for the Blog06 collection.

TREC Blog evaluations show that the most significant improve-ment on MAP over the the topic-relevance baseline is 17.0% for 50 queries from 2007, and the best average opinion-finding improve-ments over the standard topic baselines is 11.8% for 50 queries from 2008 [13, 18]. It can be observed from Table 5 that the im-provement on MAP over the baseline reaches as high as 18.5% for our best run of MRDP. The improvement on MAP for Blog06(op), RD, and MBoP are also higher than 16.0%. For the COAE08 col-lection, the increase in MAP for MHP is also a little higher that those of  X  X otel X  and PRB, although the improvement is not as sig-nificant.
We also investigated whether more opinion words lead to better retrieval performance. Figure 1 shows the change of MAP when the number of opinion words varies for the Blog06 collections.
We find that MAP is improved as soon as opinion words are combined into the model in query-independent runs. Why does sentiment expansion with such a small number of opinion words lead to such promising results? The reason is due to the wide cov-erage of these words. However, MAP is not improved further when more than 10 terms are used for expansion. The reason is probably due to the lack of specificity in query-independent methods.
For the query-dependent run of PRB, satisfactory improvement can be achieved when 5  X  10 opinion words are chosen for expan-sion. When more opinion words are selected, performance still increases slowly.

Considering these factors, 5 query-independent and 20 query-dependent opinion words are used for sentiment expansion.
For query-dependent sentiment expansion, another important fac-tor is the number of pseudo relevance documents. Figure 2 shows the change of MAP when the number of pseudo relevance doc-uments varies for the Blog06 collection. It shows that MAP in-creases significantly even only 3  X  4 documents are employed for Figure 2: Results of varying the number of pseudo relevance documents for the Blog06 collections using query-dependent sentiment expansion and mixture relevance model. The query-dependent runs of PRB, as well as all three mixture runs MBoP, MBP, and MRDP are compared. The X-axis shows the num-ber of pseudo relevance documents. The Y-axis gives the corre-sponding MAP. Table 9: Caparison of topic-relevance MAP, R-prec, bPref and P@10 for the Blog06 collection. The best in each column is highlighted.
 pseudo relevance feedback, and then remains stable or fluctuates somewhat. The reason is that not all documents with high rank-ing scores are relevant and opinionated, and the possibility of this decreases as the scores decrease. Therefore, 5 documents are em-ployed for pseudo relevance feedback.

We also observe the e ff ects on retrieval e ff ectiveness when vary-ing the parameters of  X  and  X  in Definitions 1 and 3, which are employed to adjust the weight of query-independent and query-dependent sentiment expansion approaches. Figure 3 gives the MAP surface of the run MRDP for the Blog06 collection. Surfaces of other mixture runs are similar. We have found that the surfaces are always concave or very close to concave, and they always have the same general form. Therefore, a simple hill-climbing search can be used to optimize MAP. Since the surface is almost concave we are likely to find the global maximum. For example, the optimal parameters for this run are  X  = 0 . 4and  X  = 0 . 4.

TREC evaluations show that a strongly performing topic-based retrieval baseline is very important in achieving good opinion find-ing retrieval performance [17]. We could also ask whether good opinion retrieval will improve topic-based retrieval. Table 9 shows the evaluation results for the topic-based retrieval for the Blog06 collection.

It can be observed that all these runs improve topic-based re-Figure 3: MAP surface over simplex of parameter values using MRDP for the Blog06 collection.  X  ranges from 0.1 to 1.0, and  X  ranges from 0.0 to 1 . 0  X   X  . The step size is 0.1. Z-axis shows the MAP. Figure 4: Query throughput of di ff erent sentiment expansion approaches on an Intel Xeon 3.00 GHz node for the Blog06 col-lection. trieval, and the improvements are significant except PRB. This means that the sentiment expansion approaches (in particular, query-indep-endent approaches) are not only e ff ective for opinion retrieval, but also for topic-based retrieval for such targets as named entities, products or concepts. The reason is because these targets are of-ten reviewed by web users.
 E ffi ciency is another important issue in information retrieval. Figure 4 shows the query throughput (that is, the number of queries processed per second) of some sentiment expansion approaches. Obviously, the baseline approach leads to the highest throughput. It can also be observed that query-independent sentiment expan-sion are much faster than query-dependent approaches. More ex-pansion terms lead to more processing time. It also takes some time to extract opinion words from the pseudo relevant documents. Therefore, query-dependent and mixture approaches result in lower throughput. However, these appro aches are still much faster than two-stage opinion retrieval, since only top-ranked documents are considered instead of all the retrieved documents. Considering both e ff ectiveness and e ffi ciency factors, we can conclude that: 1. If retrieval e ff ectiveness is preferred, mixture approaches should 2. If retrieval e ffi ciency is preferred, query independent senti-
In this paper, we have proposed the opinion relevance model , a formal framework for directly modeling the information need for opinion retrieval. In this framework, query terms are expanded with a small number of opinion words to represent the information need. We then propose a series of sentiment expansion approaches to find the most appropriate query-independent or query-dependent opin-ion words.

The proposed model has been verified on the Blog06 and COAE08 collections. The results show that very significant improvements can be obtained. We have also discussed the factors in opinion retrieval, including the number of opinion words in the expanded query, the number of documents in pseudo relevance feedback, the parameters in mixture relevance model, the impact of opinion re-trieval on topic-based retrieval, as well as the e ffi ciency issues.
Currently, the pseudo relevance feedback documents are ranked simply by their generative probabilities from the relevance model. As future work, we will take into consideration the diversity of the feedback documents, in order to retrieve more information about di ff erent facets of queried targets. Another line of interest is vary-ing the document priors. In our opinion relevance model, the doc-ument priors are set to be uniform. In fact, blogs and forum are widely used to express opinions, and their layout, link structure and user behavior may also be he lpful to judge th e quality and pop-ularity of opinions. We plan to incorporate this information as a document prior into the mixture relevance model.
This work was supported in part by the Center for Intelligent In-formation Retrieval, in part by the National Natural Science Foun-dation of China (Grant NO. 60673038), in part by the Ph.D Pro-grams Foundation of Ministry of Education of China (Grant NO. 200802460066), and in pa rt by the Shanghai Committee of Sci-ence and Technology, China (Grant No. 08511500302). Any opin-ions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. We thank Wang Bingqing, Xu Hongbo and Qian Xian for providing the Chinese sentiment resources. We also thank David Fisher for his helpful advice and support during algorithm implementation. [1] G. Amati, E. Ambrosi, M. Bianchi, C. Gaibisso, and tion of opinion words also takes some time. In fact, it takes about 120 seconds to calculate the contribution of an opinion word on the 50 training queries using an Intel Xeon 3.00 GHz node. However, the computation is implemented during training and will not slow the retrieval speed. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [3] W. B. Croft, D. Metzler, and T. Strohman. Search Engines: [4] K. Eguchi and V. Lavrenko. Sentiment retrieval using [5] D. Hannah, C. Macdonald, J. Peng, B. He, and I. Ounis. [6] V. Hatzivassiloglou and J. Wiebe. E ff ects of adjective [7] T. Joachims, H. Li, T.-Y. Liu, and C. Zhai. Learning to rank [8] S. Kullback and R. A. Leibler. On information and [9] J. D. La ff erty and C. Zhai. Document language models, [10] V. Lavrenko and W. B. Croft. Relevance-based language [11] B. Liu, M. Hu, and J. Cheng. Opinion observer: Analyzing [12] C. Macdonald and I. Ounis. The TREC Blogs06 collection: [13] C. Macdonald and I. Ounis. Overview of the TREC-2007 [14] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic [15] S.-H. Na, Y. Lee, S.-H. Nam, and J.-H. Lee. Improving [16] D. Oard, T. Elsayed, J. Wang, Y. Wu, P. Zhang, E. Abels, [17] I. Ounis, C. Macdonald, and I. Soboro ff . On the TREC Blog [18] I. Ounis, C. Macdonald, and I. Soboro ff .Overviewofthe [19] B. Pang and L. Lee. A sentimental education: Sentiment [20] B. Pang and L. Lee. Seeing stars: Exploiting class [21] B. Pang and L. Lee. Opinion mining and sentiment analysis. [22] J. M. Ponte and W. B. Croft. A language modeling approach [23] M. Taylor, H. Zaragoza, N. Craswell, S. Robertson, and [24] P. D. Turney and M. L. Littman. Measuring praise and [25] O. Vechtomova. Using subjective adjectives in opinion [26] J. Wiebe, T. Wilson, and C. Cardie. Annotating expressions [27] C. Zhai and J. La ff erty. A study of smoothing methods for [28] M. Zhang and X. Ye. A generation model to unify topic [29] W. Zhang and C. Yu. UIC at TREC 2007 Blog Track. In [30] J. Zhao, H. Xu, X. Huang, S. Tan, K. Liu, and Q. Zhang. [31] G. Zhou, H. Joshi, and C. Bayrak. Topic categorization for
