 2009b provides an example).
 in the number of parameters of the function approximator, as in the original TD algorithm. approximation arguments.
 illustrate the algorithms X  performance (Section 6). ping  X  : S  X  A  X  [0 , 1] . The value function of  X  , V  X  : S  X  R , maps each state s to a R  X  ( s )= P  X  ( s, s )= satisfies the so-called Bellman equation : everywhere, since the policy to be evaluated will be kept fixed. Assume that the policy to be evaluated is followed and it gives rise to the trajectory ( s that satisfies the following: s k  X  P We call ( s k ,r k ,s problem is still to estimate V given a finite number of transitions. The goal of policy evaluation becomes to find  X  such that V  X   X  V . the scalar-valued temporal-difference error , which is then used to update the parameter vector as follows: of convergence analysis) is assumed to satisfy the Robbins-Monro conditions:  X  parameters do not change: are drawn as in Assumption A1 and  X  = r +  X  V  X  ( s )  X  V  X  ( s ) . V manner. From now on we use the shorthand notation  X  =  X  ( s ) ,  X  =  X  ( s ) . with respect to the metric  X  D . Hence,  X  V = arg min V  X  M V  X  V 2 where V D is the weighted quadratic norm defined by V 2 TD(0) error for a given transition ( s, r, s ) is  X  = r +  X  X   X   X   X   X  . The negative gradient of the MSPBE objective function is: the  X  parameter. These weights are updated on a  X  X ast X  timescale, as follows: based on two slightly different calculations: indeed a good objective function.
 then  X   X  can be written as: The objective function that we will optimize is:  X  illustrates visually this objective function.
 d ( E [  X  V  X  ( s )  X  V  X  ( s ) ] is nonsingular. Then by (8). Details are omitted for brevity.
 tion here for convenience; it can be lifted, but the proofs become more involved. indeed converge). We now proceed to compute the gradient of this objective. S s.t. d ( s 0 ) &gt; 0 and (ii) W (  X  ) defined by W (  X   X  )= E [  X  V  X   X   X  X  X  V  X  ( s ) and where u  X  R n . Then where w = E [  X  X  ]  X  1 E [  X  X  ] .
 are zero when V  X  is linear in  X  ).
 Proof. The conditions of Lemma 1 are satisfied, so (10) holds. Denote  X  i =  X  definition and the assumptions, W ( u ) is a symmetric, positive definite matrix, so d exists in a small neighborhood of  X  . From this identity, we have:  X  1 2 and (ii) and the fact that S is finite. Now consider the identity which holds for any vector x  X  R n . Hence, using the definition of w , Using  X   X  =  X  X   X   X  and  X   X  =  X  2 V  X  ( s ) , we get Finally, observe that : which concludes the proof.
 to the nonlinear case. Weight w k is updated as before on a  X  X aster X  timescale: The parameter vector  X  k is updated on a  X  X lower X  timescale, either according to or, according to where the main reason for the projection is to facilitate convergence analysis. w note that  X  2 V  X   X  V  X  O ( of parameters of the function approximator (just like in TD(0)). projection  X  onto C , let operator  X   X  : C ( C )  X  C ( R n ) be the tangent space of  X  C at  X  . Consider the following ODE: more, U  X  C  X  K .
 GTD2 converge to K with probability one.
 Theorem 2 (Convergence of nonlinear GTD2) . Let ( s k ,r k ,s satisfy  X  Further assume that for each  X   X  C , E [  X   X   X  as k  X  X  X  .
 Proof. Let ( s, r, s ) be a random transition. Let  X   X  =  X  V  X  ( s ) ,  X   X  where f ( g ( g ( sequences, where G k =  X  ( r i ,  X  i ,w i ,s i ,i  X  k ; s { ( B for  X   X  C fixed, w  X  is the (unique) equilibrium point of where  X   X  = r +  X  V  X  ( s )  X  V  X  ( s ) . Clearly, w  X  = E  X   X   X  Then by Theorem 1 it follows that F (  X  )=  X  1 are verified.
 w C is a compact set.
 Theorem 3 (Convergence of nonlinear TDC) . Under the same conditions as in Theorem 2, the iterates computed via (13) , (15) satisfy  X  k  X  K , with probability one, as k  X  X  X  . The proof follows in a similar manner as that of Theorem 2 and is omitted for brevity. The true value function is V =(0 , 0 , 0) which is achieved as  X   X  X  X  X  . Here we used V 0 = (100 ,  X  70 ,  X  30) , a = V 0 , b = (23 . 094 ,  X  98 . 15 , 75 . 056) , with the divergence problem). All step sizes are then normalized by V the performance measure,  X  J Right panel: 9x9 Computer Go. [  X  linear function approximation. eligibility traces, and on using them for solving control problems. Acknowledgments References 89 X 129.
 Kaufmann.
 and reinforcement learning. SIAM Journal on Control And Optimization 38(2) : 447 X 469. MIT Press.
 In Advances in Neural Information Processing Systems 8 , pp. 1017-1023. MIT Press. Iteration In Advances in Neural Information Processing Systems 21 , pp. 441 X 448. Edition, Springer-Verlag.
 pp. 147 X 160.
 sity of Alberta Ph.D. thesis.
 3 :9 X 44.
 Processing Systems 21 , pp. 1609 X 1616. MIT Press.
 993 X 1000. Omnipress.
 approximation. IEEE Transactions on Automatic Control 42 :674 X 690. 1120. AAAI Press.
