 In recen t work, conditional Mark ov chain mo dels (CMM) have been used to extract information from semi-structured text (one example is the Conditional Random Field [10]). Applications range from nding the author and title in re-searc h pap ers to nding the phone num ber and street ad-dress in a web page. The CMM framew ork com bines a priori kno wledge enco ded as features with a set of lab eled train-ing data to learn an ecien t extraction pro cess. We will sho w that similar problems can be solv ed more e ectiv ely by learning a discriminativ e con text free grammar from train-ing data. The grammar has sev eral distinct adv antages: long range, even global, constrain ts can be used to disam biguate entity lab els; training data is used more ecien tly; and a set of new more powerful features can be introduced. The grammar based approac h also results in seman tic informa-tion (enco ded in the form of a parse tree) whic h could be used for IR applications like question answ ering. The spe-ci c problem we consider is of extracting personal con tact, or address, information from unstructured sources suc h as documen ts and emails. While linear-c hain CMMs perform reasonably well on this task, we sho w that a statistical pars-ing approac h results in a 50% reduction in error rate. This system also has the adv antage of being interactiv e, similar to the system describ ed in [9]. In cases where there are multi-ple errors, a single user correction can be propagated to cor-rect multiple errors automatically . Using a discriminativ ely trained grammar, 93.71% of all tok ens are lab eled correctly (compared to 88.43% for a CMM) and 72.87% of records have all tok ens lab eled correctly (compared to 45.29% for the CMM).
 H.4.0 [ Information Systems and Applications ]: [Gen-eral]; H.3.3 [ Information Storage and Retriev al ]: [Infor-mation searc h and retriev al]; H.3.5 [ Information Storage and Retriev al ]: [online information services] Algorithms, Exp erimen tation Text Tagging, Discriminativ e Mo dels, Discriminativ e Gram-mars, Conditional Random Fields, Perceptron Training, In-formation Retriev al
In this pap er, we consider the problem of automatically populating forms and databases with information that is available in an electronic but unstructur ed format. While there has been a rapid gro wth of online and other com-puter accessible information, little of this information has been schematized and entered into databases so that it can be searc hed, integrated and reused. For example, a recen t study sho ws that as part of the pro cess of gathering and managing information, curren tly 70 million work ers, or 59% of working adults in the U.S., complete forms on a regular basis as part of their job resp onsibilities.

One common example is the entry of customer informa-tion into an online customer relation managemen t system. In man y cases customer information is already available in an unstructured form on web sites and in email. The chal-lenge is in con verting this semi-structured information into the regularized or schematized form required by a database system. There are man y related examples including the im-portation of bibliograph y references from researc h pap ers and extraction of resume information from job applications. For the applications considered in this pap er, the source of the semi-structured information is \ra w text". The same approac h can be extended to work with semi-structured in-formation deriv ed from scanned documen ts (image based information) or voice recordings (audio based information).
Con tact information app ears routinely in the signature of emails, on web pages, and on fax cover sheets. The form of this information varies quite a bit; from a simple name and phone num ber to a complex multi-line blo ck con taining addresses, multiple phone num bers, emails, and web pages. E ectiv e searc h and reuse of this information requires eld extraction suc h as LastName , FirstName , StreetAd-dress , City , State , Post alCode , HomePhoneNumber etc. One way of doing this is to consider the text blo ck as a sequence of words/tok ens, and assign lab els ( elds of the database) to eac h of these tok ens (see Figure 1). All the tok ens corresp onding to a particular lab el are then entered into the corresp onding eld of the database. In this simple way a tok en classi cation algorithm can be used to perform schematization. Common approac hes for classi cation in-clude maxim um entrop y mo dels and Mark ov mo dels.
We presen t a classi cation algorithm based on discrim-inativ ely trained con text free grammars (CF G) that sig-ni can tly outp erforms prior approac hes. Besides achieving substan tially higher accuracy rates, we sho w that a CF G based approac h is better able to incorp orate exp ert kno wl-edge (suc h as the structure of the database or form), less likely to be overtrained, and is more robust to variations in the tok enization algorithm.
Free-form con tact information suc h as that found on web pages, emails and documen ts typically does not follo w a rigid format, even though it often follo ws some con ventions. The lack of a rigid format mak es it hard to build a non-statistical system to recognize and extract various elds from this semi-structured data. Suc h a non-statistical system migh t be built for example by using regular expressions and lexicon lists to recognize elds. One suc h system is describ ed in [20]. This system looks for individual elds suc h as phone num bers by matc hing regular expressions, and recognizing other elds by the presence of keyw ords suc h as \Fax", \Re-searc her", etc., and by their relativ e position within the blo ck (for example, it looks in the beginning for a name). However, because of spelling (or optical character recogni-tion) errors and incomplete lexicon lists, even the best of de-terministic systems are relativ ely in exible, and hence break rather easily . Further, there is no obvious way for these sys-tems to incorp orate and propagate user input or to estimate con dences in the lab els. For these reasons, it mak es sense to consider a statistical approac h to the problem of extract-ing information from semi-structured sources.
 A simple statistical approac h migh t be to use a Na X ve Bayes classi er to classify (lab el) eac h word individually . However suc h classi ers have diculties using features whic h are not indep enden t. Maxim um entrop y classi ers ([15]) can use arbitrarily complex, possibly dep enden t features, and tend to signi can tly outp erform Na X ve Bayes classi ers when there is sucien t data. However, a common weak-ness of both these approac hes is that eac h word is classi ed indep enden tly of all others. Because of this, dep endencies between lab els cannot be used for classi cation purp oses. To see that lab el dep endencies can help impro ve recogni-tion, consider the problem of assigning lab els to the word sequence \GREWTER JONES". The correct lab el sequence is FirstName LastName . Because GREWTER is an un-usual name, classifying it in isolation is dicult. But since JONES is very likely to be a LastName , this can be used to infer that GREWTER is probably a FirstName . Thus, a Mark ov dep endency between the lab els can be used to disam biguate the rst tok en.

Mark ov mo dels explicitly capture the dep endencies be-tween the lab els. A Hidden Mark ov Mo del (HMM) [17] mo dels the lab els as the states of a Mark ov chain, with eac h tok en a probabilistic function of the corresp onding lab el. A rst order Mark ov chain mo dels dep endencies between the lab els corresp onding to adjacen t tok ens. While it is possi-ble to use higher order Mark ov mo dels, they are typically not used in practice because suc h mo dels require much more data (as there are more parameters to estimate), and require more computational resources for learning and inference. A dra wbac k of HMM based approac hes is that the features used must be indep enden t, and hence complex features (of more than one tok en) cannot be used. Some pap ers explor-ing these approac hes include [1, 2, 4, 3, 18, 19].
A Conditional Mark ov Mo del (CMM) ([10, 5, 21]) is a dis-criminativ e mo del that is a generalization of both maxim um entrop y mo dels and HMMs. Formally , they are undirected graphical mo dels used to compute the join t score (sometimes as a conditional probabilit y) of a set of nodes designated as hidden nodes given the values of the remaining nodes (des-ignated as observ ed nodes). The observ ed nodes corresp ond to the tok ens, while the hidden nodes corresp ond to the (un-kno wn) lab els corresp onding to the tok ens. As in the case of HMMs, the hidden nodes are sequen tially ordered, with one link between successiv e hidden nodes. While a HMM mo del is generativ e, the conditional Mark ov mo del is discrimina-tive. The conditional Mark ov mo del de nes the join t score of the hidden nodes given the observ ed nodes. This pro-vides the exibilit y to use complex features whic h can be a function of any or all of the observ ed nodes, rather than just the observ ed node corresp onding to the hidden node. Lik e the Maxim um Entrop y mo dels the conditional Mark ov mo del uses complex features. Lik e the HMM the CMM can mo del dep endencies between lab els. In principle a CMMs can mo del third or fourth order dep endencies between la-bels though most published pap ers use rst order mo dels because of data and computational restrictions.
 Varian ts of conditional Mark ov mo dels include Conditional Random Fields (CRFs) [10], voted perceptron mo dels [5], and max-margin Mark ov mo dels [21]. CRFs are the most mature and have sho wn to perform extremely well on infor-mation extraction tasks ([14, 16, 15, 13, 19]). A CRF mo del is used in [9] to lab el tok ens corresp onding to con tact blo cks, to achiev e signi can tly better results than prior approac hes to this problem.
While CMMs can be very e ectiv e, there are clear limita-tions that arise from the \Mark ov" assumption. For exam-ple, a single \unexp ected" state/lab el can thro w the mo del o . Further, these mo dels are incapable of enco ding some types of complex relationships and constrain ts. For exam-ple, in a con tact blo ck, it may be quite reasonable to exp ect only one city name. However, since a Mark ov mo del can only enco de constrain ts between adjacen t lab els, constrain ts on lab els that are separated by a distance of more than one can-not be easily enco ded without an explosion in the num ber of states (possible values of lab els), whic h then complicates learning and deco ding.

Mo deling non-lo cal constrain ts is very useful, for exam-ple, in the disam biguation of business phone num bers and personal phone num bers. To see this, consider the two con-tact blo cks sho wn in Figure 2. In the rst case, it is nat-ural to lab el the phone num ber as a HomePhoneNumber . In the second case, it is more natural to lab el the phone num ber as a BusinessPhoneNumber . Humans tend to use the lab els/tok ens near the beginning to distinguish the two. Therefore, the lab el of the last tok en dep ends on the lab el of the rst tok en. There is no simple way of enco ding this very long-range dep endence with any practical Mark ov mo del.
A grammar based mo del allo ws us to \escap e the linear tyrann y of these n -gram mo dels and HMM tagging mo dels" ([11]). A con text-free grammar allo ws speci cation of more complex structure with long-range dep endencies, while still allo wing for relativ ely ecien t lab eling and learning from lab eled data. One possible way to enco de the long-range dep endence required for the above example migh t be to use a grammar whic h con tains di eren t pro ductions for business con tacts, and personal con tacts. The presence of the pro-ductions BizCont act ! BizName Address BizPhone and PersonalCont act ! Name Address HomePhone would allo w the system to infer that the phone num ber in the rst blo ck is more likely to be a HomePhone while the phone num ber in the second is more likely to be a Busi-nessPhone . The correct/optimal parse of the blo cks au-tomatically tak es the long-range dep endencies into accoun t naturally and ecien tly.

As another example imagine a system whic h has a detailed database of city and zip code relationships. Giv en a badly missp elled city name, there may be man y poten tial explana-tions (suc h as a rst name or compan y name). If the address blo ck con tains an unam biguous zip code, this migh t pro vide the information necessary to realize that \No o Yic k" is ac-tually the city \New York". This becomes esp ecially imp or-tan t if there is some ambiguit y with regards to the tok ens themselv es (whic h migh t occur for example if the tok ens are outputs of a speech recognition system, or a image based system). Therefore, if the name of the city is missp elled, or incorrectly recognized, we can use the presence of an unam-biguous zip code to mak e better predictions about the city. In a simple linear-c hain Mark ov mo del, if the state app ears between the city and the zip, the dep endence between the zip and the city is lost.

Lab eling using CMMs has been used as an appro ximation to, and as an intermediate step in, man y imp ortan t shal-low parsing problems including NP-c hunking. While CMMs achiev e reasonably good accuracy , the accuracy pro vided by a full blown statistical parser is often higher. The main ad-vantage of a CMM is computational speed and simplicit y. We argue that it is more natural to mo del a con tact blo ck using a CF G than a CMM. This is because a con tact blo ck is more than just a sequence of words. There is clearly some hierarc hical structure to the blo ck. For example, the bigram FirstName LastName can be recognized as a Name as can LastName, FirstName . Similarly , a Address can be of the form StreetAddress, City State Zip and also of the form StreetAddress . It intuitiv ely mak es sense that these di eren t forms occur (with di eren t probabilities) indep en-den tly of their con text. While this is clearly an appro xima-tion to the realit y, it is perhaps a better appro ximation than the Mark ov assumption underlying chain-mo dels.

The grammatical parser accepts a sequence of tok ens, and returns the optimal (lowest cost or highest probabilit y) parse tree corresp onding to the tok ens. Figure 3 sho ws a parse tree for the sequence of tok ens sho wn in Figure 1. The leaves of the parse tree are the tok ens. Eac h leaf has ex-actly one paren t, and paren ts of the leaves are the lab els of the leaves. Therefore, going from a parse tree to the lab el sequence is very straigh tforw ard. Note that the parse tree represen ts a hierarc hical structure beyond the lab els. This hierarc hy is not arti cially imp osed, but rather occurs nat-urally . Just like a language mo del, the substructure Name and Address can be arranged in di eren t orders : both Name Address and Address Name are valid examples of a con tact blo ck. The reuse of comp onen ts allo ws the grammar based approac h to more ecien tly generalize from limited data than than a linear-c hain based mo del. This hierarc hi-cal structure is also useful when populating forms with more than one eld corresp onding to a single lab el. For example, a con tact could have multiple addresses. The hierarc hical structure allo ws a sequence of tok ens to be aggregated into a single address, so that di eren t addresses could be entered into di eren t elds.
A con text free grammar (CF G) consists of a set of ter-minals w k V nated start sym bol N 1 , and a set of rules or pro ductions non terminals. We asso ciate a score S ( R i ) with eac h rule R . A parse tree is a tree whose leaves are lab eled by ter-minals and interior nodes are lab eled by non terminals. Fur-ther if a node N j i is the lab el of an interior node, then the child nodes are the terminals/non terminals in i where R i : N j i ! j . The score of a parse tree T is given by P w w 2 : : : w m is a parse tree whose leaves are w 1 w 2 : : : w Giv en the scores asso ciated with all the rules, and a given sequence of terminals w 1 w 2 : : : w m , the CKY algorithm can compute the highest scoring parse tree in time O ( m 3 r ), whic h is reasonably ecien t when m is relativ ely small.
Generativ e mo dels suc h as probabilistic CF Gs can be de-scrib ed using this form ulation by taking S ( R i ) to be the logarithm of the probabilit y P ( R i ) asso ciated with the rule. If the probabilit y P ( R i ) is a log-linear mo del and N be deriv ed from the sequence w a w a +1 : : : w b (also denoted N i ) w a w a +1 : : : w b ), then P ( R i ) can be written as f f k g F k =1 is the set of features and ( R i ) is a vector of pa-rameters represen ting feature weigh ts (possibly chosen by training). Z ( ;a;b;N j ! j ) is called the partition function and is chosen to ensure that the probabilities add up to 1.
In order to learn an accurate generativ e mo del, a lot of e ort has to be spent learning the distribution of the gener-ated leaf sequences. Since the set of possible leaf sequences are very large, this requires a large amoun t of training data. However, in the applications of interest, the leaves are typ-ically xed, and we are only interested in the conditional distribution of the rest of the parse tree given the leaves. Therefore, if we set out to only learn the conditional dis-tribution (or scores) of the parse trees given the leaves, we can poten tially manage with considerably less data (and less computational e ort).

A similar observ ation has been made in the mac hine learn-ing comm unit y. Man y of the mo dern approac hes for classi -cation are discriminativ e (e.g. Supp ort Vector Mac hines [6] and AdaBo ost [8]). These techniques typically generalize better than generativ e techniques because they only mo del the boundary between classes (whic h is closely related to the conditional distribution of the class lab el), rather than the join t distribution of class lab el and observ ation.
A generativ e grammar de nes a language, and asso ciates probabilities with eac h sen tence in the language. In con-trast a discriminativ e grammar only asso ciates scores with the di eren t parses of a particular sequence of terminals. Computationally there is little di erence between the gen-erativ e and discriminativ e mo del -the complexit y for nding the optimal parse tree (the inference problem) is iden tical in both cases. For our generativ e mo del, the scores asso ciated with the rule R i : N j i ! i is given by when applied to the sequence w a w a +1 : : : w b . Note that in this case the features can dep end on all the tok ens, not just the subsequence of tok ens spanned by N j i . The discrimina-tive mo del allo ws for a richer collection of features as we do not require indep endence between the features. Since a dis-criminativ e mo del can alw ays use the set of features that a generativ e mo del can, there is alw ays a discriminativ e mo del whic h performs at least well as the best generativ e mo del. In man y exp erimen ts, discriminativ e mo dels tend to outp er-form generativ e mo dels[5, 6, 9, 10, 14].
As we men tioned before, the hierarc hical structure of con-tact blo cks is not arbitrary . It is fairly natural to com bine a FirstName and a LastName to come up with a Name . This leads to the rule Name ! FirstName LastName . Other pro ductions for Name include We can build on Name by mo deling titles and suxes us-ing pro ductions FullName ! Name , FullName ! Ti-tle Name Suffix . We can construct other rules based on commonly occurring idioms. For example, we migh t have Loca tion ! City State Zip . Suc h a grammar can be constructed by an \exp ert" after examining a num ber of examples.

Alternativ ely an automatic grammar induction technique may be used. In our case, we used a com bination of the two. Based on a database of 1487 lab eled examples of con tact records dra wn from a div erse collection of sources, we had a program extract commonly occurring \idioms" or patterns. A human exp ert then sifted through the generated patterns to decide whic h made sense and whic h did not. Most of the rules generated by the program, esp ecially those whic h occurred with high frequency , made sense to the human ex-pert. The human exp ert also took some other considerations into accoun t, suc h as the requiremen t that the pro ductions were to be binary (though the pro ductions were automati-cally binarized by another program). Another requiremen t was imp osed by training requiremen ts men tioned in Section 4.4.
The features selected included easily de nable functions like word coun t, regular expressions matc hing tok en text (lik e Cont ainsNewLine, Cont ainsHyphen, Cont ainsDig-its, PhoneNumLike ), tests for inclusion in lists of standard lexicons (for example, US rst names, US last names, com-monly occurring job titles, state names, street suxes), etc. These features are mostly binary , and are de nable with minimal e ort. They are similar to those used by the CRF mo del describ ed in [9]. However in the CRF mo del, and in all CMMs, the features can only relate the sequence of ob-serv ations w i , the curren t state s t , the previous state s and the curren t time t (i.e. f j ( s t ; s t 1 ; w 1 ; w
In con trast the discriminativ e grammar admits additional features of the form f k ( w 1 ; w 1 ; : : : ; w m ; a; b; c; N where N j i spans w a w a +1 : : : w b . In principle, these features are much more powerful because they can analyze the se-quenc e of words asso ciated with the curren t non-terminal. For example, consider the sequence of tok ens Mavis Wood Products . If the rst and second tok ens are on a line by themselv es, then Wood is more likely to be interpreted as a LastName . However, if all three are on the same line, then they are more likely to be interpreted as part of the compan y name. Therefore, a feature AllOnTheSameLine (whic h when applied to any sequence of words returns 1 if they are on the same line) can help the CF G disam biguate between these cases. This type of feature cannot be included in a conditional Mark ov mo del.
The standard way of training a CF G is to use a cor-pus annotated with tree structure, suc h as the Penn Tree-bank[12]. Giv en suc h a corpus, algorithms based on coun ting can be used to determine the probabilities (parameters) of the mo del. However, annotating the corp ora with the tree-structure is typically done man ually whic h is time consum-ing and exp ensiv e in terms of human e ort. The only train-ing data required for training the Mark ov mo dels are the sequences of words and the corresp onding lab el sequences. In this section we sho w that we can automatically generate the parse tree required for training the grammars from just the lab el sequences for a certain class of grammars.
Giv en a parse tree T for a sequence w 1 w 2 : : : w m , let the reduced parse tree T 0 be the tree obtained by deleting all the leaves of T . Figure 4 sho ws the reduced parse tree obtained from Figure 3. In this reduced parse tree, the lab el sequence ` ` 2 : : : ` m corresp onds to the leaves. We can think of this reduced tree as the parse tree of the sequence ` 1 ` 2 : : : ` over a di eren t grammar in whic h the lab els are the termi-nals. This new grammar is easily obtained from the original grammar by simply discarding all rules in whic h a lab el oc-curs on the LHS. If G 0 is the reduced grammar, then we can use G 0 to parse any sequence of lab els. Note that G 0 can parse a sequence ` 1 ` 2 : : : ` m if and only if there is a sequence of words w 1 w 2 : : : w m with ` i being the lab el of w that G is lab el-unam biguous if G 0 is unam biguous (i.e., for any sequence ` 1 ` 2 : : : ` m , there is at most one parse tree for this sequence in G 0 ). To generate a parse tree for a lab el un-ambiguous grammar, given the lab el, we use the follo wing two step metho d. 1. Generate a (reduced) parse tree for the lab el sequence 2. Glue on the edges of the form ` i ! w i to the leaves of It is very easy to see that given any sequence of words w 1 : : : w m , and their corresp onding lab els ` 1 : : : ` gives us the unique parse tree for w 1 : : : w m whic h is compat-ible with the lab el sequence ` 1 : : : ` m (if one exists). There-fore, this metho d allo ws to generate a collection of parse trees given a collection of lab eled sequences.

Doing this has at least two adv antages. First, it allo ws for an apples-to-apples comparison with the CRF based meth-ods since it requires no additional human e ort to generate the parse trees (i.e., both mo dels can work of exactly the same input). Secondly , it ensures that changes in grammar do not require human e ort to generate new parse trees.
There is a natural extension of this algorithm to handle the case of grammars that are not lab el-unam biguous. If the grammar is not lab el-unam biguous, then there could be more than one tree corresp onding to a particular lab eled ex-ample. We could then pick an arbitrary tree, or possibly a tree that optimizes some other criterion. We could even use an EM-st yle algorithm to learn a probabilistic grammar for the reduced grammar. We exp erimen ted with some gram-mars whic h have a mo derate amoun ts of lab el-am biguit y. In this case, we simply picked a tree with the smallest heigh t. In our exp erimen ts, we did not observ e any performance degradation by mo derate amoun ts of ambiguit y.
The goal of training is to nd the parameters that max-imize some optimization criterion, whic h is typically tak en to be the maxim um likeliho od criterion for generativ e mo d-els. A discriminativ e mo del assigns scores to eac h parse, and these scores need not necessarily be though t of as probabili-ties. A good set of parameters maximizes the \margin" be-tween correct parses and incorrect parses. One way of doing this is using the technique of [21]. However, we use a simpler algorithm to train our discriminativ e grammar. This algo-rithm is a varian t of the perceptron algorithm and is based on the algorithm for training Mark ov mo dels prop osed by Collins in [5]. Supp ose that T is the collection of training data ( w i ; ` a ; T a ) j 1 i m , where w i = w i 1 w i 2 the collection of words, ` i = ` i 1 ` i 2 : : : ` i n ing lab els, and T i is the parse tree. For eac h rule R in the grammar, we seek a setting of the parameters ( R ) so that the resulting score is maximized for the correct parse T i of w i for 0 i m . Our algorithm for training is sho wn in Figure 5. Con vergence results for the perceptron algorithm app ear in [7, 5] when the data is separable. In [5] some gen-eralization results for the inseparable case are also given to justify the application of the algorithm.
Kristjansson et. al introduced the notion of correction propagation for interactiv e form lling tasks [9]. In this sce-nario the user pastes unstructured data into the form lling system and observ es the results. Errors are then quic kly corrected using a drag and drop interface. After eac h cor-rection the remaining observ ations can be relab eled so as to yield the lab eling of lowest cost constrained to matc h the corrected eld (i.e. the corrections can be propagate d ). For inputs con taining multiple lab eling errors correction prop-agation can save signi can t e ort. Any score minimization framew ork suc h as a CMM or CF G can implemen t correc-tion propagation. The main value of correction propagation can be observ ed on examples with two or more errors. In the ideal case, a single user correction should be sucien t to accurately lab el all the tok ens correctly .

Supp ose that the user has indicated that the tok en w i actually has lab el ` i . The CKY algorithm can be mo di-ed to pro duce the best parse consisten t with this lab el. Suc h a constrain t can actually accelerate parsing, since the searc h space is reduced from the set of all parses to the set of all parses in whic h w i has lab el ` i . CKY returns the optimal constrained parse in the case where all alternativ e non-terminals are remo ved from the cell asso ciated with w
For our exp erimen ts, we used 1487 con tact records for training and 400 con tact records for testing. The data was collected from web pages and email, and was hand-lab eled with 24 lab els ( FirstName, MiddleName, LastName, NickName, Suffix, Title, JobTitle, Comp anyName, Dep artment, AddressLine, City, State, Countr y, Post al-Code, HomePhone, Fax, Comp anyPhone, DirectCom-panyPhone, MobilePhone, Pager, Email, Inst antMes-sage, WebP age ). Only the lab els for these examples was generated by hand. The complete parse tree for these ex-amples were generated as describ ed in the previous section.
Eac h example tok en sequence was analyzed with a set of ad hoc features. Appro ximately 100 simple regular ex-pression features were used, including IsCapit alized, All-Caps, IsDigit, Numeric, Cont ainsD ash, EndsInPeriod, Const ainsA tSign , etc. In addition there are 9 lexicon lists including: LastNames, FirstNames, States, Cities, Countries, JobTitles, Comp anyNameComponents, Ti-tles, StreetNameComponents .

Using these basic features as input, one addition high level feature is added: the output of a tok en classi er. The to-kens in the sequence are lab eled using a boosted collection of decision trees [8]. Eac h tree is of depth 3 and there are a total of 42 trees in the nal classi er. The tok en classi-cation performance of this algorithm is surprisingly high. Often it is within a few percen t of the best Mark ov mo del.
For these exp erimen ts we compare with the voted percep-tron conditional Mark ov mo del due to Collins [5]. The form of the Collins mo del is very similar to the more well kno wn CRF (the deco ding algorithms are iden tical). While exp er-imen ts sho w that the two systems are quite comp etitiv e, the implemen tation of the learning algorithm for the Collins mo del is much simpler. Our results using the Collins mo del are very similar to earlier results obtained by Kristjansson et. al on a similar problem [9].

The boosted classi er is used as an input feature both for the Collins mo del and the CF G mo del. The boosted classi er achiev es quite a reasonable level of performance by itself. When the boosted classi er was run on the data set corresp onding to the results sho wn in Table 1, it achiev ed a word error rate of 13.49% and a record error rate of 65.86%. We have noticed that using the boosted classi er as an in-put feature speeds con vergence of the learning by orders of magnitude. Our conjecture is that the boosted classi er frees the structural mo dels from mo deling the local evidence and allo ws them to focus on learning dep endencies.
In addition the CF G mo del is given access to a set of re-gion features whic h cannot be included in Collins mo del (as describ ed above). There are a total of 80 regions features, including features suc h as Cont ainsNewLine and Begin-sOnNa turalBound ary whic h test for natural end of eld mark ers like new lines and commas. Other region features are extensions of the above men tioned tok en based regular expression and lexicon features to sequences of tok ens.
There are a num ber of ways of comparing the qualit y of di eren t lab eling techniques. The most obvious is to ex-amine the word lab eling error rate. While this migh t be the most appropriate criterion for a batc h setting, there are other appropriate criteria for interactiv e settings. In the in-teractiv e con text, it is imp ortan t to minimize the num ber of actions required of the user. Every time even one word of the record is incorrectly lab eled, the user is forced to correct the lab eling. Therefore, the percen tage of records in whic h all lab els are correctly assigned is perhaps a better metric. Both types or results are summarize in Table 1. Interest-ingly the Collins mo del has a fairly high word accuracy rate while performing poorly on record accuracy .

In cases where there is more than one mislab eled tok en in the record, it is possible to propagate the correction that the user enters for one of the mislab eled tok ens to correct other tok ens as well. The last row of Table 1 sho ws the per-cen tage of records con taining at least two mislab eled tok ens that were completely corrected with just one user action. In summary , the CF G approac h lab els more than 70% of the records completely correctly . For those records where there are more than 2 errors, the CF G corrects the entire record with a single word correction 50% of the time. This perfor-mance is signi can tly better than the best results obtained with a Mark ov mo del.

A num ber of exp erimen ts were performed to determine the relationship between training set size and testing error. Both the CMM and CF G were trained with di eren t sized training sets. We exp erimen ted with training sets of size 10%, 20%, 30%, 40%, 50% 60% and 70% of our base train-ing set. For eac h case we picked 5 random samples of the prescrib ed size and trained both the CMM and the gram-mar on this reduced training set. In every case, we observ ed that the grammar outp erformed the Collins mo del and Fig-ures 6 and 7 sho w the impro vemen t of word error rate and the record error rate of the grammar over the CRF. It can be seen that there is some variabilit y in the impro vemen t, but the variabilit y decreases with increase in the size of the training set. Because of the strong prior available to the grammar (the rules of the grammar), the grammar is able to better generalize even when there is just a little training data. The CMM is not able to do quite so well when the training size is small and hence the grammar sho ws a large impro vemen t in word error rates over the CRF for small training sizes. However, even when there is a large amoun t of data, the grammar sho ws a large impro vemen t over the CMM.
 The CF G is also better able to propagate user corrections. As sho wn in Table 1, the CMM can use a single user correc-tion to x all errors in about 15% of the cases where there are at least two mislab eled tok ens. In con trast, the CF G can propagate the user correction to x over 50% of these cases.
This pap er marries the powerful tools of statistical natu-ral language pro cessing to the analysis of non-natural lan-guage text. Exp erimen ts demonstrated that a discrimina-tively trained con text free grammar can more accurately ex-tract con tact information than a similar conditional Mark ov mo del.

There are sev eral adv antages pro vided by the CF G sys-tem. The CF G, because its mo del is hierarc hically struc-tured, can generalize from less training data. What is learned about BusinessPhoneNumber can be shared with what is learned about HomePhoneNumber , since both are mo d-eled as PhoneNumber . The CF G also allo ws for a rich collection of features whic h can measure prop erties of a se-quence of tok ens. The feature AllOnOneLine is a very powerful clue that an entire sequence of tok ens has the same lab el (e.g. a title in a pap er, or a street address). Another adv antage is that the CF G can propagate long range lab el dep endencies ecien tly. This allo ws decisions regarding the rst tok ens in an input to e ect the decisions made regard-ing the last tok ens. This propagation can be quite complex and multi-faceted.

The e ects of these adv antages are man y. For example a grammar based approac h also allo ws for selectiv e retraining Figure 6: Record error-rate reduction using a discriminativ ely-trained grammar over a CMM Figure 7: Word error-rate reduction using a discriminativ ely-trained grammar over a CMM of just certain rules to t data from a di eren t source. For example, Canadian con tacts are reasonably similar to US con tacts, but have di eren t rules for postal codes and street addresses. In addition a grammatical mo del can enco de a stronger set of constrain ts (e.g. there should be exactly one city, exactly one name, etc. ). Grammars are much more robust to tok enization e ects, since the two tok ens whic h result from a word whic h whic h is split erroneously can be analyzed together by the grammar's sequence features
The application domain for discriminativ ely trained con-text free grammars is quite broad. It should be possible to analyze a wide variet y of semi-structured forms suc h as resumes, tax documen ts, SEC lings, and researc h pap ers. [1] Vina jak R. Bork ar, Kaustubh Deshm ukh, and Sunita [2] Remco Bouc kaert. Low level information extraction: [3] Claire Cardie and David Pierce. Prop osal for an [4] Ric h Caruana, Paul Hodor, and John Rosen berg. High [5] M. Collins. Discriminativ e training metho ds for [6] Corinna Cortes and Vladimir Vapnik. Supp ort-v ector [7] Y. Freund and R. Schapire. Large margin [8] Y. Freund and R. E. Schapire. Exp erimen ts with a [9] T. Kristjansson, A. Culotta, P. Viola, and [10] John La ert y, Andrew McCallum, and Fernando [11] C. D. Manning and H. Sch X  utze. Foundations of [12] M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn tyre, [13] Andrew McCallum. Ecien tly inducing features of [14] Andrew McCallum and Wei Li. Early results for [15] Kamal Nigam, John La ert y, and Andrew McCallum. [16] David Pin to, Andrew McCallum, Xing Wei, and [17] L.R. Rabiner. A tutorial on hidden mark ov mo dels. In [18] Tobias Sche er, Christian Decomain, and Stefan [19] Fei Sha and Fernando Pereira. Shallo w parsing with [20] J. Stylos, B. A. My ers, and A. Faulring.
 [21] B. Task er, D. Klein, M. Collins, D. Koller, and
