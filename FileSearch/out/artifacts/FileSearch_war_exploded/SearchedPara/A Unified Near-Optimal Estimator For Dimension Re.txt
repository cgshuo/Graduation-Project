 Faculty of Computing and Information Science Dimension reductions in the l In many applications, it is often the case that we only need th e l A l ( 0 &lt;  X   X  2 ) properties in massive data using a small (memory) space. Denote the leading two rows in the data matrix A by u is to choose an appropriate norm. For example, the l form K ( x, y ) = exp (  X   X  P classifications. The l to be a natural measure of sparsity [6]. In the extreme case, w hen  X   X  0+ , the l the Hamming norm (i.e., the number of non-zeros in the vector ). l norm other than l ( matrix B now contains enough information to recover the original l Applying stable random projections on u and v Thus, the task is to estimate the scale parameter from k i.i.d. samples x when we seek estimators that are both accurate and computati onally efficient. and using the symmetry of S (  X , d  X  Lemma [10] for dimension reduction in l improved for certain  X  , especially for large samples (e.g., as k  X  X  X  ). 1.1 Our Contribution: the Fractional Power Estimator The basic idea is straightforward. We first obtain an unbiase d estimator of d  X  We then estimate d involves only P k 1.2 Applications The method of stable random projections is useful for efficiently computing the l or distances) in massive data, using a small (memory) space. When we treat  X  as a tuning parameter, i.e., re-computing the l stable random projections will be even more desirable as a cost-saving device. We assume k i.i.d. samples x Lemma 1 Suppose x  X  S  X , d If  X  = 2 , i.e., x  X  S (2 , d (2) ) = N (0 , 2 d (2) ) , then for  X  &gt;  X  1 ,  X (1  X  z ) X ( z ) =  X  sin(  X z ) and the duplication formula  X ( z ) X  z + 1 2 = 2 1  X  2 z The fractional power estimator is defined in Lemma 2. See the proof in Appendix A. Lemma 2 Denoted by  X  d where
Asymptotically (i.e., as k  X  X  X  ), the bias and variance of  X  d Note that in calculating  X  d all other terms are basically constants and can be pre-compu ted. (except for  X  = 2 ), which will be proved in Lemma 3. 3.1 Special cases exists for  X  1 &lt;  X  &lt;  X  when  X  &lt; 2 and exists for any  X  &gt;  X  1 when  X  = 2 . When  X  = 2 , since  X   X  (2) = 1 , the fractional power estimator becomes 1 arithmetic mean estimator. We will from now on only consider 0 &lt;  X  &lt; 2 . estimator, which is asymptotically optimal when  X  = 0+ [13]. variance as the geometric mean estimator . 3.2 The Asymptotic (Cram  X  er-Rao) Efficiency For an estimator  X  d and asymptotically when  X  = 0+ .
  X  d median estimator  X  d see that the fractional power estimator  X  d only consider  X  &lt; 2 , the efficiency of  X  d 3.3 Theoretical Properties We can show that, when computing the fractional power estimator  X  d Lemma 3 Part 1: g (  X  ;  X  ) = 1 is a convex function of  X  .
 Part 2: For 0 &lt;  X  &lt; 2 , the optimal  X   X  = argmin 3.4 Comparing Variances at Finite Samples estimators can be computed exactly without simulations.
 Figure 3 indicates that the fractional power estimator  X  d mance unless  X  is close to 2. After k  X  50 , the advantage of  X  d k = 10 ), it is quite accurate except when  X  approaches 2. The fractional power estimator  X  d in because the power 1 / X   X  is just a constant. However, P k  X  [3], which proved that there is no hope to recover the origina l l and linear estimators without incurring large errors.
 additional simulations (not included in this paper) indica te that  X  d probability behavior as the geometric mean estimator, when  X   X  1 . While there are already many papers on dimension reductions in the l the l machine learning to consider the l tuning parameter and re-run the learning algorithms many times for better performance. work will help advance the state-of-the-art of dimension re ductions in the l By Lemma 1, we first seek an unbiased estimator of of d  X  whose variance is A biased estimator of d Because f ( x ) is convex, removing the O 1 We call this new estimator the  X  X ractional power X  estimator : where we plug in the estimated d  X 
The optimal  X  , denoted by  X   X  , is then to re-write g (  X  ;  X  ) as With respect to  X  , the first two derivatives of g (  X  ;  X  ) are
Also, tive (and hence we need P  X 
We show that when  X  &lt;  X  1 ,  X  X 
