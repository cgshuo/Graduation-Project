 Question answering (QA) over a la rge-scale knowledge base (KB) such as Freebase is an importa nt natural language processing application. There are linguisti cally oriented semantic parsing techniques and machine learning motivated statistical methods. Both of these approaches face a key challenge on how to handle diverse ways natural questions ca n be expressed about predicates and entities in the KB. This paper is to investigate how to combine these two approaches. We frame th e problem from a proof-theoretic perspective, and formulate it as a proof tree search problem that seamlessly unifies semantic parsi ng, logic reasoning, and answer ranking. We combine our word entity joint embedding learned from web-scale data with other surface -form features to further boost accuracy improvements. Our real-time system on the Freebase QA task achieved a very high F1 score (47.2) on the standard Stanford WebQuestions benchmark test data. H.3.3 [ Information Search and Retrieval ]: Selection process; I.2.3 [ Deduction and Theorem Proving ]: Deduction; I.2.6 [ Learning ]: Concept learning; I.2.7 [ Natural Language Processing ]: Language parsing and understanding Algorithms, Experimentation Question answering, Freebase, Joint embedding, Proof tree Question Answering (QA) from an existing knowledge base (KB) has drawn significant interest from both the industry and the academia [12], [8], [10], [3], [4], [2], [16], [19], [7], and [14]. QA on large scale structured KBs are critical to modern web search engines such as Bing and Google, spoken language dialog systems such as Cortana or Siri [11], and integrated search and browsing applications [9]. However, open -domain QA with complex natural language and logic reasoning rema ins a major scientific and engineering challenge. The scale of the KB and the difficulty for machines to interpret spoken language questions accurately make open-domain QA an unsolved problem. We focus on our text-based QA system over a well-known large scale knowledge base Freebase [5]. We use a version of Freebase as referenced in h ttp://www-nlp.stanford.edu /software/sempre/, which contains 41m entities, 19k properties, and 596m assertions [4]. The underlying text-based QA technology discussed in this paper can be easily extended for other spoken language or multimodal interactive systems [11][11]. One of the key challenges is how to handle the diverse ways natural language questions are expressed about predicates and entiti es in the KB. Traditionally, a semantic parser is used to conve rt questions in natural language utterances into logical forms such as lambda expressions, which are then evaluated against the KB. Such a semantic parser may be learned from pairs of utterances and logical forms, but such labeled training data are typically associated with high development cost and thus limit coverage of the trained semantic parser [21]. Recently, a number of novel approaches have been proposed to loosen the requirement of KB-specific logical forms and to utilize models learned from large amounts of data not covered by the KB [13], [3], [4]. We follow along this path and present a new practical approach that demonstrates a significant improvement over current state-of-the-art systems on the We bQuestions evaluation data set [3]. Our approach is different fro m ParaSempre in two ways: first we use a domain-independent word -entity embedding system in enhancing the quality of entity lin king; second we allow arbitrarily complex logic formula generation to help us search for the best match, while ParaSempre uses predefined templates to generate finite logic formula. In our approach, ground facts in the KB are used to derive an infinite number of ground logical fo rmulae (i.e., no free variables) in the form of proof trees. An answer to a question is backed by a proof tree that uses ground facts in the KB as input and generates a logical formula that ma tches the input question. The QA process is thus a logic reasoning process to find a proof that generates a logical formula to best match the input question. Our KB-based QA system can be viewed as a decoding problem where the search space is composed of proof trees derived from the KB. The objective function of the decoder is our match score between the output of a proof tr ee and the input question. The former (proof tree output) is a logical formula, while the latter (input question) is a natural langua ge utterance. The physical interpretation of the match score is related to the probability for the natural language utterance to express the corresponding logical formula. We define our system match scor e between a logical formula and a question utterance as a log-linear s uperposition of several features. The features are based on the logical formula, the question utterance, and/or a set of utterances realized from the logical formula. A layered pruning approach is adopted to significantly improve our decoding efficiency. Our system consists of three layers: L0 derives anchor entities to form the leaf nodes of our proof trees. L1 further searches for the best N candidate proof trees using only the most efficient match score features. L2 ranks the best N proof trees using a more comprehensive set of match score features. We demonstrate a competitive QA system in terms of accuracy. In our benchmarking with the Stanfo rd WebQuestions dataset, which contains questions commonly asked by web users, our system achieved F1 measure of 47.2 on the test set using the same dev and test data split and evaluation script as published at http://www-nlp.stanford.edu/software/sempre/. The F1 measure is significantly higher than the best published result of 39.9 achieved by a single system ([4]), or 41.8 from an ensemble of two best systems ([7]). All words and entities can be jointly embedded into a low-dimensional intent space through massive scale statistical learning. The training data are text-entity pairs. There are several means to obtain such training pairs. For ex ample, we can use entity names, aliases, descriptions, etc. contained in Free base itself. User interaction data such click-throu gh data logged by search engines are also very valuable. We mixed and deduped text-entity pairs from several sources to create a ma ssive training corpus with over 50 billion text-entity pairs to develop our domain-independent language embedding. Every word (  X  ) and every entity (  X  ) are embedded into a low-dimensional vector space with a joint objective function. The parameters to be learned are the vectors for each word (  X  information (MMI) between word and entity pairs ([15]): Where  X  (  X , X  ) are empirical (observed) pair probability and  X  (  X  ) =  X   X  (  X , X  )  X  . Informally,  X   X   X  X   X  measure between vector  X   X  and  X   X  , and usually dot product works reasonably well. Formally, the model predicts  X  (  X | X  ) as: The gradient descent algorithm is us ed to estimate our model, with gradient calculated using the back propagation method. We use the learned embeddings in two ways: first we use them in L0 entity linking to rank entity candidates for a given question; second we use word-entity embedding similarity as feature to L2 proof tree ranking. Every ground fact in the KB is a ground (no free variable) logical formula that is deemed true. Multiple facts can be combined through a reasoning process to achieve a new fact, which is another logical formula that is also true. If the new fact matches the input question, it provides an answer to the input question. The logical reasoning process can be described by a deduction with KB facts as input and the new fact as output. The logical formulae are formulated after combinators because we would like to match them against natural language utterances. For example,  X  X arack Obama X  X  s pouse is Michelle Obama X  is expressed as where bo and mo means entities for  X  X arack Obama X  and  X  X ichelle Obama X . the sentence. For example,  X  X arack Obama married Michelle Obama X  is expressed as i.e., there exists an event e whose verb is marry, subject is bo, and object is mo. Please note event e is a real entity in the KB, but it is not explicitly mentioned in natural language utterances. The most basic combinators in our logical formulae are the properties in the KB (of type  X  X  X  X  X  X  X  X  X  X  X  ) and the  X = X  combinator (of type  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  ). We also introduce a limited number of additional combinators inspired by natural language, such as "v X , "s X , and "o X  in the above expression. Inference rules are specified to de rive new logical formula. For example, the following inference rule derives  X  X arack Obama married Michelle Obama X  from two facts in the KB: And the following rule further de rives  X  X arack Obama X  X  spouse is Michelle Obama: X  The purpose of inference rules is to enable introducing human knowledge easily into the QA system. We specify syntax to generate surface-form utterances from logical formulae in CFG-like production rules. For example: where ) X ( X  can be interpreted as the set of utterances  X  can realize, and ) X ( X  is the type of  X  . The realized utterances are used to provide mostly surface-form features to compute a match score of a logical formula to the input question. Specifically, the match score is defined as where  X  is a logical formula,  X  is the input question utterance, and  X  is a realized utterance  X  produces.  X log (  X | X , X  ) is defined with a superposition of various log-linear features: where each feature  X   X  might depend on only some of  X , X , X  . For example, paraphrasing fe atures are only between  X  and  X  . The search graph starts with individual ground facts contained in the KB. They derive new logical formulae using inference rules. Every logical formula has a match score against the input question. The goal is to find the logical formula with the highest match score: where KB  X  X  means that KB can derive  X  . We use best-first search algorithm. At every step, an existing logical formula L with the highest heuristic future match score is applied to generate more logical formulae, which are pushed to the priority queue according to their future match scores. We currently use the following simple heuristics to obtain future match scores. We create a directed graph of terminal symbols, where an edge is added between two terminal symbols when there is an inference rule that mentions the terminal symbols in the premise and in the result respectively. For example, there is an edge from  X  X arry X  to  X  X pouse X  in the directed graph. Then the syntactic production rules are augmen ted with descendent terminal symbols up to a certain depth. For example, in addition to producing  X  X ho did Barack Obama marry, X  the corresponding formula also produces  X  X ho did Ba rack Obama spouse X  as a future natural language utterance, and is only used to calculate the heuristic future match score. The future utterance is closer to the real utterance, e.g.,  X  X ho is Barack Obama X  X  spouse. X  underestimate of the true match score. On the other hand, a good paraphrasing model complements the simple heuristic, although a complex paraphrasing model may be impractical to be used in our search due to runtime efficiency concerns. The top candidates from the search are ranked by a paraphrasing model. This is a log-linear model similar to the one used in [4], but is different in the feature sets and training algorithm. We used the following groups of features in our paraphrasing model: 1. Matching scores from L1 candidate search. 2. 13 features from [17]. 3. A subset of association features proposed in [4]. The subset 4. Cosine similarity score between the vector representations for 5. L0 entity ranking score for the entity appear in the candidate. The parameters of the log-linear model is learned using L-BFGS, with L1-regularization to avoid over fitting [1]. We use WebQuestions [3] dataset fo r training and evaluation of our system. WebQuestions dataset c ontains 5,810 question-answer pairs. Following [3], we use the same subset (3,778 question-answer pairs) as the development set (Dev), and use the other part (2,032 question-answer pairs) for evaluation. We also used the same Freebase KB that was used by [3]. However we don X  X  use off-the-shelf KB query engine, instead we process the KB data file, and load it into memory of our workstation for serving as it is now practical to do so with current hardware spec. We use the following search parameters in L1. At most 50 entities nodes. The best-first search is capped at 100k logical formulae (with duplicates). The complexity of logical formulae is limited to 4 levels of composition. Top 10k logical formulae are deduped and passed onto L2 ranker. With these search parameters, we are able to achieve an average L1 latency of 923ms single-threaded on the test set on a work-station with Intel Xeon E5-2630 CPU @ 2.60GHz and 256GB DDR3 RAM @ 1600MHz Our system is compared with other systems in terms of F1 score. We use the evaluation script by [4] published at http://www-nlp.stanford.edu/software/sempre /. All other systems were reported by [7], except for the scores marked with *, which are clarified in [20]. To understand the contribution of the joint word-entity embedding feature, we performed two variant experiments because the feature the feature from L2, and the other removes it from both L0 and L2. The results are shown in Table 2, and indeed show that it contributes significant end-to-end ac curacy gain when used both in L0 and in L2. Both semantic parsing and repr esentation learning have been applied to the QA problem on a la rge KB. We attempted to unify the two approaches with a proof-theoretical frame-work. The problem is framed as searching for a proof tree from the KB to answer the natural language question. There are several advantages of our new approach. Firstly, the logic forms explored by our decoding process are not limited by simple templates and thus we can potentially expand answering arbitrarily compositional questions. Secondly, instead of approaching the KB from the input question, our approach reaches the input question from the KB. This means semantic parsing is embedded in the whole question answering process, and all facts in the KB can be used as early as possible to guide our improved semantic parsing. Thirdly, our approach easily takes in representations learned from the KB itself and other domain-indepe ndent sources. In particular, we have shown that our maximum mutual information derived word-entity joint embedding can signi ficantly improve accuracy of our QA system albeit these embedding vectors are domain-independent without task sp ecific data adaptation. Our QA system significantly outperfo rms the state of the art system on the same Stanford WebQuestions benchmark data. The proposed approach can be easily extended to other spoken language or multimodal interactive systems. [1] G. Andrew and J. Gao. 2010. Scalable training of L1-[2] J. Bao, N. Duan, M. Zhou, T. Zhao. 2014. Knowledge-Based [3] J. Berant, A. Chou, R. Fros tig, and P. Liang. 2013. [4] J. Berant and P. Liang. 2014. Semantic Parsing via [5] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. [6] A. Bordes, J. Weston, and N. Usunier. 2014a. Open [7] A. Bordes, S. Chopra, and J. Weston. 2014b. Question [8] Q. Cai. and A. Yates. 2013. Large-Scale Semantic Parsing [9] Z. Chen, J. Sun, and X. Hua ng. 2014. Web Information at [10] A. Fader, L. Zettlemoyer, and O. Etzioni. 2013. Paraphrase-[11] X. Huang, J. Baker, and R. Reddy. 2014. A Historical [12] O. Kolomiyets and M.-F. Moens. 2011. A survey on [13] T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. [14] R. Mooney. 2014. Semantic Parsing, Past, Present and [15] J. R. Pierce. 198 0. An Introduction to Information Theory: [16] M. Steedman. 2014. Robust Se mantics of Semantic Parsing, [17] S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using [18] Z. Wang, S. Yang, H. Wang, and X. Huang. 2014. An [19] X. Yao and B. Van Durme. 2014a. Information Extraction [20] X. Yao, J. Berant, and B. Van Durme. 2014b. Freebase QA: [21] L. Zettlemoyer and M. Collins. 2005. Learning to map 
