 1. Introduction
In this paper, we propose method for improving machine transliteration using an ensemble of several dif-ferent transliteration models. First, this paper describes a method about transliteration production using dif-ferent machine transliteration models and transliteration ranking with web data and relevance scores given by each transliteration model in English-to-Korean transliteration and in English-to-Japanese transliteration.
Web data as a context for weighting each produced transliteration filters out noisy transliterations. Finally, we present impact of our machine transliteration method on IR effectiveness through experiments.
Machine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. For example, the English word  X  X  X ata X  X  is names and technical terms especially from languages in Roman alphabets to languages in non-Roman alpha-bets such as from English to Korean, Japanese, and Chinese. Because the proper names and technical terms, which are frequently transliterated, are usually selected as representative index terms for texts, proper han-dling of the transliterations is important for an effective information retrieval system. Transliterations are one of the main sources of the Out-of-Vocabulary (OOV) problem ( Fujii &amp; Ishikawa, 2001; Lin &amp; Chen, 2002 ). Handling transliterations depending on dictionary lookup, therefore, is an impractical way. One way to solve the problem is machine transliteration. Nowadays, machine transliteration as an assistant of cross language applications, such as Machine Translation (MT) ( Al-Onaizan &amp; Knight, 2002; Knight &amp; Graehl, been recognized as an important research issue. In the area of CLIR, machine transliteration bridges the gap between the transliterated localized form and its original form by generating all possible transliterated forms from their original form (or all possible original forms from their transliterated form). For example, machine transliteration gives a help to query translation in CLIR where proper names and technical terms frequently appear in source language queries. In the area of MT, machine transliteration prevents translation failure when translations of proper names and technical terms are not registered in a translation dictionary. Machine transliteration, therefore, may affect the performance of MT and CLIR system.

Three machine transliteration models have been studied as described in Fig. 1 : called grapheme transliteration model ( w G ), phoneme 3 -based transliteration model ( w literation model ( w GP ). They are classified in terms of units to be transliterated. w model because w G directly transforms source language graphemes to target language graphemes without any phonetic knowledge of source language words ( u ST ). w use of phonemes as a pivot when it produces target language graphemes from source language graphemes.
Therefore w P usually needs two steps: the first step is to produce phonemes from source language graphemes ( u SP ), and the second step is to produce target language graphemes from phonemes ( u both source language grapheme and phoneme ( u (SP)T ) when it produces target language transliterations. Here-after, we will use a source grapheme for a source language grapheme and a target grapheme for a target lan-guage grapheme.

Transliterations produced by the three models are usually different because they depend on different infor-mation. Basically, transliteration is the phonetic process, like w neme-based transliterations. For example, the standard Korean transliterations of  X  X  X ata X  X ,  X  X  X mylase X  X , and  X  X  X eomycin X  X  are the phoneme-based transliteration, deita , the grapheme-based transliteration, amillaaje , and the combination of grapheme-based transliteration and phoneme-based transliteration, neomaisin , respectively. The three transliteration models ( w G , w P neously in order to reflect the dynamic behaviors of transliteration.
Fig. 2 shows the basic system architecture of our ensemble-based transliteration model that makes use of three transliteration models to consider the complex process of transliteration. Our model is composed of to generate transliterations using three different transliteration models. Then the transliterations are ranked with web data and relevance scores given by each transliteration model in the transliteration ranking step.
The main goal of the transliteration production step is to produce all possible transliterations reflecting dynamic behaviors of transliteration. In other words, transliteration production tries to make transliterations where relevant transliterations are included as many as possible. The basic assumption of transliteration ranking with web data is that relevant transliterations will be more frequently used in real-world texts than to rank the transliterations. If a transliteration appears in more WWW documents, then it gets a higher rank or it gets a higher score. The assumption of transliteration ranking with relevance scores is that the relevant bottom ranks.

The rest of this paper is organized as follows: In Sections 2 and 3 , we will describe two core parts of our ensemble-based transliteration model. Section 4 shows experiments on machine transliteration and on appli-cation of our ensemble-based transliteration model to information retrieval. Section 5 shows discussion about machine transliteration models. We will conclude our paper in Section 6 . 2. Transliteration production based on three machine transliteration models In this paper, we implement three different machine transliteration models ( w machine learning techniques. Fig. 3 summarizes the differences among the three transliteration models and their component functions. w G is composed of one component function called u a source grapheme ( S ) to a target grapheme ( T ); while w u SP : S ! P and u PT : P ! T where u SP is a function for producing pronunciation and u ducing target graphemes based on phonemes ( P ). Like w P and u (SP)T : S  X  P ! T . The difference between w P and w
While w P uses only the phonemes, w GP uses source graphemes and its corresponding phonemes when it gener-ates target graphemes. We describe their differences with two functions, u To train each component function, we should define the features that represent training instance and data.
Table 1 shows five feature types, f S , f P , f GS , f GP f
GS , and f S,GS is a symbol representing both f P and f GP feature types. How to model each component function with the feature types will be given in Sections 2.1 and 2.2 . 2.1. Component functions of each transliteration model
Table 2 shows the definitions of four component functions that we used. They are defined in terms of their input and output. Their role in each transliteration model is to select the most relevant output among output candidates derived from their input. The performance of transliteration models, therefore, strongly depends on their component functions. In other words, how well each component function is modeled is the key to making a good machine transliteration system. The modeling highly depends on the feature type and context.
From the viewpoint of the feature type, u ST , u PT , and u in Table 2 . Depending on the difference, three component functions show their strong points and weak points in machine transliteration. u ST is good at producing grapheme-based transliterations while it shows weak points for phoneme-based transliterations. On the contrary, u transliterations but it has disadvantage in producing grapheme-based transliterations. For  X  X  X mylase X  X  and its standard Korean transliteration amillaaje , which is a grapheme-based transliteration, u correct transliteration; while u PT tends to produce wrong ones like aemeolleiseu derived from /AE M AH L EY S/, which is pronunciation of  X  X  X mylase X  X . On the other hand, u is the standard Korean transliteration of  X  X  X ata X  X  and a phoneme-based transliteration; while u wrong ones like data . u (SP)T adopts merits of u ST , and u PT by using correspondence between source grapheme and phoneme. The correspondence gives power for u (SP)T to produce both grapheme-based transliteration and phoneme-based transliteration. Furthermore, the correspondence gives important clues for resolving transliteration ambigui-ties in many cases. 6 For example, phoneme /AH/ produces high ambiguities in machine transliteration because it can be mapped to almost every single vowels in source language and target language (the underlined graph-
Korean counterparts, and sinem a , hoseut e ru , hor o koosuto in their Japanese counterparts). If we know the correspondence between source grapheme and phoneme in this case, we can more easily infer the correct transliteration of /AH/, because the correct target grapheme corresponding to /AH/ usually depends on the source grapheme corresponding to /AH/. On the other hand, Korean transliterations of source grapheme  X  X  X  X  X  are various like a , ae , ei , i , and so on. In this case, phonemes corresponding to source grapheme  X  X  X  X  X  can help a component function to resolve the transliteration ambiguities like Table 3 .In Table 3 , the underlined source grapheme  X  X  X  X  X  in the example column is pronounced as the phoneme in the phoneme column. The cor-rect Korean transliterations corresponding to source grapheme  X  X  X  X  X  can be more easily found, like in the Kor-ean grapheme column, by means of phonemes in the phoneme column.

Though u (SP)T is more effective than both u ST and u PT in many cases, u points when the standard transliteration is strongly biased to either grapheme-based transliteration or pho-neme-based transliteration. In that case, one of source grapheme and phoneme, which does not contribute on producing the correct transliteration, sometimes, makes it difficult for u rect transliteration using the other.

Because u ST , u PT , and u (SP)T are the core part of w G the three component functions become those of three transliteration models that each component function participates in. We combine transliterations produced by the three transliteration models in order to make up for the weak points of one transliteration model by the strong points of the others.

Transliteration usually depends on context. For example, source grapheme  X  X  X  X  X  can be differently translit-of  X  X  X  rt X  X . 7 In making use of context information, determining context window size is important. Too narrow context window can degrade the transliteration performance due to lack of context information. For example,
Too wide context window can also degrade the transliteration performance because it decreases power to resolve transliteration ambiguities. For these reasons, many previous works determine their context window size as 3. In this paper, we also use the same context window size as the previous work s( Goto, Kato, Uratani, shown in Section 4 through an experiment.

Table 4 shows how to select the most relevant output in each component function using context informa-tion. In the table, L1 X  X 3, R1 X  X 3, and C0 represent the left context, right context, and current context (or focus), respectively. u SP produces the most relevant phoneme for each source grapheme. Let SW = s be an English word, then SW s pronunciation can be represented as a sequence of phonemes produced by u such as P SW = p 1  X  p 2 p n where p i = u SP ( s i ). u SP a pronunciation dictionary, which contains English words and their pronunciation. This paper uses The CMU
Pronouncing Dictionary ( CMU, 1997 ), which contains 120,000 English words and their pronunciation. The second step is to estimate the pronunciation. If an English word is not registered in the pronunciation dictio-nary, we must estimate its pronunciation. Produced pronunciation is used for u u ST , u PT , and u (SP)T produce target graphemes using their input. Like u use their previous outputs represented by f T . Depending on input and output of each component function described in Table 2 , u ST , u PT ,and u (SP)T produce a sequence of target graphemes like T
SW = s 1  X  s 2 s n and P SW = p 1  X  p 2 p n . 2.2. Machine learning algorithms for each component function
In this section, we will describe a way of modeling component functions using three machine learning algo-rithms: maximum entropy model, decision tree, and memory-based learning, which are probability based model, rule based model, and vector-based model (or example-based model). Because u share the similar framework, we limit our focus to u (SP)T 2.2.1. Maximum entropy model
The maximum entropy model (MEM) is a widely used probability model that can incorporate heteroge-mum entropy model, an event ev is usually composed of a target event (te) and a history event (he), say ev =  X  te, he  X  . Event ev is represented by a bundle of feature functions, fe of a certain characteristic in event ev. A feature function is a binary-valued function. It is activated (fe i (ev) = 1) when it meets its activating condition, otherwise it is deactivated (fe 1996; Miyao &amp; Tsujii, 2002 ). u SP and u (SP)T based on the maximum entropy model can be represented as formula (1) . History events in each component function are made from the left, right, and current context. For example, history events for u grapheme and phoneme to be transliterated and f X ( l , m ) the position l to the position m . Target events are a set of outputs derived from history events in each com-maximizes formula (1) . One important thing in designing a model based on the maximum entropy model is to determine feature functions which effectively support certain decision of the model. Our basic philosophy of feature function design for each component function is that context information collocated with the unit of interest is an important factor. With the philosophy, we determined the activating conditions (or history events) of the feature functions by combination of features in feature types. Possible feature combinations for history events are between features in the same feature type and between features in the different feature types. The used feature combinations in each component function are listed in Table 5 .

In formula (2) , history events of u SP and u (SP)T are represented as he(SP) and he(SPT), respectively. Target events are also represented in the same manner like te(SP) and te(SPT). Because the conditional probability in formula (1) can be represented with feature functions, it can be rewritten as formula (2) . Formula (3) shows examples of feature functions for u SP and u (SP)T ; Table 4 is used for deriving the feature functions. fe sents l th for u SP and fe m represents m th feature function for u correspond to f S , f P , and f T at C0 in Table 4 , respectively. f an event where f S i (C0 in u SP ) is  X  X  X  X  X , f P i 1 (L1 in u event where f S i (C0 in u (SP)T ) is  X  X  X  X  X , f P i (C0 in u each component function based on MEM, Zhang s maximum entropy modeling tool is used ( Zhang, 2004 ). 2.2.2. Decision tree
Decision tree learning is one of the most widely used and well-known methods for inductive inference ( Mitchell, 1997; Quinlan, 1986 ). ID3, which is a greedy algorithm and constructs decision trees in a top-down manner, adopts information gain that measures how well a given feature (or attribute) separates training examples according to their target class ( Manning &amp; Schutze, 1999; Quinlan, 1993 ). We use C4.5 ( Quinlan, 1993 ), which is the well-known tool for decision tree learning and implementation of Quinlan s ID3 algorithm. Training data for each component function is represented by features of feature types in the context of L3 X  training data ( Mitchell, 1997 ). Fig. 4 shows a fraction of constructed decision tree for u to-Korean transliteration (note that the left side represents the decision tree for u the decision tree for u (SP)T ). A set of the target classes in the decision tree for u target class, and circles indicate a decision node. In order to simplify our examples, we use only the f ing decision trees.) Intuitively, the most effective feature for u
C0, and R1 X  X 3 because the correct outputs of u SP and u (SP)T the decision nodes with a sequence of C( f S ) = o, R1( f similar manner, the decision tree for u (SP)T produces target grapheme (Korean grapheme) o for the instance x (SPT) by retrieving the decision nodes from C0( f P ) = /AO/ to R1( f 2.2.3. Memory-based learning
Memory-based learning (MBL) is an example-based learning method. It is also called instance-based learn-ing and case-based learning method. It is based on a k -nearest neighborhood algorithm ( Aha, 1997; Aha, vector. In the training phase, MBL puts all training data as examples in a memory, and clusters some examples with a k -nearest neighborhood principle. It then produces an output using similarity-based reasoning between between x and Y is estimated by a distance function, D ( x , Y ). MBL selects an example y learning tool called TiMBL (Tilburg memory-based learner) version 5.0 ( Daelemans et al., 2002 ).
Training data for MBL is represented in the same form as training data for decision tree. Note that the target classes for u SP and u (SP)T , which MBL outputs, are phoneme and target grapheme, respectively. Like Fig. 4 , we use only the f S and f P in order to simplify our examples. Fig. 5 shows examples of u based on MBL for English-to-Korean transliteration. All training data are represented with their features in the context of L3 X  X 1, C0, and R1 X  X 3 and their target classes for u ory through a training phase. Feature weighting for dealing with features of differing importance is also per-formed in the training phase. In Fig. 5 , u SP based on MBL outputs the phoneme /AO/ for x (SP) by comparing the similarities between x (SP) and Y using distance metric D ( x (SP), Y ). With the similar manner, u on MBL outputs the target grapheme o . 3. Transliteration ranking
In this paper, we use two kinds of ranking method, relevance-based transliteration ranking and web-based transliteration ranking. Once transliterations are produced for a given English word through three different transliteration models, the transliterations are ranked by the two ranking methods. 3.1. Web-based transliteration ranking
The assumption of transliteration ranking with web data is that relevant transliterations will more fre-quently appear in WWW documents than non-relevant ones do. Grefenstette (1999) used phrase web fre-quency to disambiguate all possible English translations of German and Spanish compound nouns. With the similar manner, we use web frequency (the number of WWW documents where transliterations exist) for transliteration ranking.

It is important to consider a transliteration and its source language word at a time rather than a translit-relevant transliterations corresponding to source language word rather than to find transliterations, which are frequently used in the target language. Therefore, the most desirable case is to find WWW documents where transliterations are used as translations of the source language word. Bilingual Phrasal Search (BPS), where a phrase composed of a transliteration and its source language word is used as a query of a search engine such as { amillaaje  X  X  X mylase X  X  X , enables a search engine to find the WWW documents that fit our aim. Fig. 6 shows
Korean and Japanese WWW documents retrieved by the BPS for  X  X  X mylase X  X  and its Korean/Japanese trans-literations, amillaaje and amiraaje . The retrieved WWW documents usually contain a transliteration and its source language word as a translation pair in the parenthesis expressions, like the rectangles in Fig. 6 .
There is dilemma between the quality and coverage of retrieved WWW documents. Though BPS gives high-quality WWW documents that fit our aim of transliteration ranking, the coverage of BPS, however, is relatively low. It means that we cannot retrieve WWW documents for some transliterations with BPS. For fore, the alternative search methods are necessary when BPS fails to retrieve WWW documents. Bilingual
Keyword Search (BKS), which is first applied when BPS fails, and Monolingual Keyword Search (MKS), which is used when both BPS and BKS fail, are the alternative search methods.

Like BPS, BKS makes use of two keywords including a transliteration and its source language word as a query of a search engine. BPS retrieves WWW documents where two keywords exist as a phrase, but BKS retrieves WWW documents if the two keywords exist just in the same document. Due to this property of cially when the noisy transliterations are one-syllable transliterations. For example, mok , which is one of quency than mukeu , which is the standard transliteration of  X  X  X ook X  X , because mok is a common noun, which frequently appears in Korean texts as the meaning of  X  X  X eck X  X . However, BKS can improve coverage without great loss of quality of retrieved WWW documents if transliterations are composed of more than two syllables.
Though BKS has higher coverage than BPS does, BKS also fails to retrieve WWW documents for some cases. For this case, MKS is applied. In MKS, a transliteration alone is used as a query for a search engine.
Compared with translation model and language model in machine translation, both BPS and BKS can be regarded as the translation model; while MKS plays a role as the language model. Though MKS does not give information whether a transliteration is the correct transliteration corresponding to a given source language word, it can give information whether a transliteration tends to be a target language word. The three search methods are sequentially applied from BPS to MKS until one of them retrieves at least one WWW documents for transliterations corresponding to one source language word; If one search method fails to retrieve WWW documents for all transliterations, then the next search method is applied.

Along with three different search strategies described above, three different search engines are used in order to get more WWW documents. Search engines to be used should satisfy two conditions: (1) to support Kor-ean/Japanese WWW document retrieval, and (2) to support phrasal search as well as keyword search. Goo-gle, 8 Yahoo, 9 and MSN 10 that satisfy the two conditions are chosen as search engines.
Web frequencies acquired from the three search methods and the three search engines are used as formula (4) where c i is the i th transliteration produced by three transliteration models, e is c
RF is a ranking function for transliterations, WF is a function for web frequency, NWF is a function for nor-malized web frequency, C is a set of produced transliterations and j is an index for the j th search engine. We use normalized web frequency (NWF) as a ranking factor (RF). Normalized web frequency (NWF) is web frequency (WF) divided by the total web frequency of all transliterations corresponding to one source lan-guage word. Then the score of each transliteration is calculated by summing up the normalized web frequency of the transliteration given by the three search engines:
Table 6 shows the ranking example for English word  X  X  X ata X  X  and its possible Korean transliterations, deiteo , deita ,and deta . In the table, the normalized WF BPS (NWF NWF BPS ( X  X  X ata X  X , deiteo ) = 94100/(94100 + 67800 + 54) = 0.5811.
 NWF BPS ( X  X  X ata X  X , deita ) = 67800/(94100 + 67800 + 54) = 0.4186.
 NWF BPS ( X  X  X ata X  X , deta ) = 54/(94100 + 67800 + 54) = 0.0003.
 Then, the ranking score of deiteo is calculated by summing up NWF engine as follows: RF BPS ( X  X  X ata X  X , deiteo ) = 0.5810 + 0.7957 + 0.3080 = 1.6848.

Using the RF value for each transliteration, we can make ranked transliterations: c the second place, and c 3 is the last place. 3.2. Relevance-based transliteration ranking
Though the web-based transliteration ranking method may be effective, it shows limitations when there is no web data containing transliterations to be ranked. Therefore we need another ranking method which can rank transliterations independent of web-data in order to handle such the case. Our relevance-based transliteration ranking method makes use of a relevance score given by each transliteration model. When each transliteration literation model. The three transliteration models give their own relevance scores to produced transliterations be the i th transliteration derived from e , SR( w , e , c model w and S Rel ( e , c i ) be the relevance score of c less of transliteration model. Therefore we get S Rel ( e , c score of c i for each transliteration model (SR( w , e , c
Finally, transliterations are ranked by both the web-based transliteration ranking method and the relevance-based transliteration ranking method like formula (6) , where k (0 6 k 6 1) is a weighting parameter for S 4. Experiments and evaluation 4.1. Experiments on machine transliteration 4.1.1. Experimental setup
We perform experiments for English-to-Korean and English-to-Japanese transliteration. English-to-Kor-ean test set (EKSet) ( Nam, 1997 ) consists of 7185 English X  X orean pairs X  X he number of training data is 6185 and that of test data is 1000. EKSet contains no transliteration variation; therefore, there is one trans-literation for an English word. English-to-Japanese test set (EJSet), which is an English X  X atakana pair in literations for an English word is 1.15. Evaluation is performed by word accuracy (W.A.), which was used as the evaluation measure in the previous works: We perform four experiments as follows:
Comparison Test I: Performance evaluation of grapheme-and phoneme-based transliteration model and the previous works (to investigate the effect of correspondence between source grapheme and phoneme on machine transliteration).

Comparison Test II: Performance evaluation of transliteration production for each transliteration model (to investigate how well our system produces transliterations and to investigate contribution of each machine transliteration model to transliteration production ).

Context Window Size Test: Performance evaluation depending on context window size (to investigate the effect of context window size on the performance of machine transliteration).

Ranking Test: Performance evaluation of transliteration ranking (to investigate how well our system ranks transliterations). 4.1.2. Experimental results
Table 7 shows results of Comparison Test I . In the table, MEM, DT, and MBL represent maximum entropy model, decision tree, and memory-based learning, respectively. GDT ( Kang, 2001; Kang &amp; Choi, 2000 ), GPC ( Kang &amp; Kim, 2000 ), GMEM ( Goto et al., 2003 ), and HWFST ( Bilac &amp; Tanaka, 2004 ), which are one of machine transliteration methods showing good performance in English-to-Korean transliteration and Eng-lish-to-Japanese transliteration, are compared with w GP . They are trained and tested with the same data as w
GP . In GDT (the decision tree based transliteration method), decision trees, which transform each source grapheme to target graphemes, are learned and then they are straightly applied to machine transliteration.
GPC and GMEM are the transliteration network based method for English-to-Korean and English-to-Japa-nese transliteration, respectively. They share the similar framework in constructing a transliteration network composed of nodes and arcs. A node represents a chunk of source graphemes and its corresponding target grapheme. An arc represents a possible link between nodes and it has a weight showing its strength. HWFST is the hybrid model-based transliteration methods. In the HWFST, w then w G and w P are combined through the linear interpolation style. Imp. means the performance improve-ment of each transliteration method compared to GDT. Totally, w ment, about 15 X 23%, in English-to-Korean transliteration, and about 15 X 43% in English-to-Japanese transliteration. Comparison between previous works and w GP
MBL, MEM, and DT in w GP , MBL shows the best performance because it shows better abilities to handle transliterations of source words containing idiomatic sequence of alphabets such as  X  X -tion X  X ,  X  X -ment X  X ,  X  X -ware X  X , and so on than the others. The ability may be deceived from the fact that MBL is example-based learning method and vector-based model.

Table 8 shows results of Comparison Test II for EKSet and EJSet. The table summarizes the performance of each transliteration model based on three different machine-learning algorithms. The performance of translit-eration production is represented as w G + w P + w GP , which indicates the performance when transliterations produced by the three transliteration models are combined.
In comparison among w G , w P , and w GP , w G and w P depend on either source grapheme or phoneme, while w GP dynamically use both source grapheme and phoneme. In other words, w when either source grapheme or phoneme that they did not consider holds the key for producing a correct the viewpoint of transliteration production , we find that w models for correct transliteration production. The correct transliteration production means that at least one is correct among all of produced transliterations. High performance in transliteration production can be acquired if a correct transliteration, which one model fails to produce, can be acquired by the others.
By combining three transliteration models, we additionally improve 15 X 43% W.A. in English-to-Korean and 10 X 35% in English-to-Japanese compared with w G , w P
Table 9 shows the results of Context Window Size Test . We use MBL based method, which shows the best performance among machine learning algorithms. Experiments are performed by changing the size of context window from 1 to 6. The result indicates that the best performance is shown when the context window size is 3.
When the context window size is 1, there are many cases where the correct transliterations are not produced due to lack of contextual information. For example, in order to produce correct target language grapheme of the context window size is 3.

Table 10 shows results of Rank Test . In this test, we use MBL, which shows the best performance among the three machine-learning algorithms. We evaluate the performance of the three transliteration ranking meth-which the transliteration production step makes no errors (CTC). In this test, k = 0.8 is used for S ( e , c uation with ALL will show the overall performance of our ensemble-based machine transliteration model.
Evaluation with CTC will show the performance of the transliteration ranking step when transliteration pro-duction makes no error. In the table, the Top-n considers whether the correct transliteration is in the Top-n ranked transliterations. The average number of produced Korean transliterations is 4.43 and that of Japanese ones is 4.12; note that w P and w GP produce more than one transliteration because of pronunciation variations.
The ranked results in both English-to-Korean transliteration and English-to-Japanese transliteration indicate that our transliteration ranking method effectively filters out noisy transliterations and locates the correct mance of our machine transliteration system (for ALL) as well as the performance of transliteration ranking (for CTC) is relatively good. Especially, the performance of CTC shows that web data as language resources to rank transliterations is very useful. In comparison between S literation to be ranked. 4.1.3. Analysis of results (should be revised depending on three ranking methods)
We define two error types called production error and ranking error . The production error happens when there is no correct one among all of produced transliterations. The ranking error happens when the standard transliteration does not appear in Top-1 of ranked transliterations.

We examined the effects of BPS, BKS, and MKS on transliteration ranking. Table 11 shows the perfor-mance of our method when transliterations are ranked by each search method. In Table 11 , RTC represents the number of test data which does not cause production error and NTC represents the number of test data ranked by each search method. NTC will show the coverage of each search method and the proportion of
RTC to NTC will show the upper bound of the performance when each search method is applied. Difference between RTC and NTC will show the number of production errors.

In case of BPS, the highest performance is recorded among the three search methods. BPS handles about 860 cases among 1000 cases that is the total number of test data in EKSet and EJSet. BPS tends to retrieve desirable WWW documents containing transliteration pairs in parenthesis expressions as described in Fig. 6 .
This property leads us to analyze the ranking errors. We find that the main reason of the ranking errors in BPS is transliteration variations. The transliteration variations contribute to the ranking errors in two aspects.
First, when web frequencies of transliteration variations are higher than those of the standard ones, the rank of the variations will be higher than that of the standard one. Table 12 shows some examples of ranking errors ation in produced transliterations. In this case, ranking errors are caused by production errors. Eighty-six cases in EKSet and 71 cases in EJSet are caused by this reason.

In case of BKS and MKS, the number of NTC is relatively small. Because BPS retrieves WWW documents if possible, BKS and MKS can deal with only small number of cases. Table 11 shows that the main reason why BPS fails to retrieve WWW documents is the production error (Note that most cases dealt with BKS and
MKS show production error except 23 (7 + 16) 12 cases in EKSet and 27 (23 + 4) also shows that BKS is more effective than MKS.

Tradeoff between the quality and coverage of retrieved WWW documents is an important factor in trans-literation ranking. BPS preferring the quality to the coverage of retrieved WWW documents effectively plays its role with high quality and reasonable coverage. 4.2. Experiments on information retrieval using machine transliteration results 4.2.1. Experimental setup
We investigate the effects of our machine transliteration system on information retrieval. KT SET 2.0 ( Park, Choi, Kim, &amp; Kim, 1996 ), Korean information retrieval test set, and NTCIR-1 test collection of the two test collections. The KTSET contains 2725 kinds of Korean transliterations corresponding to 2391 source language words in documents. The queries contain simple keywords. NTCIR-1 test collection contains about 130,000 kinds of Japanese transliterations corresponding to about 110,000 source language words in documents. The queries are composed of four parts including title, description, narrative and concept, which represent title of a query, short description of a query, long description of a query, and concept keyword of a query, respectively. Among four types of query, the concept query is chosen for Japanese information retrieval in our experiment because it partly contains bilingual information and it is the most effective one among short queries (title, description, and concept).

Because the test collections are for monolingual information retrieval, it is difficult to directly apply our transliteration system to information retrieval experiment. Therefore, we recover the local language query corresponding English words manually, (back-transliterated query: B-Query ) and then automatically translit-erates the English words in B-Query into Korean or Japanese using our machine transliteration system (transliterated local language query: T-Query ). Table 14 shows examples of L-Query, B-Query, and T-Query for Korean and Japanese test collections. Note that pure Korean and Japanese nouns in the query are not substituted into English words during constructing B-Query from L-Query, such as  X  X   X  X  in Japanese.
Transliterations in the T-Query should appear in at least one WWW document; in other words, their ranking score given by S Web in transliteration ranking is above 0. Note that T-Query does not always include trans-literations (or query terms) in L-Query, because sometimes a transliteration system can not produce the same transliterations in L-Query from English words in B-Query. However, in our experiment, T-Query always includes all query terms in L-Query because our transliteration system always produces the same translitera-tion in L-Query. Sometimes our system produces transliteration variations, which does not exist in L-Query. Therefore, T-query can be regarded as expanded queries of L-Query.

In this experiment, the performance of information retrieval system when L-Query, B-Query, and T-Query are applied is compared. The purpose of the comparison between L-Query and T-query is to show the effect of transliteration variations on information retrieval system. Because T-Query and L-Query share the same query terms except transliteration variation, which only the T-Query contains, T-Query will show the performance of information retrieval when transliteration variations, which our transliteration system produces, are con-sidered. The purpose of the comparison between B-Query and T-query is to show the effect of machine trans-literation on information retrieval system. B-Query and T-Query share the pure Korean query terms or pure
Japanese query terms and B-Query contains English query terms corresponding to transliterations or translit-eration variations in T-Query. Therefore, the difference between the B-Query and T-Query makes it possible to investigate the performance change of information retrieval system when our machine transliteration system is applied to English query terms.
 are evaluated with interpolated 11 point average precision ( Salton, 1989 ). 4.2.2. Experimental results Table 15 shows the performance of Korean information retrieval and Japanese information retrieval when L-Query, B-Query, and T-Query are used. Improved % means the performance improvement compared to L-Query. Transliteration variations in T-Query contribute to 10.6% improvement for Korean and 7.6% improvement for Japanese. Our machine transliteration system (comparison between B-Query and T-Query) contributes 30.11% performance improvement for Korean and 58.75% improvement for Japanese. The results indicate that our transliteration system contribute significant performance improvement in information retrie-val system by automatically producing transliterations and transliteration variations.

Fig. 7 shows the performance changes depending on the retrieval models in Lemur system. We examine the results retrieved with the following six retrieval models: (1) TFIDF retrieval model (TFIDF), (2) Okapi BM25 retrieval function (OKAPI), (3) KL-divergence language model based retrieval method (KL), (4) InQuery retrieval model (INQUERY), (5) CORI collection selection (CORI_CS), (6) Cosine similarity model (COS). Results show that OKAPI gives the best performance in both languages. The performance of T-Query is higher than that of L-Query and B-Query regardless of retrieval models and languages. Transliterations and formance improvement of information retrieval system. 4.2.3. Analysis of results
We examined the results depending on difference between L-Query and T-Query. If query terms in L-Query and T-Query are different, then they are LTD otherwise LTS. There are two conditions where L-Query and
T-Query are LTS. First, there is no English term in B-Query. This means that L-Query and T-Query are com-posed of only the pure Korean or Japanese terms. Second, query terms, which are transliterations, in L-Query and T-Query are the same. This means that our transliteration system does not produce transliteration vari-ations, which do not exist in L-Query. In our experiment, T-Query always includes all query terms in L-Query when T-Query and L-Query are LTD. Because only the LTD can affect the performance change, we will focus on results of LTD queries rather than LTS queries. The number of LTD is 28 for Korean and 51 for Japanese and that of LTS is 22 for Korean and 32 for Japanese, in our experiment.

For example, the Korean L-Query ( X  X  X ISC X  X , weokeuseuteisyeon ) and T-Query ( X  X  X ISC X  X , liseukeu weokeuseuteisyeon , weokseuteisyeon ) is an LTD query. In the T-Query, the transliteration of  X  X  X ISC X  X  ( lise-ukeu ) and the transliteration variation of weokeuseuteisyeon ( weokseuteisyeon ) are the query terms that differentiate the L-Query and the T-Query. The L-Query retrieves 16 relevant documents while the T-Query does 24 relevant documents, among 26 relevant documents in the test collection for this query. 11 pt avg. precision of the L-Query and the T-Query is 0.1512 and 0.3949, respectively. Table 16 shows TF (Term fre-quency) and DF (document frequency) of the T-Query terms for relevant documents (Rel) and for all docu-ments (All). Table 16 gives an evidence for performance improvement. liseukeu , which is the transliteration of
RISC and does not exist in L-Query, is the main factor for performance improvement because it is one of effective query terms in the T-Query for retrieving relevant documents.

Fig. 8 shows recall/precision curves for the performances of LTS queries (K_LTS: 0.4374 and J_LTS: 0.3724 in 11-pt. avg. precision) and LTD queries (K_LTD_LQ: 0.2691, K_LTD_TQ: 0.3617, J_LTD_LQ: 0.3085 and J_LTD_TQ: 0.3394 in 11-pt. avg. precision). Note that LTD_LQ and LTD_TQ mean L-Query and T-Query in LTD, respectively. For Korean, T-Query (K_LTD_TQ) shows 34.38% performance improve-ment compared with L-Query (K_LTD_LQ). For Japanese, T-Query (J_LTD_TQ) shows 10.0% performance improvement compared with L-Query (J_LTD_LQ). 5. Discussion
There are two research topics related to transliterations. One is transliteration pair acquisition and the for given source words. The transliteration pair acquisition task is to find phonetic cognate in two languages and it is usually composed of two steps: phonetic conversion step and phonetic comparison. The phonetic con-version step phonetically converts words in one language to that in the others, like machine transliteration.
Then the converted words in one language and the original words in the other language are compared in order form Japanese katakana words into English words with some Japanese-to-English back-transliteration rules or a back-transliteration system. Therefore, machine transliteration can serve as one of components in the transliteration pair acquisition method by offering a machine-generated transliterated form. On the other as training data in machine transliteration. Therefore, the two research topics are complementary used for solving the problem related to transliterations.

Some machine transliteration methods have been proposed. Though they adopt the diverse array of infor-mation and mechanisms in order to produce transliterations for given source language words, they can be summarized in terms of information type , information usage , ensemble and ranking as described in Table 17 .
From the viewpoint of information type , GST ( Jeong, Myaeng, Lee, &amp; Choi, 1999; Lee, 1999; Lee &amp; Choi, GMEM ( Goto et al., 2003 ), and GJSC ( Li, Zhang, &amp; Su, 2004 ) are grapheme-based transliteration model ( w while PWFST ( Knight &amp; Graehl, 1997 ) and PST ( Lee, 1999 ) are phoneme-based transliteration model ( w and OURS are grapheme-and phoneme-based transliteration model ( w 17 , represents correspondence between source grapheme and phoneme, grapheme-and phoneme-based model can be re-classified into hybrid transliteration model ( w (CRULE and OURS) depending on whether it makes use of the correspondence or not. In the hybrid trans-literation model, w G and w P are modeled with WFSTs or source-channel model. Then w bined through the linear interpolation style. In their w P probability model, phoneme-to-target grapheme probability model, and target language word probability models, are considered. In their w G , only the source grapheme-to-target grapheme probability model is con-sidered. The main disadvantage of the hybrid model is lack of consideration on dependency between source grapheme and phoneme in the combining process; while CRULE and OURS consider the dependency by using the correspondence between source grapheme and phoneme. Note that the first character in the name of each method, like G , P , H , and C , represents w G , w 4.1.2 (see Table 8 ), w GP rather than w G and w P show higher performance because w source grapheme and phoneme. Note that GMEM and GDT are indirectly compared with our transliteration method because (1) GMEM is similar to our w G method based on MEM and (2) GDT is similar to our w method based on DT. In comparing w C (OURS) and w H (HWFST) in Table 7 , w than w H . This indicates that correspondence between source grapheme and phoneme is more useful than just combining both source grapheme and phoneme in machine transliteration.

From the viewpoint of information usage , a transliteration method tends to achieve better performance when it adopts wide context window and considers previous outputs. For example, GMEM and CRULE that satisfy this conditions gives more accurate results than GST, GNN, GDT, and PST which do not satisfy the give more powerful transliteration ability to machine transliteration systems. The previous works, however, limit their context window size to 3, because the context window size over 3 degrades the performance of their change the performance ( Kang &amp; Kim, 2000 ).

One transliteration model alone has limitation on reflecting dynamic behaviors of transliteration. Therefore several different transliteration models should be complementary used to achieve a high-performance machine transliteration system. The ensemble in Table 17 tells us whether a transliteration method uses transliteration results produced by different transliteration models. Though HSC and HWFST falling into the hybrid transliteration model use two different transliteration models ( w models for constructing their model rather than for producing transliteration results; while OURS makes use of transliteration results produced by three different transliteration model ( w son, only our method uses the ensemble.

A transliteration system that gives top-n result is more applicable to machine translation and information retrieval, because the applications need transliterations ranked by relevance factors to which the translitera-tion system assigns. There are two ways of transliteration ranking. One is relevance-based ranking and the other is web-based ranking. The relevance-based ranking method makes use of probability or relevance score given by a transliteration model. If a transliteration model produces a transliteration from a source word with high probability or high relevance score, the rank of the transliteration will be high. Text-based ranking 2005 ). In other words, text-based ranking method ranks transliterations by investigating how many docu-ments or web documents contain each transliteration or how frequently each transliteration appears in texts.
Therefore, a transliteration with high frequency (or document frequency) will be in high rank in text-based ranking method. In Table 17 ,  X  X  X elevance X  X  represents the relevance-based ranking method and  X  X  X ext X  X  repre-sents the text-based ranking method. From the viewpoint of the ranking method, previous works except
PWFST and HSC did not consider the text-based ranking method. PWFST uses English word counts to rank transliterations and HSC uses web-based ranking. Without text-based ranking method, noisy transliterations, ranking method is necessary for a transliteration system to filter out the noisy transliterations. However, the relevance-based ranking method as well as the web-based ranking method should be considered simulta-neously because a transliteration system depending on only the text-based ranking method may fail in ranking a transliteration due to absence of text containing the transliterations to be ranked. Our method considers the two ranking method at a time.

As a result, a good transliteration system should consider: (1) source grapheme and phoneme along with their correspondence simultaneously, (2) wide contexts and previous outputs, (3) dynamic behavior of trans-model satisfies the four conditions.

Though our ensemble-based transliteration model effectively produces transliterations, it shows its draw-back when there are production errors (in other words, if there is no correct transliteration among transliter-ations produced by three transliteration models, see Section 4.1.3 ). This is the main reason of errors in our model. Therefore, a way of effectively producing correct transliterations to be ranked is the key to improve the performance of a transliteration system. One possible solution may be another ensemble of transliteration models or machine learning methods. For example, an ensemble of three machine-learning algorithms for the three transliteration models proposed in this paper, may be helpful to reduce the production errors. 6. Conclusion
This paper has described an ensemble-based transliteration model, which reflects dynamic behaviors and real-world usages of transliterations. The generation-and-test paradigm is applied; transliterations generated by our transliteration model are tested in IR domain.

This paper contributes to machine transliteration research in three aspects. First, this paper shows that cor-respondence between source grapheme and phoneme is very useful for machine transliteration. In comparison test among w GP , w G , and w P in the same condition, w GP w , which relyon either source grapheme or phoneme. Second, this paper shows that an ensemble of different transliteration models is one of ways for reflecting dynamic behaviors of transliteration. The ensemble of dif-ferent transliteration models makes it possible for a machine transliteration system to produce the correct literation models can cooperate with each other by making up for the weak points of each transliteration model through the ensemble. Finally, this paper shows that transliteration ranking based on web data and relevance scores is one of ways for filtering out noisy transliterations. Because web data reflects real-world usage, noisy transliterations, which are not used in target language, can be filtered out. Through the three strong points, our ensemble-based transliteration model shows 78 X 80% word accuracy, which is about 28 X  59% improvements compared with the previous works.

The experiment on information retrieval shows that our transliteration model is useful for improving the performance in information retrieval system. Transliterations and transliteration variations produced by our transliteration system were evaluated on KTSET and NTCIR-1 test collection for Korean and Japanese, respectively. T-Query improves the performance of information retrieval system about 10 X 34% compared to the L-Query and about 30 X 58% compared to B-Query.

However, further research is needed in transliteration production and transliteration ranking in order to improve our ensemble-based transliteration model. First, further research about an ensemble of various trans-literation models and various machine learning algorithms such as SVM ( Vapnik, 1995 ) may be necessary to avoid errors in transliteration production. Second, a more sophisticated transliteration ranking method, which is independent of web data, may be necessary to rank transliterations.
 Acknowledgements
This work was supported by the Korea Ministry of Science and Technology, the Korea Ministry of Com-merce, Industry and Energy, and the Korea Science and Engineering Foundation (KOSEF).
 References
