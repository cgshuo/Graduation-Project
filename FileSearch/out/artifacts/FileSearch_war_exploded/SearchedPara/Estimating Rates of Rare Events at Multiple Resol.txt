 We consider the problem of estimating occurrence rates of rare events for extremely sparse data, using pre-existing hierarchies to perform inference at multiple resolutions. In particular, we focus on the problem of estimating click rates for (webpage, advertise-ment) pairs (called impressions ) where both the pages and the ads are classified into hierarchies that capture broad contextual infor-mation at different levels of granularity. Typically the click rates are low and the coverage of the hierarchies is sparse. To overcome these difficulties we devise a sampling method whereby we analyze a specially chosen sample of pages in the training set, and then es-timate click rates using a two-stage model. The first stage imputes the number of (webpage, ad) pairs at all resolutions of the hierar-chy to adjust for the sampling bias. The second stage estimates click rates at all resolutions after incorporating correlations among sibling nodes through a tree-structured Markov model. Both mod-els are scalable and suited to large scale data mining applications. On a real-world dataset consisting of 1/2 billion impressions, we demonstrate that even with 95% negative (non-clicked) events in the training set, our method can effectively discriminate extremely rare events in terms of their click propensity.
 Categories and Subject Descriptors: H.1[Information Systems]: Models and Principles General Terms: Algorithms Keywords: Imputation, Hierarchy, Clickthrough Rate, Tree-structured Markov model, Maximum Entropy, Internet Advertising, Algorith-mic Advertising
Web advertising supports a large swath of today X  X  Internet ecosys-tem with an estimated $15.7 billion in revenues for 2005 ( www. cnnmoney.com ). Some of these ads are textual and some are graphical. Contextual advertising or Content Match (CM) refers to the placement of commercial textual advertisements within the content of a generic web page, while Sponsored Search (SS) ad-vertising consists in placing ads on result pages from a web search engine, with ads driven by the originating query. In contextual ad-Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Figure 1: (a) Distribution of impression events and (b) click events. Plots are on log-log scale but ticks are on the original scale. 99.7% of impression events had no clicks. vertising usually there is a commercial intermediary, called an ad-network , in charge of optimizing the ad selection with the twin goal of increasing revenue (shared between publisher and ad-network) and improving user experience. Typically the ad-network and the publisher are paid only when the user clicks on an advertisement.
In this paper we examine data generated by a content match sys-tem where every showing of an ad on a webpage (called an im-pression ) constitutes an event. Some pages are much more popular than others and generate many more impressions, but for all im-pressions the click rate is at most a few percent. Figure 1(a) shows the frequency of (page, ad) pairs and Figure 1(b) shows the same distribution for a subset of impressions where a user clicks on the ad being shown on the page. Clearly, an overwhelming majority of (page, ad) pairs are extremely rare while a small fraction account for a large fraction of total impressions and clicks.

Finding ads suitable to a given page depends on many factors including: the page content, the publisher and ad-network business aims, the target audience for the advertisers, and the experience of the web user [14]. Estimating clicks per impression, or click-through rates (CTRs, henceforth), for a (page, ad) pair is a very important tool in finding such relevant ads for webpages. However, rate estimation is difficult due to sparsity in the impression distri-bution and rarity of clicks. Sparseness is pervasive since a large fraction of webpages and ads tend to be ephemeral. This occurs because webpages are often modified, content is generated dynam-ically, and ads are updated on a regular basis. Naive statistical esti-mators based on frequencies of event occurrences incur high statis-tical variance and fail to provide satisfactory predictions, especially for rare events. The usual procedure involves either removing or aggregating rare events to focus on the frequent ones [11]. While this might help in estimation at the  X  X ead X  of the curve, the loss in information leads to poor performance at the  X  X ail. X  In Internet advertising, the tail accounts for several billion dollars annually, making reliable CTR estimation an important problem.

We describe a statistical method to estimate rates of rare events in scenarios where these events are organized in an existing, well un-derstood, and intuitive hierarchy. Examples abound in the online-advertising domain and beyond: users organized in a geographic hierarchy, movies organized by genres, books organized by subject matter, webpages and ads organized by semantic content, disease symptoms organized by syndromic groups, and so on. The hier-archical structure is expected to induce correlations among sibling nodes at every level of the hierarchy. Can we exploit such correla-tions to improve our estimates? We demonstrate that the answer is in the affirmative in our application of Internet advertising.
Classical machine learning approach converts each page and ad into a set of features and learns a model to predict CTRs. How-ever, the number of features required in content match is large (we consider page features like URL, title, HTML tags, words, etc.; ad features like title, bidded phrases, the  X  X anding X  webpage of the advertiser, and so on). Aggregating results from such models to es-timate CTRs at coarser resolutions generally incurs high variance. We note that the simple strategy of using hierarchies as features fails to incorporate correlations that exist among siblings nodes.
In this paper, we propose a method to estimate rates of rare events at multiple resolutions, as determined by existing hierarchies for both pages and ads. These have been created and constantly re-fined by domain experts; they are well understood and routinely used in applications. Apart from ease of interpretability, there are other advantages in using such hierarchies. First, they provide a natural and intuitive framework to analyze CTRs at multiple scales, from coarser to successively finer resolutions. The hierarchies in-duce a tree structure on the CTRs, that is, CTRs of page and ad node pairs from finer resolutions are nested within node-pairs from coarser resolutions. The structure is expected to induce correlations in CTRs among siblings at each resolution. Also, at coarser reso-lutions, aggregation helps in combating data sparseness and pro-viding reliable estimates for rare events. Note that aggregation can substantially reduce variance but is likely to incur bias. However, if the hierarchy is informative and siblings are homogeneous, the variance reduction will far outweigh the increase in bias.
Such multi-resolution estimates are extremely useful and serve several purposes. First, the multi-resolution modeling framework provides better estimates at the finest resolution by  X  X orrowing X  information from coarser resolutions. In fact, coarser resolution es-timates act as priors that influence estimates at finer resolutions; the amount of influence depending on data sparsity. Second, the ability to estimate CTRs with precision at multiple scales helps in discov-ering macro level patterns that are hard to estimate reliably from feature based models, especially with sparse data. Among other things, the macro level information can be used to track the sys-tem through time for emerging trends, sudden changes, etc. In fact, such estimates can also help the user design efficient and targeted online experiments to explore opportunities that could be poten-tially lucrative (see [13] for an example.)
We assume that there exist classifiers that can accurately classify a page (ad) to a unique path in the page (ad) hierarchy. However, classification of a page into the hierarchy require features that are obtained by crawling the page. Crawling is an expensive operation that induces cost on both the ad-network and the page publishers in terms of network bandwidth, storage and computational resources. Furthermore, as we operate retrospectively on historical data, some URLs in the logs do not exist any more and we cannot crawl them, e.g., dynamic webpages, or webpages requiring user authorization. We restrict our study to pages that can be crawled. Further, to re-duce computation time and load on the system, we crawl only a sample of the webpages that receive impressions, and project our results over the entire set of events. In fact, sampling from major-ity class is a standard practice when predicting a rare response[9]. Moreover, we show that the results are not significantly affected over a wide range of sampling fractions.

However, sampling introduces bias that needs to be adjusted for in the estimation process. In traditional classification or regres-sion problems with rare response, this is done by an adjustment to the weight of observations. Such strategies don X  X  work in our case since we are estimating a response at multiple resolutions in a hierarchy. We propose a two stage model where the first stage model estimates impressions at all resolutions after correcting for the sampling bias. Conditional on the estimated impressions, our second stage model estimates the CTRs at all levels of the hierar-chy through a tree-structured Markov model. Both models exploit the hierarchical structure and are scalable.
We provide a method to estimate rates of rare events at multiple resolutions, where these resolutions are derived from a pre-existing hierarchy. We illustrate our method on a large-scale Content Match application. A sampling-based approach is used to reduce crawl-ing costs. We then present a two-stage modeling approach to es-timate CTRs at all resolutions. The first stage algorithm imputes impression volume at all resolutions after adjusting for the sam-pling bias. The imputation is implemented through a simple iter-ative proportional fitting algorithm (IPF). Conditional on the im-puted impressions, a tree-structured Markov model estimates the click rates after accounting for correlations induced by the hierar-chical structure. Our methods are highly scalable, and we empir-ically demonstrate the efficacy of our approach in estimating rare click rates at finer resolutions by using reliable estimates at coarser resolutions. Through validation experiments, we show that our ap-proach can accurately predict rates of events that do not even occur in the training data by utilizing structure present in the hierarchies.
The paper is organized as follows. An overview of our method is given in Section 2, and details in Section 3. Detailed experimental results are shown in Section 4. We discuss related work in Section 5 and finally conclude in Section 6.
We analyze clicks and impressions from a subset of historical logs of a current Content Match system. The logs contain a large number of (page, ad) impressions, a small fraction of which gener-ate clicks. Pages and ads are classified into a pre-existing hierarchy where the nodes correspond to broad contextual themes (e.g., ski-ing  X  winter sports  X  sports). We estimate CTRs using a two stage procedure. First, we crawl a sample of URLs from the logs and im-pute impression volume at all resolutions. Then, we estimate CTRs at these resolutions, using clicks and imputed impression volumes. Before describing our method, we first lay out our notation.
Let i and j represent nodes from the page and ad hierarchies re-spectively, and let ij denote an arbitrary element the cross-product of the two hierarchies. We shall refer to the elements in the cross-product as regions . In general, regions may overlap with each other, but in this paper, we only consider estimation for a set of regions Z that form a tree. In other words, for any two regions r one and only one of the following is true: r 1  X  r 2 , r 2 r = r 2 or r 1  X  r 2 =  X  . For ease of exposition, we further sim-plify our problem and assume that the page and ad hierarchies are identical (as is indeed the case in our illustrative application), and only consider regions ij where both i and j belong to the same depth in the hierarchy. Let the hierarchy be a tree with L + 1 lev-els, with the root at depth 0 and leaves at depth L &gt; 0 . For any region r  X  Z , let d ( r ) represent its depth in the tree, and pa ( r ) its parent region in the tree. Denote by Z ( ` )  X  2 Z the set of all regions obtained by considering combinations of nodes in the page and ad hierarchies at depth ` ( ` = 0 ,  X  X  X  ,L ). Note that Z root and consists of one region, and all other regions are subsets of Z (0) . The importance of the hierarchy can be stated as follows: both the impression volumes and CTRs for regions in Z ( ` +1) the same parent in Z ( ` ) are expected to be correlated. Accounting for such correlations would induce smoothness in the estimation process and is expected to reduce the overall mean squared error. This is similar in spirit to time series and spatial modeling where accounting for autocorrelations that are induced due to proximity in time or space have an impact on overall predictive performance [2, 8].
Obtaining features for both ads and pages is necessary for classi-fication into their respective hierarchies. Features on ads are read-ily available from historical records, so all ads can be classified and the exact impression volume for every node in the ad hierarchy can be computed. However, features on pages require crawling. As discussed in Section 1, it is not possible to crawl all pages and hence we restrict ourselves to those that could be crawled. Also, since crawling is an expensive operation, we further reduce cost by crawling only a sample of pages from this restricted set. Our goal is to estimate the impression volume for every region r  X  Z after adjusting for this sampling bias.

We define a page to be clicked if it has received at least one click in our data, and to be crawlable if crawling the page does not lead to an error (e.g.,  X  X age not found X  or  X  X uthorization re-quired X ). Since the number of pages P c in the clicked pool is rel-atively small, we attempt to crawl all of them, resulting in P crawlable pages. From the majority class of non-clicked pages, we take a random sample of size P s , of which P s,c are crawlable. In fact,  X  = ( P c,c + P s,c ) / ( P c + P s ) provides an estimate of the fraction of crawlable pages (see table 1). Scaling all impressions volumes in the ad hierarchy (which are known) by  X  provides corre-sponding estimates conditioned on the crawlable pool. Henceforth, unless otherwise mentioned, all our inferences are conditional on the crawlable pages. For each crawled page, we include all (page, ad) impressions associated with that page from historical data, and map them to the corresponding regions in Z . This yields the num-ber of sampled impressions in each region r  X  X  .

Given the sampled impressions, our goal is to estimate the true number of impressions for all regions of interest in Z ( ` ) Since all ad classifications are known, we know the total number of impressions at each node in the ad hierarchy. To obtain an unbi-ased estimate of the total number of impressions for the page hi-erarchy nodes, the sampled impression totals at each level of the page hierarchy are scaled up by a constant factor, this factor be-ing selected so that the total number of impressions match the total impressions in the historical data at each level. We also obtain a lower bound on impression volume for each region, which is the to-tal number of impressions in the region obtained from our sample. Thus, we have region totals that are prone to sampling variability and marginal totals that are based on a much larger set of observa-tions and have smaller variance. How do we distribute the excess impressions missed by the sampling process among the regions? First, by forcing the estimates to conform as closely as possible to the more accurate marginal impression totals at each node in the page and ad hierarchies, we reduce the overall variance of our re-gion estimates.[10] considered a similar problem in a different con-text for 2  X  2 tables. Second, by definition, the sum of estimated im-pressions for children regions nested within a parent region should agree with that of the parent. We accomplish these by imputing ex-cess impressions using a maximum entropy formulation, subject to the constraints mentioned above. Section 3 provide the details.
The second stage model is used to obtain CTR estimates in all regions, conditional on the imputed impression volumes. Denot-ing the exact number of clicks in a region r by c r and the imputed number of impressions by  X  N r , the maximum likelihood estimate (MLE) for the CTR  X  r is given by  X   X  r = c r /  X  N r . The variabil-ity in this estimator gets higher with smaller sample size. Thus, a ratio of 1 / 10 is less reliable than 10 / 100 . Our method smoothes the MLEs using the hierarchical structure of the regions: regions sharing the same parent are expected to share some common char-acteristics and hence similar  X  r values. We exploit this correlation by modeling the CTRs through a multi-resolution tree-structured Markov model [5, 7].

The central idea of the model is as follows. We assign a state variable to each node in Z . Conditional on the states, the observed CTRs are independent. Smoothing of CTR estimates is accom-plished through a Markovian model on the states. In particular, we assume that the states of children sharing the same parent are drawn from a distribution centered around the state of the parent. These sequences of recursive one-step Markovian distributions defined in a bottom-up fashion specify a joint distribution on the entire state space of CTR values. The posterior distribution of the state vari-ables given the data provides the smoothed CTR estimates. Note that although the observed CTRs are conditionally independent, un-conditionally they are not: The Markovian structure on the states induce dependencies in the observations.

An attractive feature of the model is its efficient computational aspect. For known values of all variance components, the posterior of the states can be computed by a Kalman filter algorithm that per-forms a filtering step in a bottom-up fashion from the leaves to the root followed by a smoothing step in a top-down fashion from the root to the leaves. However, since the variance components asso-ciated with the model are unknown, we estimate them through an Expectation-Maximization (EM) algorithm [1]. The EM algorithm involves iterating through the filtering and smoothing steps several times (for our application, less than 25 ) until convergence.
We now describe the first and second stage models in detail. The first stage model corrects for the sampling bias in imputing impres-sion volumes, followed by the second-stage model that estimates CTRs at multiple resolutions.

Figure 2: Impressions in a region, row, and column for Z (2)
For the sake of notational simplicity, we assume that the set of regions Z consists of two successive levels of nested regions cor-responding to depths 1 and 2 respectively. Generalization to all regions formed by the full page and ad hierarchies follows easily. Let IJ and ij denote regions in Z (1) and Z (2) respectively. Let n r and m r denote impressions in region r from the clicked and sampled non-clicked pools of webpages. Thus, lb r = n r + m provides a lower bound on the impression volume for region r . Let N r denote the true impression volume in region r that is to be es-timated. Consider the linear transformation x r = N r  X  lb solve our estimation problem in terms of x r s and derive estimates of N r as  X  N r =  X  x r + lb r , where  X  x r is our estimate of x one can interpret x r  X  X  as excess impressions to be allocated to ad-just for the sampling bias. We split the subsequent discussion into three parts: (a) data preparation and consistency, (b) constraints on the imputation model, and (c) using the constraints to estimate the impression volumes.

A page (ad) classified to a node i in the taxonomy is assumed to belong to the entire path from i to the root. Also, a page (ad) may get classified to a node at depth other than L (i.e., leaf level). However, this may create inconsistencies in the total number of impressions and clicks obtained at different levels. For instance, the total number of impressions (clicks) for a group of children regions may be strictly smaller than the impressions (clicks) of the parent region they are nested within. To ensure consistency, we uniformly distribute the extra impressions and clicks in a parent node among its children. The steps are repeated at every level in a top-down fashion. Thus, each impression in a non-leaf region is guaranteed to come from some smaller region nested within it.

The region impressions are estimated subject to certain linear constraints. We impose three sets of constraints, all of which are expressed in terms of the excess impressions x r (Figure 2). The first set of constraints, called column constraints , ensures that the sum of impressions along a column adds up to the total impressions for the corresponding node in the ad hierarchy:
X X where a j ( a J ) is the total impression volume for node j ( J ) in the ad hierarchy, and CS ( . ) . represents the excess impressions in the column that were missed by the sampling process. Note that for a node J at level 1 in the ad taxonomy, a J = P j : pa ( j )= J pa ( j ) denotes the parent of node j , that is, the column impressions total for a level 1 node is the sum of the column totals of its children in level 2 . Also, P j CS (2) j = P J CS (1) J = TotExcess , where TotExcess is the total number of excess impressions in the data. The second set of constraints, called row constraints , preserve im-pression volumes at nodes in the page hierarchy and are given as follows: where RS ( . ) . represents the excess impressions aggregated for each node in the page taxonomy, and K (1) and K (2) are constants for levels 1 and 2 . The underlying assumption is that for each sampled impression, there are K ( . ) times as many excess impressions from the non-clicked pool that did not appear in the sample. Since we randomly sample pages from the non-clicked pool, this simple ad-justment is reasonable. The constants K ( . ) are chosen to preserve total impression volume, i.e., we choose them so that P i P block constraints , ensure that the excess impressions allocated to a region at level 1 equals the sum of excess impressions allocated to regions nested within it at level 2 : We note that the true impression volumes satisfy the block con-straints, hence it is necessary to impose them in our imputation. Analogous row, column and block constraints are imposed at all other levels ` ( ` = 0 ,  X  X  X  ,L ) .

Given a set of positive initial prior values { x r (0) } for all regions r  X  Z , we want a solution { x r } which is as close as possible to the prior { x r (0) } but satisfies all the row, column and block constraints. This is equivalent to finding a solution that has the smallest discrepancy from the prior in terms of Kullback-Leibler divergence, subject to the linear constraints [6]. It is also referred to as the Maximum Entropy model, since when { x r (0) } is uniform, the solution maximizes the Shannon entropy.
 We solve this Maximum Entropy imputation problem using an Iterative Proportional Fitting (IPF) algorithm [6], which iterates cyclically over all the constraints and updates the x r values to match the constraints as closely as possible. Specifically, at the t ation, suppose a constraint of the form P r k r x r = C is being violated ( k r = 0 or 1 for all our constraints). Let the current value C ( t ) of the LHS be C ( t ) = P r k r x r ( t ) , where C IPF adjusts each element x r involved in the constraint by a constant factor C/C ( t ) to get the new values x r ( t + 1) = x r Note that this update rule ensures non-negativity of the final solu-tion. Such updates are performed for all constraints until conver-gence. Our algorithm jointly estimates all x r s by iterating through a series of top-down and bottom-up scalings. Here, we provide a description for a two level tree; a complete description of the al-gorithm for an arbitrary number of levels is provided in Figure 3. At the t th iteration, we start with level 1 , and modify { x { x IJ ( t + 1) } after adjusting for the row and column constraints. This changes the values of { x ij ( t ) } s at level 2 to { x justing for the corresponding block constraints. Now, we switch Initialization: From iteration t to t + 2 : x ( t + 1) , where ch ( r ) are all children regions nested within r for row and column constraints. This completes the top-down step. In the bottom-up step, the leaf regions (in this example, regions at depth 2 ) do not change, i.e., x ij ( t + 2) = x ij ( t + 1) . Using the block constraints, the values at level 1 change to x  X  IJ P satisfy the level-1 constraints, ending with x IJ ( t + 2) . The top-down and bottom-up steps are iterated until convergence. In all our experiments, the algorithm converges rapidly, requiring at most 156 iterations for an error tolerance of 1% .
 One variable in our imputation algorithm is the choice of prior. We assume x r (0)  X  lb r ; this ensures that we distribute the ex-cess impressions in proportion to the lower bounds obtained from the crawled sample as closely as possible subject to the linear con-straints. An alternative is to simply use the traditional IPF algo-rithm, which starts with a prior of x r (0)  X  1 , and computes the x r values for each level separately, using only the row and col-umn constraints. It can be shown that this automatically satisfies the block constraints as well, due to the relationships between the row and column sums at different levels. However, the prior dis-tributes the excess impressions using an independence model and does not incorporate the a-priori interaction information we have in the lower bounds. We show empirically in Section 4 that the lower-bound prior outperforms the naive scheme significantly, for regions at all levels.
This section is organized as follows. We first discuss data trans-formation used to facilitate our analysis. This is followed by a de-scription of the tree-structured Markov model used to obtain smoothed estimates of CTRs. Finally, model-fitting via an EM algorithm is described.

The distribution of raw CTRs is extremely skewed and the vari-ance depends on the mean (roughly, V ar  X  mean/  X  N r ), as shown later in Figure 6. Instead of modeling our data on the original scale, we use a transformed scale. A squared-root transform is commonly recommended for count data but we use the more stable Freeman-Tukey transformation [8], defined as follows: where c r is the number of clicks in region r , and  X  N r puted number of impressions. This transformation has a number of advantages, especially when modeling rare rates [8]. The second term provides a way to distinguish between zeros on the basis of the number of impressions, e.g., zero clicks from 100 impressions corresponds to a smaller transformed CTR than zero clicks from only 10 impressions. It also tends to symmetrize the otherwise ex-tremely skewed rate distribution. Most important is its variance stabilization property, which makes the variance of the distribution independent of the mean (roughly, V ar  X  1 /  X  N r ). This holds for our data as shown later in Figure 7.

We describe the tree-structured Markov model used to model our data. In our context, the set of regions Z forms a tree structure, and we model the data through a generative model wherein the CTRs of the regions are connected to each other at different resolutions in a Markovian fashion. Each region r has a transformed rate y is observed, a covariate vector u r , and a latent state S ate vector can represent any region-specific information, such as prior knowledge distinguishing different region  X  X ypes. X  Examples might include percentage of total visits distributed by geography.
For simplicity, we assume a single covariate per level. In fact, in our example dataset, u T r = 1 for all r giving us one covariate for each level in the region hierarchy. Conditional on the states { S being known, we assume the observations y r to be independently distributed as a Gaussian: where  X  ( d ( r )) is the unknown coefficient vector attached to covari-ates at level d ( r ) , and V r is the unknown variance parameter. Intu-itively, the latent S r variables are adjusting for effects that are not accounted for by the covariates. However, estimating one S region leads to severe overfitting; hence smoothing on S r essary. We perform smoothing by exploiting dependencies induced by the tree structure of regions: where w r  X  N (0 ,W r ) for all r  X  X  \Z (0) . Also, w r is indepen-dent of S pa ( r ) and S Root = W Root = 0 . Figure 4 shows the model in graphical notation.

Since it is not feasible to estimate a separate W r and V region, we need to make some assumptions on their structure. In our case, we assume that all regions at the same level have the same W r value: W r = W ( ` ) for all r  X  Z ( ` ) . Modeling assumptions on V r depend on the data and the tree structure of regions. In our case, V ar ( y r )  X  1 /  X  N r (from Equation 5 and [8]). Hence, we as-sume that there is a V such that V r = V/  X  N r for all r  X  X  ratios W r /V r determine the amount of smoothing that takes place with our model. Intuitively, if W r is large relative to V ling S r s are drawn from a distribution that has high variance and hence we achieve little smoothing. In fact, if W r /V r  X   X  , then S r  X  ( y r  X  u T r  X  ( d ( r )) ) and we perfectly fit the training data. On the other extreme, if W r /V r  X  0 , then S r  X  0 and we fit a re-gression model given by the covariates, with the maximum possible smoothing. As we show in Section 4, the smoothing achieved by this method performs well on our real-world dataset.

To understand the dependency structure imposed by the model, we examine the correlations implied by the state equations. From Equation 7 and the independence of w r and S pa ( r ) , it follows that Thus, the variance in the states S r depends only on the depth of re-gion r , and increases as we move from coarser to finer resolutions. Also, for any two regions r 1 and r 2 at depth ` sharing a common ancestor q at depth ` 0 &lt; ` , the covariance between the state values is given by Cov ( S r 1 ,S r 2 ) = V ar ( S q ) , which depends only on ` . Hence the correlation coefficient of nodes at level ` whose least common ancestor is at level ` 0 is given by which depends only on the level of the regions and the distance to their least common ancestor. We note that y r s are independent conditional on S r s; however the dependencies in S r s impose de-pendencies in the marginal distribution of y r s.

Model fitting is accomplished through an EM algorithm that es-timates the posterior distribution of { S r } s and {  X  ( d ( r )) provides point estimates of the variance components { W ( ` ) V . Next, we provide the core ideas of the algorithm, with details presented in the appendix.

The heart of the algorithm is a Kalman filtering step which effi-ciently estimates the posterior distribution of { S r } s for fixed values of the variance components. The Kalman filtering algorithm itself consists of two steps, namely, a filtering step that aggregates infor-mation from the leaves up to the root, followed by a smoothing step that propagates the aggregated information in the root downwards to the leaves. The former collects information from the children and passes it to the parents while the latter passes information from parents to children. To provide intuition on the filtering step, note that we can invert the state equations to express parent states in terms of their children states: V ar (  X  r ) = W ( d ( r )) B r . This provides a basis for parents to col-lect information from their children. Starting from initial estimates in the Kalman filtering and smoothing steps, then recomputes these variance and covariate components, and repeats the process until convergence. Specifically, at step t + 1 , it first computes the ex-pected log-likelihood of the conditional distribution of all the state variables { S r } given the current estimates of all variance and co-variate components { W ( ` ) ( t ) } , V ( t ) , {  X  ` ( t ) } and the data { y This is called the E-step, and uses the posterior distributions of the state variables from the Kalman filtering and smoothing steps de-scribed above. This is followed by the M-step, where we find the parameters { W ( ` ) ( t + 1) } , V ( t + 1) and {  X  ` ( t + 1) } that max-imize the conditional distribution of { S r } . These new estimates are now used at the next timestep. Thus, the Kalman filtering and smoothing steps are used in the  X  X nner loop X  of the EM algorithm.
The complexity of the Kalman filtering and smoothing steps is linear in the number of regions both in terms of computing time and memory requirements. On all our experiments, EM converged in less than 25 iterations. This demonstrates the scalability of our method.
We performed experiments on a large corpus of real (page, ad) data obtained as a snapshot from a set of selected servers of an ac-tive content match system. We will call this dataset DFULL . In Section 4.1, we describe our experimental setup and the charac-teristics of DFULL . We then verify the correctness of the first-stage algorithm for imputing impression volumes in Section 4.2, and demonstrate the effectiveness of our tree-structured Markov model in Section 4.3.
For the time period considered, we obtained approximately 503 million impressions. Out of a total of approximately 6 million pages, 32 , 000 were in the clicked pool. From the sampled non-clicked pool, about 13 , 000 crawlable pages were classified into the page hierarchy. We used the same hierarchy for both ads and pages. Our hierarchy consists of 7 levels, of which the top 3 (other than the root) were considered for our analysis. They contain 20 , 223 and 972 nodes respectively. Level-1 regions with less than 50 clicks were removed along with their subtrees, leaving us with a final hierarchy of 18 , 177 and 860 nodes at levels 1 , 2 and 3 re-spectively. The data shows significant sparseness in click volumes, especially at finer resolutions. In fact, about 76% and 95% of re-gions at levels 2 and 3 had no clicks. The maximum likelihood estimator predicts zero CTR for all zero-click regions, rendering it useless for modeling rare rates. We show that our tree-structured model provides informative estimates of CTRs for these zero-click regions by  X  X orrowing X  strength from their ancestors in the region tree.
 Figure 5: Correlation of imputed impressions with lower-bounds from DFULL : Each bar gives the correlation value for level 3 , and the increase in correlation when we look at levels 2 and 1 successively. For each pair of bars, the left bar corresponds to the independence prior and the right bar to the lower-bound prior. The lower-bound prior leads to much better correlation.
We validate the accuracy of our first-stage imputation scheme through three sets that are obtained by considering a nested se-quence of sampled non-clicked pages (with sampling fractions 1 / 4 , 2 / 4 and 3 / 4 ) from DFULL . Correlation coefficients between im-puted impressions  X  N r in each sample and lower-bounds lb provide a measure of agreement. The rationale is as follows: when all pages are crawled, lb r in DFULL is exactly the truth; when DFULL is only a larger validation set, the region lower-bounds are good approximations to the truth. Hence, strong correlations of the sub-samples with lower bounds in DFULL provide evidence of a good imputation algorithm. Figure 5 shows the correlation be-tween  X  N r from the samples and lb r from DFULL 1 . We also com-pare imputation schemes with two different priors { x r (0) } : the lower-bound prior (LB) which assumes x r (0)  X  lb r and the in-dependence prior with x r (0)  X  1 . Our findings are summarized below.
Logarithms are used to tame the skew on the original scale but all results on the original scale were qualitatively similar
As discussed in Section 3.2, we use the Freeman-Tukey (F-T) transformation to model our data. Figure 6 is obtained by binning the maximum likelihood estimates of CTRs with bin sizes of 100, and plotting p  X  N r times the variance in each bin against the mean in the bin. The variance clearly increases with the mean. Figure 7 shows the same plot for F-T transformed data, and no such trend is visible. This demonstrates the variance stabilization aspect of the F-T transform on our data.

Before fitting the tree-structured Markov model, we conduct an exploratory analysis to investigate evidence of sibling correlations in CTR values. We used two statistics: (1)  X  X oran X  X  I , X  which is used as a measure of correlation among nearby regions in spatial statistics [8], and (2) F -ratio, which is used in analysis of variance (ANOVA) to compare the mean squared error among sibling groups to the mean squared error within sibling groups. Positive correla-tions are indicated by I &gt; 0 and F &gt; 0 , with I = 1 indicating maximum homogeneity among siblings. In Table 2, both Moran X  X  I and F -ratio show evidence of moderate global correlations (with statistical significance) in levels 2 and 3 with the effect being more pronounced at level 2 . Thus, sibling regions are indeed correlated. Next, we discuss results on our tree-structured Markov model. Our analysis was performed on a sample (henceforth DSAMPLE ) of DFULL formed by sampling two-thirds of the pages from DFULL . We fit our model to DSAMPLE and validate its predictions on DFULL . As discussed in Section 3.2, the smoothing achieved by our model depends on the ratios W r /V r =  X  N r W ( d ( r )) /V , with lower val-Table 2: Moran X  X  I and ANOVA scores show moderate correla-tions among siblings. Table 3: Smoothing by level, given in terms of median (  X  W ( ` ) /V . Higher values indicate less smoothing ues indicating more smoothing. Table 3 provides an estimate of median (  X  N r ) W ( ` ) /V for each level, and Figure 8 shows, for each level, the percentage change of the estimated values from the raw observed values. There is almost no smoothing at level 1 due to abundant data. Smoothing increases at finer resolutions as in-dicated by decreasing values of median (  X  N r ) W ( ` ) /V . Also, the amount of smoothing reduces with increasing values of  X  N these results are expected and intuitive, and serve as sanity checks on the model X  X  performance.

We next demonstrate that CTR estimates at the finest resolution (level 3 ), where there is extreme data sparseness, benefit from us-ing prior knowledge available at coarser resolutions (levels 2 and 1 ) through our tree-structured Markov model ( TS hereafter). To estab-lish a baseline, we consider two competing models. The first, called Level-Mean ( LM ), performs smoothing similar to TS except that the S r values are drawn from a prior centered around 0 instead of be-ing centered around the parent S pa ( r ) . This model  X  X hrinks X  CTR estimates towards the mean CTR at level 3 and does not exploit information from coarser resolutions. The second is No-Shrinkage ( NS ), a naive model which assigns a CTR estimate of 1 / ery region with zero clicks at level 3 . Unless otherwise mentioned, all our validation is reported at level 3 , and all CTR estimates are plotted on the square-root scale for exposition purposes.
For validation, we consider the set of regions R  X  Z (3) with zero clicks in DSAMPLE . Let R 0  X  R be the set of regions which also had zero clicks in DFULL , and R &gt; 0 = R \ R 0 be the re-Figure 10: CTR estimates from regions in R 0 and R &gt; 0 NS , and TS . gions with at least one click. CTR estimates are derived from the model for each region in R , and then tested against labeled data in DFULL ; higher scores for regions in R &gt; 0 relative to regions in R indicates better model performance, especially for predicting rare events which are the main focus of this paper.

Before providing a comparative analysis of the three models, we provide additional insights on how smoothing occurs for TS and LM . Figure 9 shows a plot of estimated CTRs in R versus  X  values of  X  N r approximately above 400 , the CTR estimates are al-most dictated by  X  N r and hence not interesting. However, the CTRs estimates from TS for  X  N r &lt; 400 are remarkably different from those obtained through LM . While the latter still maintains the trend of being mostly determined by  X  N r , estimates from TS show a lot of variability. This occurs because in the absence of click infor-mation at level 3 and small values of  X  N r , TS learns the CTRs by using information available at coarser resolutions. This is exactly what the tree-structured Markov model is designed to do: transmit information to sparse regions by learning patterns at coarser resolu-tions. The transmission is determined by the hierarchical structure imposed on the regions by the page and ad hierarchies, which are assumed to group regions that are similar and expected to have ho-mogeneous CTRs. This provides evidence in favor of our hypoth-esis that the pre-existing hierarchies created using domain knowl-edge can enhance CTR predictions. Indeed, restricting ourselves to regions of R with  X  N r &lt; 400 , Figure 10 shows the distribution Figure 11: ROC plot for TS , LM , NS , and a random predictor that knows the proportion of | R &gt; 0 | to | R | . LM and NS are almost indistinguishable. of CTRs for each of TS , LM and NS in both R &gt; 0 and R model should predict higher CTRs for regions from R &gt; 0 to R 0 . Only TS shows this discriminatory power, with strong statis-tical significance (t-statistic value=6.7, p-value=0). Corresponding results for LM and NS are weak; t-value=-2.3 (p-value=0.02) and t-value=2.03 (p-value=0.04) respectively, and have low statistical significance
Figure 11 shows ROC curves obtained using the model estimates for R and the true labels from DFULL . Better performance is in-dicated by larger area between the curve traced by the model esti-mate and the straight line passing through the origin. The straight line corresponds to a random classifier which labels an example as positive with probability equal to the proportion of positives in the training set; this baseline is stringent in our context since an estimate of the proportion of | R &gt; 0 | to | R | is not available from the training data. TS is superior to all three: LM , NS and the ran-dom classifier (even without knowing the probability of the positive class).

Overall, these results show the discriminatory power of our tree-structured Markov model, and its ability to predict CTRs even for extremely rare events (in fact, events which never even occur in the training data).
Multi-resolution modeling has recently become popular in the engineering and statistics literature, especially in the context of time series and spatial statistics. A rich literature in spatial statis-tics, known as the modifiable areal unit problem (MAUP), is related to our problem. The term MAUP was first coined by geographers Openshaw and Taylor [12] and refers to change in statistical infer-ence based on the spatial resolution at which the data is analyzed. Two ways in which MAUP manifests itself is in the  X  X ggregation X  effect where inference changes with more aggregation and  X  X on-ing X  effect where statistical inference depends on the resolution of spatial units that are considered for analysis. However, in our case we assume the taxonomies provide us with the resolutions at which it is interesting to analyze the data. We are more closely related to recent work on multi-scale tree models where nodes in the tree cor-respond to spatial units at different resolutions. Data are observed on some nodes and the goal is to predict at other nodes (see [5, 7] and references therein). Application of such models for estimating rates in large scale web applications is novel and to the best of our knowledge not considered before.

Imputing missing values is well known in statistics. Data impu-tation to adjust bias due to non-response in surveys is routine [3]. The key in any imputation scheme is the use of an appropriate model which captures the underlying mechanism appropriately. For instance, [4] discuss a novel application in ecology whereby miss-ing pixel values are imputed using an Ising model that captures the spatial structure present in the data. The model-based procedure proposed in this paper for imputing impression volume is related to methods described in [10].
We present a method to estimate the rates of rare events at multi-ple resolutions in large-scale web applications. This is a challeng-ing problem: the feature spaces are high-dimensional, the data is extremely sparse, with few impressions compared to the size of the feature space and even fewer clicks, and the scale and constant evo-lution of the Web render the collection of complete data expensive.
We combat these problems by considering the data at multiple resolutions using pre-existing taxonomies. Our contributions are threefold: First, we present a sampling scheme which reduces vari-ability in CTR estimation by sampling only on pages from the non-clicked pool. Second, we present an algorithm to impute impres-sion volumes at all levels of the taxonomy, using the sampled data and constraints induced by the taxonomies. Third, we propose a method for simultaneously estimating CTRs at all resolutions. The method performs smoothing by using the more reliable estimates at coarser resolutions to aid inference at finer resolutions where the data sparseness problem is most extreme.

We show the effectiveness of our model on a large real-world dataset where, at the finest resolution, 95% of the regions have zero clicks. We show that even under such sparsity, our model can discriminate between regions which truly have negligible click rates from those which might get more clicks with more impres-sions. This could be extremely useful for online explore/exploit al-gorithms which, instead of having to explore each region in round-robin fashion, can now quickly home in on regions where new im-pressions have the best chances of generating clicks. [1] A.Dempster, N.Laird, and D.Rubin. Maximum likelihood [2] G. Box and G.M.Jenkins. Time series analysis:forecasting [3] D.B.Rubin. Multiple Imputation for Nonresponse in Surveys . [4] D.K.Agarwal, J. Silander, A.E.Gelfand, R.E.Dewar, and [5] H.C.Huang and N.Cressie. Multiscale graphical modeling in [6] J.N.Darroch and D.Ratcliff. Generalized iterative scaling for [7] K.C.Chou, A.S.Willsky, and R.Nikoukhah. Multiscale [8] N.Cressie. Statistics for Spatial Data . John Wiley, New York, [9] N.V.Chawla, K.W.Bowyer, L.O.Hall, and W.P.Kegelmeyer. [10] P.Li, T.Hastie, and K.Church. A sketch algorithm for [11] S.Hill, D.Agarwal, R.Bell, and C.Volinsky. Building an [12] S.Openshaw and P.Taylor. A million or so correlation [13] S.Pandey, D.Agarwal, D.Chakrabarti, and V.Josifovski. [14] C. Wang, P. Zhang, R. Choi, and M. D. Eredita.

