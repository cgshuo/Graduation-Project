 For developing intelligent E-business systems, an important building block is to automatically extract attribute information from product description pages from different Web sites. For example, a product description page may contain a number of product attributes such as resolution and ISO of a digital camera in the digital camera domain. Existing automatic Web information extraction techniques including wrappers aim at extracting the product attributes from Web pages [1,11,17]. The product attributes of interest are normally specified by users, requiring a substantial amount of domain knowledge and manual ef-fort. On the other hand, users usually are interested in a subset of the product attributes that are relevant to some features for making purchasing decision. For example, in the digital camera domain, users mainly consider the feature  X  X icture quality X , which is related to several product attributes including  X  X eso-lution X ,  X  X SO X , etc. Some other features such as  X  X upported operating systems X  may constitute tiny influence on users X  decision making. Existing information extraction methods cannot automatically identify the product attributes that are of the users X  interest. Moreover, these kinds of attributes are usually un-known in different domains. As a result, it raises the need for a method that can automatically identify the product attributes that are of users X  interests and extract these attributes from Web pages.

We develop an unsupervised learning framework for extracting popular prod-uct attributes from different Web product description pages. Unlike existing systems which do not differentiate the popularity of the attributes, we propose a framework which is able not only to detect concerned popular features of a product from a collection of customer reviews, but also to map these popular features to the related product attributes, and at the same time to extract these attributes from description pages. We explain the rationale of our framework using the following example. Fig. 1(a) shows a Web page about a netbook prod-uct. This page contains a list of description such as the text fragments  X 10.1-inch high-definition display ... X ,  X 1.5 GHz Intel Atom dual-core N550 processor ... X , and  X 2 GB installed DDR3 RAM ... X  showing different attribute information of the netbook. However, not all of them are of interests to most customers and in-fluence users X  decision. We wish to extract those attributes which are important for customers to make decision. To achieve this goal, we make use of a collec-tion of online customer reviews available from Web forums or retailer Web sites as exemplified in Fig. 1(b) to automatically derive the popular features. Note that the concerned product from the Web page does not necessarily appear in the collection of reviews. Each popular feature is represented by a set of terms with weights, capturing the association terms related to that popular features. For example, terms like  X  X creen X  and  X  color X  are automatically identified to be related to the popular feature  X  X isplay X  of a netbook by analyzing their frequency and co-occurrence information in the customer reviews. Next these terms can help extract the text fragment  X 10.1-inch high-definition display ... X  because it contains the terms  X  X creen X  and  X  X olor X . Our framework can then reinforce that terms like  X  X esolution X  and  X  X igh-definition X , which are contained in the text fragment are also related to the popular feature  X  X isplay X . These newly identified terms can be utilized to extract o ther attributes related to  X  X isplay X . On the other hand, some other attributes such as  X  X eyboard X  are not mentioned in most reviews. Hence, the text fragment  X  X omfortable keyboard ... X  will not be extracted. In a particular domain, let A = { A 1 ,A 2 ,... } be the set of product attributes characterizing the products in this domain. For example, the set of product attributes of the netbook domain include s  X  X creen X ,  X  X ulti-m edia X , etc. Given a Web page W about a certain product in the given domain. W can be treated as a sequence of tokens ( tok 1 ,...,tok N ( W ) )where N ( W ) refers to the number of tokens in W . We also define tok l,k as a text fragment composed of consecutive tokens between tok l and tok k in W ,where1  X  l  X  k  X  N ( W ). Let L ( tok l,k )and C ( tok l,k ) be the layout features and the content features of the text fragment tok l,k respectively. We denote V ( tok l,k )= A j if the text fragment tok l,k is related to the attribute A j .
 We denote A POP  X  A as the set of popular product attributes. Recall that A
POP is related to the popular features, namely, C ( R ) , discovered from a collection of customer reviews, namely, R , about some products in the same domain. Our popular attribute extraction problem can be defined as follows: Given a Web page W of a certain product in a domain and a set of customer reviews R in the same domain. The concerned product in the Web page W does not necessarily appear in R . We aim at automatically identifying all the possible text fragments tok l,k where 1  X  l  X  k  X  N ( W )in W such that V ( tok l,k )= A j and A j  X  A POP by considering L ( tok l,k ), C ( tok l,k ), and the popular features C ( R ) .Notethat A POP are automatically derived from C ( R ) beforehand and does not need to be pre-specified in advance.

Our proposed framework is composed of two major components. The first com-ponent is the popular attribute extraction component, which aims at extracting text fragments corresponding to the popular attributes from the product de-scription Web pages. Web pages are regarded as a kind of semi-structured text documents containing a mix of structured content such as HTML tags and free texts which may be ungrammatical or just composed of short phrases. Given a Web page W about a certain product in the given domain, W can be treated as a sequence of tokens ( tok 1 ,...,tok N ( W ) ). Our goal is to identify all text frag-ments tok l,k such that V ( tok l,k )= A j and A j  X  A POP where A POP  X  A . This task can be formulated as a sequence labeling problem. Precisely, we label each token in ( tok 1 ,...,tok N ( W ) ) with two sets of labels. The first set of labels contains the labels  X  X  X ,  X  X  X , and  X  X  X  denoting the beginning of an attribute, in-side an attribute, and outside an attribute respectively. The second set of labels is A j  X  A POP , i.e. the type of popular attributes. Conditional Random Fields (CRFs) have been adopted as the state-of-the-art model to deal with sequence labeling problems. However, existing standard CRF models are inadequate to handle this task due to several reasons. The first reason is that each token will be labeled by two kinds of labels simultaneously, whereas standard CRF con-siders only one kind of labels. The second reason is that the popular attributes are related to the hidden concepts deri ved from the customer reviews by the second component and are unknown in advance. This leads to the fact that su-pervised training adopted in standard CRF cannot be employed. To tackle this problem, we have developed a graphical model based on hidden CRF. The pro-posed graphical model can exploit the d erived hidden concepts, as well as the clues from layout features and text cont ent features. An unsupervised learning algorithm is also developed to extract the popular attributes.

The second component aims at automatically derive A POP from a collection of customer reviews R . This component generates a set of derived documents from R . We develop a method for selecting i mportant terms for constructing the derived documents. Latent Dirichlet Allocation (LDA) is then employed to discover latent concepts, which essentially refer to the popular features of the products C ( R ) , from the derived documents [2]. Each c  X  C ( R ) is essentially represented by a multinomial distribution of terms. For example, one popular feature is more likely to generate the terms  X  X isplay X ,  X  X esolution X ,  X  X creen X , etc., while another popular feature is more likely to generate the terms  X  X amera X ,  X  X peaker X , etc. By making use of this ter m information, our graphical model can extract the text fragments related to the popular attributes. 3.1 Our Model Fig. 2 shows the graphical model capturing the inter-dependency among the es-sential elements in the extraction problem. Each node and edge of the graphical model represent a random variable and the dependence between two connected nodes. Recall that given a Web page, we can conduct some simple preprocess-ing by analyzing the DOM structure. Th e text content in the Web page can be decomposed into a sequence of tokens ( tok 1 ,...,tok N ( W ) ). A random variable X refers to the observation from the sequence. For example, it can be the or-thographical information of the tokens, or the layout format of the Web page. Another set of random variables denoted as Y =( y 1 ,...,y N ( W ) ) ranging over a finite set of label alphabet Y refer to the class labels of each token. Recall that each tok l,k corresponds to a contiguous text fragment between tok l and tok k . Hence, each y i can be equal to  X  X  X ,  X  X  X , or  X  X  X  denoting the beginning of an attribute, inside an attribute, and out of an attribute respectively. In order to incorporate the information of the derived hidden concepts, which represent the popular product attributes discover ed from the customer reviews, we design another set of random variables U =( u 1 ,...,u N ( W ) ) ranging over A POP  X  X   X  A } where  X  A is a special symbol denoting  X  X ot-a-popular-attribute X . Essentially, each u represents the popular attribute that tok i belongs to. We use V , E Y ,and E U to denote the set of all vertices, the set of all edges connecting two adjacent y s, and the set of all edges connecting a particular y and a particular u respectively. Our model is in the form of a linear chain. Hence, the joint distribution P ( Y = y, U = u | X = x ) over the class label sequence y and the popular attribute labels u given the observation x and the set of parameters  X  can be expressed as follows by the Hammersley-Clifford theorem: where f k ( e, y | e ,x ) refers to the feature function related to x , the nodes y scon-nected by the edge e  X  E Y . Referring to the text fragments  X 10.1-inch high-definition display ... X , where tok 1 =  X 10.1-inch X , tok 2 =  X  X igh-definition X , ... , we may design a feature function f k ( e, y | e ,x )=1if y 1 = B , y 2 = I , x 1 contains anumber,and f k ( e, y | e ,x )=0otherwise. g k ( v,y | v ,x ) refers to the feature func-tion related to x , the node v represented by the vertex v  X  V ; Similarly, we may design a feature function g k ( v,y | v ,x )=1if y i = I and x i is the word  X  X igh-function related to x , the nodes u and y connected by the edge e  X  E U .Forex-ample, we may design a feature function h k ( e, y | e ,u | e ,x )=1if y i = I , u i refers to the popular product attribute  X  X isplay X  and x i is the word  X  X igh-definition X , and h k ( e, y | e ,u | e ,x )=0otherwise.  X  k ,  X  k ,  X  k are the parameters associated a function of x , is the normalization factor. As a result, the goal of our popular attribute text fragment extra ction is to find the labeling of y and u given the sequence x and the model parameter  X  which includes all the  X  k ,  X  k ,and  X  k , such that P  X  ( Y = y, U = u | X = x ) is maximized. 3.2 Inference For simplicity, we use P  X  ( y,u | x ) to replace P  X  ( Y = y, U = u | X = x )when the context is clear. Moreover, we will follow the notation in [7] to describe our method. We add the special label  X  X tart X  and  X  X nd X  for y 0 and y N ( W )+1 for easy illustration of our method. Recall that our goal is to compute arg max y,u P  X  ( y,u | x ) which can be expressed as Equation 1. For each token tok i in a sequence, we define the following | Y | X | U | matrix: We then can define the following | Y | X | Y | matrices: Given the above matrices, P  X  ( y,u | x ) for a particular y and u given x can then be computed as follows: the sequence of tokens and its observation, we can compute the optimal labeling of y and u using dynamic programming. 3.3 Unsupervised Learning We have developed an unsupervised method for learning our hidden CRF model. Given a set of M unlabeled data D , in which the observation X of each sequence is known, but the labels y and u for each token are unknown. In principle, discriminative learning is impossible for unlabeled data. To address this problem, we make use of the customer reviews to discover a set of hidden concepts and predict a derived label for u of each token. Note that the derived labels are just used in the learning and they are not used in the final prediction for the unlabeled data. As a result, we can exploit the derived label u and the observation X in learning the model. The approach of discovering hidden concepts will be described in the next section.

Since the class label y of each token is unknown, we aim at maximizing the following log-likelihood function in our learning method: Because of maximizing this log-likelihood function is intractable, we derive its lower bound according to Jensen X  X  inequality and the concavity of the logarithm function. Then the efficient limited memory BFGS optimization algorithm is employed to compute the optimal parameter sets. 3.4 Hidden Concept Discovery We observe that most customer reviews are organized in paragraphs as exempli-fied by the reviews in Fig. 1(b). To facilitate the discovery of high quality hidden concepts, we treat each paragraph as a processing document unit. For each re-view R , we first detect sentence boundaries in R using a sentence segmentator, denote a sequence of tokens in the sentence. Then linguistic parsing is invoked for each S i to construct a parse tree, in which the constituents of S i are orga-nized in a hierarchical structure. We extract all the noun phrases located in the leaf nodes of the parse tree. Let the sequence of noun phrases of S i be repre-sented by ( N i 1 ,N i 2 ,... ). For each N ij , we construct the context that is useful for latent topic discovery by considering the surrounding terms within a window size in S i . Then we define  X  ij as the derived content of N ij , and it is composed of all terms in N ij and the context terms. For example, consider a sentence S i as ( It X  X  , almost , soundless , on , the , low , setting , which , is , what , we , used , in , his , old , , , tiny , bedroom ), extracted from the review of an air purifier. The first noun phrase N i 1 in this sentence is ( the , low , setting ). If the context window size is 2, the derived content  X  i 1 for this noun phrase is ( soundless , on , the , low , setting , which , is ). We remove all stop words from  X  ij and conduct lemmatization for the remaining terms. The derived content representation  X  i of S i is obtained by  X  i = j  X  ij .

The derived document  X  for R can be obtained by gathering the derived content of all sentences. Therefore  X  =(  X  1 , X  2 ,... ). A collection of derived doc-uments for the review collection R can be obtained as above. We employ Latent Dirichlet Allocation (LDA) to discover the hidden concepts, which essentially refer to the popular features, for a domain. We have conducted the experiment to evaluate the performance of our framework with product description pages from ove r 20 different online retailer Web sites covering 7 different domains as depicted in Table 1. In addition, we have collected more than 500 customer reviews in each domain similar to the ones shown in Fig. 1(b) from retailer Web sites. These reviews were fed into the hidden concept discovery algorithm and the number of latent topics was set to 30 for each domain.

Two annotators were hired to identify the popular product attribute text fragments from the product description pages for evaluation purpose. The anno-tators first read through the reviews. Then they discussed and identified popular features as well as some sample popular product attributes for the domain. After that, such information is used to guide the annotation work of the correspond-ing domain. Text fragments corresponding to popular attributes were manually identified from product description pages. The manually extracted popular at-tribute text fragments were treated as the gold standard for evaluation. The agreement of popular attribute text fr agments between the two annotators was about 91%. The others were eliminated from the gold standard.

Since there are no existing methods that directly extract popular product attributes from Web pages and take into a ccount customers X  interest revealed in the collection of reviews of the same domain in an unsupervised manner, we implemented a comparative method based on integration of some existing meth-ods. The first comparative method is called  X  X IPS-Bayes X , which consists of two steps. For the first step, we first conduct unsupervised Web data extraction based on VIPS [3]. Since we have observed that almost all of the popular prod-uct attribute values (text fragments) are noun phrases, we apply the openNLP 1 package to conduct noun phrase extraction from the text in the product descrip-tion blocks. The identified noun phrases become the popular attribute value candidates. In the second step, we determine the popular attribute values as follows. We discover the derived hidden concepts for a domain using LDA from the customer reviews. Note that each hidden concept is represented by a set of terms with probabilities. The probability refers how likely that a term is gen-erated from a particular derived hidden concept. Next, each popular attribute value candidate is scored using Bayes theo rem. In essence, the score refers to the conditional probability that the candidate comes from a particular derived hid-den concept. Those candidates with scores greater than a certain threshold will be considered as popular attribute values. The threshold value is determined by a tuning process so that for each domain the best performance can be obtained.
Table 2 depicts the extraction performance of each domain and the average extraction performance among all domains. It can be observed that our approach achieves the best performance. The average F1-measure of our approach is 0.672, while the average F1-measure value of  X  X IPS-Bayes X  is 0.547. In addition, the paired t -test (with P&lt; 0 . 001) shows that the performance of our approach is significantly better. It illustrates that our approach can leverage the clues to make coherent decision in both product attribute extraction task and popular attribute classification task, leading to a better performance. In our approach, hidden concepts, represented by a distri bution of terms, can effectively capture the terms related to a popular attribute. As the hidden concept information, together with the content information and the layout information of each token are utilized, our hidden CRF model can effectively extract the popular attribute text fragments from description pages. Some information extraction approaches for Web pages rely on wrappers which can be automatically constructed via wrapper induction. For example, Zhu et al. developed a model known as Dynamic Hierarchical Markov Random Fields which is derived from Hierarchical CR Fs (HCRF) [17]. Zheng et al. proposed a method for extracting records and identifying the internal semantics at the same time [16]. Yang et al. developed a model combing HCRF and Semi-CRF that can leverage the Web p age structure and handle free texts for information extraction [14]. Luo et al. studied the mutual dependencies between Web page classification and data extraction, and proposed a CRF-based method to tackle the problem [9]. Some common disadvantages of the above supervised methods are that human effort is needed to prepare training examples and the attributes to be extracted are pre-defined.

Some existing methods have been developed for information extraction of product attributes based on text mining. Ghani et al. proposed to employ a clas-sification method for extracting attributes from product description texts [5]. Probst et al. proposed a semi-supervised algorithm to extract attribute value pairs from text description [11]. Their approach aims at handling free text de-scriptions by making use of natural language processing techniques. Hence, it cannot be applied to Web documents which are composed of a mix of HTML tags and free texts. The goal of extracting popular product attributes from prod-uct description Web pages is different f rom opinion mining or sentiment detec-tion research as exemplified in [4,6,8,10,12,13,15]. These methods typically dis-cover and extract all product attributes as well as opinions directly appeared in customer reviews. In contrast, our goal is to discover popular product attributes from description Web pages. We have developed an unsupervised learning framework for extracting precise popular product attribute text fragments from description pages originated from different Web sites. The set of popular product attributes is unknown in advance, yet they can be extracted considering the interest of customers through an au-tomatic identification of hidden concepts derived from a collection of customer reviews.

