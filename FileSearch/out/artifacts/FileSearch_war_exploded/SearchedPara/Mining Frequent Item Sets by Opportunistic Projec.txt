
In this paper, we present a novel algorithm OpportuneProject for mining complete set of frequent item sets by projecting databases to grow a frequent item set tree. Our algorithm is fundamentally different from those proposed in the past in that it opportunistically chooses between two different structures, array-based or tree-based, to represent projected transaction subsets, and heuristically decides to build unfiltered pseudo projection or to make a filtered copy according to features of the subsets. More importantly, we propose novel methods to build tree-based pseudo projections and array-based unfiltered projections for projected transaction subsets, which makes our algorithm both 
CPU time efficient and memory saving. Basically, the algorithm grows the frequent item set tree by depth first search, whereas necessary. We test our algorithm versus several other algorithms on real world datasets, such as BMS-POS, and on IBM artificial datasets. The empirical results show that our algorithm is not only the most efficient on both sparse and dense databases at all levels of support threshold, but also highly scalable to very large databases. H.2.8 [Database Management]: Database Applications -Data Mining. Algorithms 
Association Rules, Frequent Patterns 
Mining frequent item sets is a key step in many data mining problems, such as association rule mining, sequential pattern mining, classification, and so on. Since the pioneering work in [3], the problem of efficiently generating frequent item sets has been an active research topic. 
D be a set of transactions, where each transaction T is a set of * Also an associate professor at Hangzhou University of Commerce. # This work is supported in part by the NSF of Zhejiang, China, and the NSERC of Canada. personal or classroom use is granted without fee provided that copies are not made or distributed for profit or comroerrcia[ advantage and that requires prior specific permission and/or a fee. SIGKDD "02, July 23-26, 2002, Edmonton, Alberta, Canada. Copyright 2002 ACM 1-58113-567-X/02/0007...$5.00. very large databases, i.e., to grow the upper portion of the frequent item set tree by breadth first search and grow the lower portion by guided depth first search. [1] presents a method, TreeProjection, which represents frequent patterns as nodes of a lexicographic tree and uses the hierarchical structure of the lexieographie tree to successively project transactions and uses matrix counting on the reduced set of transactions for finding frequent patterns. The algorithm looks only at the subset of transactions, which can possibly contain the pattern by traversing the lexieographie tree in a top down fashion. This significantly improves the performance of counting the number of transactions containing a frequent pattern. TreeProjection is primarily based on pure breadth first strategy. It encountered the same problems as Apriori, such as high cost for pattern matching incurred by projecting on the fly, huge frequent item set tree, and too many database scans. [9] presents a well known depth-first algorithm, FP-growth, which is reported to be faster than TreeProjection and Apriori. It first builds up a compressed data structure, FP-tree, to hold the entire database in memory and then recursively builds conditional FP-trees to mine frequent patterns. It has performance gains since it avoids the combinatory problem inherent to candidate generate-and-test approach. However, the number of conditional FP-trees is in the same order of magnitude as number of frequent item sets. The algorithm is not sealable to sparse and very large databases. [12] proposes a memory-based hyper structure, H-struct, to store the sparse databases in main memory, and develops an H-struet based pattern-growth algorithm, H-Mine. H-Mine invokes FP-Growth to mine dense databases, hence, suffers the inefficiency caused by recursive creations of conditional FP-tree. H-Mine uses partition-based method to deal with very large databases. Because the number of local frequent patterns in all partitioned databases can be huge, H-Mine still encounters great difficulties for very large databases. DepthProjet [2] and MAFIA [6] are two new algorithms that find maximal frequent item sets by depth first search. DepthProjeet employs a selective projection and uses the horizontal bitstring representation for projected transaction subsets, whereas MAFIA uses the vertical bitmap representation with a bitmap compression schema. Both improve the efficiency of counting over the nffive counting method by a factor of 8. However, they are less efficient than the array-based representation when the average number of items in transactions is sufficiently less than the total number of items, which is usually the case for sparse and large databases. On the other hand, the compression ratio of the tree-based representation is significant for dense databases in that a node represents a relatively large number of items. Therefore, item counting in the tree-based representation is more efficient than, at lease comparable to, in the bitstfing and bitmap representations for dense databases. Moreover, the pseudo projection method is more efficient than the selective projection in DepthProjact and the compression schema in MAFIA. The organization of the paper is as follows. Section 2 defines the frequent item set tree and discusses projection strategies. Section 3 begins with introducing an array-based representation for sparse projected transaction sets and the corresponding projection method. Then, novel methods for pseudo projection of tree-based strategy usually do not maintain PTSs in the memory nor on the disk, they create PTSs on the fly. In other words, they read a transaction from the database into the memory, recursively projects the transaction from the null root down to the given level. This is a CPU-bound pattern matching task. Moreover, breadth first algorithms have to maintain the entire FIST in the memory. For dense database or for low support threshold, the huge size of FIST will exceed the capacity of the memory. The CPU-bound pattern matching and memory-bound FIST size are inherent to breadth first search strategy, even the original database can be loaded into the main memory. The advantage of breadth first search is that it is scalable to very large size of original database. In depth first search, PTSs are maintained for all nodes on the path starting from the root to the node that is currently being explored. Depth first search has the advantage that it is not necessary to re-create PTSs. This avoids the CPU-bound pattern matching inherent to breadth first search. Moreover, only the branch that is currently being explored needs to be maintained in the memory. This overcomes the limitation on the size of FIST. Depth first search is especially efficient for dense database and for low support threshold. Depth first search is usually memory based, that is PTSs are maintained in the memory. Hence, depth first search is not sealable to very large databases. Generally speaking, depth first search is more efficient, and breadth first search is more scalable. To achieve maximized efficiency and scalability, the algorithm must adapt the construction strategy of FIST, the representation of PTS, and the methods of item counting in and projection creating of PTSs to the features of PTSs. In this section, an array-based PTS representation and projecting method is discussed firstly, to find complete set of frequent items by depth first search in sparse and large databases. Secondly, novel methods for projecting tree-based PTS representation are detailed, which is highly efficient for dense databases. Thirdly, observations and heuristics are Each local item in PTS has an entry in the IL, with three fields: an item-id, a support count, and a pointer. Entries in IL are ordered by the imposed ordering. Each transaction in the PTS is represent number of transactions represented by the path starting from a root ending at the node. Items labeling nodes along any path are sorted by the same ordering as IL. All nodes labeled by the same item are threaded by the entry in IL with the same item. TTF is filtered if only local frequent items appear in TTF, otherwise unfiltered. children of the null root projected by top down exploration of the parent TTF. The key points of top down exploration of pseudo TTF are as follows. First, any pseudo TTF consists of a sub forest of its parent TTF and leaves of the sub forest are label by the same item and threaded by the entry oflL with the same item. Second, by traversing the sub forest, we can delimitate the PTS by re-threading nodes in the sub forest, count the support of each dense. Some are sparse. Real databases are of all sizes. We are going to propose a hybrid approach that maximizes efficiency and scalability for mining real databases, based on following observations and heuristics. Figure 7. Top down pseudo projecting 1-1"F 
Heuristic 3: When projecting a parent TVLA, make a filtered copy for the child TVLA as long as there is free memory. When projecting a parent TTF, delimitate the pseudo child TTF first and then make a filtered copy if it shrinks substantially sharp. 
For a given PTS, let the number of frequent items be f, the number of transactions be t, and total number of occurrences of frequent items be o. 
The exact size of TVLA is 3*f+ 2*t + (o-t), where 3*fis the size of FIL, 2*t is the size of LQs, and (o-t) is the size of arrays. The size of TTF is 3*f+ 6*n, where n is number of nodes of'VFF. 
However, the exact number of nodes of TTF is unknown before its creation. The following formula gives the worst estimate of nodes of TI'F, where u and I are the maximal and minimal length of filtered transactions. 
In our algorithm we estimate u and 1 based on the average transaction length. Numerous experiments show this estimation is always larger than the actual size. In other words, this is a pessimistic measure. The compression ratio of TTF is r = o/n. Ifr is less than 6-(t/n), the size of TTF is greater than TVLA, which is the case for sparse databases. Now we present the algorithm OpportuneProject, abbreviated as 
OP, which integrates depth first and breadth first strategy, array-based and tree-based representation, pseudo unfiltered projection and filtered projection, as listed in Figure 10. We create the upper portion of FIST in three steps. First, 
CreateCountingVector(v). We attach counting vectors to all nodes at the current level k to accumulate local supports for items in the 
PTS of each node. The counting vector has an element for the item of each sibling node that is after the node attached according to the imposed ordering. For example, possible items local to the the items of siblings that follow the node (a,3). Therefore, a length 5 counting vector is aaached to accumulate the supports for item b, c, f, m, and p. 
Second, ProjectAndCount(t,D'). We project the transaction t along the path from the root to nodes at the current level k and accumulate counting vectors, lfa transaction can be projected to a level k node and contribute to its counting vector, it may also be projected to level k+l, therefore record it in D'. Otherwise it can be removed from further consideration. This results in the reduction of the number of transactions level by level. 
Third, GenerateChildren(v). We create children for each node at the current level k for its local frequent items whose element in the counting vector has a value over the support threshold. If the node v has no child, it is removed at that time; and its parent will be deleted also ifv is the only child of its parent, and so on. 
The BreadthFirst is a recursive procedure. We use the available free memory as parameter to control breadth first search process. We now describe the datasets used in our experiments. The basic features of the datasets are listed in Table 1. BMS-POS, BMS-WebView-1 and BMS-WebView-2 are real world datasets and categorized as sparse datasets [15]. BMS-POS dataset contains several years worth of point-of-sale data from a large electronics retailer. The transaction in this dataset is a customer's purchase transaction consisting of all the product categories purchased at one time. BMS-POS has 122,449 frequent patterns at the support threshold of 0.1%, and 984,531 at 0.04%. BMS-WebView-1 and BMS-WebView-2 datasets contain several months worth of click stream data from two e-commerce web sites. BMS-WebView-1 has 3,991 frequent patterns at the support threshold of 0.1%, and 1,177,607 at 0.058%. The BMS-WebView-2 has 23,294 frequent patterns at support threshold of 0.1%, and 1,316,614 at 0.02%. Connect4 is from UCI Machine Learning Repository [18]. Each transaction in Connect4 contains legal 8-ply positions in the game of connect-4 where neither player has won yet and the next move is not forced. It is a very dense dataset in that the number of frequent patterns grows from 27,127 to 4,129,839 and 88,316,367 when support threshold reduces from 90% to 70% and 50%. IBM Artificial datasets, T25120N20kL5k with D100k~D15m are generated using a transaction data generator [4] obtained from IBM Almaden [17]. T25120N20.L5k can be regarded as something between the sparse and the dense. For example for D100k, the number of frequent patterns is 966 at support threshold of 0.5%, 12,625 at 0.25%, 601,936 at 0.195%, and 114,220,668 at 0.15%. Table 1. Basic features ofdatasets. BMS-WebView-1 BMS-WebView-2 Connect4 In this subsection, we describe the performance of our algorithm versus Apriori, FP-Growth, and H-Mine on the datasets described in the previous section. The performance measure was the execution time of the algorithms on the datasets with different support threshold. The execution time only includes the disk reading time (scan datasets) and CPU time, but excludes disk writing time (output patterns) in order to reduce the influence of relatively slow speed of disk writing. Figure 11 through 15 show the performance curves for the four algorithms on the five datasets respectively. The vertical axis is on a logarithmic scale. As we can see, the OpportuneProject algorithm outperforms the other three algorithms on all datasets. The performance improvements of OpportuneProject over other algorithms were significant at reasonably low support thresholds. For the BMS-POS, at the support thresholds over 0.4%, where the number of frequent patterns is under 6,656, the four algorithms performance because the maximum pattern length is 1. 
OpportuneProject outstrips the other three algorithms for support threshold over 0.3%. At the support threshold of 0.195, a reasonably low one where there is 601,936 frequent patterns, 
OpportuneProject requires 4 seconds, while H-Mine requires 30 seconds, FP-Growth requires 83 seconds, and Apriori requires 450 seconds. When the support threshold decreases to an even lower level, improvements of OpportuneProject are more striking. 1000 [  X  Apriori i-.~. 1 .... T 0.05 0.06 0.07 0.08 (%~.09 0.1 Support threshold 1000 i-0.1 0 0.2 Sup#r~4threshO6pd (%) 0.8 1 Figure 12. Computational performances on BMS-WebView-I ~000 ~",,=~ ~--H-Ni ne  X  OP 10000 1000 10 ~" --; -= ~. -i ~. ; 0.1 ' ' ' ' 0 0.2 0.4 0.6 0.8 1 Support threshold (%) Figure 13. Computational performances on BMS-WebView-2 
The performance results we discussed so far are all under the case that there is enough memory for all algorithms. 
The factor of OpportuneProject over Apnori increases from 100 to 3000, whereas OpportuneProject over FP-Growth decreases from 16 to 8 in the same case. .~110 ~ Figure 16 Execution time on T25120DlmN20kL5k Figure 18 shows the performance of algorithms on 
T25120N20kL5k, while the database size increase from 100k to 15m, at the support threshold of 0.2% where the maximum pattern length is 13, and the number of frequent patterns is around 17K, except 45K for D200k and 99K for D100k. Apriori fails when the database size rreaehes D2ra, and FP-Growth and H-Mine fails at D4m, because they run out of memory. It is very impressive that OpportuneProject scales almost linearly with the database size. 
For example, OppoauneProject finishes in 551 seconds on D5~ 1295 seconds on D10m, and 1961 seconds on D15m, while the memory consumed is less than 178MB. 
Figure 17 Execution time on T25120D10mN20kLSk 
Figure 18 Execution time on T25120N20kLSk with DlO0k through D 15m at the support threshold of 0.2% 
In this paper, we propose an efficient algorithm to find complete set of frequent item sets for databases of all features, sparse or dense, and of all sizes, from moderate to very large. This algorithm combines depth first approach with breadth first approach, opportunistically chooses between array-based 
