
This paper studies evolutionary clustering, which is a re-cently hot topic with many important applications, notice-ably in social network analysis. In this paper, based on the recent literature on Hierarchical Dirichlet Process (HDP) and Hidden Markov Model (HMM), we have developed a statistical model HDP-HTM that combines HDP with a Hi-erarchical Transition Matrix (HTM) based on the proposed Infinite Hierarchical Hidden Markov State model (iH 2 MS) as an effective solution to this problem. The HDP-HTM model substantially advances the literature on evolution-ary clustering in the sense that not only it performs bet-ter than the existing literature, but more importantly it is capable of automatically learning the cluster numbers and structures and at the same time explicitly addresses the cor-respondence issue during the evolution. Extensive evalu-ations have demonstrated the effectiveness and promise of this solution against the state-of-the-art literature.
Evolutionary clustering is a recently identified new and hot research topic in data mining. Evolutionary clustering addresses the evolutionary trend development regarding a collection of data items that evolves over the time. From time to time, with the evolution of the data collection, new data items may join the collection and existing data items may leave the collection; similarly, from time to time, clus-ter structure and cluster number may change during the evo-lution. Due to the nature of the evolution, model selection must be solved as part of a solution to the evolutionary clus-tering problem at each time. Consequently, evolutionary clustering poses a greater challenge than the classic cluster-ing problem as many existing solutions to the latter problem typically assume that the model selection, is still an open problem in the clustering literature.

In evolutionary clustering, one of the most difficult and challenging issues is to solve the correspondence problem. The correspondence problem refers to the correspondence between different local clusters across the times due to the evolution of the distribution of the clusters, resulting in cluster-cluster correspondence and cluster transition corre-spondence issues. All the existing methods in the literature fail to address the correspondence problems explicitly.
On the other hand, solutions to the evolutionary cluster-ing problem have found a wide spectrum of applications for trend development analysis, social network evolution analy-sis, and dynamic community development analysis. Poten-tial and existing applications include daily news analysis to observe news focus change, blog analysis to observe com-munity development, and scientific publications analysis to identify the new and hot research directions in a specific area. Consequently, evolutionary clustering has recently be-come a very hot and focused research topic.
 In this paper, we have shown the new statistical model HDP-HTM that we have developed as an effective solution to the evolutionary clustering problem. In this new model, we assume that the cluster structure at each time is a mix-ture model of the clusters for the data collection at that time; in addition, clusters at different times may share common clusters, resulting in explicitly addressing the cluster-cluster correspondence issue. we adopt the Hierarchical Dirich-let Processes (HDP) [26] with a set of common clusters at the top level of the hierarchy and the local clusters at the lower level at each different times sharing the top level clus-ters. Further, data and clusters evolve over the time with new clusters and new data items possibly joining the collec-tion and with existing clusters and data items possibly leav-ing the collection at different times, leading to the cluster structure and the number of clusters evolving over the time. Here, we use the state transition matrix to explicitly reflect the cluster-to-cluster transitions between different times, re-sulting in explicitly effective solution to the cluster transi-tion correspondence issue. Consequently, we propose the Infinite Hierarchical Hidden Markov State model (iH 2 MS) to construct the Hierarchical Transition Matrix (HTM) at different times to capture the cluster-to-cluster transition evolution.

The specific contributions of this work are highlighted as follows: (1) We have applied the recent literature on Dirich-let process and Hidden Markov Model (HMM) to solve the evolutionary clustering problem with a specific new model HDP-HTM. (2) This new model as an effective solution to evolutionary clustering substantially advances the literature in the sense that it is capable of automatically learning the number of clusters and the cluster structure at each time and at the same time addressing the cluster correspondence problem explicitly during the evolution, which makes this solution practical in many evolutionary clustering applica-tions. (3) We have demonstrated the superiority of this so-lution to the existing state-of-the-art literature in both syn-thetic data and real Web daily news data in the case of the evolutionary document clustering application.
Evolutionary Clustering , contrast to static clustering, is an emerging research topic, which processes temporal data to generate a sequence of clusterings across the time. In data mining community, Chakrabarti et al. [9] were the first to address this problem. They proposed a framework to capture the history information quantitatively measured by the history quality , and the current information measured by the snapshot quality . Within this framework, they de-signed two specific algorithms: evolutionary k-mean and evolutionary agglomerative hierarchical clustering. Later, Chi et al. [10] proposed two evolutionary spectral cluster-ing algorithms PCM and PCQ, by incorporating the tempo-ral smoothness to regularize a cost function. These methods significantly advance the evolutionary clustering literature; however, they are not able to handle the very typical sce-nario where the number of clusters at different times varies with the evolution. Recently, Tang et al. [25] proposed an evolutionary clustering solution, based on spectral clus-tering framework, to handle dynamic multi-mode networks problems, where both actor memberships and interactions evolve over the time. However, the solution still assumes a fixed number of communities(clusters) over the time.
Dirichlet Process (DP) is a distribution over the proba-bility measure on the parameter space  X  first discussed by Ferguson [13]. An explicit stick-breaking construction def-inition of DP is given by Sethuraman [23] as follows: where G is a random measure drawn from DP (  X , H ) with the concentration parameter  X  ,  X   X  k is the concentration measure on  X  k sampling from the base measure H ,  X  k is the  X  X tick-breaking weights X  discussed in detail later. Black-well and MacQueen [5] have introduced another representa-tion of DP, as the predictive distribution of the events is pro-portional to the frequency of an existing event or to a con-centration parameter for an unpresented event. Similarly, this representation may be further illustrated as the Chinese Restaurant Process (CRP) [1] to explicitly demonstrate the  X  X lustering property X  as the  X  X istribution on partition X .
In the mixture models, when the number of the mixture components is unknown apriori , DP may be incorporated as a prior distribution on the parameters formulated as the Dirichlet Process Mixture Model (DPM) [2, 12], which is able to handle an infinite or arbitrary number of compo-nents and to automatically learn such a number from the observations. In order to share the mixture components across the groups of the mixture models, Teh et al. [26] have proposed the Hierarchical Dirichlet Process model, where a hierarchical Bayesian approach is used to place a global Dirichlet process prior on the parameter space as the dis-crete base measure for the lower level DPMs.

There are many noticeable applications of DP based models in text modeling and topic analysis. Blei et al. [7] proposed the well-known Latent Dirichlet Allocation (LDA), for the text modeling and clustering with a known, constant number of the topics set in advance. For the topic evolution analysis, Blei et al. [6] have designed the prob-abilistic models to develop reasonable solutions. Based on LDA, Griffiths et al. [16] tried to identify  X  X ot topics X  and  X  X old topics X  by the text temporal dynamics. The number of the topics was decided by the Bayesian Model selec-tion. Recently, Wang et al. [27] introduced a LDA-style topic model to represent the time as an observed continue variable attempting to capture the topic evolutionary trends. However, all these models failed to automatically learn the number of the topics (i.e., the clusters) according to the un-derlying data evolutionary distribution. Further, they failed to address the correspondence issues during the evolution.
Hidden Markov Model (HMM) [22] has been widely used to model sequential or temproal data. HMM has a fi-nite hidden state space, governed by a finite transition prob-ability matrix which obeys the Markov dynamics, and a se-quence of the observations generated by these hidden states along the time. When turning to an infinite or arbitrary hid-den state space, Beal et al. [4] have first introduced the In-finite Hidden Markov Model (iHMM) also known as HDP-HMM by using the Dirichlet Process as a row in the state transition matrix. Teh et al. [26] have demonstrated that HDP may recast iHMM and have provided the Gibbs sam-pling inference mechanism for it.
 Recently, Fox et al. [14] have revisited the HDP-HMM and have developed methods which allow more efficient and effective learning from realistic time series data. Ni et al. [21] have proposed a new hierarchical nonparamet-ric Bayesian model by imposing a nested Dirichlet Pro-cess prior on the base distributions of the iHMMs to learn the sequential data. The standard approach to learning HMM is an EM-based algorithm [11] specifically known as Baum-Welch algorithm [3]. MacKay [19] has introduced a Bayesian learning procedure called the variational approx-imation to handle the overfitting problem in Baum-Welch algorithm. More recently, Gael et al. [15] have introduced a new inference algorithm for iHMM called the beam sam-pling which is more efficient and robust. Xu et al. [28] proposed two models DPChain and HDP-EVO as solutions to evolutionary clustering. Both models are able to learn the number of clusters and dynamic structures during the evolution. Here, we propose a new infinite hierarchical hidden Markov state model (iH 2 MS) for Hierarchical Transition Matrix (HTM) and provide an update construction scheme based on this model. Figure 1 illustrates this model.
Tradtionally, Hidden Markov Model (HMM) has a finite state space with K hidden states, say { 1 , 2 ,...K } .Forthe hidden state sequence { s 1 ,s 2 ,...s T } up to time T , there is a K by K state transition probability matrix  X  governed by Markov dynamics with all the elements  X  i,j of each row  X  summed to 1. The initial state probability for state i is p ( s 1 = i ) summation of all the initial probabilities equal to 1. For ob-servation x t in the observation sequence { x 1 ,x 2 ,...x given state s t  X  X  1 , 2 ,...,K } , there is a parameter  X  drawn from the base measure H which parameterizes the observation likelihood probability.
However, when dealing with a countable infinite state space, { 1 , 2 ,...K,... } , we must adopt a new model simi-lar to that in [4] for a state transition probability matrix with an infinite matrix dimension. Thus, the dimension of the state transition probability matrix now has become infinite.  X  ,the i -th row of the transition probability matrix  X  ,may be represented as the mixing proportions for all the next in-finite states, given the current state. Thus, we model it as a DP with an infinite dimension with the summation of all the elements in a row normalized to 1, which leads to an in-finite number of DPs X  construction for an infinite transition probability matrix.

With no further prior knowledge on the state sequence, a typical prior for the transition probability may be the sym-metric Dirichlet distributions. Similar to [26], we intend to construct a hierarchical Dirichlet model to keep differ-ent rows of the transition probability matrix to share part of the prior mixing proportions of each state at the top level. Consequently, we adopt a new state model, Infinite Hier-archical Hidden Markov State model (iH 2 MS), to construct the Infinite Transition Probability Matrix which is called the Hierarchical Transition Matrix (HTM).

Similar to HDP [26], we draw a random probability mea-sure on the infinite state space  X  as the top level prior from stick (  X  ) represented as the mixing proportions of each state. Here, the mixing proportion of state k ,  X  k , may also be in-terpreted as the prior mean of the transition probabilities leading to state k . Hence,  X  may be represented as the prior random measure of a transition probability DP.

For the i  X  th row of the transition matrix  X  ,  X  i ,wesam-ple it from DP (  X ,  X  ) with a smaller concentration parame-ter  X  implying a larger variability around the mean measure  X  . The stick-breaking representation for  X  i is as follows:  X  Specifically,  X  i,k is the state transition probability from the previous state i to the current state k as p ( s t = k | s
Now, each row of the transition probability matrix is rep-resented as a DP which shares the same reasonable prior on the mixing proportions of the states. For a new row corre-sponding to a new state k , we simply draw a transition prob-ability vector  X  k from DP (  X ,  X  ) , resulting in constructing a countably infinite transition probability matrix.
The transition probability constructed by iH 2 MS may be further extended to the scenario where there are more than one state at each time. Suppose that there is a countably infinite global state space S = { 1 , 2 ,...,K,... } including states in all the state space S t at each time t , where S For any state s t  X  S t at time t and state s t  X  1  X  S t  X  1 t  X  1 , we may adopt the transition probability  X  i,k to repre-sent p ( s t = k | s t  X  1 = i ) . Similarly,  X  i,k here still has the property that there is a natural tendency for a transition to appear more frequently at the current time if such a transi-tion appears more frequently at a previous time. Therefore, it is reasonable to model a row of transition as a DP with an infinite dimension. We will discuss this extension in detail later.
Let X be the observation sequence, which includes all the observations X t at each time t , where X t  X  X .Now, the question is how to represent the countably infinite state space in a hierarchical state transition matrix (HTM). Notice that, at each time, there is in fact a finite number of observa-tions X t ; the state space S t at each time t must be arbitrar-ily finite even though conceptually the global state space may be considered as countably infinite. Further, we adopt the stick-breaking representation for the DP prior [26, 18] to iteratively handle an arbitrary number of the states and accordingly the transition probability matrix up to time t .
Suppose that up to time t there are K current states and we use K +1 to index a potentially new state. Then  X  may be represented as:  X  = {  X  1 ,... X  K , X  u }  X  u =
Given  X  , the Dirichlet prior measure of i -th row of the transition probability matrix  X  i has a dimension K +1 .The last element  X  u is the prior measure of the transition prob-ability from state i to an unpresented state u . The prior distribution of  X  i is Dir (  X  X  1 ,... X  X  K , X  X  u ) . When a new state is instantiated, we sample b from Beta (1 , X  ) , and set the new proportions for the new state K new = K +1 and another potentially new state K new +1 as: Now, K is updated as K new ,  X  u as  X  new u , and the number of the states may continue to increase if yet another new state is instantiated, resulting in a countably infinite transi-tion probability matrix.

Since up to time t , there are K states in the hidden state space, it is possible to adopt Baum-Welch algorithm [22, 19] to estimate the posterior of the transition proba-bility matrix  X  by the maximum likelihood Optimization. Similar to [19], we have a Dirichlet prior on each row of  X  ,  X  i . Consequently, we have the M-step for updating  X  according to the standard Baum-Welch optimization equa-tion:  X  where approximately n t i,k is the expected number of the transitions from state i to state k up to time t , n t i is the ex-pected number of the transitions out of state i up to time t . For a state of any common observation x between two adjacent times  X  and  X  +1 up to time t , where s  X ,x and s  X  +1 ,x capture the correpondence between the states with the same data item at adjacent times. Here we use Kronecker-delta function (  X  ( a, b )=1 iff a = b and 0 otherwise) to count the number of the state transitions for all the common observations up to time t .

Conceptually, in Eq. 5 we may consider  X  X  k as the pseudo-observation of the transition from state i to k (i.e., the strength of the belief for the prior state transition), and  X  X  u as the probability of a new state transitioned from state i . Eq. 5 is equivalent to Blackwell-MacQueen urn Scheme [5] to sample a state. Thus, this posterior maximum like-lihood estimation of the transition probability to a state is equal to probability of such a state posterior sampled by the polya urn scheme [5] given a sequence of the states and ob-servations up to time t .
To capture the state(cluster) transition correspondence during the evolution at different times, we propose the HTM; at the same time, we must capture the state-state (cluster-cluster) correspondence, which may be handled by a hierarchical model with the top level corresponding to the global states 1 and the lower level corresponding to the local states, where it is natural to model the statistical process as HDP [26]. Consequently, we propose to combine HDP with HTM as a new HDP-HTM model, as illustrated in Figure 2.
Let the global state space S denote the global cluster set, which includes all the states S t  X  S at all the times t .The global observation set X includes all the observations X t at each time t , of which each data item i is denoted as x
We draw the global mixing proportion from the global states  X  with the stick-breaking representation using the concentration parameter  X  from Eq. 1. The global measure G 0 may be represented as: where  X  k is drawn from the base probability measure H with pdf h , and  X   X  k is the concentration measure on  X  k
Different from HDP, here we must consider the evolu-tion of the data and the states (i.e., the clusters). The distri-bution of the clusters at time t is not only governed by the global measure G 0 , but also is controlled by the data and cluster evolution in the history. Consequently, we make an assumption that the data and the clusters at time t are gen-erated from the previous data and clusters, according to the mixture proportions of each cluster and the transition prob-ability matrix. The global prior mixture proportions for the clusters is  X  , and the state transition matrix  X  provides the information of the previous state evolution in the history up to time t . Now, the expected number of the data items gen-erated by cluster k is proportional to the number of data items in the clusters in the history multiplied by the transi-tion probabilities from these clusters to state k ; specificially, the mean mixture proportion for cluster k at time t ,  X  t defined as follows: More precisely,  X  t is further obtained by:
Clearly, by the transition probability property,
Thus, the mean mixture proportion  X  t may be taken as the new probability measure at time t on the global clus-ter set. With the concentration parameter  X  ,wedrawthe mixture proportion vector  X  t from DP (  X ,  X  t ) .
Now, at time t , the local measure G t shares the global clusters parameterized by  X  =(  X  k )  X  k =1 with the mixing proportion vector  X  t .
 At time t , given the mixture proportion of the clusters  X  we draw a cluster indicator z t,i for data item x t,i from a multinomial distribution: Once we have the cluster indicator z t,i , data item x t,i be drawn from distribution F with pdf f , parameterized by  X  from the base measure H . Finally, we summarize the data generation process for HDP-HTM as follows. 1. Sample the cluster parameter vector  X  from the base 2. Sample the global cluster mixture vector  X  from 3. At time t , compute the mean measure  X  t for the global 4. At time t , sample the local mixture proportion  X  t by 5. At time t , sample the cluster indicator z t,i from 6. At time t , sample data item x t,i from f ( x |  X  z t,i
We denote n t i,j as the number of the state transitions from state i to j between two adjacent times up to time t . Let n t,k be the number of the data items belonging to cluster k at time t , n  X  i t,k be the number of the data items belonging to cluster k except x t,i at time t , and n t be the number of all the data items at time t . Similar to HDP [26], let m be the number of the tables (i.e., the local clusters) belong-ing to the global cluster k at time t , and m k be the number of the tables (i.e., the local clusters) belonging to the global cluster k across all the times. Finally, let x t be the data collection at time t .

In order to handle an infinite or arbitrary number of the states (i.e., clusters), we adopt the stick-breaking mecha-nism similar to what we have done in Sec. 3.3. Assume that there are K existing clusters. The global mixture propor-tion  X  = {  X  1 ,..., X  K , X  u } with  X  u being the proportion for an unrepresented cluster; when a new cluster is instanti-ated, the vector  X  is updated according to the stick-breaking construction in Eq. 4 to ensure the normalized summation equal to 1 with the probability 1. In addition, the transition probability matrix  X  is in the dimension of K +1 by K +1 , resulting in  X  t also in dimension of 1 by K +1 with the last element  X  t,u as the proportion of the unrepresented cluster.
Here, we adopt the EM [11] framework to learn the model by combining Markov Chain Monte Carlo (MCMC) method [20] to make an inference for the auxiliary variable m and other necessary variables at the E step and maximum likelihood estimation of  X  at the M step. Similar to HDP with the direct assignment posterior sampling, we also need to sample the global mixture proportion  X  from m .We update the transition probability matrix  X  by the counter statistic n t i,j up to time t according to Eq. 5. We no longer need to sample  X  t because we may just sample the cluster assignment z t at time t by integrating out  X  . Similarly, by the conjugacy of h and f , it is not necessary to sample the parameter  X  k for cluster k .
 Sampling z t
Since  X  t is distributed as DP (  X ,  X  t ) , while the cluster indicator z t,i is in a multinomial distribution with the pa-rameter  X  t , it is convenient to integrate out  X  t by the con-jugacy property. Thus, the conditional probability of the current cluster assignment z t,i for the current data item x given the other assignments z t,  X  i = z t \ z t,i and the Dirich-let parameters  X  t and  X  is: p
By Gibbs Sampling [8], we need to compute the con-ditional probability z t,i given the other cluster assignment z t,  X  i , the observation x t at time t , and the Dirichlet param-eters  X  t and  X  . where f  X  i k ( x t,i ) is the conditional likelihood of x the other data items x t,  X  i under cluster k , which by the conjugacy property of h and f could be computed by inte-grating out the cluster parameter  X  k for cluster k . f Sampling m
Again similar to HDP, in order to sample m ,wemust first sample m t , the number of the tables (i.e., the local clusters) for the clusters at time t [26]. After sampling of z , n t,k is updated accordingly. By [2, 26], we may sample m according to the following Gibbs Sampling [8]: p ( Sampling  X 
Given m , the posterior distribution of  X  is: where K is the number of the existing clusters up to time t . Consequently, it is trivial to sample  X  according to Eq. 11. Updating the Transition Matrix  X 
After we have the knowledge of the sequence of the states and observations at different times, especially the new knowledge at time t , we may adopt the maximum likelihood estimation to construct the posterior transition probability matrix  X  . After sampling z t , the state (i.e., the cluster) as-signment at time t is changed, leading to updating n t i,j cordingly. Consequently, the matrix  X  is updated according to Eq. 5.
 Hyperparameter Sampling
In the HDP-HTM model, there are the concentration hy-perparameters  X = {  X ,  X ,  X  } . According to [26, 12], we may sample these parameters by the Gamma distribution with the constant Gamma parameters discussed in detail in Sec. 5.

Finally, we summarize the EM framework as follows: 1. Initialize the transition matrix  X  ,aswellas  X  , m , and 2. The E-Step at time t : 3. The M-Step at time t : 4. Iterate 2 to 3 until convergence.
We have evaluated the HDP-HTM model in an exten-sive scale against the state-of-the-art literature. We com-pare HDP-HTM in performance with evolutionary spectral clustering PCM and PCQ algorithms [10] and HDP [26] for the synthetic data and the real data in the application of document evolutionary clustering; for the experiments in text data evolutionary clustering, we have also evaluated the HDP-HTM model in comparison with LDA [7, 17] in addi-tion. In particular, the evaluations are performed in three datasets, a synthetic dataset, the 20 NewsGroups dataset, and a Google daily news dataset we have collected over a period of 5 continuous days.
We have generated a synthetic dataset in a scenario of evolutionary development. The data are a collection of mix-ture models with the number of the clusters unknown apri-ori with a smooth transition over the time during the evo-lution. Specifically, we simulate the scenario of the evo-lution over 10 different times with each time X  X  collection according to a DPM model with 200 2-dimensional Gaus-sian distribution points. 10 Gaussian points in N ( 0 , 2I are set as the 10 global clusters X  mean parameters. Then 200 Gaussian points within a cluster are sampled with this cluster X  X  mean parameter and deviation parameter sampling from N ( 0 , 0 . 2I ) , where I is identify matrix. After the gen-eration of such a dataset, we obtain the number of the clus-ters and the cluster assignments as the ground truth. We intentionally generate different numbers of the clusters at different times, as shown in Figure 5.

In the inference process, we tune the hyperparameters as follows. In each iteration, we use the vague Gamma priors [12] to update  X  ,  X  , and  X  from  X (1 , 1) . Figure 3 shows an example of the clustering results between HDP-HTM and PCQ at time 8 for the synthetic data. Clearly, HDP-HTM has much better performance than PCQ in this synthetic data.

For a more systematic evaluation on this synthetic dataset, we use NMI (Normalized Mutual Information) [24] to quantitatively compare the clustering performances among all the four algorithms (HDP-HTM, HDP, PCM, and PCQ). NMI measures how much information two random distribution variables (computed clustering assignment and groundtruth clustering assignment) share, the larger the bet-ter with 1 as normalized maximum value. Figure 4 docu-ments the performance comparison. From this figure, the average NMI values across the 10 times for HDP-HTM and HDP are 0.86 and 0.78, respectively, while those for PCQ and PCM are 0.70 and 0.71, respectively. HDP works worse than HDP-HTM for the synthetic data. The reason is that HDP model is unable to capture the cluster transition corre-spondence during the evolution among the data collections across the time in this case while HDP-HTM is able to ex-plicitly solve for this correspondence problem; on the other hand, HDP still performs better than PCQ and PCM as HDP is able to learn the cluster number automatically during the evolution.

Since one of the advantages of the HDP-HTM model is to be able to learn the number of the clusters and the clus-tering structures during the evolution, we report this per-formance for the HDP-HTM compared with HDP on this synthetic dataset in Figure 5. Here, we define the expected number of the clusters at each time as the average number of the clusters in all the posterior sampling iterations after the burn-in period. Thus, these numbers are not necessarily integers. Clearly, both models are able to learn the cluster numbers, with HDP-HTM having a better performance than HDP. Since both PCQ and PCM do not have this capability, they are not included in this evaluation.
In order to showcase the performance of HDP-HTM model on real data applications, we apply HDP-HTM to a subset of the 20 Newsgroups data 2 . We inten-tionally set the number of the clusters at each time as the same number to accommodate the comparing algo-rithms PCQ and PCM which have this assumption of the same cluster number over the evolution. Also we se-lect 10 clusters (i.e., topics) from the dataset (alt.atheism, comp.graphics, rec.autos, rec.sport.baseball, sci.crypt, sci.electronics, sci.med, sci.space, soc.religion.christian, talk.politics.mideast), with each having 100 documents. To  X  X imulate X  the corresponding 5 different times, we then split the dataset into 5 different collections, each of which has 20 documents randomly selected from each clusters. Thus, each collection at a time has 10 topics to generate words. We have preprocessed all the documents with the standard text processing for removing the stop words and stemming the remaining words.
 To apply the HDP-HTM and HDP models, a symmetric Dirichlet distribution is used with the parameter 0.5 for the prior base distribution H . In each iteration, we update  X  ,  X  , and  X  in HDP-HTM, from the gamma priors  X (0 . 1 , 0 . For LDA,  X  is set 0.1 and the prior distribution of the top-ics on the words is a symmetric Dirichlet distribution with concentration parameter 1. Since LDA only works for one data collection and requires a known cluster number in ad-vance, we explicitly apply LDA to the data collection with the ground truth cluster number as input at each time.
Figure 6 reports the overall performance comparison among all the five methods using NMI metric again. Clearly HDP-HTM outperforms PCQ, PCM, HDP, and LDA at all the times; in particular, the difference is substantial for PCQ and PCM. Figure 7 further reports the performance on learning the cluster numbers at different times for HDP-HTM compared with HDP. Both models have a reasonble performance in automatically learning the cluster number at each time in comparison with the ground truth, with HDP-HTM having a clearly better performance than HDP in av-erage.
Figure 4. The NMI performance compari-son of the four algorithms on the synthetic dataset In order to truly demonstrate the performance of HDP-HTM in comparison with the state-of-the-art literature on a real evolutionary clustering scenario, we have manually collected Google News articles for a continuous period of five days with both the data items (i.e., words in the arti-cles) and the clusters (i.e., the news topics) evolving over the time. The evolutionary ground truth for this dataset is as follows. For each of the continuous five days, we have the number of the words, the number of the clusters, the number of the documents as (6113, 5, 50), (6356, 6, 60), (7063, 5, 50), (7762, 6, 60), and (8035, 6, 60), respectively. In order to accommodate the assumption of PCM and PCQ that the cluster number stays the same during the evolution, but at the same time in order to demonstrate the capabil-ity of HDP-HTM to automatically learn the cluster num-ber at each evolutionary time, we intentionally set the news topic number (i.e., the cluster number) at each day X  X  collec-tion to have a small variation deviation during the evolution. Again, in order to compare the text clustering capability of LDA [7, 17] with a known topic number in advance, we use the ground truth cluster number at each time as the input to
Figure 5. The cluster number learning perfor-mance of the HDP-HTM in comparison with
HDP on the synthetic dataset
Figure 6. The NMI performance comparison among the five algorithms on the 20 News-groups dataset LDA. The parameter tuning process is similar to that in the experiment using the 20 Newsgroup dataset.

Figure 8 reports the NMI based performance evaluations among the five algorithms. Again, HDP-HTM outperforms PCQ, PCM, HDP, and LDA at all the times, especially sub-stantially better than PCQ, PCM, and LDA. PCQ and PCM fail completely in most of the cases as they assume that the number of the clusters remains the same during the evolu-tion, which is not true in this scenario.

Figure 9 further reports the performance on learning the cluster numbers for different times for HDP-HTM com-pared with HDP model. In this dataset, HDP-HTM has a much better performance than HDP to learn the cluster numbers automatically at all the times.
Figure 7. Cluster number learning perfor-mance of HDP-HTM in comparison with HDP on the 20 Newsgroups dataset
Figure 8. The NMI performance comparison for all the five algorithms on the Google News dataset
In this paper, we have addressed the evolutionary clus-tering problem. Based on the recent literature on DP based models and HMM, we have developed the HDP-HTM model as an effective solution to this problem. HDP-HTM model substantially advances the evolutionary clus-tering literature in the sense that it not only performs better than the existing literature, but more importantly it is able to automatically learn the dynamic cluster numbers and the dynamic clustering structures during the evolution, which is a common scenario in many real evolutionary clustering ap-plications. In addition, HDP-HTM also explicitly addresses the correspondence issue whereas all the existing solutions fail to do so. Extensive evaluations demonstrate the effec-tiveness and the promise of HDP-HTM in comparison with the state-of-the-art literature.
Figure 9. The cluster number learning perfor-mance of HDP-HTM in comparison with HDP on the Google News dataset This work is supported in part by NSF (IIS-0535162 and IIS-0812114). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.
