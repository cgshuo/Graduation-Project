 Chinese word segmentation is to generate reasonable word sequence from non-delimited sentences. The most popular model is character labelling model [11, 9] with statistical supervised approach [1, 6], which assign positional labels (B, M, E, S) to characters according to the context. In these approaches, character context is represented by features that strongly depend on the handcrafted fea-ture template. Although feature template can easily incorporate the linguistic knowledge, it is indeed a heavy burden to design an appropriate feature template due to the feature diversity and uncertain local window size.
 ability to bypass the feature engineering. Zheng et al. [14] applied the architec-ture of SENNA [4] to CWS and pos-tagging to avoid the feature engineering and also speed up the training process with a perceptron-style algorithm. Pei et al. [8] proposed a Max-Margin Tensor Neural Networks for CWS and mod-[3] proposed a gated recursive neural network to model the complicated combi-nations of the contextual characters to simulate the feature template. Chen et al. [2] introduced the LSTM neural network for CWS to capture the potential long-distance dependency.
 local window provides the rigid context for a character, and makes it difficult labelling character  X  S  X  , the character  X  P  X  in local window is useless due to the appearance of a caesura sign. Actually, no matter how long the window size is, the characters before the caesura sign are meaningless for labelling  X  S  X . On the contrary, longer the window size is, the more noise will be introduced from left context. However, when labelling  X  u  X  , the character  X  {  X  , which is helpful while is out of the local window. In addition, the local window size is an important hyper-parameter that apparently affect the model performance. It is costly to determine the most suitable size, which may change with the language and the segmentation criterion.
 current neural network with memory cell and gates, which endow it the ability to determine forget or memorize the historical information. We are wondering how well the bi-directional LSTM (Bi-LSTM) neural network can capture related context in whole sentence without local window. Therefore, we present a window-free Bi-LSTM neural network for CWS. The experiments show that Bi-LSTM sequence labelling. The local window is not essential in Bi-LSTM based CWS system. The character sequence labelling model is the most popular model for CWS. Traditional approaches are based on the feature template, while the neural net-work approaches are based on the character embedding. All of them rely on the local window to extract the context. As Figure 2 shows, the conventional neural model for CWS consists of a lookup table layer, linear layers and nonlinear layers, and a output layer. embedding. The embeddings in local window are concatenated to form the con-text of a character. The linear layer combine the input vector and the nonlinear layer do the nonlinear transformation with sigmoid or tanh function and so on. The output layer computes the label probability distribution of each character. dynamic algorithm. In conventional neural network based models, the local con-text is the feature. As a result, local window is of great importance to the model performance. 3.1 LSTM Neural Network for CWS LSTM was proposed to address the gradient vanish of recurrent neural network. A LSTM unit consists of a memory cell and three gates. c t is the memory cell stores the content of content a unit. i t is the input gate to control the scale of current input. f t is the forget gate to control forget extent of the last content. o is the output gate to control the scale of the output. The LSTM unit can be formalised as: tion. However, their model also needs window to capture the right context. Inspired by Chen et al. , we wonder whether the Bi-LSTM Neural network can thoroughly eliminate the local window or not. Therefore, we introduce the Bi-LSTM architecture into the CWS. 4.1 Model Architecture identical to other previous works. For the absence of window, the character embeddings are transferred into the forward and backward layer without being concatenated. The forward layer accepts the positive sequence of embeddings, while the backward layer deals with the inverted sequence of embedding. These two layers work separately. Thus, we can get two hidden states which The two states are concatenated into one and transferred to the next layer that can be another Bi-LSTM layer or a logistic layer. The logistic layer transforms softmax function. 4.2 Training The parameter  X  in our model contains: the lookup table, 6 parameter matrixes for each LSTM layer (equation 1-5), the parameter matrix of logistic layer. We use the max-likelihood criterion to train our model, and we import the L 2 regu-larization item into the objective function to prevent overfitting. The objective function is: where M is the sentence number, N is the sentence length, P ( tag | c ) is the conditional probability,  X  is the regularization constant.
 where E [ g 2 ] t is the average of the squared gradients at time t, and the  X  is the decay constant, is a constant added to better condition the denominator, g t is the gradient of the parameters at time t .  X  and are hyper-parameter. et al.[2] 5.1 Datasets We run experiments on three widely used benchmark datasets, PKU, MSRA [10] and CTB6(LDC2007T36) [12]. For PKU and MSRA datasets, we take the first 90% sentences of the training data as training set and the rest 10% sentences as development set according to the previous works. For CTB6 dataset, we divide the training, development and test sets according to Yang et al. [13]. All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag. We adopt F1-score as the evaluation measure. 5.2 Experiment Results We implement three models: LSTM-No-Window ,Bi-LSTM-No-Window, Bi-LSTM-Window. We investigate these three models on PKU, MSRA and CTB6 dataset. We refer to the result of Chen et al. [2] as LSTM with window (LSTM-Windows). Table 2 shows the F1-Score of models. Compared with Bi-LSTM models and LSTM-Windows model, the LSTM-No-Window model achieves much lower F1-Score by a large gap. From that, we conclude that LSTM-No-Window is insuf-ficient to capture context and the local window can provide abundant contex-t information. However, on Bi-LSTM model, local window slightly affect the performance. On MSRA dataset, the window-free model performs better than window-based model. Therefore, we can conclude that the Bi-LSTM model has strong ability to capture related context information for Chinese word segmen-tation, which means local window is not essential for Bi-LSTM based model. Table 3 compares our result with the previous neural network based CWS mod-els. Although our model does not have a local window to extract the context feature, it outperforms previous neural network model on CTB6 dataset, and lag (-0.3, [3] is based on the recursive neural network). The results suggest that the window-free Bi-LSTM neural network can automatically capture the related context for Chinese word segmentation. Although the local window appears in every previous neural network based approaches, it is not essential for Bi-LSTM based one. The most popular model for CWS is the sequence labelling model proposed template feature. In recent years, neural network based approaches [7, 14, 8, 3] were proposed to avoid the feature engineering. All neural networks based CWS rely on the local window, which restricts the related context in a fixed scope. Our work is inspired by Chen et al. [2]. They exploit a LSTM neural network model exploits a Bi-LSTM neural network to capture the related context and discard the local window thoroughly and achieves comparable or even better performance. In this paper, we present a window-free Bi-LSTM based Chinese word seg-mentation model. Compared with the local window based Bi-LSTM model, the window-free Bi-LSTM model achieves the identical or better performance. Com-pared with other previous local window based neural CWS approach, our model achieves the competitive or better performance. The result shows that Bi-LSTM neural network can automatically capture the related context and the local win-dow is not essential for Bi-LSTM based CWS. The research is supported by National Natural Science Foundation of China (Contract 61202216). Liu is partially supported by the Science Foundation Ire-land (Grant 12/CE/I2267 and 13/RC/2106) as part of the ADAPT Centre at Dublin City University. We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions.

