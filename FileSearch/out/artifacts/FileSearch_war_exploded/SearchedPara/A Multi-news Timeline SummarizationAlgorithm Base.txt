 Important facts about the news event summary are those which describe the developing progress of it. When people create summaries, they choose the sen-tences which contain key words such as major figures, places, people, dates and so on to form a short story about the event. Understandably in automatic sum-marization as well, it X  X  useful to choose those key words to represent general facts and the important factors.

Everyday numerous news reporting diverse events are published on the In-ternet. There have been many news services(e.g. Google News) to group news into events, and then produce a short su mmary for each event. However, most of the news articles are not well-prepared for the progress of the event. There are lots of redundant or duplicated messages among reports about the same event. Multi-news summarization aims to extract the essential information about the event progress from these news articles . The timeline summarization helps to reorganize the sentences in order to get a better reading experience.
A particular challenge for multi-document summarization is how to weighing the importance of each sentence, which either depend on the words or some other latent information stored in the s entence. This requires an effective meth-ods to analyze the hidden information. Literally, there are many synonym and polysemous words which bring lots of difficulties when computing relationships among sentences. To avoid the influence of those words, Latent semantic anal-ysis (LSA) [10] is used to find the core words of a document. LSA is a robust unsupervised technique for deriving an implicit representation of text semantics based on observing co-occurrence of words to find semantic units of news.
Another challenge for multi-document summarization is how to deal with duplicated information among these documents about the same topic or event. Reporters usually like to review some pre vious issues and then tell the readers what happens now. It X  X  helpful for reade rs to know the developing progress about the event and also a solution to this challenge. As stated, an event goes through a life cycle of birth, growth, maturity and death. This can be reflected from special terms utilized for describing different events that experience a similar life cycles. Aging theory [11] is a model exploited in event detection task which tracks life cycles of events using energy function. The energy of an event increases when the event becomes popular, and it diminishes with time. Similarly, it can also be used for summarization to find out the daily hot terms of events.
In this paper, the timeline summary is generated by considering both temporal and semantic characteristics from the n ews regarding the same event. Sentences and publishing time are extracted from th e news articles. The features are ex-tracted from five aspects to represent ea ch sentence. Then, classification model is built with SVM. Finally, from these candidates, sentences are chosen to form the summary and displayed them with timeline, so that people can track the progress of event easily and quickly.

The remainder of this paper is organized as follows: Section 2 reviews related works on summarization and aging theory. Section 3 proposes an approach of leveraging aging theory to gain sentence feature and train the logistic regression classification model. Section 4 describes the experiments and discusses. Section 5 presents the conclusions and future plans. 2.1 Multi-document Summarization Numbers of methods about multi-document summarization have been developed recently. Generally speaking, these meth ods can be divided into extractive sum-marization and abstractive summarization. The main idea of extractive summa-rization is to assign scores to different words in each sentence, and then the high-score sentences will be chosen as the su mmary. While abstractive summarization usually do some information fusion[12], compression[13] and reformulation[14] to get the summary sentences. This paper will focus on the former method.
One of the most popular extracting method of multi-document summarization methods is MEAD[15], which represents ea ch sentence with some sentence-level features such as term frequency, sentence position, first-sentence overlap, etc. Wan[16] proposed an extracting approach based on manifold-ranking about the information richness and novelty. News Bla ster[18] clusters news into events and apply the MultiGen system to find simila r senteces and reform the pieces of sen-tences to create the summary. Allan[19] used  X  X sefulness X ,  X  X ovelty X  to represent the sentence and provide summaries of news topics. They aimed to process the news stream and create the useful summary sentences. Wong[20] investigated a co-training method to train the model by combining labeled and unlabeled data, which used four kinds of features ca n be categorized as surface, content, relevance and event features.

Most recently, the multi-document tim eline summarizati on gains enough at-traction from researchers and engineers. The timeline can help readers to know the developing progress of the event. The redundancy of summary is required to be very low and the key properties of event to be retained. Lots of timeline summarization methods and applications have been developed lately. ETS[21] formulated the task as an optimization problem via iterative substitution from a set of sentences with four requirements. Tran[22] investigated five different sentence features and leveraged SVMRa nk to optimize the summarization task and proposed an un-biased criteria which c an model timeline X  X  characteristics. Zhao[23] involved social attention to compute the importance and capture the social attention by learning users X  collective interests in the form of word dis-tributions from Twitter. Yan[1] proposed to model trans-temporal correlations among component summaries for timelines, using inter-date and intra-date sen-tence dependencies. Nedunchelian[2] reused the MEAD and add the timestamp feature to implement their timeline summarization. Binh Tran[3] extracted the temporal information and surface features to train the regression model for pre-dicting the summary sentences. M. Georg escu[24] offered an online system that supports the entity-centered, temporal analysis of event-related information in Wikipedia based on the detection and summarization of updates in the webpage. Binh Tran[4] put forward a framework for automatically constructing timeline summaries from web news articles. In this framework, they extracted some fea-tures for each date about articles and se ntences X  publishing time and reference time. 2.2 Aging Theory In the different stages of an event, there exist different hot words to reflect the facts about the event. An effective way to extract hot topic words is that capturing variations in the distribution of different word terms occurred in the documents regarding the event. Therefore, tracking the terms to know the cur-rent stage of the life cycle is very important. It X  X  proved that the aging theory is effective to track the current stage of news life cycle[11]. It models the news event X  X  life cycle into four stages, birth, growth, decay, and death. These four stages can be reflected by the key words X  distribution. Namely, aging theory is used to model the appearance and disappearance of terms over time. Chen[5] applied this theory to model the news event X  X  life cycle and utilized the concept of energy to track it. In order to gain the summary of multi-documents of news domain, aging theory is cons idered to be effective and efficient for extracting the features of sentences. The approach for extracting the timeline information from news regarding the same event and its corresponding reports is proposed as follows. Sentences are extracted from the training set of new s articles and in a second step the fea-tures are computed for each sentence. Finally, SVM model is used to train these features with over-sampling technology to recognize the summary sentence. The pipeline for this process is depicted in F ig. 1. Hereinafter pr esents the key con-cepts and methods for the timeline summarization for news regarding the same topic.
 3.1 Key Concepts Event : News event often refers to a series of news articles about someone or something during a period of time. These articles form a news event which tell us the cause, the progress and the results about it.

Timeline Summaries: Generally speaking, timeline is a kind of display forms for the summaries. Timeline summaries should show us the progress of this topic instand of just displaying the message according to the time sequence. Under this condition of the requirement, timeline summary of each day should describe the most important thing happened in that day regard some specific event. The formal definition of event timeline summarization are defined as follows:
Input : Given a set of news articles D = { d 1 ,d 2 ,...,d n } regarding the same event and these articles cover the who le progress of the event in the time span T = { t 1 ,t 2 ,...,t m } . Each news is segmented into sentences and then sentences are grouped by the publishing date to form S = { s 1 ,s 2 ,...,s m } ,where s i is a sentence group of sentences published in the i day.

Output : The multi-document timeline summarization should output the sum-maries along the date and each summary is the main idea of what occured in that day, i.e. O = { o 1 ,o 2 ,...,o m } ,where o i means the summary of sentences from all the sentences of that day s i . 3.2 Sentence Feature Selection In order to represent the most important thing happened in that day, the sum-mary should consider the i mportance, the novelty, a nd the topic hot terms. All the features used in the experiments are shown in Table 1.

Surface feature : This contains features computed by basic statistics, such as the length of sentence, the counts of noun words and stop words, the position in this document and paragraphs, and whether it contains person name or not.
Importance feature : This feature aims to represent the importance of the sentence. The significance of sentences is computed through linear combination of term weights with latent semantic analysis. The function is where TF w is the term frequency of word w and LSA w is the weight of word in the LSA results. Aging feature : This feature is used to measure the life cycle of each word. The frequency of a word changes as event going on, so we use the association between word and time interval to indicate its energy which is defined as follows: where E w,t is the energy of word w in time interval t ;  X  is the transfer factor;  X  is the decay factor; C w,t is the contribution degree of word at the time interval t ; F denote the the energy function which can convert the sum of similarities into a bounded extent. The sigmoid function is used as energy function and it is defined as: The C w,t function is defined as : where nf is an empirical constant, TF w,t is the term frequency of word w in the time span t , IDF w is the IDF value of word w , W is the whole word set during time span t .

In reality, no words describing a special event point will retain popular forever, they will decay over time. In other words, each word has a different energy value during the time span. In order to represent the word X  X  life circle realistically, we cut down the energy of word by a decay factor  X  at the end of every time interval. Once the energy value became negative, it will be replaced by zero and stop decaying.

Topic feature : Each topic contains lots of sentences, i.e. each sentence con-tributes some information to the whole set to expand the topic. This paper uses the link analysis to compute the latent semantic between sentences. First, topic terms and topic elements can be found through the word frequency analysis. Then, the similarity among sentences cons truct a indirect graph of link relation-ship. The similarity value is computed with the cosine function and the function is defined as: where s i denotes the i sentence and v i denotes the word vector of sentence i .Since similarity value can be computed between every two sentences, the relationship forms a similarity graph. Last, pageRank algorithm is used to assign the weight to each node in this graph. The pageRank value is treated as the topic feature of the sentence. The function is :
Novelty feature : In order to avoid some sentences with the same meanings be selected as the summary, the novelty of sentences should be taken into con-sideration. The novelty value is computed by the distance with the summary of last time span. The larger the distance, the more novel the sentence. In this research, the Jaccard similarity is used to gain this value. The function is : where summary ex is the summary sentence in the last time span. Since the Jaccard similarity is used to model the difference over words appearance between the current sentence and the summary sentences, the novelty between the current sentence and summary sentences can be measured. 3.3 Model Training In the case of annotation data, this summarization task is considered as a classi-fication problem. The positive data are s entences labeled as summary sentences, otherwise are negative. However, in each document only a few sentences will be labeled as the summarization senten ce, i.e. the dataset is imbalanced. When the classifier is created over such an imbalanced dataset, the classifier will pre-fer the majority side [7]. In order to improve the precision of minority class, SMOTEBoost[7] method is used to sampling data for training the classification model. This method combines SMOTE [8] and boost technology. It has been proved effective for imbalanced data set. 4.1 Datasets and Design Since 2001, the US National Institute of Standards and Technology (NIST) has organized large-scale shared tasks for automatic text summarization within the Document Understanding Conference 1 (DUC) and the Summarization track at the Text Analysis Conference 2 (TAC). This paper choose one generic summa-rization and one guided summarization dataset to test the proposed approach from these datasets.

DUC-2002 dataset is used for generic summarization task in 2002. This dataset consist of 59 document sets, each in tw o formats -one with just the original TREC tagging, and one with an additional rough sentence tagging. Each docu-ment set contains around 12 documents f rom different day about the same event. Each event is also provided about 6 labeled summary sentences with the limit of 200 words. This limit is also set in our experiments.

TAC-2010 dataset is used for guided summarization task. The task aims to encourage summarization sy stems to make a deeper linguistic (semantic) anal-ysis of the source documents. Thus, the previous method can be replaced that relying only on document word frequen cies to select important concepts and create the summary covere d the predefined category. This dataset contains 906 documents around 46 topics from the N ew York Times, the Associated Press, the Xinhua News Agency newswires, a nd the Washington Post News Service. Each topic has two sets of documents, A and B, each containing 10 documents. The difference is that documents in B were published later than that in A. It X  X  also an update summarization task, since it X  X  required to create an 100 words updated summarization about documents in B. This dataset is good for the time-line summarization, because the summary should contains the important facts about the predefined category and show the update information. The summary sentences are relabeled based on the re sults from NIST in order to adapt the timeline summarization. 4.2 Annotation Method The original annotation results of DUC-2002 and TAC-2010 are not prepared for the timeline summarization. In other words, the timeline summary sentences should not only contain those sentences with the true facts but also should along the time. We annotated these two dataset s with three steps and the details are as follows: First, all the candidate sentences with publishing date are ranked in a descending order of importance. Seco nd, the top 1 sentence is selected as the summary sentence and sentences published in the same day will be removed from the queue. Third, the next sentence in the queue will be selected and do this loop until the summary sentences can cover the whole time span of the event. The improved datasets are named as DUC-2002T and TAC-2010T. 4.3 Baselines The experiment is started with the preprocessing like indexing, filtering out the stop words and segmenting news documents into sentences. Then the proposed method is used to the data set and gener ates a timeline for each chosen event. Some widely used multi-document summarization methods are implemented as the baselines.

Random The selecting method for random is that one sentence will be chosen as the summary sentence randomly from the whole document.

MEAD 3 is a famous multi-document summarization tool which extracts sum-mary sentence based on the centroid value , positional value and first-sentence overlap.

Cluster considers that there are different t hemes in an event, so it divides sim-ilar sentences together into different clus ters and then selects one representative sentence from each main cluster.

Allan is a similar timeline system from diff erent aspects, which dividing sen-tences into on-event and off-event while ranking them with useful and novelty.
Wong combined the supervised and semi-s upervised learning and used co-training method to train the labeled data and unlabeled data. 4.4 Evaluation Metric In this part, ROUGE toolkit[9] is adopted, which is officially applied by DUC for document summarization performance evaluation, to evaluate the experi-mental results and compare these algorithms with each other. It X  X  a recall based measure which compute unigram or bigram words matching between the auto-summarization summary and human-lab eled summary sentences. All the sum-maries were truncated to the words limit size without removing the stop words. 4.5 Results LibSVM 4 is used to train the classification model. In order to reduce the af-fect of imbalanced training data, over-sampling technology is used. First, some instances are sampled from the minority. Second, new instances are created by linear combining these selected instances. Third, these new instances are put into the training set. The model in the proposed approach 1 is trained without over-sampling, while the approach 2 is trained with over-sampling technology. Rouge-1 and Rouge-2 are used to measure the performance about recall on both datasets. Results are shown in Table 2 and Table 3. Precision value measures the percentage of labeled summary sent ences among all the summary sentences created by the classifier.
 Experiments on DUC-2002T. From Table 2, it can be seen that our ap-proach 2 using SVM and over-sampling obtains the best results, followed by the approach 1 then Wong and MEAD . There is no surprise that Random provides the worst results. The approach 2 performs better than approach 1 is well un-derstanded, since the resampling can gain more training samples which enhance the classifier. The Allan  X  X  approach is close to the timeline summarization, but the results are not good due to the fact that only useful and novelty features are not enough to maintain the information of sentence. The Cluster method X  X  bad performance is due to the random select ing strategy from the cluster center. Wong  X  X  method combined the supervised a nd semi-supervised learning which make it hard to turning the parameters. Both approach 1 and Wong shared most of features, except the aging features. The results show that the aging feature can improve the performance of summarization.
 Experiments on TAC-2010T. Table 3 shows the results on the TAC-2010T dataset. The Allan method performs better than MEAD . The reason is that events in this dataset have more than 20 documents, which provide enough information about useful and novelty compared to the previous dataset.
From this two experiments, we can get the information that summarization methods based on machine learning are performance better than others. The result of MEAD are better than Cluster , mainly because this method use some surface feature. The Cluster method is the worst in our experiments, since this method cluster the same meaning sentences and choose one from cluster ran-domly, which introduce lots of uncertainty. The Allan method does not perform stable, that it will depend on the amount of documents and the content of each sentence. Our approach 2 performed better than Wong and approach 1 on pre-cision and Rouge-1 metric which mainly due to over-sampling method and the new feature about aging. The over-sampling technology make the minority class can be classified better and this is proved in our experiments. In this paper, a novel approach involved aging theory is proposed to create timeline summarization, which focus on the news reports regarding the same event. After basic pre-precessing work, f eatures of each sentence are extracted, including surface features, importance fe atures, aging features, novelty features and topic features. Then the multi-docu ment summarization task is considered as pair-wise classification task and the training data is well prepared. The final classification model is trained with SVM and over-sampling technology is used to improve the performance. Experiment results show that the proposed approach performs better compared with other widely used methods. In the future, the learning to rank framework will be tried with this group of features and we will do more experiments on differ ent feature combinations.
 Acknowledgments. This work is supported by the National Natural Science Foundation of China ( No. 61370137, 61272361) and the 111 Project of Beijing Institute of Technology.

