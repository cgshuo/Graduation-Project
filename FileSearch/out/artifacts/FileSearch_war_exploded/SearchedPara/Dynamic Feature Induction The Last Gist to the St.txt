 Feature engineering typically involves two processes: the process of discovering novel features with domain knowledge, and the process of optimizing combina-tions between existing features. Discovering novel features may require linguistic background as well as good understanding in machine learning such that it is often difficult to do. Optimizing feature combi-nations can be also difficult but usually requires less domain knowledge and more importantly, it can be as effective as discovering new features. It has been shown for many tasks that approaches using simple machine learning with extensive feature engineering outperform ones using more advanced machine learn-ing with less intensive feature engineering (Xue and Palmer, 2004; Bengtson and Roth, 2008; Ratinov and Roth, 2009; Zhang and Nivre, 2011).
 Recently, people have tried to automate the second part of feature engineering, the optimization of fea-ture combinations, through leading-edge models such as neural networks (Collobert et al., 2011). Coupled with embedding approaches (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), neural networks can find the optimal feature combinations using techniques such as random weight initialization and back-propagation, and have established the new state-of-the-art for several tasks (Socher et al., 2013; Devlin et al., 2014; Yu et al., 2014). However, neural networks are not as good at optimizing combinations between sparse features, which are still the most dom-inating factors in natural language processing.
This paper introduces a new technique called dy-namic feature induction that automates the optimiza-tion of feature combinations (Section 3), and can be easily adapted to any NLP task using sparse features. Dynamic feature induction allows humans to focus on the first part of feature engineering, the discovery of novel features, while machines handle the second part. Our approach was experimented with two core NLP tasks, part-of-speech tagging (Section 4) and named entity recognition (Section 5) and showed the state-of-the-art results for both tasks. 2.1 Nonlinearity in NLP Linear classification algorithms such as Perceptron, Winnow, or Support Vector Machines with a linear kernel have performed exceptionally well for various NLP tasks (Collins, 2002; Zhang and Johnson, 2003; Pradhan et al., 2005). This is not because our feature space is linearly separable by nature, but sparse fea-tures introduced to NLP yield very high dimensional vector space such that it is rather forced to be linearly separable. For example, NLP features for a word w i typically involve the word forms of w i  X  1 and w i (e.g., f i  X  1 , f i ). If the feature space is not linearly separable with these features, a common trick is to introduce  X  X igher X  dimension features by joining  X  X ower X  dimen-sion features together (e.g., f i  X  1 f i ). The more joint features we introduce, the higher chance we get for the feature space being linearly separable although these joint features can be very overfitted.
Let us define low dimensional features as the primi-tive features such as f i  X  1 or f i , and high dimensional features as the joint features such as f i f i +1 . 1 Low dimensional features are well explored for most NLP tasks; it is the high dimensional features that are quite sensitive to specific tasks. Finding high dimensional features can be a manual intensive work and this is what dynamic feature induction intends to take over. 2.2 Related Work Kudo and Matsumoto (2003) introduced the polyno-mial kernel expansion that explicitly enumerated the feature combinations. Our approach is distinguished because they used a frequency-based PrefixSpan al-gorithm (Pei et al., 2001) whereas we used the online learning weights for finding the feature combinations. Goldberg and Elhadad (2008) suggested an efficient algorithm for computing polynomial kernel SVMs by combining inverted indexing and kernel expansion. Their work is focused more on improving support vector machines whereas our work is generalized to any linear classification algorithm. Okanohara and Tsujii (2009) introduced an approach for generating feature combinations using ` 1 regular-ization and grafting (Perkins et al., 2003). Although we share similar ideas, their grafting algorithm starts with an empty feature set whereas ours starts with low dimensional features, and their correlation parame-ters  X  i,y are pre-computed whereas ours are dynami-cally determined. Strubell et al. (2015) suggested an algorithm that dynamically selected strong features during decoding. Our work is distinguished because we do not run multiple training phases as they do for figuring our strong features. The intuition behind dynamic feature induction is to keep populating high dimensional features by joining low dimensional features together until the feature shows how features are induced during training: 1. Given a training instance ( x 1 ,y 1 ) , where x 1 is a 2. Let us refer  X  X trong features for y against  X  y  X  to 3. Given a new training instance ( x 2 ,y 2 ) , combi-4. The extended feature set x 2 is fed into the classi-Thus, high dimensional features in F are incremen-tally induced and learned along with low dimensional features during training. During decoding, each fea-ture set is extended by the induced features in F , and the prediction is made using the extended feature set. The size of F can grow up to |X| 2 , where |X| is the size of low dimensional features. However, we found that |F| is more like 1 / 4  X |X| in practice.
The following sections explain our approach in de-tails. Sections 3.1, 3.2, and 3.3 describe how features are induced and learned during training. Sections 3.4 and 3.5 describe how the induced features are stored and expanded during decoding. 3.1 Feature Induction Algorithm 1 shows an online learning algorithm that induces and learns high dimensional features during training. It takes the set of training instances D and the learning rate  X  , and returns the weight vector w and the set of induced features F .
 Algorithm 1 Feature Induction The algorithm begins by initializing the weight vector w , the diagonal vector g , and the induced feature set F (lines 1-2). For each instance ( x,y )  X  D where y is the gold-label for the feature set x , it predicts  X  y maximizing w  X   X  ( x,y 0 , F )  X  I y ( y 0 ) , where I is defined as follows (lines 4-5): The feature map  X  takes ( x,y, F ) , and returns a d  X  l -dimensional vector, where d and l are the sizes of features and labels, respectively; each dimension con-If certain combinations between features in x exist in F , they are appended to the feature vector along with the low dimensional features (see Section 3.5 for more details). The indicator function I allows our algorithm to be optimized for the hinge loss for mul-ticlass classification (Crammer and Singer, 2002): ` h = max[0 , 1 + w  X  (  X  ( x,  X  y, F )  X   X  ( x,y, F ))] If y is not equal to  X  y (line 6), the partial vector  X  is measured (line 7), and g and w are updated (lines 8-9) by AdaGrad (Duchi et al., 2011), where the learning rate  X  is adjusted by g (in our case,  X  = 1E-5). Once w is updated, the d -dimensional vector v is generated (line 10), where [ ... ] y returns only the portion of the values relevant to y (Figure 2).

The i  X  X h element in v represents the strength of the i  X  X h feature for y against  X  y ; the greater v i is, the stronger the i  X  X h feature is. Next, indices of the top-k entries in v are collected in the ordered list L (line 11), representing the strongest features for y against  X  y . 4 Finally, the pairs of the first index in L , representing the strongest feature, and the other indices in L are added to the induced feature set F (lines 12-13). For example, if L = [ i,j,k ] such that v i  X  v j  X  v k &gt; 0 , two pairs, ( i,j ) and ( i,k ) , are added to F .
For all our experiments, k = 3 is used; increasing k beyond this cutoff did not show much improvement. Notice that all induced features in F are derived by joining only low dimensional features together. Our algorithm does not join a high dimensional feature with either a low dimensional feature or another high dimensional feature. This was done intentionally to prevent from the feature space being exploded; such features can be induced by replacing  X  with F in the line 10 as follows: It is worth mentioning that we did not find it useful for joining intermediate features together (e.g., ( j,k ) in the above example). It is possible to utilize these combinations by weighting them differently, which we will explore in the future. Additionally, we exper-imented with the combinations between strong and weak features (joining i  X  X h and j  X  X h features, where v &gt; 0 and v j &lt; 0 ), which again was not so useful. We are planning to evaluate our approach on more tasks and data, which will give us better understand-ing of what combinations are the most effective. 3.2 Regularized Dual Averaging Each high dimensional feature in F is induced for making classification between two labels, y and  X  y , but it may or may not be helpful for distinguishing labels other than those two. Our algorithm can be modified to learn the weights of the induced features only for their relevant labels by adding the label in-formation to F , which would change the line 13 in Algorithm 1 as follows: However, introducing features targeting specific la-bel pairs potentially confuses the classifier, especially when they are trained with the low dimensional fea-tures targeting all labels. Instead, it is better to apply a feature selection technique such as ` 1 regulariza-tion so the induced features can be selectively learned for labels that find those features useful. We adapt regularized dual averaging (Xiao, 2010), which effi-ciently finds the convergence rates for online convex optimization, and works most effectively with sparse feature vectors. To apply regularized dual averaging, the line 1 in Algorithm 1 is changed to: c is a d  X  l -dimensional vector consisting of accu-mulative penalties. t is the number of weight vectors generated during training. Although w is technically not updated when y =  X  y , it is still considered a new vector. Thus, t is incremented for every training in-stance, so t  X  t + 1 is inserted after the line 5. c is updated by adding the partial vector  X  as follows (to be inserted after the line 7): Thus, each dimension in c represents the accumula-tive penalty (or reward) for a particular feature and a label. At last, the line 9 is changed to: ` 1 ( c ,t, X  )  X  The function ` 1 takes c , t , and the regularizer pa-rameter  X  tuned during development. If the absolute value of the accumulative penalty c i is greater than  X   X  t , the weight w i is updated by  X  and t ; otherwise, it is assigned to 0 . For our experiments, RDA was able to throw out irrelevant features successfully, and showed improvement in accuracy; in fact, dynamic feature induction without RDA did not show as much improvement over low dimensional features. 3.3 Locally Optimal Learning to Search Features in most NLP tasks are extracted from struc-tures (e.g., sequence, tree). For structured learning, we adapt  X  X ocally optimal learning to search X  (Chang et al., 2015b), that is a member of imitation learning similar to DA GGER (Ross et al., 2011). LOLS not only performs well relative to the reference policy, but also can improve upon the reference policy, show-ing very good results for tasks such as part-of-speech tagging and dependency parsing. We adapt LOLS by setting the reference policy as follows: 1. The reference policy  X  determines how often the 2. For the first epoch, since  X  is 0 . 95 , y is randomly 3. After every epoch,  X  is multiplied by 0 . 95 . This For our experiments, LOLS gave only marginal im-provement, probably because the tasks we evaluated, part-of-speech tagging and named entity recognition, did not yield complex structures. However, we still included this in our framework because we wanted to evaluate our approach on more tasks such as depen-dency parsing where learning to search algorithms show a clear advantage (Goldberg and Nivre, 2012; Choi and McCallum, 2013; Chang et al., 2015a). 3.4 Feature Hashing Feature hashing is a technique of converting string features to vectors (Ganchev and Dredze, 2008; Wein-berger et al., 2009). Given a string feature f and a hash function h , the index of f in the vector space is determined by taking the remainder of the hash code: The divisor  X  is tuned during development. Feature hashing allows to convert string features into sparse vectors without reserving an extra space for a map whose keys and values are the string features and their indices. Given a feature index pair ( i,j ) representing strong features for y against  X  y (Section 3.1), the index of the induced feature can be measured as follows: For efficiency, feature hashing is adapted to our sys-tem such that the induced feature set F is actually not a set but a  X  -dimensional boolean array, where each dimension represents the validity of the correspond-ing induced feature. Thus, the line 13 in Algorithm 1 is changed to: For the choice of h , xxHash is used, that is a fast non-cryptographic hash algorithm showing the per-3.5 Feature Expansion Algorithm 2 describes how high dimensional features are expanded from low dimensional features during containing only low dimensional features and returns high dimensional features.
 Algorithm 2 Feature Expansion For every combination ( i,j )  X  x l  X  x l , where i and j represent the corresponding feature indices (lines 2-3), it first measures the index k of the feature com-bination (line 4), then checks if this combination is valid (Section 3.4). If the combination is valid, mean-ing that ( F k = True ) , k is added to x l + h (line 5). mensional features. 4.1 Corpus The Wall Street Journal corpus from the Penn Tree-bank III is used (Marcus et al., 1993) with the stan-dard split for part-of-speech tagging experiments. Set Sections Sentences ALL OOV TRN 0-18 38,219 912,344 0 DEV 19-21 5,527 131,768 4,467 TST 22-24 5,462 129,654 3,649 4.2 Tagging and Learning Algorithms A one-pass, left-to-right tagging algorithm is used for our experiments. Such a simple algorithm is chosen because we want to see the performance gain purely from our approach, not by a more sophisticated tag-ging algorithm (Toutanova et al., 2003; Shen et al., 2007), which may improve the performance further.
For learning, the final algorithm from Section 3 is used. Additionally, mini-batch is applied, where each batch consists of training instances from k -number of sentences, causing the sizes of these batches different. We found that grouping instances with respect to the sentence boundary was more effective than batching them across arbitrary sentences. For all our experi-ments, the learning rate  X  = 0 . 02 and the mini-batch boundary k = 5 were used without tuning. 4.3 Ambiguity Classes The ambiguity class of a word is the concatenation of all possible tags for that word. For example, if the word  X  X tudy X  can be tagged by NN (common noun) or VB (base verb), its ambiguity class becomes NN VB . Instead of building ambiguity classes only from the training dataset, we automatically tagged a mixture of guity classes using the automatic tags before training. This was motivated by Moore (2015), who showed extraordinary results on the out-of-vocabulary words by limiting the classification to the ambiguity classes collected from such large corpora. We used the ClearNLP POS tagger (Choi and Palmer, 2012) for tagging the data (about 141M words), threw away tags appearing less than a certain threshold, and created the ambiguity classes. For each word, tags appearing less than 20% of the time for that word were discarded. As the result, about 2M ambiguity classes were collected from these datasets. 4.4 Feature Template Table 2 shows the template for low dimensional fea-tures. Digits inside the curly brackets imply the con-text windows with respect to the word w i to be tagged. For example, f { 0 ,  X  1 } represents the word-forms of w , w i  X  1 , and w i +1 . No joint features (e.g., f 0 f 1 are included in this template; they should be automat-ically induced by dynamic feature induction.
Orthographic (Gim  X  enez and M ` arquez, 2004) and word shape (Finkel et al., 2005) features are adapted from the previous work. The positional features indi-cate whether w i is the first or the last word in the sen-tence. Word clusters are trained on the same datasets in Section 4.3 using Brown et al. (1992). 4.5 Development The regularization parameter  X  (Section 3.2) and the modulo divisor  X  (Section 3.4) are tuned during de-velopment through grid search on  X   X  [1E-9, 1E-6] and  X   X  [1.5M, 5M] . Table 3 shows the accuracies achieved by our models on the development set. M 0 : baseline 97.09 86.14 365,400 M 1 : M 0 + ext. ambi. 97.37 91.92 365,409 M 2 : M 1 + clusters 97.45 91.96 372,181 M 3 : M 1 + dynamic 97.42 92.10 468,378 M 4 : M 2 + dynamic 97.48 92.21 473,134 M 0 used the tagging and the learning algorithms in Section 4.2 and the feature template in Section 4.4, where the ambiguity classes were collected only from the training dataset; dynamic feature induction was not used for M 0 . By applying the external ambiguity classes in Section 4.3, M 1 achieved about a 5.8% im-provement on OOV. M 2 gained small improvements by adding word clusters. Coupled with dynamic fea-ture induction, M 3 and M 4 gained about 0 . 04 % and 0 . 2 % improvements on average for ALL and OOV.
For both M 3 and M 4 , about 100K more features were generated from M 1 and M 2 , implying that about 25% of the features were automatically induced by dynamic feature induction. It is worth pointing out that improving upon M 1 was a difficult task because it was already reaching near the state-of-the-art. The external ambiguity classes by themselves were strong enough to make accurate predictions such that the induced features did not find a critical role in the classification. 4.6 Evaluation Table 4 shows the accuracies achieved by the models from Section 4.5 and the previous state-of-the-art approaches on the evaluation set.
 Manning (2011) 97.29 89.70 Manning (2011) 97.32 90.79 X Shen et al. (2007) 97.33 89.61 Sun (2014) 97.36 -Moore (2015) 97.36 91.09 X Spoustov  X  a et al. (2009) 97.44 -X S X gaard (2011) 97.50 -X Tsuboi (2014) 97.51 91.64 X This work: M 0 97.18 86.35 This work: M 1 97.37 91.34 X This work: M 2 97.46 91.23 X This work: M 3 97.52 91.53 X This work: M 4 97.64 92.03 X The results on the evaluation set appear much more promising. Still, the biggest gain was made by M 1 , but our final model M 4 was able to achieve a 0.8% im-provement on OOV over M 2 , and showed the state-of-the-art results on both ALL and OOV. Interestingly, M 2 showed a slightly lower accuracy on OOV than M 1 even with the additional word cluster features. On the other hand, M 2 did show a slightly higher accu-racy on ALL, indicating that the model was probably M 4 was still able to achieve improvements over M 2 on both ALL and OOV, implying that dynamic fea-ture induction facilitated the classifier to be trained more robustly. 5.1 Corpus The English corpus from the CoNLL X 03 shared task is used (Tjong Kim Sang and De Meulder, 2003) for named entity recognition experiments.
 5.2 Feature Template Table 6 shows the feature template for NER, adapting the specifications in Table 2. Following the state-of-the-art approaches (Table 8), word clusters are trained on the Reuters Corpus Volume I (Lewis et al., 2004) using Brown et al. (1992). Named entity gazetteers trained on the datasets in Section 4.3 using Mikolov et al. (2013) and appended to the sparse feature vec-tors as dense vectors. Note that the word embedding features did not participate in dynamic feature induc-tion; it was not intuitive how to combine sparse and dense features together so we left it as a future work. f e p 5.3 Development The regularization parameter and the modulo divisor are tuned during development through the same grid search in Section 4.5. Table 7 shows the precisions and the recalls achieved by our models on the devel-opment set (the F1-scores are shown in Table 8). M 0 used the tagging and the learning algorithms in Section 4.2 and the feature template in Section 5.2, excluding the gazetteer, cluster, and embedding fea-tures; dynamic feature induction was not applied to M 0 . M { 1 , 2 , 3 } gained incremental improvements from the gazetteer, cluster, and embedding features, respec-tively. M 4 showed 0 . 36 % and 0 . 67 % improvements on precision and recall respectively, and generated about 40K more features compared to M 3 . This is about 23% increase in features that is similar to the increase shown in Table 3. 5.4 Evaluation Table 8 shows the F1-scores achieved by our models All models showed improvements over their prede-cessors; the improvements made in TST were more dramatic than the ones made in DEV although they followed a very similar trend. Notice that M 3 , not us-ing dynamic feature induction, showed very similar scores to Ratinov and Roth (2009). This was not sur-prising because M 3 adapted many features suggested
M 4 achieved about 0.5% improvements over M 3 , showing the state-of-the-art result on TST. Consid-ering that M 3 was already near state-of-the-art, this improvement was meaningful. It was interesting that Suzuki and Isozaki (2008) achieved the state-of-the-art result on DEV although their score on TST was much lower than the other approaches. This might be because features extracted from the huge external data they used were overfitted to DEV, but more thor-ough analysis needs to be done. On the other hand, Passos et al. (2014) achieved the near state-of-the-art result on DEV while it also got a very high score on TST by utilizing phrase embeddings, which we will look into in the future. In this paper, we introduced a novel technique called dynamic feature induction that automatically induces high dimensional features so the feature space can be more linearly separable. Our approach was evaluated on two NLP tasks, part-of-speech tagging and named entity recognition, and showed the state-of-the-art results on both tasks. The improvements achieved by dynamic feature induction might not be statistically significant, but important because they gave the last gist to the state-of-the-art; without this last gist, our system would have not reached the bar.

It is worth mentioning that we also experimented with several feature templates including many joint features without applying dynamic feature induction. The results we got from these manually induced fea-tures were not any better (often worse) than the ones achieved by dynamic feature induction, which was very encouraging. In the future, we will experiment our approach on more NLP tasks such as dependency parsing and conference resolution where induced fea-tures should play a more critical role. We concede that our approach is more empirically motivated than theoretically justified. For instance, the choice of k (line 11) or the combination configu-ration for L (line 13) in Algorithm 1 are rather empir-ically derived. All the parameters are automatically tuned by running grid searches on the development sets (Sections 4.5 and 5.3); it would be intellectually intriguing to find a more principled way of adjusting these hyper-parameters than just brute-force search.
The locally optimal learning to search is used to help structured learning although it gives a relatively smaller impact to the tasks involving sequence clas-sification such as part-of-speech tagging and named entity recognition. This framework is used because we plan to apply our approach on more structurally oriented tasks such as dependency parsing and AMR parsing. Our work is also related to feature group-ing, which has been shown to be beneficial in learn-ing high-dimensional data (Zhong and Kwok, 2011; Suzuki and Nagata, 2013). It will be interesting to compare our work to the previous work and see the strengths and weaknesses of our approach.
 We gratefully acknowledge the support of the Yahoo Academic Career Enhancement Award, the IPsoft Development Enhancement Grant, the University Re-search Committee Award, and the Infosys Research Enhancement Grant. Any contents expressed in this material are those of the authors and do not necessar-ily reflect the views of these awards and grants.
