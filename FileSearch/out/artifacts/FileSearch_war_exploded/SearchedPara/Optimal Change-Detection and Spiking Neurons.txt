 below, neuronal change-detection is a prime example of such a problem. cueing, and neuromodulation. The Generative Model Suppose we have sequential inputs x time  X   X  { 0 , 1 , . . . } , and by a distribution f denotes the sudden, hidden change time.  X  has an initial probability P (  X  =0)= q distribution thereafter: P (  X  = t ) = (1  X  q concerned with finding the optimal decision policy for repor ting the change from f from all observations made so far to the control (or action) s et,  X  ( x The action a The Loss Function speed and accuracy. The total loss is the expectation of this loss function over  X  and  X  :
L  X  ,  X  l  X  (  X ,  X  );  X   X  = An optimal policy  X   X  minimizes L policies. Using the notation P  X  (  X   X   X  ) +  X  = The cumulative posterior probability P loss evaluation and policy optimization: Bayes Rule gives us the iterative update rule for the cumulat ive posterior P P x monotonically related posterior ratio  X  Optimal Policy: Threshold Crossing (1) the conditional termination cost , C  X  at every time step); (3) ess inf , the largest ( a.s. ) r.v. less than ( a.s. ) every r.v. X As an example of Bellman X  X  Equation,  X  gion S  X  [0 , 1] and a continuation region C = [0 , 1] \ S , such that  X  ( P action as soon as P process in P the dynamic equation for  X  T on  X  , then the finitely recursive relation  X  T horizon optimal policy  X   X  is finite for all decision policies that stop a.s. in finite time),  X  infinite-horizon optimal policy  X   X  . We also note the following self-evident lemma. Lemma. Suppose { g where P Theorem. C Proof. C where g with P with P and P horizon limit. If C action a such P of minimizing the empirical loss as a function of the decision t hreshold B  X  [0 , 1] . In the following, we focus on the specific case where f tive rate parameters  X  assume that the generative parameters  X  assume, without loss of generality, that  X  the inputs ( 0 or 1 ). When the parameters satisfy c  X  (  X  process is driven to cross the threshold even in the absence o f any input spikes. of perturbation around the optimal value.
 Repeated Change-Detection and Firing Rate As long as each detection event is generated from the same mod el parameters ( q, q the repetition is q f q is influenced by the detection policy, the optimization of th e policy is not influenced by q it consists of comparing C q , which simply adds a linear factor to both terms.
 of changes, the loss function of Eq. 2 can be rewritten as L mean firing rate when the inputs are generated from f applicable (as opposed to f and maximizing the  X  X timulus-evoked X  firing rate during f Optimality and Dynamics of Leaky Integrate-and-Fire dynamical variable  X  threshold 0 . 65 / (1  X  0 . 65) (or equivalently when P in the last section), a change is reported and the whole proce ss resets to  X  the dynamics of a leaky integrate-and-fire neuron [3]. Let a , f 1 ( x t ) When x x t = 0 f imminent.
 Relationship Between Input and Output Firing Rates parameter a has the expected values: Given Eqs. 5 and 6, we can write down an approximate, explicit expression for  X   X   X   X  t | f i  X  X  X  a i (  X   X  t  X  1  X  + q )= a t i  X   X  0  X  + a i q has a single shallow minimum at B = 0 . 65 .  X  Top panel: a typical example of the dynamics of  X  induces the change-detector fire much more frequently. Note that  X  frequency quickly after the increase in input spikes (top).
 Given the decision threshold B ,  X   X  steps it takes to reach the threshold for for x time steps of input integration to reach the threshold). We t herefore have And therefore the ratio of the output firing rates, r Since the arguments of log in both the denominator and numerator are greater than 1 , r Therefore, when the input rates are such that  X  that r we define the function g (  X  and  X   X  Multi-source change-detection approach similar to that taken in [5]. The source f i , i  X  { 1 , 2 } emits observations x i Bernoulli process that changes from rate  X  i a geometric distribution with parameter q i , and the prior probability P (  X  i = 0) is q i is to detect  X  , min(  X  Defining the individual posteriors P i We can also define the corresponding overall posterior ratio a stimulus appearing ( q prior leads to false alarms as well as reducing detection del ay. as a function of the individual posterior ratios  X  i to that of Sec. 2, we can show that if the generative and cost pa rameters are such that  X  bounded by  X  0 t , such that  X  change. To see this, consider the case when  X  1 ( q but the overall detector would not. are consistent with their observed effects at the cellular l evel. noradrenergic depletion diminishes this advantage [6].
 with a higher prior probability q acetylcholine potentiates neurons and increases their res ponsiveness to sensory inputs [8]. within the framework.
 nature of the computation change from input-processing to d ecision-making. 2AFC discrimination [14].
 are desirable from a computational point of view.
 One important assumption we made in our model is that the cost of detection delay is linear in that the best fixed threshold policy is still far from optimal detection. transition is always from f to one that detects the onset ( f similar formal tools from controlled dynamic processes.
 Acknowledgments We thank Bill Bialek, Peter Dayan, Savas Dayanik, and Sophie Deneve for helpful discussions.
