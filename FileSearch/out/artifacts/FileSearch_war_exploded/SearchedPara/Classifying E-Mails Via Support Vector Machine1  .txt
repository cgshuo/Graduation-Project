 As the Internet grows at a tremendous speed, E-mail has become a widely used form of communication. E-mail has gained enormous popularity not only as a means for letting friends and colleagues exchange messages, but also as a medium for conducting electronic commerce. Unfortunately, the convenience and inexpensiveness of E-mail also make it ov erused by companies, organizations or people to promote products and spread information, which serves their own purposes. The unsuspecting mailboxes of users may often be crammed with E-mail messages a large portion of which are not of interest to them. Searching for interesting messages out of a thousand unread ones is tedious and annoying. As a consequence, the need for a personal E-mail filter is pressing.

For the problem of classifying E-mail documents, the objects to be classified are semi-structured textual documents consisting of two portions. One portion is a set of structured fields with well-defined semantics and the other portion is a number of variable length sections of free text. We would like to emphasize this feature in our study because information from both portions is important. In the case of E-mail messages, the fields in the mail header such as the sender and the recipient are very informative when we determine how interesting the message part is. On the other hand, the interestingness of an E-mail message from the same sender also depends on the content of the body message.
There have been a number of approaches developed for E-mail classification [8, 4, 10, 2]. In this paper, we propose a novel approach to classifying E-mails us-ing the Support Vector Machine (SVM). In particular, we treat E-mail messages as a specific kind of plain text files with structured features, the implication being that our feature set is relatively large (since there are thousands of dif-ferent terms in different E-mail files). To speed up the process and reduce the space/computational cost, we enhance SVM with Principal Component Analysis (PCA) [7] which is used as a preprocessor to reduce the data in terms of dimen-sionality so that the input data becomes more classifiable. Note that, PCA only pre-processes the input features to SVM classification model. We also evalu-ate the performance of E-mail classification with two SVM mechanisms, i.e. C -support vector machine and v -support vector machine algorithms. We conduct a series of experiments on a relatively large dataset composed of real personal E-mails, and discuss the behaviors of the classification approach in detail. The experimental results show that this approach provides superior performance in terms of recall and precision.

The rest of this paper is organized as following: The next section introduces some related work. In section 3, we present our method for E-mail classification. In sections 4, we present the experimental results and the analysis. Finally, we give the conclusions in section 5. This section reviews related work in the area of junk E-mail filtering and Prin-cipal Component Analysis. There are a lot of research works on junk E-mail filtering in the literature [2, 10, 4, 3]. Sahami et al. proposed a Bayesian approach to filtering junk E-mail in [10]. The proposal considers domain specific features in addition to raw text of E-mail messages. It enhances the performance of a Bayesian classifier by handcrafting and incorporating many features indicative of junk E-mail. The authors proposed two classifications as the Probabilistic Classification and the Domain Specific Properties. Representing each individual message as a binary vector, the proposed method detects junk mail in a straight-forward manner using a given pre-classified set of training messages. In [2], the authors compared methods for learning text classifiers focusing on the kinds of classification problems that might arise in filtering personal E-mail messages. In [4], the E-mail documents to be classified a re regarded as semi-structured textual documents comprising two parts. One part is a set of structured fields with well-defined semantics, while the other is a number of variable-length sections of free text. However, not many text classifiers take both portions into consideration. Moreover, conventional classification techniques may not be effective when han-dling variable-length free text. In [3], a model based on the Neural Network was proposed, which handles fields which having pre-defined semantics as well as the variable length free-text fields for obtaining higher accuracy. In [5], the authors propose an E-mail classifying system using Support Vector Machine. The results outperform other three conventional methods in that it provides acceptable test performance in terms of accuracy and speed, without compromising with long training time.

In this paper, we propose a new model based on a hybrid technique to im-prove the efficiency of the system, which integrates Support Vector Machine and Principal Component Analysis together. PCA [7] is a widely used method for applications in signal/image filtering and pattern classification. It can transform data in the original space into another feature space, reduce the dimensionality of the input data, while keeping the most significant information. It examines the variance structure in the dataset and determines the directions along which the data exhibits high variance. The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible. Working as a pre-processor of SVM E-mail classifier, it can make the input data more classifiable and reduce the dimensionality of the training and validation dataset by using only the first several features, thereby s peeding up the convergence of the train-ing process, e.g. 10% number of features after PCA transformation can capture more than 95% information of the data. In this section, we discuss the main phases of proposed method for E-mail classifi-cation in detail. We first introduce the background information for our technique. Second, a model for E-mail classification based on SVM is presented. Third, we describe how to integrate the PCA technique in the SVM-based model. 3.1 Support Vector Machine The Support Vector Machine (SVM) [1] can be simply regarded as a task that is to find an optimal hyperplane which linea rly separates the sample set in their feature space. More precisely, suppose we have a sample set, say X i , (i = 1, 2, ,N),where X i is a k -dimensional vector which means that X can be expressed as a vector of k features. Then, we can map the sample space into their k -dimension feature space. For each sample X i , we assign a target value or class label d i with value -1 or 1. After that, we try to find a hyperplane which linearly separates the samples according to their target values. There might be many hyperplanes satisfying the requirement if there exists one such hyperplane. For these hyperplanes, we define the margin of separation , denoted as R ,asthe separation between the hyperplane and the closest data points. The goal of SVM is to find the optimal hyperplane with the maximum margin of separation.
Here, we give some more details of SVM-based classification algorithms. SVM is a relatively new learning approach to solve two class pattern recognition prob-lems based on structural risk minimization principles. Figure 1 shows a very simple example of SVM classifier. Each circle in the figure represents a docu-ment in the feature space.

The algorithms to obtain the optimal hyperplane have been well studied, such as using quadratic programming technique, e.g. method of Lagrangian multipli-ers. We show one method as follows: Given: training sample ( d i ,y i ) i = 1 ... N, where d i , is a training document and y is the desired output, either 1 or -1.  X  The problem is to solve for w and b :  X  The decision formula is g ( d )= w T .d + b and the hyperplane for decision is 3.2 Model of SVM for E-Mail Classification The work on E-mail filtering can be mapped onto the framework of text classifi-cation. An E-mail message is regarded as a document. A judgment of whether it is interesting or not is viewed as a class label given to the E-mail document. The processing via the SVM involves three steps, namely pre-processing , training and testing , as shown in figure 2. The Feature Extraction box refers to pre-processing, which we will discuss shortly. For the training data, once we obtain the selected features, we feed them into the SVM and generate a classifier. The testing data are used to validate the efficiency of the SVM model. We adopt two classes of SVM algorithms, namely the C -support vector machine algorithm and the v -support vector machine algorithm, to handle the training problem. Details about these algorithms would be discussed shortly.

The reasons that we use SVM are described as following: (1) Firstly, when training E-mail classifiers, one has to deal with very many features. As SVM uses over-fitting protection, which does not necessarily depend on the number of features. It has the potential of hand ling these large features. (2) Secondly, there are few irrelevant features in E-mail classification. One way to avoid these high dimensional input spaces is to assume that most of the features are irrele-vant. The feature selection process tries to determine these irrelevant features. Unfortunately, in E-mail categorization, there are only very few irrelevant fea-tures. A good classifier should combine many features (learn a  X  X ense X  concept), while that an aggressive feature selection process may cause loss of information. (3) Thirdly, document vectors are usually sparse. For each E-mail document, the corresponding document vector contains only few entries that are non-zero. SVM is well suited for problems with dense concepts and sparse instances. (4) Finally, most text categorization problems are linearly separable, and the motivation of SVM is to find the optimal linear separators.

The dataset is divided into the following disjoint sets: 1. Training set: This dataset is used to train the SVM. In the pre-processing 2. Validation set: The error of the SVM output averaged over this dataset is 3. Testing set: After the validation process, the SVM classifier is applied to the Pre-processing of the Dataset. E-mail messages are semi-structured docu-ments that possess a set of structured fields with predefined semantics as well as a number of variable-length free text fields. The headers of a message are structured fields and usually contain information pertaining to the document, such as the sender, the date, the domain etc. The main contents of the message are variable length free text fields, such as the subject and the body. Both the structured fields and the free-text portion may contain important information for determining the class which a message belongs to. Therefore, an effective E-mail classifier should be able to collect features from both the structured fields and the free text. In our work, we generate two kinds of input features for each E-mail in the dataset. The features are as follows: 1. Structured features are features represented by structured fields in the header 2. Textual features. We use general text processing method to handle the textual Training of SVM. With the results of pre-processing, we perform the training of SVM and generate a classifier for E-mail filtering. A two-dimensional array of feature vector is obtained for interesting and uninteresting E-mail messages. The vector contains features extracted from both the message header and the body. In this work, we use the C -support vector machine and the v -support vector machine algorithms to learn the SVMs. These two methods are briefly described in the following.
 There are two class labels { X  1 , 1 } as the result space for the classification. Given L training vectors x i  X  R n ,where n is the dimensionality of the vector, i =1 ,...,L ,thereare L labeled training examples: ( x 1 ,y 1 ) ,..., ( x L ,y L ), where y  X  X  X  1 , 1 } . The primal problem of C -support vector algorithm is defined as: subject to following constraints:
Parameter C is used to penalize variable  X  i . However, it is often hard to select an appropriate C value. Sch  X  olkopf et al. proposed the v -support vector classifica-tion in [11]. They introduce a new parameter v which allows one to control the number of support vectors and errors. The primal problem of v -SVM is defined as: subject to following constraints: More specifically, it has been proved that parameter v is an upper bound of the fraction of the margin errors and a lower bound of the fraction of support vectors.
 Testing of SVM. In the testing stage, we will test the efficiency of the classifier. 1. Like the process in the training part, we generate a feature vector from the 2. We apply the classifier, which has been trained in the training stage, to the 3.3 Principal Component Analysis of Datasets We use the PCA method to accelerate the training process of the SVM. The purpose of using PCA is to reduce the dimensionality of the feature space while retaining the most feature information. Finding the principal components is basically a mathematical problem of finding the principal singular vectors of the input dataset using the singular value decomposition method.

In our approach, we use the PCA method as following: Firstly, in the training phase, we apply the principal component analysis on the training and validation dataset. We transform the training and va lidation dataset into the singular vector space and calculate the eigenvectors and eigenvalues for the covariance matrix of the dataset. The dataset for the new data space can then be produced by multiplying the eigenvector matrix with the original data. In the experiments, we select a number of the most principal components in the new data space as the dataset with reduced dimensionality. Secondly, in the testing phase, we transform the testing dataset into the same space as the training and validation data. This is done by simply multiplying it with the eigenvector matrix produced in the first step. In the experiments, we use the mySVM software tool for computation of the C -support vector machine and the v -support vector machine algorithms [9]. MySVM uses a novel decomposition algorithm that attains optimality by solving a sequence of much smaller sub-problems. It proved to be better to iteratively decompose the problem into a small working set S and minimize the target function on the working set only, keeping the other variables fixed [6]. All the experiments have been conducted on a SUN E450 machine with SUN OS 5.7. We have used a total of 2000 personal E-mails as the dataset for our experiments. We manually label each E-mail as interesting or uninteresting for the experiments. The number of messages that are interesting is 1500, while that of those uninteresting is 500. The whole dataset was split into three portions randomly for different purposes, i.e. training set, validation set, and testing set. The meanings of these datasets are self-explanatory.

We use recall and precision as the performance metrics of the classifier. Al-though the training stage is relatively more time-consuming, the testing stage is very efficient. The recall and precision for interesting E-mails are defined as: mails, N i is the total number of E-mails classified as interesting ,and N ii is the number of correctly-classified interesting E-mails. 4.1 Effect of Feature Selection As we described before, the E-mails have structural features and free-text content features. The subject of E-mail message usually contains much more information about whether the E-mail is interesting or not. Therefore, if subjects are given double weights, the result may be more accurate. The primary content features which appear at least in four messages have 2160 dimensions. In other words, there are totally 2160 word features. We can then obtain the following schemes (The following strings in the parentheses are the respective notations for them): 1. The scheme with only body and subject features (sub+con); 2. The scheme with structure features and E-mail body features (str+con); 3. The scheme with structure features, subject features and body features 4. The scheme with structure features, double weighted subject features and 5. The scheme with structure features, triple weighted subject features and the In each scheme, the dataset has two representations, i.e. TF-IDF and binary form. The results are shown in table 1 and plotted in figure 3.

From the results, we can see that the binary feature of the structure, sub-ject and body of E-mail has the best performance. From the results of scheme  X  X ub+con X  and  X  X tr+con X , we can see that the structured features contain less classification information than the subject features. However, comparing the re-sults of  X  X ub+con X ,  X  X tr+sub+con X , and  X  X tr+dousub+con X , we can assert that the structured features do contribute to improving the classification problem. It is important to note that the structured features and the subject features con-tain a lot of classification information, despite the fact that, when more weight is put on the subject features, the overall performance improves only marginally. 4.2 Effect of PCA We use the 2160 features of the dataset, and do the principal component analy-sis and generate the new feature space with different dimensions (PCs). The performance of the SVM after PCA is depicted in figure 4.

The PCA method allows us to select the most important features for the classification effectively and efficiently. From the results, we can see that the average precision and recall of E-mail classification are above 92% even for the first 28 features, compared with about 95% when the PCA method is not used, where 2160 features are used (as shown in table 1). Moreover, when the number of dimensions is small( &lt; 200), the performance after PCA processing is very stable. When the number of dimensions is 100, the result is near optimal, i.e. as good as full features used. Furthermore, the training time and space cost is only around 5% of the original method when PCA is not used. From these results, we can see that PCA could select a small set of features which can describe the whole features of the dataset. Therefore, the time and space costs could be reduced without compromisin g the performance. We also observe that adding many  X  X nimportant X  features to the SVM does not necessarily improve the performance of the classification, as the performance does not improve as the number of dimensions increases. 4.3 Results of the C -SVM In the C -support vector algorithm, parameter C affects the performance for the case when the training data is not linear separable by a linear SVM. In general, there should be an optimal value for this parameter. However, the optimal value of C cannot be obtained by examining the training data and it is unfair to do so by examining the test data. Only by using a validation set, would it be possible to optimize C . The experiment is conducted as follows: (1) First, use the default C , conduct the experiment to solve the optimal problem, and then find the performance on the validation set; (2) Pick another C and repeat step (1) until the performance on the validation set is optimal.

We use the dot and the radial kernel in this experiment. We also use binary feature vectors composed with structure features, subject features and body features. The result of the dot kernel algorithm is shown in table 2. When parameter C is decreased, the number of support vectors increases. MySVM provides the mechanism to search for the optimal C by adding or mul-tiplying some delta increase to the current C value. From the table we can see that when C is set bigger than 0.6, there is no improvement of the precision performance. Therefore, we can search for the optimal C -value between 6e-5 and 0.6 with pace 5 (by multiplication). The optimal C is 0.1875, with the respective performance results as follows: precision op = 96.83%, recall op = 95.60%.
Table 3 shows the results of the radial kernel algorithm. Same as the dot kernel algorithm, the number of bounded SVs increases as C decreases (the result is not presented here). However, when parameter C is increased, the precision and the number of support vectors both increases. This is different from the dot kernel support vector machine. When C is greater than 6, the performance improves little as C increases. From the three criter ia listed in the table, namely the precision ,the recall ,andthe number of support vectors ,wecanseewhen  X  is in [0.8, 1.0], the performance is optimal. We also note that the dot kernel algorithm is more suitable for E-mail classification compared to the radial kernel algorithm, and we only show the results of dot kernel algorithm in the following part of the paper. 4.4 Results of the v -SVM Parameter v controls the upper bounds of the fraction of errors and the lower bounds of the fraction of support vectors. An increase in v allows for more errors and wider margin. The larger the v value is selected, the more points are allowed to lie inside the margin. As for the kernel, we use the popular radial kernel function k ( x, y )= exp (  X   X  x  X  y 2 ), where  X  =1 . 0. The result is shown in table 4. Although the v -SVM was proposed to better control the number of support vectors and errors, it does not show better performance than C -SVM, i.e. the C -SVM algorithm is more suitable in E-mail classification scenario. 4.5 Comparison with Other Schemes We also compare our model with the Decision Tree [4], the Naive Bayesian Clas-sifier method [10], the Neural Network method [3] and the original SVM method [5]. Because of different feature selections, we only compare the optimal perfor-mance for the five methods. To clarify the presentation, we name our method as PSVM which adopts C -support vector algorithm, features selected from  X  X truc-tured fields+subject+body X , binary feature representation and enhanced with PCA.

Table 5 shows the performance comparison with other schemes, where the space is the feature size used for classification. We can see that PSVM method yields comparable precision and recall rate with NN and original SVM methods, but is more effective in terms of space and CPU cost. The reason is that our pro-posed PSVM method captures all the features including structure information, subject and body text. Additionally, the PCA transformation makes the input data more classifiable with fewer features. This paper models E-mail messages as a combination of structured features and textual features, which motivates the work of classifying such documents based on these features. We presented a SVM model which embeds PCA as a preproces-sor to E-mail classification. Different ways of feature selection for the model were also evaluated. Our study indicated that the classification process could be en-hanced by selecting features from both the structured part and the content part of the E-mails. The experiments showed that our SVM model provided good performance in filtering junk E-mails.

For future work, we plan to incorporate other techniques (e.g. E-mail address analysis, filthy word identification) into our method for better performance in classification. We can also expand the work from 2-class classification to multiple-class method, and further to a hierarchy of classes as well.

