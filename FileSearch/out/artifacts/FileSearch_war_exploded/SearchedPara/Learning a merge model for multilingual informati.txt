 1. Introduction
Multilingual information retrieval (MLIR) is usually carried out by first performing cross-language information retrieval the lists are merged to produce a multilingual result list. Because of various translation and retrieval qualities in different collections, how to merge a unique result list that includes more relevant documents from different collections has become one of the major issues in MLIR.

In the literature, most traditional merging methods for MLIR are heuristic approaches such as raw-score ( Savoy, Le Calve, normalized-by-topk merging ( Lin &amp; Chen, 2003 ). Based on a similar assumption that relevant documents are homogeneously distributed over monolingual result lists, these heuristic methods locally adjust or normalize the scores of retrieved docu-ments to produce a multilingual result list. However, if the assumption is invalidated, these heuristic methods have a large decrease of precision in the merging process. Instead of directly merging monolingual result lists, the 2-step merging strat-with respect to each query term to obtain a multilingual result list indirectly. Although performing well, 2-step merging is also seriously damaged by several factors such as the number of meaningful terms presenting in a collection and the pro-portion of relevant documents in a collection.
In this paper, we propose a learning approach for the MLIR merging process. To conduct this learning method, we first present a number of features that may influence MLIR merging. These features are mainly extracted from three levels: query, document, and translation. After the feature extraction, we use a learning-based ranking algorithm, FRank ( Tsai, Liu, Qin,
Chen, &amp; Ma, 2007 ), to construct a merge model from the extracted features. The merge model generated by FRank is then used to merge the monolingual result lists retrieved from different collections into a multilingual one. In our experiments, formance of our proposed method. For a comparison, several merging methods are also carried out, including the traditional merging methods, the 2-step merging strategy ( Mart X   X  nez-Santiago et al., 2006 ), and the merging method based on logistic regression ( Si &amp; Callan, 2005a ). The experimental results show that the proposed method significantly improves merging quality on two different types of datasets. In addition to the enhancement, via the generated merge model, we can also rec-ognize the key factors that influence MLIR merging.

This paper revisits and extends our preliminary research reported in Tsai, Wang, and Chen (2008) . In particular, this paper includes additional experiments that explore the performance of the proposed method to result merging under the condi-tions that the merging features are trained together with the retrieval features. These experiments are intended to address the criticism of why in the preliminary efforts, the retrieval model and merging model are treated as two separate models.
The results reported here demonstrate that this practice is reasonable because, if treated in one model, these two different types of features tend to interfere with each other. As a result, the performance would be deteriorated. Additionally, we also conduct experiments to further analyze the importance of the presented merging features. Via these experiments, we found that the results are consistent with those observed from the model learned by the FRank ranking algorithm, thereby support-ing that the merging model can be regarded as an auxiliary model to enhance merging quality in MLIR.

The main contribution of our work includes the development of a learning scheme for the MLIR merging process. To the best of our knowledge, this study is the first attempt to use a learning-based ranking algorithm to construct a merge model for the merging process. Under this scheme, several traditional methods can be regarded as special cases of our method. For example, if a merge model is a uniform model, then the corresponding merging process acts like raw-score merging. In addi-tion, our contribution also includes the presentation of a number of features possibly affecting MLIR merging, and the anal-ysis of important features via the merge model generated by FRank. This information might provide us more insight and understanding into MLIR merging.

The remainder of this paper is organized as follows. In Section 2 , we briefly review previous work on MLIR merging. Sec-tion 3 presents a number of features and describes the use of a learning-based ranking algorithm to construct a merge model.
Section 4 describes evaluation metric and the details of experimental datasets. We then report and discuss experimental re-sults in Section 5 . Additionally, Section 6 analyzes the features in depth by means of the model generated by the proposed method. We finally conclude our paper and provide several directions for future work in Section 7 . 2. Related work
In the literature, several heuristic methods for the MLIR merging process have been proposed. We below review these heuristic methods. Given several monolingual result lists, raw-score merging ( Savoy et al., 1997 ) directly sorts the original scores of retrieved documents to obtain a multilingual result list; round-robin merging ( Savoy et al., 1997 ) interleaves re-trieved documents by their ranks only to produce a multilingual result list. Another well-known merging approaches also include the ways of using normalization techniques on the scores of retrieved documents. The main idea behind the normal-ized score methods is to map the scores into the same scale for a reasonable comparison. Normalized-score merging ( Powell et al., 2000 ) uses the score of the top one document to normalize the other documents in the same list, and then sorts the normalized scores to obtain the final list. Instead of using the top one document, normalized-by-topk, uses the average score of the top k documents to produce a multilingual result list. Observed from our experiments, these traditional methods tend to have a similar performance because they are all based on a similar assumption that relevant documents are homoge-neously distributed over the monolingual result lists. In Moulinier and Molina-Salgado (2003) , moreover, a normalized col-lection score is proposed to adjust the similarity score between a document and a query. However, since the normalized collection score only reflects the relation between a translated query and a document collection, the method of normalized collection score may not work well if a query is not being translated very well.

Lin and Chen (2004) postulated the degree of translation ambiguity and the number of unknown words can be used to model the MLIR merging process. They presented the following formulas to predict the effectiveness of MLIR: where W i is the merging weight of query i in a CLIR run, T number of unknown words in query i ; n i is the number of query terms in query i , and c formulas.

Mart X   X  nez-Santiago et al. (2006) proposed a 2-step merging strategy to produce a multilingual result list. Instead of di-rectly merging the monolingual result lists into a multilingual one, 2-step merging uses re-indexing techniques on the doc-uments retrieved with respect to each query term, and then employs the re-indexed dataset to produce a multilingual result list. By the re-indexing techniques, 2-step merging can globally consider relevant terms within data collections. Although performing well, the 2-step merging strategy would seriously be damaged by some factors such as the number of meaningful terms presenting in a collection and the proportion of relevant documents in a collection.

The related work also includes the studies on collections fusion and search results merging in distributed information retrieval (DIR). Although DIR environments tend to be monolingual and uncooperative, some related techniques have been applied for MLIR merging with several degree of success. Si and Callan (2003, 2005b) proposed a semisupervised learning regression are proposed, these methods aiming to predict the probability of binary relevance (i.e., relevant and irrelevant) according to a set of independent variables. Although also based on learning techniques, these methods lack the consider-ation of dataset with multi-level relevance judgements. In this paper, therefore, we propose a novel merging method for
MLIR merging by using a learning-based ranking algorithm, in which multi-level relevance judgements can be considered. 3. Learning a merge model
Fig. 1 illustrates the framework for MLIR, in which each collection is a monolingual collection. This figure shows that MLIR is typically carried out by first performing CLIR on separate collections. Once a monolingual result list has been retrieved in each collection, all the lists are merged into a multilingual result list. Thus, the MLIR framework consists of two models: one is retrieval model for retrieving documents from each monolingual collection; another is merge model for merging all the monolingual result lists into a multilingual one.
 In the MLIR framework, the retrieval model is usually set as several standard IR techniques such as bm 25 ( Robertson &amp; for the retrieval model. As for the merge model, conventional merging methods are mostly based on some heuristics such as raw-score merging and round-robin merging. This paper attempts to propose a learning scheme to generate a merge model.
Under this learning scheme, several conventional merging methods can be regarded as special cases of our method. For example, if the generated merge model is a uniform model, then our method acts like raw-score merging. Furthermore, our method is similar to round-robin merging if the generated merge model predicts the merging weight of a document by means of its reciprocal rank.

Below, we describe how to use a ranking algorithm based on learning techniques to construct a merge model. We first present a number of features possibly affecting the MLIR merging process. Then, we describe how to build a merge model by the FRank ranking algorithm. Through the merge model generated by the learning-based ranking algorithm, we expect to identify critical features that really influence MLIR merging.
 3.1. Feature set
Table 1 lists the features used to construct a merge model in this study. There are 62 features extracted from three levels: query, document, and translation; moreover, all the features are represented by real numbers in our experiments. According to the extraction level, we describe these features in detail as follows.

On document level, only two features are extracted in our experiment. These two features are document length and title length; they represent the number of words in a document and in a document title, respectively. We consider these two fea-tures mainly because of their abilities of indicating the amount of information within a document. In this study, no retrieval feature is used to construct a merge model, although retrieval features (e.g., tf and idf ) can also be regarded as document-level features. Our experiments show that the retrieval features, if included, usually tend to dominate the generated merge model. As a result, this situation would lead to a difficulty in identifying the important features affecting MLIR merging. To emphasize the merging process, therefore, we use only document length (DLength) and title length (TLength) to construct a merge model.

For query-level feature extraction, we first manually classify the terms within a query into several pre-defined categories, and then extract query-level features according to these categories. In our experiments, we focus on proper names since they usually play an important role in retrieving documents. For a query, therefore, each query term is labeled as one of the fol-lowing categories:
In addition, we also use an additional category: named entity (NET), which contains all the query terms classified into the above categories. For those terms unable to be precisely labeled, we simply classify them into two categories: concrete nouns
Four query examples selected from our experimental datasets will be shown in Section 6.1 . These four examples consist of the query descriptions and query terms with the corresponding labels.

After the above labeling, we then extract query-level features from the labeled dataset. For a query, the feature set com-prises the number of query terms (#QT) and compound words (#CW). In addition, the feature set also consists of the number of the query terms classified into the pre-defined categories (e.g., #PPN and #Loc), and the corresponding percentage with respect to total query terms (e.g., %PPN and %Loc). These query-level features are extracted mainly because we consider dif-ferent types of query terms would influence the MLIR merging process differently. Through these features, therefore, we ex-pect to realize the relation between query difficulty and merging performance.

On translation level, we extract several features capable of indicating the translation quality of a query for a language. The translation-level features include the languages used in a query and in a document (i.e., QLanguage and DLanguage); the val-number of translation equivalents within a query (i.e., AvgTAD). For instance, for a language, if a query has two query terms both with three translation equivalents, then the value of AvgTAD of the query is  X  3  X  3  X  = 2  X  3.

Furthermore, the translation-level features also consist of the number of translatable query terms (i.e., #TQT), the number of translatable compound words (i.e., #TCW), and the corresponding ratios to total query terms (i.e., %TQT and %TCW). A query with more highly ambiguous query terms, whose number of translation equivalents P 3, usually tends to be not well-translated. Therefore, the translation-level features also include the number of highly ambiguous terms (i.e., #HAT) and the corresponding ratio to total query terms (i.e., %HAT). In addition to translatable query terms, we also consider the features of out of vocabulary terms, such as #OOV and %OOV. The idea of OOV features is also extended to the terms classified into various categories, thereby generating the features such as #OLoc and %OLoc. Using these translation-level features, we expect to realize the effect of translation quality to merging performance. In the next subsection, we describe a learning-based ranking algorithm to construct a merge model. Through the merge model generated by the ranking algo-rithm, we can realize the effects of these extracted features to MLIR merging. 3.2. The construction of a merge model
The FRank ranking algorithm ( Tsai et al., 2007 ) is adopted to construct a merge model in this paper. FRank is a learning-based ranking algorithm with a novel loss called fidelity loss based on RankNet X  X  probabilistic ranking framework ( Burges et al., 2005 ). Our experimental collection has four-level relevance judgements, including highly relevant (3), relevant (2), partially relevant (1), and irrelevant (0); therefore, merging on such a collection can also be regarded as a ranking applica-judgements. For the details of FRank, please refer to ( Tsai et al., 2007 ).
 According to FRank X  X  generalized additive model, a merge model can be represented as: where m t  X  x  X  is a weak learner, a t is the learned weight of m the merge model generated by FRank, we can examine the effect of each feature to MLIR merging by means of the learned weight a t . Upon completion of the merge model, we combine it with a retrieval model by using linear combination. In our experiments, the retrieval model is set as bm 25; then the proposed method can be represented as: where k is the combination coefficient of bm 25. In the above model, the number of selected weak learners t and the com-bination coefficient k are two tunable parameters that can be determined on a validation dataset.

In our method, the merge model M t  X  x  X  generated by FRank can be regarded as a supplementary model to enhance the merging quality of bm 25. This practice is consistent with the procedure of MLIR, in which a retrieval model is aimed at retrieving documents on each monolingual collection, and a merge model at merging all the monolingual result lists into a multilingual one. Moreover, this practice also provides us a way independent from the retrieval model to examine the crit-ical features influencing MLIR merging.

In addition to the separate approach, we also conduct an experiment that bm 25 is also a document-level feature for the construction of merge model. Note that, due to the non-linear feature transformation within FRank, this combined practice is different from the separate approach with a specific k . The aim of this experiment is to explore whether two different types and to investigate the difference between separate and combined models. 4. Data collections and evaluation metric
In our experiments, three CLIR test collections in NTCIR3, 4, and 5 ( Chen et al., 2003; Kishida et al., 2005, 2004 ) were used to build datasets for MLIR merging. These collections are mainly for the evaluation of CLIR performance on four languages:
Chinese (C), Japanese (J), Korean (K), and English (E). With respect to a query, documents within the collections are labeled with four-level relevance judgements, including highly relevant, relevant, partially relevant, and irrelevant. Because of lack of Korean resources, we only used CJE documents to build experimental datasets. Table 2 lists the details of the CLIR test collections.

We here describe how to build datasets for the MLIR merging experiments. For each collection, we used English topics as source queries to retrieve English, Chinese, and Japanese documents; that is, there were three retrieval processes: E X  X  mono-lingual retrieval, and E X  X  and E X  X  crosslingual ones. In addition, query terms were mainly composed of the terms in the con-cept field of a topic description. When translating an English query term for crosslingual retrieval, we chose two translation candidates with the highest frequency in the corresponding language corpus. For example, if an English query term has three
Chinese translation candidates, we select the top two candidates that appear frequently in Chinese corpus. The dictionary we used for translating English query terms contains 171,002 entries, and that for translating Japanese query terms contains 121,599 entries. After the E X  X , E X  X , and E X  X  retrieval processes, we then obtained three lists of monolingual results for a query. For each monolingual result list, we only used the documents whose similarity scores are not zero for the construction of our experimental datasets.

After obtaining the experimental datasets, we then conducted experiments on the datasets to merge the three monolin-gual result lists into a multilingual one. For a comparison, several merging methods were carried out in our experiments, including raw-score, round-robin, normalized-by-top1, normalized-by-top k , and 2-step merging. Among these methods, we refer to the raw-score, round-robin, normalized-by-top1, and normalized-by-top k merging methods as traditional ones.
Instead of directly merging the monolingual result lists, the 2-step merging strategy uses re-indexing techniques to indi-rectly obtain the multilingual result list; therefore, we regard this strategy as a variant merging approach. In addition to the above heuristic methods, two learning-based merging methods are also performed, including one based on logistic regression ( Si &amp; Callan, 2005a ) and ours based on the FRank ranking algorithm. The corresponding settings for these two learning-based methods are described in Section 5.1 .

Traditional IR measure, Mean Average Precision (MAP), was used as evaluation metric in our experiments. Given a result list for query q i , the average precision for q i can be defined as follows: cating whether the document at position j is relevant. Once all APs for all queries have been obtained, the MAP can be cal-of NTCIR X  X  CLIR task. 5. Experiments of learning a merging model 5.1. Experimental settings
Table 3 lists the percentage of retrieved relevant documents to total retrieved relevant documents in different experimen-tal datasets. Regarding the distribution of retrieved relevant documents, NTCIR3 and NTCIR5 are both more balanced than NTCIR4; in addition, as indicated in Table 2 , the number of articles in NTCIR5 is also more than those in NTCIR3 and NTCIR4. Due to the above reasons, the monolingual result lists in NTCIR5 were chosen as training dataset, and those in NTCIR3 and
NTCIR4 as two different types of testing datasets: NTCIR3 is a balanced one, and NTCIR4 is an imbalanced one. This practice assists us to further examine the performance of the comparison methods on two different types of datasets. For each testing datasets, we selected 10 queries as a validation to determine the parameters within the learning-based merging methods.
For the method based on logistic regression, we directly used the binary code of mySVM ( R X ping, 2000 ). The parameter c within mySVM was tuned on the validation datasets. According to the validation results, the parameters c were set to 2.5 for the NTCIR3 testing dataset and 2.0 for NTCIR4. For our method, we implemented the FRank ranking algorithm ( Tsai et al., 2007 ) to construct a merge model from the proposed 62 features, which were normalized to the values between 0 and 1 in the experiments. The parameters of our methods were also tuned on the validation datasets. According to the results, the number of weak learners t and the combination coefficient k were set to 7 and 0.07 for NTCIR3 as well as 13 and 0.01 for NTCIR4. In addition to the above separate models, namely that the proposed 62 features are trained separately from bm 25, we also conducted an experiment of bm 25 being trained together with the proposed features, which is referred to as combined model. 5.2. Experimental results
Table 4 lists the experimental results of all referenced methods on two different types of testing datasets. Numbers in the run of raw-score merging (baseline) on the same dataset at 95% confidence level. On the balanced NTCIR3 testing dataset, the performance of the traditional merging methods is similar to that of the baseline. In comparison, our method with sep-arate models and the 2-step merging strategy both significantly outperform the baseline. As observed from the table, how-ever, our method with combined model has no improvements with respect to the baseline; as to the merging method based on logistic regression, it can improve the merging quality, but its improvement fails to pass the significance test. On the unbalanced NTCIR4 testing dataset, in contrast to other merging methods, ours with separate models also significantly out-performs the baseline; this consequence arises mainly because our method can overcome the problem of unbalanced distri-bution by means of FRank X  X  ability of coordinating the proposed merging features. For our method with combined model, however, its performance is also under that of baseline mainly because the combined model mixes the retrieval model and the merging model as one single model, and thus causes these two different types of models interfere with each other.
For our method with separate models, we also conducted an experiment to further examine the effect of combination of our separate models with different k . Regardless of the performance on NTCIR3 or NTCIR4, our separate models with var-ious k is constantly above raw-score merging, except for k  X  0; this exception is due to the fact that, when k  X  0, our method uses only merge model to produce a multilingual result list, which is insufficient for such a retrieval task. Furthermore, as observed from the figure, the merging process of our separate-model approach is the same as raw-score merging when k  X  1. From this point of view, the proposed merge model can be considered an auxiliary model to enhance the merging quality of raw-score merging. 5.3. Discussions
According to different types of merging methods, we offer some observations and discussions about the experimental re-sults as follows: 6. Feature analysis 6.1. Feature analysis via learned merge model
Table 5 lists four query examples in the NTCIR4 testing dataset. Each query example in the table consists of query id, query description, labeled query terms, and the distribution of retrieved relevant documents. In addition, Table 6 lists the most effective features found in the first 100 iterations of FRank. According to their average learned weight, these features are selected, including the top seven positive ones and eight negative ones. Through these selected features, we attempt to find more clues about the MLIR merging process. According to feature extraction level, we provide an analysis on these se-lected features as follows: Document-level features:
Two document-level features, i.e., TLength (+) 1 and DLength ( ), are both selected as the most effective features in our experiments. Their weights indicate that with larger title length, a document has more merging weights; however, with larger document length, a document has fewer merging weights. This situation is similar to that of retrieving documents, in which document length is typically used to normalize the similarity score of a document. Therefore, we can consider that a document with larger document length usually tends to have more noise information for the processes of retrieving and merging documents.
 Query-level features:
As indicated in Table 6 , there are six query-level features selected as the most effective features in our experiments, including %AN (+), %TT (+), %Loc (+), %PPN (+), %EN ( ), and %Org ( ). This consequence occurs mainly because in our experimental datasets, a query with more abstract nouns, technical and location terms usually tends to be well-trans-lated; however, a query with more event and organization names tends to be poorly-translated. For the merging pro-cess, therefore, a well-translated query is supposed to have more merging weights; in contrast, a poorly-translated query has fewer merging weights. Take an example in Table 5 that the query terms in Qid 49, such as Habibie Admin-istration (AN) and Indonesia (Loc), are relatively easier to be translated by our dictionary than those terms in Qid 15, such as International Monetary Fund (Org) and foreign exchange crisis (EN); therefore, our merge model will assign more merging weights to the simple 2 Qid 49 than the difficult Qid 15. From this point of view, our merge model can also be considered a suitable model capable of discovering the relation between query difficulty and merging performance.
 Translation-level features:
As indicated in Table 6 , there are seven translation-level features selected in our experiments, including %TQT (+), #TCW (+), #HAT ( ), #OOrg ( ), #OTV ( ), #ONET ( ), and DLanguage ( ). These features indicate that for a language, a query with more translatable query terms or translatable compound words tends to have more merging weights; however, with more highly ambiguous terms or OOV terms as named entities, transitive verbs, and organization names, a query would have few-than those of other level features; therefore, we think the key factors influencing MLIR merging are translatable query terms, compound words, and the OOV terms as named entities, transitive verbs, and organization names. This information gives us a principled way of dealing with translation procedure while conducting a crosslingual retrieval. For example, we can con-centrate on the translation quality of those query terms as named entities when conducting a crosslingual retrieval. The neg-ative sign of document language (DLanguage) is due to its value settings, thereby indicating that our merge model prefers
English documents to Chinese and Japanese ones. 6.2. Experiments of feature analysis
We conduct a further experiment for investigating the effects of different types of query terms on the performance of mono-each term in horizontal axis stands for a type of query terms, and vertical axis represents the performance without the type of query terms. As indicated in the figure, the value of MAP with all of query terms is 0.261, and lacking a certain type of query terms will cause a loss on the retrieval performance. Below we offer some observations and discussions about this experiment.
The query terms with types of organization names (Org), personal names/alias (PN), event names (EN), and technical terms (TT) have the largest effects. In particular, MAPs of technical terms and personal names/alias decrease 22.72% and 16.09%, respectively. Take the following topics as examples:
The query terms of the type location/country name do not have large effects when they are removed in the following examples:
The above examples show that some types of query terms have strong relationship, i.e., one can cover another. We further remove two types of query terms together at one time and analyze their effects on performance. Table 7 shows the exper-iment results. We can find that the precisions of the combinations (personal names, technical terms), (event names, technical terms), (organization names, personal names), (personal names, event names), (organization names, abstract nouns), and (event names, transitive verbs) decrease more than those of other combinations. It means the pairs of types contain more
In summary, different named entities contribute different retrieval power. If they are missing due to query translation issue, the overall retrieval performance may be decreased accordingly. 7. Conclusions The contribution of this work includes the proposition of a learning approach for the MLIR merging process. We use the
FRank ranking algorithm to construct a merge model for merging monolingual result lists into a multilingual one. Experi-mental results demonstrate that, even performing on a dataset with the unbalanced distribution of relevant documents, the proposed merge model can significantly improve merging quality. Moreover, the contribution of this work also includes the presentation of several features affecting MLIR merging, and the feature analysis via the merge model generated by
FRank. In conclusion, the merge model indicates that for MLIR merging, the key factors are the number of translatable terms and compound words; in addition, the number of OOV terms as named entities, transitive verbs, and organization names also plays an important role in the merging process. This information provides us more insight and understanding into the MLIR merging process. Several research directions remain for future work: References
