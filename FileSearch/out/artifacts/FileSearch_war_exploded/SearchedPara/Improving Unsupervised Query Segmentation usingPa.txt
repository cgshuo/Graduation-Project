 We present a generic method for augmenting unsupervised query segmentation by incorporating Parts-of-Speech (POS) sequence information to detect meaningful but rare n -grams. Our initial experiments with an existing English POS tagger employing two different POS tagsets and an unsupervised POS induction technique specifically adapted for queries show that POS information can significantly improve query segmentation performance in all these cases.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation Algorithms, Measurement, Experimentation Query segmentation, POS tagging, IR evaluation
Query segmentation is the process of breaking down Web search queries into their constituent structural units, thereby aiding the retrieval process [8]. An example of such a seg-mented query is arthur conan doyle | short stories | buy online , where pipes ( | ) represent segment boundaries. Segmentation is considered as one of the first steps to query understanding, and has attracted a good amount of atten-tion [8, 9, 11]. Unsupervised query segmentation algorithms use statistical word association scores (WAS) [6] to mine potential segments from a corpus, and thus, suffer from the problem of missing out the rarer ones. In this research, we address this problem by applying a novel intuition that POS sequence patterns of frequent segments can be leveraged for extracting the rarer ones that are generally present in the Stanford Tagger uses the PTB tagset that has 36 tags 1 . Re-c ently, Petrov et al. [10] proposed a universal tagset (UTS) which contains 12 tags and provided a mapping between the PTB (and many other) tags and UTS tags. In order to understand the effect of granularity of the tagset, we also run our experiments for the UTS tagset, which are simply obtained by one-to-one mappings of the PTB tags of the queries labeled by the Stanford Tagger.

Since English Web search queries do not necessarily fol-low the syntax of the English language, the appropriate-ness of tagsets such as PTB or UTS for tagging Web search queries is questionable. Therefore, we also experiment with a completely unsupervised POS induction technique based on graph clustering [5] that induces the tagset as well as the tagger from first principles without making any assumptions about the syntactic structure of the language. Moreover, the technique automatically generates the number of tags (clus-ters). The original method is simplified for queries so as to assign a unique tag to a word (by suitably removing the Viterbi tagging step in [5]), irrespective of the context. This ensures a fast and lightweight tagger that is suitable for a Web search setting. We refer to this tagger (and the associ-ated tagset) as Bie-S (S = Simplified). Table 1 shows parts of sample clusters generated by the Bie-S algorithm on our query log. As we can see, clusters are focused around topics like food, sports, governance, and video games. The method resulted in 405 distinct tags.
Intuition. Traditional unsupervised query segmentation algorithms use a WAS to build a lexicon of meaningful n -grams [9, 12], which is subsequently used to generate the most likely segmentation for a query. Such methods fail to identify rare word n -grams as potential segments. The rarer n -grams, nevertheless, almost always follow the same syn-tactic structure (or POS sequence pattern) as the frequent ones, and their rarity is by virtue of the rarity of the words rather than the underlying syntactic construction. This fun-damental observation led us to the intuition that the WAS http://bit.ly/JY5lwb
Table 2: IR performance with different tagsets. purpose: (a) the value of the function diminishes with each successive iteration, which is necessary because otherwise eventually all n -grams will enter the lexicon; (b) as i grows, this factor approaches unity, which ensures convergence; (c) this factor is proportional to the logarithm of count ( P j , i ), which is usually desirable because frequency distributions of n -grams typically follow power laws.
We use the state-of-the-art association score CSR [6] and the related query segmentation algorithm [9]. There are two important novelties in the overall method when applied to the context of queries: (a) A decision is made on the signif-icance of a word n -gram w only on the basis of the number of queries which contain all the terms of w , thus disallow-ing frequently misleading unigram statistics to interfere with the decision, and (b) the segmentation procedure is capable of segmenting queries using only a query log, not relying on any other external resource. In our approach, we do not use the initial lexicon L 0 to segment queries; rather we use the lexicon L  X  i where  X  i is the iteration at which convergence oc-curs. We refer to the segmentation produced using L 0 as the original segmentation Orig , over which we aim to improve.
Dataset. For training our system, we use a query log ( Q ) sampled from Bing Australia in May 2010, consisting of 16 . 7 M ( M = Million) queries (11 . 9 M distinct). This query log was POS tagged using the Stanford Tagger (using both PTB and UTS tags) as well as the Bie-S algorithm. For evaluating segmentations generated by our approach, we use the dataset released by Saha Roy et al. [11] containing 500 Web search queries 2 with associated URLs and relevance judgments (approx. 30 / query, 0  X  2 scale, average rating of three annotators). The queries in this dataset are slightly long (5  X  8 words) where segmentation is actually meaningful from an IR perspective. We use the commercially popular open source Apache Lucene 3 to search this collection [11]. Queries 1 to 250 have been used as the development set and 251 to 500 as the test set.

IR-based evaluation. We evaluate our approach us-ing the state-of-the-art IR-based segmentation evaluation framework [11]. The framework uses an oracle-based ap-proach to measure the IR potential of a segmentation algo-rithm. Several versions of a segmented query are generated by variously quoting the multi-word segments. The per-formance of the segmentation algorithm, for each query, is assumed to be the performance of the quoted version for the query that retrieves the best results. Table 2 reports http://bit.ly/ZS0ybI http://lucene.apache.org/core/ Tagset PTB UTS Bie-S Lexicon convergence iteration 30 70 30 S egmentation convergence iteration 80 70 90 Peak IR performance iteration 50 50 10 Optimum  X  100 10 1000
Table 5: Number of iterations and the optimal  X  . continue to change, albeit converging towards specific val-ues. Therefore, we explore an alternative convergence crite-rion, which is when the segmentation of the queries stabilize for our development set. Nevertheless, we observed that the segmentations so obtained does not necessarily lead to maximum IR performance (say, in terms of nDCG@10). In Table 5 we report the number of iterations required for these two types of convergence  X  lexicon and segmentation, and also the number of iterations after which peak IR perfor-mance was achieved. For all our experiments, the param-eter  X  was tuned on the development set using grid-search for maximizing nDCG@10, and the optimal values for each tagset are also reported in Table 5.

We observe that Bie-S, which is a deterministic and hence a fast approach, takes only 10 iterations to reach its peak IR performance that is comparable to the nDCG of other tagsets, whereas the other approaches take 50 rounds. This is definitely a big advantage for the unsupervised POS in-duction approach. For all the tagsets, the nDCG@10 at seg-mentation convergence is slightly less than the peak value, though this difference is not statistically significant.
Frequent POS patterns. The ten most frequent pat-terns in the lexicons for the PTB and the UTS tagsets turned out to be Nn Nn, Nn Nn Nn, Jj Nn Nn, Jj Nn, Nn Nns, Nn Nn Nns, Nn In Nn, Fw Fw, Jj Jj Nn, Jj Nn Nns , and Noun Noun, Noun Noun Noun, Adj Noun Noun, Adj Noun, Noun Adp Noun, Noun Verb, Noun Noun Verb, Verb Noun, Adj Adj Noun, Noun Verb Noun respectively. The Bie-S tags are system-generated and hence are not readily interpretable.
We have described some initial experiments for enhanc-ing unsupervised query segmentation using POS sequences, with promising results but scope for improvement. This study connects two orthogonal approaches to segmentation or chunking of text fragments  X  those that rely on purely statistical word association measures [8, 9] and those that try to incorporate linguistic information, used commonly for Natural Language (NL) chunking [1]. While POS tagging of queries have received attention recently [7], and has been used in supervised query segmentation for detecting English noun phrases [2, 3, 4], as far as we know this is the first work that applies POS tagging to unsupervised query seg-mentation, explores an unsupervised POS induction strat-egy for queries and proposes a method for combining POS sequence information with WAS. Moreover, unlike past ap-proaches where POS tags are used to identify NL phrases in the queries, our method employs POS sequence information to detect rare n -gram patterns that may or may not corre-spond to any NL construct, and in fact, can be quite unique to the structure of Web search queries.

