 The broad goal of supervised learning is to effectively lear n unknown functional dependencies be-examples. This paper is at the intersection of two key topics that arise in this context. Applications of multivariate regression models include ch emometrics, econometrics and computa-tional biology. Multivariate Regression may be viewed as th e classical precursor to many modern techniques in machine learning such as multi-task learning [15, 16, 1] and structured output pre-dependencies between output variables to learn joint model s that generalize better than those that treat outputs independently.
 The second topic is that of sparsity [3], variable selection and the broader notion of regulariza-problems where the number of input variables may exceed the n umber of examples, the only hope cies being explored by the learning algorithm. This capacit y control may be implemented in various Estimation of sparse models that are supported on a small set of input variables is a highly active and very successful strand of research in machine learning. It encompasses l Lasso [19]) and matching pursuit techniques [13] which come with theoretical guarantees on the groups of features. For instance, Group Lasso [23] extends L asso, and Group-OMP [12, 9] extends matching pursuit techniques to this setting.
 In view of these two topics, we consider here very high dimens ional problems involving a large number of output variables. We address the problem of enforc ing sparsity via variable selection in multivariate linear models where regularization become s crucial since the number of parameters grows not only with the data dimensionality but also the numb er of outputs. Our approach is guided exhibit weak correlation with each individual output. It is also desirable to leverage information variables. In the presence of noisy data, inclusion decisio ns made at the group level may be more robust than those at the level of individual variables.
 To efficiently satisfy the above desiderata, we propose Multivariate Group Orthogonal Matching Pursuit (MGOMP) for enforcing arbitrary block sparsity patterns in multivariate regression coef-particular, MGOMP can handle cases where the set of relevant features may differ from one re-(e.g. S-OMP of [21]), as simultaneity of the selection in MGO MP is enforced within groups of related output variables rather than the entire set of outpu ts. MGOMP also generalizes the Group-technique.
 We then focus on applying MGOMP to high-dimensional multivariate time series analysis prob-selection, namely that of inferring key influencers in onlin e social communities, a problem of in-creasing importance with the rise of planetary scale web 2.0 platforms such as Facebook, Twitter, and innumerable discussion forums and blog sites. We rigoro usly map this problem to that of infer-ring causal influence relationships. Using special cases of MGOMP, we extend the classical notion tent of a community of bloggers. The sparsity structure of th e resulting model induces a weighted causal graph that encodes influence relationships. While we use blog communities to concretize the application of our models, our ideas hold more generally to a wider class of spatio temporal Empirical results on a diverse collection of real-world key influencer problems clearly show the value of our models. Let us begin by recalling the multivariate regression model , Y = X  X  A + E , where Y  X  R n  X  K is the output matrix formed by n training examples on K output variables, X  X  R n  X  p is the data matrix whose rows are p -dimensional feature vectors for the n training examples,  X  A is the p  X  K matrix formed by the true regression coefficients one wishes to estimate, and E is the n  X  K error matrix. The row vectors of E , are assumed to be independently sampled from N (0 ,  X  ) where  X  is the K  X  K error covariance matrix. For simplicity of notation we assu me without loss of generality that the columns of X and Y have been centered so we need not deal with intercept terms. The negative log-likelihood function (up to a constant) cor responding to the aforementioned model can be expressed as concatenation of the OLS estimates for each of the K outputs taken separately, irrespective of  X  . This suggests its suboptimality as the relatedness of the re sponses is disregarded. Also the OLS estimator is known to perform poorly in the case of high dimen sional predictors and/or when the that are based on dimension reduction. Among those, variabl e selection methods are most popular presence of high dimensional feature vectors as well as poss ibly a large number of responses. able selection by imposing a sparsity structure on A . Let I = { I by L (possibly overlapping) groups of input variables where I ables where O muted so that groups go over contiguous indices. We now outli ne a novel algorithm, Multivariate Group Orthogonal Matching Pursuit (MGOMP), that seeks to minimize the negative log-likelihoo d 2.1 Multivariate Group Orthogonal Matching Pursuit The MGOMP procedure performs greedy pursuit with respect to the loss function the sample estimate using residual error obtained from runn ing univariate Group-OMP for each response individually. In addition to leveraging the group ing information via block sparsity con-straints, MGOMP is able to incorporate additional informat ion on the relatedness among output objective tr ( Y  X  XA ) T ( Y  X  XA ) , thereby implicitly assuming that  X  = I .
 Before outlining the details of MGOMP, we first need to introd uce some notation. For any set of output variables O  X  { 1 , . . . , K } , denote by C C to columns corresponding to the output variables in O, and by C columns and rows. For any set of input variables I  X  { 1 , . . . , p } , denote by X X to columns corresponding to the input variables in I. Furthermore, to simplify the exposition, we assume in the remainder of the paper that for each group of i nput variables I orthonormalized, i.e., X The MGOMP procedure iterates between two steps : (a) Block Va riable Selection and (b) Coefficient matrix re-estimation with selected block. We now outline th e details of these two steps. Block Variable Selection : In this step, each block, ( I block ( I Note that when the minimum attained falls below , the algorithm is stopped. Using standard Linear Algebra, the block variable selection criteria simplifies t o count in the block selection process.
 Coefficient Re-estimation: the selected block of variables ( I regression coefficient matrix is then re-estimated as A ( m ) =  X  A C comes into play, and the problem can not be decoupled. Howeve r, a closed form solution for (4) can be derived by recalling the following matrix identities [8], where vec denotes the matrix vectorization,  X  the Kronecker product, and I the identity matrix. From (5), we have For a set of selected blocks, say M , denote by O ( M ) the union of the output groups in M . Let  X   X  and (6) one can show that the non-zero entries of vec (  X  A form formula for the coefficient re-estimation step.
 To conclude this section, we note that we could also consider preforming alternate optimization of the objective in (1) over A and  X  , using MGOMP to optimize over A for a fixed estimate of  X  , and using a covariance estimation algorithm (e.g. Graphical La sso [5]) to estimate  X  with fixed A . 2.2 Theoretical Performance Guarantees for MGOMP In this section we show that under certain conditions MGOMP can identify the correct blocks of variables and provide an upperbound on the maximum absolute difference between the estimated agreement with the specification of the output groups, namel y that C different output groups.
 For each output variable group O in the true model for the regressions in O not included. Similarly denote by M groups included in the true model, and M Before we can state the theorem, we need to define the paramete rs that are key in the conditions for consistency. Let  X  namely  X  X For each output group O { v Then we define  X  notes the Moore-Penrose pseudoinverse of X . We are now able to state the consistency theorem. Theorem 1. Assume that  X  such that &gt; 1  X  p (2 ln(2 |M good | / X  )) / X  X ( M good ) .
 Proof. The multivariate regression model Y = X  X  A + E can be rewritten in an equiva-lent univariate form with white noise:  X  Y = ( I applying the MGOMP procedure is equivalent to applying the G roup-OMP procedure [12] to the output groups originally considered for MGOMP. The theorem then follows from Theorem 3 in [12]  X  groups, the entries in  X  Y do not mix components of Y from different output groups and hence the error covariance matrix does not appear in the consistency c onditions.
 regression coefficient: min MGOMP compared to Group-OMP on individual regressions. Int uitively, through MGOMP we are combining information from multiple regressions, thus improving our capability to identify the correct groups. 2.3 Simulation Results We empirically evaluate the performance of our method again st representative variable selection selection accuracy we use the F precision and R denotes the recall. To compute the variable group F ods, we consider the  X  X oldout validated X  estimates. Namely , we select the iteration number that minimizes the average squared error on a validation set. For univariate methods, we consider indi-each setting, we ran 50 runs, each with 50 observations for tr aining, 50 for validation and 50 for testing.
 We consider an n  X  p predictor matrix X , where the rows are generated independently accord-ing to N N
K (0 ,  X  ) , der polynomial expansion: [ Y X ( X i,j ) q . T 1 , . . . , T M are the target groups. For each k , each row of [ A 1 ,T zero entry of A degree polynomial expansion. The number of regressions is s et to 60. We consider 20 regression groups ( T Table 2: Average F by variants of MGOMP, Group-OMP and OMP under the settings of Table 1.
 A dictionary of various matching pursuit methods and their c orresponding parameters is provided in Table 1. In the table, note that MGOMP(Parallel) consists in running MGOMP separately for each regression group and C set to identity (Using C estimated from univariate OMP fits has negligible Overall, in all the settings considered, MGOMP is superior b oth in terms of prediction and vari-able selection accuracy, and more so when the correlation be tween responses increases. Note that MGOMP is stable with respect to the choice of the precision ma trix estimate. Indeed the advantage of MGOMP persists under imperfect estimates (Identity and s ample estimate from univariate OMP for MGOMP, which has only one stopping point (MGOMP has one pa th interleaving input variables for various regressions, while GOMP and OMP have K paths, one path per univariate regression). 3.1 Model Formulation We begin by motivating our main application. The emergence o f the web2.0 phenomenon has set in platforms such as blogs, twitter accounts and online discus sion sites are large-scale forums where structured user-generated web content presents new challe nges to both consumers and companies munity opinion as a whole? How can a company identify blogger s whose commentary can change The problem of finding key influencers and authorities in onli ne communities is central to any vi-approach to this problem would treat it no different from the problem of ranking web-pages in a hyperlinked environment. Seminal ideas such as the PageRan k [17] and Hubs-and-Authorities [11] web. However, the mechanics of opinion exchange and adoptio n makes the problem of inferring authority and influence in social media settings somewhat di fferent from the problem of ranking generic web-pages. Consider the following example that typ ifies the process of opinion adoption. A consumer is looking to buy a laptop. She initiates a web searc h for the laptop model and browses several discussion and blog sites where that model has been r eviewed. The reviews bring to her the laptop and in a few days herself blogs about it. Arguably, conditional on being made aware of speaker quality in the reviews she had read, she is more likel y to herself comment on that aspect We formulate these intuitions rigorously in terms of the not ion of Granger Causality [7] and then employ MGOMP for its implementation. For scalability, we wo rk with MGOMP (Parallel), see table 1. Introduced by the Nobel prize winning economist, Cl ive Granger, this notion has proven beyond what can be predicted based on the past values of Y alone.
 Let B dictionary of K words and the time-stamp of each blog post, we record w k,t k th word for blogger B i = [ w (1  X  i  X  G ) , where T is the timespan of our analysis. Our key intuition is that aut horities and influencers are causal drivers of future discussions and opinions in the community. This may be phrased in the following terms: Granger Causality: A collection of bloggers is said to influence Blogger B content (blog posts) is predictive of the future content of B logger B and more so than the past content of Blogger B [ B 1 , . . . , B influences blogger B method for the responses concerning blogger B plication of a Granger test on B graph over bloggers, which we call causal graph, where edge weights are derived from the underly-ing regression coefficients. We refer to influence measures o n causal graphs as GrangerRanks . For example, GrangerPageRank refers to applying pagerank on th e causal graph while GrangerOutDe-gree refers to computing out-degrees of nodes as a measure of causal influence. 3.2 Application: Causal Influence in Online Social Communit ies Proof of concept: Key Influencers in Theoretical Physics : Drawn from a KDD Cup 2003 task, total, the data spans 137 months. We created document term ma trices using standard text processing techniques, over a vocabulary of 463 words chosen by running an unsupervised topic model. For each of the 9200 authors, we created a word-time matrix of siz e 463x137, which is the usage of the topic-specific key words across time. We considered one year , i.e., d = 12 months as maximum time lag. Our model produces the causal graph shown in Figure 1 showing influence relationships cording to GrangerOutDegree (also marked on the graph), Gra ngerPagerRank and Citation Count. The model correctly identifies several leading figures such a s Edward Witten, Cumrun Vafa as au-measure of authority given disciplined scholarly practice of citing prior related work. Thus, we consider citation-count based ranking as the  X  X round truth  X . We also find that GrangerPageRank and GrangerOutDegree have high positive rank correlation w ith citation counts (0.728 and 0.384 respectively). This experiment confirms that our model agre es with how this community recognizes its authorities.
Figure 1: Causal Graph and top authors in High-Energy Physic s according to various measures. Real application: IBM Lotus Bloggers : We crawled blogs pertaining to the IBM Lotus software time period of 376 days. We considered one week i.e., d = 7 days as maximum time lag in this ap-most significant causal relationships between bloggers, ou r causal graphs allow clearer inspection of the authorities and also appear to better expose striking sub-community structures in this blog community. We also computed the correlation between PageRa nk and Outdegrees computed over our causal graph and the hyperlink graph (0.44 and 0.65 respe ctively). We observe positive corre-by domain experts. structure is induced by groupings defined over both input and output variables. We have shown that extended notions of Granger Causality for causal inference over high-dimensional time series can problem of identifying key influencers in online communitie s, leading to a new family of influence measures called GrangerRanks. We list several directions o f interest for future work: optimizing not only between bloggers but also between communities of bl oggers; incorporating the hyperlink graph in the causal modeling; adapting our approach to produ ce topic specific rankings; developing online learning versions; and conducting further empirica l studies on the properties of the causal graph in various applications of multivariate regression.
 Acknowledgments We would like to thank Naoki Abe, Rick Lawrence, Estepan Meli ksetian, Prem Melville and Grze-gorz Swirszcz for their contributions to this work in a varie ty of ways. [1] Andreas Argyriou, Theodoros Evgeniou, and Massimilian o Pontil. Convex multi-task feature [2] Leo Breiman and Jerome H Friedman. Predicting multivari ate responses in multiple linear [3] M. Elad. Sparse and Redundant Representations: From The ory to Applications in Signal and [4] Ildiko E. Frank and Jerome H. Friedman. A statistical vie w of some chemometrics regression [6] M. Gomez-Rodriguez and J. Leskovec and A. Krause. Inferr ing Networks of Diffusion and [7] C. Granger. Testing for causality: A personal viewpoint . Journal of Economic Dynamics and [8] D. Harville. Matrix Algebra from a Statistician X  X  Perspective . Springer, 1997. [9] J. Huang, T. Zhang, and D. Metaxas D. Learning with struct ured sparsity, ICML 2009. [10] T. Joachims. Structured output prediction with suppor t vector machines. In Joint IAPR In-[11] Jon M. Kleinberg. Authoritative sources in a hyperlink ed environment. Journal of the ACM , [12] A.C. Lozano, G. Swirszcz, and N. Abe. Grouped orthogona l matching pursuit for variable [13] S. Mallat and Z. Zhang. Matching pursuits with time-fre quency dictionaries. IEEE Transac-[17] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagera nk citation ranking: Bringing order [20] A.N. Tikhonov. Regularization of incorrectly posed pr oblems. Sov. Math. Dokl , 4:16241627, [21] J.A. Tropp, A.C. Gilbert, and M.J. Strauss. Algorithms for simultaneous sparse approximation: [23] M. Yuan and Y. Lin. Model selection and estimation in reg ression with grouped variables. [24] Ming Yuan, Ali Ekici, Zhaosong Lu, and Renato Monteiro. Dimension reduction and coef-
