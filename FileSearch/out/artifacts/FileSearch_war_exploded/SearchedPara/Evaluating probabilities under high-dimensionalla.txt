 Latent variable models capture underlying structure in data by explaining observations as part of a more complex, partially observed system. A large number of probabilistic latent variable models have been developed, most of which express a joint distribution P ( v , h ) over observed quantities v and their unobserved counterparts h . Although it is by no means the only way to evaluate a model, a natural question to ask is  X  X hat probability P ( v ) is assigned to a test observation? X . In some models the latent variables associated with a test input can be easily summed out: P ( v ) = P h P ( v , h ) . As an example, standard mixture models have a single discrete mixture component of the latent variable.
 More complex graphical models explain data through the combination of many latent variables. This provides richer representations, but provides greater computational challenges. In particular, marginalizing out many latent variables can require complex integrals or exponentially large sums. One popular latent variable model, the Restricted Boltzmann Machine (RBM), is unusual in that the posterior over hiddens P ( h | v ) is fully-factored, which allows efficient evaluation of P ( v ) up to a constant. Almost all other latent variable models have posterior dependencies amongst latent variables, even if they are independent a priori.
 Our current work is motivated by recent work on evaluating RBMs and their generalization to Deep Belief Networks (DBNs) [1]. For both types of models, a single constant was accurately approxi-mated so that P ( v , h ) could be evaluated point-wise. For RBMs, the remaining sum over hidden variables was performed analytically. For DBNs, test probabilities were lower-bounded through a variational technique. Perhaps surprisingly, the bound was unable to reveal any significant im-provement over RBMs in an experiment on MNIST digits. It was unclear whether this was due to looseness of the bound, or to there being no difference in performance.
 A more accurate method for summing over latent variables would enable better and broader evalua-tion of DBNs. In section 2 we consider existing Monte Carlo methods. Some of them are certainly more accurate, but prohibitively expensive for evaluating large test sets. We then develop a new cheap Monte Carlo procedure for evaluating latent variable models in section 3. Like the variational method used previously, our method is unlikely to spuriously over-state test-set performance. Our presentation is for general latent variable models, however for a running example, we use DBNs (see section 4 and [2]). The benefits of our new approach are demonstrated in section 5. The probability of a data vector, P ( v ) , is the normalizing constant relating the posterior over hidden computing normalizing constants exists in physics, statistics and computer science. In principle, there are many methods that could be applied to evaluating the probability assigned to data by a latent variable model. We review a subset of these methods, with notation and intuitions that will help motivate and explain our new algorithm.
 In what follows, all auxiliary distributions Q and transition operators T are conditioned on the current test case v , this is not shown in the notation to reduce clutter. Further, all of these methods assume that we can evaluate P ( h , v ) . Graphical models with undirected connections will require the separate estimation of a single constant as in [1]. 2.1 Importance sampling Importance sampling can in principle find the normalizing constant of any distribution. The algo-rithm involves averaging a simple ratio under samples from some convenient tractable distribution over the hidden variables, Q ( h ) . Provided Q ( h ) 6 = 0 whenever P ( h , v ) 6 = 0 , we obtain: Importance sampling relies on the sampling distribution Q ( h ) being similar to the target distribution Finding a tractable Q ( h ) with small divergence is difficult in high-dimensional problems. 2.2 The Harmonic mean method harmonic mean method, also called the reciprocal method, gives an unbiased estimate of 1 /P ( v ) : In practice correlated samples from MCMC are used; then the estimator is asymptotically unbiased. It was clear from the original paper and its discussion that the harmonic mean estimator can behave very poorly [4]. Samples in the tails of the posterior have large weights, which makes it easy to construct distributions where the estimator has infinite variance. A finite set of samples will rarely include any extremely large weights, so the estimator X  X  empirical variance can be misleadingly low. In many problems, the estimate of 1 /P ( v ) will be an underestimate with high probability. That is, the method will overestimate P ( v ) and often give no indication that it has done so. Sometimes the estimator will have manageable variance. Also, more expensive versions of the estimator exist with lower variance. However, it is still prone to overestimate test probabili-ties: If 1 /  X  P HME ( v ) is the Harmonic Mean Estimator in (2), Jensen X  X  inequality gives P ( v ) = 1 E 1 /  X  P HME ( v )  X  E  X  P HME ( v ) . Similarly log P ( v ) will be overestimated in expectation. Hence the average of a large number of test log probabilities is highly likely to be an overestimate. Despite these problems the estimator has received significant attention in statistics, and has been used for evaluating latent variable models in recent machine learning literature [5, 6]. This is under-standable: all of the existing, more accurate methods are harder to implement and take considerably longer to run. In this paper we propose a method that is nearly as easy to use as the harmonic mean method, but with better properties. 2.3 Importance sampling based on Markov chains Paradoxically, introducing auxiliary variables and making a distribution much higher-dimensional than it was before, can help find an approximating Q distribution that closely matches the target distribution. As an example we give a partial review of Annealed Importance Sampling (AIS) [7], a special case of a larger family of Sequential Monte Carlo (SMC) methods (see, e.g., [8]). Some of this theory will be needed in the new method we present in section 3.
 Annealing algorithms start with a sample from some tractable distribution P 1 . Steps are taken with a series of operators T 2 ,T 3 ,...,T S , whose stationary distributions, P s , are  X  X ooled X  towards the To compute importance weights, we need to define a  X  X arget X  distribution on the same state-space: izing constant. The e T operators are the reverse operators , of those used to define Q AIS . For any transition operator T that leaves a distribution P ( h | v ) stationary, there is a unique corre-sponding  X  X everse operator X  e T , which is defined for any point h 0 in the support of P : The sum in the denominator is known because T leaves the posterior stationary. Operators that are their own reverse operator are said to satisfy  X  X etailed balance X  and are also known as  X  X e-versible X . Many transition operators used in practice, such as Metropolis X  X astings, are reversible. Non-reversible operators are usually composed from a sequence of reversible operations, such as the component updates in a Gibbs sampler. The reverse of these (so-called) non-reversible operators is constructed from the same reversible base operations, but applied in reverse order.
 The definitions above allow us to write: We can usually evaluate the P  X  s , which are unnormalized versions of the stationary distributions of the Markov chain operators. Therefore the AIS importance weight w ( H ) = 1 / [  X  X  X  ] is tractable as long as we can evaluate P ( h , v ) . The AIS importance weight provides an unbiased estimate: As with standard importance sampling, the variance of the estimator depends on a divergence be-tween P AIS and Q AIS . This can be made small, at large computational expense, by using hundreds or thousands of steps S , allowing the neighboring intermediate distributions P s ( h ) to be close. 2.4 Chib-style estimators Bayes rule implies that for any special hidden state h  X  , P ( v ) = P ( h  X  , v ) /P ( h  X  | v ) . (8) This trivial identity suggests a family of estimators introduced by Chib [9]. First, we choose a generated by a Markov chain that explores the posterior distribution P ( h | v ) . The most naive esti-Obviously this estimator is impractical as it equals zero with high probability when applied to high-dimensional problems. A  X  X ao X  X lackwellized X  version of this estimator,  X  p ( H ) , replaces the indi-cator function with the probability of transitioning from h ( s ) to the special state under a Markov chain transition operator that leaves the posterior stationary. This can be derived directly from the operator X  X  stationary condition:
P ( h  X  | v ) = X where P ( H ) is the joint distribution arising from S steps of a Markov chain. If the chain has stationary distribution P ( h | v ) and could be initialized at equilibrium so that is achieved asymptotically and the estimator is consistent regardless of how it is initialized. If T is a Gibbs sampling transition operator, the only way of moving from h to h  X  is to draw each element of h  X  in turn. If updates are made in index order from 1 to M , the move has probability: Equations (9, 11) have been used in schemes for monitoring the convergence of Gibbs samplers [10]. It is worth emphasizing that we have only outlined the simplest possible scheme inspired by Chib X  X  general approach. For some Markov chains, there are technical problems with the above construc-tion, which require an extension explained in the appendix. Moreover the approach above is not what Chib recommended. In fact, [11] explicitly favors a more elaborate procedure involving sampling from a sequence of distributions. This opens up the possibility of many sophisticated developments, e.g. [12, 13]. However, our focus in this work is on obtaining more useful results from simple cheap methods. There are also well-known problems with the Chib approach [14], to which we will return. We start with the simplest Chib-inspired estimator based on equations (8,9,11). Like many Markov chain Monte Carlo algorithms, (9) provides only (asymptotic) unbiasedness. For our purposes this is not sufficient. Jensen X  X  inequality tells us That is, we will overestimate the probability of a visible vector in expectation. Jensen X  X  inequality also says that we will overestimate log P ( v ) in expectation.
 Ideally we would like an accurate estimate of log P ( v ) . However, if we must suffer some bias, then a lower bound that does not overstate performance will usually be preferred. An underestimate of P ( v ) would result from overestimating P ( h  X  | v ) . The probability of the special state h  X  will often be overestimated in practice if we initialize our Markov chain at h  X  . There are, however, simple counter-examples where this does not happen. Instead we describe a construction based on a sequence of Markov steps starting at h  X  that does have the desired effect. We draw a state sequence from the following carefully designed distribution, using the algorithm in figure 1: a sample from an equilibrium sequence with distribution P ( H ) defined in (10). This can be checked by repeated substitution of (5). This allows us to express Q in terms of P , as we did for AIS:
Inputs: v , observed test vector 1. Draw s  X  Uniform ( { 1 ,...S } ) 3. for s 0 = ( s + 1) : S 5. for s 0 = ( s  X  1) :  X  1 : 1 7. P ( v )  X  P ( v , h  X  ) . The quantity in square brackets is the estimator for P ( h  X  | v ) given in (9). The expectation of the reciprocal of this quantity under draws from Q ( H ) is exactly the quantity needed to compute P ( v ) : Although we are using the simple estimator from (9), by drawing H from a carefully constructed Markov chain procedure, the estimator is now unbiased in P ( v ) . This is not an asymptotic result. As long as no division by zero has occurred in the above equations, the estimator is unbiased in P ( v ) for finite runs of the Markov chain. Jensen X  X  implies that log P ( v ) is underestimated in expectation. Neal noted that Chibs method will return incorrect answers in cases where the Markov chain does not mix well amongst modes [14]. Our new proposed method will suffer from the same problem. Even if no transition probabilities are exactly zero, unbiasedness does not exclude being on a particular side of the correct answer with very high probability. Poor mixing may cause P ( h  X  | v ) to be over-estimated with high probability, which would result in an underestimate of P ( v ) , i.e., an overly conservative estimate of test performance.
 The variance of the estimator is generally unknown, as it depends on the (generally unavailable) auto-covariance structure of the Markov chain. We can note one positive property: for the ideal Markov chain operator that mixes in one step, the estimator has zero variance and gives the correct answer immediately. Although this extreme will not actually occur, it does indicate that on easy problems, good answers can be returned more quickly than by AIS. In this section we provide a brief overview of Deep Belief Networks (DBNs), recently introduced by [2]. DBNs are probabilistic generative models, that can contain many layers of hidden variables. Each layer captures strong high-order correlations between the activities of hidden features in the layer below. The top two layers of the DBN model form a Restricted Boltzmann Machine (RBM) which is an undirected graphical model, but the lower layers form a directed generative model. The original paper introduced a greedy, layer-by-layer unsupervised learning algorithm that consists of learning a stack of RBMs one layer at a time.
 Consider a DBN model with two layers of hidden features. The model X  X  joint distribution is: by the second layer RBM. By explicitly summing out h 2 , we can easily evaluate an unnormalized Figure 2: AIS, our proposed estimator and a variational method were used to sum over the hidden states for obtained as a byproduct of the greedy learning procedure, and an AIS estimate of the model X  X  parti-tion function Z , [1] proposed obtaining an estimate of a variational lower bound: The entropy term H (  X  ) can be computed analytically, since Q is factorial, and the expectation term was estimated by a simple Monte Carlo approximation: Instead of the variational approach, we could also adopt AIS to estimate P h 1 P  X  ( v , h 1 ) . This would be computationally very expensive, since we would need to run AIS for each test case. In the next section we show that variational lower bounds can be quite loose. Running AIS on the entire test set, containing many thousands of test cases, is computationally too demanding. Our proposed estimator requires the same single AIS estimate of Z as the variational method, so that we can evaluate P ( v , h 1 ) . It then provides better estimates of log P ( v ) by approximately summing over h 1 for each test case in a reasonable amount of computer time. We present experimental results on two datasets: the MNIST digits and a dataset of image patches, extracted from images of natural scenes taken from the collection of Van Hateren (http://hlab.phys.rug.nl/imlib/). The MNIST dataset contains 60,000 training and 10,000 test im-ages of ten handwritten digits (0 to 9), with 28  X  28 pixels. The image dataset consisted of 130,000 training and 20,000 test 20  X  20 patches. The raw image intensities were preprocessed and whitened as described in [15]. Gibbs sampling was used as a Markov chain transition operator throughout. All log probabilities quoted use natural logarithms, giving values in nats. 5.1 MNIST digits In our first experiment we used a deep belief network (DBN) taken from [1]. The network had two hidden layers with 500 and 2000 hidden units, and was greedily trained by learning a stack of two RBMs one layer at a time. Each RBM was trained using the Contrastive Divergence (CD) learning rule. The estimate of the lower bound on the average test log probability, using (17), was  X  86 . 22 . To estimate how loose the variational bound is, we randomly sampled 50 test cases, 5 of each class, and ran AIS for each test case to estimate the true test log probability. Computationally, this is equivalent to estimating 50 additional partition functions. Figure 2, left panel, shows the results. The estimate of the variational bound was  X  87 . 05 per test case, whereas the estimate of the true test log probability using AIS was  X  85 . 20 . Our proposed estimator, averaged over 10 runs, provided an answer of  X  85 . 22 . The special state h  X  for each test example v was obtained by first sampling from the approximating distribution Q ( h | v ) , and then performing deterministic hill-climbing in log p ( v , h ) to get to a local mode.
 AIS used a hand-tuned temperature schedule designed to equalize the variance of the intermediate log weights [7]. We needed 10,000 intermediate distributions to get stable results, which took about 3.6 days on a Pentium Xeon 3.00GHz machine, whereas for our proposed estimator we only used S =40 , which took about 50 minutes. For a more direct comparison we tried giving AIS 50 minutes, which allows 100 temperatures. This run gave an estimate of  X  89 . 59 , which is lower than the lower bound and tells us nothing. Giving AIS ten times more time, 1000 temperatures, gave  X  86 . 05 . This is higher than the lower bound, but still worse than our estimator at S = 40 , or even S = 5 . Finally, using our proposed estimator, the average test log probability on the entire MNIST test data was  X  84 . 55 . The difference of about 2 nats shows that the variational bound in [1] was rather tight, although a very small improvement of the DBN over the RBM is now revealed. 5.2 Image Patches In our second experiment we trained a two-layer DBN model on the image patches of natural scenes. The first layer RBM had 2000 hidden units and 400 Gaussian visible units. The second layer repre-sented a semi-restricted Boltzmann machine (SRBM) with 500 hidden and 2000 visible units. The SRBM contained visible-to-visible connections, and was trained using Contrastive Divergence to-gether with mean-field. Details of training can be found in [15]. The overall DBN model can be viewed as a directed hierarchy of Markov random fields with hidden-to-hidden connections. To estimate the model X  X  partition function, we used AIS with 15,000 intermediate distributions and 100 annealing runs. The estimated lower bound on the average test log probability (see Eq. 17), using a factorial approximate posterior distribution Q ( h 1 | v ) , which we also get as a byproduct of the greedy learning algorithm, was  X  583 . 73 . The estimate of the true test log probability, using our proposed estimator, was  X  563 . 39 . In contrast to the model trained on MNIST, the difference of over 20 nats shows that, for model comparison purposes, the variational lower bound is quite loose. For comparison, we also trained square ICA and a mixture of factor analyzers (MFA) using code from [16, 17]. Square ICA achieves a test log probability of  X  551 . 14 , and MFA with 50 mixture components and a 30-dimensional latent space achieves  X  502 . 30 , clearly outperforming DBNs. Our new Monte Carlo procedure is formally unbiased in estimating P ( v ) . In practice it is likely to underestimate the (log-)probability of a test set. Although the algorithm involves Markov chains, importance sampling underlies the estimator. Therefore the methods discussed in [18] could be used to bound the probability of accidentally over-estimating a test set probability.
 In principle our procedure is a general technique for estimating normalizing constants. It would not always be appropriate however, as it would suffer the problems outlined in [14]. As an example our method will not succeed in estimating the global normalizing constant of an RBM.
 For our method to work well, a state drawn from e T ( h ( s )  X  h  X  ) should look like it could be part of an equilibrium sequence H  X  P ( H ) . The details of the algorithm arose by developing existing Monte Carlo estimators, but the starting state h ( s ) could be drawn from any arbitrary distribution: As before the reciprocal of the quantity in square brackets would give an estimate of P ( v ) . If an could perform better. We are hopeful that our method will be a natural next step in a variety of situations where improvements are sought over a deterministic approximation.
 Acknowledgments This research was supported by NSERC and CFI. Iain Murray was supported by the government of Canada. We thank Geoffrey Hinton and Radford Neal for useful discussions, Simon Osindero for providing preprocessed image patches of natural scenes, and the reviewers for useful comments. References There are technical difficulties with the original Chib-style approach applied to Metropolis X  X astings and continuous latent variables. The continuous version of equation (9), doesn X  X  work if T is the Metropolis X  X astings operator. The Dirac-delta function at h = h  X  contains a significant part of the integral, which is ignored by samples from P ( h | v ) with probability one. Following [11], the fix is to instead integrate over the generalized detailed balance relationship (5). Chib and Jeliazkov implicitly took out the h  X  = h point from all of their integrals. We do the same: The numerator can be estimated as before. As both integrals omit h = h  X  , the denominator is less than one when T contains a delta function. For Metropolis X  X astings: T ( h  X  h  X  ) = q ( h ; h  X  ) and averaging min(1 ,a ( h ; h  X  )) provides an estimate of the denominator. In our importance sampling approach there is no need to separately approximate an additional quan-a trivial underestimate P ( v )=0 . (Steps 3 X 6 need not be performed in this case.) On repeated runs, the average estimate is still unbiased, or an underestimate for chains that can X  X  mix. Alternatively,
