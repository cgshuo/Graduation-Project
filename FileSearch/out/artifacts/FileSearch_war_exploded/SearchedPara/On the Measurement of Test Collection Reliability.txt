 The reliability of a test collection is proportional to the number of queries it contains. But building a collection with many queries is expensive, so researchers have to find a balance between reliability and cost. Previous work on the measurement of test collection reliability relied on data-based approaches that contemplated random what if scenar-ios, and provided indicators such as swap rates and Kendall tau correlations. Generalizability Theory was proposed as an alternative founded on analysis of variance that provides reliability indicators based on statistical theory. However, these reliability indicators are hard to interpret in practice, because they do not correspond to well known indicators like Kendall tau correlation. We empirically established these relationships based on data from over 40 TREC collections, thus filling the gap in the practical interpretation of Gener-alizability Theory. We also review the computation of these indicators, and show that they are extremely dependent on the sample of systems and queries used, so much that the required number of queries to achieve a certain level of relia-bility can vary in orders of magnitude. We discuss the com-putation of confidence intervals for these statistics, providing a much more reliable tool to measure test collection reliabil-ity. Reflecting upon all these results, we review a wealth of TREC test collections, arguing that they are possibly not as reliable as generally accepted and that the common choice of 50 queries is insufficient even for stable rankings. H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation.
 Experimentation, Measurement, Reliability.
 Test Collection, Evaluation, Reliability, Generalizability Theory, TREC.

The purpose of evaluating the effectiveness of an Informa-tion Retrieval (IR) system is to assess how well it would sat-isfy real users. The main tool used in these evaluations are test collections, which comprise a collection of documents to search, a set of queries Q , and a set of relevance judg-ments that contains information as to what documents are relevant, and to which degree, to the queries [16]. Given the results returned by a system A for one of the queries q  X  Q , an effectiveness measure uses the information in the relevance judgments to compute a score  X  q, A that represents the effectiveness of the system for that query. After run-ning the system for all queries in the collection, the average  X 
Q , A = 1 |Q| P  X  i, A is usually reported as the main measure of system effectiveness, representing the expected behavior of the system for an arbitrary new query. When comparing two systems A and B , the main measure reported is the av-erage effectiveness difference  X   X  Q , AB =  X  Q , A  X   X  Q , B on this difference, we conclude which system is better.
The immediate question to ask is: how reliable are those conclusions about system effectiveness? Ideally, researchers would evaluate the system with the set of all possible queries that a user might request. In such a case, we could be sure that the true average performance of the system corresponds to the score we computed with the collection. The prob-lem is that building such a collection is either impractical for requiring an enormous amount of queries and relevance judgments, or just impossible if the potential query set is not defined, which use to be the case because we can not account for future queries that do not yet exist. Therefore, the query set Q in a test collection must be regarded as a sample from the universe of all queries, and the sample mean  X  Q , A as an estimate of the tru e effectiveness mean  X  But because we are estimating this score with a sample of queries, our estimates are erroneous to some degree. The results may change drastically with a different query set Q so much that differences between systems could be reversed.
An evaluation result is reliable if it can be replicated with another collection: if the set of queries Q suggests that sys-tem A outperforms system B , we can be very sure that the conclusion would hold for a different set of queries Q  X  , and in the end, for the universe of all queries. A simple way to make a collection reliable is to include many queries; the more we employ the smaller the variance of the estimates and thus the more reliable the conclusion. The problem is that more queries also means more cost to create the collection, so re-searchers have to find a balance between the reliability of the results and the cost of the collection. To this end, it is necessary to develop indicators of test collection reliability.
Several works in the last fifteen years have studied the pro blem of reliability in IR evaluation experiments. The ba-sic methodology consisted in evaluating a series of systems with two different and random sets of queries, computing several reliability indicators that measured how similar those evaluations were. Using different query sample sizes and ran-domizing query selection, researchers were able to map query set size to reliability and extrapolate results to larger query sets. The data used consisted in runs submitted to several TREC tracks (mostly the Ad Hoc tracks), and the sets of queries employed in each edition. While these approaches are clearly faithful to the data, they are limited in that the full query set had to be partitioned in two disjoint sets to comply with the assumption that they were independent.
In 2007 Bodoff and Li [6] proposed Generalizability The-ory (GT) as an alternative [7, 18]. GT is grounded on analy-sis of variance components, which allows to dissect the vari-ability in effectiveness scores and figure out how much of it is due to system differences, query difficulty, assessors, etc. In an ideal evaluation setting, we would like all variance to be due to actual differences between systems and not due to query variability; if the queries in the collection are too varied, or differences between systems too small, then we need many queries to ensure that our estimates are reliable. From these variance components GT allows researchers to estimate the reliability of a test collection even before it has been created. Based on some previous data, GT can es-timate the reliability of a collection with a larger number of queries, more than one assessor providing judgments for each query, etc. GT provides indicators for the stability of both the absolute scores and the relative differences by computing different variance ratios.

The main advantages of GT against the traditional data-based approaches are that 1) it is based on statistical theory, 2) it is easy to employ because it does not require tedious and repetitive what if scenarios, and 3) it allows us to es-timate the reliability of a collection or experimental design that does not exist yet. But it has disadvantages too: 1) it is unknown the extent to which reliability indicators are af-fected by the data used to estimate variance components, and 2) it is very hard to interpret them in practical terms.
We address these two problems of GT applied to the mea-surement of test collection reliability. In the next section we review past work following data-based approaches and the reliability indicators used. We then review the use of GT and discuss the motivation for this work. In Section 3 we show how the initial data used in GT studies has a very large effect on the results, discussing minimum sample sizes and interval estimators. Section 4 reports a study to provide an empirical mapping between GT-based indicators of reliabil-ity and the well known data-based ones. Next we discuss the reliability of several TREC collections based on the results from previous sections, presenting conclusions in Section 6.
Several indicators of test collection reliability have been proposed in the literature. This section reviews traditional indicators found in the early data-based studies and the GT-based indicators more recently proposed.
Given a query set Q and a similar set Q  X  of the same size, we can define the following data-based reliability indicators:
One of the first reliability studies was conducted in 1998 by Voorhees [20], who analyzed the effect of having differ-ent assessors provide relevance judgments. Employing a methodology based on randomization, she concluded that the absolute scores could suffer wide variations between as-sessors, but that the ranking of systems was seldom altered, establishing  X  = 0 . 9 as the de facto minimum on ranking similarity. She also studied swap rates as a function of  X   X  and suggested a minimum of 25 queries to have a somewhat stable ranking. Also in 1998, Zobel [24] studied the effect of pool depth on absolute system scores, extrapolating trends to larger pool depths. He also compared different statistical procedures in terms of power and conflict ratios.

Buckley and Voorhees [8] compared in 2000 the reliability of various effectiveness measures by mapping effectiveness differences to error rates. Extrapolating to 50 queries, they concluded that  X   X   X  0 . 05 produced less than 1.5% sys-tem s waps when computing Average Precision (AP), while other measures such as Precision at cutoff 10 (P@10) pro-duc ed 3.6% of swaps. In 2002, Voorhees and Buckley [22] extended their work with other collections and methods, but again extrapolating trends. They concluded that with 50 queries the sensitivity of AP was  X  a = 0 . 05, while increasing the query set size to 100 would yield  X  a = 0 . 03. They also reported large differences across collections and effectiveness measures. Lin and Hauptmann [13] showed that the empiri-cal model used by Voorhees and Buckley can be derived the-oretically, and that the three factors affecting reliability are query set size, mean effectiveness scores, and variability of scores. Sanderson and Zobel [17] also revisited this work by computing relative sensitivity and incorporating statistical procedures to account for score variability. They concluded  X  r = 10% with AP if coupled with statistical significance, and  X  r = 25% if not. They observed very similar relative sensitivity between AP and P@10, arguing the use of more queries with fewer judgments as previous work suggested that much of the score variability is due to queries [4].
In 2007 Sakai [15] used similar methods to compare the reliability of several effectiveness measures, though he did not extrapolate to larger query sets. He computed  X  cor-relations, absolute sensitivity  X  a and a variation of  X  r observed that these indicators were not very correlated with statistical significance, arguing the importance of consider-ing score variability rather than just means. Voorhees revis-ited in 2009 [21] the use of statistical procedures with the TREC Robust 2004 collection, computing reliability indi-cators with an unprecedented set of 100 queries, therefore avoiding the need to extrapolate to the usual size 50. When using AP, she observed power  X  = 47% and conflict ratios  X   X  = 2 . 7% and  X  + = 0 . 04%. She showed again that P@10 is less reliable than AP also in these terms; and that nDCG showed higher reliability (agreeing with Sakai [15]). She also found that minor conflicts were usually coupled with large relative differences, thus suggesting that researchers employ several large collections to draw general conclusions.
Bodoff and Li [6] proposed Generalizability Theory [7, 18] as an alternative to measure test collection reliability that directly addresses variability of scores rather than just the mean as was common before. GT has two stages: a Gener-alizability study (G-study) to estimate variance components based on previous data, and a Decision study (D-study) that subsequently computes reliability indicators for a different experimental design. We consider a fully crossed design and decompose variability of scores into three components: vari-ance due to actual differences among systems (  X  2 s ), vari-ance due to differences in difficulty among queries (  X  2 variance due to the system-query interaction effect whereby some systems are particularly good (or bad) for some queries (  X  s : q ). The variance due to other effects, such as assessors, is in our case confounded with the interaction effect.
Using Analysis of Variance (ANOVA) procedures, these variance components can be estimated from previous data: w here E M  X  is the expected Mean Square of component  X  , and n s and n q are the number of systems and queries [7, 18]. These estimates can be used to compute the proportion of total variance that is due to each of the effects, such as how much of it is due to actual differences between systems.
In the D-study, we can use the variance estimates from the G-study to compute the reliability of a larger query set. To this end, two reliability indicators are usually employed:
The main advantage of these indicators is that they allow us to estimate the reliability of an arbitrary query set size n so there is no need to follow the traditional methodologies based on random what if scenarios and extrapolation. From equations (4) and (5) it can be seen that the reliability of the collection increases as n  X  q increases, because the estimates of query difficulty (i.e. average system performance per query) are more precise. These indicators were used by Kanoulas and Aslam [12] to derive the gain and discount functions of nDCG that yield optimal reliability when n  X  q is constant.
With simple algebraic manipulation, we can calculate the minimum number of queries needed to reach some level of relative or absolute stability  X  : w hich can be used to estimate how many more queries we need to add to our collection for it to be reliable. The main use of this approach can be found in the TREC Million Query Track [2, 1], which set out to study whether many queries with a few judgments yield more reliable results than a few queries with many judgments. The conclusion was that n q  X  80 queries are sufficient for a reliable ranking, while n q  X  130 are needed for reliable absolute scores.
The two problems of GT can be clearly spotted at this point. First, equations (1) to (3) show that we do not com-pute the true  X  2  X  variance components, but just estimates  X   X   X  based on some previous data. If we use a different, yet similar set of systems or queries to estimate these variance components, the resulting E X   X  2 and  X   X  scores might be very different. In a revised paper, Bodoff [5,  X  4.6] briefly dis-cussed this issue and argued that differences are marginal. However, he reports the results when randomly selecting only one system per research group instead of all of them, and only one trial of such experiment. We argue that this situation is not representative because the full set of systems and the reduced set after removing runs by the same groups are actually very similar to begin with, so it is expected that reliability scores do not change much. Also, only one such randomly reduced set is compared, so there is really no ev-idence to support that claim. Likewise, he further suggests that as few as five queries or systems are often enough to provide stable estimates of the variance components in the G-study [5,  X  3.1]. We further analyze this issue in Section 3.
Second, equations (6) and (7) allow us to estimate the minimum number of queries n  X  q to reach some stability level  X  , but the greater question is: how much is stable enough? Bodoff [5] mentions that in most Social Science applications a stability coefficient of 0.8 is acceptable, but there is no similar standard for Engineering applications. Kanoulas and Aslam [12] set  X  = 0 . 95 as the target in their experiments, but this choice is arbitrary. In their analysis of the Mil-lion Query Track 2007 [2] and 2008 [1], Allan et al. [1] and Carterette et al. [9, 10] also set E  X  2 = 0 . 95 as the tar-get. They mention in a footnote that in their experiments E  X  2 = 0 . 95 approximately corresponded to  X  = 0 . 9, but details are omitted. We study this issue in Section 4 by empirically mapping GT-based indicators onto data-based indicators that are easier to understand and use in practice.
To measure the effect of the number of queries and number of systems used in the G-study to estimate variance compo-nents, we use data from 43 TREC collections covering 12 tasks across 10 tracks, from TREC 3 to TREC 2011 (see Table 1). As in previous studies [22, 17, 6, 21], we remove the bottom 25% of systems so that our results are not ob-scured by possibly buggy implementations. For each collec-tion, we randomly selected n q = 5 queries and computed the variance components using the full set of systems. We then calculated E  X  2 and  X  for the full query set size, and the required number of queries to reach 0.95 stability. This was repeated with increments in n q of 5 queries, up the maxi-mum permitted by the collection or 100. For each query set size, we ran 200 random trials, each of which can be con-sidered as the possible data available for a G-study when analyzing a test collection design. The same process was followed by varying the initial number of systems n s and using the full set of queries instead.

Figure 1 shows the variability in G-study results 1 . For each collection and initial number of queries used, the y-axis plots the length of the span covering 95% of the E X   X  2 and observations in the 200 random trials. The right hand side plots show the same span lengths, but for different number of systems used in the G-study. As expected, the queries have a larger effect. Most importantly, we see that the average span length with just 5 queries is about 0.5 across collec-tions. That is, the stability estimates could be as low as 0.3 or as high 0.8, for example, just depending on the particular set of queries we use in the G-study. In fact, estimates of the minimum number of queries required can vary in orders of magnitude if not using enough data. For example, with as many as 30 initial queries and all 184 systems from the Microblog 2011 collection, GT may suggest from 63 to 133 queries to reach E  X  2 = 0 . 95. Similarly, from 40 initial sys-tems and all 34 queries from the Medical 2011 collection, GT may suggest from 109 to 566 queries. In general, at least 50 queries and 50 systems seem necessary for 95% of estimates to be within a 0.1 span. This means that GT may be trusted to measure the reliability of an existing collection, but that
Given the amount of datapoints displayed in this paper, we recommend to access the full-color version available online. , n ( n researchers should be cautious when planning a collection based on the results of a handful of systems and queries.
These results clearly evidence the need for a measure of confidence on GT indicators. Bodoff [5] suggests the use of confidence intervals to account for this variability, but only computes them for the variance components in the G-study. Confidence intervals for the ultimately more useful D-study can be worked out from various variance ratios (see equa-tions (8) and (9) 2 ). Feldt [11] derived exact 100(1  X  2  X  )% confidence intervals for the ratio  X  =  X  2 s / X  2 e under the as-sumption of normally distributed scores. The confidence interval on E  X  2 ( n  X  q ) is computed using the endpoints in (8): Arteaga et al. [3] derived approximate 100(1  X  2  X  )% confi-dence intervals for the ratio  X  =  X  2 s /  X  2 s +  X  2 q +  X  assuming a normal distribution of scores. The confidence interval on  X  n  X  q is computed using the endpoints in (9):
Brennan [7,  X  6] discusses different methods to compute confidence intervals in both G-studies and D-studies, show-ing that the above intervals work reasonably well even when the normality assumption is violated. The right hand side of Table 1 reports the point and 95% interval estimates of the stability of the 43 TREC collections we consider in this paper. These intervals provide a more suitable estimate of test collection reliability because they account for variabil-ity in the G-study. For example, researchers could use these intervals to infer the required number of queries to reach the lower endpoint of the interval instead of the point estimate:
To empirically derive a mapping between GT-based and data-based reliability indicators, we again used the 43 TREC collections in Table 1. For each collection we proceeded as follows. Two random and disjoint query subsets of size n q = 10 were selected from the full set of queries; let these subsets be Q and Q  X  . The full set of systems was evaluated with both query subsets, and all data-based reliability indi-cators in Section 2.1 were computed, along with the two GT-based indicators according to Q and Q  X  . This was repeated
F  X  : df 1 ,df 2 is the quantile function of the F distribution with df 1 and df 2 degrees of freedom. In our fully crossed design, df s = n s  X  1, df q = n q  X  1, and df e = ( n s  X  1)( n q  X  1). with increments in n q of 10 queries, up to the maximum permitted by the collection. For query subset size we ran 50 random trials, each trial providing us with 32 datapoints (E X   X  2 and  X   X  according to Q and to Q  X  , mapped to  X   X  , X   X   X   X   X  ,  X   X  + ,  X   X  a ,  X   X  r and  X   X  ). Theoretically though, E  X  related to  X  ,  X  AP ,  X  ,  X   X  ,  X  + and  X  a because it measures the stability of relative differences, while  X  is better related to  X  and  X  because it measures the stability of absolute scores. We thus mapped only these combinations.

Figure 2 shows the mappings. For each collection we fitted a model with all available datapoints. However, we dropped points for which E X   X  2 &lt; 0 . 8 and  X   X  &lt; 0 . 5 so that the trends were not affected by mappings with such small stability to be even practical. These thresholds were chosen based on the observed stability of the 43 TREC collections; about 85% of them show larger stability scores (see Table 1). This resulted in over 28,000 points for each plot. In the top three plots (  X  ,  X  AP and  X  ) we fitted the model y = x a , where a is the parameter to fit. This resulted in the desired theoretical behavior that lim x  X  1 y = 1 and lim x  X  0 y = 0, that is, when all variability is due to system differences  X  should be 1 because the ranking cannot be altered, and if all variance is due to queries then  X  should be 0 because the rankings are completely random. Similarly, in the bottom four plots we fitted the model y = (1  X  x ) a , such that lim x  X  1 y = 0 and lim x  X  0 y = 1, that is,  X  should for example be 0 if there is no variability due to queries.
 As the first plot shows, all 43 collections do actually need E  X  2 &gt; 0 . 95 to reach  X  = 0 . 9. In general, E  X  2 = 0 . 95 cor-responds to  X   X  0 . 85, and on average E  X  2  X  0 . 97 is needed across collections to reach  X  = 0 . 9. The two clear exceptions are found in the Million Query Track. The 2008 collection is the one that reaches the target  X  = 0 . 9 with the lowest stability (E  X  2  X  0 . 93), while the 2007 collection needs the largest (E  X  2  X  0 . 98). Note that these were the two collec-tions for which the E  X  2 = 0 . 95  X   X  = 0 . 9 correspondence was established [1, 9, 10]. It should be noted here that these fits have an exponential-like shape, meaning that it is hard to achieve a mid level of  X  , but once E  X  2 is large enough small improvements in stability translate into large improve-ments in  X  . However, the relation between n  X  q and E  X  2 logarithmic-like shape, meaning that it is increasingly more expensive to improve E  X  2 to begin with. Thus, it should be considered the required effort for slight improvements in  X  .
The second plot shows quite high  X  AP scores at these lev-els of relative stability, but generally below  X  . This suggests that the swaps in the rankings are still happening between systems at the top of the rankings [23]. The third plot shows that at these stability levels it is expected to observe statis-tical significance in about 80% of system comparisons. In the middle right plot we can see that the proportion of con-flicting results is generally below the  X  = 0 . 05 significance level when E  X  2  X  0 . 9.
Researchers interested in the particular mapping for one of these collections may use the estimates in Table 1 and the plots in Figure 2 to get a better understanding of the evaluation results and draw more informed conclusions. To assess the reliability of future collections and guide in their development process, we fitted a single model using all avail-able data instead of one model per collection. Figure 3 shows these fits, along with 95% and 90% prediction inter-vals that theoretically cover 95% and 90% of all future ob-servations. In terms of sensitivity, the middle left plots show that  X  a  X  0 . 03 for E  X  2  X  0 . 9, which is about 60% of what Voorhees and Buckley reported for the Ad Hoc tracks [22]; although the intervals cover their values well. In the bottom left plot we see that  X  r  X  20% for  X   X  0 . 75, generally agree-ing with Sanderson and Zobel [17]. As to statistical signifi-cance, we replicated Voorhees X  X  [21] study with random sets of 50 queries from the Ad Hoc 7-8 topics and Robust 2004 systems. The average relative stability is E X   X  2  X  [0 . 81 , 0 . 88], which corresponds to  X   X  [37% , 54%],  X   X   X  [3 . 9% , 7 . 8%] and  X  +  X  [0 . 38% , 1 . 3%]. These are again larger than she reported, but the intervals cover her values well.
Overall, these models produce a decent fit on the data, and they fill the gap between data-based methodologies and Generalizability Theory. They provide a valuable tool to rapidly assess and easily understand the reliability of a test collection design. 90% (light grey) prediction intervals.
The last columns in Table 1 report point and 95% inter-val estimates of the stability of the 43 TREC collections we considered. Collections in the same group correspond to the same tasks, providing a historical perspective on the reliabil-ity of the collections used so far since 1994 and for a variety of tasks. For example, the average relative stability in the Ad Hoc collections was E  X  2  X  [0 . 86 , 0 . 93], which according to Figure 3 corresponds to  X   X  [0 . 65 , 0 . 81]. For the Web Ad Hoc collections we find E  X  2  X  [0 . 8 , 0 . 93], which would corre-spond to  X   X  [0 . 53 , 0 . 81]. There are large differences within some tasks, such as Web Distillation, Genomics, Terabyte and Enterprise. This is further evidence of the variability in D-study results due to the data used in the G-study. Except for a few particular cases though, the computation of con-fidence intervals smooths the problem. Across collections the averages are E  X  2 = 0 . 88 and  X  = 0 . 74, with some tasks having very low scores. According to Figure 3 the expected  X  correlation is 0.69 with variations from 0.49 to 0.95, that is, much lower than desired.

Figure 4 plots the historical trend of test collection relia-bility. The left plot shows that relative stability has varied in the (0.8,1) interval for the most part, but most importantly it suggests that the stability of collections has decreased very Table 1: Summary of all 43 TREC collections analyzed. Query sets with slightly with the years. The clear exceptions are again the Million Query Track collections, which specifically aimed at increasing the number of queries. Within each task it ap-pears that stability tended to decrease as the tasks got older despite that query set sizes were normally unaltered. The second plot shows that this decrease in stability could be due to system variance getting smaller with the years. That is, systems perform more similarly as the tasks get older, indicating that retrieval techniques are generally improved. The right plot shows that query difficulty also varied within tasks. Sudden peaks may be explained by changes in the document set or in the task definition. The general trend suggests that queries are getting more alike with the years, further contributing to the decrease in reliability.
Bodoff [5,  X  5] discusses the incorporation of the document set as another facet in Generalizability Theory, much like queries and systems, to measure variability due to docu-ments [14]. He argues that it does not make sense in gen-eral, because we do no assign performance scores for indi-vidual documents but for sets of documents (e.g. the first k retrieved when computing P @ k ). In our case we could com-pare different editions of the same task but with different document sets to get a (weak) clue of the variability due to documents. For example, the Ad Hoc task of the Web Track shows quite different stability scores in the first three edi-tions (WT2g and WT10g collections) compared to the last three editions (ClueWeb09), given that they all used the standard query set size of 50. Similarly, the Expert Search task in the Enterprise Track shows very different stability levels when using the W3C collection or the CERC collec-tion. We must bear in mind though that these differences might actually be due to the systems and queries used, which varied from year to year.

From the confidence intervals in Table 1, we used the mod-els fitted in Section 4 to provide in Table 2 the estimated data-based reliability scores for all 43 collections. It is evi-dent that expected  X  correlations are well below the desired 0.9 in most cases. In that line, some collections are clearly not reliable, such as the Web Distillation 2003, Genomics Ad Hoc 2005, Terabyte 2006, Enterprise Expert Search 2008, or the very recent Medical 2011 and Web Ad Hoc 2011. Re-garding the expected RMS Error of absolute scores, we can see that collections are somewhat stable, but with clear ex-ceptions such as Web Distillation 2003, Novelty 2004 and Enterprise Expert Search 2008.

The last two columns in Table 2 report intervals on the number of queries, as per equations (12) and (13), required to achieve 0.95 stability. In general the number of queries needs to be at least doubled, and in many cases a few hun-dred queries seem to be needed. This is particularly interest-ing for the most recent collections, such as Web Ad Hoc 2010 and 2011, Medical 2011 and Microblog 2011, which stick to the traditional size of 50 queries but need about 200. What becomes clear from these figures is that the ideal size of a collection depends greatly on the task it will be used for, and thus it is not appropriate to fix some acceptable size such as 50 or 100 throughout tasks. Each task has different characteristics and should be analyzed accordingly.
In this paper we discussed the measurement of test col-lection reliability from the perspective of traditional data-based methodologies and of Generalizability Theory. GT is regarded as a more appropriate, easy to use, and power-ful method to assess reliability, but it has two drawbacks. First, we showed that GT is very sensitive to the particular sample of systems and queries used to estimate reliability of a larger query set. We showed that about 50 systems and 50 queries are needed for robust estimates of collection re-liability. Therefore, researchers should be cautious in using GT when building new collections from scratch. To account for all this variability we discussed a more robust approach based on interval estimates of the stability indicators, which helps in making more appropriate decisions regarding num-ber of queries or different structure in the experimental de-sign. Second, we empirically established a mapping between GT-based and traditional data-based indicators to help in-terpreting results from GT which, otherwise, do not have a clear and easily understandable meaning. Based on these results, we reviewed the reliability of 43 TREC test collec-tions, evidencing that some of them are very little reliable. We show that the traditional choice of 50 queries is clearly not enough even for stable rankings, and in most cases a couple hundred queries are needed. Our results also show that the ideal query set size varies significantly across tasks, suggesting that we avoid the use of some fixed size such as 50 or 100 and that we analyze tasks and collections separately.
There are two clear lines for future research. First, we completely ignored the assessor facet in our study. It is ev-ident that different assessors provide different results, so it would be interesting to include them in the analysis. Sec-ond, although we fitted the theoretically correct models, it is clear that they can be improved (see for instance Power and RMS Error in Figure 3). IR evaluation experiments generally violate assumptions of GT, such as normality of distributions and random sampling, so different models and features to better fit the actual data should be investigated.
We created some scripts for the statistical software R that can help researchers perform all these computations to easily assess the reliability of custom test collection designs. They can be downloaded from http://julian-urbano.info . [1] J. Allan, J. A. Aslam, B. Carterette, V. Pavlu, and [2] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, [3] C. Arteaga, S. Jeyaratnam, and G. A. Franklin. [4] D. Banks, P. Over, and N.-F. Zhang. Blind Men and [5] D. Bodoff. Test Theory for Evaluating Reliability of [6] D. Bodoff and P. Li. Test Theory for Assessing IR Test [7] R. L. Brennan. Generalizability Theory . Springer, [8] C. Buckley and E. M. Voorhees. Evaluating Evaluation [9] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, [10] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, [11] L. S. Feldt. The Approximate Sampling Distribution [12] E. Kanoulas and J. A. Aslam. Empirical Justification [13] W.-H. Lin and A. Hauptmann. Revisiting the Effect of [14] S. Robertson and E. Kanoulas. On Per-Topic Variance [15] T. Sakai. On the Reliability of Information Retrieval [16] M. Sanderson. Test Collection Based Evaluation of [17] M. Sanderson and J. Zobel. Information Retrieval [18] R. J. Shavelson and N. M. Webb. Generalizability [19] J. Urbano, M. Marrero, and D. Mart  X  X n. A Comparison [20] E. M. Voorhees. Variations in Relevance Judgments [21] E. M. Voorhees. Topic Set Size Redux. In ACM [22] E. M. Voorhees and C. Buckley. The Effect of Topic [23] E. Yilmaz, J. A. Aslam, and S. Robertson. A New [24] J. Zobel. How Reliable are the Results of Large-Scale
