 The class imbalance problem is en countered in a large number of practical applications of machin e learning and data mining, for example, information retrieval a nd filtering, and the detection of credit card fraud. It has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often resu lts in a classifier X  X  suboptimal performance. This is even more true when the imbalanced data are also high dimensional. In such cases, feature selection methods are critical to achieve optim al performance. In this paper, we propose a new feature selec tion method, Feature Assessment by Sliding Thresholds (FAST), which is based on the area under a ROC curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. FAST is compared to two commonly-used feature selection methods, correlation coefficient and RELevance In Estimating Features (RELIEF), for imbalanced data classification. The experimental results obt ained on text mining, mass spectrometry, and microarray data sets showed that the proposed method outperformed both RELIEF and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred, the classification performance of the proposed method was significantly improved compared to correlation and R ELIEF-based methods. I.5.2 [ Pattern Recognition ]: Design Methodology  X  feature evaluation and selection. Algorithms. Feature selection, imbalanced data classification, ROC. One of the greatest challenges in machine learning and data mining research is the class imbalance problem presented in real-world applications. The class im balance problem refers to the issues that occur when a dataset is dominated by a class or classes that have significantly more samples that the other classes of the dataset. Imbalanced classes are seen in a variety of domains and many have major economic, co mmercial, and environmental concerns. Some examples include text classification, risk management, web categorization, medical diagnosis/monitoring, biological data analysis, credit card fraud detection, oil spill identification from satellite images. While the majority of learning methods are designed for well-balanced training data, data imbalance presents a unique misclassification costs for the two classes are different (i.e., cost-sensitive classification) and accord ingly, the overall classification rate is not appropriate to evaluate the performance. The class imbalance problem could hinder the performance of standard machine learning methods. For exam ple, it is highly possible to achieve the high classification accuracy by simply classifying all samples as the class with majority samples. The practical applications of cost-sensitive cl assification arise frequently, for example, in medical diagnosis [1], in agricultural product inspection [2], in industrial production processes [3], and in automatic target detection [4]. Analyzing the imbalanced data thus requires new methods than those used in the past. The majority of current research in the class-imbalance problem can be grouped into two categor ies: sampling techniques and algorithmic methods, as discussed in two workshops at the AAAI conference [5] and the ICML conference [6], and later in the sixth issue of SIGKDD Exploration (see, for example, a review by Weiss [7]). The sampling methods involve leveling the class samples so that they are no longe r imbalanced. Typically, this is done by under-sampling the larger class [8-9] or by over-sampling the smaller one [10-11] or by combination of these techniques [12]. Algorithmic methods include adjusting the costs associated with misclassification so as to improve performance [13-15], shifting the bias of a classifier to favor the rare class [16-17], creating Adaboost-like boosting schemes [18-19], and learning from one class [20]. The class imbalance problem is even more severe when the dimensionality is high. For example, in microarray-based cancer classification, the number of f eatures is typically tens of thousands [21]; in text classifica tion, the number of features in a bag of words is often more than an order of magnitude compared to the number of training documents [22]. Both sampling techniques and algorithmic methods may not work well for high dimensional class imbalance probl ems. Indeed, van der Putten and van Someren analyzed the COIL challenge 2000 datasets and concluded that to overcome overfitting problems, feature selection is even more important than classification algorithms [23]. A similar observation was made by Forman in highly imbalanced data classification pr oblems [22]. As pointed out by Forman,  X  X o degree of clever induction can make up for a lack of predictive signal in the input space X  [22]. This holds even for the SVM which is engineered to work with hyper-dimensional datasets. Forman [22] found th at the performance of the SVM could be improved by the judicious use of feature selection metrics. It is thus critical to develop effective feature selection methods for imbalanced data classi fication, especially if the data are also high dimensional. While feature selection has been extensively studied [24-30], its importance to class imbalance problems in particular was recently realized and attracted increasing attention from machine learning and data mining community. Mladenic and Grobelnik examined the performance of different feature selection metrics in classifying text mining data fro m the Yahoo hierarchy [31]. After applying one of nine different filte rs, they tested the classification power of the selected features using na X ve Bayes classifiers. Their results showed that the best me trics choose common features and consider the domain and learning machine X  X  inherent characteristics. Forman found impr oved results with the use of multiple different metrics, but th e best performing results were those selected by metrics that focused primarily on the results of the minority class [22]. Zheng, Wu, and Srihari empirically tested different ratios of features indica ting membership in a class versus features indicating lack of membership in a class [32]. This approach resulted in better accuracy compared to using one-sided metrics that solely score features indicating membership in a class and two-sided metrics that si multaneously score features indicating membership and lack of membership. One common problem with standard evaluation statistics used in previous studies, like information gain and odds ratios, is that they are dependent on the choice of the true positive (TP), false positive (FP), false negative (FN), and true negative (TN). These parameters are set based on a preset threshold. Consider imbalanced data classification with two different feature sets. The first feature set may yield higher TP, but lower TN, than the second feature set. By varying the decision threshold, the second feature set may produce higher TP and lower TN than the first feature set. Thus, one single threshold cannot tell us which feature evaluate a classifier's predictive power [33]. If we vary the classifier's decision threshold, we can find these statistics for each threshold and see how they vary based on where the threshold is placed. A receiver operating characteristic, or ROC curve, is one such non-parametric measure of a cl assifier's power that compares the true positive rate with the false positive rate. While the ROC curve has been extensively used for evaluating classification performance in class imbalance probl ems, it has not been directly applied for feature selection. In this paper, we construct a new feature selection metric base d on an ROC curve generated on optimal simple linear discriminants and select those features with the highest area under the curve as the most relevant. Unlike other feature selection metrics which depend on one particular decision boundary, our metric evaluates features in terms of their performance on multiple decision hyperplanes and is more appropriate to class imbalance problems. The rest of our paper is organized as follows. Section 2 provides a brief discussion about two commonly-used filter methods: correlation coefficient (CC), and RELevance In Estimating Features (RELIEF). In section 3, we follow with a description of the proposed new method, Feature Assessment by Sliding Thresholds (FAST). In secti on 4, we present the results comparing the performance of the linear support vector machines (SVM) and 1-nearest neighbor (1-NN) classifiers using features selected by each metric. These results are measured on two microarray, two mass spectrometry, and one text mining datasets. Finally, we give our concl uding remarks in section 5. In this section, we briefly re view two commonly-used feature selection methods, CC and RELIEF. The correlation coefficient is a statistical test that measures the strength and quality of the relati onship between two variables. Correlation coefficients can range from -1 to 1. The absolute value of the coefficient gives the strength of the relationship; absolute values closer to 1 i ndicate a stronger relationship. The sign of the coefficient gives the direction of the relationship: a positive sign indicates then the tw o variables increase or decrease with each other and a negative sign shows that one variable increases as the other decreases. In machine learning problems, the correlation coefficient is used to evaluate how accurately a feature predicts the target independent of the context of other features. The features are then ranked based on the correlation score [25]. For problems where the covariance cov( i X , Y ) between a feature ( i target ( Y ) and the variances of the feature (var( (var( Y )) are known, the correlation can be directly calculated: Equation 1 can only be used when the true values for the covariance and variances are know n. When these values are unknown, an estimate of the correlation can be made using Pearson's product-moment correla tion coefficient over a sample of the population ( x i , y ). This formula only requires finding the mean of each feature and the target to calculate: where m is the number of data points. Correlation coefficients can be used for both regressors and classifiers. When the machine is a regressor, the range of values of the target may be any ratio scale. When the learning machine is a classifier, we restrict the range of values for the target to 1  X  . We then use the coefficient of determination, or 2 ) ( i R , to enforce a ranking of the features according to the goodness of linear fit between individual features and the target [25]. When using the correlation coefficient as a feature selection metric, we must remember that the correlation only finds linear relationships between a feature and the target. Thus, a feature and the target may be perfectly related in a non-linear manner, but the correlation could be equal to 0. We may lift this restriction by using simple non-linear preprocessing techniques on the feature before calculating the correlation coefficients to establish a goodness of non-linear relationship fit between a feature and the target [25]. Another issue with using correla tion coefficients comes from how we rank features. If features are solely ranked on their value, with features having a positive score getti ng picked first or vice versa, then we risk not choosing the features that have the strongest relationship with the target. Conve rsely, if features are chosen based on their absolute value, Zheng, Wu, and Srihari argue that we may not select a ratio of positive to negative features that gives the best results based on the imbalance in the data [32]. Finding this optimal ratio takes empirical testing, but it can result in extremely strong results. RELIEF is a feature selection metric based on the nearest neighbor rule designed by Kira and Rendell [34]. It evaluates a feature based on how well its values differentiate themselves from nearby points. When RELIEF selects any specific instance, it searches for two nearest neighbors: one from the same class (the nearest hit), and one from the other class (the nearest miss). We then calculate the relevance of each attribute A by the rule: W ( A ) = P (different value of A | nearest miss) This is justified by the thinking that instances of different classes should have vastly different valu es, while instances of the same class should have very similar values. Because the true probabilities cannot be calculated, we must estimate the difference in equation 3. This is done by calculating the distance between random instances and thei r nearest hits and misses. For discrete variables, the distance is 0 if the same and 1 if different; for continuous variables, we use th e standard Euclidean distance. We may select any number of instances up to the number in the set, and more selections indi cate a better approximation [35]. Algorithm 1 details the pseudo-c ode for implementing RELIEF. Algorithm 1 (RELIEF): Set all W ( A ) = 0 FOR i =1 to m The original version of RELIEF suffered from several problems. First, this method searches onl y for one nearest hit and one nearest miss. Noisy data c ould make this approximation inaccurate. Second, if there are instances which have missing values for features, the algor ithm will crash because it cannot calculate the distance betw een those instances. Kononenko created multiple extensions of R ELIEF to address these issues [35]. RELIEF-A allowed the al gorithm to check multiple nearest hits and misses. RELIEF-B, C, and D gave the method different ways to address missing values. Finally, RELIEF-E and F found a nearest miss from each different class instead of just one and used this to better estimate the separab ility of an instance from all other classes. These extensions adde d to RELIEF's adaptability to different types of problems. In this section, we propose to assess features based on the area under a ROC curve, which is determined by training a simple linear classifier on each feature and sliding the decision boundary for optimal classification. The new metric is called FAST (Feature Assessment by S liding Thresholds). Most single feature classifiers set the decision boundary at the mid-point between the mean of the two classes [25]. This may not be the best choice for the decision boundary. By sliding the decision boundary, we can increase the number of true positives we find at the expense of classifying more false positives. Alternately, we could slide the threshold to decrease the number of true positives found in order to avoid misclassifying negatives. Thus, no single choice for the decision boundary may be ideal for quantifying the separation between two classes. We can avoid this problem by classifying the samples on multiple thresholds and gathering statistics about the performance at each boundary. If we calculate the true positive rate and false positive rate at each threshold, we can build an ROC curve and calculate the area under the curve. Becau se the area under the ROC curve is a strong predictor of performan ce, especially for imbalanced data classification problems, we can use this score as our feature ranking: we choose those features with the highest areas under the curve because they have the best predictive power for the dataset. By using a ROC curve as the means to rank features, we have introduced another problem: deciding where to place the thresholds. If there are a larg e number of samples clustered together in one region, we would like to place more thresholds between these points to find how separated the two classes are in this cluster. Likewise, if there is a region where samples are sparse and spread out, we want to avoid placing multiple thresholds between these points so as to avoid placing redundant thresholds between two points. On e possible solution is to use a histogram to determine where to place the thresholds. A histogram fixes the bin width and varies the number of points in each bin. This method does not accomplish the goals detailed above. It may be the case that a particular histogram has multiple neighboring bins that have very fe w points. We would prefer that these bins be joined together so that the points would be placed into the same bin. Likewise, a hi stogram may also have a bin that has a significant proportion of the poi nts. We would rather have this bin be split into multiple differe nt bins so that we could better differentiate inside this cluster of points. We use a modified histogram, or an even-bin distribution, to correct both of these problems. Instead of fixing the bin width and varying the number of points in each bin, we fix the number of points to fall in each bin and vary the bin width. This even-bin distribution accomplishes both of the above goals: areas in the feature space that have fewer sa mples will be covered by wider bins, and areas that have many samples will be covered by narrower bins. We then take the mean of each sample in each bin as our threshold and classify each sample according to this threshold. Algorithm 2 details th e pseudo-codes for implementing FAST. Algorithm 2 (FAST): K : number of bins N : number of samples in dataset M : number of features in dataset Split = 0 to N with a step size N/K FOR i = 1 to M Sort X Calculate area under ROC by tpr , fpr One potential issue with this im plementation is how it compares to the standard ROC algorithm of using each possible threshold as the standard is simpler but re quires more computations. We conducted a pilot study using th e CNS dataset to measure the difference between the FAST algorithm and this standard. Our findings showed that with a para meter of K=10, 99% of the FAST scores were within plus-minus 0.02 of the exact AUC score, and 50% were within plus-minus 0.005. Additionally, the FAST algorithm was nearly ten times as fast. Thus, we concluded that the approximation scores were sufficient. Note that the FAST method is a two-sided metric. The scores generated by the FAST method ma y range between 0.5 and 1. If a feature is irrelevant to classifica tion, its score will be close to .5. If a feature is highly indicative of membership in the positive or negative class or both, it will have a score closer to 1. Thus, this method has the potential to select both positive and negative features for use in classification. We tested the effectiveness of correlation coefficient, RELIEF, and FAST features on five different data sets. Two of the data sets are microarray sets, two are mass spectrometry sets, and one is a bag-of-words set. Each of the microarray and mass spectrometry data sets has a small number of samples, a large number of features, and a signifi cant imbalance between the two classes. The bag-of-words data set also has a small number of samples with a large number of features, but we artificially controlled the class skew to show differences in performance on highly imbalanced classes vers us balanced classes. The microarray sets were not preprocessed. The mass spectrometry sets were minimally preprocesse d by subtracting the baseline, reducing the amount of noise, trimming the range of inspected mass/charge ratios, and normalizi ng. The bag-of-words set was constructed using RAINBOW [36] to extract the word counts from text documents. These data sets are summarized in Table 1. Because the largest data set has 320 samples, we used 10-fold cross-validation to evaluate the tr ained models. Each fold had a fold are combined with each other to obtain test results for the entire data set. To stabilize the results, we repeated the cross-validation 20 times and averaged over each trial. CNS LYMPH OVARY PROST NIPS The standard accuracy and error statistics quantify the strength of a classifier over the overall data se t. However, these statistics do not take into account the class di stribution. Forman argued that this is because a trivial majority classifier can give good results on a very imbalanced distribution [22]. It is more important to classify samples in the minority class at the potential expense of misclassifying majority samples. However, the converse is true as well: a trivial minority classifier will give great results for the minority class, but such a classi fier would have too many false alarms to be usable. An ideal classifier would perform well on both the minority and the majority class. The balanced error rate (BER) st atistic looks at the performance of a classifier on both classes. It is defined as the average of the error rates of two classes as show n in equation 4. If the classes are balanced, the BER is equal to the global error rate. It is commonly used for evaluating imbalanced da ta classification [42]. We used this statistic to evaluate trained classifiers on test data. We evaluated the performance of FAST-selected features by comparing them with features chosen by correlation coefficients and RELIEF. Many researchers have used standard learning algorithms that maximize accuracy to evaluate imbalanced datasets. Zheng [32] used the Na ive Bayes classifier and logistic regression methods, and Forman [22] used the linear SVM and noted its superiority over decision trees, Naive Bayes, and logistic regression. The object of study in these papers, and in our research, was the performance of the feature selection metrics and not the induction algorithms. Thus, we chose to evaluate the metrics using the performance of the linear SVM and 1-NN classifiers. These classifiers were chosen based on their differing classification philosophies. The 1-NN method is a lazy algorithm that defers computation until cla ssification. In contrast, the SVM computes a maximum separating hype rplane before classification. The classification results are summarized in Figs. 1-10, where dashed lines with square markers indicate classifiers using RELIEF-selected features (with one nearest hit and miss), dashed lines with star markers indicate classifiers using correlation-selected features, and dashed lin es with diamond markers indicate classifiers using FAST-selected features (with 10 bins). The solid black line indicates the baseline performance where all the features are used for classification. Figures 1 and 2 show the BER versus the number of features selected using an 1-NN classifier and a linear SVM for CNS data, respectively. FAST features significantly outperformed RELIEF and correlation features when using the 1-NN classifier. When using the SVM classifier, FAST features performed the best for less than 40 features; and for more than 40 features, there was little difference between feature se ts. For all the cases, using a small set of features outperforms the baseline with all the original features. Similar results can be obtained for other datasets. For example, Figures 3 and 4 show th e results for LYMPH data with an 1-NN and a linear SVM, respectively. Due to page limits, we are not able to show the results for all the four datasets. Instead, we include the average results here. Figures 5 and 6 show the BER scores averaged over the four datasets with an 1-NN classifier and a SVM, respectively. For comparison, the baseline performance of the classifier using all features is also included. Another evaluation statistic commonly used on imbalanced datasets is the area under the ROC ( AUC). This statistic is similar in nature to the BER in that it weights errors differently on the philosophy of FAST. FAST selects features that maximize the AUC, so it is reasonable to believe that a learning method using FAST-selected features would also maximize the AUC. We also used this statistic to evaluate trained classifiers on test data. Figures 7 and 8 show the AUC scores averaged over the four datasets with an 1-NN classifier and a SVM, respectively. Not surprisingly, FAST outperforms CC and RELIEF. Figure 5. BER averaged over CNS, LYMPH, OVARY, and Figure 6. BER averaged over CNS, LYMPH, OVARY, and Figure 7. AUC averaged over CNS, LYMPH, OVARY, and Figure 8. AUC averaged over CNS, LYMPH, OVARY, and 
Figure 11. Training data distribution of CNS with the two The average results in Figures 6 and 8 agree with the belief that SVM's are robust for high-dimensional data. Up to 100 RELIEF-selected features did not improve the BER or the AUC of the SVM. Additionally, up to 100 correlation-selected features did not improve the BER. On the ot her hand, the SVM using more than 30 FAST-selected features did see a significant improvement on both BER and AUC. Thus, our results agree with the general finding that SVM's are resistant to feature selection, but also agree with the findings presented by Forman [22] that SVM's can benefit from prudent feature selection. Specific examples of this improvement in our datasets can be seen in Figures 2 and 4 using FAST on the BER scores for the CNS and LYMPH datasets, respectively, and in Figures 9 and 10 using FAST on the AUC scores for the CNS and PROST datasets, respectively. The results for the 1-NN classifiers, seen in Figures 5 and 7, are even more striking. Both RELIEF and correlation-selected features improved on the baseline performance of the classifier significantly for a minimum of 45 features selected. FAST-selected features saw a significant jump in performance over that seen using RELIEF and correlation-selected features; the 1-NN classifiers using only 15 FAST-selected features beat the baseline. 
Figure 12. Training data distribution of CNS with the two 
Figure 13. Training data distribution of CNS with the two Why would FAST features outperform correlation and RELIEF features by such a significant margin for both 1-NN and SVM classifiers? We visualized the features selected by the correlation, RELIEF, and FAST methods to answ er this question. We show the training data of the CNS dataset with the two best features. Figures 11-13 show the data using the best two RELIEF features, the best two correlation features, and the best two FAST features respectively. FAST features appear to separate the two classes and group them into smaller clusters better than correlation and RELIEF features. This may explain why FAST features perform better using bot h the SVM and 1-NN classifiers; SVM's try to maximize the distance between two classes, and 1-NN classifiers give the best results when similar samples are clustered close together. Finally, we show the effects of different class ratios on the performance of each feature selection metric. Figures 14 and 15 show the BER versus class ratios for the NIPS dataset with the SVM and 1-NN classifiers, respectively. Not surprisingly, as the class ratio increases, the BER tends to increase accordingly. For both the 1-NN and SVM classifi ers, correlation and FAST features performed comparably well for datasets up to a 1:8 class ratio. For the 1:16 ratios, FAST features performed significantly better than correlation features. RELIEF features did not perform well on this dataset for any of the class ratios. We conclude that FAST features perform better than RELIEF and correlation features; this boost in performance is especially large when the selected feature set is small and when the classes are extremely imbalanced. Because using less features helps classifiers avoid overfitting the data when the sample space is small, we believe that the FAST metric is of interest for use in learning patterns of real world data sets, especially those that have imbalanced classes and high dimensionality. Classification problems involving a small sample space and large feature space are especially prone to overfitting. Feature selection methods are often used to increas e the generalization potential of a classifier. However, when the dataset to be learned is imbalanced, the most-used metric s tend to select less relevant features. In this paper, we propos ed and tested a feature selection metric, FAST, that evaluates the relevance of features using the area under the ROC curve by sliding decision line in one-dimensional feature space. We compared the FAST metric with commonly-used RELIEF and correlation coefficient scores on two mass spectrometry and two mi croarray datasets that have small sample sizes and imbalanced distributions. FAST features performed considerably better than RELIEF and correlation features; the increase in performance was magnified for smaller feature counts, and this makes FAST a practical candidate for feature selection. One interesting finding from this research was that correlation features tended to outperform RELIEF features for class imbalance and small sample problems, especially when the SVM classifier was used. This may have occurred because the correlation coefficient takes a global view as to whether a feature accurately predicts the target; in contrast, RELIEF, especially when the number of nearest hits and misses selected is small, has a local view of a feature's relevancy to predicting the target. If there are small clusters of points that are near each other but far away from the main cluster of points, these points can act as each others' nearest hits while being a great distance from the nearest misses. Thus, features that have this quality could be scored rather high when they are, in fact, highly irrelevant to classification. There is strong ev idence for this claim in Fig. 11. There are multiple small clusters of points, some from the majority class and some from the minority class, that are close to each other but a significant distance away from the nearest miss. This would greatly affect the score of these two features and make them appear more relevant. Figures 5-8 clearly point to this deficiency as the performance of both SVM's and 1-NN classifiers using RELIEF features is only marginally better (or worse) than chance and significantly behind classifiers using correlation or FAST features. Our future work will investigate the use of other metrics for feature evaluation. For example, researchers have recently argued that precision-recall curves are preferable when dealing with highly skewed datasets [43]. Whether or not the precision-recall curves are also appropriate to small sample and imbalanced data problems remains to be examined. This work is supported by th e US National Science Foundation Award IIS-0644366. We would also like to the reviewers for their valuable comments. [1] Nunez, M. 1991. The use of background knowledge in [2] Casasent, D. and Chen, X.-W. 2003. New training strategies [3] Verdenius, F. 1991. A method fo r inductive cost optimization. [4] Casasent, D. and Chen, X.-W . 2004. Feature reduction and [5] Japkowicz, N. editor 2000. Proceedings of the AAAI X 2000 [6] Chawla, N., Japkowi cz, N., and Kolcz, A. editors 2003. [7] Weiss, G. 2004. Mining with rarity: A unifying framework. [8] Kubat, M. and Matwin, S. 1997. Addressing the curse of [9] Chen, X., Gerlach, B., and Casasent, D. 2005. Pruning support [10] Kubat, M. and Matwin, S. 1997. Learning when negative [11] Chawla, N., Bowyer, K., Hall, L., and Kegelmeyer, P. 2002. [12] Estabrooks, A., Jo, T., and Japkowicz, N. 2004. A multiple [13] Domingos, P. 1999. MetaCost: a general method for making [14] Elkan, C. 2001. The foundations of cost-sensitive learning. [15] Fawcett, T., Provost, F. 1997. Adaptive fraud detection. Data [16] Huang, K., Yang, H., King, I., Lyu, M., 2004. Learning [17] Ting, K. 1994. The problem of small disjuncts: its remedy on [18] Chawla, N., Lazarevic, A., Ha ll, L., and Bowyer, K. 2003. [19] Sun, Y., Kamel, M., Wang, Y. 2006. Boosting for learning [20] Raskutti, A. and Kowalczyk, A. 2004. Extreme rebalancing for [21] Xiong, H and Chen, X. 2006. Kernel-based distance metric [22] Forman, G. 2003. An extensive empirical study of feature [23] Van der Putten, P. and van Some ren, M. 2004. A bias-variance [24] Guyon, I., Weston J., Barnhill, S., and Vapnik, V. 2002. Gene [25] Guyon, I., and Elisseeff, A. 2003. An introduction to variable [26] Weston, J. et al. 2000. Feature selection for support vector [27] Chen, X. and Jeong, J. 2007. Minimum reference set based [28] Chen, X. 2003. An improved branch and bound algorithm for [29] Yu, L. and Liu, H. 2004. Efficien t feature selection via analysis [30] Pudil, P., Novovicova, J., and K ittler, J., 1994. Floating search [31] Mladenic, D. and Grobelnik, M. 1999. Feature selection for [32] Zheng, Z., Wu, X., and Srihari, R. 2004. Feature selection for [33] Lund, O., Nielsen, C., Lundeg aard, C., and Brunak, S. 2005. [34] Kira, K. and Rendell, L. 1992. The feature selection problem: [35] Kononenko, I. 1994. Estimating attributes: Analysis and [36] McCallum, A. 1996. Bow: A tool kit for statistical language [37] Pomeroy, S. et al. 2002. Prediction of central nervous system [38] Shipp, M. et al. 2002. Diffuse large b-cell lymphoma outcome [39] Petricoin, E. et al. 2002. Use of proteomic patterns in serum to [40] Petricoin, E. et al. 2002. Serum proteomic patterns for [41] Roweis, S. 2008. http://www. cs.toronto.edu/ roweis. [42] MPS, 2006. Performance predicti on challenge  X  evaluation. [43] Davis, J. and Goadrich, M. 2006. The relationship between 
