 387 Soda Hall Berkeley, CA 94720-1776 Learning algorithms need to make assumptions about the problem domain in order to generalise well. These assumptions are usually encoded in the regulariser or the prior. A generic learning algo-rithm usually makes rather weak assumptions about the regularities underlying the data. An example of this is smoothness. More elaborate prior knowledge, often needed for a good performance, can be hard to encode in a regulariser or a prior that is computationally efficient too.
 Interesting hybrids between both extremes are regularisers that depend on an additional set of data available to the learning algorithm. A prominent example of data-dependent regularisation is semi-supervised learning [1], where an additional set of unlabelled data, assumed to follow the same distribution as the training inputs, is tied to the regulariser using the so-called cluster assumption . A novel form of data-dependent regularisation was recently proposed by [11]. The additional dataset for this approach is explicitly not from the same distribution as the labelled data, but represents a third  X  neither  X  class. This kind of dataset was first proposed by Vapnik [10] under the name Universum , owing its name to the intuition that the Universum captures a general backdrop against which a problem at hand is solved. According to Vapnik, a suitable set for this purpose can be thought of as a set of examples that belong to the same problem framework, but about which the resulting decision function should not make a strong statement.
 Although initially proposed for transductive inference, the authors of [11] proposed an inductive classifier where the decision surface is chosen such that the Universum examples are located close to it. Implementing this idea into an SVM, different choices of Universa proved to be helpful in various classification tasks. Although the authors showed that different choices of Universa and loss functions lead to certain known regularisers as special cases of their implementation, there are still a few unanswered questions. On the one hand it is not clear whether the good performance of their algorithm is due to the underlying original idea, or just a consequence of the employed algorithmic relaxation. On the other hand, except in special cases, the influence of the Universum data on the resulting decision hyperplane and therefore criteria for a good choice of a Universum is not known. In the present paper we would like to address the second question by analysing the influence of the Universum data on the resulting function in the implementation of [11] as well as in a least squares version of it which we derive in section 2. Clarifying the regularising influence of the Universum on the solution of the SVM can give valuable insight into which set of data points might be a helpful Universum and how to obtain it.
 complement of the subspace spanned by the principal directions of the Universum set. Furthermore, we demonstrate that the least squares version of the Universum algorithm is equivalent to a hybrid between kernel Fisher Discriminant Analysis and kernel Oriented Principal Component Analysis. In section 4, we validate our analysis on toy experiments and give an example how to use the geometric and algorithmic intuition gained from the analysis to construct a Universum set for a real world problem. 2.1 The Hinge Loss U -SVM We start with a brief review of the implementation proposed in [11]. Let L = standard SVM can compactly be formulated as In the implementation of [11] the goal of bringing the Universum examples close to the separating on the Universum points examples twice with opposite labels and obtain an SVM like formulation which can be solved with a standard SVM optimiser. 2.2 The Least Squares U -SVM The derivation of the least squares U -SVM starts with the same general regularised error minimisa-tion problem least squares versions of SVMs [9]. Expanding (2) in terms of slack variables  X  and  X  yields Minimising the Lagrangian of (3) with respect to the primal variables w ,b,  X  and  X  , and substituting their optimal values back into (3) yields a dual maximisation problem in terms of the Lagrange multipliers  X  . Since this dual problem is still convex, we can set its derivative to zero and thereby obtain the following linear system associated with labelled examples and 1 C The solution (  X  ,b ) can then be obtained by a simple matrix inversion. In the remaining part of this paper we denote the least squares SVM by U ls -SVM. 2.3 Related Ideas Although [11] proposed the first algorithm that explicitly refers to Vapnik X  X  Universum idea, there exist related approaches that we shall mention briefly. The authors of [12] describe an algorithm for the one-vs-one strategy in multiclass learning that additionally minimises the distance of the separating hyperplane to the examples that are in neither of the classes. Although this is algorithmi-cally equivalent to the U -SVM formulation above, their motivation is merely to sharpen the contrast between the different binary classifiers. In particular, they do not consider using a Universum for binary classification problems.
 There are also two Bayesian algorithms that refer to non-examples or neither class in the binary classification setting. [8] gives a probabilistic interpretation for a standard hinge loss SVM by estab-lishing the connection between the MAP estimate of a Gaussian process with a Gaussian prior using a covariance function k and a hinge loss based noise model. In order to deal with the problem that the proposed likelihood does not integrate to one the author introduces a third  X  the neither  X  class, A similar idea is used by [4], introducing a third class to tackle the problem that unlabelled examples parameters of the label distribution are independent of input points with unknown, i.e., marginalised value of the label. To circumvent this problem, the authors of [4] introduce an additional  X  neither  X  class to introduce a stochastic dependence between the parameter and the unobserved label in the discriminative model. However, neither of the Bayesian approaches actually assigns an observed example to the introduced third class. The following two sections analyse the geometrical relation of the decision hyperplane learnt with one of the Universum SVMs to the Universum set. It will turn out that in both cases the optimal solutions tend to make the normal vector orthogonal to the principal directions of the Universum. The extreme case where w is completely orthogonal to U , makes the decision function defined by w invariant to transformations that act on the subspace spanned by the elements of U . Therefore, the Universum should contain directions the resulting function should be invariant against. In order to increase the readability we state all results for the linear case. However, our results generalise to the case where the x i and z j live in an RKHS spanned by some kernel. 3.1 U -SVM and Projection Kernel For this section we start by considering a U -SVM with hard margin on the elements of U . Further-more, we use  X  = 0 for the  X  -insensitive loss. After showing the equivalence to using a standard SVM trained on the orthogonal complement of the subspace spanned by the z j , we extend the result to the cases with soft margin on U .
 Lemma A U -SVM with C U =  X  ,  X  = 0 is equivalent to training a standard SVM with the training element of U . Proof: Since C U =  X  and  X  = 0 , any w yielding a finite value of (1) must fulfil  X  w , z j  X  + b = 0 for the projection operator onto the orthogonal complement of that set. From the previous argument, we the same as a standard SVM where the x i have been replaced by P U  X  x i .
 The special case the lemma refers to, clarifies the role of the Universum in the U -SVM. Since the resulting w is orthogonal to an affine space spanned by the Universum points, it is invariant against as filters that extract certain features from given labelled or test examples x , using the Universum algorithms means suppressing the features specified by the z j .
 Finally, we generalise the result of the lemma by dropping the hard constraint assumption on the Universum examples, i.e. we consider the case C U &lt;  X  . Let w  X  and b  X  the optimal solution of (1). We have that The right hand side can be interpreted as an  X  L 1 variance X . So the algorithm tries to find a direction w  X  such that the variance of the projection of the Universum points on that direction is small. As C
U approaches infinity this variance approaches 0 and we recover the result of the above lemma. 3.2 U ls -SVM, Fisher Discriminant Analysis and Principal Component Analysis In this section we present the relation of the U ls -SVM to two classic learning algorithms: (kernel) oriented Principal Component Analysis (koPCA) and (kernel) Fisher discriminant analysis (kFDA) [5]. As it will turn out, the U ls -SVM is equivalent to a hybrid between both up to a linear equality constraint. Since koPCA and kFDA can both be written as maximisation of a Rayleigh Quotient we start with the Rayleigh quotient of the hybrid them. As indicated in the equation, the numerator is exactly the same as in kFDA, i.e. the inter-class variance, while the denominator is a linear combination of the denominators from kFDA and koPCA, i.e. the inner class variances from kFDA and the noise variance from koPCA.
 As noted in [6] the numerator is just a rank one matrix. For optimising the quotient it can be fixed to an arbitrary value while the denominator is minimised. Since the denominator might not have full rank it needs to be regularised [6]. Choosing the regulariser to be || w || 2 , the problem can be rephrased as As we will see below this problem can further be transformed into a quadratic program Ignoring the constraint  X  &gt; 1 k = 0 , this program is equivalent to the quadratic program (3) of the U ls -SVM. The following lemma establishes the relation of the U ls -SVM to kFDA and koPCA. Lemma For given C L and C U the optimisation problems (4) and (5) are equivalent.
 Proof: Let w , b ,  X  and  X  the optimal solution of (5). Combining the first and last constraint, we get w and using this value of b , we obtain the objective function (4). So we have proved that the minimum value of (4) is not larger than the one of (5).
  X  0 . But because w &gt; ( c +  X  c  X  ) = 2 , we have The above lemma establishes a relation of the U ls -SVM to two classic learning algorithms. This further clarifies the role of the Universum set in the algorithmic implementation of Vapnik X  X  idea as proposed by [11]. Since the noise covariance matrix of koPCA is given by the covariance of the Universum points centered on the average of the labelled class means, the role of the Universum as a data-dependent specification of principal directions of invariance is affirmed.
 The koPCA term also shows that both the position and covariance structure are crucial to a good covariance of Universum about its mean, and the distance between Universum and training sample means projected onto w shows that either quantity can dominate depending on the data at hand. In the next section, we demonstrate the theoretical results of this section on toy problems and give an example how to use the insight gained from this section to construct an appropriate Universum. 4.1 Toy Experiments The theoretical results of section 3 show that the covariance structure of the Universum as well as its absolute position influence the result of the learning process. To validate this insight on toy data, we sample ten labelled sets of size 20 , 50 , 100 and 500 from two fifty-dimensional Gaussians. Both dimensions and high standard deviation (  X  3 ,..., 50 = 10 ) in the remaining 48. The two Gaussians are displaced such that the mean of  X   X  i =  X  0 . 3 exceeds the standard deviation by a factor of 3 . 75 in the first two dimensions but was 125 times smaller in the remaining ones. The values are chosen such that the Bayes risk is approx. 5% . Note, that by construction the first two dimensions are most discriminative.
 We construct two kinds of Universa for this toy problem. For the first kind we use a mean zero Gaussian with the same covariance structure as the Gaussians for the labelled data (  X  3 ,..., 50 = 10 ), the results of section 3 the Universa should be more helpful for larger anisotropy. For the second kind of Universa we use the same covariance as the labelled classes but shifted them along the line between the means of the labelled Gaussians. This kind of Universa should have a positive effect on the accuracy for small displacements but that effect should vanish with increasing amount of translation.
 Figure 1 shows the performance of a linear U -SVMs for different amounts of training and Universum data. In the top row, the degree of isotropy increases from left to right, whereas  X  = 10 refers to the complete isotropic case. In the bottom row, the amount of translation increases from left to right. As expected, performance converges to the performance of an SVM for high isotropy  X  and large translations t . Note, that large translations do not affect the accuracy as much as a high isotropy. However, this might be due to the fact the variance along the principal components of the Universum is much larger in magnitude than the applied shift. We obtained similar results for the U ls -SVM. Also, the effect remains when employing an RBF kernel. Figure 1: Learning curves of linear U -SVMs for different degrees of isotropy  X  and different amounts of 4.2 Results on MNIST digits 5 and 8 on MNIST data. Training sets of size 1000 were used, and other digits served as Universum data. Using different digits as universa, we recorded the test error (in percentage) of U -SVM. We also computed the mean output (i.e.  X  w , x  X  + b ) of a normal SVM trained for bi-nary classification between the digits 5 and 8, measured on the points from the Universum class. Another quantity of interest measured was the angle between covariance matrices of training and Universum data in the feature space. Note that for two covariance matrices C X and C Y corre-sponding to matrices X and Y (centered about their means), the cosine of the angle is defined and Y . These quantities have been documented in Table 1. All the results reported are averaged over 10-folds of cross-validation, with C = C U = 100 , and  X  = 0 . 01 . 4.3 Classification of Imagined Movements in Brain Computer Interfaces Brain computer interfaces (BCI) are devices that allow a user to control a computer by merely using his brain activity [3]. The user indicates different states to a computer system by deliberately changing his state of mind according to different experimental paradigms. These states are to be detected by a classifier. In our experiments, we used data from electroencephalographic recordings (EEG) with a imagined-movement paradigm. In this paradigm the patient imagines the movement the brain activity by the intermediate tissue of the skull, the signals from all sensors are demixed via an independent component analysis (ICA) applied to the concatenated lowpass filtered time series of all recording channels [2].
 In the experiments below we used two BCI datasets. For the first set (D ATA I) we recorded the EEG activity from three healthy subjects for an imagined movement paradigm as described by [3]. The second set (D ATA II) contains EEG signals from a similar paradigm [7].
 We constructed two kind of Universa. The first Universum, U C 3 consists of recordings from a third condition in the experiments that is not related to imagined movements. Since variations in signals from this condition should not carry any useful information about imagined movement task, the classifier should be invariant against them. The second Universum U nm is physiologically moti-vated. In the case of the imagined-movement paradigm the relevant signal is known to be in the so called  X  -band from approximately 10  X  12 Hz and spatially located over the motor cortices. Unfor-tunately, signals in the  X  -band are also related to visual activity and independent components can be found that have a strong influence from sensors over the visual cortex. However, since ICA is un-supervised, those independent components could still contain discriminative information. In order to make the learning algorithm prefer the signals from the motor cortex, we construct a Universum U nm by projecting the labelled data onto the independent components that have a strong influence from the visual cortex.
 the inner loop was used for model selection and the outer for testing. We exclusively used a linear kernel. Table 2 shows the mean zero-one loss for D ATA I and D ATA II and the constructed Universa. On the D ATA I dataset, there is no improvement in the error rates for the subjects FS and JL com-pared to an SVM without Universum. Therefore, we must assume that the employed Universa did not provide helpful information in those cases. For subject JH, U C 3 and U nm yield an improvement for both Universum algorithms. However, the differences to the SVM error scores are not signifi-cantly better according to a two-sided sign test. The U ls -SVM performs worse than the U -SVM in almost all cases.
 On the D ATA II dataset, there was an improvement only for subject S2 using the U -SVM with the U nm and U C 3 Universum (8% and 3% improvement respectively). However, also those differences are not significant. As already observed for the D ATA I dataset, the U ls -SVM performs constantly worse than its hinge loss counterpart.
 The better performance of the U nm Universum on the subjects JH and S2 indicates that additional information about the usefulness of features might in fact help to increase the accuracy of the clas-sifier. The regularisation constant C U for the Universum points was chosen C = C U = 0 . 1 in both cases. This means that the non-orthogonality of w on the Universum points was only weakly penalised, but had equal priority to classifying the labelled examples correctly. This could indicate that the spatial filtering by the ICA is not perfect and discriminative information might be spread over several independent components, even over those that are mainly non-discriminative. Using the U nm Universum and therefore gently penalising the use of these non-discriminative features can help to improve the classification accuracy, although the factual usefulness seems to vary with the subject. In this paper we analysed two algorithms for inference with a Universum as proposed by Vapnik [10]. We demonstrated that the U -SVM as implemented in [11] is equivalent to searching for a hyperplane which has its normal lying in the orthogonal complement of the space spanned by Uni-versum examples. We also showed that the corresponding least squares U ls -SVM can be seen as a hybrid between the two well known learning algorithms kFDA and koPCA where the Universum points, centered between the means of the labelled classes, play the role of the noise covariance in koPCA. Ideally the covariance matrix of the Universum should thus contain some important invari-ant directions for the problem at hand.
 The position of the Universum set plays also an important role and both our theoretical and exper-imental analysis show that the behaviour of the algorithm depends on the difference between the means of the labelled set and of the Universum set. The question of whether the main influence of the Universum comes from the position or the covariance does not have a clear answer and is probably problem dependent.
 From a practical point, the main contribution of this paper is to suggest how to select a good Uni-versum set: it should be such that it contains invariant directions and is positioned  X  X n between X  the two classes. Therefore, as can be partly seen from the BCI experiments, a good Universum dataset needs to be carefully chosen and cannot be an arbitrary backdrop as the name might suggest.
