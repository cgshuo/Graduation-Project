 Bayesian networks [1] (BN) are a graphical representation for probability distributions. They are a popular framework in AI and uncertainty processing. Eliciting BN from domain experts can be a laborious and expensive process in large-scale applications, growing interest in learning BN from data. A BN consists of a graph structure and a set of local probability distributions. Learning BN can be decomposed into two parts: discovering the graph structure and then the parameters for the graph structure. Current methods are effective in learning both the graph structure and parameters when data are complete, and can learn the parameters from incomplete data when the BN structure is known. However, learning BN structure from incomplete data is still a challenging problem. 
Current techniques for learning BN are mostly based on a scoring approach which is characterized by devising a score metric for a candidate network structure and commonly used metrics can be decomposed into independent terms each of which corresponds to one variable, such as the BIC, BDe or MDL metric [5] . When data are incomplete, we can no longer decompose the scoring function in BN construction. As a consequence, we are unable to perform local search for the network  X  in other words, a local change in one part of the network can affect the evaluation of a change in another part of the network. On the other hand, because some statistics are unknown, we cannot compute the scores of the network directly. There have been some methods proposed to solve those problems. Heckerman et al. presented some methods for the latter problem [2] . Those methods first use either EM (expectation-maximization) or gradient ascent (a grad ient-based optimization) to compute the MAP parameters, then use either Laplace approximation or Bayesian information criterion (BIC) [3] to compute the approximate scores of the network using larger samples and approximation methods. Unfortunately, because of the large search space of the network and the errors produced by the approximate scores, the efficiency of learning BN is very low and the learned BN do not have enough confidence [4] .

Friedman improved the methods presented by Heckerman et al. and proposed a structural EM (SEM) algorithm to learn BN from incomplete data [5] . The SEM algorithm consists of two parts: learning parameters and searching for the structure. In fact, the parametric learning part in SEM is a parametric EM algorithm that mainly computes the expected statistics of missing data. When searching for the BN structure, the SEM algorithm uses expected statistics instead of sufficient statistics that are unknown to make the scoring function have a closed form under certain assumptions. It can improve the expected score of the learned network at each iteration and make the structures converge at an optimal structure. Although able to improve the learning efficiency to some extent, SEM always stops at local optima. 
Besides halting at local optima, Friedman also pointed out that the computation of the expected statistics is the main bottleneck in applying this technique to large-scale domains. Aiming at the problem of local opt ima, various researchers have presented several variants of SEM [6-8] . Unfortunately, those variants don X  X  take into account the complexity of the time. When computing the expected statistics, we need to use an inference procedure of BN. But with the increase of the samples and missing data, the computation of the expected statistics is very huge [5] . Heckerman has proved that large-sample learning of Bayesian networks is NP-Hard [9] . Therefore, how to reduce to learning complex BN. So far there is little research on the problem. 
Recently, parallel processing has become a useful technique for scaling up huge computations [10] , and there has been some work on parallel learning BN [11-13] . Chu and Xiang presented a technique for using para llelism to speed up leaning decomposable Markov networks [11] . Lately, W. Lam et al. explored parallel algorithms for BN construction based on the K2 algorithm [12-13] . Although those algorithms can speed up learning BN, they have relied on the assumption that data are complete. This assumption is not very realistic, since most real world situations involve incomplete using parallel learning algorithm for BN to deal with incomplete data. Structural EM (PL-SEM), is proposed to learn BN with incomplete data. PL-SEM adopts a parallel parametric EM algorithm to parallelize the parametric learning part of SEM. The parallel EM algorithm parallelizes the E-step and M-step of the SEM algorithm to compute expected statistics. PL-SEM uses a parallel algorithm to compute the expected statistics and the parameters of the candidate network. It effectively computes the expected statistics, and greatly reduces the time complexity of learning BN. 
The rest of the paper is organized as follows. Section 2 briefly reviews the framework for learning BN based on the EM algorithm. In Section 3, we present the parallel SEM algorithm for learning BN, called PL-SEM. Section 4 provides our experimental results and an analysis. Finally, Section 5 gives a summary. information to find a network that fits the database D as much as possible. 
With the complete data, we can decompose the scoring function that evaluates the candidate network into a summation of terms, where each term consists of local network doesn X  X  affect the evaluation of a change in another part of the network  X  that is, the scoring function only needs to compute the scores of local structures that are changed. 
For example, let BN=(G,  X  ) be a Bayesian network, and a finite set U={X i , 1  X  i  X  n} be discrete random variables. Suppose D={x 1 ,..., x n } is a training set where each x i has a value for some (or all) variables in U, and N X (x) is the number of instances in D. Note that N X (.) is well-defined only for complete datasets. Given a training data set D, we use the Bayesian Information Criterion (BIC) [3] to rank candidate network structures, using the BIC score of each candidate BN, written Score (BN : D), by the following equation: We can further decompose the Score (BN : D) as follows. Where n is the size of the dataset and i is the node in the graph in Eq.(2). Dim[G] is the number of independent parameters in the graph G, and 
When the data are incomplete, we can no longer decompose the likelihood function network can affect the evaluation of a change in another part of the network. In order to be able to learn BN with incomplete data, Friedman presented an algorithm to learn BN structure from incomplete data based on a framework of EM, called the Structural EM algorithm (SEM). The basic idea of SEM is as follows. Let O  X  U be a set of observable variables and H=U-O be a set of hidden variables. We assume that we have a class of models G={M 0 ,...,M n } such that each model M  X  G is parameterized by a vector  X  M where each (legal) choice of values of  X  M denotes the hypothesis that the underlying distribution is in the model M. From now Pa(X i ) denotes the parents of X i . function. 
Then with fixed  X  * and D, the SEM algorithm tries to find the choice of (M:  X  * , D) that maximizes the scoring function of the structure, i.e. searching for the space of the models for a model (or models) that maximizes Eq. (1) or Eq. (2). Therefore, the SEM algorithm consists of two steps: learning parameters and the structure. 
Step 1 (referred to as the E-step). The SEM algorithm exploits the current model and parameters to compute expected statistics using a parametric EM algorithm, and then uses them to complete the incomplete data and re-evaluate the parameters of the current model. 
Step 2 (referred to as the M-step). With the completed data, the scoring function has the decomposition property. The SEM algorithm performs a local change on the Step 1 until it stops at local optima. 
According to the above-mentioned algorithm, it X  X  at Step 1 that SEM exploits a parametric EM algorithm to learn the parameters of the current network. It X  X  also the most expensive step because of the co mputation of the expected statistics. Fortunately, Step 2 is a local search procedure that changes one arc at each move and can efficiently evaluate the gains by adding or removing an arc. Such a procedure can also re-use the computations performed in the previous stages to evaluate changes to how to reduce the computations of Step 1 is crucial for reducing the time complexity of SEM. 
Based on the above observations, starting with a parallelized procedure for Step 1 to compute the expected statistics, our parallel SEM algorithm is proposed in this paper to reduce the time complexity of learning BN structure. At Step 1, most of the time of SEM algorithm is spent on choosing the optimal parameters for M i using the EM algorithm. Therefore, PL-SEM uses a parallel parameter learning algorithm for SEM. PL-SEM reduces the computations in this step by parallel computing the expected statistics and the parameters of the underlying BN, so it improves the efficiency of learning BN. 3.1 Parallel Computing of the Expected Statistics When learning the current network parameters using the EM algorithm with missing data or hidden variables, at the E-step, EM uses expected statistics instead of sufficient statistics to complete missing data. Computing the expected statistics needs to compute simple example is shown in Figure 1. 
According to Figure 1, we can conclu de that the E-step consists of two components: 1) constructing a proper inference engine; and 2) computing the expected statistics. 
If the parameters  X  are the joint probability distribution of the network and are available for each processor, the same operation can be performed on each sample simultaneously. We parallelize the loop by evenly distributing the samples across parallel processors. If we partition the N samples into P blocks, each processor handles roughly N/P samples. The j X  X h processor is given a responsibility for samples N , where i=(j)(N/P)+1, ..., (j+1)(N/P). 
Data parallelization means that the same operation can be performed on different data items simultaneously. The E-step also repeats computing Eq.(6) on each sample at the same time. Therefore, the E-step is inherently data parallel. From Figure 1, the computation of Eq.(6) for each sample is independent. Therefore, the E-step avoids the cost of communication between different processors and makes it up to maximal parallelism because of data parallelization. It is an important property of data samples. Therefore, we can exploit more processors to improve the learning efficiency of PL-SEM. 3.2 Parallel Computing of BN Parameters M-step uses the sufficient statistics to learn the MLE (Maximum Likelihood Estimation) parameters for the current network. With the complete data, the M-step can decompose the likelihood function L(  X  :D) by exploiting the inherent conditional independence of BN. Let X={X 1 ,X 2 ,...X n }be the random variables of BN, x i [m] be a instance of X i in the m X  X h sample, Pa i [m] be the instances of the parents of X i , and the parameters  X  be ready to be evaluated, Then L(  X  :D) is as follows. 
Therefore the likelihood function is decomposed into n independent local likelihood functions. With the sufficient statistics, we can further decompose L i (  X  i :D). Let N(X i ,Pa i ) be the sufficient statistics of X i . 
Unfortunately, with incomplete data, the sufficient statistics can X  X  be computed from samples directly, and the likelihood function L(  X  :D) no longer has the property of decomposition. 
But the good news is that the M-step can exploit the expected statistics computed the decomposition property. Therefore, the M-step can also be parallelized. 
Assuming  X  L and  X  G are the local and global parameters of the current BN model, respectively, in the traini ng process, a processor P 0 first gets a random structure of the BN and learns parameters  X  G from it and the data D. Then, P 0 distributes these parameters to other available processors by using MPI_Bcast, which is one of the basic functions of the MPI (Message-Passing Interface) library [14] and all other nodes parameters  X  G to compute the expected statistic s for its partition. Thirdly, each processor P j needs to exchange its expected stat istics with others using MPI_Bcast to compute the local parameters  X  L . At last, according to the number of random  X  global parameters  X  G . MPI_Allreduce is also a MPI function which combines values from all the other nodes to the root node and distribute the results back to all independently decide when it should exit the loop. Unfortunately, each processor needs to communicate with others at the M-step, and this leads to some time cost. 3.3 Outline of the PL-SEM Algorithm According to the above analysis, the outline of PL-SEM is as follows. 
Step 1. Input the training data D; i=1; initiate the structure of BN and  X  G at P 0; and distribute it to all available processors by using MPI_Bcast. Step 2. Assign the samples to processor P j. Step 3. Parallel compute the optimal parameters of the current network. computes Eq.(6) for its partitions. Parallel M-step: each processor P j exchanges its expected statistics using MPI_Bcast. According to th e number of random variables, PL-SEM assigns the available processors to calculate each local parameter  X  L and calls MPI_Allreduce to sum up the local parameters  X  L to obtain the new global parameters  X  G. 
Step 4. With the completed data, PL-SEM performs a local change on the structure of the candidate network until it finds a network that most fits the data. Step 5. i=i+1. 
Step 6. If i&lt;Maxitetaion or  X  G has not converged, goto step 3; otherwise return. We have developed our parallel source code on a high performance computing cluster that has the following configurations: (Xeon 3.0G(2M)*2, 1G DDR400*2, SCSI 73G)*9, which means that there is a master node and 17 computing nodes, Linux OS, OSCAR cluster management system and the cl uster is connected by Gigabit Ethernet. The processors communicate with each other by using MPI. 
We have chosen two Bayesian networks on the Web [15]: Asian and Alarm. Asian network is a popular Bayesian Network with 8 discrete nodes/valuables taking 2 values, which could be used to diagnose patients arriving at a chest clinic. The Alarm network was constructed from expert knowledge as a medical diagnostic alarm message system for patient monitoring. The domain has 37 discrete nodes/valuables taking between 2 and 4 values, connected by 46 directed arcs. 
Fig.2 shows the execution time of PL-SEM for the Asia network with 10% and 30% missing data and 1000, 2000 and 3000 samples, respectively. From this figure, 1062 K. Yu, H. Wang, and X. Wu increase of the processors, es pecially the number of the processors up to 12, the difference in the execution time is minimal between 1000 and 3000 samples when the missing date are 30%. 
Fig.3 depicts the execution time of PL-SEM on the Alarm network which is far more complex than the Asia network. With 10% missing data and 1000 samples, the execution time at one processor is more than 1800 seconds. With the increase of time is reduced to around 100 seconds. With 5000 samples and 20 % missing data, the time reaches 58080 seconds (about 16 hours). Fortunately, PL-SEM reduces it to 2000 seconds or so (about 0.6 hours) and the speed-up also arrives at 20 when the number of processors is up to 16. Th erefore, PL-SEM effectively reduces the execution time. (Note that all missing data of the samples are produced at random.) 
Due to data parallelization at the E-step, PL-SEM effectively avoids the cost of communication with the increase of the pro cessors. Unfortunately, at the M-step the processors need to communicate with each other. The time cost of communication will ascend with the increase of the proces sors. This affects the performance of PL-SEM to some extent. Learning Bayesian networks with incomplete data is currently a hot research topic. Existing research efforts lay a heavy emphasis on how to avoid stopping at local optima. There is little research on improving the time complexity. With the increases of missing data and hidden variables, the computation is very huge. Therefore, many algorithms for learning Bayesian networks (with missing data) can X  X  effectively learn from large-scale samples. In this paper, the PL-SEM algorithm has been presented based on the EM framework, and it exploits a parallel algorithm to reduce the computation of the expected statistics . PL-SEM does not take into account the problem of local optima. Fortunately, at Step 4, PL-SEM can choose another stochastic simulation such as the genetic algorithm, simulated annealing or MCMC (Monte Carlo Markov Chain) to perform local search to avoid local optima. Therefore, PL-SEM has provided a framework for parallel structure learning. 1. Ghahramani Z. An Introduction to Hidden Markov Models and Bayesian Networks. 2. Chickering DM, Heckerman D. Efficient approximations for the marginal likelihood of 3. Schwarz G. Estimating the dimension of a model. Ann. Stat., 6: 461-464, 1978. 4. Wang S-C, Yaun S-M. Research on Learning Bayesian Networks Structure with Missing 5. Friedman N. The Bayesian Structural EM Algorithm. UAI-98 , 1998. 6. Tian F, Lu Y, Shi C. Learning Bayesian Networks with Hidden Variables Using the 8. Luna JEO, Zanusso MB. Revisited EM Algorithms for Learning Structure and Parameters 9. Chickering DM, Heckerman D, Meek C. Large-Sample Learning of Bayesian Networks is 10. Anderson TE, Culler DE, Patterson DA. A Case for NOW. IEEE Micro , vol. 15, no. 1, 54-12. Lam W, Segre AM. A Distributed Learning Algorithm for Bayesian Inference Networks,
