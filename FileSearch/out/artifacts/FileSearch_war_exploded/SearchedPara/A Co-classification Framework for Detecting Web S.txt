 Social media are becoming increasingly popular and have attracted considerable attention from spammers. Using a sample of more than ninety thousand known spam Web sites, we found between 7% to 18% of their URLs are posted on two popular social media Web sites, digg.com and de-licious.com . In this paper, we present a co-classification framework to detect Web spam and the spammers who are responsible for posting them on the social media Web sites. The rationale for our approach is that since both detection tasks are related, it would be advantageous to train them simultaneously to make use of the labeled examples in the Web spam and spammer training data. We have evaluated the effectiveness of our algorithm on the delicious.com data set. Our experimental results showed that the pro-posed co-classification algorithm significantly outperforms classifiers that learn each detection task independently. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Algorithms, Experimentation, Measurement, Security Social Media, Web Spam, Classification
Web spamming refers to any deliberate activity to pro-mote fraudulent Web pages in order to mislead Web users into believing they were viewing legitimate Web pages. Since search engines play a pivotal role in guiding users to find relevant information on the World Wide Web, much of the early spamming activities were directed toward misguiding search engines into ranking spam pages unjustifiably higher. Despite the extensive research in this area, Web spam de-tection remains a critically important but unsolved problem because spammers may adjust their strategies to adapt to the defense mechanisms employed against them.

Social media are becoming increasingly popular and have attracted considerable attention from spammers. From a list of 94 , 198 spam Web sites extracted from a benchmark email spam data [9], we found 6 , 420 (  X  7%) of them were posted at digg.com and 16 , 537 (  X  18%) of them were posted at delicious.com . While there has been considerable re-search to detect spam from hyperlinked Web pages to im-prove search engine performance, spam detection from social media is still in its infancy, with existing work focusing pri-marily on detecting spam in blogs [3, 7, 6] and online review forums [4]. Unlike spam detection from hyperlinked Web pages, there are richer amount of data available in social media that can be utilized for Web spam detection, such as the links between users, hyperlinks between the Web con-tent, and links between users and the Web content. Users may also assign a set of keyword tags to annotate the Web content they have posted. Therefore, a key challenge is to systematically incorporate all the heterogeneous data in a unified framework to improve Web spam detection.

In addition to Web spam detection, it is useful to iden-tify the spammers who are responsible for posting such links to prevent them from future spamming activities. This is a challenging task because some legitimate users may unknow-ingly post links to spam Web sites, while some spammers may deliberately add links to non-spam Web sites to avoid detection. This paper assumes that spammers tend to post considerably higher amount of Web spam compared to non-spammers. Since both Web spam and spammer detection tasks are related, it would be advantageous to train their classifiers simultaneously to make use of their labeled exam-ples. To the best of our knowledge, there has not been any prior work on the joint detection of Web spam and spam-mers, an approach which we termed as co-classification .
This paper presents a co-classification framework for Web spam and spammer detection in social media based on the maximum margin principle. Specifically, we formalize the joint detection tasks as a constraint optimization problem, in which the relationships between users and their submit-ted Web content are represented as constraints in the form graph regularization. To ground the discussion of our frame-work, we use as an example the social bookmarking Web site delicious.com . While the concepts in this paper are pre-sented for the social bookmarking domain [2], our proposed framework is applicable to other social media Web sites where the following data are available: (1) links between users, (2) links between users and their submitted Web con-tent (social news, blogs, opinions, etc), and (3) tags or other content-based features derived from the Web content. We also show that our framework is applicable to both super-vised and semi-supervised learning settings. Experimental results using the delicious.com data set showed that our co-classification framework significantly outperforms classi-fiers that learn each detection task independently.
We begin with a brief discussion of the terminology and notations used in this paper. While the terminology intro-duced here are based on the features available at delicious. com , they are also applicable to other social media Web sites.
The overall data can be represented as a graph G = ( V, E ), where V = B X  X  is the set of nodes (bookmarks and users) and E = E b  X  E u is the set of links. Our work is based on the following two assumptions: (1) If ( u, b )  X  E b and b is a spam bookmark, then u is more likely to be a spammer. (2) If ( u, v )  X  E u and v is a spammer, then u is also likely to be a spammer. As will be shown later in Section 3, these assumptions are enforced as graph regularization constraints in our proposed co-classification framework.

Throughout this paper, matrices are denoted by boldface capital letters like X and vectors are denoted by boldface small letters like y (for column vector) or y T (for row vec-tor), where T is the transpose operator. Elements of matri-ces and vectors are of the form X ij and y j , respectively.
In this section, we first formalize the Web spam and spam-mer detection problems. We then present the derivation of our proposed co-classification framework.
Suppose we are given: Furthermore, let X b (or X u ) be a matrix whose i th row corresponds to the feature vector of bookmark (or user) i ,
Given V b , V u , E b , and E u , the goal of Web spam and spammer detection tasks is to learn a pair of classifiers: (1) f ( x ( b ) ) that accurately maps a bookmark x ( b ) to its class label y ( b )  X  X  X  1 , +1 } and (2) f u ( x ( u ) ) that accurately maps a user x ( u ) to its class label y ( u )  X  X  X  1 , +1 } .
The classifiers used in this paper are extensions of the least-square support vector machine (LS-SVM), a variant of maximum margin classifier proposed by Suykens et al. [8]. Specifically, we may construct a LS-SVM classifier for bookmarks by minimizing the following objective function:
L b ( w b , b b , e ,  X  ) = Similarly, the corresponding objective function for classify-ing users can be expressed as follows: L ( w u , b u ,  X ,  X  ) = 1 However, by training them independently, the classifiers do not make use of the labeled examples in the Web spam and spammer training data as well as the link information be-tween the users and their corresponding bookmarks.
Instead of solving the optimization problems for classify-ing bookmarks and users independently, our co-classification framework utilizes additional information from the link struc-ture between users and bookmarks in E b to ensure that their solutions are consistent with each other.

The objective function for our co-classification framework of bookmarks and users can be written as follows: where ( w b , w u , b b , b u , e ,  X  ,  X  ,  X  ) are the model param-eters to be estimated from training data and (  X  1 ,  X  2 ,  X   X  ) are the user-specified parameters. For our experiments, we set  X  1 =  X  2 = 1 whereas  X  3 and  X  4 are estimated from the data via ten-fold cross validation. The term associated with  X  3 is used to enforce the constraint due to links be-tween users. To understand the rationale for this term, note that the value of the objective function increases whenever w of a spammer). Thus, the graph regularization term can be viewed as penalizing models that assign non-spammers as fans of spammers. This idea was inspired by prior works on using graph regularization methods to combine link struc-ture and content information in Web pages [10, 1]. The parameter  X  ij is the weight of the directed edge E u ( i, j ). In-stead of assigning a binary 0/1 weight to every pair of nodes (users), we normalize the weight based on the out-degree of the source node, i.e.,  X  ij = 1 /
Similarly, the last term in the objective function is used to penalize models in which non-spammers are allowed to bookmark many spam pages: where  X  i,j = 1 /
The objective function given in (1) can be solved in a supervised or semi-supervised learning setting, depending on the nodes used to express the graph regularization con-straints in E b and E u . In a supervised learning setting, E b and E u involve only bookmarks and users that are part of the labeled training data. For semi-supervised learning, the constraints due to E b and E u in (1) include unlabeled bookmarks and users as well. The solution of the objective function is obtained by taking the derivative of L with re-spect to each of the model parameters and setting them to zero. This reduces to a system of linear equations that can be expressed in matrix notation as follows: where I d is a d  X  d identity matrix, 0 d is a d -dimensional vector of 0s, 1 d is a d -dimensional vector of 1s, and The block structure of the matrix equation suggests that the system of linear equations can be further decoupled into two subproblems, one for learning the parameters of the book-mark classifier and the other for user classifier. Equations (2) and (3) can be solved for the model parame-ters (  X ,  X , b u , b b ). Furthermore, w u and w b can be expressed in terms of  X  ,  X  , Z u and Z b . As a result, a user x ( u ) a bookmark x ( b ) test can be classified as follows: f u w the proposed co-classification framework assumes a linear classifier, it can be extended to non-linear models using the well-known kernel trick . However, we have considered only linear kernels in this study.
 Algorithm 1 summarizes the high-level overview of our Co-Class algorithm. The LinearSolver function solves the sys-tem of linear equations given in (2) and (3). The Classify function takes as input the model parameters and unlabeled examples to generate their predicted class labels. Algorithm 1 Co-Class Algorithm
This section presents our experiment results to demon-strate the effectiveness of our Co-Class algorithms. We use a real-world data set acquired from delicious.com to eval-uate our algorithms. The dataset consists of posting history for nearly 3 million users, whose feature vectors are gen-erated from about 2 . 5 million tags. In addition, the data also contains about 110 , 000 bookmarks, whose feature vec-tors include about 300 , 000 tags. The user-bookmark links, E b and user-user links E u are also obtained by preprocess-ing the user profile pages. URLs harnessed from the email
Training data Bookmark User = 10% Recall Precision F 1 Recall Precision F 1
Training data Bookmark User = 30% Recall Precision F 1 Recall Precision F 1 spam benchmark data in [9] are used to label the spam book-marks. A user is labeled as a spammer if he/she posted to at least one of those spam bookmarks. To test the perfor-mance of our algorithms, we randomly selected a sample of 20 , 000 bookmarks and 20 , 000 users. 20% of the data set were identified to be spam (or spammers) and the remaining 80% were non-spam (or non-spammers).

For comparison purposes, we use SVM-light [5] to build a pair of support vector machine classifiers that learn the clas-sification of users and bookmarks independently using their respective training data. We reported the precision, recall, and F 1 -measures after performing repeated 10-fold cross val-idation on the data. To make the problem more challenging, for each fold, we use 10% of the data for training while the remaining 90% are used as test data. We repeated the ex-periment 20 times by sampling another 20 , 000 bookmarks and users from the original data. Table 1 shows the relative performance of linear and nonlinear SVM against the semi-supervised Co-Class algorithm. Clearly, the F 1 -measure for Co-Class is significantly higher than that for SVM, both in terms of classifying bookmarks and classifying users. The F -measure for bookmark classification is higher than that for user classification but the margin of improvement using Co-Class is larger in terms of classifying users than classi-fying bookmarks. We suspect this is due to the additional information in the link structure of E u , which helps to boost the performance of our co-class algorithm when classifying users. The results also showed that most of the improvement in Co-Class is due to its higher recall value. Since the data set is moderately skewed, SVM tends to classify many of the examples into the larger class, which explains its lower recall value. In contrast, graph regularization using the link struc-ture in E b and E u allows our Co-Class algorithm to identify more spam and spammers from the data set without losing its precision. We also increased the percentage of training data during 10-fold cross validation from 10% to 30%. The results in Table 1 showed our algorithm significantly out-performed linear SVM, though the margin of improvement is less than that obtained using 10% training data.
This paper focuses on the problem of Web spam and spam-mer detection in social media Web sites. Unlike search en-gine spam, Web spam from social bookmarking and social news aggregator Web sites are potentially damaging because it may direct users to malicious Web sites that compromise browser security. To overcome this problem, we presented a co-classification framework that simultaneously trains clas-sifiers for detecting Web spam and spammers. We demon-strated that such a strategy is more effective than learning each task independently. For future work, we plan to ex-tend the methodology to incorporate data from multiple so-cial media Web sites. For example, an interesting research direction is to investigate the co-classification of bookmarks and users from both delicious.com and digg.com . Further-more, it would be useful to investigate methods for reducing the number of user parameters in our learning framework.
This research was partially supported by ONR Grant Num-ber N00014-09-1-0663 and the Army Research Office. [1] J. Abernethy, O. Chapelle, and C. Castillo. Web spam [2] F. Chen, J. Scripps, and P. Tan. Link mining for a [3] K. Ishida. Extracting spam blogs with co-citation [4] N. Jindal and B. Liu. Opinion spam and analysis. In [5] T. Joachims. http://svmlight.joachims.org/. [6] G. Koutrika, F. A. Effendi, Z. Gyongyi, P. Heymann, [7] Y. Lin, H. Sundaram, Y. Chi, J. Tatemura, and B. L. [8] J. Suykens, T. Gestel, J. Brabanter, B. Moor, and [9] S. Webb, J. Caverlee, , and C. Pu. Introducing the [10] T. Zhang, A. Popescul, and B. Dom. Linear prediction
