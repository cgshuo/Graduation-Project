 For increasingly sophisticated use cases end users often need to extract, combine, and aggregate information from various (often dynamically generated) web pages from multiple we bsites. Current search engines do not focus on combining information from various web pages in order to answer the overall information need of the user. Semantic Web and Linked Data usually take a static view on the data and rely on providers X  cooperation. In this paper, we present a novel approach that enables end users to easily extract data from web pages while they browse, store it locally in their browser as well as structure, integrate and search such data. We propose Datalog rules for integrating and searching the extracted data. We show how cleaning steps and integration rules can be reused to accelerate the cleaning and integration of extracted data. The proposed approach is implemented as a browser plugin. We present its implementation details and report on our evaluation of the plugin concerning user experience and browsing time saving. H.3.3 [ Information Search and Retrieval ]: Query formulation, Search process, Selection process; H.3.5 [ On-line Information Services ]: Data sharing, Web-based services Web Data Extraction, Web Data Integration and Search
For many practical purposes end users need information that is scattered across multiple websites. Even though current search en-gines index the content of most of the web pages, in many cases, end users still have to do a lot of work manually to compile the required information together. For example, as of today, Google does not deliver satisfactory results for queries similar to  X  X rogram chairs of all previous CIKM conferences X . In contrast what typical search engines deliver, complex information need requires com-plementary information that, if combined, satisfy the information need of an end user. In order to obtain the required information an end user has to pose multiple queries to a search engine, browse through the results, extract and aggregate the relevant information fragments outside of the found web pages.

The Semantic Web [4] addresses the need for semantic descrip-tions of web page content while Linked Data [5] focuses mainly on data hosted on web servers that may or may not be connected to web pages. Both Semantic Web and Linked Data rely on providers X  cooperation for annotating their websites or providing semantic data. Information retrieval has focused on analyzing such browsing paths or click trails of millions of users mainly for the purpose of improving web search results. Click trails can be used as endorse-ments to rank search results more effectively [12], trail destination pages can themselves be used as search results [14], and the con-cept of teleportation can be used to navigate directly to the desired page [13]. The statistics based click analysis methods typically fail to consider semantics of user queries and pages. Furthermore, the models cannot differentiate whether a frequently used path actually satis fi es the information need or not.

In this paper, we propose a system that allows end users to easily extract information from web pages that they fi nd important, inte-grate the information with the help of Datalog rules, and query the information in structured way. The extracted information and the rules are stored locally in the browser. We deal with the extrac-tion of data from web pages in Section 2. Our extraction technique enables end users to copy fragments of information from different web pages during normal browsing, and automatically converting the copied information into a relational table. In Section 3, we show how end users are able to clean the newly extracted infor-mation, give it meaning, and integrate it with the already existing information. We have implemented the approach as browser plu-gin. We present the implementation details and our fi rst evaluation results in Section 4. We conclude in Section 6 after discussing re-lated work in Section 5.
In this section, we present how end users can easily extract in-formation from any web site and store it locally as tables. A typi-cal example of such information could be submission deadlines or important dates of relevant conferences. Having such information in structured form locally can already save some time. If the in-formation extracted at different times from different sources could be easily integrated a user can draw even more value from the ex-tracted information. In the second part of this section, we present how the extracted information can be integrated by an end user.
The extraction step converts n HTML elements to a table. The extraction algorithm presented in Algorithm 1 is called when an end user selects n HTML elements for data extraction. Before invoking the algorithm a node is added as the parent node of the n selected
Algorithm 1: Extraction 1 compress(element); 2 data  X  new Array(); 3for i=0; i &lt; element.childNodes.length; i  X  i+1 do 4 row  X  new Array(); l  X  0; 5for j  X  0; j &lt; element.childNodes[i].childNodes.length; 6 cellValue  X  element.childNodes[i].childNodes[j].nodeValue; 7if cellValue AND cellValue.trim().length&gt;1 then 8 row[l]  X  cellValue.trim(); l  X  l+1; 9 textnodes  X  textNodesUnder(element. 10 childNodes[i].childNodes[j]); 11 for var k=0; k&lt;textnodes.length; k  X  k+1 do 12 cellValue  X  textnodes[k].nodeValue; 13 if cellValue AND cellValue.trim().length&gt;1 then 14 row[l]  X  cellValue; l  X  l+1; 15 data.push(row); elements. The algorithm is called with the newly added root node as input. As fi rst step, it invokes Algorithm 2 for compressing the input DOM tree.

Algorithm 2: Compression of an HTML Tree 1 removeEmptyChildNodes(element); 2 child  X  element. fi rstChild; 3 while child do 4 compress(child); child  X  child.nextSibling; 5 removeEmptyChildNodes(element); 6if !element.nextSibling AND !element.previousSibling AND 7 element.parentNode then 8 child  X  element. fi rstChild; 9 while child do 10 child  X  c.nextSibling; element.parentNode. 11 appendChild(element. fi rstChild);
Algorithm 3: Remove Empty Child Nodes 1 child  X  element. fi rstChild 2 while child do 3if !child.textContent OR 4 temp  X  child.nextSibling; 5 child  X  temp; 6continue ; 7 child  X  child.nextSibling;
We are interested only in the text nodes in the DOM tree. How-ever, if we simply discard all nodes except the text nodes we will lose the information about the existence of the relationships among the text nodes. If we do not remove any nodes at all, then we will often obtain a tree in which there is only one node at the fi rst level. This is because in many cases a node is appended to an element for structuring and layout purposes. The key idea of Algorithm 2 is to remove every node that has no siblings by moving its children one level up in the tree. In order to remove any nodes that have no text content Algorithm 2 makes use of Algorithm 3 before and after invoking itself recursively for its child nodes.

The tree obtained after termination of the initial call of Algo-rithm 2 for the root node has the following properties: (i) every node (except text nodes) has at least one direct or indirect child node with text content, and (ii) there is no node (except text nodes) that is the only child of its parent node. When the compression is fi nished, Algorithm 1 interprets the compressed DOM tree as a tree of height two. The nodes at the fi rst level correspond to the n se-lected HTML elements, and the nodes at the second level represent the property values of their corresponding parent node. Since the relations between the fi rst level nodes and the second level nodes are unknown, the extracted tree can be simply represented as a two-dimensional array in which the fi rst dimension represents entities corresponding to the n selected HTML elements, and the second dimension represents the values of some properties of the corre-sponding entity at the fi rst level. Algorithm 1 creates such a two-dimensional array after the compression step. Note that the proce-dure  X  X extNodesUnder X  returns an array of all the direct and indirect descendant text nodes of a node. Since Algorithm 1 is general in the sense that it can extract data for any n HTML elements of any type such as HTML tables, HTML lists, and HTML DIV elements etc., as long as they have text content.

Table 1 shows a snapshot of the bibliography data of Jeffery Ull-man X  X  publications extracted from his DBLP web page 1 . Note that in case of DBLP author publication pages our algorithm is able to separate the authors, the title, p roceedings, pages etc. even though they are shown as one entry in the original web pages. This is because the extraction algorithm stretches the breadth of a row, i.e. number of columns of stretched to maximum. However, this comes with a price that the columns are sometimes misaligned (note e.g. that not all the titles are in the same column).

The problem of misalignment of columns is solved to a large extent in the compact version of the extraction algorithm. We ob-tain the compact version of Algorithm 1 by replacing Lines 6 by cellValue  X  element . childNodes [ i ] . childNodes [ j ] . textContent ;and removing Lines 9 to 14. In case of DBLP author publication pages the extracted table will have the same structure as the original table, except that our algorithm skips second column of the original table since it does not have any text content. Table 2 shows a snapshot of the data about the MIT Computer Science faculty members ex-tracted from the MIT CS page 2 ,andTable3thesameDBLPtable that was base of the table presented in Table 1 with the compact version of our algorithm. Note that even though the information on the source web page is not in the table format, the table produced by our extraction algorithm is well structured. For example, in contrast to the expanded version, the compact version aligns all the group memberships of a faculty member appear in the last column.
While the generality of the extraction algorithm enables extrac-tion of data from not only HTML tables but from any HTML el-ements, it only delivers an intermediate table that a user in most cases would wish to edit before storing it persistently. For exam-ple, our current extraction algorithm does not try to extract property names but generates random property names (not shown in Algo-rithm 1). Furthermore, the extracted data needs to be integrated with the already existing data so that it can contribute to answers to queries de fi ned using a uni fi ed querying schema. Table 3: Snapshot of publications of Jeffrey Ullman extracted from DBLP with the compact version of Algorithm 1
Manual cleaning, aligning and integration of data in an extracted table is tedious in most if not all cases. Especially, when a user has to repeat the same steps for similar data extracted from the same website. Consider for example the case that an end user has ex-tracted the bibliography data of the publications of  X  X effrey Ull-man X  from DBLP, and edits and annotates the extracted data man-ually. Even though having the structured data locally is very useful the end user may not want to perform all the editing steps again for the bibliography data of the publications of  X  X onald Knuth X  that he/she extracts at some other point in future. Automatic techniques usually require signi fi cant computing resources, background knowl-edge and time [11] that is not given in our end user based system that is supposed to run entirely inside a web browser.

Therefore, we aim at a technique in which user X  X  table cleaning actions are de fi ned in such as way that they can recorded and reused
Figure 1: Example cleaning rule for extracted table in Table 1 for other tables if applicable. However, if arbitrary cleaning is al-lowed, the cleaning steps may not be reusable for similar data. For example, an end user that is not interested in Donald Knuth X  X  pub-lications after 2010 removes top k rows from the bibliographic data of  X  X onald Knuth X  at DBLP 3 (as of today k = 5). Applying the same cleaning steps to the bibliographic data of  X  X effrey Ullman X , would lead to unwanted results. Therefore, we only allow cleaning steps that are de fi ned for all rows of an extracted table.
A cleaning rule takes a table as input. In our example, the clean-ing rule in Figure 1 takes table t3 as input which is the table of Jeffrey Ullman X  X  publications extracted from DBLP and a snapshot of which is presented in Table 3. Table t3 has 2 columns named and B in the rule but we could have chosen different names as well. The body of a cleaning rule can contain further atoms (predicates along with their arguments). The atoms in the body of a rule are connected by the symbol  X  &amp;  X .

In our example, we have in total six atoms in the body, i.e. we have fi ve more atoms after apart from the atom corresponding to the input table. The second atom removes a row if the value in column B is empty. (see the fi rst row of the table in Table 3). In the second atom "blank" denotes a system constant for referring to an empty value. The matches predicate takes four arguments: (i) the input variable, (ii) a regular expression, (iii) the begin index and optionally the end index for selecting the elements of the array containing the matches, and (iv) the output variable to contain the selected elements.The third atom binds the fi rst match of the reg-ular expression [^:]+ (which means all characters except  X : X ) in the value of the variable B to the output variable C . After the eval-uation of this atom variable C will contain the author names of the publication in the current row (see in Table 3 that the author names and the rest of the bibliographical data are separated by a  X : X ). The next matches atom binds the bibliography information other than the author names to a variable D . We store this information in only in order to process it further (Note that D is not carried to the view being de fi ned by this rule). The next atom binds the variable E to the fi rst match of all characters except  X . X  in D .Thatis,the evaluation of this atom causes E to contain the title of the publi-cation in the current row. Finally, the last atom binds variable to all the information appearing after the title such as conference, year, volume, number, publisher etc. The cleaning rule in our ex-ample de fi nes a view ct3 with the columns A , C , E ,and the latter three columns are computed in the rule body as described above while the fi rst column remains same as in the input table. It is worth mentioning that this cleaning rule works perfectly for all tables containing bibliography data of any author extracted from DBLP. The resulting table ct3 is presented in Table 4.
The cleaned tables can be easily integrated by writing appropri-ate Datalog rules with the already existing data in user X  X  database. We adopt the so-called  X  X lobal as view X  or GAV approach [9]. In the GAV approach the target/query schema is de fi ned in terms of the source relations. For example, the rule pubOf("Jeffrey Ullman",A,B,C,D):-ct3(A,B,C,D) states that the publications in relation ct3 are publications of Jef-frey Ullman. The rule de fi nes the relation pubOf in terms of the source relation ct3 . There can be more than one rule with the same predicate in the head. In such a case the head predicate contains the union of the tuples satisfying the bodies of the rules. For example, assuming a predicate csPubs that means Computer Science pub-lications, and a clean table ct4 similar to ct3 but containing the publications of Donald Knuth. The rules together de fi ne the view csPubs as the union of publications of Jeffrey Ullman and publications of Donald Knuth.

The real power of integration rules lies in their ability to de fi ne views over multiple tables. Consider for example the clean table corresponding to Table 2 and lets call it ct2 .Therule de fi nes that the values in column A of table ct2 are faculty mem-bers. Now a view, say facPubs , that contains publications of only faculty members can be easily de fi ned with the rule facPubs(B,C,D,E):-pubOf(A,B,C,D,E)&amp;faculty(A) .

As in the case of cleaning rules, manually de fi ning integration rules for each table completely can become tedious with time. Now we show how our system suggests a set of integration rules for a supplied table.

Step 1: Suppose there are m clean tables and n rules in total. We create m groups of rules where each rule in i th group has table i in the body. Grouping the rules in this way can be done in O ( n ) time as it can be done by simply iterating over all the rules. Note that a rule may be in more than one group since the body of a rule may contain more than one clean table.

Step 2: For a given clean table that is supposed to be integrated we identify relevant (potentially applicable) rule groups by com-paring the arity of the clean table with that of the m clean tables. We discard the rule groups of the tables that have different arity and continue with the remaining rule groups. Now we rank the selected rule groups by the provenance (origin) of their corresponding ta-bles. Assuming that the table ct3 has already been integrated and an end user wishes to integrate the table ct4 , the above method can fi nd the group containing the rules: pubOf("Jeffrey Ullman",A,B,C,D):-ct3(A,B,C,D) csPubs(A,B,C,D):-ct3(A,B,C,D) .

Ste p 3: The suggested rules are not always directly applicable, e.g., in case of ct4 the constant "Jeffrey Ullman" must be replaced by "Donald Knuth" . In general, table dependent con-stants may occur in the body as well. Therefore, before applying the rules for the new clean table, our system asks the end user to provide replacement values for each such constant.

Once the tables have been integrated and views have been de-fi ned, it is rather straightforward to query the information inside the tables. In the simplest form a query predicate is just a predicate (a table name or a view name). Complex queries can be speci fi ed by rules that de fi ne the query predicate in the same way as views are de fi ned. In general, more than one rules may be used to de fi ne the query predicate, where each such rule de fi nes an intermediary view. Note that it is also possible to de fi ne and query recursively de fi ned views. In order to answer a query, the system fi rst creates the overall rulebase by adding the query rules, if there are any, to the rules that map the clean tables to the query schema (as described above). Then, the system imports all the data in all the clean tables as facts. More precisely, each row in each clean table is imported as a fact. Finally, the system evaluates the query predicate in the back-ward chaining fashion. That is, for evaluating the head predicate of a rule the system fi rst evaluates the body predicates. The leaves of such an evaluation tree are extensional predicates that correspond to the clean tables, and their evaluation consists of direct retrieval of the corresponding facts.
We have implemented a browser plugin called  X  X mart Web Brows-ing Plugin X  for Chrome and Firefox. In this section, we present implementation details of the plugin and results of its evaluation.
The user interface of the plugin is seamlessly integrated in the browser. Figure 2 presents the architecture of the Smart Web Brows-ing plugin for Chrome. Regarding the communication between the various components, it holds that the front end scripts communicate with the backend scripts via asynchronous message passing. Also the frontend scripts at different levels cannot communicate directly with each other but only indirectly via backend scripts. Note that this is not our design choice but a requirement since the Chrome plugin architecture does not allow the upper level scripts to com-municate directly for security reasons.

The database symbol in the lowest layer represents the local stor-age object facility provided by Chrome. Local storage is a (key-value) data store provided by all modern web browsers to allow plugins and websites to store information locally in the browser of an end user. We map tables to the (key-value) model as fol-lows: for each table we store a (tableId, meta-information) pair, where meta-information is an array that among other contains the row count of the table. For each row of the table, we store a (rowId,rowdata) pair where rowId is concatenation of tableId and row number, and rowdata is an array of strings. The lowest layer also contains Javascripts that are instantiated once for the lifetime of the plugin. That is, as long as the plugin is active only one in-stance of the Javascripts for  X  X ackground X , and  X  X nformation Search X  exists. The purpose of the  X  X ackground X  Javascript is to act as a mediator between the backend scripts and the local storage as well as for communication between the upper level scripts.

The second layer from top contains javascripts that run in the context of the currently viewed web page. They provide methods for easier selection of HTML elements (for dragging and dropping into the plugin area). It not only supports selection of adjacent ele-ments such as rows of a table, but also selection elements that may be scattered across different web pages. For the purpose of remem-bering items it uses a clipboard that is provided by the Background Javascript. Finally, the Javascripts at the top most level provide the plugin UI. The UI uses JQuery UI items such as top menus, context menus and tabs for displaying different types of informations.
The plugin allows an end user to easily select HTML elements from any web page, and drag &amp; drop them into the plugin area. Upon such a drag&amp;drop action the extraction algorithm (refer to Al-gorithm 1) is invoked for the dropped HTML element. The ex-tracted table is shown to the user visually. We use the handsonta-bles Javascript library 4 for displaying the tables.

We have implemented functionalities for editing the extracted ta-bles on top of the handsontables Javascript library. In particular, we allow (i) addition/removal of rows, addition/removal of columns, and (ii) de fi ning values of the cells. For editing a table, a user needs to right-click on the table, row or cell (depending on the required edit action). This opens up a context menu in which the required edit action can be selected. Upon such a selection a dialog box appears in which the user can enter the condition and the desired change as described in Section 3.

The extracted tables are automatically saved in the local stor-age of the browser as soon as they are created. The clean tables are stored automatically after every edit operation. For saving the rules, we currently require the end user to press a button. In case of tables the table data is serialized as a two dimensional JSON array. The array and the table name is then sent to the background script via XMLHttpRequest. The background script parse the array and stores the table data in the key-value format as described above. Rules are stored as strings in the local storage. Currently, we use the key  X  X ules X  that is associated with a value of type array to store the rules.

We use the epilog Javascript library 5 for allowing end users to search in the tables in their local data base. Currently, the users have to write the Datalog query into a text box. We believe that a simpler interface on top of the Datalog engine is required to make the search usable for a wider range of end users.

More information about the Smart Web Browser plugin-in in-cluding screenshots can be found at the project web site 6
One of the main advantages of our system is saving the brows-ing time of end users for the case when they search for structured information. For evaluating our system we did a user case study with four users. We split them into two groups of two users each. Group 1 was equipped with the Smart Web Browsing plugin while Group 2 was not equipped with the Smart Web Browsing plugin. We gave both the groups the same set of information requirements and measured the average time they required for fi nding the re-quired information. The set of information requirements consisted of the following elements: (i) fi nd date and place of birth of all US presidents. (ii) fi nd bibliography information of the 3 latest jour-nal papers of each Stanford CS professor. (iii) count the number of Turing Awards for each country. For each of the information requirements members of Group 1 performed far better than the members of Group 2. Precisely, Group 1 required in average lit-tle over 2 minutes for the fi rst task while Group 2 required approx. 6 minutes for it. For task 2, Group 1 required little over 9 min-utes while Group 2 required little under 30 minutes for gathering the required information. For task 3 the performance of the groups was roughly equal since there exists a Wikipedia page that lists all Turing award winners along with the country they belong to.
There are a lot of server based or of fl ine systems such as [10] that try to learn structured data from unstructured documents. Such systems require a signi fi cant amount of documents, computation resources and time to be able to derive structured data from un-structured documents. While they solve their purpose very well, they are not directly applicable for our case. Ours is an end user oriented approach and we aim at an interactive system in which ex-traction, alignment and search need to happen in almost real time. We also do not aim at learning structured information from unstruc-tured documents, but leave it to the end user to decide which infor-mation is important enough that he/she would like to remember and share with his/her friends. From this perspective, DBPedia [2] come closer to our system. However, DBPedia is restricted to infor-mation in Wikipedia. With our system end users are able to extract structured information from any web page even if the web page lies in the so-called Deep Web [3]. Freebase [6] is another database that allows users to collaboratively create, structure, and maintain gen-eral human knowledge such as about popular people, movies, and sports etc. Public read/write access to Freebase is allowed through an HTTP based graph-query API using the Metaweb Query Lan-guage (MQL) as a data query and manipulation language.

There are a couple of browser plug-ins for recording a user X  X  browsing actions. Most prominent of them are perhaps a research prototype CoScripter [7], and a commercial system iMacros [8]. Both of them are very similar in their functionality as they allow users to record, share and execute their scripts. However, their sup-port is limited to browsing actions, i.e. do not support extraction, integration and search of data from the Web. They incorporate key-word based very basic search for scripts and as we mentioned in Section the scripts can be reused only for exactly the same use case as the one when they were recorded. With our technique for gener-alization of scripts we have increased the reusability of the scripts.
In [1] an approach for semi-automatically generating wrappers is presented. The key idea is to exploit the formatting information in pages from the source to hypothesize the underlying structure of a page. From this structure the system generates a wrapper that fa-cilitates querying of a source and possibly integrating it with other sources. Even though our scripts that relate to extraction of data can be seen as a wrapper, our scripts are immune to changes in the layout of the pages from where the data extracted. This is because we allow end users to manually select the HTML elements that they wish to extract the data from.
In this paper, we have presented a novel approach for involving end users in the creation of the structured data from web content in contrast to the original Semantic Web and Linked Data visions that mainly rely on providers to host annotated web pages or RDF data respectively. We have shown how end users can easily extract data from any web page, structure it, and store it locally. This is al-ready very useful since it allows structured search over the visited information at a later stage without needing to visit the same pages again. We have also presented how end users can integrate the ex-tracted data with the help of Datalog rules and how queries over the uni fi ed view on the data can be answered. The case of web pages that have semantic data available is a special case in which the ex-traction can be done automatically, cleaning becomes almost obso-lete, and integration step is much easier. The proposed approach is implemented as a browser plugin, called Smart Web Browsing plugin, that can be downloaded and used by everyone for free. We have presented implementation details of the plugin as well as well as reported evaluation results about user experience and saving of browsing time. Our proposed formalization of cleaning steps and integration rules enables their reuse, and thus accelerates the clean-ing and integration of extracted data. Development of automatic methods for fi nding appropriate cleaning and integration rules for a given table is planned as future work. [1] N. Ashish and C. A. Knoblock. Wrapper generation for [2] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, [3] M. K. Bergman. The deep web: Surfacing hidden value. The [4] T. Berners-Lee, J. Hendler, and O. Lassila. The Semantic [5] C. Bizer, T. Heath, and T. Berners-Lee. Linked data -the [6] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. [7] A. Cypher, M. Dontcheva, T. Lau, and J. Nichols. No Code [8] iOpus. Browser scripting, data extraction and web testing by [9] M. Lenzerini. Data integration: a theoretical perspective. In [10] A. Pivk, P. Cimiano, Y. Sure, M. Gams, V. Rajkovic, and [11] E. Rahm and H. H. Do. Data cleaning: Problems and current [12] A. Singla, R. White, and J. Huang. Studying trail fi nding [13] J. Teevan, C. Alvarado, M. S. Ackerman, and D. R. Karger. [14] R. W. White and J. Huang. Assessing the scenic route:
