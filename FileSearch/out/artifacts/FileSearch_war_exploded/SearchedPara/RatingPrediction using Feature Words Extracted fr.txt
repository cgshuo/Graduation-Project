 We developed a simple method of improving the accuracy of rating prediction using feature words extracted from cus-tomer reviews. Many rating predictors work well for a small and dense dataset of customer reviews. However, a prac-tical dataset tends to be large and sparse, because it often includes too many products for each customer to buy and evaluate. Data sparseness reduces prediction accuracy. To improve accuracy, we reduced the dimension of the feature vector using feature words extracted by analyzing the re-lationship between ratings and accompanying review com-ments instead of using ratings. We applied our method to the Pranking algorithm and evaluated it on a corpus of golf course reviews supplied by a Japanese e-commerce com-pany. We found that by successfully reducing data sparse-ness, our method improves prediction accuracy as measured using RankLoss.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval | information ltering
Rating prediction is a practically important problem, be-cause it enables the market to estimate how satis ed a cus-tomer will be with a service [1, 2, 5]. Given a dataset of customer reviews, rating predictors (like the Pranking al-gorithm [1]) can learn and predict the rating of a target customer. These algorithms work well for a small and dense dataset. However, the problem is that in practice a dataset is large and sparse, because it often includes too many prod-ucts for each customer to buy and evaluate (Figure 1). The dimension of the feature vector increases as fast as the in-crease in the number of products. For example, the matrix of the EachMovie dataset is reportedly sparse (5% full) [5]. To address this problem, we developed a way to reduce the di-mension of a feature vector using feature words extracted by analyzing the relationship between ratings and accompany-ing review comments. Figure 1 illustrates our approach. We Fi gure 1: An illustration of our approach in the case of restaurant reviews. Left: The original dataset has ratings only in the blue cells and the whole matrix is sparse. Right: Instead of using customers, we use the feature vector of feature words extracted from the review comments. This reduces the dimension of the feature vector from 6 to 2, so the whole matrix becomes dense. The score of each cell is an average of the scores rated with each word. reduced the dimension of the feature vector and increased the density of the matrix by extracting\delicious"and\terri-ble"as feature words. We extracted a word as a feature word when it appeared frequently and was accompanied with a polarized score. The words \delicious" and \terrible" ap-peared many times with high and low scores and therefore were selected as feature words. \Very" was not selected as a feature word; although it also appeared many times, its ac-companying score varied. Our method of analyzing ratings and accompanying review comments is a simple but powerful way of extracting feature words that can capture a charac-teristic of each rating aspect well. This is often dicult to implement by relying only on natural language-processing techniques or other corpora such as WordNet [3, 4]. As shown in Figure 1, the original dataset is often sparse. Crammer et al. proposed lling in blank cells with an inter-mediate score [1], which results in low prediction accuracy. We evaluated the relationship between data sparseness and the accuracy of the Pranking algorithm. Given a target customer and a product, the Pranking algorithm predicts the rating that the customer is likely to give the product by learning from the dataset how the customer has rated other products in the past. We used the Book-Crossing Dataset 1 , which is a relatively dense dataset because of the h ttp://www.informatik.uni-freiburg.de/ cziegler/BX/ translated each extracted word into English. nature of the service. The original matrix of this dataset is 15% full. We generated a sparse dataset by removing data from randomly selected cells and applied it to the Pranking algorithm. We evaluated prediction accuracy using Ran-kLoss [1], RankLoss = products, ^ y t is the t -th predicted output score, and y t -th desired output score. As shown in Figure 3 (a), the sparser the dataset becomes, the lower the prediction accu-racy becomes. This result motivated us to solve the problem of data sparseness by reducing the dimension of the feature vector.
We applied our technique to the Pranking algorithm and evaluated it on a corpus of golf course reviews supplied by a Japanese e-commerce company 2 . Each review was accompa-nied by a set of three aspects, each on a scale of 1-5: overall experience, golf course, and cost performance. To extract feature words related to each aspect, we computed the score for each word as the average of all rated scores accompanied by a review comment that included the word. The score is shown as \average" in Figure 2. As a result, the score of a word irrelevant to the aspect was close to the total average score, and a word that captured the characteristic of the as-pect had a high or low average score. Figure 2 shows that our method successfully extracts interesting feature words. Note that our method extracts not only positive and nega-tive words but also words that explain the semantic context of the aspect. For example, \Inoue" and \Seiichi," shown in the Course table, refer to a famous golf course designer who has designed many golf courses in Japan. The negative side of the Course table includes words such as \weed," \river," and \sand pit," from which one learns that a customer's low h ttp://rit.rakuten.co.jp/rdr/index.html rating is caused by complaints about the conditions of a golf course.

We used the extracted feature words as the feature vector of the Pranking algorithm and reduced the dimension to increase the density of the data. The original dataset had 85,000 customers and was sparse (0.5% full). We selected 100 feature words with high and low average scores, thereby reducing the dimension of the feature vector from 85,000 to 100. As a result, data density was increased (30% full). We evaluated the RankLoss (Figure 3 (b)-(d)), and our results outperformed the results of the original sparse dataset.
We have demonstrated that data density is important for prediction accuracy. We have developed a novel feature vec-tor using feature words that successfully improves prediction accuracy as measured using RankLoss. [1] K. Crammer. Pranking with ranking. NIPS'01 , pages [2] E. F. Harrington. Online ranking/collaborative ltering [3] M. Hu and B. Liu. Mining and summarizing customer [4] J. Kamps, M. Marx, R. J. Mokken, and M. de Rijke. [5] A. Sashua and A. Levin. Ranking with large margin
