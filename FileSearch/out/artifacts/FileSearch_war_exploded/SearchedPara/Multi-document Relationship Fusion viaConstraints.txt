 Single document information extraction of named entities and relationships has received much atten-tion since the MUC evaluations 1 in the mid-90s (Ap-pelt et al., 1993; Grishman and Sundheim, 1996). Recently, there has been increased interest in the extraction of named entities and relationships from multiple documents, since the redundancy of infor-mation across documents has been shown to be a powerful resource for obtaining high quality infor-mation even when the extractors have access to little or no training data (Etzioni et al., 2004; Hasegawa et al., 2004). Much of the recent work in multi-document relationship extraction has focused on the extraction of isolated relationships (Agichtein, 2005; Pasca et al., 2006), but often the goal, as in single document tasks like MUC, is to extract a tem-plate or a relational database composed of related facts.

With databases containing multiple relationships, the semantics of the database impose constraints on possible database configurations. This paper presents a statistical method which picks relation-ships which violate few constraints as measured by a probabilistic database model. The constraints are hard constraints, and robust estimates are achieved by accounting for the underlying extraction/fusion uncertainty.

This method is applied to the problem of con-structing management succession timelines which have a rich set of semantic constraints. Using con-straints on probabilistic databases yields F-Measure improvements of 5 to 18 points on a per-relationship basis over a state-of-the-art multi-document extrac-tion/fusion baseline. The constraints proposed in this paper are used in a context of minimally super-vised information extractors and present an alterna-tive to costly manual annotation. This paper considers management succession databases where each record has three fields: a CEO X  X  name and the start and end years for that person X  X  tenure as CEO (Table 1 Column 1). Each record is represented by three binary logical pred-a company, x is a CEO X  X  name, and y 1 and y 2 are years. 2 ), inoffice ( x 2 , y 2 ) , y 1 &lt;y 2  X  predates ( x 1 , x ) , inoffice ( x 1 , y 1 ), inoffice ( x 2 , y 2 )  X  y 1  X  y table, c is a company, x i is a person, and y i is a time.
In this setting, database semantics allow for the derivation of other implicit relationships from the database: the immediate predecessor of a given CEO ( precedes ( x 1 , x 2 )), all predecessors of a given CEO ( predates ( x 1 , x 2 )) and all years the CEO was in of-fice ( inoffice ( x , y 3 )), where x 2 is a CEO X  X  name and t a year (Table 1 Column 2).

These implicit relationships and the original ex-plicit relationships are governed by a series of se-mantic relations which impose constraints on the permissible database configurations (Table 1 Col-umn 3). For example, it will always be true that a CEO X  X  start date precedes their end date: Multi-document extraction of single relationships exploits redundancy and variety of expression to ex-tract accurate information from across many docu-ments. However, these are not the only benefits of extraction from a large document collection. As well as being rich in redundant information, large docu-ment collections also contain a wealth of secondary relationships which are related to the relationship of interest via constraints as described above. These secondary relationships can yield benefits to aug-ment those achieved by redundancy. There are typically two steps in the extraction of sin-gle relationships from multiple documents. In the first step, a relationship extractor goes through the corpus, finds all possible relationships r in all sen-tences s and gives them a score p ( r | s ) . Next, the relationships are fused across sentences to generate one score  X  r for each relationship.

This paper proposes a third step which combines the fusion scores across relationships. This section first presents a probabilistic database model gener-ated from fusion scores and then shows how to use this model for multi-document fusion. 3.1 A Probabilistic Database Model A relationship r is defined to be a 3-tuple r t,a,b = r ( t,a,b ) , where t is the type of the relationship (e.g. start ), and a and b are the arguments of the binary relationship. 3
To construct a probabilistic database for a given corpus, the weights generated in relationship fusion are normalized to provide the conditional probability of a relationship given its type: where  X  r is the fusion score generated by the extrac-tion/fusion system. 4 By applying a prior over types p ( t ) , a distribution p ( r 1 ,t 1 ) can be derived. Given strong independence assumptions, the probability of an ordered database configuration R = r 1 ..n of types t
As proposed, the model in Equation 1 is faulty since the relationships in a database are not inde-pendent. Given a set of database constraints, certain database configurations are illegal and should be as-signed zero probability. To address this, the model in Equation 1 is augmented with constraints that ex-plicitly set the probability of a database configura-tion to zero when they are violated.

A database constraint is a logical formula  X  . For the constraints presented in this paper, all constraints  X  are modeled with two terms  X   X  and  X   X  where:
For a set of relationships, a constraint holds if  X  (  X  ) is true, and the constraint applies if  X   X  (  X  ) is true. A constraint  X  (  X  ) can only be violated (false) when the constraint applies, since: ( false  X  X ) = true.
In application to a database, each constraint  X  is quantified over the database to become a quantified constraint  X  r 1 ..n . For example, the constraints that a person X  X  start date must come before their end date is universally quantified over all pairs of relationships in a configuration R = r 1 ..n : This constraint applies to start and end relationships whose CEO argument matches and is violated when the years are not in order. If the quantified constraint  X  1 ..n is true for a given database configuration r 1 ..n then it holds .

To ensure that only legal database configurations are assigned positive probabilities, Equation 1 is augmented with a factor To include a constraint  X  , the database model in Equation 1 is extended to be: where Z is the partition function and corresponds to the total probability of all database configurations. A set of constraints  X  1 ..Q =  X  1 .. X  Q can be integrated similarly: With these added constraints, the probabilistic database model assigns non-zero probability only to databases which don X  X  violate any constraints. 3.2 Constraints on Probabilistic Databases for Though the constrained probabilistic database model in Equation 2 is theoretically appealing, it would be infeasible to calculate its partition func-tion which requires enumeration of all legal 2 n databases. This section proposes two methods for re-scoring relationships with regards to how likely they are to be present in a legal database configu-ration using the model proposed above. The first method is a confidence estimate based on how likely it is that  X  holds for a given relationship r 1 : where the expectation that the constraint holds is equivalent to the likelihood ratio between the database probability models with and without con-straints. In effect, this model measures the expec-tation that the constraint holds for a finite database  X  X ook-ahead X  of size  X  (  X  )  X  1 .

With this method, for a constraint to reduce the confidence in a particular relationship by half, half of all configurations would have to violate the con-straint. 5 Since inconsistencies are relatively rare, for a given relationship  X   X  ( r,t )  X  1 (i.e. almost all small databases are legal).
To remedy this, another factor  X   X  is defined simi-larly to  X   X  , except that it takes a value of 1 only if the constraint applies to that database configuration. An applicability probability model is then defined as: The second confidence estimate is based on how likely it is that the constraint is holds in cases where it applies (i.e. is not violated):  X  When the constraint doesn X  X  apply it cannot be vio-lated, so this confidence estimate ignores those con-figurations that can X  X  be affected by the constraint.
Recall that  X   X  ( r,t ) is the likelihood ratio be-tween the probability of configurations in which r holds for constraint  X  and all configurations. In con-trast,  X   X , X  ( r,t ) is the likelihood ratio between the database configurations where r applies and holds for  X  and the database configurations where  X  ap-plies. In the later ratio, for confidence in a particular relationship to be cut in half, only half of the con-figurations which might actually contain an incon-sistency would be required to produce a violation. 6 As a result,  X   X , X  ( r,t ) gives a much higher penalty to relationships which create inconsistencies than does  X   X  ( r,t ) .

In order to apply multiple constraints, indepen-dent database look-aheads are generated for each constraint q : For a particular relationship type, these confidence scores are calculated and then used to rank the rela-tionships via: Databases with different precision/recall trade offs can be selected by descending the ranked list. 7 In order to test the fusion method proposed above, human annotators manually constructed truth data of complete chief executive histories for 18 Fortune-500 companies using online resources. Extraction from these documents is particularly difficult be-cause these data have vast differences in genre and style and are considerably noisy. Furthermore, the task is complicated to start with. 8
A corpus was created for each company by is-suing a Google query for  X  X EO-of-Company OR Company-CEO X , and collecting the top ranked doc-uments, generating up to 1000 documents per com-pany. The data was then split randomly into training, development and testing sets of 6, 4, and 8 compa-nies.
 Training : Anheuser-Busch, Hewlett-Packard, Dev. : Boeing, Heinz, Staples, Textron Test : General Electric, General Motors,
Ground truth was created from the entire web, but since the corpus for each company is only a small web snapshot, the experimental results are not simi-lar to extraction tasks like MUC and ACE in that the corpus is not guaranteed to contain the information necessary to build the entire database. In particular, if the database is consistent. many CEOs from pre-Internet years were either in-frequently mentioned or not mentioned at all in the database. 9 In the following experiments, recall is re-ported for facts that were retrieved by the extraction system. 4.1 Relationship Extraction and Fusion A two-class maximum-entropy classifier was trained for each relationship type. Each classifier takes a sentence and two marked entities (e.g. a person and a year) 10 and gives the probability that a relation-ship between the two entities is supported by the sentence. For each relationship type, one of the ele-ments is designated as the  X  X ook X  in order to gener-ate likely negative examples. 11 In training, all entity pairs are collected from the corpus. The pairs whose  X  X ook X  element doesn X  X  appear in the database are thrown out. The remaining pairs are then marked by exact match to the database. In testing, the rela-tionship extractor yields the probability p ( r | s ) of an entity pair relationship r in a particular sentence s .
The features used in the classifier are: unigrams between the given information and the target, dis-tance in words between the given information and the target, and the exact string between the given in-formation and the target (if less that 3 words long).
After extraction from individual sentences, the re-lationships are fused together such that there is one score for each unique entity pair. In the case of per-son names, normalization was performed to merge coreferent but lexically distinct names (e.g.  X  X hil Condit X  and  X  X hilip M. Condit X ).

In the following experiments, the baseline fusion score is: 4.2 Experimental Results Given the management succession database pro-posed in Section 2, Table 2 enumerates a set of quan-tified constraints. Information extraction and fusion were run separately for each company to create a probabilistic database. In this section, various con-straint sets are applied, either individually or jointly, and evaluated in two ways. The first measures per-relationship precision/recall using the model pro-Figure 1: Precision/Recall curve for start(x,t) rela-tionships. The joint constraint  X 2,3,5,8 X  is the best performing, even though constraints  X 3 X  and  X 8 X  (not pictured) alone don X  X  perform well. posed and the second looks at the precision/recall of a heterogeneous database with many relationship types. Both evaluations examine the ranked lists of relationships, where the relationships are ranked by rescoring via constraints on probabilistic databases (Equation 3) and compared to the baseline fusion score (Equation 4). The evaluations use two stan-dard metrics, interpolated precision at recall level i ( P R i ), and MaxF1:
Figures 1, 2, and 3 show precision/recall curves for the application of various sets of constraints. Ta-ble 3 lists the MaxF1 scores for each of the con-straint variants. For start and end , the majority of constraints are beneficial. For precedes , only the constraint that improved performance constraints both people in the relationship to be CEOs. Across all relationships, performance is hurt when using the constraint that there could only be one relationship of each type for a given CEO. The reason behind this is that the confidence estimate based on this constraint favors relationships with few competitors, and those relationships are typically for people who are infrequent in the corpus (and therefore unlikely to be CEOs).

The best-performing constraint sets yield between 5 and 18 points of improvement on Max F1 (Ta-ble 3). Surprisingly, the gains from joint con-Figure 2: Precision/Recall curve for end(x,t) rela-tionships alone. The joint constraint  X 2,6 X  is the best performing. Figure 3: Precision/Recall for precedes(x,y) rela-tionships alone. Though the constraint  X  X irst Before Second (8) X  helps performance on start(x,t) relation-ships, the only constraint which aids here is  X  X nly CEOs Succession (9) X . straints are sometimes more than their additive gains.  X 2,3,5,6,8 X  is 6 points better for the start rela-tionship than  X 2,3,5,6 X , but the gains from  X 8 X  alone are negligible.

These performance gains on the individual rela-tionship types also lead to gains when generating an entire database (Figure 4). The highest performing constraint is the  X  X EOs Only (2) X  constraint, which outperforms the joint constraints of the previous sec-tion. One reason the joint constraints don X  X  do as well here is that each constraint makes the confi-dence estimate smaller and smaller. This doesn X  X  have an effect when judging the relationship types individually, but when combining the relationships results, the fused relationships types ( start , end ) be-
Constraint Set Start End Pre. DB  X  (baseline) 31.2 35.8 34.5 37.9 Only One (1) 10.5 7.2 -38.1 CEOs Only (2) or (9) 43.3 39.4 39.4 42.9 Start Before End (3) 40.8 32.8 -40.9 No Overlaps (4) 31.5 35.9 -36.8 Inoffice After Start (5) 32.5 --38.2 Inoffice Before End (6) -36.5 -37.4 End is Start (7) 7.3 8.0 20.7 39.2
First before Second (8) 31.4 35.6 26.3 38.1 2,5,6 43.3 40.8 -42.7 2,3,5,6 43.9 43.3 -42.2 2,3,5,6,8 49.3 43.9 26.3 40.9 Table 3: Max F1 scores for three relationships Start (x,t), End (x,t) and Pre cedes(x,y)) in isolation and within the context of whole database DB . The joint constraints perform best for the explicit rela-tionships in isolation. Using constraints on implicit derived fields (Inoffice and Precedes) provides ad-ditional benefit above constraints strictly on explicit database fields (start, end, ceo). come artificially lower ranked than the unfused rela-tionship type ( ceo ). The best performing contrained probabilistic database approach beats the baseline by 5 points. Techniques for information extraction from min-imally supervised data have been explored by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). Those techniques propose methods for estimating extractors from ex-ample relationships and a corpus which contains in-stances of those relationships.

Nahm and Mooney (2002) explore techniques for extracting multiple relationships in single document extraction. They learn rules for predicting certain fields given other extracted fields (i.e. a someone who lists Windows as a specialty is likely to know Microsoft Word).

Perhaps the most related work to what is pre-sented here is previous research which uses database information as co-occurrence features for informa-tion extraction in a multi-document setting. Mann Figure 4: Precision/Recall curve for whole database reconstruction. Performance curves using con-straints dominate the baseline. and Yarowsky (2005) present an incremental ap-proach where co-occurrence with a known relation-ship is a feature added in training and test. Cu-lotta et al. (2006) introduce a data mining approach where discovered relationships from a database are used as features in extracting new relationships. The database constraints presented in this paper provide a more general framework for jointly conditioning multiple relationships. Additionally, this constraint-based approach can be applied without special train-ing of the extraction/fusion system.

In the context of information fusion of single rela-tionships across multiple documents, Downey et al. (2005) propose a method that models the probabili-ties of positive and negative extracted classifications. More distantly related, Sutton and McCallum (2004) and Finkel et al. (2005) propose graphical models for combining information about a given entity from multiple mentions.

In the field of question answering, Prager et al. (2004) answer a question about the list of composi-tions produced by a given subject by looking for re-lated information about the subject X  X  birth and death. Their method treats supporting information as fixed hard constraints on the original questions and are ap-plied in an ad-hoc fashion. This paper proposes a probabilistic method for using constraints in the con-text of database extraction and applies this method over a larger set of relations.

Richardson and Domingos (2006) propose a method for reasoning about databases and logical constraints using Markov Random Fields. Their model applies reasoning starting from a known database. In this paper the database is built from ex-traction/fusion of relationships from web pages and contains a significant amount of noise. This paper has presented a probabilistic method for fusing extracted facts in the context of database ex-traction when there exist logical constraints between the fields in the database. The method estimates the probability than the inclusion of a given relationship will violate database constraints by taking into ac-count the uncertainty of the other extracted relation-ships. Along with the relationships explicitly listed in the database, constraints are formed over implicit fields directly recoverable from the explicit listed re-lationships.

The construction of CEO succession timelines us-ing minimally trained extractors from web text is a particularly challenging problem because of noise resulting from the wide variation in genre in the cor-pora and errors in extraction. The use of constraints on probabilistic databases is effective in resolving many of these errors, leading to improved precision and recall of retrieved facts, with F-measure gains of 5 to 18 points.

The method presented in this paper combines symbolic and statistical approaches to natural lan-guage processing. Logical constraints are made more robust by taking into account the uncertainty of the extracted information. An interesting area of future work is the application of data mining to search for appropriate constraints to integrate into this model.

