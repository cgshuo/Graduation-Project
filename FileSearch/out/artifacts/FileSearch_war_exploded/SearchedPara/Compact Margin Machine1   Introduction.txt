 Semi-supervised learning paradigm, which blossoms out to utilize data suffi-ciently, has attracted more and more attention in machine learning commu-nity [1]. Semi-Supervised Support Vector Machine(S3VM) [2, 3], the semi supervised extension of support vect or machine, is a state-of-the-art semi-supervised learning algorithm. It uses all the data no matter labeled or not to detect the margin and achieves signifi cant improvement in practice. However, S3VM abandons the instances which are not support vectors. The framework [4] utilizes general unlabeled data, but still wasteful for non-support vectors, which stops them from further improving the performance.

To utilize the data more sufficiently and efficiently, we notice that compact-ness of projected data provides global information and thus is important for classification. Linear Discriminant Analysis [5] is a successful algorithm to uti-lize the information. Algorithm in [6] whitens the data when seeking decision boundary. Gaussian Margin Machine [7] controls the projected data compact-ness under a distribution assumption. Universum SVM [8] constrains range by data that is related but not belonged to any category. Another criterion that compresses the range of projected data is Relative Margin Machine[9] which is motivated from an affine invariance perspective and some probabilistic prop-erties. Although these algorithms balance global and local information, these methods merely exploit labeled data but turn blind eyes to unlabeled data.
In this paper, we propose a method to use the information contained by instances sufficiently. After brief introductions to S3VM and LDA in Section 2, our framework incorporating LDA criterion with S3VM is presented in Section 3 and a relaxation of the criterion is given to make the optimization tractable. We also derive a special variant which can be viewed as the semi-supervised extension of Relative Margin Machine. The generalization bound is analyzed in Section 4. Finally, the experimental results are reported in Section 5. from a certain data distribution D ,where u -l , x i  X  R d ( i =1 , 2 ,...,l + u ) and y j  X  X  X  1 , 1 } , ( j =1 , 2 ,...,l ) is the label of instance x j . The problem we want to solve is seeking a hypothesis h : R d  X  X  X  1 , +1 } which can classify the unlabeled data and unseen instances sampled from D . 2.1 Semi-supervised SVM(S3VM) Many semi-supervised methods find the suitable hypothesis by minimizing the criterion which utilize both labeled data and unlabeled data as where H is the reproducing kernel Hilbert spa ce introduced by kernel function K , is a common classification loss function, and 2 is another loss function which utilizes unlabeled data only. S3VM employs hinge loss as 1 and symmetric hinge loss as 2 . C 1 and C 2 are the parameters to balance the loss between labeled data, unlabeled data and function complexity. We take the set of linear form of So the formulation of (1) could be transformed as follow: 2.2 Linear Discriminative Analysis(LDA) The basic principle of LDA is projecting the data into a subspace in which the instances in different categories can be scattered and the instances in the same category can be concentrated together. Let m i = 1 n is the sample mean of class C i where n i is the number of samples in C i ,the mean of projected instances is given by  X  m i = 1 n { X  1 , +1 } . So the distance between the projected means, between-class distance , is  X  m  X  1  X   X  m 1 = w T m  X  1  X  w T m 1 . LDA seeks the largest between-class distance relative to total within-class variance whichisdefinedby i =  X  1 , +1 s i where s i = j  X  X  between-class distance to total within-class variance , J w =  X  m  X  1  X   X  m 1 The data compactness after projecting i sveryimportantin reflecting the data structure and can indeed help in classify ing the data. It is necessary to restrict the range of projected data while seeking the largest margin. Within-class vari-ance defined in LDA provides us a natural way to measure the projected data compactness. We modify s i as j  X  X  S3VM. We propose the model, Compact Margin Machine , as follow ( f ( x ) stands for w T x + b ): s.t. y i f ( x i )  X  1  X   X  i ,i =1 ,...,l | f ( x i ) | X  1  X   X  i ,i = l +1 ,...,l + u the trivial solution that assigns all the instances the same label [10]. We can use branch-and-bound algorithms to search the global optimal solution. However, the computational complexity is too high. We relax the constraints to make the optimization process easier and it can be proved that our relaxed constraints imply upper bounds of original loss functions. the loss will be no less than c defined above.
 Proof. As the constraints (4)-(7) have the same form, without loss of generality, we consider the constraint (7) and the others are similar. Obviously, if there is asolution w that satisfies | w T x i + b | X  1 2 (  X  +  X  i ), then we have The inequality is derived from triangle inequality. We obtain that the relaxed constraints provide an upper bound and n i =1 c ( x i ; L , U ,f )  X  n i =1  X  i . * + Proposition 1 suggests that the relaxed constrains are helpful in reflecting the projected data compac tness. Mathematically, CMM is relaxed as: Note that relaxed constraints modify both of the loss functions for labeled and unlabeled data given by S3VM actually. The loss function for the labeled data is 1 =max { 0 , 1  X  yf ( x ) } + C 3 C compact  X  -hinge loss . On the other hand, the loss function of the unlabeled data is adapted as 2 =max { 0 , 1  X  X  f ( x ) |} + C 3 C laxed compact  X  -symmetric hinge loss . The loss functions are shown above. It is not easy to solve the optimization problem (8) directly because of the non-convex property of relaxed compact  X  -symmetric hinge loss .As[2],MixInteger Programming can find the global optimal solution of (8). However, the compu-tational complexity of MIP is usually very high. By employing Concave-Convex Procedure(CCCP) [11], (8) can be solved effciently [12]. We set x i = x i  X  u ,i = l + u +1 ,...,l +2 u, y i =1 ,i = l +1 ,...,l + u, y i =  X  1 ,i = l + u +1 ,...,l +2 u and rewrite the relaxed Compact Margin Machine criterion (8) as the sum of a convex part J convergence. The convergence of CCCP has been shown by [13]. The steps of algorithm are shown in Algorithm 1. 3.1 Special Variant and the Relationship to RMM In this section, we derive a special variant from (8) which can be solved more efficiently. We set the parameters C 2 = C 3 ,  X  = 0 in (8), so the loss function of unlabeled data is the blue one in Fig.1. The mathematical form is as following: This special variant of relaxed Compact Margin Machine utilizes the labeled data to control the margin and both the labeled and unlabeled data to compress the range of projected data. It can be view ed as a semi-supervised extension of RMM. RMM introduces the relaxed constraints (8) from the other motivations such as affine invariance perspective and some probabilistic properties. From this aspect, we can conclude that our model achieves the same properties that are given by RMM. In this section, we derive the empirical transductive Rademacher complexity [14] for function classes relaxed Compact Margin Machine which can be plugged into uniform Rademacher error bound directly.

Firstly, we define the function class of S3VM as H D = { w T x | 1 2 w T w  X D} and the function class of relaxed Compact Margin Machine as F D , C 3 = { w T x | drawn from the same distribution for conv enience of proof. However, in practice, we may use training dataset and testing dataset as the extra dataset. We can derive the empirical transductive Rademacher complexity of relaxed CMM and S3VM.
 Lemma 2. The transductive Rademacher complexity of Semi-Supervised SVM R matrix of the data. * + Lemma 3. The transductive Rademacher complexity of relaxed Compact Mar-Based on theorems in [14], the following corollary provides an upper bound on the error rate: Corollary 4. Fix  X &gt; 0 ,let F be the set of function. Let c 0 = 32 ln 4 e 3 and bound holds: where  X  i =max { 0 , 1  X  y i f ( x i ) } .
 The proposed algorithm is evaluated on two benchmark datasets, MNIST 1 and TDT2 2 , to illustrate the effectiveness. We compare our model with SVM, Ker-nel LDA, RMM, LapSVM and S3VM. We implement our algorithm, RMM and LapSVM based on CVX 3 . The SVM and S3VM are solved by SVM light 4 .Among all the experiments, we select the best ker nel by 10-fold cross-validation. The other parameters are also tuned beforehand. We chose the digits  X 8 X  vs  X 9 X  from MNIST dataset for binary classification problem because these two digits are difficult to discriminate visually. In TDT2 dataset, we chose categories randomly. We conduct experiments on several different amount of labeled and unlabeled data. The final test accuracy is given as the average of 10 independent trials on the test dataset. The training data are selected randomly e ach time. We further select a certain num-ber of labeled data from the select ed training dataset randomly.
 The weakness of large marg in criterion can be observed from the results. When labeled data are rare, the algorithms that compress the range of projected data, such as LDA and RMM, perform better than SVM. Our semi-supervised algorithm achieves significant improvem ent on the two datasets with benefit from the compact margin. Fortunately, our algorithm suffers the slightest depression compared with others when the unl abeled data hurt the classifiers. We propose a novel semi-supervised algorithm which can utilize the data suffi-ciently. To make our method capable to handle large-scale applications, we employ Concave-Convex Procedure to solve the n on-convex problem. A semi-supervised extension of RMM can be derived from ou r model. Moreover, we provide theo-retical analyses to guarantee the performance of our algorithm, and finally experi-mental results show that the proposed al gorithm improves the accuracy. A efficient global optimization method for exact solution is our future direction.
