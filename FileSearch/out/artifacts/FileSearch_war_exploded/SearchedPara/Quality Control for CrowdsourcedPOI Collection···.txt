 The idea of crowdsourcing is to outsource human intelligence tasks to a large number of unspecified people via the Internet. Stimulated by the successes of varioius crowdsourcing projects, and the emergence of crowdsourcing market-places, such as Amazon Mechanical Turk, crowdsourcing is rapidly expanding in various areas. Since crowd workers comprise people with different skill levels and diligence, the quality of their work is quite uneven, and sometimes untrustworthy workers called spam workers produce significantly low-quality work. Therefore, quality control of crowdsourcing is one of the central issues in crowdsourcing introduce redundancy, that is, to assign a single task to multiple workers and aggregate their answers to obtain a more reliable answer. Not only simple aggre-gation methods, such as majority voting and averaging have been proposed but multiple-choice tasks, tasks with more complex structured responses, such as been addressed.
 Although a number of efforts have been made to develop quality control methods for respective tasks, there has been no quality control method for POI (point of interest) collection tasks, which require workers to enumerate POIs, in spite of its great significance. One of the important applications is citizen science, which calls for voluntary participation of amateur people in collecting scientific information to cover wide areas of interests. There are several citizen science projects collecting location information on animals, such as birds X  nests and diseased or bleached points of the coral reefs of Hawaii tasks also appear in crowdsensing, which asks people to submit current location information together with additional information such as pictures of POIs. Col-lecting location information such as road-side parkings [ 10 ] can be considered as POI collection tasks.
 However, the quality control issue of a POI task has not been addressed because of its task-specific uncertainty. The intrinsic difficulty of quality con-trol in POI collection tasks is the lack of correspondence between each POI and each of workers X  answers. In other words, we cannot distinguish between two given answers corresponding to two distinct POIs and those corresponding to the identical POI. Therefore, the ideas in existing studies assuming the corre-spondences between questions and answers cannot be directly applied to the POI collection task.
 We consider that the difficulty involved in the quality control issue of POI tasks is caused by the fact that there are two types of uncertainties in POI collection tasks: workers neither necessarily provide correct answers nor provide exactly the same answers for a specific POI because of the variations in numer-ical values. Considering these two types of uncertainty enables us to propose a quality control method for POI collection tasks. Our quality control method has two-stages: answer clustering followed by reliability estimation . As illustrated in Figure 1 , the first stage is designed to select the representative answers that indi-cate distinct POIs, and the second to find the correct answers from among them. we construct an answer clustering method by incorporating cannot-link con-straints [ 16 ]intothe exemplar clustering formula proposed by Elhamifar et al. [ 5 ]. Our new clustering formula enables us to divide answers into clusters and choose the most representative answer from each cluster, while the answers given by the identical worker are separated from each other by cannot-link constraints. Second, we develop a new reliability estimation method by modifying the HITS algorithm [ 7 ]. The modified HITS algorithm using a normalized iterative matrix handles  X  X treakers, X  who give many more answers in crowdsourced enumera-tion [ 6 , 14 ], while the original HITS algorithm suffers from these workers. tasks and evaluated the performance of our two-stage quality control method. Our method appropriately handles the two types of uncertainties pertaining to the answers and provides high quality answer lists. In fact, it makes 6.4% to 19.4% improvements of the F-measures over the baseline methods in four experimental tasks. In addition, we synthetically added spam workers to the original datasets and confirmed that our method is robust against the presence of spam workers. Our method curbed the decreases of the F-measures by 8 to 72 . 5% of those of NONE in four tasks.
 1. We indicate there are two types of uncertainties in crowdsourced POI col-2. We introduce a two-stage quality control method that deals with the two 3. We describe experiments using four POI collection tasks and investigate We propose a two-stage quality control method to obtain the set of correct answers from the answers enumerated by workers. As shown in Figure 1 , all the answers are clustered so that those in each cluster indicate the same POI in the first stage. Each cluster is represented by a single answer, which we call a representative answer . The objective of this answer clustering stage is to reduce the uncertainty due to noises in answers, and remove redundant answers indi-cating the same POIs. Subsequently, the second stage estimates the reliability of each representative answer, and then, we remove representative answers with low reliability. 2.1 Problem Setting We first define the quality control problem in crowdsourced POI collection. A POI collection task requires crowdworkers to enumerate location information (i.e., longitude and latitude) of POIs. The workers are usually expected to give one or more answers (as many as they can provide) that indicate distinct POIs according to explicit instructions.
 Let us assume that there are W workers. Let T i denote the set of answers given by worker i , N the total number of the answers collected from all the workers, and T = T 1  X  X  2  X  X  X  X  X  X  W the set of all the answers. In addition, we assume that we have a distance d uv between every pair of answers Given the answer set {T i } i =1 ,...,W and the distance matrix ( d ) u,v =1 ,...,N , our goal is to obtain a subset of given answers, each answer in the answer subset correctly satisfies the task requirement and indicates a distinct POI. 2.2 Answer Clustering To select representative answers from noisy and duplicate answers, we apply a clustering method and consider that the answers in each cluster indicate the identical POI. Standard clustering methods usually adopt the cluster centers as the representatives of the obtained clusters; however, this often results in undesirable representatives because the middle point of several data points, such as locations of restaurants, frequently falls in unreasonable points, such as the middle of a road. This observation leads us to resort to exemplar clustering , which chooses representatives from among data points.
 We assume that the answers given by a single worker indicate distinct POIs even if some of the answers are close to each other. This assumption motivates us to introduce cannot-link constraints in the exemplar clustering. The cannot-link constraints assign the answers given by a single worker to distinct clusters. We believe this is a reasonable assumption, because we can prevent workers from providing multiple answers indicating the identical POI by giving appropriate instructions, and simple methods may be effective for detecting untrustworthy workers who provide the same answer many times to earn easy money. Although several constrained clustering methods such as COP-Kmeans [ 16 ] have been proposed, no constrained extensions of exemplar clustering methods exist; therefore, we formalize a constrained exemplar clustering by incorporating cannot-link constraints into an exemplar clustering method.
 Exemplar Clustering. We first review the convex exemplar clustering pro-posed by Elhamifar et al. [ 5 ]. Let z uv  X  [0 , 1] denote the probability that answer v belongs to the cluster represented by answer u and Z =( z the representative matrix. Given the distance matrix D , the exemplar clustering problem is defined as where z u, : denotes the u -th row of Z and  X &gt; 0 is a regularization parameter that indirectly controls the number of clusters. When q is chosen appropriately, the regularization term || z u, : || q forces z u, : = 0 for some to ensure the convexity of the object function ( 1 ).
 Constrained Exemplar Clustering. We create a constrained exemplar clus-tering method by appending cannot-link constraints to the exemplar clustering problem ( 1 ). We first assume that z uv  X  X  0 , 1 } . We can ensure that none of two answers of worker i belongs to representative answer u by adding the constraint, v  X  T i z u,v  X  1. In order to force any pair of the answers given by a single worker to belong to distinct clusters, we add the constraints where z : ,v denotes the v -th column of Z . We relax z uv z uv  X  [0 , 1], and then, our constrained exemplar clustering is formulated as We then adopt answer u such that z u, : &gt; 0 as a representative answer, which represents the cluster. Let U = { u  X  X  | z u, : =0 } denote a set of representa-tive answers. 2.3 Reliability Estimation As discussed in the Introduction, some of the representative answers given by the first answer clustering stage may not be correct; hence, we next estimate the reliability of each representative answer, as shown in Step 2 of Figure 1 . We assume that an answer is reliable if it is supported by multiple reliable workers; a worker is reliable if s/he provides many reliable answers. This notion is similar to that employed in the HITS algorithm [ 7 ], a link analysis algorithm for estimating the importance of Web pages. In HITS, a Web page is considered highly authoritative if it has links from multiple good hub pages, and a Web page is regarded as a reliable hub if it has links to multiple authorities. The HITS algorithm estimates both the authority and hub values of each Web page using the link structure between the pages. We modify the HITS algorithm by using a normalized iterative matrix and apply it to the answer reliability estimation with analogies between authorities and answers and between hubs and workers. The modified HITS using the normalized iterative matrix performs well even when we have some streakers.
 Modified HITS. We first introduce how to apply the HITS algorithm to reli-ability estimation. Let a =( a 1 ,...,a M ) denote a vector of authority values of representative answers in U , where M is the size of U ,and denote a vector of hub values of workers. Instead of the hyperlink structure used in the HITS, in reliability estimation we exploit the connection weights between workers and answers. Let us denote by ( u 1 ,u 2 ,...,u M sentative answers in T . We define the connection weight matrix as L ik = v  X  T i z v,u k , which represents the total amount of supports for the representative answer u k of worker i .
 Given L and the initial values of authority a and of hub h rithm updates a and h iteratively as a  X   X  L h , h  X   X  La , and  X &gt; 0 are normalization constants to keep the norms of a and h constant. The update operations can be combined into a  X  c L La , where solution of a that we seek is obtained by finding the eigenvector corresponding to the largest eigenvalue of L L . We finally output a set of answers authority values are greater than some threshold, .
 In order to estimate the reliability of workers accurately in the presence of streakers, we updated the authority scores by a normalized matrix instead of L L as a  X  c N  X  1 / 2 L LN  X  1 / 2 a , where N = diag( n denotes the number of answers given by the i -th worker. We evaluated the effectiveness of the proposed two-stage quality control method, using four real-world datasets, on POI collection tasks. A comparison with two baselines demonstrates that our method appropriately handles the two types of uncertainties of the answers in these tasks and provides high quality answer lists. In addition, we confirm the robustness of the proposed method against the participation of spam workers by synthetically adding spam workers to the original datasets. 3.1 Datasets We conducted experiments on several POI collection tasks posted to the Lancers crowdsourcing marketplace ( http://www.lancers.jp ). A POI collection task asks workers to enumerate the longitude and latitude of points that satisfy a given requirement of the task. We ordered four collection tasks for POIs in Japan asking for locations of telephone booths around Shimbashi station (Task 1), noodle restaurants around Takamatsu station (Task 2), mail boxes around Ueno station (Task 3), and public toilets around Shinjuku station (Task 4). Workers were shown a Google Maps-based interface where a target area was specified, and asked to extract the latitude and longitude of a point that they thought satisfied the given requirement. The workers were instructed to enumerate as many points as possible but not to provide multiple points indicating the same POI. Their rewards were propotional to the number of their answers. spots satisfying the requirement of each task. We obtained the list of noodle restaurants near Takamatsu station from an online restaurant review portal ( http://tabelog.com/ ) as the ground truth for Task 2. The other online por-tals we used to create the ground truths for Tasks 1, 3, and 4 were http://www. telmap.net/ , http://postmap.org/ and http://toilet.blog.shinobi.jp/ .Thenum-ber of answers, workers, and ground truth answers in Tasks 1, 2, 3, and 4 were (133,4,96), (63,7,58), (122,14,84), and (82, 11, 150), respectively. Figure 2 shows the distribution of the accuracy and the number of answers provided by each worker. 3.2 Evaluation Methodology We used precision, recall, the F-measure, and AUCs (areas under the receiver operation characteristic curve) as our evaluation metrics. We now explain the method of judging whether each answer is correct by using coordinate data. Let denote an answer represented as a two-dimensional vector, (longitude In POI tasks, we considered that each obtained answer v  X  X  was close to one of the answers appearing in the ground truth dataset, namely,  X  g  X  X  , || v  X  g || task. d denotes a distance threshold value; we set d =0 . goal was to find distinct POIs, we judged that only a single answer was correct if multiple answers in P were sufficiently close to the identical ground truth answer in
G , and that a single answer was tied with a single ground truth answer even if there were multiple ground truth answers near it.
 We conducted two types of experiments. First, we compared the quality of the answers selected by our method (called CL+modHITS), those selected only by the clustering stage (called CL), and the original set of the answers with-out any quality control (called NONE) to verify the effectiveness of each stage of our two-stage method, by calculating precisions, recalls, and the F-measures as the evaluation metrics. We employ these metrics because the baseline meth-ods have no threshold hyperparameter. Note that we only used the subset of the ground truth answers that were enumerated by at least one of the workers, because the rest of them were not useful for comparing the methods. Second, we compared the performance of our method with one consisting of the clus-tering and HITS (called CL+HITS) and one consisting of the clustering and a majority vote, which regards the number of workers indicating a cluster as the cluster X  X  reliability (called CL+MAJ) by calculating AUCs as the evaluation metrics. We employ the AUC because all the tested methods have threshold hyperparameters. It should be noted that the set of answers obtained by NONE corresponds to T , CL corresponds to U , and CL+modHITS, CL+HITS, and CL+MAJ corresponds to P .
 We calculated the Euclidean distance between each pair of answers v as the distance matrix D . In the answer clustering stage, we set solving the problem as a linear program. The regularization parameter to  X  =0 . 2. 3.3 Evaluation with Original Data We first conducted experiments with the original datasets, (i.e., without syn-thetic spam workers). Figure 3 shows the comparison of the F-measures and Figure 4 , and Table 1 show the comparisons of precision-recall curves and AUCs. In all the tasks, our two-stage method achieved higher F-measures than any other method. The improvements of the F-measure of CL+modHITS from that of NONE for Tasks 1, 2, 3, and 4 were 8 . 1%, 18 . 1%,19 . We observed the effectiveness of both the answer clustering and the reliability estimation by comparing NONE with CL, and CL with CL+modHITS, respec-tively. The F-measures were improved by the answer clustering in Tasks 2, 3, and 4, while not in Task 1 because the number of workers and that of multiple answers indicating the identical POI were so small that the decrease of recall affected the F-measure more than the increase of precision. Nonetheless, the reliability estimation was effective in all the tasks, which indicates the answer clustering appropriately clustered the answers given by workers. Moreover, we observed our method still allows us to control a balance between precision and recall or improve either of them to attain the objectives of tasks by adjusting the threshold in the reliability estimation, , as shown in Figure 4 . paring the AUCs with those of the majority vote and the HITS algorithm as showninTable 1 . In all the tasks, the performance of the modified HITS algo-rithm was the best, or at least, comparable to the best one. The HITS algorithm also performed well in some tasks, but not in Task 2 or 4 because in these tasks, streakers provided relatively unreliable answers as compared to the other work-ers, as shown in Figure 2 . When a part of the answers given by an unreliable streaker accidentally coincide with those by trustworthy workers, the streaker acquires a high reliability value in the reliability estimation stage; therefore, the answers of the unreliable streaker are considered reliable. In order to prevent the reliabilities of streakers from rising up too much, the modified HITS normalized the iterative matrix by the number of answers given by each worker, and thereby, it successfully handled this problem. 3.4 Evaluation with Synthetic Spam Workers Although we observed only a few apparent spam workers in our experiments as shown in Figure 2 , the number of workers may easily increase if we pay them more money or we use a different crowdsourcing service. In order to investigate the robustness of our proposed method against the presence of spam workers, we synthetically added spam workers to our datasets and investigated the effects. Specifically, we generated artificial datasets by the following steps. In each task, we first fixed the number of spam workers, and sampled the number of answers of each spam worker from a Poisson distribution x i  X  Po ( average number of answers per worker in the original dataset. Next, we sampled pairs of longitude and latitude from inside the specified area from a uniform distribution and counted the pairs as answers of spam workers. We added the synthetic data to the original datasets. We repeated the experimental procedure 20 times for each setting.
 Figure 5 shows the F-measures for different numbers of spam workers. In all the tasks, our two-stage quality control method is quite robust to the pres-ence of spam workers; the CL+modHITS method more effectively curbed the performance reduction as compared with NONE and CL. A number of unsupervised methods have been proposed for quality control in crowdsourcing. A groundbreaking study was conducted by Dawid and Skene [ 4 ], who modeled the differences in reliabilities of workers (doctors in their context) to estimate true answers (diagnoses). Extensions of the Dawid-Skene model have recently been studied in depth by, for example, introducing task difficulty [ 18 ] or incorporating the affinity between workers and tasks [ 17 ].
 yes-or-no questions) or multiple-choice questions, several studies addressed the quality control problem in more complex tasks. Chen et al. [ 3 ], Yi et al. [ 20 ], and Matsui et al. [ 11 ] proposed quality control methods for item ordering tasks; Wu et al. [ 19 ] focused on tasks with annotation tasks on sequential data, such as a named entity recognition task in natural language texts; Venanzi et al. [ 15 ] showed community-based aggregation can learn accurate estimations of worker accuracies from a limited amount of interactions; Lin et al. [ 9 ] addressed tasks with unstructured output formats, and Baba and Kashima [ 1 ] focused on more general tasks where workers do not necessarily agree on one answer. enumeration tasks, no quality control method was provided for them. Tian and Zhu [ 13 ] proposed a quality control method for tasks with multiple cor-rect answers; however, their method cannot be applied to the POI collection tasks where candidates of answers are not explicitly given to the workers. To the best of our knowledge, our work is the first attempt to address quality control for crowdsourced POI collection tasks. We addressed the quality control problem for POI collection tasks that ask workers to enumerate correct location information of POIs. In order to resolve the uncertainties of enumerated answers and obtain reliable answers, we proposed a two-stage quality control method consisting of an answer clustering stage and a reliability estimation stage. The answer clustering stage was designed for filtering out redundant answers indicating the same POIs. We formalized a constrained exemplar clustering to utilize the assumption that multiple answers given by a single worker indicate distinct POIs. The reliability estimation stage used the HITS algorithm to estimate the reliability of answers as well as that of workers in order to select the correct answers from among the representative answers obtained in the previous stage, and modified it to handle the problem caused by streakers.
 The experimental results on four POI collection tasks showed that our method successfully produced reliable answer sets and was quite robust to the presence of spam workers. Our future work includes developing a model that can usefully be applied to general enumeration tasks where workers can be asked to provide character strings and the distances between answers are not given.
