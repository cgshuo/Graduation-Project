 Truth discovery is a long-standing problem for assessing the va-lidity of information from various data sources that may provide different and con fl icting information. With the increasing promi-nence of data streams arising in a wide range of applications such as weather forecast and stock price prediction, effective techniques for truth discovery in data streams are demanded. However, exist-ing work mainly focuses on truth discovery in the context of static databases, which is not applicable in applications involving stream-ing data. This motivates us to develop new techniques to tackle the problem of truth discovery in data streams.

In this paper, we propose a probabilistic model that transforms the problem of truth discovery over data streams into a probabilistic inference problem. We fi rst design a streaming algorithm that infers the truth as well as source quality in real time. Then, we develop a one-pass algorithm, in which the inference of source quality is proved to be convergent and the accuracy is further improved. We conducted extensive experiments on real datasets which verify both the ef fi ciency and accuracy of our methods for truth discovery in data streams.
 H.2.8 [ Database Management ]: Database Applications Algorithms, Design, Experiments Data Stream, Truth Discovery
Truth discovery is an extensively studied topic in databases and its importance has been widely recognized by the research commu-nity [19, 6, 7, 14, 29, 25, 11]. In this paper, we study the problem of Truth Discovery in data streams. Many data stream management applications require integrating data from multiple sources in real
Figure 1: Truth Discovery in Weather Forecast for New York time. For each entity, each source provides a value for it. However, the values of the entity from different sources may be con fl icting, as some being true while others being false. To provide the true value for the entity, it is vital that the data stream management system-s are capable of resolving such con fl icts and discovering the true values.
 Consider a set of con fl icting weather forecast values for New York at one timestamp as shown in Figure 1(a). The truth discovery for Figure 1(a) is to resolve the con fl icts and fi nd the true weather forecast values for New York in Figure 1(b). For example, the value  X  X loudy X  provided by the source ACCU is false information.
Previous works [3, 8, 6, 7, 10, 17, 18, 26, 27, 29, 31, 5] on truth discovery mainly focus on static databases. They de fi ned the problem of truth discovery in the context of static databases. The de fi nition is based on the quality of data sources and con fl icting values for the entity. A data source that often provides true values is given a high score for its accuracy. A value for an entity that is provided by accurate sources is considered to be more likely true. Iterative methods were proposed to alternatively discover the true values and estimate the accuracy of the sources.

In recent years, advances in mobile technologies have led to the proliferation of many online data intensive applications, in which data in streaming format are being collected continuously in large volume and high speed. Effective and ef fi cient truth discovery methods for such high speed data streams are essential to a wide range of applications, such as weather forecast, stock price pre-diction, fl ight schedule checking, etc. Moreover, there is also a rising need to share huge amounts of data in various commercial and scienti fi c applications. Whether or not the data is naturally in streaming format, its sheer volume makes it impractical to make multiple passes of the data for truth discovery, while it is also unre-alistic to assume that the data can be loaded into main memory for truth discovery. More compelling reasons of studying data stream integration can be found in [23]. However, as discussed in [14, 31], existing methods focus on static databases, where the prob-lem of truth discovery is solved using iterative methods. Thus, it is dif fi cult for these methods to discover truth in data streams, s-ince their techniques are based on iterative updates of the score of source quality and values for the data, which requires the entirety of the data for the processing (i.e., the data needs to reside in main memory, or otherwise the cost of random disk access incurred will be too high).

To develop ef fi cient and effective techniques for truth discovery in data streams, we should address the following three major chal-lenging computational issues:
None of the existing methods for truth discovery have effectively addressed the above computational issues. In fact, the best existing approach that addresses these issues is probably the method ma-jority vote , which simply considers the value returned by the ma-jority of the sources. However, this method is known to be error-prone [31, 14], since the method values the quality of all sources equally. In general, an effective truth discovery method should take into consideration the difference in the quality of various sources.
In this work, we formulate the problem of truth discovery in data streams, and address all the three computational issues stated above by proposing a generative model for truth discovery. We assume that there exists a true value for the entity among the con fl icting values provided by different sources. Note that, our focus is not on fuzzy value integration and hence if a true value does not ex-ist, there is nothing or no truth to fi nd. The proposed generative model for the collected con fl icting values from various sources is based on two fundamental factors, which are data uncertainty and source quality . Within our proposed model, we transform the truth discovery problem into a probabilistic inference problem. We de-rive the optimal solution for the inference problem and propose an iterative algorithm to converge it. Then, we improve the iterative algorithm and design a streaming algorithm that infers the truth as well as source quality in real time. Then, we develop a one-pass algorithm, in which the inference of source quality is proved to be convergent and the accuracy is further improved. Speci fi cally, we compute the posterior distribution of all possible values, and fi nd the most probable one with the maximum probability. Intuitively, this model best explains all the possible values reported by sources and the con fl icting values in the data streams.

Figure 2 illustrates the important concepts and main ideas in the architecture of our truth discovery in data streams. As the sources can be heterogenous, we fi rst employ a semantic mapping for the values provided by various sources, such that the values for truth Figure 2: A Conceptual View of Truth Discovery in Data Streams discovery are in a consistent manner. For example, we consider the meaning of the weather conditions  X  X ainy X  and  X  X et X  to be the same in weather forecast truth discovery. We also group  X  X artly Sunny X  and  X  X ostly Cloudy X  , and consider them to be the same as  X  X lear X . At each time t , the system collects a set of con fl icting values for entity i as V t i = { v 1 ,v 2 ,...,v k } from multiple data sources. Next, the system resolves the con fl icts and discovers the true value v in V t i based on the current data uncertainty and source quality. Then, the system updates the data uncertainty and source quality based on the inferred value v and con fl icting values
We summarize the main contributions of our work as follows. Organization. The rest of the paper is organized as follows. Section 2 surveys the related work. Section 3 presents our proba-bilistic model and formulates the problem of truth discovery in data streams. Section 4 introduces the optimization algorithms for the proposed problem. We report the experimental results in Section 5 and conclude the paper in Section 6.
Truth discovery for con fl icting values is a fundamental problem in databases. The main challenges of such a problem are to resolve data inconsistency [1]. The problem of truth discovery for static databases was fi rst formalized by Yin et al. [29] and an iterative algorithm was proposed to jointly infer the truth values and source quality. Pasternack and Roth developed several web-link based al-gorithms and proposed a linear-programming based algorithm [17]. They also introduced a generalized framework that incorporates background knowledge into the truth fi nding process [18]. Galland et al. introduced several fi x-point algorithms to predict the truth values of the facts [10]. Wang et al. proposed an EM algorithm for discovering the truth in sensor networks [24]. Yin and Tan explored semi-supervised truth discovery by utilizing the similarity between data records [30]. Dong et al. studied the source selection prob-lem for truth discovery [9]. Kasneci et al. developed a probabilistic model for truth discovery from several knowledge bases [12]. Pal et al. tackled the problem of evolving data integration [16]. Li et al. conducted an experimental study on existing algorithms [14]. A comprehensive survey of truth discovery techniques can be found in [8].

There are works that focus on other interesting aspects of da-ta integration which are related to the problem of truth discovery. The Q system [22, 21, 28] develops an information need driven paradigm for data integration. The copying relationship detection in data integration was studied in [3, 7, 6, 19]. Liu et al. proposed an early-return data integration method when enough con fi dence is gained for the data from the unprocessed sources that are un-likely to change the answer [15]. Mehmet et al. addressed the privacy-aware data integration [13]. Several works on data fusion in wireless sensor networks and RFID systems are also related to our problem, which discover the true location or reading from a set of observed readings from the sensors [20, 32, 33]. However, these techniques fi rst train the sensor model based on training datasets and then infer the true reading based on the trained models, which are not suitable for truth discovery in data streams.

Nonetheless, none of the above-mentioned algorithms are appli-cable for handling data streams. This is because the existing meth-ods mainly require the entire dataset for processing. More recently, Zhao et al. studied the truth discovery problem using Gibbs sam-pling and they showed that their algorithm outperforms prior meth-ods [31]. An incremental algorithm was also proposed, which is based on the trained model from the batch databases, to discover the truth of the new data. However, their incremental algorithm assumes a training phrase on a given batch dataset, which is not ef fi cient enough, or even not possible, for processing data stream integration in many applications.

Contrary to all the above-mentioned work, we develop ef fi cient algorithms for truth fi nding in data streams, which satisfy the con-straints of one-pass nature, short response time, and limited mem-ory usage.
In this section, we present a probabilistic approach that trans-forms the problem of truth discovery over data streams into a prob-abilistic inference problem. We derive the optimal solution for the inference problem and devise an iterative method. Speci fi cally, we calculate the posterior distribution of all possible values, and fi nd the most probable one with the maximum probability. Intuitively, the proposed model explains all the possible votes by the sources on the con fl icting values. We propose a truth discovery method based on the generative process of the votes in the data streams.
We start by illustrating the general process of truth discovery in data streams. After that, we introduce some basic notions and no-tations in Section 3.1, and de fi ne the problem in Section 3.2. Then, we present a generative process for the vote on con fl icting values in Section 3.3 and de fi ne the probabilistic model in Section 3.4.
Now, we illustrate the general process of truth discovery for the vote on con fl icting values by sources using the following example.
E XAMPLE 1. Consider a set of con fl icting weather forecast val-ues for New York City at time t as shown in Figure 3(a). We aim to report the correct weather conditions in real time. We fi rst extract the weather forecast values  X  X loudy X  and  X  X howery X  X rovided by the sources. We then record the votes towards the extracted val-ues in Figure 3(b). For example, the source ACCU votes only for  X  X loudy X  , the source Underground votes only for  X  X howery X , and the source WFC also votes for  X  X howery X .

The process of truth discovery is to validate the correctness of each value provided by the sources. An example of true weather condition is given in Figure 3(c).
The con fl icting values for an entity at time t are a set of values provided by sources, which are exclusive. We denote the con fl ict-ing values for entity i at time t by V t i = { v t 1 ,i ,v where v t k,i is the value by source k for entity i at time ple, the weather forecast values for New York City at time v
WFC , New York } = {Cloudy, Showery, Showery}. The value v either literal or numeric. We consider the con fl icting values for entities at time t as V t and the sequential con fl icting values at d-ifferent timestamps as V = { V 1 ,V 2 ,...,V T } where T can be in fi nite.
We now consider the vote by different data sources for an entity where { o t i, 1 ,v 1 ,...,o t i,K,v 1 } is the vote of sources on value for entity i and n t i is the number of possible values for entity at time t . For example, the vote by three sources ACCU, Under-ground and WFC for the weather forecast values of New York at time t in Figure 3(b) by vote set O t New York ={ { o t New York o o Thevotevalue o can either be 0 or 1. We consider the vote for O = { O 1 ,O 2 ,...,O T } where T can be in fi nite.
We denote the truth value for an entity i at time t as Z t { z V truth in the values for entity i at time t , V t i ,otherwise An example of the truth value for entity New York is given in { 0 , 1 } .
Existing work [8, 7] usually models the quality of data sources using single accuracy value. However, using single accuracy value may not explain the possible mistakes made by the source such as false positive and false negative mistakes. Thus, we propose a more general quality model using confusion matrix for each source s vote by the source. The quality model of source s is given by Consider a con fl icting value set for an entity i , V t i value Z t i , we explain the vote o t i,s,v of source s on the value V . Based on the confusion matrix of the quality model  X  s ,there are four cases of the vote, given by give a vote m given the value truth n .

We de fi ne  X  s 11 as true positive rate,  X  s 10 as false negative rate,  X  01 as false positive rate, and  X  s 00 as true negative rate, where  X 
We now explain the vote by referring to the source given by the example in Figure 3. Suppose the true weather forecast val-ue is  X  X howery X  as given in Figure 3(c), we then explain the vote by sources ACCU and Underground. The probability of voting by source ACCU on  X  X loudy X  is based on its false positive rate  X  01 and the probability of voting on  X  X howery X  is based on it-s false negative rate  X  ACCU 10 . We simulate the vote by the sources based on the confusion matrix of the quality model. We can see that the source Underground casts a vote on the value  X  X howery X  while the source ACCU does not cast a vote. We conclude that the true positive rate  X  Underground 11 is higher. On the other hand, the source ACCU makes the mistake on voting the true value, since its false negative rate  X  ACCU 10 is high. Similarly, the source Underground does not cast a vote on a false value  X  X loudy X  , which illustrates its high true negative rate  X  Underground 00 . In our proposed source qual-ity model, the accuracy of the source depends on both true positive rate and true negative rate.

We model the quality of a set of sources S by a collection of confusion matrices  X = {  X  1 , X  2 ,..., X  | S | } .
We now formulate the problem of truth discovery in data streams as follows.

Given sets of con fl icting values V 1 ,V 2 ,...,V  X  provided by a set of sources S , we aim to validate each value of the entities in the fectively addressed: (1) one-pass nature : the streaming collections of values can be only read once; (2) limited memory usage : only the current collection of values can be kept in main memory; and (3) short response time : the total running time should be linear to the size of streaming collections of values, and the validation of the values in each set V t should be performed online.
Given a set of sources, we illustrate the generative process for the observed vote O .We fi rst denote a set of parameters  X  = where  X  X  X   X  is the hyper-parameters for confusion matrices and the hyper-parameters for value uncertainty.
For each entity i at time t , its truth Z t i consists of a set of val-z from the Bernoulli distribution [2]. The Bernoulli distribution is the most widely used distribution for binary random variables, which generates value 1 with success probability  X  and value 0 with failure probability 1  X   X  . Thus, the probabilistic generation for validators z value set V t i .

The probability  X  t  X  ,v models the the uncertainty of value the truth or not. We assume that  X  t  X  ,v is generated by a Beta dis-tribution [2]. The Beta distribution generates a continuous value within an interval [0 , 1] with two parameters  X  1 and  X  0 the Beta distribution to generate  X  t  X  ,v because the Beta distribution is the conjugate prior [2] of the Bernoulli distribution. The prob-abilistic generation of value uncertainty  X  t  X  ,v with hyperparameter  X  X  X   X  =(  X  1 , X  0 ) is given by where  X  is a gamma function [2],  X  1 is the prior truth count, and  X  is the prior false count for the values to be the truth in the data streams.
We now show the generative process for each confusion matrix  X  property that  X  s 11 +  X  s 10 =1 and  X  s 00 +  X  s 01 =1 . For brevity, we give the generative process for true positive rate  X  s negative rate  X  s 00 of source s .

We fi rst assume that the true positive rate  X  s 11 is generated from a Beta distribution with hyperparameters  X  11 and  X  10 in by where  X  11 is the prior true positive count and  X  10 is the prior false negative count for the confusion matrix  X  s .

We then assume that the true negative rate  X  s 00 is also generated from a Beta distribution with hyperparameters  X  00 and  X  01 given by where  X  00 is the prior true negative count and  X  01 is the prior false positive count for the confusion matrix  X  s . We show the generative process of the votes made by each source. For the con fl icting values of each entity i , we assume that the votes by source s is generated from the Bernoulli distribution based on the confusion matrix  X  s and the truth value for entity Z the Bernoulli distribution is a suitable choice for its generation. The probabilistic generation for the vote o t i,s,v is given by Forexample,ifthevalue v is the truth in V t i (i.e. z t i,v the vote o t i,s,v is generated by the true positive rate negative rate  X  s 10 of source s .
In the previous discussion, we described a generative process for the votes O . We now formally de fi ne a probabilistic model that represents the underlying joint distribution over the generation of prior distribution for the truth  X  , truth value Z , source quality  X  and the votes O .

Given hyper-parameters  X  = {  X  X  X   X , we factorize the joint distribution over Z ,  X  ,  X  and O ,givenby p ( X  ,Z,  X  ,O | S,  X  )= p ( X  | where and the probability distributions p (  X  t  X  ,v | ) , spectively. For brevity, we omit the conditional part of the joint distribution p (  X ,Z,  X  ,O | S,  X  ) and abbreviate it to p (  X ,Z,  X  ,O ) in the rest of this paper.

Based on the model, the problem of truth fi nding for observed vote can be transformed into a standard probabilistic inference prob-lem, namely, fi nding the maximum a posterior (MAP) con fi gura-tion of the truth Z conditioning on O .Thatisto fi nd where p ( Z | O ) is the posterior distribution of Z given the votes (and  X  ). However, it it dif fi cult to compute the posterior distribu-tion of Z , where
This distribution is intractable to compute due to the coupling between  X  and  X  . To tackle this problem, we develop an ef fi cient and effective approximation algorithm in the next section.
In this section, we propose the algorithms to approximate the dis-batch optimization algorithm for the proposed problem by assum-ing that T is a fi xed value. Then, we introduce two online algo-rithms for solving the problem of truth discovery over data streams.
We present a variational algorithm for discovering the truth with fi xed T .We fi rst restrict the variational distribution to a family of distributions that factorize as follows: q ( X  ,Z,  X ) = ( Thus the calculation of the joint probability distribution can be re-duced to the product of multiple distributions and thus the compu-tation cost can be greatly reduced.

The choice of variational distributions is not arbitrary and we require the distribution in the same family of the model probability distribution and take the following parametric form: where Here,  X  ,  X  ,  X  1 and  X  0 are the variational parameters.
Thus, the inference for the truth value in Equation 8 can be sim-pli fi ed as follows:
Z  X  = [arg max
The goal of the variational algorithm is to fi nd the variational distribution that is close to the true posterior p ( X  ,Z,  X  equivalent to optimizing the variational parameters  X  ,  X  with respect to some distance measure, given by
In this work, we adopt the Kullback-Leibler (KL) divergence which is commonly used to measure the difference between two distributions. It is de fi ned as where KL divergence is a function of the variational parameter-s  X  ,  X  ,  X  1 and  X  0 . However, directly optimizing the KL diver-gence is infeasible because the KL divergence involves the term p ( X  ,Z,  X  | O ) , which is intractable.

Instead, we solve an equivalent maximization problem, whose objective function is de fi ned as
L ( q )=
The equivalence between these two optimization problems can easily be seen as their objective functions sum up to a constant
In order to maximize the objective function L ( q ) ,wetakethe derivatives of it with respect to the variational parameters and  X  0 , and set these derivatives to zeros. For clarity, we put all the derivations in the Appendix. We report the solutions to the optimization problem by  X  for all s =1 ,..., | S | ; t =1 ,...,T ; i =1 ,...,N ; j  X  X   X  (  X  ) is the Digamma function which is the logarithmic derivative of the Gamma function  X (  X  ) ,givenby
In this section, we develop a streaming truth fi nding algorithm called StreamTF , in Algorithm 1. The StreamTF algorithm is able to heuristically fi nd the truth with one-pass nature, short response time and limited memory usage. Furthermore, StreamTF algorithm is also capable for truth discovery in the case that the quality of data sources evolves.
 The idea of the StreamTF algorithm is based on the sequential Bayesian estimation, given by p (  X  t | O 1 ,O 2 ,...,O t )  X  p ( O t |  X  t  X  1 ) p (  X  where  X  t  X  1 is the estimated variational parameters at time indicates that we can use a posterior p (  X  t  X  1 | O 1 ,O as the prior and infer the variational parameters  X  t based on the StreamTF algorithm that fi nds the truth and estimates the source quality sequentially. We notice that Equations 15 and 16 for esti-mating the source quality can also be represented as where we can interpret the terms {  X  0 ,j + T  X  1  X  positive rate of source s , denoted as (  X  s 0 ,j ) T  X  1 spectively. Next, we consider (  X  s 0 ,j ) T  X  1 and (  X  s or parameters for estimating data truth  X  T and data uncertainty Then, we estimate source quality (  X  s 0 ,j ) T and (  X  s estimated  X  T and  X  T ,givenby The StreamTF is outlined in Algorithm 1. We now show how the StreamTF algorithm effectively addresses the three computation is-sues stated in Section 3.2. First, StreamTF achieves one-pass nature since it is obvious that the algorithm reads the data only once. Sec-ond, StreamTF achieves limited memory usage because it only uses memory of size of one collection of votes at any time in the stream. Third, StreamTF achieves short response time since the algorith-m reports the truth online and our experiments also verify that our algorithm can process a lot of collections of votes in one second, which is in effect real time response. Algorithm 1 Streaming Truth Finding ( StreamTF ) old Output: Variational parameters  X  t ,  X  t ,  X  t 1: for t =1  X  X  X  do 2: for each source s  X  S do 3: Set true negative rate  X  s 0  X  (  X  s 0 ) t  X  1 4: Set true positive rate  X  s 1  X  (  X  s 1 ) t  X  1 5: end for 6: repeat 7: for entity i :1  X  N do 8: for each value v  X  V t i do 9: Update  X  t i,v by Equation 13 10: end for 11: end for 12: Update  X  t i by Equation 14 13: until change in 1 14: for each source s  X  S do 15: Update true negative rate (  X  s 0 ) t by Equation 17 16: Update true positive rate (  X  s 1 ) t by Equation 18 17: end for 18: end for 19: return  X  t ,  X  t ,  X  t .
We can further improve the StreamTF algorithm if the size of the dataset is known, by which we can design a one-pass algorithm that not only satis fi es the three computational issues of data stream pro-cessing stated in Section 3.2, but also stochastically maximizes the objective function in Equation 12, i.e., L ( q ) . Such a one-pass algo-rithm is particularly useful for processing massive static databases.
We observe that the objective function L ( q ) can be represented as T functions of the variational parameters, given by quality.

The challenge of this problem is the inference for parameters  X  . The reason is that we only have to keep one collection of votes O algorithm based on the stochastic natural gradient algorithm [4]. We model the streaming collections of votes O 1 ,O 2 ,...,O be sampled from uniform distribution, that is, h ( O t )= expectation of the objective function is given by
Then, we optimize Equation 19 by repeatedly sampling the col-lection of votes at different times, and applying the update Algorithm 2 One-Pass Truth Finding ( 1PassTF ) Input: Observed votes O , input data size T , a threshold Output: Variational parameters  X  ,  X  ,  X  1: De fi ne  X  t =(  X  + t )  X   X  2: for t =1  X  T do 3: repeat 4: for entity i :1  X  N do 5: for v  X  V t i do 6: Update  X  t i,v by Equation 13 7: end for 8: end for 9: Update  X  t i by Equation 14 10: until change in 1 11: for each source s  X  S do 12: Update variational parameters  X  s by Equation 20 13: end for 14: end for 15: return  X  ,  X  ,  X  . The derivation of Equation 20 can be found in the Appendix.
To guarantee the convergence of source quality, we set the decay factor as the function of  X  t =(  X  + t )  X   X  where the parameters control the learning rate of old  X  s i,j to be forgotten. We set and  X &gt; 0 such that  X  the estimation of  X  s i,j can converge to a stationary point.
T HEOREM 1. (Online Optimization [4]) The general greedy de-scent method converges if and only if its learning rates  X 
The detailed proof to the above theorem can be found in [4], and we give the intuition of the convergence on  X  s i,j here. The ratio  X  t =(  X  + t )  X   X  is a function of t and becomes smaller af-ter the algorithm is run for more iterations. The function (  X  i,j . The update on the variational parameter  X  s i,j is the product comes less after the algorithm iterates more. Thus, the inference of parameter  X  s i,j becomes convergent.

The 1PassTF algorithm is outlined in Algorithm 2. It is easy to see that the 1PassTF algorithm also addresses the three compu-tation issues stated in Section 3.2 by following the same analysis given to the StreamTF algorithm at the end of Section 4.2.
In this section, we evaluate the effectiveness and ef fi ciency of our algorithms. All the algorithms, including those we compared with in the experiments, were implemented in Java and tested on ma-chines with Windows OS, Intel(R) Core(TM2) Quad CPU 2.66Hz, and 8GB of RAM.
We use three real datasets to evaluate the performance of our algorithms. Some statistics of the datasets are reported in Table 1. Weather. We collected the weather forecast data in May 2013 for 285 US cities that have a population of at least 100,000. The weath-Datasets #Votes O t i #Sources #Values Entropy Avg Dev Avg Dev Flight 35k 35 2.809 1.159 0.710 0.147 Weather 154k 7 2.148 2.502 0.531 0.412
Stock 21k 51 7.581 4.027 1.227 0.073 er forecast data are modeled as streams reported hourly from dif-ferent sources. The source is obtained as follows. We searched  X  X eather forecast X  on Google and collected the deep-web sources from the top 100 returned results. Among them, we chose the sources where the weather forecast data are encoded in the URL. Then, we selected the sources that forecast the weather hourly and removed the sources whose data were copied from other sources. Finally, we obtained a set of seven sources. We also collected the historic weather data for these cities from a normal weather fore-historic weather data as the truth data, since they are recorded after the day. On the other hand, the weather forecast data may contain some mistakes as the data are based on some kind of prediction. Flight. The fl ight data contains 1200 fl ights from three airlines (AA, UA and Continental) in one month. The sources include the of fi cial websites of the three airlines, and 32 third-party websites that provide fl ight information for the airlines. We consider the data provided by the of fi cial websites of the three airlines as the gold standard.
 Stock. The stock data contains 1000 stocks from 55 sources over one month. We use the data provided from NASDAQ100 as the gold standard.
 These two datasets are used as benchmark datasets in the experi-mental study in [14]. The gold standards are the same as suggested in [14]. For some sources of both Flight data and Stock data, they may copy the values from other sources to provide the values. We use the well-known copy-detection method in [7, 19] to remove these sources.
For continuous values in the datasets such as the departure time of fl ight and stock price, we transform their values into discrete format by the notations of tolerance and bucketing (as suggested in [14]) as follows. http://www.weatherforyou.com/ http://cs.binghamton.edu/~xianli/ truthfinding.htm
We now report the consistency of the datasets above. We consid-er the observed values and votes at time t as V t and O t , respective-ly. We use the average number of values to measure the uncertainty of the data streams. We employ the entropy to measure the confu-sion of the con fl icting values for the sources. Both average number and entropy are popular measurements for data consistency, which are also used in the experimental study of the work in [14]. The details of the statistics of the real datasets can be found in Ta-ble 1.

We evaluate the performance of our batch algorithm and the two streaming algorithms using the above datasets. To measure the ef-fectiveness of our methods, we de fi ne the average accuracy (AVG), minimum accuracy (MIN) and standard deviation of accuracy (DE-V). We fi rst partition the collection of the data streams into buckets of the same size and compute the accuracy of the algorithm for each bucket. Then, we compute the average accuracy and minimum ac-curacy over all the buckets as AVG and MIN, respectively. Finally, we compute the standard deviation of accuracy based on the buck-et accuracy and average accuracy as DEV. By default, we set the size of each bucket to be 300. For the streaming algorithms, we al-so evaluate their robustness by varying the decay factor  X  decay seed  X  .
We next evaluate the performance of our streaming algorithm-s, StreamTF and 1passTF , for the following three measures: (1) accuracy, (2) running time, and (3) robustness. Since there is no existing algorithm tackling truth fi nding over data streams, we use the incremental algorithm LTMinc [31] as the baseline for compar-ison. We use 10%-40% percentage of the data for training the LTM model for LTMinc, denoted by LTMinc 0 . 1 -LTMinc 0 . 4 , respective-ly.
We now present the result of accuracy of the algorithms in Ta-ble 2. For all the three datasets, both StreamTF and 1passTF achieve signi fi cantly higher accuracy than LTMinc. Both the average accu-racy and minimum accuracy of StreamTF and 1passTF are higher than those of LTMinc.
 Notably, the accuracy of 1passTF is higher than that of StreamT-F, which can be explained as follows. Both StreamTF and 1passT-F sequentially fi nd the truth as well as estimate the source qual-ity over the data streams. The difference between StreamTF and 1passTF is on the estimation of source quality. The StreamTF algorithm estimates online the source quality based on sequential bayesian estimation in Equation 17. However, the estimation of source quality based on sequential bayesian cannot optimize the likelihood objective function. On the contrary, the 1passTF algo-rithm stochastically optimizes online the objective function in E-quation 19 in order to accurately estimate the source quality using gradient descent in Equation 20. As a result, the performance of 1passTF is better than that of StreamTF.
We report the running time of the algorithms in Figures 4(a), 4(b) and 4(c), respectively. We sequentially pass the votes for the entity O t i to our algorithms. For all the datasets, StreamTF and 1passTF are faster than LTMinc, and able to process data streams at high speed.

It is also interesting to see that the running time of StreamTF and 1passTF decreases when more data have been processed (i.e., the running time increases sub-linear with the increase in the amount of data being processed), as shown in Figures 4(a), 4(b) and 4(c). The streaming algorithms keep estimating online the confusion matrix for source quality. They take more iterations to converge when esti-mating the source quality from the initial period of the data streams. As we know, the confusion matrix of the source quality is station-ary. As the time passes by, our streaming algorithms take less and less iterations to converge. Finally, we fi nd that the number of itera-tions for inferring the source quality becomes one when the estima-tion of source confusion matrix reaches the stationary point. Thus, the time cost of our streaming algorithms can be greatly reduced as more data is being processed.
We evaluate the robustness of the 1passTF algorithm by varying its parameters, decay ratio  X  and decay seed  X  , to validate its ef-fectiveness. We measure the performance of the algorithms by the average accuracy and minimum accuracy (indicated by adding the pre fi x X  min- X  to the algorithm names in the fi gures), respective-ly. We denote the 1PassTF algorithm on different datasets such as fl ight, weather and stock by 1PassTF f , 1PassTF w and 1PassTF respectively. The results in Figure 5 show that our 1passTF algo-rithm is robust as it achieves quite consistent high accuracy for dif-ferent values of decay ratio and decay seed. It is worth mentioning that the running time of 1passTF also remains stable for different values of decay ratio and decay seed.
In this paper, we studied the problem of truth discovery in data streams, which has a wide range of data stream applications such as weather forecast and fl ight scheduling. We proposed a probabilis-tic model that transforms the problem of truth discovery over data streams into a probabilistic inference problem. We fi rst developed a streaming algorithm that discovers the truth under the constraints of one-pass nature, limited memory usage and short response time. Then, we also proposed a one-pass algorithm that is able to stochas-tically optimize the probabilistic inference of source quality, which is able to further improve the accuracy of the streaming algorithm. As for empirical study, we veri fi ed the effectiveness and ef fi cien-cy of our algorithms using three real datasets from the data stream applications of weather forecast, fl ight scheduling and stock price prediction. The experimental results validate the effectiveness of our algorithms, in terms of both integration accuracy and running time.
 Acknowledgments. We thank the reviewers for giving us many constructive comments, with which we have signi fi cantly improved our paper. This research is supported in part by SHIAE Grant No. 8115048, MSRA Grant No. 6903555, and HKUST Grant No. FS-GRF14EG31. [1] M. Arenas, L. Bertossi, and J. Chomicki. Consistent query [2] C.M.BishopandN.M.Nasrabadi. Pattern recognition and [3] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti. [4] L. Bottou. Online learning and stochastic approximations. [5] H. Chen, W.-S. Ku, H. Wang, and M.-T. Sun. Leveraging [6] X. L. Dong, L. Berti-Equille, and D. Srivastava. Integrating [7] X. L. Dong, L. Berti-Equille, and D. Srivastava. Truth [8] X. L. Dong, A. Halevy, and C. Yu. Data integration with [9] X. L. Dong, B. Saha, and D. Srivastava. Less is more: [10] A. Galland, S. Abiteboul, A. Marian, and P. Senellart. [11] L. Jia, H. Wang, J. Li, and H. Gao. Incremental truth [12] G. Kasneci, J. V. Gael, D. Stern, and T. Graepel. Cobayes: [13] M. Kuzu, M. Kantarcioglu, A. Inan, E. Bertino, E. Durham, [14] X. Li, X. L. Dong, K. Lyons, W. Meng, and D. Srivastava. [15] X. Liu, X. L. Dong, B. C. Ooi, and D. Srivastava. Online [16] A. Pal, V. Rastogi, A. Machanavajjhala, and P. Bohannon. [17] J. Pasternack and D. Roth. Knowing what to believe (when [18] J. Pasternack and D. Roth. Making better informed trust [19] A. D. Sarma, X. L. Dong, and A. Halevy. Data integration [20] D. Smith and S. Singh. Approaches to multisensor data [21] P. P. Talukdar, Z. G. Ives, and F. Pereira. Automatically [22] P. P. Talukdar, M. Jacob, M. S. Mehmood, K. Crammer, [23] N. Tatbul. Streaming data integration: Challenges and [24] D. Wang, T. Abdelzaher, L. Kaplan, and C. C. Aggarwal. On [25] D. Wang, L. Kaplan, H. Le, and T. Abdelzaher. On truth [26] M. Wu and A. Marian. Corroborating answers from multiple [27] M. Wu and A. Marian. A framework for corroborating [28] Z. Yan, N. Zheng, Z. G. Ives, P. P. Talukdar, and C. Yu. [29] X. Yin, J. Han, and P. S. Yu. Truth discovery with multiple [30] X. Yin and W. Tan. Semi-supervised truth discovery. In [31] B.Zhao,B.I.Rubinstein,J.Gemmell,andJ.Han.A [32] Z. Zhao and W. Ng. A model-based approach for r fi ddata [33] Z. Zhao, D. Yan, and W. Ng. A probabilistic convex hull
