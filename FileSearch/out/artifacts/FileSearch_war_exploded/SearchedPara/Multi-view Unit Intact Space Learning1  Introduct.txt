 Due to the rapid development of information technology, an increasing amount of multi-view data has been generated from diverse domains [24]. Different domains can be taken as different views. For example, in web page classification task, the description of a web page can be partitioned into two views, namely the words occurring on that page and the words occurring in hyperlinks pointing to that page [3]. Although different views exhibit heterogeneous properties of data, they are coupled by some underlying relations and exhibit some common structures. Properly combining the information from different views by exploring the relations will improve the learning performance. This leads to the emergence of multi-view learning, which is a challenging task in the field of machine learning. spectives. In [11] [12], two simple schemes are introduced to combine multiple views. One is feature concatenation, which concatenates variables in all views into one view and applies the single-view learning method on the combined view. The other is to combine the single-view learning objective functions of all views into a multi-view learning model. However, both two schemes do not work quite well in improving the learning performance, especially when the multiple views are from different representation spaces [20]. An alternating improved variant is the weighted combination scheme. Instead of naive convex combination of objective functions of different views, a hyperparameter is used to control the distribution of weight parameters [21] [22] [5]. Furthermore, an auto-weighted multi-view learning framework is proposed to learn an optimal weight without introducing any additional parameter [13]. The weighted scheme is effective in some relatively simple applications but may easily degenerate in some complex tasks.
 approach has also been utilized [3]. Three main assumptions are made, namely sufficiency, compatibility and conditional independence. Based on these assump-tions, the co-training approaches can produce relatively good performance by iteratively maximizing the mutual agreement between two distinct views. The scheme is attractive due to its effectiveness and some variants have been devel-oped such as co-EM [14] [4] and co-regularization [18] [17] [12]. However, the co-training approaches may produce poor results in some cases where the three rigorous assumptions cannot be satisfied.
 is based on the idea that all the views share a latent subspace. One representative technique is canonical correlation analysis (CCA), which models the relationships between several sets of variables [6] [10]. Multi-view Fisher discriminant analysis is another subspace learning approach based on Fisher discriminant analysis [8] [26]. Additionally, some efforts have been made to find low-dimensional embed-ding of data to fulfill the multi-view learning tasks, such as spectral analysis [22] and stochastic neighbor embedding [23].
 space learning , which has drawn a large amount of attention [25]. The model is based on the view insufficiency assumption, i.e. each individual view only captures partial information. It aims to integrate the information from different views and find a latent intact space of data which contains all information. It is demonstrated to be effective on several real-world applications. Despite the success, this method has two shortcomings. First, the model adopts two regularization terms to control the scale of the latent feature vectors. Due to this reason, two regularization parameters are needed for trade-off between the reconstruction error and regularization terms. Determining these parameters by cross validation costs extra computation resources and does not work well in unsupervised learning. The second drawback is that the optimization process for solving the problem is too time-consuming because it involves matrix inversion. is based on an assumption that the latent intact space of data is actually the surface of a unit hypersphere, namely unit intact space assumption . Based on the assumption, the latent intact space representations of objects are points on the surface of the unit hypersphere. Therefore, there is no need of regulariza-optimization algorithm is designed to solve the model based on the proximal gradient method [15]. Extensive experiments have been conducted on four real-world datasets to demonstrate the effectiveness of our model. 2.1 Background Recently, an interesting multi-view learning model termed multi-view intact s-pace learning was proposed in [25] and has drawn a large amount of attention. Based on the view insufficiency assumption, the model aims to find a latent in-tact space which integrates information of data from all views. Given a dataset containing N objects collected from M views, the data representation in the v -th view can be represented using a data matrix X v = [ x v 1 ,..., x v N ]  X  R D v  X  N where D v is the dimensionality of the v -th view. In this work, the view generation func-tions are assumed to be linear which extracts the feature from the intact space Z to a D v -dimensional feature space X v . Therefore, a view generation function from the intact space to the v -th view can be expressed as f v ( z i ) = W v z i where W v  X  R D v  X  d ,  X  v = 1 ,...,M is the view generation matrix and z i is the feature representation of an object in the d -dimensional intact space. The feature rep-resentation of the latent intact space for the whole dataset can be expressed in matrix form as Z = [ z 1 ,..., z N ]  X  R d  X  N . Adopting the Cauchy loss to measure the reconstruction error, the model is formulated as min where k X k F is the Frobenius norm and k X k 2 is the L 2 norm. In the model, two regularization terms are introduced to control the scale of the latent fea-ture vectors and two regularization parameters are needed for trade-off between reconstruction error and regularization, which can be determined by cross vali-dation. An iteratively reweighted residuals (IRR) algorithm is designed to solve the model. 2.2 Multi-view Unit Intact Space Learning Despite the success, the aforementioned method has two shortcomings. First, the model adopts two regularization terms to control the scale of the latent fea-ture vectors. Due to this reason, two regularization parameters are needed for trade-off between the reconstruction error and regularization terms. Determin-ing these parameters by cross validation costs extra computation resources and does not work well in unsupervised learning. The second drawback is that the optimization process for solving the problem is too time-consuming because it involves matrix inversion. Therefore, we propose an improved model to solve the two shortcomings.
 actually the surface of a unit hypersphere, namely unit intact space assumption . Based on the assumption, the latent intact representations of objects are points on the surface of the unit hypersphere so that all the feature representation vectors in the latent intact space have the same length, i.e. all the vectors are unit vectors. The assumption is reasonable for data clustering or classification problem when measuring the similarity among data points by the angles between two feature vectors [7]. Given two objects with unit feature vectors x and y , the Euclidean distance between them is k x  X  y k 2 = p 2  X  2 x T y and the cosine distance is 1  X  x T y . It is easy to find that the two distance measures are positively correlated, i.e. the Euclidean distance is equivalent to the angles between the two unit vectors. Therefore, after obtaining the unit intact space representation of data, the Euclidean distance can be used to measure the similarity between data objects. For illustration purpose, Figure 1 shows the main idea of the model. Fig. 1: Illustration of unit intact space assumption : the red point on the hy-persphere stands for an object in the unit intact space Z . Applying the view generation matrix W v , we get the view representations of all the objects in feature space X v .
 the scale of latent intact feature vectors is controlled by directly adding a unit-length constraint for all the objects. The model is called Multi-View Unit Intact space Learning (MVUIL). The formulation of the model is pler where no prior parameters are needed.
 By using alternating optimization scheme, the optimization problem in Eq. (2) can be decomposed into two subproblems over the view generation matrices { W v } and the latent intact feature vectors { z i } respectively. In particular, an iterative optimization method based on proximal gradient scheme [15] is pro-posed to solve the problem. 3.1 Update Latent Feature Vectors in Unit Intact Space In this subsection, by fixing the view generation matrices { W v } , the latent intact feature vectors { z i } are updated. By introducing a penalty function the subproblem with respect to z i is equivalent to minimizing the following k -th iteration, we consider a quadratic model to approximate f z ( z i ) in the form where  X  X  ,  X  X  is the inner product of vectors, t k is the step size in the k -th iteration, respect to z i at z ( k ) i . In Eq. (5), the first two terms stand for the linearized part of f z at z ( k ) i and the last term measures the local error. Then an approximation model of L z is given by We can get the ( k + 1)-th solution in following closed form where prox t the updating formula of z i is given by 3.2 Update View Generation Matrices generation matrices { W v } are updated. The subproblem with respect to W v becomes minimizing the following which is an unconstrained minimization problem. We can solve the problem by the following updating formula where W v ( k ) is the solution during the k -th iteration,  X  k is the step size in the k -th iteration, and  X  X  w W v ( k ) = 2 W v ( k ) Z  X  X v Z T is the partial gradient of L w with respect to W v at W v ( k ) .
 feature representations of data from their multi-view feature representations. The pseudo code of the optimization algorithm is given in Algorithm 1. In our experiment, we use the same step-size rule as recommended in [2], namely the constant step-size or backtracking step-size.
 Algorithm 1 Multi-view unit intact space learning 3.3 Convergence Analysis Since we utilize the proximal gradient method to solve the optimization problem, the convergence analysis about { z i } can be obtained as follows. Assume that the gradient  X  f z satisfies the Lipschitz condition and L ( f z ) is the Lipschitz constant. We can get the convergence condition of z i according to the theorems in [2]. Let { z i } be the sequence generated by the proximal gradient method with either a constant or a backtracking step-size rule, then by applying the theorems in [2] for every n &gt; 1 we have and  X  = L 0 /L ( f z ) for the backtracking case 1 . Moreover, z ( k  X  1) i  X  z ( k ) i as k  X  X  X  . Therefore, the value of { z i } will gradually converge in the iteration. 3.4 Complexity Analysis In order to demonstrate the efficiency of our algorithm, we give computation-al complexity analysis and make comparison with the multi-view intact space learning algorithm. We first analyze the time complexity of our method. Since the optimization procedure is iterative, we begin with analyzing the complexity in each iteration and then obtain the total complexity of the whole algorithm. For simplicity, we denote D 1 = P M v =1 D v . In each iteration, the computation time of calculating the gradient during updating unit intact feature vectors { z i } is O ( D 1 dN ). The computation time of calculating the gradient during updating view generation matrices { W v } is O ( D 1 dN ). The updating time for { z i } and { W v } are respectively O ( dN ) and O ( D 1 d ). Therefore, the total time complexity of our algorithm is O ( T 1 D 1 dN ) where T 1 is the number of iterations. [25] can be analyzed as follows. Like the analysis before, we first consider the com-plexity in each iteration. According to the description in [25], the computational complexity for calculating the weight function is O ( D 1 dN ). The time complexity for evaluating the latent intact feature for one object is O ( D 1 d 2 + d 3 ) since the complexity for calculating the matrix inversion is O ( d 3 ). For N objects in the whole dataset, the complexity is O ( D 1 d 2 N + d 3 N ). For the v -th view, the com-plexity of evaluating the view generation function is O ( D v d 2 + D v dN + d 2 N + d 3 ). For all views, the computation complexity is O ( D 1 d 2 + D 1 dN + d 2 N + d 3 ) in one iteration. Therefore, assuming that the number of iterations is T 2 , the total complexity of the algorithm is O ( T 2 D 1 d 2 N + T 2 d 3 N ), which is several orders of magnitude larger than our method. In this section, extensive experiments are conducted to demonstrate the effec-tiveness of our method by regarding it as the preprocessing step of the multi-view clustering task. That is, k -means is applied on the unit intact space representa-tions of the multi-view data, the performance of which is compared with several start-of-the-art clustering algorithms. 4.1 Datasets and Evaluation Measures Handwritten Numeral Dataset: Multiple Features (Mfeat) dataset is a hand-written numeral image dataset from UCI machine learning repository [1]. The dataset contains 2000 images from 10 classes. In our experiment, three kinds of features are used to represent each image. The fac feature stands for 216 profile correlations, the fou feature stands for 76 Fourier coefficients and the kar feature stands for 64 Karhunen-Love coefficients.
 dataset whose objects are all news stories collected from three news sources, namely BBC, The Guardian and Reuters. The original dataset contains 984 new articles covering 416 distinct news stories. However, not all news are reported by all three medias. In our experiment, only 169 stories reported by all three sources are used so that each object has three views. All the stories from three sources use TF-IDF representaion.
 datasets constructed from the single-view BBC and BBCSport corpora by split-ting news articles into related segments of text. The construction is done by first separating the raw documents into segments and then assigning segments randomly to views. Both datasets are split into four views and every view uses TF-IDF features to represent the data. For the BBC dataset 685 news objects are collected and for the BBCSport dataset 116 news objects are collected. mance namely normalized mutual information (NMI), clustering accuracy (AC-C) and purity (PUR), which are all widely used. We can see [19] for detailed information. 4.2 Parameter Analysis In this subsection, we analyze the effect of the dimensionality d of the latent unit intact space on the learning performance. By using different dimensionality d , we analyze the clustering performance on three evaluation measures. The results are shown in Figure 2. From the figure, our algorithm performs well when the dimensionality d lies in some relatively wide range. For example, it produces good results with d  X  [100 , 160] on Mfeat and with d  X  [3500 , 5000] on 3Sources. The empirical results show that a value closed to the average dimensionality of all views is a suitable choice for generating good results. 4.3 Comparison Results In this subsection, comparison experiments are conducted on the clustering per-formance between the proposed Multi-View Unit Intact space Learning (MVUIL) for clustering and several state-of-the-art algorithms, including clustering algo-rithms and the original multi-view intact space learning for clustering. We will first briefly introduce the compared algorithms and then analyze the comparison results on the four real-world datasets.
 periments. The first type is the traditional single-view clustering algorithm, in-cluding k -means (KM) [27], affinity propagation (AP) [9] and normalized cut (NC) [16]. The other type is the state-of-the-art multi-view clustering algorith-m, including multi-view k -means (MVKM) [5], co-training spectral clustering (CTMS) [11], co-regularized spectral clustering (CRMS) [12] and multi-view affinity propagation (MVAP) [20]. Besides, we also compare the performance of our method with multi-view intact space learning (MVIL) [27]. After obtaining the intact representation of data, we utilize k -means to get clustering results. clusters, the ground-truth cluster number is used. For the single-view methods, different features from several views are concatenated to generate a combined view for each dataset, on which the single-view methods are applied. For single-view and multi-view affinity propagation, the similarity graphs are set as recom-mended in the original papers. For single-view and multi-view spectral clustering methods, we use Gaussian kernel to measure the similarity between two objects dard deviation  X  is set as the median of Euclidean distances between all pairs of objects [19]. For both MVIL and the proposed MVUIL, the same dimensionality of intact space is used for each dataset. And for MVIL, the two regularization parameters are tuned to generate the best clustering performance. The results on three evaluation measures are given in Table 1, Table 2 and Table 3. In what follows, we analysis the comparison results.
 achieve the highest purity. However, the two algorithms produce the worst per-formance in terms of ACC. It is because that the true number of clusters are not given for the two algorithms so that the two algorithms tend to separate some ground-truth classes into several small clusters. Considering all the three measures, we find that CRMS produces the best results. This is mainly due to the fact that spectral methods outperform other clustering methods on image datasets. Nevertheless, our method still achieves relatively good performance on this dataset. tering algorithms to generate good clustering results. Besides, the multi-view learning approach based on weighted combination (i.e. MVKM) is also not ap-plicable for this dataset. When comparing KM and MVKM, we find MVKM produces worse results than KM in terms of all the three measures. The phe-nomenon implies that the weighted combination approach is not suitable for Table 1: Comparison on NMI over 100 runs. For each dataset, the first line represents mean and the second line represents standard deviation. The highest mean NMIs are highlighted in bold.
 Table 2: Comparison on ACC over 100 runs. For each dataset, the first line represents mean and the second line represents standard deviation. The highest mean ACCs are highlighted in bold.
 Table 3: Comparison on PUR over 100 runs. For each dataset, the first line represents mean and the second line represents standard deviation. The highest mean PURs are highlighted in bold.
 improving the clustering performance. We also find that CTMS and CRMS are both effective approaches to combine information from different views. MVAP has relatively better performance that it improves the values on NMI and pu-rity. It is shown that MVIL produces worse result than CTMS but our method produces better result, implying that MVIL fails to find the intact space but our method succeeds. Overall, our method achieves the best clustering result compared with other methods in terms of all three measures.
 based on weighted combination (i.e. MVKM) is not applicable, i.e. it produces worse results than single-view KM. Besides, we find MVAP produces lower ACC and higher PUR than AP, implying that it tends to separate the true clus-ters into small clusters. CTMS also performs relatively well on this dataset. Compared with these methods, both our method and MVIL have made signifi-cant improvement on clustering performance. Our method performs better and it nearly doubles the NMI produced by CTMS. All the other algorithms, in-cluding both single-view and multi-view algorithms, produce poor results whose NMI values are all below 0.31. The main reason may be that, on this dataset, each original news article is split into four segments, taken as four different but complementary views. All the compared clustering methods fail to recover a complete feature space, leading to poor clustering performance. However, our method finds a unit intact space for data, which provides a relatively complete feature representation for clustering.
 method as BBC, the dataset has the similar property. Therefore, the multi-view approaches mentioned above have similar performance to those on the BBC dataset. Our method has also made significant improvement on clustering performance. The underlying reason is the same as the previous BBC dataset. In this paper, we propose a multi-view learning method termed multi-view unit intact space learning . It is an improved model of multi-view intact space learn-ing which aims to learn a latent intact space integrating the information from different views. Although multi-view intact space learning succeeds in finding intact space, the model has two obvious shortcomings, namely the adoption of two regularization parameters and the high time complexity of optimization. Based on the unit intact space assumption , our model can find the intact feature representation without introducing any prior parameters. An efficient algorithm is designed to solve the model based on proximal gradient method. Experiments on real-world datasets demonstrate the effectiveness of our method.

