 1. Introduction
Neural Networks (NNs) have been widely used in pattern recogni-tion, function approximation and many other application areas in recent years and have shown their strength in solving hard problems. Because of characteristics of low power consumption and small size,
Embedded Systems (ESs) have also been widely used in automatic control, handheld devices, home appliances and many others.
However, mapping of resultant NNs onto fast and compact ESs is a difficult task. The problem is that the conventional NNs, which have real weights and non-linear activation function, are expensive to store weights and implement calculation in ESs. NNs having limited precision weights and biases are easier and less expensive to implement in ESs and the storage of the limited precision weights is much easier to achieved. A look-up table for all the hidden and output neurons greatly reduces the complexity of the ESs imple-mentation, because there is no need to implement complicated non-linear activation functions.

There have been some researches focusing on the relationship between NNs and its weights precision. It can be concluded that weights precision not only affect the convergence performance of training algorithms but also affect the NNs X  learning capability. The convergence performance and learning capability is relatively weaker when the weights precision is lower ( Khan and Wilson, 1996 ; Drafhici, 2002 ). These researches X  intention is to reduce weights storage requirements and improve processing speed when NNs implement in digital hardware or analog hardware by reducing weights precision.

The quantization methods of non-linear activation function have been studied by several papers. First, the non-linear activa-tion function is replaced by n straight lines according to a fitting error and each line approximates a certain curve ( Jian and Bin, 2009 ). Second, the non-linear activation function is quantized as a look-up table ( Marchesi et al., 1993 ; Yi et al., 2008 ). But the first method still needs multiplications and the second method does not relate the look-up table to weights precision, so the computa-tion speed is not very fast or the calculation is not accurate enough. In this paper, we propose a new quantization method which quantizes the non-linear activation function as a look-up table according to weights precision, and the activation function can be easily computed through this look-up table.

The training algorithm chosen , has a great influence on these neural networks (NNs) with limited precision weights. The researches on training algorithm using limited precision weights can be divided into two categories, namely: the algorithms improving the existing algorithms and the new algorithms not based on existing algorithms 1998 ; Chieueh and Goodman, 1988 ; Alibeik et al., 1995 ; Kamio et al., 2000 ). A multiple-threshold method (MTM) has been proposed for using discrete-weight NNs ( Chieueh and Goodman, 1988 ). In this method, the continuous weights of a fully trained NNs are quantized into discrete valued weights using a non-linear function. A Quantized
Gradient Descent Rule (QGDR) algorithm has been presented using a modified version of Back-propagation (BP) algorithm ( Alibeik et al., 1995 ). In this method, initially the network is trained with floating-point weights using BP algorithm, then the weights are quantized and the network is trained using QGDR, last each weight ( W changed in the opposite direction of dE / dW i for an amount equal to one quantization step, then weights quantization and QGDR are repeated until the desired weights precision is reached. Unfortu-nately, all methods above do not discretize the non-linear activation function as a look-up table in training process. When a fully trained network works, a simple look-up table is used instead of the non-linear activation function, which brings new error. Therefore, we propose a simple training algorith m called Quantize Back-propaga-tion Step-by-Step (QBPSS), which trains the neural networks with limited precision weights and uses a look-up table according to weights precision, cannot bring the new error.
 The remainder of this paper is divided into four sections. In
Section 2 , analyze and demonstrate the learning capability of neural networks with limited precision weights (LPWNN). Meth-ods of designing and training LPWNN are discussed in Section 3 , including learning rate, the number of hidden neurons, weights precision, quantization of non-linear activation function and Quantify Back-propagation Step-by-Step (QBPSS) algorithm. The
Section 4 presents the intelligent vehicle road recognition experi-ment, computer simulation results and an application of the optimized LPNN in embedded systems. The final section contains our conclusion and a short discussion. 2. Learning capability of neural networks with limited precision weights
Formally, a neural network consists of Q layers, where the first layer denotes the input, the last one, Q , is the output, and the intermediate layers are the hidden layers. It is assumed that the q -th layer has n q neurons. Each neuron implements a hyperplane in the n q 1 -dimensional space of its inputs as follows: w where w q ij is the discrete connection weight from the j -th neuron at the ( q 1)-th layer to the i -th neuron at the q -th layer, y output of the j -th neuron belonging to the q -th layer and y denotes the discrete bias of the i -th neuron at the q -th layer. Then we use the following definition.
 precision b as: b  X  w j w i ( b 4 0, j i  X  1), so the weights w 0,1, y ,[ r / b ]}. For instance, if b  X  0.1 and w A [ 1, 1], then the weights can be selected from the set of { 1.0, 0.9, y ,0, 0.9, 1.0}.

First, we will introduce how the w eights range affect the learning capability of neural networks (NNs) ( Drafhici 2002 ). With the weights range restricted, the hyperplanes mesh becomes very rare with large interstices in-bet ween hyperplanes. It can be easily imagined from above that different patterns will be easily put in a same interstice such that the patterns from different classes are more difficult to be separated by the hyperplanes. In other words, the learning capability of NNs is relatively weaker when the weights range is smaller.

Second, we will analyze how the weights precision affect the learning capability of NNs under the same weights range. Assum-ing the weights range as [ 1,1] in 2-dimensional space and weights precision b equals to 1 and 0.1, respectively (see Fig. 1 ).
From the picture we can see: when b equals to 1, the number of different angles of hyperplanes is only three, but when b equals to 0.1, the number is much more than three; when b equals to 1, the hyperplane with one kind angle has several positions to be put, but when b equals to 0.1, there are much more positions to be put. Obviously, under the same weights range, the lower of the weights precision is, the rarer of the mesh it is. In the same way, the patterns from different classes are much more difficult to be separated by the hyperplanes. Namely, the weights precision has seriously affected the learning capability of NNs.

Thus, the learning capability of neural networks with limited precision weights (LPWNN) becomes more limited as the weights precision becomes lower. At the same time, it is difficult for conven-tional training algorithms to guarantee the convergence when the weights precision is limited to relatively low. But this inability can be compensated for by improving the conventional training algorithms or proposing other new algorithms. 3. Design neural networks with limited precision weights and its training algorithm 3.1. The number of hidden neurons and learning rate
The number of hidden neurons plays an important role in training procedure. When the number is not large enough, the training algorithm cannot guarantee the convergence even not converge, because the number cannot be large enough to form a decision region as complex as that required by the problem. And yet the number is excessively large, the convergence procedure will become very slow because of mass calculation.

In a previous paper ( Huang and Babri, 1998 ) we have studied that single-hidden layer feedforward networks with at most N hidden neurons (including biases) can learn N distinct samples with zero error.

If the learning rate is small enough, then convergence to the local minimum in question is guaranteed. However, very small learning rate can lead to long training times. And too large learning rate will diverge from the solution. In practical training with limited precision weights, it is found that selection of small learning rate to high precision weights and selection of relatively large learning rate to low precision weights will be suitable. Moreover, the network performs well with learning rate of a  X  0.05 when the weights precision b is equals to 0.1. 3.2. Weights precision
Neural networks with lower preci sion weights, which require less memory for storing the weights and less expensive floating point units in order to perform the computations involved, are better suited for embedded systems impl ementation than the high preci-sion weights ones. But, the learning capability and convergence performance of networks will become weaker as the weights preci-sion becomes lower. In order to resolve the above contradiction, we should select different weights pre cision for different problems. And it is found in the experiment that the precision of b  X  0.1 is often adequate for pattern recognition problem. 3.3. Quantization of non-linear activation function
The computation efficiency of activation functions plays a key role in training process and working process with fully trained neural networks because activation functions are called much frequently in the calculation process. Moreover, simple calcula-tion greatly reduces the complexity of embedded systems imple-mentation. Generally, the sigmoid function is the most widely used activation function for its nonlinear, continuous and deri-vable properties, and a common example is the logistic function such as s ( x )  X  1/1  X  e x . Because of these properties, the compu-tation efficiency of sigmoid function will be low in embedded systems. Therefore, a quantization strategy called quantized the non-linear activation function as a look-up table according to weights precision, which can greatly improve the computation efficiency, is presented. Fig. 2 shows the quantization strategy.
The output of a neuron with precision of b can be calculated as follows: y  X  round b  X  s  X  net  X  X  X  2  X  where y b is the output with precision of b , round b ( x ) is the quantization function with precision of b , s is the non-linear activation function and net  X  P WX  X  y . Because the derivation of the activation function is as follows: s 0  X  net  X  X  s  X  net  X  X  1 s  X  net  X  X  X  3  X 
Accordingly, the derivation of quantized activation function can be calculate as follows: s 0  X  net  X  X  y b  X  1 y b  X  X  4  X 
Then we can quantize the non-linear activation function as a look-up table according to weights precision by Eq. (2). When a fully trained neural network works, a simple look-up table is used instead of a complicated calculation with a sigmoid function, which greatly reduces the complexity of implementation. Fig. 2 shows the results after being quantized when the weights preci-sion b is 0.1. As is shown in Fig. 2 , the sigmoid function can be replaced by 11 intervals. Obviously, higher the precision is, more the intervals will be. But this will increase the length of the look-up table and reduce the computation efficiency. On the other hand, as we have observed, the weights precision is 0.1, then the look-up table has 11 intervals, the fully trained networks can work well in embedded systems. Thus, for a given application a tradeoff between effectiveness and precision has to be considered. 3.4. Quantize back-propagation step-by-step (QBPSS)
The conventional Back-propagati on (BP) algorithm is one of the most general methods for supervised training of multilayer neural networks. The algorithm has two primary modes of operation: feedforward and learning. The weights can be updated as follows: q  X   X  k  X  1  X  X  W q  X  k  X  X  a D W q  X  k  X  X  W q  X  k  X  X  a d q D W q  X  k  X  X  @ E  X  k  X  E  X  k  X  X  1 2
For the hidden layer: d
For the output layer: d  X  X  D Y Q  X  s 0  X  net Q  X  X  9  X  where Q is the number of layers, D  X  X  d 1 , d 2 , ... , d vector, E ( k ) is the mean-square error at iteration k , W weight vector at the q -th layer at iteration k , D W q ( k ) is the negative gradient vector, n Q is the number of neurons at the
Q -th layer, q  X  Q 1, Q 2, y ,1, Y q  X  X  y q 1 , y q 2 , ... , y vector at the q -th layer and the derivation of activation function at the q -th layer is calculated by Eq. (3).

Generally, the conventional BP algorithm can get a good conver-gence because of real weights and without quantization operation.
And the convergence process is mainly based on gradient descent search which updates weights according to the gradient information.
At the beginning of the process, the convergence rate is fast and the weights are updated quickly because the absolute value of gradient is large. As the process goes on, the incremental changes in weights decrease gradually and the convergence rate becomes slower.
Accordingly, if the conventional BP algorithm trains the network with limited precision ( b  X  0.1) weights, it can hardly converge.
Because the weights increment must be larger than or equal to 0.1 at one step, then the weights can be updated. Otherwise, the increment will be quantized to zero by quantization operation and weights will be trapped in one of the spurious local minima caused by weight quantization. A good training algorithm based on BP algorithm is proposed in this paper called Quantize Back-propagation
Step-by-Step (QBPSS), which tra ins neural networks with limited precision weights efficiently.

In the QBPSS training process, the weights precision is 0.1, the allowed error function E allowed is 0.05, e is actual error, Count is the epoch number and the largest allowed epoch number is 200. Then update all the weights as follows:
W  X  k  X  1  X  X  round b  X  W q  X  k  X  X  a D W q  X  k  X  X  Z D W q  X  k 1  X  X  X  10  X  where Z (0 r Z o 1) is the momentum and D W q ( k 1) is the negative gradient vector at iteration k 1. Introducing a momen-tum allows the network to learn more quickly when plateaus in the error surface exist. Initially, the network is trained with relatively high precision ( b 1 o 0.1) weights and a look-up table quantized by b 1 , because the high precision weights are more easily trained, until some error or iteration number is reached.
Then the weights are quantized to lower precision and its new value falls on the edge of the quantized region which centers on the original weights. On this basis, repeating the quantization process until the desired weights precision and the allowed error are all reached (see Fig. 3 ). And the rules on the step-by-step quantization are shown in Table 1 .

The training procedure with limited precision weights consists in the following steps.
 Step 1 : Quantize the weights precision to  X  b 1 ( b 1 o 0.1).
Step 2 : Quantize the non-linear activation function as several Step 3 : Initialize the neural networks with quantized weights. Step 4 : Present input patterns and desired patterns to networks.
Step 5 : Calculate the outputs d ( Q ) and error e ,if e 4 E Step 6 : Update the weights precision b by Table 1 .

Step 7 : Count  X  Count  X  1, if Count 4  X  200, jump to Step 8 , else
Step 8 : Stop the training procedure. 4. Intelligent vehicle road recognition
In this section, the performance of the proposed Quantize Back-propagation Step-by-Step (QBPSS) algorithm for training neural net-works with limited precision weights (LPWNN) is evaluated using two experiments on different scale network.
 First, the performance of QBPSS is compared with MTM and
QGDR, respectively, to approximate the following function on small-scale network g  X  x  X  X  sin  X  2 p x  X  X  1 2 , 0 r x r 1  X  11  X 
In this experiment, a 1-10-1 network has been used and the desired weights precision is 0.1, E allowed is 0.01, learning rate  X  0.05, momentum is 0.9 and the largest allowed iteration number is 10,000. The training set is obtained by sampling the function at the points x  X  0, 0.1, y , 0.9, 1.0, and 20 simulations are conducted for each algorithm in this experiment.
 The simulation results of the three algorithms are listed in
Table 2 and the generalizations of the network trained by the three algorithms are shown in Fig. 4 .

The actual outputs from LPWNN trained by QBPSS are more similar to the desired outputs than trained by QGDR and MTM. Furthermore, QBPSS has higher success rate on small-scale network.
Second, we test the performance of QBPSS for LPWNN on
Aufrere et al., 2001 ; Yamada et al., 1996 ) on relatively large-scale network. As a comparison, we still choose MTM and QGDR algo-rithm. Then the performance of the optimized LPWNN trained by
QBPSS is evaluated by comparing to conventional neural networks with double-precision floating-po int weights on road recognition in PC systems and ARM 9 embedded systems, respectively.
 The training input samples are 80 representative images (see
Fig. 5 ) taken by CMOS camera which is installed on the top and the front of intelligent vehicle. Each picture is illustrated by a (8 16)-pixel image (black pixel  X  F; white pixel  X  0). The track is made up of black feature lines over white road surface, which is in the middle of the road. And the desired output picture is also denoted by a (8 16)-pixel image (see Fig. 5 ), which detects the black feather lines and has a clear track.

Generally, in order to train the neural networks more easily, the sample images will be converted to binary images based on threshold. Then we will get eighty 8-by-16 binary matrices and each matrix will be converted to 1-by-128 matrix, so the inputs matrix is a 80-by-300 matrix.

In other words, there are 80 input vectors, each vector is [ x x , y , x 128 ]. Accordingly, the input layer and output layer need 128 neurons. If the hidden layer has i neurons, the networks can be denoted by 128-i -128 (see Fig. 6 ). The parameters in Fig. 6 are: x , x 2 , y , x 128 the elements of each sample input vector; SP .1, SP .2, y , SP .80 the sample image 1, sample image 2, y ,sample DP .1, DP .2, y , DP .80 the desired image 1, desired image 2, desired image 80. In this paper, we do not directly use the binary vectors; in contrast, we use 0.1 instead of 0 and 0.9 instead of 1 in
SP .1
SP .2
SP .9
SP .80 ... ... ... ... order to train the networks easily. Now, the vector [0, 1, 0, can be denoted by [0.1, 0.9, 0.1, y , 0.9, 0.1, 0.9]. At the end, each component of the outputs from output layer will be normalized as follows: out  X  f  X  y  X  X 
A 128-30-128 networks has been used for these simulations and the training has been stopped when the value of the error function E allowed , has been E allowed r 0.1.

The activation function, which has been used in QGDR and MTM, for the hidden layer and the output layer is all logistic function. But in QBPSS the activation function will be quantized as a look-up table. The desired weights precision is 0.1, learning rate is 0.05, momentum is 0.9 and the largest allowed iteration number is 3000.
The parameters in Table 3 are: min the minimum number of iterations, mean the mean number of iterations, max the max-imum number of iterations, succ. simulations succeeded out of 50.
The convergence performance of QGDR, MTM and QBPSS on road recognition is shown in Table 3 . From the table, we can see that
QGDR has a lowest success rate, MTM and QBPSS have the same success rate 100%. But with further analysis, the success rate of
MTM is superficial and unreal. Because the weights are real in the training process and the activation function is logistic function.
When a fully trained networks works, a simple look-up table is used instead of the non-linear activation function, which brings new error and enlarge the error value (see Fig. 11 ). Furthermore, in Table 3 , QBPSS needs the least number of iterations and has the most quick convergence speed.

QBPSS shows its great capability in convergence than QGDR and MTM. This also can be analyzed from Figs. 7 and 8 .Inthe convergence process of QBPSS from precision 0.01 to 0.1 (see Fig. 9 ), the error value becomes large but the process still converge to the desired value. This process is consistent with the procedure of QBPSS algorithm (see Fig. 3 ).

One advantage of QBPSS is that the weights trained by QBPSS will not bring new error when the fully trained neural network works. But, QGDR and MTM will bring new errors. From Fig. 10 , we can see that QGDR has the biggest error when the network works. From Fig. 11 , MTM has bigger actual error than QBPSS, though MTM and QBPSS have both converged to 0.1.

In addition, Fig. 12 presents the generalization capability of the neural networks with limited precision weights (LPWNN) trained by QGDR, MTM and QBPSS. The actual outputs represent the response of the LPWNN, as we can see, the actual outputs from
LPWNN trained by QGDR are far from the desired outputs, the actual outputs from LPWNN trained by MTM are similar to the desired outputs, but the actual outputs from LPWNN trained by QBPSS are the more similar to the desired outputs. Moreover, the actual output images from LPWNN trained by MTM have many noises, but the output images from LPWNN trained by QBPSS has little noises and are more clear than the above images. So LPWNN trained by QBPSS has the greatest generalization capability among the three networks.

Another advantage of the optimized neural networks with limited precision weights (LPWNN) is to reduce the memory for storing the weights and reduce the complexity of networks calculation. Table 4 shows the efficiency between conventional neural networks and the optimized LPWNN and works on two different platforms. The results show that the computing effi-ciency has been greatly improved when optimized LPWNN is working in ARM 9 embedded systems (about 7 times). 5. Conclusion and discussion
The optimized neural networks with limited precision weights (LPWNN) have been studied in this paper, which reduce the memory for storing the weights and increase the speed of networks computing. This kind of networks is better suited for embedded systems implementation. In addition to, a new training algorithm QBPSS has also been presented, which trains the LPWNN with a look-up table instead of non-linear activation function in order to decrease the inaccuracy when the LPWNN works in embedded systems. The results indicate that the optimized networks and the References
