 We describe Streamwatchr, a real-time system for analyzing the music listening behavior of people around the world. Streamwatchr collects music-related tweets, extracts artists and songs, and visu-alizes the results in three ways: (i) currently trending songs and artists, (ii) newly discovered songs, and (iii) popularity statistics per country and world-wide for both songs and artists.
 H.5 [ Information Interfaces and Presentation ]: H.5.5 Sound and Music Computing Algorithms, Human Factors, Measurement Music, stream processing
Social media is changing the way we consume music. Online music services such as iTunes, Spotify, last.fm, and YouTube en-able us to access music from everywhere, anytime, and share our playlists with the world in the form of tweets, or status updates. People tweeting the tracks they are currently listening to generate more than half a million tweets per day. This offers us insights into people X  X  music listening behavior at world scale. Historically, this type of research has been mostly based on surveys, or music charts [1], either of which is limited in scope or use, as it is often privately held. The most important drawback, though, we believe, is the data gathering process itself, which decouples what people listen to (or buy) from the context within which they do this. So-cial media can complement this data, as it is mostly about people X  X  activities [3, 7], with music being an important one [6].
There are two major challenges in mining social media for study-ing music listening behavior, apart from the sheer volume of in-coming data: (i) how to identify music related content, and (ii) how to deal with the semi-structured, unedited nature of user generated content. For the fi rst challenge, Hauger and Schedl [2] used three popular music hashtags on Twitter f or identifying tweets potentially related to music: #iTunes, #nowplaying, and its shorthand, #np. We use the same set of hashtags, plus #spotify. Tweets tagged with #iTunes and #spotify are automatically generated by the respective software music players, while those tagged with #nowplaying are not associated with a particular source and may contain additional
For the second challenge, we follow [2, 6] and use a set of reg-ular expressions to generate a candidate set of artists and songs, knowledge base). The difference with previous work is that we use YouTube search for increasin g the recall of our method and develop tailored webpage extractors for the #iTunes and #spotify tagged tweets; we provide a comparison of these methods in the next section. Another dimension to this challenge is to identify the geolocation of a tweet. Schedl [5] uses the Yahoo! Placemaker API for this purpose, however, we fi nd that the rate limits imposed by an open geo database, for mapping a tweet X  X  coordinates (or extract the twitterer X  X  coordinates from their pro fi le description or location) to a geolocation.

In this demonstrator we present Streamwatchr: a real-time sys-tem for analyzing music listening behavior at world scale. Stream-watchr aims at (i) mapping unstructured, user generated content to structured data in real-time and (ii) providing up-to-date visualiza-tions of what the world is listening to, what songs and artists are trending, and what will be the next big music hit. Streamwatchr can be accessed at http://streamwatchr.com .
Streamwatchr consists of a multi-stage approach for identifying songs and artists from tweets. The process is informed by obser-vations on a training set of manually annotated tweets tagged with music-related hashtags. The assessment exercise revealed that the dif fi culty of correctly extracting the artist and song from a tweet ranges from very low to very high; see Table 1 for examples of tweets from which music information is easy and hard to extract, respectively. Also, many tweets originate from radio stations that post their airplay. Streamwatchr ignores radio users by fi ltering usernames that match  X  X adio, X   X  X m, X  or  X  X lay X  in them. Below, we describe our multi-stage approach.
 Check hashtag. First, we check which hashtag is used to determine the next step. #spotify and #itunes tweets are transferred to the 9 3 http://musicbrainz.org 9 4 http://www.geonames.org Table 1: Examples tweets from which music information is easy and hard to extract.

Easy #nowplaying Richard Marx -Angelia #np funeral for a friend/love lies bleeding -dream theatre #nowplaying Hey Soul Sister-Train Hard (with answers) They say love is blind oh baby you so blind. #np (G-Dragon -That xx)
When you feel my heat Look into my eyes It X  X  where my demons hide It X  X  where my demons hide Don X  X  get too close
It X  X  dark inside #np (Imagine Dragons -Demons) you got mud on your face you big disgrace somebody better put you back into your place #np #queen #wewillrockyou (Queen -We will rock you) Page extraction stage, while #np and #nowplaying are moved to the Baseline extraction stage.
 Page extraction. We follow the URL contained within the tweet and fetch the page. From this page we extract the artist and song, according to site speci fi c regexes.
 Baseline extraction. If a tweet matches certain basic regexes, we use these to extract candidate artist and song. These candidates are then issued to MusicBrainz and if a match is found on the combina-tion of artist and song, we store this result. If not, or if the regexes do not match anything, we continue to the next stage.
 YouTube extraction. Based on the observation that people often refer to songs using lyrics, misspelled names, or other ways of  X  X re-ative X  writing, we need a large, music-related resource with user-generated content, rather than a generic resource such as Wikipedia (that can be used to provide semantics to arbitrary tweets [4]). YouTube fi ts this description, as it is one of the main platforms for music (video) dissemination, in which each video comes with user-generated metadata (e.g., lyrics, comments, tags). We use the full tweet as query to YouTube, fi lter the results by category (Music), and retrieve the top 10 results. We then use regexes to extract can-didate artists and songs from all results and rank these by weighted frequency ( fi rst result is more important than result 10). We move down the ranked list and try to match combinations of candidates (as song and artist) in MusicBrainz. In case a match is found, we store the result, otherwise we move to the fi nal step. Fuzzy extraction. In case we cannot fi nd an exact match in Music-Brainz, we look for a matching artist in our candidate list from the YouTube extraction stage. Having found an artist, we use the other top candidate as song, even though we might not be able to match it in MusicBrainz.

The multi-stage approach ensures an ef fi cient approach by start-ing with fast methods (Page and Baseline) and only moving to more expensive methods if the fast methods fail.
 To develop and test our extraction pipeline we need a set of anno-tated tweets. To this end we manually annotate two sets of tweets, other hashtags as they depend solely on the Page extraction step, which is too basic to fail. From the two sets of tweets one is used as training and development set (consisting of 250 tweets) and the other set is used as test set (200 tweets). For all tweets we iden-tify the song title and artists, including their MusicBrainz identi fi er. Table 2: Test results in Precision, Recall, and F-measure of three extraction methods and their combination (All) for songs (top) and artists (bottom). Recall is measured over the number of tweets offered to the method (e .g., YouTube only processes tweets that Baseline could not resolve).
 Tweets for which either the song, the artist, or both are missing are annotated with  X  X NK X  for the particular fi eld.

To test the various extraction methods we apply our complete pipeline to the tweets in the test set and record the extracted in-formation and the method that is responsible for this information. Table 2 shows the results from the extraction methods. If a tweet is annotated with  X  X NK X  but a method assigned a real band or song, it is listed as an error.

The results show what we expected to fi nd: the Baseline method performs extremely well on precision, missing only one of the 29 extracted song and artist pairs. At the same time, this method is only capable of processing 14.5% of all tweets (29 out of 200). Most previous work on Twitter and music (e.g., [2, 5, 6]) only re-port on using regular expressions on tweets to extract information. Our analysis suggests this leads to a signi fi cant loss in information. For those tweets for which regular expressions fail, we apply our YouTube-based method. We fi nd an increase of recall compared to the baseline, combined with a drop in precision. This method is capable of dealing with almost half of the total number of tweets, and more than half of the tweets offered to this method. The pre-cision of this method remains fairly high, something which does not hold for the Fuzzy method. This method shows a substantial drop in precision, especially for song titles. However, because of the relatively high recall its F-measure is still higher than that of the Baseline method.

Finally, if we combine the methods into our complete pipeline (All), we obtain the highest recall and F-measure scores for both song title and artist name extraction.
 Statistics To show the amount of data processed by Streamwatchr, we report on statistics collected between July 24 X 29, 2013 (5 days). In total we processed 2.2m music-related tweets. For 838k tweets (38%) we could not extract an artist and/or a song. For the remain-der, 19k tweets (0.9%) were resolved by the Page extractor; 374k tweets (17%) by the Baseline extractor, and 964k tweets (44%) by the YouTube extractor (incl. Fuzzy).
The extraction of song titles and artists is only a necessary step in our demonstrator to support four types of analyses. In this section we discuss four functions our demonstrator offers to gain insights in the listening behavior of people: charts , currently popular , dis-covery ,and geo analysis . Charts One of the obvious functions our demonstrator offers are charts. We collect statistics for both songs and individual artists on an hourly basis, which allows us to plot the number of plays at a fairly detailed level. By aggregating the data, we can show plots for any time period at any detail level (e.g., per month or per year). By requesting plots of the data, users can quickly identify trends in popularity or (un)expected peaks in listening behavior. Currently popular While the charts represent a somewhat old fash-ioned view of popularity, the currently popular function of our demonstrator tries to exploit the streaming character of Twitter. As the tweets, and therefore the music-related tweets, fl ow into Streamwatchr as a stream, we present the user with a real-time list of the most popular artists. A trad itional popul arity ranking would monitor the stream for a set period of time, counting plays of each song, and after this period report on the fi nal ranking. Streamwatchr uses a metric that rewards songs that are played often in a short pe-riod of time and punishes songs that were not played for a while.
More formally, Eq. 1 represents our temporal popularity score, tp , for item x (song or artist) at time t : tp t ( x )= Here,  X  is a parameter indicating the  X  X amping X  factor (we set  X  =0 . 693 ),  X  t,t  X  1 represents the difference in seconds between this occurrence and the previous occurrence of x ,and  X  is the time unit, which we set to 60. From Eq. 1 we can see that if an item occurs for the fi rst time, it is assigned score 1 and as soon as it is played again, we dampen the current score. Items that occur fre-quently in a short period of time are dampened less and receive a higher score. The method requires all items to be updated when a new occurrence enters the system. Given the amounts of data we are processing this is not feasible. We therefore use Eq. 1 to update the items that actually occur and, when a user requests the page, we recalculate the tp t for the top ranked items (by tp t  X  1
The top of the resulting list of s ongs or artists represents what is currently popular on Twitter and this can be used as input to, for example, apps that allow users to play the currently trending music or to systems that inform radio stations and clubs about the popular music of this moment.
 Music discovery One of the hardest things for music lovers is to keep track of new music. Since we are continuously monitoring music listening behavior, we can quickly detect new music that is on the rise. By presenting newly discovered songs in Streamwatchr we offer a service to people looking for easy access to new music.
We employ a heuristic method to discover new music. First, a song needs to have at least 50 plays in one hour to be added to our list of candidate discoveries. We remove songs that have previously been discovered and fi nally check to see if the song was already played more than a week ago. The intuitions behind these decisions are the following: (i) For a song to be discovered it needs to have a certain level of attractiveness, represented by a substantial number of plays within an hour. (ii) Songs that were already played more than a week ago are not considered to be  X  X ew X  and can therefore no longer be discovered. A typical pattern we fi nd for songs that should be discovered by Streamwatchr shows a couple of spread out plays in the 2 X 3 days before discovery, followed by a sudden move upwards in the number of plays. We aim at presenting new songs to users of Streamwatchr right at the beginning of this move upwards.
 Geo analysis The fi nal analysis that is facilitated by the demon-strator are location-based charts. When possible we extract the geo information from tweets using either the coordinates in the tweet or the user-provided location string in a user X  X  pro fi le. Although the percentage of geotagged tweets is very low (  X  1%), we believe that showing local charts can prove insightful for many users.
Using a map of the world we allow users to select the country for which they want to explore the charts. As mentioned before, we store plays for artists and songs on an hourly basis, which allows for a detailed analysis. For future extensions we plan to implement comparison possi bilities between countries, e.g., show popularity for an artist or song for two or more countries at the same time in a plot, or use different colors on the world map to indicate the date when a song became popular in particular countries. This allows one to analyze which countries play an important role in de fi ning the world music scene.
We have described Streamwatchr, which aims to give new in-sights into people X  X  music listening behavior as reported on Twit-ter. Streamwatchr extracts song title and artist information from music-related tweets and presents it in a variety of ways: (i) tra-ditional charts and (ii) location-based counterparts, (iii) real-time popularity rankings, and (iv) discovery of new music.

Although Streamwatchr in its current form allows for various ways of analyzing and accessing music data, we envision three im-portant and useful future extensions. First, to improve the level of insights we can gain from the data, we want to offer comparisons between either artists or between countries.
 A second extension is automatic peak detection and explanation. From observing plots we can see that certain artists have peaks in which many people listen to their songs. This raises an interesting question: why are all these people listening to this artist?
Finally, we want to look into contextualizing music. We are us-ing Twitter and people do not only post #np tweets, but also other, more informative, messages. To what extent can we use these other tweets to contextualize reports on listening to music? [1] D. J. Hargreaves, D. Miell, and R. A. Macdonald. What Are [2] D. Hauger and M. Schedl. Exploring Geospatial Music [3] A. Java, X. Song, T. Finin, and B. Tseng. Why We Twitter: [4] E. Meij, W. Weerkamp, and M. de Rijke. Adding semantics to [5] M. Schedl. Leveraging Microblogs for Spatiotemporal Music [6] M. Schedl and D. Hauger. Mining Microblogs to Infer Music [7] W. Weerkamp and M. de Rijke. Activity Prediction: A
