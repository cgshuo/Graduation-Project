 Due to the growing availability of digital textual documents, automatic text clas-sification (ATC) has been actively studied to organize a vast amount of unstruc-tured documents into a set of categories, based on the textual contents of the document. Most automatic classification systems analyze documents statistically and linguistically, determine important terms from the documents, and gener-ate vector representations from these important terms. A good text-to-vector representation is necessary in order to enhance ATC and accomplish effective document retrieval [ 1 ], [ 6 ], [ 14 ], [ 15 ].
 many studies have been carried out with showing success in adopting unsuper-texts. Word embedding features have actively studied on word analogies, word similarity, chunking, and named entity recognition (NER). Word embeddings are also used in ATC [ 22 ], but there remains the task of investigating how word embedding features can be infused into existing statistical features. In ATC, words or terms in a certain document are Zipf distributed, that is, most of the words in some documents appear a few times or completely absent in other documents or in some categories. These infrequent words usually can-not be fully trained by term frequency-based approaches. Weighting approaches like the TF.IDF thus give positive discrimination to infrequent terms by biasing them against frequent terms. Furthermore, the training set may not have enough discriminative features to obtain a good vector space model (VSM). Some doc-uments in the training or test set may not share enough information to classify the test set properly. Therefore, word embedding can be useful as input to clas-sification models or as additional features to enhance existing systems. In this paper embedding vectors are generated using the global vectors (GloVe) model [ 5 ].
 The motivation for exploiting word embedding features for ATC can be attributed to two main properties. First, in generating a more information-rich VSM, it is interesting to understand how continuous embedding features may assist to enhance ATC. Second, there is a demand for document representation to integrate semantic VSM into statistical VSM.
 In this paper, we propose a new Centroid-Means-Embedding (CME)-based Semantically-Augmented Statistical-VSM (SAS-VSM) approach that exploits infusing embedding features, where the degree of semantic similarity is esti-mated using word co-occurrence information from unlabeled texts into features for ATC. This study makes the following major contributions with introducing the CME based SAS-VSM approach to address ATC.  X 
The word embedding vectors help to enrich categorical performances, and the augmented approaches outperformed all baseline approaches.  X 
The CME approach enriches the existing statistical VSMs using semantic knowledge.  X  The CME enriches every category performance on the 20 Newsgroups and
RCV1-v2 datasets over the statistical VSM-based system.  X 
The proposed CME-based SAS-VSM is a prominent approach in ATC. In the ATC, the construction of VSM has always been considered as the most important step. Most ATC systems analyze documents statistically, determine important terms from document space D = { d 1 ,d 2 , ...d vector representation from these important terms in order to reduce the complex-ity of the documents and make them easier to handle. To generate text-to-vector representation, two properties are main concern to determine the important terms from documents: the widely used statistical-VSM and recently-focused semantic-based VSM.
 Of greater interest with both of these VSMs to enhance ATC, we introduce SAS-VSM that merges together statistical VSM and word-co-occurrence-based continuous embedding vectors. The architecture of SAS-VSM for a document space can be represented as: where || denotes the concatenation of two different VSMs. An SAS-VSM feature vector x ( d ) for a document d is: where x Stat ( d ) is a statistical feature vector and x Sem vector.
 approaches for a given corpus. In contrast, to represent the Semantic-VSM, we consider a context prediction GloVe model for learning word embedding. Word embedding is useful to inject additional semantic features to the existing VSM. It is an open question how continuous word embedding features should be infused into discrete weights of term vectors. We considered two approaches to repre-senting the SAS-VSM: (1) the primary approach, which shows the motivation to incorporate two different VSM and (2) the CME approach. 2.1 Primary Approach In the above formulation of SAS-VSM, where the Statistical-VSM denotes term weightings based on discrete weights of terms for a corresponding document d . A Statistical-VSM vector is an x Stat ( d )= x Stat 1 ( d ) , ... , x i ( d ) is defined as: where f ( t i ) is a term weighting function representing any weighting approach for term t i which will be later discussed on Section 3. In contrast, a Semantic-SVM embedding matrix V , x Sem ( d ) is defined as: { t 1 ,t 2 , ...t n matrix V is an M  X  N matrix with the vocabulary size M and the dimensionality N of word embedding vector. That is, V is defined as follows: Each row v i represents the embedding vector for term t i primary approach which leads to generate centroid-means-embedding vector. From the Eqn. 2, we can see that new updated augmented features for doc-ument d are incorporated with discrete and continuous weights. However the existing supervised discrete weights based on different term weighting schemes are remained free from getting continuous weight. 2.2 CME with SAS-VSM In this proposed approach, we will introduce how to infuse continuous word embedding vectors into existing discrete weights by rewriting Eqn. 1 and 2. We first compute a sum centroid embedding (SCE) for a candidate document d .The SCE is the sum of all continuous embedding weights for the column vectors of that correspond to word embedding of terms in a certain document d . Therefore, the SCE weight of a certain term t i  X   X  for a given document d can be represented as: where A is an M -dimensional row vector. In the next computational step, we compute the mean of the SCE which we call centroid-means-embedding (CME) for a certain term t i in document d .
 We then rewrite Eqn.1 as: From the Eqn. 2, the new generated weight gets a larger weight than exist-ing vector which may turn training ovefit. We therefore scale the embedding vectors by setting a hyper parameter. The goal of using hyper parameter is to scale large weights that overfit the training data. We introduce Gaussian or nor-mal distribution based scaling function for a certain document d to scale each new generated weight for SAS-VSM. The hyper parameter  X  =(  X  document d can be denoted as: where the mean  X  d and standard deviation  X  d for a document d are calculated from candidate documents as: where  X  denotes the element-wise multiplication of two row vectors. Recently, several studies have been conducted using different term weighting weighting approach for ATC. The common TF.IDF [ 12 ], [ 14 ], is defined as: where D denotes the total number of documents in the training corpus, tf ( t is the number of occurrences of term t i in document d ,#( t documents in the training corpus in which term t i occurs at least once, #( t is referred to as the documents frequency (DF), and D/ #( t document frequency (IDF) of term t i .
 two different weighting approaches, TF.IDF.ICF and TF.IDF.ICS global document-indexing-based IDF and class-indexing-based inverse class fre-quency (ICF) and inverse class space density frequency (ICS with local weights term frequency (TF). We can also define two class-indexing-based weighting approaches TF.ICF and TF.ICS  X  F. These two representations of class-indexing-based category mapping are represented as: where C denotes the total number of predefined categories in the training corpus, c ( t i ) is the number of categories in the training corpus in which term t at least once, c ( t i ) c is referred to as the class frequency (CF), and and C TF.IDF.ICF and TF.IDF.ICS  X  F for a certain term t i in document d with respect to category c k , are defined in as: W W In this section, we provide empirical evidence for the effectiveness of the proposed approaches. In this evaluation, we employ two commonly-used ATC datasets: 20 Newsgroups 1 and RCV1-v2/LYRL2004 [ 16 ]. We employ a 10-fold cross validation scheme for the 20 Newsgroups dataset in which the dataset is randomly divided into 10 subsets. For each fold, one subset is used for testing and the remaining subsets are used for training. For the RCV1-v2/LYRL2004 dataset, we split the corpus into training and test data, which is discussed in Ren and Sohrab [ 1 ]. We have kept the same splits and experiment setup that are used in Ren and Sohrab [1]. The standard evaluation metrics like precision, recall, F micro-average of precision, recall, and F 1 -measure are used to judge the system performances. Please refer to Ren and Sohrab [ 1 ] for more details. 4.1 Experimental Datasets To evaluate the performance of the proposed model with existing different base-line weighting approaches, we conducted our experiments using the 20 News-groups and RCV1-v2/LYRL2004, which are widely used benchmark collections in the ATC task. 20 Newsgroups Dataset. The first dataset that we used in this experiment is the 20 Newsgroups, which is a popular dataset to use against machine learn-ing techniques such as ATC and text clustering. It contains approximately 18,828 news articles across 20 different newsgroups. For convenience, we call the 20 categories: Atheism (Ath), CompGraphics (CGra), CompOsMsWindows-Misc (CMWM), CompSysIbmPcHardware (CSIPH), CompSysMacHardware (CSMH), CompWindowsx (CWin), MiscForsale (MFor), RecAutos (RAuto), RecMotorcycles (RMot), RecSportBaseBall (RSB), RecSportHockey (RSH), Sci-Crypt (SCry), SciElectronics (SEle), SciMed (SMed), SciSpace (SSpa), SocReli-gionChristian (SRChr), TalkPoliticsGuns (TPG), TalkPoliticsMideast (TPMid), TalkPoliticsMisc (TPMisc), and TalkReligionMisc (TRMi). RCV1 Dataset. The RCV1 dataset, RCV1-v2/LYRL2004 is adopted, which contains a total of 804,414 documents with 103 categories from four parent documents which are labeled with at least once. We found that only approximate 23,000 documents out of 804,414 are labeled with at least once. To create larger dataset for the single-label classification problem, we extracted all the documents which are labeled with two categories, a parent and a child category. Then we removed the parent category from the document label and child category is assigned in order to produce the single-label classification problem. From RCV1-v2/LYRL2004, a single category is assigned to a total of 219,667 documents and there are 54 different categories in total. We have kept the same split, the first 23,149 documents as for training and the remainder 196,518 documents are for testing according to RCV1-v2/LYRL2004. 4.2 Word Embedding Training with GloVe Model In this paper, the word embedding matrix V M  X  N is generated using GloVe model. We consider the GloVe model for learning word representation from unla-beled data to generate word embedding vectors, since it is outperformed other methods on word similarity and NER tasks. The GloVe model is an weighted least squares regression model that performs global matrix factorization with a local context window models. In this work, the word embedding vectors are generated from the available source code 2 . All parameters were left at default values in this toolbox. 4.3 Support Vector Machine Classifier In the machine learning workbench, support vector machine (SVM) has been achieved great success in ATC and considered as one of the most robust and accurate methods among all well-known algorithms [ 15 ]. Therefore, as a learn-ing classifier, SVM-based classification toolbox SVM-multiclass experiment. All parameters were left at default values. The regularization param-eter c was set to 1.0. 4.4 Results with the 20 Newsgroups and RCV1 dataset In this paper, we compare our primary and CME approaches for SAS-VSM with baseline weighting schemes including TF, TF.IDF, TF.ICF, TF.ICS TF.IDF.ICF, and TF.IDF.ICS  X  F approaches. In Tables 1, 2, 3, and 4, EV=4, EV=10, EV=20, and EV=40 indicate the vector sizes of word embedding that are injected in SAS-VSM with respect to different weighting approaches. Results with the Primary Approach. Tables 1 and 2 show the performance comparison with micro-F 1 on six different term weighting approaches over the 20 Newsgroups and RCV1 datasets using the SVM classifier. In Table 1, it is noticeable that the primary approach shows a marginal improvement over the baseline weighting approaches. In some cases when feeding with a bit larger aug-mented vectors including EV=10, EV=20, and EV=40, the performance shows a minimal drop from the baseline TF and TF.ICS  X  F approaches. In Table 2, it is also noticeable that the results on RCV1 show marginal improvements over all baseline weighting approaches except for a minimal drop on TF.IDF.ICF. Results with the CME Approach. Tables 3 and 4 show the performance comparison with micro-F 1 on six different term weighting approaches over the 20 Newsgroups and RCV1 datasets using the SVM classifier. Table 3 shows that by applying CME to SAS-VSM over the different weighting approaches, CME enriches system performance not only from the baseline approaches but also from our proposed primary approach using SAS-VSM. In Table 4, it is also noticeable that the CME-based SAS-VSM approach outperforms all the baselines and primary proposed approaches.
 Categorical Performance Comparison. In ATC, besides overall perfor-mance, it is also important to judge the categorical performance for a certain dataset. Because of space limitation we only provide the TF.IDF categorical per-formance for 20 Newsgroups dataset. Fig. 1 shows the categorical performance basedonF 1 -measure, where our primary approach is performing lower than the baseline on some categories. In contrast, the CME-based SAS-VSM approach shows its superiority over the baseline classifiers on 19 out of 20 categories. 4.5 Discussions The results of the above experiments show that the CME-based SAS-VSM consistently outperforms over the baseline approaches, including TF, TF.IDF, TF.ICF, TF.ICS  X  F, TF.IDF.ICF, and TF.IDF.ICS  X  F, which are used to create statistical VSMs. From the results with our primary approach, it is important to note that the combination of statistical and semantic VSM can marginally improve the system performance. In the primary approach, it is noticeable that continuous-valued embedding matrix is updated with both statistical and semantic knowledge but the discrete weights are remaining unchanged in the primary approach. In contrast, we introduce CME approach in the SAS-VSM to update the discrete weight and provide semantic knowledge into statistical VSMs. These results indicate that our CME-based SAS-VSM approach can sig-nificantly improve the system performance.
 Our experiments also show that the CME-based SAS-VSM is a novel VSM that produces a consistently higher performance over different term weighting approaches. Thus, the word embedding vectors are useful to enhance ATC. Ren and Sohrab [ 1 ] performed their experiments with eight different weighing approaches: local weight TF incorporated with global weights including coeffi-cient correlation (TF.CC), mutual information (TF.MI), odds ratio (TF.OR), probability based (TF.PB), relevance frequency (TF.RF), IDF (TF.IDF), IDF.ICF (TF.IDF.ICF), and IDF.ICS  X  F (TF.IDF.ICS  X  F). The results showed that the class-indexing-based TF.IDF.ICS  X  F is useful with an SVM classifier. The TF.IDF-.ICS  X  F approach showed its superiority in all the categories of the 20 Newsgroups and a majority of the Reuters-21578 datasets using SVM. This work emphasizes on statistical supervised approach with semantic information for a certain document, which is neither discussed nor empirically evaluated. Jeffrey et al. [ 5 ] introduced global vectors for word representation where the work proposed specific weighted least square model that trains global word-word co-occurrence counts and produce a word vector space. The results demonstrate that the GloVe model outperforms existing models over word analogy, word similarity, and NER tasks. This work left a key note that word embedding vectors canbeusedasfeaturesinATC.
 Jiang et al. [ 3 ] introduced a distributional prototype approach for utilizing the embedding features applied on NER. The basic idea of the distributional prototype features is that similar words are supposed to be tagged with the same label. The experiment result shows that continuous embedding features improve the system performance for NER.
 tics of categories using WordNet and replaced the IDF function with a semantic weight (SW). The TF.SW approach that outperformed TF.IDF in overall sys-tem performance but it was unable to outperform TF.IDF on the categorical performance. In this study, we investigated the effectiveness of exploiting word embedding in the ATC and proposed a novel CME-based SAS-VSM for ATC.
 experiment results, it is noticeable that the proposed CME-based SAS-VSM can significantly improve the performances of different weighting approaches, includ-ing TF, TF.IDF, TF.ICF, TF.ICS  X  F, TF.IDF.ICF, and TF.IDF.ICS fore, this approach can apply to any existing weighting approaches to improve the existing system. Second, a properly feeding method for augmented features can be useful as input to a VSM, especially SAS-VSM to enhance ATC. Third, the results of this study indicate that the proposed CME-based SAS-VSM can signif-icantly improve the categorical performance for different weighting approaches in two different datasets. The proposed approach is very effective to enhance ATC. Forth, SVM is considered one of the most robust and accurate classifica-tion methods in machine learning workbench, and here our results show that the CME-based SAS-VSM is effective with SVM method in two different datasets to address classification task.
 scale multi-label hierarchical text classification for Wikipedia medium and large datasets 4 . It might be interesting to investigate the behavior of SAS-VSM for the large scale datasets which have thousands of categories and one or more categories are assigned for a certain document in order to address multi-label hierarchical classification.
 Acknowledgement. This work has been partially supported by JSPS KAK-ENHI Grant Number 25330271.

