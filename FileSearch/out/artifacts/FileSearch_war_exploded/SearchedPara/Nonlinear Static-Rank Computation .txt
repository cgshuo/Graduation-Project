 Mainstream link-based static-rank algorithms (e.g. PageRank and its variants) express the importance of a page as the linear combination of its in-links and compute page importance scores by solving a linear system in an iterative way. Such linear algorithms, however, may give apparently unreasonable static-rank results for some link structures. In this paper, we examine the static-rank computation problem from the viewpoint of evidence combination and build a probabilistic model for it. Based on the model, we argue that a nonlinear formula should be adopted, due to the correlation or dependence between links. W e focus on examining some simple formulas which only consider the correlation between links in the same domain. Experiments conducted on 100 million web pages (with multiple static-rank quality evaluation metrics) show that higher quality static-rank could be yielded by the new nonlinear algorithms. The convergence of the new algorithms is also proved in this paper by nonlinear functional analysis.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Nonlinear static rank, Link aggregation, Probabilistic model Link-based static ranking, the activity of calculating importance scores for web documents from a link graph, has shown to be very important for web search. Among all existing static ranking algorithms, PageRank [15] could be the most popular one. Given a link graph, the PageRank of any page v is calculated as follows, B and 1. An alternative formula is to have the (1-c ) term above divided by N , the total number of pages in the link graph. The PageRank of all pages can be computed via an iterative process (with possible normalization after every iteration to have the sum of PageRank values to be 1.0 or N ). According to the above formula, it is clear that the PageRank of a page is a linear combination of the PageRank values of all its in-links. The intuition behind the PageRank algorithm is that a page can have a high PageRank if it has many back links or some of its back links have high PageRank values. PageRank is based on a random surfer model. The PageRank algorithm may give apparently unreasonable static-rank results for some link structures, as illustrated in Figure 1. In the figure, page D 1 has 50,000 in-links, with all of them coming from the same site domain2.com; while page D 2 has only 1,000 in-links, coming from 100 sites. Assume all the in-links of D D have the same static-rank value and the same number of out-links. According to the standard static-rank methodology (Formula 1.1), the PageRank of D 1 would be much larger than that of D 2 . This is contrary to our intuition. Intuitively, if a page is linked to by a lot of domains, it should have, with very high probability, larger static-rank than another page which is linked to by numerous pages from few domains 1 . Web pages in one domain are typically maintained by one editor or a small group of editors. It is easy for the editors to build (intentionally or unintentionally) many pages in the domain linking to one destination page. While for domains controlled by different owners, it is much harder or less likely for them to add links pointing to the same page. It is therefore  X  safer  X  to assign high static-rank to pages linked to by many domains than pages linked to by only few domains. 
Figure 1. The in-link information of two pages ( D 1 and D Please keep in mind that, with such an argument, we assume all the back-links in the figure have the same static-rank value and the same number of out-links. The above problem cannot be solved by differentiating inter -domain links from intra -domain links (one straight -forward optimization upon the classical PageRank algorithm), because all links in the figure are inter -domain ones. Other PageRank enhancement techniques (e.g. biased surfer or dynamic damping factor) are also not able to entirely solve the problem (referring to Section 2 for details).
 In this paper, we examine the static -rank computation problem from the viewpoint of evidence combination and build a probabilistic model to address the problem . Given a web page , we treat each of its back link s as a piece of evidence indicating the importance of the page. The static -rank of a page is defined by the probability of the page being important, which is determined by the pieces of evidence corresponding to its back links . The problem then becomes how to combine the pieces of linking evidence to get the static -rank score of a page . We demonstrate by probability theory that, when all links pointing to a destination page are independent, the static -rank of the page can be computed via the linear combination of its back links (just like in Formula 1.1 ). However, also according to our model, if the links are correlated, the linear combination function does not hold anymore . The higher the correlation , the smaller the static -rank score is relative to the linear combination of its in -links . In the extreme case that other links are fully determined by one link, the static -rank can be computed as other links do not exist . Observing that many links are highly correlated in reality, we argue that nonlinear formulas should be utilized in static -rank computation , in order to con sider the correlation s among links . The correlation s between links are quite complex in reality, resulting in complex rank combination formulas. In this paper, we examine a simple formula which only covers, in a simple way, the correlation b etween links coming from one domain . With the nonlinear formulas being adopted in static -rank computation, it is natural to ask the following questions, The rest of the paper is organized as follows: In Section 2, we review related work. The probabilistic model for rank aggregation is proposed and discussed in Section 3. In Section 4, we describe the algorithms which consider the dependence between links from the same domain. Experiments are conducted on a real dataset in Section 5 to verify the effectiveness of our new algorithms . PageRank [15] and HITS [12], both proposed in 1998 , may be two of the earliest static ranking algorithms for web pages . In HITS , each page has a  X  hub  X  score and an  X  authority  X  score. The hub score of a page is determined by the authority values of its forward links; while the authority score of a page depends on the hub values of its back links.
 A lot of research work has been done to study and improve the PageRan k algorithm . A comprehensive mathematic al analysis of choosing the damping factor is given by [5]. Topic -sensitive PageRank [7] is developed to provide a set of PageRank vectors biased on a set of topics. For a given topic, by pre -computing a non -uniform jumping vector which will contribute to the nodes in each iteration, a particular ranking vector could be got. If the query fall s into one of the predefined categories, the corresponding ranking vector will be used. Query -dependent PageRank [1] adjusts the contribution given by one out -link by considering the relevance between a query and a page . An intelligent surfer model is prop osed in [16] where s everal features (anchor text, relative position , HTML tags, etc) are used to assign different weight s to different out -links of a page. To combat with web page spam, a method is introduced in [20] to discover link farm pages by identify ing a spam seed set first and expand ing it iteratively. TrustRank [6] also starts from a white list and propagates trust through links to other pages. Site structure is studied in [3, 2 1] to improve static -rank computation. All of the above existing efforts adopt the same linear method to combine contributions to a page, which is fundamentally different from our work . Base d on probabilistic theory, t his paper studies static -rank computation from the viewpoint of link aggregation, and proposes nonlinea r formulas to address the problem . The performance of static -rank calculation is important for large link graphs. Several methods to speed up the process are proposed in [9, 10]. Although the nonlinear algorithms we proposed are not designed to improve sta tic -rank calculation speed, we observe from experiment al results that the number of iterations is reduced with our nonlinear static -rank algorithms. Some theory work [14, 4] about static -rank computation has also been done. A survey on PageRank computation is given in [2].
 Tsaparas [ 19] studies using the MAX function (a nonlinear operator) to compute the hub weights of web pages in the HITS [12] algorithm.
 In addition to static -rank computation, nonlinear algorithms have been studied and used in other appli cations. Shi et al. [18] propose to combine document dynamic ranking scores using a nonlinear formula (although seems linear), by considering the dependency among the scores. In the web mining area, a nonlinear function is employed in (the Formula 3.5 of) [23] to compute the semantic similarity between terms or phrases. Here we treat the static -rank computation problem from the viewpoint of probabilistic evidence combination. Our goal is to estimate the static -rank of web pages using probability theory. We function of the probability) of the page being important, given the information of all its back -links. We define the following event s related to page v and its back -links, The following probabilities are related to the above events, Our problem is then to estimate P ( A | E 1 , E 2 ... , E and P ( A ). Formally, we would like to find a function f to satisfy, Now we start from the simple case that all back -links of a page are independent.
 For simplicity of statement, we first consider the case of m =2, that is, two pages u 1 and u 2 link to page v . The problem is to estimate P ( A | E 1 , E 2 ) with the following independence assumptions, The first assumption means that E 1 and E 2 are independent; and the second one demonstrates that E 1 and E 2 are conditionally independent given A . Given the above independenc e assumption s, by using Baye sian Formula, we have, We define represents the log -probability -gain of A given E , the meaning of which is the increment in the log -probability value of A after considering evidence E . Please note that LP ( A ) is the prior log -probability of A before we know evidence E , and LP ( A | E ) is the posterior log -probability of A after evidence E is considered. measure of the impact of evidence E to event A . According to Formula 3.3, we have, The above formula demonstrates that when the independence assumption s (in Formula 3.1 and 3.2) are satisfied, the log -the log -probability -gains of A given one single piece of evidence respectively . For m &gt;2, if the pieces of evidence are mutually independent and conditional independent giv en A , it is easy to prove (by following a similar procedure ) that , The static -rank of a page could be t he (linear ) combination of its base -rank (the rank value when it does not have any back -links) and the rank contribution from all of its back -links. Thus the rank of page v can be expressed as , where c is a parameter with its value in [0, 1]. For simplicity, w e also have assume d that the base -rank of all pages is 1.0 in the above formula. By combining Formula 3.5 and 3.6, we get , If we estimate G ( A | E i ) with R ( u i )/ d ( u i the same as Formula 1.1 . Therefore, our model shows that when all back -links of a page satisfy the independen ce assumptions , the importance score of the page can be computed via the linear combination of (the scores of ) the back -links. The independence of links is assumed in the previous section t o simplify our analysis and to get an elegant formula for rank aggregation . The independence assumptions, however, do not hold in reality. For example, links in multiple pages generated by the same template are not independent . If we have already learned that one page u 1 links to a destination page v , then it is more likely that another page u 2 generated by the same template also links to v than if we do not know the linking information from u Another example, the links posted by the same person on different blog sites are not independent. Spam links intentionally created by the same spammer pointing to one specific page are also not independent one another. It is hence necessary to consider the correlation between links in our model.
 Two correlated events or random variables E 1 and E positively or negatively dependent. In the case of positive dependence, knowing that E 1 has occurred increases the cha nce of E , formally, dependen t. By analyzing the above examples, we learned that the dependence between links to the same destination page tends to be positive. So in the following discussion, positive dependence is assumed , where Formula 3.1 2 will be replaced by Formula 3.8. Still by Bayesian theory, Formula 3. 3 becomes, Please refer to Appendix -D for the derivation of the above Formula. As a result, Formula 3.4 becomes accordingly , Please note that an equivalent way of stating that E independent is:  X   X  2 |  X  1 =  X  (  X  2 ) . Th e above formula indicates that when events E 1 and E positively correlated, page v  X  s static -rank gain given both the events is smaller than the sum of static -rank gains caused by E only and E 2 -only respectively. In the extreme case that E depends on E 1 (i.e. P ( E 2 | E 1 )=1), it is easy to prove that Th e above formula means the static -rank gain of v given E is the same as the gain given E 1 only . It is reasonable, since event E brings no extra information about the importance of v in addition to E 1 . In order to compute the static -rank of a page v when the back -nonlinear function h , such that The specific expression of function h directly depends on how these links are correlated. Strictly speaking, different pages may have different function s for aggregating their back -links , if the link correlations are different between them . In spite of this, some constraints or conditions are available which every reasonable link aggregation fu nction h should satisfy . Some of them are listed as follows (where  X  &gt; 1 ), When a page has only one back -link, we should have C1 should be satisfied for a reasonable aggregation function . For any back -link E i , we have already known the log -probability -gain considered . After other back -links ( E 1 ... , E i -1 discovered, the probability of page v being important should get larger than if we consider E i only (or at least keep unchanged) , because more links are added to demonstrate the importance of the page . Thus  X   X   X  1 , ... ,  X   X   X  X  X   X   X   X  should hold for eve ry i . So condition C2 should be satisfied. For the case of m =2, condition C 3 can be derived directly from Formula 3.10 . Moreover , it is not h ard to extend Formu la 3.10 to more than two events by following a similar procedure . Thus C3 should be satisfied.

Table 1 . List of functions each of which does NOT satisfy all Please note that the three constraints above are necessary but may not be sufficient for becoming a reasonable link aggregation function. Symmetry should not be a constraint here. In fact, the aggregation function in Formula 4.6 (Section 4) is not sym metric. the above conditions (C1~C 3). It is however not the case. For example, NONE of the functions listed in Table 1 is a reasonable link aggregation function, because, for each functio n, there is at least one constraint which the function does NOT satisfy.
 On the other hand, valid link aggregation functions do exist. In the next section, w e will study some of them and illustrate how to build new static -rank algorithms by adopting the functions. In this section , we first (in Subsection 4.1) analyze some formulas which satisfy all the three constraints and therefore can be treated as valid link aggregation functions. And then in Subsection 4.2 , we build a nonlinear s tatic -rank algorithm, with the simple assumption that links from the same domain are correlated and links from different domains are mutually independent. The following functions can be proved 3 to satisfy all the three constraints (C1~ C3) in the previous section . Therefore we treat them as valid link aggregation functions in this paper. Here h 3 can be treated as a special case of h 2 (with  X  = +  X  ). For a better understanding of the properties of the above three nonlinear functions, we plot them in Figure 2 (by varying m ), with increases, the nonlinear function s increase much slower than function mx (corresponding to the case of mutual independence). Figure 2. Comparing  X   X   X   X  =1 with the nonlinear functions in 
Formulas 4.1 to 4.3 ( x 1 = x 2 = ... = x m =1.0, m =1, 2, 4 ... , 8192) The core of a static -rank algorithm is the ranking formula where the static -rank of a page is expressed as a function of its back -links . In this section, we construct nonlinear ranking formulas by making specific assumptions about the correlatio n among links We prove in Appendix -C that the function h 1 (in Formula 4.1) satisfies the constraints C1~C3. The proof for h 2 straight -forward. and then adopt ing appropriate nonlinear link -aggregation functions. Our primary assumption here is: Links coming from one domain are correlated while links from different domains are mutually independent.
 Generally speaking, the forward links of pages in the same domain typically have higher correlation than the links in a group of random pages, because pages in the same domain are more likely to be maintained by the same group of editors or to be discussin g the same topic. As a result, if it is already known that page u 1 links to page v , it is more likely that another page u same domain also links to v than if we do not know the linking information from u 1 to v . Here we adopt the link aggregation function h 1 in Formula 4.1 to represent the positive correlation between links coming from one domain. That is, from one single domain. Meanwhile, we ignore the correlation between links from different domains, that is, from mutually different domains.
 According to the above assumptions, given a web page v and its back links, the link aggregation function is as follows, where n is the number of domains linking to page v , m number of pages linking to v in domain i , u i,j domain i linking to v , and E i,j is the event that u Formula 4. 6 into Formula 3.6, we get our nonlinear static -rank formula for page v , According to the above formula, the static -rank of a page is the nonlinear combination of its back -links. A new static -rank algorithm can be acquired i f the above formula is exploited to replace the linear ranking formula (Formula 1.1) in the standard PageRank algorithm . W e call the new algorithm NL -LOG (which means that the algorithm adopts a nonlinear ranking formula containing logarithm operations). Given the new nonlinear algorithm , we hope the static -rank of all pages can still be computed via an iterative process, just as in the standard PageRank algorithm. To do so, we should first demonstrate that our new nonlinear algorithm converg es.
 The convergence of the standard PageRank algorithm is determined by the convergence of iteratively solving the following linear system, where x is a vector representing the static -rank of all pages , and A is a matrix . It can be proved by linear algebra or the theory of Markov chain that the above linear system can be solved in an iterative way (with convergence guarantee) . The convergence of our new algorithm, however, cannot be proved this way, b ecause we ar e solving a nonlinear system . In Appendix A, we give a proof of the convergence of our new algorithm, which is based on nonlinear functional analysis , an area of mathematics for solving nonlinear problems . Our new algorithm is not only able to converge theoretically , but also converg es fast empirically . E xperiment s conducted on our dataset (referring to Section 5) show that it takes much fewer iterations for our algorithm to converge than the standard PageRank algorithm . Till now we suppose the function h 1 (in Formula 4.1 ) is adopted to aggregate the links from the same domain . Now we consider the case that the function h 2 in Formula 4.2 (assuming p =2 here) is adopted to take the role of h 1 . By following the similar procedure We call a static -rank algorithm NL -SQRT -1 if it adopts the above ranking formula.
 the followin g nonlinear ranking formula , Ranking algorithms corresponding to the above formula are called NL -SQRT -2. Similarly, for nonlinear function h 3 (Formula 4.3), we have Please refer to Appendix B for the proof of the convergence of Formulas 4.8 to 4.10. Formulas 4.7 to 4.10 are our resultant nonlinear static -rank formulas acquired by exploiting the correlation among links coming from the same domain. In this section, we conduct experiments on a large -scale realistic dataset to test the performance of our nonlinear algorithms. Dataset The link graph for th e experiments consists of around 100 million Chinese web pages (and 1.3 billion links ) which was crawled from the web from September 2006 to March 200 7. To crawl pages, several hundred popular home pages were chosen as seeds for our crawler , which then conducted a breadth -first traversal over the Web. The crawled pages are distributed in about 535000 domains (or web sites). We choose such a dataset for experiments because query logs and user labeling information are available for this dataset for facilitating the evaluation of static -rank quality (referring to Section 5.2).
 System Environments and Data Processing The pages are distributed (via URL hashing) and processed in a small cluster consisting of 10 workstations. By parsing the pages in a distributed way, a link graph is constructed and stored in the cluster. Slightly different from ordinary web graphs, domain information of all vertices is recorded in order to implement our nonlinear algorithms. A parallel version of each static -rank algo rithm is built and run on the cluster. In each iteration, every workstation calculates static -rank for its local pages and propagates static -rank contribution information to other rankers. Algorithms and Parameters The following four static -rank algorithms will be studied and compared in experiments, of every page being calculated via Formula 1.1) . In all the algorithms, parameter c is fixed to be 0.85 , a value suggested in [ 15]. Three criteria are adopted to evaluate and compare different static -rank algorithms: click -through correlation, search result s quality , and the speed of convergence . If we could know the  X  ideal  X  static -rank values of all pages, we would be able to evaluate the static -rank quality of an algorithm . Since it is impossible in reality to know the  X  ideal  X  page quality values, we adopt u ser access information obtained from toolbar software as a rough indication of page importance. Intui tively, the more important a page is, the more times of user access would be . We used two types of user access information collected by the search toolbar of Bing (http://www.bing.com) : user -click -through , and user -input . Every entry in our aggregated clic k-through /user -input data contains a URL and the overall number of times that users click/input the URL. There are respectively 345 million and 1.26 billion distinct URLs contained in our user -input data and click -through data . For a given collection of URLs, assume L the URL ordering according to user access information , and L the URL ordering according to the static -rank values generated by a static ranking algorithm. We measure static -rank quality of the algorithm by computing the Kendall t au correlation [ 11] of L L . The Kendall tau rank correlation coefficient evaluates the degree of similarity between two sets of ranks given to a same set of objects. Kendall X  X  tau is defined as where  X   X  is the number of concordant p airs,  X   X  is the number of discordant pairs in the ranking lists , and L is the number of elements in every set . The coefficient has value s in [-1,1 ] for all orders, and a greater value indicates a higher positive correlation between the two lists. The standard Kendall X  X  tau measurement does not differentiate the discrepancies among high -rank items with those among low -rank ones. Intuitively, we should give more emphasis on top -ranked pages here. To address this problem, Yilmaz et al. [2 2] proposed a new rank correlation metric, AP correlation, as an extension of Kendall tau. As future work, we plan to adopt the new metric to compute the correlation. Currently, by manually observing the top -ranked pages generated by various approaches, we found that the top pages generated by the nonlinear algorithms are apparently more reasonable than those yielded by the standard PageRank algorithm . Kendall X  X  tau evaluation is performed over the intersections of the user access information and the link graph in orde r to measure the correlation between each ranking scheme and user preferences. We believe that the results are convincing since most of the high quality pages in the link graph fall into the intersections. We b elieve good static -rank values, when properly combined with dynamic ranking scores, should be helpful to yield good search results. Thus another way of evaluating static -rank quality is through the quality of the final search results.
 To generate search results, we buil t an inverted index for the web pages we crawled . And 1000 queries were selected as our query set from the query log of Bing (http://www. bing .com) . The ranking function used for computing page scores is the linear combination of static -rank and the dynamic ranking scores computed via the BM25 formula [ 17] (with the parameters at their optimal values with respect to the query set) . For each static -rank algorithm, top 10 results were returned for each query . We then collected the search results and had them judged by six labelers, with each result page marked with a relevance level from 1 (meaning  X  bad  X  ) to 5 (meaning  X  perfect  X  ). We adopted nDCG (normalized discount ed cumulative gain [8]) to evaluate the quality of search results. The nDC G metric has two parameters: discount factor b , and gains for all labeled relevance levels. In our experiments, the value of the discount factor b is fixed to be 2. And the gain value for the 5 relevance levels (from 1 to 5) are 0.01, 1, 3, 7 and 15, respe ctively. We adopt these values because a lot of other projects in our organization are using exactly the same values in evaluation.
 In addition to producing better results, practical static -rankers are intended to calculate in shorter time. We measure the speed of convergence with the number of iterations before the algorithm terminates with the observation that the time cost per iter ation for different algorithms are quite close to each other . Different terminating conditions (measured by the L1 residual between the experiments. Figure 3 shows the results of Kendall tau correlation for the static -rank algorithms we assessed (for both Clickthrough and User -input, the five bars from left to right are respectively BASIC, NL -LOG, NL -SQRT -1, NL -SQRT -2, and NL -MAX ). According to the figure , the four nonlinear algorithms have higher Kendall tau correlations with both user click -through and user -input than the standard PageRank algorithm. For the click -through and user -input data, the correlations improve by 19.5% and 3 0.4% respectively , by taking into account the correlation between back -links . The four nonlinear algorithms perform comparably.
Table 2. Search quality measured by nDCG@n for various 
NL -LOG 0.499 (  X  5.7%) 0.472 (  X  5.1%) 0.452 (  X  5.9%) 0.422
NL -SQRT -1 0.496 (  X  5.1%) 0.470 (  X  4.7%) 0.450 (  X  5.4%)
NL -SQRT -2 0.500 (  X  5.9%) 0.476 (  X  6.0%) 0.456 (  X  6.8%) NL -MAX 0.493 (4.4%) 0.463 (3.1%) 0.446 (4.4%) 0.415 (4.0%) The results o f nDCG@n evaluation are presented in Table 2. It is observed that, like in the previous evaluation, the nonlinear algorithms perform quite close in quality again (from around 0.50 for nDCG@1 to around 0.42 for nDCG@10) and together outperform the BASIC algorithm (from 0.47 for nDCG@1 to 0.40 for nDCG@10). In general, the nonlinear algorithms we adopted boost nDCG up by more than 4 %. 
Table 3. Sign test results for comparing various stati c-rank NL -SQRT -1 &gt;&gt;  X  NL -SQRT -2 &gt;&gt;  X   X  We use the sign test to determine whether the search quality improvement is significant after the new nonlinear static -rank algorithms are applied . Table 3 displays the sign test results for comparing every pair of static -rank algorithms. In the table,  X  &gt;&gt;  X  indicates the P value is less than 0.01 and reaches the level of significant difference. And  X   X   X  means the P value is larger than 0.05, and therefore we cannot conclude the search quality of one algorithm is better than another. We can see from the table that the nonlinear static -rank algorithms lead to significantly better search results (in terms o f nDCG@3) than the BASIC algorithm. Also from the table, the differences between the nonlinear algorithms seem not significant. Similar observations can be made for nDCG@1, nDCG@5, and nDCG@10. Table 4 shows the convergence speed (meas ured by the number of iterations and the average time per -iteration ) of various algorithms. It can be seen that some of the nonlinear algorithms takes significantly fewer iterations to converge than the basic (linear) algorithm . For example, the NL -LOG alg orithm t akes about 85% fewer iterations (from 111 to 17) to converge with a residual of 0.001, compared with the basic PageRank algorithm . Even the NL -MAX algorithm, which has the lowest convergence speed among the four nonlinear algorithms, reduces about 66.7% of iterations with a residual of 0.001. We can see that, among the nonlinear algorithms, a lgorithm NL -SQRT -2 converges fastest, followed by NL -LOG . It can also be observed from Table 4 that the nonlinear algorithms only take a bit more time (about 5% ) per -iteration than the BASIC algorithm. Thus the overall computation time is reduced by utilizing the nonlinear algorithms. Therefore the time of static -rank computation can be significantly reduced if we choose a proper aggregation function.
 The huge di fferences of convergence speed among the algorithms are somewhat unanticipated , because the algorithms only differ in the way of expressing the static -rank of a page by the rank of its back -links. We leave it for future work the analysis of underlying reason for this phenomenon . 
Table 4. Convergence speed comparison among algorithms (metric: number of iterations ; the best results are in bold ) 
Algorithm NL -SQRT -1 23 37 51 65 430 s NL -SQRT -2 9 11 14 17 431 s In summary, the above experimental results demonstrate that our nonlinear algorithms outperform the classic PageRank algorithm both in static -rank quality (measured by two distinct criteria) and in convergence speed. This demonstrates that it is reasonable and necessary to consider the correlation among links in static -rank comp utation. All the four nonlinear algorithms generate static -rank values of very similar quality. While in terms of convergence speed, the NL -LOG and NL -SQRT -2 seem to be more effective than other algorithms. In this paper, we proposed a probabilistic link -aggregation model for static -rank computation, based on which some nonlinear algorithms were built which outperform the basic PageRank algorithm in terms of both static -rank quality and convergence speed. Most existing work fo r improving PageRank quality focus es on changing the contribution of out -links ( by dynamically setting the damping factor or adjusting the weight of links ). Differently, our work focuses on link -aggregation. By this way, we provide another angle to optimiz e static -rank computation . As future work, it is desirable to conduct more experiments using larger and various kinds of datasets in order to further validate our conclusions in this paper. Although we observed, in experiments, the improvement of convergen ce speed (i.e. the reduction in the number of iterations) with our nonlinear algorithms, the reason for that remains unclear till now. It will be interesting to study in theory or at least in intuition why this happens.
 In the paper , only the correlation b etween links coming from the same domain is considered (in a simple way). Further performance improvement is expected to be achieved if more types of dependence among links are discovered and exploited. For example, it may be possible to mine the correlati on between domains via their IP addresses and the linking patterns between them. New static -rank formulas can then be built by considering the correlation between domains.
 It would be interesting future work to s tudy the characteristics of the static -rank values generated by our nonlinear algorithms. For example , do the values still obey a power law distribution? And what is the relationship between the static -rank values yielded by the nonlinear algorithms and those generated by the standard PageRank algorithm? Another interesting direction is to study the possibility of combining nonlinear static -rank with existing PageRank enhancement techniques. [1] R. Baeza -Yates, E. Davis. Web Page Ranking using Link [2] P. Berkhin. A Survey on PageRank Computing. Internet [3] K. Bharat, B. -W. Chang, M. Henzinger and M. Ruhl. Who [4] M. Bianchini, M. Gori and F. Scarselli. Ins ide PageRank. [5] P. Boldi, M. Santini and S. Vigna. PageRank as a Function of [6] Z. Gyongyi, H. Garcie -Molina and J. Pedersen. Combating [7] T. H. Haveliwala. Topic -Sensitive PageRank. In WWW [8] K. Jarvelin and J. Kekalainen. IR evaluation Methods for [9] S. Kamvar, T. Haveliwala , C. Manning and G. Golub. [10] S. Kamvar, T. Haveliwala , C. Manning and G. Golub. [11] M. G. Kendall . Rank Correlation Methods, 4th edition.
 [12] J. M. Kleinberg. Authoritative Sources in a Hyperlinked [13] M.A. Krasnoselskii and P.P. Zabreiko . Geometric Methods [14] A. Y. Ng, A. X. Zheng and M. I. Jordan. Stable Algorithms [15] L. Page, S. Brin, R. Motwani and T. Winograd. The [16] M. Richardson and P. Domingos. The Intelligent Surfer: [17] S. E. Robertson. Overview of Okapi Projects. J ournal of [18] S. Shi, R . Song, and J.-R. Wen. Latent Additivity: [19] P. Tsaparas. Using Non -Linear Dynamical Systems for Web [20] B. Wu and B. D. Davison. Identifying Link Farm Spam [21] G.-R. Xue, Q. Yang, H. -J. Zeng, Y. Yu and Z. Chen. [22] E. Yilmaz, J. Aslam and S. Rober tson. A New Rank [23] H. Zhang, M . Zhu, S . Shi, and J .-R. Wen. Employing Topic Prove the convergence of our nonlinear algorithm NL -LOG (with ranking formula 4. 7). Most of the following notations, definitions and theorems are from [ 13]. Banach space : A normed space  X  with the norm  X  is called a Banach space if  X  is a complete metric space for the metric  X  defined on  X  by the formula d ( x , y )=|| x -y || .
 Here we omit the detailed definitions for the terms appeared above for simplicity but present an example in most books about nonlinear analysis that t he Euclidean space s  X   X  , where the norm of x =( x 1 ,..., x n ) is given by || x ||=  X   X  2  X   X  =1 Cone : A closed convex set K  X  E (where E is a Banach space) is called a cone if x  X  K and x  X  0 implies  X  X  X  X  K for  X  X  0 and  X  X  X  X  K for  X  &lt; 0 . A cone K defines a partial ordering : we write  X  X  X  X  or  X  X  X  X  if y -x  X  K , and x &lt; y or y &gt; x if y -x  X  X  X  X   X  .  X  in  X  such that a neighborhood of  X  0 is contained in  X  . Normal cone : A cone is called normal if there is an N &gt;0 such that 0  X  x  X  y implies || x ||  X  N || y || and N does not depend on x and y . Operator : Let  X   X  X  X   X  . An  X  -dimensional vector field (a.k.a map , operator )  X  assigns to each  X   X   X  a vector  X  (  X  )  X   X  say, a vector field on  X  is a mapping  X  :  X   X   X   X  . If we choose a coordinate system in  X   X  , then  X  may be represented as where x 1 ,..., x n are the coordinates of x and  X  components of  X  . Positive operator : The operator  X  is called positive on a set  X   X  X  X  (where  X  is always a Banach space) if  X  (  X  )  X  X  X  . Monotone o perator : The operator  X  is said to be monotone on a set  X   X  X  X  if  X  X  X  X  (  X  ,  X  X  X  X  ) implies  X  (  X  )  X  X  X  (  X  ) .  X  -concave operator : We consider a nonlinear operator  X  which is positive on a cone  X  in a Banach space  X  . Fix a nonzero element  X  0 in  X  . The operator  X  is called  X  0 -concave on  X  if for each nonzero element  X   X   X  there are  X  (  X  ) ,  X  (  X  ) &gt;  X  (here  X   X  means the choice of  X  depends on  X  ) such that and if for each  X   X   X  with  X  1 (  X  )  X  0  X   X  (  X  )  X   X  (  X  1 (  X  ) ,  X  1 (  X  ) &gt;  X  ) we have that where  X  (  X  ,  X  ) &gt; 0 . Fixed point : A vector  X   X  is called a fixed point of operator  X  if it is a solution to the equation  X  =  X  (  X  ) . Theorem 1 ( Schauder X  X  Principle ) : Let  X  be a Banach space and  X  a nonempty closed convex subset of  X  . If  X  is a continuous operator mapping  X  into a compact (closed and bounded) subset of  X  , then  X  has at least one fixed point.
 Theorem 2 : Let  X  be a solid normal cone and  X  0 the interior of  X  . If A is a  X  0 -concave monotone operator and  X   X   X   X  is a nonzero solution to equation  X  =  X  (  X  ) , then for each initial approximation  X  0  X   X  the approximating sequence  X   X  =  X  (  X   X   X  1 ) converges to  X   X  . (  X  = 1 , 2 , ... ) Please ref er to [ 13] for the proof of the above theorems. We define a vector set K as { x  X  X  X   X  |0  X  X  X   X  , i =1...,n} , where  X  is the number of pages in the link graph. K is closed since boundary is included, and is convex since for each  X  ,  X  in  X  and any  X   X  0 and  X   X   X   X  for  X  &lt; 0 if  X   X   X  . Therefore  X  is a cone . Thus we derive a partial ordering on K :  X   X   X  if  X   X   X   X   X  , which means  X   X   X  iff every coordinate of  X  is less than or equal to the corresponding coordinate of  X  .  X  is solid since any x &gt;0 has a neighborhood in K . And K is normal because for N =1, 0  X  x  X  y  X   X   X  X  X   X  . As discussed in the previous sections, we are calculating the rank score for each page by iteratively solving the equation (4. 7): Here we rewrite this equation in a n equivalent nonlinear operator style. We declare operator  X  :  X   X  X  X  and define  X  for any  X   X   X  , where S represents a domain , j represents a page ,  X  is the rank exists a hyperlink from  X  to  X  or 0 otherwise ). For the above operator A , we will prove that: 1) A has at least one fixed point x *; and 2) for each initial value  X  0  X   X  , the sequence x = A ( x n -1 ) converges to  X   X  (i.e.  X   X  is the only fixed point ) . We define a set  X  X  X  X  as follows, For a vector  X  X  X  X  , since ln(1+ a + b )  X  ln(1+ a )+ln(1+ b ) for any a, b  X  0, we have It is clear that  X  maps  X  into a compact (closed and bounded) subset of  X  . According to Theorem 1 we know that  X  has at least one fixed point  X   X   X  X  X  . To adopt Theorem 2, here we first prove that the operation A defined in Section 8.1.3.2 is a  X  0 -concave monotone operator .  X  (  X  )  X  X  X  ,  X  (  X  )  X  X  X  . Therefore  X  is positive . For arbitrary  X  ,  X   X   X  ,  X   X   X  , it is easy to prove that  X  (  X  )  X   X   X  (  X  ) Therefore  X  is monotone .
 For a fixed nonzero element  X  0 in  X  and any given  X   X   X   X   X  , we define  X  =  X  (  X  ) , and define  X  = min max Each non -constant component of operator A is a composition of ln(1 +  X  ) pattern. For any  X  X  X  X  and  X   X  ( 0 , 1 ) we define We can prove that  X  (  X  ,  X  ) &gt; 1 , because ln 1 +  X  X  X  &gt;  X  ln 1 +  X  for any  X   X  X  X  + and  X  X  X  (0,1) . Define  X   X  ,  X  =  X  (  X  ,  X  )  X  1 &gt; 0 . It is also clear that  X  (  X  ,  X  )  X  &lt; 1 . Therefore we have Thus A is a  X   X  -concave operator.
 According to Theorem 2, given any initial value  X  approximating sequence  X   X  =  X  (  X   X   X  1 ) converges to  X   X  . This also means  X   X  is the only fixed point of A (otherwise there is another fixed point  X   X   X   X   X  . Tak ing  X   X  as the initial value, the sequence  X   X  =  X  (  X   X   X  1 ) clearly converges to  X   X  . But according to Theorem 2, it should converges to  X   X  . Therefore  X   X  =  X   X  ).  X  Prove the convergence of NL -SQRT -1, NL -SQRT -2, and NL -MAX. Since the proof is quite similar with the case of NL -LOG, we only list the key differences here to save space . First, t he operator A is changed from Formula A -1 t o, where p , q are integers satisfying p &gt;1 and q  X  p . For NL -SQRT -1, p = q =2; for NL -SQRT -2 , p =2, q =1; while for NL -MAX, p = q =  X  . Second, we define the following set  X   X  X  X  to replace Formula A -2, where z i is the static -rank of page i computed by applying the standard PageRank algorithm to the link graph (without domain information utilized). For a vector  X  X  X  X  , since  X   X  +  X   X  for any a, b  X  0, we have Thus  X  maps  X  into  X  itself (which is compact).
 Third, for any  X  X  X  X  and  X   X  (0,1), we give the following definition of p ( x , t ) to replace the one in Formula A -4,  X  (  X  ,  X  )  X  1 &gt; 0 , we have Thus A is a  X   X  -concave operator.
 Therefore, b y replacing the Formulas A -1 to A -5 with B -1 to B -5, we can prove the convergence of NL -SQRT -1, NL -SQRT -2, and NL -MAX by using the similar procedure with that in Section 8.1.3. Prove that the h function of formula 4.1 satisfies the constraints (C1 ~ C 3) in Section 3.
 Proof: Constraint C1 is satisfie d, because Constraint C2 is satisfied, because Constraint C 3 is satisfied, because Please keep in mind that x i &gt;0, according to our assumption. The derivation of Formula 3.9 , given the Formulas 3.2 and 3.8
