 Machine-learned classification and ranking techniques often use ensembles to aggregate partial scores of feature vectors for high accuracy and the runtime score computation can become expensive when employing a large number of en-sembles. The previous work has shown the judicious use of memory hierarchy in a modern CPU architecture which can effectively shorten the time of score computation. How-ever, different traversal methods and blocking parameter set-tings can exhibit different cache and cost behavior depending on data and architectural characteristics. It is very time-consuming to conduct exhaustive search for performance comparison and optimum selection. This paper provides an analytic comparison of cache blocking methods on their data access performance with an approximation and proposes a fast guided sampling scheme to select a traversal method and blocking parameters for effective use of memory hierar-chy. The evaluation studies with three datasets show that within a reasonable amount of time, the proposed scheme can identify a highly competitive solution that significantly accelerates score calculation.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Ensemble methods; query processing; cache locality
Ensemble-based machine learning techniques have been proven to be effective for dealing data-intensive applications with complex features and document ranking is a represen-tative application benefiting from use of the large number of ensembles. For example, in the Yahoo! learning-to-rank challenge [7], all winners have used some forms of gradient boosted regression trees, e.g. [8]. The total number of trees reported for ranking can be upto 3,000 to 20,000 [10, 5, 11], or even 300,000 or more using bagging method [13]. Rank-ing for large ensembles is expensive. As reported in [14], it takes more than 6 seconds to rank the top-2000 results for a query processing a 8,051-tree ensemble and 519 features per document on an AMD 3.1 GHz core. If such an algorithm is used to compute scores for a large number of vectors in applications such as classification, the total job is also very time consuming.

The previous work addressed the speedup of runtime exe-cution for ensemble-based ranking in several aspects includ-ing tree trimming [3] for a tradeoff of ranking accuracy and performance, earlier exit [6], and loop unrolling [4], and en-semble restructuring for a tree-based model [12]. Memory access can be 100x slower than L1 cache and un-orchestrated slow memory access incurs significant cost, dominating the entire computation. The work shown in [14, 12] proposes a cache-conscious blocking method for better cache locality. However, there are other block methods to select and it is an open problem how to identify the best cache blocking method and parameter settings given different data and ar-chitecture characteristics. Experimentally determining this choice can be extremely time-consuming and the compara-tive result may not be valid any more with a change of un-derlying feature vector structure or architecture. This paper provides an analysis of multiple blocking methods with dif-ferent data traversal orders, which provides better insight on program execution performance and leads a fast approx-imation to select the optimized structure.

Here, we consider the fast computation of ensemble-based scoring that aggregates and derives final scores for n fea-ture vectors using m ensembles. For testing and comparing performance in ranking q sampled queries, the time cost for searching through all combinations can be as high as O ( m 2  X  n 2  X  q ). The main contribution of this work is to de-velop an analytic framework to compare memory access per-formance of data traversal under multi-level caches to find the fastest program execution with effective use of memory hierarchy. Our scheme results in a much smaller complexity with O ( m  X  n  X  q ). Our experiments with three datasets cor-roborate the effectiveness of search cost reduction while the guided approximation identifies a highly competitive block-ing choice. We also demonstrate the use of this scheme with QuickScorer [12] and for batched query processing.

The rest of the paper is organized as follow. Next, we describe the background information and related work. Sec-tion 3 discusses the design considerations. Section 4 gives a comparative analysis on different blocking methods. Section 5 presents evaluation results. Finally, Section 6 concludes the paper.
Given n feature vectors and an ensemble model that con-tains m scorers, these vectors and scorers fit in memory. The ensemble computation calculates a score for each fea-ture vector and each scorer contributes a subscore to the overall score for a vector. For example, for ranking a docu-ment set with an additive regression tree model [8, 5], each document is represented as a feature vector and each tree can be stored in a compact array-based format [4]. Follow-ing the notation in [6], Algorithm 1 shows the DS method with the two-loop standard execution order. At each out loop iteration i , all scorers are used to gather subscores for a vector before moving to another vector. The dominat-ing cost is slow memory accesses when scorers read feature vector values and update partial values.

Algorithm 1: DS standard method for score calculation. for i = 1 to n do
Tang et. al [14] proposed a 2D cache blocking structure called SDSD as depicted in Algorithm 2 which partitions the program in Algorithm 1 into four nested loops. The inner two loops process d feature vectors with s trees. To simplify the presentation, we assume n/d and m/s are inte-gers. By fitting the inner block in fast cache, this method can be much faster than DS. There are other possible cache blocking methods with different data traversal orders and it is an unanswered question on how to choose among them. Also, in [14] there is no cost analysis on how to set a proper parameter for the size of blocking in terms of s and d val-ues. While choices of their values can be restricted to fit in the fast cache, they can still be fairly large. For example, s and d can still reach upto 3,276 and 11,440 respectively in some of our experiments shown in Section 5. Assume m and n are smaller than these upper bound numbers, s ranges from 1 to m and d ranges from 1 to n and there are m  X  n combinations to compare as they all fit in different levels of cache. Since running each test query takes O ( n  X  m  X  q ), the total cost is O ( m 2  X  n 2  X  q ). For instance, given n = 10 , 000, m = 3 , 000, q = 1000, the total time takes over 1,141 years with one core, assuming it takes 40 nanoseconds to compute a partial score for a vector with a scorer. If we sample each of s and d values with step gap 100, the total one-core time is over 41 days without knowing if such sampling finds a solution competitive to the optimum. While running such a sampling can be fully parallelized, we still need a faster scheme with well-guided approximation.

Algorithm 2: 2D blocking with SDSD structure. for j = 0 to m s  X  1 do
There are other performance speedup techniques proposed in the previous work to speedup fast ranking score computa-tion, which can be summarized into two categories. The first category is to achieve a tradeoff between ranking efficiency and accuracy. In [6], an early exit optimization was devel-oped to reduce scoring time while retaining a good ranking accuracy. In [16, 17], ranking is optimized to seek the trade-off between efficiency and effectiveness. Asadi et.al [3] con-sidered the fact that compact, shallow, and balanced trees yield faster computation and generated such trees with trim-ming technique. The second category is to improve effi-ciency given a fixed model. The work in [4] proposed an architecture-conscious solution called VPred that converts control dependence of code to data dependence and employs loop unrolling with vectorization. Lucchese et.al proposed the QuickScorer (QS) algorithm [12] which traverses mul-tiple trees in an interleaved manner and accelerates with bit-wise operations. They propose a block-wise variant of QS (called BWQS) by partitioning trees into blocks and ap-plying QS to each block of trees. Given different dataset characteristics, it is an open problem how to find the op-timal partitioning. Also there are other ways to arrange blocking and our work is complementary and can be used to compare different options.
There are six ways of loop blocking depending on the or-der of data traversal: DSD, SDS, DSDS, DSSD, SDDS, and SDSD. Following the naming in [15], symbol D here stands for a loop control over feature vectors and S stands for a loop control over scorers. For example, DSDS means that feature vector traversal is controlled by the outermost and the third outermost loops while scorer traversal is controlled by the second and the innermost loops. The inner two loops access d vectors and s scorers.

Figure 1 illustrates the execution and data traversal order of these methods. Figure 1(a) shows that DSD initially visits one scorer and d vectors. Then it visits another scorer and the same d vectors. Figure 1(b) depicts that SDS initially visits one vector and s scorers. Then it visits another vector and the same s scorers. Figure 1(c) illustrates DSDS which visits scorers and vectors block by block and row by row. Figure 1(f) illustrates SDSD which visits scorers and vectors block by block and column by column.

Our objective is to compare these blocking methods and find a value for s and d to minimize the time cost of score computation under a constraint 1  X  s  X  m, 1  X  d  X  n . We have the following considerations. Figure 2: Data access flow of CPU with memory hierarchy.
The following parameters are used in assessing the average memory access cost of processing n feature vectors with m scorers. We assume that CPU has three levels of caches: L1, L2, and L3 and the three level setting is popular in the currently available processors from Intel and AMD. Let  X  1 the read or write cost of accessing L1 and cost for accessing other cache is  X  1 multiplied by a constant ratio. Namely c  X  1 is the cost of accessing L2, c 3  X  1 is the cost of accessing L3, and c 4  X  1 is the cost of accessing memory.

Our analysis separates the cost for accessing feature vec-tors and scorers. Without losing the generality, let A i total amount of data access to feature vectors at cache level i + 1 while  X A i be the total amount of accesses to scorers at cache level i +1 and  X  is the average frequency ratio between access of feature vectors and scorers during computation.
The total data access cost is the summation of the cost of accessing each level of memory hierarchy: where  X  S ,  X  S ,  X  S ,  X  D ,  X  D , and  X  D are the miss rates of L1, L2 and L3 to access scorers and feature vectors respectively. Data accesses flow from CPU to memory for feature vectors is illustrated in Figure 2. A 1 = A 0  X  D is the total num-ber of feature data access to L2 due to their misses to L1; A 2 = A 0  X  D  X  D is the total number of feature data access to L3. A 3 = A 0  X  D  X  D  X  D is the total number of data access to memory.
 Then the time cost divided by A 0  X  1 is defined as where T D = 1 +  X  D c 2 +  X  D  X  D c 3 +  X  D  X  D  X  D c 4 and T 1 +  X  S c 2 +  X  S  X  S c 3 +  X  S  X  S  X  S c 4 .

Since A 0 and  X  1 are constants, in the rest of the analysis, we focus on computing the above data access cost ratio T .
Notice that once data is brought from memory hierarchy, the arithmetic computing cost of all four methods is the same. Thus we just need to analyze and compare the data access cost ratio T for the four traversal methods. In prac-tice, data access cost often weights more than arithmetic cost.

Due to the restriction on the paper length, we first present the analysis of cache performance for DSD and then list the result of DSDS, SDSD, and SDS as the case subdivision and cost derivation process are similar. Finally we describe an approximate scheme to select the best structure by taking advantages of the derived data access cost ratio for the four methods.
Algorithm 3: The program structure of DSD method. for all vector blocks do
Algorithm 3 lists the program control structure of DSD and a vector block contains d vectors. Once a scorer s i loaded to cache, it will be used by d vectors in the inner most loop. Then the next scorer s i +1 will go through the same d vectors. If we choose d properly such that d vectors fit in cache, we do not need to load them from memory for each scorer. Figure 3 illustrates how the cost of score computation could change when value d increases from 1 to n . The impact of d value on the cost is segmented with respect to the size of L1, L2, and L3. When d is small, the d vectors can fit in L1 cache, and there is an advantage of reusing these d vectors within L1 cache. Thus d should be as large as possible. When d value becomes too big, the benefit of leveraging L1 cache decreases because d vectors may not fit in L1 any more and therefore the access cost can increase with larger d value. We can reason similarly when d vectors fit or do not fit in L2 and L3 caches.

We will clarify the tradeoff of increasing d value when we derive a more concrete analysis. Let Fsize be the average data size of each feature vector. Without introducing more symbols, we also let L 1 ,L 2 and L 3 represent the size of L1 cache, L2 cache and L3 cache respectively in a formula expression. To assess the impact of increasing d values, we divide the increasing range into four parts as illustrated in Figure 3.
The cache access behavior of inner most loop in Algo-rithm 3 is affected by the average size of each scorer. For example, a larger scorer footprint leaves little space for L1 to host feature vectors. Figure 4 illustrates that we need to consider the following four scenarios and for each sce-nario, we need to further consider the four d range cases discussed above. Let Ssize represent the average data size of each scorer and the four scenarios corresponding to the root branches in Figure 4 are defined as follows. Figure 4: Range cases of d considered under different sce-narios for DSD.

To simplify the analysis, we assume that m and n are sufficiently large so that m scorers do not fit in L3 cache, and also n feature vectors do not fit in L3 cache.
Under Scenario 1, we first compute T S as follows. Since each scorer is loaded once for the inner loop most and will re-used d times for computing the subscores for d vectors in the inner most loop. Then the L1 cache miss ratio  X  S  X  1 /d . If there is an L1 cache miss for a scorer, L2 cache miss and L3 cache miss can occur with a high chance because the unseen new scorer has not been used ever and thus it is fetched from memory. Thus  X  S  X  1 and  X  S  X  1.
 We shall estimate T D under 4 different ranges of d values following Figure 3. We call these 4 range cases under DSD as DSD i where 1  X  i  X  4. The total cost ratio of accessing scorers for DSD is where and  X  i ,  X  i and  X  i are cache miss rates for accessing feature vectors under range case i . Note that in differentiating these miss rate of different cases, we use script  X  i  X  instead of  X  D,i  X  in order to simplify the presentation. Table 1 summarizes the cost of DSD for Scenario 1 when each scorer fits in L1 cache on average.

Range Case DSD 1 : d vectors fit in L1. Once a scorer is loaded to L1, the inner most loop load d feature vectors to L1 and these vectors stay and will be available in L1 when a new scorer is fetched to L1. Given m scorers, each feature vector in L1 is accessed m times and there is one 1 miss initially and the rest of m  X  1 accesses will hit L1. Thus the L1 cache miss ratio with respect to feature vectors is  X  1  X  1 /m . If there is an L1 cache miss for a feature vector, there must be an L2 cache miss and L3 cache miss. Thus,  X  1  X  1 and  X  1  X  1. Plugging into Equation 1, we get the total cost ratio:
Range case DSD 2 : d vectors fits in L2. Once a scorer is loaded to L1, the inner most loop can load a feature vector and keep it at least at L2 when a new scorer is loaded. Given there are m scorers, A 2 /A 0  X  1 /m . Namely  X  2  X  A /A 0  X  1 /m . Since L2 can hold d vectors needed for inner most loop, A 2  X  A 3 . Thus  X  2 = A 3 /A 2  X  1.

Range case DSD 3 : d vectors fit in L3. Once a scorer is loaded to L1, the inner most loop can load a feature vector and keep it at least at L3 when a new scorer is loaded. Given there are m scorers, A 3 /A 0  X  1 /m . Namely  X  3  X  A /A 0  X  1 /m .

Range case DSD 4 : d vectors donot not fit in L3. In this case, A 0  X  A 1  X  A 2  X  A 3 . In this case, actually we put n documents in the inner loop. It X  X  obvious to see that L1, L2 and L3 X  X  cache miss ratio are all 1 because comparing to the memory size, even L3 cache size is too small.  X  4  X  1,  X  4  X  1 and  X  4  X  1.

Cases d vectors T DSD i =  X T s + T D  X 
DSD 1 L1  X  +  X  c 4 d
DSD 2 L2  X  +  X  c 4 d
DSD 3 L3  X  +  X  c 4 d
DSD 4 memory  X  +  X  c 4 d
For Scenario 2 when a scorer fits in L2 on average, there are only 3 range cases: DSD 2 ,DSD 3 , and DSD 4 . L1 miss rate in T D becomes 1 and T S adds c 2 as T S  X  1 + c 2 + For Scenario 3 where a scorer fits in L3, there are only 2 possible cases to consider: DSD 3 and DSD 4 . L1 and L2 miss rates in T D become 1 and T S adds c 3 as T S  X  1+ c For Scenario 4 where a scorer fits memory only, there is one case to consider: DSD 4 . Its T D does not change while T
S  X  1 + c 4 .
The data access cost for SDS, DSDS, and SDSD is listed in Appendix A. There is a total of 28 range cases considered in these four methods: DSD i , SDS i , DSD i S j , and SDS where 1  X  i,j  X  4. The cost results of these 28 cases can be used in the following two aspects.
To illustrate the second point above, we show that the following proposition is true and can narrow the search scope from 28 to 4 range cases.

Proposition 1. When each feature vector fits in L1 and each score fits in L1 on average, and  X c 4 d candidates with the lowest access cost are among range cases DSD 2 , DSD 2 S 1 , SDS 2 D 2 , and SDS 2 D 1 .

A proof is listed in Appendix B. Yahoo!, MS, and MQ datasets discussed in Section 5 fall into the condition of this proposition when each regression tree used is not too big (e.g. containing upto 50 leaves). The range of d and s values for these datasets is listed in Table 3 and Table 2 of Section 5. When a regression tree contains 150 leaves, ratio c 4 /s 2 is getting close to 1, cases SDS 3 D 1 and SDS can be competitive as a best candidate. Thus with such a condition, we can search for 6 cases instead of 28 cases.
In summary, a guided sampling scheme conducts the fol-lowing steps. 1) Identify data and architecture parameters. When possible, apply Proposition 1 or its variation to elim-inate some of 28 range cases from the cost analysis of DSD, SDS, DSDS, and SDSD. 2) For each of selected range cases, choose blocking factor d i and s i under a constraint that vec-tors and scorers accessed in the inner most loop of DSD and SDS or in the two inner most loops of DSDS and SDSD fit in the corresponding level of cache. One approach is to choose d 3) Run and collect the average query response time with m scorers and n vectors from each sampled case. Select the case and parameter setting with the lowest response time. The total complexity of this scheme with q test queries is O ( m  X  n  X  q ).
Integration with the QuickScorer method . When the ensemble computation uses the original computing al-gorithm for gradient boosted regression trees (e.g. [8, 5]), the main data structure of each scorer is a tree. To use the BWQS algorithm [12], we treat each scorer as the applica-tion of QS on a block of trees. The following parameters are involved: the size of a scorer changes when different parti-tioning is adopted while the number of inner-loop scorers ( s ) and inner-loop vectors ( d ) can vary too for different blocking methods. Thus we add a partitioning search loop on the top of the aforementioned comparison and sampling scheme to select the best partitioning.

Batched query processing . When the ensemble score computation is used for query processing where n is small, d value of the inner most loop limited by n can be insufficient to explore the cache locality and the effectiveness of blocking degrades. When batch processing is allowed, we can boost the cache utilization by processing feature vectors from mul-tiple queries in fast cache, which essentially raises n values. One application of such batched processing is to conduct an offline experiment to assess the ranking performance of an algorithm in answering a large number of queries and there is no need to output ranking results immediately.

For an online ranking application, the ranking results need to be produced promptly. While reaching a high throughput, batching a large number of queries can increase the average waiting time of batched queries and affect the response time. With this constraint in mind, we set a limit on the largest waiting time allowed in choosing a batch size for a higher throughout with a modest increase of response time.
This section provides an experimental comparison of dif-ferent cache blocking methods and validates the effectiveness of the selected method with unoptimized ones. The evalua-tion tasks are listed as follows: (1) Illustrate the fast com-parison of the 28 range cases for using DSD, SDS, SDSD and DSDS with guided sampling. (2) Integrate our cache blocking selection algorithm with the QS algorithm [12] for tree-based ranking. (3) Assess the batched query processing in improving the throughput when n is small.

We implement the blocking methods using C compiled with GCC optimization flag -O3. Experiments are con-ducted on a Linux CentOS 6.6 server with 8 cores of 3.1GHz AMD Bulldozer FX8120 and 16GB memory. FX8120 has 16KB of L1. We set L 2  X  1 MB as 2MB L2 cache is shared by two cores. Its 8MB L3 cache is shared by 8 cores and since L3 hosts tree data useful for multiple queries, we set L 3  X  2 MB . The cache line is of size 64 bytes. For AMD Bulldozer, c 2 is around 7.3, c 3 is around 25.1, and c around 80.9. We have also conducted experiments in a 24-core Intel Xeon E5-2680v3 2.5 GHz server with L 1 = 32 KB , L 2 = 256 KB, , and L 3  X  2 . 5 MB per core. The Intel results are similar and thus we mainly report the AMD numbers.
The following learning-to-rank datasets are used as evalu-ation benchmarks. (1) Yahoo! dataset [7] with 700 features per document feature vector. (2) MSLR-30K dataset [2] with 136 features per document vector. (3) MQ2007 dataset [1] with 46 features per document vector. Table 2 shows the range of d values when fitting d vectors in different cache levels for these 3 datasets. Table 2: The vector counts for fitting in differnt cache levels.
We use LambdaMART [5] for ranking with additive tree ensembles and derive tree ensembles using the open-source jforests [9] package. To assess score computation in pres-ence of a large number of trees, we have also used a bagging method [13] to combine multiple ensembles and each ensem-ble contains additive boosting trees. Because the size of a scorer affects the cache performance and parameter choices, we generate the size of each tree with several settings: 10 leaves per tree, 50 leaves per tree, and 150 leaves per tree. Table 3 shows the range of s values when fitting s scorers in different cache levels under three choices of the regression tree size. The  X  value is about 1 because the basic access operation of a scorer is to fetch 1 tree node and then a doc-ument feature. Each of them fits in one cache line. The default total number of trees used is about 20,000 for Ya-hoo! dataset, 10,000 for MS, and 4,000 for MQ. We also use other numbers of trees in our experiments. When using the QS method [12], each scorer is a meta tree merged from mul-tiple trees and  X  value is around 4 because the basic access operation of a scorer fetches elements from 4 data structures and then a document feature. Table 3: The tree counts for fitting different cache levels.
The above data sets contain 23 to 120 documents per query with labeled relevancy judgment. In practice, a search system with a large dataset ranks thousands or tens of thou-sands of top results after a preliminary selection. To evalu-ate the score computation in such a setting, we synthetically generate more matched document vectors for each query. In this process, we generate relatively more vectors that bear similarity to those with low labeled relevance scores, because a large percentage of matched results per query are less rele-vant in practice. The number of vectors per query including synthetically generated vectors varies from 3,000 to 10,000 for Yahoo! dataset, from 2,000 to 6,000 for MS, and from 1,000 to 4,000 for MQ.

Metrics. We mainly report the average time of com-puting a subscore for each vector under one tree. With n matched vectors scored using an m -tree model, this scoring time multiplied by n and m is the scoring time per query. The throughput is the number of feature vectors scored per second. The number reported here is measured in a multi-core environment where each query is executed in a single core.
Table 4 shows the score computing time of a vector per tree in nanoseconds under different cache blocking cases for Yahoo!, MS and MQ datasets.  X  X ! 10 X  means Yahoo! dataset and each regression tree has 10 leaves. Row 2 is the scoring time of DS without cache blocking. The cost of all 28 cases under 4 cache blocking methods using guided sam-pling are listed, starting from Row 3. Under Proposition 1, our scheme searches the optimum only from four cases DSD 2 , DSD 2 S 1 , SDS 2 D 1 , and SDS 2 D 2 . The correspond-ing entries in this table are marked in a gray color. For Yahoo! dataset with 150 leaves per tree, as we discussed in Section 4.2, extra two cases SDS 3 D 1 , and SDS 3 D 2 also compared and thus marked in a gray color. For each column from column 2, entry marked  X  ?  X  indicates the small-est value is found and this entry is considered to be highly competitive. Our comparison scheme selects DSD 2 as the best range case with d = 373 for Yahoo! dataset under all three tree size settings, d = 1928 for MS 50 leaves case and d = 5720 for MQ 10 leaves case. It selects SDS 2 D 2 with d = 1928 and s = 1638 for MS 10 leaves case. Figure 5: Time cost and cache miss of DSD as d varies.
The running cost of the above guided sampling in CPU hours with one core is shown the second row of Table 5 and can be completed within about 10 hours using a 8-core server. We have also conducted exhaustive search with greatly-increased sampling points to obtain an estimated op-timum solution. The best cases identified in the estimated optimum are listed in the third row of Table 5 and ex-actly match what has been selected by our guided sampling scheme. The fourth row of the Table 5 shows the sample error which is the cost difference ratio between the optimum solution and the solution approximated by our scheme. The difference is within 2.2%. The fifth and sixth rows are the best cases and difference ratio obtained on the Intel ma-chine. The error is within 2.4% while all best cases of the estimated optimum match those of the approximated solu-tion. The above result shows that our guided sampling can find a highly competitive blocking solution within reason-able hours using a modest server and such a solution can result in upto 6.57x response time reduction compared to DS without cache blocking.

Impact of blocking size on time cost and cache miss rate . In Section 4, we have used Figure 3 to illustrate the correlation between data access time cost and blocking Table 4: Scoring time of one vector per tree in nanoseconds for different cache blocking range cases. size in deriving the cost for DSD. Figure 5 shows the ex-perimental result to validate. This figure shows time cost curve of DSD and L3 miss rate measured using Linux tool perf when d value varies for Yahoo! dataset with 50 leaves (a) and 150 leaves (b) per tree. When d is too small, cache is not fully utilized and the cost of T S for DSD derived in Section 4.1.2 is large. When d is too big which falls into case DSD 3 or DSD 4 , the cost curve matches the analysis in Appendix A that T DSD 4 &gt; T DSD 3 &gt; T DSD 2 .
There is also a correlation between the overall time cost and L3 cache miss when d varies. For small d , the coefficient for L3 cost c 3 in T S is big. For large d , there is more L3 cache miss, making T D bigger as shown in Section 4.

Impact of m and n values on time cost. Figure 6 shows the time cost per tree per document when m changes from 2,000 to 20,000 for all datasets. In this experiment, we generate extra trees for MS and MQ datasets. It shows that with sufficiently large value of m , the cache behavior does not change much and the processing cost is about the same for different m values.
 Table 5: CPU hours for comparison, sampling errors, and best cases.

Figure 7 shows the time cost per tree per vector when n changes from 1 to 100,000. When n is smaller than 100, the performance drops significantly and cache is not fully uti-lized. When n is larger than 1000, the cost becomes stable Figure 6: Scoring time per vector per tree when m changes. Figure 7: Scoring time per vector per tree when n changes. and there is no much reduction. We will discuss the ex-periment results when batched query processing is allowed shortly.
We integrate our scheme with the BWQS algorithm [12] as follows. Step 1: Given m trees and let  X  be the number of trees that will be merged to use the QS method. The number of scorers is m 0 = m/ X  . Step 2: Given m 0 scorers and n vectors, use our scheme to find the best blocking method and parameters. Step 3: Repeat Step 1 and Step 2 for a different sampling choice of  X  . Step 4: The  X  with the smallest time cost yields the best overall performance.

Figure 8 shows the BWQS results under the different num-ber of scorers m 0 where m is fixed as 20,000 and m 0 varies from 1 to 20,000. Notice that when m 0 is small, each scorer contains many trees and does not fit in L1 or even L2. The Figure 8: Scoring time of a vector per tree when varying the number of BWQS scorers.
 Table 6: Use of the comparison and selection scheme with BWQS scorers and with the original regression tree scorers. best m 0 found for Y!64, Y!32, Y!16 and MS64 are 20, 10, 4 and 200, respectively. For Y!64, when m 0 = 20, case DSD with d = 75 reaches the best performance with 62.89ns scor-ing time. If d is chosen as 1, the scoring time would be 89.33ns. Here the constraint to derive d value is explained as follow. Let F be the number of features in each document vector and L be the number of leaves in each tree. Following [12], the size of d document vectors is 4 dF bytes and the QS data structure is composed of 6 parts: the result bit vectors with size d X   X  L 8 , the thresholds with size 4  X L , the offsets with size 4 F , the tree ids with size 4  X L , the bitvectors with size 8 , and leaves with size 4  X L . The total size of d vectors and QS data structure is 4 F ( d + 1) + ( d  X  L 8 + (12 + which needs to fit in L3 cache because one BWQS scorer may not fit in L 2. For Yahoo! dataset with F = 700, when L = 64 and  X  = 1000, we can derive d = 75 . 4 with L 3  X  2 MB .

Table 6 shows the scoring time per vector per tree when applying cache blocking with our comparison and selection scheme to the original tree scorer and to the BWQS scorer. This comparison shows that when the number of leaves per tree is small(10), BWQS performs better. When the number of leaves per tree increases to 150, BWQS becomes fairly slow. Our explanation is listed as follows. The core QS scheme has a complexity sensitive to the number of tree nodes detected as  X  X alse X  nodes because bit-wise operations need to be conducted for all such nodes. When the number of  X  X alse X  nodes is large and linear to L , the overall time cost grows at linearly to increasing of L for small L . When L &gt; 64, the bit operation has to be carried by multiple 64-bit instructions and there is additional overhead for managing this complexity. For a large tree with many false nodes, QuickScorer can become very expensive. On the other other hand, the original regression tree algorithm has a complexity logarithmically proportional to L . It should be mentioned that the scoring time per vector per tree reported here seems to be slower than what was reported in [12] for L = 64. That can be caused by a difference in dataset characteristics, our code implementation, and test platform. We will investigate this issue in the future work.
We illustrate the benefit of batched query processing when n is small. Table 7 shows the throughput under different batch sizes when ranking only 10 document vectors ( n = 10). The throughput is defined as the number of queries pro-cessed per second. The last row shows the throughput when DS without cache blocking is used. When batch size is 10, for MS 50, the average processing time is reduced from 79.79ns to 42.93ns. When the batch size becomes much bigger, the benefit is not significant any more while there is an increase of waiting time. Thus a modest batch size is sufficient in Table 7: Throughput under different batch size when n = 10. this case to reach upto 1.86x throughput performance im-provement.
The main contribution of this paper is a fast comparison and selection scheme to find an optimized cache blocking method with guided sampling. Our analysis estimates the data access cost of different methods approximately, which provides a foundation to select sampling points in comparing different methods and in narrowing search space.

The evaluation studies with 3 datasets show that different blocking methods and parameter values can exhibit differ-ent cache and cost behavior and our guided sampling can identify a highly competitive solution among DSD, SDS, DSDS, and SDSD methods in a reasonable amount of hours using a modest multi-core server. The difference between the selected solution and the estimated optimum is within 2.4% and the response time of this solution can be 6.57x faster than DS without cache blocking. The analytic cost analysis shows that the search space for datasets such as Yahoo!, MS, and MQ can be greatly narrowed by taking advantages of data and architectural characteristics. When the number of feature vectors per query is small, cache uti-lization is affected and if allowed, batched query processing can bring upto 1.86x performance improvement. The evalu-ation demonstrates that our scheme can be used to find the optimized partitioning for QuickScorer.

Acknowledgments . We thank the anonymous referees for their thorough comments. This work is supported in part by NSF IIS-1528041 and IIS-1118106. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.

Similar to the analysis of DSD in Section 4, this appendix section estimates the data access cost of SDS, DSDS, and SDSD. We skip the details on how the results are derived and summarize them in the following tables. We use a similar naming of range cases for these 3 methods: SDS i , DSD i S and SDS i D j where 1  X  i,j  X  4. Notation D i in each range case name means that d vectors fit in cache level i , but not i  X  1. Level i = 4 refers to memory. Notation S i means that s scorers fit in cache level i , but not i  X  1.

Table 8 lists the access cost ratio of SDS for the scenario when a feature vector fits in L1. For the scenario when one vector fits in L2 only, there are 3 range cases to consider: SDS 2 ,SDS 3 , and SDS 4 . Formula T S is the same while T adds c 2 as: T D = 1 + c 2 + c 4 s . For the scenario when a vector fits in L3 only, there are only 2 cases to consider: SDS 3 , and SDS 4 . Formula T S is the same while T D adds c as T D = 1 + c 3 + c 4 s . For the scenario when a vector fits in memory only: there is one case to consider: SDS 4 . T the same while T D changes as T D = 1 + c 4 .
 Table 8: Cost of SDS when 1 feature vector fits in L1.
Table 9 lists the data access cost ratio of DSDS for the scenario when a feature vector fits in L1. For the scenario when one vector fits in L2, there are only 6 range cases to consider: DSD 2 S 2 , DSD 3 S 2 , DSD 4 S 2 , DSD 3 S 3 , DSD and DSD 4 S 4 . Formula T S does not change while T D adds an extra c 2 term. For the scenario when one vector fits in L3, there are only 3 cases to consider: DSD 3 S 3 , DSD 4 and DSD 4 S 4 . Formula T S does not change while T D adds an extra c 3 term. For the scenario when one vector fits in memory only, there is only one case to consider: DSD 4 S 4 Formula T S does not change while T D adds an extra c 4 term. Table 9: Cost of DSDS when 1 feature vector fits in L1.
Table 10 lists the data access time ratio for SDSD when a feature vector fits in L1. Symbol  X  i,j denotes L1 cache miss rate in the corresponding case SDS i D j ; Symbol  X  i,j is the corresponding L2 miss rate.

For the scenario 2 when one scorer fits in L2, there are 6 range cases to consider: SDS 2 D 2 , SDS 3 D 2 , SDS SDS 3 D 3 , SDS 4 D 3 and SDS 4 D 4 .  X  in T D becomes 1 and T
S adds an extra c 2 term. For the scenario when one scorer fits in L3, there are 3 cases to consider: SDS 3 D 3 , SDS and SDS 4 D 4 .  X  and  X  in T D becomes 1 and T S adds an extra c 3 term. For the scenario when one scorer does not fit L3, there is only one case to consider: SDS 4 D 4 . Its T does not change while T S adds an extra c 4 term.

For each of DSD, SDS, DSDS, and SDSD methods, we eliminate its range cases that do not qualify for the best candidate as follows. .
 [1] Lector 4.0 datasets. http://research.microsoft.com/en-[2] Microsoft learning to rank datasets. [3] Nima Asadi and Jimmy Lin. Training Efficient [4] Nima Asadi, Jimmy Lin, and Arjen P De Vries.
 [5] Christopher J. C. Burges, Krysta Marie Svore, [6] B. Barla Cambazoglu, Hugo Zaragoza, Olivier [7] Olivier Chapelle and Yi Chang. Yahoo! Learning to [8] Jerome H. Friedman. Greedy function approximation: [9] Yasser Ganjisaffar, Rich Caruana, and Cristina Lopes. [10] Pierre Geurts and Gilles Louppe. Learning to rank [11] Andrey Gulin, Igor Kuralenok, and Dmitry Pavlov. [12] Claudio Lucchese, Franco Maria Nardini, Salvatore [13] Dmitry Yurievich Pavlov, Alexey Gorodilov, and [14] Xun Tang, Xin Jin, and Tao Yang. Cache-conscious [15] Jiancong Tong, Gang Wang, and Xiaoguang Liu.
 [16] Lidan Wang, Jimmy Lin, and Donald Metzler.
 [17] Lidan Wang, Jimmy Lin, and Donald Metzler. A
