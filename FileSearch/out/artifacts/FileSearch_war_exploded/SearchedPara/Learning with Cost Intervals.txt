 Existing cost-sensitive learning methods require that the un -equal misclassification costs should be given as precise val-ues. In many real-world applications, however, it is gen-erally difficult to have a precise cost value since the user maybe only knows that one type of mistake is much more severe than another type, yet it is infeasible to give a pre-cise description. In such situations, it is more meaningful to work with a cost interval instead of a precise cost value. In this paper we report the first study along this direction. We propose the CISVM method, a support vector machine, to work with cost interval information. Experiments show that when there are only cost intervals available, CISVM is signi f-icantly superior to standard cost-sensitive SVMs using any of the minimal cost, mean cost and maximal cost to learn. Moreover, considering that in some cases other information about costs can be obtained in addition to cost intervals, such as the distribution of costs, we propose a general ap-proach Codis for using the distribution information to help improve performance. Experiments show that this approach can reduce 60% more risks than the standard cost-sensitive SVM which assumes the expected cost is the true value. I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms; Design; Experimentation Cost-sensitive learning, misclassification costs, cost in tervals
In real-world applications, such as medical diagnosis, di-rect marketing and intrusion detection, different classific a-tion errors often lead to different losses. For example, in medical diagnosis, the loss of misdiagnosing a patient as healthy is much more serious than misclassifying a healthy person as sick, because the former may lead to the loss of a life. Unfortunately, traditional research assumes that a ll the classification errors will result in the same loss. Thus, standard classification methods try to minimize the number of errors rather than the total cost. To deal with unequal costs, cost-sensitive learning has attracted much attenti on in recent years [14, 8, 19, 20, 17, 21, 13, 7, 12, 11] .
Existing cost-sensitive learning methods work with fixed cost values. The cost information is provided by domain knowledge and is taken as a group of precise values. The classifiers will then be well tuned to reduce the total cost w.r.t. these particular cost values. However, in many real-world situations, although the user knows that one type of mistake is more severe than another type, it may be difficult to specify a precise cost value. The aspects that can cause imprecise costs include but not limited to:  X  Inherent impreciseness. Some information is naturally stochastic, e.g., one may donate $1 or $5 randomly.  X  Unknown information. Sometimes we can X  X  know every-thing about a system. For example, customers X  SSN and salary information can X  X  be obtained because of privacy.  X  Variations in modeling. In the process of modeling risk, the approach may sample data, transform input space, and use different methods or parameters. Due to these variations, the model may not provide precise assessment for costs.  X  Expert opinions. Experts may have different opinions.
And sometimes, experts can just give fair estimates of real risks.  X  Dynamic environments. Environments always change. The risk may change with time, e.g., due to the appearance of a new competitor. While in some applications, the same model will be used in different scenarios.

In many situations, though precise information is not avail -able, other useful information can be obtained. One of the most common and practical form to represent an imprecise value is to bound it with an interval. We call cost infor-mation represented by intervals cost intervals . There are at least three possible ways to obtain cost intervals:  X  Natural upper and lower bounds. In some applications, costs naturally have upper and lower bounds. For ex-ample, the buying price of a stock is $3, and someone determines to hold it when the price is between $5 and $8, then the profit is between $2 and $5 if the decision is right.  X  Transforming from confidence intervals. When there are no clear upper and lower bounds of cost, we can use a confidence interval of cost instead. For example, the 95% confidence interval indicates cost will appear in the inter-val with a probability of 0.95. Then it X  X  reasonable to use the lower and upper bounds of this interval to approxi-mate the real situation. To extreme, the 100% confidence interval indicates the true cost value is definitely in it and it can be used as cost interval if its range is appropriate. Confidence intervals are easy to get in many applications.
In medicine, the inclusion of confidence interval in anal-ysis results has been a standard [9, 18]. For example, it was reported in 2007 that [18], compared with women having non-proliferative lesions, the risk for breast can-cer is greater for women with atypical ductal hyperplasia of (IRR=5.0; 95% CI=2.26-11.0). It means that the in-cidence rate ratio is about 5.0 and the 95% confidence interval is [2 . 26 , 11 . 0].  X  Expert opinions. Sometimes, we can ask experts to pro-vide cost intervals according to their experiences. It will be much easier for experts to give a cost interval than a  X  X recise X  cost value.

In this paper we report the first study along the direction of learning with cost intervals. Current cost-sensitive me th-ods can be applied only when precise costs are given. To the best of our knowledge, there is no method learning with cost intervals. A related work is [16], which considers the situation that costs change over time. But it assumes that the true cost values are known at the time of classification. In our assumption, the true cost values are always unknown. ROC curve [2] can evaluate classifiers under imprecise class distributions or misclassification costs. Larger AUC (area under ROC curve) indicates better performance. This es-sentially assumes that nothing whatsoever is known about the relative severity of costs, which is a very rare situatio n in real-world problems. In our problem settings, cost inter -vals are known. The relationship to related work is shown in Figure 1.

An intuitive way to handle cost intervals is that, taking some value in a cost interval as the true cost, such as the minimal value, mean value, or maximal value, and then ap-plying standard cost-sensitive learning methods. We will show theoretically that, they are not the best solutions. We propose CISVM (Cost-Interval-sensitive SVM), a support vector machine, to work with cost intervals. Experiments show that CISVM is significantly superior to all of them.
We also consider the situation in which more cost infor-mation is known in addition to cost intervals. For example, we can get distribution information of costs in some applica -tions. We call them cost distributions . We propose a general approach Codis to exploit the COst DIStribution informa-tion to help improve performance. We will show theoreti-cally that Codis can minimize the expected risk, while stan-dard cost-sensitive methods taking the expected cost value as the true cost could not. Experiments show that Codis can reduce 60% more risks than standard cost-sensitive SVM.
Section 2 formulates the problem of learning with cost in-tervals. Section 3 presents the CISVM method and Section 4 gives some analysis. Section 5 considers the situation in which cost distributions are known. Section 6 reports em-pirical results and Section 7 concludes the paper.
In binary classification problems, suppose X is an input space and Y is a class label space with y  X  X  X  1 , +1 } . Data are drawn i.i.d. from distribution P r ( X, Y ) and a training That is, costs are only dependent on classes. The cost for any example of class y being misclassified to class y  X  is cost ( y, y Thus, costs can be represented by a cost matrix. Since there are only 2 classes, assume that correct prediction costs 0, a nd let c + and c  X  denote the cost of misclassifying a positive and negative example, respectively. Assume that positive class has higher cost, i.e., c +  X  c  X  . Since the optimal decisions are unchanged when a cost matrix is multiplied by a positive constant [8], we can simplify the costs by fixing the cost of negative class so that we only need to consider the cost of positive class, i.e., c  X  = 1, c + = c ( c  X  1).

For a classifier  X  : X  X  Y , the loss of misclassifying an example ( x, y ) (denoted by 0-c loss) is: where I ( a ) = 1 if a = true and 0 otherwise.

Previous cost-sensitive learning assumes that cost c is a fixed value. The goal is to find a classifier  X   X  minimizing the empirical risk
When the true cost is unknown and a cost interval is avail-able, we assume that the unique true cost C  X  is a random value in the interval [ C min , C max ]. The goal is to learn a classifier H  X  minimizing the true risk  X  R  X  =  X  R (  X , C fortunately, since the true cost is unknown,  X  R  X  can X  X  be obtained to guide the learning process. To overcome this difficulty, some risk  X  R s can be used instead to learn a clas-sifier, which is in fact determined by some cost C s , i.e.,  X  R s =  X  R (  X , C s ). Since in general  X  R s and C s are different from the true risk  X  R  X  and the true cost C  X  , respectively, we call them X  X urrogate risk X  and X  X urrogate cost X . By minimiz-ing surrogate risk  X  R s , the optimal classifier  X   X  s is expected to minimize the true risk  X  R  X  . But this is infeasible since is unknown. However, since the true cost can be any value in the cost interval, it is expected that any possible risk of  X  s should be small enough. Obviously, not all surrogate risk will be good enough for this purpose, so an appropriate sur-rogate cost C s must be carefully chosen. Thus, in order to learn a classifier making any possible risks small enough, we can formulate the problem of learning with cost intervals as Eq. 3, by considering C s as a variable for learning.
Note that, it doesn X  X  matter for C s to be different from C as long as  X  R (  X , c ) is small enough for all possible c values.
Let C = 0 . 5( C min + C max ) denote the mean cost, and d = 0 . 5( C max  X  C min ). Then, a random value c  X  [ C min , C can be represented as c = C + " , where " is a random value in interval [  X  d, d ] and  X  "  X  X  X  d .

Since there are infinite constrains in Eq. 3, it is intractable to get optimal solutions. To overcome this difficulty, CISVM tries to solve a relaxation with a small number of informa-tive constraints. Considering the worst case risk is the upp er bound of the risks w.r.t. any c in [ C min , C max ], the opti-mal solution of minimizing it can make all the constraints in Eq. 3 hold. That is, if  X   X  s = arg min  X  sup c  X  R (  X , c ) and the worst case risk is appropriate to be used as surrogate risk  X  R (  X , C s ) to guide the learning process. However, the worst case risk could be far away from the true risk. When C  X  = C min , it overestimates the risk the most. So the opti-mal solution of minimizing the worst case risk could not make the true risk small enough sometimes. That is, if H  X  = arg min  X   X  R (  X , C  X  ) and  X  R ( H  X  , C  X  ) =  X  , then &gt; could happen. CISVM overcomes this difficulty by minimiz-ing another risk  X  R (  X , C  X  ) in the meanwhile to avoid overfit-ting to surrogate risk, where C  X   X  = C s . If the distribution of cost were known, the expected risk could be suitable to eval-uate a classifier in imprecise environments. Unfortunately, only cost intervals are known. However, in this situation, th e risk w.r.t. the mean cost C (called  X  X he mean risk X  1 ) has the smallest maximal distortion of the true risk (see Section 4), so it is the best choice to reflect how good a classifier performs on the entire interval. Therefore, CISVM works by minimizing the worst case risk and the mean risk in the meanwhile. Though similar to the worst case risk, the best case risk is often used for analysis in imprecise environments , it is too optimistic to guarantee other possible risks are sma ll enough, so it is not a good candidate for either of  X  R (  X , C or  X  R (  X , C  X  ).
 Assuming that the prediction function is f = w T x + b , CISVM utilizes a surrogate loss in the following form:
L ( C p , f ( x ) , y ) = I y =+ [ C p  X  yf ( x )] + + I y =  X  where [ a ] + = max ( a, 0) and I a = I ( a ). It means that, the loss for a negative and positive example is L  X  = [1  X  yf ( x )] and L + = [ C p  X  yf ( x )] + , respectively. We use L ( C L ( C p , f ( x ) , y ) when this will not cause confusion.
As we know, for fixed costs, existing methods gain cost sensitivity by introducing a parameter to bias toward ex-pensive class. For example, sampling-based methods [8, 21] sample more examples of the expensive class. Instance-weighting-based methods [15] give higher weights to the ex-pensive class examples. Thresholding-based methods [6, 8, 21] move decision threshold toward the inexpensive class. Other methods such as cost-sensitive SVMs [3] and cost-sensitive boosting methods [12] also have such parameters to control the tradeoff between different classes. We call thi s parameter the cost parameter . For fixed costs, it is reason-able to let the cost parameter equal to the true cost. Some work also indicated the best value of the cost parameter is usually unequal to the true cost [5, 13]. Chawla et al. [4] further proposed a method to automatically find the best value according to the results on a validation set. Similarl y, a cost parameter is needed in the cost interval learning prob -lem to achieve cost sensitivity, which is C p in Eq. 4 to make tradeoff between different classes.

Let  X  R L ( C p ) denote the empirical loss on training set S :
For the surrogate loss in form of Eq. 4, the worst case risk is  X  R L ( C max ), which is guaranteed by Theorem 1. And the mean risk is  X  R L ( C ). In addition, the best case risk is  X  R
Theorem 1. Suppose C p is a random value in interval [ C min , C max ] , then the worst case risk w.r.t. loss function L ( C p ) is achieved when C p = C max :
Proof. sup C = sup C = sup " X = X = X = X = X n Corollary 1. The best case risk w.r.t. loss function L ( C p ) is achieved when C p = C min , which is  X  R L ( C
If the optimal solutions of minimizing  X  R (  X , C s ) and were identical, the relaxed problem that CISVM tries to solve would be degenerated to minimizing  X  R (  X , C s ) only, and thus its optimal solution would not be good enough for the original learning problem in Eq. 3. In fact, minimizing the worst case risk  X  R L ( C max ) is not equivalent to minimizing the mean risk  X  R L ( C ).
 Corollary 2. Suppose n + is the size of positive class. Then,  X  R L ( C )+ d  X  n +  X  sup C is equivalent to minimizing an upper bound of  X  R L ( C max since d  X  n + is a constant.

Proof. sup C = X  X  X =  X  R L ( C ) + d  X  n + .
 Here, equality holds only when yf ( x )  X  C for all positive examples. Since this requires almost all positive examples be support vectors, equality does not hold in general.
To minimize the worst case risk  X  R L ( C max ) and the mean risk  X  R L ( C ) in the meanwhile, CISVM firstly learns a varia-tion of support vector machine to minimize  X  R L ( C max Figure 2: 5 loss functions l , L CI , L CS ( C max ) , L CS L
CS ( C min ) are shown with each is plotted for posi-tive example (marked by  X  +  X ) and negative example (marked by  X   X   X ) separately. L CS (  X  ) represents for any of L CS ( C max ) , L CS ( C ) , and L CS ( C min ) . L convex upper bound of the true loss l . then minimizes  X  R L ( C ) by parameter selection. In partic-ular, to minimize the worst case risk, we set the surrogate loss of CISVM as: L
CI ( f ( x ) , y ) = L ( C max , f ( x ) , y ) (7) L CI is a convex upper bound of 0-c real loss function l ( c ) in Eq. 1 for any cost c . See Figure 2 for an illustration. Then, CISVM minimizes the regularized loss of L CI : where ( x ) is a feature map induced by a kernel function. The dual form can be easily derived.

To minimize the mean risk  X  R L ( C ), CISVM selects and kernel parameters according to  X  R L ( C ). That is, among the classifiers achieving the optimal worst case risk, CISVM selects the one minimizing  X  R L ( C ). When a validation set is available, 0-c loss l ( C ) can be used instead of L ( C ) to assess the mean risk on the validation set.

Therefore, CISVM involves two parts: (1) minimizing the regularized worst case risk (Eq. 8) by learning a variation of SVM; (2) minimizing the mean risk by parameter selection on a validation set. The pseudo code is shown in Alg. 1. A cost interval is determined when C max and C are fixed, so CISVM provides a unique solution for each cost interval.
This section gives some analysis to compare existing cost-sensitive methods with CISVM on handling cost intervals.
Cost-sensitive SVM (CSSVM) [3] can only work with fixed costs. Its loss function for class-dependent cost is: L
CS ( C p , f ( x ) , y ) = I y =+ C p [1  X  yf ( x )] + + I where, C p is the cost parameter. For a given cost c , C p
Intuitively, we can enable CSSVM to handle cost intervals by assuming that the true cost is a particular value in inter-Algorithm 1 CISVM Algorithm Input:  X  S = { ( x i , y i ) } n i =1 : training set, with y i  X  X  X  1 , +1 }  X  V : validation set  X  [ C min , C max ]: cost interval  X  { ( , k ) } : a set of parameters, where k is the kernel pa-1: For each ( i , k i ), train a SVM f i = w T ( x )+ b by solving 2: Let C = 0 . 5( C min + C max ) be the mean cost. 3: Evaluate the mean risk on validation set V for each f i Output: H ( x ) = arg min f i  X  R V ( f i , C ) val [ C min , C max ]. In this paper, we consider three versions of CSSVM: CSmin , CSmean and CSmax , by setting C p as C min , C and C max , respectively. Their loss functions are shown in Figure 2, which are different from L CI , the loss function of CISVM.

Let  X  R Lcs ( C p ) be the empirical loss on training set S :
Then similar to the analysis in the previous section, for the loss function in form of Eq. 9, the worst case risk is  X  R
Lcs ( C max ), the mean risk is  X  R Lcs ( C ), and the best case risk is  X  R Lcs ( C min ). Hence, CSmax minimizes the worst case risk  X  R Lcs ( C max ), CSmean minimizes the mean risk  X  R
Lcs ( C ), and CSmin minimizes the best case risk  X  R Lcs The optimal solutions of minimizing the worst case risk and the mean risk are not equivalent.

Lemma 1. Suppose C p is a random value in [ C min , C max then the worst case risk w.r.t. loss function L CS ( C p achieved when C p = C max : Corollary 3. The best case risk w.r.t. loss function L
CS ( C p ) is achieved when C p = C min , which is  X  R Lcs
Corollary 4.  X  R Lcs ( C )  X  sup C only when yf ( x )  X  1 for all positive examples. Since this requires many positive examples being non-support-vectors , equality does not hold in general.

The difference between CSmax , CSmean CSmin and CISVM is that they use different surrogate risks to guide the learnin g process. However, no matter what surrogate risk is used, it is generally unequal to the true risk, so there is always dis-tortion of the true risk. In a good cost interval learning method, the maximal distortion of the true risk should be as small as possible. Based on this idea, we define the ro-bustness of a cost-sensitive loss function on a cost interva l as follows:
Definition 1. Suppose  X  ( C p ,  X  ( x ) , y ) is a cost-sensitive is 0-c real loss in Eq. 1. Examples are drawn from P r ( X, Y ) . Then, the distortion of  X  ( C p ) on cost interval [ C min Figure 3: Comparison of robustness of CISVM and CSmax . There are 3 loss functions: 0-c loss l ( C max ) , L ( C max ) (i.e., L CI ), L CS ( C max ) . The figure shows the loss for positive examples: l + ( C max ) , L + CI , L + CS When the true cost C  X  = C min , the distortion of the true risk is largest for all of these loss functions, distort( L CI ) = C max  X  C min and distort( L CS ( C max is defined as the worst case distortion of  X  ( C p ) . Let C [ C min , C max ] denote the true cost, then: distort(  X  ( C p )) = sup inf C p distort( l ( C p )) can be used as a baseline when quanti-fying robustness, since l is the real loss and will not introduce additional distortion of risk.

The robustness of  X  ( C p ) is then defined as The larger the robust value, the better the robustness. robust = +  X  when there is no distortion.

Theorem 2. The robustness of loss functions of CISVM and CSmax is 0.5 and 0.25, respectively. Therefore, CISVM is more robust than CSmax .

Proof. inf C p distort( l ( C p )) = inf C p sup C  X   X  C p  X  C  X  distort( Lci ) = sup distort( Lcs ( C max )) = sup
Both CISVM and CSmax try to minimize the worst case risk, but CISVM is more robust than CSmax , which is shown in Theorem 2. The robustness of loss functions of CISVM and CSmax is 0.5 and 0.25, respectively. See Fig-ure 3 for an illustration. Similarly, for CSmean and CSmin , the robust value is 0.5 and 0.25, respectively. CISVM also tries to minimize the mean risk whose loss function L ( C ) has a robust value of 1.

Thus, we contribute the success of CISVM to two aspects: (1) it minimizes the worst case risk and the mean risk in the meanwhile; (2) it is very robust.
In additional to cost intervals, sometimes we can get more information on the costs. In some applications, experts can provide the cost distribution information base on their ex-periences. The normal and uniform distributions are very popular and can be easily recognized from experience. For complex distributions, we can build models to assess costs. Then the values provided by different models can be re-garded as samples from the underlying cost distribution. When cost distributions are known, many difficulties en-countered in learning with cost intervals no more exist. Let R (  X , c ) be the expected risk of loss l for some cost c : where p + = p ( y = +) and p  X  = p ( y =  X  ). f n is false negative rate, and f p is false positive rate.

Assume that cost c is independently drawn from distribu-tion v with domain C 2 , which is independent of X . Then the goal is to find a classifier  X  minimizing the expected risk over v :
Note that, this is different from learning with example-dependent costs [19, 20, 3, 11], where examples { ( x i , y are drawn from distribution D with domain X  X  Y  X  C , and costs are often assumed as c i = cost ( x i , y i , y y is the prediction for x i . The goal of example-dependent cost-sensitive learning is to minimize: In our setting, costs are independent of examples. That is, distribution v is not related to X . When c changes to c , (e.g., due to time change), the cost of any misclassified positive example will be c  X  .

When a classifier  X  is learned, R CD should be used for evaluation. It is equivalent to R (  X , E [ c ]), see Eq. 16. That is, the expected cost E [ c ] should be used to evaluate risk.
In the above we assume known cost distribution. When there are only samples of the underlying cost distribution the goal is to minimize the empirical form of R CD :
Eq. 16 shows for a fixed classifier  X  , the expected risk over distribution v is the risk w.r.t. the expected cost E [ c ]. An intuitive way to utilize cost distributions is to take E [ c ] as the true cost and then exploit standard cost-sensitive meth-ods. Unfortunately, there is no guarantee that this solutio n holds only when  X  is independent of c . A learned classifier  X  is fixed and is therefore independent of c , so Eq. 16 holds. Algorithm 2 Codis Algorithm 1: for t = 1 to T do 2:  X  S =  X  3: for i = 1 to n do 6: end for 8: end for But in the learning process,  X  is a function in hypothesis space  X  , so it is dependent on cost c .

Recall the cost parameter C p described in Section 3. Cost-sensitive methods use C p to make tradeoff between different classes according to their costs. Therefore, C p can be re-garded as a function dependent on the true cost c : C p = g ( c ). Since  X  is dependent on C p in the learning process, it is a function of c :  X  =  X  ( C p ) =  X  ( g ( c )). We then have:
Proposition 1. v is a cost distribution.  X  is a function in hypothesis space  X  to be learned. It has cost parameter C :  X  ( x ) =  X  ( x, C p ) . C p is dependent on the true cost c : C p = g ( c ) . Then  X  ( x ) =  X  ( x, g ( c )) , and the following holds when g ( c )  X  = c or  X  is a nonlinear function of g ( c ) . Inequality holds in general, since  X  is generally a nonlinear function of g ( c ) , and there is no guarantee that g ( c ) should be c even for fixed costs.
 Therefore, standard methods taking the expected cost E [ c ] as the true cost and then minimizing the correspond-can be easily derived that cost distribution learning probl em can be reduced to a special case of example-dependent cost-sensitive learning problem, which is guaranteed by Theorem 3.
 Theorem 3. Suppose example ( x, y ) is i.i.d. drawn from P r ( X, Y ) , and cost c is i.i.d. drawn from v ( C ) . Suppose P r and v are independent. Then new example ( x, y, c ) can be considered to be i.i.d. drawn from a joint distribution D with domain X  X  Y  X  C , with p D ( x, y, c ) = p P r ( x, y ) p p ( a ) denotes the pdf of sample a drawn from distribution A . We have the following holds for any function  X  with cost parameter C p = g ( c ) : Proof.
 E Cost distribution v is independent of X by definition, so Theorem. 3 holds. Based on this theorem, Codis , a general method exploiting example-dependent cost-sensitive meth-ods is proposed to utilize cost distributions. Firstly, a co st sample c i is drawn from v (or provided by a risk model) for each example ( x i , y i ) in training set S to form a new exam-dependent cost-sensitive method A is called to learn a clas-sifier minimizing the risk of  X  S . Furthermore, to reduce the variance caused by sampling from v , we can draw cost multi-ple times from v for a single example ( x i , y i ) since v and P r are independent. Therefore, we can repeat the first two steps several times and ensemble all the classifiers. The pseudo code is shown in Alg. 2. We use 0-1 voting for ensemble. It is possible to use other kinds of ensemble to get even better performance and which will be studied in the future. The learning algorithm A can be any standard cost-sensitive method which works with example-dependent costs, such as Costing [20] and example-dependent cost-sensitive SVM [3], which both work by assigning each example a weight pro-portional to its own cost c i .
We compare CISVM and CSmin , CSmean , CSmax , with standard SVM as baseline on 15 binary UCI data sets [1]: breast-cancer , breast-w , credit-a , credit-g , diabetes , german , haberman , heart-statlog , ionosphere , liver , sonar , spambase , spect , spectf and tic-tac-toe . We treat each class of a data set as positive class in turn. Thus, we actually have 30 tasks Since class imbalance may affect cost-sensitive methods [10 ], a balanced subset is used for each task to focus on the diffi-culty caused by imprecise costs. 25 cost intervals are used, which are shown in Table 1. 30 times stratified hold-out tests are carried out, with 2 / 3 data as training set and 1 / 3 data as test set. All methods use RBF kernels. Parameters are selected for every cost interval by 5-fold cross validation on training data from  X  X  0 . 01 , 0 . 1 , 1 , 10 , 100 } , and kernel pa-rameter is selected as { 1 / 10 , 1 / 2 , 1 , 2 , 10 } times the mean squared distance of training set. There is no guidelines on how to select parameters for standard cost-sensitive metho ds to handle cost intervals. To test CISVM rigorously, we use the mean risk to select parameters for all methods . We use standard SVM-style implementation by invoking MOSEK 4 .
Since the true cost is unknown, we need to evaluate how good a classifier performs in different situations. Here, quad ri-section points of an interval are considered in turn as the tr ue them test points P1-P5.
Due to page limit, we can only show part of the results 5 Figure 4 shows the performance of each method on two med-ical data ( heart-statlog1 , spect1 ) and a spam detection data classifier  X  X ull X  which always predict positive is also show n. Table 2: Win/tie/loss counts of method-in-row vs. ( spambase2 ) at P1, P3 and P5. Table 2 summaries the re-sults of paired t -tests and sign-tests for each test point sepa-rately. Table 3 shows the loss of each method on the first set of data sets for some cost intervals, where we count the num-ber of significantly best performance (SBP) [22] achieved by each method. SBPs are the best performances and there is no significant difference among them according to t -tests results. The SBP counts are further summarized in Table 4 for each cost interval and each test point.  X  At P1. The true cost is C min . CSmin is significantly better than CSmean and CSmax . It is not a surprise, since CSmin learns with the true cost. CSmean and CSmax use C p larger than the true cost, which obviously causes the risk being overestimated. While CISVM is comparable to CSmin , and beats CSmean and CSmax significantly. Note that, P1 is the most far point from CISVM X  X  cost parameter ( C p = C max ), but in general it does not prevent CISVM from effectively reducing risk. Some work indicated the best C p value is usually unequal to the true cost [5, 13]. This might be the reason why CSmin is not always the best at P1.  X  At P2. None of the methods learns with the true cost. CSmin is comparable to CSmean and CSmax . CSmean performs better than CSmax . While CISVM beats them all significantly.  X  At P3. The true cost is C , and CSmean learns with the true cost. It is not surprising that CSmean is significantly better than CSmin and CSmax . The latter two methods un-derestimates and overestimates the risk, respectively. Whi le CSmean is significantly worse than CISVM.  X  At P4. None of the methods learns with the true cost. CSmin has the most unappropriate cost parameter among all methods, and the results show it is significantly worse than the others. CSmean and CSmax are comparable.
 While CISVM is significantly better than CSmin and CSmean , and comparable to or slightly better than CSmean .  X  At P5. The true cost is C max . Both CSmax and CISVM learn with the true cost. Again, CSmin is the worst one, due to its under-bias. CSmax does not achieve to perform better than CSmean , but CISVM does. CISVM is comparable to or slightly better than CSmax .  X  Over all test points, CISVM beats all other methods sig-nificantly. The SBPs of CISVM are over 20% more than others (Table 4). CSmax is better than CSmin yet worse than CSmean . The reason might be that, though CSmean could not achieve the minimal worst case risk, it is more robust than CSmax . The results of CSmin show that min-Table 5: Grouped Results of t -tests: CISVM vs. Others. imizing the best case risk performs well only when the true cost is very close to the minimal cost.

To study how CISVM X  X  performance will change with dif-ferent cost intervals, we split 25 cost intervals into 12 gro ups according to different width and means and show the t -tests results for each group in Table 5. Since there are different numbers of intervals contained in each group, we normal-ize the win/tie/lose counts for the ease of comparison, with total counts shown in the row size . It shows that, when the mean cost is larger than 20, the performance of CISVM decreases, yet it is still comparable to or better than other methods. While when the mean cost is smaller than 20, the performance of CISVM is very similar for intervals with different width and means.

From the experiments we conclude that: (1) The success of CISVM implies that learning with cost intervals should be solved specially, and standard cost-sensitive methods cou ld not handle it well. (2) CISVM is generally the best method over the whole interval. Though we consider only 5 test points, it is very likely that CISVM is the best over the whole interval since it is better than or comparable to the methods learning with the true costs. Even at P1, where CISVM makes the most wrong guess of the true risk, CISVM still beats other methods sometimes. (3) Generally, CISVM is better than CSSVM with cost parameter being any value in a cost interval.
To see whether more cost information will help improve performance, we consider two typical types of distributions, uniform and normal distributions, in our experiments: U (1 , 3), There are 15 cost distributions altogether. For Codis , we use example-dependent cost-sensitive SVM [3] as learning al -gorithm A and set T = 9. We compare Codis with CSSVM taking E [ c ] as the true cost on the first 15 data sets used above. The settings for the 30 trails experiment and param-eter selection are as same as that in the previous section.
Table 6 shows the results for each cost distribution sep-arately. The results of SVM and CSSVM for normal dis-tributions with the same mean value are all the same since they just use the expected cost. It can be found that Codis can reduce 60% more risks than CSSVM. Paired t -tests and a following sign-tests both at significance level 95% are als o carried out. It shows that Codis is significantly better than other methods for any cost distribution on any data set in all 225 cases (15  X  15). Moreover, Codis is very robust to different cost distributions since its results are very simi lar. For a specific cost distribution N (10 , 2), its 98.8% confidence interval is [5,15] which can be handled by CISVM. The ex-pected cost is 10 which is used as the true cost in CSSVM. In this case, CSSVM is equivalent to CSmean . The results can be found in Table 3 at P3 since the true cost is the mean value. It shows Codis is much better than CISVM, and CISVM is much better than CSSVM. All the results indicate that when more information can be obtained in ad-dition to cost intervals, a great advantage can be obtained by using the additional information appropriately.
In many real-world applications, it is generally difficult to have precise costs, while relatively easier to get cost in -tervals. This paper reports the first study on learning with cost intervals. We propose the CISVM method and experi-ments show that it is significantly superior to standard cost -sensitive SVMs. We also consider the situation where distri -butions are known about costs in addition to cost intervals, and propose the Codis approach which is able to exploit cost distributions to reduce 60% more risks than standard cost-sensitive SVM. This research was supported by the NSFC (60635030), the JiangsuSF (BK2008018), and the 973 Program (2010CB327903).
