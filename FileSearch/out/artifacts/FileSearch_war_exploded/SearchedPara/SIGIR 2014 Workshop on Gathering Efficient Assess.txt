 Evaluation is a fundamental part of Information Retrieval, and in the conventional Cranfield evaluation paradigm, sets of relevance assessments are a fundamental part of test collections. This workshop revisits how relevance assessments can be efficiently created, seeking to provide a fo rum for discussion and exploration of the topic. H3.3 [ Information Search and Retrieval ] Relevance assessment; Evaluation; Test collections Evaluation has always played a vital part in Information Retrieval, exemplified by the importance of TREC and other similar efforts, such as CLEF and NTCIR. The problems and issues surrounding the building of test collections ha ve been studied in detail by many researchers, and in particular, the problem of creating relevance assessments. A wide variety of methods ha ve been proposed for creating relevance assessments, including the sampling of documents from judgment pools, the use of interact ive search and judge (ISJ), the simulation of queries and relevance judgments based on search logs, and crowdsourcing. Releva nce, has, of course, been extensively studied from a wide variety of perspectives. The dynamic human judgment process during searching has also been studied in great detail, as part of the overall information seeking process. The focus has often been on the criteria by which users (rather than assessors) judge the relevance of a document to a task, but much of this work has also investigated how a user X  X  conception of relevance changes over time, as the search process develops. Less work in Inform ation Retrieval, however, has focused in detail on individual a ssessor behavior, with a notable exception of [1], while [2] investigated the effort required by assessors to judge the relevance of different types of document. Given the importance of relevance assessments to IR, and the rise of new methods of gathering rele vance assessments, there is need to revisit and discuss some of the foundational IR work on relevance in light of recent research. Some questions to be addressed include: What techniqu es or mix of techniques can be used to most efficiently create relevance judgments? For modern IR, should binary relevance judgment be used, or graded? How difficult is judging relevance for different types of individual, different types of content, and different types of topic? And finally, can we learn more a bout how assessors go about the relevance judging process, and a pply this knowledge to more efficiently create sets of relevance judgments? The workshop focuses on how relevance assessments can be efficiently created, including: how the method of generating assessments, via conventional m eans or crowdsourcing, affects the judgments gathered, such as i ssues of assessor expertise and payment; the process by which individuals, or groups of individuals, assess documents; issues relating to the effort required to generate relevance a ssessments for diffe rent types of topic, and different type s of material (text, web, image, video, etc. and multiple languages); and to revisit the concept of  X  X elevance X , from a practical, operational standpoint, for the purposes of IR evaluation. The aim of the workshop will be to provide a forum where short research papers can be presente d, reporting work which may not conventionally be published in pape rs at formal venues, including  X  X ractice and experience X  papers concerning relevance assessment gathering, and position papers concer ning the concepts and issues. This work was supported in by the Arts &amp; Humanities Research Council UK, Digital Transformations in the Arts and Humanities (grant AH/L010364/1). [1] Al-Harbi, A. L. and Smucker, M. D. User Expressions of [2] Villa, R. and Halvey, M. Is relevance hard work?: evaluating 
