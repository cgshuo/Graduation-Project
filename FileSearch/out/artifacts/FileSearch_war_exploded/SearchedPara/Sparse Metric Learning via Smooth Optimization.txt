 For many machine learning algorithms, the choice of a distance metric has a direct impact on their success. Hence, choosing a good distance metric remains a challenging problem. There has been much work attempting to exploit a distance metric in many learning settings, e.g. [8, 9, 10, 12, 20, 22, 23, 25]. These methods have successfully indicated that a good distance metric can significantly improve the performance of k -nearest neighbor classification and k -means clustering, for example. A good choice of a distance metric generally preserves the distance structure of the data: the dis-tance between examples exhibiting similarity should be relatively smaller, in the transformed space, than between examples exhibiting dissimilarity . For supervised classification, the label information semi-supervised clustering, the side information conveys the information that a pair of samples are similar or dissimilar to each other. Since it is very common that the presented data is contaminated by noise, especially for high-dimensional datasets, a good distance metric should also be minimally influenced by noise. In this case, a low-rank distance matrix would produce a better generalization performance than non-sparse counterparts and provide a much faster and efficient distance calcula-tion for test samples. Hence, a good distance metric should also pursue dimension reduction during the learning process.
 tion (equivalently distance matrix) that combines and retains the advantages of existing methods [8, 9, 12, 20, 22, 23, 25]. Our method can simultaneously conduct dimension reduction and learn a low-rank distance matrix. The sparse representation is realized by a mixed-norm regularization used in various learning settings [1, 18, 21]. We then show that this non-convex mixed-norm regulariza-tion framework is equivalent to a convex saddle (min-max) problem. Based on this equivalent rep-resentation, we develop, in Section 3, Nesterov X  X  smooth optimization approach [16, 17] for sparse metric learning using smoothing approximation techniques, although the learning model is based on a non-differentiable loss function. In Section 4, we demonstrate the effectiveness and efficiency of our sparse metric learning model with experiments on various datasets. of symmetric d times d matrices will be denoted by S d . If S  X  S d is positive definite, we write the set of d times d orthonormal matrices. For any X, Y  X  R d  X  q ,  X  X, Y  X  := Tr ( X &gt; Y ) where Tr (  X  ) denotes the trace of a matrix. The standard Euclidean norm is denoted by k X k . Denote by class label y i (not necessary binary) and let x ij = x i  X  x j .
 distance matrix M = P &gt; P which defines a distance between x i and x j given by Our sparse metric learning model is based on two principal hypotheses: 1) a good choice of distance matrix M should preserve the distance structure, i.e. the distance between similar examples should be relatively smaller than between dissimilar examples; 2) a good distance matrix should also be able to effectively remove noise leading to dimension reduction.
 D , where S denotes the similarity pairs and D denotes the dissimilarity pairs based on the label information. Equivalently, larization ranges from element-sparsity for variable selection to a low-rank matrix for dimension reduction [1, 2, 3, 13, 21]. In particular, for any `  X  N d , denote the ` -th row vector of P by P ` and k P ` k = ( Motivated by the above observation, a direct way would be to enforce a L 1 -norm across the vector ( k P 1 k , . . . , k P d k ) , i.e.
 show that projected space  X  x does not mean that its row-vector (feature) should be sparse. Ideally, we ex-pect that the principal component of  X  x can be sparse. Hence, we introduce an extra orthonormal transformation U  X  O d and let  X  x i = PUx i . Denote a set of triplets T by distance matrix learning formulation: regularization was used in [1, 18] for multi-task learning and multi-class classification to learn the sparse representation shared across different tasks or classes. 2.1 Equivalent Saddle Representation We now turn our attention to an equivalent saddle (min-max) representation for sparse metric learn-ing (3) which is essential for developing optimization algorithms in the next section. To this end, we need the following lemma which develops and extends a similar version in multi-task learning [1, 2] to the case of learning a positive semi-definite distance matrix. Lemma 1. Problem (3) is equivalent to the following convex optimization problem Proof. Let M = UWU &gt; in equation (3) and then W = U &gt; MU . Hence, (3) is reduced to the following Now, for any fixed M in equation (5), by the eigen-decomposition of M there exists e U  X  O d where, in the last equality, we use the fact that V  X  O d , i.e.
 Schwartz X  X  inequality implies that this back into (6) yields || V &gt;  X  ( M ) V || (2 , 1)  X  equation (5) the result follows.
 From the above lemma, we are ready to present an equivalent saddle (min-max) representation of problem (3). First, let Q 1 = { u  X  :  X   X  X  , 0  X  u  X   X  1 } and Q 2 = { M  X  X  d + : Tr ( M )  X  where T is the cardinality of triplet set T i.e. T = # {  X   X  X } .
 Theorem 1. Problem (4) is equivalent to the following saddle representation  X  ( Tr ( M  X  )) 2  X  yields that Tr ( M  X  )  X  Observe that s + = max { 0 , s } = max  X  { s X  : 0  X   X   X  1 } . Consequently, the above equation can be written as min M  X  X  theorem. 2.2 Related Work There is a considerable amount of work on metric learning. In [9], an information-theoretic approach to metric learning (ITML) is developed which equivalently transforms the metric learning problem to that of learning an optimal Gaussian distribution with respect to an relative entropy. The method of Relevant Component analysis (RCA)[7] attempts to find a distance metric which can minimize the covariance matrix imposed by the equivalence constraints. In [25], a distance metric for k -means clustering is then learned to shrink the averaged distance within the similar set while enlarging the average distance within the dissimilar set simultaneously. All the above methods generally do not yield sparse solutions and only work within their special settings. Maximally Collapsing Metric Learning (MCML) tries to map all points in a same class to a single location in the feature space via a stochastic selection rule. There are many other metric learning approaches in either unsupervised or supervised learning setting, see [26] for a detailed review. We particularly mention the following work which is more related to our sparse metric learning model (3).  X  Large Margin Nearest Neighbor (LMNN) [23, 24]: LMNN aims to explore a large margin nearest neighbor classifier by exploiting nearest neighbor samples as side information in the training set. Specifically, let N k ( x ) denotes the k -nearest neighbor of sample x and define the similar set S = triplet set T is given by equation (2), the framework LMNN can be rewritten as the following: where the covariance matrix C over the similar set S is defined by C = x ) &gt; . From the above reformulation, we see that LMNN also involves a sparse regularization term experimental section. Large Margin Component Analysis (LMCA) [22] is designed for conducting classification and dimensionality reduction simultaneously. However, LMCA controls the sparsity by directly specifying the dimensionality of the transformation matrix and it is an extended version of LMNN. In practice, this low dimensionality is tuned by ad hoc methods such as cross-validation.  X  Sparse Metric Learning via Linear Programming (SMLlp) [20]: the spirit of this approach is closer to our method where the following sparse framework was proposed: However, the above 1-norm term learned sparse model would not generate an appropriate low-ranked principal matrix M for metric learning. In order to solve the above optimization problem, [10] further proposed to restrict M to the space of diagonal dominance matrices: a small subspace of the positive semi-definite cone. Such a linear programming problem. Nesterov [17, 16] developed an efficient smooth optimization method for solving convex program-dimensional real vector space E . This smooth optimization usually requires f to be differentiable with Lipschitz continuous gradient and it has an optimal convergence rate of O (1 /t 2 ) for smooth problems where t is the iteration number. Unfortunately, we can not directly apply the smooth opti-mization method to problem (4) since the hinge loss there is not continuously differentiable. Below we show the smooth approximation method [17] can be approached through the saddle representa-tion (7). 3.1 Nesterov X  X  Smooth Approximation Approach smoothing techiniques. To this end, we introduce some useful notation. Let Q 1 (resp. Q 2 ) be non-empty convex compact sets in finite-dimensional real vector spaces E 1 (resp. E 2 ) endowed with norm k X k 1 (resp. k X k 2 ). Let E  X  2 be the dual space of E 2 with standard norm defined, for any
Smooth Optimization Algorithm for Sparse Metric Learning (SMLsm) 1. Let  X  &gt; 0 , t = 0 and initialize u (0)  X  X  1 , M (  X  1) = 0 and let L = 1 2  X  2. Compute M  X  ( u ( t ) ) and  X   X   X  ( u ( t ) ) = (  X  1 +  X  X  X  , M  X  ( u ( t ) )  X  :  X   X  X  ) 6. Set t  X  t + 1 . Go to step 2 until the stopping criterion less than  X  k A k 1 , 2 = max x,u { X  Au, x  X  2 : k x k 2 = 1 , k u k 1 = 1 } .
 Now, the min-max problem considered in [17, Section 2] has the following special structure: Here, b  X  ( u ) is assumed to be continuously differentiable and convex with Lipschitz continuous gra-dient and  X  f ( x ) is convex and differentiable. The above min-max problem is usually not smooth and Nesterov [17] proposed a smoothing approximation approach to solve the above problem: Here, d 2 (  X  ) is a continuous proxy-function , strongly convex on Q 2 with some convexity parameter  X  d  X   X  4.1]. Indeed, it was established in [17, Theorem 1 ] that the gradient of  X   X  is given by Hence, the proxy-function d 2 can be regarded as a generalized Moreau-Yosida regularization term to smooth out the objective function.
 As mentioned above, function  X   X  in problem (12) is differentiable with Lipschitz continuous gra-smooth approximate problem (12). The optimal scheme needs another proxy-function d ( u ) as-sociated with Q 1 . Assume that d ( u 0 ) = min u  X  X  x  X   X  Q 2 can be simultaneously obtained, see [17, Theorem 3]. Below, we will apply this general scheme to solve the min-max representation (7) of the sparse metric learning problem (3), and hence solves the original problem (4). 3.2 Smooth Optimization Approach for Sparse Metric Learning We now turn our attention to developing a smooth optimization approach for problem (4). Our main idea is to connect the saddle representation (7) in Theorem 1 with the special formulation (11). To this end, firstly let E 1 = R T with standard Euclidean norm k X k 1 = k X k and E 2 = S d with Frobenius norm defined, for any S  X  X  d , by k S k 2 2 = are respectively given by Q 1 = { u = ( u  X  :  X   X  T )  X  [0 , 1] T } and Q 2 = { M  X  S d + : Tr ( M )  X  p X  X  = x jk x &gt; jk  X  x ij x &gt; ij . In addition, we replace the variable x by M and With the above preparations, the saddle representation (7) exactly matches the special structure (11) operator A can be estimated as follows.
 Lemma 2. Let the linear operator A be defined as above, then k A k 1 , 2  X  for any M  X  X  d , k M k 2 denotes the Frobenius norm of M .
 Proof. For any u  X  X  1 and M  X  X  d , we have that
Tr Combining the above inequality with the definition that k A k 1 , 2 = max k u k 1 = 1 , k M k 2 = 1 We now can adapt the smooth optimization [17, Section 3 and Theorem 3] to solve the smooth approximation formulation (12) for metric learning. To this end, let the proxy-function d in Q 1 be the standard Euclidean norm i.e. for some u (0)  X  Q 1  X  R T , d ( u ) = k u  X  u (0) k 2 . The smooth optimization pseudo-code for problem (7) (equivalently problem (4)) is outlined in Table 1. One can stop the algorithm by monitoring the relative change of the objective function or change in the dual gap.
 The efficiency of Nesterov X  X  smooth optimization largely depends on Steps 2, 3, and 4 in Table 1. the following problem The next lemma shows it can be efficiently solved by quadratic programming (QP).
 Lemma 3. Problem (15) is equivalent to the following s  X  = arg max where  X  = (  X  1 , . . . ,  X  d ) are the eigenvalues of decomposition solution of problem (15) is given by M  X  ( u ) = U diag ( s  X  ) U &gt; .
 Proof. We know from Von Neumann X  X  inequality (see [14] or [4, Page 10]), for all X, Y  X  S d , that Tr ( XY )  X  with X = It was shown in [17, Theorem 3] that the iteration complexity is of O (1 / X  ) for finding a  X  -optimal solution if we choose  X  = O (  X  ) . This is usually much better than the standard sub-gradient descent method with iteration complexity typically O (1 / X  2 ) . As listed in Table 1, the complexity for each iteration mainly depends on the eigen-decomposition on ming to solve problem (15) which has complexity O ( d 3 ) . Hence, the overall iteration complexity of  X  -optimal solution. As a final remark, the Lipschitz given by the L = 1 2  X  loose in reality. One can use the line search scheme [15] to further accelerate the algorithm. LMNN method [23], (2) the Sparse Metric Learning via Linear Programming ( SMLlp ) [20], (3) the information-theoretic approach for metric learning ( ITML ) [9], and (4) the Euclidean distance based k-Nearest Neighbor (KNN) method (called Euc for brevity). We also implemented the iter-ative sub-gradient descent algorithm [24] to solve the proposed framework (4) (called SMLgd ) in order to evaluate the efficiency of the proposed smooth optimization algorithm SMLsm . We try to exploit all these methods to learn a good distance metric and a KNN classifier is used to examine the performance of these different learned metrics.
 The comparison is done on four benchmark data sets: Wine, Iris, Balance Scale, and Ionosphere, which were obtained from the UCI machine learning repository. We randomly partitioned the the training set, and performed evaluation on the test sets. We repeat the above process 10 times and then report the averaged result as the final performance. All the approaches except the Eu-clidean distance need to define a triplet set T before training. Following [20], we randomly gen-erated 1500 triplets for SMLsm, SMLgd, SMLlp, and LMNN. The number of nearest neighbors parameter for SMLsm, SMLgd, SMLlp, and LMNN was also tuned via cross validation from The first part of our evaluations focuses on testing the learning accuracy. The result can be seen in Figure 1 (a)-(d) respectively for the four data sets. Clearly, the proposed SMLsm demonstrates best performance. Specifically, SMLsm outperforms the other four methods in Wine and Iris, while method. SMLgd showed different results with SMLsm due to the different optimization methods, which we will discuss shortly in Figure 1 (i)-(l). We also report the dimension reduction Figure 1(e)-(h). It is observed that our model outputs the most sparse metric. This validates the advantages of our approach. That is, our method directly learns both an accurate and sparse distance metric simultaneously. In contrast, other methods only touch this topic marginally: SMLlp is not optimal, as they exploited the one-norm regularization term and also relaxed the learning problem; LMNN of the distance matrix. ITML and Euc do not generate a sparse metric at all. Finally, in order to examine the efficiency of the proposed smooth optimization algorithm, we plot the convergence graphs of SMLsm versus those of SMLgd in Figure 1(i)-(l). As observed, SMLsm converged much faster than SMLgd in all the data sets. SMLgd sometimes oscillated and may incur a long tail due to the non-smooth nature of the hinge loss. For some data sets, it converged especially slow, which can be observed in Figure (k) and (l). In this paper we proposed a novel regularization framework for learning a sparse (low-rank) distance matrix. This model was realized by a mixed-norm regularization term over a distance matrix which (saddle) representation involving a trace norm regularization. Depart from the saddle representation, we successfully developed an efficient Nesterov X  X  first-order optimization approach [16, 17] for our metric learning model. Experimental results on various datasets show that our sparse metric learning framework outperforms other state-of-the-art methods with higher accuracy and significantly smaller dimensionality. In future, we are planning to apply our model to large-scale datasets with higher dimensional features and use the line search scheme [15] to further accelerate the algorithm. The second author is partially supported by the Excellent SKL Project of NSFC (No.60723005), China. The first and third author is supported by EPSRC grant EP/E027296/1. Figure 1: Performance comparison among different methods. Subfigures (a)-(d) present the aver-age error rates; (e)-(h) plots the average dimensionality used in different methods; (i)-(l) give the convergence graph for the sub-gradient algorithm and the proposed smooth optimization algorithm. [1] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. NIPS , 2007. [2] A. Argyriou, C. A. Micchelli, M. Pontil, and Y. Ying. A spectral regularization framework [3] F. R. Bach. Consistency of trace norm minimization. J. of Machine Learning Research , 9 : [4] J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and [5] S. Boyd and L . Vandenberghe. Convex optimization . Cambridge University Press, 2004. [6] J. F. Bonnans and A. Shapiro. Optimization problems with perturbation: A guided tour. SIAM [7] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning a mahalanobis metric from [8] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively with ap-[9] J. Davis, B. Kulis, P. Jain, S. Sra, and I. Dhillon. Information-theoretic metric learning. ICML , [10] G. M. Fung, O. L. Mangasarian, and A. J. Smola. Minimal kernel classifiers. J. of Machine [11] A. Globerson, S. Roweis, Metric learning by collapsing classes. NIPS , 2005. [12] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood component anal-[13] T. Hastie, R.Tibshirani, and Robert Friedman. The Elements of Statistical Learning . Springer-[14] R.A. Horn and C.R. Johhnson. Topics in Matrix Analysis . Cambridge University Press, 1991. [15] A. Nemirovski. Efficient methods in convex programming . Lecture Notes, 1994. [16] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course . Springer, 2003. [17] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming , [18] Obozinski, B. Taskar, and M. I. Jordan. Joint covariate selection and joint subspace selection [19] J. D. M. Rennie, and N. Srebro. Fast maximum margin matrix factorization for collaborative [20] R. Rosales and G. Fung. Learning sparse metrics via linear programming. KDD , 2006. [21] N. Srebro, J.D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. NIPS , [22] L. Torresani and K. Lee. Large margin component analysis. NIPS , 2007. [23] K. Q. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest [26] L. Yang and R. Jin. Distance metric learning: A comprehensive survey. In Technical report,
