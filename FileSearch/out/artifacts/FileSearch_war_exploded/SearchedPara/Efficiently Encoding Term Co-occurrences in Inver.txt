 Precomputation of common term co-occurrences has been successfully applied to improve query performance in large scale search engines based on inverted indexes. The results of such precomputations are traditionally stored as addi-tional posting lists in the index. During query evaluation, these precomputed lists are used to reduce the number of query terms, as the results for multiple terms can be ac-cessed through a single precomputed list. In this paper, we expand this paradigm by considering an alternative method for storing term co-occurrences in inverted indexes. For a selected set of terms in the index, we store bitmaps that encode term co-occurrences. A bitmap of size k for term t augments each posting to store the co-occurrences of t with k other terms, across every document in the index. At query evaluation, size k bitmaps can be used to answer queries that involve any of the 2 k combinations of the additional terms. In contrast, a precomputed list, although typically shorter, can only be used to evaluate queries containing all of its terms. We evaluate the bitmaps technique we pro-pose, and the baseline of adding precomputed posting lists and show that they are complementary, as they capture dif-ferent aspects of the query evaluation cost. We perform an experimental evaluation on the TREC WT10g corpus and show that a hybrid strategy combining both methods signif-icantly lowers the cost of query evaluation compared to each method separately.
 H.3.3 [ Information Search and Retrieval ]: Selection process
The work was done while the author was at Yahoo! Re-search Algorithms, Measurement Inverted index, term co-occurrence, precomputation, bitmaps
Inverted indexes have been successfully deployed to solve scalable retrieval problems where documents are represented as bags of terms. Each term t is associated with a posting list , which encodes the documents that contain t . The cost of query evaluation depends both on the number of query terms and the length of their posting lists. To reduce this cost, previous approaches [16, 18] have used precomputation of common subqueries.

This precomputation is performed in advance, during in-dex construction, and it is then used during query evaluation where multiple query terms are replaced with one compos-ite term. The precomputed combinations are determined based on their frequencies in logs of past usage of the sys-tem. Precomputed lists are stored in the index as regular posting lists, and thus their number is limited by the avail-able memory. For example, terms New and York can be substituted with the precomputed list for New York. While this approach has been shown to be successful in reducing the query evaluation time, it is limited by the fact that each additional posting list can only be used for queries contain-ing all of the pre-joined terms. Moreover, given additional available memory, only a limited number of term combi-nations can be precomputed. We use this precomputation technique as a baseline.

In this paper we introduce a more flexible way to encode term co-occurrence using bitmaps . A bitmap is associated with each posting and encodes co-occurrence of the post-ing term with a fixed set of other terms, chosen offline at index construction time. Given k bits we can encode the co-occurrence of k different terms with the posting list term in a list with bitmaps. This allows us to use the posting list of a single term to resolve queries involving that term and any of the 2 k combinations of the chosen terms. Had we chosen to represent each of these combinations by a sepa-rate posting list, the memory cost, as well as the complexity of picking the right combinations during query evaluation, would have become prohibitive. As they represent many Figure 1: An example index with bitmaps for terms term combinations, bitmaps are triggered more frequently during query evaluation, although this benefit is somewhat offset by the fact that postings lists with bitmaps are typi-cally longer than precomputed lists.

In Figure 1 we present an example to illustrate the in-tuition behind the use of bitmaps. On the left of the fig-ure we have an index with five posting lists corresponding to the terms New, York, City, Hall and the precomputed list for New York . Each posting list has document identi-fiers (docids) in sorted order. Besides docids , we represent bitmaps inside the oval. In this example each bitmap is 2 bits long. The terms corresponding to each bit in the bitmap are identified inside the parenthesis. An empty oval denotes no bitmaps for that term. There are two lists with bitmaps in this figure: York and Hall. The postings for term York contain bits for its co-occurrence with terms New and City. If we examine the bitmap for the posting of York in docid 2 we can see that it indicates that document 2 also contains term New but it does not contain term City. This is also apparent by the presence of docid 2 in New X  X  posting list and by its absence in City X  X  posting list.

On the right side of Figure 1 we show a sample query workload using the terms in the index. The index bitmaps have been optimized for this workload. We now provide some intuition on how to chose the terms to be represented in the bitmaps. Examining the workload we notice that the terms New and York co-occur in many of the queries. Thus, it would be beneficial to place a bit for New in York X  X  list or vice versa. Since the posting list of York is shorter, we chose to place a bit for New in York X  X  list. As terms City and Hall are both present in queries involving terms New and York, we can also add a bit for these two. In this example we are limited to bitmaps of size 2, we chose to add a bit for City in York X  X  list. Now the first two queries can be evaluated by accessing only York X  X  posting list. Similar reasoning is used to store the bits for New and City into Hall X  X  posting list.
To contrast bitmaps with precomputed lists, consider again the query New York. This query can be answered either with the precomputed posting list New York or with York X  X  list, which contains a bitmap that encodes co-occurrences of term New. In both cases only one list is accessed. The precomputed list has the advantage of being shorter, as it only contains the documents in the intersection of the lists for New and York. Therefore, using the precomputed list is more efficient as it contains only the docids that belong to the query results. On the other hand, bitmaps are more space efficient and more flexible  X  York X  X  posting list can be used to answer queries York, New York, York City and New York City, while the precomputed posting list for New York can only be used in two of these cases. In this pa-per, we show that the bitmaps and precomputed posting lists capture different aspects of the query evaluation cost. These complementary properties lead to improved perfor-mance when using both techniques simultaneously.
To determine which terms should be listed in the bitmaps of each posting list, we develop a model of the query eval-uation cost as a function of the number of query terms and the lengths of the posting lists corresponding to these query terms. This cost model allows us to formulate the problem of selecting bitmap terms for each posting list as an opti-mization problem. Given a query workload (e.g., from a historical query log) and a memory budget, the goal is to select the optimal set of bitmap terms for all posting lists in the index, so as to minimize the evaluation cost of the work-load. The optimization problem is NP-hard, but we show that the cost function is submodular, allowing for efficient approximation [15].

In this study we focus on the analysis of Boolean eval-uation algorithm for conjunctive (AND) queries, which are prevalent in display advertising scenarios [24]. Boolean AND queries are also common in the first phase selection for web search queries, where the first phase results are reranked by machine learning scoring methods. We experimentally evaluate precomputed lists, bitmaps and their combination on the TREC WT10g corpus and a query workload derived from the AOL query log [20]. We show that the two tech-niques are complementary, as their combination achieves higher latency reduction than each technique individually. Latency reduction ranges from 25% for 3% growth in index size, to 71% for 4-fold index size increase. We also show that both precomputation techniques benefit not only past queries, contained in the workload used for building the in-dex, but also new, previously unseen  X  X ong tail X  queries.
In summary, the main contributions of this paper are the following.
The rest of the paper is organized as follows. Section 2 contains background on indexing and query evaluation. In Section 3 we model the cost function of query evaluation and discuss the trade off between index size and evaluation time. Section 4 describes the index construction algorithms for bitmaps and precomputed lists, while Section 5 presents the corresponding query evaluation algorithms. Section 6 describes our experimental results. Section 7 reviews the related work. Finally, Section 8 presents our conclusions and future research directions.
Most search engines and information retrieval systems use inverted indexes as their main data structure for full-text indexing [28]. We remark that many modern applications are geared for high-throughput and low-latency scenarios (see, for example, [9, 24]), and consequently, the index data structures reside in main memory and are not swapped in from disk. There is a considerable body of literature on and query evaluation (e.g. [7, 17, 26, 28]) algorithms.
In inverted indexes, the occurrence of a term t within a document d is called a posting . A posting has the form  X  docid, payload  X  , where docid is the document identifier of d and where the payload is used to store arbitrary information about each occurrence of t within d . In this paper, we use part of the payload to store the co-occurrence bitmaps. The set of postings associated with a term t is stored in a posting list .
 Each posting list is sorted in increasing order of docid . Often, B-trees [13] or skip lists are used to index the posting lists [12, 19]. This facilitates searching for a particular docid within a posting list, or for the smallest docid in the list greater than a given docid . In this work we use the following basic operations on posting lists: 1. first(): returns the list X  X  first posting; 2. next(): returns the next posting or signals the end of 3. search(d): returns the first posting with docid  X  d , or
Although both precomputed lists and bitmaps can be used with different query evaluation algorithms, in this paper, we assume conjunctive Boolean queries and the document-at-a-time query evaluation model (DAAT) [23], commonly used in Web search engines. With Boolean queries no extra infor-mation, such as term weights, is required in the bitmaps and precomputed lists. In DAAT, the documents that satisfy the query are usually obtained via a join of the posting lists of the query terms. Given a conjunctive query q = t 1 t 2 ...t a search algorithm returns R  X  the set of docids of all docu-ments that match all terms t 1 t 2 ...t n .

Let L 1 ,L 2 ,...,L n be the posting lists of terms t 1 ,t respectively. A naive algorithm would scan L 1 ,L 2 ,...,L and return the set of documents that appear in all the lists. Clearly, the naive algorithm accesses P n i =1 | L i | postings, where | L i | is the number of postings in L i .
 A more efficient algorithm is Max Successor [8], shown in Algorithm 1. It sorts terms in ascending order of their list lengths and traverses them in parallel. In each iteration, the algorithm checks whether the current candidate document from the shortest list appears in other lists. Instead of scan-ning over possibly numerous postings in longer lists, this step is implemented by skipping to the first position con-taining docid greater than or equal to the current candidate. If all lists contain the current candidate, it is added to the result set and the position in the shortest list is advanced by 1. Otherwise, the algorithms advances the shortest list to the following potentially matching document. The algo-rithm iterates until it reaches the end of one of the lists. The advantage of this algorithm that the number of list accesses is proportional to the length of the shortest list. We use this algorithm as the basis for our analysis and evaluation. Algorithm 1 Max Successor: search ( q ) 1: Assume that | L 1 | X | L 2 | X  ...  X | L n | 2: R  X  X  X  3: d 1  X  L 1 .first () 4: while d 1 is defined do 5: for i  X  2 to n do 6: d i  X  L i .search ( d 1 ) 7: if d 1 6 = d i then 8: d 1  X  max ( d i ,L 1 .next ()) 9: break 10: else if i = n then 11: R  X  R S d 1 12: d 1  X  L 1 .next () 13: return R
Our work trades off the size of the index with the query evaluation speed. Generally, query latency is directly cor-related with the number of postings that are accessed dur-ing the evaluation of the query. We begin by analyzing the cost of query evaluation as a function of the lengths of the postings lists that are accessed and define an analytic cost function. We then discuss the trade off between the size of the index and query evaluation performance and show that bitmaps and precomputed lists capture different aspects of this trade off.
The latency of the Max Successor query evaluation algo-rithm [8] increases both when the algorithm has to access more lists, and when the lists accessed are themselves longer. Understanding the exact interplay between these two param-eters is integral to making principled decisions as to which precomputed lists and bitmaps should be added to the in-dex.

In each iteration of the algorithm the cursor in the shortest list advances by at least 1, and therefore the main loop of the algorithm (line 4) is executed at most | L 1 | times. For the remaining lists, in each iteration the algorithm advances the cursors to the next relevant docid , skipping other postings along the way (line 6). These skips are significant, as the total number of accesses to a secondary list is sublinear in the length of the list.

With this in mind, we considered the family of functions expressed as: where G is the sublinear function quantifying the number of accesses to the secondary lists. We validated this cost func-tion experimentally using the TREC WT10g corpus and the AOL query log [20], measuring the lengths of the accessed postings lists and the evaluation time for each query. Typ-ical implementation of skipping by B-trees or binary search suggests that the function G is logarithmic in shape. To test this hypothesis we considered G ( x ) to be coming from the parametric family: The best fit to the data was achieved by setting C and C 2 = 1, which yielded an R 2 (R squared correlation coefficient) value of 0 . 65. Thus, this simple function explains almost 2 / 3 of the variation that occurs in the data. For the remainder of the paper, we then fix the cost function for evaluating query q to be:
We now describe how precomputed lists and bitmaps min-imize the two components of the above cost function (1) the shortest list length | L 1 | and (2) the random access cost 12 + log | L i | . Suppose terms t 1 and t 2 frequently occur as a subquery and assume | L 1 | X | L 2 | .

Using precomputed lists we would store the co-occurrences | L 1  X  L 2 | as it contains only the documents in the inter-section of these lists. Therefore, precomputed lists reduce not only the number of posting lists accessed during query evaluation, but also the size of these lists, potentially im-pacting the length of the shortest list in the query. This technique decreases both components of the cost function while increasing the index by | L 1  X  L 2 | postings.
In the case of bitmaps, we add a bit to the payload of each posting in L 1 , where the value of the bit is 1 for postings whose document contains t 2 and 0 otherwise. This allows the query evaluation algorithm to avoid accessing L 2 , cutting the second component of the cost function, 12 + log | L The extra space is a bit for each posting in L 1 .

To demonstrate the complementary nature of the two pre-computation techniques we analyze their applicability to dif-ferent queries in our evaluation dataset (described in Sec-tion 6). For each query of at least two terms we computed the minimum relative intersection size (MRIS)  X  the rela-tive size of the shortest list resulting from an intersection of two query terms to the shortest list of a single term: the optimal precomputed list of two terms for this particular query: the lower the MRIS, the lower evaluation cost can be achieved; and, moreover, the lower is the storage cost of that precomputed list. Figure 2 shows the cumulative distribu-tion function of queries as a function of their MRIS. While the majority of the queries have low MRIS and thus can benefit from precomputed lists, a non-negligible fraction of queries have relatively high MRIS, where precomputed lists are less beneficial. Indeed, in our experimental results we show that the potential benefit of precomputed lists declines sharply with increasing MRIS, while the benefit of bitmaps rises moderately (see Figure 5).

We note here that adding bitmaps to postings does not prevent delta compression of the docids . Traditionally, for disk based indexes docids are compressed in compact array that is separated from the payload to pack as many docids as feasible in the available memory. Accessing the payload is done only when needed and requires additional I/Os. For in-memory indexing, the access penalty is a lot smaller and Figure 2: CDF of fraction of queries as a function of both options are applicable: storing the bitmap with the compressed docids , as well as using a separate payload array to store the bitmap. In either option the added cost of ac-cessing the payload is captured by the C 1 and C 2 constants.
Given a typical query workload and additional memory budget, our goal is to find the optimal set of bitmaps and precomputed lists that minimizes the query evaluation cost function based on the given workload.
At index construction time, the central question is which bitmaps and precomputed lists should be added to the in-dex to reduce the expected latency under the given query load. In both of these approaches we face a tension between the improvement in latency caused by using precomputed objects (whether bitmaps or precomputed lists) and the ex-tra space required for precomputed results. Moreover, the problem does not decompose nicely, the contribution of a particular precomputation (whether bitmap or precomputed list) depends on the set of bitmaps and precomputed lists already added to the index. We first consider the problems of selecting bitmaps and precomputed lists separately, and then propose a hybrid algorithm for selecting them simulta-neously.
In the case of bitmaps the extra space required for adding a bitmap for term t j to term t i  X  X  list is exactly | L i ery posting in L i grows by one bit. The benefit of the extra bitmap varies. Suppose, for example, that we have terms New, York, and City, with list lengths | L New |  X  | L | L York | , and the queries are New York, City York, and New York City. Consider the benefit of adding a bitmap for term New to City X  X  posting list. In the case that no pre-vious bitmaps exist, this bitmap improves the evaluation of query New York City from | L York | ( G ( | L New | ) + G ( | L to | L York | G ( | L City | ). However, if the list York already has bits for terms New and City, the extra bitmap provides no additional benefit. In this case the total latency would be | L
York | regardless of whether a bit for term New was added to City X  X  list.

This leads us to formulate the bitmap selection problem as follows. Let B be the association matrix where b ij = 1 if there is a bit for term t j in list L i  X  X  bitmap, and 0 otherwise. For example, we set b City New = 1 in the example above. Given a set of bitmaps B and a query q , F ( B,q ) is the latency of evaluating q with the bitmaps indicated by B . Here we assume q is evaluated using the optimal set of lists and bitmaps minimizing the evaluation cost. Finally, let S denote the total space available for storing extra information and Q = { q 1 ,q 2 ,... } the query workload. The problem is then: Although the problem looks daunting, the cost function F has extra structure which allows us to find a near optimal solution. Consider the benefit of an extra bitmap, b ij , when a previous set B has already been selected. This is exactly F ( B  X  X  b ij } ,q )  X  F ( B,q ). Contrast this with the extra benefit when a larger set,  X  B  X  B has already been selected, F ( { b ij } ,q )  X  F (  X  B,q ). The cost function defined in Section 3.1 exhibits diminishing returns, that is, the extra benefit of b ij can only decrease as other bitmaps are added. More formally, the function is submodular.

Lemma 1. Let F be the cost function as above, and B  X   X  B be two sets of bitmaps already selected. Then for any b F (  X  B  X  X  b ij } ,q )  X  F (  X  B,q )  X  F ( B  X  X  b ij } ,q )  X  F ( B,q ) .
Since the sum of submodular functions is itself submodu-lar, we know that the objective in the mathematical problem defined above is also submodular. We now appeal to the the-ory of submodular function maximization subject to a bud-get constraint. As [15] shows, the simple greedy algorithm achieves a constant factor approximation to the optimum. At every iteration, for every potential bitmap b ij , the algo-rithm computes the ratio of the benefit to the increase in index size: Then b ij with the maximum  X  ij is selected as the next bitmap to be added, and the benefit ratios  X  ij are recom-puted.

Lemma 2 ([15]). The greedy algorithm finds a set B that forms a 1 / 2 (1  X  1 / e )  X  approximation to the optimal value of P q  X  Q F ( B,q ) .
The problem and the algorithm for selecting precomputed posting lists is similar in spirit to the one for bitmaps. Given a set of precomputed lists P = { p } ij , where p ij is the indi-cator variable representing whether the results of query t were precomputed, we denote by F ( P,q ) the cost of evalu-ating query q given P . We assume q is evaluated using the optimal set of posting lists minimizing the evaluation cost. Adding an extra precomputed list p to P can obviously only reduce F , but at the cost of storing a new list of size | L
Note that although precomputed lists can encode conjunc-tions of more than two terms, the utility of precomputed lists drops sharply with each new term, as the list requires that all of the terms be present in the query. We found that in the AOL query log (described in Section 6) the 100 most frequent three-term conjunctions appear in only one quarter of the 100 most frequent two-term conjunctions. Moreover, since the number of possible multi-term conjunctions grows exponentially with the number of terms in a query, consider-ing them during index construction and during query evalu-ation becomes computationally expensive. We thus consider only precomputed lists of two terms that capture most of the benefit of such lists.

We can again describe the optimization problem as fol-lows: An argument similar to the one in the last section shows that the objective function remains submodular and thus the same submodular function optimization technique de-scribed in [15] can be applied to quickly obtain a constant approximation to the optimum solution. Given a set of al-ready selected precomputed lists P , we sort the potential precomputed lists by: and select the precomputed list p ij that maximizes  X  0 ij
Lemma 3 ([15]). The greedy algorithm finds a set P that forms a 1 / 2 (1  X  1 / e )  X  approximation to the optimal value of P q  X  Q F ( P,q ) .
Unfortunately, we cannot solve the optimization problem for selecting both bitmaps and precomputed list as above. The difficulty arises from the fact that while precomputed lists can carry bitmaps, we can only add bitmaps to the pre-computed lists that are already selected. That is, the benefit of adding a bitmap to a precomputed list increases (becomes positive) after adding that precomputed list, violating sub-modularity requirement of the greedy algorithm [15]. Since we are not aware of an alternative optimization technique for achieving constant approximation to the optimum solution for such a problem, we resort to a heuristic approach.
One strategy could be to partition the budget between the two methods, and use each of the above two algorithms to first select precomputed lists and then bitmaps (some of which are added to the precomputed lists). However, deciding upon the budget fraction allocated to precomputed lists and to bitmaps may be hard, as the fraction depends on the distribution of the posting list lengths as well as on the query workload.

We thus use a hybrid algorithm that in each step selects a precomputed list or a bitmap that maximizes marginal ben-efit relative to the precomputed lists and bitmaps selected so far. That is, at each step we select either b ij or p has the maximum marginal benefit given by Equations 1 and 2. To make the marginal benefit functions  X  ij and  X  directly comparable, they have to be normalized according on the number of bits used for each bitmap and posting in precomputed lists. Let the number of bits per posting used for a bitmap be  X  1 (naturally,  X  1 = 1), and the number of bits per posting in a precomputed list be  X  2 (  X  2 is the size of the  X  docid, payload  X  tuple, in our experiments  X  2 = 32 bits). Then, we divide  X  ij by  X  1 and  X  0 ij by  X  2 .
Given a query q and its matching posting lists our goal is to select a subset L of the lists to use for query evaluation. When there are no precomputed lists or bitmaps present, this algorithm has no choice but to use one list per query term. However, when additional information in the form of bitmaps or precomputed lists is available some lists may not be necessary.
In the case of bitmaps, we can formally state the problem facing the algorithm as follows. We are given a set of query terms t 1 ,t 2 ,...,t n and a set of postings lists associated with each term L 1 ,L 2 ,...,L n . In addition, the algorithm has access to the association matrix B where b ij = 1 if there is a bit for term t j in list L i  X  X  bitmap. Our goal is to find a subset of the lists that minimizes the query cost, yet covers all of the terms. Formally, let L  X  { L 1 ,L 2 ,...,L n } be the set of lists that we will be using for query evaluation. L covers the query q if and only if: Then the goal is to find L that covers q and minimizes F ( B,q ). Finding the optimum subset of lists is an NP-complete problem, which follows via a simple reduction from the set cover problem. We therefore use a greedy algorithm to select the best subset of the lists (Algorithm 2). Algorithm 2 Query rewrite algorithm using bitmaps 1: Assume that | L 1 | X | L 2 | X  ...  X | L n | 2: Unmark terms t 1 ,t 2 ,...,t n 3: L X  X  X  4: for i  X  1 to n do 5: if t i is unmarked then 6: L X  X  X  X  L i } 7: Mark t i 8: for j  X  i + 1 to n do 9: if b ij = 1 then 10: Mark t j 11: return L
The algorithm begins with all of the query terms un-marked. It proceeds by examining lists in increasing order of their lengths, since our cost function (Section 3.1) has a monotone dependence on the length of the lists  X  processing longer lists takes more time. If a given list L is unmarked, it is marked and added to L . The algorithm then checks to see if any of the longer unmarked lists are present in the bitmap of L (and can hence be evaluated using only the information in L ). It marks all of the lists that satisfy this condition and continues with the next shortest unmarked list. Although this greedy heuristic carries no formal optimization guaran-tees, it performed well in practice (see Table 1).
In the case of precomputed lists, the approach is nearly identical to the one described in the previous section. Our goal is to find the set of lists that minimize the cost func-tion and jointly cover all of the query terms. Algorithm 3 begins with all of the query terms unmarked and examines the posting lists in increasing order of their lengths. If a given list for term t i is unmarked, the algorithm marks it and looks for a precomputed list of term t i and another un-marked term t j . If found, the precomputed list L ij is added to L , otherwise L i is added.
 Algorithm 3 Query rewrite algorithm using precomputed lists 1: Assume that | L 1 | X | L 2 | X  ...  X | L n | 2: Unmark terms t 1 ,t 2 ,...,t n 3: L X  X  X  4: for i  X  1 to n do 5: if t i is unmarked then 6: L  X  L i 7: Mark t i 8: for j  X  i + 1 to n do 9: if p ij = 1  X  t j is unmarked then 10: L  X  L ij 11: Mark t j 12: break 13: L X  X  X  X  L } 14: return L
When both bitmaps and precomputed lists are available, we use a hybrid algorithm that first invokes Algorithm 3 to identify precomputed lists and then invokes Algorithm 2 for removing some of these lists that are covered by bitmaps in shorter lists. The rationale for first looking for precom-puted lists is the higher potential benefit of identifying a pre-computed list that becomes the shortest list for the query, therefore minimizing | L 1 | (the first component of our cost function defined in Section 3.1).
We conducted series of experiments to evaluate our algo-rithms. All experiments were performed on a Linux-based 8-core 1.8GHz server with 16GB memory. Our index construc-tion and retrieval algorithms were implemented as single-threaded Java applications. We report in memory list access latencies measured after query rewrite and after preloading all posting lists into memory, averaged over several runs. We focus on the evaluation of in memory indexes since the strict requirements of high-throughput and low-latency, combined with the fact that today X  X  commodity server machines have main memory that can exceed the disk capacities of a decade ago, makes disk-based indexes rarer for the large scale appli-cations such as web search [9] or online advertising platforms that are the focus of our work. Although typical indexes are much larger than the memory capacity of a single ma-chine, the indexes are typically divided into many partitions (shard) so that each partition fits into memory. The intent of our experiments is to highlight the benefits of precompu-tation in a single index partition.

We indexed the TREC WT10g corpus consisting of 1.68 million web pages. We extracted the textual content of the documents and discarded HTML tags. We built an inverted index where each posting contains a docid of four bytes and variable size payload containing bitmaps. Bitmap sizes vary from 0 to 32 bits, rounded up to the next byte boundary. The size for the basic index (with no precomputed lists or bitmaps) was 1.5GB. In our experiments we allowed for pre-computation budget of up to three times the index size, which means that our largest index of 6GB (4.5GB occu-pied by precomputed results) still fits in memory.

The average query latency we measure directly translates to hardware costs of serving a query stream. The average latency of a query over the original index was 2.5 millisec-onds. This latency corresponds to 1 on the y-axis in the figures showing query latency.

For query workload, we used the AOL query log [20]. We sorted all queries according to their timestamps and dis-carded queries containing non-alphanumeric characters, as well as all additional information contained in the log be-yond query strings. The resulting 23.6M queries were split into training and testing sets. The training set consists of the first 21M queries from the AOL log, spanning 2.5 months. The testing set is a sample of 50K queries from the remain-ing 2.6M queries, spanning the following two weeks. The training set was used for creating indexes while the testing set was used for query evaluation.

Using the (properly anonymized) AOL query log has be-come very common in the research community, despite the controversy surrounding its release and subsequent with-drawal, as it is the largest and the most recent of the publicly available query logs. To make our results reproducible, we have chosen to use the AOL log, and not a proprietary query log.
We first examine the effects of each of the two precomputa-tion techniques alone and then study their combination. We allowed for a memory budget equal to the size of 25% of the original index, for precomputed results. We then evaluated the benefit of allocating this budget for (1) bitmaps only, (2) precomputed lists (the baseline), and (3) both bitmaps and precomputed lists using the hybrid algorithm (Section 4.3). The ratio between the average query latency when using the index with precomputed results and the average latency using the original index is depicted in Figure 3. The figure shows that precomputed lists alone reduce the average query latency by 32% while the bitmaps alone by 41%. A combination of the two techniques achieves an even higher latency reduction of 53%.

We note that the above is not a completely fair compar-ison, as our algorithm only allows for pairwise intersection lists, while a bitmap may contain information about multiple terms. In order to demonstrate that this is not a limitation, we observe that going from pairwise intersections to triples does not reduce the size of the most frequently accessed lists significantly. The total number of postings in the lists for the top 1000 most frequent bigrams is 173.9 million, whereas the total number of postings in the top 1000 trigram lists is 169.9 million, a reduction of only 2.4%. At the same time, the top 1000 trigram lists cover less than one third of the queries covered by the top 1000 bigram lists. Thus, not al-lowing for larger intersection lists is not a major limitation. Furthermore, this data suggests that the top trigrams have a significant overlap with each other, and, we find this to be the case. Of the top 1000 trigrams 92.5% share a bigram with at least one other trigram in the list. This is exactly the situation where the bitmap approach is superior to simple intersection lists.

We next evaluate two strategies of allocating the shared memory budget for bitmaps and precomputed lists: (1) al-locating a fixed fraction of the memory budget for bitmaps and precomputed lists, first selecting precomputed lists and then bitmaps using algorithms described in sections 4.1 and 4.2; and (2) bitmaps and precomputed lists simultaneously using the hybrid algorithm (Section 4.3). The ratio between the average query latency when using the index with pre-computed results and the average latency using the original index is depicted in Figure 4. It is evident that the hybrid index construction algorithm successfully finds the optimal allocation without the need to decide on how to partition the memory budget between bitmaps and precomputed lists.
Finally, we use the index constructed using the hybrid algorithm and compare the fraction of queries that benefit from at least one precomputed list or at least one bitmap as a function of query MRIS (defined in Section 3.2). Figure 5 shows that while queries with low MRIS are effectively opti-mized using precomputed lists, as MRIS increases their effec-tiveness drops to almost zero. The effectiveness of bitmaps, on the other hand, is independent of the MRIS, thus their marginal benefit (captured by the hybrid algorithm) rises with MRIS.
We next evaluate the marginal benefit of allocating mem-ory for precomputed results using the hybrid algorithm. Fig-ure 6 shows the average query latency as a function of the precomputation budget, from 0% (the original index without precomputation) to 300% (precomputed results occupy 3/4 of the index). The figure shows that allocating as little as 3% of additional memory for precomputed results decreases average query latency by 25%. As the index grows to twice the original size, the latency decreases by almost 2/3. The benefit of further increasing precomputation budget is rela-tively low.

Note that although we experimented with encoding term co-occurrences with single bits (  X  1 = 1), the results suggest that using multiple-bit encodings (e.g., to indicate terms proximity or term weights) would yield lower but still favor-able index size vs. latency tradeoff. For example, for  X  25% latency decrease would grow by the factor of 4, that is, the index would increase by 12% only.
Many query streams, such as user queries to search en-gines, follow a  X  X eavy tail X  distribution  X  queries that have never been seen before comprise as much as half of all ar-riving queries [2]. It is thus important to not overfit the training data. To evaluate the effect of precomputation on long tail queries, we identify all queries in the test set that did not appear in the training set. These queries comprise 46% of our test set. Figure 7 shows the latency of all queries and compares it to that of the long tail queries, with and without precomputation. Note that without precomputa-tion the average latency of long tail queries is lower by 22% than the average latency of all queries, since the posting lists
Figure 3: Query latency reduction with different pre-
Figure 4: Query latency reduction as a function of mem-
Figure 5: Fraction of queries optimized by at least one
Figure 6: Query latency reduction with increasing pre-of terms in long tail queries tend to be shorter than aver-age. This could possibly happen due to higher fraction of rare terms (e.g., names, foreign words, misspellings) in these queries. Yet, although the potential benefit from precompu-tation is lower for the long tail queries, and despite these exact queries did not appear in the training set, precompu-tation reduces their average latency by 33%.

To further demonstrate that precomputed results do not overfit the training we fix index and examine the average query latency of AOL queries spanning two weeks. Figure 8 shows that the average latency does not change, indicat-ing that in this timeframe, while the query mix might have changed, the common subqueries remain the same and thus there is no degradation in performance. We conclude that for web search query logs, precomputation successfully cap-tures common subqueries submitted in the future.
We next evaluate how well the greedy query rewrite al-gorithm performs compared to the optimal algorithm. We identify the optimal query rewrite by evaluating our cost function on all possible rewrites given the index and select-ing the one with the lowest cost. Table 1 summarizes the results for different precomputation budgets ( X  X vg. lists X  stands for the average number of posting lists used to com-pute query results). Expectedly, as the precomputation bud-get increases, the quality of our heuristic approximation goes down, albeit only slightly. Even when half of the index con-tains precomputed results, the evaluation cost due to heuris-tic approximation is only two percent higher than with the optimal rewrite.
 Avg. lists after heuristic rewrite 2.49 2.13 1.92 Avg. lists after optimal rewrite 2.48 2.12 1.90 Table 1: Query rewrite performance for precompu-tation budget of 3%, 25%, and 100% of the original index size.
Figure 7: Query latency reduction of long tail queries
Figure 8: Query latency reduction as a function of time
While in this work we focus on query evaluation costs, and not on efficient index construction, we report index construc-tion overhead using straightforward batch implementation of the algorithms we describe. We used a single-threaded code that used up to 16G of memory.

Recall that the TREC WT10g corpus we index consists of 1.68 million documents of average length 217 (after dis-carding non-textual content), corresponding to 365 million postings. Index inversion takes about 50 minutes. For se-lecting bitmaps, precomputed lists, and the hybrid approach we used the same code that could be configured to imple-ment either approach. For precomputation budget of 25%, the analysis of the query log (21M training queries) and the greedy submodular optimization take about 160 minutes, while adding the selected precomputation lists and bitmaps to the index takes additional 13 minutes. Thus, the total overhead of adding precomputed results to the index is of the same order as the time it takes to construct the standard inverted index. Obviously, for a partitioned index, index construction can be parallelized across partitions.
To the best of our knowledge, this is the first work that uses the posting list payloads to encode term co-occurrence. Several other works explored using payloads for different kinds of precomputation. In [25] the authors propose to in-dex phrase queries using a combination of a nextword and a phrase index, achieving significant reduction in the query evaluation latency at the expense of small increase of the index size. In [11] the authors explore the use of the posting payloads to improve processing of phrase queries by using al-gebraic signatures of the content preceding the current post-ing. In [27] the authors propose to precompute and store in the index term proximity information, and then use it to speed-up retrieval.

There is an extensive line of works on list intersection techniques, [4, 5, 8, 10] to name a few. These works mainly focus on the worst-case complexity of computing intersec-tions of arbitrary lists. Conversely, in this paper our goal is optimizing query performance for the given index and query workload, exploiting the fact that some lists are more likely to be intersected than the others.

One of the first works that explored the use of precom-puted posting lists was done by Long and Suel [18]. They proposed a three-level caching strategy, where the second level consisted of precomputed posting lists of frequently occurring pairs of terms. A similar strategy was also pro-posed in the context of P2P search [22]. In [16], the au-thors show how to use precomputed posting lists together with early termination in order to improve the query per-formance of information retrieval systems. The contribution of that work is combining these two techniques using rigor-ous theoretical analysis. In addition, the authors performed empirical tests on the TREC GOV2 data set and on real web queries showing the performance gains of their early termination method.

Several authors have focused on improving top-k query performance through optimized DAAT and TAAT query evaluation algorithms (e.g. [7, 23]). These algorithms fo-cus on vector space model queries and use upper bounds on term weights to skip posting entries that are guaran-teed to not influence query results. Turtle and Flood [23] propose both TAAT and DAAT versions of the max score algorithm. Broder et. al [7] propose the WAND (Weighted AND) DAAT algorithm, which uses upper bounds to reduce the number of full score computations. In [1] weight quan-tization is used to reduce the evaluation cost at the expense of slight decrease in accuracy.

Another related line of work is the use of fancy lists to improve query performance [17, 21]. Fancy lists are small posting lists that contain the documents with the highest score for each term in the dictionary. In [21], for instance, a new DAAT algorithm that improves upon the DAAT max score [23] is proposed. This algorithm uses fancy lists to set a better initial threshold for DAAT max score .
In this paper we introduced the concept of bitmaps for optimizing query evaluation over inverted indexes. Bitmaps allow for a flexible way of storing information about term co-occurrences and complement the traditional approach of precomputed lists. We proposed a greedy procedure for the problem of selecting bitmaps and precomputed lists that is a constant approximation to the optimal algorithm. The analysis of bitmaps and precomputed lists over the TREC WT10g corpus shows that the hybrid approach achieves 25% query performance improvement for 3% growth in index size and 71% for 4-fold index size increase. An interesting future work is exploring the application of bitmaps to non-Boolean top-k retrieval. In this case one needs to store extra infor-mation in the bitmaps, such as weights in the case of vec-tor space queries and term positions in the case of phrase queries. [1] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space [2] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, [3] R. Baeza-Yates and B. Ribeiro-Neto. Modern [4] R. A. Baeza-Yates. A fast set intersection algorithm [5] J. Barbay, A. L?pez-Ortiz, and T. Lu. Faster adaptive [6] S. Brin and L. Page. The anatomy of a large-scale [7] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and [8] J. S. Culpepper and A. Moffat. Compact set [9] J. Dean. Challenges in building large-scale information [10] B. Ding and A. C. K  X  onig. Fast set intersection in [11] C. du Mouza, W. Litwin, P. Rigaux, and T. Schwarz. [12] M. Fontoura, E. J. Shekita, J. Y. Zien, [13] H. Garcia-Molina, J. Ullman, and J. Widom. Database [14] S. Heinz and J. Zobel. Efficient single-pass index [15] A. Krause and C. Guestrin. A note on the budgeted [16] R. Kumar, K. Punera, T. Suel, and S. Vassilvitskii. [17] X. Long and T. Suel. Optimized query execution in [18] X. Long and T. Suel. Three-level caching for efficient [19] S. Melnik, S. Raghavan, B. Yang, and [20] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [21] T. Strohman, H. R. Turtle, and W. B. Croft.
 [22] B. B. Sudarshan, S. Chawathe, V. Gopalakrishnan, [23] H. Turtle and J. Flood. Query evaluation: strategies [24] S. Whang, C. Brower, J. Shanmugasundaram, [25] H. E. Williams, J. Zobel, and D. Bahle. Fast phrase [26] I. Witten, A. Moffat, and T. Bell. Managing [27] H. Yan, S. Shi, F. Zhang, T. Suel, and J.-R. Wen. [28] J. Zobel and A. Moffat. Inverted files for text search
