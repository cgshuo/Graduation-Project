 Barnab  X as P  X oczos bapoczos@cs.cmu.edu Zoubin Ghahramani zoubin@eng.cam.ac.uk Jeff Schneider schneide@cs.cmu.edu Measuring dependence between random variables is an important problem in statistics, information theory, and machine learning with a wide range of applications in science and engineering. The most well-known de-pendence measure is the Shannon mutual information, which has found numerous applications recently. Al-though this is the most popular dependence measure, it is only one of the many other existing ones. In par-and Tsallis- X  mutual information ( Tsallis , 1988 ). Other interesting dependence measures include the maximal correlation coefficient ( R  X enyi , 1959 ), kernel mutual information ( Gretton et al. , 2003 ), the general-ized variance and kernel canonical correlation analysis ( Bach , 2002 ), the Hilbert-Schmidt independence crite-rion ( Gretton et al. , 2005 ), the Schweizer-Wolff mea-sure ( Schweizer &amp; Wolff , 1981 ), and the distance based correlation ( Sz  X ekely et al. , 2007 ).
 There is a tremendous list of dependence applications. They have been used, for example, in causality detec-tion, feature selection, active learning, structure learn-ing, boosting, image registration, independent compo-nent and subspace analysis. For more applications and references, please see the supplementary material. One reason why so many dependence measures have been defined in the literature is that the problem is challenging and researchers and practitioners are not satisfied with the available measures and estimators ( Fernandes &amp; Gloor , 2010 ). As Schweizer &amp; Wolff ( 1981 ) formalized in their dependence axioms, a good dependence measure I has to have several properties. The most important ones are as follows. (i) Depen-dence I ( X ) is defined for X = ( X 1 , . . . , X d )  X  R d -dimensional random variables. (ii) I ( X 1 , . . . , X is invariant to permutation. (iii) 0  X  I ( X ), and I ( X ) = 0 iff ( X 1 , . . . , X d ) are independent variables. (iv) I ( X 1 , . . . , X d ) is invariant to strictly increasing transformation of X i variables. For more discussion on these axioms, see the Appendix. Among the above mentioned dependence measures, only the R  X enyi, Tsal-lis information, and the Schweizer-Wolff measure is in-variant to strictly increasing transformations. In addition to these constraints on the dependence measure, we also want an efficient estimator that is consistent, robust to outliers, has fast convergence rate, and can be used in high-dimensions too. De-pendence estimation is very challenging, especially in nonparametric situations when we cannot assume that the observations have an underlying density function belonging to some parametric family. Many of the above mentioned dependence measures can be defined as some functionals of the density, thus an obvious way for their estimation would be to estimate the densities first. The density function, however, is a nuisance pa-rameter in our case, and its estimation X  X specially in higher dimensions X  X s known to be very difficult. Due to these difficulties, all the existing dependence estimators have their own shortcomings. For exam-ple, the bound on the convergence rate of the R  X enyi and Tsallis information estimator ( P  X al et al. , 2010 ) suffers from the curse of dimensionality. The avail-able reproducing kernel based dependence measures are not invariant to strictly increasing transformation of the X i marginal random variables. The estimator of Sz  X ekely et al. ( 2007 ) is not robust; one single large enough outlier can arbitrarily ruin the estimator. The main contributions of the paper are as follows. (i) We introduce a new dependency measure I that satis-fies the above listed axioms. (ii) We prove that I can be efficiently estimated, and the calculation of the es-timator is simple. The estimator is consistent, robust to outliers, and uses rank statistics only. (iii) We also provide an upper bound on the rate of convergence and derive a test of independence. This bound shows that the estimator can be efficiently used in large di-mensions too.
 Our main idea is to combine empirical copula trans-formations with reproducing kernel based divergence estimators. We will show that the empirical copula transformation only slightly affects the convergence rate, but the resulting dependence estimator possesses all the above mentioned required properties. The pro-posed method is illustrated in Figure 1 .
 One might wonder why it is important for a depen-dence measure to be invariant to strictly increasing transformations of the marginal variables. One reason for this is that in many scenarios we need to com-pare the estimated dependencies. This is the case for example in feature selection and low-dimensional em-bedding of random variables. In these problems we can think of dependence as a  X  X istance X  between ran-dom variables in the sense that when the dependence is large, then the random variables are  X  X lose X  to each other, and when the dependence is small, then the variables are far. However, if certain variables are measured on different scales, then this distance can be much different from the distance using other scales. As a result, it might happen that different features would be selected by the feature selection algorithm if we measured a quantity e.g. in grams, kilograms, pounds, or if we used log-scale. This is an odd situa-tion that can be avoided with dependence measures that are invariant to strictly increasing transforma-tions of the marginal variables. As an application, we will show how the proposed dependence measure can be used for feature selection and low-dimensional embedding of distributions.
 The proofs can be found in the supplementary mate-rial. There we also discuss the robustness properties of the estimators and show how to use them in inde-pendence tests.
 Notation : In the rest of the paper X  X  P will de-note that the random variable X has distribution P . E ( X ) and  X  ( X ) stand for the expectation and stan-dard deviation of X , respectively. For a random vari-able X  X  R ,  X [ X ] denotes the standardized variable, that is,  X [ X ] . = ( X  X  E [ X ]) / X  ( X ), which has zero mean and unit variance. U [ a, b ] stands for the uniform dis-tribution in the interval [ a, b ]. X 1: m is shorthand no-tation for the set of random variables { X 1 , . . . , X m The cardinality of a set S is denoted by | S | . In this section we review some important properties of the Maximum Mean Discrepancy (MMD), which is a quantity used to measure the distance between dis-tributions ( Borgwardt et al. , 2006 ; Fortet &amp; Mourier , 1953 ). An appealing property of this quantity is that it can be efficiently estimated from independent and identically distributed (i.i.d.) samples.
 Definition 1. Let F be a class of functions, P , Q be probability distributions. The MMD between P and Q on the function class F is de ned as follows,
M [ F , P, Q ] . = sup Let H = { f : X  X  R } be a reproducing kernel Hilbert Space (RKHS) with feature map  X  ( x )  X  H ( x  X  X ), that  X  ( x ) = k (  X  , x ), and f ( x ) =  X  f,  X  ( x )  X  H called the reproducing property of the RKHS. Later we will also need the definition of universal kernels. Definition 2 (Universal kernel) . A kernel k : X  X  X  X  R is universal whenever the associated RKHS H is dense in C ( X ) , the space of bounded continuous functions over X , with respect to the L  X  norm. Steinwart ( 2001 ) has shown that the Gaussian and Laplace kernels are universal. Let  X  [ P ] . = E X  X  P [  X  ( X )] = for this quantity to exist is E X  X  P; X  X   X  P k ( X , X  X  where X and X  X  are independent variables having dis-tribution P .
 For general F function sets, M [ F , P, Q ] can be difficult to calculate and is not even symmetric in P and Q . Nonetheless, when F is a unit ball of RKHS H , then for all f  X  X  we also have that  X  f  X  X  , which implies that M [ F , P, Q ] = M [ F , Q, P ]. Furthermore, in this case M 2 [ F , P, Q ] has a simple form that makes efficient estimations possible. This is stated formally in the following lemma ( Borgwardt et al. , 2006 ).
 Lemma 3. When F is a unit ball of RKHS H and  X  [ P ] &lt;  X  ,  X  [ Q ] &lt;  X  , then M 2 [ F , P, Q ] =  X   X  [ P ]  X   X  [ Q ]  X  2 H = E where X and X  X  have distribution P , Y and Y  X  have distribution Q , and these random variables are all in-dependent from each other.
 In the remainder of the paper we will always assume that F is a unit ball of RKHS H . Let X 1: m = ( X 1 , . . . , X m ) be an independent and identically dis-tributed (i.i.d.) sample drawn from distribution P , ple with distribution Q .
 A biased (but asymptotically unbiased) estimator for M [ F , P, Q ] can be easily given using the law of large numbers:
M
An unbiased estimator for M 2 [ F , P, Q ] (when m = n ) has also been derived in Borgwardt et al. ( 2006 ): which is a one sample U -statistic with h (  X  i ,  X  j ) . k ( X i , X j ) + k ( Y i , Y j )  X  k ( X i , Y j )  X  k ( X  X  random variables. From the r.h.s. of Lemma 3 , one can unbiasedness of the estimator M 2 u [ F , X 1: m , Y 1: m Below we review a few important properties of the copula of multivariate distributions that we will use in our work ( Nelsen , 1998 ).
 The copula plays an important role when we study the dependence among random variables. The marginal variables X 1 , . . . , X d are independent from each other, if and only if the copula distribution is the multivari-ate uniform distribution. In turn, we can measure the dependence of the X 1 , . . . , X d random variables by measuring how far the copula distribution is from the uniform distribution. The copula contains all the information that we need to measure dependence, and it is invariant to any nonlinear strictly increasing trans-formations of the marginal variables.
 The copula can be defined by the Sklar X  X  theorem ( Sklar , 1959 ) as follows. Let X = ( X 1 , . . . , X d ) be a random variable. Denote the marginal cdf X  X  of X j by F j : R  X  [0 , 1]. Sklar X  X  theorem states that a multivariate cumulative distribution function H ( x 1 , . . . , x d ) = Pr( X 1  X  x 1 , . . . , X d  X  x where C is a unique distribution function on the range of the F i cdf functions. This distribution function is called the copula of the joint distribu-tion H . The distribution of the copula C is the same as the joint distribution of Z = ( Z 1 , . . . , Z d F ( X ) = ( F 1 ( X 1 ) , . . . F d ( X d ))  X  R d random vari-ables. When the F i cumulative distribution func-tions are invertible, then F ( X ) have uniformly dis-tributed marginal distributions on [0 , 1], and the cop-ula distribution can be calculated as C ( y 1 , . . . , y H ( relation of the joint distribution H , marginal distribu-tions F i , and copula C is illustrated in Figure 2 . Let U = ( U 1 , . . . , U d )  X  [0 , 1] d be a random vari-able with uniform distribution on the d -dimensional unit cube, U  X  U [0 , 1] d . We define the dependence among continuous random variables X 1 , . . . , X d as the MMD distance between the joint copula and the d -dimensional uniform distribution : Definition 4. Let x 1 , x 2  X  R . A function g : R  X  R is strictly increasing, if g ( x 1 ) &lt; g ( x 2 ) for all x It is easy to see that I ( X 1 , . . . , X d )  X  0, and I ( X 1 , . . . , X d ) = I ( g 1 ( X 1 ) , . . . , g d ( X g i strictly increasing functions. In other words, formations of the marginal variables.
 The following lemma states that I ( X 1 , . . . , X d ) is in-deed a well-defined dependence measure when kernel k is universal.
 Lemma 5. Let the kernel k be universal on [0 , 1] d  X  X 1 , . . . , X d are independent of each other. In what follows we will provide a consistent estimator for I ( X ) = I ( X 1 , . . . , X d ). Let k : R d  X  R d  X  R a kernel function of RKHS H , and let Z . = F ( X ) be a random variable drawn from the copula. Introduce the following terms: Thanks to Lemma 3 , it is easy to see that I ( X ) = M 2 ( F , P Z , P U ) =  X   X  [ P Z ]  X   X  [ P U ]  X  E E Our goal is to estimate I ( X ) using the X 1: m i.i.d. sam-ple. This expression is the expected value of the kernel k evaluated in random variables drawn from the uni-form and the copula distributions. Assume that we already have a Z 1: m i.i.d. sample from the copula dis-tribution. For simple kernel functions, the expectation w.r.t. the uniform distribution has a simple form. For example, when we use the Gaussian kernel, we have the following unbiased estimator for I 2 ( X ): which can be expressed by the erf Gauss error function. For more complicated kernels, however, these integrals can not be calculated analytically, therefore we need to approximate them by sampling. In what follows we will investigate this case.
 Let U 1: n = U 1 , . . . , U n be an i.i.d. sample drawn from the U [0 , 1] d distribution, and let X , X 1 ,. . . , X m i.i.d. samples having distribution P X . The F 1 , . . . , F d distribution functions are unknown, but we can es-timate them efficiently using the empirical distribution functions. For x, x j  X  R and 1  X  j  X  d , let b
F j ( x ) . = b F j ( x ; X j 1: m ) . = We call the maps F ,  X  F the copula transformation , and the empirical copula transformation , respectively. The sample ( b Z 1 , . . . , b Z m ) . = ( b F ( X 1 ) , . . . , is called the empirical copula ( Dedecker et al. , 2007 ). Note that the j -th coordinate of b Z i (1  X  i  X  m ) equals where rank( x, A ) is the number of elements of A less than or equal to x . Also, observe that the random vari-ables b Z 1 , . . . , b Z m are not even independent. Nonethe-less, as we will see from Lemma 7 , the empirical cop-sample ( Z 1 , . . . , Z m ) . = ( F ( X 1 ) , . . . , F ( X copula distribution of P X . Using ( 2 ), we have that From ( 1 ), we can also see that In these expressions Z 1: m is not available to us. We es-timate them using the empirical copula, b Z j . = b F ( X j = 1 , . . . , m . An estimator for I 2 ( X ) can be given by b I ( X 1: m ), where m ( m  X  1) b I 2 u ( X 1: m ) . = m ( m  X  1) M 2 u [ F , b  X  To calculate this quantity, we only need the ranks of the marginal variables in the sample. Note that b I ( X 1: m ) is not an unbiased estimator of I ( X ), but we keep the notation b I 2 u to denote that it is derived from the estimator M 2 u .
 Using the definition of M b , we can also propose an-other estimator for I ( X ): b
I Both estimators are extremely simple to implement requiring only kernel evaluations on the transformed data and the uniform variables. One can also see that the estimators are robust assuming k is bounded in [0 , 1] d  X  [0 , 1] d (but can be unbounded outside of this region, e.g. polynomial kernel). Thanks to the empiri-cal copula transformation, we only need rank statistics ( b
Z 1: m ) in the estimation, but the actual values of X 1: m sample points are not used. The contribution of one single sample point is diminishing in the estimator as we increase the sample size. Therefore, one arbitrar-ily large outlier sample point cannot ruin the statistics arbitrarily badly. For more discussion on this, see the Appendix.
 In what follows we will analyze the theoretical proper-ties of these estimators. Assume that the kernel func-tion k (  X  , z ) is uniformly Lipschitz continuous on [0 , 1] i.e. there exists L &gt; 0 such that for all z , z 1 , z 2 we have that | k ( z 1 , z )  X  k ( z 2 , z ) | X  L  X  z 1  X  ical example is the Gaussian kernel, for which it holds that there exists L &gt; 0 such that for all z , z 1 , z 2 [0 , 1] d exp(  X  Lemma 6. For all z i  X  [0 , 1] d , 1  X  i  X  4 , we have that | k ( z 1 , z 2 )  X  k ( z 3 , z 4 ) | X  L  X  z 1  X  z 3  X  + L  X  The effect of the empirical copula transformation can be studied by a version of the classical Kiefer-Dvoretzky-Wolfowitz theorem due to Massart; see e.g. Devroye &amp; Lugosi ( 2001 ). As a simple implication of this theorem, one can show that b F is a consistent estimator of F , and the convergence is uniform: Lemma 7 (Convergence of the empirical copula) . Let X 1 , . . . , X m be an i.i.d. sample from a probability dis-tribution over R d with marginal cdf 's F 1 , . . . , F d F ( X ) be the copula de ned above, and let b F ( X 1: m ) be the empirical copula transformation. Then, for any  X   X  0 ,
Pr Let 0  X  k ( x, y )  X  K be a bounded kernel function. The following theorems state the almost sure consis-tency of the dependence estimators, and provide upper bounds on the rate of convergence.
 Theorem 8 (Almost sure consistency) . Almost surely we have that From the below theorem it follows that when n grows fast enough, then b I b is almost surely consistent as well. Theorem 9 (Almost sure consistency) . Let n = g ( m ) for some function g such that lim m  X  X  X  g ( m ) =  X  . Al-most surely it holds that As these bounds show, the proposed dependence esti-mators can be used in high-dimensions as well; they do not suffer from the curse of dimensionality. Based on these estimators, one can derive independence tests too. For details, see the Appendix. The above defined I ( X ) dependence measure is in-variant to strictly increasing transformations of the marginal variables. In this section we discuss the ben-efits of this property in the feature selection problem. Let us have d real valued features { X 1 , . . . , X d } , and a target value Y . Numerous feature selection meth-ods use dependence estimation for selecting the most relevant features to predict the target value Y . If we want to select h features, then one obvious approach would be to select those h features that together have the highest dependence with Y . This subset selection problem, unfortunately, is very difficult. Therefore, several approximations and heuristics have been pro-posed. For example, according to the so-called max-relevance criterion ( Peng &amp; Ding , 2005 ), our goal is to select a feature set S  X  { X 1 , . . . , X d } , which max-imizes the average dependence between the features and the target: This approach might select highly redundant features, i.e. the dependence among these features could be large. This redundancy can be measured by the ex-pression When two features highly depend on each other, then probably we do not lose too much if we remove one of them. Therefore, our goal is to maximize relevance while minimizing the redundancy among the features b S = arg max All we need is a good estimator for I ( X i , X j ) and I ( X i , Y ) dependencies. Equation ( 3 ) and ( 4 ) objec-tives are popular tools for feature selection. Here we will not discuss the advantages and disadvantages of them. We, however, would like to point out that when someone uses objectives that involves dependence es-timation, then we want these dependencies to be in-variant to strictly increasing transformations of the marginal variables. We illustrate the theoretical contributions of this pa-per through a series of numerical experiments demon-strating properties of the copula-based kernel depen-dency measure.
 The M ( F , P X , directly, without copula transformation, to estimate dependence. In order to use this approach, we need to generate m sample points from the product distribu-tions of the marginals. Let  X  i (1 : m ), (1  X  i  X  d ) de-note independent random permutations of { 1 , . . . , m } Then  X [ X 1: m ] . = ( X 1 can be considered as samples from the distribution. In other words, if X 1: m is stored in a d  X  m dimensional sample matrix and we inde-pendently permute the elements of each row, then the distributions of the rows (the marginal distribu-tions of X ) remain the same, but they become in-dependent from each other. For brevity, we will call the M ( F , P X , measure. 6.1. Feature Selection In this experiment we show that I ( X ) can achieve bet-ter performance in feature selection than MMD with-out copula transformation ( M ( F , P X , We constructed the following random variables: X 1  X  U [0 , 1], X 2  X  U [0 , 500], Y = 500 sin(4  X X 1 ). The task in this experiment was to choose the feature between X 1 and X 2 that contains the most information about Y . This feature is of course X 1 since Y is a deter-ministic function of it, and X 2 is independent of Y ; it does not contain any information about Y . 300 sam-ple points from the joint distrbutions of ( X 1 , Y ) and ( X 2 , Y ) are shown in Figure 3(a) and Figure 3(b) , re-spectively. The empirical copula transformed points of ( Y, X 1 ) and ( Y, X 2 ) are displayed in Figure 3(c) and Figure 3(d) . When we simply use MMD without copula transformation ( M ( F , P Y ;X i , P Y  X  P X i )), then interestingly we got that the estimated dependence be-tween Y and X 1 ( M b ( F , ( Y, X 1 ) 1: m ,  X [( Y, X 1 ) column (A) of Figure 3(e) ) was smaller than the estimated dependence between between Y and X 2 (
M ure 3(e) ). As we can see in this problem, the MMD without copula transformation could not select the right feature. However, when we used copula trans-formation, then the estimated dependence was larger between Y and X 1 than between Y and X 2 . The val-ues of b I b (( Y, X 1 ) 1: m ) and b I b (( Y, X 2 ) 1: m the (C) and (D) columns of Figure 3(e) . In this exper-iment we used Gaussian kernel with  X  = 1. 6.2. Feature Standardization A frequently used feature preprocessing step is to stan-dardize the features, that is, linearly transform them to have zero mean and unit variance ( X [ X ]). One might wonder if this simple transformation can solve the problem of Section 6.1 . Below we show an example, where that we have only two zero mean unit variance features, and the MMD feature selection method that is not invariant to the strictly increasing transforma-tions of the features selects a feature that is actually independent from the target value.
 Let U  X  U [0 , 1], X 1 . =  X [1 /U 2 ], V  X  U [0 , 1] X 2 . =  X [ V ] independent random variables, and let Y . =  X [sin(4  X X 1 )]. The variables are standardized so they have zero mean and standard variation 1. We sampled 4,000 i.i.d. observations from our observed features X 1 and X 2 . The task again was to select the feature that contains the most information about Y . The solution to this problem is X 1 again. The meanings of the columns in Figure 4 are the same as in Figure 3(e) . When we simply use MMD with-out copula transformation, then the estimated depen-dence between between Y and X 1 was smaller than between Y and X 2 ( M b ( F , ( Y, X 1 ) 1: m ,  X [( Y, X 1 and (B), respectively). The MMD without copula transformation could not select the right feature. How-ever, when we use copula transformation first, then we can see that the estimated dependence between Y and X 1 is larger than between Y and X 2 , as expected. (C) and (D) show b I b (( Y, X 1 ) 1: m ) and b I b (( Y, X respectively.
 6.3. Housing Dataset In the following experiment we study our estima-tors on the Housing dataset from the UCI repository ( Frank &amp; Asuncion , 2010 ). The dataset contains 506 instances of 14 real valued attributes. The attributes contain various features including per capita crime rate by town, full-value property-tax rate per $10000, av-erage number of rooms per dwelling, percentage of lower status of the population, median value of owner-occupied homes in $1000 X  X , etc. Our goal is to predict some of these attributes and select the most important features for this prediction. Since the dataset contains very different features, it is highly nontrivial how to scale them for feature selection when the applied de-pendence measure is not invariant to strictly increas-ing transformations of the marginals. This, however, is not an issue for our proposed dependence measure. In this experiment our goal was to predict the  X  X edian value of owner-occupied homes in $1000 X  X  X  (feature 14) using one single feature. We used m = n = 300 in-stances for training, and the rest of the data for testing. We applied Gaussian kernel (  X  2 = 1 / 12) in the estima-tors. The MMD without copula transformation chose the  X  X verage number of rooms per dwelling X  (feature 6) as the closest feature. When instead of MMD we used the proposed b I b estimator, it selected the  X  X ower status of the population X  (feature 13). To study the prediction errors of the selected features, we trained linear regressors for each feature using them as ex-planatory variables. The prediction errors on the test data are shown in Figure 5 . In this experiment the smallest error was achieved by the feature that b I b se-lected (feature 13). MMD without the copula transfor-mation selected the feature that gave only the second smallest error (feature 6).
 Low-dimensional embedding can help us visualize the pairwise dependence structure of random variables. For each feature X i , X j , we estimated the d ( i, j ) = exp(  X  I ( X i , X j )) quantities. This d ( i, j ) is large when X i , X j is independent, and small when the depen-dence between them is large. We considered them as  X  X istances X  (although the triangle inequality does not hold between them), and then applied multi-dimensional scaling to embed them into a 2d space. The Housing dataset was used in this experiment too using the same set-up as in the previous study. To es-timate the dependence between the features, we tested again b I b (Figure 6(a) ) and MMD without copula trans-formation (Figure 6(b) ). We can observe that the lo-cations of these embedded points are very different. If we applied any strictly increasing transformations to the marginal variables, it would not affect the em-bedding with copula transformation, but we would get very different results when we use MMD without cop-ula transformation. For more numerical experiments, see the supplementary material.
 We introduced a new RKHS-based dependence mea-sure that operates on the copula of continuous distri-butions. We have shown that the dependence measure is invariant to strictly increasing transformations of the marginal variables, and this property is important in feature selection and low-dimensional embedding of distributions. We also proposed estimators that are al-most surely consistent, robust, use rank statistics only, and do not suffer from the curse of dimensionality. We derived upper bounds on the rates of convergence and illustrated the theory through a series of numerical ex-periments.
 Bach, Francis R. Kernel independent component anal-ysis. JMLR , 3:1 X 48, 2002.
 Borgwardt, K., Gretton, A., Rasch, M., Kriegel, H.,
Sch  X olkopf, B., and Smola, A. Integrating structured biological data by kernel maximum mean discrep-ancy. Bioinformatics , 22(14):e49 X  X 57, 2006.
 Dedecker, J., Doukhan, P., Lang, G., Leon, J.R., Louhichi, S., and Prieur, C. Weak Dependence:
With Examples and Applications , volume 190 of Lec-ture Notes in Statistics . Springer, 2007.
 Devroye, L. and Lugosi, G. Combinatorial Methods in Density Estimation . Springer, 2001.
 Fernandes, A. and Gloor, G. Mutual information is critically dependent on prior assumptions: would the correct estimate of mutual information please identify itself? Bioinformatics , 26:1135 X 1139, 2010. Fortet, R. and Mourier, E. Convergence de lar  X eparation empirique vers la r  X eparation th  X eorique. Ann. Scient. Ecole Norm , 70:266 X 285, 1953.
 Frank, A. and Asuncion, A. UCI ma-chine learning repository, 2010. URL http://archive.ics.uci.edu/ml .
 Gretton, A., Herbrich, R., and Smola, A. The kernel mutual information. In Proc. ICASSP , 2003.
 Gretton, A., Bousquet, O., Smola, A., and Sch  X olkopf, B. Measuring statistical dependence with Hilbert-Schmidt norms. In ALT , pp. 63 X 77, 2005.
 Nelsen, R. An Introduction to Copulas (Lecture Notes in Statistics) . Springer, 1998.
 P  X al, D., P  X oczos, B., and Szepesv  X ari, Cs. Estimation of renyi entropy and mutual information based on generalized nearest-neighbor graphs. In NIPS , 2010. Peng, H. and Ding, C. Feature selection based on mu-tual information: Criteria of max-dependency, max-relevance, and min-redundancy. PAMI , 27, 2005. R  X enyi, A. On measures of dependence. Acta. Math. Acad. Sci. Hungar , 10:441 X 451, 1959.
 R  X enyi, A. On measure of entropy and information. In 4th Berkeley Symposium on Math., Stat., and Prob. , pp. 547 X 561, 1961.
 Schweizer, B. and Wolff, E. On nonparametric mea-sures of dependence for random variables. The An-nals of Statistics , 9, 1981.
 Sklar, A. Fonctions de rpartition n dimensions et leurs marges. Publ. Inst. Statist. Univ. Paris , 8:229 X 231, 1959.
 Steinwart, I. On the influence of the kernel on the consistency of support vector machines. JMLR , 2: 67 X 93, 2001.
 Sz  X ekely, G. J., Rizzo, M. L., and Bakirov, N. K. Mea-suring and testing dependence by correlation of dis-tances. Annals of Statistics , 35:2769 X 2794, 2007. Tsallis, C. Possible generalization of boltzmann-gibbs
