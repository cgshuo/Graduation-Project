 Changyou Chen 1 , 3 Changyou.Chen@nicta.com.au RSISE, Australian National University, Australia Vinayak Rao 2 vrao@gatsby.ucl.ac.uk Dept. Statistical Science, Duke University, USA Wray Buntine 3 , 1 Wray.Buntine@nicta.com.au National ICT, Canberra, Australia YeeWhye Teh 4 y.w.teh@stats.ox.ac.uk Dept. Statistics, University of Oxford, UK We list some of the notation used in this paper in Table 1 for reminder. A.1. Definitions Mixed Normalized Random Measures (MNRM) Thinned Normalized Random Measures (TNRM) A.2. Preliminary Lemmas TNRM and their variants.
 Lemma 1 below is a celebrated formula for L  X evy processes know as the L  X evy-Khintchine formula . measure  X  (d w, d  X  ) . For any measurable function f : W  X   X   X  X  X  R + , the following formula holds: of  X   X  is given by where u  X  R and i is the imaginary unit.
 known Palm formula.
 for any nonnegative function f : R +  X   X  7 X  R + , the following formula holds: where N ( f ) = R and TNRM.
 functions of bounded support on W . Then for each f  X  BM + ( W ) and each g on ( M , B ( M )) , sity e  X  f  X  .
 A.3. Normalized Generalized Gamma Processes with the following form 1 : the paper.
 following sections.
 in (6), let f : R + 7 X  X  X  R + be a measurable function, the Laplace functional of  X   X  g is given by where  X   X  g ( f ) = R be found in other references such as (James et al., 2009; Favaro &amp; Teh, 2012). relative mass (Chen et al., 2012a)), the marginal posterior is given by: where (1  X   X  ) n We have the following property for the MNRM.
 Proof First, from the definiton we have  X   X  of each random measure q rt  X   X  r using Lemma 1: that the characteristic functional of  X   X  t is The L  X evy intensity of  X   X  t is thus P # R r =1  X  r ( w/q rt , X  ) /q rt . The following two properties are proved for TNRMs.
 P r q rt  X  r (d w, d  X  ) .
 using the characteristic function of a CRM (4).
 be calculated as are marginally normalized random measure with L  X evy measures P # R r =1 q rt  X  r (d w, d  X  ). L  X evy measure formed by the random atoms of the corresponding Poisson process.
 joint distribution for {  X  t } and observations with statistics { n tw } is Now we introduce an auxiliary variable u t for each t via Gamma identity, and the joint becomes Now integrate out all the z tk  X  X  in the exponential terms we have: the Poisson process has posterior intensity of structed from it.
 C.1. Mixed Normalized Random Measures C.1.1. Posterior Inference for Mixed Normalized Generalized Gamma Processes with likelihood can be expressed as Now introduce auxiliary variables { u t } using Gamma identity, the joint becomes We assume the L  X evy measure is factorized as measures re-writing Q r = Q ( R r ) and integrating out  X  r  X  X  by applying Lemma 2 we get: TNRM without further statement.
 sampled as follows: Sampling ( s tl ,g tl ) : The posterior of ( s tl ,g tl ) is Sampling M r : The posterior of M r follows a Gamma distribution: Sampling u t : The posterior distribution of u t is: Sampling  X  : From (12), we first instantiate a set of jumps { w rk } as C.1.2. Posterior Inference for Mixed Normalized Generalized Gamma Processes with Slice for more detailed introduction of the underlying ideas.
 Starting from (11), we introduce a slice auxiliary variable v tl for each observation such that Now (11) can be rewritten as measure {N r } becomes a compound Poisson process: see Appendix C.2.2 for the derivation.
 Now the sampling goes as: Sample v tl : v tl is uniformly distributed in interval (0 ,w g Sample M r : M r follows a Gamma distribution as
Sample q rt : q rt can also be rejection sampled by using the following proposal Gamma distribution:
Sample  X  : Based on (16), the posterior of  X  is proportional to: C.2. Thinned Normalized Random Measures C.2.1. Marginal Posterior for Thinned Normalized Generalized Gamma Processes measure X  X he normalized generalized Gamma process.
 tion and statistics defined in Table 1, the marginal posterior for the TNGG is given by where in the last line P augmented space R +  X   X   X { 0 , 1 } . Given the observed data, the likelihood is given by where z rtk  X  Bernoulli( q rt ) , 0  X  q rt  X  1.
 Now introducing auxiliary variables ~u via the Gamma identity, we have Denote  X  = { 0 , 1 } X  X  X  X  X  X { 0 , 1 } L  X evy-Khintchine formula (3) and Lemma 2 we have last equation follows by applying the following result where the summation in (26) is the Taylor expansion of (1 +  X  )  X   X  1. A Marginal Sampler for TNGG We derive a marginal sampler for TNGG based on the posterior (20). To z set of jump size variables w rk  X  X  distributed as Further denote u = ( u 1 ,  X  X  X  ,u T ), and b as a length T binary vector, and denote then the first parenthesis in (21) can be rewritten as where &lt;  X  ,  X  &gt; denotes the inner produce. Based on this, the sampling goes as Sample M r : M r has a Gamma distributed posterior as variables z rtk for ( k : n  X  rk &gt; 0) using the following rule With these latent variables, sampling for other parameters goes as Sample u t : the posterior of u t has the following form: Sample q rt : the posterior of q rt follows: Sample  X  : From (20),  X  has the following posterior: C.2.2. Posterior Inference for the TNGG via Slice Sampling a threshold, thus turning the inference from infinite parameter spaces to finite parameter spaces. that Based on (23), now the joint posterior of observations and related auxiliary variables becomes L Poisson random measure {N r } becomes so the density is: of the Poisson distribution with mean A under value k .
 Further integrate out all the { z rtk }  X  X , we have C.2.3. Bound analysis in the experiments).
 notation: Also denote the last term in (32) as  X  Q r ( L r ), i.e. , We use the following inequality: Then we have the upper bound for  X  Q r ( L r ): Similarly, we have the lower bound: C.2.4. Sampling Sample v tl : v tl is uniformly distributed in interval (0 ,w g Sample z rtk : For those w rk  X  X  with observations from time t , clearly the posterior is Algorithm 1 PMMH sampling for M r and u t 1: repeat 3: Sample the Bernoulli variables z rtk  X  X  4: Use these jumps and z rtk  X  X  to evaluate the approximated likelihood (35). 5: Propose a move 7: Use these jumps to evaluate the approximated likelihood (35). 8: Do the accept-reject step using (44). 9: until converged C.2.5. Prediction in the Slice Sampler able to do prediction, we need to introduce an extra slice variable v t ( L indicator variable s t ( L sampled during the inference, sampling for v t ( L sampling for ( s t ( L because its observation x t ( L except that we need to using L t + 1 observations instead of L t .
 et al., 2006), and develop a marginal sampler for it.
 An HNGG mixture is then defined as D.1. Marginal Sampler for the HNGG  X  conditional posterior of an NGG in Lemma 5, the sampling for the HNGG now goes as follows:  X  Sampling dish index s ji for customer x ji : this follows a similar way as the HDP  X  Sampling mass parameters M and M 0 : Using Gamma priors for M and M 0 , the posterior are simply  X  Update  X  :  X  can be updated using the prediction probabilities for an NGG as the corresponding papers).
 E.1. (Lin et al., 2010) X  X  sampler that stated in their theorem: specification (which is Dir(  X  1 ( X ) , X  2 ( X ))).
 model where the transition probability doesn X  X  depend on the number of observations. should remain O (1) independent of the past.
 E.2. (Lin &amp; Fisher, 2012) X  X  sampler for these models appears to be much more straightforward than it actually is. ( H s  X  X  in their notation for the DP case) is with their model and thus is not correct.
 NICTA is funded by the Australian Government as represented by the Department of Broadband, Communi-program. VR was funded by DARPA MSEE. YWT was funded by the Gatsby Charitable Foundation. Statist. , 37(2):697 X 725, 2009.
 Report arXiv:1205.4159, ANU and NICTA, Australia, May 2012a. URL http://arxiv.org/abs/1205.4159 . modeling. In ICML . 2012b.
 Favaro, S. and Teh, Y. W. MCMC for normalized random measure mixture models. Stat. Sci. , 2012. 41(2):337 X 348, 1992.
 Stat. , 20(1):241 X 259, 2011.
 Ann. Statist. , 33(4):1771 X 1799, 2005.
 increments. Scand. J. Stat. , 36:76 X 97, 2009.
 In NIPS . 2010.
 Neal, R. M. Slice sampling. Ann. Statist. , 31(3):705 X 767, 2003.

