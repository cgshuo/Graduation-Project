
Bagging and boosting are well-known ensemble learning meth-ods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, i.e., they require multiple passes through the training data. In previous work, we presented online bagging and boosting algorithms that only require one pass through the training data and pre-sented experimental results on some relatively small datasets. 
Through additional experiments on a variety of larger syn-thetic and real datasets, this paper demonstrates that our online versions perform comparably to their batch counter-parts in terms of classification accuracy. We also demon-strate the substantial reduction in running time we obtain with our online algorithms because they require fewer passes through the training data. 
Traditional supervised learning algorithms generate a sin-gle model such as a decision tree or neural network and use it to classify examples. 1 Ensemble learning algorithms com-bine the predictions of multiple base models, each of which is learned using a traditional algorithm. Bagging [3] and 
Boosting [4] are well-known ensemble learning algorithms that have been shown to be very effective in improving generalization performance compared to the individual base models. Theoretical analysis of boosting's performance sup-ports these results [4]. 
In previous work [7], we developed online versions of these algorithms. Online learning algorithms process each train-ing instance once "on arrival" without the need for storage and reprocessing, and maintain a current hypothesis that re-flects all the training instances seen so far. Such algorithms have advantages over typical batch algorithms in situations where data arrive continuously. They are also useful with 1In this paper, we only deal with the classification problem. requires prior specific permission and/or a Ice. KDD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 very large data sets on secondary storage, for which the multiple passes required by most batch algorithms are pro-hibitively expensive. In Sections 2 and 3, we describe our online bagging and online boosting algorithms, respectively. 
Specifically, we describe how we mirror the methods that the batch bagging and boosting algorithms use to gener-ate distinct base models, which are known to help ensemble performance. 
In our previous work, we also discussed our theoretical results and some empirical comparisons of the classification accuracies of our online algorithms and the corresponding batch algorithms on some relatively small datasets. In Sec-tion 4, we review the previous experiments and further ex-plore the behavior of our online algorithms through experi-ments with larger datasets--both synthetic and real. Con-sistent with previous work, we run our online bagging and boosting algorithms with lossless online algorithms for de-cision trees and Naive Bayes classifiers--for a given training set, a lossless online learning algorithm returns a hypoth-esis identical to that returned by the corresponding batch algorithm. Overall, our online bagging and boosting algo-rithms perform comparably to their batch counterparts in terms of classification accuracy. We also compare their run-ning times. If the online base model learning algorithm is not significantly slower than the corresponding batch algo-rithm, then the bagging and online bagging algorithms do not have a large difference in their running time in our tests. 
On the other hand, our online boosting algorithm runs sig-nificantly faster than batch boosting. For example, on our largest dataset, batch boosting ran four times longer than online boosting to achieve comparable classification accu-racy. 
Sometimes our online boosting algorithm significantly un-derperforms batch boosting for a small number of training examples. Even when the training set is large, our online algorithm may underperform initially before finally catching up to the batch algorithm. This characteristic is common with online algorithms because they do not have the luxury of viewing the training set as a whole the way batch algo-rithms do. We experiment with "priming" online boosting by running it in batch mode for some initial subset of the training set and running in online mode for the remainder of the training set. In most of the experiments that we dis-cuss in this paper, priming leads to improved classification performance. 
We also compare the base models' error rates on the train-OnlineBagging(h, d) Figure 1: Online Bagging Algorithm: h is the set of M base models learned so far, d is the latest training exam-ple to arrive, and Lo is the online base model learning algorithm. ing set under batch and online boosting. This is an impor-tant comparison because these errors axe used to calculate the weights of the training examples supplied to the differ-ent base models. They are also used to assign weights to the base models for use when classifying a new example. The closer these errors are, the more similar the training example weights and base model weights axe in the two algo-rithms, leading to more similar classification performances. The base model error rates exhibit similar trends in batch and online boosting, which partly explains the similar clas-sification accuracies we have obtained so far. 
Our experiments with synthetic datasets are meant to compare batch and online boosting with base models hav-ing small, medium, and large errors. Our three synthetic datasets are of varying difficulty for learning Naive Bayes classifiers. We show that boosting behaves differently on these datasets and that online boosting mirrors these be-haviors. 
Given a training dataset of size N, standard batch bagging creates M base models, each trained on a bootstrap sam-ple of size N created by drawing random samples with re-placement from the original training set. Each base model's training set contains K copies of each of the original training examples where which is the Binomial distribution. As N ~ co, the dis-tribution of K tends to a Poisson(1) distribution: P(K = k) -~ e~l). As discussed in [7], we can perform bag-ging online as follows: as each training example is presented to our algorithm, for each base model, choose the example 
K ~ Poisson(1) times and update the base model accord-ingly (see figure 1). New instances are classified the same way in online and batch bagging--by unweighted voting of the M base models. 
Online bagging is a good approximation to batch bag-ging to the extent that their base model learning algorithms produce similar hypotheses when trained with similar dis-tributions of training examples. In past work [7], we proved that if the same original training set is supplied to the two bagging algorithms, then the distributions over the training sets supplied to the base models in batch and online bag-ging converge as the size of that original training set grows to infinity. We further proved, for some very simple learn-ing algorithms (K-Nearest Neighbor and contingency-table learning), that the convergence of the distributions over the Initial conditions: A~ = 0, A~n w = 0. Figure 2: Online Boosting Algorithm: h is the set of M base models learned so far, d is the latest training exam-ple to arrive, and Lo is the online base model learning algorithm. training sets leads to convergence of the classification per-formance of online bagging to that of batch bagging. We are working on tightly characterizing the learning algorithms for which we obtain this type of convergence. 
Our online boosting algorithm is designed to correspond to the batch boosting algorithm, AdaBoost.M1 [4]. Ad-aBoost generates a sequence of base models hi,..  X , hM us-ing weighted training sets such that the training examples misclassified by model h,~_ 1 are given half the total weight when generating model hm and the correctly classified ex-amples are given the remaining half of the weight. 
Our online boosting algorithm (Figure 2) is similar to our online bagging algorithm except that when a base model misclassifies a training example, the Poisson distribution pa-rameter (A) associated with that example is increased when presented to the next base model; otherwise it is decreased. 
Just as in AdaBoost, our algorithm gives the examples re;s-the correctly classified examples are given the remaining half of the weight. 
One area of concern is that, in AdaBoost, an example's weight is adjusted based on the performance of a base model on the entire training set while in online boosting, the weight adjustment is based on the base model's performance only on the examples seen earlier. To see why this may be an issue, consider running AdaBoost and online boosting on a training set of size 10000. In AdaBoost, the first base model hi is generated from all 10000 examples before being tested on, say, the tenth training example. 2 In online boosting, hi is generated from only the first ten examples before being 2Recall that we test base model h,~ on the training exam-ples in order to adjust their weights before using them to generate base model hm+l. 360 
Table 1: The datasets used in our experiments. For the Soybean and Census Income datasets, we have given the sizes of the supplied training and test sets. 
For the remaining datasets, we have given the sizes of the training and test sets in our five-fold cross-validation runs. Table 2: P(Aa = OIAa+x,C ) for a E {1,2,...,19} in Synthetic Datasets 
Breast Cancer [5] 
Forest Covertype 464809 tested on the tenth example. Clearly, at the moment when the tenth training example is being tested, we may expect the two hl's to be very different; therefore, h2 in AdaBoost and h2 in online boosting may be presented with different weights for the tenth training example. This may, in turn, lead to different weights for the tenth example when gener-ating h3 in each algorithm, and so on. Intuitively, we want online boosting to get a good mix of training examples so that the base models and their normalized errors in online boosting quickly converge to what they are in AdaBoost. 
The more rapidly this convergence occurs, the more similar the training examples' weight adjustments will be and the more similar their performances will be. In the next section, we demonstrate, for some of our larger datasets, that this appears to happen. strate the performance of our online algorithms relative to their batch counterparts. For decision trees, we have reim-plemented the lossless ITI online algorithm [8] in C++; batch and online Naive Bayes algorithms are essentially iden-tical. We ran these experiments on Dell 6350 computers having 600MHz Pentium III processors and 2GB of mem-ory. datasets (Census Income and Forest Covertype) from the 
UCI KDD archive [1], and three synthetic datasets. We give their sizes and numbers of attributes and classes in Table 1. prior probability of each class is 0.5, and every attribute ex-cept the last one is conditionally dependent upon the class and the next attribute. We set up the attributes this way because the Naive Bayes model only represents the prob-abilities of each attribute given the class, and we wanted data that is not realizable by a single Naive Bayes classifier so that boosting is more likely to yield improvement. The probabilities of each attribute except the last one (A~ for a E {1, 2,..., 19}) are as shown in Table 2. 
The only difference between the three synthetic datasets is P(A~o]C). In Synthetic-l, P(A2o -= OIC = O) = 0.495 and 
P(A2o = OIC = 1) = 0.505. In Synthetic-2, these probabili-ties are 0.1 and 0.8, while in Synthetic-3, these are 0.01 and 0.975, respectively. 
Figures 3 and 4 are scatterplots comparing the errors of the batch and online versions of bagging and boosting. The in the figures represents one dataset. To reduce clutter, we do not show error bars in our figures, however we performed significance tests (t-test, a = 0.05) and discuss the results later in this paper. The batch algorithm accuracies are aver-ages over ten runs of five-fold cross-validation for a total of 50 runs, except for the Soybean and Census Income datasets where we performed ten runs with the supplied training and test set. We tested our online algorithms with five random orders of each training set generated for the batch algorithms (order matters for online boosting, even with a lossless learn-ing algorithm) for a total of 250 runs (50 runs on the Soybean and Census Income datasets). We tested bagging and boost-ing with decision trees only on some of the smaller datasets (Promoters, Balance, Breast Cancer, Car Evaluation) be-cause the lossless decision tree algorithm is too expensive with larger datasets in online mode. Bagging and online bagging perform comparably in all our tests. Boosting and online boosting perform comparably on all except the very small Promoters data.set. 
The largest dataset for which we ran the bagging and boosting algorithms with decision trees was the Car Evalu-ation dataset from the UCI Repository. Figure 5 shows the learning curve. Batch and online bagging with decision trees perform almost identically (and always significantly better than a single decision tree). AdaBoost also performs sig-nificantly better than a single decision tree for all numbers of examples. Online boosting struggles at first but performs comparably to AdaBoost and significantly better than single decision trees for the maximum number of examples. Note that online boosting's performance steadily becomes closer to that of AdaBoost as the number of examples grows, as one expects from an online algorithm when compared to its batch version. 
Figure 6 shows the learning curves for the Census Income dataset. Batch and online boosting perform comparably to each other and significantly outperform a single model for all numbers of examples. On the other hand, bagging and online bagging do not improve significantly upon a sin-gle Naive Bayes classifier. Bagging does not improve upon Naive Bayes on any of the datasets, which we expected be-cause of the stability of Naive Bayes [3], i.e., small changes in the dataset do not significantly change each Naive Bayes classifier, so that almost all of the base models tend to vote the same way for a given example. Online bagging always performs comparably to batch bagging in our experiments; therefore, online bagging also does not improve upon Naive Bayes. 
Figure 7 gives a scatterplot similar to Figure 4 except that the online boosting algorithm trains in batch mode with some initial portion of the training set and online mode with the remainder. In primed mode, batch training was done with the lesser of the first 20% of the dataset or the first 10000 training examples. Overall, primed online boosting improves upon the unprimed version. Only in case of the Promoters dataset with Naive Bayes classifiers did priming yield significant improvement over unprimed online boost-ing. Nevertheless, we did achieve some improvement through priming in all cases except Promoters and Breast Cancer with decision trees, and Soybean, Car Evaluation, and For-est Covertype with Naive Bayes. 
As we discussed earlier, in the Car Evaluation dataset's learning curves (Figure 5), online boosting significantly un-derperforms batch boosting for all but the maximum number of examples. Figure 8 displays the original batch boosting and online boosting learning curves along with primed on-line boosting with the first 200 training examples learned in batch mode. Primed online boosting with decision trees performs comparably to batch boosting for all numbers of examples, i.e., its performance gets close to batch boosting's performance much quicker. 
Figures 9 and 10 show the average errors on the train-ing sets of the consecutive base models in batch and online boosting with Naive Bayes for the second synthetic dataset and Census Income dataset, respectively (see the full pa-per [6] for more such graphs). As mentioned earlier, the closer these errors axe in batch and online boosting, the closer the behavior of these two algorithms. We depict the average errors for the maximum number of base models gen-erated by the batch boosting algorithm. For example, on the Census Income dataset, no run of batch boosting ever gen-erated more than 22 base models. This happens because if the next base model that is generated has error greater than 0.5, then the algorithm stops. Our online boosting al-gorithm always generates the full set of 100 base models be-cause, during training, we do not know how the base model errors will fluctuate; however, to classify a new example, we only use the first L base models such that model L + 1 has error greater than 0.5. 
The base model errors of online and batch boosting are quite similar for Synthetic-2: the first base model performs quite well in both batch and online boosting. Both algo-rithms then follow the pattern of having subsequent base models perform worse, which is typical because subsequent base models are presented with previously misclassified ex-amples having higher weight, which makes their learning problems more difficult. In the Census Income dataset, the performances of the base models also follow this general trend, although more loosely. Figures 11 and 12 contain the average running times of 
Naive Bayes and the ensemble algorithms with Naive Bayes base models for the Census Income dataset and Forest Cover-type dataset, respectively. Both the online and batch ensem-ble algorithms use a learning algorithm for the Naive Bayes base models that requires one pass through the training set. 
As the number of training examples increases, we expect the rate of growth of the running time to be less for our online ensemble algorithms than for the batch algorithms. Our on-line algorithms require only one pass through the training set whereas batch bagging requires one pass per base model (to generate its training set and perform the training) and batch boosting requires two passes per base model (once to generate the Naive Bayes classifier and once to test the newly-generated classifier on the training examples to up-date their weights). However, for small numbers of training examples, the running time may be greater for online learn-ing because the greater number of passes required through the data structures that represent the base models may out-weigh the greater number of passes required through the training set. Also, in case of base models for which online learning takes much more time than batch learning, the to-tal execution time for the online ensemble algorithm would be much greater than for the batch algorithm. Additionally, our online boosting algorithm always generates and updates 100 base models, whereas boosting often generates fewer base models as discussed above. 
The running time for online boosting is substantially less than for batch boosting on both Census Income (20 minutes vs. 7.1 hours on the entire dataset) and Forest Covertype (4.3 hours vs. 18.8 hours). Relative to the boosting al-gorithms, the running times of the bagging algorithms are negligible. 
This paper discusses online versions of the popular bag-ging and boosting algorithms. We have demonstrated that they mostly perform comparably to their batch counterparts in terms of classification accuracy. We experimented with priming our algorithm by running an initial subset of the training set in batch mode and then processing the remain-ing examples online and achieved improvement by doing so. 
We also demonstrated the comparable performance of on-line boosting and batch boosting in more detail by examin-ing the errors of the base models on the training set, which directly affect the weights given to the training examples in the different stages of boosting. We have also shown that, if the online base model learning algorithm has a run-ning time comparable to the corresponding batch algorithm, then the running time of online boosting can be much lower than batch boosting, demonstrating the significant savings obtained by processing the training set just once. 
In addition to continuing empirical work with large datasets and different base model learning algorithms, we are work-ing on several theoretical tasks including tightly character-izing the class of learning algorithms for which convergence between online and batch bagging can be proved and de-veloping an analytical frarnework for online boosting. We axe also investigating the case of lossy online base model 5O ~ 35 '~ 30 m 25 ~-20 Figure 3: Test Error Rates: Batch Bagging vs. 
Online Bagging. A star indicates that the two al-gorithms used decision tree base models while a square indicates Naive Babes base models. o 0.85 0 ,-0.8 ,, 0.7 
Figure 5: dataset. ~ 35 8 30 m 25 ~ 20 ,g co 0.8 
I-t-0.4 Figure 9: Dataset. c 25000 v 20000 
E 15000 Figure 11: Running Times for Census Income Dataset. learning and its effect on ensemble performance. 
The Wisconsin Breast Cancer dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. The Forest Covertype is Copyrighted 1998 by Jock A. Blackard and Colorado State University. [1] S.D. Bay. The UCI KDD archive, 1999. (URL: [2] C. Blake, E. Keogh, and C.J. Merz. UCI repository of [3] L. Breiman. Bagging predictors. Machine Learning, [4] Yoav Freund and Robert E. Schapire. A [5] O. L. Mangasarian, R. Setiono, and W. H. Wolberg. [6] Nikunj C. Oza and Stuart Russell. Experimental [7] Nikunj C. Oza and Stuart Russell. Online bagging and [8] P.E. Utgoff, N.C. Berkman, and J.A. Clouse. Decision 
