 This paper tackles the efficiency problem of making recom-mendations in the context of large user and item spaces. In particular, we address the problem of learning binary codes for collaborative filtering, which enables us to effi-ciently make recommendations with time complexity that is independent of the total number of items. We propose to construct binary codes for users and items such that the preference of users over items can be accurately preserved by the Hamming distance between their respective binary codes. By using two loss functions measuring the degree of divergence between the training and predicted ratings, we formulate the problem of learning binary codes as a discrete optimization problem. Although this optimization problem is intractable in general, we develop effective relaxations that can be efficiently solved by existing methods. Moreover, we investigate two methods to obtain the binary codes from the relaxed solutions. Evaluations are conducted on three public-domain data sets and the results suggest that our pro-posed method outperforms several baseline alternatives. H.3.3 [ Information Search and Retrieval ]: Information filtering; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Recommender systems, Collaborative filtering, Learning bi-nary codes, Discrete optimization, Relaxed solutions
With the rapid growth of E-commerce, hundreds of thou-sands of products, ranging from books, mp3s to automobiles, are sold through online marketplaces nowadays. In addition, millions of customers with diverse backgrounds and prefer-ences make purchases online, generating great opportunities as well as challenges for E-commerce companies  X  How to match products to their potential buyers not only accurately but also efficiently. Since collaborative filtering is an essen-tial component for many existing recommendation systems, it has been actively investigated by a wide range of previ-ous studies to improve its accuracy [1, 19]. On the other hand, due to the nature of their applications, collaborative filtering systems are usually required to learn and predict the preferences between a large number of users and items. Therefore, for a given user, it is important to retrieve prod-ucts that satisfy her preferences efficiently, leading to fast response time and better user experience. Naturally, the problem can be viewed as a similarity search problem where we seek X  X imilar X  X tems for a given user. Recent studies show that binary coding is a promising approach for fast similarity search [9,13,14,17,21]. The basic idea is to represent data points by binary codes that preserve the original similarities between them. One significant advantage of this approach is that the retrieval of similar data points can be conducted by searching for data points within a small Hamming dis-tance, which can be performed in time that is independent of the total number of data [17]. However, no prior stud-ies have been focused on constructing binary codes for both users and items in the context of collaborative filtering  X  to the best of our knowledge  X  a gap we propose to fill in this paper.

One key obstacle that hinders direct exploitation of the existing approaches to learning binary codes to the collab-orative filtering context is that most of them assume the similarities between any pairs of data points are given ex-plicitly, e.g., in the form of kernel functions or similarity graphs [13, 21, 24]. However, in collaborative filtering, the similarities between users and items are not known explic-itly. In fact, the main goal of collaborative filtering algo-rithms is to estimate and predict unobserved similarities between users and items from the training data in order to make recommendations. In this paper, we address the problem of learning binary codes for collaborative filtering. Specifically, we propose to learn compact yet effective binary codes for both users and items from the training rating data. Unlike previous works on learning binary codes, we do not assume the similarity between users and items are known ex-plicitly. Therefore, the binary codes we construct not only accurately preserve the observed preferences of users, but they also can be used to predict the unobserved preferences, making the proposed method conceptually unique compared with the existing methods.
 Our approach is based on the idea that the binary codes assigned to users and items should preserve the preferences o f users over items. Two loss functions are applied to mea-sure the divergence between the training data and the es-timates based on the binary codes. Unfortunately, thus formulated, the resulting discrete optimization problem is difficult to solve in general. Through relaxing the binary constraints, it turns out the relaxed optimization problem can be solved effectively by existing solvers. Moreover, we propose two effective methods for rounding the relaxed so-lutions to obtain binary codes. One key property of the binary codes obtained by the proposed method is that the degree of preferences of a user to items can be measured by the number of common bits between their corresponding binary codes. Hence, the major advantage of representing users and items by binary codes is to enable fast search: In order to provide recommendations for a user, we need only to search items with binary codes within a small Hamming distance to the binary codes of the given user. We evalu-ated the proposed method over three data sets and compare it with several baseline alternatives. The results show that the binary codes obtained by the proposed method can pre-serve and predict the preferences of users more accurately than the baselines.

The contributions of this paper are essentially threefold: 1) We propose to learn binary codes for collaborative filter-ing that accurately preserve the preference of users, which generalizes the existing works of learning binary codes to the context of collaborative filtering. 2) We propose a frame-work to learning binary codes based on training rating data, which leads to relaxed optimization problems that can be solved effectively by the state-of-the-art optimization tech-niques. 3) Our experimental evaluations show that the bi-nary codes obtained by the proposed method can preserve the preference of users better than several baseline alterna-tives.

The rest of the paper is organized as follows: In Section 2, we briefly review existing studies for collaborative filtering and learning binary codes. In Section 3, we first formu-late the problem of learning binary codes for collaborative filtering as a discrete optimization problem and introduce the two loss functions used in this work. Then, the learn-ing algorithm proposed with detailed derivations based on transforming and relaxing the discrete optimization problem so that it can be optimized efficiently. Moreover, we discuss two different methods for rounding real-valued solutions to obtain binary codes. The evaluations are described and ana-lyzed in Section 4. We conclude our work and present several future research directions in Section 5.
Many studies on recommender systems have been focused on collaborative filtering approaches. These methods can be categorized into memory-based and model-based. The reader is referred to the survey papers [1,19] for a compre-hensive summary of collaborative filtering algorithms.
Recently, matrix factorization has become a popular di-rection for collaborative filtering [2,11,15,16]. These meth-ods are shown to be effective in many applications. Specif-ically, matrix factorization methods seek to associate both users and items with latent profiles represented by vectors in a low dimensional Euclidean space that can capture their characteristics. Specifically, the preference of a user on an item is usually measured by some similarity, such as the dot-product, between their low dimensional profiles. These studies are related to our work in the sense that both of them aim to find certain representations that preserve the prefer-ence between users and items. However, one key difference is that our work aims to find binary codes in Hamming space for representing users and items, which has the nice prop-erty that the retrieval of interesting items for a user can be performed in time that is independent of the total number of items [17].

Another direction of collaborative filtering investigate the problem of using binary codes to create fingerprints for users [3 X 5]. The idea is create binary fingerprints for each user us-ing randomized algorithms so that the similarity between users can be approximated according to the fingerprints. However, these studies do not address the problem of si-multaneously representing users and items by binary finger-prints. Thus, the preference of users on items can not be es-timated directly from these fingerprints. On the other hand, the binary codes learnt by our proposed method can be di-rectly used to measure the preference of users over items.
The problem of learning binary codes for fast similar-ity search has been investigated in several studies [12, 13, 17, 21] Locality sensitive hashing tries to construct binary codes that preserve certain distance (e.g., L p distance) be-tween different points with high probability, which is usually archived by random projection [6,8]. In [18], the problem of learning effective binary codes is solved by utilizing the idea of boosting. The work of [17] proposes to learn binary codes making use of Restricted Boltzmann Machine (RBM) for fast similarity search of documents. Recent work focuses on constructing binary codes based on a given similarity func-tion [14,20,21]. The basic idea is to apply spectral analysis techniques to the data and embedding data points into a low dimension space. For example, the work [21] investigate the requirements for compact and effective binary codes. Their solution relies on spectral graph partition, which can be solved by eigenvalue decomposition of the Laplacian matrix for the graph. It has been shown that these methods archive significant performance improvements in terms of preserving the similarity between data points. Although several exten-sion of this method has been studied [7, 24], these meth-ods only consider the problem of obtaining binary codes for one type of entities. However, in collaborative filtering, two types of entities, users and items, are naturally involved and thus should be consider simultaneously, which makes it dif-ficult to applies these method directly.

The work [23] proposes to learn binary codes for both doc-uments and terms by viewing the term-document matrix as a bipartite graph and apply the method proposed in [21] to obtain the binary codes. However, this method can not deal with the problem of unobserved/missing ratings in collab-orative filtering. As shown in our experiments, the binary codes obtained by this method quickly overfit the training data and lead to poor prediction accuracy.
In this section, we describe the proposed method for learn-ing binary codes for collaborative filtering. We first describe the general formulation for this problem through optimiza-tion using squared and pairwise loss functions, respectively . Then, the learning method based on solving the relaxed problem is derived in detail. Finally, we discuss two meth-ods to obtain binary codes from the real-valued solutions of the relaxed problems.
The goal of collaborative filtering is to recommend inter-esting items to users according to their past ratings on the items. Formally, we assume that r ui represents the rating of user u  X  U for item i  X  I , where U and I are the user and item space, respectively. Without loss of generality, we fur-ther assume that r ui is a real number in the interval [0 , 1]. Moreover, we assign binary codes f u  X  { X  1 , 1 } B for each user u and h i  X  { X  1 , 1 } B for each item i , where B is the length of the binary codes. Our goal is to construct binary codes for users and items that preserve the preferences be-tween them  X  the degree of preference of user u over item i can be estimated by the similarity between their binary codes f u and h i . A natural way to define the similarity between user u and item i is the fraction of common bits in their binary codes f u and h i , leading to the similarity function, f u and h i , respectively. I ( ) denotes the indicator function that returns 1 if the statement in its parameter is true and zero otherwise.

It is easy to check that the following holds for the similar-ity function sim( , ) defined above: where dist H ( f u , h i ) is the Hamming distance between two binary codes f u and h i . The above fact suggests that the smaller the Hamming distance is, the more similar their bi-nary codes become. Therefore, in order to find items with similar binary codes to a user represented by f u , it is suf-ficient to search items i within a small Hamming distance dist H ( f u , h i ). This allows us to find similar items in time that is independent to the total number of items [17].
In order to make accurate recommendations to users, we need to find binary codes f u and h i for users and items such that the preferences between them are preserved by the similarities between their respective binary codes. In addition, in collaborative filtering, we only observe a subset of all the possible ratings { r ui | ( u, i )  X  O} where O  X  U  X  I and we need to recommend items to users according to their preferences over items whose ratings are unobserved. There-fore, the key for accurate recommendations is to construct binary codes that can not only preserve the observed rat-ings but also accurately predict the preferences of users on unobserved items.

Our approach to learn binary codes is to estimate them from observed ratings. Specifically, we propose to construct binary codes that minimize the degree of divergence between the observed ratings and the ratings estimated from the bi-nary codes. To this end, we apply two objective functions to measure the degree of divergence between the observed ratings and the model estimates:
Additionally, we also require the binary codes to be bal-anced  X  we would like each bit of the binary codes to have equal chance to be 1 or  X  1. The balance constraint is equiv-alent to maximizing the entropy of each bit of the binary codes, which indicates that each bit carries as much infor-mation as possible. Specifically, we will enforce the following constraints to the binary codes: The above constraints motivate the following regularized ob-jective function for learning binary codes. For example, for squared loss, we have the following objective function: where the first term is the loss over observed ratings and the second term represents that we prefer balanced binary codes. The parameter  X  controls the trade-off between minimizing the empirical errors and the enforcement of the constraints. k k indicates Euclidean norm of a vector.

Similarly, we have the following regularized objective func-tion for the pairwise loss function:
The objective functions defined in Equation (3) and Equa-tion (4) are defined over the discrete space { X  1 } B , which makes them difficult to optimize in general. Therefore, we propose to solve it approximately by transforming the ob-jective functions and then relaxing the space of solutions to be [  X  1 , 1] B . For the sake of concreteness, we describe our method for solving the squared loss in Equation (3) in de-t ail. The pairwise loss in Equation (4) can be optimized in a similar approach.

First, we notice that for binary codes f, h  X  { X  1 } B , the following property holds: sim( f, h ) = 1 Thus, by substituting the above equation into the regular-ized objective function defined in Equation (3), we can ex-press the objective function for squared loss as follows: A widely used approach to obtain approximate solutions to the above discrete optimization problem is to relax the space of solution to be real values and thus enables the applica-tion of the continuous optimization techniques to solve the problem. To this end, we first relax the space of solution to be real vectors in [  X  1 , 1] B and then we will round the real-valued solutions into { X  1 } B . The details of rounding are discussed in Section 3.3.

It is also interesting to note that the above formulation also reveals a nice connection between learning binary codes and the matrix factorization approaches that are widely ap-plied in collaborative filtering. In particular, the first term in (5) is the objective function that factorizes the linearly trans-formed matrix of observed ratings to find low-dimensional representations for users and items. The second term is dif-ferent from the usual  X  2 regularization used in traditional matrix factorization since we would like the binary codes to balanced rather than close to zero in this case.

Given the relaxed problem, the partial derivatives of the objective function L reg with respect to f u and h i can be expressed as follows: The relaxed problem can be solved by methods such as LBFGS [25] and stochastic gradient descent. After solving the relaxed optimization problem defined in Equation (5), we obtain real-valued vectors  X  f u and  X  [  X  1 , 1] B for each user u and item i . In this section, we propose two methods to obtain binary codes f u and h i  X  { X  1 } B from these real-valued vectors.
A straightforward method is to find binary vectors f u and h  X  { X  1 } B that are closest to  X  f u and  X  h i . Specifically, we seek to optimize the following objective function to obtain f u for all u  X  U : subject to P u f u = 0 . Similarly, we can obtain h i for all item i  X  I by: subject to P u h i = 0 .
 It turns out that the optimization problems defined in Equation (6) and Equation (7) have the following solution: and where median( ) represents the median of a set of real num-bers.
Another method to obtain the binary codes from the re-laxed solution  X  f u and  X  h i makes use of the structure of the solutions to the relaxed optimization problem. Similar ideas have been investigated for spectral clustering [22] and we ex-tend the idea to the context of learning binary codes. First, we observe that if  X  f u and  X  h i are optimal solutions for (5), then Q  X  f u and Q  X  h i are also optimal solutions achieving the same value of the objective function for an arbitrary orthog-onal matrix Q  X  R B  X  B , i.e., Q T Q = I . This observation can be proved as follows: L = X +  X  ( k X = X where the second equation utilizes the fact the Q is an or-thogonal matrix. The above observation shows that apply-ing orthogonal transformations to an optimal solution of re-laxed optimization problem does not change the value of the objective function, which motives the following method to obtain binary codes from the relaxed solution: subject to: Intuitively, instead of directly finding binary codes that ar e close to the relaxed solutions, we seek binary codes that are close to some orthogonal transformation of the relaxed solutions. Introducing the orthogonal transformation Q not only preserves the optimality of the relaxed solutions but provides us more flexibility to obtain better binary codes.
The optimization problem defined in Equation (8) can be solved by minimizing with respect to f u , h i and Q alterna-tively.

Optimization with respect to f u and h i . Specifically, we first fix the orthogonal transformation Q and optimizing with respect to f u and h i : subject to P u f u = 0 , P i h i = 0 . The solution can be ex-pressed as follows: and where ( Q  X  f u ) ( k ) represents the k -th element of the trans-formed vector Q  X  f u .

Optimization with Respect to Q . In this case, we fix f u and h i for all u  X  U and i  X  I . Then we solve the following optimization problem to update the orthogonal transformation Q : subject to the constraint Q T Q = I, where F = [ f 1 , . . . , f  X  F = [  X  f 1 , . . . ,  X  f |U| ], H = [ h 1 , . . . , h |I| k k F indicates the Frobenius norm. The following theorem enables us to solve the optimization problem efficiently by singular value decomposition:
Theorem 1. Let U DV T be the singular value decompo-sition of the matrix ( F  X  F T + H  X  H T ) . Then, Q = U V imizes the objective function defined in Equation (9).
Proof. First notice that the objective function L ( Q ) = k F k 2 + k  X  F k 2  X  trace( F  X  F T Q T )+ k H k 2 + k Therefore, the optimization problem is equivalent to max-imizing the following function subject to the orthogonality constraint Q T Q = I : trace( F  X  F T Q T )+trace( H  X  H T Q T ) = trace(( F  X  Let us consider the Lagrange L ( Q,  X ) = trace(( F  X  F T + H  X  H T ) Q T )  X  where  X  is a symmetric matrix. By taking the gradient with respect to Q , we have Thus,  X  = ( F  X  F T + H  X  H T ) Q T = U DV T Q T , which implies that  X  2 = U D 2 U T . Hence,  X  = U DU T . Substituting it into the above equation, we have Q = U D  X  1 U T U DV T = U V T .

In general, we perform the above two steps alternatively u ntil the solution converges and obtain the binary codes f and h i .
In this section, we describe the experiments conducted to evaluate the proposed method for learning binary codes. For the sake of simplicity, we denote the proposed method with squared loss and pairwise loss defined in Equation (3) and Equation (4) by CFCodeReg and CFCodePair , respectively.
We apply two evaluation metrics to evaluate the perfor-mance of CFCodeReg and CFCodePair . Our goal is to eval-uate whether the obtained binary codes can accurately pre-serve the preferences of users to items. The evaluation met-rics are described as follows:
In order to evaluate the performance of using binary codes for recommending top-K items to users, we apply the follow-ing evaluation metrics: We used three data sets to evaluate the performance of CF-CodeReg and CFCodePair , MovieLens , EachMovie and Netflix with their statistics summarized in Table 1. h ttp://www.grouplens.org/node/73
We split the three data sets into training and test sets as follows: for Movielens and EachMovie , we randomly sample 80% ratings for each user as the training set and the rest 20% is used as the test set. These two data sets are very sparse and thus a lot of ratings are not observed, which may lead to biased evaluation results for precision. Therefore, we also construct a dense data set from the netflix data as follows: We first select 5000 items with the most ratings and then sample 10000 users with at least 100 ratings to construct a relatively dense data set. For this data set, we sample 20% ratings for each users as the training set and the rest ratings are used as the test set. For all three data sets, we generate five independent splits and report the averaged performance in our evaluations. Moreover, we exclude all ratings in the training set and use only the ratings in the test set when computing the evaluation metrics.
In Section 3.3, we describe two methods for obtaining bi-nary codes from the approximate real-valued solutions. We now compare the performance of these methods. To this end, we denote the method that rounding to closest binary codes described in Section 3.3.1 as Closest and the method using orthogonal transformation described in Section 3.3.2 by OrthTrans . We apply CFCodeReg and CFCodePair with binary codes of length 10 to all the three data sets and com-pare the binary codes obtained by Closest and OrthTrans . We report the performance on three data sets measured by precision in Figure 1. From Figure 1, we can see that the binary codes obtained by OrthTrans outperform the corre-sponding binary codes obtained by Closest , which suggests that the OrthTrans can obtain better approximations for bi-nary codes. Intuitively, OrthTrans is more flexible than Clos-est through introducing the orthogonal transformation and thus enable us to obtain better approximation. We will use OrthTrans to obtain binary codes in the rest of our evalua-tions.
We compare CFCodeReg and CFCodePair to the following baselines: h ttp://www.netflixprize.com/ Figure 1: Performance of two rounding methods on M ovieLens , EachMovie and Netflix data sets
We apply the proposed CFCodeReg and CFCodePair to all three data sets and compare the obtained binary codes to the two baselines described in Section 4.4.1. Specifically, we plot the performance measured by DCG and precision with respect to the length of the binary codes in Figure 2, Figure 3 and Figure 4 for MoveiLens , EachMovie and Netflix data sets, respectively.

We can observe that DCG and precision of both CFCodeReg and CFCodePair increase in most cases with larger length of binary codes. Hence, the performance of CFCodeReg and CFCodePair improves when the number of bits increases. Therefore, we conclude that both methods can utilize the available bits to preserve the preference of users more accu-rately. We can also observe from the figures that the binary codes obtained by CFCodeReg and CFCodePair outperform other baselines in terms of DCG. Thus, the proposed method can better preserve the relative orders between items accord-ing to the preference of users. Moreover, the improvement over baselines in terms of precision indicates that the binary codes obtained by CFCodeReg and CFCodePair can be used to recommend interesting items to users accurately. Comparing the performance of CFCodeReg and CFCode-Pair , we can find that CFCodePair outperforms CFCodeReg in most cases, which suggests that the pairwise loss func-tion is more suitable for learning binary codes. This is be-cause the pairwise loss function emphasis more on the orders between different items rather than their absolute ratings, which makes it a more reasonable loss function in the rank-ing scenario for collaborative filtering.

Another interesting observation is that SH does not work very well in our case. Specifically, it overfits the training data very quickly when the length of binary codes increases. By observing the results, we find that SH usually fits the training data very well. However, it frequently assign simi-lar distances for users and items whose ratings are not in the training set. Therefore, its performance over the test set is reduced. In order to further investigate this point, we vary the length of binary codes and plot the variance of Hamming distances on unobserved ratings for binary codes generated by SH and CFCodeReg in Figure 5. We can observe that the variances produced by SH decrease when the length of binary codes grows. On the other hand, the variance generated by CFCodeReg are generally much higher than those generated by SH , which indicates that CFCodeReg generates more di-verse codes when the length of binary codes increases. We think the reason is that SH usually fits the observed similar-ities while fails to predict the unobserved ones. This obser-vation verifies that CFCodeReg can not only fit the observed preferences very well, but it also can predict the unobserved preference accurately.
We investigate the impact of the regularization parameter  X  for the propose methods. To this end, we report the per-formance of CFCodeReg and CFCodePair measured by DCG with respect to different values of  X  in Figure 6. We only show the results on the MovieLens data set due to the lack of space. From Figure 6, we can observe that the perfor-mance measured by DCG first increases and then decreases in most cases, which indicates that a good value of  X  can enhance the learning process and thus improves the accu-racy of learnt binary codes. In general, the value of  X  can be determined by cross validation .
 In our experiments, the relaxed optimization problem of Equation (5) is solved by LBFGS, which is an effective it-erative solver for optimization problems. In Figure 7, we present the performance measured by DCG with respect to the number of iterations on MovieLens data set. We can observe from Figure 7 that the performance measured by DCG both training and test set increases when the number of iterations grows. The training process usually converges in about one hundred iterations.
It is also interesting to compare CFCodeReg to the low-rank matrix factorization method that is widely exploited for collaborative filtering. Since CFCodeReg is restricted to use binary codes in order to facilitate fast search, it can be viewed as an approximation of the low-rank factoriza-tion methods. Thus, it is natural to investigate how close CFCodeReg can approximate the performance of low-rank matrix factorization. To this end, we vary to length of the binary codes from 10 to 110 and report the performance measured by DCG on MovieLens data set in Figure 8. We also report the performance of low-rank matrix factoriza-tions when varying the rank of the factorization. We can see that the performance of CFCodeReg increases in gen-eral when the length of the binary codes grows and become very close to the performance of low-rank matrix factoriza-tions. On the other hand, the performance of low-rank ma-trix factorizations is slightly reduced when the number of latent dimensions increases which is generally explained by the overfitting of the training data. Figure 5: Variance of predicted similarity on un-k nown ratings with respect to the length of binary codes
We also compare the efficiency of obtaining top-K recom-mendations. In particular, for MF we compute the predicted scores for every item for a given user and then select top-K items with the highest scores. For CFCodeReg , we retrieve items with binary codes within Hamming distance 3 to the binary codes of the user. We measure efficiency by the total time required to generate recommendations for all users. To this end, we run the recommendation program for 10 times and report the average running time. The evaluation is con-ducted on a server with 16G main memory and use one of its eight 2.5GHz cores. On MovieLens data set, CFCodeReg takes 0.586 seconds to process all users while MF takes 64.9 seconds. The significant efficiency improvement is expected and can be explained by the fact that CFCodeReg only goes through a small fraction of items while MF computes the prediction for all items. This confirms that recommendation efficiency can be significantly improved by utilizing binary codes.
In this paper, we address the problem of learning binary codes that preserves the preferences of users to items. In particular, we propose a framework that constructs binary codes such that the Hamming distances of a user and her preferred items are small. By applying two loss functions, the problem is formulated as a discrete optimization prob-lem defined on the training ratings data set. It turns out that the resulting optimization problem can be solving ap-proximately by transforming the objective function and re-laxing the variables to real values. Moreover, we study two Figure 6: Performance measured by DCG with re-s pect to the regularization parameter  X  on MovieLens data set Figure 7: Performance measured by DCG with re-s pect to the number of iterations on MovieLens data set methods to obtain the binary codes from the real-valued approximations. Experiments on three data sets show that the proposed methods outperform several baselines and thus and can preserve the preference of users more accurately.
For future research directions, we plan to further inves-tigate other methods for solving the discrete problem more accurately. Specifically, we can investigate how to apply semidefinite programming for relaxing the original problem. Another direction is to study how to learn binary codes in-crementally. In particular, we would like to construct the binary codes bit by bit in a sequential and incremental man-ner. It has the advantage that new bits of binary codes can be introduced to improve the accuracy without re-training all of the bits. Finally, the problem of incorporating features for users and items, such as demographical features for users and descriptions of items, to learn the binary codes can be also very interesting. Figure 8: Comparison of low-rank matrix factoriza-t ion and CFCodeReg on MovieLens data set Part of the work is supported by NSF IIS-1116886, NSF IIS-1049694 and NSFC 61129001/F010403. [1] G. Adomavicius and a. Tuzhilin. Toward the next [2] D. Agarwal, B. Chen, and P. Elango. Fast online [3] Y. Bachrach and R. Herbrich. Fingerprinting Ratings [4] Y. Bachrach, E. Porat, and J. Rosenschein. Sketching [5] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google [6] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. [7] J. He and W. Liu. Scalable similarity search with [8] P. Indyk and R. Motwani. Approximate nearest [9] H. J  X egou, T. Furon, and J.-J. Fuchs. Anti-sparse [10] J. Kek  X  al  X  ainen. Binary and graded relevance in IR [11] Y. Koren. Factor in the neighbors: Scalable and [12] B. Kulis and T. Darrell. Learning to hash with binary [13] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing [14] M. Norouzi and D. Fleet. Minimal Loss Hashing for [15] A. Paterek. Improving regularized singular value [16] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. [17] R. Salakhutdinov and G. Hinton. Semantic hashing. [18] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose [19] X. Su and T. M. Khoshgoftaar. A Survey of [20] J. Wang and S. Kumar. Sequential projection learning [21] Y. Weiss, A. Torralba, and R. Fergus. Spectral [22] S. X. Yu and J. Shi. Multiclass spectral clustering. In [23] D. Zhang, J. Wang, D. Cai, and J. Lu. Laplacian [24] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught [25] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. Algorithm
