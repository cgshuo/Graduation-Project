 Jos  X  e Miguel Hern  X  andez-Lobato JMH 233@ CAM . AC . UK Neil Houlsby NMTH 2@ CAM . AC . UK Zoubin Ghahramani ZOUBIN @ ENG . CAM . AC . UK Collaborative filtering (CF) data consists of ratings by users on a set of items. CF systems learn patterns in this data to make accurate predictions, for example, in order to recom-mend new items to users. Typically, most users rate only a small fraction of the available items, so most of the CF data is missing . Probabilistic matrix factorization (MF) models have become popular for CF because i) they can be ro-bust to overfitting and come with automatic estimates of uncertainty (Mnih &amp; Salakhutdinov, 2007); ii) they can be adapted to different data types, such as continuous, binary or ordinal data (Stern et al., 2009); and iii) they can theoret-ically handle missing data in a formal manner. A common assumption that MF models make is that CF data is missing at random (MAR) (Little &amp; Rubin, 1987). That is, the pro-cess that selects the observed data (the observation process) is independent of the value of this unobserved data. When the data is MAR, the observation process can be ig-nored and standard inference methods can be used with-out introducing bias. However, there is evidence that CF data is missing not at random (MNAR) (Marlin &amp; Zemel, 2007). Consider the following scenarios: i) users only watch movies that they like, and only rate movies that they watch; ii) a user only provides extreme ratings, that is, they only provide feedback particularly bad or good items; iii) certain basic items (e.g. stationary) are simply expected to function correctly and these items are only rated when they are defective. The first scenario is an example of global censoring where low-valued ratings are usually missing. The second two are examples of local censoring where missing ratings for specific users or items take higher or lower values on average than the observed ones. In these scenarios dependencies between the missing data values and the observation process exist, and consequently the data is not MAR. When the MAR assumption is incorrect, inferences can be biased and prediction accuracy suffers. The lack of robustness of the MAR assumption has been widely addressed in the statistics literature. However, al-most all probabilistic models for CF ignore this issue. We fill this gap by extending state-of-the-art MF models for CF to the MNAR scenario. The general approach for dealing with MNAR data is to learn jointly a complete data model (CDM), that explains how the data is generated, and a miss-ing data model (MDM), that explains the observation pro-cess for the data (Little &amp; Rubin, 1987). Our CDM is a new MF model for ordinal rating data with state-of-the-art pre-dictive performance. This model uses hierarchical priors to increase robustness to the selection of hyper-parameters and is heteroskedastic in the sense that the ratings of dif-ferent users and items can exhibit different levels of noise (Lakshminarayanan et al., 2011). Our MDM is also a MF model that can capture complex dependencies between the missing data and the observation process, such as those de-scribed in the previous paragraph. We perform efficient inference in the resulting MF model for MNAR data (MF-MNAR) using expectation propagation (Minka, 2001) and stochastic variational inference (Hoffman et al., 2013). Experimentally, the combination of the MDM and the CDM in MF-MNAR produces gains in both the model-ing of the ratings and the modeling of the data observa-tion process. We also find that MF-MNAR outperforms the MAR version of this method, other state-of-the-art MF MAR alternatives (Paquet et al., 2012) as well as an alter-native model for MNAR rating data based on mixtures of multinomials (Marlin &amp; Zemel, 2009). In summary, we present the first attempt to model MNAR data using proba-bilistic matrix factorization. Our results are promising and we expect that additional research in this domain will yield significant further improvements in CF systems. The theory of missing data has been widely studied. We review here the main principles developed in (Little &amp; Ru-bin, 1987) in the context of rating data. Our data is formed by ratings r i,j given by user i on item j , where i = 1 ,..., n and j = 1 ,...,d . We collect these into an n  X  d rating matrix R = { R O , R  X O } , where R O and R  X O denote the the sets of observed and missing entries in R , respectively. For each r i,j , we define a Bernoulli random variable x i,j that indicates whether r i,j is observed ( x i,j = 1) or not ( x i,j = 0) , and collect all the x i,j in the n  X  d binary ma-trix X . We assume that R is generated by a complete data model (CDM) with parameters  X  , and X is generated by a missing data model (MDM) with parameters  X  . Both models may also share a set of latent variables Z . The joint distribution for R , X , and Z given  X  and  X  is Most machine learning focuses on the estimation of the MDM, which is normally ignored in CF systems.
 The mechanisms for missing data are usually divided into three classes (Little &amp; Rubin, 1987): completely missing at random (CMAR), missing at random (MAR) and miss-ing not at random (MNAR). CMAR is the most restrictive assumption, where the probability of observing a rating is independent of the value of any rating or latent variable generated by the CDM, that is, p ( X | R , Z ,  X  ) = p ( X |  X  ) . With MAR data, the observation probability depends only upon the value of the observed data and the MDM param-eters, that is p ( X | R ,  X  , Z ) = p ( X | R O ,  X  ) . This assump-tion is popular in machine learning because it means that the MDM can be ignored without introducing any biases during inference. Under the MAR assumption, the likeli-hood for  X  given R O is Since p ( X | R O ,  X  ) is constant with respect to  X  , we can ignore the MDM when learning  X  .
 However, in the general case of MNAR data, X will not be independent of R or Z . In this case, the step from (2) to (3) does not hold and, when integrating over Z and summing over R  X O , one must weight the CDM likelihood p ( R , Z |  X  ) by the observation probabilities given by the MDM p ( X | R , Z ,  X  ) . This is because the binary matrix X has information about the possible values that R  X O or Z may have taken. Maximum likelihood estimates and Bayesian inference will be biased if the MAR assumption is used but the data is MNAR. For example, consider users who mainly rate items that they like and seldom rate items that they dis-like. Under the MAR assumption, the estimated CDM will over-estimate the value of the missing ratings that have not yet been provided by such users.
 We can correct this observational bias by jointly learning the CDM and the MDM. We now describe how to do this with an ordinal matrix factorization model for MNAR data. We are given a dataset D = { r i,j : 1  X  i  X  n, 1  X  j  X  d, r i,j  X  X  1 ,...L } , ( i,j )  X  X } of discrete ratings by n users on d items, where the possible ratings are 1 &lt; ... &lt; L and O is the set of pairs of users and items for which a rating is available. D is a subset of the entries of a com-plete n  X  d rating matrix R . In practice, D contains only a small fraction of the entries in R . We model the location of the entries included in D using an n  X  d binary matrix X , where x i,j = 1 if r i,j  X  D (observed) and x i,j = 0 other-wise (missing). We first describe a probabilistic model for the generation of R , then we describe another model that generates X given R . The first model is the complete data model (CDM) and the second one the missing data model (MDM). Our new method for ordinal Matrix Factorization with data Missing Not At Random (MF-MNAR) is formed by combining these two models into a single model. The factor graph for the distribution implied by MF-MNAR is shown in Figure 2. In this graph the square nodes cor-respond to factors in the distribution and the circular nodes represent random variables. The edges show dependencies of factors on variables (Kschischang et al., 2001). A full description of the factors in Figure 2 is given in the supple-mentary material. Figure 1 visualizes the generative model of MF-MNAR, which is described in detail below. 3.1. The Complete Data Model We describe now the CDM in the left half of Figure 2. Full details are in the supplementary material. We propose a matrix factorization model for the rating matrix R . This model has three key features: i) an appropriate ordinal like-lihood for rating data (rather than the usual Gaussian like-lihood); ii) heteroskedastic noise, that is, variable noise for each user and item; and iii) hierarchical priors to increase robustness to selection of hyper-parameter values. We assume that R is generated as a function of two low rank latent matrices U  X  R n  X  h and V  X  R d  X  h , where h min ( n,d ) . Each discrete rating r i,j in R is deter-mined by i) the scalar u T i v j , where u i is the vector in the i -th row of U and v j corresponds to the j -th row of V , and ii) a partition of R into L  X  1 contiguous intervals with boundaries b j, 0 &lt; ... &lt; b j,L , where b j, 0 b j,L =  X  . The value of r i,j is obtained from the interval in which u T i v j falls. Note that the interval boundaries are different for each column (item) of R . In practice data is noisy. We model this by adding zero-mean Gaussian noise i,j to u T i v j before generating r i,j and introducing the la-tent variable a i,j = u T i v j + i,j . The probability of r where  X  is the Heaviside step function. This likelihood is dence of (4) on all the entries in b j and not only on b r and b r i,j allows us to learn b j . We put a prior on each b j to learn these item specific boundaries, p ( b j | b Q terval boundaries. To avoid specifying these base bound-aries, we use a hyper-prior p ( b 0 ) = Q L  X  1 k =1 N ( b v ) , where m b 0 1 ,...,m b 0 L  X  1 and v 0 are hyper-parameters. Real world rating matrices exhibit variable levels of noise across rows and columns (Lakshminarayanan et al., 2011). To model this heteroskedasticity, we allow the additive noise i,j to be row and column dependent: i,j has a Gaus-sian prior with zero-mean and variance  X  row i  X   X  col j , where  X  i and  X  column of R , respectively. Define c i,j = u T i v j , then the conditional distribution for a i,j given c i,j ,  X  row i and  X  noise levels we put Inverse Gamma priors on  X  row i and  X  We use a hierarchical Gaussian prior for the two low-rank matrices U and V . We select m
U and m V are mean parameters for the rows of U and V , respectively. These are given factorized Gaussian hyper-priors. Similarly, v U and v V are variance param-eters for the rows of U and V and are given factorized Inverse Gamma hyper-priors. The parameters of these Gaussian hyper-priors are given standard values and the parameters of the Inverse Gamma hyper-priors are in the supplementary material.
 We collect the boundary vectors b j into the d  X  ( L  X  1) matrix B , the a i,j and c i,j variables into the n  X  d matrices A and C , and the row and column noise levels into the n and d dimensional vectors  X  row and  X  col , respec-tively. The joint distribution for R and the parameters  X  = { U , V , B , A , C ,  X  row ,  X  col , b 0 , m U , m V is This specifies the CDM. We now describe the MDM that explains which entries of R are contained in the set of ob-served ratings D , as specified by the binary matrix X . 3.2. The Missing Data Model The MDM generates the binary matrix X as a function of R . We also assume a matrix factorization model for X . However, our MDM has two main differences to the CDM. Firstly, we use a likelihood for binary data and secondly, we use additional variables to model the effect of each rating X  X  value r i,j on its observation status x i,j .
 We define two additional low rank matrices E  X  R n  X  h and F  X  R d  X  h . X is obtained as a function of E , F , R and some additive noise. In particular, we assume x where  X  { X } is the Heaviside step function, I [  X  ] is the binary indicator function, z  X  R is a bias that governs the overall sparsity of X and g i,j  X  R is an i.i.d. noise variable with a logistic c.d.f.  X  ( x ) = 1 / (1 + exp(  X  x )) . This generative process results in a logistic likelihood, which is commonly used in binary classification tasks.
 The parameters  X  row i,l , X  col j,l  X  R determine the influence of the value of r i,j on whether r i,j is contained in D or not. Importantly, this influence can vary across the rows and columns of R . A large positive value of (  X  row i,l increases the probability that r i,j is included in D when r ability. Thus  X  row i,l captures effects such as some users i being more likely to rate movies they like, and others rat-ing movies they dislike, while  X  col j,l captures the analogous effects for movies j . We collect the  X  row i,l and the  X  n  X  L and d  X  L matrices  X  row and  X  col . The likelihood for the missing data model is We use fully factorized standard Gaussian priors for all the parameters in (6). Finally we introduce row and column specific offset biases in E and F by setting to one all the entries in one of the columns in E and in another of the columns of F (details in the supplementary material). Let  X  be the set of variables  X  = { E , F ,z,  X  row ,  X  The joint distribution for X and  X  given R is 3.3. The Joint Model We obtain MF-MNAR by combining the MDM with the CDM. Recall from Section 2 that R O is the set of ob-served entries in R and R  X O is the set of non-observed entries. The posterior distribution over the parameters  X  of the CDM, the parameters  X  of the MDM and the miss-ing data itself R  X O given X (which ratings are observed) and R O (the values of the observed ratings) is where p ( R O , X ) is a normalization constant. The factor graph for the resulting model is shown in Figure 2. The graph includes 19 factors, described in the supplementary material. To predict the value of an entry r i,j in R  X O have to marginalize (8) with respect to  X  ,  X  and all of the entries in R  X O . This is intractable and we have to use computational approximations. We describe next how to approximate the posterior with a tractable distribution. As in most non-trivial Bayesian models, the posterior (8) is intractable. Therefore we approximate this distribu-tion using expectation propagation (EP) (Minka, 2001) and variational Bayes (VB) (Ghahramani &amp; Beal, 2001). We approximate p (  X  ,  X  , R  X O | R O ) with Q (  X  ,  X  , R  X O Q 1 (  X  ) Q 2 (  X  ) Q 3 ( R  X O ) , where Q 1 , Q 2 and Q 3 are given by Algorithm 1 Approximate Inference in the Joint Model The parameters of Q are fixed by running EP and VB on the complete data model (CDM) and VB on the missing data model (MDM). We use EP for the CDM as it performs well in ordinal regression (Chu &amp; Ghahramani, 2005). How-ever, EP is known to perform poorly for factors correspond-ing to matrix factorizations, so we use VB for these. In par-ticular, we use a stochastic version of VB called stochastic variational inference (SVI) (Hoffman et al., 2013) so that our computational cost scales with the number of observed ratings |O| and not with the size of R , which can be very large. Algorithm 1 summarizes our inference procedure. We first adjust the components in Q for the two models in-dependently. After that, we co-train the models. For this, we iteratively re-adjust Q by jointly refining the approx-imate posterior of each model while taking into account the predictions of the other. Full details can be found in the supplementary material. The code for our MF-MNAR method is publicly available at http://jmhl.org . 4.1. Predictive Distribution in the Joint Model Given Q , we can approximate the joint model X  X  posterior th column of R takes value l , conditioned on a specific value of x i,j . When x i,j = 0 , we assume that the entry was not selected by the MDM and it is missing. When x i,j = 1 , the entry was selected by the MDM and should have been observed, but its value is unknown (for example, if it was held out in a test set). The value of p JM i,j,l approximated using of the CDM and the MDM generated using the approxi-mate posterior Q , respectively, and their exact values can be found in the supplementary material. In this formula,  X  p i,j,l ( x i,j ) improves the prediction of the CDM,  X  p by using the information available in x i,j .
 Marlin &amp; Zemel (2009) have proposed a method for mod-eling MNAR rating data in collaborative filtering. They use a Mixture of Multinomials (MM) as their CDM. In the MM model, there are K clusters of users and each item has K multinomial rating distributions associated to it. To rate item j , all the users in the k -th cluster sample from the k -th multinomial distribution associated with the j -th item. The EM algorithm is used to adjust the MM model with MAR data. This MM CDM is simple enough to be extended to MNAR data at a low computational cost. However, in prac-tice, the MM model lacks flexibility and is often outper-formed by more powerful MF approaches such as the MF method described in Section 3.1.
 Marlin &amp; Zemel (2009) propose two different missing data models for the MM CDM. The first, CPTv, assumes that p ( x i,j | r i,j ) =  X  r i,j , where  X  is an L -dimensional probabil-ity vector such that  X  l is the probability that r i,j is observed if its value is l . The second MDM, Logitvd, assumes that tion,  X  j  X  R models a per-item bias and the parameters  X  ,..., X  L  X  R model the dependence of x i,j on the value of r i,j . These MDMs can be jointly estimated with the MM CDM using EM. The resulting method is computationally efficient and has approximately the same cost as learning the MM under the MAR assumption.
 Our MDM (see Section 3.2) is more flexible than CPTv or Logitvd. The variables  X  row and  X  col allow us to encode dependencies between r i,j and x i,j that can change across users, items and values of r i,j . For example, we can capture effects such as groups of users with different rating behav-iors: users that rate what they like and others that rate what they dislike; or certain items that are only rated by users who dislike them. Furthermore, we use a MF component to capture global effects that are independent of R . For ex-ample, a set of items that are strongly promoted and hence observed with high frequency. We also do full Bayesian in-ference instead of MAP estimation as in Logitvd and CPTv. This makes our MDM robust to overfitting problems. Fi-nally, we obtain the same scalability as CPTv and Logitvd by using stochastic inference methods (see Section 4 and the supplementary material).
 MNAR rating data is also considered by Steck (2010). This method is similar to the BPR algorithm (Rendle et al., 2009) and works by optimizing the parameters of a non-probabilistic MF model with respect to a ranking-based loss function. Steck (2010) does not learn a generative model for the data and optimizes a metric that is robust to MNAR data. The resulting method can be applied to recommendation, but not to rating prediction tasks. The previous pioneering works have addressed the problem of modeling MNAR rating data. However, these early ap-proaches are limited to relatively simple models. Bayesian MF models are highly flexible and often yield state-of-the-art predictive performance on rating data. We present the first approach to extend these methods to the MNAR sce-nario. We analyze the performance of our Matrix Factorization model with data Missing Not At Random (MF-MNAR) in a series of experiments with synthetic and real-world rating data. We compare MF-MNAR with several bench-mark methods including i) a version of MF-MNAR that as-sumes data Missing At Random (MF-MAR); ii) a Mixture of Multinomials model for MAR rating data (MM-MAR) (Marlin &amp; Zemel, 2009); the iii) CPTv and iv) Logitvd models for MNAR rating data proposed by Marlin &amp; Zemel (2009); v) a state-of-the-art method for ordinal matrix fac-torization with MAR data (Paquet et al., 2012) (Paquet) and finally, vi) an oracle method that always predicts labels ac-cording to their empirical frequencies in the test set (Ora-cle). In all MF methods we use a latent rank of size 20. In the mixture of multinomials we use 20 components. 6.1. Datasets Our first synthetic dataset is a toy example that illustrates the differences between rating data missing at random (MAR) and rating data missing not at random (MNAR) (Steck, 2010). The dataset contains items that are horror movies (items 1 to 50) or romance movies (items 51 to 100) and users who are romance-lovers (users 1 to 100 and 201 to 300) or horror-lovers (users 101 to 200 and 301 to 400). We consider discrete ratings with values from 1 to 5. Romance-lovers (horror-lovers) will rate romance movies (horror movies) by sampling from a multinomial with prob-the probability of rating the movie with value i . Similarly, romance-lovers (horror-lovers) will rate horror movies (ro-mance movies) using a multinomial with probability vector p , where p 0 i = p 6  X  i . The left-hand plot in Figure 3 shows the complete rating matrix for this dataset. We have con-sidered two missing data mechanisms. In the first (MAR), each matrix entry is observed independently with proba-bility p  X  0 . 23 . In the second (MNAR), users 1 to 200 are  X  X ositive X , they tend to rate what they like. Each rating from these users is observed with probability p i , where i is the value of the entry. However, users 201 to 400 are  X  X egative X , rating what they do not like. Each rating with value i is now observed with probability p 0 i . The plot in the middle of Figure 3 shows the observed data for the MAR setting, while the right-hand plot shows the observed data for the MNAR setting. In the latter case, there is a clear Positive Negative pattern relating the value of an entry with its observation status. Note that the pattern changes from rows 1-200 to rows 201-400.
 The second synthetic dataset is obtained by sampling from a matrix factorization model. We generate two 500  X  10 matrices U and V with standard Gaussian i.i.d. entries. We then generate C = UV T  X  3 and partition R in contiguous intervals with boundaries  X  X  X  ,  X  6 ,  X  2 , 2 , 6 ,  X  . For each c i,j , we create an r i,j  X  { 1 ,..., 5 } according to the in-terval in which c i,j lies. We generate two extra 500  X  10 matrices E and F with standard Gaussian i.i.d. entries. For each r i,j we set the binary variable x i,j to 1 with probabil-function and e i and f j are the i -th and j -th rows of E and F . We consider two missing data mechanisms. In the first (MAR), all the z 1 ,...,z 5 are 0 when we generate the x i,j In the second (MNAR), ( z 1 ,...,z 5 ) = (  X  3 ,  X  3 ,  X  3 , 3 , 3) . In the latter case we expect to see many more ratings with value 4 and 5 than in the MAR scenario.
 We considered several real-world rating datasets. The MovieLens 100k and 1M datasets 1 include ratings from 1 to 5 on movies. The Yahoo! Web-scope R3 dataset 2 . con-tains ratings from 1 to 5 on songs. We also analyzed a dataset obtained from the reviewer bidding process for the 2013 NIPS conference. This dataset contains ratings from 1 to 4 given by reviewers on papers. Finally, the Movie Tweetings dataset 3 includes ratings on movies collected from Twitter. We pre-processed this dataset to map the original ratings from 0 to 10 to the interval [1 , 5] using the rule r mapped = d r original / 2 e except for 0, which was mapped to 1. Furthermore, because this dataset is very sparse, we only kept the users and movies that have at least 10 ratings. Table 1 shows a summary of the datasets. Note that only the Yahoo and the synthetic datasets include data entries with x i,j = 0 . These entries were collected for the Yahoo dataset by eliciting ratings on items selected randomly, not by the users themselves. In all the other real-world datasets we only have the entries from the rating matrix R that are selected by the missing data mechanism, that is, entries for which x i,j = 1 . 6.2. Experimental Protocol For each dataset, we collect the available rating entries r with x i,j = 0 in a special test set; this set is only avail-able for the Yahoo and synthetic datasets. After that, we randomly split the observed ratings (the r i,j with x i,j into a training set with 99% of the ratings and a standard test set with the remaining 1%. We use small standard test sets to avoid interfering with the actual missing data mech-anism. When an r i,j with x i,j = 1 is added to the standard test set we fix that x i,j to zero to indicate that the entry r is not observed. Each method is adjusted on each training set and then evaluated on the corresponding standard and special test sets by computing the average predictive log-likelihood of the ratings. The whole process is repeated 40 times. The supplementary material contains results for other performance metrics: root mean squared error, mean absolute error, and predictive accuracy.
 The previous procedure focuses mainly on evaluating the performance of the complete data model. However, we are also interested in evaluating the missing data model. For this we try to find the x i,j that initially took value one but were flipped to zero during the generation of the standard test sets. We use recall at N to measure performance as this is a popular metric in recommendation tasks (Gunawardana &amp; Shani, 2009). For each row i of X , we use the missing data model to compute the probability that the variables x i,j with value zero in that row originally took value one. We select the top N = 10 variables with highest probabil-ity and the fraction of variables x i,j with original value one that are recovered and average the recall across rows. 6.3. Results Table 2 shows the average test log-likelihood obtained by each method on the standard rating test sets. The best per-forming method is highlighted in bold and the statistically indistinguishable results, according to a paired t -test, are underlined. MF-MNAR is the best method on the real-world datasets and on the SMF-MNAR dataset. Further-more, in accordance with intuition, MF-MNAR is better than MF-MAR on the synthetic datasets with MNAR data, while the opposite result occurs on the synthetic datasets with MAR data. The models based on mixtures of multi-nomials, MM, CPTv and Logitvd, obtain the best perfor-mance on the SRH-MNAR and SRH-MNAR datasets. This is because these datasets were generated by sampling from multinomials, as assumed by these models. The oracle method is generally outperformed by most methods since it predicts the same values for all entries. Finally, Paquet X  X  method is less accurate than MF-MNAR and MF-MAR.
 Table 3 shows the average recall obtained by the missing data models of those methods that assume MNAR data. In this table we have also included the results obtained by the missing data model of MF-MNAR (MDM) without cou-pling it with the complete data model. This allows us to evaluate the gains produced in MDM by using the predic-tions of the complete data model. We also include the re-sults of a baseline (Freq) that, for each row i of R , ranks the variables x i, 1 ,...,x i,d with value zero by their empiri-cal frequency across rows. The missing data model of MF-MNAR obtains the best results in all cases, except again in-tuitively on the SMF-MAR and SHR-MAR datasets, where the MAR assumption is appropriate. Overall, all the meth-ods outperform Freq except CPTv, which performs worst. Our sample of the SHR-MAR dataset seems to be particu-larly suited to Logitvd, which performs best in this case. Finally, Table 4 shows the average test log-likelihood of each technique on the special test sets for predicting ratings when x i,j = 0 . In this case, the Oracle baseline obtains the best results on most datasets with MNAR data, assuming that the Yahoo dataset is MNAR. The Oracle method al-ways makes the same probabilistic prediction for each test entry. This shows the difficulty of making accurate predic-tions on MNAR data. Despite this, MF-MNAR is better than MF-MAR in all of the datasets with MNAR data, ex-cept on the real-world Yahoo dataset, where MF-MAR per-forms better. In the MAR datasets, MF-MAR is better than MF-MNAR, as expected. Note that Logitvd and CPTv do not produce any improvement on the SRH-MNAR dataset with respect to MM. This is because these models make in-correct assumptions about the missing data mechanism that was used to generate this dataset. We have presented the first practical implementation of a probabilistic matrix factorization (MF) model for ordi-nal matrix data with entries missing not at random (MF-MNAR). The missing data model in MF-MNAR is a MF model for binary matrices in which the observation prob-ability of a matrix entry depends on the entry X  X  value, with different dependence strengths across rows and across columns. The complete data model in MF-MNAR is an ordinal MF method that generates state-of-the-art predic-tions on rating data on its own. Approximate Bayesian in-ference in MF-MNAR is implemented using expectation propagation and variational Bayes. We achieve scalability to large datasets by using stochastic inference methods that randomly sub-sample missing matrix entries. The combi-nation of the missing and complete data models in MF-MNAR produces gains in both the modeling of the missing data mechanism and the modeling of the ordinal ratings. Acknowledgements NMTH is a recipient of the Google Europe Fellowship in Statistical Machine Learning. JMH acknowledges support from Infosys Labs, Infosys Limited and from the Spanish Direcci  X  on General de Investigaci  X  on, project ALLS (TIN2010-21575-C02-02).
 Chu, Wei and Ghahramani, Zoubin. Gaussian processes for ordinal regression. In Journal of Machine Learning Research , pp. 1019 X 1041, 2005.
 Ghahramani, Z. and Beal, M. J. Advanced Mean Field
Method X  X heory and Practice , chapter Graphical mod-els and variational methods, pp. 161 X 177. 2001.
 Gunawardana, Asela and Shani, Guy. A survey of accu-racy evaluation metrics of recommendation tasks. The
Journal of Machine Learning Research , 10:2935 X 2962, 2009.
 Hoffman, Matthew D., Blei, David M., Wang, Chong, and
Paisley, John. Stochastic variational inference. Journal of Machine Learning Research , 14:1303 X 1347, 2013. Kschischang, Frank R., Frey, Brendan J., and Loeliger, H.-A. Factor graphs and the sum-product algorithm.
IEEE Transactions on Information Theory , 47(2):498 X  519, 2001.
 Lakshminarayanan, Balaji, Bouchard, Guillaume, and Ar-chambeau, Cedric. Robust Bayesian matrix factorisa-tion. In International Conference on Artificial Intelli-gence and Statistics , pp. 425 X 433, 2011.
 Little, R.J.A. and Rubin, D.B. Statistical Analysis With Missing Data . Wiley Series in Probability and Statistics -
Applied Probability and Statistics Section Series. Wiley, 1987.
 Marlin, Benjamin M. and Zemel, Richard S. Collabora-tive filtering and the missing at random assumption. In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence , 2007.
 Marlin, Benjamin M. and Zemel, Richard S. Collaborative prediction and ranking with non-random missing data.
In Proceedings of the Third ACM Conference on Recom-mender Systems , RecSys  X 09, pp. 5 X 12, 2009.
 Minka, Thomas P. Expectation propagation for approx-imate bayesian inference. In Proceedings of the Sev-enteenth conference on Uncertainty in Artificial Intelli-gence , pp. 362 X 369, 2001.
 Mnih, Andriy and Salakhutdinov, Ruslan. Probabilistic matrix factorization. In Advances in neural information processing systems , pp. 1257 X 1264, 2007.
 Paquet, Ulrich, Thomson, Blaise, and Winther, Ole. A hier-archical model for ordinal matrix factorization. Statistics and Computing , 22(4):945 X 957, 2012.
 Rendle, Steffen, Freudenthaler, Christoph, Gantner, Zeno, and Schmidt-Thieme, Lars. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artificial In-telligence , pp. 452 X 461, 2009.
 Steck, Harald. Training and testing of recommender sys-tems on data missing not at random. In Proceedings of the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining , KDD  X 10, pp. 713 X 722, 2010.
 Stern, David H, Herbrich, Ralf, and Graepel, Thore.
Matchbox: large scale online bayesian recommenda-tions. In Proceedings of the 18th international confer-
