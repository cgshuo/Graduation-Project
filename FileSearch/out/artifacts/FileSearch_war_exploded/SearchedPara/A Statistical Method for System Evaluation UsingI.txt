 We consider the problem of large-scale retrieval evaluation, and we propose a statistical method for evaluating retrieval systems using incomplete judgments. Unlike existing tech-niques that (1) rely on effectively complete, and thus pro-hibitively expensive, relevance judgment sets, (2) produce biased estimates of standard performance measures, or (3) produce estimates of non-standard measures thought to be correlated with these standard measures, our proposed sta-tistical technique produces unbiased estimates of the stan-dard measures themselves.
 Our proposed technique is based on random sampling. While our estimates are unbiased by statistical design, their variance is dependent on the sampling distribution employed; as such, we derive a sampling distribution likely to yield low variance estimates. We test our proposed technique using benchmark TREC data, demonstrating that a sampling pool derivedfromasetofrunscanbeusedtoefficientlyandef-fectively evaluate those runs. We further show that these sampling pools generalize well to unseen runs. Our exper-iments indicate that highly accurate estimates of standard performance measures can be obtained using a number of relevance judgments as small as 4% of the typical TREC-style judgment pool.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  Performance evaluation Theory, Measurement, Experimentation Evaluation, Sampling, Average Precision
We gratefully acknowledge the support provided by NSF grant CCF-0418390.
 Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00.
We consider the problem of large-scale retrieval evaluation and propose a statistical technique for efficiently and effec-tively estimating standard measures of retrieval performance via random sampling. Standard methods of retrieval evalua-tion can be quite expensive when conducted on a large-scale. For example, in many years of the annual Text REtrieval Conference (TREC), upwards of 100 runs each consisting of a ranked list of up to 1,000 documents were submitted with respect to each of 50 or more topics. In principle, each document in the collection would need to be assessed for relevance with respect to each query in order to evaluate many standard retrieval measures such as average precision and R-precision; in practice, this is prohibitively expensive. TREC instead employs depth pooling wherein the union of the top k documents retrieved in each run corresponding to a given query is formed, and the documents in this depth k pool are judged for relevance with respect to this query. In TREC, k =100hasbeenshowntobeaneffectivecutoffin evaluating the relative performance of retrieval systems [8, 12], and while the depth 100 pool is considerably smaller than the document collection, it still engenders a large as-sessment effort: in TREC8, for example, 86,830 relevance judgments were used to assess the quality of the retrieved lists corresponding to 129 system runs in response to 50 topics [11]. The table given below shows the relationship between pool depth and the number of judgments required per topic on average for various TRECs.
 Shallower pools [12] and greedily chosen dynamic pools [6, 2] have also been studied in an attempt to alleviate the as-sessment effort; however, such techniques tend to produce biased estimates of standard retrieval measures, especially when relatively few relevance judgments are used. Recently, Buckley and Voorhees proposed a new measure, bpref ,and they show that when bpref is employed with a judged sam-ple drawn uniformly from the depth 100 pool, the retrieval systems can be effectively ranked [5]. However, bpref is not designed to approximate any standard measure of retrieval performance, and while bpref is correlated with average pre-cision, especially at high sampling rates, these correlations and the system rankings produced degenerate at low sam-pling rates.

Our goal in this work is to efficiently and accurately es-timate standard measures of retrieval performance. Unlike previously proposed methodologies which tend to produce biased estimates of standard measures using few relevance judgments or methodologies based on estimating measures other than the most widely reported standard measure, our methodology, by statistical design, produces unbiased es-timates of the standard measures of retrieval performance themselves.

The core of our methodology is the derivation, for each measure, of a distribution over documents such that the value of the measure is proportional to the expectation of observingarelevantdocumentdrawnaccordingtothatdis-tribution. (In the case of average precision, the distribu-tion is over pairs of documents, and the observation is the product of the relevances for the pair drawn.) Given such distributions, one can estimate the expectations (and hence measurement values) using random sampling. By statisti-cal design, such estimates will be unbiased .Furthermore, through the use of the statistical estimation technique of importance sampling [1], we show how low variance esti-mates of multiple retrieval measures can be simultaneously estimated for multiple runs given a single sample. In sum, we show how both efficient and effective estimates of stan-dard retrieval measures can be inferred from a random sam-ple, thus providing an alternative to large-scale TREC-style evaluations.

We tested our methodology using the benchmark TREC data collections, and results are reported for TRECs 7, 8, and 10. We compare the performance of TREC-style depth pools to sampling pools of an equivalent size. Generally speaking, as the number of relevance assessments is de-creased (either by considering a lower depth pool, in the case of TREC, or by sampling at a lower rate, in the case of the proposed technique), the bias and variance of the estimates inferred from TREC-style depth pools increases, while only the variance of estimates inferred from sampling pools increases. At equivalent levels of judgment effort, the estimates produced by sampling are consistently better than those produced by depth pooling, often dramatically so.
While TREC-style depth pools are used to evaluate the systems from which they were generated, they are also of-ten used to evaluate new runs which did not originally con-tribute to the pool. Depth 100 TREC-style pools generalize well in the sense that they can be used to effectively evalu-ate new runs, and this has been a tremendous boon to re-searchers who use TREC data. We also show that random sampling pools generalize well, achieving estimation errors on unseen runs comparable to the estimation errors on runs from which the sample was drawn.

In the sections that follow, we describe our methodology and the results from experiments using the TREC collec-tions. We conclude with a summary and directions for fu-ture research.
In this section, we sketch our proposed statistical tech-nique for efficiently and effectively estimating standard re-trieval measures from random samples. 1 While many of the details are somewhat complex and/or necessarily omitted for space considerations, the basic ideas can be summarized as follows: 1. For each measure, we derive a random variable and as-2. Given that the value of a measure can be viewed as the 3. Finally, to implement this methodology effectively ,one
In the sections that follow, we describe each of these ideas in more detail, and we then present extensive experiments using TREC data.
While our goal is to simultaneously estimate multiple mea-sures of performance over multiple lists, we begin by con-sidering the problem of estimating average precision from a random sample. Unlike R-precision or precision at stan-dard cutoffs, deriving a sampling distribution for average precision is non-trivial, and it yields a distribution which is empirically quite useful for estimating the other measures of interest.

By definition, average precision ( AP ) is the average of the precisions at all relevant documents, or equivalently, the
A somewhat different treatment of this material may be found in our recent workshop work-in-progress report [4]. Table 1: (Left) Weights associated with pairs of ranks; normalizing by Z yields an asymmetric joint distribution. (Right) Symmetric weights; normal-izing by 2 Z yields the symmetric joint distribution JD . sum of the precisions at all relevant documents divided by R , the number of documents relevant to the query. Let SP be this sum of precisions at all relevant documents. In what follows, we first discuss estimating SP , and later we discuss estimating R ; our estimate of AP will be the ratio of the estimates of SP and R .

One can compute sum precision as follows, where Z is the length of the retrieved list, rel ( i ) is the binary relevance of the document at rank i ,and R is the number of documents relevant to the query.
 SP = Thus, in order to evaluate SP , one must compute the weighted product of relevances of documents at pairs of ranks, where for any pair j  X  i , the associated weight is 1 /i . (See Table 1, left.)
In order to view this sum as an expectation, we define an event space corresponding to pairs of ranks ( i,j ), a ran-dom variable X corresponding to the product of the binary relevances rel ( i )  X  rel ( j ), and an appropriate probability dis-tribution over the event space. One such distribution cor-responds to the (appropriately normalized) weights given in Table 1(left); for convenience, we shall instead define a symmetrized version of these weights (see Table 1(right)) and the corresponding joint distribution JD (appropriately normalized by 2 Z ). It is not difficult to see that where the expectation is computed with respect to either distribution. Thus, if U is a multiset of pairs drawn accord-ing to JD , we obtain the following estimate for SP
One is often faced with the task of evaluating the average precisions of many retrieval systems with respect to a given query (as in TREC), and in a naive implementation of the technique described, the documents judged for one system will not necessarily be reused in judging another system. In contrast, TREC creates a single pool of documents from the collection of runs to be evaluated, judges that pool, and evaluates all of the systems with respect to this single judged pool. In order to combat this potential inefficiency, we shall construct a single distribution over pairs of documents de-rived from the joint distributions JD s associated with every system s .

We shall effectively be sampling from a distribution dif-ferent from the one necessary to estimate the expectations desired. To combat this, we introduce scaling factors as fol-lows. Let D ( i,j ) be the joint distribution over documents from which we effectively sample. Note that i and j now de-note documents, not ranks. Similarly abusing notation, let JD s ( i,j ) denote the joint distribution over documents (not ranks) required to estimate the proper expectation for sys-tem s . We define scaling factors SF s ( i,j ) which correspond to the ratio between the desired and sampling distributions where D s is the distribution induced by D over documents retrieved by s .Wethenhave where Z s is the length of the list returned by system s and U s  X  U is the subset of samples corresponding to documents retrieved by s . Note that the above formulation holds for any sampling distribution D . In what follows, we describe a heuristic for determining a good sampling distribution X  one which corresponds to a distribution over documents (for efficiency) and which explicitly attempts to minimize the variance in the estimates produced (for accuracy).
The technique described above can be used to estimate the average precision of one or more retrieval runs with respect to any given query. However, it is relatively inefficient: (1) On a per run basis, independent and identically distributed (i.i.d.) pairs of documents are drawn and judged, but the induced pairs of judged documents across i.i.d. samples are not used. In order to combat this potential inefficiency, we shall instead draw a sample from a distribution over docu-ments and consider all induced pairs of judgments.
In determining a sampling distribution D ,weconsidertwo factors. First, we impose the condition that D be a symmet-ric product distribution, i.e., D ( i,j )= M ( i )  X  M ( j )forsome (marginal) distribution over documents M . The purpose for this is efficiency: we will sample documents according to M and consider all induced pairs of documents, which will be distributed (approximately) according to D . Second, we seek a D which explicitly attempts to minimize the variance in our estimator, for accuracy. We begin by considering the latter factor.

Variance minimization. For a sampling distribution D and a given system s ,let D s be the distribution induced by D over pairs of documents contained in the list returned by system s . Furthermore, let Y be the random variable rel ( i )  X  rel ( j )  X  SF s ( i,j ) such that SP = Z s  X  Z s and R are fixed, in order to minimize the variance of AP , we must minimize the variance of Y .

Var [ Y ]= E [ Y 2 ]  X  E 2 [ Y ] To minimize this variance, it is enough to minimize the first term since SP /Z s is fixed. Employing importance sampling techniques for minimizing the variance of Monte Carlo esti-mators, one can derive that the best sampling distribution D is the distribution induced by JD s over relevant documents. (See Anderson [1] for an example of such a derivation.) Of course, we do not have the complete relevance judgments necessary to calculate the ideal sampling distribution. How-ever, the marginal distribution MD s ( i )= sociated with the average precision sampling distribution JD s ( i,j ) has been shown to be a reasonable prior for rel-evant documents [3], and using such a prior one can ar-gue that a sampling distribution D s ( i,j ) proportional tails omitted for space considerations.) D s ( i,j ) is a product distribution having identical marginals with respect to i and j ;let MD s ( i ) be the marginal associated with D s ( i,j ).
If our task were to estimate the performance of only one retrieval system, we could sample documents according to MD s ( i ), consider all induced pairs of documents, and es-timate AP using appropriate scaling factors. However, in general our task is to simultaneously estimate AP for N systems from a single sample. We obtain a final sampling
The expression must be normalized to form a distribution. marginal M ( i ) by averaging the marginals associated with each system s .
 We finally note that in a typical TREC setting, one averages AP over 50 queries to obtain a final estimate of the perfor-mance of a system, and this averaging results in a further significant variance reduction.

Exact computation of scaling factors. M ( i )isthe distribution we use for sampling documents, and given a sample of K such documents, we consider all K 2 induced pairs and estimate the required expectations from these in-duced pairs and appropriate scaling factors. For sufficiently large K , the distribution over induced pairs will approximate the associated product distribution D ( i,j )= M ( i )  X  M ( j ); however, the actual distribution is multinomial.
As a consequence, if K s is the size of the subset of K sampled documents which are retrieved by system s ,one obtains the following final scaling factors:
Finally, we derive the exact form of the multinomial sam-pling distribution over induced pairs. Sampling K docu-ments (with replacement) from a distribution M and form-ing all K 2 pairs of documents yields a multinomial distribu-tion I over the possible outcomes. Let k =( k 1 ,k 2 ,...,k correspond to counts for the sampled documents where k 1 + k +  X  X  X  + k W = K and k i is the count associated with doc-ument i .ThenPr( k )=
For i = j and k&gt; 2, the induced pairs distribution is derived as follows I ( i,j )= When i = j , a similar derivation yields I ( i,i )= 1
To obtain an estimate for AP , we must know or obtain estimates for SP , R and Z s , and the expectation described above. We have described in detail how to estimate SP , and Z s is a known quantity (the length of the system X  X  re-turned list). However, R , the total number of documents relevant to the given query, is not typically known and must also be estimated. Sophisticated approaches for estimating R exist [9]; however, in this preliminary study we employ techniques similar to those described above. In order to estimate R (as calculated by TREC), one could simply uni-formly sample documents from the depth 100 pool. Given that our sample is drawn according to M ( i ) instead, one can employ appropriate scaling factors to obtain the correct estimate.
To estimate precision-at-cutoff c , one could simply uni-formly sample documents from the top c in any given list. Given that we sample documents according to M ( i ), we again employ appropriate scaling factors to obtain correct estimates for PC ( c ).

R-precision is simply the precision-at-cutoff R .Wedo not know R ; however, we can obtain an estimate b R for R as described above. Given this estimate, we simply estimate PC ( b R ).
Below we give a summary for implementing the sampling method. Figure 3 can serve as a guide. (1) For each run s , use the joint distribution over pairs of documents, JD s ( i , j ), dictated by SP such that sampling pairs of documents according to JD would yield SP in ex-pectation. (2) To minimize variance, we introduce a prior over rel-evant documents (Figure 3, step 2). Let MD s ( i )bethe marginal distribution of JD s ( i,j ), and let D s ( i,j )bethe (appropriately normalized) joint distribution corresponding tribution of D . This is the sampling distribution over doc-uments that would be used for run s . (3) Over all runs, compute M ( i ), the average of these sampling distributions over documents. M ( i ) is our single, final sampling distribution for the query (all runs). (4) Sample K documents with replacement according to M ( i ) (Figure 3, black box) until T unique documents are drawn; judge these documents. ( T is the desired apriori judgment effort.) Generate all K 2 pairs of judged docu-ments. (5) Compute the multinomial induced pairs distribution (Figure 1, step 1), I ( i,j ); this is the  X  X ffective X  distribu-tion over pairs of documents from which we sampled. From I ( i,j )and JD s ( i,j ), compute the required scaling factors. (6) From the induced pairs and the scaling factors, com-pute the estimates of SP for each run (Figure 1black box). (7) Estimate R using the sampled documents drawn ac-cording to M ( i ) and appropriate scaling factors (Figure 3). (8) Estimate AP by the ratio of the estimates for SP and R . Note that the estimate of a ratio is not necessarily the ratio of estimates. More accurate ratio estimates derived via a second order Taylor series approximation [10] were tested, and they were generally found to be of little benefit for the computational effort required.
A question of particular importance is how can we use the samples generated by our method to evaluate a new run, i.e. a run that did not contribute to the sampling distribu-tion. In order for sampling to work correctly, there should be sufficient sampled documents in the new run so that the evaluation using sampling is meaningful.

The evaluation (estimation) methodology is independent of the fact that the run participated to the sampling process; therefore it can be applied to the new runs in the same way as for the runs used in producing the sample. On the scale factor computation, the numerator is a function of the ranks of sampled documents in the new list and the denominator is computed based on the sampling distribution conditioned to the new run.

In TREC data, it is already the case that the actual pools are created from only a subset of the submitted runs. In all our experiments, to computing the average sampling distri-bution, we only use the runs that contributed to the pool ( training systems ) and use the runs that did not contribute to the pool as testing systems . In the plots that follow, the dots (  X  ) in the plots correspond to the training systems, and the pluses (+) correspond to the testing systems.
We tested the proposed sampling method as a mechanism for estimating the performance of retrieval systems using data from TRECs 7, 8 and 10. We used mean average pre-cision (MAP), mean R-precision (MRP), and mean preci-sion at cutoffs 5, 10, 15, 20, 30, 100, 200, 500, and 1000 (MPC(c)) as evaluation measures. We compared the esti-mates obtained by the sampling method with the  X  X ctual X  evaluations, i.e. evaluations obtained by depth 100 TREC-style pooling. The estimates are found to be consistently good even when the total number of documents judged is far less than the number of judgments used to calculate the actual evaluations.

To evaluate the quality of our estimates, we calculated three different statistics, root mean squared (RMS) error (how different the estimated values are from the actual val-ues, i.e. RMS = tual and e i are the estimates values), linear correlation co-efficient  X  (how well the actual and estimated values fit to a straight line), and Kendall X  X   X  (how well the estimated mea-sures rank the systems compared to the actual rankings). Figure 4: Sampling vs. depth pooling mean av-erage precision estimates at depths 1 and 10 in TREC8. Each dot (  X  ) corresponds to a distribution-contributor run and each plus (+) to a distribution-non-contributor run (there are 129 runs in TREC8.) Note that in contrast to the RMS error, Kendall X  X   X  and  X  do not measure how much the estimated values differ from the actual values. Therefore, even if they indicate perfectly correlated estimated and actual values, the estimates may still not be accurate. Hence, it is much harder to achieve small RMS errors than to achieve high  X  or  X  values. Be-cause of this, we mainly focus on the RMS error values when evaluating the performance of the sampling method.
Since the performance of the sampling method varies de-pending on the actual sample, we sampled 10 times and picked a representative sample that exhibited typical per-formancebasedonthethreeevaluationstatisticsused.
We report the results of the experiments for MAP, MRP, and MPC(100) on TREC8 in Figure 4, Figure 5, and Fig-ure 6, respectively. As can be seen, on TREC8 , for both depth= 1(on avg 29 judgments/query) and depth= 10 (on avg 200 judgments/query), there is a significant improve-ment in all three statistics when sampling is used versus the TREC-style pooling for all the measures. The sampling estimates have reduced variance and little or no bias com-pared to depth pooling estimates. This can be seen from the great reduction in the RMS error when the estimates are obtained via sampling. Furthermore, the bottom-right plots of all three figures show that with as few as 200 relevance judgments on average per query, the sampling method can very accurately estimate the actual measure values which were obtained using 1,737 relevance judgments on average per query.

Figure 7 illustrates how MAP estimates using TREC-style depth pooling compare in terms of  X  and Kendall X  X   X  with those obtained using sampling as the depth of the pool changes. For depths 1to 10, we first calculated the number of documents required to be judged using TREC-style depth pooling. Then, for each depth, we formed 10 different samples of the same size as the required judgment Figure 5: Sampling vs. depth pooling mean R-precision estimates at depths 1 and 10 in TREC8. Figure 6: Sampling vs. depth pooling mean prec at cutoff 100 estimates at depths 1 and 10 in TREC8. set for each corresponding depth and calculated the average  X  (left column) and  X  (right column). As can be seen in the figure, for all TRECs the sampling method significantly outperforms the TREC-style depth pooling method at all depths.
 For comparison purposes, we also include the average Kendall X  X   X  value of bpref obtained using random samples [5] of the given size to the plots in the second column. The Kendall  X  values for bpref are the average values computed over 10 different random sampling distributions.
While the MAP (and MRP and MPC) show improved performance over depth-style pooling in both mean and bias, there are certain situations when one needs the results of a single query, hence not taking advantage of the massive Figure 7: Linear correlation coefficient and Kendall X  X   X  error comparisons for mean average pre-cision, in TRECs 7, 8 and 10. Figure 8: Sampling estimates for a query with mixed system performance. Dots (  X  ) represent training runs; pluses ( + ) represent testing runs. . variance reduction achieved by averaging over 50 queries. It is certainly not expected to see the same kind of performance on per query basis; however our results show definite usable query estimates (Figure 8). The method described in this paper is self-contained for a query, i.e. estimates for a query are not dependant on any data from other queries.
On a different setup, one may want to analyze only one run over all queries (Figure 9) Note that this setup is not self contained as the sampling method requires a set of runs (not only the one plotted) and associated judgments.
It is important that the performance of sampling over the testing runs is virtually as good as the performance over the training runs. Note that the testing systems do not directly contribute to the sampling pool; their documents are sampled only because they happen to appear in training runs.

The trend of RMS error, as sample size increases from Figure 9: Sampling estimates for a fixed typical run (Sab8A1) with MAP = 0.25, all queries. . Each dot (  X  ) is an AP for a query estimate (total 50); MAP estimate is plotted as  X   X   X . depth 1to depth 10 equivalent for training and testing sys-temsisshowninFigure10. On x -axis the units are the depth-pool equivalent number of judgments converted into percentages of depth-100 pool.
We propose a statistical technique for efficiently and effec-tively estimating standard measures of retrieval performance from random samples, and we demonstrate that highly ac-curate estimates of standard retrieval measures can be ob-tained from judged subsamples as small as 4% of the stan-dard TREC-style depth 100 pool.

This work leaves open a number of question for further research: (1) In standard TREC settings, all documents in the depth 100 pool are judged and no documents outside the pool are judged. Our work indicates that more judging effort should be placed on documents near the top of ranked lists (they have high sampling probabilities) and less judg-ing effort should be placed on documents near the bottom of ranked lists (they have low sampling probabilities). What is the optimal sampling distribution, and how does it change as a function of the collection or systems to be evaluated? Consider evaluating retrieval system on the web or with re-spect to the TREC Terabyte track, for example. (2) Given that our technique is based on random sampling, one could in principle derive high probability confidence intervals for the estimates obtained, and such confidence intervals would be quite useful in practice. (3) Finally, we have prelimi-nary results which show that given accurate estimates of retrieval measures obtained from a judged subsample, one can accurately infer relevance assessments for the remaining unjudged documents. Such a technique would have obvious benefits in the production of large test collections. [1] E. C. Anderson. Monte carlo methods and importance [2] J.A.Aslam,V.Pavlu,andR.Savell.Aunifiedmodel Equivalent depths are indicated on the plot. [3] J. A. Aslam, V. Pavlu, and E. Yilmaz. Measure-based [4] J. A. Aslam, V. Pavlu, and E. Yilmaz. A sampling [5] C. Buckley and E. M. Voorhees. Retrieval evaluation [6] G.V.Cormack,C.R.Palmer,andC.L.A.Clarke.
 [7] W.B.Croft,A.Moffat,C.J.vanRijsbergen, [8] D. Harman. Overview of the third text REtreival [9] P.Kantor,M.-H.Kim,U.Ibraev,andK.Atasoy.
 [10] J. A. Rice. Mathematical Statistics and Data Analysis . [11] E. M. Voorhees and D. Harman. Overview of the [12] J. Zobel. How reliable are the results of large-scale
