 Topic representation mismatch is a key problem in topic-oriented summarization for the specified topic is usually too short to un-derstand/interpret. This paper proposes a novel adaptive model for summarization, AdaSum, under the assumption that the sum-mary and the topic representation can be mutually boosted. Ada-Sum aims to simultaneously optimize the topic representation and extract effective summaries. This model employs a mutual boost-ing process to minimize the topic representation mismatch for base summarizers. Furthermore, a linear combination of base summa-rizers is proposed to further reduce the topic representation mis-match from the diversity of base summarizers with a general learn-ing framework. We prove that the training process of AdaSum can enhance the performance measure used. Experimental results on DUC 2007 dataset show that AdaSum significantly outperforms the baseline methods for summarization (e.g. MRP, LexRank, and GSPS).
 H.3.1. [ Content Analysis and Indexing ]: Abstracting Methods; I.2.7. [ Natural Language Processing ]: Text Analysis Algorithms, Performance, Experimentation.
 Topic-oriented Summarization, Topic Representation, Boosting, Ada-Sum
Automated summarization usually deals with concatenating text-span excerpts (i.e., sentences, paragraphs, etc.) into a human under-standable document summary and it dates back to the 1950 X  X  [17]. Along with the fast growth of information amounts on the inter-net, a recent need for multi-document summarization has also been exerted and thus several algorithms have been proposed. Most of the researchers have concentrated on the topic-oriented sentence-extraction summarization methods [19, 14, 18]. Most of former approaches are based on the assumption that specified topics are re-liable enough to be used as "gold standards" for automatic summa-rization. However, the specified topic is usually too simple to un-derstand/interpret, and there exists topic representation mismatch between the specified topic and the expected topic.

To address this challenge, several methods for topic representa-tion have been developed and applied to topic-oriented summariza-tion [16, 2, 11]. In order to alleviate the topic representation mis-match problem in summarization, most previous approaches rec-ognized related terms by term clustering. As first proposed in [16], Lin et al. represented the topic of a document (or a document col-lection) by using a set of terms, known as topic signatures (TS). Barzilay et al. [2] proposed the use of content models to capture constraints on topic selection and organization for texts in a partic-ular domain. In recent research, Harabagiu and Lacatusu [11] rep-resented topics as a structure of themes, where a theme is defined as a cluster of sentences from different documents with similar se-mantic information.

In this paper, we aim to develop a novel summarization algorithm that can optimize any performance measures used through topic representation optimization. Inspired by the work of AdaBoost for classification [8] and AdaRank for information retrieval [22], we propose an adaptive model for topic-oriented multi-document sum-marization, referred to as AdaSum. Normally, the generated sum-mary represents the estimation of the expected topic in the hypoth-esis space (document collections), we assume that the summary and the topic representation can be mutually boosted. In order to minimize the expected risk, AdaSum utilizes a linear combination of base summarizers as its model. During the process of summa-rization, it repeats the process of re-weighting the generated sum-maries, creating a base summarizer, and calculating a weight for the summary.

We show that AdaSum algorithm can optimize topic represen-tation and generate more effective summaries. The proposed ap-proach is evaluated on the DUC 2007 dataset and the results show that the effectiveness of the proposed approach for both topic rep-resentation and document summarization. With comparison to the baseline summarizers, AdaSum shows significantly improvement in performance measures. Moreover, in comparison with the top performing state-of-the-art systems in DUC 2007, our proposed model is also competitive.

The remainder of the paper is organized as follows. Section 2 motivates the need for topic interpretation and representation with a mutual boosting process. The proposed AdaSum is described in Figure 1: Algorithm bias of summarization for topic represen-tation mismatch, where d is the document collection to be sum-marized, S e is the expected summary of d , and S a is an auto-matic summary of a base summarizer. And symbols plus ( + ) and minus (  X  ) stand for the target sentences with summary state and the another target sentences with non-summary state respectively. detail in Section 3. We set up the experiments in Section 4 and give the results and discussions in Section 5. After a summary of related work in Section 6, we conclude this paper and give future work in Section 7.
As a particular kind of multi-document summarization, topic-oriented multi-document summarization aims to produce a sum-mary with respect to a specified topic. Topic-oriented multi-document summarization is faced with more challenges than general multi-document summarization, that the information in the generated sum-mary must represent the specified topic. Unfortunately, the speci-fied topic is usually too simple to understand, and this problem causes the topic representation mismatch in summarization, the same as in IR and QA. For this reason, many current summariza-tion algorithms can only partially represent aspects of the expected topic. Thus, there usually exists bias between the expected topic and the estimated topic. In this paper, we call the above bias as algorithm bias, for the mismatch is caused by the diversity of sum-marization algorithms.

For topic-oriented summarization, the expected summary should represent all aspects of the expected topic. It is a condensed rep-resentation for the expected topic on the summary space. Given a document collection d and the original topic t 0 , the expected topic, t , can be represented by the expectation on the summary space, S . Then, the task of summarization is to generate automatic sum-maries with less difference against the expected summary (or man-ual summary). Suppose there are two kinds of states, where one kind corresponds to the summary state and the other corresponds to non-summary state. Let  X  + ( s ) be the set of sentences with sum-mary state, and  X   X  ( s ) be the set of sentences with non-summary state. Figure 1(a) is an expectation summary for the specified topic, symbols plus ( + ) and minus (  X  ) stand for target sentences with summary state and sentences with non-summary state. S e is the set of sentences with summary state, and d  X  S e is the set of sentences with non-summary state in the document collection d . As Figure 1(a) shows, the expected summary should include all the sentences from  X  + ( s ) with excluding every sentence from  X   X  ( s ) . Unfortu-nately, with the topic representation mismatch in current summa-rization algorithms, the derived summary could inevitably contain sentences from  X   X  ( s ) . For example, as shows in Figure 1(b), there are two sentences with non-summary state in the estimated sum-mary S a , while two sentences with summary state is misplaced in d  X  S a . Figure 2: The process of mutual boosting for summarization, where d is the document collection to be summarized, t 0 and t are the original specified topic and the expected topic respec-tively. In order to alleviate the bias between t 0 and t back process is proposed to optimize the topic representation, where h is the base summarizer.

In order to alleviate the topic mismatch problem for summa-rization, we need effective summarization methods to represent as many aspects of the expected topic as possible. That is, in order to optimize the summarization performance, it is essential to min-imize the expected bias between the estimated topic and the ex-pected topic. For topic-oriented document summarization, the ex-pected summary should be consistent with the topic representation of the specified topic. Thus, there exists a mutual reinforcement between the specified topic and the derived summary. With the mutual reinforcement between the expected topic and the derived summaries, we can assume that the summary and the topic rep-resentation can be mutually boosted, where the derived summary depends on the expected topic and the expected topic can be repre-sented with the derived summary. More precisely, we can optimize the estimation of topic representation with a mutual boosting pro-cess in the hypothesis space.
In this paper, we aim to develop a novel summarization algo-rithm that can directly optimize any performance measures used in document summarization. Intuitively, a good topic representa-tion is benefit to derive a high performance summary, and a high performance summary can more effectively represent the expected topic. Therefore, there exists a reinforcement between the esti-mated topic and the derived summary. And with making use of this reinforcement, estimated topics and derived summaries can be mutually boosted.

As Figure 2 shows, a novel adaptive summarization model can be formulated as a process of mutual boosting, where the k timated topic t k is related to the estimated summary S k  X  1 sponding to its previous topic t k  X  1 and the base summarizer h
The estimated summary S k  X  1 = h ( t k  X  1 , d ) . Thus, the expected topic t e can be represented by k rounds X  feed-back of estimated summaries, and the expected summary S e approximated, relative to t e (the third part in Figure 2). Normally, as a form of feedback, the topic representation can be optimized with the topic mismatch as small as possible. Consequently, the performance of base summarizer can be improved with the adap-tive process for topic representation.

However, as for the diversity of base summarizers, a single base summarizer usually biases to represent some aspects of the topic representation. In order to further improve the diversity of base summarizers, a linear combination of base summarizers can be em-ployed. In that, the process in the second part of Figure 2 can also be formulated as a linear combination of adaptive models for topic representation, where each adaptive model for topic representation corresponds to a base summarizer. Inspired by the boosting algo-rithms for classification and IR, AdaBoost and AdaRank, we have proposed a novel adaptive model (AdaSum) for topic-oriented sum-marization. AdaSum utilizes a linear combination of base summa-rizers as its model. As to minimize the expected bias between the expected topic and the estimated topic, a general learning frame-work is employed based on the performance measures of summa-rization.

Consequently, the process of summarization consists of two steps: (1) a mutual boosting process is employed to derive a new summa-rizer which can represent the expected topic as good as possible, so as to optimize the topic representation and minimize the expected bias between the estimated topic and the expected topic ( the sec-ond part in Figure 2); (2) a linear combination of base summarizers is employed to obtain a new summarizer which can boost the per-formance of derived summaries with a general learning framework.
In the following subsections, we present the design for two steps of our approach, namely adaptive model for topic representation and general learning framework for summarization.
With the assumption of mutual boosting between topics and sum-maries, we first utilize an adaptive boosting process for improving summarization performance with topic representation optimization. The goal is to find the optimized topic representation t the estimated topic is as much as possible similar to the expected topic representation. It is difficult to directly measure the similar-ity between the estimated topic and the expected topic, because the expected topic is inconvenient to obtain directly. We instead at-tempt to maximize the similarity between the estimated topic and the original topic in Equation 1. where t is the estimated topic, t k is the optimized topic, t original topic for the document collection d , h is a base summa-rizer, and  X  (  X  ) is the topic representation (e.g. terms X  distribution of topics).

The adaptive model for topic representation is showed in Figure 3, where t 0 is the original topic for the document collection d , and t is the corresponding target expected topic. Given a specified base summarizer h , with the adaptive boosting process for topic repre-sentation, an estimated topic t i is derived from the previous topic t i  X  1 and its corresponding estimated summary h ( t i  X  1 the adaptive boosting process can be formulated a linear combina-tion as follows: where  X  i = sim(  X  ( h ( t i  X  1 , d )) ,  X  ( t 0 ))  X  sim(  X  ( t Figure 3: The adaptive model for topic representation, where the dashed circle is the target expected topic for t t , t 2 , ..., t k  X  1 , t k are the estimated topics. The estimated topic for the i t h round is obtain from the previous estimated topic t i  X  1 and the base summarizer h . With the adaptive boosting process for topic representation, the estimated topic has mini-mum difference to the expected summary. similarity improvement between the estimated topic and the orig-inal topic. Specifically, when there is no improvement in similar-ity between the estimated topic and the original topic, the adaptive boosting process is automatically terminated.
In order to obtain a new summarizer to boost the performance of derived summaries, we define the general framework of learning for summarization.

In summarizing (testing), given a document collection d and topic t , the system returns a set of sentences, which are top sen-tences in the document collection d according to descending or-der of the relevance scores to the topic t . The relevance scores are calculated with a summarizing function (model). In learning (training), a number of document collections with specified top-ics and their corresponding summarizations (sets of retrieved sen-tences from document collections) are given. The objective of learning is to construct a summarizing function which achieves the best results in summarizing of the training data in the sense of min-imization of a loss function. Ideally the loss function is defined on the basis of the performance measures used in testing.

In training, a set of document collections D = { d 1 , d 2 is given. d i can be described as a set of sentences, i.e. d { s n ( d i ) denotes the size of document collection d i . Each document collection d i is associated with a specified topic t i and a list of summary state of sentence s ij , i.e., whether s ij belongs to the man-ual summary given by humans. Let Y = { 0 , 1 } , then y stands for the target sentence s ij is with summary state, otherwise with non-summary state.

A feature vector x ij =  X  ( s ij , t i )  X  X is created from each Thus, the training set can be represented as S = { ( d i , t
The objective of learning is to create a summarizing function f : X 7 X  X  , such that, for each document collection d and its cor-responding specified topic t , the sentences in d can be assigned rel-evance scores using the function f and be ranked according to the scores. Then top sentences are picked out as topic-oriented sum-marization for d and t . We specially define a permutation mapping t , and the summarizing function. Let d i = { s i 1 , . . . , s identified by the indexing set I = { 1 , . . . , n ( d i ) } , then  X  ( d is defined as a mapping from I to Y n ( d i ) . We use  X  j th element of  X  ( d i , t i , f ) , to denote the summary state of item j (i.e. s ij ). So the learning process comes down to be that of mini-mizing the loss function which presents the disagreement between  X  ( d i , t i , f ) and the list of summary state y i , for all of the docu-ment collections with their corresponding topic.

In summarization, some performance measures are used to eval-uate the "goodness" of a summarizer or a summarizing function. Conventionally, the performance of a summarizer is evaluated by the manual summary. Normally, a manual summary based mea-sure can be defined, as in ROUGE [15]. In this paper, we utilize performance measures. The first argument of E is the permutation mapping  X  created using the summarizing function f on d i second argument is the list of summary state y i given by humans based on the specified topic. E measures the agreement between  X  and y i , i.e., the derived summary and the manual summary.
As an example of performance measures, we present the defi-nitions of MAP in summarization. Given a document collection d , the corresponding list of summary states y i , and a permutation mapping  X  i on d i , average precision for summarization of d defined as: where  X  i =  X  ( d i , t i , f ) ,  X  i ( j ) denotes the summary state of s
The score evaluated by ROUGE is also a popular performance measure for summarization, which is described in section 4.2 in detail.

In learning framework for summarization, the loss function L can be defined on the basis of such general summarization perfor-mance measures, signed as L ( E ) .

In this paper, we define the summarization model as a linear combination of base summarizing functions (base summarizers): where h k ( x ) is a base summarizer,  X  k is its weight, and K is the number of base summarizers. Thus, the hypothesis space for the learning process can be defined as: The learning process for summarization is a process of choosing an appropriate function f from hypothesis space H (a given set of functions). The goal is to find f ( x ) so that the expected loss of f is as small as possible, i.e. where E (  X  ) denotes the expectation of a random variable.
Given a training set f ( x ) in H that minimizes the empirical loss:
With the formal description of learning framework given above, we propose an algorithm inspired by boosting methods, which op-timizes a loss function based on the summarization performance measures. The new summarization algorithm is referred to as "Ada-Sum" and described in Figure 4.

AdaSum takes a training set S = performance measure function E and the number of iteration K as parameters. AdaSum runs K rounds and at each round it creates a base summarizer h k ( k = 1 , . . . , K ) . Finally, it outputs a summa-rizing function f by linearly combining the base summarizers. As can be seen from the algorithm, in each iteration round, Ada-Sum maintains a distribution of weights over the document collec-tions in the training set, which is denoted as P k at round k . AdaSum sets all of the initial weights equally so that P 1 ( i ) = 1 /m . At each round, it increase the weights of those data that are not learned well in f k , the model created so far. As a result, the learning at the next round will be focused on the creation of a base summarizer that can work on those "hard" data.

At each round, a base summarizer h k is construction based on training data with weight distribution P k . The goodness of a base summarizer is measured by the performance measure E weighted by P k :
Once a base summarizer h k is built, AdaSum chooses a weight  X  k &gt; 0 for the base summarizer. Intuitively,  X  k measures the importance of h k .

A summarizing model f k is created at each round by linearly combining the base summarizers constructed so far h 1 , ..., h weights  X  1 , ...,  X  k . f k is then used for updating the distribution P
AdaSum is a simple yet powerful method. More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed below. In addition, AdaSum has several other ad-vantages when compared with the existing summarization meth-ods such as Mutual Reinforcement Principle (MPR) [23], LexRank [6] and GSPS [24]. Firstly, AdaSum can incorporate any perfor-mance measures, provided that the measure is topic based and in the range of [  X  1 , +1] . In contrast, the existing methods only mini-mize loss functions that are loosely related to the manual summary based measures [16]. Moreover, AdaSum employs a more reason-able framework for performing the summarizing task than the exist-ing methods. The adaptive learning process of AdaSum is a novel viewpoint for multi-document summarization.
We consider an efficient implementation for drawing out the fea-tures of data
A feature vector x =  X  ( s, t ) is created from each sentence-topic pair ( s, t ) . All of x span the feature space X . Each feature (each element of  X  ) stands for some relation between sentence and topic, e.g. similarity, distance and correlation. In summarization, a summarizer assigns a relevance score to every sentence in doc-ument collection, this score can also be viewed as one feature in X .

In this paper, we utilize the adaptive model proposed in section 3.1 to create each feature in like manner. Some well-known al-gorithm (systems) can be improved in the adaptive model, their outputs form the features in X .

Based on the construction of feature space, we choose the feature as base summarizer, which has the optimal weighted performance among all of the features in X : where P ( i ) denotes the weight of training instance ( d
Obtaining base summarizers by this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features. Note that features which are not selected in the training phase will have a weight of zero.
The same ideas are also used in our experiments in section 5.
The algorithm AdaSum is equivalent to forward stagewise addi-tive modeling [12] using the exponential loss function
A direct and effective loss measure for multi-document summa-rization is hard to find, and the existing measures are the manual summary based. Ideally, we want to maximize the summarization accuracy in terms of a performance measure on the training data: This is equivalent to minimize the loss
Because E may be a noncontinuous function and thus be difficult to handle, we instead attempt to minimize the convex upper bound of the loss in (6): i.e. we utilize the exponential loss function (5) in learning pro-cess. In this paper, we consider an additive model (4). Then the optimization of (7) can practise the gradient-descent paradigm in function space. One must solve (  X  k , h k ) = arg min for the summarizer h k and corresponding coefficient  X  k to be added at each step.

Following the above idea, we get the algorithm in Figure 4.
In order to evaluate our proposed approach, we select three meth-ods, MRP, LexRank and GSPS as the base summarizers in the experiments, for they are the state-of-art summarization methods. Furthermore, we compare AdaSum with the DUC 2007 top per-forming systems. We use the ROUGE metrics on DUC 2007 data sets for comparison. And the ROUGE score of the start-of-the-art systems came from Hoa X  X  overview of DUC 2007 [4].
Every year, Document Understanding Conferences (DUC 2 ) eval-uates competing research group X  X  summarization systems on a set of summarization tasks. In DUC 2005, DUC 2006 and DUC 2007, the task is to produce summaries of document collections in re-sponse to short topic statements that define what the summaries should address. The summaries are limited to 250 words in length. The DUC 2007 task was a complex question-focused summariza-tion task that required summaries to piece together information from multiple documents to answer a question or set of questions as posed in a DUC topic. NIST 3 Assessors developed a total of 45 DUC topics to be used as test data. For each topic, the asses-sor selected 25 related documents from the Associated Press, New York Times and Xinhua text collection and formulated a DUC topic statement, which was a request for information that could be an-swered using the selected documents. The topic statement could be in the form of a question or set of related questions and could in-clude background information that the assessor thought could help clarify his/her information need. The assessor also indicated the "granularity" of the desired response for each DUC topic. That is, they indicated whether they wanted the answer to their ques-tion(s) to name specific events, people, places, etc., or whether they
Document Understanding Conferences, http://duc.nist.gov/
National Institute of Standard and Technology, http://www.nist.gov/ wanted a general, high-level answer. Only one value of granular-ity was given for each topic, since the goal was not to measure the effect of different granularity on system performance for a given topic, but to provide additional information about the user X  X  prefer-ences to both human and automatic summarization. Automatic text summarization has drawn a lot of interest in the NLP and IR communities in the past years. Recently, a series of government-sponsored evaluation efforts in text summarization have taken place in both the United States and Japan. The most famous DUC evaluation is organized yearly to compare the sum-maries created by systems with those created by humans. Follow-ing the recent adoption of automatic evaluation techniques (such as BLEU/NIST) by the machine translation community, a similar set of evaluation metrics -known as ROUGE -were introduced for both single and multi-document summarization. ROUGE in-cludes four automatic evaluation methods that measure the sim-ilarity between summaries: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S. Formally, ROUGE-N measures the n-gram recall between a candidate summary and a set of reference summaries. ROUGE-N is computed as follows:
ROUGE-N = ROUGE-L uses the longest common subsequence (LCS) metric to evaluate summaries. In this method, a sequence R = [ r 1 , r is considered to be a subsequence of another sequence S = [ s ..., s n ] if there exists a strictly-increasing sequence of indices for S (i.e. I = [ i 1 , i 2 , ..., i k ] such that for all j = 1 , 2 , ...k, s The longest common subsequence for R and S can be defined as the sequence common to both R and S with the greatest length. ROUGE-L is based on the assumption that pairs of summaries with longer LCS scores will be more similar than those summaries with shorter LCS scores. To capture this generalization, if we assume that summary sentence X and Y can be represented as a sequence of words, an LCS-based F-measure can be calculated to estimate the similarity between a reference summary X of length m and a candidate summary Y of length n as follows: Here, LCS ( X, Y ) is equal to the length of the LCS of X and Y and  X  = P lcs /R lcs . LCS can be also used to compute an F-measure for an entire summary, not just a single sentence. The summary-based LCS F-measure can be computed as follows: R where u represents a reference summary of m words, C represents a candidate summary of n words, and LCS  X  ( r i , C ) is the LCS score of the union LCS between r i and the candidate summary C.
In this section we experimentally evaluate the quality of AdaSum performed on DUC 2007 dataset. Two different sets of experiments are presented. The first one focuses on the effectiveness of the topic representation optimization and the topic expansion during the feedback process. The latter experiments focuse on evaluating the summary performance on a real document collection.
In order to study how to understand the expected topic, we use the first ten document collections from the large-scale common data set evaluation conducted within the DUC 2007. For each collec-tion, the input for summarization is available, along with a specified topic for the input and the document collections to be summarized that year. Each of the inputs containes around 25 documents and the summaries were 250 words long. As the following description shows, with the process of feedback, a short topic included few terms is expanded to a long related terms list.
 Table 1: Topic representation optimization of AdaSum, where the first and second columns are the identity number and corre-sponding title of selected topics, and the last column is the term distribution of optimized topic representation with our Ada-Sum.

Table 1 illustrates the topic representation optimization with our proposed feedback process in this paper. Results from the third col-umn are the term distribution of estimated topics for the final esti-mated summary, and the terms are generated with feedback rank-ing. The bold terms are important for topic identification. Intu-Figure 5: AdaSum vs. base summarizers performance compar-ison, where AdaSum can significantly improve the ROUGE-2 scores and ROUGE-SU4 scores. itively, the derived term distribution in the third column can op-timize the original topic represented by the specified topic in the second column.

The success of AdaSum in this task demonstrates its flexibility and effectiveness, and indicates that our proposed model is suf-ficiently expressive to represent more important topic properties. Thus, with topic representation optimization, the derived summary with topic representation could be more relative to the specified topic, and which is likely to reduce the topic representation mis-match between the original topic and the expected topic. Table 2: Performance comparison with AdaSum and base sum-marizers. With comparison to the average scores on ROUGE-2 and ROUGE-SU4 scores for MRP, LexRank and GSPS, Ada-Sum can achieve significant improvement, and the last column is the improvement of AdaSum on both ROUGE scores.

In this section, we study the performance of AdaSum on the dataset of DUC 2007. Firstly, we choose the first ten document collections to evaluate the summarization performance achieved by AdaSum, and the summary performance for this ten collections is evaluated by ROUGE-2 and ROUGE-SU4 metrics. We utilize three base summarizers as features (MRP, LexRank and GSPS). Table 2 shows the average recall achieved by AdaSum and the base sum-marizers on DUC 2007 dataset respectively, where every test set is relative to a specified topic. The 2nd column and the 3rd column in Table 2 show the average recall on ROUGE-2 and ROUGE-SU4 respectively, and the last column represents the improvement of AdaSum against the base summarizers on both ROUGE scores.
From the results in Table 2, we can see that AdaSum improves the summarization performance on both ROUGE scores. In com-parison with the scores of base summarizers, AdaSum can obtain significant improvement on both ROUGE scores. To illustrate the performance, we plot the curve of ROUGE-2 scores and ROUGE-SU4 scores in Figure 5. Looking at the results, AdaSum is effective to improve the summarization performance. Moreover, we investi-gate the summarization performance of AdaSum on the whole 45 document collections of DUC 2007, and the results are shown in Table 3. With comparison to three base summarizers, AdaSum can gain improvement about 23 . 0% , 18 . 4% and 5 . 6% on ROUGE-2 metric. Table 2 and Table 3 prove our proposed model is effec-tiveness for topic-oriented summarization. These results confirm our hypothesis about the benefit of adaptive summarization model, which also confirm that our proposed approach can significant im-prove the performance of summarization.
 Table 3: Performance comparison with AdaSum and base sum-marizers on DUC 2007 dataset. With comparison to the av-erage scores on ROUGE-2 and ROUGE-SU4 scores for MRP, LexRank and GSPS, AdaSum can also achieve significant im-provement, and the last column is the improvement of AdaSum on both ROUGE scores.

We also investigate the significant performance of our proposed model with significant test experiments. The paired samples t -tests experiments on ROUGE scores in Table 4 show these differences on both ROUGE scores to be statistically significant at p &lt; 0 . 05 both for individual judges and average scores across all judges, where six pairs are the ROUGE-2 scores and ROUGE-SU4 scores among AdaSum and three base summarizers respectively. The re-sults of Sig. (1-tailed) indicate that our AdaSum performs statisti-cally better than every base summarizer at p &lt; 0 . 05 .
In order to further confirm our hypothesis, we conduct the third experiment. We compare the summary performance of our pro-posed approach with the top performing systems in DUC 2007. With comparison to the average scores of ROUGE-2 on the first ten document collections, AdaSum can outperform the 2nd rank system, Microsoft. Although AdaSum can not outperform the best system, the difference is slight. Furthermore, in comparison with the average scores of ROUGE-2 on the whole 45 document col-lections of DUC 2007, AdaSum can obtain the 4th place. The re-sults confirm that AdaSum performs well as compared to the state-of-the-art systems competing in DUC. In this paper, we choose MRP, LexRank and GSPS as the base summarizers, thus the re-sults mostly tend to confirm the superiority of our proposed ap-proach. Ideally, if given more empirical base summarizers and further optimization for algorithms, our proposed approach could achieve more significant results. ROUGE-SU4 scores.

Automatic summarization is the process of automatically cre-ating a compressed version of a given text so that it can provide useful information to the user. Summarization techniques leverage a wide range of Natural Language Processing (NLP) and discourse information. Some focus primarily on techniques that have been developed in Information Retrieval [10], while most try to leverage both IR approaches and some aspects of NLP [13]. In the previ-ous research, most efforts on extractive summarization have been concentrated on statistical and NLP approaches. For example, the diversity of concepts covered by a document [3] has been first ex-plored by Carbonell and Goldstein in 1998. They proposed to use Maximal Marginal Relevance (MMR), which selects summary sen-tences that are both relevant to the user query and least similar to the previously chosen ones. In [23], a mutual reinforcement prin-ciple (MRP) is employed to iteratively extract key phrases and sen-tences from a document. Most recently, graph-based ranking meth-ods have been proposed for document summarization. Radev et al. [20] described an extractive multi-document summarizer which extracts a summary from multiple documents based on the docu-ment cluster centroids. Also, Erkan et al. [6] proposed a stochastic graph-based method (LexRank) for computing relative importance of textual units for NLP, where the sentence importance is mea-sured by the concept of eigenvector centrality in a graph represen-tation of sentences. In [24], a graph-based sub-topic partition algo-rithm is proposed by ranking sentence importance with a "person-alized" LexRank and removing redundancy with subtopic partition, where the global features are taken as the "personalized" vector for LexRank.

In what follows, we review the work that are closely related to our approach. There are two topics which are related to our current work -boosting in classification and AdaRank in IR.
Boosting is a general technique for improving the accuracies of machine learning algorithms. The basic idea of boosting is to re-peatedly construct "weak learners" by re-weighting training data and form an ensemble of weak learners such that the total perfor-mance of the ensemble is "boosted". Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) [8], which is designed for binary classifica-tion (0-1 prediction). Later, Schapire and Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 deci-sions [21]. Extensions have been made to deal with the problems of multi-class classification [8, 9], regression [5], and ranking [7]. In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the "exponential loss function" with respect to the training data.
The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query. It is a common prac-tice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1]. Xu and Li have pro-posed a learning algorithm within the framework of boosting called AdaRank [22], which can minimize a loss function directly defined on the basis of general IR performance measures, and the optimiza-tion of loss function is based on queries. The objective of AdaRank is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function. Ideally the loss function is defined on the basis of the performance measure used in testing. AdaSum can be viewed as a machine learning method for loss function optimization of sum-maries performance measures, based on a different approach.
In this paper, we propose a novel adaptive model for summariza-tion, AdaSum, under the assumption that the summary and the topic representation can be mutually boosted. AdaSum aims to simulta-neously optimize the topic representation and extract effective sum-maries. In order to alleviate the topic mismatch problem in sum-marization and minimize the expected risk between specified topic and expected topic, AdaSum utilizes a linear combination of base summarizers as its model. Moreover, a general learning framework is proposed to construct a new summarizer with a linear combina-tion of base summarizers. The model can naturally make full use of the reinforcement between the topic and the derived summaries by employing a boosting approach to optimize the topic represen-tation and generate effective summaries. We prove that the train-ing process of AdaSum can exactly enhance the performance mea-sure used. Experimental results on DUC 2007 dataset show that AdaSum significantly outperforms the baseline methods of MRP, LexRank, and GSPS. Compared to the top systems in DUC 2007, our proposed approach is also very competitive.

The study has two main contributions: (1) a mutual boosting process is employed to optimize the topic representation and min-imize the expected bias between the estimated topic and the ex-pected topic, (2) inspired by the work of AdaBoost and AdaRank, a linear combination of base summarizers is employed to obtain a new summarizer which can boost the performance of derived sum-maries with a general learning framework. In addition, our pro-posed model offers other advantages: ease of implementation, the-oretical soundness, efficiency in training, and high performance in summarization. To some extent, AdaSum can be viewed as a model of learning to summarize.

As for future research, we plan to exploit more effective mea-sures to evaluate the similarity between the expected topic and the estimated topic for the optimization of topic representation. And introducing more empirical base summarizers for the general learn-ing framework can further optimize the performance of our pro-posed model. Moreover, we would like to adapt our propose model to task-focused summarization problems, such as web search result snippets. This research is supported by The 973 National Basic Research Program of China (2004CB318109) and The 863 National High-Tech Research and Development Plan of China (2006AA01Z452 &amp; 2007AA01Z441). We would like to thank Professor Jian-Yun Nie for helpful discussions and invaluable suggestions. Finally, We would like to thank the anonymous reviewers for their insightful comments. [1] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [2] R. Barzilay and L. Lee. Catching the drift: Probabilistic [3] J. G. Carbonell and J. Goldstein. The use of mmr, [4] H. T. Dang. Overview of duc 2007. In Proceedings of [5] N. Duffy and D. P. Helmbold. Boosting methods for [6] G. Erkan and D. R. Radev. Lexrank: Graph-based lexical [7] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An [8] Y. Freund and R. E. Schapire. A decision-theoretic [9] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic [10] J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. [11] S. M. Harabagiu and V. F. Lacatusu. Topic themes for [12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of [13] E. Hovy and C. Lin. Automated text summarization in [14] K. Knight and D. Marcu. Statistics-based summarization -[15] C.-Y. Lin. Rouge: a package for automatic evaluation of [16] C.-Y. Lin and E. H. Hovy. The automated acquisition of topic [17] H. Luhn. The automatic creation of literature abstracts. IBM [18] D. Mallett, J. Elding, and M. A. Nascimento.
 [19] I. Mani and M. T. Maybury. Advances in Automatic Text [20] D. R. Radev, H. Jing, M. Sty, and D. Tam. Centroid-based [21] R. E. Schapire and Y. Singer. Improved boosting algorithms [22] J. Xu and H. Li. Adarank: a boosting algorithm for [23] H. Y. Zha. Generic summarization and keyphrase extraction [24] J. Zhang, X. Cheng, and H. Xu. Gspsummary: A
