 This paper addresses the problem of evaluating a given model in terms of its predictive performance. In practice, it is not always possible to evaluate a model on held-out training data; consider, for instance, the following scenarios. When a readily trained model is shipped and deployed, training data may be held back for reasons of privacy. Secondly, training data may have been created under laboratory conditions and may not entirely reflect the test distribution. Finally, when a model has been trained actively, the labeled data is biased towards small-margin instances which would incur a pessimistic bias on any cross-validation estimate.
 This problem has recently been studied for risks X  i.e., for performance measures which are integrals of a loss function over an instance space [7]. However, several performance measures cannot be expressed as a risk. Perhaps the most prominent such measure is the F  X  -measure [10]. For a given respectively, and n fn the number of false negatives. Then the classifier X  X  F  X  -measure on the sample is defined as view and raises the question which quantity of the underlying distribution the F -measure actually We will then show that F  X  is a consistent estimate of a quantity that falls into this class. f ( x ) = arg max y p ( y | x ;  X  ) be the corresponding hypothesis.
 Like any risk functional, the generalized risk is parameterized with a function ` : Y  X  Y  X  R determining either the loss or X  X lternatively X  X he gain that is incurred for a pair of predicted and true label. In addition, the generalized risk is parameterized with a function w that assigns a weight weight 1 and gives no consideration to other instances. Equation 2 defines the generalized risk: The integral over Y is replaced by a sum in the case of a discrete label space Y . Note that the n , a consistent estimator can be obtained by replacing the cumulative distribution function with the empirical distribution function.
 Proposition 1. Let ( x 1 ,y 1 ) ,..., ( x n ,y n ) be drawn iid according to p ( x ,y ) . The quantity is a consistent estimate of the generalized risk G defined by Equation 2.
 Proof. The proposition follows from Slutsky X  X  theorem [3] applied to the numerator and denomina-tor of Equation 3.
 Consistency means asymptotical unbiasedness; that is, the expected value of the estimate  X  G n con-verges in distribution to the true risk G for n  X  X  X  . We now observe that F  X  -measures X  X ncluding precision and recall X  X re consistent empirical estimates of generalized risks for appropriately cho-sen functions w .
  X f  X  ( x ) + (1  X   X  ) y and ` = 1  X  ` 0 / 1 , where ` 0 / 1 denotes the zero-one loss. Proof. The claim follows from Proposition 1 since Having established and motivated the generalized risk functional, we now turn towards the problem of acquiring a consistent estimate with minimal estimation error on a fixed labeling budget n . Test an active estimation process that selects test instances according to an instrumental distribution q . When instances are sampled from q , an estimator of the generalized risk can be defined as where ( x i ,y i ) are drawn from q ( x ) p ( y | x ) . Weighting factors p ( x i ) q ( x between test and instrumental distributions. Because of the weighting factors, Slutsky X  X  Theorem using the instrumental distribution q = p .
 pends on q . Our overall goal is to determine the instrumental distribution q such that the expected deviation from the generalized risk is minimal for fixed labeling costs n : The bias-variance decomposition expresses the estimation error as a sum of a squared bias and a variance term [5]: Lemma 1 shows that Bias 2 [  X  G n,q ] is of order 1 n 2 .
 Lemma 1 (Bias of Estimator) . Let  X  G n,q be as defined in Equation 4. Then there exists C  X  0 with The proof can be found in the online appendix. Lemma 2 states that the active risk estimator  X  G n,q is asymptotically normally distributed, and characterizes its variance in the limit. Lemma 2 (Asymptotic Distribution of Estimator) . Let  X  G n,q be defined as in Equation 4. Then, with asymptotic variance where n  X  X  X   X  X  X  denotes convergence in distribution.
 A proof of Lemma 2 can be found in the appendix. Taking the variance of Equation 8, we obtain E can be approximately minimized by minimizing  X  2 q . In the following, we will consequently derive a sampling distribution q  X  that minimizes the asymptotic variance  X  2 q of the estimator  X  G n,q . 2.1 Optimal Sampling Distribution The following theorem derives the sampling distribution that minimizes the asymptotic variance  X  2 q : Theorem 1 (Optimal Sampling Distribution) . The instrumental distribution that minimizes the asymptotic variance  X  2 q of the generalized risk estimator  X  G n,q is given by A proof of Theorem 1 is given in the appendix. Since F -measures are estimators of generalized risks according to Corollary 1, we can now derive their variance-minimizing sampling distributions. Corollary 2 (Optimal Sampling for F  X  ) . The sampling distribution that minimizes the asymptotic variance of the F  X  -estimator resolves to Algorithm 1 Active Estimation of F  X  -Measures input Model parameters  X  , pool D , labeling costs n . output Generalized risk estimate  X  G n,q  X  . 1: Compute optimal sampling distribution q  X  according to Corollary 2, 3, or 4, respectively. 2: for i = 1 ,...,n do 3: Draw x i  X  q  X  ( x ) from D with replacement. 4: Query label y i  X  p ( y | x i ) from oracle. 5: end for 6: return  X f  X  ( x ) + (1  X   X  ) y and ` = 1  X  ` 0 / 1 . Starting from Theorem 1, we derive The claim follows by case differentiation according to the value of f  X  ( x ) .
 Corollary 3 (Optimal Sampling for Recall) . The sampling distribution that minimizes  X  2 q for recall resolves to Corollary 4 (Optimal Sampling for Precision) . The sampling distribution that minimizes  X  2 q for precision resolves to Corollaries 3 and 4 directly follow from Corollary 2 for  X  = 0 and  X  = 1 . Note that for standard risks (that is, w = 1 ) Theorem 1 coincides with the optimal sampling distribution derived in [7]. 2.2 Empirical Sampling Distribution Theorem 1 and Corollaries 2, 3, and 4 depend on the unknown test distribution p ( x ) . We now turn pool can be sampled and then labeled at a cost. Drawing instances from the pool replaces generating them under the test distribution; that is, p ( x ) = 1 m for all x  X  D .
 constitutes an analogy to active learning: In active learning, the model-based output probability an asymptotic bias in our estimator (Equation 4). Finally, Theorem 1 and its corollaries depend on Algorithm 1 summarizes the procedure for active estimation of F -measures. A special case occurs when the labeling process is deterministic. Since instances are sampled with replacement, elements may be drawn more than once. In this case, labels can be looked up rather than be queried from the deterministic labeling oracle repeatedly. The loop may then be continued until the labeling budget is exhausted. Note that F -measures are undefined when the denominator is zero which is the case when all drawn examples have a weight w of zero. For instance, precision is undefined when no positive examples have been drawn. 2.3 Confidence Intervals ( x such confidence intervals are approximate for finite n , but become exact for n  X  X  X  . We compare active estimation of F  X  -measures according to Algorithm 1 (denoted active F ) to esti-mation based on a sample of instances drawn uniformly from the pool (denoted passive ). We also consider the active estimator for risks presented in [7]. Instances are drawn according to the opti-according to Equation 4 using q = q  X  0 / 1 (denoted active err ). 3.1 Experimental Setting and Domains For each experimental domain, data is split into a training set and a pool of test instances. We mada [11]). All methods operate on identical labeling budgets n . The evaluation process is averaged periment is discarded ( i.e., there is no data point for the method in the corresponding diagram). Spam filtering domain. Spammers impose a shift on the distribution over time as they implement new templates and generators. In our experiments, a filter trained in the past has to be evaluated with respect to a present distribution of emails. We collect 169,612 emails from an email service provider between June 2007 and April 2010; of these, 42,165 emails received by February 2008 are used for training. Emails are represented by 541,713 binary bag-of-word features. Approximately 5% of all emails fall into the positive class non-spam .
 Text classification domain. The Reuters-21578 text classification task [4] allows us to study the effect of class skew, and serves as a prototypical domain for active learning. We experiment on the ten most frequently occurring topics. We employ an active learner that always queries the example with minimal functional margin p ( f  X  ( x ) | x ;  X  )  X  max y 6 = f initialized with one labeled training instance from each class, another 200 class labels are queried. Digit recognition domain. We also study a digit recognition domain in which training and test data originate from different sources. A detailed description is included in the online appendix. 3.2 Empirical Results We study the performance of active and passive estimates as a function of (a) the precision-recall trade-off parameter  X  , (b) the discrepancy between training and test distribution, and (c) class skew in the test distribution. Point (b) is of interest because active estimates require the approximation Effect of the trade-off parameter  X  . For the spam filtering domain, Figure 1 shows the average emails received between February 2008 and October 2008. The active generalized risk estimate active F significantly outperforms the passive estimate passive for all three measures. In order to reach the estimation accuracy of passive with a labeling budget of n = 800 , active F requires Figure 3: Text classification: Estimation error over number of labeled data for infrequent (left) and Error bars indicate the standard error.
 for other frequent and infrequent classes, all results are included in the online appendix. Figure 3 (right) shows the estimation error of active F , passive , and active err on all ten one-versus-rest problems as a function of the problem X  X  class skew. We again observe that active F outperforms passive consistently, and active F outperforms active err for strongly skewed class distributions. Sawade et al. [7] derive a variance-minimizing sampling distribution for risks. Their result does not cover F -measures. Our experimental findings show that for estimating F -measures their variance-minimizing sampling distribution performs worse than the sampling distributions characterized by Theorem 1 , especially for skewed class distributions.
 Active estimation of generalized risks can be considered to be a dual problem of active learning; in active learning, the goal of the selection process is to minimize the variance of the predictions or the variance of the model parameters, while in active evaluation the variance of the risk estimate is reduced. The variance-minimizing sampling distribution derived in Section 2.1 depends on the and decide on instances whose class labels are queried. This is analogous to many active learning algorithms. Specifically, Bach derives a sampling distribution for active learning under the assump-compensate for the bias incurred by the instrumental distribution, several active learning algorithms use importance weighting: for regression [8], exponential family models [1], or SVMs [2]. Finally, the proposed active estimation approach can be considered an instance of the general prin-ciple of importance sampling [6], which we employ in the context of generalized risk estimation. F  X  -measures are defined as empirical estimates; we have shown that they are consistent estimates of a generalized risk functional which Proposition 1 identifies. Generalized risks can be estimated actively by sampling test instances from an instrumental distribution q . An analysis of the sources of estimation error leads to an instrumental distribution q  X  that minimizes estimator variance. The optimal sampling distribution depends on the unknown conditional p ( y | x ) ; the active generalized risk estimator approximates this conditional by the model to be evaluated.
 Our empirical study supports the conclusion that the advantage of active over passive evaluation to the quality of the model as measured by the model-based likelihood of the test labels. In our experiments, active evaluation consistently outperformed passive evaluation, even for the greatest divergence between training and test distribution that we could observe. Proof of Lemma 2 P distributed with tion f ( x,y ) = x y yields where  X  f denotes the gradient of f and  X  is the asymptotic covariance matrix of the input arguments Furthermore,  X  f ( G E [ w i ] , E [ w i ]) T  X   X  f ( G E [ w i ] , E [ w i ]) From this, the claim follows by canceling q ( x ) .
 Proof of Theorem 1 To minimize the variance with respect to the function q under the the normalization con-straint R q ( x ) d x = 1 we define the Lagrangian with Lagrange multiplier  X  Equation under the side condition is given by tion of c in Equation 20 implies the theorem.
 We gratefully acknowledge that this work was supported by a Google Research Award. We wish to thank Michael Br  X  uckner for his help with the experiments on spam data. [1] F. Bach. Active learning for misspecified generalized linear models. In Advances in Neural [2] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In Pro-[3] H Cram  X  er. Mathematical Methods of Statistics , chapter 20. Princeton University Press, 1946. [4] A. Frank and A. Asuncion. UCI machine learning repository, 2010. [5] S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilemma. [6] J. Hammersley and D. Handscomb. Monte carlo methods . Taylor &amp; Francis, 1964. [7] C. Sawade, N. Landwehr, S. Bickel, and T. Scheffer. Active risk estimation. In Proceedings of [8] M. Sugiyama. Active learning in approximately linear regression based on conditional expec-[9] S. Tong and D. Koller. Support vector machine active learning with applications to text classi-[10] C. van Rijsbergen. Information Retrieval . Butterworths, 2nd edition, 1979. [11] M. Yamada, M. Sugiyama, and T. Matsui. Semi-supervised speaker identification under co-
