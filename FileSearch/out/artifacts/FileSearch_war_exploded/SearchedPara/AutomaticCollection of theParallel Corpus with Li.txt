 As an important resource for many natural language processing applications, such as cross -language retrieval [1] , machine trans lation [2] , parallel corpus ha ve been one of the key resources . Currently, various websites are bilingual or multilingual. For some international organizations or educational institutions, they will build the webpages in several languages on the site to sh are the information oversea. The websites in some countries where people speak two or more languages will also have bilingual or mult i-lingual webpages. It becomes more and more common to find the parallel corpora from some known bilingual websites . For exa mple, in Hong Kong governmental we b-sites, there are lots of English -Chinese bilingual webpages, while in Canadian go v-ernmental websites, we can find many English -French bilingual webpages as well. M any available systems can find the se bilingual websites an d they detect the parallel URLs with the URL naming rules, e.g., STRAND [3,4,5] , PTMiner [6], PupSniffer [7]. 
A typical strategy of collecting parallel corpora from bilingual websites involves four fundamental steps: (1) Locat ing the candidate bilingual w ebsites; (2) Crawling for URLs of candidate parallel web pages; (3) Matching and filtering parallel web pages; (4) Extracting parallel text pairs from the obtained webpages.

Prior knowledge has been widely used when locating the candidate bilingual we b-site s. Most of the systems use the word lists of one language or some anchor info r-mation to search on the search engine [5, 6]. Then the search engines will return the URLs of candidate bilingual websites. After crawling for the URLs of webpages, researchers n eed to find the true parallel webpages within candidate bilingual we b-site s . Due to some constructing rules, when building a bilingual website , some given pairing patterns will be inserted into the parallel webpage URLs, such as the  X  english  X  and  X  chinese  X  in the following pair of bilingual webpages: Webpage in English: ht t p://www.swd.gov.hk/vs/english/police.html Webpage in Chinese : ht t p://www.swd.gov.hk/vs/chinese/police.html correct pairing patterns in candidate bilingual websites. The algorithm of patterns matching results in large -scale computation . Meanwhile t he URL pattern -based mi n-ing may raise concerns on high bandwidth cost and slow download speed. S ome r e-searchers t ry to find more bilingual webpages via link analysis and they also find out we utilize the search rules of search engine websites and URL patterns with high cre d ibility to obtain bilingual websites from the Internet. The method avoids getting too much irrelevant websites from the search engine and cost s less time. 
The paper is structur ed as follows: S ection 2 gives a brief review of the related work in the field. Section 3 provides the methodology of our experiment. Section 4 presents the evaluation and analysis that we obtain from the data. Section 5 draws some conclusions and we point out the shortages of our experiment and future works. For many data -dr iven task of NLP, how to get parallel corpora in an efficient way has been the focus in many research projects for years . There are numerous systems a u-tomatic ally acquiring parallel corpora from multilingual websites, for example, STRAND [3,4,5] , PTMiner [ 6]. Many researchers are trying to improve the acquis i-tion performance, such as PupSniffer [7], BITS [8] , WPDE [9] , the DOM tree alig n-ment model [10] and Bitextor [11] . Some researchers use search engines to find para l-lel webpages. M icroblog has also been one of the resource s to get parallel corpora [12]. Among the relevant research, we can find two main types of detecting parallel webpages: the way based on the URL patterns and the way based on the HTML stru c-ture. Researchers make use of anchor texts or HT ML files in order to find some a p-parent patterns in the websites or in the URLs which represent different languages , especially the language pattern pairs in the URLs to find more parallel websites. In this paper, we find the parallel webpages via high cre dible URL patterns using the search engine.
The way based on URL patterns aims at using naming rules of URLs to detect b i-lingual websites. In early times, researchers are using the re -defined substrings [5, 9] and then comes some automatic ways [13]. Final ly, extracted URL pairs are verified based on automatic string pattern recognition instead of prior knowledge [14] . Ye, S. et al [13] made a research of relationship between the content and structure of bili n-gual websites URLs. The URL patterns and HTML str ucture have also been co m-bined to find parallel websites [15]. They use the HTML structure to go through the d irected graph of bilingual websites simultaneously . The way based on HTML structure advocate s using the structure information of HTML. If the two webpages are parallel to each other, they may have corresponding websites links in the HTML content that connect to another pair of parallel webpages. However, in this way we may just find a little amount of parallel webpages and calc u-lations will be much more than the way based on URL patterns. What  X  s more, in the same bilingual website, content structure of the two languages pages may not be tota l-ly same like each other.

In our approach, we search for bilingual websites with language -specific URL su b-stri ngs and replace the bilingual URL patterns to find a likely candidate pair of para l-patterns that we find out in the research? In the research of Kit and Ng [14], they d e-f ine the linking power of pattern based on the number of URL pairs that it can match. Enhanced algorithms are proposed based on their research to match more bilingual webpages . Zhang and Yao [7] get the global credibility of pattern based on statistical ana lysis about the link relationship of seed websites available. They also defined the bilingual credibility of a website via link analysis. Depending on the data that we get from their research, we don  X  t need to download all the parallel websites or do any c omplex pattern matching algorithm within a website. This section first ly introduces the idea of exploring parallel websites via the search engine and then it continues to show the detailed steps of the approach. 3.1 Can we collect bilingual websit es with little priori knowledge ? There are large -scale of parallel webpages on the Internet. Some researchers explore bilingual websites based on the bilingual URL patterns and they also give the credibi l-ity of identified URL patterns [7]. We first make s imple search queries with high credible URL patterns, like  X  en  X  ,  X  eng  X  ,  X  e nglish  X  . The search query is made according study, we use Google as the search engine. Table 1 shows the numbers of websites after we eliminat ed the duplicated ones.
From the Table 1, we can see that a certain number of multilingual websites are found by limiting the URL. M ultilingual websites containing the Chinese la n-guage are around 20% of all the websites from search engine. In the front part of retur ning results, websites are basically the English pages of th ose multilingual websites. 3.2 Framework and key technologies Based on above observations, we can make search rules of  X  inurl: source language pattern  X  to locate webpages in the source language of bil ingual websites. Then, we use identified URL pattern pairs to find the existing URL in the target language of bili n-gual websites. Finally, we will get parallel webpages in a bilingual website. The whole procedure of this method can be divided into t wo step s: I. Get the URLs of c andidate bilingual w ebsites in the source language; I I . Ge t the URLs of c andidate p arallel w ebsites in the target language. Figure 1 shows the overall framework of our approach.

Firstly, with search engine and sea rch rules , we get some candidate bilingual we b-sites, most of these sites are the source language pages of parallel sites. 
Secondly , URLs of seed websites were processed in two mode s in order to make new search terms. Under the conditions of new search rul es, we use the new search terms for a second batch of search and collect the results. From our point of view, the results are actually the websites in the source language of candidate parallel websites. 
Thirdly , URLs of the results will have a small subst itution. We replaced the source language pattern of each URL with target language pattern to generate a new URL . These pattern pairs are given in the previous research (Zhang and Yao, 2013) . If t he generated URL is judged to be an existing w ebsite , we assu me that the new URL and former URL is a pair of parallel websites in the candidate parallel websites . W e made a final evaluation for all the URL pairs at last .
 Source Language URLs of the Candida te Parallel Websites Detection One feature of our way is we make use of search rules to find high quality parallel websites from the search engine. When we use the search rules of  X  inurl:  X , we can find a certain number of multilingual websites from the search engine and some of them have parallel webpages in the si te . In this way, we try to add more search rules to define the URL. Then we can get source language URLs of the candidate parallel websites. The main steps of this part are as follows: i. Search candidate bilingual websites ii. Search webpages of candida te bilingual websites in the source language
The f irst step is finding candidate bilingual websites. In this study, the source la n-guage is English and target language is Chinese. W e aim at the governmental (gov.hk), educational (edu.hk) and organizational (org.hk) types of websites in Hong Kong . To limit the websites types, we add the search rule of  X  site:  X . In order to avoid getting too many PDF files, the search rule of  X  filetype:html  X  is included. Here is an example of the first search query we put in th e search box of Google engine . The second step of this part is finding URLs of candidate parallel websites that in English language . The first thing is reprocessing the URLs of websites that we get from the first search. T hen we search again on the search engine with the reprocessed ones . T he URLs from the first search are processed in two ways, we call them  X  X  o-main Mode X  and  X  X irectory Mode X  here. (1) Domain Mode: We eliminate the duplicated webpages of seed websites, and keep the second -level domain of each URL. For example, the original URL is  X  http://www.immd.gov.hk/en/services/hk -visas/visit -transit.html  X  . After processing, the domain mode URL is  X  www.immd.gov.hk  X . Then we eliminate the duplicated ones Internet. Because of expiration s , web server changes or other reasons, some domains are invalid. So we just make use of the accessible ones and a new search query is shown here :
In this mode , the file type is defined as html, htm, asp, aspx and php. If the number example of the search query. (2) Directory Mode: We keep each seed websites URL of the web directory of  X  /en/  X ,  X  /eng/  X  , X  /english/  X  . We will also give an example here , the original URL is  X  http://www.immd.gov.hk/en/services/hk -visas/visit -transit.html  X  . After processing, th e directory mode URL is  X  www.immd.gov.hk/en/  X  . Also we eliminate the duplicated ones again and a new search query is shown here :
The file type is also defined as html, htm, asp, aspx and php and we collect all the se arch results during Directory Mode no matter if the number of URLs is below 100. We also do the search without file type definition. You can see that we are trying to find the websites under the web directory of  X  www.immd.gov.hk/en/  X  as many as po s-sible.

F inally, the search results that we get from the search engine in these two modes are seen as the URLs of c andidate w ebsites that in English language. We will use these URLs in the succeeding part.
 Target language URLs Generation Our study is conducted on t he basis of identified bilingual URL pairing patterns as described in Zhang and Yao [7] . Their research is conducted on top of the re -implementation of the intelligent web agent to automatically identify bilingual URL pairing patterns as described in Kit a nd Ng [13] . For some engineering purposes, the web builders will put some static pairing patterns in the pairs of parallel webpages within the same domain. So in the research, they first detected the candidate pairing patterns from candidate URL pairs. For example, the candidate pattern &lt;en,tc&gt; can be detected from the following URL pairs: Webpage in English: http://www.hkgb.gov.hk/en/news/press_20120227.html Webpage in Chinese: http://www.hkgb.gov.hk/tc/news/press_20120227.html
Then they use the detected U RL patterns to match URLs in a web domain for ide n-tifying bilingual webpages [7]. The noisy patterns would be filtered out by threshol d-ing the credibility of a pattern, which can be defined as:
W here N(p,w) is the number of webpages matched into pairs by pattern p within website w , and |w| the size of w in number of webpages.

There are some patterns generalizing across domains. They set the global credibi l-ity of such a pattern p like this: We make use of the patterns with high credibility which are given in their research. The search results that we get from the p revious part are the URLs including the cha r-choose 5 candidate pairing patterns respectively to do the substitution according to their credibility in the research of Zha ng and Yao (2013) . Then w e replace the chara c-from high to low order ), and we have th e URL:
And a new URL after replacing will be like:
Special attention should be paid here. We didn  X  t just replace the  X  en  X  with  X  tc  X  . We replace the  X  /en/  X  with  X  /tc/  X  . If not it w ill a ppear such circumstance , the original URL is:
The new URL will be like: So the replacement doesn X  X  make any sense.
 Also, for a pairing pattern &lt;english, chinese&gt;, if the  X  english  X  character string in the many similar situation s during the replacement.

Then new URL is checked after replacing to see whether it exists or not. If the r e-turned http response code of the URL is 200, we assume that the original URL and generated URL is a pair of bilingual parallel websites. Otherwise we keep replacing with the lower credibility bilingual URL pairing patterns until we find an existin g website. If the five generated URLs are all checked nonexistent , then we will filter this one and move to the next URL. After the replacements of all the URLs which we get from Section 3.2, we get pairs of URLs which are all existed. One is the URL of c a ndidate w ebsites that in English language. Another one is the URL that we genera t-ed with candidate pairing patterns. We assume that these pairs of URL are actually the pairs of parallel webpages in candidate bilingual websites . 4.1 Experimental Data In this paper, we focused on the governmental, educational and institutional types of sites in Hong Kong when obtaining parallel pages. These three types of the websites are much standard than other types of websites in the content organization and HTML structure. Table 2 shows the number of websites that we get at first.

We deal with th is websites in two modes:  X  X omain Mode X  and  X  X irectory Mode X  to get candidate bilingual websites and do the next search .
In the Domain Mod e, we keep the second -level domain of each URL from seed websites and eliminate the duplicated ones. Table 3 shows the data of websites that we get after processing. The percentage of English -Chinese websites is higher than the first observation data. Then we just choose the domain that in English -Chinese language to be candidate websites.

In the Directory Mode, w e keep each seed websites URL to the web directory of  X / en / X ,  X / eng / X  , X / english / X , after eliminating the duplicated ones, we get the data in Tab le 4. We use all the websites in the directory mode to be candidate websites.
After reprocessing the websites that we get at first , we get candidate websites. Then we make new search queries with search rules and candidate websites limit. After e liminat ing the duplicated ones in the search results, w hat we get finally is actually the websites in English language of candidate parallel websites. The next step is pattern replacing. Table 5 shows the top 5 identified bilingual URL pairing patterns li st that we get from the Pupsniffer Evaluation Website 1 , a we b-site designed by Zhang and Yao ( 2013) . They released the ir research data on th is website and do the evaluation.

Finally we get the pairs of candidate bilingual web pages. The web pages in En g-lish language are the search results of candidate web sites generated via two modes. The websites in Chinese language are the existing websites after replacing URL pa t-terns. Table 6 shows the number of search results in two modes and the number of existing websites after we checked those replaced URLs. T he du plicated ones have been eliminated here. We can see that more than half of the candidate websites have corresponding websites after URL pattern substitution.
 4.2 Results Evaluation A web interface was implemented in the Pupsniffer Evaluation Website 2 for evalua t-ing the candidate English -Chinese webpage pairs which we finally get. The quality of bilingual webpages found by us is evaluated manually. Two people (one PhD and one master student) took part in this evaluation. Due to the large amount of retrieved pairs , we only randomly sampled and evaluated part of all pairs. (1) Result of Bilingual Web Pages Collecting 88 915 web pages pairs 3 are found totally via two modes of searching ways. We made an evaluation of 4 460 pairs randomly and the number of the false bi lingual web pages pairs is 409. Table 7 shows the precisions by different methods.

The precision of our meth o d is 90.8% 4 and lower than the results of other two methods. However, t here are seve ral advantages we need to mention here. Our pr o-posed method has lower time cost. In the other methods, they have to match a mass of the URL patterns within a website in order to find the true parallel webpages. We just make use of the few certain URL patte rns pairs based on the previous works. By u s-ing the URL patterns of corresponding languages, we don  X  t need to detect the la n-guage of websites in advance. Moreover, the method is independent of languages. There is n o complex algorithm and we don  X  t need to w aste too much time on calcula t-ing. All we need is the search engine and a little prior knowledge about URL pairing patterns with high credibility. That is to say, our method is fast and easy while the precision is not low as well. (2) Error Analysis
Accor ding to Table 7, there are 409 false bilingual web pages pairs. We analyze these false pairs and classify them into four categories shown in Table 8.
 I: Monolingual: The pairs URLs are in the same language.
 II: Fake Bitext: The pairs URLs are false bitext according t o their content.

III: Error of the content: One of the pages or both of the pages have been me n-tioned to have moved to other pages or not to exist in the certain websites any more.
IV: Invalid Websites: After the loading of the websites, it shows  X  404  X  or other hint that the web page is invalid.

Table 9 shows the distribution of false pairs. Nearly half of the incorrect pairs are due to the monolingual reason and another problem is the error of the content. If we can identify the language of the URL pairs, theoretically, we can filter out the mon o-lingual URL pairs.
 (3) URL analysis
According to the 4460 pairs of URLs in the evaluation, we made an analysis about the site types of the URLs and the pattern distribution. Table 10 shows the distribution of different site types and their precisions.

From the Table 10, we can find that the precision of the governmental websit es ranks the first in the three types. It reaches the precision of 95.05% which is absolut e-ly above the total precision of our method. The other two types  X  precisions are all under 90%. It indicates that during our experiment, the governmental type of the we b-sites will be much more easily found compared with the educational websites and the organizational websites. In this paper we have presented a way to min e bilingual webpages with the help of search engine and the patterns rep lacing. When choosing the high credibility bilingual URL pairing patterns to do the replacement, we can find the corresponding Chinese URL in a fast way. The experiment ultimately collected a total of 153 , 683 the En g-lish websites of the parallel sites, wh ere there are 88 915 new URLs are determined to exist on the Internet. And the accuracy of actual parallel pages is 90 . 8 %. Though the provement.

In the future work, we pla n to extract bilingual websites of other website types and search for the webpages of other districts like Taiwan, etc. We will also find the pa t-terns with high credibility of different language pairs to see if the method still works on detecting the paral lel websites of other languages.
 This work is supported by National Natural Science Foundation of China through the grant (No.70903032), Major Projects of National Social Science Fund (13&amp;ZD174), and National Social Science Fund Project (N o.14BTQ033).

