 New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e. not reported prev iously). With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NE D task, two term reweighting approaches are proposed to impr ove NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortiu m (LDC) datasets TDT2 and TDT3 show that the proposed m odel can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems. H.3.3 [ Information Systems ]: Information Search and Retrieval; H.4.2 [ Information Systems Applications ]: Types of Systems  X  decision support .
 Algorithms, Performance, Experimentation Topic Detection and Tracking, New Event Detection, Named Entity, Real-time Indexing Topic Detection and Tracking (T DT) program aims to develop techniques which can effectively organize, search and structure news text materials from a vari ety of newswire and broadcast media [1]. New Event Detection (NED) is one of the five tasks in TDT. It is the task of online iden tification of the earliest report for each topic as soon as that report arrives in the sequence of documents. A Topic is defined as  X  X  seminal event or activity, along with directly related events and activities X  [2]. An Event is defined as  X  X omething (non-trivial) happening in a certain place at a certain time X  [3]. For instance, when a bomb explodes in a building, the exploding is the seminal event that triggers the topic, and other stories on the same t opic would be those discussing so on. Useful news information is usually buried in a mass of data generated everyday. Therefore, NED systems are very useful for people who need to detect novel information from real-time news stream. These real-life needs often occur in domains like financial markets, news analysis, and intelligence gathering. In most of state-of-the-art (currently) NED systems, each news story on hand is compared to all the previous received stories. If all the similarities between them do not exceed a threshold, then the story triggers a new event. They are usually in the form of cosine similarity or Hellinger similarity metric. The core problem of NED is to identify whether two stories are on the same topic. Obviously, these systems cannot take advantage of topic information. Further more, it is not acceptable in real applications because of the large amount of computation required in the NED process. Other systems organize pr evious stories into clusters (each cluster corresponds to a topic) , and new story is compared to the previous clusters instead of stories. This manner can reduce comparing times significantly. Ne vertheless, it has been proved that this manner is less accurate [4, 5]. This is because sometimes stories within a topic drift far away from each other, which could lead low similarity between a story and its topic. On the other hand, some proposed NED systems tried to improve accuracy by making better use of named entities [10, 11, 12, 13]. However, none of the systems ha ve considered that terms of different types (e.g. Noun, Verb or Person name) have different effects for different classes of st ories in determining whether two stories are on the same topic. For example, the names of election candidates (Person name) are very important for stories of election class; the locations (Location name) where accidents happened are important for stories of accidents class. So, in NED, there still exist following three problems to be investigated: (1) How to speed up the detection procedure while do not decrease the detection accuracy? (2) How to make good use of cluster (topic) information to improve accuracy? (3) How to obtain better news story represen tation by better understanding of named entities. Driven by these problems, we ha ve proposed three approaches in this paper. (1)To make the det ection procedure faster, we propose a new NED procedure based on news indexing-tree created dynamically. Story indexing-tree is created by assembling similar stories together to form news cl usters in different hierarchies according to their values of similarity. Comparisons between current story and previous clus ters could help find the most similar story in less comparing times. The new procedure can reduce the amount of comparing times without hurting accuracy. (2)We use the clusters of the fi rst floor in the indexing-tree as news topics, in which term we ights are adjusted dynamically according to term distribution in the clusters. In this approach, cluster (topic) information is us ed properly, so the problem of theme decentralization is avoided. (3)Based on observations on the statistics obtained from traini ng data, we found that terms of different types (e.g. Noun and Ve rb) have different effects for different classes of stories in de termining whether two stories are on the same topic. And we propose to use statistics to optimize the weights of the terms of different types in a story according to the news class that the story belongs to. On TDT3 dataset, the new NED model just uses 14.9% compar ing times of the basic model, while its minimum normalized co st is 0.5012, which is 0.0797 better than the basic model, and also better than any other results previously reported for this dataset [8, 13]. The rest of the paper is organized as follows. We start off this paper by summarizing the previous work in NED in section 2. Section 3 presents the basic model for NED that most current systems use. Section 4 describe s our new detection procedure based on news indexing-tree. In s ection 5, two term reweighting methods are proposed to improve NED accuracy. Section 6 gives our experimental data and evalua tion metrics. We finally wrap up with the experimental results in Section 7, and the conclusions and future work in Section 8. Papka et al. proposed Single-Pass clustering on NED [6]. When a new story was encountered, it wa s processed immediately to extract term features and a query representation of the story X  X  content is built up. Then it was co mpared with all the previous queries. If the document did not trigger any queries by exceeding a threshold, it was marked as a new event. Lam et al build up previous query representations of story clusters, each of which corresponds to a topic [7]. In this manner comparisons happen between stories and clusters. Recent years, most work focus on proposing better methods on comparison of stories and document representation. Brants et al. [8] extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of documents. Good improvements on TDT bench-marks we re shown. Stokes et al. [9] utilized a combination of evidence from two distinct representations was the usual free text vector, the other made use of lexical chains (created using WordNet) to build another term vector. Then the two representa tions are combined in a linear fashion. A marginal increase in effectiveness was achieved when the combined representation was used. Some efforts have been done on how to utilize named entities to improve NED. Yang et al. gave location named entities four times weight than other terms and named entities [10]. DOREMI research group combined semantic similarities of person names, location names and time together with textual similarity [11][12]. UMass [13] research group split document representation into two parts: named entities and non-na med entities. And it was found that some classes of news coul d achieve better performance using named entity representation, while some other classes of news could achieve better performance using non-named entity representation. Both [10] and [13] used text categorization technique to classify news stories in advance. In [13] news stories are classified automatically at first, and then test sensitivities of names and non-name terms for NED for each class. In [10] frequent terms for each class are removed from document representation. For example, wo rd  X  X lection X  does not help identify different elections. In their work, effectiveness of different kinds of names (or terms with different POS) for NED in different news classes are not investigated. We use statistical analysis to reveal the fact and use it to improve NED performance. In this section, we present the basic New Event Detection model which is similar to what most current systems apply. Then, we propose our new model by extending the basic model. New Event Detection systems use ne ws story stream as input, in which stories are strictly time-ordered. Only previously received stories are available when dealing with current story. The output is a decision for whether the current story is on a new event or not and the confidence of the decision. Usually, a NED model consists of three parts: story representa tion, similarity calculation and detection procedure. Preprocessing is needed before generating story representation. For preprocessing, we tokenize words, recognize abbreviations, normalize abbreviations, add part-of-speech tags, remove stopwords included in the stop list used in InQuery [14], replace words with their stems using K-stem algorithm[15], and then generate word vector for each news story. We use incremental TF-IDF model for term weight calculation [4]. In a TF-IDF model, term frequency in a news document is weighted by the inverse documen t frequency, which is generated from training corpus. When a new term occurs in testing process, there are two solutions: simply ignore the new term or set df of the term as a small const (e.g. df = 1). The new term receives too low weight in the first solution (0) and too high weight in the second solution. In incremental TF-IDF model, document frequencies are updated dynamically in each time step t : where D t represents news story set received in time t , and df means the number of doc uments that term w occurs in, and df means the total number of documents that term w occurs in before time t . In this work, each time window includes 50 news stories. Thus, each story d received in t is represented as follows: d weight d t w weight d t w weight d t w  X  where n means the number of distinct terms in story d , and where N t means the total number of news stories before time t , and tf ( d , w ) means how many times term w occurs in news story d . We use Hellinger distance for the calculation of similarity similarity is defined as follows: For each story d received in time step t , the value is a score used to determine whether d is a story about a new topic and at the same time is an indication of the confidence in our score exceeds the threshold  X  new , then there exists a sufficiently similar document, thus d is a old story, otherwise, there is no sufficiently similar previous document, thus d is an new story. Traditional NED systems can be classified into two main types on the aspect of detection procedure: (1) S-S type, in which the story on hand is compared to each story received previously, and use the highest similarity to determine whether current story is about a new event; (2) S-C type, in which the story on hand is compared to all previous clusters each of which representing a topic, and the highest similarity is used for final decision for current story. If the highest similarity exceeds threshold  X  new , then it is an old story, and put it into the most similar cluster; otherwise it is a new story and create a new cluster. Previous work show that the first manner is more accurate than the second one [4][5]. Since sometimes stories within a topic drift far away from each other, a story may have very low similarity with its topic. So using similarities between stories for determining new story is better than using similarities between story and cl usters. Nevertheless, the first manner needs much more comparing times which means the first manner is low efficient. We pr opose a new detection procedure which uses comparisons with previo us clusters to help find the most similar story in less comparing times, and the final new event decision is made according to the most similar story. Therefore, we can get both the accuracy of S-S type methods and the efficiency of S-C type methods. The new procedure creates a news indexing-tree dynamically, in which similar stories are put together to form a hierarchy of clusters. We index similar stories together by their common ancestor (a cluster node). Dissi milar stories are indexed in different clusters. When a story is coming, we use comparisons between the current story and previous hierarchical clusters to help find the most similar story which is useful for new event decision. After the new event decision is made, the current story is inserted to the indexing-tree for the following detection. The news indexing-tree is de fined formally as follows: where r is the root of S-Tree , N C is the set of all cluster nodes, N is the set of all story nodes, and E is the set of all edges in S-Tree . We define a set of constraints for a S-Tree : i . , is an non-terminal node in the tree C ii N i  X  X   X  ii . , is a terminal node in the tree S ii N i  X  X   X  iii . , out degree of is at least 2 C ii N i  X  X   X  iv . , is represented as the centroid of its desendants C ii i N  X  X   X  For a news story d i , the comparison procedure and inserting procedure based on indexing-tree are defined as follows. An example is shown by Figure 1 and Figure 2. Comparison procedure: nodes with highest similarities, e.g., C 1 2 and C 1 3 Step 2 : for each selected node in the last step, e.g. C to all its direct child nodes, and select  X  nodes with highest nodes. Step 3 : record the terminal node with the highest similarty to d e.g. s 5 , and the similarity value (0.20). Inserting d i to the S-tree with r as root: Find the node n which is direct child of r in the path from r to the terminal node with highest similarity s , e.g. C Otherwise, if n is a terminal node, then create a cluster node instead of n , and add both n and d i as its direct children; if n is an non-terminal node, then repeat this procedure and insert d sub-tree with n as root recursively. Here h is the length between n and the root of S-tree . the cluster represents the stories in it. Hence we add no constraints on the maximum of tree X  X  height and degree of a node. Therefore, we cannot give the complexity of this indexing-tree based procedure. But we will give the number of comparing times needed by the new procedure in our experiments in section7. In this section, two term reweighting methods are proposed to improve NED accuracy. In the first method, a new way is explored for better using of cl uster (topic) information. The second one finds a better way to make use of named entities based on news classification. TF-IDF is the most prevalent mode l used in information retrieval systems. The basic idea is that the fewer documents a term appears in, the more important the term is in discrimination of documents (relevant or not relevant to a query containing the term). Nevertheless, in TDT doma in, we need to discriminate documents with regard to topics rather than queries. Intuitively, using cluster (topic) vectors to compare with subsequent news stories should outperform using stor y vectors. Unfortunately, the experimental results do not support this intuition [4][5]. Based on observation on data, we find the reason is that a news topic usually contains many directly or indirectly related events, while they all have their own sub-subjects which are usually different with each other. Take the topic described in section 1 as an example, events like the explosion and salvage have very low similarities with events about crimin al trial, therefore stories about trial would have low similarity w ith the topic vector built on its previous events. This section focuses on how to effectively make use of topic information and at th e same time avoid the problem of content decentralization. At first, we classify terms into 5 classes to help analysis the needs of the modified model: Term class A: terms that occur frequently in the whole corpus, e.g.,  X  X ear X  and  X  X eople X . Terms of this class should be given low weights because they do not help much for topic discrimination. Term class B : terms that occur frequently within a news category, e.g.,  X  X lection X ,  X  X torm X . They are useful to distinguish two stories in different news categories. However, they cannot provide information to determine whether two stories are on the same or different topics. In another wo rds, term  X  X lection X  and term  X  X torm X  are not helpful in differentiate two election campaigns and two storm disasters. Therefore, terms of this class should be assigned lower weights. infrequently in other topics, e.g., the name of a crash plane, the name of a specific hurricane. News stories that belong to different topics rarely have overlap terms in this class. The more frequently a term appears in a topic, the more important the term is for a story belonging to the topic, therefore the term should be set higher weight. Term class D : terms that appear in a topic exclusively, but not frequently. For example, the name of a fireman who did very well in a salvage action, which may appears in only two or three stories but never appeared in other topi cs. Terms of this type should receive more weights than in TF-IDF model. However, since they are not popular in the topic, it is not appropriate to give them too high weights. Term class E : terms with low document frequency, and appear in different topics. Terms of this class should receive lower weights. Now we analyze whether TF-IDF model can give proper weights to the five classes of terms. Obviously, terms of class A are lowly weighted in TF-IDF model, which is conformable with the requirement described above. In TF-IDF model, terms of class B are highly dependant with the numbe r of stories in a news class. TF-IDF model cannot provide low weights if the story containing the term belongs to a relative small news class. For a term of class C , the more frequently it appears in a topic, the less weight TF-IDF model gives to it. This strongly conflicts with the requirement of terms in class C . For terms of class D , TF-IDF model gives them high weights correctly. But for terms of class E , TF-IDF model gives high weights to them which are not conformable with the requirement of low weights. To sum up, terms of class B, C, E cannot be properly weighted in TF-IDF model. So, we propose a modified model to resolve this problem. stories in a first-level cluster (a direct child node of root node) are on the same topic. Therefore, we make use of a first-level cluster to capture term distribution ( df for all the terms within the cluster) within the topic dynamically. KL divergence of term distribution term weights: weight d t w where where df c ( w ) is the number of documents containing term w within cluster C , and N c is the number of documents in cluster C , and N t is the total number of documents that arrive before time step t .  X  is a const parameter, now is manually set 3. KL divergence is defined as follows [17]: within the topic, and the less it o ccurs in other topics, it should be assigned higher weights. Obviously, modified model can meet all the requirements of the five term classes listed above. Previous work found that some classes of news stories could achieve good improvements by giving extra weight to named entities. But we find that terms of different types should be given different amount of extra weight for different classes of news stories. We use open-NLP 1 to recognize named entity types and part-of-speech tags for terms that appear in news stories. Named entity types include person name, organization name, location name, date, time, money and percentage, and five POSs are selected: none (NN), verb (VB), adjective (JJ), adverb (RB) and cardinal number (CD). Statistical analysis shows topic-level discriminative terms types for different classes of stories. For the sake of convenience, named entity type and part-of-speech tags are uniformly called term type in subsequent sections. Determining whether two stories are about the same topic is a compute correlations between terms and topics. For a term t and a topic T , a contingence table is derived: defined to be [16]: News topics for the TDT task are further classified into 11  X  X ules of interpretations X  (ROIs) 2 . The ROI can be seen as a higher level class of stories. The average correlation between a term type and a topic ROI is computed as: where K is the number of term types (set 12 constantly in the paper). M is the number news classes (ROIs, set 11 in the paper). P represents the set of all terms of type k , and R occurs in topic T . Because of limitation of space, only parts of the term types (9 term types) and parts of news classes (8 classes) are listed in table 2 with the averag e correlation values between them. The statistics is derived from labeled data in TDT2 corpus. (Results in table 2 are alread y normalized for convenience in comparison.) types in topic discrimination w ith respect to different news classes. We can see that, location name is the most useful term type for three news classes: Natural Disasters, Violence or War, Finances. And for three other categories Elections, Legal/Criminal Cases, Science and Discovery , person name is the most discriminative term type. For Scandals/Hearings, date is the most important information for topic discrimination. In addition, Legal/Criminal Cases and Finance topics have higher correlation with money terms, while Scie nce and Discovery have higher correlation with percentage terms. Non-name terms are more stable for different classes. From the analysis of table 2, it is reasonable to adjust term weight according to their term type and the news class the story belongs to. New term weights are reweighted as follows: weight d t w news class c and term type k . In the work, we just simply use statistics in table 2 as the rewe ighting parameters. Even thought using the statistics directly may not the best choice, we do not discuss how to automatically obtain the best parameters. We will try to use machine learning techniques to obtain the best parameters in the future work. In the work, we use BoosTexter [20] to classify all stories into one of the 11 ROIs. BoosTexter is a boosting based machine learning program, which creates a series of simple rules for building a classifier for text or attribute-value data. We use term weight generated using TF-IDF model as f eature for story classification. We trained the model on the 12000 judged English stories in TDT2, and classify the rest of the stories in TDT2 and all stories in TDT3. Classification results are used for term reweighting in formula (11). Since the class labels of topic-off stories are not given in TDT datasets, we cannot give the classification accuracy here. Thus we do not discuss the effects of classification accuracy to NED performance in the paper. We used two LDC [18] datasets TDT2 and TDT3 for our experiments. TDT2 contains news stories from January to June 1998. It contains around 54,000 st ories from sources like ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America etc. Only English stories in the collection were considered. TDT3 contains approximately 31,000 English stories collected from October to December 1998. In addition to the sources used in TD T2, it also contains stories from NBC and MSNBC TV broadcasts. We used transcribed versions of the TV and radio broadcasts besides textual news. TDT2 dataset is labeled with a bout 100 topics, and approximately 12,000 English stories belong to at least one of these topics. TDT3 dataset is labeled with about 120 topics, and approximately 8000 English stories belong to at least one of these topics. All the topics are classified into 11  X  X ules of Interpretation X : (1)Elections, (2)Scandals/Hearings, (3)Legal/C riminal Cases, (4)Natural Disasters, (5)Accidents, (6)Ongoi ng Violence or War, (7)Science and Discovery News, (8)Finance, (9)New Law, (10)Sports News, (11)MISC. News. TDT uses a cost function C Det that combines the probabilities of missing a new story and a false alarm [19]: where C Miss means the cost of missing a new story, P the probability of missing a new story, and P Target probability of seeing a new story in the data; C P Nontarget means the probability of seeing an old story. The cost C
Det is normalized such that a perfect system scores 0 and a trivial system, which is the better one of mark all stories as new or old, scores 1: New event detection system gives two outputs for each story. The new event or not. The second part is a score indicating confidence of the first decision. Confidence sc ores can be used to plot DET curve, i.e., curves that plot false alarm vs. miss probabilities. Minimum normalized cost can be de termined if optimal threshold on the score were chosen. To test the approaches proposed in the model, we implemented and tested five systems: System-1: this system is used as base line. It is implemented based on the basic model described in s ection 3, i.e., using incremental TF-IDF model to generate term weights, and using Hellinger distance to compute document similarity. Similarity score normalization is also employed [8 ]. S-S detection procedure is used. System-2: this system is the same as system-1 except that S-C detection procedure is used. System-3: this system is the same as system-1 except that it uses the new detection procedure whic h is based on indexing-tree. System-4: implemented based on the approach presented in section 5.1, i.e., terms are reweighted according to the distance between term distributions in a cluster and all stories. The new detection procedure is used. System-5: implemented based on the approach presented in section 5.2, i.e., terms of different types are reweighted according to news class using trained parameters. The new detection procedure is used. The following are some other NED systems: System-6: [21] for each pair of stories, it computes three similarity values for named entity, non-named entity and all terms respectively. And employ Suppor t Vector Machine to predict  X  X ew X  or  X  X ld X  using the similarity values as features. System-7: [8] it extended a basic incremental TF-IDF model to include source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, etc. System-8: [13] it split document representation into two parts: named entities and non-named entities, and choose one effective part for each news class. Table 3 and table 4 show topic-weighted normalized costs and comparing times on TDT2 and TDT3 datasets respectively. Since no heldout data set for fine-tuning the threshold available for experiments on TDT2, we only report minimum normalized costs for our systems in table 3. System-5 outperforms all other systems including sy stem-6, and it performs only 2.78e+8 comparing times in detection procedure which is only 13.4% of system-1. When evaluating on the normalized costs on TDT3, we use the optimal thresholds obtained from TD T2 data set for all systems. System-2 reduces comparing times to 1.29e+9 which is just 18.3% of system-1, but at the same time it also gets a deteriorated minimum normalized cost which is 0.0499 higher than system-1. System-3 uses the new detec tion procedure based on news indexing-tree. It requires even less comparing times than system-2. This is because story-story comparisons usually yield greater similarities than story-cluster ones, so stories tend to be combined together in system-3. And system -3 is basically equivalent to system-1 in accuracy results. System-4 adjusts term weights based on the distance of term distribu tions between the whole corpus and cluster story set, yiel ding a good improvement by 0.0468 compared to system-1. The best system (system-5) has a minimum normalized cost 0.5012, which is 0.0797 better than system-1, and also better than any other results previously reported for this dataset [8, 13]. Further more, system-5 only needs 1.05e+8 comparing times which is 14.9% of system-1. Figure5 shows the five DET curves for our systems on data set TDT3. System-5 achieves the minimum cost at a false alarm rate of 0.0157 and a miss rate of 0.4310. We can observe that System-4 and System-5 obtain lower miss probability at regions of low false alarm probabilities. The hypothesis is that, more weight value is transferred to key term s of topics from non-key terms. Similarity score between two stor ies belonging to different topics are lower than before, because their overlapping terms are usually not key terms of their topics. Figure 3 shows the minimum nor malized costs obtained by system-3 on TDT3 using di fferent parameters. The  X  init parameter is tested on six values spa nning from 0.03 to 0.18. And the  X  parameter is tested on four values 1, 2, 3 and 4. We can see that, when  X  init is set to 0.12, which is the closest one to  X  are lower than others. This is easy to explain, because when stories belonging to the same topic are put in a cluster, it is more reasonable for the cluster to repr esent the stories in it. When but there is no much difference between 3 and 4. Figure 4 gives the comparing times used by system-3 on TDT3 with the same parameters as figure 3. The comparing times are strongly dependent on  X  init . Because the greater  X  stories combined together, the mo re comparing times are needed for new event decision. So we use  X  init =0.13 ,  X  =3 ,  X  = 0.15 for system-3, 4, and 5. In this parameter setting, we can get bot h low minimum normalized costs and less comparing times. We have proposed a news i ndexing-tree based detection procedure in our model. It redu ces comparing times to about one seventh of traditional method without hurting NED accuracy. We also have presented two extensi ons to the basic TF-IDF model. The first extension is made by ad just term weights based on term distributions between the whole co rpus and a cluster story set. And the second extension to basi c TF-IDF model is better use of term types (named entities types and part-of-speed) according to news categories. Our experiment al results on TDT2 and TDT3 datasets show that both of the two extensions contribute significantly to improvement in accuracy. We did not consider news time information as a clue for NED task, since most of the topics last for a long time and TDT data sets only span for a relative short period (no more than 6 months). For the future work, we want to collect news set which span for a longer period from internet, and integrate time information in NED task. Since topic is a relative coarse-grained news cluster, we also want to refine cluster granularity to event-level, and identify different events and th eir relations within a topic. This work is supported by the National Natural Science Foundation of China under Grant No. 90604025. Any opinions, findings and conclusions or reco mmendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. [1] http://www.nist.gov/speech /tests/tdt/index.htm [2] In Topic Detection and Tracking. Event-based Information [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald, [4] Y. Yang, T. Pierce, and J. Carbonell. A Study on [5] J. Allan, V. Lavrenko, D. Ma lin, and R. Swan. Detections, [6] R. Papka and J. Allan. On-line New Event Detection Using [7] W. Lam, H. Meng, K. Wong, a nd J. Yen. Using Contextual [8] B. Thorsten, C. Francine, and F. Ayman. A System for New [9] S. Nicola and C. Joe. Comb ining Semantic and Syntactic [10] Y. Yang, J. Zhang, J. Ca rbonell, and C. Jin. Topic-[11] M. Juha, A.M. Helena, and S. Marko. Applying Semantic [12] M. Juha, A.M. Helena, and S. Marko. Simple Semantics in [13] K. Giridhar and J. Allan. Te xt Classification and Named [14] J. P. Callan, W. B. Croft, and S. M. Harding. The INQUERY [15] R. Krovetz. Viewing Morphology as An Inference Process. [16] Y. Yang and J. Pedersen. A Comparative Study on Feature [17] T. M. Cover, and J.A. Thomas. Elements of Information [18] The linguistic data consortiu m, http://www.ldc,upenn.edu/. [19] The 2001 TDT task definition and evaluation plan, [20] R. E. Schapire and Y. Singer. Boostexter: A Boosting-based [21] K. Giridhar and J. Allan. 2005. Using Names and Topics for 
