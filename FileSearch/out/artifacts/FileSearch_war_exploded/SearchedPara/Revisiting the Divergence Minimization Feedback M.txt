 Pseudo-relevance feedback (PRF) has proven to be an ef-fective strategy for improving retrieval accuracy. In this paper, we revisit a PRF method based on statistical lan-guage models, namely the divergence minimization model (DMM). DMM not only has apparently sound theoretical foundation, but also has been shown to satisfy most of the retrieval constraints. However, it turns out to perform sur-prisingly poorly in many previous experiments. We investi-gate the cause, and reveal that DMM inappropriately tackles the entropy of the feedback model, which generates highly skewed feedback model. To address this problem, we pro-pose a maximum-entropy divergence minimization model (MEDMM) by introducing an entropy term to regularize DMM. Our experiments on various TREC collections demon-strate that MEDMM not only works much better than DMM, but also outperforms several other state of the art PRF methods, especially on web collections. Moreover, unlike existing PRF models that have to be combined with the original query to perform well, MEDMM can work effec-tively even without being combined with the original query. H.3.3 [ Information Search and Retrieval ]: Relevance feedback; Retrieval models Algorithms, Theory Divergence minimization; maximum entropy; additive smooth-ing; query language model
Pseudo-relevance feedback (PRF) is an important general technique for improving retrieval accuracy in all retrieval models [13, 12, 2, 7, 15, 1, 14, 11, 9, 4, 5, 10, 3]. The basic idea of PRF is to assume that a small number of top-ranked documents in the initial retrieval results are relevant, and se-lect from these documents related terms to estimate a more accurate query model, which generally leads to improvement of retrieval performance.

In this paper, we revisit an interesting PRF method based on statistical language models, namely the divergence min-imization model (DMM) [15]. DMM uses an idea similar to the Rocchio algorithm in the vector space model [13]: it assumes that the feedback model is a language model that is very close to the language model of every document in the pseudo-relevant document set, but far away from the collection language model which can be regarded as an ap-proximation of the non-relevant language model, and then casts the estimation of the feedback model as an optimiza-tion problem. DMM is not only apparently theoretically well-justified [15], but also has been shown to successfully satisfy most of the retrieval constraints [3]. However, DMM turns out to perform surprisingly poorly in many experi-ments (e.g., [9]), as compared with other feedback methods (e.g., the relevance model [7] and the mixture model [15, 14]) that do not satisfy the retrieval heuristics well [3]. In order to understand the cause, we revisit and investigate DMM analytically, and reveal that the major cause is that the objective function of DMM inappropriately tackles the en-tropy of the feedback language model , which generates highly skewed feedback model.

To address this problem, we propose a maximum-entropy divergence minimization model (MEDMM) by introducing an entropy term to regularize DMM. Our experiments on various TREC collections show that MEDMM not only works much better than DMM, but also outperforms other baseline PRF methods, especially on web collections that are nois-ier. Moreover, unlike existing PRF methods that have to be combined with the original query to perform well, MEDMM alone, without being interpolated with the original query, can already lead to consistently effective performance. Retrieval using language models is often based on the KL-divergence retrieval function [6], where a document lan-guage model and a query language model are estimated re-spectively, and the document is scored based on the nega-tive KL-divergence between the document language model and the query language model. The query language model  X 
Q intuitively captures what the user is interested in, and thus would affect retrieval accuracy significantly. Without feedback information, query language models are often esti-mated by using the MLE method on the original query text: query Q ,and | Q | is the total number of terms in the query.
In the scenario of pseudo-relevance feedback, we assume the top-ranked documents F = { d 1 ...d | F | } from the initial retrieval are relevant, based on which we can estimate a feedback language model  X  F . It has been widely accepted that the feedback model  X  F has to be interpolated with the original query model  X  Q to perform well [15, 1, 9]. where  X  is the interpolation coefficient to control the amount of feedback.

The divergence minimization model (DMM) [15] was pro-posed for estimating  X  F . DMM uses an idea similar to the Rocchio algorithm in the vector space model [13]: it as-sumes that the feedback model  X  F is a language model that is very close to the language model of every document in the pseudo-relevant document set F , but far away from the collection language model  X  C which can be regarded as an approximation of the non-relevant language model. Specifi-cally, KL-divergence is used to measure the distance between language models, and the problem of computing  X  F is cast as solving the following optimization problem: where  X  d is the smoothed language model of feedback doc-ument d ,  X  d = 1 | F | assumes uniform contribution of each document d to the feedback model, and  X  is a non-negative parameter to force  X  F to be different from the background language model  X  C .

To diagnose the problem of DMM, we re-write the above optimization function (Formula 2) as follows: where H (  X  F )=  X  w p ( w |  X  F )log p ( w |  X  F )istheentropyof  X  ; H (  X  F , X  d )and H (  X  F , X  C ) are the cross entropy between  X  F and  X  d and between  X  F and  X  C , respectively.
We can see a term  X   X  (1  X   X  ) H (  X  F ) X  that captures the en-tropy of feedback model  X  F in Formula 3, and another term  X   X   X H (  X  F , X  C ) X  that penalizes non-discriminative words:
On the one hand, we should ensure the impact of the entropy term to prevent  X  F from being too skewed as a dis-tribution over words. Previous work has also shown the en-tropy heuristic is useful to improve the robustness of PRF [5].
On the other hand, we should generally choose a non-negative  X  to force  X  F to be different from the background model, and thus penalize non-discriminative words, an IDF-like effect. The effectiveness of the IDF heuristic has also been demonstrated in the literature of PRF [15, 9].
Although DMM naturally incorporates both the entropy term and the IDF term, there is a dilemma: the penalization of non-discriminative words (i.e., increasing  X  ) will reduce the impact of the entropy term (i.e., decreasing (1  X   X  )), and vice versa. This dilemma appears to be inevitable in DMM because the same parameter  X  is shared by the two terms. We hypothesize that it is this dilemma that causes the poor performance of DMM.
One possible way to relax this problem in DMM is to reduce the value of  X  without dampening the IDF effect. Note that in the DMM optimization function (Formula 3),  X  needs to be smoothed to avoid the zero-probability problem. DMM employs the standard smoothing methods using a col-lection language model  X  C [16]. We argue that, although the smoothing methods using a collection language model, such as the Dirichlet prior smoothing, have been shown to be ef-fective for smoothing document language models in retrieval [16], they may be inappropriate for the estimation of query language models because they tend to introduce too many non-discriminative terms into  X  F . As a result, if we use such smoothing methods, we have to increase the value of  X  to remove those non-discriminative terms, and this will further reduce the impact of the entropy term.

To prevent using a large  X  ,weevaluatesmoothingmeth-ods that do not rely on the collection language model, and find that the additive smoothing method [8] works effec-term frequency of w in d , | d | is the length of d , | V total number of distinct terms in the feedback document set F ,and  X  is a parameter that is set to 0 . 1whichworkswell in our experiments. As far as we know, our work is the first to study smoothing methods for query language models.
Although the above method may relax the dilemma, it does not solve the dilemma completely, because the weight of the entropy term  X   X  (1  X   X  ) X  still has a lower bound In order to find a way out of the dilemma, we introduce an additional entropy component  X (1  X   X   X   X  ) H (  X  F ) X  into Formula 3 to further emphasize the entropy term, leading to the following optimization problem: Now, we can see that the dilemma is solved: the IDF term H (  X  F , X  C ) and the entropy term H (  X  F ) have independent parameters  X  and  X  .

In addition, we propose another extension to further im-prove DMM by weighting feedback documents appropriately. DMM treats each feedback document equally by setting  X  = | , which may be non-optimal. In fact, appropriately weight-ing feedback documents has been shown to be useful in PRF [9, 3]. Following the relevance model [7], we set a weight for d based on the posterior of document language model:
Finally, observing the constraint w  X  V p ( w |  X  F )=1,we can use the Lagrange Multiplier approach to solve this op-timization problem and the following solution is obtained: This new feedback model is labeled as MEDMM .There are two free parameters: at the 0 . 05 level using the Wilcoxon non-directional test.
We use four TREC collections: WT2G, WT10G, Ro-bust04 and AP, which represent different sizes and genre of text collections. The queries are taken from the title field of the TREC topics. We use the Lemur toolkit to carry out our experiments. For all the datasets, the preprocessing of documents and queries involve Porter X  X  stemming and stop-word removal using a standard InQuery stopword list. An overview of the involved query topics, the total number of relevance judgments and the total number of documents in each collection are shown in Table 1.

Following previous work [15, 9], we fix the document lan-guage model, and focus on the evaluation of query lan-guage models. Specifically, we use a Dirichlet smoothing method [16] to estimate the document language models in all the experiments, where we set the Dirichlet prior to 1 , 000. We also fix the two basic parameters of PRF [15, 9], i.e., the number of feedback documents and the number of expansion terms, to their typical values 10 and 50, respectively. Although there are two free parameters, i.e.,  X  and  X  ,in MEDMM, our experiments show that their optimal settings appear to be stable across collections (see Figure 1). There-fore, we empirically set  X  =0 . 1and  X  =1 . 2forMEDMM in all the following experiments, which works well.
Our baseline methods include (1) the basic MLE query language model without feedback (MLE), as well as 3 stan-dard PRF based query language models: (2) the original divergence minimization model (DMM); (3) the regularized mixture model (RMM) [14], which works robustly and does not need to be interpolated with the original query; and (4) a state-of-the-art PRF query language model, the relevance model 3 (RM3) [1, 9].

Some existing works have also attempted to further ex-tend PRF in language models by incorporating additional evidences (e.g., term proximity [10]), or by using a feature-based method (e.g., [11, 4]) on top of standard PRF meth-ods. Generally, they either depend on the standard PRF methods, or take the standard PRF methods as features. And these methods mainly focus on different feedback ev-idences that are orthogonal to our work. In this regard, these methods are not involved in our experiments to avoid unnecessary apples-to-oranges comparison.
 Table 3: Comparison of different feedback models. Superscripts 0 / 1 / 2 indicate that the MAP improve-ment over MLE/DMM/RM3 is significant at the 0 . 05 level using the Wilcoxon non-directional test.
The top-ranked 1000 documents for each run are com-pared in terms of their mean average precisions (MAP), which also serves as the objective function for parameter training. In addition, we also consider the precision at top-10 documents (P@10), as well as the robustness index (RI)
Above all, we compare the performance of  X  X ure X  X eedback models without being interpolated with the original query model. The goal is to investigate whether a feedback model is able to capture the  X  X ssence X  of feedback documents while not drifting away from the query topic; we believe that an ideal feedback model should not rely on the interpolation with the original query to perform well. The comparison is summarized in Table 2, where we set the feedback coefficient  X  =1 . 0 (see Formula 1) for all methods (Note that we give priority to DMM by also optimizing its parameter  X  ), except RMM that does not require  X  . We first compare MEDMM with RMM, since RMM was purposely designed to work well without being interpolated with the original query. We can see that, MEDMM and RMM both consistently outperform other PRF methods and the MLE baseline, but MEDMM is clearly more effective which outperforms RMM in all ex-cept one case. In contrast to MEDMM and RMM, RM3 and DMM often do not improve over MLE, especially on Web collections WT10G and WT2G that are noisier. In summary, we demonstrate that MEDMM can be used to es-timate a more accurate and robust feedback language model, relaxing the constraint that a feedback language model has to be interpolated with the original query.

Next, we allow DMM, RM3 and MEDMM to be interpo-lated with the original query, and the interpolation coeffi-cient is tuned using 2-fold cross-validation, where the query topics are split into even and odd number topics to form the two folds. The performance comparison is reported in Table 3. We can see that MEDMM outperforms not only DMM but also the widely-accepted RM3. We also ob-serve that the improvements of MEDMM over DMM and RM3 are larger on the heterogeneous Web collections (i.e., WT10G and WT2G) than on the homogeneous news collec-tions (i.e., Robust04 and AP) 2 , suggesting that MEDMM is more noise-tolerant. Moreover, we find that MEDMM is not only more effective but also more robust across queries than existing methods in terms of RI.

Both Table 2 and 3 show that MEDMM outperforms its original version DMM by 6%-15% in terms of MAP on all collections, confirming empirically that fixing the revealed deficiencies of DMM indeed leads to better PRF models.
One may be interested in how much the newly introduced entropy term contributes to the improvement. To answer this question, we only apply the entropy term to DMM, while excluding other extensions (i.e., additive smoothing and document weighting), which leads to another run, la-beled as DMM ent . We also do a similar cross-validation evaluation on DMM ent , and report its performance in Ta-ble3. WecanseethatDMM ent outperforms DMM clearly in both effectiveness and robustness, suggesting that ensuring the feedback model to have relatively high entropy is desir-able, though MEDMM is still more effective than DMM ent thanks to the additive smoothing and document weighting.
Finally, we examine the sensitivity of retrieval perfor-mance with respect to different parameters in MEDMM, including  X  ,  X  and  X  (see Formulas 1 and 5). We first fix  X  =0 . 95,  X  =1 . 2and  X  =0 . 1. And when we look into one parameter, we only free that parameter while still fixing the other two. We plot the curves of MAP against each param-eter in Figure 1. It demonstrates that all these parameters work stably across collections. The optimal  X  value is usu-ally close to 1 . 0, say 0 . 95, suggesting again that MEFB works well without even being combined with the original query. The optimal  X  value is usually larger than 1 . 0, suggesting the additional entropy term is necessary; and the perfor-mance decreases quickly when we reduce  X  to be less than 1 . 0, confirming empirically that the entropy term in DMM, i.e.,  X  =1  X   X &lt; 1 . 0, does not work well.  X  works well in a small range around 0 . 1 for MEDMM, while the optimal  X  value in DMM is around 0 . 3 [15], mainly because that the additive smoothing method does not introduce as much noise as the Dirichlet prior smoothing method in DMM.
We revisited the divergence minimization feedback model, and suggested the hypothesis that the reason why this ap-parently theoretically well-justified model does not perform well empirically is because it generates highly skewed feed-back model. We then proposed to improve it by using an entropy term, leading to the Maximum Entropy Divergence Minimization Model (MEDMM). Evaluation results on mul-tiple collections show that MEDMM outperforms not only the original divergence minimization model but also several other state of the art feedback models, and that MEDMM can even perform effectively without being combined with the original query, which relaxes the widely-accepted con-straint that any feedback model has to be interpolated with the original query. Our work suggests that ensuring a feed-back model to have relatively high entropy may be an in-teresting new constraint applicable to all feedback methods. Further exploration of this new hypothesis would be an in-teresting future research direction.
