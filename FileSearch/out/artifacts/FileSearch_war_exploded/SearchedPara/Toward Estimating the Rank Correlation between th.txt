 The Kendall  X  and AP rank correlation coefficients have become mainstream in Information Retrieval research for comparing the rankings of systems produced by two dif-ferent evaluation conditions, such as different effectiveness measures or pool depths. However, in this paper we focus on the expected rank correlation between the mean scores observed with a test collection and the true , unobservable means under the same conditions. In particular, we propose statistical estimators of  X  and AP correlations following both parametric and non-parametric approaches, and with special emphasis on small topic sets. Through large scale simulation with TREC data, we study the error and bias of the estima-tors. In general, such estimates of expected correlation with the true ranking may accompany the results reported from an evaluation experiment, as an easy to understand figure of reliability. All the results in this paper are fully reproducible with data and code available online.
 Evaluation; Test Collection; Correlation; Kendall; Average Precision; Estimation
The Kendall  X  [3] and AP [8] rank correlation coefficients are widely used in Information Retrieval to compare rank-ings of systems produced by different evaluation conditions, such as different assessors [6], effectiveness measures [4] or topic sets [1]. One reason for this success is their simplicity: they provide a single score that is easy to understand.
In this paper we tackle the problem of estimating the cor-relation between the ranking of systems obtained with a test collection and the true ranking under the same conditions. Such estimates can make a nice companion to a set of eval-uation results, as a single figure of the reliability of the ex-periment. Voorhees and Buckley [7] proposed to report a similar figure in terms of sensitivity, that is, the minimum difference required between two systems to ensure a maxi-mum error rate in relative comparisons. Common practice nowadays is to report the p -value of a statistical significance test run either for each pair of systems (e.g. t -test) or for the whole set (e.g. ANOVA and F -test). They provide a sense of confidence about individual pairs of systems or about a swap somewhere in the ranking, but they do not give a general idea of how similar the observed ranking is to the truth.
We propose parametric and non-parametric approaches to estimate the  X  and AP correlations. Through large scale simulation with TREC data, we show that they have very low bias and small error even for mid-sized collections.
Let A = h a 1 , . . . , a m i and B = h b 1 , . . . , b m scores of the same set of m systems as observed under two different evaluation conditions, such that a i and b i refer to the i -th system. In many situations we are interested in the distance between the two rankings. Considering systems in pairs, a distance can be computed by counting how many pairs are concordant or discordant between the two rankings: a pair is concordant if their relative order is the same in both rankings, and discordant if it is the opposite. Kendall [3] followed this idea to define his  X  correlation coefficient  X  = #concordants  X  #discordants which evaluates to  X  1 when the rankings are reversed, +1 when they are the same, and 0 when there are as many concordant pairs as there are discordant. Note that the term #discordants / total can be interpreted as the expected value of a random experiment: pick two arbitrary systems and return 1 if they are discordant, or 0 if they are concordant. The Kendall  X  coefficient can thus be interpreted in terms of the probability of discordance.

Yilmaz et al. [8] followed this idea to define a correlation coefficient with the same rationale as Average Precision. It is similar to Kendall  X  , but it penalizes more if swaps occur between systems at the top of the ranking, much like AP penalizes more if the non-relevant documents appear at the top of the results. In particular, they considered that one of the rankings, say B , is the true ranking and the other one is an estimate of it. The random experiment is now as follows: pick one system at random from A and another one ranked above it, and return 1 if they are discordant, or 0 if they are concordant. Their AP correlation coefficient can then be defined just as in (1) as follows: Note that  X  AP also ranges between  X  1 and +1.
The previous section contemplated the case where we com-pute the correlation between two given rankings A and B . In this section we study the case where we are given a ranking A obtained with the sample of topics in the test collection, and want to estimate its correlation with the true ranking B over the population of topics, which is of course unknown. For simplicity, let us first assume that the systems are al-ready sorted in descending order by their mean score. Let us further define D ij as the random variable that equals 1 if systems i and j are discordant and 0 otherwise, that is, whether they are swapped in the true ranking. Both  X  and  X 
AP can be re-defined from (1) and (2) in terms of D ij alone: Since they are just a linear combination of random variables, their expectations are as in (3) and (4) but replacing D ij with E[ D ij ]. Note that each D ij is a Bernoulli random vari-able, so its expectation is just the probability of discordance E[ D ij ] = P (  X  i  X   X  j &lt; 0) = p ij . The problem of estimating the correlation with the true ranking thus boils down to es-timating the probability that any two systems are swapped. The next subsection presents four ways of achieving this.
Since each p ij is estimated independently from the other systems, let us simplify notation here to just p . In addition, let X 1 , . . . , X n be the differences in effectiveness between the two systems and for each of the n topics in the collection. The problem is therefore to estimate p = P (  X  &lt; 0) from these n observations. Recall that systems are assumed to be ranked by mean observed scores, so X &gt; 0 .

In the following we present two parametric estimators based on the Central Limit Theorem (CLT) and then two non-parametric estimators based on resampling.
The CLT tells us that X i s approximately normally dis-tributed with mean  X  and variance  X  2 /n as n  X   X  . Using the cdf of the normal distribution we can therefore estimate the probability of discordance. However, our estimates are likely off with small samples (see Section 3.1.2), so we as-sume X i  X  N (  X ,  X  2 ) and employ the t distribution to ac-count for the uncertainty in estimating  X  2 . Standardizing, we have that where T n  X  1 is the cdf of the t distribution with n  X  1 de-grees of freedom. The estimates  X   X  and  X   X  are computed via Maximum Likelihood as where s is the sample standard deviation. The C n factor [2] ensures that E[ X   X  ] =  X  . This bias correction is applied be-cause, even though s 2 is an unbiased estimator of  X  2 , by Jensen X  X  inequality s is not an unbiased estimator of  X  .
The problem when estimating  X  from a small sample is that the observations are likely to be concentrated around the mean and seldom occur near the tails. As a consequence, (7) is likely to underestimate the true dispersion in the pop-ulation. If the sample contains a few dozen observations this is not expected to be a problem, but with very small samples of, say, just 10 topics, it might be.

We propose a new and generic estimator to avoid this problem. Let us consider a distribution function F with parameter  X  . A random sample from this distribution is expected to uniformly cover the quantile space, that is, all quantiles are equally likely to appear in the sample. Thus, when we are given a sample we may force them to uniformly cover the quantile space and then select the  X  that minimizes the observed deviations. For instance, if our sample contains only one observation, we force it to correspond to the quan-tile 1 / 2; if we have two observations then we force them to be the quantiles 1 / 3 and 2 / 3. In general, if R i is the rank of X i within the sample, it will correspond to the R i / ( n + 1) ation of an observation X i is therefore The Minimum Squared Quantile Deviation estimator is then the one that minimizes the sum of squared deviations: Let us assume again a normal distribution, so that The sum of squared deviations is thus
X  X  2  X  a nd the second term cancels out because P e i = 0. To find the  X  and  X  that minimize this expression, we simply differ-entiate, equal to 0, and solve. The partial derivatives are a nd therefore, the estimators are
As above, the probability of discordance is estimated with the cdf of the t distribution as in (5), but using estima-tors (8) and (9) instead of (6) and (7).
In both the ML and MSQD estimators above we assumed that scores are normally distributed, but this is clearly not strictly true. A non-parametric alternative is the use of re-sampling to estimate the sampling distribution of the mean and from there the probability of discordance.
 placement from our original observations, and compute their sample mean X  X  . This experiment is replicated T = 1 , 000 times, yielding sample means X  X  1 , . . . , X  X  T . By the law of large numbers, the distribution of these sample means con-verges to the sampling distribution of X a s T  X   X  . The probability of discordance can thus be estimated as the frac-tion of times that X  X  i i s negative:
A potential problem with resampling from the original observations is again that estimates from very small samples are likely off. An alternative is to approximate the true pdf via Kernel Density Estimation, and use it to estimate the probability of discordance. The estimated pdf has the form w here k is the pdf of the kernel and h is the bandwidth. Next, we need to estimate the sampling distribution of the mean, which is basically the distribution of the sum of n variables drawn from  X  f . For n = 2 this requires the evalua-tion of the self-convolution of  X  f as follows:  X  f which involves the sum of n 2 terms. In general, for n vari-ables this requires the evaluation of n n terms, which is clearly unfeasible even for small samples, so instead we resort to Monte Carlo methods. As with the RES estimator, we gen-erate a random sample X  X  1 , . . . , X  X  n from  X  f and compute the mean X  X  . After T replications, the probability of discor-dance is estimated as the fraction of times that X  X  i i s nega-tive. We set T = 1 , 000 replications and use gaussian kernels.
There are two properties of the correlation estimators that we are interested in, namely error and bias. Error refers to the expected difference between the estimate and the truth. Here we measure absolute error, thus quantifying the ex-pected magnitude of the error when estimating the correla-tion of a given collection: Even if the error is small, it could tend to be in the same direction, that is, over-or underestimating the correlation. Bias refers to this tendency, measured as the expected dif-ference between the estimated and the true correlation: If the bias is positive it means that the estimator tends to overestimate the correlations. In general, we seek estimators with small error and zero bias.
From the above definitions it is evident that we need to know the true ranking of systems  X  , but this is of course unknown. To solve this problem we resort to the simulation method proposed by Urbano [5]. Given the topic-by-system matrix of scores B from an existing collection, it generates a new matrix A with the scores by the same set of systems over a new and random set of topics. There are two impor-tant characteristics of this method that are appealing for us. First, the simulated scores are realistic, as they maintain the same distributions and correlations among systems as in the original collection. Second, it is designed to ensure that the expected mean score of a system is equal to the mean score in the original collection, that is, E A s = B s . For us, this means that the true mean scores are fixed to be the mean scores in the original collection, that is,  X  s := B s . This al-lows us to analyze the error and bias of the estimators with a large number of simulated, yet realistic test collections.
We use the TREC 6, 7 and 8 ad hoc collections as eval-uated with Average Precision. As is common practice, we first drop the bottom 25% of results to avoid effects of possi-bly buggy systems. From each original collection, we simu-late 1 , 000 new collections of sizes n = 10 , 20 , . . . , 100 topics, leading to a total of 30 , 000 simulated collections. For each of them, we estimate  X  and  X  AP using each of the estimators defined above, and also compute the true correlations (recall that this is possible because the true system scores are fixed upfront when simulating new collections). Finally, for each correlation coefficient, original collection, topic set size and estimator, we compute expected error and bias.
 Two baselines are used to compare our estimators to. They are based on a split-half method that randomly splits the available topic set in two subsets, and then computes the correlations as if one was the truth and the other one the estimate. This is replicated a number of times for different subset sizes, up to a maximum of n/ 2 topics. The obser-vations are then used to fit a model and extrapolate the expected correlation with n topics. This simple estimator is found for instance in [7, 4]. Here we run 2 , 000 replicates to fit the model y = a  X  e b  X  x , and sample topics with and without replacement, leading to baselines SH(w) and SH(w/o).
Figure 1 shows that the error of the estimators is larger with small collections. This is somewhat expected, because collections with too few topics are unstable and the rankings of systems vary too much to begin with. The error seems to plateau at about 0.025 in all our estimators, though with small collections of just 10 topics they are expected to be off by about 0.065. With the usual 50 topics, the expected error is 0.035. We can finally observe that the typical SH Figure 1: Error of the estimators of  X  ( left) and  X  AP (right) for each of the three original collections. estimators are clearly outperformed by all our proposed es-timators. In general, with 30 X 40 topics they behave almost the same, but with small samples MSQD is slightly better.
Figure 2 shows that the correlations tend to be overesti-mated, especially with small collections, but this time we see clear differences among estimators. MSQD behaves much better than the others, especially with very small collections. With only 10 topics ML outperforms KD because there is just too little data to properly approximate the pdf , but with 20 or more topics it does a very good job at approximating the true distribution. ML, on the other hand, assumes a normal distribution and can therefore be less faithful to the data. Even at around 40 X 50 topics KD gets to slightly out-perform MSQD for the same reason. Overall, they seem to plateau at about 0.004, and RES always performs worse than the others. Finally, the SH estimator with replacement has a roughly constant bias of about 0.055. The SH estimator without replacement shows a clearly biased behavior prob-ably due to the choice of model.
In this paper we present two estimators of the Kendall  X  and AP rank correlation coefficients between the mean system scores produced by a test collection and the true, unobservable means. We proposed parametric and non-parametric alternatives, and through large scale simulation with realistic collections we showed that even with small topic sets the estimators have little bias and the errors are generally small with collections of medium size. These esti-mators may prove useful as an easy to understand indicator Figure 2: Bias of the estimators of  X  ( left) and  X  AP (right) for each of the three original collections. of reliability in the results of an evaluation experiment.
In light of the expected error with individual collections, our future work will mainly focus on the development of in-terval estimates. We also plan to study other estimators of discordance as well as the application of a fully bayesian ap-proach to estimate correlations. All the results in this paper are fully reproducible with data and code available online at http://github.com/julian-urbano/sigir2016-correlation . Acknowledgments. Work supported by the Spanish Gov-ernment: JdC postdoctoral fellowship, and projects TIN2015-70816-R and MDM-2015-0502. Florentino dimisi  X on.

