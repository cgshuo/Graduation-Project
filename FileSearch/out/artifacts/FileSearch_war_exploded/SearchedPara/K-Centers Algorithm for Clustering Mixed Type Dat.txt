 As a well-known data mining method, cluster analysis plays an important role in been applied to a variety of fields. 
The basic K-means algorithm, using the centroid (mean) of objects with numeric then, many clustering algorithms have been proposed to improve cluster analysis from different perspectives. Bezdek et al developed a fuzzy version of the K-means algorithm[2]. The K-means algorithm is wide ly used for clustering not only because However, working on only numeric data limits the use of the K-means algorithm when much categorical data is frequently dealt with. The K-modes algorithm, using a new dissimilarity measure and frequency-based method to update modes, can then cluster categorical data[3] . The K-prototypes algorithm then integrates the K-means and K-modes algorithms to allow for clustering mixed numeric and categorical valued data sets[4,5]. Moreover, the fuzzy K-mode s and fuzzy K-prototypes algorithms are developed based on fuzzy set theory[6,7], and they have built up clustering for mixed numeric and categorical valued data. However, both of the K-modes and K-prototypes take into consideration the effect of other attribute values with low frequency on accuracy. 
A new similarity measure is put forward in this paper, which considers the algorithm based on the K-prototypes algorithm can also cluster data of mixed nature (numeric and categorical). A the number of categorical attribute A j . An object D X dimension vector satisfies m p  X   X  0 . Specifically, when 0 = p all the attributes are categorical and when m p = all attributes are numeric, while attributes are of mixed type when m p &lt; &lt; 0 . Definition 2. Let z is used to represent the mean of attribute For example, if categorical attribute 
Z is composed of two parts: numeric dissimilarity and categorical dissimilarity. In definition 3, Euclidian distance is used for numeric attributes, while the categorical attributes. The determination of weight parameters  X  and  X  are relatively complicated. If 0 p = , which means data is categorical type, then 1 , 0 = =  X   X  ; if m p = , which data is mixed type, it is difficult to choose the right weight parameters. Generally, we set 1 =  X  and choose a greater weight parameter for  X  if we give emphasis to categorical valued attributes or a smaller value for  X  otherwise. The K-centers clustering technique, using the objective function similar to that of the basic K-means algorithm, redefines centroids and the dissimilarity of categorical minimize the objective function  X  X  X  where 1  X   X  is the fuzzy parameter. When 1  X  = , it is hard clustering, and when 1  X   X  , it  X  = , but may belong to several clusters indefinitely when 1  X  &gt; . 
Similar to the K-means algorithm, minimizing the objective function F is a nonlinear programming problem. The K-centers algorithm focuses on a incremental minimize the objective function F , and then regulate W and minimize objective optimized further. Theorem 1. Let centroids * Z be fixed  X  then the objective function F is minimized if and only if W satisfies: 
When 1 =  X  , the objective function F is linear programming problem and when computed using the Lagrange Multiplier. Theorem 2. Let membership matrix * W be fixed, then the objective function F numeric attribute For categorical attribute
The equation (2) can be transformed as followings: 
It is more reasonable that for categori cal data sets, the K-centers algorithm can derive centroids by computing the percentage of the reciprocal of attribute values, considering the effect of attribute values with different frequencies. The following properties of the K-centers algorithm are intuitive: Property 1. If 1 | | 1 n . However, it can be neglected because of low frequency. Thus the centroid for Property 2. For an attribute percentage of different categorical attribute values. occupy lower percentage. 
The K-centers algorithm, both hard and fuzzy clustering, can be described as follows: 
It can be proved that the iteration of the K-centers algorithm, the fuzzy parameter and the convergence of the objective function satisfy theorem 3. Theorem 3. If the fuzzy parameter  X  is large enough, the membership matrix cannot determine definitely which cluster the data belong to. So the fuzzy parameter  X  cannot be assigned to too large a value in the K-centers algorithm, otherwise clustering results may be unsatisfactory. and thus the time complexity is approximately ) ( n O , which indicates that the relationship between the size of data sets and computation complexity is linear. So the sets effectively. However, the K-centers algorithm also needs to be improved. For also predefines a us er-specified number k and cannot deal with outliers effectively. To evaluate the performance of the K-centers algorithm, experiments have been performed on several data sets and then the impact of different parameters on the algorithm is discussed. The data for the experiments are from UCI machine learning repository, including categorical databases soybean and voting, mixed type databases credit and cleve. Usually, numeric data are measured in different units. To deal with this problem, the transformation is necessary before clustering in order to map all the numeric data to the range [0, 1]. Then, acco rding to experiment results, we make comparisons between the K-centers, K-modes and K-prototypes algorithms. 
When data are clustered by hard K-centers clustering algorithm, an object is only partitioned to one cluster; when data are clustered by fuzzy K-centers clustering membership matrix describes belongings of each object to different clusters, it fails to clearly indicate which cluster one object belo ngs to. In practice, one object is assigned 
Experiment results can be analyzed using the accuracy is the number of objects shared by the cluster l and its original data set, n is the size of data sets. 
As Fig. 1 shows, for categorical data, the accuracy of hard K-centers algorithm is close to that of hard K-modes algorith m, but the accuracy of fuzzy K-centers fuzzy parameter  X  increases, the accuracy of the K-centers algorithm improves rapidly, but that of the K-modes algorithm d ecreases slightly. It is obvious that the K-centers algorithm is more accurate than the K-modes algorithm when applied to soybean database. Similarly, the accuracy of the K-centers and K-modes algorithms on voting database can also be compared in Fig. 2. In general, when the fuzzy parameter increases, the clustering accuracy is relatively stable no matter it is the K-centers or K-modes algorithm, that is, there is not notable improvement, which means the K-centers and K-modes algorithms can result in rather accurate results. But the K-centers algorithm remains superior to the K-modes algorithm. Furthermore, we also find that the results are stable using the fuzzy K-centers algorithm. The algorithm will produce the same result on voting database no matter how initial centroids are chosen. But for the fuzzy K-modes algorithm, results vary with different initial centroids. 
The impact of weight parameter  X  used in clustering data with mixed numeric and categorical values cannot be neglected, because improper  X  will have great influence on knowledge. In most cases, if categorical data is dealt with, we may choose a larger value difficulty with how to identify the value of  X  . Therefore, different values of  X  are tried in begins to decrease. Through further analysis, we find that when the fuzzy parameter  X  is larger than 1.1, the accuracy remains unchanged no matter whatever the initial centroids are, but when the fuzzy parameter  X  is 1.2, clustering accuracy reaches the peak. Given the same fuzzy parameter  X  , the best  X  is shown to be 0.1 when the accuracy is the highest. Fig. 3 also shows the clustering accuracy of the K-prototypes algorithm with varying fuzzy parameters. In general, with different fuzzy parameters, impact on the clustering accuracy. As shown in Fig. 3, the accuracy of K-centers can exceed 0.82, which indicates that the K-centers algorithm excel the K-prototypes algorithm in clustering credit database. 
Fig. 4 shows how K-centers and K-prototypes behave when applied to cleve database with different weight  X  . The accuracy of the K-centers algorithm improves but the accuracy of the K-prototypes algo rithm decreases slightly. After careful algorithm are not stable when initial centroids differ. As for fuzzy K-centers algorithm, the results are stable, which means that the algorithm is not sensitive to the change of initial centroids. The better weight value for the K-centers algorithm is 0.3. The clustering accuracy reaches 0.84 when  X  is 2. But the accuracy of K-prototypes algorithm is less than 0.78, which seems that the K-centers algorithm is more effective than K-prototypes applied to cleve database. 
For space limit, not all experimental curves or graphs are given above and the results are also observed as for many othe r data sets. Domains that regularly produce data of mixed nature will be benefited although it is needed to provide further justification for these results. 
In conclusion, the fuzzy K-centers algorithm gives more accurate clustering results initial centroids. Compared with the K-prototypes algorithm, the K-centers algorithm produces more effective results. As the extension to the K-prototypes algorithm, the K-centers algorithm can cluster mixed type data more effectively. In hard and fuzzy K-centers algorithm, we propose a new update method for centroids. Unlike the K-modes and K-prototypes algorithms, which only focus on attribute values with the highest frequency, the proposed algorithm considers attribute values with various frequencies. The K-centers algorithm is illustrated to be able to perform clustering better in most cases. The experiments on UCI machine learning database also show that the K-centers algorithm produce more accurate results than the K-modes and K-prototypes algorithms. Nevertheless, further improvements still can be made on the K-centers algorithm. For example, we can combine the K-centers algorithm with genetic algorithm so as to overcome local optimum. How to provide general guideline for users to choose appropriate parameter  X  ,  X  is also worth further research. Acknowledgments. This research was supported by the Natural Science Foundation of China (No. 70301004). 
