 Due to the extreme variety and complexity of language use, current text information modeling of  X  X eep structures X  of text that are related to meaning and understanding is still a very weak (or missing) aspect. Further improvements of retrieval effectiveness step, much work has been done in the research of meaning (or concept) based re-trieval, with major efforts devoted to approaches with word sense explicitly incorpo-insufficiency in word sense discrimination and representation. 
A well-known fundamental problem of IR models using index-term based text rep-resentations is the polysemy and synonymy issues (Kowalski and Maybury 2000), which has plagued IR research over the years. Synonymy decreases recall and polysemy decreases precision, leading to poor overall retrieval performance. For example, the vector-space model or VSM (Salton and Lest 1968, Salton 1971), so far the most widely used, may suffer from the problem of word-by-word match. Usually enhancement method to compensate the precise word-matching retrieval. Some data analysis techniques, such as LSI (Deerwester et al 1990) using  X  X atent X  semantic cate-synonymy problem significantly in some cases (but may also lead to limited effec-tiveness for the polysemy problem), though the resulting approaches usually have nothing to do with true meaning of documents. indexing, sometimes termed  X  X ense/semantic VSMs X . Note that the precursor of VSM, a  X  X otion space X  introduced by Luhn (1957), was actually a VSM in a concept space. Such a text representation could be language-independent. And if documents straightforward manner. But a common problem for the sense/concept (or any other common system of concepts for indexing and retrieval, and hence no agreement be-tween the semantic representation of documents and queries. term-VSM and sense-VSM in a more comprehensive text representation. We pro-weights, a document is represented by a matrix of term-by-sense weights, which leads to a matrix-based model, called a  X  X ense matrix model X  (SMM) for information re-trieval (Swen 2003). The model provides a framework in which many matrix proper-ties and computation techniques can be int roduced or developed for IR, and similarity measures can be properly defined by matrix norms or other attributes. The model also introduces novel techniques for document transformation using well-developed data analysis techniques such as multiway array decomposition and discrete cosine trans-form (DCT). 
In the next section we present an outline of the major points of SMM. Section 3 in-troduces DCT on document vectors and matrices. Section 4 describes our DCT im-plementation and experimental results. A summary is presented in section 5. We start from the point of view of the  X  X emantic triangle X  (Ogden and Richards 1930) as an illustration of the fundamental problem of IR (despite its known insufficiencies apply it to IR (details of the triangle model will not be elaborated here). 
In this model, the three ends of refrent/thing, thought/concept and symbol/word direct association between symbols (here our info queries) and things (relevant docu-inevitably introduced in the indirection. The problem may be largely attributed to the fact of irregular many-to-many mapping from words to senses. As a whole, the map-ping from words and senses in any natural language is in an irregular manner. 
In reality, such an irregularity must be admitted and represented in some way. Our tion then follows quite naturally, namely, 
Here ==&gt; stands for reduction. A straightforward manner to make use of such combined information in an IR formalism is that we should collect all the terms along with the senses they may or actually have in the document, and then index the docu-ment by a term-sense network for retrieval, 
This network relationship of words and senses may be further represented by a ma-trix of index-term by sense weights, resulting in a matrix representation of documents. A document collection is then represented as a term-sense-document space: In this space a sense becomes a term-by-document matrix (and hence the name SMM ). Clearly, such a matrix-based retrieval model is a  X  X ense expansion X  of VSM: VSM X  X  document vector of term weights is  X  X xpanded X  or  X  X plit X  (distributed) along immediate consequence is that documents and queries sharing no common terms may measures can be constructed. 2.1 Measure of Matrix Similarity There are several possible methods to evaluate the similarity between document ma-trices. Since matrices are vectors with more restrictions, norms for a matrix space can be defined. For any matrix norm, we may define the matrix distance by 
The concept of  X  X ngle X  between matrices may also be introduced in correspon-dence to vector angle. First, we may introduce a  X  X ormalized distance X , namely, dis-tance between normalized matrices: 
Then a correct angle may be defined: tors, this angle is proportional to the standard vector angle: 
Secondly, also note that we may define to be the cosine of the  X  X ngle X  between any two multipliable matrices A and B , based on the compatibility condition of matrix norms: AB A B b  X  , which ensures serve the defining conditions of vector norms. This definition generally holds for any multipliable matrices. However, to our docume nt matrix, there are two different pos-sible definitions of matrix angles: matrix angle similarity, namely, 
These are some of the options SMM provides for specific implementations. Other measurability (Swen 2003). For example, if we prefer measuring the similarity be-tween the reduced sense vectors, we would have where 1 2.2 Simpler SMM Cases applicable cases. Unary SMM. The simplest SMM, where only one sense (say,  X  X elevance degree X ) is defined for all index terms, and the document representation is n-by-1 matrix, namely a vector of single-sense weights. probability p i to be relevant for one case, then the probability of being the other case is q i = 1  X  p i . The document matrix of a binary SMM then takes the form as follows: VSM into two components according to a further importance assignment method, should be determined in specific applications (e.g., a prior probability distribution). POS SMM. The  X  X op-most X  senses of words are arguably the words X  part-of-speeches. A  X  X art-of-speech SMM X  in which the sense dimensions are the part-of-speeches of weights of a term X  X  part-of-speeches. ments. The simple one it to index each &lt;Word/POS&gt; pair as a VSM term, but record restricted terms). The standard VSM term weightings are directly applicable to these where the document matrix of takes the form the { p i , j } parameters may be set to the POS probability distribution of words in the collection be considered (instead of being computed for each document). Since current POS tagging has succeeded considerably, the weighting of POS SMM X  X   X  X enses X  can be expected to be effective and robust. When more realistic word senses beyond part-of-speeches are incorporated into elements for various senses, which we call sense weighting , corresponding VSM term weighting. 2.3 Other Applications SMM is usable to a wide range of applications where VSM is commonly used, with be  X  X eutral X  to the index terms so long that document matrices with multipliable sense dimensions are used, which means that SMM has the ability to compare documents in different languages. Advantages include: SMM: 
There is no necessary for a preprocessing of English-to-Chinese translation (or vice The only prerequisite is that: each language involved should be indexed with the same or convertible sense set (and in the same order). Studies from multi-lingual WordNet guage are largely common is prevailing. Thus such a cross-lingual sense set is possible. 
Using SMM for cross-lingual text filtering is also in the same principle, where the documents and user profiles can be in different languages but with compatible sense tion). On the other hand, like VSM, SMM with some similarity measures may be limited to monolingual cases. 
As with standard pattern recognition, most of the existing text classification meth-ods are based on VSM. Many of existing methods can be adapted to use with SMM by simply modifying the similarity measures. For example, clustering using a docu-rectly adaptable to SMM. An SMM version of the kNN method is also available, using the matrix distance measure to determine the k nearest neighbors and to let them vote for the class of the input sample. 
The SVM (support vector machine) categorization method may be modified to work with the matrix documents. To achieve an  X  X MM based SVM X , we simply re-place the decision function and the VSM-based SVM kernel to their SMM versions In this way, we treat the matrix data as a (restricted) vector in the  X  X lattened space X . 2.4 Document Transformation Documents represented as 1-way or 2-way array data can be transformed for specific ment preprocessing to eliminate stop words, terms of extreme low frequency, format case, some appropriate data transforms may be applied to extract independent or less fundamental important issue is to reduce the data samples to a simpler common space document transformation, namely MAD (Multiway Array Decomposition) and DCT (Discrete Cosine Transform) for text retrieval. 
We first discuss using MAD for SMM. The DCT is to be discussed in the later sections. SMM with MAD. Dimensionality reduction is a common practice in multiway data analysis (Kiers 2000). The term-sense-document data set of SMM is a good candidate simplification, truly independent components can be derived via techniques devel-oped in factor analysis. A commonly used data analysis method is orthogonal decom-position of multiway arrays. It is used to reduce the dimensionality of data representa-generalized E-Y decomposition of the term-sense-document  X 3-way array X  is applica-ble (Sidiropoulos and Bro 2000): N d respectively. Such decomposition may be illustrated as follows: The largest possible F is called the rank of D , rank( D ). For F '  X  F , way array to D (under some conditions, e.g., the leas-squares). The above result can be used to establish a dimensionality reduction model for SMM. First we introduce two quantities, The reduced matrices over the F -space of an original document D k and query Q are It is easy to see that SMM/MAD is quite different from LSI (Deerwester et al 1990), though the purpose looks similar. In LSI, term correlation is solely based on term co-search quality when the actual sense of a query term differs from its average meaning. In SMM/MAD, the sense dimensions add a more stable association between terms and documents. On the other hand, when using existing PARAFAC implementation of the decomposition, one could expect the computational overhead of SMM/MAD to be significantly larger than LSI. Since the document matrices are highly sparse, more efficient and/or approximate decomposition algorithms could be introduced for this specific case. compressing coding, such as JPEG, MPEG, H.261 (video telephony), etc. In DCT, the reduced common space is the (discrete)  X  X requency space X  (as opposed to the original of fixed dimensions. VSM with DCT. The 1-dimensional DCT operates on an n-dimensional feature term vector d is or written in expanded form,  X  ( )cos , ( ) (if 0) or (if 1 ), 1.. dci dci i inin
The transformed vector (with components in the frequency space) has a property that elements of higher intensity occur first from lower dimension positions, including quency, such that most high-frequency components will have very small values. It is easy to verify that T is an orthogonal transform: crete cosine transform (IDCT), is thus 
To suppress vector components of low intensity, a quantization processing is ap-plied on the transformed vector: constants on different elements, 
Some elements of ( q 1 , q 2 , ..., q n ) may be the same. We call such a quantization fac-tor list a quantization table.
 When documents are transformed, similarity measure is then computed using the DCT X  X d and quantized vectors, 12 12 12 sim(,) cos(,) cos(,) =  X  X  X  dddddd . SMM with DCT. As with image compression, the 2-d document matrices of SMM are appropriate objects for DCT processing. The 2-dimensional DCT operates on a document term-sense matrix D, defined as follows: where two quantization factors or tables for the term and sense dimensions are used. 
The transformed matrix has the property that elements of higher intensity occur from the upper left corner of lower frequencies, and most high-frequency matrix ele-ment have small values. Similarity measures are then computed for the quantized document matrices: 12 12 12 sim(,)cos(,) cos(,) =  X  X  X  DDDDDD . search (other tests including some unary SMM cases). We have so far tried the DCT method on the standard test collections that come with SMART version 11 (SMART 1992). These test sets would reflect at leas t partially the actual effects introduced by document DCT. Comparison experiments were made to show the differences between simple quantization table was used in all the tests: 
The VSM is specified to use the simple remove_s stemming and no weighting. The same VSM term weights are then delivered to a DCT indexing procedure. The same inverted retrieval procedure is used for the output vector files of both. The results are listed as follows. In these experiments, marginal to medium improvements were ob-served. Such a consistent performance may be attributed to the effectiveness of DCT. 
When the VSM uses optimal and robust term weighting, it will usually outper-forms the DCT model using the above simple quantization configuration. We think that an optimal quantization table should play the same role for DCT as term weight-form much better than this simplest case did. In this paper we present an outline of the sense matrix model SMM, on which much investigating retrieval effectiveness. The similarity judgment introduced by SMM can be regarded as a  X  X ecall device X  that increases recall (compared to VSM). The prob-hence would result in better retrieval effectiveness. We expect that SMM with effec-tive part-of-speech tagging may provide a good example for such a study. 
The document DCT experiments also indicate that DCT on documents may have the potential to lead to improved retrieval effectiveness. So far our DCT implementa-tion has achieved marginal improvements. Many issues remain open for research. We the framework of SMM includes many aspects and subcases that each alone may need more thorough understanding and experiments. The TREC dataset that we recently applied for will allow us to conduct more experiments at large in this research. Cur-rently we are experimenting SMM for the TREC 2004 Robust Track. The results will be discussed elsewhere. 
