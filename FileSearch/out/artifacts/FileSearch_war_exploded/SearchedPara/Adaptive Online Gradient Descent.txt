 which we denote number of rounds of the game.
 (Here,  X  K ( v ) denotes the Euclidean projection of v on to the convex set K .) Algorithm 1 Online Gradient Descent (OGD) 1: Initialize x 1 arbitrarily. 2: for t = 1 to T do 3: Predict x t , observe f t . 5: end for Zinkevich showed that the regret of this algorithm grows as  X  = 1 / ( Ht ) . Increasing convexity makes online convex optimization easier. same regret rates in both cases X  O (log T ) for uniformly convex functions and O ( convex functions? In this paper, we present an adaptive algorithm of this kind. the form give the  X  , . . . ,  X  T that minimizes the final regret bound.
 R n and suppose that sup be the largest value such that for any x  X   X  K , Algorithm 2 Adaptive Online Gradient Descent 1: Initialize x 1 arbitrarily. 2: for t = 1 to T do 3: Predict x t , observe f t . 4: Compute  X  t = 1 2 6: Update x t +1 =  X  K ( x t  X   X  t +1 (  X  f t ( x t ) +  X  t x t )) . 7: end for Theorem 1.1. The regret of Algorithm 2 is bounded by analyzed recently in [4, 5]. Section 4 (Theorem 4.1), where the result is extended to arbitrary norms. Theorem 2.1. Suppose we set  X  t +1 = 1 H In particular, loosening the bound, in a linear regret bound. However, we know from [2] that a O ( next section we provide an algorithm which interpolates between O (log T ) and O ( the regret depending on the curvature of the observed functions. towards the origin x 0 . Applying Theorem 2.1, we obtain the following result. f ( x ) + 1 2  X  t k x k 2 with for any sequence of non-negative  X  1 , . . . ,  X  T , then Proof. By Theorem 2.1 applied to functions  X  f t , H t +  X  t for which proves the the theorem.
 have 1 2 intermediate rates between log T and this section.
 P Lemma 3.1. Define where C t  X  0 does not depend on  X  t  X  X . If  X  t satisfies  X  t = C t H is proved similarly.
 Now, suppose Consider two possibilities. If  X  1: T &lt;  X   X  1: T , then If, on the other hand,  X  1: T  X   X   X  1: T , then Using the inductive assumption, we obtain Proof. (of Theorem 1.1) By Eq. 2 and Lemma 3.1, provided the  X  t are chosen as solutions to It is easy to verify that minimizing f 1 ( x ) and regret is negative on that round.
 the bound on the regret suffered by Algorithm 2 by a factor of O ( D 2 ) . achieves the same rates.
 { f t } , the bound on regret of Algorithm 2 is O ( Proof. Let  X  1 = Hence, the regret of Algorithm 2 can never increase faster than tions of [3].
 Algorithm 2 is O (log T ) .
 Proof. Set  X  t = 0 for all t . It holds that R T  X  1 2 P T t =1 G 2 t H of rates under assumptions on the curvature of functions.
 Corollary 3.3. Suppose H t = t  X   X  and G t  X  G for all 1  X  t  X  T .  X  1 = T  X  and  X  t = 0 for 1 &lt; t  X  T . Note that P (1  X   X  )  X  1 . Hence, Warmuth [6].
 convex function h if other than ` p as well). addressed in [6] for a variety of functions h ).
 Algorithm 3 General-Norm Online Gradient Descent 1: Input: convex function h 2: Initialize x 1 arbitrarily. 3: for t = 1 to T do 4: Predict x t , observe f t . 6: Let x t +1 = arg min x  X  K B h ( x, y t +1 ) be the projection of y t +1 onto K . 7: end for norms: the General-Norm Online Gradient Algorithm with  X  t +1 = 1 H potential function. By assumption on the functions f t , for any x  X   X  K , Combining both observations, x Summing over all iterations and recalling that  X  t +1 = 1 H 2 R T  X  equality stated before, Thus, by our assumption B h ( x, y )  X k x  X  y k 2 , we have Plugging back into Eq. (4) we get analogue of Theorem 1.1 for general norms.
 Algorithm 4 Adaptive General-Norm Online Gradient Descent 2: for t = 1 to T do 3: Predict x t , observe f t 4: Compute  X  t = 1 2 7: Let x t +1 = arg min x  X  K B h ( x, y t +1 ) be the projection of y t +1 onto K . 8: end for k X  f t ( x t ) k  X   X  G t . The regret of Algorithm 4 is bounded by sup x  X  K k x k = A = B and recover the results of Theorem 1.1.
