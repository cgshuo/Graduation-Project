 In recent years, due to the increasing popularity of dealing with a large number of data data mining. The problem of Association Rule Mining (ARM) in large transactional databases was introduced in [1, 5], Its basic idea is to discover important and interesting associations among the data items such that the presence of some items in a transaction association is as follows: 
The above form means that customers who buy  X  X read X  and  X  X ilk X  are likely to buy butter with 80% confidence. Inspired by work [1], several fast algorithms based on the level-wise Apriori framework[2]and partitioning [9] are proposed to remedy the performance bottleneck of Apriori. In addition, several novel mining techniques, such as parallel algorithms [10, 19], uncertain algorithm [4, 11] and other techniques[8, 12, 17], also received much attention lately. 
Most of the prevalent approaches assume that the transactions only carry Boolean information and ignore the valuable knowledge inherent in the quantities of the items. To find the association rules, the Boolean appr oach assumes that all we need to know is whether an item is contained in a transaction or not. Thus, Boolean association rules have the advantages that they are easy to inte rpret. However, the major drawback is that a given data matrix V (with e.g. amounts spent per customer per product) is converted to a binary matrix by treating non-zero amounts as plain  X 1 X  X . This approach simplifies the data mining algorithms but tends to lose lots of valuable information. 
In fact, considering that the quantities of the items in many datasets contain valuable information for us, it is necessary to provide a definition of association rules when the algorithm is the work [6], where they provided a stronger set of rules as Ratio Rules . A rule under this framework is expressed in the following form: 
This rule states that for each a amount spent on bread, a customer normally spends a b amount on milk and c amount on butter. Given such a definition, Ratio Rules allow quantitative information to be expressed in many practical applications, including forecasting such as  X  X f a customer spends $ a on bread, how much will s/he spend on butter? X  and  X  X hat-if X  scenarios such as  X  X e expect the demand for bread, how much butter should we stock up on?  X . 
Principal Component Analysis (PCA) is often used in data mining applications to discover the eigen-vectors of a dataset. Ratio Rules [6] can represent the quantitative associations between items as the principal eigen-vectors of a data matrix, where the values a, b and c in the example above correspond to the projections of the eigenvector in the space defined by bread, milk and butter. Because PCA factorization requires only contains a negative value, is containing negative values, because a customer X  X  spending should always be positive (there is no consideration of profit here). In this paper, we present a method to address this problem. 
Our method amounts to a novel application of non-negative matrix factorization (NMF) [7]. Like PCA, the NMF is aimed at learning latent components; unlike PCA, that all principle components are positive. However, we cannot directly apply NMF for We need to provide a bridge to bring NMF closer to association rules. In this work, we propose a novel method called Principal Non-negative Sparse Coding (PNSC), for learning the latent components. Furthermore, we extend the definition of Ratio Rules with all the ratio coefficients constrained to be non-negative. An example of such a rule according to this definition would be: 
This rule implies that the customer who spends $ 1 on bread tends to buy $ 2 of milk and $ 5 of butter. We will illustrate that the Ratio Rules by PNSC can also support a variety of important tasks such as forecasting and answering  X  X hat-if X  scenarios. introduces Principal Non-negative Sparse Coding (PNSC) and compares it with PCA. Section 5 presents the experimental results. Section 6 concludes the paper. sets of items. The data used in the notable Apriori algorithms [1] is market basket data that is naturally binary (two-valued), that we refer to here as Boolean. Either an item has been purchased by a customer and is in his/her market basket, indicated by a value of 1 (true), or it has not, indicated by a value of 0 (false). Although Although finding association rules in two-valued categorical data has been well researched, Problems occur when trying to find these types of rules in data with pure numeric (quantitative) or mixed definition to include quantitative data, and proposed quantitative association rules [16]. Ratio Rules are introduced. In the framework of Ratio Rules, Principal Component for these types of association rules: 
Boolean association rules[15, 18]: 
Quantitative association rules [16]: 
Ratio Rules [6]: 
In the Ratio Rules listed above, a , b and c are used to denote relative coefficient of the corresponding items, that is the association among such items can be represented where each Ratio Rule corresponds to a eigen-vector found by the PCA. Because PCA coefficient of rule can be of arbitrary numerical values. Obvious it is not interpretable in the context of association rules. The problem that we tackle is as follows. Given a large set of N customers and M products organized in a NM  X  matrix V where each row corresponds to a dollar amount spent by customers on the products. The goal is to find all Ratio Rules of the form: 
The above form means that there are some latent components, which represent the non-negative associations between the items of the dataset V . That is, customers who frequency.  X  matrix V . Each row vector of the matrix can be thought of as a M-dimensional point. Fig 1. (b) lists such 2-points distribution in graphical form. Here we assume that the dataset is made up of two clusters. Each cluster corresponds to a latent component that implies the association between items in the dataset. Given this set of N points, the items. We list two Ratio Rules discovered by PCA[6] in Fig.2 (a), where one contains negative values: 
According to the above definition of Ratio Rules, negative association between items ( X  X read X  and  X  X utter X ) does not make sense. However, the PCA does not prevent this from happening. Furthermore, the Ratio Rules which tend to represent the association between items often do not give the latent associations behind the distribution of these points. In Fig 1.(b), it is obvious that Ratio Rules found by PCA are deviated with the latent associations between items. 
In fact, from Fig.1 (b) we find that the latent associations are not mutually associations among the items correctly. Compar ed to Fig.2 (a), Fig 2(b) illustrates the Ratio Rules captured by our proposed PNSC in this work. Surprisingly, each rule could Furthermore, our method guarantees that all the values of Ratio Rules is positive. For example: From Fig.2 (b), we can find that the customers of one cluster mainly depend on the first rule where their relative spending amounts between bread and butter are closely to the (0.64:0.36). In addition, in this work, a support measurement is designed to illustrate the importance of each rule for the entire dataset. Let a set of N M-dimension training records be given as a M N  X  matrix V , with each 
PM basis components by a MP  X  matrix W , where each record can be represented as a ratio combination of the basis componen ts using the approximate factorization: corresponding basis components. factorization. PCA factorization requires that the basis components that are the columns of W are orthogonal and the rows of H are mutually orthogonal. One major components, and the data is represented as linear combinations of these vectors with positive and negative coefficients. In many applications, the negative components 
T 2.50 4.39 
T 3.91 8.44 
T 3.99 11.56
T 3.65 8.20 
T 4.99 15.58
T 4.64 14.05
T 6.92 24.69
T 2.75 4.08
T 6.14 3.67
T 4.24 1.64
T 4.32 2.32 ... ... ...

Tn 4.36 2.55 items, anomalies such as the following can happen, which is clearly not desirable: 4.1 Non-negative Matrix Factorization procedure for matrix factorization which imposes non-negative instead of orthogonal non-negative. 
In contrast to PCA, NMF uses the Euclidean distance of a matrix V from another matrix Y , defined as factorization is defined as where ,0 WH  X  means that all entries of W and H are non-negative. The above optimization can be done by multiplicative update rules [7]. 4.2 Non-negative Sparse Coding (NNSC) Although NMF is successful in matrix factorization, the NMF model as defined by the constrained minimization of Equation (2) does not impose the sparse constraints. Therefore, it can hardly yield a factorizatio n, which reveals local sparse features. The significantly non-zero latent coefficients. Related sparse coding is proposed in the work of [13] for matrix factorization. 
Considering both the non-negativity constraints and the sparseness are important for learning latent component, Non-Negative Sparse Coding (NNSC) is provided in [14], Here, we briefly the algorithm NNSC in [14] which combines the non-negativity and sparseness constraints in a matrix factorization. Definition 1. Non-negative sparse coding (NNSC) of a non-negative data matrix (. . : 0) ij Vieij v  X  X  is given by the minimization of As for estimating the hidden components H , the author [14] developed a multiplicative algorithm by iterating the following update rule: matrix W can be updated by the following three steps: 
In [14], the authors have proved that the objective function is non-increasing under the above iterative updating rules, and the convergence of the iteration is guaranteed. 4.3 Principal Non-negative Sparse Coding (PNSC) When a dataset V is decomposed with W and H , the column vectors of W make up a new basis components space, and each column value of H represent the corresponding projection on the new basis component space. In other words, every row coefficient of whole, the sum of every row vector of H represents the importance of corresponding base for the whole dataset. Therefore, we define a support measurement after normalizing every column of H : Definition 2. For every rule (column vector) of W , we define a support measurement: 
Consequently, we can measure the importance of each rule for the entire dataset by their s upport values. The high value of s upport implies more importance of such rule is given by for the whole dataset V . because they are more important than the other rules for the entire dataset. About the selection of k value, a simple method is taken such as: From above form (9), the Ratio Rules are obt ained effectively according that the sum of support values. Experiments are performed on synthetic and real datasets to illustrate that the proposed method is effective in mining Ratio Rules between items on quantitative matrix. Synthetic Dataset: We have applied both the PNSC and the PCA to a dataset that consists of two clusters, which contains 25 Gaussian distribution points on a x-y plane (generated with  X  =[3;5],  X  =[1,1.2;1.2,2]) and 50 points on y-z plain. (generated with  X  =[3;5],  X  =[2,1.6;1.6,2]) (Fig.3). Ratio Rules on PNSC: Table 1.lists all rules (columns of W ) and corresponding support values. According to association between items, are: 
In addition, Fig.4 (a) indicates all the data projection in PNSC subspace possess the corresponding support value (0.665) of 1 rule is consistent with intuition. Ratio Rules on PCA: According to the method by PCA, we retain two corresponding eigen-vectors as Ratio Rules in Table 2. From Table 2, we find some entities of Ratio Rule to contain negative values, which cannot be explained intuitively the association between such items. Furthermore, in the work [6], no measurement is given to rank the importance of Ratio Rules for the input dataset, thus we can not separate those Ratio Rules that are more important for the entire dataset. In addition, from Fig.4 (b), the projection of a dataset in a PCA space does not have the sparsity features, which are the features of PNSC instead. Real Dataset: NBA ( 459 11  X  ) The reason why we select this dataset is that it can give an intuitive meaning of such PNSC. Based on a general knowledge of basketball and through examination of these rules, we conjecture the 1 RR represents the agility of a player, which gives the Ratio of possesses once of assist per game will be also steal the ball once. 2 RR shows the number Ratio Rule can be interpreted with the following: an average player who makes better in method cannot give such ratio information behind the dataset. In addition, in the Table 4, we list the results according to the method by PCA and we will find some entities of intuition to explain the association that a player who adds 0.28 minute of play time will obtain ( -0.374) rebound per game and (-0.320) times of fouls. In this paper, we have defined the Principal Non-negative Sparse Coding (PNSC) as a combination of sparse coding with the constraints of non-negative matrix factorization. Based on PNSC algorithm, an extension of association rules is provided for learning latent components in large database. Although this approach is a special case of NMF quantifiable associations between items. We are very grateful to the anonymous reviewers and editor. Their many helpful and constructive comments and suggestions helped us significantly improve this work. 
