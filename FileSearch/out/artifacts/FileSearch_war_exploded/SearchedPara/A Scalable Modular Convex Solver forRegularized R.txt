 A wide variety of machine learning problems can be de-scribed as minimizing a regularized risk functional, with dif-ferent algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Ma-chines (SVMs), Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper de-scribes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as ` 1 and ` 2 penalties. At present, our solver implements 20 different estimation problems, can be easily extended, scales to millions of observations, and is up to 10 times faster than specialized solvers for many applications. The open source code is freely available as part of the ELEFANT toolbox. I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Optimization, Convexity Support Vectors, Gaussian Processes, Training Algorithms, Large-Scale, Bundle Methods, Parallel Optimization
At the heart of many machine learning algorithms is the problem of minimizing a regularized risk functional. That is, one would like to solve is the empirical risk. Moreover, x i  X  X are referred to as training instances and y i  X  Y are the corresponding labels. Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. l is a nonnegative loss function measuring the discrepancy between y and the predictions arising from using w . We assume that it is convex in w . For instance, w might en-ter our model via l ( x, y, w ) = (  X  w, x  X  X  X  y ) 2 . Finally,  X ( w ) is a convex function serving the role of a regularizer with regularization constant  X  &gt; 0.

If we consider the problem of predicting binary valued la-bels y  X  { X  1 } , we may set  X ( w ) = 1 2 k w k 2 , and the loss l ( x i , y i , w ) to be the hinge loss, max(0 , 1  X  y i  X  w, x recovers linear Support Vector Machines (SVMs) [25, 36]. On the other hand, using the same regularizer but changing the loss function to l ( x i , y i , w ) = log(1 + exp(  X  y yields logistic regression. Extensions of these loss functions allow us to handle structure in the output space [1]. Chang-ing the regularizer  X ( w ) to the sparsity inducing k w k to Lasso-type estimation algorithms [30, 39, 8].

The kernel trick is widely used to transform many of these algorithms into ones operating on a Reproducing Ker-nel Hilbert Space (RKHS). One lifts w into an RKHS and replaces all inner product computations with a positive def-inite kernel function k ( x, x 0 )  X  X  x, x 0  X  .

Examples of algorithms which employ the kernel trick (but essentially still solve (1)) include Support Vector regression [41], novelty detection [33], Huber X  X  robust regression, quan-tile regression [37], ordinal regression [21], ranking [15], max-imization of multivariate performance measures [24], struc-tured estimation [38, 40], Gaussian Process regression [43], conditional random fields [28], graphical models [14], expo-nential families [3], and generalized linear models [17].
Traditionally, specialized solvers have been developed for solving the kernel version of (1) in the dual, e.g. [9, 23]. These algorithms construct the Lagrange dual, and solve for the Lagrange multipliers efficiently. Only recently, re-search focus has shifted back to solving (1) in the primal, e.g. [10, 25, 36]. This spurt in research interest is due to three main reasons: First, many interesting problems in diverse areas such as text classification, word-sense disambiguation, and drug design already employ rich high dimensional data which does not necessarily benefit from the kernel trick. All these domains are characterized by large datasets (with the number of data points of the order of a million) and very sparse features (e.g. the bag of words representation of a document). Second, many kernels (e.g. kernels on strings [42]) can effectively be linearized, and third, efficient fac-torization methods (e.g. [18]) can be used for a low rank representation of the kernel matrix thereby effectively ren-dering the problem linear.

For each of the above estimation problems specialized solvers exist, and the common approach is to write a new solver for every new domain. Unfortunately, many implementations do not scale well and the scalable ones (e.g. SVMStruct [40]) are restricted to a rather specialized set of applications. Par-allel solvers are even more difficult to find. Finally, the is-sue of data locality is rarely addressed, e.g. situations where data is owned by several entities which are unwilling to share their parts of X and Y .

In this paper, we address all the above issues by develop-ing a fast, efficient, scalable, and parallel convex solver which can efficiently deal with data locality issues. We achieve this by decoupling the computation of the objective function and its gradient from the actual solver module. Our architecture is modular, i.e., one can plug and play with many different backend solvers many different loss functions, and different regularizers. In particular, we describe an efficient variant of the bundle method and provide rates of convergence.
The outline of our paper is as follows. In section 2 we will describe bundle methods and adapt them to our set-ting. We will also provide rates of convergence which are significantly better than those reported previously in litera-ture; this stems from a sophisticated analysis which modifies and tightens previous proofs. In section 3 we will describe various loss functions implemented in our solver, while sec-tion 4 will describe the architecture of our solver. We will also demonstrate the ease with which one can plug a off-the-shelf solver like LBFGS into our framework. Section 5 is devoted to extensive experimental evaluation which shows that our implementation is up to 10 times faster than state-of-the-art specialized solvers in many applications, and we conclude with an outlook and discussion in section 6. Proofs are relegated to the appendix. The basic idea behind a cutting plane method is as follows: Given a convex function g ( w ), it is always lower-bounded by its first-order Taylor approximation, i.e., This gives rise to the hope that if we have a set W = { w 1 , . . . , w n } of locations where we compute such a Tay-lor approximation, we should be able to obtain an ever-improving approximation of g ( w ) [22]. See Figure 2 for an illustration. Formally, we have which means that g ( w ) can be lower-bounded by a piece-wise linear function. Moreover, the approximation is exact at all w i  X  W . Note that if g ( w ) is not differentiable ev-erywhere, we can pick an arbitrary element of the subdif-ferential (which always exists) and use it instead of  X  w Finally, if g ( w ) is twice differentiable, we can use the latter to bound the deviation between the lower bound and g ( w ) by using the mean value theorem. We have where  X  M d 2 ( w, W ). Here M is an upper bound on the largest eigenvalue of the Hessian  X   X   X  2 w g ( w )  X   X  , and d min  X  w  X  W k w  X   X  w k 2 2 denotes the squared Euclidean distance between w and the set W .

Instead of minimizing g ( w ) directly, cutting plane meth-ods minimize it approximately by iteratively solving a linear Figure 1: A convex function (solid) is bounded from below by Taylor approximations of first order (dashed). Adding more terms improves the bound. program arising from its lower bound. Bundle methods are cutting plane methods stabilized with the Moreau-Yosida regularizer. Formally, they add a k w  X  w t  X  1 k 2 regularizer term to the objective to prevent the solution at time step t from moving too far away from the previous solution w t  X  1 Note that our algorithm is closely related but not identical to bundle methods. In our case, the regularizer is already built into the objective function. We describe details now.
Since the loss function l is assumed to be non-negative and convex, it follows that R emp ( w ) is also convex and lower bounded by 0. Moreover, denote From (3) it follows that We are now able to define our algorithm to minimize J ( w ). Algorithm 1 Bundle Method
Initialize i = 1, w 0 = 0, and W = { w 0 } . repeat until converged
The following lemma shows that Algorithm 1 makes con-tinuous progress towards the optimal solution.
 Lemma 1 Denote by w  X  the minimizer of J ( w ) and let J  X  be its minimum value. Then, the following holds: Moreover, the series J  X  i is monotonically increasing and the series J + i is monotonically decreasing.
 The value J + i  X  J  X  i is a lower bound on the duality gap := J i +1 ( w i )  X  J i ( w i ), and our algorithm stops once this quantity is reduced below a pre-specified tolerance .
In order to make progress in our analysis and in the im-plementation, we rewrite the problem of minimizing J i ( w ) as a constrained optimization problem. One can check that this amounts to solving minimize subject to  X  a j , w  X  + b j  X   X  for all j  X  i and  X   X  0 . (10b) We proceed to analyzing ` 1 and ` 2 regularization. Linear programming: If  X ( w ) = k w k 1 we can cast the Quadratic programming: If  X ( w ) = 1 2 k w k 2 2 , the dual Since the instances of (10), e.g. the linear and quadratic programs, will not change substantially after every iteration (we only add one more constraint at a time), a hotstart solver can be suitable to update the solution. Hence the time spent in the solver is typically small in comparison to the cost of computing gradients.
 Theorem 2 Let G  X k  X  w R emp ( w ) k be a bound on the norm of the subdifferential, and let  X ( w ) = 1 2 k w k 2 . Then the bundle method produces a duality gap of at most after t steps, where Note that this bound is significantly better than that of [40, 35], since it only depends logarithmically on the value of the loss and offers an O (1 / ) rate of convergence rather than the O (1 / 2 ) rate in previous papers. This is largely due to an improved analysis and would easily translate to algorithms of the SVMStruct type. It explains why the number of steps required is often considerably less than those predicted in theory  X  the previous bounds were rather loose.
 Corollary 3 Whenever the norm of the subdifferentials of R emp ( w ) is bounded for all k w k X  p 2 R emp (0) / X  , the bundle method will converge to any given precision.
 Corollary 4 The bundle method converges for any contin-uously differentiable loss function l .
Vectors are by default in columns; [  X  X  ] and [  X  ,  X  ] refer to matrices assembled in row and column fashion, respectively. Also, we use x  X  0 to imply x i  X  0 for all i .
A multitude of loss functions are commonly used to derive seemingly different algorithms. This often blurs the similar-ities as well as subtle differences between them, often for historic reasons: Each new loss is typically accompanied by at least one publication dedicated to it. We now discuss some commonly used loss functions. Tables 1 and 2 contain a choice subset of simple losses. More elaborate ones are discussed below. as described in Table 1. In this case a simple application of instance, for squared loss we have  X  This means that if we want to compute l and  X  w l on a large number of observations x i , represented as matrix X , we can make use of fast linear algebra routines to pre-compute This is possible for any of the 13 loss functions (and many more) listed in Table 1. The advantage of this unified repre-sentation is that implementation of each individual loss can be done in very little time. The computational infrastruc-ture for computing Xw and g &gt; X is shared.

Matters are slightly more complicated when maximizing the area under the ROC curve, various F  X  scores, Preci-sion@k, and ordinal regression losses, as proposed in [24, 25]. All those functions rely on  X  w, x i  X  to perform classification or ranking between the observations x i . Hence, we may use fast linear algebra to pre-compute f = Xw . Subsequently we sort f by its values, which yields the permutation  X  , i.e., the vector f  X  is sorted. We now describe the operations needed for multivariate scores: ROC Score: Let us assume that f is sorted, n + denote the number of positive examples, n  X  the number of negative examples, and n = n +  X  n  X  the total number of pairs whose labels do not match. It is well known that the ROC score is the fraction of examples ranked in the correct order, i.e., number of pairs ( i, j ) such that f i  X  f j for y i &lt; y n . Assume that y ij  X  { X  1 } . [24] shows that this translates into the loss l ( X, y, w ), which is given by Moreover, [24] shows that this can be maximized and the terms P i [ y ij  X  1] and P j [ y ij  X  1] for the maximizer y be obtained in linear time, once f is sorted. This allows us to compute the gradient  X  w l ( X, y, w ) Ordinal Regression performs essentially the same opera-tion. The only difference is that y i need not take on binary values any more. Instead, we may have an arbitrary number of different values y i (e.g. 1 corresponding to  X  X trong reject X  up to 10 corresponding to  X  X trong accept X , when it comes to ranking papers for a conference). [25] generalizes the results of [24] to show that also in this case the value and gradients of l can be computed in linear time, once f is sorted.
Soft Margin [38] max y 0 ( f y 0  X  f y +  X ( y, y 0 )) e y
Scaled Soft Margin [40] max y 0  X   X  ( y, y 0 )( f y 0  X  f ))  X   X  ( y, y 0 )( e y  X   X  e y ), where y  X  is the argmax of the loss
Softmax [14] log P y 0 exp( f y 0 )  X  f y h P Document Ranking [29] show that a large number of ranking scores (normalized discounted cumulative gain, mean reciprocal rank, expected rank utility, etc.) can be optimized directly by minimizing the following loss: l ( X, y, w ) = max Here c i is a monotonically decreasing sequence, the docu-ments are assumed to be arranged in order of decreasing relevance,  X  is a permutation, the vectors a and b ( y ) de-pend on the choice of a particular ranking measure, and a (  X  ) denotes the permutation of a according to  X  . In this case, a linear assignment algorithm will allow us to find the permutation maximizing the loss and compute the gradients subsequently.

A similar reasoning applies to F  X  scores [24]. Note that in all cases we may use fast linear algebra routines to accelerate the computationally intensive aspects Xw and g &gt; X .
Next we discuss  X  X ector X  loss functions, i.e., functions where w is a matrix (denoted by W ) and the loss depends on W x . Table 2 contains a number of cases (Note that in the table we use e i to denote the i -th canonical basis vector). Most notable are the cases of structured estimation, where is a large-margin loss [38] and  X ( y, y 0 ) is the misclassifica-tion error by confusing y with y 0 . Rescaled versions were proposed by [40] (we have  X  = 1 in Table 2). Log-likelihood scores of exponential families share similar expansions.
In these cases we may again take recourse to efficient lin-ear algebra routines and compute f = XW , which is now a matrix by means of a matrix-matrix multiplication. Like-wise, gradients are efficiently computed by g &gt; X , where g is now a matrix of the dimensionality of the number of classes. Let us discuss the following two cases: Ontologies for Structured Estimation: For hierarchical labels, e.g. whenever we deal with an ontology [7], we can use a decomposition of the coefficient vector along the hierarchy of categories.

Let d denote the depth of the hierarchy tree, and assume that each leaf of this tree corresponds to a label. We repre-sent each leaf as a vector in N d , which encodes the unique path from the root to the leaf. For instance, if the tree is binary, then we have y  X  X  0 , 1 } d .
 We may describe the score for class y by computing  X  w y , x  X  = P along the path from w y 1 to w y 1 ,...,y d . Assuming lexico-graphic order among vectors w , we can precompute the in-ner products by a matrix-matrix multiplication between the matrix of all observations X and W . A simple dynamic pro-gramming routine (depth-first recursion over the ontology tree) then suffices to obtain the argmax and the gradients of the loss function for structured estimation. See [7] for implementation details.
 Logistic Hierarchical Model: The same reasoning ap-plies to estimation when using an exponential families model. The only difference is that we need to compute a soft-max over paths rather than exclusively choosing the best path over the ontology. Again, a depth-first recursion suffices.
In this case, we need to solve one of the two problems: l ( x, y, w ) = max or l ( x, y, w ) = log X In both cases,  X  ( x, y ) is a user-defined feature map which describes the relationship between data and labels, e.g. by Figure 2: The architecture of our solver. Top: se-rial version. Bottom: parallel version. Loss values and their gradients, computed on subsets of the data (Data 1 to Data N), are combined for the overall gradient computation. means of a set of cliques as in the case of conditional random fields [28] or a Max-Margin-Markov network [38]. Note that in [38]  X  = 0, whereas in [40]  X  = 1.

The gradients and function values are normally computed by dynamic programming. In the first case this is done by solving for the argmax y 0 of the loss, which yields  X  ( x, y  X  ( x, y ) as the gradient. In the second case, this is achieved by computing a soft-max , or equivalently the expected value of  X  ( x, y 0 ) with respect to the exponential families distribu-tion induced by the sufficient statistics  X  ( x, y ).
Note that the user only needs to implement these opera-tions for his model to take advantage of the overall optimiza-tion and parallelization infrastructure, including the access to a range of different regularizers  X ( w ).
Recall that Algorithm 1 has two distinct computationally intensive stages: generating a i and b i , which requires com-puting the value and derivative of the empirical risk R emp and subsequently solving a quadratic program. The latter, however, only contributes to a small amount to the overall cost of solving the optimization problem, in particular for large amounts of data, as we shall see in Section 5. Note also that in addition to our bundle method approach there exists a large number of alternative optimization methods which are able to solve (1) efficiently, provided that they have ac-cess to the value R emp ( w ) and the gradient  X  w R emp ( w ).
Keeping in line with this observation, our architecture abstracts out the computation of R emp ( w ) and  X  w R emp from the bundle method solver (see Figure 2). The solver part deals with the regularizer  X ( w ) and is able to query the loss function for values of R emp ( w ) and  X  w R emp needed. This is very similar to the design of the Toolkit for Advanced Optimization (TAO) [5].

Depending on the type of loss function, computing R emp can be very costly. This is particularly true in cases where l ( x, y, w ) is the log-likelihood of an intractable conditional random fields or of a corresponding Max-Margin-Markov network. This effect can be mitigated by observing that both R emp ( w ) and its gradient can be computed in parallel by chunking the data and computing the contribution due to each chunk separately. In effect, we insert a multiplexer between the solver and the loss functions (see Figure 2). This multiplexer has the sole purpose of broadcasting the values of w to the losses and summing over the values and gradients of the individual losses before passing those values on to the solver.

The loss functions themselves only interact with the solver via an interface which queries their values and gradients. This means that it is trivial to add losses or solvers without changing the overall architecture.
By taking advantage of the unified design of Algorithm 1 our algorithm has several advantages over custom-built solvers: Simplicity: Our setup is surprisingly straightforward. This Modularity in the solver: It is possible to add more solvers Modularity in the loss function: In the same fashion it Parallelization: Since dealing with R emp ( w ) is the domi-Vectorization: Many of the computations especially dense One of the major advantages of our approach is that data can be stored locally on the units computing values and gradients of the loss functions. It is not necessary that the main solver has access to the data. Nor is it necessary that individual nodes share the data, since all communication re-volves around sharing only values and gradients of R emp ( w ).
Since our architecture is modular (see figure 2), we show as a proof of concept that it can deal with different types of solvers, such as an implementation of LBFGS [6] from TAO [5]. There are two additional requirements: First, we need to provide a subdifferential and value of the regularizer  X ( w ). This is easily achieved via Second, we need a method to assess solution quality. Lemma 5 Let x  X  R n , assume that f is convex on R n and let  X  &gt; 0 . Moreover let g ( x )  X   X  x f ( x ) +  X x . Then we have min Using the above lemma we can lower bound the minimum of the regularized risk functional. Using the definition we are able to bound the number of significant figures via This provides us with a good convergence monitor for all cases using quadratic regularization. Note that bound is not applicable when we use the ` 1 regularizer  X ( w ) = k w k However, this is a situation where convergence with LBFGS is poor, since k w k 1 is not continuously differentiable in w .
While the bundle methods use the past gradients to lower bound the convex objective function, BFGS is a quasi-Newton method that uses the past gradients to estimate the inverse of the Hessian. Furthermore, to bound the memory and computational costs associated with storing and updating the complete inverse Hessian, the LBFGS algorithm uses only the past n gradients ( n is user defined).

LBFGS is known to perform well on continuously differ-entiable problems, such as logistic regression, least-mean-squares problems, or conditional random fields [34]. But, if the functions are not continuously differentiable (e.g., the hinge loss and its variants) then LBFGS may fail. Empir-ically, we observe that LBFGS does converge well even for the hinge losses if we have a large amount of data at our disposition. We conjecture that this is due to the fact that unlike in the k w k 1 regularization, the non-differentiability is  X  X moothed-out X  by averaging over many losses.
We now demonstrate that out algorithm is versatile by comparing it with other (state-of-the-art) solvers in different estimation problems such as classification (soft margin loss), regression ( -insensitive loss), and ranking (NDCG ranking loss [29]). We also show that the algorithm is scalable in the cases of varying numbers of observations, regularization magnitudes, and numbers of computers.

The experiments were carried out on a cluster of 1928 1.6GHz Itanium2 CPUs and 3.2 Gbyte/s bidirectional band-width per link 2 . The time reported for the experiments are the CPU time. One exception is for parallel experiments where we report the CPU and network communication time. We use the datasets in [25, 36] for classification tasks. For regression tasks, we pick some of the largest datasets in Lu  X  X s Torgo X  X  website 3 . Since some of the regression datasets http://nf.apac.edu.au/facilities/ac/hardware.php http://www.liacc.up.pt/  X  ltorgo/Regression/ DataSets.html are highly nonlinear, we perform a low rank incomplete Cholesky factorization [19] and keep the dimension less than 1000. For ranking tasks, we use an MSN web search data with 1000 queries.

For classification experiments, we compare our software bmrm with svmperf 4 [25] and libsvm [9]. For regression ex-periments, we compare bmrm with svmlight [23] and libsvm . To the best of our knowledge, the competing softwares are the state-of-the-art in the estimation problems we are inter-ested at. We do not have competing software for the ranking experiment.
To see the scaling behaviors of different solvers, we car-ried out this experiments with the regularization constant  X   X  { 1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00001, 3e-05, 1e-05, 3e-06 } . The results are shown in Fig-ure 3. As can be seen from the figure, bmrm is usually faster than svmperf and significantly outperforms libsvm 5 . The difference between bmrm and svmperf is very small in log-log scale but in reality bmrm can be up to 10 times faster than svmperf .
We investigated the scaling behavior of bmrm in regression problems, in particular, using the -insensitive loss function. The regularization constants used are the same as in classi-fication experiment. The results are shown in Figure 4. As can be seen from the figure, bmrm is significantly faster than both svmlight and libsvm .
We now study the scaling behavior of bmrm in terms of varying dataset sizes. Unlike the in [25] where regulariza-tion constant  X  was well-chosen, we fixed  X  = 10  X  5 for all experiments here. We report the CPU time for training a binary soft margin classifier on every sub-datasets in Fig-ure 5.
We used an improved and much faster version of svmperf provided by the author. libsvm solves problem in the dual and known to be slower then  X  X rimal X  solver in the case of linear kernel. Figure 5: Runtime of bmrm on different sub-dataset (chunk) size in log-log scale plot.

It is interesting to see from Figure 5 that bmrm took longer time to converge on smaller sub-dataset than in the case of larger sub-dataset. This is probably because the empirical risk R emp defined on smaller sub-dataset is  X  X oisier X  than that defined on larger sub-dataset. When given a fixed reg-ularization magnitude  X  , the  X  X oisier X  problem is clearly harder to solve and hence take iterations. For larger sub-dataset sizes, the choice of  X  seems appropriate and the time grows with the number of observations.
Here, we the study of the scaling behavior of bmrm in terms of parallelization by solving three different estimation prob-lems, namely binary classification (soft margin loss), ordinal regression, and ranking (NDCG loss), on a cluster of work-station. The number of computers we used ranges from { 1, 2, 4, 8, 16, 32, 40, 48, 56, 64 } . The time reported in Figure 6 is the sum of CPU and network communication time.

Figure 6 shows the algorithm for soft margin loss compu-tation scales inversely proportional to the number of nodes (computers) n , i.e., the time is given by T ( n ) = s + t/n , where s is the time required to solve the inner subproblem using linear or quadratic programming solver and t the time for loss value and gradient computation. The ranking and ordinal regression plots in Figure 6 also show that the  X  does not affect this scaling behavior directly.

For ranking experiments, the algorithm does not scales as well as in classification cases. The main reason is that the ranking dataset is grouped into queries of different sizes. The nodes that have larger queries tend to take longer than those with smaller queries. This caused some nodes to stay in idle state before getting the updated weight vector w from the master node.
We address the issue of the rate of convergence of bmrm and its dependence on the value of  X  . In Figure 7, we plot the quality of the solution  X  (see Proof of Theorem 2) as a function of number of iterations for various values of  X  for many datasets. Not surprisingly we see that the number of iterations depends on  X  : for decreasing  X  the number of iterations increases. This is consistent with Theorem 2. For most datasets, the rate starts very fast then slows down and becomes linear.
It is worth noting that throughout previous experiments, we have demonstrated that our solver can deal with many loss functions from classification, regression to ranking. There are many more loss functions that our solver can deal with such as quantile estimation, novelty detection, least squared regression etc. For these problems our method works well and exhibits similar nice convergence properties as described in previous sections. Unfortunately, it is very hard to find competing software to compare our solver with, and due to space constraint, we choose to omit the results. Results for these problems will be published in a separate report. Related Work Our work is most closely related to the prize-winning paper of Joachims [25]. In fact, for a par-ticular solver, namely a bundle method, a set of loss func-tions l , namely binary ` 1 soft-margin loss, ROC-scores, F scores and ordinal regression, and a particular regularizer  X , namely quadratic regularization, both methods are equiv-alent. The advantage in our solver is the use of efficient linear algebra tools via PETSc [2], the modular structure, the considerably higher generality in both loss functions and regularizers, and the fact that data may be decentralized.
Moreover, our work is related to [11], where MapReduce is used to accelerate machine learning on parallel comput-ers. We use similar parallelization techniques to distribute the computation of values and gradients of the empirical risk R emp ( w ) to a cluster of workstations. Given the lack of availability of a robust MapReduce implementation (only Java versions such as Hadoop are freely accessible), our im-plementation details differ significantly.

Finally, several papers [26, 10] advocate the use of Newton-like methods to solve Support Vector Machines in the  X  X ri-mal X . However, they need to take precautions when dealing with the fact that the soft-margin loss function used in an SVM is only piecewise differentiable. Instead, our method only requires subdifferentials , which always exist for convex functions, in order to make progress. The large number of and variety of implemented problems shows the flexibility. Extensions and Future Work We believe that the frame-work presented in this paper will prove very fruitful in the future. Besides applications of the solver to other estimation problems, such as covariate shift correction and distributed novelty detection there are a number of extensions: Summary On a wide variety of datasets our algorithm out-performed state-of-the-art custom built solvers, converging to the optimal solution within 500 iterations through the dataset. Unlike other bespoke custom built solvers, our soft-ware is open source and freely available for download. We invite the research community to contribute to the codebase as a shared resource ( http://elefant.developer.nicta. com.au/ ).
 Acknowledgments
We thank Sam Roweis and Yasemin Altun for helpful discussions, and Thorsten Joachims for providing his im-proved svmperf and some datasets. Also, we sincerely thank ANUSF team for their great help with the experiments. Part of this work was done when Quoc Le was at NICTA. NICTA is funded through the Australian Government X  X  Backing Aus-tralia X  X  Ability initiative, in part through the Australian Re-search Council.
Proof Lemma 1. Recall that the Taylor expansion is ex-the upper bound on J  X  follows. Since J ( w )  X  J i ( w ) for all w it also follows that their minima satisfy the same inequality. Since w i is the minimizer of J i ( w ) the lower bound follows.
The third claim follows from J i ( w )  X  J j ( w ) for all w and all i  X  j , hence the series J  X  i is monotonic. Since the min is being taken over a larger set at every iteration, the series J i is monotonically decreasing.
 To prove Theorem 2 we need an auxiliary lemma: Lemma 6 The minimum of 1 2 dx 2  X  cx with c, d &gt; 0 and x  X  [0 , 1] is bounded from above by  X  c 2 min(1 , c/d ) .
Proof. The unconstrained minimum of the problem is x  X  = c/d with minimum value  X  c 2 / 2 d . If c  X  d , this is also the constrained minimum. For c &gt; d we take x  X  = 1, which yields d/ 2  X  c . The latter is majorized by  X  c/ 2. Taking the maximum over both minima proves the claim.

Proof Theorem 2. Denote by  X  i the maximizer of D i (  X  ), i.e. the dual problem (12). We know by strong duality that D (  X  i ) = J i ( w i ), which provides a lower bound on the min-imum J  X  of J ( w ). Moreover, we know by Lemma 1 that is an upper bound on the size of the duality gap and therefore on the quality of the solution. After solving D i +1 (  X  ) we will obtain a new lower bound, i.e. J  X  i +1 = D i +1 (  X  i +1 is monotonically decreasing, it follows that if we can lower bound the increase in J  X  i in terms of i we are going to obtain a convergence guarantee for J + i  X  J  X  i , too.
Obviously, we cannot compute J i +1 ( w i +1 ) explicitly, since it involves solving a quadratic program. However, in order to obtain a lower bound on the improvement, we simply consider a line-search along the line joining [  X  i , 0] to [0 , 1]. Denote by  X   X  (  X  ) :=  X   X  X  i , (1  X   X  )  X  the solution obtained by optimizing (12) along this line, for  X   X  [0 , 1]. Clearly, any such  X   X  (  X  ) is feasible.

We need to analyze two separate regimes of the line search: a) the maximum is attained for  X   X  (0 , 1) and b) the maxi-mum is attained for  X  = 1. In order to apply Lemma 6 we need to compute linear and quadratic terms of D i (  X   X  (  X  )) and express i in terms of  X  i .

Since the Taylor expansion is exact for all w j , we have denote the extended matrix [ A a i +1 ] and by  X  b the extended w i =  X  1  X   X  A[  X  i , 0]. This allows us to write J Moreover, since J i ( w i ) = D i (  X  i ) we may write Taking differences yields Next we compute the linear and quadratic terms of D i (  X   X  (  X  )). l =  X   X  D i (  X   X  (  X  )) = i and q =  X  2  X  D i (  X   X  (  X  )) = 1  X  [  X  i ,  X  1] &gt;  X  A &gt;  X  Using Lemma 6 it follows directly that the improvement in the dual and therefore the decrease in  X  i  X   X  i +1 is at least min(1 , l/q ). We now bound q further:
Since  X w i is in the convex hull of a 1 . . . a i , it follows that k  X w i k 2  X  G 2 , and hence q = 1  X  k  X w i  X  a i +1 k 2  X  4 G means that  X  i  X   X  i +1  X   X  i 2 min(1 ,  X  i  X / 4 G 2 ). Furthermore, until  X  i  X  4 G 2  X  we have  X  i +1  X   X  i 2 . This happens for at most log 2 `  X  0  X  4 G 2  X  steps. But, the initial dual optimization problem is empty, hence its value is 0, and we know that  X  is R emp (0). Therefore, for log 2 R emp (0)  X   X  2 log 2 G  X  2 steps we halve the upper bound on the improvement.

Subsequently, the reduction in  X  i +1 is at least  X  2  X / 8 G leading to the difference equation  X  i +1  X   X  i  X   X  2 i  X / 8 G Since this is monotonically decreasing, we can upper bound this by solving the differential equation  X  0 ( t ) =  X   X  with the boundary conditions  X  (0) = 4 G 2 / X  . This in turn we will need 8 G 2  X  X   X  2 more steps to converge.

Proof Corollary 3. The initial value of the regular-ized risk is R emp (0) and the empirical risk is a nonnegative function. Hence we know that w never must exceed a ball of radius p 2 R emp (0) / X  . Since we start from w = 0, we will never exit this ball. Hence G is bounded within this ball and Theorem 2 applies.

Proof Lemma 5. Since f is convex, for all x and x 0 . The right hand side is minimized by setting x =  X  1  X   X  x f ( x 0 ). Plugging this into the lower bound yields the right hand side of (15).
