 } Classification algorithms and document representation ap-proaches are two key elements for a successful document classification system. In the past, much work has been con-ducted to find better ways to represent documents. How-ever, most of the attempts rely on certain extra resources such as WordNet, or they face the problem of extremely high dimension. In this paper, we propose a new document repre-sentation approach based on n-multigram language models. This approach can automatically discover the hidden seman-tic sequences in the documents under each category. Based on n-multigram language models and n-gram language mod-els, we put forward two text classification algorithms. The experiments on RCV1 show that our proposed algorithm based on n-multigram models alone can achieve the simi-lar or even better classification performance compared with the classifier based on n-gram models but the model size of our algorithm is much smaller than that of the latter. Another proposed algorithm based on the combination of n-multigram models and n-gram models improves the micro-F1 and macro-F1 values from 89.5% to 92.6% and 87.2% to 91.1% respectively. All these observations support the va-lidity of our proposed document representation approach. I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation ; I.5.4 [ Pattern Recognition ]: Applications X  Text processing Algorithms, Performance, Experimentation N-Gram models, N-Multigram models, Document Represen-tation, Text Classification Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
With the rapid explosion of text in digital form, auto-matic text classification (TC) has been a hot research topic. As a specific type of pattern classification tasks, TC has two essential aspects. One is the classification algorithm and the other is document representation. During the past decades, a large number of categorization algorithms have been proposed for TC such as na  X   X ve bayes [16], k-nearest neighbor [28], support vector machines [9], boosting [24] and rule learning algorithms [25]. However, the performance of the classification systems also tightly depends on the docu-ment representation.

Document representation refers to the selection of appro-priate features to represent documents. A bag-of-word rep-resentation scheme is widely used in text classification due to its simplicity and efficiency [23]. Under this scheme, doc-uments are represented by bags of terms, each term being an independent feature of its own. A document can be rep-resented as a vector. Each item in the vector corresponds to an individual term and its value can be defined as a binary indicator or the absolute frequency or more elaborated mea-sures like TF*IDF [23]. Although bag-of-words has been ap-plied in many Information Retrieval (IR) fields, it has many shortcomings. For example, it can not reflect the relation-ship between words, such as the order in which the terms appear in the document and the syntax etc. Also, it can not distinct the senses of a single word under different context even if the senses are totally different.

One way to address the above problems is to use features with coarser granularity, such as phrases to replace or aug-ment single words. Intuitively, phrases can reduce the uncer-tainty of the meaning of single words. For example, in  X  X ava Script X , the meaning of the word  X  X ava X  can only be  X  X  type of programming language X  instead of  X  X sland of Indonesia X . Some researchers manage to get phrases through the back-ground knowledge base in form of simple ontologies such as WordNet [17], while others apply Natural Language Pro-cessing (NLP) methods together with some heuristic rules to generate phrases [13]. Part of these approaches can ob-tain certain improvement of the classification performance, but others do not work. Another way to address the draw-backs of bag-of-words representation is to use sense-based features instead of word-based features [11]. To detect the senses of a word, some extra resources, such as WordNet are needed. There are also some other approaches which rep-resent documents through hidden concepts discovered from the documents or use language models, such as n-gram mod-els [3, 1, 21].
Most of the above methods either need extra resources which may not be available for some text classification tasks, or they need very large space for models. In this paper, we enrich the document representation approaches for text classification purposes by applying a language model named n-multigram [6]. Our proposed approach is supposed to overcome the disadvantages of the previous document repre-sentation schemes. n-multigram can automatically discover the hidden semantic sequences in the documents under each category. A sequence is a frequently-occurring pattern in a given collection which consists of several continuous words in documents. It is not required to be a meaningful phrase, although it always turns out to be. The lengths of the dis-covered sequences are not necessarily same as in n-gram which makes it more flexible. The sequences are discovered through an Expectation-Maximization algorithm together with Viterbi training. In the training process, the vocab-ulary of the sequences and their probabilities are updated iteratively to maximize the likelihood of the training data.
Based on n-multigram models we put forward a text clas-sifier and compare it with the classifiers based on n-gram models. The experiments on a subset of RCV1 dataset show that the classifier based on n-multigram models can achieve similar or even better performance compared to the clas-sifier based on n-gram models in terms of both micro-F1 and macro-F1. However, the model size of our algorithm is much smaller than that of the latter. Another classifier we put forward is based on the combination of the n-multigram models and the n-gram models. This classifier improves the micro-F1 value from 89.5% to 92.6% and the macro-F1 value from 87.2% to 91.1%.

The rest of this paper is organized as follows: In Section 2, we present the related work on text representation. Then we give some detailed descriptions about the language models including n-gram models and n-multigram models in Section 3. In Section 4, we present the baseline classifier based on n-gram models together with our proposed classifiers based on n-multigram models and the combination of n-gram models and n-multigram models. Section 5 describes the experi-mentsonRCV1aswellassomediscussions. Finally,we conclude our work in Section 6.
Much work has been conducted to find out effective ap-proaches to represent document for text classification. We classify them into four groups: 1) Represent a document by phrases [2, 18, 13]; 2) Use the senses of words to represent a document [11, 22]; 3) Augment document representation by hidden concepts in a document [3]; 4) Employ language models, such as n-gram models [1, 21]. [2] and [18] represent documents by extracted phrases. The phrase extraction depends on the background knowl-edge embedded in existing ontologies such as WordNet, the MeSH (Medical Subject Headings) Tree Structure Ontology. In [2], Stephan et al. studied the representation approach on three data corpus and the experimental results proved the effectiveness of the approach. By extracting phrases based on a large publicly available knowledge base called the Uni-fied Medical Language System (UMLS), Yetisgen-Yildiz and Pratt came to the same conclusion as in [2] on a dataset composed of medicine documents [18].

In [13], Lewis used syntactic parsing to create indexing phrases. These phrases correspond to pairs of words in one of several specified syntactic relationships in the original document (e.g. verb and head noun of subject, noun and modifying adjective, etc.). The sparseness of the phrases, that is, the large number of different phrases and the low frequency of occurrence of individual phrases makes it hard to estimate the relative frequency of phrases, which further limits the contribution of the syntactic phrase representa-tion. What is more, a syntactic phrase representation is highly redundant (there are large numbers of phrases with essentially the same meaning), and noisy (since redundant phrases are not assigned to the same set of documents). To address these weaknesses, Lewis tried to cluster the phrases to recognize groups of redundant phrases and replace them with a single one. While the phrase clusters can improve per-formance, the improvement is not significant. Lewis ascribes the poor performance to the formation of phrases through syntactic parsing and the poor semantics of the phrases. In this paper, we propose a fully automatic approach to gener-ate semantic phrases. We do not limit the length of a phrase to two as Lewis did and there is no constraint on the rela-tionship between the words in the phrases. The discovered phrases are supposed to indicate the document content more precisely than single words.

Another attempt for document representation is to use sense-based features instead of word-based features [11, 22]. While words are immediately observable within a document, senses (meanings) are hidden . For instance, when the word  X  X pple X  appears in a document, it is not obvious whether it means a kind of fruit or an IT company. Therefore, a proce-dure is needed for recovering the senses from the words used in a specific context, which itself is a hot research problem: Word Sense Disambiguation [4, 8]. In [11], Kehagians et al. worked on a subset of the annotated Brown Corpus in which the sense of each word in the document is known. The sense space is defined by WordNet.

Kehagians et al. compared four document representation approaches. The first approach named Word Boolean(WB) is to represent a document by a vector in which each item is 0 or 1 to indicate whether the term appears in the doc-ument. The second approach named Word Frequency(WF) represents a document by a vector with each item being the number of times the word appear in the document. The other two approaches named Sense Boolean(SB) and Sense Frequency(SF) are computed in an exactly analogous man-ner to WB and WF, making use of senses rather than words. By testing the four representation approaches with several algorithms, the authors conclude that the sense-based repre-sentation approaches do not present an attractive alternative to word-based approaches.

Instead of converting each word to its sense and repre-senting a document by the senses, Ramakrishnan and Bhat-tacharyya represent a document by a vector of ranked synsets [22]. More information about synsets can be found in [17]. To get the ranked synsets, they construct a semantic graph for each document based on the words in the document and the synsets in WordNet as well as the lexical relations be-tween the synsets. After that, some graph-based ranking algorithms such as pagerank [20] are employed to rank the synsets. The experiments on the 20 Newsgroups dataset show that the representation approach based on certain rank-ing algorithms can improve the classification performance.
Cai and Hofmann proposed to use concept-based docu-ment representation generated through Probabilistic Latent Semantic Analysis (pLSA) to supplement word-or phrase-based features [3]. The overall approach can be decomposed into three stages: In the unsupervised learning stage, pLSA is used to derive concepts and to create semantic document representations over these concepts. In the second step, weak hypotheses are constructed based on both term fea-tures and concept features. The third stage is the combina-tion stage, which uses AdaBoost to combine weak hypothe-ses and to integrate term-based and concept-based informa-tion. The experiments on two standard document collec-tions from different domains support the validity of their approach.

Last but not least existing work to represent the docu-ments is by language models, specifically speaking, by n-gram models, which can exploit the relationship between words. N-gram models model the language with the proba-bility distribution of one word coming after the previous n-1 words. N-gram models can not only represent documents, it can also be directly used to classify documents as shown in [1, 21].

Although the approaches given above can make up for the bag-of-words representation scheme to some extent, there is still much space for improvement: (1) for the first two kinds of approaches, most of them need extra resources such as WordNet, which limit their generalization. For example, we can depend on WordNet to handle collections in En-glish. However, there are many collections in some languages which have no counterparts of WordNet. (2) in the first two kinds of approaches, the phrases are generated without con-sidering the labels of the documents. So the ability to dis-criminate documents between categories for the purpose of classification can be improved. (3) a complex framework is needed to leverage the concept-based document representa-tion in [3]. What is more, the concepts are extracted with-out considering the category information, which is actually useful for classification. (4) n-gram can improve the classifi-cation performance as shown in [21, 1]. However, the size of the vocabulary of grams in n-gram is huge due to the com-bination. Then a large amount of training data are needed to obtain a creditable model.
Language can be viewed as a stream of words put out by a source. Due to the syntactic and semantic constraints, the words are not independent. Many language models have been proposed to catch the characteristics of natural lan-guages. In this part, we will describe two kinds of lan-guage models which can serve as document representation approaches. In the next part, we would introduce two text classifiers based on these models separately together with a novel classifier based on the combination of the two kinds of models. N-gram models are a kind of widely used language models. It assumes that the probability of one word in a document depends on its previous n  X  1 words. Given a word sequence W = w 1 w 2 ,...,w T , the probability of W can be calculated as follows by the chain rule of probability:
Under n-gram models X  assumption, the only words rele-so p ( s ) can be written as: p ( w i | w i  X  n +1 ...w i  X  1 ) can be estimated from a corpus with Maximum Likelihood criteria. That is: where c (  X  ) denotes the number of occurrence.

In real-world applications, p ( w i | w i  X  n +1 ...w i under-estimated due to the data sparseness in a training data set. For example, many grams are assigned zero prob-ability when they do not appear in the training data. This is unreasonable since the zero count may be generated because of the insufficient data. To solve this problem, some mecha-nisms are indispensable to assign non-zero probability of po-tentially missing n-grams. The widely used methods include linear interpolation or back-off estimators. Linear interpo-lation involves an EM procedure to optimize the weight for each component [5]. Back-off models are relatively simple and are used in this paper, as employed by [21].

In back-off models, the smoothing function is as follows: = p ( w i where is the discounted probability and  X  ( w i  X  n +1 ...w i  X  1 malization constant calculated to be: =
An n-gram is first matched against the language model to see if it has been observed in the training corpus. If that fails, the n-gram is then reduced to an n-1-gram by shortening the context by one word. The discounted prob-ability (5) can then be computed using different smooth-ing approaches. A properly smoothing method can not only help prevent zero probabilities, but also improve the estima-tion accuracy of the language model. The standard smooth-ing approaches include linear smoothing [19, 27], absolute smoothing [19], Good-Turing smoothing [10] and Witten-Bell smoothing [27]. In this paper, we just consider the last two methods for simplicity.
 Good-Turing smoothing: Good-Turing smoothing dis-counts the frequency of r by GT r =( r +1) n r +1 /n r where n r denotes the number of events which occur exactly r times in training data [19]. The discounted probability is calcu-lated as [10]: Witten-Bell smoothing: Witten-Bell smoothing is very similar to Laplace smoothing [15], except it reserves prob-ability mass for out of vocabulary (OOV) values, whereas Laplace smoothing does not. Here the discounted probabil-ity is calculated as: where D ( w i  X  n +1 ...w i  X  1 ) is the number of distinct words that can follow w i  X  n +1 ...w i  X  1 in the training data [27]. In the uni-gram model, this corresponds to the size of vocabulary.
While in n-gram models, the statistical dependencies be-tween words are of fixed length n along the whole sentence, in n-multigram models, the dependencies are of variable lengths. n-multigram models assume that a sentence is a concatenation of independent variable-length sequences of words and the number of words in each sequence is not neces-sarily same but is at most n . The parameters of n-multigram models consist of the probability of each sequence. An es-timation of the set of parameters from a training corpus W can be obtained through a Maximum Likelihood (ML) esti-mation from incomplete data [7], where the observed data is the string of words W, and the unknown data is the seg-mentation S underlying the string of words. Thus iterative ML estimation of the parameters can be computed through an EM algorithm. The algorithm is given in Algorithm 1. Algorithm 1 : EM algorithm for parameter estimation
Step1: Collect all possible sequences from the training text; remove the sequences appearing strictly less than a given number of times c 0 to avoid overfitting; calculate their initial probability as their relative frequency;
Step2a: E-step: tosegmentthetrainingdataintose-quences according to the estimated probability distribu-tion;
Step2b: M-step: to estimate the probability distribution from the segmentation; sequence probabilities falling un-der a threshold p 0 are set to 0, except those of length 1 which are assigned a minimum probability p 0 ;
Step2c: go to step2a if the likelihood of the training text can be improved or the number of iterations has not reached the predefined number K;
In Algorithm 1, Step 1 initializes the parameters; Step2a is the E-step of the EM algorithm which focuses on the seg-mentation of training text into sequences. When segmenting the text, we need to calculate the likelihood; Step2b is the M-step which estimates the par ameters from the segmenta-tion obtained in Step2a. Now we give the explanation of the key components of this algorithm.
 Likelihood Calculation: Let W = w 1 w 2 ...w T denote a string of words, and S denote a possible segmentation which contains q sequences of words: S = s 1 s 2 ...s q . The n-multigram computes the joint likelihood L ( W, S )ofthetextstream associated with the segmentation S as the product of the probabilities of the successive sequences:
Denoting as S the set of all possible segmentations of W into sequences of words, the likelihood of W is : Algorithm 2 : Viterbi for segmentation
Initialization:  X  (0) = 1;  X  (0) = 0 //  X  ( t ) denotes the maximal likelihood of w 1 w 2 ...w on the previously estimated probability distribution of se-quences. //  X  ( t ) refers to the last segmenting position when we ob-tain  X  ( t )
Iteration:  X  ( t )= max  X  ( t )= t  X  arg max Termination:
Backtracking: i ( k )=  X  ( i ( k +1)  X  1) // i ( k ) represents the starting position of each sequence. Segmentation: Given a stream of text W = w 1 w 2 ...w T , we can segment it in different ways. The number of the segmentations can be up to O (2 T ). Infact,amongtheset of all possible segmentations of W, one segmentation usually contributes most of the likelihood of W according to our observation. We can use Viterbi algorithm to get the most likely segmentation. The algorithm is given in Algorithm 2. Estimation: After getting the segmentations of the train-ing text, we can estimate the parameters in two ways. One is to take all the segmentations into consideration. Another one is to consider the most likely segmentation only. For the former approach, the parameters can be estimated as follows: c ( s i ,S ) represents the number of occurrence of sequence s i in the segmentation S ; c ( S ) represent the total number of sequences in S ; X  ( k ) is the parameters obtained at iteration k; L ( S | W ; X  ( k ) ) is the conditional likelihood of the segmen-tation S given W, at iteration k.
When considering the most likely segmentation alone, we can simplify (13) as follows: S  X  ( k ) denotes the most likely segmentation based on the parameter obtained in iteration k.

As shown in [6], the performances of the two above ap-proaches are similar. So in this paper, we adopt the second one which is much simpler.

To calculate the probability of a test document under a given n-multigram model, the main problem we concern is how to handle unknown words which do not appear in the training data. In this paper, we first scan the test document to find out the number of the unique unknown words (in-stead of the number of occurrence of the unknown words) and then assign each of them the probability p 0 .Afterthat, we normalize the probability distribution to make sure the summation of all sequences X  probability is 1.
It is straightforward to construct text classifiers based on the n-gram models [21]. Given an n-gram model, it is easy to get the probability of a document to indicate the likelihood that the document is generated by this model. After training the n-gram model on the training data in each category, we can use the model to classify a test document d by: By using Bayes rule, this can be written as: Here, p ( d | c ) is the likelihood of d under category c ,which can be computed by an n-gram language model. The prior p ( c ) can be estimated from training data or can be used to incorporate more assumptions. p c ( w i | w i  X  n +1 ...w estimated from the training data in category c using a back-off model introduced in section 3.1.

Thus the classifier based on n-gram models overall is to learn a separate back-off language model for each category by training on a data set from that category. Then, to cat-egorize a new text d , we supply d to each language model, evaluate the likelihood of d under the model, and pick the winning category according to Equation (16). In this paper, two smoothing methods are employed, one is good-turing smoothing and the other is witten-bell smoothing. We use C
NG and C NW to represent the classifiers based on n-gram models with these two smoothing methods respectively.
Just as n-gram models can act as text classifiers, n-multigram models can be classifiers also. The frameworks of them are similar. During the training stage, we construct a separate
Table 1: Denotations of the four variations of C M n-multigram model for each category by training on the text from that category. Given a test document, we calculate its probability under each model, and assign the category under which the document has the largest probability to it. When calculating the probability under a model, we first find out the most likely segmentation of the test document using the viterbi algorithm shown in algorithm 2 and then compute the probability according to equation (9). C M is used to denote the classifier based on n-multigram models.
By intuition, the longer a sequence, the more specific its meaning and the more powerful its discriminative ability for text classification. So we put forward two variations of C
M to give bonus to longer sequences. We can give the bonus when doing segmentation by changing equation (11) to equation (17) which will affects the segmentation results and tends to produce longer sequences. We can also give bonus after the segmentation by changing equation (9) to (18) which just adds bonus to the calculated likelihood. In equation (17) and (18), b ( s ) is the bonus function of s .Two bonus functions are given in equation (19) and (20).  X  ( t )= max Now we prove the correctness that b 1 ( s ) will give bonus to longer sequences. The proof of b 2 ( s ) is similar. Given a se-quence s , the bonus for it is e | s | X  X  s | .If s is segmented into s
It is obvious that b 2 ( s )givesmuchmorebonustothe longer sequences than b 1 ( s ). Dependingonthetimewegive the bonus (during or after segmentation) and the function of the bonus, we get four variations of the classifier based on n-multigram, the denotations of the variations is summarized in Table 1.
In this section, we will put forward a classifier based on the combination of the n-multigram models and the n-gram models. The workflow of the proposed classifier is shown in Figure 1. From the above description, we can see that the n-multigram models can detect the frequent and usu-ally meaningful sequences in the text. So it is natural to use the sequences generated by n-multigram models to rep-resent the documents instead of the single words. Then a document changes from a sequence of words to a sequence of  X  X equences X . From the text represented by sequences, we can construct classifiers based on n-gram models. Af-ter that, we can apply the classifiers on the test documents which have been converted to sequences according to the corresponding n-multigram models.

Specifically, similar to n-gram and n-multigram based clas-sifiers, the classifier based on the combination of n-gram and n-multigram models detects the label of a test document as follows: where: We use c n  X  Multigram to denote the n-multigram model con-structed from the documents of category c and c n  X  gram to denote the n-gram model trained on the segmented results of the training documents according to c n  X  Multigram . d is the most likely segmentation of d according to c n  X  Multigram which is essentially a sequence of  X  X equences X . p ( d | c is the probability of d under c n  X  gram . For simplicity, we set n = 3 and employ the Witten-Bell smoothing approach when training the n-gram models on the training data rep-resented by sequences. We use C M + N to represent the clas-sifier based on the combination of n-multigram models and n-gram models.
 Figure 1: Framework of the classifier based on the combination of n-gram and n-multigram models.
Different categories have different usage of words and ex-pressions. For example, among the several different disci-plines in computer science,  X  X ata mining X , as a sequence, appears more frequently in  X  X rtificial Intelligence X  than in  X  X etwork X  while  X  X ata transition X  behaves oppositely. These sequences are more discrimi native when we perform text classification. We can label a document with the sequence  X  X ata transition X  as  X  X etwork X  with higher confidence than to label a document with the single word  X  X ata X  or  X  X ransi-tion X  X revenbothofthetwowordswhichappearindifferent parts of the document. However, it is hard to construct the vocabulary of the specific sequences for each category man-ually. So in this paper, we propose to use n-multigram mod-els to automatically generate such sequences and construct classifiers based the generated sequences.

N-gram models capture the specialty of natural language from another aspect, that is the occurrence of a word de-pends on its previous words. Traditionally, n-gram models are trained on single words. In this paper, we propose a novel classifier by applying the n-gram models on the se-quences produced through n-multigram models. This clas-sifier is supposed to perform better than the classifiers based on n-gram models and n-multigram models separately.
From the description of the training algorithm of the n-multigram models, it is easy to see that the time complexity for training classifiers based on n-multigram models is O(K M  X  T  X  n) where K is the number of iteration; M is the number of training documents; T is the average length of the training documents in terms of words and n is the maximum length of the generated sequences. The time complexity for training classifiers based on the n-gram models is similar to that for the n-multigram models based classifier except that n-gram models training do not need run K iterations. However, the back-off step to improve the accuracy of n-gram models takes extra time. The training time for the classifier based on the combination of the n-gram models and the n-multigram models is about the sum of the complexity for training classifiers based on each single model. The time complexity for testing under each kind of classifiers is similar to the training complexity. The space complexity of the three classifiers is at the same o rder in theory. However, in reality, the model size of C M is much smaller than that of C
N and the sizes are similar for C N and C M + N .
As shown in [21], the classifier based on n-gram models is one of the most promising classifiers for text classifica-tion which beats Support Vector Machine (SVM) on some corpora. Therefore, in this paper, we take n-gram as the baseline and compare it with our proposed classifiers in two aspects: (1) we compare it with the classifier based on n-multigram models; (2) we compare it with a classifier based on combination of n-gram models and n-multigram models.
Our experiments are conducted on RCV1, a new bench-mark collection for text classification research [14]. RCV1 is drawn from one of the online databases produced by Reuters which is the largest international text and television news agency. RCV1 was intended to consist of all and only En-glish language stories produced by Reuters journalists be-tween August 20, 1996, and August 19, 1997. The stories cover the range of content typical of a large English language international newswire. They vary from a few hundred to several thousand words in length. The number of documents contained in RCV1 is about 35 times more than that of the popular Reuters-21578 collection and its variants [12].
In this paper, in order to speed up our experiments, we select 10% documents from the top 10 categories which re-sults in 47042 documents. Table 2 shows the number of the sampled documents in the three largest categories and three smallest categories. In order to reduce the uncertainty of data split, a 3-fold cross validation procedure is applied in our experiments. That is, we randomly split the documents into 3 folds and pick up one fold as the test data and the other two folds as the training data each time.
 Table 2: The number of sampled documents in the three largest and smallest categories.

We employ the standard measures to evaluate the per-formance of text classification, i.e. precision, recall and F1-measure [26]. Precision (P) is the proportion of actual positive class members returned by the system among all predicted positive class members returned by the system. Recall (R) is the proportion of predicted positive members among all actual positive class members in the data. F1 is the harmonic average of precision and recall which is defined as F 1=2  X  P  X  R/ ( P + R ).

To evaluate the average performance across multiple cat-egories, there are two conventional methods: micro-average and macro-average. Micro-average gives equal weight to ev-ery document, while macro-average gives equal weight to every category, regardless of its frequency. Statistical signif-icance tests are useful in order to verify to which extent the claim of an improvement can be backed by the observations on the test set. For the experiments we report in this paper, we focused on a pair-difference t-test on an improvement of individual F1 scores for the different classes that have been evaluated in each experiment. By convention, we say that a difference between means at the 95% level is  X  X ignificant X , a difference at 99% level is  X  X ighly significant X  and a difference at 99.9% level is  X  X ery highly significant X .
Some preprocessing steps are applied. All the documents are converted into lower-case. Each document is tokenized with a stop-word remover and Porter stemming. To speed up the classification, a simple feature selection method, known as  X  X ocument frequency thresholding (DF) X  [29], is applied in our experiments, where we only select the top 60000 words.
Before coming to the details of the experiment, we provide an example to illustrate the output of n-multigram mod-els, which is helpful to obtain some straightforward ideas of the advantages of n-multigram models. Take the sentence  X  X resident Guntis Ulmanis arrived in France on a two-day official visit, whose goal is to draw attention to security is-sues in the Baltic region. X  as an example, after removing the stopwords and stemming, we apply n-multigram models to segment the sentence into sequences and the result looks like  X / presid gunti ulmani / arrive / franc / dai offici visit / goal / draw attent / secure issu / baltic / region / X . As we can see, the resulted sequences are meaningful, such as  X  X resident Guntis Ulmanis (presid gunti ulmani) X  and  X  X e-curity issues (secure issu) X . This case shows the ability of the n-multigram models to generate reasonable text repre-sentations.
Table 3 and Table 4 show the classification performance of different classifiers in terms of micro-F1 and macro-F1 respectively. The best result for each classifier is in bold. For the classifiers based on n-multigram models, besides the maximum length of sequences ( n ), there are three param-eters which need tuning. They are the occurrence threshold ( c 0 ), the probability threshold ( p 0 ) and the number of iteration ( K ). The parameters are set as follows: c 0 =5, p =10  X  7 and K = 10, for the results shown in Table 3 and Table 4. We will study the parameters in next section. Table 3: Classification performance of different clas-sifiers in terms of micro-F1 Table 4: Classification performance of different clas-sifiers in terms of macro-F1
From the above tables, we can see that C NG and C NW do not make much difference which means that the two smoothing approaches, good-turing smoothing and witten-bell smoothing, behave similarly when they are applied for text classification. The same observation is shown in [21].
The results obtained by C M and C MB 11 are similar when the length of the multigram is changed from 1 to 4, although C MB 11 gets some minor improvement in terms of micro-F1 in some cases and C M outperforms C MB 11 in terms of macro-F1 in some cases. The reason to explain this observa-tion is as follows: the categories with relatively more train-ing data tend to generate more sequences which may coin-cide with those generated in the categories with less training data. When we give bonus to longer sequences, some test documents at the boundary of a large category and a small category will have higher probability to be classified into the larger categories. Though this bias can give the test docu-ments at the boundary which are from the larger categories with right labels and improve the micro-F1 value but the wrong decision on the test documents which are from the relatively smaller categories will affect the F1 values of the smaller categories more obviously, which then reduces the macro-F1 value.

As shown in Table 3 and Table 4, the classifiers based on the n-multigram models achieve the similar or even better performance compared to the classifiers based on the n-gram models. This observation is encouraging since the model size for the former classifiers is much smaller than that of the latter ones.

From Table 3 and Table 4, we can see that C M + N ,the classifier based on the combination of n-gram models and n-multigram models outperforms both the classifiers based on n-gram models ( C NG and C NW ) and n-multigram models ( C M and C MB 11 ) respectively. C M + N improved the micro-F1 from 0.895 to 0.926 and the macro-F1 from 0.872 to 0.911 when compared to the best result obtained by classifiers based on a single model only. Using the t-test, we find that the difference between C M + N and other classifiers is  X  X ery highly significant X  at  X  =0 . 0003 .

The model size (in terms of the number of items) for C M (a n-multigram models based classifier) is much less than that of C NW (an n-gram models based classifier). The for-mer is only 30.0% of the latter when n equals to 2 and 19.4% when n equals to 3. The model size of C M + N is larger than that of C NW , but it is only increased by 15.6%. From Table 5, we can also see that for C M , the increase of the model size when we change n from 2 to 3 is much smaller than that when we change n from 1 to 2. Actually, when we in-crease n to some larger values, the size of the model does not change obviously. The reason may be that two-word and three-word sequences are common in English but the sequences with four or more words are rare. However, when we increase the parameter n of C NW , the model size will be increased dramatically.
For the classifiers based on n-multigram models, four pa-rameters need tuning. They are the maximum length of sequences ( n ), the occurrence threshold ( c 0 ), the probabil-ity threshold ( p 0 ) and the number of iteration ( K ). For the variations of the classifier based on n-multigram models, we also need to test the different kinds of bonus functions.
Table 6 and Table 7 show the classification performance of C M and its variations when c 0 variesfrom1to15in terms of micro-F1 and macro-F1 respectively. The values for p 0 and K are fixed: p 0 =10  X  7 , K = 7. The best results for each case are in boldface. From the tables, we can see that nearly all the classifiers achieve their peak performance when c 0 equals to 5. The reason lies in the fact that when c 0 is too small, too many sequences which may be noise are kept. However when c 0 is too large, some meaningful sequences will be removed. In both cases, the classification performance will be reduced.

Table 8 and 9 show the classification performance of C M and its variations when p 0 varies from 10  X  5 to 10  X  9 in terms of micro-F1 and macro-F1. The values of c 0 and K are fixed: c =5, K =7. p 0 has two roles. One is to filter sequences whose probabilities fall below p 0 , which can reduce the im-pact of noises. The other role is to guess the probability of unknown words at test stage. From the tables, we can see Table 6: Classification performance of C M and its variations in terms of micro-F1 when c 0 varies Table 7: Classification performance of C M and its variations in terms of macro-F1 when c 0 varies that for almost all the classifiers, they reach their best per-formance when p 0 equals to 10  X  7 .Infact,when p 0 varies from 10  X  7 to 10  X  8 , the performance does not change much. Such a wide range makes it easy to tune the parameter in real applications.

From Table 6 to Table 9, we can see that for the vari-ation of C M , the time when we give the bonus (during or after segmentation) does not make much difference. But the bonus function impacts the classification results obvi-ously. Compared to the function shown in equation (19), the function shown in equation (20) reduces the performance in most cases. This observation indicates that when we give Table 8: Classification performance of C M and its variations in terms of micro-F1 when p 0 varies (the log value of p 0 is shown) too much bonus to longer sequences, the negative effect of some noise sequences will affect the classification result se-riously which makes the performance worse. Another con-clusion which can be drawn from the tables is that the four variations do not make much contribution to the classifi-cation performance. Although some of them can improve the micro-F1 value in some cases, they tend to reduce the macro-F1 at the same time. The reason for this observation is given in Section 5.5.

Table 10 shows the micro-F1 and macro-F1 values ob-tained by C M when the number of iterations changes from 1 to 9. The values of other parameters are fixed as: c 0 =5; p =10  X  7 ; n = 4. It is easy to see that the performance of C
M converges quickly. This property makes C M applicable in real applications.
In this paper, we proposed an approach of document rep-resentation based on the automatically extracted sequences generated through n-multigram models. Two text classi-fiers are put forward on the basis of the proposed document presentation. One is implemented by applying n-multigram models directly which can classify the documents while gen-erating the sequences based representation at the same time. Another one is realized though the combination of n-multigram models and n-gram models. This algorithm works in three stages. In the first stage, n-multigram models are trained on the training documents for each category and the train-ing documents are segmented according to the n-multigram models into sequences. In the second stage text classifiers Table 9: Classification performance of C M and its variations in terms of macro-F1 when p 0 varies (the log value of p 0 is shown) Table 10: Performance of C M when the number of iteration ( K ) changes basedonn-grammodelsaretrainedonthesequencespro-duced in the first stage. The third stage is to classify the test documents. For each test document, the pair of mod-els (an n-multigram model and an n-gram model) from each category will be applied on the test document in turn. The document is segmented into sequences firstly according to an n-multigram model from a certain category and its probabil-ity of being generated by this category is calculated accord-ing to the corresponding n-gram model. The test document is assigned to the category which has the largest probability. We conducted a series of experiments on a subset of RCV1. The experiments show that our proposed text classification algorithms work well. Although the model size of our pro-posed algorithm based on the n-multigram models directly is much smaller than that of the n-gram models based clas-sifier, it can achieve similar or even better classification per-formance. The results also show that the proposed algo-rithm based on the combination of n-multigram models and n-gram models improves the micro-F1 and macro-F1 values from 89.5% to 92.6% and 87.2% to 91.1% respectively when compared with the classifiers based on n-gram models. The experiments are conducted through 3-fold cross validation and the t-test shows that the improvement is very highly significant to  X  =0 . 0003.
In our future work, we will test some other classification algorithms such as SVM and KNN based on our proposed document representation approaches to verify its validity. It is also necessary to conduct experiments on some other datasets to verify its adaptability. Dou Shen and Qiang Yang are supported by a grant from NEC (NECLC05/06.EG01). The authors would like to thank Prof. Mak Brian, Hui Zhao and the anonymous reviewers for their valuable comments and suggestions. [1] J. Bai and J.-Y. Nie. Using language models text [2] S.BloehdornandA.Hotho.Boostingfortext [3] L. Cai and T. Hofmann. Text categorization by [4] M. Carpuat and D. Wu. Word sense disambiguation [5] S. Chen and J. Goodman. An empirical study of [6] S. Deligne and F. Bimbot. Language modeling by [7] A. Dempster, N. Laird, and D. Rubin. Maximum [8] A. Gliozzo, C. Giuliano, and C. Strapparava. Domain [9] T. Joachims. Text categorization with suport vector [10] S. Katz. Estimation of probabilities from sparse data [11] A. Kehagias, V. Petridis, V. G. Kaburlasos, and [12] D. D. Lewis. Reuters-21578 text categorization test [13] D. D. Lewis. Representation quality in text [14] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [15] C. Manning and H. Sch  X  utze. Foundations of Statistical [16] A. McCallum and K. Nigam. A comparison of event [17] G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and [18] M.Yetisgen-Yildiz and W. Pratt. The effect of feature [19] U. Ney, H. Essen and R. Kneser. On structuring [20] L. Page, S. Brin, R. Motwani, and T. Winograd. The [21] F. Peng, D. Schuurmans, and S. Wang. Augmenting [22] G. Ramakrishnan and P. Bhattacharyya. Text [23] G. Salton. Automatic text processing: the [24] R. E. Schapire and Y. Singer. Boostexter: A [25] S. Slattery and M. Craven. Combining statistical and [26] R. C. van. Information Retrieval . Butterworths, [27] I. Witten and T. Bell. The zero-frequency problem: [28] Y. Yang. An evaluation of statistical approaches to [29] Y. Yang and J. O. Pedersen. A comparative study on
