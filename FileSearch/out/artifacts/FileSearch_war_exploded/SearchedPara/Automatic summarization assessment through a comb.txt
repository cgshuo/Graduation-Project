 1. Introduction Cho, 2012; Fan, 2010; Hedge, 2001; Pakzadian &amp; Rasekh, 2013 ).
  X  assessment determines how much the information of the source text overlaps with the information in the summary text, the content evaluation.
 number of co-occurring words. However, a short text may only contain a few words co-occurrence or even none. This is assessment of summaries, a method should consider both semantic and syntactic information in evaluation ( Kanejiya,
In this paper, we propose a method that merges semantic relations between words, and their syntactic composition for improving the accuracy of automatic summary evaluation. The semantic similarity is computed using information from a lexical database. The use of a lexical database enables our method to model human common sense knowledge.
The proposed method is called SALK: summarization assessment based on linguistic knowledge. SALK can be applied to Seo, 2004 ) and education ( Foltz, Laham, &amp; Landauer, 1999; Franzke &amp; Streeter, 2006 ). rize the works discussed and the progress of the project. 2. Related work summaries. In this section detailed information related to assessment approaches of the summaries are given. the matrix created is usually sparse.
 written summary, and allows students to produce a summary text with their own words. The summaries are evaluated based on certain features, such as cohesion, coherence, use of language and the adequacy of the summary. evaluate the content of the summary text. Summary Street ranks a student X  X  written summary by comparing the summary length, redundancy and plagiarism.
 Lin (2004) proposed an automatic summary assessment system named Recall-Oriented Understudy for Gisting ment approaches, such as ROUGE-N, ROUGE-L, and ROUGE-S. ROUGE-N is an n-gram recall between a candidate summary and a reference summary. ROUGE-L calculates the similarity between a reference summary and a candidate summary based reference summary.

Mohler, Bunescu, and Mihalcea (2011) introduced an Answer Grading System, which combines a graph alignment model similarity scores are used to weight the edges that connect the nodes in A larity measures.
 among the concepts. Based on the subsumption score between the T -graph and H -graph, the method can make a decision concerning whether the meaning of text T can be derived from text H .
 column corresponds to the document. Prevtag indicates the POS tag of the preceding word and the document can be sen-tence, paragraph or a larger unit of text.

Wiemer-Hastings and Zipitria (2001) proposed a model called the tagged model or tagged LSA, which adds syntactic could improve the performance in terms of sentence-pair similarity judgment and the structural information. 3. Proposed method: SALK
The proposed method contains the following steps: iii. Post-processing. This aims to calculate the final similarity score using Eqs. (6) and (7) .
The tasks of each stage are as follows: 3.1. Pre-processing further processing. The pre-processing module provides text pre-processing functions, such as sentence segmentation, tokenization and stop word removal. 3.2. Intermediate-processing tasks of each component are as follows: 3.2.1. The word set Given two sentences S 1 and S 2 , a  X  X  X ord Set X  X  is created using distinct words from the pair of sentences. Let second step to continue the loop by the next word from S 1 process is shown in algorithm 1.
 Algorithm 1. The creation of  X  X  X ord set X  X  Input: Sentence 1, Sentence 2;
Output: WS ={ W 1 , W 2 , ... , W n }, WS denotes an array that includes all distinct words from two sentences; 1: Let W be a word of the Sentence 1 or Sentence 2; 2: Let RW be the root of word W , it is obtained using Word Net; 3: Let L be the length of Sentence 1 or Sentence 2; 4: Set l =0; 5: For each W , 6: Jump to step 5; iterate until l 6 L ; 3.2.2. Semantic similarity between words a word order vector and semantic vector. Given two words W through these steps: III. Determine the number of synonyms of each word; IV. Determine Least Common Subsume (LCS) of two words and their length; Mihalcea, Corley, &amp; Strapparava, 2006; Warin, 2004 ): synonyms of word w, and IC  X  w  X  is the information content of word w based on the lexical database Word Net. 3.2.3. Semantic similarity between sentences two semantic vectors. The following equation is used to calculate the semantic similarity between sentences: where S 1  X  w 11 ; w 12 ; ... ; w 1 m  X  X  and S 2  X  w 21 ; w is the weight of the j th word in vector S p ; m is the number of words.
 Algorithm 2. Lexical semantic vector Input: sentence 1, sentence 2,  X  X  X ord set X  X ;
Output: semantic vector; 1: Let S be either sentence 1 or sentence 2; 2: Let W t be a word of the word set; 3: Let RW be the root of the word W t , it is obtained using the Word Net; 4: Let W be a word of S ; 5: Let SSM denotes the semantic similarity measure between words; 6: Let L be the length of S ; 7: Set l =0; 8: For each W t , 3.2.4. Word order similarity between sentences low these steps, for each word w from the word set. If the W appears in the sentence S to the index position of the corresponding word in the sentence S
S , a semantic similarity measure is calculated between word W and each word from the sentence S
Finally, the value of the cell is set to the index position of the word from the sentence S
If there is not a similar value between the word W and all of the words in the sentence S ing equation is used to calculate word order similarity between sentences: where O 1  X  d 11 ; d 12 ; ... ; d 1 m  X  X  and O 2  X  d 21 ; d the weight of the j th cell in vector O p .
 Algorithm 3. Word order vector Input: sentence 1, sentence 2,  X  X  X ord set X  X ;
Output: Lexical vector; 1: Let S be either sentence 1 or sentence 2; 2: Let W t be a word of the word set; 3: Let RW be the root of the word W t , it is obtained using the Word Net; 4: Let W be a word of S ; 5: Let SSM denotes the semantic similarity measure; 6: Let L be the length of S ; 7: Set l =0; 8: For each W t , 3.2.5. Sentence similarity measurement order similarity. The similarity measure is computed as follows: tic and syntactic similarity measures are assumed to be equally important. 3.3. Post-processing following equations to calculate the Final Score (FS) for any student X  X  written summary: the summary text. MSS is the Maximum Similarity Score between a sentence from the summary text and all the sentences 4. System implementation An automatic summarization assessment system based on our method, which contains six main modules, is displayed in
Fig. 3 . Source text interface  X  allows the user to upload the source text for summary exercise. Summary practice interface  X  allows the user to exercise their summary writing ability using the source text.
Grading  X  aims to apply the proposed method to the student summary to determine the similarity measure between the summary text and the original text.
 of important sentences.
 Important sentences  X  the aim of this module is to determine important sentences from the original text. sentences from the source text have been selected to create a summary text.

To give appropriate feedback to students about the missing information in each student summary, the module of impor-their summaries and resubmit them to the system.
 The module of important sentences consists of a few processes: tive. The results of this function are sent to the keyword method and title method. work, five words with the most frequency are selected as keywords.
 text and determines whether it is the first or the last sentence of a paragraph.
 Title method ( Kupiec et al., 1995 )  X  extracts all the nouns and verbs from the title of the original text. sentences that contain one or more of these cue phrases are considered more important than sentences without cue phrases ( Zhang, Sun, &amp; Zhou, 2005 ).
 Sentence selection  X  chooses N sentences with a high score ( N is a predefined value). matched sentence in summary text, the system provides feedback to the student.
 words) and 173,941 senses of words.
 system, the results will be displayed to the user using the result interface component, as shown in Fig. 5 . 5. Experiments ment techniques, such as LSA, N-gram, BLEU, LSA_Ngram, LSA_ERB and ROUGE (N-gram, LCS, Skip bigram), which are employed by most of the important summary evaluation systems. To do this, we now explain our experiments on the single-document summarization datasets provided by Document Understanding Conference ( http://duc.nist.gov ). 5.1. Data set method we used the document datasets DUC 2002 and corresponding 100-word summaries generated for each of docu-ments. DUC 2002 contains 567 documents-summary pairs from Document Understanding Conference. It is worth mention-candidate summary. In our experiments, the documents and corresponding summaries were randomly divided into two separate dataset. In the first experiment, 300 documents and corresponding summaries were used for parameter tuning (the threshold and lambda). In the second experiment, the performance of the proposed method was compared with the tion of the datasets.
 ing method; and (3) two of them were doctorate candidates with good reading skills and understanding ability in the English language participated in the current study.

Human expert assessment  X  the human experts were asked to score the candidate summaries on a scale of 0 X 1 with respect to how much the information in the original text overlapped with the information in the student summary.
Each of the two assessors independently produced scores for every summary. Finally, each of the candidate summaries was assigned the average of the scores that the human experts had assigned to each student summary. average correlation coefficient between the human raters was 0.61. This value indicated that our assessors had good agreement ( Landis &amp; Koch, 1977 ) for grading each candidate summary.
 reference similar (denoted by Ref sim ), reference dissimilar (denoted by Ref summaries. The human expert would break every candidate summary into a number of sentences and then compare each a common basis for comparing Ref sim and candidate summary, if the candidate summary used different words from the converted to a common word. In the synonym test, unlike the similar test, for comparing Ref if the candidate summary used different words from the Ref 5.2. Assessment techniques
In order to make inferences about our proposed method, we compare the SALK with the other existing base assessment steps we present each of these assessment techniques.
 sures the similarity between two texts based on the number of n-grams co-occurring in two compared texts. The N-gram match ratio is calculated as follows: where N is used for the length of the N-gram and Count match erence summary and a candidate summary. Count (N-gram) is the number of N-grams in the reference summaries. The N-gram match ratio (Eq. (9) ) is used to compare all the sentences in the reference summary and candidate summaries as follows: where m P n , n and m range from 1 to 4, W n  X  1 =  X  m n  X  1  X  .

ROUGE metric is also used to evaluate summaries. It contains ROUGE-N, ROUGE-L and ROUGE-S that compute the simi-larity between two texts. We describe these methods as follows.
 ROUGE-N, compares two summaries based on total number of matches. It is calculated as follows: where N is used for the length of the N-gram and Count match erence summary and a candidate summary. Count (N-gram) is the number of N-grams in the reference summaries. ROUGE-L calculates the similarity between a reference summary and a candidate summary based on the Longest Common Subsequence (LCS). It measures the similarity between two summaries using Eq. (12) . the candidate summary. LCS  X  R ; S  X  is the length of an LCS of the reference summary and the candidate summary. P putes the precision of LCS  X  R ; S  X  ; R lcs  X  R ; S  X  computes the recall of LCS  X  R ; S  X  and b  X  P gaps. ROUGE-S measures the similarity between two summaries based on common skip-bigrams using Eq. (13) . maries and candidate summaries. P skip 2  X  R ; S  X  computes the precision of SKIP 2  X  R ; S  X  ; R out the cell values. These approaches are as follows: Term frequency : the cell is filled out with the frequency of the word in the sentence. Binary Value : the cell is filled out with 0/1 according to the existence of the word in the sentence. using Eq. (14) : where j N j is the total number of sentences in the input text, and n angular matrix X m n can break down into the product of three matrices: an orthogonal matrix U which is called the right singular vector. The equation for singular value decomposition of X is as follows:
Finally, in order to reduce the number of dimensions, a few elements of matrix K
K 1 1 . A new matrix, X 0 m 1 , is created by multiplying three matrixes. X
After generating the compressed matrix for the reference summary and candidate summary, a vector for each sentence erence vector and the candidate vector can then be calculated as an indication of their semantic similarity. larity between two texts.
 words in a candidate summary that matches words in a reference summary divided by the total number of words in the candidate summary. BLEU X  X  n-gram precision is defined as: erence summary and a candidate summary, and Count (N-gram) is the number of N-grams in the candidate summary. their score.

He et al. (2009) proposed a summary assessment system based on the LSA method and N-gram co-occurrence with the aim of assessing students X  written summaries. However, a score is assigned to student summary using Eq. (20) . student X  X  essay.
 the document vector obtained from the student summary and R  X  ~ r sponding to the references; a is equal to the 0.5. 5.3. Performance analysis We conduct our analysis and assess the SALK based on the single-document summarization datasets provided by
Document Understanding Conference. The performance of the SALK is compared with other evaluation techniques. We car-mary and reference similar are related.
 summary and reference dissimilar are unrelated.

Synonym test  X  to determine the ability of the proposed method to assess the candidate summary based on its content and the different synonym terms in the summary must not have an impact on the performance. the score that has been given by a human expert.
 Evaluation measure ing test.
 B is calculated by averaging the maximum similarity measures between sentences.
 Similarity test
The similarity score between a candidate summary and Ref sim between the number of candidate summaries for which the similarity scores exceed the threshold and the total number of candidate summaries. The following equation is used to calculate the accuracy rate: tactic information. Both parameters in the current experiment were found using 300 documents and corresponding summaries. The documents and summaries used for this experiment were taken from DUC 2002. various tests: similarity test  X  AR sim  X  , dissimilarity test  X  AR using Eq. (26) .
 The best values of Table 2 have been marked in boldface.
 0.9 as the value. Therefore, we can recommend this threshold and lambda for use on the rest of the data set.
LCS, Skip bigram). We apply these methods to the 267 previously unused documents and corresponding summaries using accuracy of (82.86%) in comparison with the best existing method, LSA ERB, which has an accuracy of (73.50%). Detailed comparison
From the comparison of the accuracy values for other methods, SALK obtains a considerable improvement. Table 4 dis-the proposed method improves the existing methods. We see that among the existing methods the LSA ERB displays the method LSA ERB, SALK improves the performance of the LSA ERB method as follows: 9.73% (Similar test), 7.51% (Dissimilar test), 18.23% (synonym test) and 17.20% (Grading test) in terms of accuracy. two hypotheses, a) null hypothesis (denoted H 0 ), our method is not able to obtain high accuracy and improve the performance compared with the current methods, and b) the alternative hypothesis (denoted H able to obtain high accuracy and improve the performance compared with the current methods. outperforms the other methods.
 method obtained better accuracy.
 Discussion  X  from Tables 3 and 4 we make the following main observations. Our method outperforms all other methods. summaries ( Deerwester et al., 1990; P X rez et al., 2005 ). (b) Given two sentences (i.e., S 1 : John helps Ravi; S 2 2012; Lin, 2004 ), SALK is able to give credit to S 1 ,if S other alternative and shorter substrings for similarity measuring ( Alguliev et al., 2011; Lin, 2004 ). number of co-occurring words). The proposed method improves this limitation. posed method compares two texts on the sentence level based on the words in compared sentences. LSA with high dimen-text similarity ( Kanejiya et al., 2003; P X rez et al., 2005; Wiemer-Hastings &amp; Zipitria, 2001 ). 6. Conclusion and future work
We believe that automatic summarization assessment has become an important part of the text summarization. Since summary writing assessment is an arduous and tedious task, we proposed a method (called SALK), which merges semantic strong evidence that SALK outperformed other methods. As we already saw, SALK was able to obtain an accuracy of mented SALK into an automatic summarization assessment system to grade student written summaries in the English language.

Further, the common way to assess the content of the summaries is to compare them with a reference summary, which text as input to assess the summary.
 formance of our proposed method. One solution is that, in addition to WordNet, other knowledge resources, such as Wikipedia and other large corpus should be used.
 References
