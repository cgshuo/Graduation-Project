 Sanjeev Arora arora@cs.princeton.edu Rong Ge rongge@cs.princeton.edu Yoni Halpern halpern@cs.nyu.edu David Mimno mimno@cs.princeton.edu Ankur Moitra moitra@ias.edu David Sontag dsontag@cs.nyu.edu Yichen Wu yichenwu@princeton.edu Michael Zhu mhzhu@princeton.edu Topic modeling is a popular method that learns thematic structure from large document collections without human supervision. The model is simple: documents are mixtures of topics, which are modeled as distributions over a vocabulary (Blei, 2012). Each word token is generated by selecting a topic from a document-specific distribution, and then selecting a specific word from that topic-specific distribution. Posterior inference over document-topic and topic-word distributions is intractable  X  in the worst case it is NP-hard even for just two topics (Arora et al., 2012b). As a result, researchers have used approx-imate inference techniques such as singular value decomposition (Deerwester et al., 1990), variational inference (Blei et al., 2003), and MCMC (Griffiths &amp; Steyvers, 2004).
 Recent work in theoretical computer science focuses on designing provably polynomial-time algorithms for topic modeling. These treat the topic modeling problem as one of statistical recovery : assuming the data was generated perfectly from the hypothesized model using an unknown set of parameter values, the goal is to recover the model parameters in polynomial time given a reasonable number of samples.
 Arora et al. (2012b) present an algorithm that prov-ably recovers the parameters of topic models provided that every topic contains at least one anchor word that has non-zero probability only in that topic. If a docu-ment contains this anchor word, then it is guaranteed that the corresponding topic is among the set of top-ics used to generate the document. The input for the algorithm is the second-order moment matrix of word-word co-occurrences. The algorithm proceeds in two steps: a selection step that finds anchor words for each topic, and a recovery step that reconstructs topic dis-tributions given those anchor words.
 Anandkumar et al. (2012) present a provable algo-rithm based on third-order moments that does not re-quire separability, but, unlike the algorithm of Arora et al., assumes that topics are not correlated. Al-though standard topic models like LDA (Blei et al., 2003) assume that topic proportions in a document are uncorrelated, there is strong evidence that topics are dependent (Blei &amp; Lafferty, 2007; Li &amp; McCallum, 2007): economics and politics are more likely to co-occur than economics and cooking.
 Both algorithms run in polynomial time, but the bounds that have been proven on their sample complexity are weak and their empirical runtime performance is slow. It is also unclear how they perform if the data does not satisfy the modeling assumptions. Subsequent work improves efficiency: Bittorf et al. (2012) and Gillis (2012) reduce the number of linear programs to be solved in the anchor word selection step. Gillis &amp; Vavasis (2012) use linear projection ideas to avoid linear programs entirely. The contributions of the current paper are three-fold. First, we present a combinatorial anchor selection al-gorithm that does not require solving linear programs. So long as the separability assumption holds, we prove that this algorithm is stable in the presence of noise and thus has polynomial sample complexity, running in seconds on very large data sets. As the anchor se-lection step is not a bottleneck, we do not empirically compare similar algorithms that have appeared in independent work by other authors (though not with a similar proof of correctness, see Section 4 for details). Second, we focus on the recovery step of Arora et al. (2012b), which computes topic-word distributions us-ing matrix inversions. This step, which does noticeably affect performance, is sensitive to noise (whether from sampling or from model lack-of-fit) and so has high sample complexity and poor results even on synthetic data. We present a simple probabilistic interpretation of the recovery step that replaces matrix inversion with gradient-based inference. Third, we present an empirical comparison between recovery-based algo-rithms and existing likelihood-based algorithms with respect to empirical sample complexity on synthetic distributions and performance of the algorithms on real-world document corpora. Our algorithm performs as well as collapsed Gibbs sampling on a variety of metrics, and runs at least an order of magnitude faster, and as much as fifty times faster on large datasets, allowing real-time analysis of large data streams. Our algorithm inherits the provable guarantees of Arora et al. (2012a;b) and results in simple, practical implementations. We view this work as combining the best of two approaches to machine learning: the tractability of statistical recovery with the robustness of maximum likelihood estimation. We consider the learning problem for a class of admix-ture distributions that are frequently used for proba-bilistic topic models. Examples of such distributions include latent Dirichlet allocation (Blei et al., 2003), correlated topic models (Blei &amp; Lafferty, 2007), and Pachinko allocation (Li &amp; McCallum, 2007). We de-note the number of words in the vocabulary by V and the number of topics by K . Associated with each topic k is a multinomial distribution over the words in the vocabulary, which we will denote as the column vector A k of length V . Each of these topic models postulates a particular prior distribution  X  over the topic distri-bution of a document. For example, in latent Dirichlet allocation (LDA)  X  is a Dirichlet distribution, and for the correlated topic model  X  is a logistic Normal distribution. The generative process for a document d begins by drawing the document X  X  topic distribution W d  X   X  . Then, for each position i we sample a topic assignment z i  X  W d , and finally a word w i  X  A z i . We can combine the column vectors A k for each of the K topics to obtain the word-topic matrix A of dimen-sion V  X  K . We can similarly combine the column vectors W d for M documents to obtain the topic-document matrix W of dimension K  X  M . We empha-size that W is unknown and stochastically generated: we can never expect to be able to recover it. The learning task that we consider is to find the word-topic matrix A . For the case when  X  is Dirichlet (LDA), we also show how to learn hyperparameters of  X  .
 Maximum likelihood estimation of the word-topic distributions is NP-hard even for two topics (Arora et al., 2012b), and as a result researchers typically use approximate methods. The most popular ap-proaches are variational expectation maximization (Blei et al., 2003), which optimizes a lower bound on the likelihood, and Markov chain Monte Carlo (McCallum, 2002), which asymptotically samples from the posterior distribution.
 Arora et al. (2012b) present an algorithm that prov-ably learns the parameters of a topic model given sam-ples from the model, provided that the word-topic dis-tributions are separable (Donoho &amp; Stodden, 2003): Definition 2.1. The word-topic matrix A is p -separable for p &gt; 0 if for each topic k , there is some word i such that A i,k  X  p and A i,k 0 = 0 for k 0 6 = k . Such a word is called an anchor word : when it occurs in a document, it is a perfect indicator that the document is at least partially about the corresponding topic, since there is no other topic that could have generated the word. Suppose that each document is of length D  X  2, and let R = E  X  [ WW T ] be the K  X  K topic-topic covariance matrix. Let  X  k be the expected proportion of topic k in a document generated accord-ing to  X  . The main result of Arora et al. (2012b) is: Theorem 2.2. There is a polynomial time algorithm that learns the parameters of a topic model if the number of documents is at least
M = max O Algorithm 1. High Level Algorithm Input: Textual corpus D , Number of topics K , Tolerance parameters a , b &gt; 0.
 Output: Word-topic matrix A , topic-topic matrix R Q  X  Word Co-occurences( D ) Form {  X  Q 1 ,  X  Q 2 ,...  X  Q V } , the normalized rows of Q .
S  X  FastAnchorWords( {  X  Q 1 ,  X  Q 2 ,...  X  Q V } , K , (Algorithm 4)
A,R  X  RecoverKL( Q, S , b ) (Algorithm 3) return A,R where p is defined above,  X  is the condition number of R , and a = max k,k 0  X  k / X  k 0 . The algorithm learns the word-topic matrix A and the topic-topic covariance matrix R up to additive error .
 The Arora et al. (2012b) algorithm is not practical X  X t must solve V linear programs to identify anchor words, which is inefficient, and uses matrix inversion to recover A and the parameters of  X  , which is unstable and sensitive to noise X  X ut it forms the basis of our improved method, Algorithm 1. Both anchor selection and recovery take as input the V  X  V matrix of word-word co-occurrence counts Q , whose con-struction is described in the supplementary material. Q is normalized so that the sum of all entries is 1. We introduce a new recovery method based on a prob-abilistic framework. The original recovery procedure (which we call  X  X ecover X ) from Arora et al. (2012b) is as follows. First, it permutes the Q matrix so that the first K rows and columns correspond to the anchor words. We will use the notation Q S to refer to the first K rows, and Q S , S for the first K rows and just the first K columns. If constructed from infinitely many documents, Q would be the second-order moment matrix Q = E [ AWW T A T ] = A E [ WW T ] A T = ARA T , with the following block structure: Q = ARA T = where D is a diagonal matrix of size K  X  K whose rows correspond to anchor words. Next, it solves for A and R using the algebraic manipulations outlined in Algorithm 2.
 For finite data, the matrix inversion in Algorithm 2 results in substantial imprecision. The returned A and R matrices can even contain small negative values, re-quiring a subsequent projection onto the simplex. As shown in Section 5, the original Recover algorithm per-forms poorly relative to a likelihood-based algorithm. Algorithm 2. Original Recover (Arora et al., 2012b) Input: Matrix Q , Set of anchor words S Output: Matrices A , R Permute rows and columns of Q Compute ~p S = Q S ~ 1 (equals DR ~ 1)
Solve for ~z : Q S , S ~z = ~p S (Diag( ~z ) equals D  X  1 Solve for A T = ( Q S , S Diag( ~z ))  X  1 Q T S
Solve for R = Diag( ~z ) Q S , S Diag( ~z ) return A,R Algorithm 3. RecoverKL Input: Matrix Q , Set of anchor words S , tolerance parameter .
 Output: Matrices A , R Normalize the rows of Q to form  X  Q .

Store the normalization constants ~p w = Q ~ 1. for i = 1 ,...,V do end for A 0 = diag( ~p w ) C Normalize the columns of A 0 to form A .

R = A  X  QA  X  T return A,R One problem is that Recover uses only K rows of the matrix Q (the rows for the anchor words), whereas Q is of dimension V  X  V . Although ignoring the remaining V  X  K rows is sufficient for theoretical analysis, where the remaining rows contain no additional information, it is not sufficient for real data. Small sample sizes may also make estimates of co-occurrences between a word and the anchors inaccurate.
 Here we adopt a new approach based on Bayes X  rule. Consider any two words in a document w 1 and w 2 , and let z 1 and z 2 refer to their topic assignments. We will use A i,k to index the matrix of word-topic distribu-tions, i.e. A i,k = p ( w 1 = i | z 1 = k ) = p ( w 2 = i | z Given infinite data, the elements of the Q matrix can be interpreted as Q i,j = p ( w 1 = i,w 2 = j ). The row-normalized Q matrix, denoted  X  Q , which plays a role in both finding the anchor words and the recovery step, can be interpreted as a conditional probability  X  Q i,j = p ( w 2 = j | w 1 = i ).
 Denoting the indices of the anchor words as S = { s 1 ,s 2 ,...,s K } , the rows indexed by elements of S are special in that every other row of  X  Q lies in the convex hull of the rows indexed by the anchor words. To see this, first note that for an anchor word s k ,  X  where (1) uses the fact that in an admixture model w  X  w 1 | z 1 , and (2) is because p ( z 1 = k | w 1 = s k ) = 1. For any other word i , we have Denoting the probability p ( z 1 = k | w 1 = i ) as C i,k we have  X  Q i,j = P k C i,k  X  Q s and P k C i,k = 1, we have that any row of  X  Q lies in the convex hull of the rows corresponding to the anchor words. The mixing weights give us p ( z 1 | w 1 = i ). Using this together with p ( w 1 = i ), we can recover the A matrix simply by using Bayes X  rule: p ( w 1 = i | z 1 = k ) = Finally, we observe that p ( w 1 = i ) is easy to solve for since P j Q i,j = P j p ( w 1 = i,w 2 = j ) = p ( w 1 = i ). Our new algorithm finds, for each row of the empirical row normalized co-occurrence matrix,  X  Q i , the vector of non-negative coefficients p ( z 1 | w 1 = i ) that best reconstruct it as a convex combination of the rows that correspond to anchor words. Here  X  X est X  will be measured using an objective function that allows this step to be solved quickly and in parallel (inde-pendently) for each word using the exponentiated gradient algorithm. Once we have p ( z 1 | w 1 ), we recover the A matrix using Bayes X  rule. The full algorithm using KL divergence as an objective is found in Algo-rithm 3. Further details of the exponentiated gradient algorithm are given in the supplementary material. We use KL divergence as the measure of reconstruc-tion error because it lets the recovery procedure be understood as maximum likelihood estimation. In particular, the recovery procedure can be shown to find the parameters p ( w 1 ), p ( z 1 | w 1 ), p ( w 2 maximize the likelihood of the word co-occurence counts ( not the documents). However, the opti-mization problem does not explicitly constrain the parameters to correspond to an admixture model. We define a similar algorithm using quadratic loss, which we call RecoverL2. Remarkably, the running time of this algorithm can be made independent of V . The objective can be re-written in kernelized form as ||  X 
Q i  X  C T i  X  Q S || 2 = ||  X  Q i || 2  X  2 C i (  X  Q S where  X  Q S  X  Q T S is K  X  K and can be computed once and used for all words, and  X  Q S  X  Q T i is K  X  1 and can be computed once prior to running the exponentiated gradient algorithm for word i .
 To recover the R matrix for an admixture model, recall that Q = ARA T . We can find a least-squares approximation to R by pre-and post-multiplying Q by the pseudo-inverse A  X  . For the special case of LDA we can learn the Dirichlet hyperparameters. Recall that in applying Bayes X  rule we calculated for p ( z ) specify the Dirichlet hyperparameters up to a constant scaling. This constant could be recovered from the R matrix (Arora et al., 2012b), but in practice we find it is better to choose it using a grid search to maximize the likelihood of the training data. We will see in Section 5 that our non-negative recov-ery algorithm performs much better on a wide range of performance metrics than the original Recover. In the supplementary material we show that the non-negative recovery algorithm also inherits the theoretical guarantees of Arora et al. (2012b): given polynomially many documents, it returns an estimate  X  A at most from the true word-topic matrix A . We next consider the anchor selection step of the al-gorithm, in which our goal is to find the anchor words. In the infinite data case where we have infinitely many documents, the convex hull of the rows in  X  will be a simplex where the vertices of this simplex correspond to the anchor words. Given a set of V points a 1 ,a 2 ,...a V whose convex hull P is a simplex, we wish to find the vertices of P . 1 When we have a finite number of documents, the rows of  X  Q are only an approximation to their expectation: we are given a set of points d 1 ,d 2 ,...d V that are each a perturbation of a 1 ,a 2 ,...a V whose convex hull P defines a simplex. We would like to find an approximation to the vertices of P . The anchor word selection algorithm from Arora et al. (2012a) tests whether or not each of the V points is a vertex of the convex hull by solving a linear program for each word. In this section we describe a purely combinatorial algorithm for this task that avoids linear programming altogether. The new  X  X astAnchorWords X  algorithm is given in Algorithm 4. Our approach is to iteratively find the farthest point from the subspace spanned by the anchor words found so far. The main technical step is to show that if Algorithm 4. FastAnchorWords Input: V points { d 1 ,d 2 ,...d V } in V dimensions, al-most in a simplex with K vertices and &gt; 0 Output: K points that are close to the vertices of the simplex.

Project the points d i to a randomly chosen 4 log V/ 2 dimensional subspace
S  X  X  d i } s.t. d i is the farthest point from the origin. for i = 1 TO K  X  1 do largest distance to span( S ).
 end for
S = { v 0 1 ,v 0 2 ,...v 0 K } . for i = 1 TO K do to span( { v 0 1 ,v 0 2 ,...,v 0 K }\{ v 0 i } ) end for Return { v 0 1 ,v 0 2 ,...,v 0 K } .
 one has already found r points S that are close to r (distinct) anchor words, then the point that is farthest from span( S ) will also be close to a (new) anchor word. Thus the procedure terminates with one point close to each anchor word, even in the presence of noise. A practical advantage of this procedure is that when faced with many choices for a next anchor word to find, our algorithm tends to find the one that is most different from the ones we have found so far. The algorithm terminates when it has found K anchors. K is a tunable parameter of the overall algorithm that determines how many topics are fitted to the dataset. To precisely state the guarantees of our algorithm, we use the following definition of robustness, which tries to formalize the intuition that topics should be fairly  X  X istinct, X  meaning that none lies close to the affine hull of the rest.
 Definition 4.1. A simplex P is  X  -robust if for every vertex v of P , the ` 2 distance between v and the affine hull of the rest of the vertices is at least  X  . Remark: In the overall picture, the robustness of the polytope P spanned by the given points is related to the (unknown) parameters of the topic model. For example, in LDA, this  X  is related to the largest ratio of any pair of hyper-parameters in the model.
 Our goal is to find a set of points that are close to the vertices of the simplex, defined as follows: Definition 4.2. Let a 1 ,a 2 ,...a V be a set of points whose convex hull P is a simplex with vertices v ,v 2 ,...v K . Then we say a i -covers v j if, whenever a i is written as a convex combination of the vertices, a i = P j c j v j , then c j  X  1  X  . Furthermore we say that a set of K points -covers the vertices if each vertex is covered by some point in the set.
 Suppose there is a set of points A = a 1 ,a 2 ,...a whose convex hull P is  X  -robust and has vertices v ,v 2 ,...v K (which appear in A ) and that we are given a perturbation d 1 ,d 2 ,...d V of the points so that for each i , k a i  X  d i k X  .
 Theorem 4.3. Under the conditions stated in the previous paragraph, and if &lt;  X  3 / 20 K , then there is a combinatorial algorithm that given { d 1 ,...,d runs in time  X  O ( V 2 + V K/ 2 ) 2 and outputs a subset of { d 1 ,...,d V } of size K that O ( / X  ) -covers the vertices. We note that variants of this algorithm have appeared in other contexts. Our analysis rests on the following lemmas, whose proof we defer to the supplementary material. Suppose the algorithm has found a set S of k points that are each  X  -close to distinct vertices in { v 1 ,v 2 ,...,v K } and that  X  &lt;  X / 20 K .
 Lemma 4.4. The point d j found by the algorithm must be  X  = O ( / X  2 ) close to some vertex v i . In particular, the corresponding a j O ( / X  2 ) -covers v i . This lemma is used to show that the error does not accumulate too badly in our algorithm, since  X  only depends on ,  X  (not on the  X  used in the previous step of the algorithm). This prevents the error from accumulating exponentially in the dimension of the problem, which would be catastrophic for our proof. After running the first phase of our algorithm, we run a cleanup phase (the second loop in Alg. 4) that can reduce the error in our algorithm.
 Lemma 4.5. Suppose | S | = K  X  1 and each point in S is  X  = O ( / X  2 ) &lt;  X / 20 K close to distinct vertices v the farthest point found by the algorithm is d j , then the corresponding a j O ( / X  ) -covers the remaining vertex. This algorithm can be seen as a greedy approach to maximizing the volume of the simplex. This view suggests other potential approaches for finding anchor words (where the goal is better solution quality rather than run time), but that study is left as an open question for future work.
 Related work. The separability assumption has been previously studied under the name  X  X ure pixel as-sumption X  in the context of hyperspectral unmixing. A number of algorithms have been proposed that over-lap with ours including VCA (Nascimento &amp; Dias, 2004), which differs in that there is no clean-up phase, and N-FINDR (Gomez et al., 2007) which attempts to greedily maximize the volume of a simplex whose vertices are data points by changing one vertex at a time. However these algorithms have only been proven to work in the infinite data case, and for our algorithm we are able to give provable guarantees even when the data points are perturbed (e.g., as the result of sam-pling noise). Recent work by Thurau et al. (2010), Ku-mar et al. (2012) and Gillis &amp; Vavasis (2012) designs algorithms for non-negative matrix factorization un-der the separability assumption. As mentioned above, we do not focus empirical comparisons on anchor-word selection variants as this step is not a bottleneck in our overall algorithm (see Section 5.2). We compare three parameter recovery methods, Recover (Arora et al., 2012b), RecoverKL, and Recov-erL2, with a Gibbs sampling implementation (McCal-lum, 2002). 3 Linear programming-based anchor word finding is too slow to be comparable, so we use Fas-tAnchorWords for all three recovery algorithms. Using Gibbs sampling we obtain the word-topic distributions by averaging over 10 saved states, each separated by 100 iterations, after 1000 burn-in iterations. 5.1. Methodology We train models on two synthetic data sets to evaluate performance when model assumptions are correct, and on real documents to evaluate real-world performance. To ensure that synthetic documents resemble the dimensionality and sparsity characteristics of real data, we generate semi-synthetic corpora. For each real corpus, we train a model using MCMC and then generate new documents using the parameters of that model (these parameters are not guaranteed to be separable; we found that about 80% of topics fitted by MCMC had anchor words). We use two real-world data sets, a large corpus of New York Times articles (295k documents, vocabu-lary size 15k, mean document length 298) and a small corpus of NIPS abstracts (1100 documents, vocab-ulary size 2500, mean length 68). Vocabularies were pruned with document frequency cutoffs. We generate semi-synthetic corpora of various sizes from models trained with K = 100 from NY Times and NIPS, with document lengths set to 300 and 70, respectively, and with document-topic distributions drawn from a Dirichlet with symmetric hyperparameters 0 . 03. We use a variety of metrics to evaluate the learned models. For the semi-synthetic corpora, we compute the reconstruction error between the true word-topic distributions and the learned distributions. In particular, given a learned matrix  X  A and the true matrix A , we use bipartite matching to align topics, and then evaluate the ` 1 distance between each pair of topics. When true parameters are not available, a standard evaluation for topic models is to compute held-out probability , the probability of previously unseen documents under the learned model. This com-putation is intractable in general, but there are reliable approximations (Wallach et al., 2009; Buntine, 2009). Topic models are useful because they provide in-terpretable latent dimensions. We can evaluate the semantic quality of individual topics using a metric called Coherence (Mimno et al., 2011). This metric has been shown to correlate well with human judgments of topic quality. If we perfectly reconstruct topics, all the high-probability words in a topic should co-occur frequently, otherwise, the model may be mixing unre-lated concepts. Given a set of words W , coherence is where D ( w ) and D ( w 1 ,w 2 ) are the number of doc-uments with at least one instance of w , and of w and w 2 , respectively. We set = 0 . 01 to avoid taking the log of zero for words that never co-occur (Stevens et al., 2012). Coherence measures the quality of indi-vidual topics, but does not measure redundancy, so we measure inter-topic similarity . For each topic, we gather the set of the N most probable words. We then count how many of those words do not appear in any other topic X  X  set of N most probable words. Some over-lap is expected due to semantic ambiguity, but lower numbers of unique words indicate less useful models. 5.2. Efficiency The Recover algorithms, in Python, are faster than a heavily optimized Java Gibbs sampling implementa-tion (Yao et al., 2009). Fig. 1 shows the time to train models on synthetic corpora on a single machine. Gibbs sampling is linear in the corpus size. RecoverL2 is also linear (  X  = 0 . 79), but only varies from 33 to 50 seconds. Estimating Q is linear, but takes only 7 seconds for the largest corpus. FastAnchorWords takes less than 6 seconds for all corpora. 5.3. Semi-synthetic documents The new algorithms have good ` 1 reconstruction error on semi-synthetic documents, especially for larger cor-pora. Results for semi-synthetic corpora drawn from topics trained on NY Times articles are shown in Fig. 2 (top) for corpus sizes ranging from 50k to 2M syn-thetic documents. In addition, we report results for the three Recover algorithms on  X  X nfinite data, X  that is, the true Q matrix from the model used to gen-erate the documents. Error bars show variation be-tween topics. Recover performs poorly in all but the noiseless, infinite data setting. Gibbs sampling has the lowest ` 1 on smaller corpora. However, for the larger corpora the new RecoverL2 and RecoverKL algorithms have the lowest ` 1 error and smaller variance (running sampling longer may reduce MCMC error further). Results for semi-synthetic corpora drawn from NIPS topics are shown in Fig. 2 (bottom), and are similar. Effect of separability . Notice that in Fig. 2, Re-cover does not achieve zero ` 1 error even with noiseless  X  X nfinite X  data. Here we show that this is due to lack of separability, and that the new recovery algorithms are more robust to violations of the separability as-sumption. In our semi-synthetic corpora, documents are generated from an LDA model, but the topic-word distributions are learned from data and may not satisfy the anchor words assumption. We now add a synthetic anchor word to each topic that is, by construction, unique to that topic. We assign the synthetic anchor word a probability equal to the most probable word in the original topic. This causes the distribution to sum to greater than 1.0, so we renormalize. Results are shown in Fig. 3. The ` 1 error goes to zero for Recover, and close to zero for RecoverKL and RecoverL2 (not zero because we do not solve to perfect optimality). Effect of correlation . The theoretical guarantees of the new algorithms apply even if topics are correlated. To test the empirical performance in the presence of correlation, we generated new synthetic corpora from the same K = 100 model trained on NY Times articles. Instead of a symmetric Dirichlet distribution, we use a logistic Normal distribution with a block-structured covariance matrix. We partition topics into 10 groups. For each pair of topics in a group, we add a non-zero off-diagonal element (  X  ) to the covariance matrix. This block structure is not necessarily realistic, but shows the effect of correlation. Results for  X  = 0 . 05 and 0 . 1 are shown in Fig. 4. Recover performs much worse with correlated topics than with LDA-generated cor-pora (c.f. Fig. 2). The other three algorithms, espe-cially Gibbs sampling, are more robust to correlation. Performance consistently degrades as correlation in-creases. For the recovery algorithms this is due to a decrease in  X  , the condition number of the R matrix. With infinite data, ` 1 error is equal to the ` 1 error in the uncorrelated synthetic corpus (non-zero because of violations of the separability assumption). 5.4. Real documents The new algorithms produce comparable quantitative and qualitative results on real data. Fig. 5 shows three metrics for both corpora. Error bars show the distribu-tion of log probabilities across held-out documents (top panel) and coherence and unique words across topics (center and bottom panels). Held-out sets are 230 documents for NIPS and 59k for NY Times. For the small NIPS corpus we average over 5 non-overlapping train/test splits. The matrix inversion step in Recover fails for the NIPS corpus so we modify the procedure to use pseudoinverse. This modification is described in the supplementary materials. In both corpora, Recover produces noticeably worse held-out log proba-bility per token than the other algorithms. Gibbs sam-pling produces the best average held-out probability ( p &lt; 0 . 0001 under a paired t -test), but the difference is within the range of variability between documents. We tried several methods for estimating hyperparam-eters, but the observed differences did not change the relative performance of algorithms. Gibbs sampling has worse coherence than the Recover algorithms, but produces more unique words per topic. These patterns are consistent with semi-synthetic results for similarly sized corpora (details are in supplementary material). For each NY Times topic learned by RecoverL2 we find the closest Gibbs topic by ` 1 distance. The closest, median, and farthest topic pairs are shown in Table 1. We observe that when there is a difference, recover-based topics tend to have more specific words ( Anaheim Angels vs. pitch ).
 We present new algorithms for topic modeling, in-spired by Arora et al. (2012b), which are efficient and simple to implement yet maintain provable guarantees. The running time of these algorithms is effectively in-dependent of the size of the corpus. Empirical results suggest that the sample complexity of these algorithms is somewhat greater than MCMC, but, particularly for the ` 2 variant, they provide comparable results in a fraction of the time. We have tried to use the output of our algorithms as initialization for further opti-mization (e.g. using MCMC) but have not yet found a hybrid that out-performs either method by itself. Finally, although we defer parallel implementations to future work, these algorithms are parallelizable, potentially supporting web-scale topic inference. We thank Percy Liang for suggesting the use of semi-synthetic corpora, and David Blei for helpful com-ments. This work is partially supported by NSF grants CCF-0832797, CCF-1117309, and DMS-0835373, a Google Faculty Research Award, CIMIT award 12-1262, and grant UL1 TR000038 from NCATS, NIH. David Mimno and Ankur Moitra were supported by NSF Computing Innovation Fellowships, and Yoni Halpern by an NSERC Postgraduate Scholarship.

