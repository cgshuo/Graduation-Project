 In this paper, we propose a fast, memory-efficient, and scal-able clustering algorithm for analyzing transactional data. Our approach has three unique features. First, we use the concept of Weighted Coverage Density as a categorical simi-larity measure for efficient clustering of transactional datasets. The concept of weighted coverage density is intuitive and allows the weight of each item in a cluster to be changed dynamically according to the occurrences of items. Second, we develop two transactional data clustering specific eval-uation metrics based on the concept of large transactional items and the coverage density respectively. Third, we im-plement the weighted coverage density clustering algorithm and the two clustering validation metrics using a fully au-tomated transactional clustering framework, called SCALE (Sampling, Clustering structure Assessment, cLustering and domain-specific Evaluation). The SCALE framework is de-signed to combine the weighted coverage density measure for clustering over a sample dataset with self-configuring methods that can automatically tune the two important pa-rameters of the clustering algorithms: (1) the candidates of the best number K of clusters; and (2) the application of two domain-specific cluster validity measures to find the best result from the set of clustering results. We have con-ducted experimental evaluation using both synthetic and real datasets and our results show that the weighted cov-erage density approach powered by the SCALE framework can efficiently generate high quality clustering results in a fully automated manner.
 I.5.3 [ Computing Methodologies ]: Pattern Recognition-Clustering Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. Algorithms Weighted Coverage Density, AMI, LISR, SCALE
Transactional data is a kind of special categorical data, which can be transformed to the traditional row by column table with Boolean values. Typical examples of transac-tional data are market basket data, web usage data, cus-tomer profiles, patient symptoms records, and image fea-tures. Transactional data are generated by many applica-tions from areas, such as retail industry, e-commerce, health-care, CRM, and so forth. The volume of transactional data is usually large. Therefore, there are great demands for fast and yet high-quality algorithms for clustering large scale transactional datasets.

A transactional dataset consists of N transactions ,eachof which contains varying number of items . For example, t 1 = { milk, bread, beer } and t 2 = { milk, bread } are three-item transaction and two-item transaction respectively. A trans-actional dataset can be transformed to a traditional cate-gorical dataset (a row-by-column Boolean table) by treating each item as an attribute and each transaction as a row. Al-though generic categorical clustering algorithms can be ap-plied to the transformed Boolean dataset, the two key fea-tures of such transformed dataset: large volume and high dimensionality, make the existing algorithms inefficient to process the transformed data. For instance, a market basket dataset may contain millions of transactions and thousands of items, while each transaction usually contains about tens of items. The transformation to Boolean data increases the dimensionality from tens to thousands, which poses signifi-cant challenge to most existing categorical clustering algo-rithms in terms of efficiency and clustering quality.
Recently, a number of algorithms have been developed for clustering transactional data by utilizing specific features of transactional data, such as LargeItem [21], CLOPE [23], and CCCD [22]. However, all of the existing proposals suffer from one obvious drawback. All proposed clustering algo-rithms require users to manually tune at least one or two parameters of the clustering algorithms in order to deter-mine the number of clusters to be used by each run of the algorithm, and to find the best clustering result. For exam-ple, LargeItem [21] needs to set the support  X  and the weight w , CLOPE [23] has a repulsion parameter r , and CCCD [22] has a parameter MMCD as threshold on clusters merging. Unfortunately, the settings of all these parameters are man-ually executed and are different from dataset to dataset, making the tuning of these parameters extremely hard. No existing proposals, to the best of our knowledge, have of-fered general guideline for adequately setting and tuning these parameters.

Also there lacks of cluster validation methods to evaluate the quality of clustering, because clustering is an unsuper-vised procedure. Some generic measures or the interactive visualization method [7] have been developed for clustering numerical data based on statistical and geometrical proper-ties [14]. Due to the lack of meaningful pair-wise distance function, entropy-based measure has been widely used as a generic measure for categorical clustering [5, 18, 8]. How-ever, such general metrics may not be effective as far as spe-cific types of datasets are concerned, such as transactional data. It is recognized that meaningful domain-specific qual-ity measures are more interesting [17, 14]. Surprisingly, very few of the existing transactional data clustering algorithms have developed transaction mining specific clustering vali-dation measure.

In this paper we present a fast, memory-saving, and scal-able clustering algorithm that can efficiently handle large transactional datasets without resorting to manual param-eter settings. Our approach is based on two unique design ideas. First, we introduce the concept of Weighted Coverage Density (WCD) as intuitive categorical similarity measure for efficient clustering of transactional datasets. The mo-tivation of using weighted coverage density as our domain-specific clustering criterion is based on the observation that association rules mining over transactional data is inherently related to density-based data clustering [15]. Thus we de-fine the weighted coverage density based on the concept of frequent itemsets [3].

Second, we develop two transactional data specific evalu-ation measures based on the concepts of large items [21] and coverage density respectively. Large Item Size Ratio (LISR) uses the percentage of large items in the clustering result to evaluate the clustering quality. Average pair-clusters Merg-ing Index (AMI), applies coverage density to indicate the structural difference between clusters.

We implement the weighted coverage density clustering algorithm and the two clustering validity metrics using a fully automated transactional clustering framework, called SCALE (Sampling, Clustering structure Assessment, cLus-tering and domain-specific Evaluation). The SCALE frame-work is designed to perform the transactional data cluster-ing in four steps, and it can handle transactional datasets that are small or medium or large in size. In the first step it uses sampling to handle large transactional dataset, and then performs clustering structure assessment step to gen-erate the candidate  X  X est Ks X  based on sample datasets. The clustering step uses the WCD algorithm to perform the initial cluster assignment and the iterative clustering refine-ment, until the WCD of the clustering result is approxi-mately maximized. A small number of candidate clustering results are generated at the end of the clustering step. In the domain-specific evaluation step, we apply the two domain-specific measures (AMI and LISR) to evaluate the cluster-ing quality of the candidate results produced and select the best one. We have conducted experimental evaluation using both synthetic and real datasets. Our results show that the weighted coverage density approach powered by the SCALE framework can generate high quality clustering results in an efficient and fully automated manner.

The rest of the paper is organized as follows. Section 2 gives an overview of some related work. Section 3 details the definitions of key concepts used in our clustering algorithm, the algorithm description and complexity analysis. The two measures AMI and LISR for clustering results evaluation are presented in Section 4. We briefly introduce the SCALE framework in Section 5 and report our initial experimental evaluation results in Section 6. We summarize our contri-butions in Section 7.
A number of algorithms have been developed for categor-ical data clustering in recent years [4, 8, 5, 12, 11, 13, 16, 18]. Some algorithms have studied distance-like pair-wise similarity measures, such as K-Modes [16] and ROCK [13]. While it is commonly recognized that a pair-wise similarity (e.g., cosine measure, the Dice and Jaccard coefficient, etc.) is not intuitive for categorical data, there have been algo-rithms using similarity measures for a set of records. The typical set-based similarity measures are based on informa-tion theory, such as expected-entropy in Coolcat [5], MC [18] and ACE [8], mutual information based similarity in LIMBO [4] and information bottleneck in [20], and minimum descrip-tion length in Cross Association [6]. These algorithms have been focused on generic clustering structure of categorical data. Another observation is that most of these categori-cal clustering algorithms determine the number of clusters k explicitly. For example, from the earliest k-modes [16] and ROCK [13] to the latest COOLCAT [5], LIMBO [4] and MC [18], k is an input parameter of the algorithm. Assuming the k at the beginning is extremely difficult in practice.
There are also some works on u sing bipartite graph the-ory to cluster transactional data [1, 19, 10, 24, 9]. Cluster-ing algorithms based on partitioning bipartite graph usually generate co-clustering results, where columns and rows of the dataset are partitioned at the same time. If they are applied to transactional data, items and transactions are clustered simultaneously, which unnaturally splits the clus-ters that overlap over a few frequent items. Furthermore, the graph-based algorithms are often memory and time con-suming, thus inappropriate for clustering large transactional datasets.
In this section, we present the WCD clustering algorithm for transactional data. The key design idea of the WCD algorithm is the definition of the  X  X eighted Coverage Den-sity X  based clustering criterion, which tries to preserve as many frequent items as possible within clusters and controls the items overlapping between clusters implicitly.
This section is organized as follows: First, we introduce the concept of Coverage Density (CD) and Weighted Cov-erage Density (WCD) as intra-cluster similarity measures. The coverage density measure approximates the naive uni-form item distribution in the clusters and is primarily used to describe the difference between item distributions, while the weighted coverage density measure describes the frequent-itemset preferred item distribution in the clusters and is used in clustering to preserve more frequent itemsets. We also compare CD and WCD based on their connection to statis-tical and information theoretic methods. Finally we define the WCD based cluster criterion, present an overview of the WCD clustering algorithm, and provide complexity analysis of the algorithm.
We first define the notations of transactional dataset and transactional clustering result used in this paper. A trans-actional dataset D of size N is defined as follows. Let I = { I 1 ,I 2 ,...,I m } be a set of items, D be a set of N transactions, where transaction t j (1  X  j  X  N )isasetof the length of the transaction. A transaction clustering result C
In this section we illustrate the concept of Coverage Den-sity (CD) and the concept of Weighted Coverage Density (WCD) as intra-cluster similarity measures. To provide an intuitive illustration of our development of these concepts, let us map the transactions of D onto a 2D grid graph. Let the horizontal axis stand for items and the vertical axis stand for the transaction IDs, and each filled cell ( i, j )rep-resents the item i is in the transaction j . For example, a simple transactional dataset { abcd, bcd, ac, de, def } can be visualized in Figure 1.
If we look at the filled area in the graph carefully, two naturally formed clusters appear, which are { abcd, bcd, ac and { de, def } indicated by two rectangles in Figure 1. In the original graph there are 16 cells unfilled, but only 4 in the two partitioned subgraphs. The less the unfilled cells are left, the more compact the clusters are. Therefore, we con-sider that the problem of clustering transactional datasets can be transformed to the problem of how to obtain the minimized unfilled number of cells with appropriate number of partitions. Here, if we try to use bipartite graph based co-clustering method to partition the transactions and the items, the result is shown by two straight lines in the right most part of Figure 1. Obviously co-clustering will result in the association loss between item c and item d .Thisis one of the reasons why we define our clustering problem as row clustering not co-clustering in our application context. This simple example also shows that it is intuitive to visu-alize the clustering structure of the transactions when they have already been ordered in the specific way as shown in the left most of Figure 1. Thus how to order and partition the transactional dataset properly is one of the key issues of our algorithm.
 Bearing this intuition in mind, we define the first concept, Coverage Density (CD) , for evaluating the compactness of the partitions in terms of the unfilled cells only. In short, CD is the percentage of filled cells to the whole rectangle area which is decided by the number of distinct items and number of transactions in a cluster.

Given a cluster C k , it is easy and straightforward to com-pute its coverage density. Suppose the number of distinct items is M k ,theitemssetof C k is I k = { I k 1 ,I k 2 ,...,I the number of transactions in the cluster is N k ,andthe sum occurrences of all items in cluster C k is S k , then the Coverage Density of cluster C k is The coverage density reflects the compactness of a cluster intuitively. Generally speaking, the larger the coverage den-sity is, the higher the intra-cluster similarity among the transactions within a cluster.

However, the CD metric is insufficient to measure the den-sity of frequent itemset, since in the CD metric each item has equal importance in a cluster. Namely, if the cell (i, j) X  X  contribution to the coverage density consists of transac-tional contribution T i and the item contribution W j .In both transactional and item contributions are uniform, i.e., T i = T = 1 N k and W j = W = 1 M k . CD can be represented as T i  X  treating each cell with the same importance as shown in Figure 3(a).

Another problem with the CD metric is the situation where two clusters may have the same CD but different filled-cell distributions. Consider the two clusters in Figure 2: is there any difference between the two clusters that have the same CD but different filled-cell distributions? This leads us to develop a heuristic rule for identifying and se-lecting a better distribution: we consider the cluster with the coverage density focused on the high-frequency items to be better in terms of compactness than the cluster with the same CD but the filled-cell distribution is somewhat scat-tered with respect to all items.

We now introduce the concept of Weighted Coverage Den-sity (WCD) to serve for this purpose.
Concretely, we put more  X  X eight X  on the items that have higher occurrence frequency in the cluster. This definition implies that the weight of each item is not a fixed one during the clustering procedure and it is decided by the current items distribution of a cluster. Thus, the item contribution item contribution W j is defined as the ratio of occurrences of each item to the sum of occurrences of all items, namely,
By Equation (2), and without changing the transactional contribution T ,the Weighted Coverage Density of a cluster C k can be defined as follows:
Recall the example in Figure 2, by Equation (3), the weighted coverage density of the cluster on the left is 9 while the weighted coverage density of the cluster on the right is 11 15 . Therefore, the cluster on the right is better, which is consistent with our heuristic rule.
In the above section we have given the intuitive definition of CD and WCD. We will show that CD and WCD are inherently connected to some important statistical concepts.
Let random variable X represent the frequency of an item, we can consider that the occurrences of items in a cluster C k as the sample probability density function (PDF) of X , denoted by f ( X ). Therefore, CD and WCD are strongly related to the first moment (the expectation, E [ X ]) and the second moment ( E [ X 2 ]), i.e.,
Since E [ X 2 ]= E 2 [ X ]+ Var ( X ), for two clusters that have the same number of transactions (T) and E [ X ], our clustering criterion of maximizing WCD will prefer the clus-ter having higher Var ( X ), i.e., deviating more from the sce-nario that each item has similar frequency. Let p ( X = I be the proportion of item occurrences I kj in all item occur-rences in cluster C k .
 entropy of this cluster. For M k number of items, this en-tropy is maximized when each item frequency is the same, i.e., Var ( X ) = 0, when the variance is minimized. There-fore, maximizing Var ( X ) is remotely related to minimizing the entropy criterion [5, 18, 8]. We will show that WCD based criterion is more efficien t in clustering transactional data, while also generating high-quality clustering results preserving more frequent itemsets.
We define the WCD-based clustering criterion in this sec-tion and outline the design of the WCD clustering algorithm in the next section.

To define the WCD-based clustering criterion, we also take into account of the number of transactions in each cluster. For a clustering result C K = C 1 ,C 2 ,...,C K where K&lt;N , we define the following Expected Weighted Coverage Density (EWCD) as our clustering criterion function.
An EWCD-based clustering algorithm tries to maximize the EWCD criterion.

However, if EWCD is used as the only metric in clus-tering procedure, an exception occurs when the number of clusters is not restricted -when every individual transaction is considered as a cluster, it will get the maximum EWCD over all clustering results. Therefore, the number of clusters needs to be either explicitly given or implicitly determined by other parameters. To avoid blindly setting k or tuning complicated parameters, we propose the SCALE framework in Section 5, where a set of candidate  X  X est Ks X  is suggested by the BKPlot method [8].
The WCD clustering algorithm uses a partition-based clus-tering approach. It scans the dataset iteratively to optimally assign each transaction to a cluster in terms of maximizing the EWCD criterion. The entire procedure of the WCD-based clustering can be divided into three phases: the clus-tering structure assessment phase, the WCD based initial cluster assignment, and the WCD-based iterative clustering refinement phase. We call the first phase the WCD algo-rithm preparation step, whi ch can be performed by using an existing algorithm that can find the best K or best can-didate Ks. We refer to the second and third phases as the WCD clustering step, which is executed by the WCD algo-rithm.

Concretely, in the clustering structure assessment phase, we analyze the given transactional dataset to determine the candidates of critical clustering structure and generate the best K or a few candidate best Ks. One of the possible approaches to perform the first task is to use the Best K method (BKPlot) we have developed [8] for finding the best candidate Ks for clustering categorical datasets. The main idea of the BKPlot method is to examine the entropy differ-ence between the optimal clustering structures with vary-ing K and reports the Ks where the clustering structure changes dramatically. In [8] we developed a hierarchical al-gorithm that is capable of generating high-quality approx-imate BKPlots, which can capture the best Ks with small errors. The algorithm also generates a hierarchical cluster-ing tree, where the cluster seeds can be found at different Ks. One can also use other available algorithms that can find the best K in this phase.

In the initial cluster assignment phase, we take the out-puts from the clustering structure assessment phase and pro-duce an initial assignment using the WCD algorithm. Con-cretely, the WCD clustering algorithm takes the K number of clusters and the cluster seeds at the best Ks as inputs to define the initial K clusters. Eachseedrepresentsanini-tial cluster consisting of a few transactions. Given one of the best Ks, the WCD algorithm performs the clustering over the entire dataset. It reads the remaining transactions sequentially, and assigns each transaction to one of the K clusters, which maximizes the EWCD of the current cluster-ing result. Our experiments show that the BKPlot method can efficiently help reduce the search space and get high quality clustering result.

Since the initial assignment produced in the second phase may not be optimal, in the iterative clustering refinement phase, the cluster assignment is refined in an iterative man-ner until no more improvement can be made with respect to WCD on the clustering result. Concretely, the algorithm reads each transaction in a randomly perturbed order, and check if the original cluster assignment is optimal in the sense that the EWCD metric is maximized. If it is not opti-mal, the transaction is moved to currently best fitted cluster, which increases the amount of EWCD the most. Any gen-erated empty cluster is removed after a move. The iterative phase is stopped if no transaction is moved from one clus-ter to another in one pass for all transactions. Otherwise, a new pass begins. Note that the number of iterations may vary with respect to different random processing sequence and different clustering structure. It also depends on the number of clusters. Our experiments in Section 6 show that two or three iterations are enough for most well structured small datasets, while more iterations are often required for large and noisy datasets.

A sketch of the pseudo code for the WCD algorithm is given in Algorithm 1.
 Algorithm 1 WCD.main() Input: Dataset file D of transactions; Number of clusters K; Initial K seeds
Output: K clusters /*Phase 1 -Initialization*/
K seeds form the initial K clusters; while not end of dataset file D do end while /*Phase 2 -Iteration*/ while moveMark = true do end while
Finding the destination cluster for each transaction is the key step in both phases, which needs to compute/update EWCD for each possible assignment. To avoid unneces-sary access and computation, the WCD clustering algorithm keeps the summary information of each cluster in main mem-ory and updates it after each assignment. The summary information of cluster C k includes the number of transac-tions N k , the number of distinct items M k ,thesumoc-currences of items S k , the sum square occurrences of items S C , and the occurrences of each item I k [ j ] .occur .
With the summary information, we are able to incre-mentally compute EWCD. Concretely, the two functions DeltaAdd and DeltaRemove can perform the incremental computing by adding one transaction into a cluster or re-moving one transaction from it respectively. Since the two functions are similar, we only provide outline of the function DeltaAdd in Algorithm 2. Let t.I be the set of items in the transaction t .
 Algorithm 2 WCD.deltaAdd( C k , t ) float deltaAdd ( C k , t ) { }
The space consumption of WCD algorithm is quite small, since only the summary information of clusters is kept in memory. Let K stand for the number of clusters, and M stand for the maximum number of distinct items in a cluster. A total O ( K  X  M ) space is necessary for the algorithm. For a typical transactional dataset with up to ten thousand distinct items, several megabytes will be sufficient for the WCD clustering algorithm.

The most time-consuming part is the update of EWCD to find the best cluster assignment which involves DeltaAdd and DeltaRemove. Since each DeltaAdd/DeltaRemove costs O ( | t | ), the time complexity of the whole algorithm is O (  X  N  X  K  X | t | ),where  X  is the number of iterations and N is the number of transactions in dataset. Usually  X  , K and the length of transaction | t | are much smaller than N , i.e. the running time is almost linear to the size of datasets. So the WCD clustering algorithm is ideal for clustering very large transactional datasets.
We have shown the criterion function EWCD ,whichis approximately optimized by the WCD algorithm with the help of the BKPlot method. In this section, we propose two quality measures for cluster evaluation of transactional data. LISR  X  measuring the preservation of frequent itemsets Since one of the popular applications of transactional data clustering is to find localized association rules [2], we propose a new measure called Large Item Size Ratio ( LISR )toeval-uate the percentage of Large Items [21] preserved by clus-tering. The more large items ar e preserved, the higher pos-sibility the frequent itemsets are preserved in the clusters. An item is a  X  X arge Item X  when its occurrences in a cluster are above the user-specified proportion of transactions. We name the user-specified proportion as the minimum support, which is remotely connected with the  X  X inimum support X  of association rules mining. The LISR computing formula is LISR = occurrences of Large Items in cluster C k and S k stands for the total occurrences of all items in cluster C k .Intheabove formula, the number of transactions in each cluster is taken into account in order to reduce the influence of noise tiny clusters to the whole clustering result. A large LISR means high concurrences of items and implies the high possibility of finding more Frequent Itemsets [3] at the user-specified minimum support. In practice, users can provide different minimum supports they are interested for finding association rules,andthencomparethe LISRs of different clustering re-sults to decide which clustering result is the most interesting one.
 AMI  X  measuring inter-dissimilarity of clusters As we have shown previously, WCD measure evaluates the homogeneity of the cluster and tries to preserve more fre-quent itemsets, while CD only evaluate the homogeneity. Below we define the heuristic structural difference based on the CD measure, which is shown effective in describing the overall inter-cluster dissimilarity in experiments.
Given a pair of clusters C i and C j , the inter-cluster dis-similarity between the C i and C j is: Simplifying the above formula, we get d ( C i ,C j )= 1 N ( where M ij is the number of distinct items after merging two number between 0 and 1. Here, S i ( 1 M i  X  1 M ij ) describes the structural change caused by the cluster C i . Not surprisingly, when two clusters have the same set of items, that is M i = having little overlapping between the sets of items, merging them will result in a large d ( C i ,C j ). Therefore, we say the above measure evaluates the structural difference between clusters.

We propose the Average pair-clusters Merging Index (AMI) to evaluate the overall inter-dissimilarity of a clustering re-sult having K clusters.
 AMI is the average dissimilarity between all clusters. The larger the AMI is, the better the clustering quality.
Some traditional clustering methods try to optimize the clustering validity measure by combining intra-cluster sim-ilarity and inter-cluster dissimilarity [17, 14, 21]. However, this is extremely difficult in practice since we need some domain-specific weighting parameters to combine the intra-cluster similarity and the inter-cluster dissimilarity, and the setting of such parameters may differ from dataset to dataset. Thus, in our prototype implementation of the SCALE frame-work (see next section for detail), we choose to optimize them separately: we optimize the intra-cluster EWCD mea-sure with the WCD algorithm at the candidate best Ks, and use the AMI measure to select the best one. Our exper-iments show that AMI is an effective measure for indicating the globally distinctive clustering structure. In addition, LISR is used as a domain-specific measure in the SCALE framework. Our experiments in Section 6 show that the WCD algorithm can generate high quality results reflecting the domain-specific structure.
We implement the WCD clustering algorithm and the two transactional data specific clustering validity metrics using a fully automated transactional clustering framework, called SCALE (Sampling, Clustering structure Assessment, cLus-tering and domain-specific Evaluation). The SCALE frame-work is designed to perform the transactional data clustering in four steps as shown in Figure 4. SCALE uses the sam-pling step to handle large transactional dataset. Standard sampling techniques are used in the sampling step to gener-ate some sample datasets from the entire large dataset. The framework assumes the primary clustering structure (with small number of large clusters) is approximately preserved with appropriate sample size.

In the clustering structure assessment step, SCALE de-termines the candidates of critical clustering structure and generates the candidate  X  X est Ks X  based on sample datasets. In our prototype implementation, the candidates of critical clustering structure are recommended by the Best K method BKPlot developed at Georgia Tech [8]. BKPlot method studies the entropy difference between the optimal clustering structures with varying K and reports only those Ks where the clustering structure changes dramatically as the candi-date best Ks, which greatly reduces the search space of find-ing the domain-specific candidate best Ks. In the SCALE prototype, we use a hierarchical algorithm proposed in [8] to generate high-quality approximate BKPlots, which can capture the candidate best Ks with small errors. The al-gorithm also generates a hierarchical clustering tree, where the cluster seeds can be found at different Ks. The cluster-ing structure assessment step outputs the best Ks and the cluster seeds at the best Ks to the clustering step.
The clustering step applies the WCD clustering algorithm to perform initial cluster assignment. The initial assessment result is then used to guide the WCD clustering over the entire dataset in an iterative manner until no transaction is moved from one cluster to another in one pass with respect to maximizing WCD. At the end of iterative assignment refinement, a small number of candidate clustering results are generated. Now we use the domain-specific measures (AMI and LISR) to evaluate the clustering quality of the candidate results produced in the clustering step and select the best one.

We have conducted experimental evaluation using both synthetic and real datasets. The details are reported in the next section. Our results show that the weighted coverage density approach powered by the SCALE framework can generate high quality clustering results in an efficient and fully automated manner.
In this section we evaluate the WCD-based clustering ap-proach using both synthetic and real datasets. Our exper-iments show three interesting results: (1) the WCD-based measures are effective in determining the meaningful struc-tures of transactional datasets; (2) the WCD algorithm is fast and also capable of generating high-quality clustering re-sults in terms of the domain-specific measures, compared to the existing best-performance transactional data clustering algorithm, CLOPE [23]; (3) the WCD algorithm is scalable to large transactional data in terms of the related factors.
Here, we would like to give a brief introduction to CLOPE [23] at first. CLOPE also maps the item/transaction rela-tionship to a 2D grid graph. However, it defines the intra-similarity of a cluster based on the ratio of the average item occurrences to the number of distinct items. A larger such ratio means the higher intra-cluster similarity. CLOPE computes the profit of a clustering result as profit r ( C )= items in cluster C i , W ( C i ) is the number of distinct items of cluster C i and | C i | is the number of transactions in clus-ter C i . The optimal result is achieved when the profit is maximized. The repulsion parameter r is introduced in the profit computing, implicitly controlling the number of clus-ters. However, there is no guideline to find the appropriate setting of r and we also find this setting may be different from dataset to dataset, which make it difficult to apply CLOPE in practice.

Before reporting the results of our experiments, we first introduce the datesets used in the experiments. Our experiments have used two synthetic datasets: Tc30a6r1000 2L generated by us and TxI4Dx Series gener-ated by synthetic data generator used in [3]. In addition, we used two real datasets: Zoo and Mushroom from the UCI machine learning repository 1 .

Tc30a6r1000 2L dataset is generated with a two-layer clus-tering structure that is clearly verifiable, as shown in Figure 5. We use the same method documented in [8] to generate the Tc30a6r1000 2L dataset. We want to use this synthetic dataset to test how well our WCD approach can perform when the critical clustering structures of the dataset are determined correctly at the clustering structure assessment step. It has 1000 records, and 30 columns, each of which has 6 possible attribute values. The top layer has 5 clus-http://www.ics.uci.edu/  X  mlearn/MLRepository.html ters with 200 data points in each cluster, four of which have two overlapping sub-clusters of 100 data points, respectively. In Figure 5, blank areas represent the same attribute value 0, while non-blank areas are filled with randomly generated attribute values ranging from 0 to 5. Since it is a generic cat-egorical dataset, the attribute values are converted to items in order to run the WCD and CLOPE algorithms.

Figure 5: Structure
Zoo Real dataset Zoo from the UCI machine learning repository is used for testing the quality of clustering results. It contains 101 data records for animals. Each data record has 18 attributes (animal name, 15 Boolean attributes, 1 numericwithsetofvalues[0,2,4,5,6,8]andanimaltype values 1 to 7) to describe the features of animals. The animal name and animal type values are ignored in our transformed file, while the animal type also serves as an indication of domain-specific clustering structure.

Mushroom Real dataset Mushroom from the UCI machine learning repository contains 8124 instances, which is also used for quality testing. Each data record has 22 categori-cal attributes (e.g. cap-shape, cap-color, habitat etc.) and is labeled either  X  X dible X  or  X  X oisonous X . The dataset con-tains 23 species of mushroom according to the literature. Therefore, we assume the domain-specific clustering struc-ture could possibly have about 23 clusters. We use these knowledge to assess the clustering results and the effective-ness of the domain-specific measures.

Mushroom100k We also sample the mushroom data with duplicates to generate the mushroom100k of 100,000 in-stances as a real dataset for performance comparison with CLOPE.

TxI4Dx Series Data generator in [3] is used to gener-ate large synthetic transactional datasets for performance test. We first give the symbols used in order to annotate the datasets. Three primary factors are the average trans-action size T , the average size of the maximal potentially large itemsets I and the number of transactions D .Fora dataset having T = 10, I = 4 and 100 K transactions is denoted as T 10 I 4 D 100 K . The number of items and the number of maximal potentially large itemsets are always set to 1000 and 2000. We generate 5 groups of datasets from T 10 I 4 D 100 K to T 10 I 4 D 500 K by varying the number of transactions and each group has 10 randomly generated datasets at same parameters. We also generate 4 groups of datasets from T 5 I 4 D 100 K to T 50 I 4 D 500 K by setting the average length of transactions as 5, 10, 25 and 50. Also each group has 10 randomly generated datasets at same param-eters.
The candidate Ks generated for Tc30a6r1000 2L at the clustering structure assessment step is { 3, 5, 9 } .Sowerun WCD algorithm on K =3 , 5, and 9 for Tc30a6r1000 2L. In order to compare WCD algorithm with CLOPE, we run CLOPE with r changing from 1.0 to 3.0 and decide the appropriate r according to the prior knowledge of this syn-thetic dataset. Since we know the exact clustering structure of Tc30a6r1000 2L, we are able to find the CLOPE cluster-ing results matching the best number of clusters. Without the help of additional knowledge about the clustering struc-ture, it would be impossible to determine r solely by CLOPE documented in [23].

We summarize the quality of clustering results with the measure AMI (Figure 7) and LISR (Figure 6). For CLOPE results on Tc30a6r1000 2L, we show the AMI values for r varying from r =1 . 0to r =2 . 5. The best clustering results include a five-cluster clustering result with r =2 . 0anda nine-cluster clustering result with r =2 . 43. LISR curves show that CLOPE often gives much worse results at higher minimum supports. The cluster-class confusion matrix in Figure 9 also shows that WCD produces better results than CLOPE. The rows of confusion matrix stand for clusters and the columns stand for the original class definition given by the literature. Figure 7: AMI graph for Figure 9: Cluster-Class Confusion Matrix for
The candidate Ks generated at the clustering structure assessment step are { 2, 4, 7 } for Zoo and { 2, 4, 6, 15, 19, 22, 29 } for Mushroom. The literature shows the most likely K for Zoo is 7, and it could be around 23 for Mush-room. We run the WCD clustering algorithm on the trans-formed datasets with different Ks and seeds to get the candi-date clustering results. We also run CLOPE on Mushroom with parameter r =2 . 6 to get 27 clusters, as suggested in [23]. Since there are no repo rted results of CLOPE on Zoo dataset, we try CLOPE with varying r to find the result that is closest to the domain-specific structure. We find that CLOPE generates 7 clusters only when r is varied from 2.41 to 2.54, and the clustering result at r =2 . 5 is selected for comparison.
 Figure 10: LISR graph for The AMI values in Figure 8 and Table 1 show that the WCD results are better than CLOPE X  X  at the candidate Ks. To compare over different Ks, the AMI index also suggests the WCD result at K=7 is the best for Zoo, and WCD results at K=4, 6, 19 for Mushroom are outstanding.
 Figure10 and Figure11 show the comparison of WCD and CLOPE using the LISR measure for Zoo and Mushroom datasets. Consistently, the LISR graphs of Zoo (Figure10) and of Mushroom (Figure11) suggest that the WCD results at K=7 for Zoo and K=19 for Mushroom are the best re-sults. In summary, the experimental results on both Zoo and Mushroom datasets demonstrate that the LISR and AMI measures can help to find the clustering structures that are consistent with the documented structures, and that the WCD approach generates better clustering results than CLOPE, with the additional advantage of no parame-ter tuning .
It is interesting for us to find that AMI index is consistent with the results produced in the clustering structure assess-ment step using BKPlot on some datasets. Figure 7 and 8 have already shown that the AMI values at the best Ks suggested by BKPlots for the 2-layer synthetic dataset and the real zoo dataset. Figure 12 and 13 plot the AMI curves with varying K for these two datasets, respectively. The Figure 12: AMI curve for global peak values (K=5 for Tc30a6r1000 2L and K=7 for Zoo) always appears at one of the candidate Ks suggested by BKPlot.
We compare the CLOPE and WCD on the running time of algorithms by varying the number of clusters and by varying thesizeofdataset.

First, we run CLOPE by varying r from 0.5 to 4.0 with step value 0.5. The running seconds and the number of clus-ters are reported for each r . The number of clusters is 17, 18, 27, 30, 31, 32, 41 and 64 for different r values, respectively. Correspondingly, we run WCD on these numbers of clusters and get WCD running seconds. The comparison in Figure 14 shows that the cost of WCD is much less than that of CLOPE with respect to the number of clusters produced. Second, we run CLOPE on 10%, 50% and 100% size of Mushroom100k with r =2 . 0 and get the number K of clus-ters are 22, 23, and 30, respectively. Then we run WCD us-ing the same numbers of clusters on the same set of datasets. The results in Figure 15 show that WCD is also much faster than CLOPE with respect to the size of the dataset. Figure 14: The total run-
In this section we further study the performance of WCD on large synthetic transactional datasets. The time com-plexity of WCD algorithm is O (  X   X  N  X  K  X | t | ). Since the number of iterations,  X  , is not controllable, we study the other three factors: the number of transactions N ,the number of clusters K , and the average length of transactions | t | . TxI4Dx series are used in this set of experiments.
We first did experiments on 5 groups of T10I4Dx datasets with different size from 100K to 500K and set the number of clusters K =10 , 50 , 100 separately. Each group has 10 ran-domly generated datasets with the same parameter setting and we average the running time of 10 datasets as the final running time of each group. Figure 16 and Figure 17 show that the overall cost is almost linear in terms of the size of dataset and the number K of clusters.

Then we did experiments on another 4 groups of TxI4D100K datasets with different average length of transactions and different number of clusters. Figure 18 shows for small num-ber of clusters ( K  X  50), the average length of transactions is approximately linear to the running time, while for large K ,suchas K = 100, it becomes nonlinear. Since we are more interested in the clustering structure with small num-ber of clusters ( K  X  50), the WCD algorithm is also scalable to the average length of transactions.
We have presented WCD  X  a fast, memory-saving and scalable algorithm for clustering transactional data, includ-ing two transactional data specific cluster evaluation mea-sures: LISR and AMI. We implemented the WCD clustering algorithm and the LISR and AMI clustering evaluation us-ing SCALE  X  a fully automated transactional data cluster-ing framework, which eliminate the complicated parameter setting/tuning required by existing algorithms for cluster-ing transactional data. Concretely, the SCALE framework is designed to perform the transactional data clustering in four consecutive steps. It uses sampling to handle large transactional dataset, and then performs clustering struc-ture assessment step to generate the candidate  X  X est Ks X  based on sample datasets. The clustering step uses the WCD algorithm to perform the initial cluster assignment and the iterative clustering refinement. A small number of candidate clustering results are generated at the end of the clustering step. In the domain-specific evaluation step, the two domain-specific measures (AMI and LISR) are ap-plied to evaluate the clustering quality of the candidate re-sults produced and select the best one. We have reported our experimental evaluation results with both synthetic and real datasets. We show that compared to existing trans-actional data clustering methods, the WCD approach (the WCD clustering algorithm powered by the SCALE frame-work) can generate high quality clustering results in a fully automated manner with much higher efficiency for wider collections of transactional datasets. The first author is partly supported by China Scholarship Council for her one year visit at Georgia Institute of Technol-ogy. The second and the third authors are partly supported by grants from NSF CSR, ITR, Cybertrust, a grant from AFOSR, an IBM SUR grant, and an IBM faculty award. Joonsoo Bae (Dept of Industrial &amp; Sys. Eng, Chonbuk National Univ, South Korea, email: jsbae@chonbuk.ac.kr ) and Zhang Yi(Computational Intelligence Laboratory, Uni-versity of Electronic Science &amp; Tech. of China, 610054 P.R. China, email: zhangyi@uestc.edu.cn ). [1] J. Abello, M. G. C. Resende, and S. Sudarsky.
Figure 16: The total running time [2] C. C. Aggarwal, C. Magdalena, and P. S. Yu. Finding [3] R. Agrawal and R. Srikant. Fast algorithms for mining [4] P. Andritsos, P. Tsaparas, R. J. Miller, and K. C. [5] D. Barbara, Y. Li, and J. Couto. Coolcat: an [6] D. Chakrabarti, S. Papadimitriou, D. S. Modha, and [7] K. Chen and L. Liu. VISTA: Validating and refining [8] K. Chen and L. Liu. The  X  X est k X  for entropy-based [9] I. S. Dhillon. Co-clustering documents and words [10] C. H. Q. Ding, X. He, H. Zha, M. Gu, and H. D. [11] V. Ganti, J. Gehrke, and R. Ramakrishnan. Cactus: [12] D. Gibson, J. Kleinberg, and P. Raghavan. Clustering [13] S. Guha, R. Rastogi, and K. Shim. Rock: A robust [14] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. [15] T. Hastie, R. Tibshirani, and J. Friedmann. The [16] Z. Huang. Extensions to the k-means algorithm for [17] A. K. Jain and R. C. Dubes. Data clustering: A [18] T. Li, S. Ma, and M. Ogihara. Entropy-based criterion [19] N. Mishra, D. Ron, and R. Swaminathan. On finding [20] N.Tishby,F.C.Pereira,andW.Bialek.The [21] K. Wang, C. Xu, and B. Liu. Clustering transactions [22] H. Yan, L. Zhang, and Y. Zhang. Clustering [23] Y. Yang, X. Guan, and J. You. Clope: A fast and [24] H.Zha,X.He,C.H.Q.Ding,M.Gu,andH.D.

