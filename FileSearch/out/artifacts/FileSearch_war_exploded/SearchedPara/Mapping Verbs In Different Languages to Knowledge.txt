 In recent years a variety of large knowledge bases (KBs) have been constructed e.g., Freebase (Bol-lacker et al., 2008), DBpedia (Auer et al., 2007), NELL (Carlson et al., 2010), and Yago (Suchanek et al., 2007). These KBs consist of (1) an on-tology that defines a set of categories (e.g., Sport-sTeam, City ), (2) another part of the ontology that defines relations with these categories as argument types (e.g., teamPlaysInCity (SportsTeam, City) ), (3) KB entities which instantiate these categories (e.g., Steelers 2 SportsTeam ), and (4) KB entity pairs which instantiate these relations (e.g., ( Steel-ers , Pittsburgh ) 2 teamPlaysInCity ). The KB on-tology also specifies constraints (e.g., mutual exclu-sion, subset) among KB categories and relations.
Despite recent progress in KB construction, there is not yet a verb resource that maps to these KBs: lations. Such a verb resource can be useful to aid KB relation extraction. A distribution of verb phrases associated with any given KB relation is also a KB-independent representation of that relation X  X  seman-tics which can form the basis of aligning ontologies across arbitrary KBs (Wijaya et al., 2013). Given a KB and verb resources in different languages that map to the KB, we can also begin to align knowl-edge expressed in different languages.

We introduce here an approach to mapping verb phrases to KB relations using a very large ClueWeb corpus (Callan et al., 2009) as a kind of interlin-gua. Our approach grounds each KB relation in-stance (e.g., teamPlaysInCity (Steelers, Pittsburgh )) in mentions of its argument pair in this text, then represents the relation in terms of the verb phrases that connect these paired mentions (see Fig. 1). For a high coverage mapping, we train on both labelled and unlabelled data using expectation maximization (EM). We introduce argument type checking during the EM process to ensure only verbs whose argu-ment types match the relation X  X  argument types are mapped to the relation. We also incorporate con-straints defined in the KB ontology to find a verb to relation mapping consistent with these constraints.
Our contributions are: (1) We propose a scal-able EM-based method that automatically maps verb phrases to KB relations by using the mentions of the verb phrases with the relation instances in a very large unlabeled text corpus. (2) We demon-strate the effectiveness of the resource for extract-ing relation instances in NELL KB. Specifically, it improves the recall of both the supervised-and the unsupervised-verb-to-relation mapping; demon-strating the benefit of semi-supervised learning on unlabeled Web-scale text. (3) We demonstrate the flexibility of the method, which is both KB-and language-independent, by using the same method for constructing English verb resource to automat-ically construct a Portuguese verb resource. (4) We 2.1 Terminology We define a NELL KB to be a 6-tuple (
C,I C ,R,I R ,Subset,Mutex ) . C is the set of cate-gories e.g., SportsTeam i.e., c I entity-category pairs e.g., ( Cleveland , City ) i.e., I = { ( e m , c j ) | e m 2 c j , c j 2 C } .

R is the set of relations e.g., teamPlaysInCity i.e., r to be a function that when applied to a relation r tion f f
I R is the set of relation instances which are entity-relation-entity triples e.g., (Cava-liers, teamPlaysInCity , Cleveland) i.e., I { ( e m , r i , e n ) | ( e m ,e n ) 2 r i , r i 2 R, e m 2 c c
Subset is the set of all subset constraints among relations in R i.e., Subset = { ( i,k ) : I I { (person, worksFor , company) } .

Mutex is the set of all mutual exclusion con-straints among relations in R i.e., Mutex = { ( i,k ) : I physiologicalCondition) } X  X  (drug, possiblyTreats , physiologicalCondition) } =  X  .

Each KB entity e more noun phrases (NPs). For example, the entity Cavaliers , can be referred to in text using either the NP  X  X leveland Cavaliers X  or the NP  X  X he Cavs X  3 . We define N responding to entity e We define SV O to be the English Subject-Verb-form ( np phrases (NP) corresponding to subject and object, respectively, v and w is the count of the tuple. 2.2 Data Construction We construct a dataset D for mapping English verbs to NELL KB relations. First, we convert each tu-ple in SV O to its equivalent entity pair tuple(s) in SV O 0 = { ( e m ,v p ,e n ,w ) | np s 2 N en ( e m ) , np N construct D from SV O 0 as a collection of labeled and unlabeled instances.
The set of labeled instances is D ` = { ( y is a bit vector of label assignment, each bit repre-senting whether the instance belongs to a particular relation i.e., y i 0 otherwise. v vector of verb phrase counts that connect e e v connects e
The collection of unlabeled instances is con-structed from entity pairs in SV O 0 whose label assignment y are unknown (its bits are all zero) i.e., D u = { ( y SV O 0 , ( e m , ,e n ) / 2 I R } .

An instance in our dataset d therefore either a labeled or unlabeled tuple i.e., d
We let f the instance i.e., f ( e
We let f phrases that co-occur with the instance in SV O 0 i.e., f
When applied to a relation r turn the set of all verb phrases that co-occur with instances in D whose types match that of the rela-tion i.e., f f 2.3 Model We train a Naive Bayes classifier on our dataset. Given as input a collection D ` of labeled instances fier,  X   X  , that takes an unlabeled instance and predicts its label assignment i.e., for each unlabeled instance d signment y
If the task is to classify the unlabeled instance into a single relation, only the bit of the relation with the highest posterior probability is set i.e, y k where k = arg max 2.3.1 Parameter Estimation
To estimate model parameters (the relation prior probabilities  X   X  abilities of a verb given a relation  X   X  P ( v we use an Expectation Maximization (EM) algo-rithm (Nigam et al., 2006). The estimates are computed by calculating a maximum a posteriori estimate of  X  , i.e.  X   X  = arg max arg max
The first term, P ( D |  X  ) is calculated by the prod-uct of all the instance likelihoods:
The second term, P (  X  ) , the prior distribution over parameters is represented by Dirichlet priors: P (  X  ) / and  X  the priors. In this paper we set  X  1 +  X  ( P e ( v of the verb-to-relation mapping. Thus, in this paper we define P (  X  ) as:
We can see from this that  X  ( P e ( v jugate prior on P ( v parameter. This conjugate prior allows incorpora-tion of any existing knowledge (Section 2.3.2) we may have about the verb-to-relation mapping.
From Equation 2, we see that log P ( D |  X  ) contains a log of sums, which makes a maxi-mization by partial derivatives computationally in-tractable. Using EM, we instead maximize the ex-pected log likelihood of the data with respect to the posterior distribution of the y labels given by: arg max
In the E-step, we use the current estimates of the pected label assignments according to the current model. In practice it corresponds to calculating the posterior distribution over the y labels for unlabeled instances P ( y i 1) and using the estimates to compute its expected
In the M-step, we calculate a new maximum the expected log likelihood of the complete data, L (  X  | D ;  X y t ) = log ( P (  X  t )) +  X y t [ log P ( D |  X 
L c (  X  | D ; y ) bounds L (  X  | D ) from below (by ap-plication of Jensen X  X  inequality E [ log ( X )]  X  log ( EX ) ). The EM algorithm produces parameter estimates  X   X  that correspond to a local maximum of L (  X  | D ; y ) . The relation prior probabilities are thus estimated using current label assignments as:
The verb-to-relation mapping probabilities are es-timated in the same manner:
We start with  X  = | V | and gradually reduce the impact of prior by decaying  X  with a decay parame-ter of 0.8 at each iteration in the manner of (Lu and Zhai, 2008)). This will allow the EM to gradually pick up more verbs from the data to map to relations.
EM iteratively computes parameters  X  1 , ...,  X  t us-ing the above E-step and M-step update rule at each iteration t , halting when there is no further improve-ment in the value of L 2.3.2 Prior Knowledge
In our prior P (  X  ) , we incorporate knowledge about verb-to-relation mappings from the text pat-terns learned by NELL to extract relations. This is our way of aligning our verb-to-relation mappings with NELL X  X  current extractions. Coupled Pattern Learner (CPL) (Carlson et al., 2010) is a component in NELL that learns these contextual patterns for ex-tracting instances of relations and categories.
We consider only CPL X  X  extraction patterns that contain verb phrases. Given a set E traction patterns for a relation r set of extraction patterns in E verb phrase v 2.3.3 Argument Type Checking
Although some verbs are ambiguous (e.g., the verb  X  X lay X  may express several relations: mu-sicianPlaysMusicalInstrument , athletePlaysSport , actorPlaysMovie , etc), knowing the types of the verbs X  arguments can help disambiguate the verbs (e.g., the verb  X  X lay X  that takes a musicalInstru-ment type as object is more likely to express the musicianPlaysMusicalInstrument relation). There-fore, we incorporate argument type checking in our EM process to ensure that it maps verbs to relations whose argument types match:  X  In the E-Step, we make sure that unlabeled  X  In the M-step, we make sure that verbs are 2.3.4 Incorporating Constraints
In the E-step, for each unlabeled instance, given the probabilities over relation labels P ( y i 1 Mixed-Integer Program (MIP) to produce its bit vec-
The constraints among relations are incorporated as constraints on bits in this bit vector. For exam-ple, if for an unlabeled instance ( Jeff Bezos , Ama-zon ), a bit corresponding to the relation ceoOf is set then the bit corresponding to the relation worksFor should also be set due to the subset constraint: ceoOf responding to competesWith should not be set due to the mutual exclusion constraint ceoOf  X  com-petesWith =  X  . The MIP formulation for each un-labeled instance thus tries to maximize the sum of probabilities of selected relation labels after penaliz-ing for violation of constraints (Equation 7), where  X  are slack variables for Mutex constraints:
Our algorithm that includes argument type check-ing and constraints is summarized in Algorithm 1. 2.4 Portuguese Verb Mapping To map Portuguese verbs to relations in Portuguese NELL, which is an automatically and independently constructed KB separate from English NELL, we use the Portuguese NELL and Portuguese text cor-pus SV O D a mapping of Portuguese verbs to relations. Since Portuguese NELL is newly constructed, it contains fewer facts (category and relation instances) than English NELL, and hence its dataset D ` labeled instances (see Table 1).
 Adding more relation instances to Portuguese NELL can result in more labeled instances in the dataset D verb-to-relation mapping. Since each category and each relation in Portuguese NELL ontology has a one-to-one mapping in English NELL ontology, we can add relation instances to Portuguese NELL from the corresponding English NELL relations.

English NELL however, has only English noun phrases (NPs) to refer to entities in its relation in-stances. To add more labeled instances in D ing English relation instances, we need to find in-stantiations of these English relation instances in Portuguese SV O tuguese NPs that refer to English NELL entities. For example, Portuguese NP:  X  X rtria torcica interna X  for English NELL entity: internal mammary artery .
To automatically translate English NELL enti-ties to Portuguese NPs, we use DBPedia (Auer et al., 2007) which has structured information about Wikipedia pages in many languages. The idea is to map each English NELL entity e sponding English DBPedia page and therefore its information of the Portuguese page in DBPedia: its title and label as the set of Portuguese NPs corre-sponding to the English entity, N
More specifically, for each English NELL en-tity e N that can refer to the entity. We do this by com-puting Jaccard similarities (Jaccard, 1912; Chap-man, 2009) of the entity X  X  NPs with titles and la-bels of English DBPedia pages. We select pages with Jaccard similarities of more than 0.6 as can-didates e.g., for English NELL entity Brad Pitt we find candidate English pages: http://dbpedia. org/page/Brad_Pitt ( Brad Pitt, the US actor) and http://dbpedia.org/page/Brad_Pitt_ (boxer) ( Brad Pit, the Australian boxer).

Then, we construct a graph containing nodes that are: (1) the NELL entity that we want to map to DBPedia, (2) its candidate DBPedia pages, (3) other entities that have relations to the entity in NELL KB, and (4) the candidate DBPedia pages of these other entities (see Fig. 2 for the NELL entity Brad Pitt ).
We add as edges to this graph: (1) the can-refer-to edges between entities in NELL and their can-didate pages in DBPedia (dashed edges in Fig. 2), (2) the relation edges between the entities in NELL KB (black edges), and (3) the hyperlink edges be-tween the pages in DBPedia (gray edges). In this graph we want to use the knowledge that NELL has already learned about the entity to narrow its candi-dates down to the page that the entity refers to. The idea is that relatedness among the entities in NELL implies relatedness among the DBPedia pages that refer to the entities. We use Personalized Page Rank (Page et al., 1999) to rank candidate DBPedia pages in this graph and pick the top ranked page as the page that can refer to the NELL entity.

For example, to find the DBPedia page that can refer to our NELL entity Brad Pitt , we use NELL X  X  knowledge about this entity to rank its candidate pages. As seen in Fig. 2, DBPedia page of Brad Pitt , the US actor ( dbpedia:br ad pitt ) is highly con-nected to other pages ( dbpedia:ang elina jolie , db-pedia:douglas pitt , dbpedia:usa ) that are in turn connected to the NELL entity Brad Pitt . dbpe-dia:br ad pitt is thus ranked highest and picked as the page that can refer to the NELL entity Brad Pitt .
Once we have an English DBPedia page that can refer to the NELL entity e responding Portuguese page from DBPedia. The ti-tle and label of the Portuguese page becomes the set of Portuguese NPs that can refer to the NELL en-tity i.e., N ing N lation instances in SV O stances in D than double the amount of relation instances, la-beled and unlabeled instances (Table 1) than Por-tuguese NELL. In the experiments, we observe that this translates to a better verb-to-relation mapping.
Mapping NELL to DBPedia is also useful because it can align existing knowledge and add new knowl-edge to NELL. For example, by mapping to DBPe-dia, we can resolve abbreviations (e.g., the NELL entity: COO as  X  X hief Operations Officer X  in En-glish or  X  X iretor de Operac  X   X  o es X  in Portuguese), or resolve a person entity (e.g., the NELL entity: Uta-maro as  X  X itagawa Utamaro X , the virtual artist). 3.1 Pre-processing For better coverage of verbs, we lemmatize verbs in the English SV O (using Stanford CoreNLP (Man-ning et al., 2014)). We lemmatize verbs in Por-tuguese SV O 2014)) and expand contracted prepositions.

For better precision and to make our method scale to a large text corpus, we focus on mapping verbs that are important for a relation based on how often the verbs co-occur with entity pairs that match the relation X  X  argument type. For each argument type in the English SV O we consider only the top 50 verbs (in terms of tf-idf scores) for mapping. We use tf-idf scores to adjust for the fact that some verbs appear more frequently in general. For each of these verbs, we also use only the top 50 entity pairs that co-occur with the verb in the SV O (in terms of co-occurrence counts) to construct our dataset D .
 For Portuguese verb-to-relation mapping, since SV O pt is much smaller than the English SV O (i.e., it contains only about 22 million entity pair-verb triples compared to the 600 million triples in the En-glish SV O ), we use all the Portuguese entity pairs and verbs for mapping. To adjust for the fact that some verbs appear more frequently in general, we use tf-idf scores instead of co-occurrence counts for the values of v 3.2 Evaluation We set aside 10% of D ` for testing. Given a test instance t dict the label assignment y simulates the task of relation extraction where we predict relation(s) that exist between the entity pair in
We compare predicted labels of these test in-stances to the actual labels and measure precision, recall and F1 values of the prediction. We evalu-ate NELL relations that have more than one labeled scribed in section 2.2). For experiments on the En-glish NELL, we evaluate 77 relations, with an aver-age of 23 (and a median of 11) training instances per relations. For experiments on the Portuguese relation instances from English NELL, we evaluate 85 relations, with an average of 31 (and a median of 10) training instances per relations. We compare the prediction produced by our approach: EM with that of other systems: CPL , DIRT , and NB .

In CPL , we obtain verb-to-relation mapping weights from NELL X  X  CPL patterns and hand-labeled verb phrases (see Section 2.3.2). In DIRT , we obtain verb-to-relation mapping weights in an unsupervised manner (Lin and Pantel, 2001) based on their mutual information over labeled training in-stances. In Naive Bayes ( NB ) we learn the verb-to-relation mapping weights from labeled training in-stances. In contrast to the other systems, EM allows learning from both labeled and unlabeled instances.
To make other systems comparable to our pro-posed method, For NB and DIRT we add CPL weights as priors to their verb-to-relation mapping weights. For all these other systems, we also incor-porate type-checking during prediction in that unla-beled instances are only labeled with relations that have the same argument types as the instance.
We show the micro-averaged performance of the systems on leaf relations of English NELL and Por-tuguese NELL (Fig. 3), where we do not incorpo-rate constraints and classify each test instance into a single relation. We observe in both English and Portuguese NELL that the verb-to-relation mapping obtained by EM results in predictions that have a much higher recall and comparable precision.
In Figure 3, we also observe a gain in performance Portuguese NELL enriched with relation instances from English NELL obtained using our DBPedia linking in section 2.4. More labeled instances results in higher recall and precision. This shows the useful-ness of aligning and merging knowledge from many different KBs to improve verb-to-relation mapping and relation extraction in general.

We show the micro-averaged performance of the systems on all relations of English NELL and Por-tuguese NELL (Fig. 4). Here, we incorporate hi-erarchical and mutual exclusive constraints between relations in our EM , allowing a test instance to be classified into more than one relation while respect-ing these constraints. Like before, we observe that the verb-to-relation mapping obtained by EM results in predictions with a much higher recall and compa-rable precision to other systems which do not incor-porate constraints between relations.

In the experiments we also observe that NB per-forms comparably or better than DIRT . We hypothe-size that it is because NB obtains its verb-to-relation mapping in a supervised manner while DIRT ob-tains its mapping in an unsupervised manner.
We also conduct experiments to investigate how much influence type-checking has on prediction. We show performance over instances whose types alone are not enough to disambiguate their assignments (i.e., when more than one relation shares their ar-gument type signatures) to see the merits of verb-to-relation mapping on prediction (Fig. 5). We observe that verbs learned by EM results in a better predic-tion even when used without type-checking ( EM (-) Type ) than using type-checking alone (by picking majority class among relations that have the correct type) ( Type Only ). Adding type checking improves performance even further ( EM ). This shows how verbs learning is complementary to type-checking.
The results of our experiments also highlight the merit of learning from a large, though unlabeled cor-pus to improve the coverage of verb-to-relation map-ping and hence the recall of predictions. We also observe the usefulness of incorporating constraints and for merging knowledge from multiple KBs to improve performance. Another advantage of EM is that it produces relation labels for unlabeled data not yet in NELL KB. We show some of these new pro-posed relation instances as well as some of the verb-to-relation mapping obtained by EM (Table 3). EM learns in average 177 English verbs and 3310 Portuguese verbs per relation; and propose in av-erage 1695 new instances per relation for English NELL, and 6426 new instances per relation for Por-tuguese NELL. It learns less English verbs than Por-tuguese due to the filtering of English data (Sec-tion 3.1) and a high degree of inflection in Por-tuguese verbs. The smaller size of Portuguese KB also means more of its proposed instances are new. Existing verb resources are limited in their ability to map to KBs. Some existing resources classify verbs into semantic classes either manually (e.g. WordNet (Miller et al., 1990)) or automatically (e.g. DIRT (Lin and Pantel, 2001)). However, these classes are not directly mapped to KB relations. Other re-sources provide relations between verbs and their ar-guments in terms of semantic roles (e.g. PropBank (Kingsbury and Palmer, 2002), VerbNet (Kipper et al., 2000), FrameNet (Ruppenhofer et al., 2006)). However, it is not directly clear how the verbs map to relations in specific KBs.

Most existing verb resources are also manually constructed and not scalable. A verb resource that maps to KBs should grow in coverage with the KBs, possibly by leveraging large corpora such as the Web for high coverage mapping. One system that leverages Web-text as an interlingua is (Wijaya et al., 2013). However, they use it to map KBs to KBs, and obtain a verb-to-relation mapping only in-directly. They also compute heuristic confidences in verb-to-relation mappings from label propagation scores, which are not probabilities. In contrast, we map verbs directly to relations, and obtain P ( v as an integral part of our EM process.

In terms of systems that learn mappings of tex-tual patterns to KB relations, CPL (Carlson et al., 2010) is one system that is most similar to our pro-posed approach in that it also learns text patterns for KB relations in a semi-supervised manner and uses constraints in KB ontology to couple the learning to produce extractors consistent with these constraints. However, CPL uses a combination of heuristics in its learning, while we use EM. In our experiments, we use CPL patterns that contain verbs as priors and show that our approach outperforms CPL in terms of effectiveness for extracting relation instances.
In terms of the relation extraction, there are distantly-supervised methods that can produce verb groupings as a by product of relation extraction. One state-of-the-art uses matrix factorization and univer-sal schemas to extract relations (Riedel et al., 2013). In this work, they populate a database of a uni-versal schema (which involves surface form predi-cates and relations from pre-existing KBs such as Freebase) by using matrix factorization models that learn latent feature vectors for relations and entity tuples. One can envision obtaining a verb group-ing for a particular relation by predicting verb sur-face forms that occur between entity tuples that are instances of the relation. However, unlike our pro-posed method that learns mapping between typed-verbs to relations, they do not incorporate argument types in their learning, preferring to learn latent en-tity representation from data. Although this im-proves relation extraction, they observe that it hurts performance of surface form prediction because a single surface pattern (like  X  X isit X ) can have mul-tiple argument types (person-visit-location, person-visit-person, etc). Unlike our method, it is not clear in their method how argument types of surface pat-terns can be dealt with. Furthermore, it is not clear how useful prior constraints between relations (sub-set, mutex, etc.) can be incorporated in their method. In this paper, we introduce an EM-based approach with argument type checking and ontological con-straints to automatically map verb phrases to KB re-lations. We demonstrate that our verb resource is effective for extracting KB relation instances while improving recall; highlighting the value of learn-ing from large scale unlabeled Web text. We also show the flexibility of our method. Being KB-, and language-independent, our method is able to con-struct a verb resource for any language, given a KB and a text corpus in that language. We illustrate this by building a verb resource in Portuguese and in En-glish which are both effective for extracting KB rela-tions. Future work will explore the use of our multi-lingual verb resource for relation extraction by read-ing natural language text in multiple languages. We thank members of the NELL team at CMU and Federal University of Sao Carlos for their helpful datasets, comments, and suggestions. This research was supported by DARPA under contract number FA8750-13-2-0005.

