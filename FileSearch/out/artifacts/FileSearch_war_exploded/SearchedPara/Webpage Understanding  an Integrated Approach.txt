 Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and label-ing HTML elements. However, how to effectively segment and label the text contents inside HTML elements is still an open problem. Since many text contents on a webpage are often text fragments and not strictly grammatical, tra-ditional natural language processing techniques, that typi-cally expect grammatical sentences, are no longer directly applicable. In this paper, we examine how to use layout and tag-tree structure in a principled way to help under-stand text contents on webpages. We propose to segment and label the page structure and the text content of a web-page in a joint discriminative probabilistic model. In this model, semantic labels of page structure can be leveraged to help text content understanding, and semantic labels of the text phrases can be used in page structure understand-ing tasks such as data record detection. Thus, integration of both page structure and text content understanding leads to an integrated solution of webpage understanding. Exper-imental results on research homepage extraction show the feasibility and promise of our approach.
 I.5.1 [ Pattern Recognition ]: Models -Statistical Algorithms, Experimentation  X  This work is done when the author is visiting Microsoft Research Asia.
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Text Processing, Webpage Understanding, Conditional Ran-dom Fields
The World Wide Web is a vast and rapidly growing repos-itory of information, and various kinds of valuable seman-tic information are embedded in webpages. Some basic understanding of the semantics of webpages could signifi-cantly improve people X  X  browsing and searching experience. For example, in our Windows Live Product Search project (http://products.live.com), we automatically extract struc-tured product information from the web using a template-independent approach to segmenting webpages and labeling HTML elements [29]. By presenting the HTML elements according to their semantic meaning, users could save time from sifting the information from thousands of webpages.
However, little work has been carried out on segmenting and labeling the text contents inside the HTML elements of a webpage. In this paper, we study how to use layout and tag-tree structure in a principled way to help understand text contents on webpages.
We have been developing Libra (http://libra.msra.cn), an object-level academic search engine, to help scientists and students locate research materials. One of the biggest chal-lenges we are facing in Libra is how to understand researchers X  homepages better. On a researcher X  X  homepage (see Fig. 1 for an example), two types of information are important. The first one is the contact information of the researcher, such as name, address, email, and phone, and the second one is the academic information including the academic ti-tle, affiliation, academic activities, and publications. If we could identify all the related information from every re-searcher X  X  homepage, Libra could become the most compre-hensive database about researchers in the world.

However, building a webpage understanding model to iden-tify this information from every researcher X  X  homepage is non-trivial: 1. Webpages are highly heterogeneous. Rule-based web-Figure 1: The homepage of David Heckerman con-tains his contact information (name, address, and email) and academic information (title, affiliation, papers and academic activities).
 2. The attribute values of a researcher are presented in 3. The text content of a single HTML element could con-
Thus, an effective webpage understanding model should be template-independent, and the segmentation and label-ing of both page structure and text fragments are required. Existing work on web mining and natural language process-ing is ineffective due to the following two reasons.
Firstly, existing work on web mining mainly focus on page layout and format analysis using rule-based pattern mining approaches [1][8][26][27]. Little work has been done to effec-tively process the text fragments except some wrapper-based (or rule-based) approaches [14][17]. However, wrapper-based approaches could not solve our problem because they are template-dependent and could only work for webpages gen-erated by the same template.

Secondly, existing work on natural language processing cannot be directly applied to web text understanding. Be-cause the text contents on webpages are often not as regular as those in natural language documents and many of them are less grammatical text fragments. One possible method of using NLP techniques for web text understanding is to first manually or automatically identify logically coherent data blocks, and then concatenate the text fragments within each block into one string via some pre-defined ordering method. The concatenated strings are finally put into a text process-ing method, such as CRYSTAL [22] or Semi-Markov Mod-els [20], to identify target information. [22][9] are two at-tempts in this direction. This type of pre-processing based approaches has several disadvantages:
Ineffectiveness in Rule-based Webpage Segmenta-tion : The segmentation of logically coherent data blocks is non-trivial for rule-based approaches because of the diver-sity of webpages. It is demonstrated in [29] that de-coupled approaches to detecting data records without semantics of the contents are highly ineffective.

Insufficient Grammar for NLP Systems : Even if the logically coherent data blocks could be identified correctly, the parsed text fragments within these blocks are still lack of grammars. For example, the concatenation of an anchor text with a phone or an email is apparently of no meaning in NLP systems, which typically expect grammatical sentences.
Loss of Structure and Boundary in Concatena-tion : The concatenation removes or softens the boundaries of different text fragments. More importantly, it also re-moves structure formats of the HTML elements such as two-dimensional layout information and hierarchical orga-nization. These structural patterns have been shown to be very useful for page structure understanding [29][28].
Now, a natural question is why there is no existing work to effectively incorporate the work from the web mining com-munity and the natural language processing community. We believe that the answer lies in the fact that most existing page structure understanding methods are rule-based but statistical models have been the theme of text processing over the last decades [18]. For this reason, existing page structure understanding methods cannot be easily merged with statistical NLP methods in a principled manner.
Our recent work has shown that statistically regular de-pendency patterns among the HTML elements of a webpage, such as two-dimensional [29] and hierarchical [29] dependen-cies, are ubiquitous. Thus, sophisticated probabilistic mod-els, such as Hidden Markov Models or Conditional Random Fields [15], can be developed to exploit these useful depen-dencies for effective webpage structure understanding. How-ever, in these approaches, we only assign semantic labels to HTML elements, and do not segment and label the text content within an HTML element.

In this paper, we study how to explore the structural dependency patterns as shown in [29] to help understand text fragments in a principled way. Instead of using simple heuristic-based pre-processing, we propose a joint webpage understanding model to do structure understanding and text content understanding simultaneously. The joint model is an integration of Hierarchical Conditional Random Fields (HCRFs) [29] and Semi-Markov Conditional Random Fields (Semi-CRFs) [20]. It can be seen as an undirected gener-alization of the Switching Hidden Semi-Markov Model (S-HSMM) [10]. Two differences exist between S-HSMM and our model. First, our model is discriminative but S-HSMM is generative. Generally, discriminative models can incor-porate arbitrary features of the observations, but generative models must make some strong independence assumption to achieve inference tractability. Second, the S-HSMM used in [19][10] is a two-layer hierarchical model. Thus, it is not suf-ficient for webpage understanding as webpages are arbitrary hierarchical trees.

As our model performs both page structure understand-ing and text content understanding together, it can take raw webpages as inputs and identifies the desired information if exists. Thus, it is an integrated solution of webpage under-standing. In contrast, [22][9] are de-coupled approaches.
Specifically, we make the following contributions: 1. We are the first to incorporate both structure and text 2. We present an undirected graphical model which is 3. An empirical study of our model on the task of home-
The rest of the paper is organized as follows. In the next section, an overview of our approach is presented to help un-derstand how the new approach works. Section 3 formally describes the proposed model. Section 4 presents our empir-ical studies and discussions. Section 5 discusses related work and section 6 brings this paper to a conclusion. Finally, we give our acknowledgements in section 7.
For webpage understanding, choosing a good representa-tion of webpages is important. Tag-trees, which are nat-ural representations of the tag structures, are commonly used in the literature. However, tag-trees tend to reveal presentation structure rather than content structure. We need a webpage representation which can effectively keep related contents together while separating semantically dif-ferent blocks. As shown in [29], vision-trees are the best representation available for webpage understanding. Vision-trees are built using a vision-based page segmentation ap-proach, which makes use of page layout features such as font, color, and size to keep HTML elements with related contents together. We use the vision tree of a webpage as its representation format. Each node on a vision-tree repre-sents a data region (or a block) in the webpage. The root block represents the whole page. The leaf blocks are the HTML elements of the webpage. Each inner block is the ag-gregation of all its child blocks. The individual tokens inside the text leaf nodes (i.e. text fragments) are atomic units for text content understanding.

Based on the vision-tree, we propose a joint model by in-tegrating HCRFs [29] and Semi-CRFs [20] to explore struc-tural regularities to help understand text fragments. Fig. 2 shows the model structure. At coarse levels (i.e. the blocks of the vision-tree) it is a full HCRF model for page struc-ture understanding as in [29], and at the finest level (i.e. the text contents of the leaf nodes) a Semi-CRF model is Figure 2: Graph of the joint model. The upper part is an HCRF model for structure understanding, and the models in triangles descending from leaf nodes are Semi-CRF models for text understanding. introduced for segmenting and labeling the text fragment of each HTML element. To be integrated with the upper hier-archical model, the label assignments of leaf nodes must be incorporated into the segmentation 1 of a text fragment in the Semi-CRF models, that is, for Semi-CRF models a label assignment is a combination of both the segmentation of text fragments and the label assignments of the variables at leaf nodes. We will refer to these Semi-CRF models as extended Semi-CRF models in our joint approach. We assume that the Semi-CRF models at different leaf blocks share the same set of feature functions and weights. We show that although the model is an integration of two different types of mod-els, the maximum likelihood estimation can be carried out separately. Thus, existing algorithms are sufficient for the training. The most likely label assignments of the variables in HCRF and the most likely segmentation and labeling of text fragments can be found jointly. However, exact joint algorithms can be too expensive for large-scale webpage un-derstanding. Alternatively, we adopt an efficient de-coupled approximate method by first doing structure understanding and then doing text understanding with the extended Semi-CRF models. In structure understanding the effectiveness of leveraging semantic labels in both directions has been demonstrated in [29]. Here, we focus on studying the effec-tiveness of leveraging semantic labels of HTML elements in text understanding.

An Illustration Example : In Fig. 1, the publication information is grouped into small data regions or paper records. Take the first paper record as an example. All the author names are presented in the first HTML element, and the title of the paper is in the second element. The publica-tion year and conference are presented in the third element. Traditional approaches [22][9] first detect the paper record, and then process the concatenated text string of all the text elements within the paper record. The shortcomings of these methods have been discussed previously. In contrast, our approach takes an integrated procedure to identify paper records, assign semantic labels to HTML elements, and fur-ther segment text contents into purified attributes. In our model, extended Semi-CRF models are aware of the seman-tic labels of the text elements when doing segmentation and labeling. This will help identify the boundary of each at-tribute. For example, if the first element  X  X . Heckerman, C. Kadie, and J. Listgarten. X  is labeled as containing only author names by the HCRF, then the extended Semi-CRF can leverage this semantic information and easily identify the boundary of each author name. The semantic labels of HTML elements can be accurately assigned with the help of see section 3.1.2 for the formal definition.
 structure understanding as shown in [29]. Thus, structure understanding can help text content understanding.
Note that we choose Semi-CRFs for text understanding, although other models like Hidden Markov Models are pos-sible. This is because, Semi-Markov models have been ex-amined in [5][20] and demonstrated to be among the most promising methods for text segmentation and labeling, espe-cially for their great power in incorporating segment-based features. Furthermore, discriminative models generally have great flexibility in encoding arbitrary useful features for in-formation extraction compared to generative models.
In this section, we first introduce some basic concepts of Conditional Random Fields [15] and their extensions -HCRFs [29] and Semi-CRFs [20]. Then, we present a joint model for integrated webpage understanding. We show that the parameter estimation of the joint model can be per-formed independently. We present an exact algorithm and its efficient approximation to find the maximum a posterior assignment of all the variables in the joint model. In general, Conditional Random Fields (CRFs) are Markov Random Fields that are globally conditioned on observa-tions. Let G = ( V,E ) be an undirected model over a set of random variables Y and X . X are variables over the observations to be labeled and Y are variables over the cor-responding labels. Y can have non-trivial structures, such as linear-chain [15] and 2D grid [28]. The conditional distri-bution of a label assignment y (an instance of Y ) given the observations x (an instance of X ) has the form where C is the set of cliques in G ; y | c are the components of y associated with the clique c ;  X  c is a potential function defined on y | c and takes non-negative real values; Z (x) = P y Q c  X  C  X  c (y | c , x) is the normalization factor or partition function. The potential functions are expressed in terms of feature functions f k (y | c , x) and their weights  X  k
Given a set of training samples D = { (y i , x i ) } N i =0 eter estimation is to choose the feature functions X  weights to maximize the likelihood of the set of training samples. For linear-chain CRFs [15], this task can be efficiently done with dynamic programming algorithms.
An HCRF model (HCRFs) [29] is a CRF model but with the variables indexed by the vertices of a hierarchical graph. The probability distribution has the same form as in equa-tion (1). Compared with the traditional linear-chain model, an HCRF model has a different set of cliques, and thus has a different set of feature functions. For HCRFs, triangles are the maximum cliques, and feature functions defined on these cliques encode the dependencies among parent variables and their children. Thus, it can capture the dependencies be-tween the variables at adjacent levels of the graph in Fig. 2. Via the inter-level dependencies and the dependencies of the variables at the same level, HCRFs provide a way to in-corporate long distance dependencies for accurate structure understanding. Another advantage of this model is that by using the standard junction tree algorithm, it is very effi-cient to do parameter estimation and to find the maximum a posterior label assignment. In fact, the algorithm is linear in terms of the number of elements.
A Semi-CRF model (Semi-CRFs) [20] is another extension of the linear-chain CRFs for sequence data segmentation and labeling. Here, we use the same notations as in [20]. In this model, x is a token sequence and | x | is the sequence X  X  length (i.e. number of tokens). The vector s =  X  s 1 ,s 2 ,...,s segmentation of x, and each entry is a segment which is a triple s i =  X  t i , X  i ,y i  X  with t i as a start position,  X  position, and y i as the label of this segment. Thus, a seg-ment s i means that the label y i is assigned to all the observa-tions between the start position t i and the end position  X  the observation sequence x. It is reasonable to assume that segments have positive lengths and adjacent segments touch, that is, 0  X  t i  X   X  i  X  | x | and t i +1 =  X  i + 1. Let g feature function, and it depends on the current segment, the whole observation, and the label of previous segment, that is, g k ( i, x , s) = g k ( y i  X  1 ,y i ,t i , X  i , x). Let g =  X  g be a vector of feature functions and W =  X  w 1 ,w 2 ,...,w be the corresponding weight vector. As a Conditional Ran-dom Field model, the probability distribution p (s | x) is also of the form as in equation (1) but with the traditional label assignment y replaced by a segmentation s and the cliques are replaced by segments:
For Semi-CRFs, parameter estimation and finding the maximum a posterior segmentation can be efficiently carried out via a dynamic programming algorithm. The computa-tional complexity is a constant factor more than that of the traditional linear-chain model when the maximum length of the segments is assumed to be fixed. Recent work in [21] shows that the computational complexity could be further reduced by defining succinct potentials.
Now, we present the integrated webpage understanding model. The model X  X  graph is shown in Fig. 2. The up-per part is a full hierarchical model with M + 1 nodes, and at each leaf node on the vision-tree a Semi-CRF model is introduced for text segmentation and labeling. For the hi-erarchical model, we use rectangles to denote inner nodes and use ellipses to denote leaf nodes. Each node on the graph is associated with a random variable Y i , and all the variables Y = { Y i } M i =0 are organized in a hierarchy. A la-bel assignment of these variables y = { y i } M i =0 is organized in a hierarchy as Y . We partition the set into two subsets: leaf nodes; Y p,j are variables at inner nodes; and L + 1 is the number of leaf nodes.

To be integrated with the upper hierarchical model, ex-tensions must be made to Semi-CRF models. In the next two sections, we first describe the extensions of Semi-CRF models to incorporate structure understanding when doing text segmentation and labeling, and then present a joint model to integrate the two parts together.
In order to be integrated with the upper hierarchical model, the label assignments of the variables at leaf blocks (or leaf variables for short) must be incorporated into the segmenta-tion of text fragments in the lower Semi-CRF models, that is, for Semi-CRF models a label assignment is a combina-tion of both the segmentation of text fragments and the label assignments of the leaf variables. In this way, the up-per hierarchical model and the lower Semi-CRF models are integrated via the leaf variables. This is the key difference from the standard Semi-CRF model [20]. We will refer to these Semi-CRF models as extended Semi-CRFs in our joint approach. Here, we assume that the extended Semi-CRFs at different leaf blocks are conditionally independent given the leaf variables at which they are located. We also assume that the Sem-CRF models at different leaf blocks share the same set of feature functions and parameters.

Now, we take one leaf block as an example to formally define the extended model. For the leaf variable Y l,i , the extended Semi-CRF model is defined as follows. The obser-vation sequence x l,i is the text fragment at the leaf block. Let s i =  X  s i, 1 ,s i, 2 ,...,s i,n i  X  denote a segmentation of x Here, each segment is an extension of the segment of the standard Semi-CRF model to incorporate the label assign-ment of the leaf variable: s i,j =  X  t i,j , X  i,j ,y i,j ,y t i,j is a start position;  X  i,j is an end position; y i,j is the label of this segment; and y l,i is the label of the leaf variable Y l,i . We will call these leaf labels y l,i as supper labels as opposed to the labels y i,j used within the extended Semi-CRFs. Correspondingly, the feature functions for the ex-tended Semi-CRFs are also dependent on the labels of leaf variables. Let g k be a feature function. It maps a triple ( j, x , s i ) to a real value. Here, we assume that it depends on the current segment, the whole observation, the label of previous segment, and also the label of the leaf variable Y the vector of feature functions and W be the corresponding weight vector as defined before. Then, the conditional prob-ability of segmentation p (s i | x) has the same form as in (2) but with the original segmentation replaced by an extended one and the feature functions are replaced correspondingly. Now, for the joint model we define S =  X  s 0 , s 1 ,..., s be the segmentations of all the leaf blocks. Then, an assign-ment of all the variables in the joint model is a pair  X  y , S  X  where y is the label assignment of the upper hierarchical model and S is the segmentation assignment of the extended Semi-CRFs. A valid assignment  X  y , S  X  must satisfies the con-dition that the two assignments match at the leaf variables, that is, the label assignments of the leaf variables from both the upper hierarchical model and the lower extended Semi-CRF models are the same: s i  X  y l,i = y  X  y l,i , 0  X  i  X  L . In the following, we will use  X  y , S  X  to denote a valid assignment without further explanation.

Then, the joint probability distribution of our model has the following factorization form p (  X  y , S  X  X  x) = p (y | x) p (S | x , y) = p (y | x) Y where the first equation is for the chain rule, and the last equation is for the conditional independency assumption that given the label of a leaf variable, the segmentation as-signment of that leaf variable is independent from the label assignment of other variables. We shall see that this factor-ized distribution will lead to an efficient separate parameter estimation algorithm. The joint model can be viewed as an undirected generalization of the Switching Hidden Semi-Markov Model [10] since it can be viewed as the concatena-tion of many Semi-CRF models and the leaf variables act as switch variables. In equation (3), each part can be com-puted efficiently. For the hierarchical model, the conditional probability is expressed by the feature functions as in [29] p (y | x) = 1 where g k , f k , and h k are feature functions defined on three types of cliques (i.e. vertex, edge, and triangle) respectively;  X  ,  X  k , and  X  k are the corresponding weights; v  X  V , e  X  E , and t is a triangle. Z h (x) is the normalization factor of the hierarchical model.

The conditional probability for the extended Semi-CRF model: and the normalization factor is
For the joint model, each training sample in D is a pair (  X  y , S  X  i , x i ) and the log-likelihood function is ter vector of the hierarchical model and W is the parameter vector of the extended Semi-CRF models.

Substitute the distribution in (3) into the log-likelihood and we get L ( X  , W) = X The last equation is due to the fact that parameters  X  and W are independent. Thus, the maximization of L ( X  , W) is equivalent to the maximization of L h ( X ) and the maximiza-tion of L s (W).
 Now, we can perform the parameter estimation for the HCRF model and the extended Semi-CRF models sepa-rately. Here, we use the algorithm in [29] to train the hier-archical model. For the extended Semi-CRF model, similar dynamic programming algorithms as in [20] can be used to learn the parameters, but with the segmentation replaced by the extended one in order to incorporate supper labels . To compute the normalization factor Z i (x ,y l,i ), forward vectors can be recursively defined as  X  ( j,y,y l,i ) = X with the base case  X  (0 ,y,y l,i ) = 1. Here, M is the maximum segment length [20]. The normalization factor is Z i (x ,y P y  X  ( | x l,i | ,y,y l,i ). Similarly, we define the recursion as = X where  X  ( y 0 ,y,y l,i ,j  X  d,j, x) =  X  k ( j  X  d,y 0 ,y l,i ) +  X  ( j  X  d,y 0 ,y l,i ) g k ( y
Then, the expectation of feature function g k with respect to the model distribution can be computed using the same formula as in [20] but with the normalization factor and  X  ( j,y ) replaced by Z i (x ,y l,i ) and  X  k ( j,y,y l,i ) respectively. To avoid over-fitting, the spherical Gaussian prior with mean  X  = 0 and variance matrix  X  =  X  2 I is used to penalize the log-likelihood when training each part of the model.
For webpage understanding, the target is to find the best assignment of the variables in the model, that is, the pair  X  y , S  X  that has the maximum posterior probability. We have shown that parameter estimation can be performed inde-pendently for two different parts without loss of accuracy. But finding the maximum a posterior assignment is not the case. This is because unlike training webpages, there are no  X  X rue X  labels assigned to the leaf nodes of a testing webpage. Thus, all the possible assignments to a leaf variable must be computed.

Based on the junction tree algorithm [29], we can develop a joint optimization algorithm to find the most likely assign-ment. We can take the same procedure as in [29] to construct a junction tree for the upper hierarchical model, and then propagate messages on the constructed junction tree using the two-phase schedule algorithm [13]. Before running the schedule algorithm, the messages from the extended Semi-CRF models must be integrated in order to incorporate the effects of text content understanding into structure under-standing. Here, the local messages at the leaf variable Y are the normalization factor Z i (x ,y l,i ) which can be recur-sively computed using the forward algorithm as above. The local messages are incorporated by multiplying them into the initial potentials on the constructed junction tree. After initialization, the two-phase schedule algorithm runs to find the most likely label assignments of all the variables at inner nodes. At the end of the schedule algorithm, the marginal potentials of the leaf variables are incorporated into the ex-tended Semi-CRF models to find the best label assignments of the leaf variables and also the best segmentation of text fragments. This can be done using Viterbi algorithm by defining the recursion V ( j,y,y l,i ) = max with the base V (0 ,y,y l,i ) = 0. Then, the most likely label assignment of the leaf variable is the label y ? l,i that achieves the highest value max y,y l,i ( V ( | x l,i | ,y,y l,i )+  X  ( y best segmentation is the path traced by max y V ( | x l,i where  X  ( y l,i ) is the marginal potential of the variable at the end of the two-phase algorithm.

However, the joint optimization algorithm can be too ex-pensive for large-scale webpage understanding because for each label assignment of a leaf variable the dynamic pro-gramming algorithm must be run twice to collect messages and to find the best segmentation of its content. An alterna-tive efficient approximate method is to find the maximum a posterior assignment of two parts separately. First, the most likely assignment of the variables in the upper hierar-chical model is found using the junction tree algorithm [29]. Then, leaf blocks together with supper labels are further segmented using the extended Semi-CRF models. During the segmentation and labeling of text fragments, the supper labels of leaf blocks are fixed. Let y ? l,i be the most likely label assignment of the leaf variable Y l,i at the end of the first step, we compute the following recursion V ( j,y,y ? l,i ) = max with the base V (0 ,y,y ? l,i ) = 0. Then, the best segmentation is the path traced by max y V ( | x l,i | ,y,y ? l,i ).
In this approximate algorithm, although the segmenta-tion of text contents is not considered when doing structure understanding, the semantic labels assigned by the upper hierarchical model are considered to help text understand-ing. One advantage of this approximate algorithm is that after the first step of structure understanding, most of the text fragments are labeled as containing no interested infor-mation. These non-informative fragments can be kept away from further segmentation and labeling. This simple heuris-tic could significantly reduce the time complexity because most contents on webpages contain no interested informa-tion but act as decorations, supplements or something else.
In this section, we report empirical results by applying our proposed model to understand web text fragments and iden-tify structured information from researchers X  homepages. We compare our proposed model with a sequential approach that combines the state-of-the-art algorithms in record de-tection and text segmentation. The results show that our model achieves significant improvements in the final under-standing of text fragments through leveraging the semantic labels of page structures. We also study the effects of NLP features and database features if they are available. Although some datasets like the Job corpus [3] and the Address and Paper corpora [2] have been evaluated in pre-vious work, they are raw text documents and do not have HTML structures. Thus, they are not suitable for our evalu-ation since our focus is on both structures and text contents. Also our work is different from [2][5], in which the inputs are some pre-identified segments of a webpage such as address records and paper records which are treated as string se-quences in their experiments. In our method, we take raw pages as inputs and automate both the identification of ad-dress and paper records and the segmentation and labeling of the text fragments.
 As we have stated, extracting structured information from researchers X  homepages needs both structure understanding and text content segmentation and labeling. So, we eval-uate our models on this task. We setup our dataset with 292 homepages of computer science researchers. We iden-tify both academic and contact information of a researcher. For contact information, we identify his/her Name , address (including Street , City , State , and Zip code ), Phone , and Email . For academic information, we extract his/her aca-demic Title (like professor, lecturer, and etc.), Affiliation , and publications. For each paper, we identify Paper Title , Author name, Publisher type (like conference, workshop, journal, PhD thesis, and etc.), and publication Year .
The 292 homepages are randomly downloaded from the computer science departments of about 10 American Uni-versities including Stanford, MIT, CMU, and UC Berkeley, and also some homepages are from research labs like Bell labs, Microsoft Research, IBM Research, and etc. All the pages are manually labeled and all the structured informa-tion is segmented. Statistics of the dataset is shown in the first row of Table 1. You may note that the number of owner names is less than the number of homepages. This is because there are four names presented in images, and one page without any explicit name.
To our knowledge, there is no complete solution for web-page understanding (both structure understanding and text understanding). The heuristic-based pre-processing meth-ods [9][22] are not appropriate baseline methods because their rules are used for specific types of problems, and it is difficult to develop an optimal set of rules in our exper-iments. Here, we build our baseline method by first us-ing the most recently developed webpage segmentation and labeling method [29] to identify interested text fragments and then applying Semi-CRF models [20] to segment text fragments and identify structured information. When being input into Semi-CRF models, text fragments are concate-nated into strings as in [20]. Instead of concatenating all the text fragments on a webpage into one single string, which is apparently inefficient, we concatenate the fragments within each paper record as one string. The text fragments re-lated to Street , City , Region , Zip code , Email , and Phone are concatenated together, and the other text fragments includ-ing the owner X  X  Name , Title , Affiliation and non-informative text fragments are concatenated together. We refer to this method as sequential baseline method in the sequel.
As we defined in [29], there are two types of label spaces for HCRFs: leaf label space and inner label space. The leaf label space is used for labeling the leaf nodes of the vision-tree. It consists of all the target attributes, and the combi-nations: Name and Title ; Title and Affiliation ; Name, Title and Affiliation ; City and Region ; Region and Zip code ; City, Region and Zip code ; Paper Title and Author ; Author and Publisher ; Paper Title and Publisher ; Publisher and Year ; Paper Title, Author and Publisher . Note that here we in-clude the combinations in leaf label space. This is because the text fragments which are labeled by HCRFs can contain multiple attributes. The inner label space is used for label-ing the inner blocks of the vision-tree. It consists of Paper Record , Paper List , Note block , and all the above combina-tions. For the extended Semi-CRF models, there are also two types of label spaces: supper label space and attribute label space. The attribute label space consists of the names of the attributes we are interested in. The supper label space is the same as the leaf label space of the HCRF model. When finding the maximum a posterior segmentation and labeling using the extended Semi-CRF models, the upper bounds on segment lengths are dependent on the supper labels. For Title , Affiliation , Street , Paper Title , and Publisher , the up-per bounds (ranging from 10 to 14) are larger than those for Name , City , State , Zip code , Email , Phone , Author , and Year (ranging from 2 to 4).

For each attribute, the standard P recision, R ecall and F1 measure are evaluated.
In our experiment, the dataset is partitioned into two sub-sets: 50 percent for training and 50 percent for evaluation. For the training of the HCRF model and the extended Semi-CRF model, spherical Gaussian priors are used with the same standard variance 5.

The performance is shown in Table 1. We can see that the integrated model significantly outperforms the sequen-tial baseline method on all the attributes except Zip code , City , Region , Phone , and Year . The performance on Zip code is almost the same, and the baseline method can perform well on City , Region , and Phone . This is because for these attributes, text patterns are usually very distinctive, so the baseline method can perform well even without considera-tion of where they appear in a webpage. Also relatively clean presentation of contact information in homepages makes it easy to distinguish useful information from noise. For Year , after the first step of identifying paper records, they can be easily and accurately identified with the baseline method.
However, for other attributes no simple features can be used to effectively identify them. For example, both Title and Affiliation can have similar patterns (such as the first letters of words are all capitalized, in bold font, and etc.) as those of Name . For paper related attributes, Publisher can appear like a Paper Title , and Author can appear in front of or after Paper Title . This leads to more ambiguities when identifying each of them. In these cases, structural layout patterns are more helpful. In our proposed model, the upper hierarchical model can effectively incorporate hi-erarchical dependency patterns and also long distance de-pendencies to understand the semantics of text fragments. Then, the semantic labels assigned by upper HCRF model are incorporated into the lower extended Semi-CRF mod-els when doing segmentation and labeling of text fragments. This procedure can help the final identification of our inter-ested attributes. For example, if the HCRF tells that a text fragment is a list of author names even though it doesn X  X  tell where the boundaries between different author names are, then it will be much easier for the lower Semi-CRF model to identify the unknown boundaries.

From the results, we can also see that in researchers X  homepages, Name s are much easier to be found compared with Title and Affiliation . So the sequential baseline method can perform well in identifying Name . The lower perfor-mance on Email compared to that on City , Region , and Zip code is due to the fact that researchers often make their email addresses unreadable to automated programs. For Ti-tle , Affiliation , Region , and Zip code , the baseline method achieves higher recall and lower precision compared to the integrated model. This is because there are usually some false-positives on webpages which are wrongly detected by the baseline method because of its lack of structural infor-mation. But the integrated model can identify this noise information. Of course, some true-positives are missed by the integrated model. As we shall see in the next section, another part of the improvements is from the incorporation of noun phrase chunking.

Other reasons for the promising performance of our model include the incorporation of long distance dependencies in HCRFs [29] and also global features that are extracted from the alignment of the paper records in the same page. The alignment is based on the observation that each researcher always presents his publications in one similar pattern, al-though the patterns may be different for different researchers.
Much work has been done to investigate the usability of shallow or deep linguistic structures for various application tasks such as named entity extraction and language chunk-ing [7]. In contrast to deep natural language processing, shallow NLP techniques are more robust and more efficient. This is very important for scalable webpage understanding. Thus, in our experiment we incorporate the noun phrase chunking (NP-Chunking) results of a fast chunk parsing method [25]. All the text fragments on the homepages in our dataset are parsed using the method [25]. To control the bad effects of incorrect chunking, we only use the noun phrases that are at the finest level of the parsing trees [25]. We treat the tokens within one noun phrase as an individ-ual unit in Semi-CRF models during segmentation. Thus, they appear together in one unit in the final results. Tokens that are not in noun phrases are treated the same as in the approach without NP-Chunking.

The performance of our method without NP-Chunking is shown in the last row of Table 1. We can see that our method can also perform better than the baseline method even without NP-Chunking. For our model with or with-out NP-Chunking, the performance of most of the attributes does not change much. However, the F1 of the attribute Au-thor decreases by more than 8 points when NP-Chunking is not used. This is because the method [25] can always ac-curately identify the boundaries between different author names when they appear in the same elements, and each author name is identified as a noun phrase. Thus, the addi-tional constraints brought by noun phrases can help Semi-CRFs separate different author names without introducing notable errors.
One advantage of Semi-CRFs is that they can effectively incorporate segment-based features [20]. We use the exist-ing database DBLP (http://dblp.uni-trier.de/xml/) to de-fine these additional features. DBLP is a public and rela-tively clean dataset. The total number of paper records is 0.72 million, and the number of author names is 0.48 mil-lion. Here, we evaluate the performance on Paper Title and Author . To extract database features, we adopt a strict strategy of string matching. For Paper Title matching, the number of matched tokens is no less than 3, and the matched tokens are kept in sequence, and also there are no punctu-ations appearing between the matched tokens. For Author matching, the number of matched tokens is no less than 2 and no more than 4, and the first letters of all the matched tokens are capitalized. By matching of two tokens we mean the exact matching of their text characters. In our dataset, about 40 percent of paper titles and about 80 percent of authors are matched with those in the DBLP database.
To see the effect of database features, we setup differ-ent subsets by randomly sampling the whole DBLP dataset with different number of matched paper titles and authors. Different settings are in Table 2. For the setting #0, zero percent of both the matched paper titles and authors means that we do not use any database features. For #16, all the matched paper titles and authors are used. Fig. 3 shows the performance of the three different methods on Paper Title and Author with different settings as in Table 2. For other attributes ( Publisher and Year ) the performance does not change much. From the results, we can see that when in-corporating database features, the overall performance can be improved. For our method with NP-Chunking, about 3 points are achieved in F1 measure for the attribute Au-thor , and almost no difference for Paper Title . However, for the baseline method, the performance on both Paper Title and Author is significantly improved. Also the improve-ments increase when the matching ratios increase. For our method without NP-Chunking, the performance on Author can be improved much. In summary, the results show that our method can also achieve very promising results when no database or only a small one is available, but the baseline method is quite dependent on the availability of databases.
For webpage understanding, efficiency is an important is-sue. As we have stated, HCRFs admit an efficient inference algorithm which is linear in terms of the number of HTML elements. The inference of Semi-CRFs, which is quadratic in terms of the maximum segment length, is more expensive. But in practice the maximum lengths are usually not very large. Furthermore, the maximum lengths in the de-coupled algorithm are different for different supper labels. As in sec-tion 4.2 for many attributes, the maximum lengths are quite small. Thus, the inference algorithm is efficient. The aver-age running time over all testing webpages is less than 2.6s, and the average time over all paper records is less than 0.3s. It means that about 33 thousands of webpages or 288 thou-sands of paper records can be processed by one machine in one day. Recent work in [21] shows that the computational efficiency of Semi-CRFs could be further improved. We plan to implement this method in the future.
Typical natural language processing methods expect fully grammatical sentences. However, the attributes on web-pages are often presented in text fragments which always have some structures but are less in grammar. This makes the application of NLP methods much challenging to mine web data. Several attempts [22][12][9] have been made to apply NLP methods on the web by incorporating struc-ture information. [22] proposes the Webfoot to segment webpages into logically coherent segments and then applied NLP methods to extract information from non-grammatical webpages. Similarly, [9] first builds an HTML Struct Tree based on page layout information, and then encodes extrac-tion rules based on this tree representation. However, the heuristic-based methods have several disadvantages as dis-cussed in the introduction. Another problem is that [9] only identifies the text fragments that contain the target informa-tion. Thus, it is not sufficient for webpage understanding. [12] uses query strings to identify the locations of candidate extractions and then composes extraction rules with both content and structure information for different types of for-mats, such as enumerations, lists, and tables. This method can X  X  be used for other data that don X  X  have these formats. [29][28] are statistical webpage structure understanding methods. In [29][28] we show that inner-page layout does have statistically regular patterns such as two-dimensional dependencies within small data records and hierarchical de-pendencies in the whole webpage. We also show that these structural regularities can be explicitly explored in a statis-tical model for structure understanding tasks such as record detection and labeling of HTLM elements. However, these methods are not sufficient for webpage understanding due to their lack of capacity for text content understanding. [3][11][23] are wrapper induction methods which depend on the types of webpages. WHISK [23] extracts informa-tion from structured, semi-structured and free text, we fo-cus on incorporating both structures and text contents for webpage understanding. Probabilistic graphical models can take the advantage of mutual dependencies of different at-tributes for multiple slot information extraction. SRV [11] and RAPIER [3] extract only isolated slots. Thus, they lose the mutual dependencies of multiple attributes, and post-processing must be performed to re-assemble related infor-mation into records.

A probabilistic method is proposed in [2] by extending the HMM model. However, the inputs in their method are address or bibliography records which are already collected in a warehouse. But the inputs to our method are raw web-pages. Furthermore, the webpages here have structures, but the records in [2] are represented as string sequences. [5][20] are also for sequence data segmentation and labeling. Attempts to incorporate hierarchical Markov models and Semi-Markov models have been made in [19][10]. Our model can be viewed as a discriminative generalization of the Switch-ing Hidden Semi-Markov Model, and the differences between these models and ours have been discussed.
In this paper, we present an integrated model to incorpo-rate both structure understanding and text content under-standing for effective webpage understanding. To the best of our knowledge, our model is the first integrated webpage understanding model. The joint model is an integration of Hierarchical Conditional Random Fields and Semi-Markov Conditional Random Fields. At higher levels, a full hier-archical model is used to effectively incorporate structural dependency patterns for page structure understanding; and at the finest level Semi-Markov models are introduced to ex-plore the dependencies of target attributes for effective text content understanding. Although the model is an integra-tion of two different types of models, we show that it can be efficiently learned by separately learning each model. The feasibility and promise of our approach is demonstrated on a real-world webpage understanding task. The authors Jun Zhu and Bo Zhang are supported by the National Natural Science Foundation of China, Grant No. 60621062, and the National Key Foundation R&amp;D Project, Grant No. 2003CB317007 and 2004CB318108. [1] A. Arasu and H. Garcia-Molina. Extracting Structured
Data from Webpages. Proc. of SIGMOD , 2003. [2] V. Borkar, K. Deshmukh and S. Sarawagi. Automatic segmentation of text into structured records. Proc. of
SIGMOD , 2001. [3] M. E. Califf and R. J. Mooney. Bottom-up relational learning of pattern matching rules for information extraction. Journal of Machine Learning Research, 2004. [4] C.-H. Chang and S.-L. Liu. IEPAD: Information
Extraction Based on Pattern Discovery. Proc. of WWW , 2001. [5] W. W. Cohen and S. Sarawagi. Exploiting Dictionaries in Named Entity Extraction: Combining Semi-Markov Extraction Processes and Data Integration Methods.
Proc. of SIGKDD , 2004. [6] V. Crescenzi, G. Mecca and P. Merialdo.

ROADRUNNER: Towards Automatic Data Extraction from Large Web Sites. Proc. of VLDB , 2001. [7] B. Crysmann, A. Frank, B. Kiefer, S. Muller, G. Neumann, J. Piskorski, U. Schafer, M. Siegel, H.
 Uszkoreit, F. Xu, M. Becher and H-U. Krieger. An Integrated Architecture for Shallow and Deep
Processing. Proc. of ACL , 2004. [8] D. W. Embley, Y. Jiang and Y.-K. Ng.
 Record-Boundary Discovery in Web Documents. Proc. of
SIGMOD , 1999. [9] D. DiPasquo. Using HTML Formatting to Aid in Natural Language Processing on the World Wide Web.
Senior Honors Thesis, Carnegie Mellon University, 1998. [10] T. Duong, H. Bui, D. Phung and S. Venkatesh.
Activity Recognition and Abnormality Detection with the Switching Hidden Semi-Markov Model. Proc. of
CVPR , 2005. [11] D. Freitag. Information Extraction from HTML: Application of a General Machine Learning Approach.
Proc. of AAAI , 1998. [12] C. Jacquemin and C. Bush. Combining Lexical and Formatting Cues for Named Entity Acquisition from the Web. Proc. of the Joint SIGDAT conference on
Empirical methods in natural language processing and very large corpora , 2000. [13] F. V. Jensen, S. L. Lauritzen and K. G. Olesen.
Bayesian updating in causal probabilistic networks by local computation. Computational Statistics Quarterly , 4:269-82, 1990. [14] N. Kushmerick. Wrapper induction: efficiency and expressiveness. Artificial Intelligence, 118:15-68, 2000. [15] J. Lafferty, A. McCallum and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Proc. of ICML , 2001. [16] K. Lerman, L. Getoor, S. Minton and C. Knoblock. Using the Structure of Web Sites for Automatic
Segmentation of Tables. Proc. of SIGMOD , 2004. [17] K. Lerman, S. Minton and C. Knoblock. Wrapper maintenance: A machine learning approach. Journal of
Artificial Intelligence Research , 18:149-181, 2003. [18] C. Manning and H. Schutze. Foundations of Statistical Natural Language Processing. The MIT Press
Cambridge, MA, May, 1999. [19] D. Phung, T. Duong, S. Venkatesh and H. Bui. Topic
Transition Detection Using Hierarchical Hidden Markov and Semi-Markov Models. Proc. of MM , 2005. [20] S. Sarawagi and W. W. Cohen. Semi-Markov Conditional Random Fields for Information Extraction.
Proc. of NIPS , 2004. [21] S. Sarawagi. Efficient Inference on Sequence
Segmentation Models. Proc. of ICML , 2006. [22] S. Soderland. Learning to Extract Text-based Information from the World Wide Web. Proc. of
SIGKDD , 1997. [23] S. Soderland. Learning Information Extraction Rules for Semi-structured and Free Text. Journal of Machine
Learning , 1999. [24] F. Suchanek, G. Ifrim and G. Weikum. Combining
Linguistic and Statistical Analysis to Extract Relations from Web Documents. Proc. of SIGKDD , 2006. [25] T. Yoshimasa and T. Jun X  X chi. Chunk Parsing Revisited. Proc. of the 9th International Workshop on
Parsing Technologies , 2005. [26] Y. Zhai and B. Liu. Web Data Extraction Based on
Partial Tree Alignment. Proc. of WWW , 2005. [27] H. Zhao, W. Meng, Z. Wu, V. Raghavan and C. Yu. Fully Automatic Wrapper Generation for Search
Engines. Proc. of WWW , 2005. [28] J. Zhu, Z. Nie, J.-R. Wen, B. Zhang and W.-Y. Ma. 2D Conditional Random Fields for Web Information
Extraction. Proc. of ICML , 2005. [29] J. Zhu, Z. Nie, J.-R. Wen, B. Zhang and W.-Y. Ma.
Simultaneous Record Detection and Attribute Labeling in Web Data Extraction. Proc. of SIGKDD , 2006.

