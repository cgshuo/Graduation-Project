 1. Introduction
The size of the publicly indexable world-wide-web has probably surpassed several billions of documents and as yet mining, genomics, etc.

Automated Question Answering (QA)  X  the ability of a machine to answer questions, simple or complex, posed in ordinary uments, to compiled newswire reports, to the World Wide Web.
 substantial headway in factoid and list questions, researchers have turned their attention to more complex information  X  uments. Unlike simple factoid questions, complex questions often seek multiple different types of information simulta-neously and do not presuppose that one single answer could meet all of its information needs. For example, with a of this question suggests that the submitter may not have a single or well-defined information need and therefore may be
Natural Language Processing (NLP) can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information ( Amigo, Gonzalo, Peinado, Peinado, &amp; Verdejo, 2004 ).
 fully to generic, multi-document summarization. A topic-sensitive LexRank is proposed in Otterbacher, Erkan, and Radev ( Chali &amp; Joty, 2008; Chali &amp; Joty, 2008 ).

In this paper, we extensively study the impact of syntactic and semantic information in measuring similarity between the sentences in the random walk framework for answering complex questions. We apply the tree kernel functions and Ex-tended String Subsequence Kernel (ESSK) to include syntactic and semantic information. We run our experiments on the
DUC 2007 data and based on this we argue that for the complex question answering task, similarity measures based on syn-tence (answer) in a more effective way than the traditional TF IDF based similarity measures.
We organize the paper as follows: Section 2 focuses on the related work, Section 3 describes the graph-based model, Sec-low semantic kernels, Section 6 discusses the theory of Extended String Subsequence Kernel, Section 7 describes the redundancy checking and summary generation module whereas Section 8 presents the experimental details with evaluation results and finally, Section 9 concludes the paper by cuing some future directions. 2. Related work hypothesis. They try to find a good partial alignment between the typed dependency graphs representing the hypothesis cally decomposable scoring function was chosen such that the score of an alignment is the sum of the local node and edge
Synonyms and antonyms receive the highest score and unrelated words receive the lowest. Alignment scores also incorpo-the text conditioned on the typed dependency graphs as well as the best alignment between them. To make this decision parameter for regularization.
 study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences. Another good way to encode some shallow syn-
Moreover, the study of shallow semantic information such as predicate argument structures annotated in the PropBank (PB) project ( Kingsbury &amp; Palmer, 2002 ) is a promising research direction.

In Hirao, Suzuki, Isozaki, and Maeda (2004) , they represent the sentences using Dependency Tree Path (DTP) to incorpo-rate syntactic information. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESSK) to incorporate semantics in DTPs. In Kouylekov with their dependency trees. They add semantic information (i.e. named entity, synonyms and other related words) in the ment over the Bag-Of-Words (BOW) scoring methods. 3. Graph-based methods for summarization products of local and global parameters. The model is known as term frequency X  X nverse document frequency (TF IDF) mod-el. The weight vector for a sentence s is v s !  X  X  w 1 ; s and tf t is term frequency ( tf ) of term t in sentence s (a local parameter), |{ t 2 s }| is the number of sentences containing the term t .
 graph for four sentences.
 Rank performed well in the context of generic summarization.
 the sentence to other high-scoring sentences. 3.1. Relevance to the question the following formula: where N is the total number of sentences in the document cluster, and sf pears in.

We also stem out the questions and remove the stop words. The relevance of a sentence s to the question q is computed by: where, tf w , s and tf w , q are the number of times w appears in s and q , respectively.
 3.2. Mixture model et al., 2005 ): the relevance to the question to similarity to other sentences. The denominators in both terms are for normalization. We measure the cosine similarity weighted by word IDFs as the similarity between two sentences in a cluster:
Following ( Otterbacher et al., 2005 ), Eq. (1) can be written in matrix notation as follows: can be understood by the concept of a random walk on the graph representation of the Markov chain. kov chain is irreducible and aperiodic, the algorithm is guaranteed to terminate.

We claim that for a complex task like answering complex questions where the relatedness between the query sentences and the document sentences is an important factor, the graph-based method of ranking sentences would perform better if similarity between sentences. Thus, our mixture model for answering complex questions is: compute the similarity between the document sentence ( s ) and each of the query-sentences ( q the similarity between sentences. 4. Encoding syntactic and shallow semantic structures (discussed in Section 5.1 ).

Though introducing syntactic information gives an improvement on BOW by the use of syntactic parses, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models ( Moschitti et al., 2007 ).
 Initiatives such as PropBank (PB) ( Kingsbury &amp; Palmer, 2002 ) have made possible the design of accurate automatic cation of SRL to QA hence seems natural, as pinpointing the answer to a question relies on a deep understanding of the semantics of both.

For example, consider the PB annotation:
Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. head. This reduces the data sparseness with respect to a typical BOW representation.

However, sentences rarely contain a single predicate: it happens more generally that propositions contain one or more cated X  X . The SRL system outputs the following two annotations: (1) [ARG0 the Vatican located wholly within Italy][TARGET uses] [ARG1 the Italian lira][ARG2 as their (2) [ARG0 the Vatican][TARGET located][ARGM-LOC wholly][ARGM-LOC within Italy] uses the Italian encoding the dependency between the two predicates as in Fig. 3 C. We refer to this kind of nested STs as STNs (Semantic
Tree Networks). 5. Syntactic and semantic kernels for text 5.1. Tree kernels
Once we build the trees (syntactic or semantic), our next task is to measure the similarity between the trees. For this, every tree T is represented by an m dimensional vector v ( T )=( production with the restriction that no production rules can be broken into incomplete parts. Fig. 4 shows an example tree and a portion of its subtrees.

Implicitly we enumerate all the possible tree fragments 1,2, , m . These fragments are the axis of this m -dimensional 2001 ) defines the tree kernel algorithm whose computational complexity does not depend on m .
The tree kernel of two trees T 1 and T 2 is actually the inner product of where N 1 and N 2 are the set of nodes in T 1 and T 2 respectively. So, we can derive: where, we define C  X  n 1 ; n 2  X  X  P i I i  X  n 1  X  I i  X  n lowing recursive definition: 1. If the productions at n 1 and n 2 are different then C ( n 2. If the productions at n 1 and n 2 are the same, and n 1 3. Else if the productions at n 1 and n 2 are not pre-terminals, where nc ( n 1 ) is the number of children of n 1 in the tree; because the productions at n nc ( n 1 )= nc ( n 2 ). The i th child-node of n 1 is ch ( n
Note that, the tree kernel (TK) function computes the number of common subtrees between two trees. Such subtrees are the ST node, the kernel defined above computes only one match (ST ARG0 TARGET ARG1 ARG2) which is not useful. identical to allow the match of further descendants. This means that common substructures cannot be composed by a node designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a ST. 5.2. Shallow Semantic Tree Kernel (SSTK)
The SSTK is based on two ideas: first, it changes the ST, as shown in Fig. 5 by adding SLOT nodes. These accommodate structure with no children, i.e. empty).

Second, as the original tree kernel would generate many matches with slots filled with the null label, we have set a new step 0 in the TK calculation: (0) if n 1 (or n 2 ) is a pre-terminal node and its child label is null , C ( n to C ( n 1 , n 2 ), in step 3: The above changes generate a new C which, when substituted (in place of original C ) in Eq. (5) , gives the new SSTK.
We can see that the above discussed kernels are designed by either choosing an explicit mapping function and incorpo-symmetricity using the theorem in Shin and Kuboyama (2008) . 6. Extended String Subsequence Kernel (ESSK) The ESSK is a simple extension of the Word Sequence Kernel (WSK) ( Cancedda, Gaussier, Goutte, &amp; Renders, 2003 ) and information with the use of word senses. WSK receives two sequences of words as input and maps each of them into a high-dimensional vector space. WSK X  X  value is just the inner product of the two vectors. But, WSK disregards synonyms, hyp-when paraphrasing is used for the sentences ( Hirao et al., 2004 ).

We calculate the similarity score sim ( T i , U j ) using ESSK where T follows ( Hirao et al., 2004 ): sense. 1 The function val ( t , u ) returns the number of attributes common to given nodes t and u .
Here, k is the decay parameter for the number of skipped words. K
Finally, the similarity measure is defined after normalization as below: 7. Redundancy checking and generating a summary
Once the sentences are scored by the mixture model, the easiest way to create summaries is just to output the topmost N coherence.
 a coherent summary. The answer or summary consists of multiple separately extracted sentences from different documents.
Obviously, each of the selected text snippets should individually be important. However, when many of the competing sen-tences are included in the summary, the issue of information overlap between parts of the output comes up, and a mecha-nism for addressing redundancy is needed. Therefore, our summarization systems employ two levels of analysis: first, a when, before being added to the final output, the sentences deemed to be important are compared to each other and only and Carbonell (1999) observed this in what the authors called  X  X  X aximum-Marginal-Relevancy (MMR) X  X . Following ( Hovy et al., 2006 ), we modeled this by BE overlap between an intermediate summary and a to-be-added candidate summary sentence.
 8. Experiments 8.1. Evaluation setup
Over the past three years, complex questions have been the focus of much attention in both the automatic question-answering and multi-document summarization (MDS) communities. While most current complex QA evaluations (including the 2004 AQUAINT Relationship QA Pilot, the 2005 Text Retrieval Conference (TREC) Relationship QA Task, and the 2006 recent MDS evaluations (including the 2005, 2006 and 2007 Document Understanding Conferences (DUC)) have tasked sys-tems with returning paragraph-length answers to complex questions that are responsive, relevant, and coherent. marization and enable researchers to participate in large-scale experiments. We used the main task of DUC 2007 for eval-uation. The task was: well-organized 250-word summary of the documents that answers the question(s) in the topic. X  X 
NIST assessors developed topics of interest to them and choose a set of 25 documents relevant (document cluster) to each statement. These multiple  X  X  X eference summaries X  X  are used in the evaluation of summary content. complex question answering task. To accomplish this, we generate summaries for 20 topics of DUC 2007 by each of our five systems defined below: of all sentences in the collection. is the set of all sentences in the collection. (4) SYNSEM: This system measures the similarity between the sentences using both the syntactic and shallow semantic where sentences in the collection.

In our experiments, we set 0.7 as the value of d (bias) and R (overlap ratio). 8.2. Evaluation results 8.2.1. Automatic evaluation evaluation, which has been widely adopted by DUC for automatic summarization evaluation. It measures summary quality by counting overlapping units such as the n-gram (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary. ROUGE toolkit reports separate scores for n -grams ( n = 1 X 4), longest common subsequence (LCS), weighted longest common subsequence (WLCS) co-occurrences and skip bi-gram (SB) co-occurrences. Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most ( Lin &amp; Hovy, 2003 ). We showed four of the ROUGE metrics in the experimental results: ROUGE-1 (unigram), ROUGE-L (LCS), ROUGE-W (weighted LCS with weight = 1.2) and ROUGE-SU (skip bi-gram).

ROUGE parameters were set as the same as DUC 2007 evaluation setup. All the ROUGE measures were calculated by run-ning ROUGE-1.5.5 with stemming but no removal of stopwords.
 ROUGE run-time parameters: ROUGE-1.5.5.pl-2-1-u-r 1000-t 0-n 4-w 1.2-m-l 250-a
Tables 1 and 2 show the ROUGE scores of the TF IDF and SYN systems respectively. It can be noticed that almost in every cases (except ROUGE-SU) the SYN system outperforms the TF IDF system which proves the effectiveness of syntactic simi-larity over the TF IDF based similarity. Tables 3 and 4 show the ROUGE scores of the SEM system and the SYNSEM system respectively. SEM system outperforms the TF IDF and SYN systems in every measure. The SYNSEM system performs better system performs better than the TF IDF and SYN systems whereas it works worse than that of the SEM and SYNSEM systems.
The comparison between the systems in terms of their F-scores is given in Table 6 . The SYN system improves the ROUGE-1, ROUGE-L and ROUGE-W scores over the TF IDF system by 2.84%, 0.53% and 2.14% respectively. The SEM system improves the ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-SU scores over the TF IDF system by 8.46%, 6.54%, 6.56%, and 11.68%, and over the SYN system by 5.46%, 5.98%, 4.33%, and 12.97% respectively. The SYNSEM system improves the ROUGE-1,
ROUGE-L, ROUGE-W, and ROUGE-SU scores over the TF IDF system by 4.64%, 1.63%, 2.15%, and 4.06%, and over the SYN sys-tem by 1.74%, 1.09%, 0%, and 5.26% respectively. The SEM system improves the ROUGE-1, ROUGE-L, ROUGE-W, and ROUGE-
SU scores over the SYNSEM system by 3.65%, 4.84%, 4.32%, and 7.33% respectively which indicates that including syntactic feature with the semantic feature degrades the performance. On the other hand, the ESSK system improves the ROUGE-1,
ROUGE-L, ROUGE-W, and ROUGE-SU scores over the TF IDF system by 4.07%, 2.64%, 2.12%, and 1.52%, and over the SYN sys-tem by 1.19%, 2.10%, 0%, and 2.70% respectively.

Our experimental results show that, the graph-based random walk method of generating query-relevant summaries per-forms best when we measure the similarity between the sentences (and query) using their semantic structures. Similarity measure based on the syntactic structure also outperforms the traditional TF IDF based measure for this task.
Confidence Interval We show the 95% confidence interval of the important evaluation metrics for our systems to report significance for doing meaningful comparison. We use the ROUGE tool for this purpose. ROUGE uses a randomized method named bootstrap resampling to compute the confidence interval. We used 1000 sampling points in the bootstrap resam-of ROUGE-1 F-scores for all the systems. Our systems could not beat the best system in DUC-2007 because of the fact that on while producing a summary. 8.2.2. Manual evaluation
For a sample of 25 summaries 2 drawn from our different systems X  generated summaries we conduct an extensive manual ation of contents and a user evaluation to get the assessment of linguistic quality and overall responsiveness.
Pyramid Evaluation In the DUC 2007 main task, 23 topics were selected for the optional community-based pyramid eval-uation. Volunteers from 16 different sites created pyramids and annotated the peer summaries for the DUC main task using to compute the modified pyramid scores. 4 We used the DUCView.jar subset of peer summaries.

User Evaluation Some university graduate students judged the summaries for linguistic quality and overall responsive-given for meaningful comparison. 9. Conclusion between the sentences in the random walk framework for answering complex questions. We parsed the sentences into the matically extract using the ASSERT SRL system. We have used the shallow semantic tree kernel to measure the semantic similarity between two semantic trees. We have also extended our work using the Extended String Subsequence Kernel (ESSK) to include semantic information by word senses.

We evaluated our systems automatically using ROUGE and report the significance of our results through 95% confidence mance of our systems. Our experiments suggest the following: (a) similarity measures based on the syntactic tree and/or shallow semantic tree and Extended String Subsequence Kernel (ESSK) outperform the similarity measures based on the TF IDF and (b) similarity measures based on the shallow semantic tree performs best for this problem.
In future, we plan to experiment with a shallow syntactic sentence similarity measurement approach like Basic Elements (BE) to check how it performs for this task.
 Acknowledgments
We thank the anonymous reviewers for their useful comments on the earliest version of this paper. The research reported here was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada  X  discovery grant and the University of Lethbridge.
 References
