 Machine learning techniques have proved effective in recom-mender systems and other applications, yet teams working to deploy them lack many of the advantages that those in more established software disciplines today take for granted. The well-known Agile methodology advances projects in a chain of rapid development cycles, with subsequent steps of-ten informed by production experiments. Support for such workflow in machine learning applications remains primitive.
The platform developed at if(we) embodies a specific ma-chine learning approach and a rigorous data architecture constraint, so allowing teams to work in rapid iterative cy-cles. We require models to consume data from a time-ordered event history, and we focus on facilitating creative feature engineering. We make it practical for data scien-tists to use the same model code in development and in production deployment, and make it practical for them to collaborate on complex models.

We deliver real-time recommendations at scale, return-ing top results from among 10,000,000 candidates with sub-second response times and incorporating new updates in just a few seconds. Using the approach and architecture de-scribed here, our team can routinely go from ideas for new models to production-validated results within two weeks. H.4.m [ Information Systems Applications ]: Miscella-neous Agile; Recommender Systems; Machine Learning
Innovative companies often use short product cycles to gain advantage in fast-moving competitive environments. Among social networks, Facebook is known for especially frequent release cycles [11], and if(we) counts such capabili-ties as crucial to the early success of the Tagged and hi5 web sites, which today form a social network with more than 300 million registered members. We especially value the quick feedback loop between product ideas and production exper-iments, with schedules measured in days or weeks rather than months, quarters, or years.

Today, on account of the approach described here, if(we) can develop and deploy new machine learning systems, even real-time recommendations, just as rapidly as we do web or mobile applications. This represents a sharp improvement over our experience with traditional machine learning ap-proaches, and we have been quick to take advantage of these capabilities in releasing a stream of product improvements.
Our system puts emphasis on creative feature engineer-ing, relying on data scientists to design transformations that create high-value signals from raw facts. We work with com-mon and well understood machine learning techniques such as logistic regression and decision trees, interface with pop-ular tools such as R, Matlab, and Python, but invest heav-ily in the framework for data transformation, production state management, model description, model training, back-testing and validation, production monitoring, and produc-tion experimentation,
In what we believe to be a key innovation not previously described, our models only consume data inputs from a time-ordered event history. By replaying this history we can always compute point-in-time feature state for train-ing and back-testing purposes, even with new models. We also have a well-defined path to deploying new models to production: we start out by playing back history, rolling forward in time to the present, then transition seamlessly to real-time streaming. By construction, our model code works just the same with inputs that are months old as with those that are milliseconds old, making it practical to use a sin-gle model description in both development and production deployment.

In adopting the architecture and approach described here, we bring to machine learning the sort of rapid iterative cy-cles that are well established in Agile software development practice [28], and along with this the benefits.

Our approach can be summarized as follows: Key benefits include:
The primary driver of our work has been building the rec-ommendation engine for a dating product, Meet Me, that is offered within Tagged and hi5. Choosing from roughly 10 million active members, and incorporating signals from re-cent seconds, the system produces an ordered list of profiles to be presented to the user for consideration. We devote Sec-tion 2 to a detailed description of this application and the design choices that it spurred. The open source Antelope framework, described in Section 3, generalizes the concepts developed for Meet Me, adds usability improvements, and provides a reference implementation independent from the if(we) codebase and applications.
Among various features offered by the Tagged and hi5 so-cial platform, Meet Me caters most directly to those seeking romantic connections; it serves as the center of the dating offering. The user is presented with one profile at a time, and prompted with a simple question,  X  X re you interested? X  In response, the user may select from two options, variously labeled as yes or no , or . In our terminology, we describe such a response as a vote , and in this discussion we stan-dardize on the response terminology positive and negative . Two users achieve a match when both express mutual in-terest through positive votes for one another, and creating such matches is an important optimization goal for our al-gorithm. An illustrative screen shot of the Meet Me user interface appears in Figure 1.
 Figure 1: The Meet Me voting interface, shown here in the Tagged Android application. Users can touch the voting buttons at the bottom of the screen, or may swipe the pre-sented profile towards the right to register a positive vote, or towards the left to register a negative vote.

The Meet Me style of matching, adopted by Tagged in 2008 and in 2012 by hi5, following a merger, appears to have been introduced by Hot or Not Inc. in the early 2000s. In re-cent years it attracted even greater attention as embodied in the Tinder mobile dating app. Notable implementations also include those by online social discovery companies Badoo, a UK company headquartered in London, and MeetMe Inc., a US company headquartered in New Hope, Pennsylvania.
We can view our Meet Me optimization problem from one of several perspectives, but prefer a formulation from the viewpoint of the user, posing the problem as follows:  X  X iven the millions of active user profiles matching basic eligibility criteria (principally filter settings), which should we next se-lect to show? X  We believe that focusing on the user helps data scientists build empathy for the individual experience, which is an important guide to intuition. It is our conjecture and our hope that separately optimizing for individual users produces a result that is well optimized for all users. Still, in developing a model for the experience of one user, we must account for behavior of other users as well. Most obviously, we recognize that it is not sufficient to derive recommenda-tion from predictions of profiles that a user is likely to be interested in, it is also important that interest is reciprocated and mutual.

We decompose the problem by expressing the match prob-ability in terms of separate conditional probabilities: where match a  X  b represents a match between user a and user b , vote a  X  b represents a vote, either positive or negative, by user a on user b ,and vote + a  X  b represents a positive vote by user a on user b .

In decomposing the match probability into three parts, the first and third represent the likelihood that the voting user issues a positive vote. We represent in p ( vote + a  X  b | vote the likelihood that the user a will vote positive when we user b will vote on user a following vote + a  X  b , a probability that is itself influenced not only by the behavior of user b ,say how active she is and how reliably she returns to Meet Me, but also by the implementation and rules of our algorithm, for example by how we rank user a among other users who have registered positive votes on user b , and by how often our recommendations of user b to others result in positive votes. The third component, p ( vote + b  X  a | vote b  X  a  X  vote + modeled similarly to the first component, for it represents the likelihood that a vote comes out as positive, yet since our application sometimes highlights match opportunities we do better by distinguishing this situation and training a separate model for it.

Ranking users b according to p ( match a  X  b | vote a  X  b reasonable first approach to the problems of making Meet Me recommendations for a . We will describe improved ap-proaches later but first discuss early attempts at algorithm development.
Our first algorithm implementations Meet Me were foun-dationally heuristic, only later coming to incorporate ma-chine learning. We describe a progression of algorithms be-fore outlining the challenges we encountered. These chal-lenges arose not only from our approach to machine learning, but also from our system architecture, a traditional service-oriented web application, the layout and data flows of which are shown in Figure 2.
An important early recommendation algorithm employed a patented approach [30] deriving inspiration from Page-Rank [29]. Ours may be the first commercial application of PageRank to social data, though Twitter also described Figure 2: Architecture diagram of an early implementation of the Meet Me recommendation system. The API and database are based on standard web services technologies (PHP and Oracle). Recommendation candidates come from an Apache Solr search instance that first builds an index by querying the database, then stays current by process-ing change logs from the application. The ranking service (Java) operates similarly in maintaining an in-memory so-cial graph, but also issues on-demand database queries to update user profile data. Data scientists engaged in devel-opment activities such as exploratory analysis, training, and backtesting query the database to extract working sets, most often studied using R and Python. a similar approach and popularized the notion of personal-ized PageRank in a social context [12]. Whereas the original PageRank algorithm for web search can be modeled as the likelihood of a page visit by a  X  X andom surfer X  who starts at a randomly selected page, then traverses the graph along links between pages, at each hop continuing with probabil-ity  X  and stopping with probability 1  X   X  , the personalized PageRank algorithm starts the graph traversal at the node of interest, in this case at the user who is to receive recom-mendations.

Our early work demonstrated the value of latent infor-mation present in social network interactions. For example, even without explicit data on age, gender, or sexual orien-tation, inspection of top results from a personalized Page-Rank query on the graph of friend connections or message exchanges gives results that are immediately recognizable as relevant (viewing a grid of photos can be a surprisingly effec-tive way to get a quick and powerful impression of what an algorithm is doing, often proving more useful than statistical measures).

While our approach remained entirely heuristic, involving neither machine learning nor statistics, it provided plenty of parameters for experimentation. We focused on tuning pa-rameters of the personalized PageRank algorithm, as well as parameters involving the level of user activity and the level of inbound positive interest. Lacking a predictive model of user behavior, we proceeded by intuitively guided trial and error, using judgment and quick sequences of A/B tests to maximize the number of users who received matches each day.
We continue to believe that heuristics are a good way to start building recommendation engines, they test our problem understanding and can lead to good user experi-ences even with simple implementations. However, limited back-testing ability drives excess need for production ex-periments, and as the number of parameters rises it be-comes increasingly awkward to reason about how to tune them manually. When we saw gains from heuristic improve-ments plateau we began to incorporate machine learning techniques, pursuing a promise of scaling to greater model complexity.

We chose to implement an SVM-based classifier predicting p ( vote + a  X  b | vote a  X  b ) from a broad range of user details, not only age, gender and location, but also behavioral measures reflecting activity in sending and receiving friend requests and messages. We also included Meet Me activity, profile characteristics such as photos, schools, profile completeness, time since registration, profile viewing behavior, number of profile views received, ethnicity, religion, languages, sexual orientation, relationship status, and expressed reason for meeting people. Our approach might roughly be summed up as using any readily available information as a model fea-ture, a contrast to the deliberate design approach we would later take.

This combination of machine learning with heuristics led to some gains at first, but we again soon found progress faltering. It was particularly troubling that the time be-tween each improvement increased while gains realized in each decreased. In attempting to introduce new features to reflect user behavior better we encountered substantial soft-ware engineering challenges, including months spent making changes across multiple services, not only the ranking com-ponent but also the web application and database.
Among challenges we identified were the following: Figure 3: Training data consists of feature snapshots from the application database and outcomes occurring between them. These models are unable to capture feature varia-tion between snapshots, and using real-time data in produc-tion introduces an inconsistency between model training and model deployment.
With so many challenges, we were lucky to have pro-duction experiments providing a safety net, protecting us against regressions as we stumbled towards improved rec-ommendation algorithms.

The early approach described here has many shortcom-ings that leave it far from state-of-the-art. That said, in comparing notes with others we have come to believe that many of the challenges we encountered are common in in-dustry. Our hope is that the solutions we share below will be broadly useful to those who deploy machine learning for high impact, as well as to those who plan to do so.
Our answer to the struggles of previous approaches in-volves a number of deliberate choices: a departure from our previous software architecture and data architecture, spe-cific ways of constructing machine learning models, and an adherence to certain ways of working with data X  X nd with our team. These choices reinforce one another and allow an agile and iterative approach to delivering real-time recom-mendations for Meet Me.
Our architecture is driven by requirements, which can be summarized as follows:
The architecture of our solution drops dependence on the relational database powering the web application, instead relying entirely on an event history log to support our ma-chine learning systems. Importantly, we provide high-speed in-order access to event logs, and allow consumers to ac-cess both historical events and real-time streaming events using a single interface (see Listing 1). This architecture, diagrammed in Figure 5, puts the event history repository at the heart of the recommendation application.

It now becomes very natural to generate training and backtesting data for supervised learning algorithms. When-ever we encounter an event that we might want to predict, ing the state of the model features just prior to the event occurrence, together with th e event outcome. Only then do we update the state to reflect the event occurrence and con-tinue rolling forward in time. Figure 4 illustrates generating training events in this way. The training data can be for-matted for use with common statistical software, in our case RandMatlab.

Surprisingly, certain sorts of information that may not appear as event-like can benefit from representation in an event format. Take for instance zip code boundaries, ip-to-geography mappings, ISO country codes, or most any refer-ence information that might naturally be implemented using a static lookup table. In many of these cases information can evolve, if slowly. By structuring such information as fact up-date events we maintain valuable flexibility and uniformity in our abstractions.

Another important benefit of the event history architec-ture is the symmetry it creates between historical backtest-ingandreal-timestreaming. Weusethesamefeaturedef-initions and state management software in development as we do in production. This proves key to quick deployments and rapid iterative data science cycles.

The event history architecture makes it practical to gener-ate detailed training data for newly devised features, makes it straightforward and practical to deploy models based on Figure 4: Training data generated from event history has granular alignment of feature state and training outcomes. Figure 5: Diagram of the event history architecture. The event history repository serves as a central source of truth for production and development, and supports both historical access and real-time streaming. such new features in production experiments, and consoli-dates features in one piece of code that works in both de-velopment and production. It provides composable abstrac-tions that allow complex feature definitions, and it provides a single source of truth.
A central promise of Agile methodology is a more re-sponsive development cycle, one that generates quick feed-back through design, implementation, and validation phases, one that allows continuously incorporating learnings, mak-ing corrections, and exploiting opportunities. Another cen-tral promise is improving collaboration among team mem-bers. All of these are characteristics appealing for data sci-ence and for development of machine learning systems, and our approach delivers them in this context.

Figure 6 illustrates our cycle of iterative progress. It starts with data scientists developing problem understanding, and importantly, intuition. From here we propose model im-provements, typically in the form of new features. After training and establishing statistical basis for improvements through backtesting we deploy models to production and study impacts. We often realize gains but always improve our understanding, and enter the next development cycle with even stronger ideas. Additional details follow:
Problem intuition and understanding: Exploratory analysis often proceeds by asking simple questions, generally addressed with descriptive statistics and perhaps simple ag-gregates. We also encourage visualization, especially of user experience, and often find we can learn more from viewing just 100 faces, sampled from among 100,000, than we can from statistics on the aggregate. Working in this way not only helps us build our intuition but also our user empathy.
Models and feature engineering: We emphasize cre-ative features with otherwise straightforward models. The event history architecture provides us with flexible feature design capabilities, and logistic regression serves us well, in-tegrating signals typically represented by 50 X 100 features. Rather than learning a model with per-user parameters, we instead construct features that represent individual user characteristics, including how they relate to other users. These features evolve in response to individual user behav-ior, while the learned parameters of the model, applying to all users, remain fixed.

Training predictive models: Our event history accu-mulates several thousand events per second, and to compute features we need to process the entire stream. Early sam-pling is not an option because of the interconnected nature of our social network, and because features often capture inter-actions between users. On the other hand, since our models contain fewer than 100 parameters we are capable of gener-ating much more training data than we need, at least over much of the domain of the feature space. This suggests that sampling is possible for training, however we are presented with another challenge: we must be careful not to introduce feedback, as occurs when the examples in the training set are biased because they are introduced by a recommenda-tion algorithm. We address this problem by substituting a training recommendation, selected at random, in place of a ranked recommendation in 1% of instances. Doing so proved key to our ability to consistently achieve nonnegative per-formance changes when retraining models with more recent data, a foundational capability necessary for progress. We can imagine using more sophisticated sampling techniques to generate training examples, but benefiting from an enor-mous wealth of data we have not yet done so.

Production experiments: Our approach to large scale experiments is similar to that used by other consumer inter-net companies [18]. We encounter some special needs when testing Meet Me algorithms because performance ultimately depends on interactions between users. To address these, we developed a split world approach in which users are assigned at random to one of two partitions, seeing only users with the same  X  X orld X  assignment in recommendations. Split world experiments are expensive because a reduced candidate pool degrades user experience, and because this drives us to limit ourselves to running one such experiment at a time. While we rely on split world experiments for final model accep-tance, we start out evaluating model performance without a split world, assigning an initially small but progressively larger set of randomly selected users to the test group. Dur-ing experiments we also take care to observe not only the initial effect, but also whether it grows or diminishes as user behavior adapts over time.

Production operations: A number of operating proce-dures support our Agile data science approach. We continu-ously monitor model performance, with loss and bias serving as indicators of problems. These measures can be essential for catching bugs that would otherwise go undiscovered amid the complexity of personalized recommendations.
Following our implementation of the event history archi-tecture, our replacement of the original heuristic recommen-dation engine with a fully trained machine learning model implementation, and our adoption of a rapid-cycle Agile data science practice, we readily realized gains against our core optimization objectives. While the progress shown in Figure 7(a) represents contributions from various tuning ef-forts, including visual layout changes and promotions, re-sults of experiments credit improved machine learning algo-rithms for over 30% increase in Meet Me usage.

During the course of 12 months our team released 21 changes to the model and adjusted experiment parameters 163 times. During an especially intense 6-month period (May through October 2013) we released 15 changes to our models and adjusted experiments 123 times. We credit our progress to this rapid iteration. Unfortunately, towards the end of this period we suffered from increases in spam abuse (Figure 7(c)), forcing a diversion of attention from improv-ing recommendations to addressing this ever-present threat. We have deployed a number of the techniques developed here in our latest anti-spam measures, but the topic is beyond the scope of this paper.
Here we attempt to provide a flavor for the models we have tested, though the full details of our Meet Me recom-mendation algorithm remain proprietary.

Optimization objectives: Perhaps the most straight-forward approach is to optimize for the total number of matches, yet it quickly becomes clear that additional ob-jectives call for consideration: Figure 7: Meet Me metrics through a period of focused tun-ing. (a) shows progress towards increasing activity, with the 7-day average of daily voters and matchers overlaid on pro-duction changes. Vertical red lines indicate new algorithm releases, and blue lines indicate adjustments to experiment weights. (b) shows the 7-day average of daily matches, which doesn X  X  correlate with other measures of user activity. Initial models designed to optimize matches led to limited increases in matchers and voters, whereas our later algorithms in-creased these metrics despite producing fewer total matches. (c) shows an index of spam, here provided by proxy of the female positive vote rate. Spam can contribute to inflated Meet Me metrics, especially matches, but this data indicates stable spam levels through the period of greatest gains. Near the end of the period our efforts shifted to combating in-creasing spam and away from improving recommendations. While we have experimented with all of the approaches listed above, we achieve best results with the last alternative, by optimizing for the number of users predicted to engage with Meet Me on future days.

Models: We separately model the likelihood of a posi-tive vote, either p ( vote + a  X  b | vote a  X  b )or p ( vote + vote + a  X  b ), and the likelihood of a user returning to vote p ( vote b  X  a | vote + a  X  b ). For the positive vote likelihood we use logistic regression, for one because supporting tools are well-developed, but also because it provides a probability as out-put. This allows us to verify model calibration, and provides a well-defined interface between separate model components. We have also incorporated a decision stump model, but con-tinue to use logistic regression to calibrate it to outcome probabilities.

We estimate time to return using the exponential-response variant of a generalized linear model, with a threshold time to obtain a return vote probability. We note that this ap-proach is a simplified variant of the hazard based approach to user return time prediction developed by Pandora [17].
Features: We use online features only, requiring quick updates for new events and efficient in-memory implemen-tations. Features must also be very quick to access during production ranking, allowing for only a few memory accesses and perhaps some simple arithmetic, e.g., computing a ratio. A good feature is stationary, meaning that with consistent user behavior it asymptotically approaches a fixed value. For example, the number of votes in the past week repre-sents a stationary feature, as does the fraction of all votes that are positive, whereas the total number of votes or the total number of positive votes do not represent stationary features. Some of the features we have implemented include the following, listed along with examples:
Our approach stresses the composability of models and features. We routinely rank results according to the com-bination of several models, and we have experimented with unsupervised models that feed into features of higher-level supervised models. Further details of the features described above are available as part of the Antelope open source soft-ware described in Section 3.
While if(we) eschews the software development culture of  X  X ot invented here X , instead making extensive use of both commercial and free software packages, the Meet Me recom-mendation system consists almost entirely of custom code. Our abstractions and trade-off choices are somewhat differ-ent from those used in batch systems such as Hadoop, from those used in relational databases, and from those used by other stream processing software such as Spark Streaming, different enough that we choose to develop our own imple-mentations using Scala and Java.

The event history repository is a service implementing an interface similar to that outlined in Listing 1. This interface highlights the essential character of the event history. It can only do two things: 1. receive and store new events, 2. return events in time-order, possibly applying some filter. By specifying an endTime in the future (typically +  X  )the client application gains access to real-time streaming, and in a single call to getEvents can access both historical and future events.
We maintain feature state using large arrays of primitives, a packed in-memory representation allowing us to support over 10,000,000 candidate user recommendations on a com-modity server. Our framework takes care of mapping feature state to array indexes. Our present implementation supports only a dense feature layout but we can imagine implementing a layout supporting sparse feature vectors. We spent some time considering whether to lay out our features row-major or column-major format, opting for the row-major format so as to keep our implementation simpler.

Our implementation is an efficient one that seeks to limit object allocation and that aims to provide good optimiza-tion opportunities to the JVM. While we can rank &gt; 50,000 candidate users per second, we see opportunities for order-of-magnitude improvements through optimization.

High availability and scalability are provided by replica ranking servers, while a single feature building server prop-agates state changes. In the event of feature builder failure we can restart processing from the event history log, accel-erating recovery with saved sn apshots. Even at our scale, we choose to optimize for performance rather than intro-duce distributed system complexities. A modern commodity server can handle &gt; 10,000 updates per second across 100 or more features, leaving us with ample headroom. We believe that with such implementations even many large businesses have no need for distributed architectures.
The event history architecture, our approach to feature engineering, and our approach to Agile data science are gen-eral, however the implementation for Meet Me recommen-dations remains coupled both to the problem and to the if(we) platform. The Antelope open source project 1 aims to make our approach broadly available. It presently repre-sents a demonstration rather than a production tool, yet it fully illustrates the flexibility of the event history architec-ture, provides concrete examples of feature engineering, and serves as a guide for other implementations.

In our first example application, we address a Kaggle chal-lenge for product search and recommendations [1]. Using the Feature interface of Listing 2 in the context of learning to rank [14], we demonstrate use of a simple Popularity feature along with a more complex TfIdfFeature . Listing 2: The Feature interface and example implementa-tions (simplified).

In our second example, we simulate user activity in an online dating context, then build recommendations using a model with features similar to those we have deployed for Meet Me (described in Section 2.5).
We present our review of related work in four parts: Agile data science, design and architecture of machine learning systems, choice of machine learning algorithms, and other approaches to real-time processing. While we find much of this literature to be complementary to the event history architecture described here, we find no direct precedent for our design.

Agile data science: While the Agile methodology [5] is well understood and broadly practiced in many variants by software engineers, the approach is only starting to make impact on data science. Jurney [15] provides a how-to guide for performing analytical tasks using Hadoop, encouraging adoption of Agile values. This work focuses on business un-derstanding and does not address production recommenda-https://github.com/ifwe/antelope tion engines, as we do. Beyond this, the dearth of literature on Agile data science is remarkable; we imagine this may change in coming years.

Machine learning systems: The work described here has strong philosophical parallels to the Hazy project [22], which states the goal of  X  X aking it easier to build and main-tain big-data analytics. X  The emphasis on feature engineer-ing, and the belief that simple machine learning models with more or better features will often outperform more so-phisticated algorithms, rings true to our experience. We too emphasize programming abstractions and infrastructure abstractions, but choose rather different implementations. Whereas the Hazy project uses probabilistic logic program-ming, we use reactive-style Scala code to implement features. Our central infrastructure abstraction is the event history repository, whereas Hazy works against a static data model.
The Hazy team has also described a vision of a data sys-tem for feature engineering [4]. In asserting that  X  X eature man-months aren X  X  mythical, X  they suggest that whereas on traditional software projects adding more people rarely leads to proportionately faster progress [7], in trained systems loose coupling can permit a large team to work toward a common goal with only limited coordination. The proposal, most recently implemented in the DeepDive system [32], fa-cilitates feature engineering as we do, yet does not adopt a time-based data model or provide for real-time updates.
Recent work by the UC Berkeley AMP Lab reveals Velox [10], a low-latency solution to model serving and manage-ment. Velox provides important operational capabilities not previously available in open-source machine learning tool-kits. In addition to low-latency serving it provides an online update functionality that approximates continuous model retraining, as well as periodic batch retraining. We are not alone in recognizing the gap between the promise of machine learning applications and what is commonplace in industry. In comparison, by using online features only, we achieve con-tinuous production operation without needing batch opera-tions. Furthermore, in addressing feature engineering chal-lenges we contribute additional productivity improvements.
There are a variety of toolkits designed to make machine learning more practical and accessible. MLbase [20] auto-mates many of the technical steps required to build effective machine learning models. The venerable Weka [13] remains a powerful environment for machine learning, offering not only a large number of machine learning algorithms but also a variety of filters for preprocessing data. Our provisions for feature engineering are richer, and we can imagine comple-menting both of these toolkits.

Machine learning is applied extensively in online advertis-ing, and the production techniques developed at Google are relevant to our work [27, 31]. While there is no mention of a central event history or of using event processing to sup-port iterative development and expressive feature engineer-ing, the importance of real-time processing, of a log-based approach, of keeping machine learning simple, and of think-ing carefully about architecture all come through clearly.
Machine learning algorithms: We rank recommenda-tions in Meet Me by ordering candidates according to the optimization objective, matches in early implementations, and predictions for overall engagement in later implementa-tions. Here we review alternatives.

The learning to rank literature is perhaps best developed in the area of information retrieval [25], but its principles can apply to personalized people recommendations as well [14]. Our experience developing examples for Antelope sug-gests that learning to rank is a natural match with the event history architecture.

People recommendations struggle with the special chal-lenge of a high-dimensionality target space; there are many more candidates than in e-commerce or in entertainment. Collaborative filtering has widely publicized success as part of Amazon X  X  product recommendations [24], and related ma-trix factorization techniques were used in winning the Net-flix prize [19]. While we did not pursue such approaches, it would be interesting to evaluate whether they are practical in our setting.

Contextual-bandit techniques have also been adopted for personalized recommendations [23], in an approach that we believe could be combined with the creative feature engi-neering described here. In particular, we are encouraged by results showing offline evaluation using previously recorded traffic, which dovetails with an event history approach.
Real-time processing: A number of architectural par-adigms exist for processing real-time information.
Complex event processing is well established as an integra-tion pattern that fits naturally with an Agile approach [9]. The MillWheel system described by Google [3] is a platform for reliable stream processing that can execute arbitrary im-perative code. A log-centric architecture extolled by Kreps [21] is particularly well aligned with our perspectives on data processing; the immutable log represents the system X  X  sin-gle source of truth, just like our event history. Similarly, Event Sourcing and CQRS (Command &amp; Query Responsibil-ity Segregation) [6] patterns for application architecture are particularly well suited to our approach. Streaming query engines are developed in the academic literature [8] and have commercial implementations [2]. All of these platforms and perspectives could serve as a part of an implementation for our architecture.

One approach to real-time recommendations is to build a speed layer on top of a batch system [26]. While this evolu-tionary approach is sensible, and while some calculations are easier with batch processing, we believe that in most cases such hybrid architecture introduces unnecessary complexity.
The event history architecture provides a powerful ab-straction, allowing teams of data scientists to collaborate actively on machine learning projects. Our production im-plementation delivers personalized recommendations drawn from millions of candidates, an swers queries in under a sec-ond and incorporates new information in seconds. We are presently working on Antelope, an open source implementa-tion incorporating the event history paradigm and providing a flexible environment for feature engineering. We have used several machine learning tools alongside the technology de-veloped here, and can envision deeper integrations, includ-ing ones with popular data management systems, which only need to implement the interface of Listing 1.

We remain struck by the gulf between sophisticated dem-onstrations of what is possible in machine learning, and the day-to-day realities of what is practical in most organiza-tions. By bringing Agile capabilities to production data sci-ence applications we hope to narrow the gap, to help teams feel the thrill of frequently realized gains, and to help them build and deploy more of what they can imagine.
This work reflects the industr iousness and ability of the data scientists and engineers of the if(we) Relevance Team. Special thanks go to Karl Dawson and Helena Buhr for lead-ership and helpful discussions, and to Vinit Garg, Dai Li, Martin Linenweber, and Madhusudana Shashanka for tire-less efforts. [1] Data mining hackathon on (20 mb) Best Buy mobile [2] Stream processing explained. [3] T. Akidau, A. Balikov, K. Bekiro  X  glu, S. Chernyak, [4] M.Anderson,D.Antenucci,V.Bittorf,M.Burgess, [5] K. Beck, M. Beedle, A. van Bennekum, A. Cockburn, [6] D.Betts,J.Dominguez,G.Melnik,F.Simonazzi,and [7] F.P.BrooksJr. The Mythical Man-Month: Essays on [8] S. Chandrasekaran and M. J. Franklin. Streaming [9] K. Chandy and W. Schulte. Event Processing: [10] D. Crankshaw, P. Bailis, J. E. Gonzalez, H. Li, [11] D. G. Feitelson, E. Frachtenberg, and K. L. Beck. [12] P. Gupta, A. Goel, J. Lin, A. Sharma, D. Wang, and [13] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [14] L. Hong, R. Bekkerman, J. Adler, and B. D. Davison. [15] R. Jurney. Agile Data Science: Building Data [16] S. Kandel, A. Paepcke, J. M. Hellerstein, and J. Heer. [17] K. Kapoor, M. Sun, J. Srivastava, and T. Ye. A [18] R. Kohavi, A. Deng, B. Frasca, T. Walker, Y. Xu, and [19] Y. Koren, R. Bell, and C. Volinsky. Matrix [20] T. Kraska, A. Talwalkar, J. C. Duchi, R. Griffith, [21] J. Kreps. The log: What every software engineer [22] A. Kumar, F. Niu, and C. R  X  e. Hazy: Making it easier [23] L. Li, W. Chu, J. Langford, and R. E. Schapire. A [24] G. Linden, B. Smith, and J. York. Amazon.com [25] T.-Y. Liu. Learning to rank for information retrieval. [26] N. Marz and J. Warren. Big Data: Principles and best [27] H. B. McMahan, G. Holt, D. Sculley, M. Young, [28] B. Meyer. Agile!: The Good, the Hype and the Ugly . [29] L. Page, S. Brin, R. Motwani, and T. Winograd. The [30] J. Schleier-Smith. System and method of selecting a [31] D. Sculley, G. Holt, D. Golovin, E. Davydov, [32] C. Zhang, C. R  X  e, A. A. Sadeghian, Z. Shan, J. Shin,
