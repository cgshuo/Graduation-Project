 It is often difficult to learn good classifiers when the training data are missing attribute values. To deal with missing data in classification tasks, many learners first use some imputation technique to fill in the missing values, before giving the completed data to a complete-data learner. A simple imputation technique is to replace each missing value of a real-valued attribute with the mean of the observed values of the attribute (MEI), or a nominal attribute with its most commonly observed value (MCI). This is used by the WEKA implementations for many classification algorithms [4]. However, these trivial imputers generally do not help produce high-quality classifiers for incomplete data.

Many learning algorithms have algorithm-specific built-in schemes for han-dling missing value  X  e.g., na  X   X ve Bayes can simply ignore the missing attributes at both learning and classification time, and some instance-based algorithms simply set the distance measure to any attribute (of an instance) missing an entry to the associated maximum value [1]. However, these simple missing data handling methods often do not produce accurate estimates to fill in the missing values. As we anticipate that answers based on accurately imputed data will be better than those based on the original incomplete data and on less accu-rately imputed data, we expect preproce ssing the incomplete data with accurate imputers can boost the classification performance of machine learners on incom-plete data. In general, an  X  X mputation-helped learner X  uses some imputation techniques to fill in the missing values before training a classifier. Su et al. in-vestigated such imputation-helped learners in the context of numeric features, and showed that certain numerical imput ation techniques can improve classifi-cation performance [9]. However, few exis ting works specifically investigate how imputation techniques can improve classification performance on nominal data.
This work explores imputation-helped learners for nominal features. We first propose an easy-to-implement algorithm for imputing nominal features, classifier-based nominal imputation (CNI), which treats imputation as a classifi-cation task: first learn a classifier for each feature, then use this trained classifier to impute values for the missing entries. We let the notation  X  X NN-CNI(SVM) X  refer to the learning system that preprocesses the incomplete training data by learning a SVM classifier for each attribute to impute each missing value of this attribute (so if there are n attributes with missing values, this produces n different classifiers); the resulting completed dataset from this CNI(SVM) pre-processing is then given to the learner kNN ( X  X  nearest neighbors X  [1]) to produce a final classifier. In general, the  X  B -CNI( L ) X  learner first uses the learner L to learn n different classifiers for imputing values for the n attributes, then gives the completed data to the base learner B .

We first investigate the imputation performance of our proposed CNI imputa-tion using 10 machine learners as imputation classifiers, and found that (1) each of these CNI imputers has more accurate predictions than the baseline kNN imputation and MCI, and (2) CNI(SVM) (classifier-based nominal imputation that uses support vector machine) and C NI(DT) (that uses decision tree) per-form especially well.

By applying the top-performing CNI imputers to preprocess incomplete nom-inal data, our empirical experiments sh ow that these imputation techniques can significantly improve classification performance for instance-based algorithms on incomplete nominal data with either a high or low percentage of missing values. While imputation is not as critical for other learning algorithms, such as na  X   X ve Bayes and decision tree, we found that our proposed B -CNI( L ) approach can still boost their classification perform ance when the missing ratio is at or below 20%.

Section 2 describes the framework of t his paper, Section 3 provides our ex-perimental design and results, and Section 4 contains the conclusions. 2.1 Imputation for Nominal Data As many real-world datasets are missing some information, imputation tech-niques are often used to fill in the missing values with estimated values; this often leads to performance that is better than just using the original incomplete data. Imputation techniques for numeric data, such as EM (expectation maxi-mization) and BMI (Bayesian multiple imputation), involve iteratively updating estimates of means and covariance matrices, as the data is assumed to be nor-mally distributed [9]. This is, of course, typically not appropriate for nominal data.

A baseline imputation for such nominal data is MCI (most common [value] imputation), which fills the missing values with the most frequently observed value of the attribute. However, MCI distorts the distribution of the values by overestimating the most frequent value, w hich often leads to incorrect inferences. Figure 1 shows that MCI does not produce a similar shape of attribute value distribution as the original missing data, while our proposed CNI imputation does; see details in the next subsection.

Another well-known nominal imputation technique is kNN imputation (kNNI) [2], which imputes a missing value of an attribute in an instance as the most common value of that attribute in the instance X  X  k nearest neighbors. However, kNNI is not very effectiv e because of the way that kNN selects the nearest neighbors: this is based on a distance function that is problematic in the presence of incomplete data (see Section 2.3 below).

We therefore propose an easy-to-implement nominal data imputation tech-nique: classifier-based nominal imputation. This paper investigates whether this nominal imputation technique can be used to improve classification performance for machine learned classifiers on incomplete nominal data. 2.2 Classifier-Based Nominal Imputation The basic idea of classifier-based nominal imputation (CNI) is simple: treat imputation as classification. For each attribute f i with missing values, learn a classifier c i ( ... ) that takes as input the values of the other n  X  1 attributes { f uses the learning algorithm L to learn this c i ( ... ) classifier from the training instances with observed values on f i ; CNI then uses this c i ( ... ) to impute the missing values of this attribute in the remaining instances. Algorithm 1 illustrates the CNI imputation algorithm.

After imputing incomplete data using CNI( L ), B -CNI( L ) passes the resulting completed dataset to the base learner B , which produces a classifier that can then classify (possibly incompl ete) test instances. Note that the L learner must be able to deal with incomplete data as its data sample D + i (see Algorithm 1) will typically have missing values, and the c i ( ... ) classifier that it produces must also be able to handle missing information. However the base learner B will only need to deal with complete instances. The resulting classifier will predict class labels for the original incomplete test data.

Note that the values imputed for one attribute could be used in the later iterations of imputation. For example, if attribute f 1 was the first imputed at-tribute, its imputed values could be used when imputing values for f 2 and for all other subsequent attributes. However, to simplify the algorithm and focus on the general techniques, we did not use th e previously imputed values for any of the later attributes in this work.

Here, we investigate the following Imputation Learners L : decision tree (C4.5), decision table (dTable), lazy Bayesian rules (LBR), naive Bayes (NB), one rule (OneR), decision list (DecList), random f orest (RF), support vector machine (SVM), radial basis function neural networks (RBF), and multilayer perceptron neural network (MLP). Each of the learners and the CNI technique are imple-mented within the WEKA [4] framework, and use WEKA X  X  default approach for dealing with missing values, at both learning and performance time. When there are too few observed feature values to train reasonable classifiers (here, under three instances with observed values for attribute f i in D + i ), we simply use MCI to impute a value for that attribute. For comparison, we also implement the baseline nominal imputation techniques kNNI and MCI.
 Algorithm 1. CNI(ImputationLearner L )
As these classifiers are well-known, we do not provide detailed descriptions for each of them here. We use default parameter settings of WEKA for L and B learners unless they are explicitly specified otherwise. For the imputation learner L =SVM, we use the linear kernel [7]. 2.3 Using CNI to Improve Classification Performance of Machine Figure 2 shows the general B -CNI( L ) framework, which first uses some nominal learner L for imputing the missing nominal values in the training set to generate an imputed training dataset, then gives the completed dataset to a learning algorithm B (e.g., kNN) to learn a classifier. We then use this classifier to classify the (possibly incomplete) test instances.

Although it is legitimate to impute training data together with test data (ex-cluding the labels of the test data), our imputers only work on the training data, and use the original incomplete test data for evaluation. (When predicting class labels for each incomplete test instance, we use the missing data handling strategy of the classifier associated with the base learner B ). We use this imputa-tion scenario for dealing with incomplet e training/test data because, in practice, training data and test data often come in different times. Therefore, it would be impractical to impute training data and test data together in many cases.
We investigate how our proposed CNI imputation can help machine learners such as instance-based learning algorithms, na  X   X ve Bayes, decision tree, and neural network.

Instance-based learning is a kind of  X  X azy learning X  that assigns a label to a new unlabeled instance based on the  X  X  losest X  labeled instances appearing in the training set. A commonly used instance-based learning algorithm is the k -nearest neighbor algorithm, which first identifies the k neighbors nearest to the  X  X o be labeled instance X  (based on the distance values calculated between this new instance with each of the instances in the training set) and returns the majority class over these neighbors , or perhaps some variant that weights the labels of these neighbors. The distance function in instance-based learning is usually defined as [1]: where x =( x 1 ,...,x n )and y =( y 1 ,...,y n ) are instances, each over n attributes. For numeric attributes, f ( x i ,y i )=( x i  X  y i ) 2 , and for Boolean and nominal attributes, f ( x i ,y i )= I ( x i = y i )whichis0if x i = y i and is 1 otherwise. The nearest neighbor for an instance x is argmin y Dis ( x, y ) over all instances y in the training set.

Of course, Equation 1 is not defined if any f ( x i ,y i ) value is undefined. How-ever, as f ( ... ) must return an answer even if either x i or y i is missing, many nearest-neighbor systems will use extreme values  X  ie, set f ( x i ,y i )tothemaxi-mum value if either x i or y i is missing [1]. As this simple approach often leads to biased distance values, we suspect that a reasonable estimate of missing values in the training set will improve the resulting classifier.

The best value of k often depends on the datas et. In general, a larger k value reduces noise on the classification, but can impair performance if it is too big. A good k can be selected by using various tec hniques, such as cross-validation or some heuristic. We often consider k&gt; 1 nearest neighbors and set k to an odd value to prevent ties for binary classification. (For multiple classes, we use the plurality of the votes.) Some instance-based classifiers will also weight each neighbor in this vote by using 1/ Distance or 1  X  Distance , both of which give higher weights to nearer neighbors [4].

When using imputation to help instance-based algorithms, imputing the train-ing set will replace the missing values for the instances in the training set, while the missing values that have penalized di stance values in the test instances are still present. Imputation reduces the numbe r of times where insta nce-based learn-ing algorithms will use the maximized distance values.

Many other learning algorithms deal naturally with incomplete data, and there-fore imputation may not be so critically helpful. For example, na  X   X ve Bayes makes classifications based on only observed values [6], while C4.5, a well-known decision tree classifier, will in effect simply disregard the missing values during training [8]. We explored 12 nominal datasets from the UCI machine learning repository (see dataset description in Table 1) [3]. Six of the 12 datasets have both nominal and numeric attributes; these have italicized names in Table 1  X  e.g., the dataset  X  X ustralian X  has 8 nominal attributes out of a total of 14 attributes. Most of the nominal data have more than two attribute values. Here, when corrupting the data by removing values, we only remove some of the nominal attributes, and leave all of the numeric attributes.

To investigate the performance on datasets with different missing ratios, we generate five incomplete datasets for each of the above datasets by randomly deleting 10%, 20%, 30%, 40%, and 50% of the observed values  X  this is done by removing each [instance, attribute] independently, with probably 0.1 [resp., 0.2, ..., 0.5].

We first evaluate our proposed CNI( L ) imputation algorithms using different machine learned classifiers L ; we then pick the top two CNI imputers in terms of their estimation accuracy (see next subsection) to impute the incomplete nominal training sets, before applying a base learner to train a classifier on the imputed training set, which we subsequently use to classify the incomplete instances in the test set. The classificat ion results reported are the average of five cross validation folds.

We then evaluate how CNI imputation can improve the classification per-formance for machine learners kNN, na  X   X ve Bayes, decision tree, and MLP on incomplete nominal data. 3.1 Evaluation of CNI Imputation Algorithms As we generated the incomplete datasets by removing values from the complete datasets, we have the ground truth for each missing value. Therefore, our nominal imputation algorithms can be evaluated in terms of the estimation accuracy by checking the imputed values against their respective ground truth values. where P i is the estimated value produced by the imputer (for some [instance, attribute] pair), R i is the ground truth value, and N is the total number of the imputed values.

Figure 3 illustrates the imputation performance of the CNI imputers L  X  { SVM, C4.5, NB, RF } , kNNI and MCI, when 30% of the data is missing.
Our proposed nominal imputation algorithm  X  classifier-based nominal impu-tation (CNI)  X  performs significantly better than the commonly used nominal im-puters, kNNI and MCI. While each CNI( L ) imputer (that uses the learner L )we investigated outperforms kNNI and MCI, each of the top performers CNI(SVM) and CNI(C4.5) has more than 7% and 15% higher average estimation accuracies over kNNI and MCI respectively. Statis tically, CNI(SVM) and CNI(C4.5) out-perform kNNI with 1-sided t-test p&lt; 2  X  10  X  8 and p&lt; 1  X  10  X  8 respectively, and even the worst performing CNI(RBF) and CNI(MLP) still slightly outperform kNNI, albeit with p&lt; 0 . 01 and p&lt; 0 . 1. As expected, the estimation accuracies of the imputers decrease as the missi ng ratio of the datasets increases.
For all of the 10 nominal imputers we investigated, the ranking of the nominal imputers in terms of their average es timation accuracies over the 12  X  5 datasets (five datasets with different missing ratios between 10% and 50% generated from each of 12 base datasets) is: CNI(SVM), CNI(C4.5), CNI(DecList), CNI(NB), CNI(LBR), CNI(RF), CNI(OneR), CNI(dTable), CNI(RBF), CNI(MLP), kNNI, and MCI. We will therefore use the best two, CNI(SVM) and CNI(C4.5), as nominal imputers in the investigations below. 3.2 The Impact of Nominal Im puters on the Classification Now we evaluate how much nominal imputers can improve the classification performance for instance-based learning algorithms.
 We work on instance-based algorithm s with different weighting schemes. Rather than setting the best k value for each different dataset, different nominal imputer and distance weighting scheme, we will instead simply use k =5forall cases.

Table 2 is a summary of the classification results, with cells that record the av-erage classification accuracy over the 12 nominal datasets. Figure 4(a) illustrates how nominal imputers improve the classification performance of instance-based algorithms on the dataset  X  X ustralian X  over various different missing ratios.
Table 2 and Figure 4(a) show that instance-based algorithms decrease their classification accuracy fast with the increase of the missing ratio of data. With the help of nominal imputers, kNN can achieve a statistically significant improve-ment of classification accuracy (here, this isbasedona1-sidedpairedt-test,with p&lt; 0 . 05; note all statistical claims are based on this test). This is true for all the datasets with all missing ratios we investigated (in the range of 10% and 50%), and for different distance weighting schemes of instance-based algorithms, including (1-distance) weighting, inverse distance weighting, and no weighting.
We anticipate a more accurate imputation technique will further improve the classification performance. Over the 12 datasets, kNN-CNI(SVM) and kNN-CNI(C4.5) each achieve about 2 X 4% higher average classification accuracies than kNN on the original incomplete datasets, with 1-sided paired t-test p&lt; 0 . 0006 and p&lt; 0 . 001 respectively. KNN using MCI as t he preprocessor slightly outper-forms the original kNN by 1 X 2% on average, with p&lt; 0 . 01. 3.3 The Impact of Nominal Imput ers on Other Machine Learned We have shown that using high-quality imputation techniques to preprocess in-complete data will increase the classificat ion performance for i nstance-based al-gorithms on nominal data. We now further investigate whether nominal imputers can help other classifiers perform bette r on incomplete nominal data, especially those with better missing data handling schemes  X  ie, where imputation may not be as critical.
 We work on three machine learning algorithms, na  X   X ve Bayes [6], C4.5 [8], and MLP neural network [5], on the 12  X  5 datasets. We use one baseline imputer, MCI, and one top performer of our CNI imputers, CNI(SVM), as the nominal imputers for the training data before learning the classifiers. We compare with the classifiers that do not use imputation before training. Table 3 summarizes the results, and Figure 4(b) depicts the average accuracy for MLP, MLP-MCI, and MLP-CNI(SVM) on datasets of different missing ratios.
 As Table 3 and Figure 4(b) indicate, accurate nominal imputers such as CNI(SVM) can help improve classification accuracy wh en the incomplete data has low missing ratios (e.g., at or below 20%), while using an inaccurate imputer such as MCI may lead to worse classification performance than the classifier that does not use any imputation for training data. When the missing ratio goes higher (i.e., missing ratio higher than 30%), neither CNI imputers nor MCI can help the learner to improve its classificat ion accuracy. This is p artially because many classifiers, such as NB, have more effective ways to deal with incomplete data than using imputation for highly sparse data. When the missing ratio is high, the nominal imputers will become less accurate (see Section 3.1), and the benefit of using imputation will be offset by its inaccuracy.

Why are accurate nominal i mputers effective for inst ance-based learning algo-rithms on both high-missing-ratio data and low-missing-ratio data? We suspect that this is partially due to the poor way that instance-based algorithms handle missing data, as using the maximized distance calculation can lead to poor classi-fication accuracy, while using CNI imputation to preprocess incomplete nominal data before learning the base classifier will perform better.

As many real-world dataset are missing under 20% of the values, there is practical significance for our finding that an accurate nominal imputer (such as our proposed CNI imputation) can improve the classification performance of many machine learners on such incompl ete datasets, even over learners that include effective built-in schemes for handling missing data. Incomplete nominal data often make it difficult for learning algorithms to pro-duce effective classifiers. Simple schemes of imputing the missing values, such as using the most common value for nominal missing data, are often not very effec-tive. In this paper, we explore methods for improving classification performance for machine learning algorithms on incomplete nominal data. We first propose an easy-to-implement and effect ive nominal imputation algorithm: classifier-based nominal imputation (CNI), which fills in the missing values of attribute f i by the values produced by a learned classi fier that takes the other values in that instance, where this classifier is learned using as the training data the instances that contain the observed values of f i . Our empirical results show that, of the 10 classification algorithms for imputation we investigated, the support vector machine (SVM) and C4.5 decision tree perform the best as CNI imputation learners. Our CNI imputers have significantly higher estimation accuracy than the commonly used nominal imputation techniques, kNN imputation (kNNI) and most common imputation (MCI). Applying CNI imputers such as CNI(SVM) and CNI(C4.5) as the preprocessors for incomplete nominal training data can impressively improve the classification p erformance of instan ce-based learning algorithms on incomplete datasets over the entire range of missing ratios that we investigated. CNI imputers are also found helpful in improving the classifica-tion performance of machine learners that have effective missing data schemes, such as NB and C4.5, when the datasets have missing ratios at or below 20%.
In our future work, we plan to improve our CNI imputation algorithm by using the previously imputed values for the later iterations of imputations, in the order of imputations based on how dense or informative the attributes are. We also plan to explore ways to improve the instance-based learning algorithm using nominal imputations for other data types, such as mixed data (with both numeric and nominal data) and ordinal data.
 This work was conducted when X. Su was with Varolii Corporation. R. Greiner was supported by the Alberta Ingenuity Centre for Machine Learning (AICML) and Canada X  X  Natural Science and Engineering Research Council (NSERC). This work was supported by the Data Mining and Machine Learning Laboratory at Florida Atlantic University. We also thank Randall Wald for his help.
