 There has been increasing interest for efficient techniques ent application domains. We present three algorithms for (1) bivariate correlation queries, (2) multivariate correlation queries, and (3) correlation queries based on a new correla-tion measure we introduce using dynamic time warping. To support these algorithms, we use a variant of the Compact Multi-Resolution Index (CMRI). In addition to conventional nearest neighbor and range queries supported by CMRI, the proposed algorithms compute all answers to user-defined, ad hoc and parametric correlation queries. The results of our experiments indicate a speed-up of two orders of magnitude over the brute force algorithm, and an order of magnitude improvement on average, while offering more functionalities than provided by existing techniques such as StatStream and the Spatial Cone Tree.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval  X  Query formulation; Search Process.
 General Terms: Algorithms, Experimentation, Perform-ance.
 Keywords: Time series, multi-resolution index, correlation analysis, dynamically time warped correlation.
Information and data streams expressed in terms of time series are often too large in their raw form to be used by hu-mans for decision making. Economists, for example, are con-fronted with huge amounts of stock market data which they must relate to micro-economic and macroeconomic data. For example, stock market data can stream every second generating petascale databases. Efficiency is a central issue for correlation analysis and search in time series, and is re-quired for many such applications, examples of which are as follows.
 1. Stock brokers need to correlate stock market patterns 2. Sensor network data could be mined using correlation 3. Click-stream and transactional data analysis could ben-4. In pattern matching, correlation queries could be used
In this paper, we consider bivariate correlation queries that use Pearson X  X  product-moment coefficient [26], such as  X  Find all stock market segments that are correlated to the U.S. interest rate measured between two dates at correlation value exceeding r  X , and multivariate correlation queries such as  X  Find all stock market segments that are correlated to the U.S. interest rate measured, the price of housing and the U.S. exchange rate at correlation value exceeding r . X  Multi-variate correlation search over indexed time series does not appear to have been studied before. We will also introduce a new correlation measure based on Dynamic Time Warp-ing (DTW) [25] and propose an algorithm which uses this measure for searching time series. We consider the Com-pact Multi-Resolution Indexing (CMRI) technique [19] and extend it in this work to support correlation queries. This paper presents these three algorithms and reports our per-formance evaluation results.

The rest of this paper is organized as follows. Section 2 provides a background and reviews related work. In Section 3, we introduces our solutions for the three types of corre-lation queries we are proposing. Section 4 focuses on our experiments and results. Concluding remarks and future work are presented in Section 5.
This section provides technical background and reviews work related to correlation analysis and queries on time se-ries data. For the background, we recall concepts and tech-niques in dimensionality reduction, indexing techniques for time sequences and spatial access methods.

Correlation analysis has proven useful for discovering pat-terns in time series and has been applied in different fields from social sciences to engineering. Its widespread use jus-tifies its numerous past and ongoing research. For example, Shasha and Zhu [28, 29] proposed a grid structure which detects patterns that have a correlation value higher than a constant R , with complexity O ( d ), where d is the dimen-sion of the data. However, to query the database with dif-ferent values of R , it is required to rebuild the whole grid structure, which amounts to scanning the whole database for each such query. Addressing this limitation, Huang et al. [15, 16, 17] proposed a specialized solution that uses a data structure called the  X  X patial Cone Tree. X  This solution is for bivariate correlation queries only, and none of other conven-tional query types, such as range queries and nearest neigh-bor queries over value space. Furthermore, it does not sup-port variable length queries. To handle ad hoc correlation queries, another indexing scheme is needed. Our approach is to make use of current tree based indexing techniques (MRI [20] and CMRI [19]) and extend them to support correlation queries incurring minimal if any performance penalty. This yields a solution which allows a maximum flexibility in the query types and in the query parameters for a wide range of search problems.

Our work on fast correlation analysis on time series data-bases relies on research on time series indexing starting with the GEMINI framework [10]. T his framework is developed based on the fact that a dimensionality reduction method is valid (i.e., no false dismissals will incur) for similarity search-ing if and only if it can be shown to be a lower bounding measure of the real distance function used. Further in this paper, we will see that for multivariate correlation queries, this lower bounding property needs to be replaced by an upper bounding in absolute value property. But for the mo-ment, the GEMINI framework is enough for our study of bivariate correlation queries.
Many dimensionality reduction methods have been pro-posed for time series data as operations on high dimensional data in general do not perform as well as on low dimensional data. Furthermore, it has been reported that distance met-rics in high dimensional space behave abnormally [1, 5, 24]. The norm of high dimensional random vectors tends towards the mean of all norms of the random vectors. Hence the Eu-clidean distance between two random vectors which itself is a random vector tends toward a constant [24]. In other words, as dimensionality grows, the contrast provided by usual metrics decreases [24]. We thus need to take into ac-count the trade-off between size and precision. Note that a common measure of effectiveness of a dimensionality reduc-tion technique is to compare the Euclidean distance between the real time sequence and the transformed time sequence.
Discrete Fourier Transforms (DFT) applied to time series have been proposed in [11]. The idea is to use a sliding win-dow of n values over time sequences and transform the n values and keep N values, where N is the number of DFT coefficients and is much smaller than n , the length of the sliding window. The DFT transformation takes O ( nlogn ) and needs 2 N values for storage since the transformed data requires storing real and complex values. However, the space consumption of DFT can be reduced to N assuming the complex conjugate property of DFT. The Discrete Wavelet Transform (DWT) [9], improves DFT as it runs in O(n) and requires N coefficients for storage. However, in practice, DWT coefficients are very large and for sequences shorter than 10,000 elements, DFT and DWT perform the same. Comparison between DFT and DWT is discussed in [2]. More recently, a technique called Piecewise Aggregate Ap-proximation (PAA) was introduced independently in [6, 13]. This method performs as well as DWT when the size n of the sliding window is a power of 2, but performs better than DWT for other values of n . One of the issues in DWT is the window size that has to be a power of 2. When it is not the case, DWT pads the values with 0 X  X  to make the window size attain a power of 2, hence reducing the precision of the transform. As in DWT, the run time complexity of PAA is O ( n ). As an improvement over PAA, Adaptive Piecewise Constant Approximation (APCA) [7] was proposed which runs in O ( nlogn ) and which offers a better approximation than PAA if the number of segments calculated are equal. However, APCA needs 2 N coefficients for storage, a leading value and an average. All the above mentioned methods are lower bounding and satisfy the GEMINI framework require-ments [10].
Our solution will make use of PAA [6, 13] for being a fast dimensionality reduction technique. In addition to be lower bounding, the correlation value between two PAA sequences, obtained by calculating the cosine of the two sequences, is upper bounding in absolute value, which is a requirement, as we will see in Section 3.6, for multivariate correlation analysis. Furthermore, PAA can be used to index time se-ries under Dynamic Time Warping (DTW) distances [23]. Another property of PAA (though not exclusive to PAA), is that normalized PAA values can be obtained at query time dynamically from non-normalized coefficients and vice-versa, pending a small constant time cost.
The GEMINI framework [10] and the indexes for similar-ity search operations we will discuss in the next section do not hard code the use of a particular spatial access method even though they often use R-trees [14] or R*-trees [3] for illustrative purposes. The R-tree is the most generic spa-tial access method that can be used, but in most correlation queries, a specialization of spatial access methods, metric access methods, can be used since correlation queries can be mapped to Euclidean distance queries, as we will see in section 3.1. Multivariate correlation queries are special case of quadratic form distances, which are also metric measures, but since the cross-correlation matrix is known only at query time, in the most general case, we cannot directly use metric data structures. For Dynami cally Time Warped Correlation (DTWC), we cannot use metric access methods since DTW is not metric and does not satisfy the triangle inequality [23].
It is well known that the performance of R-trees decreases as the dimensionality of the data increases [27]. This jus-tifies the use of dimensionality reduction techniques that propose a trade-off between precision and size/performance or the use of high dimensional indexes such as the M-Tree [9] or the X-Tree [4]. Even then, benchmarks rarely report dimensionalities higher than 16.
To improve the performance of correlation queries on time series databases, we can use an indexing scheme to search the data efficiently. Our approach is to implement correlation queries over more general purpose indexes that are capable of supporting different types of queries while remaining ef-ficient. Therefore, it makes sense to review past indexing solutions to similarity searches on time series databases. All solutions described here are data structure agnostic: they can make use of R-trees, X-trees, M-trees, or any other spa-tial index structure.

A first indexing solution to fixed length complex queries on time series databases was first proposed in [11]. The original I-Adaptive index made use of DFT to map a high dimensional value space to a low dimensional feature space, although any dimensionality reduction technique could have been used. Distance measures over DFT coefficients lower bound the Euclidean distance in value space. Hence, the I-Adaptive index guarantees no false dismissals. The Multi-Resolution Index (MRI) proposed in [20] deals with variable length queries more efficiently than I-Adaptive. An MRI in-dex is basically a collection of I-Adaptive indexes stored at different resolutions on which specialized search algorithms can be applied. Finally, the Compact Multi-Resolution In-dex (CMRI) proposed recently [19] improves over the MRI index in terms of size, speed, and precision. In this paper, we use the CMRI index but with a modification: instead of using APCA as suggested originally in CMRI, we use PAA, for the reason mentioned earlier. We also add the notion of  X  X igests X  in order to be able to normalize data at query time (see Section 3.2).

All these indexing schemes for time series data use a slid-ing window over sequences in order to be able to match pat-terns that may start anywhere in a given sequence. Hence, to match a pattern c, d in a sequence a, b, c, d , we need to store the following windows at resolution 2: a, b , b, c , c, d . Each of these windows can be stored as multidimen-sional points in a spatial or metric access method.
As our work relies on and uses the CMRI technique, we quickly recall this indexing technique from [19]. The CMRI technique improves the MRI technique developed in [20]. MRI is a multi-dimensional index structure that supports range and nearest neighbor searching for variable length queries originally proposed in [20]. MRI uses R-tree based, multi-dimensional indexes at several resolution, and is built as follows. For sequences s 1 ,s 2 , ..., s n in the database, MRI stores a grid of I-adaptive trees T i,j where i ranges over res-olutions a to b ,and j ranges over the sequences s 1 to s For this tree, MRI stores MBRs corresponding to database sequences at different resolution levels. More precisely, T is the collection of MBRs for window size w =2 i (i.e. for resolution i ) corresponding to sequence s j . A separate tree T i,j is built for the MBRs of each sequence s j at different resolution i .The i th row of the index is represented as the is represented as the collection C j = T a,j , ..., T b,j
The CMRI index proposed in [19]improves over the size of MRI by using just one node for all data sequences but the first, without loss of information. That is, the leaves of the I-adaptive index in CMRI have the same information as the leaves in MRI. This ensures that there will be no false dis-missals or false hits using CMRI. CMRI further combines all the nodes at each resolution in CMRI to form an I-adaptive index. In other words, we have a single I-adaptive index at each resolution for the entire time series data.
Our goal is to develop algorithms for fast correlation anal-ysis which are flexible for solving problems from different do-mains and different requirements. A desired solution should support variable parameter queries, variable length queries, variable number of parameters , and different correlation mea-sures. To support variable parameter queries, we will use indexing schemes that do not hard-code query parameters. To handle variable length queries, we will use a variant of the CMRI [19] which also supports usual similarity queries such as range and nearest neighbor queries. Finally, to sup-port variable number of parameters and different correlation measures, we propose three algorithms that use this variant of the CMRI index. A frequently used measure for bivariate correlation is the Pearson X  X  product-moment coefficient [26], which we use in our correlation queries and is defined as follow: where x and y are two sequences and R is the Pearson X  X  coefficient with  X  1  X  R  X  1. This measure reflects the de-gree of linear relationship between the two input variables: +1 indicates perfect positive linear relationship, -1 indicates perfect negative, and 0 indicates there is no linear relation-ship between the two variables.
 Using PAA as dimensionality reduction and a variation of CMRI as indexing scheme, we will show that bivariate cor-relation queries on PAA feature spaces can be done without false dismissals. Given the following mapping from bivariate correlation to Euclidean distance D [29], it holds that: Given that w is the window size, R ( x, y )isthePearson product-moment correlation coefficient between x and y and  X   X  is the normalization operator, we note that the following condition holds: where D LB is the lower bounding distance measure in fea-ture space and capitalized variables X and Y denote PAA reduced series. Note that D LB is given by the following [6]:
The previous condition shows that we can do bivariate correlation analysis without modifying drastically the CMRI index and PAA algorithm. For queries where correlation is negative, we have the following condition: Algorithm 1 shows steps of performing bivariate correlation queries on a compact multi-resolution index. This algorithm is a slight modification from [19]: the range variable e 0 is set as a function of the correlation parameter r .
 Algorithm 1 Bivariate Correlation Algorithm Input: A correlation value r , a query window q ,theroot Output: The results results . 1: p i = PARTITION ( q ) 2: e 0 = 3: for i =0 to c do 4: results = RANGE SEARCH ( root , e i , p i ) 6: end for 7: results = POSTPROCESS(results)
It is often required to store a non-normalized feature space since standard complex distance queries need to be per-formed on the non-normalized value space. Although we want to keep our feature and value spaces unormalized, the bivariate correlation definition presented above requires that the feature space on which queries are performed be normal-ized. Here we show that normalization can be done at query time when using PAA. At query time, the following transfor-mation can be applied to the non-normalized feature space of dimension N of a value space of dimension n : Figure 3: Segmentation of solution space with r = 0 . 97 and grayed out rules. where i ranges from 1 to N . We can simply add  X  X igests X  to our data structure, as proposed in [29], for the mean and the standard deviation. The digest kept at every node for the variance  X  and the average  X  can be propagated recursively to containing nodes as follow: The subscripts LOW and HIGH denote the low and high value of the containing Minimum Bounding Rectangle (MBR) used in CMRI.
Multivariate correlation has the following quadratic form when the x i  X  X  are known variables given at query time [22]: where  X  x i ,y is the vector composed of y  X  X  Pearson correla-tion coefficients with x i and R ( x i ,x j ) denotes the correla-tion matrix of variable x i with x j such that r x i ,x j i = j . Note that the matrix is symmetric and that when x i = { x 1 } , the multivariate formula reduces to Pearson X  X  coefficient. Geometrically, multivariate correlation measures how well a hyperplane can fit the measured data.

Our solution for multivariate correlation queries is imple-mented on a standard R-tree [14] (R*-trees [3], R + -trees [12], or X-trees [4] could also be used without modification to the algorithm). We focus on the 2D case where we have 2 query time series such as:  X  Find all stock market segments that are correlated to the U.S. interest rate measured, the price of housing and the U.S. exchange rate at correlation value exceeding r  X . However, the algorithm described can be extended to take on more than 2 query time series.
From the quadratic form definition of multivariate corre-lation, we can describe a solution space whose boundary is a hyper-ellipsoid. Figure 3 shows an example of the 2D case. Solutions to a multivariate correlation query where corre-lation needs to be greater than or equal to a user-defined variable r lie on the boundary or outside the boundary.
While proposed for a different purpose, we adapted an algorithm from [23] to segment the upper and lower parts of the solution space. Hence, after sampling points of the solution space boundary, we can generate a lower upper-bounding segmentation U i and a upper lower-bounding seg-mentation L i from the following formulas for the 2D solution spacecase(seeFigure3): In general, for an n-dimensional solution space, a set { U,L is required for each unordered pair of coordinate axis. From this segmentation of the solution space, a set of control points is obtained, each of which include the vertices of the seg-mentation. A set of rules { R i } can be derived from the inner control points c i with coordinates c ( x ) i and c segmentation for the 2D case, but can be generalized to n-dimensions. Algorithm 2 shows the derivation of { R i } for the 2D case.
 Algorithm 2 Generate Rules Input: Lower upper-bounding control point in U and upper Output: Asetof rules . 1: rules = {} 2: for every cp i in L from i =0 do 3: repeat 4: ADD RULE : { x  X  cp ( x ) i AND y  X  cp ( y ) i } TO rules 5: until cp ( y ) i reaches max ( L i ) 6: for i = remaining control points do 7: ADD RULE : { x  X  cp ( x ) i AND y  X  cp ( y ) i } TO rules 8: end for 9: end for 10: for every cp i in U from i =0 do 11: repeat 12: ADD RULE : { x  X  cp ( x ) i ANDy  X  cp ( y ) i } TO rules 13: until cp ( y ) i reaches min ( U i ) 14: for i = remaining control points do 15: ADD RULE : { x  X  cp ( x ) i ANDy  X  cp ( y ) i } TO rules 16: end for 17: end for 18: ADD BOUNDARY RULES() #see grayed areas in Fig.1 19: return rules
The number of rules needed is O ( c ), where c is the number of control points. These rules can be implemented simply as IF-ELSE clauses. From Figure 3, we can see that if a point in solution space is a valid solution, then it must pass at least one of the checks in { R i } , even though the converse may not be true. An  X  X pper-bounding in absolute value X  property guarantees that no false dismissals will incur when using {
R i } to prune the solution space as we will see in Section 3.6. Notice that the precision of our pruning depends on the number of control points chosen to segment the solution space.
Transformation of distance measures for quadratic form distance measures was first introduced in [21]. Here, we have adapted those techniques in our context. Once a set P i of points is found from the segmentation of the solution space, we determine if they are inside or outside the bound-ary. Since the boundary is a rotated quadratic form hyper-ellipsoid, one way to decide this is to compute a transforma-tion matrix composed of two linear transformation matrices. The first matrix rotates the hyper-ellipsoid such that its ma-jor and minor axes are orthogonal to the coordinate axes. It can be obtained, according to the Principal Axes Theorem, from the normalized eigenvectors of the correlation matrix R x i ,x j . In the 2D case, this matrix will always be the same, given that R x i ,x j is a symmetric matrix with elements on its diagonal that are equal, as follows: The second linear matrix transforms the hyper-ellipsoid to a hyper-sphere, as follows.
 where the m i  X  X  are the value of the radius of the major/minor axes parallel to the hyper-dimensional coordinate system. The final transformation matrix is given as follow: Each point of P i is multiplied by the transformation matrix, and a point is a solution if and only if the following condition holds: the norm of the transformed point is larger than the radius of the hyper-sphere (usually set to 0.5): This condition is a second pruning step which yields a set of points that are exact solution to the multivariate correlation query when no dimensionality reduction is used and may yield false positives if one is used.
To perform the pruning process described previously, we map the value space or feature space to bivariate correlation space since the solution space is described in terms of bi-variate correlation variable s. If we use an index over feature space, we must also insure that the mapping from feature space to bivariate correlation space is such that the feature space correlation value upper bounds the real bivariate cor-relation in absolute value. The cosine definition of bivariate correlation satisfies this condition and corresponds to the cosine of the angle between the two target vectors. It is defined as follows: Hence, we have the following: where X and Y are the PAA feature space sequences, and x and y are the corresponding sequences in the value space.
To implement the multivariate correlation algorithm, we need to be able to apply what we described previously to an index such as the R-tree, for which we define an algorithm to prune the solution space using the segmentation rules { R over Minimum Bounding Rectangles (MBR) of the R-tree. To achieve this, we propose the concept of spanning angles which corresponds to the minimum and maximum angles  X  min and  X  max that an MBR can have with a query point Q . To speed up the computation, in order to prune MBR X  X  that do not contain points outside of the solution space seg-mentation given in Section 3.4, we can use the following facts, adapted from [17]: Hence, if a rule R i generated from the segmented solution space is of the form  X  X  &gt; 0.9 X , this rule can be mapped to  X   X  min &lt;cos mented solution space is of the form  X  X  &lt; -0.9 X , this rule can be mapped to  X   X  max &gt; X   X  cos  X  1 (0 . 9). X  In general, equa-tions (16) and (17) above use  X  min , and equations (18) and (19) use  X  max . This approach can be extended to multiple correlation parameters. We n ext summarize our algorithm for multivariate correlation queries.
Algorithm 3 below shows steps of performing the mul-tivariate correlation queries. It first obtains the solutions space transformation matrix described in Section 3.5 from the quadratic form matrix of equation (9). This is possible because we know the values x i  X  X  at query time. It then seg-ments the solutions space given by the quadratic form ma-trix into upper-bounding and lower-bounding segments for the 2D case from equations (10) and (11). Finally, through tree traversal operations, we obtain for each MBR the min-imum and maximum spanning angles to a query point and compare them against the set of rules { R i } obtained from segmentation of the solution space.
As the quadratic form equation for multivariate correla-tion is a generalization of the bivariate correlation equation, the algorithm for multivariate correlation search is a gener-alization of the bivariate correlation search algorithm. For 1-dimensional solution spaces (i.e., bivariate correlation), the quadratic form equation for multivariate correlation reduces to: Graphically, a 1-dimensional solution space is a line. Fur-thermore, the rules generated for such a solution space is directly given by the previous expression R ( y, x 1 ) 2  X  general, the number of terms in a rule for an n-dimensional solution space is n and the number of rule types (rules hav-ing the same inequality operator for each term in the rule) is k ,where k is the number of quadrants in the solution space.
Exact indexing of dynamic time warping distance mea-sures was proposed in [23] using PAA as a dimensionality reduction technique. It uses lower bounding distance mea-sures of dynamic time warping distance that can apply to value space (LB KEOGH) or to feature space (LB PAA). Algorithm 3 Multivariate Correlation Algorithm Input: A correlation parameter r , query windows q i ,the Output: Asetof results . 1: p i = PAA ( q i ) 2: M R 1 = ROTATION MATRIX( q i ,r) 3: M R 2 = SKEW MATRIX( q i ) 5: U = UPPER SEGMENTATION( q i ) 6: L = LOWER SEGMENTATION( q i ) 7: rules = GENERATE RULES( U , L ) 8: temp results = TRAVERSE TREE(root, rules, p i ) 9: for every result in temp result do 10: if | result  X  M R | &gt; radius then 11: ADD result TO results 12: end if 13: end for 14: results = POSTPROCESS( results ) 15: 16: ADJUNCT METHODS: 17: 18: ROTATION MATRIX( q i ,r) 20: COMPUTE M R 1 from eigenvectors of R x i ,x j 21: 22: SKEW MATRIX( q i ,r) 24: COMPUTE M R 2 from major and minor axis 25: 26: UPPER SEGMENTATION( q i ) 27: x i = sample points of the lower-bounding solution space 29: 30: LOWER SEGMENTATION( q i ) 31: x i = sample points of the upper-bounding solution space 33: 34: TRAVERSE TREE(root, rules, p i ) 35: if ! IS LEAF(root) then 36: for every node in root do 37: (  X  min , X  max )= GET MBR SPANNING ANGLES() 38: if CHECK RULES(rules,  X  min , X  max ) then 39: TRAVERSE TREE(node,rules, p i ) 40: end if 41: end for 42: else 43: if COMPUTE CORRELATION( p i , node ) then 44: ADD node TO results 45: end if 46: end if 47: return results 48: 49: CHECK RULES(rules,  X  min , X  max ) 50: if rule is of type r &gt; k then 51: USE  X  min &lt;cos  X  1 ( k ) 52: end if 53: if rule is of type r &lt; k then 54: USE  X  max &gt; X   X  cos  X  1 ( k ) 55: end if Dynamic time warped distance is obtained from the follow-ing recurrence equation where d ( x 1 ,x 2 )isanygivendistance measure (in our case, the Euclidean distance): DT W ( i, j )= d ( q i ,c j )+ min ( DT W of adjacent cells ) (21) Since our algorithm for bivariate correlation queries maps correlation values to Euclidean distances, we next define a new measure, called Dynamically Time Warped Correlation (DTWC), from the original DTW formula by applying our bivariate correlation condition to it. This yields: where DT W ( X, Y ) is the DTW distance between X and Y [25],  X  x denotes normalized x ,and n is the pattern length. DTWC has the same advantages over correlation that DTW distance has over Euclidean distance: it can adapt itself to unequal length and warped data sets. For example, con-sider the following two time series: x = la -1,1,1,1,4, 5, 7, 5, 4 and y = -1.1, 1.1, 4.1, 5.1, 7.1, 5.1, 4.1, 4.1, 4.1 , whose normalization are  X  x = -2.1547, -0.1547, 2.8453, 3.8453, 5.8453, 3.8453, 2.8453, 2.8453, 2.8453 and  X  y = -2.6665, -0.4665, -0.4665, -0.4665, 2.6335, 3.5335, 5.5335, 3.5335, 2.5335 . We can see that these series are simply shifted and skewed. The standard correlation between the two series is 0.5280 and would not be picked up by most correlation queries. However, the DTWC between them is 0.9088 (considering a Sakoe-Chiba band of 2) and could be picked up by a DTWC query with r&gt; 0 . 9. DTWC be-tween both series can be calcula ted from the correlation be-tween the DTW expansion of two series which is of length 11. This yields sequences -2.1547, -0.1547, -0.1547, -0.1547, 2.8453, 3.8453, 5.8453, 3.8453, 2.8453, 2.8453, 2.8453 and -2.6665, -0.4665, -0.4665, -0.4665, 2.6335, 3.5335, 5.5335, 3.5335, 2.5335, 2.5335, 2.5335 . Note that the Euclidean distance between the normalized series is 6.7794 which is larger than the DTW distance of 1.087.

Let x and y be time series. We know the following holds between the DTW distance and the Euclidean distance D : The DTW distance is the largest when no warping occurs or, in other words, when the warping path is a straight line. When this occurs, it holds that DT W ( x, y )= D ( x, y ). If the warping path deviates, this means that it has found a path where DT W ( x i ,y i ) is smaller than D ( x i ,y i seen from the DTW distance recurrence relation.
 Since the DTW distance is always smaller or equal to the Euclidean distance, the DTWC will always be greater than or equal to the standard correlation. That is: This property shows that correlation queries using DTWC might return time series that would not have been returned by correlation queries using Pearson X  X  coefficient. As corre-lation queries over sliding windows were robust face to trans-lated datasets, DTWC is robust faced to skewed datasets. Note that the exact value of the DTWC can be computed from the standard correlation between the DTW expansions of two time series. Algorithm 4 below formally states the steps for performing DTWC queries. The proof of correct-ness of this algorithm (i.e., no false dismissals will occur) relies on the correctness of the DTW range query, which is shown in [23]. We do not show details on DTW range queries and the reader can refer to [23]. The basic idea behind DTW range queries is to find an upper bound and a lower bound to a query sequence from which a measure called LB KEOGH, which lower bounds DTW, can be cal-culated. For PAA sequences, an equivalent lower bounding measure called LB PAA exists.
 Algorithm 4 DTW Correlation Algorithm Input: A correlation value r , a query window q , the root of Output: The results results . 1: p i = PARTITION ( q ) 2: e 0 = 3: for i =0 to c do 4: results = DTW RANGE SEARCH ( root , e i , p i ) 6: end for 7: results = POSTPROCESS(results)
We have evaluated the performance of our algorithms for fast correlation analysis and queries on time series datasets on a typical desktop PC using 3.2 GHz Intel Pentium IV processors with 504 MB of RAM running Windows XP. Our test programs were written in C++ and MATLAB programs were used to verify correctness of the test programs. We ran 15 runs of different correlation query instances that returned from 0 to 1000 records. After performance analysis, our al-gorithms were shown to provide online response times to complex correlation queries in standard computing environ-ments and showed significant improvement over brute force sequential algorithms. For our experiments, we have used real life stock market data, about 800 stocks sampled ev-ery day for a period of 400 days, of total size approximately 3 MB generating an index size of approximately 115 MB (using 5 resolutions and reduced data dimensionality of 8).
Also we have tested the performance of complex correla-tion queries with single tree accesses to our indexing scheme, which is a variant of CMRI. When multiple tree accesses are needed, in the worse case (when the query and the indexed data do not allow pruning between tree accesses which hap-pens when there is an exact match), the execution time and the number of disk I/O X  X  required increase linearly with the number of tree accesses. In both cases where data was in memory and on disk, for the bivariate and multivariate cor-relation query algorithms, ev en when we have up to 5 differ-ent tree accesses for a single query, we achieved an order of magnitude improvements in run time over brute force algo-rithms. Theoretically, point queries on R-trees offer O ( logn ) performance, where n is the number of data points. The complexity of range queries is O ( n +( n  X  1) / ( fanout in the worse case which occur when all the n points are retrieved. Since the number of results is often very small compared to n , the performance is closer to point queries, on average.

As mentioned earlier, to the best of our knowledge, no other correlation analysis algorithm offers the same flexi-bility with ad-hoc, user-defined queries. However, here we compare our work to other solutions described in the litera-ture, i.e., StatStream [28, 29] and the Spatial Cone Tree [15, 16, 17], for fixed correlation parameter values (to compare with StatStream) and fixed query length (to compare with the Spatial Cone Tree). StatStream offers a worse case run time of O ( d ), where d is the number of dimensions of the grid structure used to index the time series data. Hence, for queries with fixed correlation parameters, StatStream offers the best solution. However, if we want variable values of cor-relation parameters through implementing StatStream, we would need to scan the whole dataset in order to regenerate the grid structure, i.e., similar to the brute force sequen-tial algorithm. [15, 16, 17] report 45% to 85% savings on computational costs for the Spatial Cone Tree solution. An advantage of our proposed algorithms is that they scale well in comparison: for single tree accesses (fixed query lengths), our solution offers 80% to 95% savings on I/O operations compared to the brute force sequential algorithm on real life data. The main advantage of our solution is support for ad-hoc correlation queries, multivariate correlation queries, and dynamically time warped correlation queries.

Figures 4 and 5 show average speed-ups over the brute force algorithms for queries with 0 to 1000 results. The in-memory case measures performance in terms of run time, and the on-disk case measures in terms of I/O operations. Performance is measured against different values of MAXN-ODES, which is the number of nodes in an R-tree node.
As for the performance analysis of DTWC queries, they have the same complexity as DTW queries and should have similar performance indicators as the latter since a DTWC query is more or less a DTW query over a normalized data set. Interested readers can refer to [23] for detailed perfor-mance evaluations.
We introduced algorithms for correlation analysis and queries and described how they differ from those proposed in [15, 16, 17, 28, 29]. The fastest solution is StatStream which runs in O ( d ). However, the grid structure it uses to store the sliding windows is built and optimized for a fixed corre-lation query parameter. Hence, to use variable correlation query parameters, one would need to rebuild the entire in-dex which means scanning the whole time series database. The Spatial Cone Tree solves this problem by making use of a specialized tree-based index. However, the original Spatial Cone Tree did not support variable length queries efficiently. Both StatStream and Spatial Cone Tree are specialized in-dex structures built for bivariate correlation analysis. Our solutions make use of R-trees and MRI/CMRI [20, 19] vari-ants. Hence, standard complex variable length similarity queries such as range queries and nearest neighbor queries can be handled. Furthermore, the algorithms we proposed solve variable bivariate correlation queries, multivariate cor-relation queries and the newly introduced DTWC query.
As a future work, we plan to extend our solution for fast correlation analysis to distributed and parallel environ-ments. This makes sense as the huge data to be analyzed may be scattered around multiple hosts. This work was supported in part by Natural Sciences and Engineering Research Council (NSERC) of Canada, and by Concordia University. [1] C. Aggarwal, A. Hinneburg, and D. Klein. On the [2] D. Agrawal, Y. Wu, and A. E. Abbadi. A comparison [3] N. Beckmann, H. Kriegel, R. Schneider, and [4] S. Berchtold, D. Keim, and H. Kriegel. The x-tree : [5] K. Beyer, J. Goldstein, R. Ramakrishnan, and [6] K. Chakrabarti, E. Keogh, S. Mehrotra, and [7] K. Chakrabarti, E. Keogh, S. Mehrotra, and [8] A. Chakraborti. An outlook on correlations in stock [9] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An [10] C. Faloutsos. Searching multimedia databases by [11] C. Faloutsos, Y. Manolopoulos, and M. Ranganathan. [12] C. Faloutsos, T. Sellis, and N. Roussopoulos. The [13] C. Faloutsos and B. Yi. Fast time sequence indexing [14] A. Guttman. R-trees: A dynamic index structure for [15] Y. Huang, V. Kumar, S. Shekkara, and P. Zhang. [16] Y. Huang, V. Kumar, S. Shekkara, and P. Zhang. [17] Y. Huang, V. Kumar, S. Shekkara, and P. Zhang. [18] R. Juday, A. Mahlanobis, and B. V. Kumar.
 [19] S. Kadiyala and N. Shiri. A compact multi-resolution [20] T. Kahveci and A. Singh. Optimizing similarity search [21] R. Kataoka, Y. Sakurai, S. Uemura, and [22] K. Kelley and S. Maxwell. Sample size for multiple [23] E. Keogh and C. Ratanamahatana. Exact indexing of [24] J. Lee and M. Verleysen. Nonlinear dimensionality [25] C. Myers and L. Rabiner. A level building dynamic [26] K. Pearson. Mathematical contributions to the theory [27] T. Sellis and Y. Theodoridis. A model for the [28] D. Shasha and Z. Zhu. Statstream: Statistical [29] D. Shasha and Z. Zhu. High performance discovery in
