 We consider the problem of information retrieval evaluation and the methods and metrics used for such evaluations. We propose a probabilistic framework for evaluation which we use to develop new information-theoretic evaluation metrics. We demonstrate that these new metrics are powerful and generalizable, enabling evaluations heretofore not possible.
We introduce four preliminary uses of our framework: (1) a measure of conditional rank correlation, information  X  , a powerful meta-evaluation tool whose use we demonstrate on understanding novelty and diversity evaluation; (2) a new evaluation measure, relevance information correlation , which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultane-ously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference , which allows us to determine whether systems with similar perfor-mance are in fact different.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (efficiency and ef-fectiveness) Experimentation; Theory; Measurement Information Retrieval, Search Evaluation
In order to improve search engines, it is necessary to ac-curately measure their current performance. If we cannot measure performance, how can we know whether a change was beneficial? In recent years, much of the work on infor-mation retrieval evaluation has focused on user models [7, 18] and diversity measures [1, 10, 24] which attempt to accu-rately reflect the experience of the user of a modern internet search engine. However, these measure are not easily gener-alized. In this work, we introduce a probabilistic framework for evaluation that encompasses and generalizes current eval-uation methods. Our probabilistic framework allows us to view evaluation using the tools of information theory [11]. While our framework is not designed to coincide with user experience, it provides immediate access to a large number of powerful tools allowing for a deeper understanding of the performance of search engines.

Our framework for evaluation is based on the observa-tion that relevance judgments can also be interpreted as a preference between those documents with different rele-vance grades. This implies that relevance judgments can be treated as a retrieval system, and that evaluation can be considered as the  X  X ank X  correlation between systems and relevance judgments. To this end, we develop a probabilis-tic framework for rank correlation based on the expecta-tion of random variables, which we demonstrate can also be used to compute existing evaluation metrics. However, the true value of our framework lies in its extension to new information-theoretic evaluation tools.

After a discussion of related work (Section 2), we intro-duce our framework in Section 3. In Section 4, we demon-strate that our framework allows for an information theoretic understanding of Kendall X  X   X  [17], information  X  , which we use to define a conditional version of the rank correlation be-tween two lists conditioned on a third. In Section 5, we de-fine a new evaluation measure based on our framework: rel-evance information correlation . We validate our measure by showing that it is highly correlated with existing measures such as average precision (AP) and normalized discounted cumulative gain (nDCG). As a demonstration of the versa-tility of our framework when compared to, for example, user models, we show that our measure can be used to evaluate a collection of systems simultaneously (Section 6), creating an upper bound on the performance of metasearch algorithms. Finally, in Section 7, we introduce information difference , a powerful new tool for evaluating the similarity of retrieval systems beyond simply comparing their performance.
This material is based upon work supported by the Na-tional Science Foundation under Grant No. IIS-1256172. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation (NSF).
Search systems are typically evaluated against test collec-tions which consist of a corpus of documents, a set of top-ics, and relevance assessments X  X hether a subset of those documents are relevant with respect to each topic. 1 example, the annual, NIST-sponsored Text REtrieval Con-ference (TREC) creates test collections commonly used in academic research. The performance of systems is assessed with regards to a specific task. A traditional search task is to attempt to rank all relevant documents above any non-relevant documents. For this task, systems are evaluated in terms of the average trade-off between their precision and recall with respect to multiple topics. For a given topic, Let g  X  { 0 , 1 } be the relevance grade of the document at rank i , and let R be the number of relevant documents in the collection. At rank k , The trade-off between the two is measured by average preci-sion , which can be interpreted as the area under the precision-recall curve.
 Average precision does not include information about docu-ment quality and degrees of relevance, and is an inherently recall-oriented measure. It is therefore not suitable for eval-uating commercial web search engines.

With the growth of the World Wide Web, test collec-tions began to include graded, non-binary relevance judg-ments, e.g. G = { non-relevant, relevant, highly relevant } or G = { 0 ,..., 4 } . To make use of these graded assess-ments, J  X  arvelin and Kek  X  al  X  ainen developed normalized dis-counted cumulative gain (nDCG) [15]. nDCG also has the advantage that it can be evaluated at arbitrary ranks, and can therefore be used for precision-oriented tasks like web search.

Unlike average precision, which has a technical interpre-tation, nDCG can be best understood in terms of a model of a hypothetical user. In this model, a user will read the first k documents in a ranked list, deriving utility from each document. The amount of utility is proportional to the doc-ument X  X  relevance grade and inversely proportional to the rank at which the document is encountered. We first define discounted cumulative gain (DCG).
 Since the range of DCG will vary from topic to topic, it is necessary to normalize these scores so that an average can
For historical reasons, the set of relevance assessments is often referred to as a QREL . be computed. Normalization is performed with regard to an ideal ranked list. If DCG 0 @ k is the maximum possible DCG of any ranked list of documents in the collection then
However, one does not always know how many documents are relevant at each level, and therefore the ideal list used for normalization is only an approximation. Moffat and Zo-bel [18] introduced a measure, rank-biased precision (RBP), that addresses this issue. In RBP, the probability that a user will read the document at rank k is drawn from a geo-metric distribution, whose parameter,  X   X  [0 , 1), models the user X  X  persistence. Given a utility function u : G  X  [0 , 1], commonly defined as where d is the maximum possible relevance grade, RBP is defined as the expected utility of a user who browses accord-ing to this model.
 Since RBP is guaranteed to be in the range [0,1) for any topic and  X  , it does not require normalization.

Craswell et al. [12] introduced the Cascade model of user behavior. In this model, a user is still assumed to browse documents in order, but the probability that a user will view a particular document is no longer assumed to be indepen-dent of the documents that were viewed previously, i.e. a user is not assumed to stop at a particular rank, or at each rank with some probability. Instead, the user is assumed to stop after finding a relevant document. This implies that if a user reaches rank k , then all of the k  X  1 documents ranked before it were non-relevant. Craswell et al. demon-strated empirically that this model corresponds well to ob-served user behavior in terms of predicting the clickthrough data of a commercial search engine.

Chapelle et al. [7] developed an evaluation measure, ex-pected reciprocal rank (ERR), based on the Cascade model. Let R i denote the probability that a user will find the doc-ument at rank i to be relevant. Then in the Cascade model, the likelihood that a user will terminate his or her search at rank r is If we interpret the previously defined utility function (Equa-tion 6) as the probability that a user will find a document relevant, i.e. R i = u ( g i ), then we can computed the ex-pected reciprocal rank at which a user will terminate his or her search as
In this work, we propose an alternative, information-theoretic framework for evaluation. The first step is to reformulate these measures as the expected outcomes of random exper-iments. Computing evaluation measures in expectation is not uncommon in the literature, and we are not the first to suggest that reformulating an evaluation measure as an ex-pectation allows for novel applications. For example, Yilmaz and Aslam [30] formulated average precision as the expec-tation of the following random experiment: 1. Pick a random relevant document, 2. Pick a random document ranked at or above the rank 3. Output 1 if the document from step 2 is relevant, oth-Their intention was to accurately estimate average precision while collecting fewer relevance judgments (a process also applied to nDCG [32]). However, this formulation led to new uses, such as defining an information retrieval-specific rank correlation measure,  X  AP [31], and a variation of average precision for graded relevance judgments, Graded Average Precision (GAP) [21].

Our work uses pairwise document preferences rather than absolute relevance judgments. The use of preferences is somewhat common in IR. For example, many learning-to-rank algorithms, such as LambdaMart [3] and RankBoost [13], use pairwise document preferences in their objective func-tions. Carterette et al. [4, 5] explored the collection of prefer-ence judgments for evaluation, showing that they are faster to collect and have lower levels of inter-assessor disagree-ment. More recently, Chandar and Carterette [6] crowd-sourced the collection of conditional document preferences to evaluate the standard assumptions underlying diversity evaluation, for example that users always prefer novel doc-uments. Relative document preferences can also be inferred from the clickthrough data collected in the logs of commer-cial search engines [16]. These preferences can be used for evaluation without undertaking the expense of collecting rel-evance judgments from assessors.
Mathematically, one can view the search system as pro-viding a total ordering of the documents ranked and a partial ordering of the entire collection, where all ranked documents are preferred to unranked documents but the relative prefer-ence among the unranked documents is unknown. Similarly, one can view the relevance assessments as providing a par-tial ordering of the entire collection: in the case of binary relevance assessments, for example, all judged relevant docu-ments are preferred to all judged non-relevant and unjudged documents, but the relative preferences among the relevant documents and among the non-relevant and unjudged doc-uments is unknown. Thus, mathematically, one can view retrieval evaluation as comparing the partial ordering of the collection induced by the search system with the partial or-dering of the collection induced by the relevance assessments .
To formalize and instantiate a framework for comparing such partial orderings, consider the simplest case where we have two total orderings of objects, i.e., where the entire  X  X ollection X  of objects is fully ranked in both  X  X rderings. X  While such a situation does not typically arise in search sys-tem evaluation (since not all documents are ranked by the retrieval system nor are they fully ranked by relevance as-sessments), it does often arise when comparing the rankings of systems induced by two (or more) evaluation metrics; here Kendall X  X   X  is often the metric used to compare these (total order) rankings.

In what follows, we define a probabilistic framework within which to compare two total orderings, and we show how tra-ditional metrics (such as Kendall X  X   X  ) are easily cast within this framework. The real power of such a framework is shown in subsequent sections: (1) the framework can be easily generalized to handle the comparison of two partial orderings, such as arise in search system evaluation, and (2) well-studied, powerful, and general information-theoretic metrics can be developed within this generalized framework.
Consider two total orderings of n objects. There are n 2 (unordered) pairs of such objects, and a pair is said to be concordant if the two orderings agree on the relative rankings of the objects and discordant if the two orderings disagree. Let c and d be the number of concordant and discordant pairs, respectively. Then Kendall X  X   X  is defined as follows: If we let C and D denote the fraction of concordant and discordant pairs then Kendall X  X   X  is defined as Note that c + d 6 = n 2 if there are ties. 2
To define a probabilistic framework, we must specify three things: (1) a sample space of objects, (2) a distribution over this sample space, and (3) random variables over this sample space. Let our sample space  X  be all possible 2  X  n 2 ordered pairs of distinct objects, and consider a uniform distribution over this sample space. For a given ranking R , define a random variable X R :  X   X  { X  1 , +1 } that outputs +1 for any ordered pair concordant with R and  X  1 for any ordered pair discordant with R .

X R [( d i ,d j )] = We thus have a well-defined random experiment : draw an ordered pair of objects at random and output +1 if that or-dered pair agrees with R  X  X  ranking and  X  1 otherwise. Since all ordered pairs of objects are considered uniformly, the expected value E [ X R ] of this random variable is zero.
Given a second ranked list S , one can similarly define an associated random variable X S . Now consider the random experiment of multiplying the two random variables: the product X R  X  X S will be +1 precisely when the pair is con-cordant  X  X .e. both lists agree that the ordering of the objects is correct (+1) or incorrect (  X  1), and the product will be  X  1 when the pair is discordant  X  X .e. the lists disagree. In this probabilistic framework, Kendall X  X   X  is the expected value
Kendall defined two means by which  X  can account for ties, depending on the desired behavior. Imagine comparing two ranked lists, one of which is almost completely composed of ties.  X  A , defined above, approaches 1.  X  B includes the num-ber of ties in the denominator, and therefore approaches 0. We believe that the former approach is appropriate in this context. Since QRELs are almost exclusively composed of ties (recall that all pairs of unjudged documents in the corpus are considered to be tied), using the latter would mean that effect of the relatively rare meaningful compar-isons would be negligible. of the product of these random variables:
The real power of this framework is in the definition of these random variables: (1) the ability to generalize them to compare partial orderings as arise in system evaluation, and (2) the ability to measure the correlation of these random variables using information-theoretic techniques.
In Section 3, we defined Kendall X  X   X  as the expected prod-uct of random variables. The following theorem allows us to restate Kendall X  X   X  equivalently as the mutual information between the random variables.

Theorem 1. I ( X R ; X S ) = 1+  X  2 log(1+  X  )+ 1  X   X  2 log(1  X   X  ) . (For a proof of Theorem 1, see Appendix). Unlike Kendall X  X   X  , the mutual information between ranked lists ranges from 0 on lists that are completely uncorrelated to 1 on lists that are either perfectly correlated or perfectly anti-correlated.
If we restrict our attention to pairs of lists that are not anti-correlated, then the relationship is bijective. Given this fact, we define a variant of Kendall X  X   X  , information  X  : where X R is the ranked list random variable defined in Equa-tion 12 observed with respect to the uniform probability distribution over all pairs of distinct objects. By refram-ing Kendall X  X   X  equivalently in terms of mutual information, we immediately gain access to a large number of powerful theoretical tools. For example, we can define a conditional information  X  between two lists given a third. For lists R and S given T ,
Kendall X  X   X  can tell you whether two sets of rankings are similar, but it cannot tell you why. Information  X  can be used as a meta-evaluation tool to find the underlying cause of correlation between measures. We demonstrate the use of information  X  as a meta-evaluation tool by using it to an-alyze measures of the diversity of information retrieval sys-tems. In recent years, several diversity measures (e.g. [1, 10, 24]) have been introduced to evaluate how well systems per-form in response to ambiguous or underspecified queries that have multiple interpretations. These measures conflate sev-eral factors [14], including: a diversity model that rewards novelty and penalizes redundancy, and a measure of ad hoc performance that rewards systems for retrieving highly rel-evant documents. We wish to know not only whether two diversity measures are correlated, but also the similarity be-tween their component diversity models. Using Kendall X  X   X  , we can observe whether the rankings of systems by each measure are correlated. But even if they are correlated, this could still be for one of two reasons: either both the di-versity and the performance components evaluate systems similarly; or else one of the components is similar, and its effect on evaluation is dominant. However, if the measures are correlated when conditioned on their underlying perfor-mance components, then this must be due to similarities in their models of diversity.
 F igure 1: Per-query information  X  (conditional rank correlation) between the TREC and NTCIR gold standard diversity measures conditioned on their underlying performance measures.
 We measured this effect on the the TREC 2011 and 2012 Web collections [8, 9]. Note that the performance measures are evaluated using graded relevance, while the diversity measures use binary judgments for each subtopic. All eval-uations are performed at rank 20. Figure 1 shows the rank correlation between ERR-IA and D#-nDCG, the primary measures reported by TREC and NTCIR [26], when condi-tioned on their underlying performance models. Each query is computed separately, with each datapoint in the figure corresponding to a different query. Table 1 shows the re-sults of conditioning additional pairs of diversity measures (now averaged over queries in the usual way) on their per-formance models. The results in Figure 1 are typical of all pairs of measures on a per-query basis.

Our results confirm that while diversity measures are very highly correlated, most of this correlation disappears when one conditions on the underlying performance model. This indicates that most of the correlation is due to the similar-ity between the performance components and not the di-versity components. For example, in TREC 2010, ERR-IA and  X  -nDCG have an information  X  of almost 0.9. How-ever, when conditioned on ERR, the similarity falls to only 0.25. This means that while these two measures are mostly ranking systems for the same reason, that reason is simply ERR. However, of the 0.9 bits that are the same, 0.25 are due to some factor other than ERR. This other factor must presumably be the similarity in their diversity models. T able 1: TREC 2010 and 2011 information  X  (condi-tional rank correlation) between diversity measures conditioned on ad hoc performance measures.
In this section, we demonstrate an extension of our prob-abilistic framework for evaluation to measuring the corre-lation between a system and the incomplete ranking gener-ated by a set of relevance judgments. This allows us to de-fine an information-theoretic evaluation measure, relevance information correlation . While our measure has novel appli-cations, we will demonstrate that the evaluations produced are consistent with those of existing measures.

To compute mutual information, we must define a sam-ple space, a probability distribution, and random variables. Let the sample space,  X  = { ( d i ,d j ) } , be the set of all or-dered pairs of judged documents. This means that we are ignoring unjudged documents, rather than considering them non-relevant. This is equivalent to computing an evaluation measure on the condensed list [23] created by removing all non-judged documents from the list. We define the proba-bility distribution in terms of the QREL to ensure that all ranked lists will be evaluated using the same random experi-ment. Let P = U | I ( g i 6 = g j ) , where g i represents the relevance grade of document d i , be the uniform probability distribu-tion over all pairs of documents whose relevance grades are not equal. We define a QREL variable Q over ordered pairs of documents as Note that this definition can be applied to both graded and binary relevance judgments.

We now turn our attention to defining a ranked list ran-dom variable over ordered pairs of documents ( d i both document d i and d j appear in the ranked list, than our output can simply indicate whether d i was ranked above d . If document d i appears in the ranked list and d j does not, then we will consider d i as having been ranked above d , and vice versa. If neither d i nor d j is ranked, we will output a null value. If we were to instead restrict our at-tention only to judged document pairs where at least one document is ranked, then a ranked list consisting of a single relevant document followed by some number of non-relevant documents would have perfect mutual information with the QREL X  X ll of the ranked relevant documents appear before all of the ranked non-relevant documents. However, this system must be penalized for preferring all of the ranked non-relevant documents to all of the unranked relevant doc-uments. If we instead use a null value, our example ranked list would almost always output null. This behavior would be independent of the QREL, meaning the two variables will have almost no mutual information. In effect, the null value creates a recall component for our evaluation measure; no system can have a large mutual information with the QREL unless it retrieves most of the relevant documents.
Another problem we must consider is that mutual infor-mation is maximized when two variables are completely cor-related or completely anti-correlated. Consider an example ranked list consisting of a few non-relevant documents fol-lowed by several relevant documents and then many more non-relevant documents. Since this example ranked list will disagree with the QREL on almost all document pairs, its random variable will have a very high mutual information with the QREL variable. The system is effectively being rewarded for finding the subset of non-relevant documents that happen to be present in the QREL. To address this, we truncate the list at the last retrieved relevant document prior to evaluation.
 Let r i represent the rank of document d i in the list S . Then the ranked list variable R S is defined as
R S [( d i ,d j )] =
We define our new measure, Relevance Information Cor-relation , as the mutual information between the QREL vari-able Q and the truncated ranked list variable R RIC is computed separately for each query, and then aver-aged, as with mean average precision.

In order to compute RIC we must estimate the joint prob-ability distribution of document preferences over Q and R. This could be done in various ways. In this work, we use the maximum likelihood estimate computed separately for each query. Since the MLE requires a large number of ob-servations, RIC is only accurate for recall-oriented evalu-ation. In future work, we intend to explore other means of estimating P ( Q,R ) that will allow RIC to be used for precision-oriented evaluation as well.

We also note that RIC has no explicit rank component, and would therefore seem to treat all relevant documents equally independent of the rank at which they were ob-served. However, there is an implicit rank component in that a relevant document that is not retrieved early in the list must be incorrectly ranked below many non-relevant docu-ments. This argument is similar in spirit to Bpref [2].
Our measure is quite novel in its formulation, and makes many non-standard assumptions about information retrieval evaluation. Therefore it is necessary to validate experimen-tally that our measure prefers the same retrieval systems as existing measures. Note that for two evaluation measures to be considered compatible, it is sufficient that they rank sys-tems in the same relative order; it is not necessary that they always assign systems similar absolute scores. For example, a system X  X  nDCG is often higher than its average precision.
To show that RIC is consistent with AP and nDCG, we computed the RIC, AP, and nDCG of all systems submit-ted to TRECs 8 and 9. Figure 2 shows the output of RIC plotted against AP (top) and nDCG (bottom) on TRECs 8 (left) and 9 (right) [28, 29]. TREC 8 uses binary relevance judgments. TREC 9 (right) uses graded relevance judgments. Table 2: Discriminative power of (graded) AP and nDCG vs. RIC judgments. TREC 9 uses graded relevance judgments, re-quiring the use of graded average precision. Inset into each plot is the output of the measures on the top ten systems. For each experiment, we report the Kendall X  X   X  and Spear-man X  X   X  [27] rank correlations for all systems, and for the top ten systems. With Kendall X  X   X  values of at least 0.799 on all systems and 0.644 on top ten systems, the ranking of systems by RIC is still highly correlated with those of both AP and nDCG. However, RIC is not as highly correlated with either AP or nDCG as AP and nDCG are with each other. Note that the correlation between RIC and GAP on TREC 9 is highly monotonic, even if is not particularly lin-ear. This implies that the two measures do rank systems in a consistent relative order, even if RIC is a biased estimator of GAP.

To further validate our measure, we also compute the dis-criminative power [22] of the various measures. Discrimina-tive power is a widely used tool for evaluating a measure X  X  sensitivity i.e. how often differences between systems can be detected with high confidence. A high sensitivity can be seen as a necessary, though not sufficient, condition for a good evaluation measure. Discriminative power is defined as the percentage of pairs of runs that are found to be sta-tistically significantly different by some significance test. As per Sakai, we use a two-tailed paired bootstrap test with 1000 bootstrap samples per pair of systems. Our results are displayed in Table 2. As measured by discriminatory power, we see that RIC is at least as sensitive, if not more so, than AP and nDCG.
In Section 5, we defined an evaluation measure in terms of mutual information. One advantage of this approach is that collections of systems can be evaluated directly by consid-ering the output of their random variables jointly, without their needing to be combined. For a collection of systems, denoted S 1 through S n , the relevance information correla-tion can be defined as In this section, we will show that this produces a natural upper bound on metasearch performance that is consistent with other upper bounds appearing in the literature. We compare our upper bound against those of Montague [19]. Montague describes metasearch algorithms as sorting func-tions whose comparators, as well as the documents to be sorted, are defined in terms of collections of input systems. By also using the QREL as input, these algorithms can es-timate upper bounds on metasearch performance. These bounds range from the ideal performance that cannot pos-sibly be exceeded by any metasearch algorithm, to descrip-tions of reasonable metasearch behavior that should be sim-ilar to the performance of any quality metasearch algorithm.
Montague defines the following upper bounds on metasearch: 1. Naive: Documents are sorted by comparison of rele-2. Pareto: If document A is ranked above document B by 3. Majoritarian: If document A is ranked above docu-We will compare our direct joint evaluation with these up-per bounds, and several metasearch algorithms commonly used as baselines in the IR literature: the CondorcetFuse metasearch algorithm [20], and the comb family of metasearch algorithms [25].
 Figure 3: RIC of systems output by metasearch algorithms (Fusion System) versus RIC of systems computed directly ( S 1 ,...,S 10 ) without combining.
We examined the direct evaluation and metasearch per-formance of collections of ten randomly selected systems. Experiments were performed on TREC 8 and 9, with both binary and graded relevance judgments. To conserve space, we only show the results from TREC 8. The results from TREC 9 were highly similar, both when using binary and graded relevance judgments.

Figure 3 shows the RIC of the system output by a metasearch algorithm plotted against the joint RIC of the input sys-tems, and Table 3 shows various measures of their corre-lation. Montague found that combANZ is inferior to Con-dorcetFuse and combMNZ, CondorcetFuse and combMNZ Table 3: Correlation between joint distribution and metasearch algorithms (Kendall X  X   X  , Spearman X  X   X  , root mean square error). perform comparably to the Majoritarian bound, and the Naive bound is not appreciably better than the Pareto bound. If direct evaluation and the Naive bound are both reason-able estimates of the actual upper bound, then these re-sults should be confirmed by Figure 3 and Table 3, as in-deed they are. Note that there is almost no correlation be-tween the joint evalution and the weakest metasearch al-gorithm, combANZ: combANZ does not approximate the upper bound on metasearch. The correlation improves as the quality of the metasearch algorithm improves, and it does so in a manner consistent with Montague. The cor-relations between the joint evaluation and the output of combMNZ, CondorcetFuse, and the Majoritarian bound are similar; while they are still biased as estimators, the corre-lation is beginning to approach monotonicity. Finally, with a root mean square error of 0.039, the joint evaluation esti-mation of the upper bound is essentially identical to that of the Naive upper bound. If the Naive upper bound is a rea-sonable estimate of the upper bound on metasearch perfor-mance, then so is the joint evaluation of the input systems.
In this section, we introduce a novel application of our probabilistic framework. Imagine that you are attempting to improve an existing ranker. On what basis do you decide whether or not your changes are beneficial? One typically evaluates both systems on a number of queries, and mea-sures the difference in average performance. If one system outperforms the other, whether you have made an improve-ment is clear. But what happens when the systems perform similarly? It could be that your new system is essentially unchanged from your old system, but it is also possible that the two systems chose highly different document and just happened to have very similar evaluation scores. In the lat-ter case, it may be possible to create a new, better system based on a combination of the two existing systems.
We propose to measure the magnitude of the difference between systems in their ranking of documents for which we have relevance information, rather than the magnitude of the difference between their performance. We denote this new quantity as the information difference between systems. Our definition of information difference is inspired by the Boolean Algebra symmetric difference operator as applied to information space (see Figure 4). Figure 4: Information difference corresponds to the symmetric difference between the intersections of the systems with the QREL in information space (red portion of the Venn diagram).

As a preliminary validation of information difference, we analyzed the change in AP and information difference be-tween pairs of systems submitted to TREC 8, selected at random. We expect the two to be somewhat directly cor-related, since, in general, if two systems rank documents similarly, we would expect them to have similar AP. How-ever, we expect that they will not be highly correlated, since we believe that information difference is much more infor-mative. Our intuition is supported by Figure 5, which shows the magnitude of the change in AP on the horizontal axis, and the information difference on the vertical axis. Figure 5: Scatter plot of information difference and the magnitude of change in AP of random pairs of TREC 8 systems.

To demonstrate the utility of information difference, we sorted all the systems submitted to TREC 8 by AP and separated them into twenty equal-sized bins. By construc-tion, each bin contained systems with small differences in performance. Our goal is to distinguish between similar and dissimilar systems within each bin. To this end, all systems within each bin were compared with one other (see Table 4). When the system pairs were sorted by their information dif-ference, both systems in the first 27 pairs were submitted by the same group, whereas sorting by |  X  AP | produced no discernible pattern. It is reasonable to assume that these systems were different instantiations of the same underlying technology. We can therefore conclude that information dif-ference is able to determine whether systems with the same underlying performance are in fact similar, as desired. Table 4: The systems from TREC 8 were binned by average precision. Information difference and  X  AP were computed for all system pairs within each bin. Sorting by information difference, both systems in the first 27 pairs were submitted by the same group.
In this work, we developed a probabilistic framework for the analysis of information retrieval systems based on the correlation between a ranked list and the preferences induced by relevance judgments. Using this framework, we devel-oped powerful information theoretic tools for better under-standing information retrieval systems. We introduced four preliminary uses of our framework: (1) a measure of condi-tional rank correlation, information  X  , which is a powerful meta-evaluation tool whose use we demonstrated on under-standing novelty and diversity evalution; (2) a new evalu-ation measure, relevance information correlation , which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch perfor-mance; and (4) a measure of the similarity between rankers on judged documents, information difference , which allows us to determine whether systems with similar performance are actually different.

Our framework is based on the choice of sample space, probability distribution, and random variables. Throughout this work, we only used a uniform distribution on appropri-ate pairs of documents. However, not all document pairs are equal. The use of additional distributions is an imme-diate avenue for improvement that we intend to explore in future work. For example, a geometric distribution may be employed to force our evaluation tools to concentrate their attention at the top of a ranked list.

The primary limitation of our evaluation measure as im-plemented in this work is that it is only applicable to recall-oriented retrieval tasks. In future work, we intend to develop a precision-oriented version that is applicable to web search. Given such a measure, judgments can be combined in the way systems were in our upper bound on metasearch. In that way, a small number of expensive to produce nominal rel-evance judgments, a somewhat larger number of somewhat less expensive preference judgments, and a gold-stander ranker could all be used simultaneously to evaluate systems.
Finally, we intend to explore the application of informa-tion difference to the understanding of information retrieval models. For example, BM25 and Language Models have long been used as baselines in information retrieval experiments. On the surface, these two models appear to be completely different. And yet, the two share deep theoretical connec-tions [33]. Using information difference, we can determine whether their theoretical similarities outweigh their superfi-cial differences in terms of how they rank documents. [1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, [2] Chris Buckley and Ellen M. Voorhees. Retrieval [3] Christopher J.C. Burges. From ranknet to lambdarank [4] Ben Carterette and Paul N. Bennett. Evaluation [5] Ben Carterette, Paul N. Bennett, David Maxwell [6] Praveen Chandar and Ben Carterette. Using [7] Olivier Chapelle, Donald Metlzer, Ya Zhang, and [8] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and [9] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and [10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. [11] Thomas M. Cover and Joy A. Thomas. Elements of [12] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill [13] Yoav Freund, Raj Iyer, Robert E. Schapire, and [14] Peter B. Golbus, Javed A. Aslam, and Charles L.A. [15] Kalervo J  X  arvelin and Jaana Kek  X  al  X  ainen. Cumulated [16] Thorsten Joachims. Optimizing search engines using [17] M. G. Kendall. A New Measure of Rank Correlation. [18] Alistair Moffat and Justin Zobel. Rank-biased [19] Mark Montague. Metasearch: Data Fusion for [20] Mark Montague and Javed A. Aslam. Condorcet [21] Stephen E. Robertson, Evangelos Kanoulas, and [22] Tetsuya Sakai. Evaluating evaluation metrics based on [23] Tetsuya Sakai. Alternatives to Bpref. In Proceedings of [24] Tetsuya Sakai and Ruihua Song. Evaluating diversified [25] Joseph A. Shaw and Edward A. Fox. Combination of [26] Ruihua Song, Min Zhang, Tetsuya Sakai, Makoto P. [27] C. Spearman. The proof and measurement of [28] E. M. Voorhees and D. Harman. Overview of the [29] E. M. Voorhees and D. Harman. Overview of the ninth [30] Emine Yilmaz and Javed A. Aslam. Estimating [31] Emine Yilmaz, Javed A. Aslam, and Stephen [32] Emine Yilmaz, Evangelos Kanoulas, and Javed A. [33] Chengxiang Zhai and John Lafferty. A study of Theorem 1. I ( X R ; X S ) = 1+  X  2 log(1+  X  )+ 1  X   X  2 log(1  X   X  ) .
Proof. Denote X R and X S as X and Y . Consider the following joint probability distribution table.
 Observe that: a + b + c + d = 1; C = a + d , D = b + c , and therefore  X  = a + d  X  b  X  c ; and since document pairs appear in both orders, a = d and b = c .

The joint probability distribution can be rewritten as fol-lows.
 Observe that the marginal probability P ( X ) = P ( Y ) =
I ( X ; Y ) = KL ( P ( X,Y ) || P ( X ) P ( Y )) Since P ( X,Y ) = C 2 , D 2 , C 2 , D 2 and P ( X ) P ( Y ) = I ( X,Y ) = 2  X  C Since C + D = 1 and  X  = C  X  D , we have that  X  = 2 C  X  1,
In terms of C , if H 2 represents the entropy of a Bernoulli random variable , 3
I ( X ; Y ) =  X  H 2 ( C ) + 1
Corollary 1. For two ranked lists R and S , I ( X R ; X S 1  X  H 2 ( K ) where K = 1  X   X  2 is the normalized Kendall X  X   X  distance between R and S .
H 2 ( p ) =  X  p lg p  X  (1  X  p ) lg(1  X  p ). Note that H H (1  X  p ).
