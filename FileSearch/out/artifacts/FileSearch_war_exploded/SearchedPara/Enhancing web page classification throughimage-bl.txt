 1. Introduction
Today X  X  search engines and document classification techniques are mostly based on textual information and do not take into account those information related to visual layout features. Actually, in the sense of human as a single object. Indeed, when a web page is presented to the user, the spatial and visual cues help him to unconsciously divide the page into several semantic part containing different types of information beside its main topic, for example: navigation bar, copyright, contact information and so on. The detection of this semantic content structure may improve the performance of web information retrieval tasks such as classifi-cation or clustering. In Kovacevic, Diligenti, Gori, and Milutinovic (2002) the authors provide a mechanism headers, navigation bars, left and right menus, footers, and informative parts. The functional role of each which intuitively characterize each class of common visual component.

In Cai, Yu, Wen, and Ma (2003a, 2003b) a Vision-based Page Segmentation (VIPS) algorithm has been proposed. VIPS algorithm, based on the assumption that semantically related contents are usually grouped content of a web page is mapped in a hierarchical structure in which each node corresponds to a block. VIPS algorithm makes full use of page layout features: first it extracts all the suitable blocks from the html DOM visually do not cross any block. With this information the web page semantic structure is constructed.
Another important approach, presented in Mehta, Mitra, and Karnick (2005) , is based on the opposite assumption that in a web page semantically related contents are not always grouped together, but may be dis-block as equally important for the purpose of document classification.

In this paper we propose a metric for computing importance taking into account also the importance of the visual block in which the term is located. The idea of using structural information for measuring the impor-tance of different parts in a web page has been proposed recently in Yi and Liu (2003), Huang, Yu, Li, Xue, and Zhang (2005) and Song, Liu, Wen, and Ma (2004) . In particular in Yi and Liu (2003) the authors propose a feature weighting technique to deal with web page noise, i.e. information that is not part of the page main contents. They first build a compressed structure tree to capture the common structure of a set of web pages, and then use an information gain based method to evaluate the importance of each node (block) in the tree.
Based on the tree this information is used to assign a weight to each word feature. Another supervised approach to compute the importance of a block in a web page is proposed in Song et al. (2004) , where the block importance estimation is formulated as a learning problem. A visual segmentation approach is used algorithm is instantiated to construct an importance model. Both this methods require to manually build a training set of annotated data. An unsupervised method for estimating block importance and relevance with of Language Modeling proposed in Ponte and Croft (1998) which estimate it as the probability that a docu-ment generates a given set of blocks.

In this paper, we present an alternative unsupervised approach which, to improve the efficacy of informa-information related to important visual cues, represented by images. This approach is based on the assump-
Of course not all images are equally important, for example banners or logos are noisy information, therefore we need to associate and evaluate the semantic content of an image. For this purpose we can see a web page as vertical separators. Each block X i 2 H denotes a semantic part of the web page, do not overlap any other blocks and can be recursively considered as a web page.
 image-block is defined as a visual block which contains an image. To assign higher weights to important terms contained into image-blocks by performing a visual layout analysis, we propose a new metric, called Inverse Term Importance .

In this paper we also propose three different query independent methods to evaluate image-blocks impor-tance. These approaches consist of three main steps: first we evaluate the importance of each term within its image-block, we then evaluate image-block importance within the document and use this evaluation for smoothing the term weights accordingly. The traditional TFxIDF scoring method is modified into an Image
Weighted scoring function called IWTFxIDF, which takes into account the image-block importance. The pro-posed approach has been validated with respect to different classification algorithms.

The outline of this paper is the following. In Section 2 we present our methodology along with the descrip-tion of our Information Retrieval system functionality. In Section 3 we describe the datasets and the perfor-mance measure used for validating our approach. In Section 4 experimental results are presented and discussed. Finally, in Section 5 conclusions are derived. 2. The information retrieval system model 2.1. Overview of the system
In this section we present the Information Retrieval system where the visual block analysis function and the proposed term and block evaluation methods have been integrated. The entire work-flow is depicted in Fig. 1 .
Pages are crawled from the Web and indexed into a repository. Html pages, before being indexed, are pro-cessed by the Image-Block Analysis Module to weight terms according to the importance metric described formance obtained by using the new metric. In the following we illustrate in details the main modules of our system. 2.2. Visual block analysis
During the indexing phase, a pre-processing activity is performed to make page cases insensitive, remove stop words, acronyms, non-alphanumeric characters, html tags and apply stemming rules, using Porter X  X  suffix stripping algorithm. Since the indexing module is able to manage different document formats, for every pro-for the important terms belonging to the important image-blocks, which is a subset of the document vocab-ulary; if the web page does not contain any image the second field is empty.

For html pages the VIPS analysis functionality is instantiated for segmenting the web page into semantic the exploration of the web page DOM tree and therefore has O  X j E jj D j X  time complexity, where j E j is the number of nodes in the DOM tree, and j D j is the number of web pages in the repository. Nevertheless, this web page its complexity is O  X  1  X  .

After the layout analysis has been performed, terms strictly related to an image-block are captured, eval-uated and stored into the index. As mentioned before, the term importance evaluation is based on the assump-tion that images are the components of the page which mainly attract the attention of the user. However, we example banners or gifs referred to webmaster e-mail. In Fig. 2 it is possible to see an example of important image-blocks: image-blocks 1 and 2 (red frame) are considered meaningful for the document, while image-block 3 (green frame), which reflect noisy information, is considered not important. We identify the most informative image-blocks and their most important terms, by using the Inverse Term Importance Metric as described in Section 2.3 . 2.3. Basic term scoring procedure
When a user submits a query about a topic of interest, a set of document Q is retrieved from the document collection D stored into the index. To prepare the input for the classification module the document set Q is mapped into a matrix M  X  X  m ij . Each row of M is represented by a document d , following the Vector Space
Model presented in Salton, Wong, and Yang (1975) : where j V j is the number of terms contained in the set of document Q and w j th document. Usually this weight is computed using the TFxIDF approach, presented in Salton and Buckley (1998) , as follows: where TF  X  t i ; d j  X  is the Term Frequency , i.e. the number of occurrences of term t verse Document Frequency . IDF  X  t i  X  is a factor which enhances the terms which appear in fewer documents, while downgrading the terms occurring in many documents and is defined as where DF  X  t i  X  is the number of documents containing the i th term.

In our approach the weight w ij is defined introducing a refined version of TF  X  t
Term Frequency (IWTF), which takes into account important image-block terms which are likely to describe the document content; in particular the IWTF is defined as follows:
The basic idea is to augment TF  X  t i ; d j  X  by a  X  X  X isual sensitive  X  term scoring TI  X  t in (3) , is maintained to evaluate the usefulness of a word into the entire collection: Note that if a document does not contain images the IWTF reduces to the traditional TF measure.
To compute TI  X  t i ; d j  X  , let K j be the set of image-blocks contained in document d terms contained in k 2 K j . Given a term t i 2 I k belonging to d where r ikj represents the number of occurrences of term t occurrences of term t i in document d j 2 Q .

This ITI metric is based on the assumption that if a subset of terms belongs to an image-block k and these the document topic or we have a multi-topic document. An ITI for document j . Increasing values of ITI ikj imply decreasing importance of the term t d ing ones.

Given a document d j we consider only those terms t i having ITI
The value of this threshold has been studied during the experimental phase, evaluating the performance of the sure (TI) can be now defined as
Note that TI can be considered related to the Information Gain given by term i for identifying the document topic.
 The Term Importance computation, which is performed during the indexing phase, is characterized by a
W and for all documents in D . 2.4. Visual block importance estimation
To evaluate more precisely the terms importance according to the document topic, we need to evaluate each image-block importance and to smooth the terms importance taking into account the block importance in which each word is located, leading us to a smoothed version of the ITI metric defined in (6) . If a term is web page content, and then the value of TI must be increased accordingly. Eq. (7) is then modified into sine-based and Iterative Block Importance . The idea of the proposed methods is that if a term is considered in a significant block, its weight will be increased. 2.4.1. ITI-based block importance method
In the ITI-based block importance method the importance of an image-block is computed as its average term importance: fore modified in
The ITI-based block importance method can be synthesized by a procedure that computes, for each term belonging to W , the ITI coefficient and its subsequent smoothed value. The complexity of this procedure second one estimates g ITI for every terms belonging to W . Since the ITI-based block importance method is applied to all web documents D crawled from the web, its global time complexity is O  X j W jj D j X  . 2.4.2. Cosine-based block importance method
The Cosine-based block importance computes the image-block importance as the cosine between the image-block k and the entire document d j using the Vector Space Model: where k q . In this case the ITI is modified as block with IBI  X  k  X  P q are rewarded.

The Cosine-based block importance method can be described by the pseudo-code reported in Fig. 3 . Fol-lowing this procedure, having a set of image-blocks K H  X  and j W j terms, the time required by the Cosine-based block importance method is O  X j W jj K 2.4.3. Iterative block importance method
The third proposed method is based on the idea that an image-block importance depends not only on its terms, but also on its possible relations with other image-blocks. In fact, might be cases in which TI  X  t high because t i appears frequently in the document, but only in some unimportant image-blocks. If we do not take in to account these unimportant image-blocks, the TI value may decrease. We then propose an iterative procedure to eliminate unimportant image-blocks and to compute the TI values using only important image-blocks.

Recalling that K j be the set of all image-blocks contained in the document d tance method can be synthesized in the following pseudocode.

Following this process we evaluate for each image-block k its IBI, and if the image-block k is considered unimportant, we than delete k updating the remaining image-blocks. At the end we find the set of image-blocks that most probably are related to the document topic.

With respect to its computational complexity, this method can be synthesized by three nested iterative cycle (see Fig. 4 ): the first one is over all documents d j 2 D , the second one is over the image-block set K document d j , and the last one includes the computation of ITI and IBI coefficients for all the terms t
Following this simple procedure, the computational cost of the Iterative Block Importance method can be described by a O  X j W jj K H jj D j X  time complexity.

In the following table we report the time (s) taken by the Iterative Block Importance method to process a document on a Pentium M 735 processor. The minimum, maximum and average times has been computed on a sample of 10,000 documents with up to 35,000 terms. (More details on the dataset are given in Section 3 ).
 g  X  0 : 3 0.031 5.89 0.32 g  X  0 : 4 0.032 6.81 0.31 g  X  0 : 5 0.031 7.12 0.34 g  X  0 : 5 0.032 5.51 0.31 3. Dataset and evaluation measure
To evaluate the effectiveness of the proposed term weighting approach, we compared the performance ing techniques proposed in this paper. In this section we describe the dataset used and the performance eval-uation metric adopted. The main steps of the experimental phase performed to validate our approach are described below.

Step 5. Performance evaluation. To evaluate the classification performance, we used the widely adopted F -4. Experimental results
The proposed metric and its related scoring functions have been evaluated with respect to the learning algo-rithms, performing a 10-folds Cross Validation as testing method. In Tables 2 X 6 we compared the F -measure an increasing number of terms belonging to the dataset vocabulary because the results of our approaches become asymptotic with respect to the standard TFxIDF scoring technique. The scoring function used during the performance evaluation are describe in the following:
IWTFxIDF(1) : all blocks are considered equally important; the weight w taking into account any image-block evaluation method.

IWTFxIDF(2) : the importance of each image-block is computed as the average importance of its terms; this scoring technique compute w ij following the ITI-based Block Importance estimation presented in Sec-tion 2.4.1 .

IWTFxIDF(3) : the block importance is computed as the cosine function between the block and the entire document vector; IWTFxIDF(3) computes the weight w ij taking into account the Cosine-based Block
Importance estimation presented in Section 2.4.2 , considering a block k important for document d q  X  0 : 7.

IWTFxIDF(4) : the block importance is computed through a procedure that iterative evaluate and delete unimportant blocks; IWTFxIDF(4) computes the weight w ij following the Iterative Block importance esti-mation process presented in Section 2.4.3 .
 We used as benchmark the traditional TFxIDF scoring function.

Table 2 shows that our scoring function, for T  X  50, computed according to the Iterative Block Importance approaches (IWTFxIDF(4)) always outperforms the traditional TFxIDF scoring method, IWTFxIDF(2) works well, while IWTFxIDF(1) and (3) perform poorly. Tables 3 and 4 indicates that our scoring methods, for T  X  100 and 150, respectively, are better and, in the worst case, equal to the standard approach. More specifically, the Iterative Block Importance and ITI-based approach (IWTFxIDF(4) and (2)) methods produce obtained by the ITI-based approach (IWTFxIDF(2)) and decreasing F -measure values for the Iterative Block
Importance (IWTFxIDF(4)). Table 6 shows that the Iterative block importance method (IWTFxIDF(4)) pro-duces significantly better classification quality increasing T to 500.

There are some remarks which lie outside the goal of this paper, but are quite interesting. A first consid-eration regards a performance comparison between Bayesian Network and the remaining learning algorithm:
Bayesian Network show a decreasing performance relative to the other algorithms as the number of features increases. This behavior could be explained by the overfitting caused by a high number of terms (nodes). A second observation regards a performance comparison between the K -Nearest Neighbor methods: 1NN approach defines a better model compared with 5NN method because data can overlap and, with an increas-ing number of neighbors, the performance tends to decrease. A third remark is about Support Vector
Machines: this linear classifier, which simultaneously minimizes the empirical classification error and maxi-ing to note that Multinomial Naive Bayes, which is characterized by the absence of parameters, has similar performances with respect to the Support Vector Machine. This consideration lead us to consider Multinomial
Naive Bayes, in our experiment, a very good learning algorithm for its performance stability and its param-eters independence.

A final remark regarding the performance is related to the influence that semantic image interpretation could have during the terms scoring phase and consequently during the learning task. Even if we did not include an image interpretation approach in our framework, analyzing only the surrounding text of an image, we plan to introduce some additional models related to image understanding to improve the precision during terms evaluation. Gao, Fan, Xue, and Jain (2006) and Li and Perona (2005) are interesting approaches able to analyze and understand image semantic through hierarchial models. In this direction we aim at combining image and textual analysis, to extract a more precise and meaningful semantic related to important image-blocks.

In Fig. 5 we compared, for any learning algorithm, the F -measure values obtained varying the number of terms between the standard technique and the scoring method which produces the more stable result in terms of averaged F -measure over the threshold g . The Iterative Block Importance method seems to be the terms scoring procedure producing the greater number of  X  successes  X  . However, if we consider the average F -mea-for Bayesian Network, the ITI-based approach for 1NN and the scoring technique that consider all block equally important (IWTFxIDF(1)) for the Support Vector Machines. Naive Bayes, MultiNomial Naive Bayes,
Decision Tree and 5NN, Fig. 5 a, c, d and f, gives the best performance on average with the IWTFxIDF(4) based on the Iterative Block Importance method.

With respect to the learning task, the time complexity related to the model building depends on every single classification algorithm. In Tables 7 X 13 we compared the required computational time (in s) in the learning phase for each classification algorithm. Since the time performance obtained by using the proposed scoring methods are stable with respect to the parameter g , we report the average time obtained g equal to 0.3, 0.4, 0.5 and 0.6. The time measurements are computed on a Pentium M 400 MHz, Windows XP system.
Note that the proposed scoring technique ensures a good compromise between classification accuracy and learning time, in particular for 1NN, 5NN and Multinomial Naive Bayes. A further interesting remark regards the comparison between Multinomial Naive Bayes and Support Vector Machines: the time performance val-idate the consideration that Multinomial Naive Bayes classifier obtain the same good performance in terms of accuracy of the Support Vector Machines, while requiring a much lower learning time.
 5. Why our approach works better than traditional TFxIDF
We believe that the main reason related to the success of our approaches is that images represent one of the most important elements reported in a web page. To prove our intuition at the basis of our approaches we provide simple numerical statistics. Consider the set of documents used during the experimental phase.
Fig. 6 a shows how many document are structured to include different number of images. As reported only 210 documents are plain text, while more than half of documents contain approximately between from 1 to 5 images. One of the most interesting things that confirm our initial assumption is showed in Fig. 6 b. We reported a comparison between the number of images belonging to the original document dataset and the number of image-blocks that our block evaluation methods considered as important. Indicatively all of our approaches identify more than half of image-blocks as related to the topic of the documents in which are located, and then considered important. Moreover, after the identification of important image-blocks, terms policy is driven to select those attributes with better discriminative power. 6. Conclusions and future works
In this paper we presented a novel approach for improving the performance of document classification sys-tems. This approach is able to identify important image-blocks contained in a web page to recognize those terms that are more informative for the entire document. The main advantage of our approach lies in its abil-ity to distinguish the most important content from less important and noisy information. Moreover our meth-ods are query independent, unsupervised and independent from any spatial constraints related to important contents. Results show that our approaches outperform the standard TFxIDF model in terms of classification accuracy.
 Acknowledgements We thank Ilaria Giordani and Silvia Dondi, for their help in software development.
 References
