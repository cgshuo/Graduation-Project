 By a data stream one refers to a learning model in which the training examples arrive on-line. One usually assumes t hat the examples are received one at a time. However, it is common to update t he hypothesis and compute the required sufficient statistics not following each new example, but only after gathering a small set of new examples [1,2]. Thus, the data stream could also be bursting. In any case, the stream can be considered to be infinite, because it is unimaginable that all data received cou ld be stored into the mai n memory of a computer.
One of the basic knowledge representations of machine learning, decision trees , is among the first formalisms learning of which has been studied in this setting [1,2,3,4,5]. The str eaming data received contains s uccessive training examples each of which consists of values for a pair of random variables X ,Y .The k elements of the instance vector X are called attributes ; X = A 1 ,...,A k .An attribute may be nominal-or continuous-valued (numerical). The class labels Y usually come from a small nominal set. The aim is to maintain an adaptive anytime model of determining the value of Y based on the attribute values X . One is allowed to process the data only in the order that it arrives without storing (all of) it. As the intention is to operate in real time, only constant time per example may be used to update the statistics sufficient to determine which attribute is the appropriate one to be tested in a node of the evolving tree.
Usually there is a training phase in which a model of the data is built based on examples, and an application phase in which the model is used to classify instances whose class is not known. These phases can also overlap and, therefore, we want to have an anytime model. Also the case in which there is no single final concept to be learned, but it changes over time, has been tackled [3,5].
In decision tree learning an attribute evaluation function is used to decide which attribute X  X  value to test in a node of the evolving tree. For a nominal attribute one simply grows a subtree for each of the few separate values that the attribute may take. Numerical attributes with (infinitely) many possible values, on the other hand, need to be processed s omehow. It is common to discretize the continuous value range into a small number of disjoint intervals. We will demonstrate that one can efficiently maintain sufficient statistics for obtaining an optimal multi-way split for the value range of the attribute in question in the on-line data stream model even if the dat a cannot be stored. Optimality in this context means that we need to guarantee that the attribute evaluation function can attain its best value on the data even after the modifications done.
Multi-way splitting of numerical attributes has not often been applied in the streaming data context. Previous work has mostly relied on recursive binariza-tion of the numerical value range. The notable exception is the recent work of Gama and Pinto [6] in which two-layer histogram discretization was introduced. Gama et al. [2] also applied multi-way splits consisting of ten equal-width bins in the leaves of a tree in their VFDTc system. The leaves in VFDTc decision trees are functional  X  i.e., contain a na  X   X ve Bayes classifier  X  but the internal nodes use the standard binarization approach for numerical attributes.

Our work continues the cut point analysis of optimal splitting originally ini-tiated by Fayyad and Irani [7]. This line o f research has been continued later in the batch learning setting [8,9,10,11]. We will recapitulate the main ideas of this work in Section 3. In the data stream model Jin and Agrawal [4] have examined reducing the number of cut point candidates that need to be taken into account. Domingos and Hulten [1] introduced the VFDT system, which learns Hoeffding trees  X  decision trees with a similarity guarantee to those learned by conven-tional batch algorithms such as CART [12] and C4.5 [13]. The standard Hoeffd-ing inequality is used to show that the attribute chosen to a node in a tree is, with a high probability, the same as the one that would have been chosen by the batch learner with access to all of the d ata. VFDT chooses an attribute to the root based on n first examples, after which the pr ocess continues recursively in the leaves down to which the succeeding examples are passed. Hoeffding bounds allow to solve the required n for reaching the user-requested confidence level.
Domingos and Hulten [1] did not elaborate on how to handle numerical at-tributes in Hoeffding trees. They just proposed that the commonly-used thresh-olded binary splitting of the numerical value range be applied; i.e., only tests of the form A i &lt;t j are used. The value range of a numerical attribute may get multi-way splitted through subsequent binary splits of the induced subintervals. Gama et al. [2] put forward an instantiation of VFDT in which Information Gain function ( IG ) of C4.5 was used to evaluate a ttributes. For each numerical attribute A i a (balanced) binary search tree (BST) is maintained. It records for every potential threshold t j the class distribution of the binary partition induced by the test A i &lt;t j . Exhaustive evaluation over the tests is used to choose the one to be placed to the evolving decision tr ee. However, all threshold candidates for one attribute can be evaluated during a single traversal of the BST.
Obviously, updating the BST takes O (lg V ) time, where V is the number of different values that the attribute in question takes. This price needs to be paid in order to be able to choose the best binary split. One has to sort the values and  X  as the value range is unknown from the outset  X  a time proportional to the number of potential cut points needs to be paid per example, unless one is willing to risk finding the optimal cut point.

Jin and Agrawal [4] proposed an approach for pruning intervals from the range of a numerical attribute. They first discretize a numerical value range into equal-width intervals after which a statistical test decides which intervals appear unlikely to include a split point and can, thus, be pruned. In addition they showed that Hoeffding bounds can be reached for IG and Gini function [12] with a lesser number of samples than in the original VFDT.

Gama et al. [2] also used functional leaves in the tree instead of the simple majority class strategy. Before the algorithm has decided which attribute test to assign to a leaf in the evolving tree, Na  X   X ve Bayes can be used to give predictions for instances that arrive needing to be classified. For numerical attributes the common approach of discretization into ten equal-width bins (when possible) is used in the na  X   X ve Bayes classifiers.

CVFDT [3] adapts the VFDT system to concept drift. With the concept changing over time, it is necessary to in crementally update the model built for the examples. The real-time operation requirement does not allow to rebuild the model for examples in a sliding window from scratch. Instead, Hulten et al. [3] proposed to build an alternative subtree for those nodes that do not pass the Hoeffding test in light of the sufficient statistics maintained for a sliding window of examples. When the alternate s ubtree X  X  performance on new examples overtakes that of the old one, it is inserted to the tree. To grow the shadow tree one uses the standard techniques of VFDT, thus ensuring real-time operation.
In the UFFT system [5] concept drift is detected through the reducing accu-racy of the na  X   X ve Bayes classifier installed int o an internal node. Whenever, a change in the target concept is identified, the subtree rooted at the corresponding node and its associated statistics is pruned into a (functional) leaf, and build-ing of a new subtree may begin anew. Numerical attributes are handled using the common normality assumption. The sufficient statistics for each numerical attribute in this case are simply the mean and variance per class.

Gama and Pinto [6] use histograms for data stream discretization. They induce either an equal-width or an equal-frequen cy discretization for the value range. The approach uses two layers of intervals; layer 1 maintains statistics for an excessive number of intervals and layer 2 composes the final dis cretization based on these statistics. Layer 1 is maintained on-line by updating the counters in the appropriate interval whenever a ne w example is received. If a user-defined condition is met, an interval may be split in two; e.g., to keep the intervals of approximately equal width. Layer 2 produces, on need basis, the final histograms by merging intervals of layer 1. Building of the second level discretization is confined on the cut points of layer 1, and may thus be inexact. The shortcomings of recursive binary splitting of the value range of a numerical attribute could potentially be avoided if one-shot multi-way splits were used instead [14,15]. However, multi-way splitting can be computationally expensive. Hence, the most popular approaches are based on multi-way splitting through successive binary splits [14,15]. Such partitions cannot, though, be guaranteed to be optimal with respect to the attribute evaluation function being used. Without loss of generality, let us consider only one real-valued attribute X . A training sample S = { ( x 1 ,y 1 ) ,..., ( x n ,y n ) } consists of n labeled examples. For each ( x, y )  X  S , x  X  R and y is the class label of x in C = { c 1 ,...,c m } . A k -interval discretization of the sample is generated by picking k  X  1 interval thresholds or cut points T 1 &lt;T 2 &lt;  X  X  X  &lt;T k  X  1 .Let T 0 =  X  X  X  and T k =  X  , then the set of k  X  1 thresholds defines a partition k i =1 S i of the set S so that S
The simplest attribute evaluation function is Training Set Error ( TSE ). Let  X  ( S i )denotethe error ,orthe number of disagreements , with respect to class c j in the set S i . That is, if all instances in S i were predicted to belong to class c j , we would make  X  j ( S i ) errors. Furthermore, let  X  ( S i ) denote the minimum error on S i .Aclass c j  X  C is one of the majority classes of S i , if predicting class c j leads to minimum number of errors on S i , i.e.,  X  j ( S i )=  X  ( S i ).
Given a k -interval partition k i =1 S i of S , where each interval is labeled by a majority class, its TSE is the minimum number of training instances falsely classified in the partition. The global minimum error discretization problem is to find a partition k i =1 S i that has the minimum attribute evaluation function value over all partitions of S . The maximum number of intervals k may also be given as a parameter. Then the problem is to find the optimal partition among thosethathaveatmost k intervals. This is called bounded-arity discretization.
If one could make its own partition interval out of each data point, this dis-cretization would have zero training erro r. However, one cannot discern between all data points. Only those that differ in their value of X can be separated from each other. E.g., in the data set shown in Fig. 1 there are 27 integer-valued in-stances of two classes;  X  and  X  . Interval thresholds can only be set in between those points where the attribute value changes. Therefore, one can process the data into bins , one for each existing attribute value. Within each bin we record its class distribution. This information suffices to evaluate the goodness of the partition; the actual data set does not need to be maintained.
 The sequence of bins has the minimal attainable misclassification rate for TSE . However, the same rate can usually be obtained with a smaller number of intervals. Fayyad and Irani X  X  [7] analysis of the entropy function has shown that cut points embedded into class-uniform in tervals need not be taken into account, only the end points of such intervals  X  the boundary points  X  need to be con-sidered to find the optimal discretization. Thus, optimal splits of Average Class Entropy and IG fall on boundary points. Hence, only they need to be examined in optimal binary partitioning of the value range of a numerical attribute.
Elomaa and Rousu [10] showed that the same is true for many commonly-used evaluation functions. By this analysis we can merge together adjacent class uniform bins with the same class label to obtain example blocks (see Fig. 1). The boundary points of the value range are the borders of its blocks. Block construction still leaves all bins with a mixed class distribution as their own blocks. A dynamic programming algorithm lets one find optimal arity-restricted multi-way partitions efficiently in these cases [8,9,10].

Subsequently, a more general property was also proved for some evaluation functions [11]: segment borders  X  points that lie in between two adjacent bins with different relative class distributions  X  are the only points that need to be taken into account. It is easy to see that segment borders are a subset of boundary points. Example segments are easily obtained from bins by merging together adjacent bins with the same relative class distribution (see Fig. 1). Let us now consider how bins, blocks, and segments may change when new examples are received from a data stre am. We need to maintain exact counts on instances of different classes observe d for each numerica l attribute. Prior to observing the data we do not have any knowledge of the value range of a numerical attribute; what are its extreme values?, which values in the range are actually observed?, etc. Therefore, it is necessary to maintain a BST recording the observed values and class counts for each numerical attribute as proposed by Gama et al. [2]. The worst-case upda te time requirement per example is O (lg V ), where V is the number of bins (observed so far). Truly constant-time update would require giving up on recording exact bin counts (cf. [6]).
Effectively, the BST sorts the observ ed examples. Hence, we have access to bins and through them to segments also within the streaming data model. In the BST of Gama et al. [2] class frequencies are stored into the internal nodes of the tree. They only want to find the optimal binary split of the value range. Hence, it suffices to know the class distributions at both sides of the split. We, on the other hand, need to know class frequencies for all bins, which is easier to implement by storing relevant information only to the leaves. This modification does not change the asymptotic time requirement of BST processing; the maximum path length remains at O (lg V ). For segments a linked list with appropriate pointers to and from the BST is a suitable data structure.

Bins are atomic intervals in multi-way splits  X  they cannot be divided further in univariate discretization. A new ex ample received from the data stream can fall into one of the existing bins, in which case its class distribution changes, except when the bin happens to be class uniform and the new example is of the same class. In any case the bin counts (in the BST) need to be updated. Otherwise, the new example falls outside of the existing bins and makes up a new bin with a trivially uniform class distribution. The new bin may also be the first or the last one observed in the value range.
 We can consider bins to correspond to the first layer of intervals in Gama and Pinto X  X  [6] histogram discretization. However, for exact bin counting we need to maintain the BST instead of being able to just use a simple matrix. For anytime prediction, we need to cover the whole value range of a numerical attribute through binning. In particular, empty i ntervals are not permitted. Therefore, when we have observed values only sparsely from the range, we need to extend the actual bins to cover the whole rang e. Such intervals, of course, may get divided contrary to actual bins.

Blocks and segments, then, correspond to the second layer of intervals which can be constructed by merging together intervals of the first layer. In the follow-ing let us talk about segments instead of blocks and segments. Both are class coherent in any case and blocks are a special (uniform) case of segments.
What changes do updates on bins bring to segments? Let us consider a bin that made up a segment by itself without being merged together with any of the other bins. The changing class distribution of the bin makes it a candidate for being merged together with one of its neighbor segments. Hence, the distribution in the two adjacent segments needs to be checked. In the best case, the two neighbors and the changing bin all three merge together to make up a new segment. However, the merging cannot propagate further as only the middle bin X  X  class distribution has changed. Th e differences that existed between the adjacent segments and their neighbo rs remain unchanged (see Fig. 2).
In the second scenario the bin that r eceives the new example belongs to a segment. As the bin X  X  class distribution now changes, it can no longer belong to the same segment as before. Therefore, the bin needs to be taken out of the segment. If it was in the middle of the segment, the segment breaks into three new ones (see Fig. 3). On the other hand, if the bin was the head or the tail bin of the segment, we have to check whether it can be merged together with its other neighbor (if one exists) now that the class distribution has changed.
Because bins are extended to cover the whole value range, a new example cannot actually fall outside o f the existing intervals. The interval that receives the previously unseen attribute value, though, splits into two (extended) bins and the relation of class distribution in these two with their adjacent segments needs obvious checking.

In summary, in all the possible cases on ly local changes to segments are needed due to receiving a new example from the da ta stream; at most two adjacent seg-ments have to be examined. Hence, the required updates only take a constant time. Prior to these changes, though, the example is directed down the BST at the cost of O (lg V ). Our subsequent experiments will also examine the signifi-cance of this cost in real-world domains.

Quite often there is no single concep t to track from the data stream, but rather the target changes over time [3 ]. Then it does not suffice to keep stack-ing incremental changes to the decision tree, but at some point one needs to forget old examples that are not instan ces of the current concept. The simplest approach is to have a sliding window of length W of the most recent examples and maintain a decision tree consistent with them. The overhead for using such a window is constant. Let us consider this scenario without paying attention to details of window length selection and updating. We only consider what changes from the point of view of bin and segment maintenance.

The straightforward approach is to delete from the BST the oldest example in the window before (or after) inserting a new example to the BST. Deletion of an example causes similar changes to bins and segments as insertion of an example. Hence, the deletion can be ha ndled with local changes in constant time. Of course, we now traverse the BST twice doubling the time requirement of an update. Asymptotically, though, updates are as efficient as in single concept decision trees. Finally, let us point out that in a window of length W there can be at most W different values for a numerical attribute. Thus, in this scenario the maximum overhead for using the BST is a constant of the order O (lg W ). Let us now experiment with some larger data sets from the UCI repository which contain truly numerical values. Some data sets  X  like the letter and digit recognition domains  X  include seemingly numerical attributes, which in reality are nominal ones. In our evaluation we disregard attributes labeled as numerical if they have less than ten different values. We also overlook those examples that have missing values for numerical attributes. We report results individually for all numerical attributes rather than co nsider the average results (cf. [11]).
Fig. 4 shows for eleven UCI domains the reduction in cut points obtained by moving from bins to segments. The figures on right are the attributes X  bin counts and the bars represent the relative fraction of resulting segments. It is immediate that truly continuous attributes are rare. Usually the number of bins is (clearly) less than 10% of the number of examples. The exceptions are found from domains Abalone , German ,and Segmentation . The highest BST search cost that these segment counts yield is lg 5 827  X  12 . 5( Covertype ).
The reduction percentage varies from 80% to 0%. Usually for attributes with a high number bins large reductions are obtained by moving to operate on segments (see e.g., Adult and Segmentation ). However, in domains Abalone , Covertype ,and Yeast only quite small reductions are recorded for all attributes.
The domains Euthyroid and Hypothyroid are equivalent except for labeling of examples. Hence, the attribute bin counts are the same for these two domains. Nevertheless, there are notable differen ces in the numbers of segments in the two domains. This, of course, follows from the fact that bins only depend on attribute values, while segments also depend on the class distribution.
 Our second test monitors for the change of bins and segments using the Waveform data generator [12]. Fig. 5 displays for three attributes the evolution of the number of bins and segments when the number of examples grows from one thousand to ten million. The curves eventually stabilize to be more or less lin-ear. Because the x -axis is in logarithmic scale, the true growth rate for bins and segments is also logarithmic. Hence, this experiment indicates that the cost of using the BST to sort the examples by their attribute values is only of the doubly logarithmic order in the number of examples (c. 10  X  2lglg10 7 for our largest stream size). Segments slightly lose their advantage as more and more examples are received; the relative fraction of seg ments grows closer to the number of bins. Quite understandably, when segments contain many examples, the probability that two adjacent bins have exactly the same class distribution reduces. This work has extended the applicability of cut point analysis [7] to the streaming data context. However, our method only allows to solve the unbounded case efficiently. When an arity bound needs t o be enforced, the approach would seem to require quadratic processing [8,9,10]. Some form of relaxation is needed to overcome this computational burden.

Our empirical evaluation showed that for some attributes blocks and seg-ments can be of much help with only meager costs. On the other hand, some attributes  X  even whole domains  X  resist taking advantage of cut point analysis. This work has been supported by Academy of Finland projects Intents (206280), Alea (210795), and  X  X achine learning and online data structures X  (119699).
