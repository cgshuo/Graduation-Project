 1. Introduction
Text classification is one of the challenging research topics due to the necessity to organize and categorize growing num-ber of electronic documents worldwide. So far, text classification has been successfully applied to various domains such as 2011 ), web page classification ( Ozel, 2011 ) and sentiment analysis ( Maks &amp; Vossen, 2012 ).

A conventional text classification framework consists of preprocessing, feature extraction, feature selection, and classifi-cation stages. The preprocessing stage usually contains the tasks such as tokenization, stop-word removal, lowercase conversion, and stemming. The feature extraction stage generally utilizes the vector space model ( Salton, Wong, &amp; Yang, 1975 ) that makes use of the bag-of-words approach ( Joachims, 1997 ). The feature selection stage, most of the time, employs feature selector ( Uysal &amp; Gunal, 2012 ). Finally, the classification stage uses well-known and successful pattern classification algorithms, e.g., support vector machines, decision trees, artificial neural networks, and na X ve Bayesian classifier ( Theodoridis &amp; Koutroumbas, 2008 ).
 processing step may also influence this success noticeably. Common behaviour in text classification studies is to apply alpha-betic tokenization, stop-word removal, lowercase conversion and stemming, without deeply examining their contributions  X  to classification accuracy. Few researchers have analysed the influence of preprocessing tasks on text classification at some depth. For instance, effectiveness of stop-word removal and stemming are investigated for English news datasets in ( Song,
Liu, &amp; Yang, 2005 ). It is concluded that the impacts of stop-word removal and stemming are small. However, it is suggested to apply stop-word removal and stemming in order to reduce the dimensionality of feature space and promote the efficiency of the text classification system. The effects of lemmatization, stemming and stop-word removal are examined on English in most cases. On the other hand, the influence of word normalization (stemming or lemmatization) on text categorization is negative rather than positive. It is suggested that applying stop-word removal and omitting word normalization can be the best choice for text classification. The use of stop-word removal, stemming and different tokenization schemes on spam e-
SVM is surprisingly good when stemming and stop-word removal are not used. However, some stop-words are rare in spam messages and they should not be removed from feature list in spite of being semantically void. Besides, selection of the right tokenization schema may contribute to the performance of spam filtering. Furthermore, the influence of preprocessing tasks including tokenization, stop-word removal, and stemming are studied on trimmed versions of Reuters 21578, Newsgroups and Springer in ( Pomik X lek &amp; Rehurek, 2007 ). It is concluded that selection of stemmer and removal of stop-words has very little impact on the overall classification results. Besides, the effect of stemming on Arabic documents is analysed in ( Duwairi, Al-Refai, &amp; Khasawneh, 2009 ). In this study, two stemming approaches were used to investigate the effects of stemming. It is reported that one of the stemming approaches improves the accuracy of the classifier. In ( Gon X alves,
Gon X alves, Camacho, &amp; Oliveira, 2010 ), stemming and pruning are applied in combination for the classification of MEDLINE documents, whereas the other preprocessing parameters such as tokenization, lowercase conversion and stop-word removal are directly applied without comparison in all experiments. It is stated that stemming and pruning contributes to the improvement of the classification accuracy. The impact of stemming and stop-word removal on Turkish texts are evaluated in ( Torunoglu, Cakirman, Ganiz, Akyokus, &amp; Gurbuz, 2011 ) using self-compiled newspaper articles from the internet. It is concluded that stemming and stop-word removal has very little impact on classification accuracy. They claim that the effect of stop-word removal and stemming is visible when the training set size is small. The influence of stemming on Turkish news articles is studied in ( Toraman, Can, &amp; Kocberber, 2011 ) as well. They conducted some experiments with five predefined experimental settings and some of these settings include preprocessing steps. It was observed that preprocessing increased accuracies in most cases.

This paper investigates the impact of widely used preprocessing tasks including tokenization, stop-word removal, low-ercase conversion, and stemming in a different manner than those of the abovementioned studies, such that all possible combinations of those preprocessing tasks are considered comparatively in two different languages, namely Turkish and
English, and on two different text domains, namely news and e-mails. In this way, contribution of the regarding prepro-cessing tasks to the classification success at various feature dimensions, possible interactions among these tasks, and also the dependency of these tasks to the language and domain studied on are extensively assessed. In order to clarify the dif-ferences of this work from the previous ones, the investigated preprocessing tasks and experimental settings are compar-atively presented in Table 1 . Tokenization, stop-word removal, lowercase conversion and stemming are abbreviated as TK,
SR, LC and ST, respectively. The experimental settings include multiple language, multiple collection, multi-class vs. bin-ary-class collection, balanced vs. imbalanced collection, and feature selection. All these items are briefly described in the following sections.

The remainder of the paper is organized as follows: Section 2 briefly explains the preprocessing methods used in the study. Section 3 describes the experimental settings including combinations of the preprocessing methods, the datasets, the feature selection method, the classification algorithm, and the success measure utilized. Details of the experimental analysis and the related results are provided in Section 4 . Finally, some concluding remarks are given in Section 5 .
 2. Preprocessing methods
Four common preprocessing steps of text classification including tokenization, stop-word removal, lowercase conversion, and stemming are considered within the scope of this paper.

In text processing, tokenization is the procedure of splitting a text into words, phrases, or other meaningful parts, namely tokens. In other words, tokenization is a form of text segmentation. Typically, the segmentation is carried out considering only alphabetic or alphanumeric characters that are delimited by non-alphanumeric characters (e.g., punctuations, whitespace).

Stop-words are the words that are commonly encountered in texts without dependency to a particular topic (e.g., con-junctions, prepositions, articles, etc.). Therefore, the stop-words are usually assumed to be irrelevant in text classification studies, and removed prior to the classification. Stop-words are specific to the language being studied as in the case of stemming.

Another widely used preprocessing step for text classification is lowercase conversion. Since uppercase or lowercase forms of words are assumed to have no difference, all uppercase characters are usually converted to their lowercase forms prior to the classification.

The aim of stemming is to obtain stem, or root, forms of derived words. Since derived words are semantically similar to their root forms, word occurrences are usually computed after applying the stemming on a given text. Stemming algorithms are indeed specific to the language being studied. Though there are different approaches ( Zemberek, 2013 ), the fixed-prefix algorithm ( Can et al., 2008 ) is computationally simple but very effective stemming tool for
Turkish language. On the other hand, the stemming algorithm introduced in ( Porter, 1980 ) is commonly employed by researchers for English. 3. Experimental settings
Combinations of the preprocessing methods, the datasets, the feature selection method, the classification algorithm, and the success measure employed within this work are briefly explained in the following subsections. 3.1. Combinations of the preprocessing methods
In this study, all possible combinations of the preprocessing methods are considered as below so that possible interac-tions between the preprocessing tasks can be revealed. Tokenization is either alphanumeric or alphabetic. Stop-word re-moval is either ON or OFF; that is, stop-words are either eliminated or kept within text. Lowercase conversion is either
ON or OFF; that is, terms are either converted to lowercase or kept in their original forms. Stemming is either ON or OFF; that is, terms are either reduced to their root forms or kept in their inflected forms. Thus, 16 different combinations are ob-tained as listed in Table 2 .
 3.2. Datasets
The preprocessing methods are evaluated on two different domains, namely e-mail and news, and in two different lan-guages, namely Turkish and English. While Turkish is one of the widely used agglutinative languages worldwide, English is a good example of non-agglutinative languages. For fair evaluation, the numbers of documents within the same domains are kept nearly identical. The e-mail datasets contain 300 training and 100 testing samples for each class, namely spam and legitimate. Class distributions of news datasets are summarized in Table 3 . Class distributions of the news datasets are the same except the tenth class.

The e-mail datasets are balanced; on the contrary, the news datasets are imbalanced. One should also note that the email datasets are used to evaluate the preprocessing tasks on binary classification problem, whereas the news datasets are used for the evaluation on multi-class classification.
 The Turkish e-mail dataset consisting of spam and legitimate e-mails was previously constituted in ( Ergin, Gunal, Yigit, &amp; Aydin, 2012 ). The English email dataset, on the other hand, is shaped using a subset of well-known Enron dataset ( Metsis, 2008 ), and top-10 classes of well-known Reuters-21578 (ModApte split) ( Asuncion &amp; Newman, 2007 ) are employed for
Turkish and English, respectively. 3.3. Feature selection method
Even though there are filter, wrapper, and embedded approaches for feature selection ( Gunal &amp; Edizkan, 2008 ), research-ers prefer the filter methods in text classification problems due to classifier independency and relatively low computation 2012 ), is employed in this paper to select informative features. 3.4. Classification algorithm and success measure
Also, the success measure is selected as well-known Micro-F1 score ( Uysal &amp; Gunal, 2012 ) for this study. 4. Experimental work During the experiments, all possible combinations of the four preprocessing tasks were considered as mentioned before.
Various feature sizes including 10, 20, 50, 100, 200, 500, 1000, and 2000 were investigated in the study so that the impact of preprocessing can be comparatively observed within a wide range of feature dimensions. These features were determined using the chi-square based feature selection method as stated previously. The results of the experimental analysis on the four datasets are illustrated in Figs. 1 X 4 . These figures include the plots of the maximum and minimum Micro-F1 scores, and the corresponding combinations of the preprocessing tasks at different feature sizes. In this way, the best and worst cases indicating the impact of preprocessing are highlighted. Considering the maximum Micro-F1 scores, the figures also provide bar charts for coverage ratios of the terms arisen from the regarding preprocessing tasks to the selected terms at different feature sizes.
 In order to clarify the interpretation of those figures, some specific examples are provided as follows:
In Turkish email dataset, the maximum Micro-F1 score is 0.9713. This score is attained when the feature size is 200 and the preprocessing combination is (TK: 1 | SR: 0 | LC: 1 | ST: 0); that is, tokenization is alphabetic, stop-word removal is
OFF, lowercase conversion is ON, and stemming is OFF. In this case, stop-word term coverage and unstemmed term cov-erage ratios are around 20% and 45%, respectively. In other words, 20% of 200 selected terms are the stop-words whereas 45% of these 200 selected terms are unstemmed terms as well.

Additionally, the minimum Micro-F1 score is 0.8913 for Turkish email dataset. This score is obtained when the feature size is 1000 and the preprocessing combination is (TK: 1 | SR: 0 | LC: 0 | ST: 1); that is, tokenization is alphabetic, stop-word removal is OFF, lowercase conversion is OFF, and stemming is ON. It should be reminded that, in case of min-imum Micro-F1 scores, coverage ratios were not taken into consideration; therefore, they were not displayed in the figures.
Based on all the information provided in these figures, the impact of preprocessing were analysed according to several aspects including accuracy, domain, language, and feature size. 4.1. Accuracy analysis
In this part, Micro-F1 scores attained by all 16 combinations of the preprocessing tasks were measured to assess the im-pact of preprocessing in terms of accuracy. The maximum Micro-F1 scores among all feature sizes, and the corresponding preprocessing combinations are listed in Table 4 for each dataset.

Considering all four datasets, the difference between the highest and lowest Micro-F1 scores at each feature size for all preprocessing combinations ranged from 0.0113 to 0.1084. More specifically, the difference was between 0.0175 and 0.0625 in Turkish email dataset, between 0.0113 and 0.0787 in English email dataset, between 0.0195 and 0.044 in Turkish news dataset, and between 0.0179 and 0.1084 in English news dataset. The amount of differences in accuracies confirms that the appropriate preprocessing combinations depending on the domain and language may improve the accuracy consider-ably. In the meantime, inappropriate preprocessing combinations may degrade the accuracy significantly as well. The impact of preprocessing was also statistically analysed using two-tailed paired t-test over the highest and lowest
Micro-F1 scores at each feature size. P -values were obtained as (0.000138, 0.016818, 0.000007, and 0.003908) for Turkish e-mail, English e-mail, Turkish news, and English news datasets, respectively. The result for English e-mail dataset was statistically significant with a significance level of 0.05 whereas the remaining three datasets obtained a significance level of 0.01.

Besides, the findings of the proposed study in terms of contribution to the accuracy were compared against the previous works mentioned in the introduction section. The comparison is presented in Table 5 , where  X  X + X  X  and  X  X   X  X  signs respectively represent positive and negative impacts, and  X  X  X /A X  X  indicate that the corresponding analysis is not available. One should note that the impacts of the preprocessing tasks for the proposed work are based on the combinations listed in Table 4 . According to the proposed work, applying alphabetic tokenization, lowercase conversion and stemming, while not applying stop-word removal has positive impact overall. Although the previous works does not analyse combinations of all four preprocessing tasks, it may be stated that the impact of stemming is consistent with most of the previous works, whereas the impact of stop-word removal seems to be opposite. The lowercase conversion was proven to be helpful; however, it cannot be com-pared to the previous works. Finally, the contribution of appropriate tokenization to the accuracy was positive, which is con-sistent with the single previous work that is the only one analysing tokenization. 4.2. Domain and language analysis In this part, the impact of preprocessing was evaluated for every domain and language considering the maximum Micro-F1 scores at each case.

Tokenization type in e-mail domain for both languages was alphabetic; however, news domain involved alphabetic tok-enization in Turkish, and alphanumeric tokenization in English. To confirm the impact of alphanumeric tokenization, nu-meric term coverage within the selected feature set of English news dataset was also computed. The coverage ratio was around 10%; in other words, 10% of the selected terms contained numeric characters. When the selected terms were further investigated, it was revealed that specifically the business related news contain significant numbers of numeric terms such losing their discriminative powers. On the other hand, there would be no such problem in case of alphanumeric tokenization.
Stop-word removal is not applied in any of the domains and languages. In order to verify the impact of stop-words on classification success, stop-word coverage within the selected feature sets of each dataset was also computed. The coverage ratios were found as 20.50%, 20.60%, 6.50%, and 11.25% for Turkish e-mail, English e-mail, Turkish news, and English news datasets, respectively. It should be noted that e-mails are usually short whereas news are much longer. Even under these circumstances, one can see from those ratios that the presence of the stop-words within the selected terms is very obvious in each domain and language. This finding is really remarkable bearing in mind that most of the text classification studies in the literature remove stop-words directly by assuming them irrelevant.

Lowercase conversion is active in both domains and languages. In other words, all characters should be converted to low-ercase without dependency to domain or language. Since lowercase conversion helps grouping the terms which contain the same information with either upper or lower case characters, less amount of features with more discrimination are attained.
Stemming is required in news domain for both languages; on the contrary, it is not applied in Turkish e-mail domain whereas it is necessary for English e-mails. Again, to validate the impact of not applying the stemming in Turkish e-mail do-main, unstemmed term coverage within the selected feature set was computed and found to be around 45%. In other words, almost half of the selected terms consist of unstemmed terms. By means of its agglutinative property, deriving large number of words from the same root by appending suffixes is possible for Turkish language. Especially in Turkish e-mails, these de-rived forms may exist frequently and they can become more discriminative than their root forms.

One can conclude that stop-words should not be removed and characters should be converted to lowercase without dependency to domain or language. However, tokenization type and stemming status may change depending on the domain and language.

Reminding that the email datasets are binary and balanced whereas the news datasets are multi-class and imbalanced, all the statements above may be generalized for different class distributions (balanced vs. imbalanced) and different numbers of classes (binary vs. multi-class) as well. 4.3. Feature size analysis
In this part, the impact of preprocessing was evaluated in terms of dimension reduction. For this purpose, the preprocess-ing tasks providing the highest Micro-F1 scores at minimum feature size for each dataset were taken into consideration as listed in Table 6 .

In e-mail domain, as a common behaviour in both languages, lowercase conversion was applied. Status of the remaining preprocessing tasks, however, varied depending on the language.

In news domain, alphabetic tokenization and lowercase conversion were common preprocessing tasks in both languages whereas status of stop-word removal and stemming were opposite for each language. While stop-word removal was applied in English news, stemming was required for Turkish news.

For Turkish language, stop-word removal was applied only on e-mail domain while alphabetic tokenization, lowercase conversion, and stemming were commonly applied in both domains. Stop-word coverage ratio within the selected feature set of Turkish news dataset was computed as 90%. Hence, it is obvious that the stop-words have a dominant impact among all terms at minimum feature dimension of Turkish news domain only.

For English language, lowercase conversion was applied, but stemming was not active in both domains. Unstemmed term coverage ratios within the selected feature sets were 20% and 10% for e-mail and news datasets in English, respectively. On the other hand, the status of tokenization type and stop-word removal were opposite for each language. While alphabetic tokenization was applied in English news domain, stop-words were kept in English e-mail domain. Stop-word coverage ratio within the selected feature set of English e-mail dataset was computed as 20%, and numeric term coverage ratio was found as 10% in English news dataset as well. 4.4. Maximum accuracy vs. minimum feature size
In this section, the preprocessing tasks, which provided the maximum Micro-F1 scores, were compared to the ones pro-viding the highest Micro-F1 scores at minimum feature size for each domain and language. The comparison is listed in Table 7 .

It is obvious from the table that lowercase conversion is the only preprocessing task that is common in all cases. In other words, lowercase conversion should be applied to achieve either maximum accuracy or minimum feature size with the high-est accuracy for all domains and languages. Another common behaviour was related to Turkish language such that alpha-betic tokenization should be applied in Turkish, regardless of the domain, to achieve either maximum accuracy or minimum feature size. No other common behaviour was observed related to the remaining preprocessing tasks for any do-main or language. 5. Conclusions
In this paper, the influence of widely used preprocessing tasks on text classification was thoroughly examined in two dif-ferent domains and languages. The examination was carried out using all possible combinations of the preprocessing tasks by considering various aspects such as accuracy, domain, language, and dimension reduction. Extensive experimental anal-ysis revealed that appropriate combinations of preprocessing tasks depending on the domain and language may provide a significant improvement on classification accuracy whereas inappropriate combinations may degrade the accuracy as well. tion steps.

Although there are particular preprocessing tasks such as lowercase conversion that improve classification success in terms of accuracy and dimension reduction regardless of domain and language, there is no unique combination of prepro-cessing tasks providing successful classification results for every domain and language studied on. Therefore, for a text clas-sification problem on any domain and in any language, researchers should carefully analyse all possible combinations of the tasks rather than completely/individually enabling or disabling them. Otherwise, classification results may significantly dif-fer. Another interesting finding of the study was the importance of stop-words while most of the text classification studies in the literature assume the stop-words irrelevant.

Since all four datasets studied on the paper have distinct characteristics in terms of domain, language, class distribution, and number of classes, the outcome of this paper may be generalized for the other text collections as well. References
