 We describe matrix computations available in the cluster programming framework, Apache Spark. Out of the box, Spark provides abstractions and implementations for dis-tributed matrices and optimization routines using these ma-trices. When translating single-node algorithms to run on a distributed cluster, we observe that often a simple idea is enough: separating matrix operations from vector opera-tions and shipping the matrix operations to be ran on the cluster, while keeping vector operations local to the driver. In the case of the Singular Value Decomposition, by taking this idea to an extreme, we are able to exploit the computa-tional power of a cluster, while running code written decades ago for a single core. Another example is our Spark port of the popular TFOCS optimization package, originally built for MATLAB, which allows for solving Linear programs as well as a variety of other convex programs. We conclude with a comprehensive set of benchmarks for hardware accelerated matrix computations from the JVM, which is interesting in its own right, as many cluster programming frameworks use the JVM. The contributions described in this paper are al-ready merged into Apache Spark and available on Spark installations by default, and commercially supported by a slew of companies which provide further services.  X  Corresponding author.
  X  Mathematics of computing  X  Mathematical soft-ware; Solvers;  X  Computing methodologies  X  MapRe-duce algorithms; Machine learning algorithms; Concur-rent algorithms; Distributed Linear Algebra, Matrix Computations, Opti-mization, Machine Learning, MLlib, Spark
Modern datasets are rapidly growing in size and many datasets come in the form of matrices. There is a press-ing need to handle large matrices spread across many ma-chines with the same familiar linear algebra tools that are available for single-machine analysis. Several  X  X ext genera-tion X  data flow engines that generalize MapReduce [5] have been developed for large-scale data processing, and build-ing linear algebra functionality on these engines is a prob-lem of great interest. In particular, Apache Spark [12] has emerged as a widely used open-source engine. Spark is a fault-tolerant and general-purpose cluster computing system providing APIs in Java, Scala, Python, and R, along with an optimized engine that supports general execution graphs.
In this work we present Spark X  X  distributed linear alge-bra and optimization libraries, the largest concerted cross-institution effort to build a distributed linear algebra and optimization library. The library targets large-scale matri-ces that benefit from row, column, entry, or block sparsity to store and operate on distributed and local matrices. The li-brary, named linalg consists of fast and scalable implemen-tations of standard matrix computations for common linear algebra operations including basic operations such as mul-tiplication and more advanced ones such as factorizations. It also provides a variety of underlying primitives such as column and block statistics. Written in Scala and using na-tive ( C++ and fortran based) linear algebra libraries on each node, linalg includes Java, Scala, and Python APIs, and is released as part of the Spark project under the Apache 2.0 license.
We restrict our attention to Spark, because it has several features that are particularly attractive for matrix compu-tations: 1. The Spark storage abstraction called Resilient Dis-2. RDDs permit user-defined data partitioning, and the 3. Spark logs the lineage of operations used to build an 4. Spark provides a high-level API in Scala that can be
There exists a history of using clusters of machines for distributed linear algebra, for example [3]. These systems are often not fault-tolerant to hardware failures and assume random access to non-local memory. In contrast, our library is built on Spark, which is a dataflow system without direct access to non-local memory, designed for clusters of commod-ity computers with relatively slow and cheap interconnects, and abundant machines failures. All of the contributions de-scribed in this paper are already merged into Apache Spark and available on Spark installations by default, and com-mercially supported by a slew of companies which provide further services.
Given that we have access to RDDs in a JVM environ-ment, four key challenges arise to building a distributed lin-ear algebra library, each of which we address: 1. Data representation: how should one partition the en-2. Matrix Computations must be adapted for running on 3. Many distributed computing frameworks such as Spark 4. Given that there are many cases when distributed ma-
Before we can build algorithms to perform distributed ma-trix computations, we need to lay out the matrix across ma-chines. We do this in several ways, all of which use the sparsity pattern to optimize computation and space usage. A distributed matrix has long-typed row and column in-dices and double-typed values, stored distributively in one or more RDDs. It is very important to choose the right for-mat to store large and distributed matrices. Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive. Three types of distributed matrices have been implemented so far.
A RowMatrix is a row-oriented distributed matrix with-out meaningful row indices, backed by an RDD of its rows, where each row is a local vector. Since each row is repre-sented by a local vector, the number of columns is limited by the integer range but it should be much smaller in prac-tice. We assume that the number of columns is not huge for a
RowMatrix so that a single local vector can be reason-ably communicated to the driver and can also be stored / operated on using a single machine.

An IndexedRowMatrix is similar to a RowMatrix but with meaningful row indices. It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector.
A CoordinateMatrix is a distributed matrix backed by an RDD of its entries. Each entry is a tuple of (i: Long, j: Long, value: Double) , where i is the row index, j is the column index, and value is the entry value.

A CoordinateMatrix should be used only when both dimensions of the matrix are huge and the matrix is very sparse. A CoordinateMatrix can be created from an RDD[MatrixEntry] instance, where MatrixEntry is a wrapper over (Long, Long, Double) . A CoordinateM-atrix can be converted to an IndexedRowMatrix with sparse rows by calling toIndexedRowMatrix . A BlockMatrix is a distributed matrix backed by an RDD of MatrixBlock s, where a MatrixBlock is a tuple of ((Int, Int), Matrix) , where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given in-dex with size rowsPerBlock  X  colsPerBlock. BlockMatrix supports methods such as add and multiply with another BlockMatrix . BlockMatrix also has a helper function vali-date which can be used to check whether the BlockMatrix is set up properly.
Spark supports local vectors and matrices stored on a sin-gle machine, as well as distributed matrices backed by one or more RDDs. Local vectors and local matrices are simple data models that serve as public interfaces. The underly-ing linear algebra operations are provided by Breeze and jblas. A local vector has integer-type and 0-based indices and double-typed values, stored on a single machine. Spark supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] in sparse format as (3, [0, 2], [1.0, 3.0]) , where 3 is the size of the vector.
We now move to the most challenging of tasks: rebuilding algorithms from single-core modes of computation to operate on our distributed matrices in parallel. Here we outline some of the more interesting approaches.
The rank k singular value decomposition (SVD) of an m  X  n real matrix A is a factorization of the form A = U  X  V T where U is an m  X  k unitary matrix,  X  is an k  X  k diagonal matrix with non-negative real numbers on the diagonal, and V is an n  X  k unitary matrix. The diagonal entries  X  are known as the singular values. The k columns of U and the n columns of V are called the  X  X eft-singular vectors X  and  X  X ight-singular vectors X  of A , respectively.

Depending on whether the m  X  n input matrix A is tall and skinny ( m n ) or square, we use different algorithms to compute the SVD. In the case that A is roughly square, we use the ARPACK package for computing eigenvalue de-compositions, which can then be used to compute a singu-lar value decomposition via the eigenvalue decomposition of A T A . In the case that A is tall and skinny, we compute A
T A , which is small, and use it locally. In the following two sections with detail the approaches for each of these two cases. Note that the SVD of a wide and short matrix can be recovered from its transpose, which is tall and skinny, and so we do not consider the wide and short case.

There is a well known connection between the eigen and singular value decompositions of a matrix, that is the two decompositions are the same for positive semidefinite matri-ces, and A T A is positive semidefinite with its singular values being squares of the singular values of A . So one can recover the SVD of A from the eigenvalue decomposition of A T A . We exploit this relationship in the following two sections.
ARPACK is a collection of Fortran77 subroutines designed to solve eigenvalue problems [6]. Written many decades ago and compiled for specific architectures, it is surprising that it can be effectively distributed on a modern commodity cluster.

The package is designed to compute a few eigenvalues and corresponding eigenvectors of a general n  X  n matrix A . In the local setting, it is appropriate for sparse or structured matrices where structured means that a matrix-vector prod-uct requires order n rather than the usual order n 2 floating point operations and storage. APRACK is based upon an algorithmic variant of the Arnoldi process called the Im-plicitly Restarted Arnoldi Method (IRAM). When the ma-trix A is symmetric it reduces to a variant of the Lanc-zos process called the Implicitly Restarted Lanczos Method (IRLM). These variants may be viewed as a synthesis of the Arnoldi/Lanczos process with the Implicitly Shifted QR technique. The Arnoldi process only interacts with the ma-trix via matrix-vector multiplies.

APRACK is designed to compute a few, say k eigenvalues with user specified features such as those of largest real part or largest magnitude. Storage requirements are on the order of nk doubles with no auxiliary storage is required. A set of Schur basis vectors for the desired k -dimensional eigen-space is computed which is numerically orthogonal to working pre-cision. The only interaction that ARPACK needs with a matrix is the result of matrix-vector multiplies.

By separating matrix operations from vector operations, we are able to distribute the computations required by ARPACK. An important feature of ARPACK is its ability to allow for arbitrary matrix formats. This is because it does not operate on the matrix directly, but instead acts on the ma-trix via prespecified operations, such as matrix-vector multi-plies. When a matrix operation is required, ARPACK gives control to the calling program with a request for a matrix-vector multiply. The calling program must then perform the multiply and return the result to ARPACK. By using the distributed-computing utility of Spark, we can distribute the matrix-vector multiplies, and thus exploit the computational resources available in the entire cluster.

Since ARPACK is written in Fortran77, it cannot im-mediately be used on the Java Virtual Machine. However, through the netlib-java and breeze packages, we use ARPACK on the JVM on the driver node and ship the computations required for matrix-vector multiplies to the cluster. This also means that low-level hardware optimizations can be ex-ploited for any local linear algebraic operations. As with all linear algebraic operations within MLlib, we use hardware acceleration whenever possible. This functionality has been available since Spark 1.1.

We provide experimental results using this idea. A very popular matrix in the recommender systems community is the Netflix Prize Matrix. The matrix has 17,770 rows, 480,189 columns, and 100,480,507 non-zeros. Below we report results on several larger matrices, up to 16x larger.

With the Spark implementation of SVD using ARPACK, calculating wall-clock time with 68 executors and 8GB mem-ory in each, looking for the top 5 singular vectors, we can factorize larger matrices distributed in RAM across a clus-ter, in a few seconds, with times listed Table 1.
In the case that the input matrix has few enough columns that A T A can fit in memory on the driver node, we can avoid shipping the eigen-decomposition to the cluster and avoid the associated communication costs.

First we compute  X  and V . We do this by comput-ing A T A , which can be done with one all-to-one commu-nication, details of which are available in [11, 10]. Since A
T A = V  X  2 V T is of dimension n  X  n , for small n (for ex-ample n = 10 4 ) we can compute the eigen-decomposition of A T A directly and locally on the driver to retrieve V and  X .
Once V and  X  are computed, we can recover U . Since in this case n is small enough to fit n 2 doubles in memory, then V and  X  will also fit in memory on the driver. U however will not fit in memory on a single node and will need to be distributed, and we still need to compute it. This can be achieved by computing U = AV  X   X  1 which is derived from A = U  X  V T .  X   X  1 is easy to compute since it is diagonal, and the pseudo-inverse of V is its transpose and also easy to compute. We can distribute the computation of U = A ( V  X   X  1 ) by broadcasting V  X   X  1 to all nodes holding rows of U , and from there it is embarrassingly parallel to compute U .

The method computeSVD on the RowMatrix class takes care of which of the tall and skinny or square versions to in-voke, so the user does not need to make that decision.
To allow users of single-machine optimization algorithms to use commodity clusters, we have developed Spark TFOCS, which is an implementation of the TFOCS convex solver for Apache Spark.

The original Matlab TFOCS library [1] provides build-ing blocks to construct efficient solvers for convex problems. Spark TFOCS implements a useful subset of this functional-ity, in Scala, and is designed to operate on distributed data using the Spark. Spark TFOCS includes support for:
The name  X  X FOCS X  is being used with permission from the original TFOCS developers, who are not involved in the development of this package and hence not responsible for the support. To report issues or download code, please see the project X  X  GitHub page
TFOCS is a state of the art numeric solver; formally, a first order convex solver [1]. This means that it optimizes functions that have a global minimum without additional lo-cal minima, and that it operates by evaluating an objective function, and the gradient of that objective function, at a series of probe points. The key optimization algorithm im-plemented in TFOCS is Nesterov  X  Os accelerated gradient de-scent method, an extension of the familiar gradient descent algorithm. In traditional gradient descent, optimization is performed by moving  X  X ownhill X  along a function gradient from one probe point to the next, iteration after iteration. The accelerated gradient descent algorithm tracks a linear combination of prior probe points, rather than only the most recent point, using a clever technique that greatly improves asymptotic performance.

TFOCS fine-tunes the accelerated gradient descent algo-rithm in several ways to ensure good performance in prac-tice, often with minimal configuration. For example TFOCS supports backtracking line search. Using this technique, the optimizer analyzes the rate of change of an objective func-tion and dynamically adjusts the step size when descending along its gradient. As a result, no explicit step size needs to be provided by the user when running TFOCS.
Matlab TFOCS contains an extensive feature set. While the initial version of Spark TFOCS implements only a subset of the many possible features, it contains sufficient function-ality to solve several interesting problems. A LASSO linear regression problem (otherwise known as L1 regularized least squares regression) can be described and solved easily using TFOCS. Objective functions are provided to TFOCS in three separate parts, which are together re-ferred to as a composite objective function. The complete LASSO objective function can be represented as: This function is provided to TFOCS in three parts. The first part, the linear component, implements matrix multi-plication: The next part, the smooth component, implements quadratic loss: And the final part, the nonsmooth component, implements L1 regularization: The TFOCS optimizer is specifically implemented to lever-age this separation of a composite objective function into component parts. For example, the optimizer may evaluate the (expensive) linear component and cache the result for later use.

Concretely, in Spark TFOCS the above LASSO regression problem can be solved as follows:
Here, SmoothQuad is the quadratic loss smooth com-ponent, LinopMatrix is the matrix multiplication linear component, and ProxL1 is the L 1 norm nonsmooth compo-nent. The x0 variable is an initial starting point for gradient descent. Spark TFOCS also provides a helper function for solving LASSO problems, which can be called as follows:
Solving a linear programming problem requires minimiz-ing a linear objective function subject to a set of linear constraints. TFOCS supports solving smoothed linear pro-grams, which include an approximation term that simplifies finding a solution. Smoothed linear programs can be repre-sented as: A smoothed linear program can be solved in Spark TFOCS using a helper function as follows:
A complete linear program example is presented here:
We now focus on Convex optimization via gradient de-scent for separable objective functions. That is, objective functions that can be written in the form of where w is a d -dimensional vector of parameters to be tuned and each F i ( w ) represents the loss of the model for the i  X  X h training point. In the case that d doubles can fit in mem-ory on the driver, the gradient of F ( w ) can be computed using the computational resources on the cluster, and then collected on the driver, where it will also fit in memory. A simple gradient update can be done locally and then the new guess for w broadcast out to the cluster. This idea is essentially separating the matrix operations from the vec-tor operations, since the vector of optimization variables is much smaller than the data matrix.

Given that the gradient can be computed using the clus-ter and then collected on the driver, all computations on the driver can proceed oblivious to how the gradient was com-puted. This means in addition to gradient descent, we can use tradtional single-node implementations of all first-order optimization methods that only use the gradient, such as accelerated gradient methods, LBFGS, and variants thereof. Indeed, we have LBFGS and accelerated gradient methods implemented in this way and available as part of MLlib. For the first time we provide convergence plots for these optimization primitives available in Spark, listed in Figure 1. We have available the following optimization algorithms, with convergence plots in Figure 1:
In Figure 1 the x axis shows the number of outer loop it-erations of the optimization algorithm. Note that for back-tracking implementations, the full cost of backtracking is not represented in this outer loop count. For non-backtracking implementations, the number of outer loop iterations is the same as the number of spark map reduce jobs. The y axis is the log of the difference from best determined optimized value. The optimization test runs were:
For all runs, all optimization methods were given the same initial step size. We now note some observations. First, ac-celeration consistently converges more quickly than standard gradient descent, given the same initial step size. Second, automatic restarts are indeed helpful for accelerating conver-gence. Third, Backtracking can significantly boost conver-gence rates in some cases (measured in terms of outer loop iterations), but the full cost of backtracking was not mea-sured in these runs. Finally, LBFGS generally outperformed accelerated gradient descent in these test runs.
There are several matrix computations on distributed ma-trices that use algorithms previously published, so we only cite them here:
To allow full use of hardware-specific linear algebraic op-erations on a single node, we use the BLAS (Basic Linear Al-gebra Subroutines) interface with relevant libraries for CPU and GPU acceleration. Native libraries can be used in Scala as follows. First, native libaries must have a C BLAS inter-face or wrapper. The latter is called through the Java native interface implemented in Netlib-java library and wrapped by the Scala library called Breeze. We consider the following implementations of BLAS like routines:
In addition to the mentioned libraries, we also consider the BIDMat matrix library that can use MKL or cuBLAS. Our benchmark includes matrix-matrix multiplication rou-tine called GEMM. This operation comes from BLAS Level 3 and can be hardware optimized as opposed to the opera-tions from the lower BLAS levels. We benchmark GEMM with different matrix sizes both for single and double preci-sion. The system used for benchmark is as follows: The results for the double precision matrices are depicted on Figure 2. A full spreadsheet of results and code is available at https://github.com/avulanov/scala-blas.
 Results show that MKL provides similar performance to OpenBLAS except for tall matrices when the latter is slower. Most of the time GPU is less effective due to overhead of copying matrices to/from GPU. However, when multiplying sufficiently large matrices, i.e. starting from 10000  X  10000 by 10000  X  1000, the overhead becomes negligible with re-spect to the computation complexity. At that point GPU is several times more effective than CPU. Interestingly, adding more GPUs speeds up the computation almost linearly for big matrices.
 Because it is unreasonable to expect all machines that Spark is run on to have GPUs, we have made OpenBlas the default method of choice for hardware acceleration in Spark X  X  local matrix computations. Note that these per-formance numbers are useful anytime the JVM is used for Linear Algebra, including Hadoop, Storm, and popular com-modity cluster programming frameworks. As an example of BLAS usage in Spark, Neural Networks available in MLlib use the interface heavily, since the forward and backpropa-gation steps in neural networks are a series of matrix-vector multiplies.

A full spreadsheet of results and code is available at https://github.com/avulanov/scala-blas
The BLAS interface is made specifically for dense linear algebra. There are not many libraries on the JVM that efficiently handle sparse matrix operations, or even provide the option to store a local matrix in sparse format. MLlib provides SparseMatrix , which provides memory efficient storage in Compressed Column Storage (CCS) format. In this format, a row index and a value is stored for each non-zero element in separate arrays. The columns are formed by storing the first and the last indices of the elements for that column in a separate array.
 MLlib has specialized implementations for performing Sparse Matrix  X  Dense Matrix, and Sparse Matrix  X  Dense Vec-tor multiplications, where matrices can be optionally trans-posed. These implementations outperform libraries such as Breeze, and are competitive against libraries like SciPy, where implementations are backed by C. Benchmarks avail-able at https://github.com/apache/spark/pull/2294. We described the distributed and local matrix computa-tions available in Apache Spark, a widely distributed cluster programming framework. By separating matrix operations from vector operations, we are able to distribute a large number of traditional algorithms meant for single-node us-age. This allowed us to solve Spectral and Convex opti-mization problems, opening to the door to easy distribu-tion of many machine learning algorithms. We conclude by providing a comprehensive set of benchmarks on accessing hardware-level optimizations for matrix computations from the JVM.
 We thank all Spark contributors, a list of which can be found at: https://github.com/apache/spark/graphs/contributors Spark and MLlib are cross-institutional efforts, and we thank the Stanford ICME, Berkeley AMPLab, MIT CSAIL, Databricks, Twitter, HP labs, and many other institutions for their sup-port. We further thank Ion Stoica, Stephen Boyd, Em-manuel Candes, and Steven Diamond for their valuable dis-cussions. [1] Stephen R Becker, Emmanuel J Cand`es, and [2] Austin R Benson, David F Gleich, and James [3] L Susan Blackford, Jaeyoung Choi, Andy Cleary, [4] Weizhu Chen, Zhenghao Wang, and Jingren Zhou. [5] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: [6] Richard B Lehoucq, Danny C Sorensen, and Chao [7] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan [8] Brendan O X a  X  A  X  ZDonoghue and Emmanuel Candes. [9] Reza Bosagh Zadeh. Large linear model parallelism [10] Reza Bosagh Zadeh and Gunnar Carlsson. Dimension [11] Reza Bosagh Zadeh and Ashish Goel. Dimension [12] Matei Zaharia, Mosharaf Chowdhury, Michael J [13] Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge To find information about the implementations used, here we provide links to implementations used in Figure 2. 1. Netlib, Reference BLAS and CBLAS http://www.netlib. 2. Netlib-java https://github.com/fommil/netlib-java 3. Breeze https://github.com/scalanlp/breeze 4. BIDMat https://github.com/BIDData/BIDMat/ 5. OpenBLAS https://github.com/xianyi/OpenBLAS 6. CUDA http://www.nvidia.com/object/cuda home new. 7. NVBLAS http://docs.nvidia.com/cuda/nvblas
