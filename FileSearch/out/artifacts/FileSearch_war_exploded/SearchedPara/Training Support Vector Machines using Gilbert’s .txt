 around the computation of an optimal separating hyper-plane. This hyperplane is typically obtained by solving a constrained quadratic programming problem, but may also be located by solving a nearest point problem. Gilbert X  X  Algorithm can be used to solve this nearest point problem but is unreasonably slow. In this paper we present a modified version of Gilbert X  X  Algorithm for the fast computation of the Support Vector Machine hyperplane. We then compare our algorithm with the Nearest Point Algorithm and with Sequential Minimal Optimization. Keywords: Support Vector Machines, Gilbert X  X  Al-gorithm, Nearest Point Algorithm, Sequential Minimal Optimization 1 Introduction Support Vector Machines are classifiers which were devel-oped in [21], [6], [8], and elsewhere. The maximal margin hyperplane was presented in [21], the use of kernels was suggested in [6], and the soft margin generalization was given in [8]. Support Vector Machines are very adaptable (able to assume polynomial, Radial Basis Function, and Neural Network forms) and have been applied successfully to a variety of problems. For an excellent introduction to Support Vector Machines see [7], [3]. For an encyclopedic treatment see [22].
 constrained quadratic programming problem (SVM QP). Since the SVM QP problem is often too large for standard solvers, SVM specific training algorithms are used. Some of the original work in this area includes [17], [19], [12], [9], [16], and [15]. In [17] a decomposition of the full SVM QP problem into subproblems is proposed, in [19] this de-composition is used with subproblems of the smallest pos-be found by solving In the case of linearly separable two class data the Support Vector Machine classifier uses exactly this maximal margin hyperplane, illustrated in Figure 1(a).
 solving the dual problem In this, the linear SVM quadratic programming problem, the solution w  X  has the form i y i  X  i x i , where many of the  X  i are zero. The x i corresponding to the nonzero  X  i are known as support vectors.
 margin hyperplane may also be found geometrically [15], [2], [20], [3]. Suppose we form the set of secants S = X  X  Y = { x i  X  x j : y i =1 ,y j =  X  1 } between our two classes X = { x i : y i =1 } and Y = { x j : y j =  X  1 } .We can locate the normal to the maximal margin hyperplane by finding the point s  X  on the convex hull of the secant set S closest to the origin. This observation is illustrated in Figures 1(b) and (c) and exploited in our algorithm for training Support Vector Machines.
 indeed a linear discriminant, the more typical SVM is nonlinear. This nonlinearity is achieved by remapping the original data. Specifically, a linear Support Vector Machine is trained on the images of the original data under a nonlinear map  X : R n  X  F ,where F is a Hilbert space. What X  X  more, this calculation is accomplished implicitly using a kernel  X  : R n  X  R n  X  R associated with  X  which satisfies The nonlinear Support Vector Machine quadratic program-ming problem is obtained by replacing the inner products in the linear SVM QP problem by kernels to get the more general SVM QP problem For more information on this approach see [7], [6], and [22].
 margin hyperplane is shown with normal s  X  . closest point in the convex hull of the secants to the origin. Gilbert also made some observations on the convergence properties of the sequence w k . We note merely that Gilbert X  X  Algorithm will converge in a finite number of steps if s  X   X  S , but that asymptotic convergence is very likely when s  X  /  X  S . Gilbert X  X  Algorithm is illustrated in Figure 2 parts (a) and (b).
 Gilbert X  X  Algorithm in Figure 2(b) is not only likely but very slow as well (  X  1 /n ). It was for this reason that Gilbert X  X  Algorithm was abandoned in [15]. In Figure 2(b), however, it appears that Gilbert X  X  Algorithm converges to s  X  in angle much faster than in norm. We confirmed this observation by computing ( w k  X  s  X  ) / ( w k s  X  ) and com-paring the resulting convergence to 1 (  X  1 /n 2 ) with the convergence of w k  X  s  X  to 0 (  X  1 /n ). Since the SVM maximal margin hyperplane is determined by direction and not by length, we can use this angle convergence criterion to potentially improve the p erformance of Gilbert X  X  Algo-rithm for training Support Vector Machines. In this paper, we investigate this hypothesis using several experiments. the rate of convergence in angle of Gilbert X  X  Algorithm by using an associated sequence of averages computed as follows. To compute m 1 , we iterate Gilbert X  X  Algorithm until g  X  (  X  w k  X  1 )= g  X  (  X  w j  X  1 ) for some j&lt;k .Weset m process as if we were restarting Gilbert X  X  Algorithm from w k . In this manner, we obtain a sequence m 1 , m 2 ,... of averages of points in Gilbert X  X  Algorithm which also converge to s  X  (since the points in Gilbert X  X  Algorithm to s  X  . where the  X  i also depend on k . We need to cal-S is indexed by m and that X and Y are indexed by i and j ,sothat x i  X  X and x j  X  Y (see Section 2 for definitions of X, Y, and S ). Now observe that g In this case g  X  X ( x ) is meant to be the point x i 0  X  X with smallest index i 0 such that x  X  x i 0  X  x  X  x i for all i ,and similarly for g  X  Y ( x ) . (See also [15] for an explanation of this decomposition.) g we keep an inner product cache with the values w k  X  1  X  x i and w k  X  1  X  x j as we progress through the points in Gilbert X  X  Algorithm. The idea of using such a cache was originated in [19] and adopted in [15].
 g  X  (  X  w k  X  1 )  X  w k  X  1 2 . These quantities can be computed using the above inner product cache along with w k  X  1 2 . To facilitate the computation of w k  X  1 2 we observe that containing 90% of the original data. We used the remaining 10% of each data set as a validation set. For each algorithm, we used a convergence tolerance of = . 001 and a Radial Basis Function kernel  X  ( x , y )=exp(  X  x  X  y 2 2  X  2 ) .As suggested in [15], we used  X  =1 / Data,  X  =2 for the Wisconsin Breast Cancer Data, and  X  = (1) the number of kernel evaluations needed to train a SVM on the data; (2) the percentage of support vectors found (number of support vectors found versus number of data vectors); and (3) and the percentage of the validation set correctly classified. These statistics were selected to measure speed (1) and quality of solution (2) &amp; (3). The number of kernel evaluations performed was suggested as a measure of speed in [15] because the various algorithms considered (including NPA a nd SMO) were implemented using inner product caches. Thus, the cost of updating said caches (number of kernel evaluations performed) was the main computational expense.
 margin of the resulting SVM for each algorithm, where the solution margin is the distance between the two classes as separated by the SVM hyperpl ane (the distance between the dotted lines in Figure 1(a)). The solution margin was used on the x-axis to compare the different algorithms because it is a constant relative to the different formulations used by our algorithm, NPA, and SMO. This method of comparison is also used in [15]. The statistics for the Two Spirals Data can be found in Figure 3; the statistics for the Wisconsin Breast Cancer Data can be found in Figure 4; and the statistics for the Adult-4a Data can be found in Figure 5.
 Figures 3, 4, and 5, that our algorithm is similar in speed to both NPA and SMO for large margins (corresponding to small values of C ), and significantly faster than NPA and SMO for small margins (large values of C ). It is also interesting to note that our algorithm provides Support Vector Machines with representations which are similar (in terms of number of support vectors) to NPA and SMO in some cases (part (b) of Figures 3 and 4), but which can be much more efficient (fewer support vectors) in other cases (part (b) of Figure 5). Finally, we note that all three algorithms performed equally on the validation set (part (c) of Figures 3, 4 and 5).
 be faster for small margins and should find fewer support vectors, we can offer the following conjectures. The performance of our method for small margins is likely due to the phenomenon discussed in Section 3 given as motivation for the averages m 1 , m 2 ,... . A small margin indicates a situation where the convex hull of the secant set Figure 4. Wisconsin Breast Cancer Data.
 Here we plot the performance statistics for
SMO, NPA and Gilbert X  X  Algorithm for SVMs on the Wisconsin Breast Cancer Data. The data is presented as in Figure 3. The success rates on the validation set are identical for all three algorithms. is close to the origin and the support vectors are relatively distant (as in the triangle in Figure 2). this is a difficult situation for training a SVM but is also the situation that motivated our heuristic for locating s  X  using the averages m 1 , m 2 ,... .
 may also be related to the same discussion. When Gilbert X  X  Algorithm reaches a stage where it is iterating over the same set of support vectors, these support vectors will most likely be a subset of all possible support vectors, specifically the support vectors which are furthest from the origin (resulting in the largest moves for Gilbert X  X  Algorithm). The other support vectors will have little or no effect on Gilbert X  X  Algorithm and will be passed over in the sequence. Since our method is based on Gilbert X  X  Algorithm, these same support vectors will be passed over by our method as well. 5 Conclusions Our work is similar to the work in [15]. Gilbert X  X  Algorithm was also considered in [15], but dismissed because of slow convergence in its final stages. We have, on the other hand, successfully applied Gilbert X  X  Algorithm to Support Vector Machines via a simple modification. Our modification made an ailing algorithm into a fast method for training SVMs and might similarly accelerate existing algorithms. a number of advantages over both NPA and SMO. First, it is simpler (no heuristics) than SMO and much simpler than NPA. Second, Gilbert X  X  Algorithm for SVMs runs almost as fast (in cpu time) coded in MATLAB as NPA in Fortran and SMO in C++. Thus, it may be faster than both NPA and SMO when coded in C++. (MATLAB is generally much slower than either Fortran or C++.) Third and last, the Support Vector Machines produced by Gilbert X  X  Algorithm for SVMs are as accurate as those produced by NPA and SMO and may have more efficient representations (fewer support vectors). 6 Acknowledgements This work has been supported by research grant DOD-USAF Office of Scientific Research F49620-99-1-0034and by Sandia National Laboratories Laboratory Directed Re-search and Development. The Wisconsin Breast Cancer Data is made available by Dr. William H. Wolberg at the University of Wisconsin Hospital, Madison. Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Depart-ment of Energy under Contract DE-AC04-94AL85000. References
