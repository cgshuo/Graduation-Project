 For many low-resource languages, spoken language resources are more likely to come with translations than with transcriptions. Most of the world X  X  lan-guages are not written, so there is no orthography for transcription. Phonetic transcription is possible but too costly to produce at scale. Even when a mi-nority language has an o ffi cial orthography, people are often only literate in the language of formal ed-ucation, such as the national language. Neverthe-less, it is relatively easy to provide written or spoken translations for audio sources. Subtitled or dubbed movies are a widespread example.

One application of models of bilingual speech data is documentation of endangered languages. Since most speakers are bilingual in a higher-resource language, they can listen to a source lan-guage recording sentence by sentence and provide a spoken translation (Bird, 2010; Bird et al., 2014). By aligning this data at the word level, we hope to automatically identify regions of data where further evidence is needed, leading to a substantial, inter-pretable record of the language that can be studied even if the language falls out of use (Abney and Bird, 2010; Bird and Chiang, 2012).

We experiment with extensions of the neural, at-tentional model of Bahdanau et al. (2015), work-ing at the phone level or directly on the speech sig-nal. We assume that the target language is a high-resource language such as English that can be auto-matically transcribed; therefore, in our experiments, the target side is text rather than the output of an au-tomatic speech recognition (ASR) system.

In the first set of experiments, as a stepping stone to direct modeling of speech, we represent the source as a sequence of phones. For phone-to-word alignment, we obtain improvements of 9 X 24% absolute F1 over several baselines (Och and Ney, 2000; Neubig et al., 2011; Stahlberg et al., 2012). For phone-to-word translation, we use our model to rerank n -best lists from Moses (Koehn et al., 2007) and observe improvements in BLEU of 0.9 X 1.7.
In the second set of experiments, we operate di-rectly on the speech signal, represented as a se-quence of Perceptual Linear Prediction (PLP) vec-tors (Hermansky, 1990). Without using transcrip-tions or a lexicon, the model is able to align the source-language speech to its English translations nearly as well as GIZA ++ using gold transcriptions.
Our main contributions are: (i) proposing a new task, alignment of speech with text transla-tions, including a dataset extending the Spanish Fisher and CALLHOME datasets; (ii) extending the neural, attentional model to outperform exist-ing models at both alignment and translation rerank-ing when working on source-language phones; and (iii) demonstrating the feasibility of alignment di-rectly on source-language speech. To our knowledge, there has been relatively little re-search on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to cre-ate. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations.

Such data can be found, for example, in subti-tled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parlia-ment Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 o ffi cial EU languages, as well as their inter-pretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages.

As mentioned in the introduction, a stepping-stone to model parallel speech is to assume a rec-ognizer that can produce a phonetic transcription of the source language, then to model the transfor-mation from transcription to translation. We com-pare against three previous models that can oper-ate on sequences of phones. The first is simply to run GIZA ++ (IBM Model 4) on a phonetic tran-scription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modifica-tion of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bah-danau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequence of vectors, then decodes it to generate the output. At each step, it  X  X ttends X  to di ff erent parts of the encoded sequence. This model has been used for translation, image cap-tion generation, and speech recognition (Luong et al., 2015; Xu et al., 2015; Chorowski et al., 2014; Chorowski et al., 2015). Here, we briefly describe the basic attentional model, following Bahdanau et al. (2015), review the extensions for encoding struc-tural biases (Cohn et al., 2016), and then present our novel means for adapting the approach handle paral-lel speech. 3.1 Base attentional model The model is shown in Figure 1. The speech signal is represented as a sequence of vectors S 1 , S 2 ,..., S m . For the first set of experiments, each S i is a 128-dimensional vector-space embedding of a phone. For the second set of experiments, each S i is the 39-dimensional PLP vector of a single frame of the speech signal. Our model has two main parts: an en-coder and a decoder. For the encoder, we used a bidi-rectional recurrent neural network (RNN) with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997); we also tried Gated Recurrent Units (Pezeshki, 2015), with similar results. The source speech signal is encoded as sequence of vec-tors H S = ( H 1 (1  X  j  X  m ) is the concatenation of the hidden states of the forward and backward LSTMs at time j .
The attention mechanism is added to the model through an alignment matrix  X   X  R n  X  m , where n is the number of target words. We add &lt;s&gt; and &lt;/s&gt; to mark the start and end of the target sentence. The row  X  i  X  R m shows where the model should at-tend to when generating target word w i . Note that P when generating w i is c i = The decoder is another RNN with LSTM units. At each time step, the decoder LSTM receives c i in addition to the previously-output word. Thus, the hidden state 1 at time i of the decoder is defined as word w i : where g is an a ffi ne transformation. We use 128 di-mensions for the hidden states and memory cells in both the source and target LSTMs.

We train this model using stochastic gradient de-scent (SGD) on the negative log-likelihood for 100 epochs. The gradients are rescaled if their L2 norm is greater than 5. We tried Adagrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), and SGD with momentum (Attoh-Okine, 1999), but found that simple SGD performs best. We implemented dropout (Srivastava et al., 2014) and the local atten-tional model (Luong et al., 2015), but did not ob-serve any significant improvements. 3.2 Structural bias components As we are primarily interested in learning accurate alignments (roughly, attention), we include the mod-elling extensions of Cohn et al. (2016) for incorpo-rating structural biases from word-based translation models into the neural attentional model. As shown later, we observe that including these components result in a substantial improvement in measured alignment quality. We now give a brief overview of these components.
 Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding H S and the previous hidden where Attend is a function that outputs m attention coe ffi cients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over di ff erent positions i . Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment,  X  i  X  1 2. sum of previous alignments, 3. source index vector, (1 , 2 , 3 ,..., m ); and 4. target index vector ( i , i , i ,..., i ). These features are concatenated to form a feature calculation, i.e.,  X  i = Attend( H i  X  1 Coverage penalty. The sum over previous align-ments feature, described above provides a basic fer-tility mechanism, however as it operates locally it is only partially e ff ective. To address this, Cohn et al. (2016) propose a global regularisation method for implementing fertility.
  X  i is normalized, such that nothing in the model requires that every source el-ement gets used. This is remedied by encouraging the columns of the alignment matrix to also sum to one, that is, larization penalty,  X  jective function where  X  controls the regularization strength. We tune  X  on the development set and found that  X  = 0 . 05 gives the best performance. We can easily apply the attentional model to paral-lel data, where the source side is represented as a sequence of phones. In cases where no annotated data or lexicon are available, we expect it is di ffi cult to obtain phonetic transcriptions. Instead, we would like to work directly with the speech signal. How-ever, dealing with the speech signal is significantly di ff erent than the phone representation, and so we need to modify the base attentional model. 4.1 Stacked and pyramidal RNNs Both the encoder and decoder can be made more powerful by stacking several layers of LSTMs (Sutskever et al., 2014). For the first set of experiments below, we stack 4 layers of LSTMs on the target side; further layers did not improve perfor-mance on the development set.

For the second set of experiments, we work di-rectly with the speech signal as a sequence of PLP vectors, one per frame. Since the frames begin at 10 millisecond intervals, the sequence can be very long. This makes the model slow to train; in our experiments, it seems not to converge at all. Fol-lowing Chan et al. (2016), we use RNNs stacked into a pyramidal structure to reduce the size of the source speech representation. As illustrated in Fig-ure 2, we stack 3 layers of bidirectional LSTMs. The first layer is the same as the encoder H S described in Figure 1. The second layer uses every fourth output of the first layer as its input. The third layer selects every other output of the second layer as its input. The attention mechanism is applied only to the top layer. This reduces the size of the alignment ma-trix by a factor of eight, giving rise to vectors at the top layer representing 80ms intervals, which roughly correspond in duration to input phones. 4.2 Alignment smoothing In most bitexts, source and target sentences have roughly the same length. However, for our task of aligning text and speech where the speech is rep-resented as a sequence of phones or PLP vectors, the source can easily be several times larger than the target. Therefore we expect that a target word will commonly align to a run of several source elements. We want to encourage this behavior by smoothing the alignment matrix.

The easiest way to do this is by post-processing the alignment matrix. We train the model as usual, and then modify the learned alignment matrix  X  by  X  is only used for generating hard alignments in our alignment evaluation experiments. We can smooth further by changing the computation of  X  i j during training. We flatten the softmax by adding a temper-ature factor, T  X  1: Note that when T = 1 we recover the standard soft-max function; we set T = 10 in both experiments. We work on the Spanish CALLHOME Corpus (LDC96S35), which consists of telephone conversa-tions between Spanish native speakers based in the US and their relatives abroad. While Spanish is not a low-resource language, we pretend that it is by not using any Spanish ASR or resources like transcribed speech or pronunciation lexicons (except in the con-struction of the  X  X ilver X  standard for evaluation, de-scribed below). We also use the English translations produced by Post et al. (2013).
We treat the Spanish speech as a sequence of 39-dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced  X  X ilver X  standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA ++ with the gdfa sym-metrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce  X  X ilver X  standard alignments between the Spanish speech and the English words.

Cleaning and splitting the data based on dialogue turns, resulted in a set of 17,532 Spanish utterances from which we selected 250 for development and 500 testing. For each utterance we have the corre-sponding English translation, and for each word in the translation we have the corresponding span of Spanish speech.

The forced aligner produces the phonetic se-quences that correspond to each utterance, which we use later in our first set of experiments as an interme-diate representation for the Spanish speech.
In order to evaluate an automatic alignment be-tween the Spanish speech and English translation against the  X  X ilver X  standard alignment, we compute alignment precision, recall, and F1-score as usual, but on links between Spanish PLP vectors and En-glish words. In our first set of experiments, we represent the source Spanish speech as a sequence of phones. This sets an upper bound for our later experiments work-ing directly on speech. 6.1 Alignment We compare our model against three baselines: GIZA ++ , Model 3P, and pialign. For pialign, in order to better accommodate the di ff erent phrase lengths of the two alignment sides, we modified the model to allow di ff erent parame-ters for the Poisson distributions for the average phrase length, as well as di ff erent null align-Model F-score  X  GIZA ++ 29 . 7  X  13 . 0 Model 3P 31 . 2  X  11 . 5 Pialign (default) 42 . 4  X  0 . 3 Pialign (modified) 44 . 0 + 1 . 3
Base model 42 . 7 + 0 + alignment features 46 . 2 + 3 . 5 + coverage penalty 48 . 6 + 5 . 9 + stacking 46 . 3 + 3 . 6 + alignment smoothing 47 . 3 + 4 . 6 + alignment / softmax smoothing 48 . 2 + 5 . 5 All modifications 53 . 6 + 10 . 9 settings -maxsentlen 200 -maxphraselen 20 -avgphraselenF 10 -nullprobF 0.001 , improving performance by 1.6% compared with the default setting. For Model 3P, we used the settings -maxFertility 15 -maxWordLength 20 , unrestricted max[Src/Trg]SenLen and 10 Model3Iterations . We chose the iteration with the highest score to report as the baseline.
The attentional model produces a soft alignment matrix, whose entries  X  i j indicate p ( s j | w i ) of align-ing source phone s j to target word w i . For evalua-tion, we need to convert this to a hard alignment that we can compare against the  X  X ilver X  standard. Since each word is likely to align with several phones, we choose a simple decoding algorithm: for each phone s j , pick the word w i that maximizes p ( w i | s j where this probability is calculated from alignment matrix  X  using Bayes X  Rule.

Table 1 shows the results of the alignment exper-iment. The base attentional model achieved an F-score of 42.7%, which is much better than GIZA ++ and Model 3P (by 13% and 11.5% absolute, re-spectively) and at roughly the same level as pialign. Adding our various modifications one at a time yields improvements ranging from 3.5% to 5.9%. Combining all of them yields a net improvement of 10.9% over the base model, which is 9.4% better than the modified pialign, 22.4% better than Model 3P, and 23.9% better than GIZA ++ . 6.2 Translation In this section, we evaluate our model on the trans-lation task. We compare the model against the Moses phrase-based translation system (Koehn et al., 2007), applied to phoneme sequences. We also provide baseline results for Moses applied to word sequences, to serve as an upper bound. Since Moses requires word alignments as input, we used various alignment models: GIZA ++ , pialign, and pialign with our modifications. Table 2 shows that transla-tion performance roughly correlates with alignment quality.

For the attentional model, we used all of the modi-fications described above except alignment smooth-ing. We also used more dimensions (256) for hid-den states and memory cells in both encoder and de-coder. The decoding algorithm starts with the sym-bol &lt;s&gt; and uses beam search to generate the next word. The generation process stops when we reach the symbol &lt;/s&gt; . We use a beam size of 5, as larger beam sizes make the decoder slower without sub-stantial performance benefits.

As shown in Table 2, the attentional model achieved a BLEU score of 14.6 on the test data, whereas the Moses baselines achieve much better BLEU scores, from 18.2 to 20.2. We think this is because the attentional model is powerful, but we don X  X  have enough data to train it fully given that the output space is the size of the vocabulary. More-over, this attentional model has been configured to optimize the alignment quality rather than transla-tion quality.

We then tried using the attentional model to rerank 100-best lists output by Moses. The model gives a score for generating the next word p ( w i | w 1  X  X  X  w i  X  1 , H S ) as in equation (1). We simply compute the score of a hypothesis by averaging the negative log probabilities of the output words, score( w 1  X  X  X  w n ) =  X  and then choosing the best scoring hypothesis. Ta-ble 2 shows the result using the attentional model as the reranker on top of Moses, giving improve-ments of 0.9 to 1.7 BLEU over their corresponding baselines. These consistent improvements suggest that the probability estimation part of the attentional model is good, but perhaps the search is not ade-quate. Further research is needed to improve the at-tentional model X  X  translation quality. Another possi-bility, which we leave for future work, is to include the attentional model score as a feature in Moses.
Table 3 shows some example translations com-paring di ff erent models. In all examples, it appears that using pialign produced better translations than GIZA ++ . Using the attentional model as a reranker for pialign further corrects some errors. Using the attentional model alone seems to perform the worst, which is evident in the third example where the at-tentional model simply repeats a text fragment (al-though all models do poorly here). Despite the of-ten incoherent output, the attentional model still cap-tures the main keywords used in the translation.
We test this hypothesis by applying the atten-tional model for a cross-lingual keyword spotting task where the input is the English keyword and the outputs are all Spanish sentences (represented as phones) containing a likely translation of the key-word. From the training data we select the top 200 terms as the keyword based on tf.idf. The relevance judgment is based on exact word matching. The attentional model achieved 35.8% precision, 43.3% recall and 36.0% F-score on average on 200 queries. Table 4 shows the English translations of retrieved Spanish sentences. In the first example, the atten-tional model identifies ma  X nana as the translation of tomorrow . In the second example, it does reason-ably well by retrieving 2 correct sentences out of 3, correctly identifying dejamos and salgo as the trans-lation of leave . In this section, we represent the source Spanish speech as a sequence of 39 dimensional PLP vec-tors. The frame length is 25ms, and overlapping frames are computed every 10ms. As mentioned in Section 4.1, we used a pyramidal RNN to reduce the speech representation size. Other than that, the model used here is identical to the first set of exper-iments.

Using this model directly for translation from speech does not yield useful output, as is to be ex-pected from the small training data, noisy speech data, and an out-of-domain language model. How-ever, we are able to produce useful results for the ASR and alignment tasks, as presented below. 7.1 ASR Evaluation To illustrate the utility of our approach to modelling speech input, first, we evaluate on the more common ASR task of phone recognition. This can be consid-ered as a sub-problem of translation, and moreover, this allows us to benchmark our approach against the state-of-the-art in phone recognition. We exper-imented on the TIMIT dataset. Following conven-tion, we removed all the SA sentences, evaluated on the 24 speaker core test set and used the 50 aux-iliary speaker development set for early stopping. The model was trained to recognize 48 phonemes and was mapped to 39 phonemes for testing. We ex-tracted 39 dimensional PLP features from the TIMIT dataset and trained the same model without any modification. Table 5 shows the performance of our model. It performs reasonably well compared with the state-of-the-art (Graves et al., 2013), con-sidering that we didn X  X  tune any hyper-parameters or feature representations for the task. Moreover, our model is not designed for the monotonic con-straints inherent to the ASR problem, which pro-cess the input without reordering. By simply adding a masking function (equation 2 from Chorowski et al. (2014)) to encourage the monotonic constraint in the alignment function, we observe a 2% PER improvement. This is close to the performance re-ported by Chorowski et al. (2014) (Table 5), despite the fact that they employed user-adapted speech fea-tures. 7.2 Alignment Evaluation We use alignment as a second evaluation, training and testing on parallel data comprising paired Span-ish speech input with its English translations (as de-scribed in  X 5), and using the speech-based mod-elling techniques (see  X 4.) We compare to a naive baseline where we assume that each English letter (not including spaces) corresponds to an equal num-ber of Spanish frames. The results of our atten-tional model and the baseline are summarized in Ta-ble 6. The attentional model is substantially lower than the scores in Table 1, because the PLP vector representation is much less informative than the gold phonetic transcription. Here, we have to identify phones and their boundaries in addition to phone-word alignment. However, the naive baseline does surprisingly well, presumably because our (unreal-istic) choice of Spanish-English does not have very much reordering.

Figure 3 presents some examples of Spanish speech and English text, showing a heat map of the alignment matrix  X  (before smoothing). Due to the pyramidal structure of the encoder, each col-umn roughly corresponds to 80ms. In the example on the left, the model is confident at aligning a lit-tle with columns 1 X 5, which corresponds roughly to their correct Spanish translation algo . We misalign the word of with columns 8 X 10, when the correct alignment should be columns 5 X 6, corresponding to Spanish translation de . The word knowledge is aligned quite well with columns 7 X 10, correspond-ing to Spanish conocimiento . The example on the right is for a longer sentence. The model is less confident about this example, mostly because there are words that appear infrequently, such as the per-sonal name Irma . However, we are still observing diagonal-like alignments that are roughly correct. In both examples, the model correctly leaves silence ( sil ) unaligned.

As a middle ground between assuming gold pho-netic transcriptions (cf. Section 6) and no transcrip-tions at all, we use noisy transcriptions by running speech recognizers for other languages on the Span-ish speech: Russian (ru), Hungarian (hu) and Czech (cz) (Vasquez et al., 2012). These distantly related languages were chosen to be a better approximation to the low-resource scenario. All three models per-form better than operating directly on the speech signal (Table 6), and notably, the Russian result is nearly as good as GIZA ++  X  X  performance on gold phonetic transcriptions. This paper reports our work to train models directly on parallel speech, i.e. source-language speech with English text translations that, in the low-resource setting, would have originated from spoken trans-lations. To our knowledge, it is the first exploration of this type. We augmented the Spanish Fisher and CALLHOME datasets and extended the alignment F1 evaluation metric for this setting. We extended the attentional model of Bahdanau et al. to work on parallel speech and observed improvements rela-tive to all baselines on phone-to-word alignment. On speech-to-word alignment, our model, without using any knowledge of Spanish, performs almost as well as GIZA ++ using gold Spanish transcriptions.
Language pairs with word-order divergences and other divergences will of course be more challenging than Spanish-English. This work provides a proof-of-concept that we hope will spur future work to-wards solving this important problem in a true low-resource language.
 This work was partly conducted during Duong X  X  in-ternship at ICSI, UC Berkeley and partially sup-ported by the University of Melbourne and National ICT Australia (NICTA). We are grateful for sup-port from NSF Award 1464553 and the DARPA LORELEI Program. Cohn is the recipient of an Australian Research Council Future Fellowship FT130101105.

