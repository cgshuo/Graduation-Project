 Probabilistic matrix factorization (PMF) and other popular approaches to collaborative filtering assume that the ratings given by users for products are genuine, and hence they give equal importance to all available ratings. However, this is not always true due to several reasons including the pres-ence of opinion spam in product reviews. In this paper, the possibility of performing collaborative filtering while attach-ing weights or quality scores to the ratings is explored. The quality scores, which are dete rmined from the corresponding review data are used to  X  X p X  X eight X  or  X  X own X  X eight X  the importance given to the individual rating while performing collaborative filtering, thereby improving the accuracy of the predictions. First, the measure used to capture the quality of the ratings is described. Different approaches for estimating the quality score based on the available review information are examined. Subsequently, a mathematical formulation to incorporate quality scores as weights for the ratings in the basic PMF framework is derived . Experimental evaluation on two product categories of a benchmark data set from Amazon.com demonstrates the efficacy of our approach. H.2.8 [ Information Systems ]: Database Applications -Data Mining; I.2.7 [ Computing Methodologies ]: Nat-ural Language Processing -Text Analysis Recommender Systems, Probabilistic Matrix Factorization, Collaborative Filtering, Review Quality
Collaborative filtering (CF), popularly used in recommen-dation systems, involves the task of predicting the missing  X 
The first and the second authors have contributed equally to the paper.
 scores or ratings in a user-item matrix by collecting prefer-ence information from similar users and/or items. The un-derlying assumption of the CF approach is that those who have agreed in the past, tend to agree again in the future. Such systems are widely deployed in various domains in-cluding movie/music recommendation (Pandora 1 ,Netflix 2 ) and product recommendation by several online retailers like Amazon.com 3 and eBay.com 4 .

Probabilistic matrix factorization (PMF) [25], which is one of the popular approaches to collaborative filtering in-fers latent factors of both users and items and estimates ratings based on the interaction of user and item factors. PMF assumes that the ratings given by users for products are genuine, hence it gives equal importance to all available ratings. However, this assumption does not always hold due to several reasons. Often, users unhappy with a seller tend to give a poor rating for the product, which does not nec-essarily reflect on the quality of the product. Other times, some sellers might deliberately give superior ratings to pro-mote their products, or they might give unjust poor ratings to competitors X  products. The presence of such spurious ratings could impact the performance of the underlying col-laborative filtering technique [19].

In this paper, the possibility of performing collaborative filtering while attaching quality scores to the ratings is ex-plored. Most online retailers allow users to provide reviews of products in natural language text. Further, some web-sites also allow users to provide feedback on how useful the reviews have been. We believe that it is possible to assess the quality of a review/rating using this additional informa-tion. The quality score can then be used to  X  X p X  X eight X  or  X  X own X  X eight X  the importance given to the individual rating in the user-item matrix. As a result, ratings with a lower quality score will have lower impact on the predicted scores, thereby improving the accuracy of the predictions.
Our approach to collaborative filtering using quality scores consists of two stages. The first stage involves estimating the quality scores for individual ratings in the data set. In or-der to quantify the quality of ratings, we use the  X  X eview helpfulness X  score defined by Kim et al. [13], which uses the feedback information provided by the users for individual re-views. This measure provides a reasonable indication of the quality when the amount of feedback is high. However, for more recent reviews that have l ess feedback, this score might http://www.pandora.com https://www.netflix.com http://www.amazon.com http://www.ebay.com not necessarily capture the true quality of the rating. For such reviews, the quality score is estimated using a regres-sion model trained on reviews that have sufficient feedback. A variety of features extracted from the review text as well as user and review metadata information are used to train the regression model. In the second stage, the quality scores estimated from the previous stage are used as weights for the ratings in the probabilistic matrix factorization framework.
The novelty of our approach lies in the integration of qual-ity scores based on product reviews with collaborative fil-tering to improve the performance of recommender systems. On the one hand, there is a large body of work on the anal-ysis of online product reviews, especially in the area of as-sessing the helpfulness of online reviews [13, 7, 16] as well detecting opinion spam [11, 15, 28, 22]. On the other hand, there is also a fair amount of work in the area of collabora-tive filtering for recommender systems [14, 30, 27] using vari-ous approaches including PMF [25] and its Bayesian variant [26]. To the best of our knowledge, this is the first paper that tries to combine online product review helpfulness with collaborative filtering to improve the overall performance of recommender systems. The efficacy of our approach is demonstrated on two product categories from a benchmark data set from Amazon.com.

The rest of the paper is organized as follows. Related work in the area of opinion spam detection and collaborative fil-tering is reviewed in Section 2. In Section 3, the mathemat-ical formulation of our model and the two stage approach to collaborative filtering using quality scores are described. The experimental methodology used to evaluate the perfor-mance of our approach is described in Section 4. Finally, the results of our experiments are discussed in Section 5.
Kim et al. [13] proposed a quantitative measure based on the review feedback information to assess the helpfulness of reviews. They trained a regression model using various fea-tures extracted from the review text and predict the help-fulness score for new reviews. O X  X ahony and Smyth [21] modeled the same problem as a classification task. Rather than predicting a score for helpfulness, they trained a clas-sifier using reputation, content, social, and sentiment based features derived from user and item metadata to classify a review as helpful or unhelpful. Danescu-Niculescu-Mizil et al. [7] studied the correlation of different aspects of review metadata with review helpfulness. The results of their study showed that there is a strong correlation between the signed deviation of the review rating to the average rating of the product. Ghose and Ipeirotis [8] model the helpfulness of reviews as a function of the user subjectivity in the reviews. The user subjectivity is in turn predicted using a classifier trained on reviews that are subjective and objective.
There are several approaches proposed in the literature for opinion spam detection [11, 15, 22, 28]. Jindal and Liu [11] trained a classifier based on user, item, and review meta-data to identify different categories of spam in online re-views. Liu et al. [15] proposed a method to detect low quality reviews in order to improve the quality of opinion summarization. They used expensive human annotation for the task of estimating the ground truth. Ott et al. [22] pro-posed approaches to detect fictitious and imaginative opin-ions that have been deliberately written to sound authentic. Deceptive spam is not easily noticeable by human readers and hence it cannot be identified by user helpfulness votes, which we consider in our work. Ott et al. acquired 400 sam-ples of spam reviews using Amazon Mechanical Turk 5 and 400 reliable genuine reviews from the Trip Advisor website and trained a classifier using n-gram text features and other linguistic metadata.

There are other approaches in literature that identify spam reviews from the perspective of recommender systems [19, 20, 29]. O X  X ahony et al. [20] examined the robustness of various collaborative recommendation techniques in the face of malicious attacks. They derived theoretical results on recommendation accuracy and stability in the presence of malicious agents. Mobasher et al. [19] analyzed various new attack models and their impact on recommendation algo-rithms through extensive simulation-based evaluation. In a more recent work, Wu et al. [29] proposed a semi X  X upervised learning algorithm to identify spam reviews/ shillings using user metadata. The spam revi ews are then removed from the training set while performing collaborative filtering. How-ever, none of these approaches provide a robust methodology to improve the performance of the recommendation systems in the presence of opinion spam.

The current approaches to recommendation systems are usually classified as content-based [2, 3], collaborative [24, 25, 26], and hybrid recommendations [18, 2, 1]. Content based approaches predict the recommendations from user and item profiles derived from cha racteristic features of users and items, such us demographic data and product descrip-tions. An alternative approach to recommendation that is heavily used when rich user and item information is not available is collaborative filtering (CF). CF makes use of past user preferences to make predictions for the future. CF algorithms try to identify similarities between users and items to predict user preferences. There are several memory based and model based approaches to collaborative filtering for recommender systems [27, 14, 25]. The most success-ful methods for CF are the latent factor models based on probabilistic matrix factorization (PMF) [25, 26]. The PMF model is described in more detail in Section 3. Yifan, Koren, and Volinsky [10] proposed an approach that uses implicit feedback about users likes and dislikes(as produced in signed networks), to assign weights for the raw ratings obtained.
The two X  X tage model for performing collaborative filtering withqualityscoresisproposedhere. In Stage 1 , the quality scores of ratings using the review and user data are esti-mated. In Stage 2 , these quality scores are used as weights assigned to ratings and weighted probabilistic matrix factor-ization is performed on the ratings to predict new recom-mendations.
 The following notation is used in the rest of the paper. There are n users and m items in the system. The users are indexed by i  X  X  1 , 2 ,...n } and items by j  X  X  1 , 2 ,...,m } The user-item rating matrix is represented as Y  X  R n  X  m where y ij represents the rating given by the i th user to item. Given the sparsely populated matrix Y ,thetaskof CF is to estimate the missing entries of Y . To perform CF, the probabilistic matrix factorization approach is used, in which the rating matrix Y , is approximated as a product of https://www.mturk.com http://www.tripadvisor.com Figure 1: Graphical model for the two stage approach to recommender systems two low rank matrices, U  X  R n  X  d and V T  X  R m  X  d ,which represent the user and item latent factors respectively. The latent factor vector of user i is denoted as u i and that of item j as v j . Independent Gaussian priors are used for user and item latent factors. With each rating, y ij , a quality score, w ij is associated, which is estimated from the corresponding review. The user and item meta data and review based features, which are used in the estimation of quality scores are represented by a i , b j and c ij respectively. The standard deviations associated with the user and item latent factors are given by  X  U and  X  V respectively. Finally, The standard deviation associated with the model for ratings is given by  X  . The graphical model for our approach is given in Figure 1.
Stage 1 involves estimating the quality score for individ-ual ratings. The quality score for a rating is reflective of the authenticity of the rating. Most websites like Amazon and Yelp allow users to indicate if reviews were helpful or not. The amount of positive feedback obtained by a product review is indicative of the authenticity of the corresponding rating. As a result, any measure that uses the feedback in-formation is useful for measuring the quality of the rating. Kim et al. [13] have proposed the measure below to quantify helpfulness for online product reviews based on the feedback information, which we use as the quality score. The quality score computed as described above is a fair in-dication of quality if the amount of feedback is sufficiently high. However, for more recent reviews that have low feed-back, this score might not necessarily capture the true qual-ity of the rating. For such reviews, the quality score is es-timated using a regression model trained on those reviews that have sufficient feedback.

First, a regression model is t rained using features ex-tracted from those reviews/ratings that have sufficient feed-back. The quality score computed using the formula de-scribed above is used as the response variable in the regres-sion model. For a review/rating that has low user feedback, the quality score is predicted using the trained regression model. Several features were examined for training the re-gression model: 1. Unigram counts or bag-of-words features 2. Features from topic modeling
In Stage 2, the quality scores estimated in Stage 1 are used to build a recommendation system based on collabo-rative filtering. Among the methods used for collaborative filtering, latent factor models have been shown to give the best performance in most scenarios. Thus for our analysis, we adapt the probabilistic matrix factorization framework [25] to incorporate quality scores. This method can be triv-ially extended to other matrix factorization based models.
The PMF model aims at inferring latent factors of users and items from the available ratings. The missing ratings are estimated based on the interaction of user and item latent factors. These factors represent various hidden dimensions of users X  tastes and preferences. The n  X  m matrix of ratings, Y , is approximated as Y = UV T ,where U  X  R n  X  d and V T  X  R m  X  d . The priors for U and V are assigned as follows: P ( U |  X  2 where, N ( x |  X ,  X  2 ) represents the Gaussian distribution with mean  X  and variance  X  2 evaluated at x . For vector valued vari-ables, N ( x |  X ,  X ) represents the multivariate Gaussian dis-tribution with mean  X  and variance  X  evaluated at x .Also,
U and latent factors respectively.

In a traditional PMF setting, the rating matrix Y is mod-eled as: where I ij is the indicator variable indicating if the rating y ij is available and  X  2 Y is the variance associated with the model for the ratings.

We now modify the existing collaborative filtering frame-work to weight the ratings using the quality scores estimated in Stage 1. The quality score is modeled as a factor which in-versely affects the variance of the prediction from the mean of the factor model. The intuition is that higher quality rat-ings are given a prior with lower deviations from the model and thus their deviations from the model mean are more heavily penalized. On the other hand, low quality scores are allowed larger deviations from the model mean. We keep the same priors for U and V ,asinEquation3. Ournew prior on Y is given by: where I ij is the indicator variable indicating if the rating is available.

Maximizing log posterior of observed Y in this model leads to the minimization objective given below, which follows an intuitive interpretation of minimizing the weighted squared error of the observed ratings, regularized appropriately:
L (  X  )= where  X  1 =  X  2 Y / X  2 U and  X  2 =  X  2 Y / X  2 V are the regularization parameters and  X  F is the Frobenius norm of a matrix. In our experiments we take  X  1 =  X  2 =  X  .Theaboveobjective is a differentiable function in u i and v j and the maximiza-tion can be performed using the Stochastic Gradient Descent (SGD) algorithm.
 Total Users 772674 46491 Total Items 493991 29477 No. Ratings in Training 1677892 75518 No. Ratings in Validation 41081 902 No. Ratings in Test 105285 2683 Table 1: Various statistics about the data sets used in ex-perimental evaluation.
Amazon.com is an online retailer that sells products like books, movies, furniture etc. Amazon widely employs rec-ommender systems to recommend products to users based on the user X  X  purchase and rating history. In our work, we use the open source data set provided by Jindal and Liu [11]. The categories of Books and Audio CDs were used for experimental evaluation as they have a reasonable number of users, products, and reviews. The other categories had a very small number of reviews, and hence were not used in our experiments. Each data set consists of three types of information: 1. Review information consisting of user ID, product ID, 2. User information consisting of explicit user metadata 3. Product information consisting of product specific de-
Our first preprocessing task was to eliminate multiple re-views for a single user-item pair. This could be due to errors in data transmission on the internet due to which the same review might have appeared multiple times, thereby result-ing in duplicate reviews. Other times, it could just be that the user X  X  opinion of a product might have changed over time. In the event that multiple such reviews were present, the latest one was retained and the rest were discarded. Sec-ondly, the entire set of ratings was split into training, vali-dation, and test sets for the recommendation system based on time. The reviews in validation appeared after the ones in training and the reviews in test were posted later than those in validation. Finally, to avoid cold start scenarios, those entries from validation and test set for which either the user or the item was not seen in the training set were removed. Table 1 gives details about the two data sets used in experimental evaluation.
Regression models were trained for predicting quality scores using three different sets of features  X  text, metadata, text and metadata. For the Books data set, all those reviews in the training set that had feedback from more than 50 users were used for training the regression model. Since the Figure 2: Histogram of words in the reviews in the Books data set. training set for Audio CDs had reviews with fewer users pro-viding feedback, reviews that had feedback from more than 20 users were used for training the regression model. The methodology used to extract different types of features for training the regression model is described below: http://www.speech.sri.com/projects/srilm http://www.cs.princeton.edu/ blei/lda-c/
Regression models were trained using the techniques de-scribed below: For the baseline estimate, the default implementation of PMF in Graphlab [17], which uses alternating least squares method to perform factorization was used. The number of latent factors was set to D = 40. Grid search was performed to tune the regularization parameter  X  using the validation data set over the range of 0 . 01 to 0 . 8. Note that the weighted PMF approach defined in Section 3.2 has the same objective as the one formulated in [10], though the latter derives the model for a system that has implicit feedback about users Model Books Audio CDs LR-metadata 0.24 0.25 LR-metadata+ LDA 5 0.23 0.24 LR-metadata+ LDA 10 0.23 0.23 LR-metadata+ LDA 50 -0.24 LASSO-metadata 0.30 0.25 LASSO-metadata+ LDA 5 0.26 0.25 LASSO-metadata+ LDA 10 0.26 0.23 LASSO-metadata+ LDA 50 -0.26 SVR-bag-of-words 0.27 0.35 SVR-metadata+bag-of-words 0.24 0.27 Table 2: RMSE for 10-fold cross validation on the train-ing set for different regression models in Stage 1.  X  X R X ,  X  X ASSO X , and  X  X VR X  refer to the models in which quality scores are predicted using logistic regression, LASSO regres-sion, and support vector regression respectively. likes and dislikes for certain items. The implementation of the latter objective in Graphlab was used to build our new recommendation system. The running time of this imple-mentation is the same as that of the default implementation of PMF in Graphlab.
 For the first baseline model, which we refer to as  X  X anilla PMF X , probabilistic matrix factorization was performed by setting weights for all reviews to 1. For the second baseline, which we refer to as  X  X econd baseline X , quality scores were estimated from the feedback votes for those reviews that had sufficient feedback (50 or more for Books and 20 or more for Audio CDs). For the remaining reviews, the weights were set to the average of the scores that were estimated using feed-back information in the previous step and weighted proba-bilistic matrix factorization was performed. Initial experi-ments with these two models showed marginal improvement in the performance of latter over former. This observation supported the hypothesis that incorporating quality scores might improve the performance of the recommender system. 10-fold cross validation was performed on the training set and root mean square error (RMSE) was computed to mea-sure the performance of regression models. Table 2 shows the cross validation RMSE sco res for different regression models. Note that due to computational complexity, LDA 50 did not run till completion on the Books data set. Hence, results for LDA 50 on the Books data set are not reported. Logistic regression models trained on both LDA and meta-data based features perform the best on both data sets. In general, the performance of LASSO and logistic regression models trained only on LDA features is inferior to that of the models trained on both metadata and LDA features. Hence, we do not report results for LASSO and logistic regression models trained only on LDA features. The performance of logistic regression models trained only on metadata features is only slightly inferior to that of the best performing mod-els, thereby indicating that the improvements obtained by adding LDA features are not be substantial. For a given set of features, logistic regression generally performs better than LASSO regression. SVR models trained on bag-of-words fea-tures generally perform poorly when compared to the other models. Here again, the performance of the SVR model trained only on bag-of-words features is substantially infe-rior to that of the model trained on both metadata and bag-of-words features. These results indicate that there might be a stronger signal in metadata based features for predict-ing the quality of reviews when compared to the current set of text features considered.
Table 3 shows the RMSE for different models on the test set for both Books and Audio CDs. On the Books data set, all models including the second baseline outperform the vanilla PMF, while on Audio CDs data set, a majority of the models outperform the vanilla PMF. Lack of sufficient data in the Audio CDs data set could possibly be the reason for inferior performance of some models. Due to lack of reviews with sufficient feedback, less reliable reviews were used to train the regression models in Stage 1. As a result, the accuracy of the scores predicted by Stage 1 could be inferior, thereby impacting the overall quality of the predictions on the Audio CDs data set.

Logistic regression model trained on metadata features is the best performing model on the Books data set and it results in an improvement of 0 . 0355 (2 . 49%) over vanilla PMF and 0 . 0344 (2 . 41%) over the second baseline. However, on the Audio CDs data set, SVR trained on metadata and bag-words features is the best performing model, with a per-formance improvement of 0 . 0175 (1 . 27%) over vanilla PMF and 0 . 0128 ( . 93%) over the second baseline. In general, mod-els trained only on metadata based features perform better than those trained on both LDA and metadata based fea-tures indicating strong signals from the metadata features used  X  time stamp, length of review text, length of review title, rank of the user and deviation of the rating from the mean rating of the product. Even though models trained with text based features are outperformed by the metadata based models, they still show significant improvement over the baseline models on both data sets, which warrants fur-ther investigation into linguistic feature engineering on re-view text. Overall, our results indicate that incorporating quality scores as weights for ratings in collaborative filtering improves the performance of recommender systems.

In our last experiment, the impact of using ratings with low quality scores in training the PMF was studied. Our hypothesis was that ratings with poor quality scores could possibly affect the predictions adversely, and hence elimi-nating them during training might further improve the per-formance of recommender systems. Figures 3 shows the dis-tribution of quality scores from LR-metadata, which is the best performing model on the Books data set. While most of ratings have reasonably high quality scores, a small num-ber of them have fairly poor quality scores. Analysis of the distribution of quality scores from SVR-metadata+bag-of-words, the best performing model on the Audio CDs data set yielded similar results. In our experiment, all ratings with a quality score less than 0.4 were eliminated and PMF was performed with the remaining ratings using the best per-forming model on both data sets. The results from these ex-periments, which we call  X  X est-Model-Low-Quality-Scores-Dropped X  are shown in Table 3. Eliminating low quality scores improved the results on the Audio CDs data set con-siderably, thereby supporting our original hypothesis. How-ever, the performance on the Books data set dropped marginally. Figure 3: Distribution of qualit y scores from LR-metadata, the best performing model on the Books data set.
Regression models were analyzed to identify features that impacted the quality of the rating. First, coefficients learned from LASSO and logistic regression on metadata features were examined on the two data sets. Both the regression models had learned similar coefficients for individual meta-data features. On both data sets, the review length had the highest positive coefficient, while the deviation of the ratings from the mean rating had the highest negative co-efficient from both models. These observations can be intu-itively explained as longer reviews are indicators of a thor-ough analysis of the product by the reviewer and hence the reviewer X  X  rating is highly reliable. On the other hand, a re-viewer giving a rating that is highly deviant from the mean rating is likely to be a spammer with a malicious intention of either boosting or degrading a product popularity and hence the negative correlation with the quality. The other features like time stamp and review title length were found to be not very influential in estimating the quality as both regression models assigned low or near-zero coefficients to these features.

Next, regression coefficients learned using LASSO and lo-gistic regression on the topics induced by LDA were exam-ined on the Books data set. Logistic regression assigned more or less the same weights for all topics induced by LDA. However, LASSO regression was able to assign differ-ent weights to different topics induced by LDA. There were two topics induced by LDA 5 that had negative coefficients. Some of the words from the former topic included infor-mation , good , great , excellent , guide , books , while the words from the latter topic included history , book , war , world , peo-ple , american . The remaining topics had low or near zero coefficients indicating that they did not play a significant role in determining the quality of the rating. While the words in the former topic indicate strong opinions which could be used to mask the real quality of the products, the words in the latter topic mostly describe different categories of books, which might not necessarily describe the quality of the product. In general, we found that LDA was more inclined to clustering thematic topics together rather than topics that were indicative of quality. This inability to dis-tinguish thematic words from quality indicators is possibly one of the reasons for the modest performance of LDA-based features in our experiments. Our analysis of topics induced by
LDA 10 yielded similar results on the Books data set. Fur-ther analysis of words induced by LDA on the Audio CDs data set did not yield any interesting observations. Over-all, these results emphasize the need for extraction of more sophisticated features from the review text.

In summary, incorporating qu ality scores or weights to ratings improves the performance of collaborative filtering in recommender systems. Our experiments with different types of features extracted from both review metadata and text indicate that some of the metadata-based features are highly indicative of the quality of the rating. Further, our exper-iments with text-based features also demonstrate promise, but also indicate the need for extraction of more sophis-ticated features from review text. Overall, we find that our two stage approach to collaborative filtering is a robust method that is capable of overcoming the negative effects caused by spurious reviews and ratings in recommender sys-tems.
Future work includes incorporating several additional fea-tures including bigrams and semantic features as described in Kim et al. [13] for learning the regression model to pre-dict quality scores. We would also like to explore measures proposed by Ghose and Ipeirotis [8] for assessing the qual-ity of reviews. The other direction of future work involves exploring feature reduction techniques like PCA to reduce the number of bag-of-words features, which we believe could help improve the performance of the regression model. In our current experiments, LDA could not induce latent word distributions that were reflective of the quality of reviews. To help improve the performance, supervised approaches like supervised LDA [4] can be explored in future. Fur-ther, to overcome the lack of sufficient reviews in data sets like the Audio CDs data set, transfer learning approaches [6] can be incorporated for the estimation of quality scores in our framework. Finally, experimental evaluation of our approach on other data sets like the Yelp academic data set and other product categories available in the Amazon data set will also be considered in the future.
In this paper, a two-stage approach to collaborative filter-ing that incorporates weights or quality scores for ratings is proposed. Several approaches to estimating quality scores using product reviews associated with the ratings are ex-amined. Experimental evaluation of our approach on two product categories from a la rge benchmark data set from Amazon.com demonstrates that the proposed two-stage ap-proach performs better than the vanilla PMF that assigns equal importance to the ratings. To the best of our knowl-edge, this is the first paper that has combined assessing re-view helpfulness with collaborative filtering to improve the overall performance of recommender systems.
 We would like to thank Raymond J. Mooney, Sreangsu Acharyya, and Gautam Muralidhar for their valuable feedback on the paper. This research was f unded by NSF gr ants I SS-1016614 and IIS-1116656.
