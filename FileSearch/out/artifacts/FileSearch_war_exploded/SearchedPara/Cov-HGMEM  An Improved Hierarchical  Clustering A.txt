 Content-Based Image Retrieval (CBIR), sear ching large image repositories according to large mixture of Gaussians into a smaller mixture while still preserving the component structure of the original mode, which is achieved by clustering the components. Without which doesn X  X  reflect the real semantics of the class. Semantic multinomial (SMN) was not only result in too complex computation, but also enlarge the time complexity. In this paper, we propose an improved hierarchical clustering algorithm-Cov-HGMEM. Image retrieval experiments show that our proposed algorithm not only inherit the exceptional computational complexity of HGMEM, but also improves retrieval efficiency and proficiency. 2.1 Image Modeling Figure 1 shows an example of learning a GMM for a given image. Each Gaussian in the model is displayed as a localized colored ellipsoid. 2.2 HGMEM Algorithm and Image Class Modeling Denote i component at child level 1 l + , image analysis, it corresponds to components of image and image class respectively. algorithm iterates between the following steps: Expectation-Step: Compute where 1 l ii  X  and () ,, G stands for Gaussian mixture component. Maximization-Step: Set GMM with only 4 components and show the result in Fig.2 (b). 2.3 Similarity Function Consider a query mixture ()
Px Gx p  X   X  components of query image and database image respectively. ALA [4] is defined where,  X  X  X   X  X  X   X  dimension of the feature space. Go to Equation (1) and co nsider the expression Consider feature space. It is obvious that expression (5) ranges from 0 to 1 , for inaccurate. But Expectation-Step holds only when i  X  goes to infinite. equation [1] (9), a being a constant, we obtain that is, Substitute Equation (7) in post probability equation [1] (9), Expectation-Step becomes We needn X  X  modify Maximization-Step, and we briefly call Equation (8) and (2) Cov-HGMEM iteration steps. influence Maximization-Step. Similar to HGMEM algorithm, we model image class as a simpler GMM using Cov-HGMEM algorithm. As shown in Fig. 2(c), Cov-HGMEM performs better than HGMEM algorithm. For, taking covariance into consideration, components with small covariance will not be absorbed into big ones. The final mixture model reflects the semantic of original class more accurately. To evaluate the efficiency of our propos ed Cov-HGMEM Algorithm in comparison to probability of class match order (PCMO) to compare the efficiencies of two clustering For each picture in the database, we obtain 20 ALA values by matching to each class. 1. When the traverse finished, we obtain average PCMO by dividing CMOC with the number of images in database. 
Fig.3 compares HGMEM and Cov-HGMEM systematically. We plot the HGMEM performs better. With parameters a in Cov-HGMEM, we evaluate it experimentally. We found algorithm has best retrieval performance when 0.001 a = ,. respectively. HGMEM. Compared with HGMEM algorithm, Cov-HGMEM tries to prevent proposed algorithm performs better than HGMEM. This work was supported in part by the National Natural Science Foundation of China (Grant No. 60572078) and by Guangdong Natural Science Foundation (Grant No. 05006349). 
