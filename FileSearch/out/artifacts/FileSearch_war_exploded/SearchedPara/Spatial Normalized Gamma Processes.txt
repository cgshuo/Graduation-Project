 Bayesian nonparametrics have recently garnered much attention in the machine learning and statis-tics communities, due to their elegant treatment of infinite dimensional objects like functions and [1] is a cornerstone of Bayesian nonparametrics, and forms a basic building block for a wide variety of extensions and generalizations, including the infinite hidden Markov model [2], the hierarchical DP [3], the infinite relational model [4], adaptor grammars [5], to name just a few. By itself, the DP is a model that assumes that data are infinitely exchangeable, i.e. the ordering of data items does not matter. This assumption is false in many situations and there has been a concerted collections of dependent random probability measures. [6] expounded on the notion of dependent DPs , that is, a dependent set of random measures that are all marginally DPs. The property of being marginally DP here is both due to a desire to construct mathematically elegant solutions, and also due to the fact that the DP and its implications as a statistical model, e.g. on the behaviour of induced clusterings of data or asymptotic consistency, are well-understood. In this paper, we propose a simple and general framework for the construction of dependent DPs on arbitrary spaces. The idea is based on the fact that just as Dirichlet distributions can be generated by drawing a set of independent gamma variables and normalizing, the DP can be constructed by drawing a sample from a gamma process (  X  P) and normalizing (i.e. it is an example of a normalized random measure [7, 8]). A  X  P is an example of a completely random measure [9]: it has the property that the random over an extended space, associate each DP with a different region of the space, and define each DP by normalizing the restriction of the  X  P on the associated region. This produces a set of dependent DPs, with the amount of overlap among the regions controlling the amount of dependence. We call this model a spatial normalized gamma process (SN  X  P). More generally, our construction can be extended to normalizing restrictions of any completely random measure, and we call the resulting dependent random measures spatial normalized random measures (SNRMs). We describe inference procedures based on Gibbs and Metropolis-Hastings sampling in Section 4 and report experimental results in Section 5. We conclude by discussing limitations and possible extensions of the model as well as related work in Section 6. We briefly describe the gamma process (  X  P) here. A good high-level introduction can be found in [10]. Let ( X  ,  X ) be a measure space on which we would like to define a  X  P. Like the DP, realizations of the  X  P are atomic measures with random weighted point masses. We can visualize the point masses  X   X   X  and their corresponding weights w &gt; 0 as points in a product space  X   X  [0 ,  X  ) . Consider a Poisson process over this product space with mean measure Here  X  is a measure on the space ( X  ,  X ) and is called the base measure of the  X  P. A sample from A sample from the  X  P is then defined as simply gamma distributed with shape parameter  X  ( S ) , thus the natural name gamma process. Divid-ing G by G ( X ) , we get a normalized random measure  X  X  random probability measure. Specifically, we get a sample from the Dirichlet process DP (  X  ) : Here we used an atypical parameterization of the DP in terms of the base measure  X  . The usual Further, the DP is independent of the normalization: D  X  X  X  G ( X ) .
 The gamma process is an example of a completely random measure [9]. This means that for mutually  X  ( d X  ) =  X  ( d X   X  S ) . Secondly, if  X  =  X  1  X   X  2 is a two dimensional space, then the projection G 00 ( d X  1 ) = R In this section we describe our proposal for constructing dependent DPs. Let ( X  ,  X ) be a probability space and T an index space. We wish to construct a set of dependent random measures over ( X  ,  X ) , one D t for each t  X  T such that each D t is marginally DP. Our approach is to define a gamma Because restrictions and projections of gamma processes are also gamma processes, each D t will be DP distributed.
 To this end, let Y be an auxiliary space and for each t  X  T , let Y t  X  Y be a measurable set. For any measure  X  over  X   X  Y define the restricted projection  X  t by Note that  X  t is a measure over  X  for each t  X  T . Now let  X  be a base measure over the product space  X   X  Y and consider a gamma process base measure  X  t : Now normalizing, We call the resulting set of dependent DPs { D t } t  X  T spatial normalized gamma processes (SN  X  Ps). stochastic process . The amount of dependence between D s and D t for s,t  X  T is related to the t are in T , the more overlap Y s and Y t have and as a result D s and D t are more dependent. 3.1 Examples We give two examples of SN  X  Ps, both with index set T = R interpreted as the time line. Generaliza-tions to higher dimensional Euclidean spaces R n are straightforward. Let H be a base distribution over  X  and  X  &gt; 0 be a concentration parameter.
 The first example uses Y = R as well, with the subsets being Y t = [ t  X  L,t + L ] for some fixed valued stochastic process { D t } t  X  R is stationary. The base measure  X  t works out to be: so that each D t  X  DP (  X H ) with concentration parameter  X  and base distribution H . We can interpret this SN  X  P as follows. Each atom in the overall  X  P G has a time-stamp y and a time-span a result, two DPs D s and D t will share more atoms the closer s and t are to each other, and no atoms with increasing | s  X  t | and independent if | s  X  t | &gt; 2 L .
 The second example generalizes the first one by allowing different atoms to have different window lengths. Each atom now has a time-stamp y and a window length l , so that it appears in DPs in the  X  ( d X dydl ) =  X H ( d X  ) dy X  ( dl ) / 2 l . The restricted projection is then so that each D t is again simply DP (  X H ) . Now D s and D t will always be dependent with the amount of dependence decreasing as | s  X  t | increases. 3.2 Interpretation as Mixtures of DPs Even though the SN  X  P as described above defines an uncountably infinite number of DPs, in practice we will only have observations at a finite number of times, say t 1 ,...,t m . We define R as the smallest collection of disjoint regions of Y such that each Y t j is a union of subsets in R . Thus each R  X  X  define G R /G R ( X )  X  DP (  X  R ) , with D R  X  X  X  D R 0 for distinct R,R 0  X  X  . Now Figure 1: The extended space Y  X  L over which the overall  X  P is defined in the second example. Not shown is the  X  -space over which the DPs are defined. Also not shown is the fourth dimension W The regions in R are the six areas formed by various intersections of the triangular areas. so each D t j is a mixture where each component D R is drawn independently from a DP. Further, the mixing proportions are Dirichlet distributed and independent from the components by virtue of each G
R ( X ) being gamma distributed and independent from D R . Thus we have the following equivalent construction for a SN  X  P: Note that the DPs in this construction are all defined only over  X  , and references to the auxiliary space Y and the base measure  X  are only used to define the individual base measures  X  R and the shape parameters of the g R  X  X . Figure 1 shows the regions for the second example corresponding to observations at three times.
 The mixture of DPs construction is related to the hierarchical Dirichlet process defined in [11] (not the one defined by Teh et al [3]). The difference is that the parameters of the prior over the mixing proportions exactly matches the concentration parameters of the individual DPs. A consequence of this is that each mixture D t j is now conveniently also a DP. The mixture of DPs interpretation of the SN  X  P makes sampling from the model, and consequently inference via Markov chain Monte Carlo sampling, easy. In what follows, we describe both Gibbs sampling and Metropolis-Hastings based updates for a hierarchical model in which the dependent now lie in a measurable space ( X ,  X ) equipped with a set of probability measures F  X  parametrized by  X   X   X  . Observation i at time t j is denoted x ji , lies in region r ji and is drawn from mixture component parametrized by  X  ji . Thus to augment (12), we have where r ji = R with probability  X  jR for each R  X  X  j . In words, we first pick a region r ji from the set R j , then a mixture component  X  ji , followed by drawing x ji from the mixture distribution. 4.1 Gibb Sampling We derive a Gibbs sampler for the SN  X  P where the region DPs D R are integrated out and replaced assigned. We also assume that the base distribution H is conjugate to the mixture distributions F  X  so that the cluster parameters are integrated out as well. The Gibbs sampler iteratively resamples the observation x ji conditioned on the other variables currently assigned to cluster c in D R , with its cluster parameters integrated out. We denote marginal counts with dots, for example m  X  Rc is the number of observations (over all times) assigned to cluster c in region R . Superscripts  X  ji means observation x ji is excluded. r ji and c ji are resampled together; their conditional joint probability given the other variables is: The probability of x ji joining a new cluster in region R is where R  X  R j and c denotes the index of an existing cluster in region R . The updates of the g R  X  X  are more complicated as they are coupled and not of standard form: To sample the g R  X  X  we introduce auxiliary variables { A j } to simplify the rightmost term above. In particular, using the Gamma identity we have that (16) is the marginal of { g R } R  X  X  of the distribution: Now we can Gibbs sample the g R  X  X  and A j  X  X : Here J R is the collection of indices j such that R  X  X  j . 4.2 Metropolis-Hastings Proposals To improve convergence and mixing of the Markov chain, we introduce three Metropolis-Hastings (MH) proposals in addition to the Gibbs sampling updates described above. These propose non-incremental changes in the assignment of observations to clusters and regions, allowing the Markov chain to traverse to different modes that are hard to reach using Gibbs sampling.
 The first proposal (Algorithm 1) proceeds like the split-merge proposal of [12]. It either splits an existing cluster in a region into two new clusters in the same region, or merges two existing clusters Gibbs sampling [12].
 the neigbors are the four regions diagonally neighbouring the current one). To improve acceptance probability we also resample the g R  X  X  associated with the current and proposed regions. The move can be invalid if the cluster contains an observation from a time point not associated with the new region; in this case the move is simply rejected.
 The third proposal (Algorithm 3) we considered seeks to combine into one step what would take two steps under the previous two proposals: splitting a cluster and moving it to a new region (or the reverse: moving a cluster into a new region and merging it with a cluster therein). Algorithm 1 Split and Merge in the Same Region (MH1) 1: Let S 0 be the current state of the Markov chain. 2: Pick a region R with probability proportional to m  X  R  X  and two distinct observations in R 3: Construct a launch state S 0 by creating two new clusters, each containing one of the two obser-4: if the two observations belong to the same cluster in S 0 then 5: Propose split: run one last round of restricted Gibbs sampling to reach the proposed state S 1 6: else 7: Propose merge: the proposed state S 1 is the (unique) state merging the two clusters 8: end if Algorithm 2 Move (MH2) 1: Pick a cluster c in region R 0 with probability proportional to m  X  R 0 c 2: Pick a region R 1 neighbouring R 0 and propose moving c to R 1 3: Propose new weights g R 0 , g R 1 by sampling both from (19) 4: Accept or reject the move Algorithm 3 Split/merged Move (MH3) 1: Pick a region R 0 , a cluster c contained in R , and a neighbouring region R 1 with probability 2: if c contains observations than can be moved to R 1 then 3: Propose assigning these observations to a new cluster in R 1 4: else 5: Pick a cluster from those in R 1 and propose merging it into c 6: end if 7: Propose new weights g R 0 ,g R 1 by sampling from (19) 8: Accept or reject the proposal Synthetic data In the first of our experiments, we artificially generated 60 data points at each of 5 times by sampling from a mixture of 10 Gaussians. Each component was assigned a timespan, ranging from a single time to the entire range of five times. We modelled this data as a collection of five DP mixture of Gaussians, with a SN  X  P prior over the five dependent DPs. We used the set-up as described in the second example. To encourage clusters to be shared across times (i.e. to avoid sim-ilar clusters with non-overlapping timespans), we chose the distribution over window lengths  X  ( w ) to give larger probabilities to larger timespans. Even in this simple model, Gibbs sampling alone usually did not converge to a good optimum; remaining stuck around local maxima. Figure 2 shows the evolution of the log-likelihood for 5 different samplers: plain Gibbs sampling, Gibbs sampling augmented with each of MH proposals 1, 2 and 3, and finally a sampler that interleaved all three MH samplers with Gibbs sampling. Not surprisingly, the complete sampler converged fastest, with Gibbs sampling with MH-proposal 2 (Gibbs+MH2) performing nearly as well. Gibbs+MH1 seemed converge no faster than just Gibbs sampling, with Gibbs+MH3 giving performance somewhere in between. The fact that Gibbs+MH2 performs so well can be explained by the easy clustering struc-ture of the problem, so that exploring region assignments of clusters rather than cluster assignments of observations was the challenge faced by the sampler (note its high acceptance rate in Figure 4). To demonstrate how the additional MH proposals help mixing, we examined how the cluster as-signment of observations varied over iterations. At each iteration, we construct a 600 by 600 binary Figure 3, we plot the average L 1 difference between matrices at different iteration lags. Some-what counterintuitively, Gibbs+MH1 does much better than Gibbs sampling with all MH proposals. Figure 2: log-likelihoods (the coloured lines are ordered at iteration 80 like the legend). Figure 3: Dissimilarity in clustering structure vs lag (the coloured lines are ordered like the leg-end).
 This is because the latter is simultaneously exploring the region assignment of clusters as well. In Gibbs+MH1, clusters split and merge frequently since they stay in the same regions, causing the cluster matrix to vary rapidly. In Gibbs+MH1+MH2+MH3, after a split the new clusters often move into separate regions; so it takes longer before they can merge again. Nonetheless, this demonstrates the importance of split/merge proposals like MH1 and MH3; [12] studied this in greater detail. We next examined how well the proposals explore the region assignment of clusters. In particular, at each step of the Markov chain, we pick the cluster with mean closest to the mean of one of the true Gaussian mixture components, and tracked how its timespan evolved. Figure 5 shows that without MH proposal 2, the clusters remain essentially frozen in their initial regions.
 NIPS dataset For our next experiment we modelled the proceedings of the first 13 years of NIPS. The number of word tokens was about 2 million spread over 1740 documents, with about 13000 unique words. We used a model that involves both the SN  X  P (to capture changes in topic distri-butions across the years) and the hierarchical Dirichlet process (HDP) [3] (to capture differences among documents). Each document is modeled using a different DP, with the DPs in year i sharing the same base distribution D i . On top of this, we place a SN  X  P (with structure given by the second over words, and has a particular timespan. Each document in year i is a mixture over the topics whose timespan include year i . Our model allows statistical strength to be shared in a more refined manner than the HDP. Instead of all DPs having the same base distribution, we have 13 dependent base distributions drawn from the SN  X  P. The concentration parameters of our DPs were chosen to encourage shared topics, their magnitude chosen to produce about a 100 topics over the whole cor-pus on average. Figure 6 shows some of the topics identified by the model and their timespans. For inference, we used Gibbs sampling, interleaved with all three MH proposals to update the SN  X  P. the Markov chain was initialized randomly except that all clusters were assigned to the top-most region (spanning the 13 years). We calculated per-word perplexity [3] on test documents (about half of all documents, withheld during training). We obtained an average perplexity of 3023.4, as opposed to about 3046.5 for the HDP. Figure 6: Inferred topics with their timespans (the horizontal lines). In parentheses are the number of words assigned to each topic. On the right are the top ten most probable words in the topics. Computationally, the 3 MH steps are much cheaper than a round of Gibbs sampling. When trying to split a large cluster (or merge 2 large clusters), MH proposal 1 can still be fairly expensive because of the rounds of restricted Gibbs sampling. MH proposal 3 does not face this problem. However we find that after the burn-in period it tends to have low acceptance rate. We believe we need to redesign MH proposal 3 to produce more intelligent splits to increase the acceptance rate. Finally, MH-proposal 2 is the cheapest, both in terms of computation and book-keeping, and has reasonably high acceptance rate. We ran MH-proposal 2 a hundred times between successive Gibbs sampling updates. The acceptance rates of the MH proposals (given in Figure 4) are slightly lower than those reported by [12], where a plain DP mixture model was applied to a simple synthetic data set, and where split/merge acceptance rates were on the order of 1 to 5 percent. We described a conceptually simple and elegant framework for the construction of dependent DPs based on normalized gamma processes. The resulting collection of random probability measures has a number of useful properties: the marginal distributions are DPs and the weights of shared atoms can vary across DPs. We developed auxiliary variable Gibbs and Metropolis-Hastings samplers for the model and applied it to time-varying topic modelling where each topic has its own time-span. Since [6] there has been strong interest in building dependent sets of random measures. Interestingly, the property of each random measure being marginally DP, as originally proposed by [6], is often not met in the literature, where dependent stochastic processes are defined through shared and random parameters [3, 14, 15, 11]. Useful dependent DPs had not been found [16] until recently, when a flurry of models were proposed [17, 18, 19, 20, 21, 22, 23]. However most of these proposals have been defined only for the real line (interpreted as the time line) and not for arbitrary spaces. dependent through Gaussian processes. A model similar to ours was proposed recently in [23], using the same basic idea of introducing dependencies between DPs through spatially overlapping case vs a (restricted) Gamma process in ours) and the construction of the DPs (they use the stick breaking construction of the DP, we normalize the restricted Gamma process). Consequently, the nature of the dependencies between the DPs differ; for instance, their model cannot be interpreted as a mixture of DPs like ours.
 locations of atoms to vary using the spatial DP approach [13]. Second, more work need still be done to improve inference in the model, e.g. using a more intelligent MH proposal 3. Third, although we have only described spatial normalized gamma processes, it should be straightforward to extend the approach to spatial normalized random measures [7, 8]. Finally, further investigations into the properties of the SN  X  P and its generalizations, including the nature of the dependency between DPs and asymptotic behavior, are necessary for a complete understanding of these processes.
