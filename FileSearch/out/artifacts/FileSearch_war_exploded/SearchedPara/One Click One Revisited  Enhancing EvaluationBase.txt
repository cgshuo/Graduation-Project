 The NTCIR-9 One Click Access Tas k ( X 1CLICK-1, X  pronounced One Click One ) was concluded in December 2011 [17]. In con trast to traditional information retrieval (IR) and web search where systems output a ranked list of items in response to a query, 1CLICK-1 required systems to output one piece of concise text, typically a multi-document summary of several relevant web pages, that fits (say) a mobile phone screen. Partici pating systems were expected to output important pieces of inform ation first, and to minimise the amount of text the user has to read in order to obtain the des ired information. The task was named One Click Access because systems were requ ired to satisfy the user immediately after the user issues a simple query and clicks on the SEARCH button. This task setting fits particularly well to a mobile scenario in which the user has very little time to interact with the system [16].

To go beyond document retrieval and design advanced information retrieval systems such as 1CLICK systems, the IR community needs to explore evaluation based on information units ( X  X Units X ) rather than document relevance [1] 1 .An iUnit should be an atomic piece of information that stands alone and is useful to the user. At 1CLICK-1, S-measure was used to evaluate participating systems based on iUnits: this is a generalisation of the weighted recall of iUnits ( X  X -recall X ), but unlike W-recall it takes the positions of retrieved iUnits into account.
Figure 1 shows a few conceptual images of texts output by 1CLICK systems, called X -strings as 1CLICK-1 systems were req uired to return a text whose target length is no more than X characters. The X -strings in Figure 1(a) and (b) are both 300-character long ( X = 300, where X is the actual length), and they contain exactly the same pieces of inform ation that are relevant to a particular query. However, while the X -string in (a) makes the user read some nonrelevant text before he can get to the relevant text, that in (b) shows the same relevant text first. In this sense, the user can reach the desired information more efficiently with (b) than with (a). While W-recall and traditional  X  X ugget-based X  evaluation metrics in summarisation and question answering regard (a) and (b) as equally effective, S-measure rewards (b ) more heavily than (a). This position-sensitive evaluation can help researchers design effective 1CLICK systems.

S-measure has a parameter called L which represents the user X  X  patience: at 1CLICK-1, where a Japanese task was evaluated, L was set to 500 based on the statistic that the average reading speed of a Japanese person is 400-600 characters per minute. Thus L = 500 implies that the user has only one minute to gather the desired pieces of informat ion. The first objective of the present study is to examine the effect of L on the evaluation outcome of the participating systems at 1CLICK-1. For example, suppose the user only has thirty seconds to read the X -string: would the official system rankings change?
The second objective is to complemen t the official evaluation reported at 1CLICK-1, by proposing a simple extension to the iUnit-based evaluation. Com-pare Figure 1(b) and (c): the two X -strings contain the same relevant information in the same positions, but the one in (c) contains more nonrelevant text: it may make the user waste more time. However , as S-measure is a recall-based metric, it cannot differentiate between (b) and (c ). Hence we introduce a precision-like metric called T-measure and a combination of S-measure and T-measure (or  X  S  X  and  X  T  X  for short) called S , and demonstrate that they provide new insight into the 1CLICK-1 systems. 2.1 Evaluating Search The present study builds on the S-measure framework for evaluating 1CLICK-1 systems [16,17]. There is an analogy between the well-known normalised Dis-counted Cumulative Gain (nDCG) [6] and S : while nDCG evaluates a ranked list of items (e.g. URLs) while discounting the value of each item based on their rank positions , S evaluates a textual output (i.e. the X -string) while discounting the valueofeachiUnitbasedontheir offset positions within the output, to reward systems that satisfy the user quickly. As S assumes that the user X  X  reading speed is constant, its position-based discounting is equivalent to time-based discount-ing, as was recently proposed by Smucker and Clarke [18]. However, the latter concerns web search results, i.e. a ra nked list of documents with snippets.
The INEX Snippet Retrieval track 2 evaluates the quality of snippets as a means to judge the relevance of the original documents within the traditional ranked list evaluation framework.  X  -nDCG , designed primarily for diversified IR evaluation, views both documents and search intents as sets of nuggets [5]. More recently, Pavlu et al. [12] have proposed a nugget-based evaluation framework for IR that involves automatic matching between documents and gold-standard nuggets. They are jointly running the NTCIR-10 1CLICK-2 task with Sakai, Kato and Song [16,17] to explore evaluation approaches based on iUnits 3 . 2.2 Evaluating Summarisation ROUGE is a family of metrics for evaluating summaries automatically [8]. The key idea is to compare a system output with a set of gold-standard summaries in terms of recall (or alternatively F-measure ), where recall is defined based on automatically extracted textual fragments such as N-grams and longest common subsequences. New automa tic summarisation metrics were also explored at the TAC (Text Analysis Conference) AESOP ( Automatically Evaluating Summaries of Peers) task 4 .

While automatic evaluation methods such as ROUGE are useful for efficient evaluation of summarisers, the S-measure framework builds on the view that automatic string matching between the system output and gold standards is not sufficient for building effective abstractive summarisers [16]. Thus, in the S-measure framework, the identification of iUnits within an X -string is done manually. More importantly, the assessor records the position of each iUnit. As we discussed earlier, this enables the S-measure framework to distinguish between systems like Figure 1(a) and (b).

The S-measure framework is similar to the pyramid method for summarisa-tion evaluation [11] in that it relies on manual matching. In the pyramid method, Semantic Content Units (SCUs) are extracted from multiple gold-standard sum-maries, and each SCU is weighted accord ing to the number of gold standards it matches with. Finally, SCU-based weight ed precision or recall is computed. Just like the automatic methods, however, these methods are position insensitive . 2.3 Evaluating Distillation The DARPA GALE distillation program evaluated ranked lists of passages out-put in response to a query (or a set of queries representing a long-standing information need). Within this framework, Babko-Malaya [3] describes a sys-tematic way to define nuggets in a bottom-up manner from a pool of system output texts. In contrast, the iUnits were defined prior to run submissions at 1CLICK-1 [17].

White, Hunter and Goldstein [20] defined several nugget-based, set retrieval metrics for the distillation task; Allan, Carterette and Lewis proposed a character-based version of bpref to evaluate a ranked list of passages [2]. Yang and Lad [21] have also discussed nugget-based evaluation metrics that are similar in spirit to  X  -nDCG, for multiple queries issued over a period of time and multiple ranked lists of retrieved passages. In Yang and Lad X  X  model, utility is defined as benefit subtracted by cost of reading . Whereas, in the S-measure framework, the cost of reading is used for directly discounting the value of iUnits. 2.4 Evaluating Question Answering In Question Answering (QA), evaluation approaches similar to those for sum-marisation exist. POURPRE , an automatic evaluation metric for complex QA, is essentially F-measure computed based on unigram matches between the sys-tem output and gold-standard nuggets [9]. As in summarisation, the matching between system outputs and gold-standard nuggets can also be done manually. Either way, the main problem with this approach is that precision is difficult to define: while we can count the number of gold-standard nuggets present in a system output, we cannot count the number of  X  X ncorrect nuggets X  in the same output. To overcome this, an allowance of 100 characters per nugget match was introduced at the TREC QA track; the NTCIR ACLIA task determined the allowance parameters based on average nugget lengths [10].

S-measure, in contrast, does not require the allowance parameter. While the allowance parameter implies that every nugget requires a fixed amount of space within the system output, the S-measure framework requires a vital string for each iUnit, based on the view that differe nt pieces of information require different lengths of text to convey the information to the user (See Section 3.1).
One limitation of S is that it can only evaluate the content of the system output, just like all other nugget-based approaches. At 1CLICK-1, readability and trustworthiness ratings were obtained in parallel with the manual iUnit matches [17], which we will not discuss further in this paper. 3.1 Task and Data 1CLICK-1, the first round of the One C lick Access task, was run between March and December 2011. The task used 60 Jap anese search queries, 15 for each ques-tion category : CELEBRITY, LOCAL, DEFINITION and QA. The CELEBRITY and LOCAL queries were selected from a mobile query log; the DEFINITION and QA queries were selected from Yahoo! Chiebukuro (Japanese Yahoo! An-swers). The four query types were sel ected based on a query log study [7]. Two types of runs were allowed: DESKTOP runs ( X  X -runs X ) and MOBILE runs ( X  X -runs X ), whose target lengths were X = 500 , 140, respectively.

For a CELEBRITY query, for example, pa rticipating systems were expected to return important biography information. They were expected to return im-portant iUnits first, and to minimise the amount of text the user has to read. For example, the iUnits for Query  X  X samu Tezuka X  (a famous Japanese cartoonist who died in 1989) represented his date of birth, place of birth, his occupation, the comic books he published and so on. The iUnit that represented his date of birth contained a vital string  X 1928.11.03 X  because this string (or something equivalent) is probably required in order to convey to the user that  X  X samu Tezuka was born in November 3, 1928. X  The length of the vital string is used for defining an  X  X ptimal X  output and for computing S . Moreover, at 1CLICK-1, each iUnit was weighted based on votes from five assessors.

Only three teams participated in the task, but ten runs based on diverse approaches were submitted to it: Teams KUIDL , MSRA and TTOKU took in-formation extraction, passage retrieval and multi-document summarisation ap-proaches, respectively 5 . Both organisers and participants took part in manual iUnit matching, using a dedicated interface which can record match positions. Every X -string was evaluated by two assessors: in this study, we evaluate runs based on the Intersection data ( I )andthe Union data ( U ) of the iUnit matches [17]. The 60 queries and the official evaluation results are publicly available 6 ,and the iUnit data can be obtained from National Institute of Informatics, Japan 7 .
For more details on 1CLICK-1, the reader is referred to the Overview paper [17]. 3.2 S-Measure and S S-measure was the primary evaluation metric used at 1CLICK-1. Let N be the set of gold-standard iUnits constructed for a particular query, and let v ( n ) be the vital string and let w ( n ) be the weight for iUnit n  X  N .The Pseudo Minimal Output (PMO) for this query is defined by sorting all vital strings by w ( n )(firstkey)and | v ( n ) | (second key) [16]. Thus, the basic assumptions are that (a) important iUnits should be presented first; and (b) if two iUnits are equally important, then the one that can  X  save more space X  should be presented first. The crude assumptions obviously may conflict with text readability, but have proven to be useful [16,17]. Let offset  X  ( v ( n )) denote the offset position of v ( n ) within the PMO. Let M (  X  N )denotethesetof matched iUnits obtained by manually comparing the X -string with the gold standard iUnits, and let offset ( m ) denote the offset position of m  X  M . Morever, let L be a parameter that represents how the user X  X  patience runs out: the original paper that proposed S used L =1 , 000, while 1CLICK-1 used L = 500. The former means that the user has about two minutes to examine the X -string, while the latter means that he only has one minute. S is defined as: Thus, all iUnits that appear after L characters within the X -string are considered worthless. When L is set to a very large value, S reduces to weighted recall (W-recall), which is position-insensitive. Also, as there is no theoretical guarantee that S lies below one, S-flat given by S =min(1 ,S -measure )maybeused instead. In practice, the raw S values were below one for all of the submitted 1CLICK-1 runs and the  X  X lattening X  was unnecessary [17]. 4.1 Effect of the Patience Parameter The official 1CLICK-1 evaluation used L = 500 (one minute) with S .Inthe present study, we vary this parameter as follows and examine the outcome: L =1 , 000 (two minutes, the original setting from Sakai, Kato and Song [16]), L = 250 (30 seconds) and L = 50 (6 seconds). Note that if L is set to an extremely small value, most of the contents of the X -strings will be ignored. This is analogous to truncating ranked lists of documents prior to IR evaluation. 4.2 Evaluating Terseness: T-Measure, T and S As was discussed earlier, S cannot distinguish between Figure 1(b) and (c). We therefore introduce a precision-like  X  Terseness X  metric for evaluating an X -string of size X : Note that the numerator is a sum of vital string lengths, and that these lengths vary, unlike traditional nugget precision. As T might exceed one, we also define T-flat given by T =min(1 ,T -measure ), although in reality T never exceeded one for our data and therefore T = T holds. Finally, following the approach of the well-known F-measure, we can define S-sharp as: where letting  X  = 1 reduces S toaharmonicmeanofS and T . However, as we regard S as the primary metric and want T to  X  X nter into the calculation only as a length penalty X  [9], we also examined  X  =3 , 5 , 10 , 20. While  X  =3 , 5 reflect the practices in QA evaluation [9,10], our experiments suggest that an even higher  X  may be suitable for 1CLICK, as we shall see later.

To sum up, S differs from the traditional nugget-based F-measure in the following two aspects: (1) It utilises the positions of iUnits for computing the recall-like S ; and (2) Instead of relying on a fixed allowance parameter, it utilises the vital string length of each iUnit for computing the precision-like T-measure. 5.1 Results on the Patience Parameter Figure 2(a) and (b) show the effect of L on the overall system ranking with Mean S with I and with U , respectively. The x -axis shows the runs sorted by Mean S ( L = 500), i.e. the official ranking. With I , Kendall X  X   X  with the official ranking are .87 (Mean W-recall), .96 ( L =1 , 000), .78 ( L = 250) and .64 ( L = 50); with U , the corresponding values are .82 (Mean W-recall), .96 ( L =1 , 000), .73 ( L = 250) and .69 ( L = 50). Thus, L =1 , 000 (two minutes [16]) produces rankings that are very similar to L = 500 (one minute), but L = 250 (30 seconds) results in substantially diffe rent system rankings. In particular, Figure 2(a) shows that while Mean S with L = 500 prefers KUIDL-D-OPEN-1 over MSRA1click-D-OPEN-2 and prefers KUIDL-D-OPEN-2 over MSRA1click-D-OPEN-1 ,Mean S with L = 250 has exactly the opposite preferences. This trend is further emphasized by Mean S with L = 50.

Recall that S with L = 250 ignores all iUnit matches between positions 250 and 500 for all of the D-runs. Thus, the above discrepancy between L = 500 and L = 250 regarding KUIDL and MSRA1click suggests that while KUIDL is good at covering important iUnits, MSRA1click is good at presenting the most important units near the beginning of the X -string. To illustrate this point, Figure 3 shows the actual X -strings of KUIDL and MSRA1click for a LOCAL query  X  Menard Aoyama Resort  X  (name of a facility). It can be observed that even though KUIDL is superior to MSRA1click in terms of the number of matches with I (4 matches vs. 3), MSRA1click is actually very good from the viewpoint of iUnit positions as indicated by the underlined texts that correspond to the iUnit matches. With I ,the S with L = 500 for KUIDL is 0.200, and that for MSRA1click is 0.332; whereas, the S with L = 250 for KUIDL is 0.120, and that for MSRA1click is 0.528. Thus the difference between two systems is magnified when L = 250.
Next, we examine the effect of L on discriminative power . Given a test col-lection with a set of runs, discriminative power is measured by conducting a statistical significance test for every pair of runs [14]. This methodology has been used in a number of evaluation studies [5,13,15,18,19], and is arguably one necessary (but by no means sufficient) co ndition of a  X  X ood X  metric. We used a randomised version of two-sided Tukey X  X  Honestly Significant Differences (HSD) test for testing statistical significance, which is known to be more reliable than traditional pairwise significance tests [4,15].

Figure 4 shows the Achieved Siginificance Level (ASL) curves [14] of S with varying L . Here, the y -axis represents the ASL (i.e. p -value), and the x -axis represents the 45 run pairs sorted by the p -value. Metrics that are closer to the origin are the ones that are highly discriminative, i.e. those that provide reliable experimental results. It can be observed that the discriminative power for L = 250 is the highest while that for L = 50 is low (naturally, as the lat-ter implies looking at only the first 50 characters of every X -string). Moreover, S with L = 250 is more discriminative than W-recall. These observations are consistent across I and U . Thus, at least for the runs submitted to 1CLICK-1, using L = 250 (user has 30 seconds) along with the official L = 500 (user has one minute) seems beneficial not only for ex amining 1CLICK systems from different angles but also for enhancing discriminative power. Based on these results, we consider L = 250 , 500 in the next section. 5.2 Results on T-Measure and S Next, we discuss T and S , which we introduced for penalising redundancy in 1CLICK evaluation. Figure 5 shows the system rankings according to Mean S , T and S (where the x -axis represents runs sorted by Mean S with L = 500), while Figure 6 shows the Kendall X  X   X  between the ranking by Mean S and one by Mean S with  X  (denoted by S X  ). Note that  X  means  X  S is  X  times as important as T  X  X ndthat S 0= T (See Eq. 3).

First, in Figure 5, T rates the four M-runs that contain  X  -M- X  in their run names (especially the two KUIDL-M runs) relatively highly, but this is because M-runs use X = 140 as the target length while D-runs use X = 500. (Had the 1CLICK-1 task received more runs, these two run types would have been ranked separately.) More interestingly, The Mean S rankings in Figure 5(a) unanimously prefer MSRA1click-D-OPEN-2 over KUIDL-D-OPEN-1 and prefer MSRA1click-D-OPEN-1 over KUIDL-D-OPEN-2 , contrary to the official Mean S ranking. This suggests that MSRA1click was actually better than KUIDL from the viewpoint of terseness. To illustrate this point, Figure 7 shows the X -strings for the QA query  X  The three duties of a Japanese citizen  X : both KUIDL and MSRA1click managed to capture the three answers and their S values are 0.977 and 0.988, respectively (note that the former underperforms the latter even in terms of S , due to one ill-placed iUnit); whereas, the T values are 0.014 and 0.400, respectively. Thus, T reflects the fact that the X -string of KUIDL is highly redundant while that of MSRA1click is almost perfect. (The figure shows how to compute S and T for the X -string of MSRA1click .) It can be observed that T and S are useful complements to S for evaluating 1CLICK systems.

Figure 8 shows the ASL curves for our proposed metrics. From the viewpoint of discriminative power, it can be observed that T is very poor, and therefore that it is safer to set  X  to a high value when using S .Tobemorespecific,it can be observed that the discriminative power of S 10 is comparable to that of S for both L = 250 (shown as dotted lines) and L = 500 (shown as solid lines). Since S 10 retains the high discriminative power of S and provide new insight to the evaluation as shown in Figures 5 and 6, we recommend S 10 for evaluating 1CLICK systems, along with the original S . This paper extended the 1CLICK-1 evaluation framework, where systems were required to return a single, concise textual output in response to a query in order to satisfy the user immediately after a click on the SEARCH button. We first showed that the discount parameter L of S-measure affects system ranking and discriminative power, and that using multiple values, e.g. L = 250 (user has only 30 seconds to view the text) and L = 500 (user has one minute), is useful: a 1CLICK system which can satisfy the user X  X  information need within one minute may be different from one which can satisfy the need within 30 seconds. Also, S with L = 250 appears to be more discriminative than S with L = 500 and W-recall, at least for the runs submitted to the 1CLICK-1 task. We then complemented the recall-like S with a simple, precision-like metric called T-measure as well as a combination of S and T , called S .Weshowed that S with a heavy emphasis on S (e.g. S 10) imposes an appropriate length penalty to 1CLICK-1 system outputs and yet achieves discriminative power that is comparable to S .

At the NTCIR-10 1CLICK-2 Task, to which over ten teams have registered to participate, T and S will be used along with S as official metrics. Moreover, at 1CLICK-2, the language scope has been extended to English and Japanese. While the evaluation framework of S , T and S should apply to any language, it would be interesting to test it in the English subtask as well. There may be language-dependent issues in defining iUnits and vital strings 8 .Moreover,we plan to look into the relationship between these metrics with readability, trust-worthiness and other qualities required of an X -string [17], and the relationship between these metrics with metrics based on automatic matching [12].
