 Machine comprehension of text is the central goal in NLP. The academic community has proposed a variety of tasks, such as information extrac-tion (Sarawagi, 2008), semantic parsing (Mooney, 2007) and textual entailment (Androutsopoulos and Malakasiotis, 2010). However, these tasks as-sess performance on each task individually, rather than on overall progress towards machine compre-hension of text.

To this end, Richardson et al. (2013) proposed the Machine Comprehension Test (MCTest), a new challenge that aims at evaluating machine comprehension. It does so through an open-domain multiple-choice question answering task on fictional stories requiring the common sense reasoning typical of a 7-year-old child. It is easy to evaluate as it consists of multiple choice ques-tions. Richardson et al. (2013) also showed how the creation of stories and questions can be crowd-sourced efficiently, constructing two datasets for the task, namely MC160 and MC500. In ad-dition, the authors presented a lexical matching baseline which is combined with the textual en-tailment recognition system BIUTEE (Stern and Dagan, 2011).

In this paper we develop an approach based on lexical matching which we extend by taking into account the type of the question and coreference resolution. These components improve the per-formance on questions that are difficult to han-dle with pure lexical matching. When combined with BIUTEE, we achieved 74.27% accuracy on MC160 and 65.96% on MC500, which are signif-icantly better than those reported by Richardson et al. (2013). Despite the simplicity of our ap-proach, these results are comparable with the re-cent machine learning-based approaches proposed by Narasimhan and Barzilay (2015), Wang et al. (2015) and Sachan et al. (2015).

Furthermore, we examine the types of questions and answers in the two datasets. We argue that some types are relatively simple to answer, partly due to the limited vocabulary used, which explains why simple lexical matching methods can per-form well. On the other hand, some questions re-quire understanding of higher level concepts such as those of the story and its characters, and/or re-quire inference. This is still beyond the scope of current NLP systems. However, we believe our analysis will be useful in developing new methods and datasets for the task. To that extent, we will MCTest is an open-domain multiple-choice ques-tion answering task on fictional stories consist-ing of two datasets, MC160 and MC500. The
Figure 1: An excerpt from story mc500.train.44 two datasets contain 160 and 500 stories respec-tively, with 4 questions per story, and 4 candi-date answers per question (Figure 1). All stories and questions were crowd-sourced using Amazon by Richardson et al., while MC500 was curated by crowdworkers. Both datasets are divided into training, development, and test sets. All develop-ment was conducted on the training and develop-ment sets; the test sets were used only to report the final results. Richardson et al. (2013) proposed a sliding win-dow algorithm that ranks the answers by form-ing the bag-of-words vector of each answer paired with the question text and then scoring them ac-cording to their overlap with the story text. We propose a modified version of this algorithm, which combines the scores across a range of win-dow sizes.

More concretely, the algorithm of Richardson et al. (2013) passes a sliding window over the story, size of which is equal to the number of words in the question-answer pair. The highest over-lap score between a story text window and the question-answer pair is taken as the score for the answer. Therefore, their algorithm makes a single pass over the story text per answer. In compar-ison, our system scores each answer by making multiple passes and summing the obtained scores. Concretely, on the first pass, we set the sliding window size to 2 tokens, and increment this size on each subsequent pass, up to a length of 30 to-kens. We then combine this score with the over-all number of matches of the question-answer pair across the story as a whole. This enables our algo-rithm to catch long-distance relations in the story. Similar to Richardson et al. (2013), we use a lin-ear combination of this score with their distance-based scoring function, and we weigh tokens with their inverse document frequencies in each indi-vidual story.

By itself, this simple enhancement gives sub-stantial improvements over the MSR baseline as shown in Table 1 ( Enhanced SW+D ), as it mea-sures the overlap of the question-answer pair with multiple portions of the story text. We build upon our enhanced scoring function us-ing stemming, rules taking into account the type of the question, and coreference. The improvements due to each of these components are presented in Table 1, and we discuss the application of corefer-ence and the rules used in more detail in the fol-lowing subsections.
 Table 1: Performance improvements on combined train and dev sets. 4.1 Coreference resolution The entities mentioned in MCTest stories are frequently referred to by anaphoric expressions throughout the story and the questions, which is ignored by the described scoring function. Therefore, we substituted each mention in a co-reference chain with its representative mention, applied the scoring function on the processed text and added the score to the original one. The chains and their represenatative mentions were ob-tained using the Stanford CoreNLP toolkit (Man-ning et al., 2014). We found that coreference im-proved performance on some question types, but decreased performance on others. Thus we devel-oped a set of question rules in order to apply it selectively, which we discuss in the next section. 4.2 Rules
ROOT Which food was not eaten ? Figure 2: Dependency tree to detect negation To account for the variety of questions in MCTest, we developed a set of rules to handle cer-tain question types differently. To this purpose, we created rules which detect numerical, tempo-ral, narrative and negation-based questions, and additionally partition questions by their wh -word.
By partitioning questions in different types, we found that who questions, which primarily deal with identifying a character in the story, benefit from the use of coreference chains described in the previous section. In addition, performance in questions aimed at selecting an appropriate noun, such as which , where or numerical questions, also improved with coreference. However, other ques-tion types, such as why questions, or questions concerning the story narrative, did not register any consistent improvement, and we opted not to use co-reference for them. This selective application of co-reference resulted in improvements on both datasets (Table 1).

We also identified negation questions as requir-ing special treatment. Some negation questions are trivially solvable by selecting an answer which does not appear in the text. However, our proposed function that scores answers according to the lexi-cal overlap with the story text is unlikely to score answers not appearing in text highly. Motivated by this observation we invert the score of each word when a question with a negated root verb was de-tected, e.g.  X  X hat did James not eat for dinner? X  , using Stanford Typed Dependencies (De Marneffe and Manning, 2008), as depicted in Figure 2. Due to this inversion a higher lexical overlap results in a lower score, improving accuracy on both MC160 and MC500 ( +negation in Table 1)
In a similar fashion we detected numerical ques-tions based on the presence of a POS tag for a cardinal number in either the question or any of the answers choices. Questions concerning the story X  X  narrative (e.g.  X  X hich is the first char-acter mentioned in the story X  ) were detected us-ing keywords (e.g. character , book , etc.). Addi-tionally, we detected temporal questions such as  X  X hat did Jane do before she went home? X  by the presence of a temporal modifier or temporal prepositions (e.g. before , while , etc.). Then we attempted to account for them by searching the text for the sentence indicating that she had gone home and reducing the weight for all subsequent sentences. However, since the improvements due to these rules were negligible, we did not include them in our final system. Nevertheless, these rules were helpful in analyzing problem areas in the datasets, as discussed in Section 6. We evaluated our system on MC160 and MC500 test sets and the results are shown in Table 2. Our proposed baseline outperforms the baseline of Richardson et al. (2013) by 4 and 3 points Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011). If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best re-sults achieved by Richardson et al. (2013).

Concurrently with ours, three other approaches to solving MCTest were developed and sub-sequently published a few months before our method. Narasimhan and Barzilay (2015) pre-sented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important. Despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model. Sachan et al. (2015) treated MCTest as a structured prediction problem, searching for a latent structure connecting the question, answer and the text, dubbed the answer-entailing struc-ture. Their model performs better on MC500 (was MSR SW+D 68.02% 59.93% Final system 72.19% 62.67%  X  MSR SW+D + RTE 69.27% 63.33% Narasimhan &amp; Barzilay 73.23% 63.75% Sachan et al. -67.83% Wang et al. 75.27% 69.94% Table 2: Performance on the MC160 and MC500 test sets, including the results of all previous work. * denotes statistically significant ( p &lt; 0 . 05 ) im-provement using McNemar X  X  test, with respect to the MSR baseline (SW+D) not tested on MC160), however the strength of our model is obtaining comparable results with a much simpler model. The work of Wang et al. (2015) is the most similar to ours, in the sense that they combine a baseline feature set with more ad-vanced linguistic analyses, namely syntax, frame semantics, coreference, and word embeddings. In-stead of a rule-based approach, they combine them through a latent-variable classifier achieving the current state-of-the-art performance on MCTest. Using the question-filtering rules mentioned in Section 4.2, we obtained individual accuracy scores per question type for the final system com-bined with RTE (Table 3). Note that these types are in three groups: i) wh-word questions (dis-joint, questions without an wh-word are in Other ), ii) classes of questions requiring non-trivial lin-guistic analysis and reasoning (not disjoint, not all questions considered), and iii) questions originally classified by crowdworkers, classifying whether the question can be answered by a single or multi-ple sentences in the story (disjoint).

Compared to the wh-word question type re-sults of Narasimhan and Barzilay (2015), our ap-proach performs better primarily on why questions (72.97% and 80.65% vs. 59.45% and 69.35% on MC160 and MC500 respectively) and slightly bet-ter on how , when and what questions on MC500. Additionally, our system is more successful in questions requiring multiple sentences to be an-swered correctly (70.31% and 63.3%vs. 65.23% and 59.9% on MC160 and MC500 respectively).
 If we remove the RTE component from our sys-Quantifiers 70.00% (20) 53.38% (37) Table 3: Performance of our final system + RTE per question type on the test sets. The number of relevant questions is in parentheses. tem, the performance on relatively simple question types such as what , who and where remains practi-cally the same, thus confirming that our approach can handle simple questions well. On the other hand, the performance on why questions drops without RTE, thus stressing the need for deeper text understanding.

There are several clear deficiencies in certain question types, particularly in handling negation. These errors provide a broad overview of the cases in which simple lexical techniques are not suffi-cient to determine the correct answer.
 Many numerical questions, particularly in MC500, require the use of simple algebra over story elements, including counting characters and objects, and understanding temporal order. One question even requires calculating the probability of an event occurring, while another one calls for complex volumetric calculation. Answering ques-tions such as these is beyond the capabilities of a lexical algorithm, and accuracies in this cate-gory are worse than on all questions. Addition-ally, lexical algorithms such as ours, which ig-nore predicate-argument structure, perform worse in the presence of quantifiers.

In MC500, the performance of our system on more abstract questions, concerning the overall narrative of the story, also demonstrates a sig-nificant inadequacy of lexical-based algorithms. Questions such as  X  What was the first character mentioned in the story?  X , which relate to the over-all narrative flow of the passage, or questions con-cerning the state of the story environment, such as  X  Where is the story set?  X , are difficult to solve without a system which understands the concept of a story. Typical question-answering methods would also struggle here.

Another type of challenging question are those which require an implicit temporal understanding of the text, i.e. questions concerning time without using a temporal modifier. For example, given a story which states that  X  John is at the beach  X , then later  X  John went home  X , a question such as  X  What did John do at home?  X  would prove itself difficult for traditional methods to answer. These questions are difficult to identify automatically by the form of the question alone, thus we cannot provide ac-curacies for them.

Our results confirm that it is easier to achieve better performance on MC160 with simple lexi-cal techniques, while the MC500 has proved more resilient to the same improvements. We also ob-served that the MC500 registers smaller improve-ments in accuracy when adding components such as co-reference. This is a consequence of the de-sign and curation process of the MC500 dataset, which stipulated that answers must not be con-tained directly within the story text, or if they are, that two or more misleading choices included. Richardson et al. (2013) demonstrate that the MC160 and MC500 have similar ratings for clar-ity and grammar, and that humans perform equally well on both. However, in many cases MC500 ap-pears to be designed in such a way to confuse lex-ical algorithms and encourage the use of more so-phisticated techniques necessary to deal with phe-nomena such as elimination questions, negation, and common knowledge not explicitly written in the story. The use of shallow methods for machine compre-hension has been explored in previous work, for example Hirschman et al. (1999) used a bag-of-words to match question-answer pairs to sentences in the text, and choose the best pair with the best matching sentence. As discussed in our analysis, such systems cannot handle well questions involv-ing negation and quantification. Numerical ques-tions, which we found to be particularly challeng-ing, have been the focus of recent work on algebra word problems (Kushman et al., 2014) for which dedicated systems have been developed.

MacCartney et al. (2006) demonstrated that a large set of rules can be used to recognize valid textual entailments. These consider phenomena such as polarity and quantification, similar to those we used in our analysis of the MCTest datasets. More complex methods, which attempt deeper modeling of text include Natural Logic (Angeli and Manning, 2014) and Combinatorial Catego-rial Grammars (Lewis and Steedman, 2013) com-bined with distributional models. While promis-ing, these approaches have been developed pri-marily on sentence-level tasks, thus the stories in MCTest are likely to present additional challenges. The recently proposed class of methods called Memory Network (Weston et al., 2014), uses neu-ral networks and external memory to answer a simpler comprehension task. Though quite suc-cessful on toy tasks, those methods cannot yet be applied to MCTest as they require much larger training datasets than the ones available for this task.

A recent approach by Hermann et al. (2015) uses attention-based recurrent neural networks to attack the problem of machine comprehension. In this work, the authors show how to generate large amounts of data for machine comprehension exploiting news websites, and how to use novel architectures in deep learning to solve the task. However, due to the need for a large dataset for training, and the focus only on questions that take entities as answers, this approach has not been ap-plied to MCTest. In this paper we developed an approach to MCTest that combines lexical matching with simple lin-guistic analysis. We evaluated it on the two MCTest datasets, MC160 and MC500, and we showed that it improves upon the original baseline by 4 and 3 percentage points respectively, while being comparable to more complex approaches. In addition, our analysis highlighted the challenges involved and in particular in the MC500 dataset. We would like to thank Tom Brown for his contri-butions in the early stages of this work.
