
Signature recognition learns signature patterns of intrusive (and normal) activities from training data, and then in detection, matches these signatures with the observed incoming data. Signature-recognition algorithms face great challenges in computer-intrusion detection. First, activity data from a computer system can easily contain millions of records per day. In addition, each record may have hundreds of data fields. Thus, an algorithm to learn signature patterns in such data must be scalable.
Second, patterns of normal and intrusive activities very likely change over time, and new forms of attacks emerge everyda y. Hence, a data-mining algorithm must have the incremental learning ability to upda te signature patterns as more training data become available. Last, the distribution for normal and intrusive data may be unclear.

Data-mining techniques, such as decision tr ees, association rules, artificial neural networks and Bayesian networks, have been used as signature-recognition algorithms for intrusion detection (Axelsson 2000). However, in many cases, they are not capable of learning signature patterns in a scalable, incremental manner. Moreover, several of them, including Bayesian networks, require the understanding of domain knowledge or data distribution.
 called clustering and classification algorithm X  X upervised (CCAS) (Li and Ye 2002).
CCAS is based on supervised clustering for learning patterns of normal and intrusive activities and instance-based learning to classify observed activities. Clustering relies very little on the distribution of data, suitable for intrusion detection. Like other in-cremental data-mining algorithms, CCAS shows sensitivity to the presentation order of training data. Recently, grid-based and density-based methods have been used to overcome this problem (Ester et al. 1998; Harsha and Choudhary 1999; Zhang 1997).
Built on concepts from these clustering methods, together with several innovative concepts, we develop a robust extension of CCAS in this paper.
Each data record has attribute vector X in p dimensions and a target class Y . Here we consider only numeric attributes. A cluster L is represented by the centroid coor-dinates XL of all the data points in it, the number of data points NL , and its class YL . A weighted Euclidean distance is used to calculate the distance from a data point
D to a cluster L , where X i and XL i are, respectively, the coordinates of D  X  X  and L  X  X  centroid on the i th dimension and r iY is the correlation coefficient between predictor X variable Y . The distance between two clusters i s calculated similarly by replacing the data point X  X  coordinates with the cluster centroid X  X  coordinates.
 sion is divided into a set of intervals within the range defined by the minimum and maximum values of data points, separating the space into cubic cells. Grid configu-ration could use different numbers of unequal intervals in different dimensions. We simplify our study by using the same number of equal grid intervals for all dimen-sions. At this research stage, we pick the best setting of this parameter from exper-imentation and expertise. Using a heuristic , the original CCAS clusters the training data points one by one based on the distance as well as the target class information.
We classify a new data point by comparing new data points with these clusters. For the binary target variable, we assign a continuous value falling in the closeness to the two target classes. We calculate the distance-weighted average of the target values of the k nearest clusters as the target value Y of a new data point D , where W j is the weight for the j th nearest cluster. The target value of this cluster is YL j .
At any instant during the supervised clustering, the cluster structure considers only the data points processed so far, reflecting a local view on training data. Presented with the data points in a different order, normal and intrusive clusters could grow differently if data points nearest in space do not come successively. Natural clus-ters of the same class may merge into larger clusters due to such unusual input order. Dividing the data space into grid cells and allowing the formation of clus-ters only within grid cells help alleviate this problem. We further strengthen CCAS with several postprocessing steps, listed with the problems they address in
Fig. 2. (1) Redistribution of data points is a common way to remedy the localisation in incremental clustering, as used in Zhang (1997). All training data points are clustered again using the produced clusters as seeds. When a seed cluster with the same class label is found to be the nearest to the incoming data point, this cluster is replaced with a new cluster, of which this data point is the centroid. We consider that the initial cluster structure is not the reliable reflection of data distribution and it func-tions as another facility to limit the growth of clusters here. We allow new clusters to emerge and thus allow bigger adjustment to the cluster structure.
 ance improves with repetition but at additional computation cost. The cluster structure becomes stable after several passes. Our experiments show that usually one round of redistribution is sufficient. duced clusters falling into neighbouring grid cells. Hence, we employ a hierarch-ical grouping procedure to regroup these clusters. This algorithm is different from the traditional hierarchical clustering in that it combines a pair of clusters into one larger cluster only when t hey not only are closest to each other but also have the same class. A single linkage method (Jain and Dubes 1988) is used in determining the distance between larger clusters. The distance between two clusters is defined as the shortest distance between any two points belonging to the respective clus-ters. can be removed. The threshold on the minimum number of data points could be based on the average number of data points in clusters and be different for different classes. However, this threshold is closely dependent on specific training data. For example, there may be very few instances of certain attack. To keep the signatures of this type of attack, the threshold for this attack type should be very small. We set the threshold to 2 in our study, a very conservative number.
 distribution is incremental. After the supe rvised grouping, we still could incorporate new available data points one by one, with or without the use of a grid. Another ad-vantage of the robust CCAS is to support the local adjustment of the cluster structure, attributed to the cluster representation and working procedures. Each step functions independently, linked by the clusters.
The clustering and postprocessing steps can be flexibly arranged. Figure 4 shows the five phases with the corresponding actions in our application. Phase 1 calculates the correlation coefficients in the distance measure. We apply the supervised grouping in phase 5 again to get a more compact cluster structure.
 target value of a new data point. However, this is not appropriate after the grid-based clustering in which grid cells limit th e formation of cluster s. Therefore, after phases 2 and 3, we use only the clusters in the grid cell of a data point. Grid cells play no role in phases 4 and 5, and then we use all the clusters in classifica-tion.
Let M be the number of the produced clusters after each phase and N the number of data points. In grid-based supervised clustering, the upper bound on the compu-tational complexity is O ( pNM ) if we search the clusters sequentially, where p is the dimensionality of attributes. If we apply a more efficient cluster-storage struc-ture, such as the one in Huang et al. (1992), the complexity can be reduced to
O ( pN ) . For the supervised cluster grouping, the complexity has an upper bound,
O ( M I ( M I  X  1 )/ 2 ) , on the number of pairwise distances of clusters, where M the number of initial clusters. We inspect these distances in the hierarchical group-ing beginning from the smallest one. The computation takes much less time because inspection terminates very soon when more and more distances are associated with clusters of different classes. The computational complexity of removing outliers is
O (
M ) .
We have applied the robust CCAS on one small data set and two large intrusion detection data sets, summarised in Fig. 5. (1) THY data is used in an empirical compar ison of decision tree, statistical and neural network classifiers (Lim et al. 2000). Taking advantage of its small size, we investigate the robustness of the robust CCAS to the presentation order of training data and the impact by the number of grid intervals. Classes 1 and 2 have much fewer representatives (93 and 191, respectively) than class 3 (3,488). The supervised grouping of clusters generates quite a few clusters for class 1 and 2, many of them containing only one data point. Therefore, we do not perform the outlier removal and Phase 5 because removal of outliers can be damaging. (2) In 2000 data, we use an exponentially weighted moving-average (EWMA) technique to obtain the smoothed occurrence frequency distribution of 284 audit-event types in Solaris operating system (Li and Ye 2002). Fifteen normal sessions and seven attack sessions are in the data stream from the host machine, called Mill, and 63 normal and 4 attack sessions in th e other stream from machine Pascal. The sessions are arranged sequentially, i.e., the data points of different sessions are not mixed in time. We show the result using the Pascal data in training and the Mill data in testing. Although using them the other way shows similar performance, more attack sessions in testing make the perform ance change more distinctive. We use two input orders in training to examine the robustness of this algorithm. We also test its sensitivity to the grid parameter. We use 6 and 11 grid intervals for input order 1. traffic connections for the 1998 DARPA Intrusion Detection Evaluation Program.
The number of grid intervals on each dime nsion is set to three after several ex-periments. In this study, we compare th e accuracy of the robust CCAS with those contest participants.
 for a categorical classification in calcula ting the confusion matrix. For the contin-uous predicted-target value, we perform th e receiver operating characteristic (ROC) analysis. A false positive (fal se alarm) occurs when an even t is predicted as intrusive but it is in fact normal. A false negative occurs when a truly intrusive event occurs without being signalled. If the target value is greater than the given signal thresh-old, the data record is signalled as intrusi ve, considered as normal otherwise. The hit rate is the ratio of the number of hits to the total number of the truly intrusive data records. The false-alarm rate is the ratio of false alarms to the total number of the truly normal data records. A ROC curve p lots hit rates and false-alarm rates for various signal thresholds. The closer it is to the top left corner, with 100 % hit rate and 0 % false-alarm rate, of a chart, the better the performance.
 for the audit sessions. The same audit event may be common in both normal and intrusive activities. We calculate a signa l ratio for each session and plot the ROC curves on sessions, with details in Li and Ye (2002). A parameter a is used in signal ratio calculation.
For THY data, the overall performance of the robust CCAS is comparable with those classification algorithms in Lim et al. (2000), reporting error rates below 0.1.
Figures 6 and 7 plot the change of error rat e with different grid intervals and input orders. For input order 1 X 3, using one grid interval yields the best classification error rate. Compared with class 3, classes 1 and 2 are poorly represented, with very few data points in the training data set. The data points of three classes are well mixed in input order 1 and 2. Class 3 points are at the beginning of input 3. Using just the redistribution and supervised cluster grouping could achieve good performance.
However, using more grid intervals, more data points of classes 1 and 2 are correctly classified. Input order 4 produces the worst performance without using grid intervals.
The training data points there are organis ed in the order of classes 3, 2, and 1. The data records of the same class tend to group together, especially for class 3, which has the majority of data points in training. However, the performance is comparable with other input orders when using more grid intervals.

Figure 8 shows the session-based ROC analysis for using 11 grid intervals and in-put order 1 on 2000 data. Three different a parameters generate three curves in each chart. The performance after phase 2 or pha se 3 is not satisfactory. After phase 4 or phase 5, all seven attack sessions are detected without false alarms. We perform ROC analysis for input order 2 with 11 grid intervals, shown in Fig. 9. Input order shows impact, because the ROC curves for this input order are worse than input order 1 after phases 2 and 3. The following postpro cessing steps improve the performance.
After phase 5, for two a, we again capture all attack sessions without generating false alarms.

There is a slight difference between six gri d intervals and 11 grid intervals after phase 2 for the same input order. However, after phase 5, they produce the same detection performance. The robust CCAS s hows robustness to the grid parameter to some extent.
Generally, finer grids may produce bette r performance because they lead to more clusters and thus allow important patterns of data points to be captured. However, too small grid intervals force natural clusters into smaller clusters. In special cases, this may generate poor cluster structures.

Figure 11 shows the ROC curves for KDD X 99 data. The best classification per-formance is improved after all postprocessing steps, closer to 90 % hit rate with near 0 % false alarm rate. The KDD Cup 1999 applies cost weights to confusion matrix cells to obtain an average cost per data record. The lower this average cost, the bet-ter the classification performance. The winning technique produces the average cost of 0.2331, with 0.5 % false alarm rate and 91.8 % hit rate. The best 17 participating algorithms have average costs from 0.2331 to 0.2684. Figure 12 shows the confu-sion matrices and calculated average cost, h it rate and false-alarm rate for the robust
CCAS. The average cost is improved to be 0.2445 after phase 5, comparable with the best participants. The false-alarm rate is about 0.9 % and the hit rate is 91 %. We present the robust CCAS X  X  scalable an d incremental data-mining algorithm.
The testing results show that the robus t CCAS makes significant improvement in detection performance and robustness to input order of training data. One important future research topic will be investigating the adaptive grid setting.

