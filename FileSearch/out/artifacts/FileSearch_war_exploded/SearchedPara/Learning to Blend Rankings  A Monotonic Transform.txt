 There have been great needs to develop effective methods for combining multiple rankings from heterogeneous domains into one single rank list arising from many recent web search applications, such as integrating web search results from multiple engines, facets, or verticals. We define this problem as Learning to blend rankings from multiple domains. We propose a class of learning-to-blend methods that learn a monotonically increasing transformation for each ranking so that the rank order in each domain is preserved and the transformed values are comparable across multiple rankings. The transformation learning can be tackled by solving a quadratic programming problem. The novel machine learning method for blending multiple ranking lists is evaluated with queries sampled from a commercial search engine and a promising improvement of Discounted Cumulative Gain has been observed. H.3.3 [ Information Systems ]: Information Search and Retrieval  X  Retrieval models; H.4.m [ Information Systems ]: Miscellaneous  X  Machine Learning Algorithms, Experimentation, Theory Blending, ranking, monotonic transformation, quadratic programming Given a set of items , a ranking of is a permutation of . There have been tremendous However in many applications, we need to integrate the rankings of items from heterogeneous domains into a single ranking of all search engines such as video search, image search, blog search, etc. For example, one set of items can be the set of documents from the Web, and the other can be the set of documents from a vertical search engine such as Blog or News search. Merging the rank lists from heterogeneous domains is a non-trivial topic, because: 1) these heterogeneous sets can share some documents, but most likely they also have many documents that are not in common; 2) Heterogeneous domains usually have different features and feature-to-relevance correlations. Take question-answering websites (e.g. Yahoo! Answer) for one example. Although the text matching and click features developed for general web can be used in the ranking of this domain, features developed with their unique page structures and user feedback, e.g., thumbs up ratings and the total number of feedbacks in Yahoo! A nswers, greatly benefit the ranking in its own domain. Even features shared by Yahoo! Answers and general web documents could have very different correlations with the relevance in the two domains. Therefore, using one universal ranking function across domains does not solve the problem well. Dedicated functions are needed in order to better rank documents within each individual domain, and new technologies that blend documents from heterogeneous domains into a single ranking list are greatly needed. We want to emphasize that this problem is generally different from the rank aggregation [4, 9] problem where one needs to merge the different rankings on the homogeneous set of items. We define the integration of rank lists from heterogeneous domains as a blending problem and formulate the problem of learning to blend rankings as follows: For each pair of item sets, we have a combined ranking of all the items in both of the item sets, presumably indicating the correct blending of the two given rankings. The optimal combined rankings are ground truth for learning to blend and could be generated in the following two steps: 1) assign relevance labels, e.g. Perfect, Excellent, Good, Fair and Bad (abbreviated to P/E/G/F/B) to each item in both rankings; 2) merge sort the ranking lists according to those labels. Blending in this way will maximize Discounted Cumulative Gain (DCG) [7] while preserving the ordering for both rankings. Given the training data  X  the combined ranking and the rankings in the individual domain, we want to learn a monotonic increasing transformation (on the ranking score in the individual domain) so that when presented with a new pair of item sets an d their associated rankings, we can use the transformed ranking scores to generate a combined ranking. In this paper, we formulate the problem as a quadratic programming problem and learn a linear monotonic the transformed scores are comparable.
 The rest of the paper is organized as follows: Section 2 describes the notations and the formal formulation of the blending problem. Section 3 develops the main algorithm, where the transformation lea rning is formulated as a quadratic programming problem. Section 4 shows the promising experimental result, evaluated with real -world data sampled from a commercial search engine. In section 5, we make conclusions and point out directions for future resear ch. To design a blending transformation, we assume that the training data consist of a set of pairs . In this work, we focus on the scenario where the order for each individual ranking is preserved after blending 1 . Blending with this constraint is very like merge sorting. For simplicity, let us assume we have two rankings. Considering , we will have where and are the numbers of items in the first and second set, respectively, and and are the items. For the rank order in each domain and , we consider two items, we have a score for each of the item, and the ranking of the obtained by sorting the scores of the items.
 Given a pair of item sets and their associated rankings, we can distinguish three cases: But our method is still applicable even this assumption is not satisfied.
 the tree cases can be addressed with one formulation.
 For and , we would have Correspondingly to and , we also have the combined ranking with totally items 2 : As required, the order of items from either li st is preserved in . Accordingly, we define two subsets of above , and correspo nds to cases where is ranked below , and define The key question is how to automatically learn a blending transformation from the training data. We propose to apply a monotonically increasing function to so that the blending would be based on and . By doing so, the order of items from each individual ranking list are auto matically preserved. is learned to be maximally conformed to the editorial blending ranking 3 . We formulate the transformation learning problem as a quadratic programming problem. subjec t to where is the total number of items from both and For simplicity, we assume no overlapping items between the two lists. Suppose we have rankings, . One wi ll be picked as the reference point and requires no transformation while the remaining transformations should be learned. If one simply assume that is linear and in the form of subject to By solving the above QP problem, we will obtain a for the linear transformation (the same will be applied to all the queries). We could also learn a for each query length, or each type of queries if query classification information is available. The constraints in Equa tion (1) are given the same weight, which can be adjusted to give higher weights to more important constraints. Other non -linear monotonic transformation should also be explored in future work. Equation (1) demonstrates the idea with two domains. The algo rithm can be easily extended to blend more than two rankings. Given ranking lists from domains, one will be picked as the reference point and there will be transformations involve all pairs of item sets from any two domains, i.e., the problem becomes subject to We evaluated the proposed algorithm wit h the problem of blending web search results with vertical search results in the domain of Yahoo! Answers. 1300 queries were sampled from the query logs of a commercial search engine, and 800 queries were used for training and 500 for validation . For each query, we have two sets of documents: general web documents and Yahoo! Answer s documents. Each document is labeled with one of five grades Perfect, Excellent, Good, Fair and Bad, in decreasing order of relevance. We have pre -generated ranking functions in each domain and the rank score or can be generated by applying the ranking function in each domain to the document in the corresponding domain. Given and , constrai ns for the QP problem can be constructed by applying merge -sort to the two rank lists and keeping the paired score preference between web documents and Answers documents. To evaluate the proposed algorithm we focus on the simple case where is a linear transformation, i.e., . 800 queries were used to learn the transformation and 500 queries were used for validation. BASELINE APPROACH. The baseline we compare to is the Na X ve blending method, where the scores of and are compared directly.
 EVALUATION METRICS. We report the widely used relevance metric Discounted Cumulative Gain (DCG) [7 ]. For a ranked list of N documents (N is set to be 10 or 1 in our experime nts), we use the following variation of DCG, where represents the weights assigned to the label of the document at position i , e.g., 10 for Perfect match, 7 for Excellent match, 3 for Good match, etc. Higher d egree of relevance corresponds to higher value of the DCG. We use DCG to indicate the average of DCG values over a set of testing queries.
 In our application, the goal is to blend the documents from Yahoo! Answers to web rank list. We reported the DCG 1 and DCG10 gain and 0.9% DCG1 gain were observed from our approach. Both improvements were statistically significant and highlighted with bond font in Table 1. In our application, the choice of  X  our experiments used did not achieve any improvement of DCG. This suggests that the rank scores from heterogeneous domains are not direc tly comparable and a blending algorithm is needed. this error rate measures how many constraints in the QP problem can not be satisfied. The error rate is reported in Table 2. Even the learned linear transformation gives an error rate of 35%. Therefore we study the optimal DCG that can be obtained by the  X  merge -sort strategy.
 UPPER BOUND FOR BLENDING . The ideal merge -sort of the two ranking lists can be considered as the best DCG10 that can be obtained via blending, i.e., the upper bound a blend ing algorithm could achieve. The best DCG10 of our test data is 7.06. Therefore there is room to improve for the blending algorithm. Section 5 will discuss the future research directions. In recent years, the ranking problem is frequently fo rmulated as a supervised machine learning problem [3, 6, 11]. These learning -to-rank approaches are capable of combining different kinds of features to train ranking functions. The problem of ranking can be formulated as that of learning a ranking function from pair -wise preference data. The idea is to minimize the number of contradicting pairs in training data. For example, RankSVM [8] uses support vector machines to learn a ranking function from preference data. RankNet [1] applies neural network and grad ient descent to obtain a ranking function. RankBoost [5] applies the idea of boosting to construct an efficient ranking function from a set of weak ranking functions. The studies reported in [12] proposed a framework called GBRank using gradient descent in function spaces, and the weak leaner is a decision tree. Cao et al. [2] proposed the listwise approach to handle the ranking problem. Furthermore, Rank aggregation [4, 9] targets the problem of merg ing the different rankings on the homogeneous set of ite ms, where items belong to the same domain. Our algorithm of formulating a pairwise ranking problem as a quadratic programming problem was inspired by the method described in [10]. Emergence of various vertical search engines such as video search, image search, and blog search, m otivate s the development of algorithms to blend rank lists from multiple domains. Unlike the traditional learning to rank or rank aggregation problem within one domain, in this paper we study the problem of combining rank lists from heterogeneous domains to obtain one single rank list. The task of learning to blend rankings is defined. The blending problem is a challenging task. There are few documents/features in common among heterogeneous domains. There fore the ranking function for each type of documents needs to be learned within the domain. However the rank scores of ranking functions in heterogeneous domains are not directly comparable, which brings difficulty for blending. To determine to combine ranking lists based on relevance, to maximize the DCG and preserve the ranking order in each domain. To achieve such a combined ranking list with the ranking lists from multiple domains, a mono tonic transformation is applied to the rank scores in each domain, such that the transformed scores become comparable. Learning the monotonic transformation that could achieve the optimal blended list, is formulated as a quadratic programming problem. In the paper, we focus on the simple case where a linear transformation is considered. We evaluated the novel learning -to-blend approach with real -world data sampled from a commercial search and observed promising results of 1.18% DCG10 gain. this paper. Adapting query -type dependent monotonic transformation is a direction to explore in future work . Advanced monotonic non -linear transformation can also be explored in future work. Also explorin g the application to more than two domains is of great interests. [1] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt [2] Zhe Cao, Tao Qin, Tie -Yan Liu, Ming -Feng Tsai, and Hang [3] C. Cortes, M. Mohri, and A. Rastogi. Mag nitude -preserving [4] Cynthia Dwork , Ravi Kumar , Moni Naor , D. Sivakuma . [5] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efficient [6] J. Guiver and E. Snelson. Learning to rank with Soft Rank [7] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain  X  X ased [8] T. Joachims. Optimizing search engines using clickthrough [9] Yu -Ting Liu, Tie -Yan Liu, Tao Qin, Zhi -Ming Ma, Hang Li. [10] Taesup Moon, Alex Smola, Yi Chang and Zhaohui Zheng. [11] J. Xu and H. Li. Adarank: a boosting algorithm for [12] Zhaohui Zheng, Hongyuan Zha, and Gordon Sun. Query -
