 Computing the similarity between unstructured records is a fundamental function in multiple applications. Approxi-mate string matching and full text retrieval techniques do not show the best performance when applied directly, since the information are limited in unstructured records of short record length. In this paper, we propose a novel probabilistic correlation-based similarity measure. Rather than simply conducting the exact matching tokens of two records, our similarity evaluation enriches the information of records by considering the correlations of tokens. We define the prob-abilistic correlation between tokens as the probability that these tokens appear in the same records. Then we compute the weight of tokens and discover the correlations of records based on the probabilistic correlations of tokens. Finally, we present extensive experimental results to demonstrate the effectiveness of our approach.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications General Terms: Measurement, Performance Keywords: Record Similarity, Probabilistic Correlation
Millions of unstructured records exist in database and in-formation systems, such as personal information manage-ment systems (PIM) and scientific literature digital library (CiteSeer). Unstructured records are often text strings of short length, thus, we can apply approximate string match-ing techniques such as edit distance [4] and q-grams [8] to measure the similarity. These character-based approaches can only capture limited similarity and fail in many cases such as various word orders and data missing. We can also treat each unstructured record as a text document and apply full text retrieval techniques to measure the record similar-ity by using cosine similarity with tf*idf [1, 2]. However, due to the short length of unstructured records, most words appear only once in a record, i.e. the term frequency (tf) is 1. Therefore, only the inverse document frequency (idf )[7] takes effect in the tf*idf [6] weighting scheme and no local features of each record are considered. Moreover, the pop-ular similarity measure used for full text, cosine similarity , is based on the assumption that tokens are independent of each other, and the correlations between tokens are ignored. Due to various information formats of unstructured records such as abbreviation and data missing, latent correlations of records can hardly be detected by considering the exact matching tokens only.

Consider two citation records with different author repre-sentations of  X  X udipto Guha, Nick Koudas, Amit Marathe, Divesh Srivastava X  and  X  X . Guha, et al. X  respectively. By using the cosine similarity which is based on the dot prod-uct of two record vectors, we have only one exact matching token  X  X uha X  and the similarity value is low. Even worse, there is no exact matching token at all between the different representations of the same conference,  X  X ery Large Data Bases X  and  X  X LDB X , and the cosine similarity value is 0 between these two representations.

Motivated by the unsuitability of string matching and full text retrieval techniques in measuring similarity between un-structured records, in this pa per, we mainly focus on devel-oping the similarity metrics based on the token correlations. The correlations between tokens are investigated based on the probability that tokens appear in the same records, and then utilized in two aspects, i.e. intra-correlation and inter-correlation. The intra-correlations consider the correlations of tokens in a record, and are utilized in the weighting of tokens. Rather than assigning equivalent term frequency to tokens, we develop the discriminative importance of each to-ken based on the degree of the token correlation with other tokens in a record. The inter-correlations represent the cor-relations of tokens between two records, which can further discover the correlations of records in addition to exactly matched tokens. Based on the correlations of tokens, we can perform the similarity evaluation on more complicate data records, e.g. with abbreviation and data missing. Our contributions in this paper are summarized as follow:
Correlations among tokens do exist, for example, the to-ken  X  X nternational X  has high probability to appear together with the token  X  X onference X  in citation records. In this sec-tion, we develop and model the correlations between tokens by considering the conditional probability.
We consider a word token based correlation. The records are cut into word tokens, and the correlation between the tokens are computed. The conditional probability is used to model the probability that tokens appear together in a record. The conditional probability is defined as follows: where Pr( t i t j ) denotes the probability that token t t appear in the same record, which can be estimated as token t i and t j appear df ( t i t j ) divided by the total number of records N [5]. The conditional probability between tokens t and t j is asymmetric, i.e. Pr( t i | t j ) =Pr( t j | t i similarity between records is often regarded as a symmetric relationship. Thus, we define the probabilistic correlation of tokens in a symmetric way: where Pr( t i t j ) is the probability that tokens t i and t in the same record. When tokens t i and t j match exactly, i.e. t i and t j are the same token, we have cor ( t i ,t which means that the probability of t i and t j belonging to the same record is equal to 1.
 Definition 1. Dictionary space . Given a set of records R ,with m tokens appearing in the records, a dictionary space is a graph G d = &lt;V d ,E d &gt; , where each vertices t V d denotes the token t i ,eachedge e d ij  X  E d denotes the probabilistic correlation of t i and t j , and the edge weight of e ij is equal to cor ( t i ,t j ).

Note that some word tokens in the dictionary highly corre-late with each other. In fact, some word tokens, for example the page number tokens  X 636 X  and  X 647 X  of citation records, always appear together in the same records. In other words, we have cor ( t i ,t j )=Pr( t i | t j )=Pr( t j | t i )=1,whichim-plies that tokens t i and t j always appear in the same records. Therefore, we can merge these kinds of word tokens together into a new token, namely phrase token .

A phrase token t p is a token comprising several tokens which always appear together in the same records. For any token t i in t p ,wehavePr( t p )=Pr( t i ) because all of the tokens in the phrase always appear in the same records and have the same probability Pr( t i ). Foranyothertoken t l the dictionary, we have Pr( t l t p )=Pr( t l t i ), which implies that the probability of any token t l appearing together with token t i of phrase token t p in a record is equal to the prob-ability that token t l appears together with phrase token t Then, we can have the correlation between the new phrase token t p and any other tokens t l in the dictionary:
By merging word tokens into phrase tokens, we can reduce the size of the dictionary and the records. Furthermore, the average number of tokens in each record is also reduced by using the phrase token representation. Since all the tokens with cor ( t i ,t j ) = 1 are merged in a new phrase token, we have the following property of token correlations.
Property 1. Consider the correlation between token t i and t in a dictionary space with phrase token. If t i and t j are not exactly matching, i.e. t i = t j ,thenwehave 0 &lt;cor ( t
The probabilistic correlation between two tokens implies the probability that these two tokens belong to the same record. Once the probabilistic correlations between tokens are investigated, we can utilize the correlations in two per-spectives, i.e. intra-correlation and inter-correlation .
The intra-correlation indicates the correlation of tokens in a single record. As shown in Figure 1, the tokens in a record might correlate with each other. A token with more and higher correlation to others implies that this token is more important in the current record where the token is. Therefore, the correlations of tokens in a record can be used in the feature weighting of the record.

The inter-correlation indicates the correlation of tokens between two records. For example, consider the token  X  X ery X  in a record r 1 and the token  X  X LDB X  in a record r 2 .Proba-bilistic correlation exists between token  X  X ery X  and  X  X LDB X , since both tokens may appear in the same records frequently throughout the entire dataset. The correlation between to-ken  X  X ery X  and  X  X LDB X  implies the probability that these two tokens belong to the same record. Considering all token correlations between r 1 and r 2 , we can estimate the prob-ability that these two records describe the the same entity, i.e. the similarity between record r 1 and r 2 .
In this section, we illustrate our probabilistic correlation-based record similarity measure. First, we discuss the weight-ing scheme of tokens in the reco rds with token correlations. Then we introduce our correlation similarity function which is also based on the correlation between the tokens.
The term frequency is adopted in full text retrieval as local features of each document. However, term frequency is probably equal to 1 in most cases of short unstructured records, which indicates that only idf takes effect in tf*idf and no local features of records are considered.
Instead of term frequency (tf) with w i = 1 in most cases, we propose a new local weighting scheme of tokens in a record, correlation weight . Since we use the conditional probability as the correlation between tokens, tokens with more and higher correlations to the others in the record are more likely to represent that record and can be treated as an important local feature. Therefore, we introduce the new token weighting scheme, which is based on the degree of the token correlation with other tokens in the same record.
Definition 2. Correlation weight . Given a record space r with an original weight w i of each token t i correlation weight of token t i in the record r is defined as: where cor ( t j ,t i ) denotes the probabilistic correlation be-tween tokens t i and t j in the record, and | r | means the total number of tokens m in the record.

The original weight w i of each token t i can be the term frequency .The correlation weight denotes the trustability and importance of the token t i in the record. A higher cor-relation weight implies a higher probability that if token t appears in the record, other tokens t j will also appear in the records. In other words, the more tokens t j that show high correlation with token t i , the higher the probability token t is relevant to the record. Furthermore, we can further combine the correlation weight with global statistic weights in the weighting scheme, for example, inverse document fre-quency (idf) . Similar to the tf*idf approach, the cow*idf weighting is defined as, cow  X  idf ( t j )= cow ( t j )  X 
The cosine similarity function is based on the exact match-ing tokens, thus tokens of various representation, for exam-ple  X  X ases X  and  X  X LDB X , are treated as two totally different tokens without any correlation. Note that the relationship between tokens of two records are single-to-single in cosine similarity , in other words, one token in record r 1 is corre-lated with no more than one token in the other record r 2
In this study, unstructured records are always short in length with limited information, thus we investigate the la-tent similarity based on token correlations of two records. In our probabilistic correlation, one token may be corre-lated with multiple tokens in the other record. In order to capture these tokens correlations in the similarity evalua-tion, we consider three kinds of inter-correlations of tokens between two records. The first correlation is between exact matching tokens, for example, the correlation between  X  X on-ference X  of two records in Figure 2. The second correlation is between two tokens which appear in both records, for ex-ample, the correlation between  X  X onference X  and  X  X LDB X  with dotted lines in Figure 2. Since these two tokens appear in both two records and the correlations have been detected respectively by the first kind o f correlations, we do not have to account for them again. The third correlation is between two tokens at least one of which does not appear in both two records. This kind of probabilistic correlation can contribute to finding the correlation of two records.

According to the different cate gories of inter-correlations, we can further constrain the probabilistic inter-correlation between tokens t i ,t j in two records r 1 ,r 2 , respectively, as:
Let M be all the pairs of tokens with inter-correlations of two records described in (5), which satisfy the user specified minimum correlation threshold cor ( t i ,t j )  X   X  .Let M the tokens of r 1 in the correlation set M and M 2 be all the tokens of r 2 in the correlation set M ,wehave M 1  X  M 2 = M . Thus, the correlation similarity function can be defined as
Definition 3. Correlation similarity .Giventwodata records r 1 and r 2 with m 1 and m 2 tokens respectively, the correlation similarity of r 1 and r 2 is defined as: where w i ,w j denote the weight of token t i ,t j respectively, and cor ( t i ,t j ) denotes the probabilistic correlation between t and t j in the token correlation set M of r 1 and r 2 .
Unlike the single-to-single correlations of exact matching pairs in cosine similarity , our correlation set M defines a multiple-to-multiple correlation of tokens as shown in Fig-ure 2. In order to normalize the similarity value, we use r 1  X  M 1  X  r 2  X  M 2 rather than r 1  X  r 2 ,where r 1 represents all the tokens and their weights in the record r i.e. r 1 = { ( t i ,w i ) | t i  X  r 1 } ,and M 1 denotes all the tokens of r 1 in the correlation set M , i.e. M 1 = { ( t i ,w i t  X  r 1 ,t j  X  r 2 , ( t i ,t j )  X  M } .Notethat r 1  X  M 1 the tokens in r 1 and their correlations with tokens in r r  X  M 1 denotes all the tokens in r 1 existing a token t j  X  with cor ( t i ,t j ) = 1, in the other words, all the tokens in r with exact matching tokens in r 2 in cosine similarity .
Our correlation similarity function relaxes the constraint of token exact matching in the cosine similarity function, by considering the further inter-correlations of tokens between two records. Therefore, the corre lation-based similarity is a generalization of the cosine similarity .

Theorem 1. The correlation similarity function is a gen-eralization of the cosine similarity function. If the minimum correlation threshold  X  =1 , then the correlation similarity is equivalent to the cosine similarity.

The probabilistic correlation-based similarity is effective, especially in evaluating records with data missing. For in-stance, we use  X  X uha et.al. X  to represent  X  X uha, S., Koudas, N., Marathe, A., Srivastava D. X , if the author list is too long in citation records. Unfortunately, these kinds of high simi-larity with data missing are difficult to address by the tradi-tional token matching approaches such as cosine similarity . In our probabilistic correlation-based similarity, we investi-gate the correlation of  X  X uha X  to other authors, since they may appear together in other records without data missing. Then we utilize these token correlations to discover the sim-ilarity of  X  X uha X  and  X  X uha, S., Koudas, N., Marathe, A., Srivastava D. X  in records r 1 ,r 2 respectively.

Furthermore, our approach can address the more compli-cated problem of the abbreviation similarity. For example, the similarity between  X  X LDB X  and  X  X ery Large Data Bases X  is not easy to detect by directly using techniques such as co-sine similarity . However, the words and their abbreviation may appear in the same record s frequently, which means that high probabilistic correlation exists between them. As theexampleshowninFigure2,wecanusethecorrela-tions between token  X  X LDB X  and {  X  X ery X ,  X  X arge X ,  X  X ata X ,  X  X ases X  } to find the similarity between  X  X LDB Conference X  and  X  X ery Large Data Bases Conference X .
The experiments run on the Cora [3] dataset. We merge all the information in each record together in an unstruc-tured record. In order to simulate and test a dirty dataset with different data missing rates, we also remove the words in the records randomly according to the user specified miss rate. For example, a dataset with 0.2 missing rate means that 20% of data are missing in the dataset. For each pair of records in the dataset, we compute the similarity to de-termine whether or not these two records describe the same entity. We adopt f-measure with precision and recall [9] as the criteria to evaluate the effec tiveness of different similar-ity measures.

We compare three similarity measures, including our prob-abilistic correlation-based similarity with phrase tokens (for short,  X  X orrelation with phrase X ), q-grams based cosine sim-ilarity with tf*idf ( X  X osine with q-grams X ), and word token based cosine similarity with tf*idf ( X  X osine with word X ).
Cora dataset . First, we present the results of three ap-proaches in the Cora dataset. The minimum correlation threshold of correlation similarity is  X  =0 . 2. The results in Figure 3 demonstrate the superiority of our correlation-based similarity measure. As shown in Figure 3, the corre-lation similarity achieves higher f-measure than the cosine similarity . The results of word tokens and q-grams in the cosine similarity approaches are quite similar.

The experiments show that our probabilistic correlation-based similarity measure achieves higher accuracy than the cosine similarity approaches. By using the probabilistic cor-relations of tokens, we can further find the latent correlated records and consequently improve the accuracy of similarity measure. The q-grams does not improve the performance of cosine similarity comparing with the word tokens, since the q-grams can not contribute more than the word tokens in dealing with complex data formats such as abbreviations. Figure 4: Accuracy with different data missing rates.
Data missing rate. Finally, we evaluate the perfor-mance of three similarity measures under different data miss-ing rates of Cora . Figure 4 reports the best results of each measure under several missing rates. For example, a dataset with 0.2 missing rate means that 20% of data are missing in the dataset. Our correlation-based similarity achieves a higher f-measure under all studied missing rates. When the data missing rate is high, e.g. 0.4, too much tokens are re-moved from the records and th e token correlations can not be constructed accurately. Thus, the accuracy of correlation similarity drops largely as well as the cosine similarity.
The experimental results demonstrate the superiority of our correlation-based approach, especially under the dataset with data missing. Due to the data missing, e.g.  X  X ery Data X  with a word  X  X arge X  missing, the exact matching pairs of to-kens reduce, which makes it difficult for cosine similarity to find the correlations betw een records. Even worse, the q-grams approach reserves the connections between words, for example, the token  X  X  D X  of  X  X ery Data X . However,  X  X  D X  does not appear in the original data  X  X ery Large Data X  without data missing. Since the sizes of records are prob-ably small, such an error token affects the similarity value largely. Therefore, the q-grams approach conducts a worse performance than the word token approaches, due to the error tokens caused by data missing.
In this paper, we propose a novel similarity measure for unstructured records based on the token correlations. We define the probabilistic correlation between two tokens as the probability that these tokens appear in the same records. A feature weighting scheme is performed based on the intra-correlation of tokens in a record. Moreover, we consider the inter-correlation of tokens in two records in our correlation similarity function. In the analysis, we show that our prob-abilistic correlation-based similarity measure is effective in dealing with various information formats such as abbrevia-tion and data missing. Furthermore, the experimental re-sults also verify that our approach achieves higher accuracy than that of the cosine similarity on measuring the similarity of unstructured records.
 Acknowledgement. Funding for this work was provided by National Grand Fundamental Research 973 Program of China under Grant No. 2006CB303000, an d the Hong Kong RGC grants DAG05/06.EG03.
