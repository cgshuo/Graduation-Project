 Mining generator patterns has raised great research interest in re-cent years. The main purpose of mining itemset generators is that they can form equivalence classes together with closed itemsets, and can be used to generate simple classification rules according to the MDL principle. In this paper, we devise an efficient algo-rithm called StreamGen to mine frequent itemset generators over a stream sliding window. We adopt a novel enumeration tree struc-ture to help keep the information of mined generators and the bor-der between generators and non-generators, and propose some op-timization techniques to speed up the mining process. We further extend the algorithm to directly mine a set of high quality classifica-tion rules over stream sliding windows while keeping high perfor-mance. The extensive performance study shows that our algorithm outperforms other state-of-the-art algorithms which perform simi-lar tasks in terms of both runtime and memory usage efficiency, and has high utility in terms of classification.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms, Experimentation, Performance Stream Data, Sliding Window, Itemset Generator, Feature Selec-tion, Classification Frequent itemset mining is one of the essential data mining tasks. Since it was firstly proposed in [1], various algorithms have been proposed, including Apriori [2] and FP-growth [12] algorithms. Many studies have also demonstrated its application in feature se-lection and associative classifier construction [18, 9, 14, 17, 3, 26, 8, 24, 5, 6].

If we divide the set of all itemset patterns into a set of equiva-lence classes, where each equivalence class contains a set of item-set patterns which are supported by the same set of input transac-tions, the closed itemsets are those maximal ones in each equiv-a subset of all itemset patterns, and thus it is possible to identify some parts of search space which are unpromising to generate any closed itemsets and can be pruned. Thus, closed itemset mining can be potentially more efficient than all itemset mining. Due to the concise representation and high efficiency, many algorithms for mining frequent closed itemsets have been proposed [20, 21, 29, 19, 25, 11].

In each equivalence class of itemset patterns, if we call the min-imal ones itemset generators, similarly we get that the set of all itemset generators is a subset of all itemset patterns, and itemset generator mining can be potentially more efficient than all itemset pattern mining too. It is also evident that the average length of item-set generators tends to be smaller than that of all itemset patterns (or closed itemset patterns). Since one of the important applica-tions of frequent itemset mining is to be used for feature selection and associative classifier construction. According to the Minimum Description Length (MDL) Principl e, generators are preferable in tasks like inductive inference and classification among the three types of itemset patterns (namely, all itemset patterns, closed item-set patterns,
Recently stream data became ubiquitous. One popular form of such kind of data is a sequence of transactions arriving in order continuously. They usually come at a high speed and have a data distribution that often evolves with time. Due to the unique char-acteristics of the stream data, it is not feasible to simply adapt the algorithms originally designed for static datasets to stream data. Hence several efforts have been devoted to frequent itemset mining and closed itemset mining over stream data [23, 28, 7, 13]. How-ever, to our best knowledge, there exists no algorithm which mines frequent itemset generators over a stream sliding window, while such an algorithm is very useful in building associative classifiers over stream data.

In this paper, we introduce an efficient algorithm, StreamGen, to mine frequent itemset generators over sliding windows on stream data. It adopts the FP-Tree structure to concisely store the transac-tions of the current window, and devises a novel enumeration tree structure to keep all the mined generators and their border to the non-generators. In the meantime, some optimization techniques are also proposed to accelerate the mining process. To demonstrate its utility, we further extend StreamGen to directly mine classifica-tion rules over a stream sliding window. The experimental study shows that StreamGen is efficient and achieves high classification accuracy.
The contributions of the paper are summarized as follows.
The remainder of this paper is organized as follows. In Section 2, we introduce the related work. In Section 3, we present the problem statement. The details of StreamGen are discussed in Section 4. Section 5 describes the extended algorithm to mine classification rules directly over a stream sliding window. The empirical results are shown in Section 6, and we conclude the paper in Section 7.
ZIGZAG [23] is an algorithm designed for mining all frequent itemsets over a sliding window. The algorithm supports batch up-date, and hence outperforms other algorithms updating one trans-action at a time when the batch size is large. [28] also discusses fre-quent itemset mining from transactional data streams. [7] proposes an algorithm called MOMENT to mine frequent closed itemsets over a stream sliding window. It adopts the FP-Tree structure to compress transactions in the current window, and an enumeration tree to maintain the mined closed itemsets. While CFI-Stream pro-posed in [13] is another algorithm which only keeps closed itemsets in its enumeration tree to further compress the storage and acceler-ate the mining process. To our best knowldge, currently there is no algorithm which mines frequent itemset generators over a stream sliding window, although there exist several frequent itemset gener-ator mining algorithms for static dataset, such as GR-Growth [15], DPM [16], and an algorithm for incremental mining of itemset gen-erators, such as [27].

One important application of frequent itemset mining is feature selection for building classification models. There are several pieces of work which try to directly mine a set of itemset patterns for clas-sification. The HARMONY algorithm [26] tries to directly mine  X  best rules for each transaction, and use them for building a rule-based classifier. [8] proposes another algorithm to mine top-K as-sociative classification rules on gene data. [5] proves that informa-tion gain should be preferred to confidence in mining classification rules, and proposes an algorithm using information gain to select rules. [6] further devises an algorithm called DDPMine to directly mine rules using a sequential covering paradigm. There is no al-gorithm which directly mines a set of itemset generators for clas-sification. [10] tries to mine sequential generators for classifying sequential data and achieves good accuracy.
Given a set of items  X  ={  X  1 ,  X  2 , ... ,  X   X  }, a transaction database consists of a set of transactions and a transaction is a tuple  X &gt; ,where  X  X  X  X  is the transaction identifier (or time stamp), and  X   X   X  . An itemset (i.e., a set of items)  X  is said to be contained in a transaction &lt; X  X  X  X  ,  X &gt; if  X   X   X  holds. The number of transac-tions containing itemset  X  is called the absolute support of the percentage of transactions that contain  X  is called the relative support . In the following we will use support to denote absolute support and relative support interchangeably when there is no con-fusion, and use  X  X  X  X   X  to denote the support of itemset  X  .
Given a user specified minimum support threshold  X  X  X  X   X  X  X  X  ,we have the following definitions.

D EFINITION 1. An itemset  X  is frequent if and only if  X  X  X  X 
D EFINITION 2. A frequent itemset generator (or shortly gener-ator)  X  is a frequent itemset where there is no itemset  X   X   X   X   X   X  and  X  X  X  X   X   X  =  X  X  X  X   X  .  X 
D EFINITION 3. An unpromising itemset is a frequent non-generator itemset.  X 
C OROLLARY 1. A frequent itemset  X  is unpromising iff such that  X   X   X   X  =  X   X   X  X  X  1 and  X  X  X  X   X   X  =  X  X  X  X   X  .  X 
The main task of this work is to mine the complete set of fre-quent itemset generators from the most recent sliding window of  X  transactions in a transactional data stream . To show the util-ity of itemset generator mining, we will also discuss how to mine generator-based classification rules over a sliding window. Figure 1 shows a running example of transactional data stream with a sliding window size of 4.
 Figure 1: A Running Example of Transactional Stream Data. We introduce the StreamGen algorithm in details in this section. First, we present and prove some common properties which will be used in the algorithm design. Then, we introduce the FP-Tree structure for storing transactions in the current sliding window in Section 4.1, the enumeration tree structure in Section 4.2, the ADD operation in Section 4.3, and the REMOVE operation in Section 4.4, respectively. Finally, we will discuss how to combine the ADD and REMOVE operations to get the integrated StreamGen algo-rithm for mining itemset generators over a stream sliding window in Section 4.5.

Based on the definitions in Section 3, we have the following properties.

T HEOREM 1. A frequent itemset  X  is a generator iff there exists no subset  X   X  such that  X   X   X   X  =  X   X   X  X  X  1 and  X  X  X  X   X   X  =  X  X  X  X 
P ROOF . Sufficiency . Assume there does not exist a subset such that  X   X   X   X  =  X   X   X  X  X  1 and  X  X  X  X   X   X  =  X  X  X  X   X  , we prove that itemset must be a generator.

Suppose  X  is not a generator, then there must exist a subset that  X  X  X  X   X  =  X  X  X  X   X   X  X  X  . According to assumption that there does not exist a subset  X   X  such that  X   X   X   X  =  X   X   X  X  X  1 and  X  X  X  X  can conclude that  X   X   X  X  X   X  &lt;  X   X   X  X  X  1 ,  X  X  X  X   X   X  X  X  =  X  X  X  X  at least one subset  X   X  such that  X   X  X  X   X   X   X   X   X  and  X   X   X   X  =  X   X   X  X  X  1 However, according to the Apriori principle, we also know that  X  X  X  X   X   X  X  X   X   X  X  X  X   X   X   X   X  X  X  X   X  must hold. Hence we further conclude that  X  X  X  X   X   X  =  X  X  X  X   X  , which contradicts the assumption. Necessity . It can be easily derived from Definition 2.

T HEOREM 2. Given a generator  X  , any subset of  X  would be also a generator.

P ROOF . Assume  X  is a generator and has a non-generator subset . Then there must exist a generator subset  X   X  X  X  of  X   X  such that  X  X  X  X   X   X  X  X  =  X  X  X  X   X   X  . We could further conclude that for the itemset  X  =  X   X  (  X   X   X   X   X  X  X  ) we have  X   X   X  and  X  X  X  X   X  =  X  X  X  X   X  , thus, we have that  X  is not a generator, which contradicts the assumption.
T HEOREM 3. Given an unpromising itemset  X  , any superset of  X  must be either unpromising or infrequent.

P ROOF . It can be easily derived from Theorem 2 and the Apriori property.
 The above three theorems were used in several studies such as [4]. Theorems 2 and 3 help define the border between generators and non-generators, and form the foundation for the enumeration tree used in our algorithm. While with Theorem 1 we can check whether an itemset  X  is a generator or not by simply checking all the item-sets which are a subset of  X  andhavealengthof  X   X   X  X  X  1 .
T HEOREM 4. Given an itemset  X  and its superset  X   X  =  X   X  {  X  } , then for any itemset  X  we have either  X   X   X   X   X   X  =  X   X   X   X   X   X   X   X   X   X   X  =  X   X   X   X   X  +1 .
 P ROOF . The proof of this theorem is obvious.
 The new property shown in Theorem 4 can be easily proved. With theorem 4, we could know whether the state of an itemset will change to another or not when a new transaction  X  arrives, and it also helps the intersection calculation which will be introduced in Section 4.3.

T HEOREM 5. Given two itemsets  X  and  X  , we have  X   X   X  if and only if  X   X   X   X   X  &gt;  X   X   X  X  X  1 .

P ROOF . Sufficiency . Assume  X   X   X   X   X  &gt;  X   X   X  X  X  1 ,wehave =  X   X   X  , thus  X   X   X  holds.

Necessity . Assume  X   X   X  ,wehave  X   X   X   X   X  =  X   X   X  , thus &gt;  X   X   X  X  X  1 holds.

Theorem 5 is used later in both the ADD and REMOVE opera-tions introduced in Sections 4.3 and 4.4. Like the Moment algorithm [7], we also adopt a variant of the FP-Tree structure [12] to help maintain a concise representation of the transactions in the current sliding window. The use of FP-Tree structure not only reduces the need for memory, but also accelerates some operations such like support counting, etc.
 The FP-Tree structure adopted in our algorithm is a variant of the CET tree used by Moment. That is, it keeps not only the frequent items, but also the infrequent items for further processing. Besides, the items are not sorted by supports, but in lexicographic order to avoid re-sorting the items each time as the sliding window moves. Note that with an FP-Tree structure it is no longer necessary to keep any transactions in the sliding window, and all the information could be retrieved or calculated easily from FP-Tree.

Figure 2 shows the FP-Tree built from the first sliding window of the running example shown in Figure 1. We notice that a trans-action ID table is used to map all those transactions in the current sliding window to those FP-Tree nodes which contain the first item in the sorted itemset of each transaction.

In the algorithm we use an enumeration tree to help maintain the whole relationship between itemsets and the mined generators. The idea of using enumeration tree was inspired by [7]. There are three types of tree nodes in the enumeration tree.
Note that we do not need the gateway node used in [7], since ac-cording to Theorem 2 there is no non-generator node whose itemset is a subset of the itemset of some generator nodes. Furthermore, the existence of infrequent and unpromising nodes forms the border of the generators to the non-generators in the enumeration tree.
To accelerate the checking operation on whether an itemset is a generator or not, we adopt a hash table structure whose key is the sum of items in one itemset, and whose value is a pointer to the enumeration tree node which contains the itemset. Each node in the same level is stored in a same hash table. Hence it is very useful to focus on the desired nodes by only checking one hash table whose level is smaller by one than the level of current node.

As described above, instead of pruning all infrequent nodes and unpromising nodes from the enumeration tree, we only prune those infrequent nodes and unpromising nodes which have an infrequent or an unpromising uncle node. The reason we adopt this strategy is that the  X  X omplete pruning X  may cost a lot when either adding or removing a transaction, and the saved time by  X  X omplete pruning X  could not even offset the time used in  X  X omplete pruning X  itself.
Figure 3 shows the enumeration tree built from the first sliding window of the running example. An ellipse with solid line style indicates a generator node, an ellip se with dotted line style indi-cates an unpromising node, and a rectangle with dotted line style indicates an infrequent node. Note that the empty set  X  is treated as a valid itemset in this paper, and the minimum absolute support threshold here is set at 2.

Figure 3: The Enumeration Tree of the First Sliding Window.
The ADD operation in StreamGen mines and maintains the set of frequent generators when a new transaction arrives. Before we introduce the details of the ADD operation, we first prove some properties which are helpful in understanding the algorithm.
T HEOREM 6. During the ADD operation, a generator node would never change its type.

P ROOF . Given a generator node  X  whose corresponding itemset is  X  X  X  X  X  X  X  X  X  X   X  , we have that for any itemset  X   X   X  X  X  X  X  X  X  X  X  X   X  ,  X  X  X  X  holds. If the new transaction is a superset of  X  X  X  X  X  X  X  X  X  X   X  , the support of  X  X  X  X  X  X  X  X  X  X   X  and all its subsets would all increase by one, hence would remain a generator node. While if the new transaction is not a superset of  X  X  X  X  X  X  X  X  X  X   X  , only some subsets of  X  X  X  X  X  X  X  X  X  X   X  would get their support increased by one and thus  X  still remains a generator node.

This theorem indicates that we only need to care about the sup-port update of a generator node, but do not need to consider the change of its node type since it would never happen.

T HEOREM 7. If the itemset of an unpromising node is a subset of the new transaction, the node would not change its type.
P ROOF . A node  X  whose itemset is  X  X  X  X  X  X  X  X  X  X   X  is unpromising means there exists a subset  X  of  X  X  X  X  X  X  X  X  X  X   X  satisfying  X  X  X  X   X  X  X  X  X  X  X  X  X  X   X  .If  X  X  X  X  X  X  X  X  X  X   X  is a subset of the new transaction, the support of both  X  X  X  X  X  X  X  X  X  X   X  and its subset  X  will be all increased by one, hence node  X  remains an unpromising node.

T HEOREM 8. Given a new transaction  X  , if an unpromising node  X  whose itemset is  X  X  X  X  X  X  X  X  X  X   X  becomes a generator node it must satisfy  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  =  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 .

P ROOF .If  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  &lt;  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 , then there does not exist any subset  X  of  X  X  X  X  X  X  X  X  X  X   X  satisfying  X   X   X  =  X   X  X  X  X  X  X  X  X  X  X  gets its support increased by one. Hence  X  would remain unpromis-ing according to Theorem 1. If  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  &gt;  X   X  X  X  X  X  X  X  X  X  X  1 , we could know that its type would also not change according to Theorem 7.

T HEOREM 9. Given a new transaction  X  , if a node  X  whose itemset is  X  X  X  X  X  X  X  X  X  X   X  satisfies  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  &lt;  X   X  X  X  X  X  X  X  X  X  X  the state of node  X  and all its descendant nodes will remain un-changed.

P ROOF . We could easily prove that the node type would not change according to Theorems 6 and 8. Furthermore, we can prove that all its descendants would also satisfy the condition of this the-orem according to Theorem 4. Hence the state of node  X  and all its descendant nodes will remain unchanged.

T HEOREM 10. When an infrequent node becomes a generator node, all its newly added child nodes will be either infrequent or unpromising.

P ROOF . When an infrequent node  X  becomes a generator node, it must satisfy  X  X  X  X   X  =  X  X  X  X   X  X  X  X  . Hence any new frequent child node of  X  ,  X   X  , must satisfy  X  X  X  X   X  =  X  X  X  X   X   X  and thus is an unpromis-ing node; and all its infrequent children are infrequent nodes.
Theorem 10 is very useful since it guarantees that we do not need to check the new child nodes if they are generator nodes, which saves much time.

After introducing the properties related to itemset generators, we will discuss how to explore these properties to design an efficient generator mining algorithm (i.e., the ADD operation) upon receiv-ing a new transaction.

Before we elaborate on the ADD operation, we first introduce a sub-procedure,  X  X  X  X  X  X  X  X  X  X  (  X  ) , which is shown in Algorithms 1 and describes how to explore a node  X  . Note that the algorithm calls a function of  X  X  X  X  X  X  X  X  X  X  X  X  (  X ,  X  ) which is used to create a new child node of node  X  with an itemset of  X  X  X  X  X  X  X  X  X  X   X   X  {  X  }. The algo-rithm creates the child node by merging the itemset of node the lexicographically largest item of the itemset of one of lings which have the same parent as node  X  . It also re-creates the child nodes which were pruned out due to the existence of an in-frequent or unpromising uncle node, and hence extends the border of the enumeration tree. Note that we only need to re-generate the child nodes with only one level lower, since Theorem 10 assures that all the newly added child nodes of a generator node which are infrequent nodes would become either infrequent or unpromis-ing. When an unpromising node  X  becomes a generator node af-ter receiving a new transaction  X  , it must satisfy  X   X  X  X  X  X  X  X  X  X  X  =  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 according to Theorem 8. Hence any new fre-quent child node  X   X  would satisfy  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X   X  =  X   X  X  X  X  X  X  X  X  X  X   X  1 or  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X   X  &lt;  X   X  X  X  X  X  X  X  X  X  X   X   X   X  X  X  1 according to Theo-rem 4. If we have  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X   X  &lt;  X   X  X  X  X  X  X  X  X  X  X   X   X  child can be safely skipped according to Theorem 9. If we have  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X   X  =  X   X  X  X  X  X  X  X  X  X  X   X   X   X  X  X  1 and the new child node is a generator, it will be traversed later.
 Algorithm 1 :  X  X  X  X  X  X  X  X  X  X  (  X  )
Algorithm 2 gives the details of the ADD operation, from which we see that it uses a right-to-left, top-down updating strategy. The top-down strategy is necessary due to the fact that the checking of whether a node is a generator node or not must examine those nodes which are on a higher level in the enumeration tree and hence those nodes must have been updated before updating the current node. While a right-to-left strategy is used due to the fact the uncle nodes used in Algorithms 1 are all on the right side to the current node and hence we must assure those nodes have already been created if they do not exist in the last sliding window.

The algorithm determines how to deal with the update accord-ing to whether  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  is equal to, less than, or great than  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  X  X  1 . Algorithm 2 :  X  X  X  X  (  X , X  )
In addition, there is no need to calculate the itemset intersection every time. Since according to Theorem 4 we can always calculate the intersection incrementally from the last intersection, and hence save a lot of time. Note that in our algorithm we do not prune all the infrequent and/or unpromising nodes whose itemsets are super-sets of the itemsets of some infrequent and/or unpromising nodes due to the potential cost of rebuilding those nodes in a later slid-ing window and the significant pruning cost. The sub-procedure  X  X  X  X  X  X  X  X  X  X  X  X  (  X  ) is used for updating the type information of current node.

Figure 4 illustrates the enumeration tree built from the first slid-ing window after adding a new transaction (i.e., the transaction with an ID of 5).
 Figure 4: The Enumeration Tree after Adding a Transaction.
Finally, we could easily prove the number of nodes in the enu-meration tree does not decrease during the ADD operation.
We will introduce the REMOVE operation in this section, which mines and maintains frequent itemset generators upon removing an old transaction.

T HEOREM 11. During the REMOVE operation, an unpromis-ing node would never change its type unless it becomes infrequent or is pruned.

P ROOF . Given an unpromising node  X  , we know that there ex-ists an itemset  X  such that  X   X   X  X  X  X  X  X  X  X  X  X   X  and  X  X  X  X   X  =  X  X  X  X  which means a removed transaction  X  either contains both  X   X  X  X  X  X  X  X  X  X  X   X  or contains neither of  X  and  X  X  X  X  X  X  X  X  X  X   X  . Thus, by remov-ing transaction  X  , the relationship between  X  and  X  X  X  X  X  X  X  X  X  X  mains unchanged, namely,  X   X   X  X  X  X  X  X  X  X  X  X   X  and  X  X  X  X   X  =  X  X  X  X   X  X  X  X  X  X  X  X  X  X  still hold.

T HEOREM 12. By removing a transaction  X  , a generator node  X  would not change its type if it satisfies  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  &gt;  X   X  X  X  X  X  X  X  X  X  X   X  1 .

P ROOF .Since  X  is a generator node, there does not exist any subset  X  such that  X   X   X  =  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 and  X  X  X  X   X  =  X  X  X  X  dition of  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  &gt;  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 means  X  thus, by removing transaction  X  , not only the support of the support of all its subsets would be decreased by one. Hence the node  X  would not change its type unless it becomes infrequent.
T HEOREM 13. By removing a transaction  X  , a generator node  X  would become an unpromising node only when it satisfies  X   X  X  X  X  X  X  X  X  X  X  =  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 and  X  X  X  X  (  X  X  X  X  X  X  X  X  X  X   X   X   X  )=  X  X  X  X   X  +1 .
P ROOF . As a generator node  X  becomes unpromising after re-moving transaction  X  , according to Theorem 1, there must exist at least one subset  X  such that  X   X   X  =  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 port  X  X  X  X   X  becomes exactly  X  X  X  X   X  X  X  X  X  X  X  X  X  X   X  . As the deletion of can at most decrease  X  X  X  X   X  by one, thus  X  X  X  X   X  =  X  X  X  X   X  +1 must hold before removing  X  , and we can derive that the only condi-tion which makes a generator node  X  becomes unpromising is that  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  =  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 and  X  X  X  X  (  X  X  X  X  X  X  X  X  X  X   X   X   X  )=  X  X  X  X   X  +1 .

T HEOREM 14. In removing an old transaction  X  , if a node whose itemset is  X  X  X  X  X  X  X  X  X  X   X  satisfies  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  &lt;  X   X  X  X  X  X  X  X  X  X  X   X  1 , the state of node  X  and all its descendant nodes will remain unchanged.
 P ROOF . The proof is similar to that of Theorem 9.

After introducing some nice properties of the generator nodes and the unpromising nodes which can be used to enhance the ef-ficiency of mining and maintaining the set of frequent generators during the REMOVE operation, we now turn to the algorithm for the REMOVE operation. Algorithm 4 shows the details of the REMOVE operation. Like the ADD operation, we also adopt a right-to-left and top-down updating strategy here. The algorithm also determines how to deal with the update according to whether  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  is equal to, less than, or great than  X   X  X  X  X  X  X  X  X  X  X   X  1 .

The REMOVE operation invokes a procedure,  X  X  X  X  X  X  X  () ,whichis depicted in Algorithm 3. Algorithm 3 is used to clean those nodes some of whose uncle nodes change their types to infrequent or unpromising. In Algorithm 3, the function  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X  ) used to clean all child nodes of node  X  , and function  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X ,  X  ) is used to remove the child node of node  X  whose itemset is  X  X   X  } . Algorithm 3 :  X  X  X  X  X  X  X  (  X  ) Algorithm 4 :  X  X  X  X  X  X  X  X  X  (  X  ) Algorithm 4 shows the details of the  X  X  X  X  X  X  X   X  operation. Like the  X  X  X  X  operation, we also adopt a right-to-left and top-down updating strategy here.

Figure 5 depicts the enumeration tree built from the second slid-ing window (namely, after adding a new transaction with an ID of 5 and removing an old transaction with an ID of 1).

Similarly, we could prove the number of nodes in the enumera-tion tree would not increase during the REMOVE operation. From the above analysis we could easily see that both ADD and REMOVE operations determine how to update the enumeration tree according to whether  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  is equal to, less than, or greater than  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  X  X  1 ,where  X  X  X  X  X  X  X  X  X  X   X  is the itemset represented by any a node  X  in the tree,  X  is the transaction being added to (or removed from) the current sliding window. Hence, we summarize the transforming matrix in Table 1.
After introducing some properties, the FP-tree like structure to store the transactions of the current sliding window, the enumer-Figure 5: The Enumeration Tree of the 2nd Sliding Window. Table 1: Transforming matrix for Add and Remove operations (  X  =  X   X  X  X  X  X  X  X  X  X  X   X   X   X   X  ,  X  =  X   X  X  X  X  X  X  X  X  X  X   X   X  X  X  1 , G = Generator, U = Unpromising, I = Infrequent) ation tree structure to maintain the set of generators (or potential generators), the ADD operation and the REMOVE operation, we can easily derive the integrated StreamGen algorithm to mine fre-quent itemset generators over a stream sliding window. The FP-tree like structure and the enumeration tree structure are initialized to empty. After receiving a new transaction, StreamGen performs the ADD operation as shown in Algorithm 2, and if the size of the current sliding window exceeds the user-specified sliding window size, it then performs the REMOVE operation as shown in Algo-rithm 4. The set of generators for the current sliding window is always maintained and can be found in the enumeration tree. In addition, although the above StreamGen algorithm mines genera-tors over a stream sliding window, we need to point out that we can easily turn it into an incremental algorithm if we do not apply the REMOVE operation, as the ADD and REMOVE operations are totally independent of each other.

Note that since the two operations are not related, we could com-bine them freely for specific environments. For example, if we only adopt ADD operation, we could easily get an incremental al-gorithm.
Since one important application of itemset generator mining is to construct concise classification rules, we further extend the Stream-Gen framework to directly mine generator-based classification rules.
Since the complete set of frequent itemset generators for the cur-rent sliding window are maintained in the enumeration tree struc-ture, it is straightforward to retrieve all the generators from the enumeration tree, which can be further used to build classifica-tion rules. Algorithm 5 shows the framework StreamGenRules for classification rule construction. It adopts the information gain as a measure of discriminative ability which has been proved better than confidence in [5]. Note that it very easy to change our framework to the confidence-based.

It is very easy to prove that all the itemsets in the same equiv-alence class have the same information gain (and, the same confi-dence) due to the fact they have the same supporting set. Hence the generators could represent all the discriminative rules.
In Algorithm 5, the function getGenerators is used to traverse the whole enumeration tree and return all the generators, however, we Algorithm 5 :  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X  ) could apply a pruning technique in the function which prunes all those unpromising child nodes whose information gain is smaller than their ancestors and return only a subset of (and sometimes only a small subset of) high quality generators.

The algorithm can be easily adapted to mine a set of rules cov-ering each transaction more than one time. However, the change could cause the loss of pruning power in  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X  ) could prune a great many of unpromising generators. Besides, there is no evidence that covering each transaction more than one times could provide a better accuracy [22].

The StreamGenRules framework seems much like the DDPMine [6] algorithm at the first glance. Both algorithms use informa-tion gain as the discriminative measure, and the sequential cov-ering paradigm to select rules. However, there exists a significant difference between StreamGenRules and DDPMine. StreamGen-Rules tries to find the best rules from the generators which are computed from the full set of transactions in the current sliding window, while DDPMine tries to find the best discriminative rules from those transactions which have not been covered and removed. Hence the rules found by StreamGenRules are globally optimal, while DDPMines finds only those locally optimal rules, thus the rules returned by StreamGenRules tend to have better accuracy in classification. From the experimental results in Section 6.2 we could easily validate that our method can achieve a better accu-racy on average. Actually, our algorithm accords with the idea first proposed in [26], which tries to find one most discriminative rule for each transaction.
In this section we evaluate the performance and classification accuracy of our StreamGen algorithm in comparison with several state-of-the-art algorithms. The performance study was conducted on a computer with Intel Core Duo 2 E6550 CPU and 2GB memory installed. A set of UCI datasets were used in the experiments, and Table 2 shows the dataset characteristics. The datasets with two class labels were used in classification accuracy evaluation, while the datasets with multiple class labels, and the horse and mushroom datasets were used in performance test. Note all these datasets are publicly available and have been used widely in evaluating various data mining algorithms.
As there is no existing algorithm which mines frequent item-set generators over a sliding window, we evaluate the runtime effi-ciency of StreamGen algorithm in comparison with three state-of-the-art algorithms which perform a similar task to StreamGen. The first algorithm is Moment which mines frequent closed itemsets over a stream sliding window [7], the second one is DPM which mines frequent itemset generators and frequent closed itemsets in equivalence classes [16], and the third one is DDPMine which di-rectly mines classification rules [6].
We used four datasets, mushroom, chess, pumsb, and connect-4, to compare the performance of StreamGen with Moment. Figure 6 shows the efficiency comparison on mushroom dataset with a slid-ing windows size of 4 , 000 and 2 , 000 , respectively.
Figure 7 depicts the evaluation results on chess dataset with a sliding window size of 2 , 000 and 1 , 000 , respectively. From the results we could find that our algorithm is significantly faster than Moment algorithm on datasets mushroom and chess.
We also evaluated the algorithm on dataset pumsb, and Figure 8 shows the result on a sliding window size of 10 , 000 and respectively. We see that StreamGen outperforms Moment in most cases except when the support is extremely high. We do not provide the result corresponding to the support of 0 . 6 ,sinceMomentran out of all the available memory (i.e., 2  X  X  X  ) while our algorithm consumed less than 100  X  X  memory.

Figure 9 demonstrates the results on connect-4 dataset with a sliding window size of 60 , 000 and 30 , 000 , respectively. The re-sults indicate a similar result that our algorithm outperforms Mo-ment in all situations. Note that we do not provide the result on a lower sliding window size as Moment could not finish in an accept-able time.
Table 3 compares the peak memory usage on the datasets used above with the lowest minimum support used in performance test. We see that StreamGen always uses less memory than Moment, thus StreamGen is more memory efficient.

To our best knowledge, DPM is the newest and fastest algorithm for mining frequent itemset generators. A na X ve way to adapt it to mine frequent itemset generators over a stream sliding window is to run DPM on each new sliding window (In the following we denote this approach by DPM-stream). Our performance compar-ison between StreamGen and DPM-stream in the data stream set-ting shows that DPM-stream is not feasible in terms of runtime efficiency. Note that the runtime of DPM-stream is measured only on the sliding windows with a full size, which means we ignored the time period for DPM-stream before the sliding window reaches its full size. If this period is also considered for DPM-stream, the runtime of DPM-stream is even longer and DPM-stream could not terminate in an acceptable time on most datasets.

Figure 10 a) shows the comparison results on mushroom dataset with a sliding window size of 4 , 000 . DPM X  X  performance is very stable when the minimum support varies, while our algorithm could be a little slower when the support becomes lower. However, the advantage of our algorithm is evident. We could see from Figure 10 a) that our algorithm is orders of magnitude faster than DPM. Figure 10 b) shows the results on chess with a sliding window size of 1 , 000 , which is similar to that on mushroom.
We also evaluated the performance on connect-4 dataset with a sliding window size of 67 , 000 . We did not provide the result on other window size because DPM is too slow and could not finish in an acceptable time. Note that 67 , 000 is almost the biggest window size given the whole size of connect-4 of 67 , 557 . The result in Figure 11 a) shows our algorithm outperforms DPM significantly.
Figure 11 b) provides the result on pumsb with an extremely large sliding window size of 49 , 000 compared with the size of the entire dataset, 49 , 046 . We do not provide the results on a lower sliding window size as DPM cannot finish in an acceptable time. We see that our algorithm is more efficient than DPM.
We also compared the efficiency of StreamGen with DDPMine in terms of mining classification rules directly from the datasets. Similar to DPM, we also adapt DDPMine to mine classification rules in stream data setting by running DDPMine on each sliding window having full window size (we denote the DDPMine-based approach by DDPMine-stream). Figure 12 a) provides the com-parison results on mushroom dataset with a large sliding window size of 8 , 000 for mining classification rules. We could easily find that DDPMine-stream is much slower than StreamGen. Actually, even one round running of DDPMine-stream on one sliding win-dow is much longer than the total time of StreamGen on all sliding windows.

Figure 12 b) shows the results on horse dataset with a sliding window size of 600 . We see that DDPMine-stream uses nearly 1 , 000 seconds while StreamGen uses less than one second. In fact, the total runtime of StreamGen on all sliding windows over the entire dataset is shorter than the runtime of DDPMine-stream on only one sliding window. Similar to the state-of-the-art classification rule mining algorithm, DDPMine, StreamGen builds a SVM classification model with its features based on those mined association rules. We compared the classification accuracy using the five-fold cross-validation scheme. Table 4 shows the accuracy comparison between StreamGen and DDPMine and some statistical information of the mined rules (e.g., the average pattern length, the maximum pattern length, and the average number of patterns) for datasets breast, adult, mushroom, hepatitus, horse, and pima, with a minimum support of 0.1. During the experiment, DDPMine ran on the entire dataset, while Stream-Gen ran with a window size equal to the dataset size (That means there is only one sliding window for StreamGen).

We see that StreamGen achieves better accuracy than DDPMine in most cases, which validates that StreamGen is effective in min-ing classification rules over stream data with high accuracy. We can also observe that the rules mined by StreamGen is much shorter than those mined by DDPMine. We have to concede that the num-ber of rules mined by StreamGen is usually larger than that of DDP-Mine, which is due to the fact that DDPMine always tries to find an optimum rule for the remaining transactions and thus usually outputs a smaller number of rules in practice. However, the sim-plicity of the rules is more important since simpler rules can be better understood and explained. Table 5 shows an example of the rules mined by StreamGen and DDPMine in one of the five folds on the mushroom dataset with a support of 0 . 1 . We could easily find that the rules mined by StreamGen are significantly simpler. Note we have tried some other support thresholds (e.g., 0.05) for these datasets, and got similar comparison results.
Many previous studies have shown that mining itemset gener-ators is very meaningful from the classification point of view ac-cording to the MDL principle. In this paper, we explore a new and Table 5: One example of the classification rules mined by StreamGen and DDPMine (  X  X  X  X  X  X  X  X  X  X  =  X  X  X  X  X  X  X  X  X  X  X  X  ,  X  X  X  X   X  X  X  X  = 0 . 1 ). challenging problem of mining frequent itemset generators over a data stream sliding window. We devise a novel enumeration tree structure to help maintain the information of the mined genera-tors and the border between generators and non-generators. We also propose some effective optimization techniques and develop the StreamGen algorithm. The comprehensive performance study shows that StreamGen outperforms several state-of-the-art algo-rithms in terms of efficiency and classification accuracy. This work was supported in part by National Natural Science Foundation of China under Grant No. 60873171 and 60833003, National Basic Research Program of China under Grant No. 2006CB303103, Basic Research Foundation of Tsinghua National Laboratory for Information Science and Technology (TNList), a HP Labs Innovation Research Pr ogram award, a research award from Google, Inc., and the Program for New Century Excellent Talents in University under Grant No. NCET-07-0491, State Edu-cation Ministry of China. [1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] M.-L. Antonie and O. R. Za X ane. Text document [4] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, and [5] H. Cheng, X. Yan, J. Han, and C.-W. Hsu. Discriminative [6] H. Cheng, X. Yan, J. Han, and P. S. Yu. Direct discriminative [7] Y. Chi, H. Wang, P. S. Yu, and R. R. Muntz. Catch the [8] G. Cong, K.-L. Tan, A. K. H. Tung, and X. Xu. Mining top-k [9] G. Dong and J. Li. Efficient mining of emerging patterns: [10] C. Gao, J. Wang, Y. He, and L. Zhou. Efficient mining of [11] G. Grahne and J. Zhu. Efficiently using prefix-trees in [12] J. Han, J. Pei, Y. Yin, and R. Mao. Mining frequent patterns [13] N. Jiang and L. Gruenwald. Cfi-stream: mining closed [14] J. Li, G. Dong, and K. Ramamohanarao. Making use of the [15] J. Li, H. Li, L. Wong, J. Pei, and G. Dong. Minimum [16] J. Li, G. Liu, and L. Wong. Mining statistically important [17] W. Li, J. Han, and J. Pei. Cmar: Accurate and efficient [18] B. Liu, W. Hsu, and Y. Ma. Integrating classification and [19] G. Liu, H. Lu, W. Lou, and J. X. Yu. On computing, storing [20] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [21] J. Pei, J. Han, and R. Mao. Closet: An efficient algorithm for [22] J. R. Quinlan and R. M. Cameron-Jones. Foil: A midterm [23] A. Veloso, W. M. Jr., M. de Carvalho, B. P X ssas, [24] A. Veloso, W. M. Jr., and M. J. Zaki. Lazy associative [25] J. Wang, J. Han, and J. Pei. Closet+: searching for the best [26] J. Wang and G. Karypis. On mining instance-centric [27] L. Xu and K. Xie. An incremental algorithm for mining [28] J. X. Yu, Z. Chong, H. Lu, and A. Zhou. False positive or [29] M. J. Zaki and C.-J. Hsiao. Charm: An efficient algorithm
