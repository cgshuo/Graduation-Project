 Co-clustering targets on grouping the samples and features simultaneously. It takes advantage of the duality between the samples and features. In many real-world application-s, the data points or features usually reside on a subman-ifold of the ambient Euclidean space, but it is nontrivial to estimate the intrinsic manifolds in a principled way. In this study, we focus on improving the co-clustering perfor-mance via manifold ensemble learning, which aims to maxi-mally approximate the intrinsic manifolds of both the sam-ple and feature spaces. To achieve this, we develop a novel co-clustering algorithm called Relational Multi-manifold Co-clustering (RMC) based on symmetric nonnegative matrix tri-factorization, which decomposes the relational data ma-trix into three matrices. This method considers the inter-type relationship revealed by the relational data matrix and the intra-type information reflected by the affinity matrices. Specifically, we assume the intrinsic manifold of the sample or feature space lies in a convex hull of a group of pre-defined candidate manifolds. We hope to learn an appropriate con-vex combination of them to approach the desired intrinsic manifold. To optimize the objective, the multiplicative rules are utilized to update the factorized matrices and the en-tropic mirror descent algorithm is exploited to automati-cally learn the manifold coefficients. Experimental results demonstrate the superiority of the proposed algorithm. I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database applications X  data mining Algorithm, Experimentation Co-clustering, manifold ensemble learning, nonnegative ma-trix tri-factorization, entropic mirror descent algorithm
A large amount of relational data emerge in a broad range of applications [17], e.g.,various images on the Internet. To handle them, clustering has established itself as a very use-ful tool and gained an increasing important role in knowl-edge management and information retrieval, etc. Tradition-al clustering belongs to unilateral learning paradigm, name-ly it only emphasizes clustering along the sample or fea-ture dimension individually. Recent works have shown that clustering the samples and features simultaneously, i.e., co-clustering, enables further improving the clustering perfor-mance, in the sense that co-clustering fully makes use of the dual interdependence between samples and features to discover hidden structures in data [10].

To this end, many co-clustering algorithms have been pro-posed, such as graph partitioning based model [10, 19] and matrix factorization based model [12, 22]. In this work, we focus on studying matrix factorization based co-clustering, which models the sample-feature relationship from the da-ta reconstruction perspective. We regard nonnegative ma-trix tri-factorization [12] as the basis of our co-clustering approach. This typical method imposes the nonnegative constraints on the decomposed matrices, which leads to a parts-based representation [15]. On the other hand, many studies have shown human generated data are usually drawn from a probability distribution that has support on or near to a submanifold [4, 5, 6, 7, 8]. As a result, some researcher-s strive to consider manifold geometrical structure in co-clustering by dual graph regularization [14, 21]. However, it is nontrivial to seek the intrinsic manifolds. To address this issue, inspired from the work in [13], we propose to approxi-mate the optimal manifold by using a convex combination of some pre-given candidate manifolds, and thus develop a nov-el co-clustering algorithm called Relational Multi-manifold Co-clustering (RMC) to improve the clustering performance via manifold ensemble learning.

We consider the inter-type relation through the relation-al data matrix and the intra-type information through the affinity matrices encoded on both the sample and feature spaces [21, 22]. In manifold ensemble learning, we assume that the intrinsic manifold of the sample or feature space lies in a convex hull of a group of pre-defined candidate mani-folds [13] and hope to approximate the intrinsic manifolds of the sample or feature space by a convex combination of these candidates. To optimize the objective, we alternative-ly update the factorized matrices and exploit the entropic mirror descent algorithm to automatically learn the mani-fold coefficients. Experiments show its effectiveness.
Here, we review some works closely related to our ap-proach. Matrix factorization based co-clustering techniques have been widely studied. The popularized nonnegative matrix factorization (NMF) [15] was proposed to learn a parts-based representation, but it focuses on the unilateral clustering. Motivated by this, the block value decompo-sition (BVD) was presented for co-clustering dyadic data [18]. It factorizes the data matrix into three components, i.e., the row and column coefficient matrices and the block value matrix. As an extension of NMF, orthogonal non-negative matrix tri-factorization (ONMTF) was studied in [12], which emphasizes the role of bi-orthogonality in three-factor NMF. Thanks to the successful applications of man-ifold learning in recent years [2], some researchers consid-er the local geometrical structure in matrix factorization based co-clustering. For example, a dual regularized co-clustering (DRCC) method based on semi-nonnegative ma-trix tri-factorization was proposed in [14]. To reduce the computational complexity of DRCC, some fast approaches were proposed [22]. Moreover, a symmetric nonnegative ma-trix tri-factorization (SNMTF) framework was developed to cluster multi-type relational data [21], which incorporates the intra-type information through manifold regularization.
However, these graph regularization based methods share a shortcoming that the estimated manifold might not be true, and it even deviates far from desired in an adverse situation. To alleviate this problem, inspired by [13], we at-tempt to approximate the true intrinsic manifold by a convex combination of some candidate manifolds.
In this part, we introduce our RMC approach including the optimization framework.
The general problem setting is to co-cluster multi-type relational data. Given a K -type relational data set X = {X 1 , X 2 , ..., X K } ,whereeach X k represents the data objects of the k -th type, we define an inter-type relational matrix R with the sub-matrix R ij  X  R n i  X  n j , i = j , which reflect-s the inter-type relationship between the i -th type and the j -th type data objects. To model the intra-type structure information, we define an intra-type relational matrix W consisting of a set of affinity matrices W k  X  R n k  X  n k encod-ed on the data, which indicates the intra-type relation of components within the k -th type data.

Typically, we focus on the case K = 2, i.e., we employ both the sample and feature type data for co-clustering. The concerned matrices are R = 0 where 0 isazeromatrix.Here, R 12 and R 21 represent the feature and sample matrix respectively, R 12 = R T 21 .Each column denotes a feature or sample vector.
SNMTF [21] decomposes one data matrix into three parts, i.e., ( K =2) tor matrices for X 1 and X 2 ,respectively, c 1 n 1 , c 2 Themiddlematrix S 12  X  R c 1  X  c 2 can be treated as a com-pact representation of R 12 [18], which absorbs the different scales of other matrices [12]. Note that BVD and ONMT-F impose nonnegative constraints on all three matrices G 1 G 2 , S , while DRCC and SNMTF both relax the nonnega-tive constraint on the matrix S , thereby allowing negative entries. Based on DRCC, SNMTF employs the symmetric matrix to simultaneously cluster multi-type relational data, and its objective is where  X  F denotes the Frobenius norm, L = D  X  W is the graph Laplacian [2], D is the diagonal matrix with D matrices G and S are designed below
G = G
It is challenging to discover an appropriate intrinsic man-ifold for graph based co-clustering methods in reality. In this work, we adopt a novel learning paradigm named man-ifold ensemble learning to maximally approximate the true intrinsic manifold. This idea is inspired from the work in [13], which combines the automatic intrinsic manifold ap-proximation and semi-supervised classification.

We assume a series of initial guesses of graph Laplacian are available and the intrinsic manifold of the sample or feature space lies in the convex hull of these pre-given can-didate manifolds. In some sense, this assumption constrains the search space, since the optimal graph Laplacian is an discrete approximation to the intrinsic manifold [13], i.e., where a set of candidate graph Laplacians C = { L 1 ,..., L is defined. Here we use  X  L i of the i -th candidate manifold to discriminate it from L k of the k -th type data.
We take advantage of the manifold ensemble learning into the symmetric nonnegative matrix tri-factorization frame-work, and thus propose a novel co-clustering approach named Relational Multi-manifold Co-clustering (RMC).

Now, it is easy to arrive at our objective, i.e., where  X &gt; 0 , X &gt; 0, the tradeoff parameter  X  is used to gov-ern the contribution of the ensemble manifold regularization to the objective, the l 2 -norm of  X  is employed to avoid the coefficient parameter over-fitting to only one manifold and the factor  X  acts as an over-fitting tolerance parameter for the manifold coefficients. Similar to [14], we do l 2 normal-ization on columns of G and compensate its norm to S .The components of G , i.e., G 1 and G 2 represent the partition matrices of the feature matrix R 21 and the sample matrix R 12 , respectively. We typically use the partition matrices to derive the co-clustering results. In this section, we explore how to optimize the objective in Eq. (3.4). It can be readily found that the objective function is non-convex in G , S ,  X  jointly, but it is convex in them respectively. So it is unrealistic to find the global minimum since no closed-form solution can be obtained. We present an alternating scheme to optimize the objective involving optimizing the manifold coefficient vector. When fixing G and  X  , the objective becomes minimizing J
S = R  X  GSG T 2 F . Taking its derivative to S and setting it to 0, then we have the update rule
When fixing S and  X  , the objective w.r.t. G reduces to minimizing To solve this problem, we introduce the Lagrangian mul-tiplier matrix  X  and its Lagrangian function is formulated as L ( G )= R  X  GSG T 2 F +  X  Tr( G T LG )+Tr(  X  G T ) . (6) Requiring its derivative to G be 0, we obtain  X  =4  X  LG  X  4 A +4 GB ,where A = RGS T , B = S T G T GS .

Since the Karush-Kuhn-Tucker (KKT) condition for the nonnegativity of G ij gives  X  ij G ij =0,wehave Similar to [11], we define L = L +  X  L  X  , A = A +  X  A  X  , B = B +  X  B  X  ,where We substitute the decomposed positive and negative parts into Eq. (7), which leads to the update rule
When fixing G and S , the objective is simplified to min  X  f (  X  )= where s i =Tr( G T  X  L i G ). It is easy to see that if we will get a trivial solution, which is undesirable to learn acompositemanifold. If  X   X  X  X  , all candidate manifold-s will receive identical weights, which is also unexpected. Therefore, it is essential to assign a proper parameter  X 
It is actually an exactly well-defined problem, i.e., the convex minimization over the unit simplex, which can be solved by the Entropic Mirror Descent Algorithm (EMDA) with a global efficiency estimate [1], as shown in Algorith-m 1. It has been shown that EMDA owns the natural ad-vantage to solve this convex problem over the unit simplex Algorithm 1 Entropic Mirror Descent Algorithm Input: L f ,  X  , s .
 Output:  X  . 1: Initialize:  X  i =1 /q . 2: for i =1to q do 4: Update each  X  i by this rule 5: Repeat Step 3 to 4 until convergence. 6: end for Algorithm 2 Relational Multi-manifold Co-clustering Input: Relational data matrices R and W ,thenumberof Output: Partition matrix G . 1: Initialize: Generate G using k-means. 2: Compute S =( G T G )  X  1 G T RG ( G T G )  X  1 . 3: Compute the manifold coefficient  X  using Algorithm 1. 4: Update the matrix G according to 5: Repeat Step 2 to 4 until convergence. = {  X   X  R q : q i =1  X  i =1 ,  X  0 } . To apply this algo-rithm, the objective function f should be a convex Lipschitz continuous function with Lipschitz constant L f with respec-t to a fixed given norm. We derive this Lipschitz constant from  X  f (  X  ) 1  X  2  X  + s 1 = L f ,where s = { s 1 ,...,s Here, we use  X  1 norm as suggested in [1].

In summary, we present our Relational Multi-manifold Co-clustering (RMC) approach in Algorithm 2. Note that here we omit the convergence proof due to space limit. Please refer to [5, 14].
In this section, we investigate the clustering performance of the proposed method on a broad range of data sets.
The important statistics of the data corpora are summa-rized in Table 1 and the brief descriptions are shown below.
Text corpora . NGroups5 is selected from the newsgroup data collection 20Newsgroups 1 . We use a subset, which con-tains five different topics, which refer to 4,052 documents. RCV1-5 is a 5-topic subset of a smaller RCV1 2 collection [9]. We select 1,200 words with the highest contribution to the mutual information between words and documents.
Image databases . The AlphaDigit 3 is a handwritten image data that contains 1,404 images, covering 20  X  16 dig-http://people.csail.mit.edu/jrennie/20Newsgroups/ http://www.daviddlewis.com/resources/testcollections/rcv1/ http://cs.nyu.edu/  X  roweis/data.html Data Sets Domain Samples Features Classes NGroups5 text 4,052 1,200 5 RCV1-5 text 3,012 1,200 5 AlphaDigit image 1,404 320 36 UMIST image 575 644 20 Leukemia2 gene 72 5,551 3
LungCancer gene 203 2,008 5 its of  X 0 X  through  X 9 X  and capitals  X  X  X  X hrough  X  X  X . UMIST is a face database referring to a range of poses from profile to frontal views. Each image is rescaled to 28  X  23 pixels.
Gene expression data . Leukemia2 and LungCancer 5 are produced by oligonucleotide-based technology [20]. Sim-ilar to [16], we removed the genes which vary little across samples to reduce the computational complexity. To explore the clustering performance of the proposed RMC algorithm, we compare it with some state-of-the-art approaches: NMF [15], GNMF[5], DRCC [14],ONMTF [12], SNMTF [21] and OSNTF [22]. K-means (KM) is treated as a baseline.

In the experiments, we ran k-means 20 times with different starting points and the best result in terms of the objective function was recorded. The average results over 20 test runs are reported. We adopt two popular criteria to measure the clustering performance [3], i.e., the accuracy (AC) and the normalized mutual information (NMI).
Except KM, the number of sample or feature clusters is set to the actual number of classes in data sets. Note that there is no parameter selection for KM, NMF and ONMTF, oncethenumberofclustersisgiven. ForGNMF,DRCC, SNMTF, OSNTF and RMC, p is fixed to 5. The regulariza-tion parameters are all searched from the grid { 0.001, 0.01, 0.1, 1, 10, 100, 500, 1500 } . For co-clustering methods, the regularization parameters for the sample and feature graph are set to the same. Except RMC, the other graph-based methods construct the Laplacian matrix using the binary weighting scheme [5].

For RMC, we empirically set  X  =0 . 1  X  [13]. To gener-ate diverse manifolds, we utilized three kinds of weighting schemes to construct the graph, i.e., the binary weighting, the Gaussian kernel, and the cosine similarity. In particular, for the Gaussian kernel, we varied the bandwidth t in a broad range of area, i.e., t = {  X  100 ,  X  60 ,  X  30 ,  X  10 , X , ifolds containing nine Gaussian graphs, one binary graph and one cosine similarity graph were used.
The average results are tabulated in Table 2 and 3. Several interesting points can be revealed below. http://images.ee.umist.ac.uk/danny/database.html http://www.gems-system.org/datasets/
Since p is fixed to 5 and  X  =0 . 1  X  ,weonlyexplore the influences of  X  here. Figure 1 shows the results (  X  = { 0 . approach enjoys satisfactory performances when  X  takes a higher value, e.g., around 500 or 1000. This indicates that the ensemble manifold regularization term should be im-posed larger weights, such that it can make more positive contributions to the objective. This paper presents a novel co-clustering approach named Relational Multi-manifold Co-clustering (RMC), which is based on the symmetric nonnegative matrix tri-factorization. It takes into account the inter-type relation and the intra-type information of both the sample and feature data si-multaneously. The basic idea is to make use of manifold en-semble learning to enhance the performance of co-clustering. To achieve this, we attempt to learn a sensible convex com-bination of candidate manifolds so that it can maximally approximate the true intrinsic manifolds of both the sample and feature spaces. In order to optimize the objective func-tion, we adopt the popular alternating optimization method to update the factorized matrices. However, different from the existing matrix factorization based co-clustering meth-ods, there is a manifold coefficient vector to be optimized in our approach, which poses a challenging task. In this work, we utilize the entropic mirror descent algorithm to optimize this coefficient vector. The effectiveness of the proposed approach is demonstrated by a number of interesting exper-iments on data collections from diverse domains.
This work was supported in part by National Natural Sci-ence Foundati on of China unde r Grants 91120302, 60905001 and 61173186, National Basic Research Program of Chi-na (2011CB302206), the Fundamental Research Funds for the Central Universities (2012FZA5017) and the Zhejiang Province Key S&amp;T Innovation Group Project (2009R50009). [1] A. Beck and M. Teboulle. Mirror descent and [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [3] D. Cai, X. He, and J. Han. Document clustering using [4] D. Cai, X. He, and J. Han. Locally consistent concept [5] D. Cai, X. He, J. Han, and T. Huang. Graph [6] D. Cai, X. He, X. Wang, H. Bao, and J. Han. Locality [7] D. Cai, X. He, X. Wu, and J. Han. Non-negative [8] D. Cai, X. Wang, and X. He. Probabilistic dyadic data [9] W. Chen, Y. Song, H. Bai, C. Lin, and E. Chang. [10] I. Dhillon. Co-clustering documents and words using [11] C. Ding, T. Li, and M. Jordan. Convex and [12] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [13] B. Geng, C. Xu, D. Tao, L. Yang, and X. Hua. [14] Q. Gu and J. Zhou. Co-clustering on manifolds. In [15] D. Lee, H. Seung, et al. Learning the parts of objects [16] W. Liu, K. Yuan, and D. Ye. On  X  -divergence based [17] B. Long, Z. Zhang, X. Wu, and P. Yu. Spectral [18] B. Long, Z. Zhang, and P. Yu. Co-clustering by block [19] M. Rege, M. Dong, and F. Fotouhi. Co-clustering [20] A. Statnikov, C. Aliferis, I. Tsamardinos, D. Hardin, [21] H. Wang, H. Huang, and C. Ding. Simultaneous [22] H. Wang, F. Nie, H. Huang, and C. Ding. Nonnegative
