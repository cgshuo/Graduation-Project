 Language models (LM) are crucial components in tasks that require the generation of coherent natu-ral language text, such as automatic speech recog-nition (ASR) and machine translation (MT). While traditional LMs use word n -grams, where the n  X  1 previous words predict the next word, newer mod-els integrate long-span information in making deci-sions. For example, incorporating long-distance de-pendencies and syntactic structure can help the LM better predict words by complementing the predic-tive power of n -grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009).

The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbi-trary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic in-formation have obtained state of the art results.
However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N -best lists (Khudanpur and Wu, 2000; Collins et al., 2005; Kuo et al., 2009). For dis-criminative models, this limitation applies to train-ing as well. Moreover, the non-local features used in rescoring are usually extracted via auxiliary tools  X  which in the case of syntactic features include part of speech taggers and parsers  X  from a set of ASR sys-tem hypotheses. Separately applying auxiliary tools to each N -best list hypothesis leads to major ineffi-ciencies as many hypotheses differ only slightly.
Recent work on hill climbing algorithms for ASR lattice rescoring iteratively searches for a higher-scoring hypothesis in a local neighborhood of the current-best hypothesis, leading to a much more ef-ficient algorithm in terms of the number, N , of hy-potheses evaluated (Rastrow et al., 2011b); the idea also leads to a discriminative hill climbing train-ing algorithm (Rastrow et al., 2011a). Even so, the reliance on auxiliary tools slow LM application to the point of being impractical for real time systems. While faster auxiliary tools are an option, they are usually less accurate.

In this paper, we propose a general modifica-tion to the decoders used in auxiliary tools to uti-lize the commonalities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction al-gorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual deci-sions. We demonstrate our approach on a local Per-ceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tag-ging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model X  X  simplicity through up-training (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is signif-icant speed improvements and a reduction in word error rate (WER) for both N -best list and the al-ready fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system . There have been several approaches to include syn-tactic information in both generative and discrimi-native language models.

For generative LMs, the syntactic information must be part of the generative process. Structured language modeling incorporates syntactic parse trees to identify the head words in a hypothesis for modeling dependencies beyond n -grams. Chelba and Jelinek (2000) extract the two previous exposed head words at each position in a hypothesis, along with their non-terminal tags, and use them as con-text for computing the probability of the current po-sition. Khudanpur and Wu (2000) exploit such syn-tactic head word dependencies as features in a maxi-mum entropy framework. Kuo et al. (2009) integrate syntactic features into a neural network LM for Ara-bic speech recognition.

Discriminative models are more flexible since they can include arbitrary features, allowing for a wider range of long-span syntactic dependen-cies. Additionally, discriminative models are di-rectly trained to resolve the acoustic confusion in the decoded hypotheses of an ASR system. This flexi-bility and training regime translate into better perfor-mance. Collins et al. (2005) uses the Perceptron al-gorithm to train a global linear discriminative model which incorporates long-span features, such as head-to-head dependencies and part of speech tags. Our Language Model. We work with a discrimi-native LM with long-span dependencies. We use a global linear model with Perceptron training. We rescore the hypotheses (lattices) generated by the ASR decoder X  X n a framework most similar to that of Rastrow et al. (2011a).

The LM score S ( w , a ) for each hypothesis w of a speech utterance with acoustic sequence a is based on the baseline ASR system score b ( w , a ) (initial n -gram LM score and the acoustic score) and  X  0 , the weight assigned to the baseline score. 1 The score is defined as:
S ( w , a ) =  X  0  X  b ( w , a ) + F ( w , s 1 ,..., s m where F is the discriminative LM X  X  score for the hypothesis w , and s 1 ,..., s m are candidate syntac-tic structures associated with w , as discussed be-low. Since we use a linear model, the score is a weighted linear combination of the count of acti-vated features of the word sequence w and its as-sociated structures:  X  i ( w , s 1 ,..., s m ) . Perceptron training learns the parameters  X  . The baseline score b ( w , a ) can be a feature, yielding the dot product notation: S ( w , a ) =  X   X  ,  X ( a , w , s 1 ,..., s m )  X  Our LM uses features from the dependency tree and part of speech (POS) tag sequence. We use the method described in Kuo et al. (2009) to identify the two previous exposed head words, h  X  2 ,h  X  1 , at each po-sition i in the input hypothesis and include the fol-lowing syntactic based features into our LM: 1 . ( h  X  2 . w  X  h  X  1 . w  X  w i ) , ( h  X  1 . w  X  w i ) , ( w 2 . ( h  X  2 . t  X  h  X  1 . t  X  t i ) , ( h  X  1 . t  X  t i ) , ( t where h. w and h. t denote the word identity and the POS tag of the corresponding exposed head word. 2.1 Hill Climbing Rescoring We adopt the so called hill climbing framework of Rastrow et al. (2011b) to improve both training and rescoring time as much as possible by reducing the number N of explored hypotheses. We summarize it below for completeness.

Given a speech utterance X  X  lattice L from a first pass ASR decoder, the neighborhood N ( w ,i ) of a hypothesis w = w 1 w 2 ... w n at position i is de-fined as the set of all paths in the lattice that may be obtained by editing w i : deleting it, substituting it, or inserting a word to its left. In other words, it is the  X  X istance-1-at-position i  X  neighborhood of w . Given a position i in a word sequence w , all hypotheses in N ( w ,i ) are rescored using the long-span model and the hypothesis  X  w 0 ( i ) with the high-est score becomes the new w . The process is re-peated with a new position  X  scanned left to right  X  until w =  X  w 0 (1) = ... =  X  w 0 ( n ) , i.e. when w itself is the highest scoring hypothesis in all its 1 -neighborhoods, and can not be furthered improved using the model. Incorporating this into training yields a discriminative hill climbing algorithm (Ras-trow et al., 2011a). Long-span models  X  generative or discriminative, N -best or hill climbing  X  rely on auxiliary tools, such as a POS tagger or a parser, for extracting features for each hypothesis during rescoring, and during training for discriminative models. The top-m candidate structures associated with the i th hy-pothesis, which we denote as s 1 i ,..., s m i , are gener-ated by these tools and used to score the hypothesis: speech tag or a syntactic dependency. We formally define this sequential processing as: . . . Here, { w 1 ,..., w k } represents a set of ASR output hypotheses that need to be rescored. For each hy-pothesis, we apply an external tool (e.g. parser) to generate associated structures s 1 i ,..., s m i (e.g. de-pendencies.) These are then passed to the language model along with the word sequence for scoring. 3.1 Substructure Sharing While long-span LMs have been empirically shown to improve WER over n -gram LMs, the computa-tional burden prohibits long-span LMs in practice, particularly in real-time systems. A major complex-ity factor is due to processing 100s or 1000s of hy-potheses for each speech utterance, even during hill climbing, each of which must be POS tagged and parsed. However, the candidate hypotheses of an utterance share equivalent substructures, especially in hill climbing methods due to the locality present in the neighborhood generation. Figure 1 demon-strates such repetition in an N -best list ( N =10) and a hill climbing neighborhood hypothesis set for a speech utterance from broadcast news. For exam-ple, the word  X  X NDORSE X  occurs within the same local context in all hypotheses and should receive the same part of speech tag in each case. Processing each hypothesis separately wastes time.

We propose a general algorithmic approach to re-duce the complexity of processing a hypothesis set by sharing common substructures among the hy-potheses. Critically, unlike many lattice parsing al-gorithms, our approach is general and produces ex-act output. We first present our approach and then demonstrate its generality by applying it to a depen-dency parser and part of speech tagger.

We work with structured prediction models that produce output from a series of local decisions: a transition model. We begin in initial state  X  0 and terminate in a possible final state  X  f . All states along the way are chosen from the possible states  X  . A transition (or action)  X   X   X  advances the decoder from state to state, where the transition  X  i changes the state from  X  i to  X  i +1 . The sequence of states {  X  0 ... X  i , X  i +1 ... X  f } can be mapped to an output (the model X  X  prediction.) The choice of action  X  is given by a learning algorithm, such as a maximum-entropy classifier, support vector ma-chine or Perceptron, trained on labeled data. Given the previous k actions up to  X  i , the classifier g :  X   X   X  k  X  R |  X  | assigns a score to each possi-ble action, which we can interpret as a probability: p plied to transition to new states  X  i +1 . We note that state definitions can encode the k previous actions, which simplifies the probability to p g (  X  i |  X  i ) . The score of the new state is then Classification decisions require a feature represen-tation of  X  i , which is provided by feature functions f :  X   X  X  , that map states to features. Features are conjoined with actions for multi-class classification, tion operation. In this way, states can be summarized by features.

Equivalent states are defined as two states  X  and  X  0 with an identical feature representation: If two states are equivalent, then g imposes the same distribution over actions. We can benefit from this substructure redundancy, both within and between hypotheses, by saving these distributions in mem-ory, sharing a distribution computed just once across equivalent states. A similar idea of equivalent states is used by Huang and Sagae (2010), except they use equivalence to facilitate dynamic programming for shift-reduce parsing, whereas we generalize it for improving the processing time of similar hypotheses in general models. Following Huang and Sagae, we define kernel features as the smallest set of atomic features  X  f (  X  ) such that, Equivalent distributions are stored in a hash table H :  X   X   X   X  R ; the hash keys are the states and the values are distributions 2 over actions: {  X ,p g (  X  |  X  ) } . H caches equivalent states in a hypothesis set and re-sets for each new utterance. For each state, we first check H for equivalent states before computing the action distribution; each cache hit reduces decod-ing time. Distributing hypotheses w i across differ-ent CPU threads is another way to obtain speedups, and we can still benefit from substructure sharing by storing H in shared memory.
 function, where int(  X  f i (  X  )) is an integer mapping of the i th kernel feature. For integer typed features the mapping is trivial, for string typed features (e.g. a POS tag identity) we use a mapping of the cor-responding vocabulary to integers. We empirically found that this hash function is very effective and yielded very few collisions.

To apply substructure sharing to a transition based model, we need only define the set of states  X  (in-cluding  X  0 and  X  f ), actions  X  and kernel feature functions  X  f . The resulting speedup depends on the amount of substructure duplication among the hy-potheses, which we will show is significant for ASR lattice rescoring. Note that our algorithm is not an approximation; we obtain the same output { s j i } as we would without any sharing. We now apply this algorithm to dependency parsing and POS tagging. 3.2 Dependency Parsing We use the best-first probabilistic shift-reduce de-pendency parser of Sagae and Tsujii (2007), a transition-based parser (K  X  ubler et al., 2009) with a MaxEnt classifier. Dependency trees are built by processing the words left-to-right and the classifier assigns a distribution over the actions at each step. States are defined as  X  = { S,Q } : S is a stack of subtrees s 0 ,s 1 ,... ( s 0 is the top tree) and Q are words in the input word sequence. The initial state is  X  0 = { X  , { w 0 ,w 1 ,... }} , and final states occur when Q is empty and S contains a single tree (the output).
 X  is determined by the set of dependency labels r  X  X  and one of three transition types:  X  Shift : remove the head of Q ( w j ) and place it on  X  Reduce-Left r : replace the top two trees in S ( s 0  X  Reduce-Right r : same as Reduce-Left r except re-Table 1 shows the kernel features used in our de-pendency parser. See Sagae and Tsujii (2007) for a complete list of features.

Goldberg and Elhadad (2010) observed that pars-ing time is dominated by feature extraction and score calculation. Substructure sharing reduces these steps for equivalent states, which are persis-tent throughout a candidate set. Note that there are far fewer kernel features than total features, hence the hash function calculation is very fast.

We summarize substructure sharing for depen-dency parsing in Algorithm 1. We extend the def-inition of states to be { S,Q,p } where p denotes the score of the state: the probability of the action se-quence that resulted in the current state. Also, fol-Algorithm 1 Best-first shift-reduce dependency parsing lowing Sagae and Tsujii (2007) a heap is used to maintain states prioritized by their scores, for apply-ing the best-first strategy. For each step, a state from the top of the heap is considered and all actions (and scores) are either retrieved from H or computed us-ing g . 3 We use  X  new  X   X  current  X   X  to denote the operation of extending a state by an action  X   X   X  4 . 3.3 Part of Speech Tagging We use the part of speech (POS) tagger of Tsuruoka et al. (2011), a transition based model with a Per-ceptron and a lookahead heuristic process. The tag-ger processes w left to right. States are defined as  X  i = { c i , w } : a sequence of assigned tags up to w i ( c i = t 1 t 2 ...t i  X  1 ) and the word sequence w .  X  is defined simply as the set of possible POS tags ( T ) that can be applied. The final state is reached once all the positions are tagged. For f we use the features of Tsuruoka et al. (2011). The kernel features are  X  While the tagger extracts prefix and suffix features, it suffices to look at w i for determining state equiv-alence. The tagger is deterministic (greedy) in that it only considers the best tag at each step, so we do not store scores. However, this tagger uses a depth-first search lookahead procedure to select the best action at each step, which considers future decisions up to depth d 5 . An example for d = 1 is shown in Figure 2. Using d = 1 for the lookahead search strategy, we modify the kernel features since the de-cision for w i is affected by the state  X  i +1 . The kernel features in position i should be  X  f (  X  i )  X   X  f (  X  i +1 While we have fast decoding algorithms for the pars-ing and tagging, the simpler underlying models can lead to worse performance. Using more complex models with higher accuracy is impractical because they are slow. Instead, we seek to improve the accu-racy of our fast tools.

To achieve this goal we use up-training, in which a more complex model is used to improve the accu-racy of a simpler model. We are given two mod-els, M 1 and M 2 , as well as a large collection of unlabeled text. Model M 1 is slow but very accu-rate while M 2 is fast but obtains lower accuracy. Up-training applies M 1 to tag the unlabeled data, which is then used as training data for M 2 . Like self-training, a model is retrained on automatic out-put, but here the output comes form a more accurate model. Petrov et al. (2010) used up-training as a domain adaptation technique: a constituent parser  X  which is more robust to domain changes  X  was used to label a new domain, and a fast dependency parser was trained on the automatically labeled data. We use a similar idea where our goal is to recover the accuracy lost from using simpler models. Note that while up-training uses two models, it differs from co-training since we care about improving only one model ( M 2 ). Additionally, the models can vary in different ways. For example, they could be the same algorithm with different pruning methods, which can lead to faster but less accurate models.

We apply up-training to improve the accuracy of both our fast POS tagger and dependency parser. We parse a large corpus of text with a very accurate but very slow constituent parser and use the resulting data to up-train our tools. We will demonstrate em-pirically that up-training improves these fast models to yield better WER results. The idea of efficiently processing a hypothesis set is similar to  X  X attice-parsing X , in which a parser con-sider an entire lattice at once (Hall, 2005; Chep-palier et al., 1999). These methods typically con-strain the parsing space using heuristics, which are often model specific. In other words, they search in the joint space of word sequences present in the lat-tice and their syntactic analyses; they are not guaran-teed to produce a syntactic analysis for all hypothe-ses. In contrast, substructure sharing is a general purpose method that we have applied to two differ-ent algorithms. The output is identical to processing each hypothesis separately and output is generated for each hypothesis. Hall (Hall, 2005) uses a lattice parsing strategy which aims to compute the marginal probabilities of all word sequences in the lattice by summing over syntactic analyses of each word se-quence. The parser sums over multiple parses of a word sequence implicitly. The lattice parser there-fore, is itself a language model. In contrast, our tools are completely separated from the ASR sys-tem, which allows the system to create whatever fea-tures are needed. This independence means our tools are useful for other tasks, such as machine transla-tion. These differences make substructure sharing a more attractive option for efficient algorithms.
While Huang and Sagae (2010) use the notion of  X  X quivalent states X , they do so for dynamic program-ming in a shift-reduce parser to broaden the search space. In contrast, we use the idea to identify sub-structures across inputs, where our goal is efficient parsing in general. Additionally, we extend the defi-nition of equivalent states to general transition based structured prediction models, and demonstrate ap-plications beyond parsing as well as the novel setting of hypothesis set parsing. Our ASR system is based on the 2007 IBM Speech transcription system for the GALE Distilla-tion Go/No-go Evaluation (Chen et al., 2006) with state of the art discriminative acoustic models. See Table 2 for a data summary. We use a modi-fied Kneser-Ney (KN) backoff 4 -gram baseline LM. Word-lattices for discriminative training and rescor-ing come from this baseline ASR system. 6 The long-span discriminative LM X  X  baseline feature weight (  X  0 ) is tuned on dev data and hill climbing (Rastrow et al., 2011a) is used for training and rescoring. The dependency parser and POS tagger are trained on su-pervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al. (2010), a state of the art broadcast news (BN) parser, with phrase structures converted to labeled dependencies by the Stanford converter.

While accurate, the parser has a huge grammar (32GB) from using products of latent variable gram-mars and requires O ( l 3 ) time to parse a sentence of length l . Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long, although the shorter up-training text data was not a problem. 7 We evaluate both unlabeled (UAS) and labeled dependency accuracy (LAS). 6.1 Results Before we demonstrate the speed of our models, we show that up-training can produce accurate and fast models. Figure 3 shows improvements to parser ac-curacy through up-training for different amount of (randomly selected) data, where the last column in-dicates constituent parser score ( 91 . 4% UAS). We use the POS tagger to generate tags for depen-dency training to match the test setting. While there is a large difference between the constituent and dependency parser without up-training ( 91 . 4% vs. 86 . 2% UAS), up-training can cut the differ-ence by 44% to 88 . 5% , and improvements saturate around 40m words (about 2m sentences.) 8 The de-pendency parser remains much smaller and faster; the up-trained dependency model is 700MB with 6m features compared with 32GB for constituency model. Up-training improves the POS tagger X  X  accu-racy from 95.9% to 97%, when trained on the POS tags produced by the constituent parser, which has a tagging accuracy of 97 . 2% on BN.

We train the syntactic discriminative LM, with head-word and POS tag features, using the faster parser and tagger and then rescore the ASR hypothe-ses. Table 3 shows the decoding speedups as well as the WER reductions compared to the baseline LM. Note that up-training improvements lead to WER re-ductions. Detailed speedups on substructure sharing are shown in Table 4; the POS tagger achieves a 5.3 times speedup, and the parser a 5.7 speedup with-out changing the output. We also observed speedups during training (not shown due to space.)
The above results are for the already fast hill climbing decoding, but substructure sharing can also be used for N -best list rescoring. Figure 4 (logarith-mic scale) illustrates the time for the parser and tag-ger to process N -best lists of varying size, with more substantial speedups for larger lists. For example, for N =100 (a typical setting) the parsing time re-duces from about 20,000 seconds to 2,700 seconds, about 7.4 times as fast. The computational complexity of accurate syntac-tic processing can make structured language models impractical for applications such as ASR that require scoring hundreds of hypotheses per input. We have presented substructure sharing, a general framework that greatly improves the speed of syntactic tools that process candidate hypotheses. Furthermore, we achieve improved performance through up-training. The result is a large speedup in rescoring time, even on top of the already fast hill climbing framework, and reductions in WER from up-training. Our re-sults make long-span syntactic LMs practical for real-time ASR, and can potentially impact machine translation decoding as well.
 Thanks to Kenji Sagae for sharing his shift-reduce dependency parser and the anonymous reviewers for helpful comments.
