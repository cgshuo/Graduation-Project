 Rating data is ubiquitous on websites such as Amazon, Trip-Advisor, or Yelp. Since ratings are not static but given at various points in time, a temporal analysis of rating data provides deeper insights into the evolution of a product X  X  quality. In this work, we tackle the following question: Given the time stamped rating data for a product or service, how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous?
We propose a Bayesian model that represents the rating data as sequence of categorical mixture models. In contrast to existing methods, our method does not require any ag-gregation of the input but it operates on the original time stamped data. To capture the dynamic effects of the rat-ings, the categorical mixtures are temporally constrained: Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time. Our method automatically determines the intervals where anomalies occur, and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions. For learning our model, we propose an efficient algorithm combining princi-ples from variational inference and dynamic programming. In our experimental study we show the effectiveness of our method and we present interesting discoveries on multiple real world datasets.
 H.2.8 [ Database Management ]: Database Applications  X  Data mining ; I.2.6 [ Artificial Intelligence ]: Learning robust mining; anomaly detection; categorical mixtures
Online rating data provides customers valuable informa-tion about products and services and supports their deci-sion making process. Exploiting and presenting this data is a key feature of websites such as Amazon, Yelp, or Tripad-visor. Besides the usefulness of rating data for customers, also companies and manufactures can benefit from it by, e.g., using the data to detect functional weaknesses of their products or changes in the customers X  satisfaction.
In this work, we propose a method for analyzing rating data that incorporates the data X  X  temporal characteristics. Given the time stamped rating data for a product or ser-vice, we aim at detecting the general rating behavior of the users (called the base behavior) as well as time intervals in which these ratings deviate from the general population (anomalous behavior). The base behavior describes the gen-eral quality of a product or service accounting for tempo-ral evolutions, e.g., resulting from decreasing quality due to technical progress of competing products. The anomalies, in contrast, deviate from the base behavior and might, e.g., oc-cur due to spammers trying to push the success of a product or due to changes in the service quality. F ig. 1: Left: Time-stamped rating data analyzed by our method (here: a hotel from TripAdvisor). Right: Extracted base behavior. Upper corner: Anomalous behavior detected in summer 2005.

In Figure 1, we show a real world example for such an effect. The data we analyzed here is a hotel from the Trip-Advisor database. On the left, we show a subset of the original time stamped data. The colors indicate the different star ratings, and the height of each bar the number of these ratings at the current time. Obviously, it is hard to analyze such data by hand. In particular, keep in mind that the ratings are not uniformly distributed over time.

On the right, we illustrate the detected base behavior of our method. As one can see, the base behavior nicely shows the general rating behavior of the users and evolves smoothly over time with primarily medium to high ratings. Addi-tionally, our method has found anomalous behavior in the months of July and August 2005. As shown on the up-per right, in these intervals, the fraction of low ratings (red and yellow) is highly increased compared to the base behav-ior (65% low ratings compared to around 30% in the base behavior). As we will show in Section 5, these anomalies occurred due to problems in the service of the hotel.
In general, our method detects time intervals in dynamic rating data which show anomalous behavior and  X  at the same time  X  it detects the base behavior if the data would not be polluted by anomalies. Besides using this princi-ple to detect weaknesses in products and services, it can generally be used to filter out rating information which be-have anomalous. Thereby, prospective customers might be provided with a cleaned history about the product; or, one might specifically highlight these time intervals to the users to provide the whole picture on a product (since otherwise these anomalous ratings are hidden in the larger set of nor-mal behavior). As an additional benefit, we can exploit our method to predict the base behavior of future ratings. Ac-cordingly, when new ratings arrive, we can estimate whether they match or deviate from the predicted behavior  X  thus, giving indication of new anomalies.

So far, there exists only a single method which analyzes temporal rating data under the presence of anomalies [7]. A potential drawback of [7], however, is the necessary aggrega-tion/binning of the data. When using a coarse aggregation, the temporal effects of the data are not well captured (in the extreme, all data is a single bin). When using a fine ag-gregation, the analyzed distributions might degenerate (in the extreme, many bins are empty). In our model we avoid this problem by directly operating on the time stamped data which is modeled via a sequence of categorical mixture mod-els. We explicitly keep into account that ratings might not uniformly arrive over time. Furthermore, the work [7] as-sumes that anomalies occur at individual points in time. Our work captures the effects of real data much better by accounting for multiple different types of anomalies appear-ing across several time intervals . Our contributions are:  X  Mining task: We present a technique for the analysis of time stamped rating data. Our method detects the base behavior of users as well as time intervals where potential anomalies occurred. Additionally, our technique can be used to predict the rating behavior at future time points.  X  Theoretical soundness: Our method is based on a sound
Bayesian model that represents the rating data as a se-quence of temporally constrained categorical mixture mod-els. To capture the temporal effects of the base behavior we use a state space model on the natural parameters of the categorical distributions.  X  Algorithm design: We propose an efficient algorithm for learning our model which combines principles from vari-ational inference and dynamic programming.  X  Effectiveness: We evaluate our method on different real world datasets and we show its effectiveness by presenting interesting findings.
In this section, we introduce our model for detecting the base rating behavior of users as well as time intervals in which anomalies have been occurred. Following convention, we do not distinguish between a random variable X and its realization X = x if it is clear from the context. As an abbreviation, we denote sets of random variables with the domain, and z is an abbreviation for the set z (  X  )  X  . Vectors of (random) variables are written in bold font, e.g. b , while the entries of the vectors are given in standard font, e.g. b We write t  X  T , as a shortcut for t  X  X  1 ,...,T } .
Preliminaries. The data we consider is a time stamped collection of ratings. Let x ( t ) i denote the i -th rating occurred at time index t , and s ( t ) the time stamp at time index t . At each time index we might observe multiple ratings (e.g. if time stamps are only measured/provided on a daily ba-sis). We denote with n ( t ) the number of ratings at time index t . We assume the data to be ordered according to time, i.e. s ( t ) occurs after s ( t  X  1) . We denote with  X  the elapsed time between time stamp s ( t 1 ) and s ( t  X  ( t ) might be observed since we do not require a fixed bin-ning or aggregation of the rating data. We denote with T the number of time indices (i.e. the number of distinct time stamps) and we assume that users can choose ratings based on a rating scale with S different ratings (e.g. stars from 1 to S ). As an abbreviation for later use, we define n s := |{ i  X  { 1 ,...,n ( t ) } | x ( t ) i = s }| to be the number of ratings at time t which possess the evaluation s  X  S .
We model the rating data including potential anomalies via a probabilistic generative process. An overview of our generative process showing the used variables and their de-pendencies is illustrated by the graphical model in Figure 2. In the following we discuss the details of this process.
Given the observed rating data X , our aim is to extract the base behavior of the users and intervals in time where anomalies occur. Since the observed data might already be polluted by anomalies, we cannot directly use it to estimate the base behavior. Instead, we assume that the observed data is obtained by mixing the (unkown) base user behavior with the (unknown) anomaly behavior. Thus, both types of behavior are represented as latent variables which are not directly observed but inferred by our technique. Technically, at each point in time we have a categorical mixture model as illustrated in Figure 3. To incorporate the temporal prop-erties of the data, we perform additional modeling:
Part 1: Mixing anomalies and base behavior. In contrast to the base behavior, which is present over the whole timespan, we assume that anomalies are rare events (otherwise, they would correspond to the majority of the data, making the term  X  X nomaly X  rather meaningless) and occur only in a specific time interval like, e.g., during a short attack of spam. Technically, instead of using an individual anomaly at each point in time, we restrict the number of anomalies to be small, i.e. smaller than a number K (we discuss later how to choose this parameter), and we tempo-rally constrain the  X  X nfluence X  of each anomaly to a small time interval. For each anomaly, we define this interval by the random variables L k and U k , denoting the lower and up-per bound of time indices at which the k th anomaly occurs. In the following, we will use the function which maps the time index t to the corresponding anomaly (or to 0 if no anomaly exists at this point in time). Here, we require disjointness of the different intervals.
Note that the use of anomaly intervals is a huge advantage in contrast to [7], which models the anomalies at each time point individually. The potential of this extension is best shown for the case of very fine grained time stamps: In this case, we usually expect only a single rating per time stamp, i.e. n ( t ) = 1. To capture anomalies at multiple consecutive points in time, the work of [7] has to use multiple anomalies, while in our work a single interval suffices.

At this point we want to mention the difference between outliers and anomalies [7]: While outliers are irregular be-havior attributed to mostly random corruptions of the data (like, e.g., measurement errors), anomalies are irregular be-havior that follow a specific pattern (like, e.g., time points with consistently low ratings due to a change in the prod-uct X  X  quality). In our work, we consider anomalous behavior.
Giving the above information, the observed data at time t is modeled as a categorical mixture model defined by Here, z ( t ) i is the indicator variable showing which ratings are anomalies.  X  ( t )  X  [0 ... 1] S is the vector describing the base behavior at time t , o k  X  [0 ... 1] S is the k th anomaly behavior, and r k is the mixing proportion. The higher the value of r k , the higher the proportion of anomalies within the corresponding interval. If no anomaly is present at time only the base behavior at this point in time. Thus, the mixture model X  X  components referring to the anomalies are constrained to specific intervals (cf. Figure 3).

For a Bayesian treatment, we equip the variables with corresponding prior distributions. We use due to the properties of conjugacy. The vector  X   X  , for exam-ple, can be used to specify prior knowledge about potential anomalies (e.g. anomalies should represent primarily low rat-ings). In all of our experiments we use  X   X  = 1 and  X   X  = corresponding to non-informative priors.

For the lower and upper bounds we exploit the idea that anomalies should appear primarily in short time intervals. That is, we assume that the probability to observe an ano-maly over a very long time frame is lower than the proba-bility to observe anomalies over only a few time points. We capture this effect by drawing the lower and upper bounds according to an exponential distribution controlled by the duration  X  ( L k ,U k ) of the anomaly interval. Formally p ( L  X  ,U  X  )  X  Please note that this is the joint distribution over all inter-vals since we require their disjointness. The larger  X  , the Fig. 3: Illustration of the generative process using temporally constrained categorical mixture models larger the bias to small anomaly intervals. Per default, if no prior knowledge is given, one should select  X  = 0. In this case, any combination of L  X  ,U  X  is equally likely, corre-sponding to a non-informative prior. Note also that  X  = 0 is a valid prior since the domain of L  X  ,U  X  is finite.
Part 2: Smoothness of the base behavior. So far, we have not specified any distribution on the base behavior  X  ( t ) . The core idea is that the base behavior should evolve smoothly over time according to the general behavior of the users. That is, we want to incorporate the temporal prop-erties of the data.

As pointed out in [6], the (mean) parameters of the cate-gorical distribution and their corresponding Dirichlet hyper-parameters are not amenable to sequential modeling. There-fore, we exploit a similar idea as proposed in [6, 17]: in-stead of operating on the (mean) parameters  X  ( t ) , we oper-ate on the natural parameters (cf. exponential family [5]). For the categorical distribution, the natural parameters are simply given by the logs of the mean values, i.e. b lie on the simplex, the natural parameters are unconstrained, leading to an elegant way of sequential modeling.

In the following, we only operate on the natural param-eters b ( t ) . If the mean parameters are required (e.g. as in Equation 2), we can simply apply the transformation Note that the term b ( t ) S is always 0. Therefore, we can ig-nore it for the remainder of the discussion, thus, operating effectively on an S  X  1 dimensional space.

Given the natural parameters b ( t ) at each time index t  X  T , we model their smoothness by exploiting the idea of linear state space systems [5] in combination with Brow-nian motion [11, 17]. First, we assume an underlying state space e b ( t ) which temporally evolves over time via
We call this space the  X  X moothed X  base behavior. Intu-itively, the state of the smoothed base behavior at time t corresponds to the old state plus a small deviation govern-ment by the noise covariance matrix  X  ( t )  X  Q . The larger the time difference between two observed ratings, the larger the corresponding covariance. That is, we effectively allow a higher change in the base behavior if the elapsed time be-tween two ratings is high. If time points are very close to each other, we allow only small changes in the base behav-ior. In the limit, this discrete-time Gaussian random walk corresponds to Brownian motion [11, 17].
 This process captures naturally the effects of rating data. In the case of movies, for example, one might see many rat-ings appearing in short time frames during the time the movie has been released to the theaters and again many ratings a few month later when the DVD has been released. Both time frames potentially describe different base behav-ior due to different audiences.

Given the smoothed base behavior, the actual base be-havior is now obtained by the simple random process which again allows a small deviation between the base be-havior and its smoothed counterpart. Note that we do not directly impose the temporal evolution between the variables b (  X  ) , but via the state space e b (  X  ) . This additional layer is in particular beneficial if the number of ratings varies strongly between the time points. If we would not use e b (  X  ) , a single time point with a huge amount of ratings could dominate most of the temporal behavior.

Finally, we add corresponding priors to the newly intro-duced parameters. By exploiting the fact of conjugacy it follows that Q is drawn from an Inverse-Wishart distribu-tion, i.e. Q  X  X   X  1 ( X  0 q , X  0 q ). The parameters  X  0 q be used to control the smoothness of the base behavior by providing prior knowledge about the noise covariance. Simi-larly, R follows an Inverse-Wishart distribution and ( e b a Normal-Inverse-Wishart distribution.

Overall, our generative process captures the temporal prop-erties of the data by modeling a smooth base behavior as well as accounting for anomalies which are constrained to occur at certain time intervals. We will show in Section 3 how we perform efficient (approximate) inference for this model.
Model Selection. So far, we assumed the number K of anomalies is given. If not apriori known, we can estimate it via model selection. We choose the Bayesian information criterion [5]. Any other criterion can be used as well.
As we will see in Sec. 3, we will integrate out all latent variables except of  X  = { L  X  ,U  X  , Q , R } . Thus, increasing the value of K by one, increases the number of free parameters in our model by about 2 (the lower and upper bound of the new anomaly interval). This is a slight overestimate since the intervals need to be disjoint and, thus, they are not completely independently free variables. Given this result, we can choose the K minimizing the BIC equation Here, the constant c denotes the parameters of the model which do not increase when increasing K . Since the value of c does not affect the optimal choice for K , we can simply set it to 0. The term L K denotes the likelihood of the data when using K anomaly intervals. We can approximate it with the technique shown in Section 3.

Prediction. Since the base behavior evolves via a linear state space system, we are able to predict the behavior at future points in time. Combining Eq. 5 and 6, it follows Thus, given estimates for e b ( T ) , R , and Q (cf. Section 3), comparing the observed ratings at time T + 1 against the predicted base behavior can be used as an indicator whether new anomalies have been occurred.
While the previous section focused on the model X  X  gener-ative process, we now present our learning technique. That is, given a set of observations X we aim at inferring the val-ues of the hidden variables which best describe the observed data. There are multiple ways to formulate this objective. In this work, we treat the variables  X  = { L  X  ,U as parameters and we are interested in finding their max-imum a posteriori estimate  X  MAP as well as the poste-rior distribution p ( V | X,  X  MAP ) of the latent variables V = { o specific realizations of the latent variables).
Since exact inference in our model is intractable, we com-pute an approximation using variational expectation-max-imization [5]. The idea is to approximate p ( V | X,  X ) by a tractable family of parametrized distributions q ( V |  X ). The parameters  X  are the free variational parameters. These parameters are optimized such that the best approximation between q and p is obtained. This corresponds to the expec-tation step of the variational EM method. Technically, we minimize the Kullback-Leibler divergence between q and p by optimizing over  X . Using Jensen X  X  inequality, minimizing the KL divergence is equivalent to maximizing the following lower bound on the log marginal likelihood [5]: where E q [ . ] denotes the expectation w.r.t. the q distribution and H the entropy.

Given an approximation of p ( V | X,  X ) via q ( V |  X ), we then determine updated parameter values for  X  by again maxi-mizing Equation 8. This corresponds to the maximization step of the EM method. 1
In short, the general processing scheme of our method is to alternatingly maximize L ( X ;  X  ,  X ) w.r.t.  X  and  X . As we will later see, we actually interweave both steps by simulta-neously optimizing parts of  X  and  X  .
In contrast to the frequently used mean field approxima-tion, which assumes a fully factorized distribution, we use p ( V | X,  X )  X  q ( V |  X ) := Y We retain the sequential structure of the smoothed base be-havior in q 5 . Indeed, as described later, we determine q via a Kalman filter where it follows that q 5 ( e b ( t ) Normal distribution given by N ( e b ( t ) | e  X  t | T , P remaining variational distributions we use q 1 ( o k ) = Dir ( o k |  X  k ) q 3 ( z ( t ) i ) = Bernoulli ( z q 2 ( r k ) = Beta ( r k |  X  k , X  k ) q 4 ( b ( t ) ) = N ( b parameters to be optimized.
Note that the distributions q 3 ( z ( t ) i ) and q 3 ( z value. Thus, in practice we do not need to keep track of n many different distributions at time t but it is sufficient to record S many distributions; one for each possible evalua-tion. We denote with  X  s t the variational parameter of the distribtion q 3 for all ratings showing evaluation s at time t . As described above, our goal is to update the values of  X  and  X  by maximizing (or more generally increasing) the value of Equation 8. One crucial requirement of our tech-nique was to ensure the efficiency of our method. In the following, we want to highlight the most important results.
A first naive solution to update the lower/upper bounds of the anomaly intervals would be to test any possible combi-nation. Obviously, this solution is not efficient and requires time O ( T 2 ) already for a single anomaly. We provide a prin-ciple which is linear in the number of time stamps.
We start with the case of a single anomaly and uniform gaps between all time stamps, i.e. it holds K =1 and  X  ( t ) for all t . Equation 3 shows the dependency between L 1 /U 1 and z . Intuitively, the bounds act as a switch on the distribution of z : if z is outside of the interval, it is the trivial 0 distribu-tion; if it is inside, it is a Bernoulli. Accordingly, assuming the posterior distribution for z (or its approximation q 3 given , an optimization of L 1 /U 1 is rather meaningless since one trivially has to capture all time points where the distri-bution is not the constant 0. Therefore, we propose to simul-taneously optimize L 1 /U 1 and q 3 to maximize Equation 8.
Observation: If we know that a time point t fulfills t  X  [ L k ,U k ], the optimal distribution of q 3 ( z ( t ) i puted independent from all other points in time. The opti-mal distribution is obtained by setting its variational param-eter  X  t,i / X  s t to the value as derived in Sec. 3.3. In particular, this value is independent of the actual values of L k and U (knowing that t  X  [ L k ,U k ]). Based on this result we can also compute the entropy for all z ( t ) i fulfilling x ( t ) i = s . If t 6 X  [ L 0) = 1 and we define the entropy H ( q 3 ) to be zero.
Using these results and the derivations of the appendix, as well as removing all terms which are independent of L 1 U 1 and q 3 , we can reformulate Equation 8 to: where we used the fact  X  ( L k ,U k ) = U k  X  L k and we defined
Intuitively, the function f 1 ( t ) measures the  X  X ain X  in the log-likelihood when adding t to the anomaly interval.
The first two terms of Eq. 9 can be removed since they are constant w.r.t. the bounds and q 3 and thus do not affect the optimal solution. Accordingly, maximizing Eq. 8/9 w.r.t. L , U 1 , and q 3 is equivalent to solving ( L  X  1 ,U  X  1 ) = arg max
Since the function f 1 is independent of L 1 /U 1 , i.e. the terms f 1 ( t ) are constant within the current optimization step, we can record all values f 1 ( t ) in a finite array of length T . Thus, the above problem corresponds to the Maximum Subarray Problem . Using Kadane X  X  algorithm [4], this prob-lem can be solved in time O ( T ).
So far, we assumed  X  ( t ) = 1 for all t  X  T . We now generalize the above result to handle varying values for  X  Fig. 4: Handling non-uniformity be applied. Figure 4 top and middle show this principle. Obviously, this principle is not suitable for huge time gaps and the new size of the array can be arbitrarily large.
Considering f 1 ( t ), it becomes apparent that its value eval-uates to  X   X  for each artificial time point. When searching for the subarray with maximal sum, these negative entries will never occur at the beginning/end of the anomaly inter-val [3]. If they would be at the beginning/end, one could easily shorten the interval to obtain a new one with higher sum. Thus, artificial time points are either completely con-tained in the interval or not included at all.

Using this result, we can safely  X  X erge X  all adjacent ar-tificial time points to a single one with the function value  X   X   X  u ( t ) , where u ( t ) is the number of artificial time points between time index t and t  X  1. Clearly, u ( t ) =  X  ( t ) the number of merged artificial time points is bounded by T  X  1. Overall, we can define a new array f 0 of size 2  X  T  X  1 where for y  X  [1 , 2  X  T  X  1]. And we now solve the problem bounded by 2  X  T  X  1 the runtime complexity is O ( T ).
We now extend our result to multiple different anoma-lies/intervals. Using multiple anomalies affects the choice of the optimal q 3 distribution (cf. paragraph  X  X bservation X  in Section 3.2.1). It is no longer sufficient to know whether t  X  [ L k ,U k ] but we also have to know which k we consider. Accordingly, for each anomaly k we have to use its individual function f k /f 0 k to measure the gain of adding a time point to the anomaly interval k . Overall, maximizing Eq. 8 using multiple intervals corresponds to solving arg max We solve the above problem by a dynamic programming technique. The necessary recursions are given by g (1 , 1) = f 0 1 (1) g (1 ,t ) = max { g (1 ,t  X  1) + f 0 g ( k,k ) = m ( k  X  1 ,k  X  1) + f 0 k ( k ) g ( k,t ) = max { g ( k,t  X  1) + f 0 k ( t ) ,m ( k  X  1 ,t  X  1) + f m ( k,k ) = g ( k,k ) m ( k,t ) = max { g ( k,t ) ,m ( k,t  X  1) } Here, m ( k,t ) (for t  X  k ) denotes the maximal value of Eq. 10 when using k intervals and data up to point t . In contrast, g ( k,t ) denotes the maximal value of Eq. 10 when the k th interval is forced to end at the t -th point in time (using k intervals and data up to t ). Obviously, g ( k,t )  X  m ( k,t ) holds. The value of m ( K,T ) is the optimal value of Eq. 10.
We only provide a brief idea of these recursions: Assume we know the optimal intervals when using k  X  1 anomalies and data up to time point t  X  1. Let these intervals be de-noted ( L 1 ,U 1 ) ,..., ( L k  X  1 ,U k  X  1 ). Additionally, assume the optimal intervals for k anomalies and data up to time point t  X  1 are given, denoted with ( L 0 1 ,U 0 1 ) ,..., ( L 0 assume the optimal intervals are given when the last interval is forced to end at time t  X  1 (we call these the g -intervals). Denote these with (  X  L 0 1 ,  X  U 0 1 ) ,..., (  X  L 0 k ,
How will the solution for k intervals and data up to t look like? We can distinguish the following cases: (1) The time point t will be included in the optimal intervals. Obvi-ously, since we are at the last point in time, it can only be represented by the k th interval. We can distinguish two subcases: (1a) The time point t is the beginning of the k th interval. In this case, the optimal intervals are ( L 1) + f 0 k ( t ). Since the last interval already ends at t , we also have g ( k,t ) = m ( k  X  1 ,t  X  1) + f 0 k ( t ). (1b) The time point t is not the beginning of the k th interval. Thus, the opti-mal solution needs to be (  X  L 0 1 ,  X  U 0 1 ) ,..., (  X  L m ( k,t ) = g ( k,t ) = g ( k,t  X  1) + f 0 k ( t ). (2) The time point t will not be included in the opti-mal intervals. In this case, since we want to find k in-tervals, the optimal solution is ( L 0 1 ,U 0 1 ) ,..., ( L m ( k,t ) = m ( k,t  X  1). For the g -intervals we have to dis-tinguish two cases: (2a) The time point t is the beginning of the k th g -interval. In this case, the new g -intervals are ( L 1) + f 0 k ( t ). (Note that we use the optimal intervals from m ( k  X  1 ,t  X  1), not the g -intervals!) (2b) The time point t is not the beginning of the k th g -interval. Thus, leading to the
Exploiting the fact g ( x,y )  X  m ( x,y ) and that we want to maximize m ( x,y ), leads to the recursion as defined above. It is easy to add data structures to the method which record the start/end positions of the optimal intervals. Solving the above recursions via dynamic programming, we obtain:
Theorem 1. The optimal values for L  X  ,U  X  and the opti-mal distributions q  X  3 can be computed in time O ( K  X  T ) .
Following the principle of [5], the optimal distribution for q can be determined by Here, the constant const absorbs all terms which are inde-pendent of z ( t ) i and, thus, do not affected the optimal dis-tribution of q 3 . The term E with respect to the distribution q taken overall all variables except of z ( t ) i . Assuming k ( t ) = k 6 = 0, and using the results from the appendix, it follows that where s = x t i . Therefore, the optimal value of the variational
The same principle can be applied for the distributions q and q 2 , leading to  X  k =  X   X  +
Optimizing the base behavior. The base behavior can be updated for each b ( t ) independently. Removing all terms from Equation 8 which are independent of b ( t ) leads to [ X The first term is given in the appendix, and H ( q 4 ( b ( t ) 2 ln(2  X ev
We absorbed all terms which are independent of b ( t ) into the constants c i . Overall, Eq. 11 can be written as a function of  X  ( t ) and v ( t ) , which we optimize using gradient ascent.
Optimizing the smoothed base behavior. Since our model corresponds to a linear system, we can use a Kalman filter/smoother to determine the distribution of q 5 . We use the Rauch-Tung-Striebel smoother. Thus, the distribution of q 5 can be computed efficiently by a forward and backward pass, leading to an update with runtime complexity O ( T ).
Since the outputs b ( t ) of the dynamic system are not ob-servations but distributions, we slightly adapt the Kalman update/innovation equations. Following the standard cal-culus of Kalman filters, the predicted mean and covariance matrix for time t (given data up to time t  X  1) are given by e  X  the measurement at time t , the measurement residual can be creased variance due to the uncertainty of the base behavior. Letting the Kalman gain be defined by K t = P t | t  X  1  X  S see that the Kalman gain is smaller for time points showing a high variance, i.e. high uncertainty, in the base behavior. These points affect the smoothed base behavior less strongly.
Continuing with the standard calculus, the updated mean and covariance are e  X  t | t = e  X  t | t  X  1 + K t  X  e t and P P t | t  X  1  X  ( I  X  K t ) T + K t  X  R  X  K T t . Here, we used the Joseph form of the covariance update equation since it holds for any value of K t . For the backward pass, the RTS smoother P to space limitations, we kindly refer to the rich literature on Kalman filter/smoother, for details about the derivations.
Optimization of Q and R. Updating Q and R follows from the properties of the conjugate prior. Note that Eq. 5 can also be written in the form e b ( t )  X  e b ( t  X  1)  X  X  (0 ,  X  Also, by the definition of the normal distribution it holds that N ( x | 0 ,  X   X   X ) =  X   X  d/ 2  X N ( x / dimensionality of the distribution. Since the terms  X   X  d/ 2 are constant when optimizing the log-likelihood, they can be ignored. Thus, Q can be seen as the covariance matrix of a Normal distribution with known mean of zero. Corre-spondingly, we can use the Inverse-Wishart distribution as its (conjugate) prior. Following the results of conjugacy, the posterior distribution of Q is an Inverse-Wishart distribu-tion W  X  1 ( X  q , X  q ) with  X  q =  X  0 q + T  X  1 and scale matrix  X  which can easily be computed by plugging in the known expectations. Given this distribution, the MAP estimate for Q can efficiently determined by selecting the mode of the Inverse-Wishart distribution, i.e. Q  X  = 1 ( S  X  1)+1+  X  The same principle can be applied for R .
Using the above optimizations and update equations, our method iteratively recomputes the values for  X  and  X . If the change in Equation 8 is less than 0 . 1% we assume con-vergence and terminate. Based on the previous results, and assuming that K,S &lt;&lt; T , each iteration is linear in the number of time stamps, i.e. we have a complexity of O ( T ).
Spotting anomalies in rating data: So far, only [7] considers the temporal analysis of rating data incorporat-ing potentially anomalous behavior. The work models the rating data as distributions over time. As mentioned in the introduction, it requires an aggregation/binning of the data and it cannot handle intervals of anomalies. We compare our technique against [7] in the experimental analysis.
Modeling of temporal continuous data: Similar to the work [7], traditional time series modeling methods such as vector autoregression [14, 13] or Kalman filter/smoother [5], analyze continuous data. They are not directly suited for our scenario of categorical data (or require a problem-atic binning). Furthermore, traditional approaches for time series modeling are sensitive to outliers. Thus, these models fail to find good approximations of the data corrupted by anomalies. Therefore, robust techniques to handle outliers have been proposed [16]. These methods are designed to handle outliers which are attributed to mostly independent, random corruptions of the data, while our work is designed to handle anomalies following a specific pattern.

Since in our work the Kalman filter operates on the (clean) base behavior, i.e. the anomalies have been  X  X emoved X  by the other mixture model components, the problem of anoma-lies is circumvented. We compare our technique against a Kalman filter in the experimental analysis.

Modeling of temporal documents: One might repre-sent the ratings at a certain point in time as a document with the words corresponding to the ratings X  evaluations. Modeling temporal document collections is handled by dy-namic topic mining [6, 17, 2]. Applying these methods on the  X  X ocuments X  generated via the ratings is questionable since each document most likely would contain only a single  X  X ord X . Ignoring this issue, further problems for our scenario are: First, [6, 2] require a binning of the documents in fixed time slots. Second, [6, 17] require that topics exist over the whole lifetime. In our work, however, anomalies exist only in specific time intervals. While [2] allows topics to appear and disappear, they prefer smooth evolutions. In our case, how-ever, anomalies abruptly appear/disappear in time. Also, all of these techniques are (of course) designed to detect multiple topics. In our scenario, however, we want to find a single base behavior which captures the general temporal evolution, enriched by a few number of anomalies.

Related applications: Multiple techniques have been proposed in the area of outlier detection [1]. While the ma-jority of techniques tackles the case of independently dis-tributed data, time-series outlier detection and outlier de-tection for streaming data are also an active field of research [1]. Both areas differ from our work since they are designed for continuous data. Also, most existing techniques con-sider outlier in the sense of independent, random errors in the data. Change detection techniques detect points in time where the state of the underlying system has changed [15]. A change might not generally indicate anomalous behavior. Indeed, even the base behavior might change over time.
Studying product ratings has been done in multiple re-search areas, all following different goals and objectives. Recommender Systems incorporate ratings and their tem-poral information [9, 10] to improve the prediction perfor-mance. Opinion mining aims at extracting the sentiment of users regarding specific products or features of a product [18]. Modeling of social rating networks, e.g., to compactly describe the underlying mechanism driving the network or to generate synthetic data, has been studied, e.g, in [12].
None of the existing methods is designed to detect anoma-lies and the underlying evolving base behavior in rating data.
We applied our method (called SpotRate due to its po-tential to spot anomalies in rating data) on over six mil-lion product ratings representing varies categories: an ex-tract of the Amazon website [8] evaluating multiple different products, another subset of the Amazon website evaluating
Fig. 5: Runtime vs. number of time stamps food products 3 , ratings of restaurants in the area of Phoenix based on Yelp, and an extract of the TripAdvisor website 4 for hotel ratings. The data consists of tuples representing the ID of a product/service to be rated, the user who evalu-ated the product, the time stamp when the rating occurred, and a star rating in the range from 1 up to 5. Addition-ally, these datasets contain textual reviews, which we used to understand and describe the results of our method.
Besides these real world datasets we used synthetic data generated based on the presented process to analyze the scal-ability and robustness of our method.
We briefly analyze the runtime of SpotRate. The runtime is primarily affected by the number of time stamps T and the rating scale S . The actual number of ratings does not affect the runtime (cf. Sec. 3.1.1). For the runtime analy-sis, we selected the product B00003TL7P from the Amazon dataset and we extended it to different length (from 1,000 to 100,000) by concatenation. Besides the original rating scale of S = 5, we used a rating scale of S = 3 by merging 1 / 2 and 4 / 5 ratings. All experiments were conducted on commodity hardware with 3 GHz CPU X  X  and 4 GB main memory.
 The results are shown in Fig. 5. Confirming our study of Sec. 3.5, the runtime increases linearly, showing the method X  X  high scalability (note the slope of 1 in the log-log plot). The overall runtime for 100,000 time stamps (which would cor-responds to 273 years when measured on a daily basis) is only about 158 minutes on commodity hardware. A brief study shows that the currently most rated products have around 20,000 (Amazon: Kindle Fire) or 8,000 (Yelp: Bot-tega Louie) ratings. Thus, even when considering the finest granularity, we highly exceed this number.

Additionally, we measured the runtime of our method when ignoring the time required for the Kalman smoother (dashed lines). As shown, the Kalman smoother contributs to around 90% of the absolute runtime. The remaining parts of our method are highly efficient.

We also studied the effect of the number of anomalies K on the runtime. According to Sec. 3.2.3, K linearly affects the runtime of the dynamic programming technique. Since the Kalman smoother (whose runtime is independent of K ) accounts for most of the absolute runtime, we only observed a very small change of only a few seconds. Thus, overall, only T and S influence our method X  X  practical applicability.
In the following, we analyze the effectiveness of SpotRate considering different aspects. We start with the model se-lection principle. For this experiment, we generated syn-thetic data according to our model. We used 4000 ratings with 5 anomaly intervals. Figure 6 shows on the (first) y-axis the obtained log-likelihood of our method when varying the number K of potential anomalies. Obviously, the gen-eral trend shows that increasing K also increases the log-likelihood: more flexibility to describe the data is given. A very high increases is obtained until the value of 5, which corresponds to the true number of anomalies. After this point, the benefit of allowing further anomalies decreases.
This effect is well captured by the BIC score, which is shown on the second y-axis of the figure. The minimal BIC value is obtained for the value of 5. Thus, the model se-lection principle introduced before can be used as a good indicator how to select the number of anomalies.

The same behavior can be observed for real world data as, e.g., shown in Fig. 7. Here we plotted the log-likelihood and BIC score for a coconut-water sold on Amazon (cf. Sec. 5.4). Again, one sees a clear minimum of the BIC value, indicating that three anomaly intervals describe the data very well.
Next, we analyze our iterative optimization. In Fig. 8 we analyze how the log-likelihood increases when we increase the number of iterations until convergence. That is, on the x-axis we count how often the variables have been up-dated, while the y-axis shows the log-likelihood. We plotted the curves for different values of K , again for the product B00003TL7P. As expected, the first iterations lead to the highest improvement in the log-likelihood. Still, we see an improvement in the later iterations, showing the effective-ness of the optimization step. As also shown in the previous experiment, a higher value of K leads to a better likelihood. Additionally, for this product, we observed that a smaller number of intervals can lead to a lower number of required iterations. In general, however, the difference in the number of iterations was not as significant as shown for this product.
Finally, we analyze the effect of  X  . Per default, a value of 0 can be selected to realize a non-informative prior. In Figure 9, we varied the value of  X  between 1 and 0. We selected K = 10. As shown, for larger values of  X  , shorter intervals are preferred. In particular, for  X  = 1 the average interval length is close to the shortest possible length of 1. For  X  = 0 larger intervals are captured. Note that  X  = 0 does not mean that the whole set of time stamps is represented as an anomaly interval. Even in the case of  X  = 0, we only report time intervals where the behavior is anomalous.
We compare SpotRate against the related technique RLA [7] and a Kalman smoother. Doing a fair comparison be-tween these approaches is challenging since the data they analyze and goals they follow are different. In particular, the
Fig. 8: Convergence analysis work [7] requires an aggregation of the data since it operates on the rating distributions. Thus, e.g., measuring the like-lihood on the categorical data is questionable. Since RLA, however, is the only existing technique which also analyzes dynamic rating data, we try to study some effects.

For comparing the methods, we use two principles: In the first experiment, we compare the base behavior detected by the methods against the known base behavior for synthet-ically generated data. The base behavior is continuous in our model as well as in RLA, and for the Kalman smoother. Thus, it is fair to, e.g., measure the Frobenius norm between the ground truth base behavior and the detected ones. We generated data with 1000 time points and added a vary-ing number of anomaly intervals to it, each covering 10 time points. We ensured that the anomaly intervals exactly match the aggregation required for RLA. Thus, this method gets a huge advantage since this assumption does not nec-essarily hold for real data. Fig. 10 (left) shows the results: Our method obtains the lowest error, it is able to detect the hidden base behavior. The Kalman smoother cannot handle anomalies and shows a high deviation to the ground truth.
In a second experiment, we evaluated whether the meth-ods are able to detect the anomalous points in time (this is only possible for SpotRate and RLA). As shown in Figure 10 (right), our method almost perfectly detects all anomalous points in time. RLA in contrast is not able to spot all points, which also explains the previously observed higher error to the base behavior. Overall, our method outperforms the competing techniques in detecting the correct base behavior as well as spotting the anomalous points in time. In the following, we will demonstrate the application of SpotRate by illustrating some of our interesting discoveries. (1) We start with the example illustrated in Fig. 1. It represents a hotel in the Caribbean evaluated on TripAd-visor. While understanding the original time-stamped data is difficult, the extracted base behavior allows an easy un-derstanding: clearly, the hotel is evaluated with mostly 4 and 5 stars. Our method found anomalous behavior in July and August 2005. In this time frame, the negative ratings highly increased. Analyzing the reviews at the detected time points, the reviewers criticized  X  X he restaurants with ridicu-lous reservation rules X   X  often showing overbooking and  X  X he nonfunctional air-conditions X  . These reviews indicate that in the given months the service of the hotel has dropped, potentially due to a highly increased number of guests. Our method was able to spot these anomalies, and it successfully smoothed out these points from the base behavior. (2) Next, we show the result for a coconut-water sold on Amazon (http://www.amazon.com/dp/B000CNB4LE).
 Applying our method leads to the base behavior as shown in Figure 11. The three detected anomaly intervals appear at the end of 2010. As shown next to the figure, the de-tected anomalies are described by distributions o k repre-senting primarily low ratings. They clearly deviate to the base behavior. Inspecting the product X  X  reviews during these times, most customers are not satisfied with the  X  X ew plas-tic bottles X  the manufacturer has introduced, leading to a bad taste. Later time points do not show this anomalous behavior, indicating that the manufacturer has solved this problem (  X  X  can understand a lot of the initial bad reviews as I thought the new plastic bottle had a bad after taste. ... I can say that the taste is much improved ...  X ). (3) Next, we want to show the benefit of extracting an evolving base behavior. Figure 12 shows the base behavior of a baby bouncer (B00005QI1G) from the Amazon data. Looking at its evaluation, it is recognizable that the majority of reviewers evaluated this product with 5 stars. At the later time points, however, the number of low and medium ratings increases. Note that these intervals are not classified as anomalies but they represent the general evolution of the product. A closer look at the product X  X  reviews at these time points explains that over time the customers were more and more unsatisfied by the product since it is  X  X ice to play but not long lasting X  and the  X  X attery simply does not last very long with the vibrating feature X  .

Discoveries via prediction. Finally, we want to show the potential of our method to detect anomalies via predic-tion. According to Eq. 7, we can predict the base behavior at future points in time. By comparing it against newly ar-riving ratings, anomalies can be spotted. We removed from all restaurants of the Yelp dataset the last 10 points in time. We applied our method on the remaining data. Figure 13 shows three restaurants whose predicted base behavior (left bar [a] of each diagram) highly deviates to the observed rat-ings (right bar [b]), thus, potentially indicating anomalies.
Inspecting the reviews of the first restaurant, we see com-ments like  X  X  X  X e been eating at Stacy X  X  for over a year so it pains me to kill them but the service [...] was pathetic. [...] I don X  X  know if its a new employee or something going wrong but I X  X  probably not going back... X  . Thus, indicating that the service quality of the (previously very highly rated) restaurant has suddenly dropped.

For the second restaurant, we observed comments like  X  X ll the prices have went up X  and  X  X he picture of the menu and prices is out dated X  , which again indicates a recent deviation to the previous behavior, potentially due to increased prices.
Finally, the reason for the abruptly appearing low rat-ings of the third restaurant seems to be caused by expand-ing/remodeling the old building. The old atmosphere of the restaurant seems not to be preserved and the larger capac-ity could not be handled by the service staff:  X  X he expanded building is nice [...] but something was lost. we didn X  X  have F ig. 11: Base behavior and anomalies (in-tervals spotted at the end of 2010). that hometown feel. we miss all the signatures and pictures. X  and  X  X  think its rad that you expanded, but if you cant handle the customer load then whats the point? X  .

Overall, by modeling the temporal evolution of the base behavior, our method is able to detect these newly occur-ring anomalies, which can then, e.g., be used to inform the corresponding companies.
We developed the method SpotRate for analyzing time stamped rating data. Our method detects the users X  base behavior as well as time intervals representing anomalies. We proposed a sound Bayesian framework which represents the rating data via temporally constrained categorical mix-ture models. It accounts for the temporal evolution of the base behavior and enables us to predict the rating behavior for newly occurring ratings. We developed an efficient algo-rithm which exploits principles of variational inference and dynamic programming. Our experimental study has shown the potential of our method to spot anomalies and to use the base behavior for studying the evolution of a product. [1] C. C. Aggarwal. Outlier Analysis . Springer, 2013. [2] A. Ahmed and E. P. Xing. Timeline: A dynamic [3] F. Bengtsson et al. Computing maximum-scoring [4] J. Bentley. Programming pearls: algorithm design [5] C. M. Bishop. Pattern recognition and machine [6] D. M. Blei and J. D. Lafferty. Dynamic topic models. [7] N. G  X  unnemann, S. G  X  unnemann, and C. Faloutsos. [8] N. Jindal and B. Liu. Opinion spam and analysis. In [9] N. Koenigstein, G. Dror, and Y. Koren. Yahoo! music [10] Y. Koren. Collaborative filtering with temporal [11] G. F. Lawler. Introduction to stochastic processes . [12] K. Lerman. Dynamics of a collaborative rating [13] R. B. Litterman. Forecasting with bayesian vector [14] H. L  X  utkepohl. New introduction to multiple time series [15] X. Song, M. Wu, C. M. Jermaine, and S. Ranka. [16] J.-A. Ting, E. Theodorou, and S. Schaal. Learning an [17] C. Wang, D. M. Blei, and D. Heckerman. Continuous [18] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating Let [[ . ]] denote the Iverson bracket, it holds: E q [[[ z [ln r k ]=  X  (  X  k )  X   X  (  X  k +  X  k ) E q [ln(1  X  r k )]=  X  (  X  Given the definition of the distribution p , it follows:  X  For k ( t ) = k 6 = 0 it holds:  X   X  E q [ln p ( x ( t ) i | ... )] = E q [[[ z ( t ) i = 1]]]  X   X   X 
E q [ P i ln p ( x ( t ) i | ... )] = E q  X   X 
