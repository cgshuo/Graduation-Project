 In an interactive classi cation application, a user may nd it more valuable to develop a diagnostic decision support method which can reveal signi cant classi cation behavior of exemplar records. Such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more e ective classi cation methods. In this paper, we propose the Sub-space Decision Path method which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most signif-icant classi cation characteristics for a given test instance are revealed. In addition, the SD-Path method can pro-vide enormous interpretability by constructing views of the data in which the di erent classes are clearly separated out. Even in cases where the classi cation behavior of the test instance is ambiguous, the SD-Path method provides a di-agnostic understanding of the characteristics which result in this ambiguity. Therefore, this method combines the abili-ties of the human and the computer in creating an e ective diagnostic tool for instance-centered high dimensional clas-si cation.
 H.2.8 [ Database Management ]: Database Applications Algorithms visual data mining, classi cation
High dimensional data is a challenge to subspace based classi cation methods such as the decision tree because of Copyright 2005 ACM 1 X 59593 X 135 X  X /05/0008 ... $ 5.00. the large number of combinations of dimensions (or sub-spaces) which have classi cation power. The basic limita-tion of such methods is that they try to create a succint summary of small number of discriminatory subspaces from an exponential number of possibilities. The particular pat-tern which is most suitable for the classi cation of a given record is speci c to that record and could be present in one of many partially overlapping subspace clusters. The succint summary may fail to capture such instance-speci c charac-teristics. This incompleteness in data characterization may result in the particular structure of the tree to be more or less suited to particular kinds of test instances. We note that this incompleteness problem extends to most classi ca-tion models such as rule based systems, neural networks, or bayesian methods in which the aim is to create a summarized and eciently usable model of the relationship between the feature and class variables [3], rather than providing com-prehensive exploratory ability for individual test instances.
In this paper, we propose an open-ended and test-instance speci c hierarchical decision process in which the primary aim is to provide diagnostic ability. We note that this scheme is not intended as an alternative to current batch-processing methods, which are relevant to classifying large numbers of test instances. This approach is more suitable for cases where detailed diagnostic information is required about in-dividual test instances during the classi cation process. We note that most previous interactive approaches [2] create a decision tree on the entire data set, and is therefore not suited to test-instance speci c diagnosis.

This paper is organized as follows. The quanti cation of instance-speci c subspaces is discussed in section 2. In section 3, we will utilize this quanti cation to develop the subspace decision path method. The empirical results are presented in section 4, and the conclusions are discussed in section 5.
In order to identify an appropriate quanti cation of the level of separability of the di erent classes in subspaces, we need a technique which is particularly sensitive to the class discrimination behavior in the locality of a given point, and is also amenable to the determination of arbitrarily shaped patterns. One way of intuitively characterizing the discrim-ination in a subspace is to quantify the di erence in class distribution at each point in the space. To this e ect, we use the method of kernel density estimation.

In kernel density estimation, we nd a continuous estimate of the density of a set of points. Let us assume that we have a data set D with N points and dimensionalit y d . The set of points in D are denoted by X 1 : : : X N . Let us further assume that the k classes in the data are denoted by C 1 : : : C num ber of points belonging to the class C i is n i , so that P i =1 n i = N . We assume that the data set asso ciated with the class i is denoted by D i . The probabilit y densit y at a given point is determined by the sum of the smo othed values of the kernel functions K h ( ) asso ciated with eac h point in the data set. Thus, the densit y estimate of the data set D at the point x is de ned as follo ws: The kernel function is a smo oth unimo dal distribution suc h as the gaussian function: We note that the kernel function is dep enden t on the use of a parameter h whic h is the level of smo othing. The ac-curacy of the densit y estimate dep ends upon this width h , and sev eral heuristic rules have been prop osed for estimat-ing the bandwidth. The widely used Silv erman rule [4] sets h = 1 : 06 N 1 = 5 , where 2 is the variance of the N data points.

We note that the value of the densit y f ( x; D ) may dif-fer considerably from f ( x; D i ) because of the di erence in distributions of the di eren t classes. Corresp ondingly , we de ne the accuracy density A ( x; C i ; D ) for the class C follo ws: We note that the above expression alw ays lies between 0 and 1. The higher this value, the greater the relativ e densit y of C compared to the other classes. We further note that the sum of the accuracy values over the di eren t classes is equal to one.
Another related measure is the inter est density at a given point x . The interest densit y of the class C i at x is denoted by I ( x; C i ; D ), and is de ned in an analogous way to the accuracy densit y. In this case, the interest densit y is the ratio of the densit y of the class C i to the overall densit y of the data. Therefore, we de ne the inter est density I ( x; C i the class C i as follo ws: The class C i is over-represen ted at x , when the interest densit y is larger than one. The interest densit y is more rev ealing from a statistical persp ectiv e than the accuracy densit y, since it is not biased by the initial distribution of classes. The dominant class at the coordinate x is denoted by CM ( x; D ), and is equal to argmax i 2f 1 ;:::k g I ( x; C Corresp ondingly , the maxim um interest densit y at x is de-noted by IM ( x; D ) = max i 2f 1 ;:::k g I ( x; C i ; D ). Both the interest and accuracy densit y are valuable quan ti cations of the level of dominance of the di eren t classes. The interest densit y is more e ectiv e at comparing among the di eren t classes at a given point, whereas the accuracy densit y is more e ectiv e at pro viding an idea of the absolute accuracy at a given point.

So far, we have assumed that all of the above computa-tions are performed in the full dimensional space. However, we can also pro ject the data onto the subspace E in or-der to perform this computation. Suc h a calculation would quan tify the discriminatory power of the subspace E at x . In order to denote the use of the subspace E in any com-putation, we will sup erscript the corresp onding expression with E . Thus the densit y in a given subspace E is denoted by f E ( ; ), the accuracy densit y by A E ( ; ; ), and the in-terest densit y by I E ( ; ; ). Similarly , the dominan t class is de ned using the subspace-sp eci c interest densit y at that point, and the accuracy densit y pro le is de ned for that particular subspace. An example of the accuracy densit y pro le (of the dominan t class) in a 2-dimensional subspace is illustrated in Figure 2(a). The test instance is also la-beled in the same gure in order to illustrate the relation-ship between the densit y pro le and test instance. We note that for large data sets estimated using the kernel densit y technique, the interest densit y pro les would have exactly the same shap e as the accuracy densit y pro les, except that they are scaled di eren tly.

The subspace speci c determination of the interest den-sity I E ( t; C i ; D ) at the test instance t is quite valuable, since it can be used to determine those characteristics whic h are most rev ealing about the class beha vior of t . In order to do so, one may wish to nd those pro jections of the data in whic h the interest densit y value IM E ( t; D ) is the maxim um. It is quite possible that in some cases, di eren t subspaces may pro vide di eren t information about the class beha vior of the data; these are the dicult cases in whic h a test in-stance may be dicult to classify accurately . In suc h cases, the user may need to isolate particular data localities in whic h the class distribution is further examined by a hier-archical exploratory pro cess.
In this section, we discuss metho ds for exploratory con-struction of decision paths. For a given test example, the end user is pro vided with unique options in exploring various characteristics whic h are indicativ e of its classi cation. To this e ect, we use the subspace determination pro cess dis-cussed in the previous section. The subspace determination pro cess nds the appropriate local discriminativ e subspaces for a given test example. These are the various possibilities (or branc hes) of the decision path whic h can be utilized in order to explore the regions in the localit y of the test in-stance. In eac h of these subspaces, the user is pro vided with a visual pro le of the accuracy densit y. This pro le pro-vides the user with an idea of whic h branc h is likely to lead to a region of high accuracy for that test instance. This vi-sual pro le can also be utilized in order to determine whic h of the various branc hes are most suitable for further explo-ration. Once suc h a branc h has been chosen, the user has the option to further explore into a particular region of the data whic h has high accuracy densit y. This pro cess of data localization can quic kly isolate an arbitrarily shap ed region in the data con taining the test instance. This sequence of data localizations creates a path (and a locally discrimina-tory com bination of dimensions) whic h rev eals the underly-ing classi cation causalit y to the user.

In the event that a decision path is chosen whic h is not Algorithm Subsp aceDe cisionPath(T est Inst.: t , Data: D ,
MaxDim: l , MaxBr anchF actor: b max , MinIR atio: ir min begin
PATH= fDg ; while not(termination) do begin end ; end
Figure 1: The Subspace Decision Path Metho d strongly indicativ e of any class, the user has the option to bac ktrac k to a higher level node and explore a di eren t path of the tree. In some cases, di eren t branc hes may be indica-tive of the test example belonging to di eren t classes. These are the \am biguous cases" in whic h a test example could share characteristics from multiple classes. Man y standard mo deling metho ds may classify suc h an example incorrectly , though the subspace decision path metho d is much more e ectiv e at pro viding the user with an intensional kno wl-edge of the test example because of its exploratory approac h. This can be used in order to understand the causalit y behind the ambiguous classi cation beha vior of that instance.
The overall algorithm for decision path construction is il-lustrated in Figure 1. The input to the system is the data set D , the test instance t for whic h one wishes to nd the diagnostic characteristics, a maxim um branc h factor b max and a minim um interest densit y ir min . In addition, we in-put the maxim um dimensionalit y l of any subspace utilized in data exploration. The value of l = 2 is esp ecially in-teresting because it allo ws for the use of visual pro le of the accuracy densit y. We note that even though it is natu-ral to use 2-dimensional pro jections because of their visual interpretabilit y, the data exploration pro cess along a given path rev eals a higher dimensional com bination of dimensions whic h is most suitable for the test instance. The branc h fac-tor b max is the maxim um num ber of possibilities presen ted to the user, whereas the value of ir min is the corresp onding minim um interest densit y of the test instance in any sub-space presen ted to the user. The variable PATH consists of the pointers to the sequence of successiv ely reduced train-ing data sets whic h are obtained in the pro cess of interactiv e decision tree construction. We initialize the list PATH to a single elemen t whic h is the pointer to the original data set D . At eac h point in the decision path construction pro cess, we determine the subspaces E 1 : : : E q , whic h have the great-est interest densit y (of the dominan t class) in the localit y of the test instance t . This pro cess is accomplished by the pro-cedure ComputeClassifSubsp aces and is describ ed in detail in a later section. Once these subspaces have been deter-mined, the densit y pro le is constructed for eac h of them by the pro cedure ConstructDensityPr o le . We note that even though one subspace may have higher interest densit y at the test instance than another, the true value of a subspace in separating the data localit y around the test instance is often a sub jectiv e judgemen t whic h dep ends both upon the inter-est densit y of the test instance and the spatial separation of the classes. Suc h a judgemen t requires human intuition whic h can be harnessed with the use of the visual pro le of the accuracy densit y pro les of the various possibilities. These pro le pro vides the user with an intuitiv e idea of the class beha vior of the data set in various pro jections. If the class beha vior across di eren t pro jections is not very consis-ten t (di eren t pro jections are indicativ e of di eren t classes), then suc h a node is not very rev ealing of valuable informa-tion. In suc h a case, the user may choose to bac k trac k by specifying an earlier node on PATH from whic h to start further exploration.

On the other hand, if the di eren t pro jections pro vide a consisten t idea of the class beha vior, then the user uti-lizes the densit y pro le in order to isolate a small region of the data in whic h the accuracy densit y of the data in the localit y of the test instance is signi can tly higher for a particular class. This is achiev ed by the pro cedure Iso-lateData . This isolated region may be a cluster of arbitrary shap e dep ending upon the region covered by the dominating class. However, the use of the visual pro le helps to main-tain the interpretabilit y of the isolation pro cess in spite of the arbitrary con tour of separation. A detailed description of this pro cess will be pro vided in a later section. The pro ce-dure returns the isolated data set L 0 along with a num ber of called the accuracy signi c anc e p ( L 0 ; C i ) of the class C pointer to this new data set L 0 is added to the end of PATH. At that point, the user decides whether further exploration into that isolated data set is necessary . If so, the same pro-cess of subspace analysis is rep eated on this node. In the follo wing subsections, we will pro vide further descriptions of the individual pro cedures in decision path construction.
In the previous section, we discussed how the kernel den-sity metho d can be used in order to analyze the discrimi-nation beha vior of the data in di eren t subspaces. In this section, we will discuss the actual details of the pro cedure ComputeClassifSubsp aces . This determines the alternativ e subspaces at a given node. The input to the pro cedure is the test instance t , the data set L , the branc h factor b the minim um interest densit y ir min , and the maxim um di-mensionalit y l of the subspaces in whic h the classi cation beha vior is to be determined. Since, we utilize visual pro les in order to pro vide the user an understanding of the data, the natural choice of l is 2, though higher dimensional sub-spaces con taining signi can t classi cation patterns are also disco vered by the sequence of hierarc hical decisions made by the user.
The overall subspace determination pro cedure uses a roll-up mec hanism analogous to [1] in whic h the accuracy den-sity is computed at the test instance t in di eren t subspaces. The set of discriminatory subspaces is main tained in F . The set of all k -dimensional candidate pro jections is denoted by S . The value of S 1 is the set of all 1-dimensional pro jec-tions f 1 ; : : : d g . The algorithm starts by initializing F to the b max 1-dimensional subspaces whic h have the highest interest densit y at the test instance t . In eac h iteration, the set S k is generated from the set S k 1 in an iterativ e mec h-anism in whic h we join the candidates in S i with the set of all singleton subspaces in S 1 . Those subspaces SS whic h have interest densit y IM SS ( t; L ) higher than any subspace in F are retained. This pro cess is con tinued until we de-termine the (at most) b max most discriminatory subspaces with dimensionalit y at most l , and interest densit y greater than ir min . For lower dimensionalities suc h as l = 2, the above pro cedure can be executed relativ ely ecien tly since the densit y needs to be calculated only at the test instance t . This can be achiev ed in a single scan of the data in order to calculate the additiv e kernel densit y value at t for the di er-ent 1-dimensional and 2-dimensional candidate subspaces. We further note that even though eac h node con tains only 2-dimensional candidates, the nal com bination of dimen-sions is determined by the path decided by the user.
Once the most discriminatory subspaces have been deter-mined at a given node, we construct the visual pro le of the accuracy densit y in these pro jections in the pro cedure ConstructDensityPr o le . We pro ceed in two steps. In order to construct the visual pro le, we compute the accuracy of the dominan t class at a set of discrete grid points. Eac h attribute range is divided into a set of interv als. The in-tersection of these interv al divisions form the grid points at whic h the accuracy densit y values are computed. The sur-face plot of these densit y values pro vides a good idea of the regions most closely related to the test instance whic h have high accuracy densit y. An example of suc h a pro le is illus-trated 1 in Figure 2(a). It is clear that the test instance is located in a region with high elev ation of the accuracy pro-le. This is because the subspace was speci cally selected in order to exp ose those subspaces in whic h the test instance has high accuracy .

However, the real value of a subspace can only be judged based on the beha vior of the entire pro le and the spatial separation of this elev ation with other regions. This is be-cause the accuracy or interest densit y at t does not pro vide a complete understanding of the class beha vior with resp ect to the remaining data set. For example, consider the sub-spaces E 1 and E 2 . The subspace E 1 may have a higher value of IM E 1 ( t; L ) ( AM E 1 ( t; L )) than the corresp onding value IM E 2 ( t; L ) ( AM E 2 ( t; L )). However, the true dis-criminatory power can only be distinguished by using the overall spatial distributions of the di eren t classes. In suc h cases, the intuition of the human is very useful in determin-ing whether a small data localit y around the test instance sho ws a signi can tly higher accuracy densit y than the re-maining data. In suc h cases, it may be desirable to isolate the particular data localit y for further exploration.
In all future accuracy pro les, we will assume that the class for whic h the accuracy densit y is displa yed is the dominan t one.
An easy way of isolating smaller segmen ts of the data is for the user to specify an accuracy densit y threshold . Suc h a choice can be made relativ ely easily by the user, once the visual pro les of the data are directly available to him. All data points in the localit y of the test instance t whic h have accuracy densit y above can then be utilized in order to determine the visual pro le. We note that even though the choice of the data isolation parameter dep ends upon the user, it should be chosen suc h that the interest ratio of the accuracy value of should be at least 1. Therefore, if the data set L con tains a total of jLj points out of whic h belong to the dominan t class, then the default value of is given by = jLj . The actual value of may then be mo di ed by the user so that a well de ned local region around the test instance can be clearly distinguished. Suc h a judgemen t can be e ectiv ely made only by human perception and intuition. This is one of the adv antages of an exploratory approac h whic h is able to incorp orate human feedbac k and intuition into the classi cation pro cess.

In order to nd the data points in the localit y of the test instance whic h have accuracy densit y above , we de ne the concept of accuracy conne ctivity between two data points. Let Q ( t; E ) be the dominan t class at the test instance t in subspace E . We assume that the accuracy densit y of this dominan t class at test instance t in subspace E is above the user speci ed threshold .

Definition 3.1. A data point x 2 D is said to be accu-racy conne cted to test instanc e t in subsp ace E at threshold , if ther e exists a path P conne cting x to t , such that for any point y 2 P , A E ( y; Q ( t; E ) ; D ) .
 In Figure 2(b), we have imp osed an accuracy threshold by utilizing a hyperplane whic h is sup erp osed on the accuracy densit y pro le. In this case, the tiny island con taining the test instance is the region from whic h the relev ant data set is isolated. We note that even though the visual represen tation is quite interpretable, the minim um bounding rectangles of the various isolated regions may be used in order to pro vide a rough idea of the relev ant parameters in the corresp onding dimensions. This does not compromise the qualit y of the exploration pro cess, since the actual isolation of the data is done using the arbitrarily shap ed regions, whereas the use of MBRs is only a posterior step in order to maximize interpretabilit y.

We use the grid discretization of the data in order to mak e an appro ximate computation of whether the test instance is accuracy connected to the data points. This discretization automatically separates out the subspace into rectangular regions. A grid rectangle is said to have accuracy larger than if the cen ter of the rectangle has accuracy densit y larger than . The rst step is to nd the unique grid rectangle con taining the test instance. Starting from this rectangle, we keep searc hing for adjacen t rectangles with accuracy densit y above the threshold , until all suc h rect-angles have been determined. Tw o rectangles are said to be adjacen t if they share one side. Finally , all the data points that lie in these rectangles are returned as the data points whic h are locally relev ant to the test instance t , and are used for further exploration in subsequen t iterations of the Subsp aceDe cisionPath algorithm.

The pro cedure also returns the accuracy signi c anc e p ( L for the class C i of the user-de ned split. Let l i and l num ber of instances of class C i in L and L 0 resp ectiv ely. Then, we de ne the accuracy signi cance of the isolated data set L 0 with resp ect to class C i as follo ws: s ( L 0 ; C i ) = ( l 0 i = jL 0 j l i = jLj ) = p ( l i We note that this signi cance factor is the num ber of stan-dard deviations by whic h the class fraction of C i in the iso-lated data set L 0 is larger than the data set L . Corresp ond-ingly , the signi cance factor of class C i is de ned as follo ws: Here ( ) is the cum ulativ e normal distribution function. The value of p is non-zero only when the isolated data set L 0 con tains a larger prop ortion of class C i than the original data set L . By appro ximating the signi cance factor with a normal distribution, we are able to quan tify the probabilistic level of signi cance that the isolated data set is signi can tly more indicativ e of a particular class.
Since the aim of this pap er is to pro vide the user with open-ended exploratory abilit y, the nal decision of termi-nation is dep enden t upon the user. At the same time, we wish to pro vide the user with some intuitiv e guidance as to when termination should tak e place. In this section, we will discuss the beha vior of the visual pro les as well as some statistical measures whic h pro vide evidence of termination. We note that isolation of data localities results in successiv e re nemen t suc h that the localit y around the test instance is increasingly dominated by a particular class. This can also be perceiv ed in the visual densit y pro le of the data at the lower levels of the decision path. In suc h cases, the test instance occurs on a plateau of high accuracy densit y in the visual pro le. Examples of suc h accuracy densit y pro les are illustrated in Figures 2(c) and 2(d). In these cases, the test instances have accuracy densities 98 : 0% and 99 : 0% re-spectiv ely in the corresp onding pro jections. It is also clear from Figures 2(c) and 2(d) that the region in the immediate localit y of the test instance is attened out at the accuracy densit y of the test instance. This beha vior is the result of successiv e data isolations whic h are akin to a visual magni-cation of the data localit y with the use of carefully chosen subspaces. Our empirical results illustrate the interesting phenomenon that even though only the small num ber of subspaces on the decision path are used for isolation of data localit y, all the subspaces on the lower nodes of the PATH start exhibiting this beha vior. This is because of the inter-attribute correlations of the di eren t attributes all of whic h pro vide consisten t information about the class beha vior.
A better statistical measure of the level of signi cance of a given path is obtained by computing the cum ulativ e dominance level of eac h class C i along PATH. Let L 0 : : : L be the nodes along PATH. Then, we de ne the cum ulativ e dominance CD ( P AT H; C i ) for the class i as follo ws: The larger this value, the greater the cum ulativ e dominance of C i along PATH. A user may choose to stop exploration along this path, when the cum ulativ e dominance of some class C i exceeds a pre-de ned threshold. Unlik e a decision tree, the user may also traverse multiple paths of the decision pro cess by bac k-trac king instead of terminating when a path has been successfully explored.

In this section, we will pro vide a detailed discussion and understanding of the adv antages of the instance-cen tered ex-ploratory approac h dev elop ed in this pap er. The ionosphere data set from the UCI mac hine learning rep ository con tains 34 attributes and two classes corresp onding to \g" or \b" dep ending upon the qualit y of radar returns from the at-mosphere. In Figure 2(e), we have illustrated the accuracy densit y of the dominan t class lab el in the highest interest densit y subspace. The corresp onding accuracy densities of the test instance ranged from between 93% to 96% for all the di eren t subspaces found. All of the branc hes had the same dominan t class lab el corresp onding to \g". Figure 2(e) illustrates the visual pro le of one of these branc hes, whic h sho ws particularly high level of discrimination in its data localit y. The accuracy densit y value of the dominan t class lab el at the test instance is 95 : 8%. In order to obtain a fur-ther idea of the local beha vior of the data around this test instance, we decided to explore into the data localit y whic h was accuracy connected to the test instance at a threshold of 75 : 0%. The isolated data was the island region con taining the test instance in Figure 2(e). Note that this region has a somewhat irregular shap e whic h cannot be characterized easily in closed form, but could be understo od more easily in this visual represen tation. Up on expanding this region further, we found that all the branc hes again corresp onded to \g" with accuracy densit y values between 98% and 99%. An example of suc h a branc h is illustrated in Figure 2(f). An interesting characteristic of this pro le is that the region in the immediate localit y of the test is somewhat attened out at a high accuracy rate. This kind of beha vior is true for all branc hes at that level. This sho ws that the pro cess of successiv e data isolations has resulted in a smaller and more re ned localit y around the test instance in whic h the user can clearly perceiv e a consisten tly high concen tration of a particular class. If all of the pro jections corresp onding to the di eren t branc hes exhibit this beha vior for the same class C i , then it is likely that the test instance belongs to C
In order to determine the e ectiv eness of the SD-P ath metho d, we also tested the overall accuracy of the approac h. In eac h case, we predicted a class lab el, if it was found to con-verge to the same value using three separate decision paths. If the class lab el was found to be inconsisten t on three sepa-rate paths, then we lab eled the corresp onding test instance as indeterminate. we further note that while we also presen t the results in comparison to the traditional C4.5 classi er, it is imp ortan t to understand that our classi er is not an alternativ e to traditional batc h classi ers whic h can classify a large num ber of test instances. Rather, it is only intended as a diagnosis tool for individual test instances. This is par-ticularly useful for test instances con taining con icting clas-si cation charactersitics in di eren t dimensions. Therefore, the only purp ose of these comparisons is to illustrate that the exploratory classi er is consisten tly more robust than a traditional classi er because of the use of human interv en-tion in the pro cess.
 In Table 1, we have illustrated the accuracy of the SD-Path metho d on a num ber of data sets from the UCI mac hine learning rep ository . In eac h case, we ran the exploratory SD-Path approac h over a set of 20 examples and found that in most cases, the SD-P ath metho d was able to obtain consis-ten t results over the di eren t paths. In the same table, we have also illustrated the e ectiv eness of the C4.5 metho d. It is clear that the SD-P ath metho d is much more accurate than the C4.5 technique. This is partially because of the fact that the SD-P ath metho d is able to exploit both the local beha vior of the test instance as well as the intuition of the user during the classi cation pro cess. Another interest-ing observ ation from Table 1 is that in eac h case, none of the test instances were classi ed inaccurately , though some were considered indeterminate by the SD-P ath metho d. The reason for this indeterminate beha vior was the fact that the test instances shared characteristics from multiple classes. This is evidence of the anamolous beha vior of the test in-stance rather than a limitation of the SD-P ath metho d. In fact, this diagnosis helps the user understand the various com binations of dimensions whic h rev eal this con tradicting beha vior.
In this pap er, we discussed the subspace decision path metho d, an e ectiv e exploratory instance-based approac h for decision path construction for high dimensional data sets. The adv antage of this metho d is that it e ectiv ely com bines the data mining pro cess with human interaction in order to pro vide good understanding of the classi cation characteristics of a given test instance. Since the pro cess uses test-instance speci c local subspace characteristics of the data, it is much more exible and concise than a deci-sion tree construction pro cess. Our empirical tests illustrate that this exibilit y also results in a signi can tly more accu-rate classi cation pro cess. The abilit y to explore multiple paths of an instance-sp eci c pro cess pro vides the user with multiple persp ectiv es of the imp ortan t characteristics in the instance. Even in cases where the classi cation beha vior of the instance is poorly de ned, the subspace decision path metho d is able to pro vide insigh t into the di eren t character-istics of the test instance whic h have con trasting beha vior. Suc h information is of great value in a num ber of business applications in whic h the causalit y of the classi cation pro-cess pro vides valuable information to the end-user. [1] R. Agra wal, R. Srik ant. Fast Algorithms for Mining Asso ciation Rules in Large Databases. VLDB
Confer ence , 1994. [2] M. Ank erst, M. Ester, H.-P . Kriegel. Towards an E ectiv e Cooperation of the Computer and the User for
Classi cation. KDD Confer ence , 2000. [3] B. Liu, W. Hsu, Y. Ma. Integrating Classi cation and
Asso ciation Rule Mining. KDD Confer ence , 1998. [4] B. W. Silv erman. Density Estimation for Statistics and
Data Analysis . Chapman and Hall, 1986.
