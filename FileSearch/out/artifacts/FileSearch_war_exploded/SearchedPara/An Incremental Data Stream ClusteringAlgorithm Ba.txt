 In recent years, data stream model is motivated by many applications that con-tinuously generate huge amount of data at unprecedented rate [1]. In this paper, we will focus on the stream clustering problem, which is a central task of data stream mining.
 study the k-median problem over data streams. Aggarwal et. al. [3] present a framework of clustering evolving data streams, which analyzes the clusters over different portions of the stream. However this framework can not give online re-sponse of queries of macro clusters. Nasrouni et. al. [7] design an immune system learning model to find evolving clusters in data streams. But this algorithm is not space and time efficient due to the use of AIS model.
 [4, 5, 6]among which grid-based clustering is an efficient method. This approach partitions the data space into many units and perform clustering on these units [6]. Recently, Park et.al. [8] propose a statistical grid-based method which iden-tifies evolving clusters as a group of adjacent dense units in data stream environ-ments. But their work is focusing on partitioning dense units and maintaining their distributions.
 stream. We partition the data space into units and only keep those units which contain relatively large number of points. An incremental clustering algorithm is presented based on these dense units. The clustering results are represented by bits to reduce the memory requirements. Extensive experiments indicate that our framework can obtain high-quality clustering with little time and space. We begin by defining the stream clustering problem in a formal way.
 tion it into non-overlapping rectangular units. The density of a unit u is defined as the number of points that belong to it, i.e. den( u )= | v i | v i  X  u | . The relative density of u is defined as follows: rel den( u ) = den( u ) / | D | , where den( u )isthe density and D is the data set we observe. If u  X  X  relative density is greater than the density threshold  X  , then u is referred to as a dense unit.As defined in [6], a cluster is a maximal set of connected dense units in d -dimensions.
 rives. We assume that data arrives in chunks X 1 ,X 2 ,...,X n ,... , at time stamps t ,t 2 ,...,t n ,... . Each of these chunks fits in main memory. Suppose that each chunk contains m points, and the current time stamp is t . We use den( u )to denote the overall density of u with respect to the t chunks that has been seen so far. The density of u with respect to the i -th chunk is denoted as den i ( u ). The relative density of a unit u is rel den( u ) = den( u ) / ( mt ). If u  X  X  relative density is greater than the density threshold  X  , then u is referred to as a dense unit at time t . At time t , the clustering result R is all the clusters found in the t chunks of data visited so far. Our goal is to compute the clustering results when the data stream continuously arrives, i.e. obtain R 1 ,R 2 ,...,R n ,... , where R i represents the result of clustering X 1 ,X 2 ,...,X i . 3.1 Basic Idea In brief, we will find the dense units and cluster these units. First, we consider what units should be maintained thus introduce the concept of local dense units. t . If unit u begins to be maintained at time i , the local relative density of u is relative density is greater than the density threshold  X  , then u is referred to as a local dense unit at time t . The following proposition holds on.
 Proposition 1. For any dense unit u at time t , it must be recorded as a local dense unit at time i (1  X  i  X  t ) . Proof. Suppose that a dense unit u is not recorded as a local dense unit at time 1 , 2 ,...,t and each chunk contains m points. We recall that the number of points that belong to u in the i -th chunk is den i ( u ). Then den i ( u ) &lt; X m . Therefore at time t , den( u )= t i =1 den i ( u ) &lt; X mt .so u is not a dense unit at current time, contrary to the hypothesis. The conclusion is accordingly established. In other words, local dense units are candidate dense units, which may become dense in the future. Therefore we maintain all the local dense units and pick up dense units among them to do clustering. We call this process dense units detection. The following proposition analyzes the error of our algorithm. Proposition 2. Assume that a certain unit u  X  X  density gradually increases so that its density with respect to the i -th chunk is ipm where m is the number of points belonging to each chunk, p is a constant from 0 to 1 that indicates the amount of increase. At the time from (1 + 1+8  X /p ) / 2 to  X /p , this unit can not be successfully detected as a dense unit.
 Proof. According to the definition of local dense units, we will not keep unit u as long as ipm &lt;  X m , i.e., i&lt; X /p . However, when its density reaches  X m , u becomes a dense unit at that time. Suppose at time k , u  X  X  density is equal to  X m . Then k i =1 ipm =  X m ,i.e., k ( k  X  1) 2 p =  X  . It can be derived that k = (1 + 1+8  X /p ) / 2. Therefore the time range when error occurs is as stated. To lighten the computational and storage burden, we propose to represent the clustering results in bits. Suppose that the dense units are sorted by their density and each of them is assigned a unique id. The Clustering Bits (CB) of a cluster r is a 0  X  1bitstring a n ,...,a 1 , where a i is a bit and n is the number of dense units. a i = 1 if and only if the i -th dense unit is in cluster r , otherwise a i =0. We can benefit from the use of Clustering Bits in both the time and space usage. 3.2 Stream Clustering Framework Based on the above two points, we summarize our stream clustering algorithm in Figure (1). We refer to this algorithm as DUCstream ( D ense U nits C lustering for data stream ). The data structures used in the algorithm include: L ,the local dense units table; Q a , the added dense units id list; Q d , the deleted dense units id list; R i , the clustering result { c 1 ,...,c s } at time stamp i . 1. map and maintain( X i ,L ): This procedure maps each data point in X i into 2. create clusters( Q ): We use a depth-first search algorithm to create clusters 3. update clusters( R i  X  1 ,Q a ,Q d ): We get the clustering result R i in an incre-common face with any old dense units, a new cluster is created containing u ; Absorption: There exits one old dense unit u such that u has common face with u , then absorb u into the cluster u is in; Mergence: There exist multiple old dense units w 1 ,w 2 ,...,w k ( k&gt; 1) that have common faces with u , then merge the clusters these dense units belong to. Absorb u into the new cluster. distinguish the following cases: Removal: If there are no other dense units in c , i.e. the cluster becomes empty after deleting u , we remove this cluster; Reduction: All other dense units in c are connected to each other, then simply delete u from c ; Split: All other dense units in c are not connected to each other, this leads to the split of cluster c .
 result R i . The data set is KDD X 99 Intrusion Detection Data, which is partitioned into chunks each consisting of 1 K points. We first examine the time complexity of DUCstream compared with the baseline methods STREAM [2] and CluStream [3]. To make the comparison fair, we make the number of clusters all five in these algorithms.Figure (2) shows that DUCstream is about four to six times faster than STREAM and CluStream. This is attributed to our use of dense units detection, Clustering Bits and good design of incremental update algorithm. main memory. Since the clustering results, represented by Clustering Bits, cost very little space, we only keep track of the number of local dense units to monitor the memory usage. Figure (3) demonstrates that after a certain time, a steady state is reached as for the number of local dense units. In general, the algorithm only requires a negligible amount of memory even when the data stream size becomes sufficiently large.
 surement SSQ, the sum of square distance. Figure (4) shows that the clustering quality of DUCstream is always better than that of STREAM because we capture the characteristics of clusters more precisely using the dense units compared with only maintaining k centers. For CluStream, it performs better when the horizon is small but the accuracy tends to be lower when the horizon becomes larger. In this paper, we propose an efficient data stream clustering algorithm based on dense units detection. This is an incremental, one-pass density-based algorithm, which finds high-quality clusters with considerably little time and memory in the data stream environment. It discards noisy and obsolete units through dense units detection. The clustering result is updated using the changed dense units. We also introduce a bitwise clustering representation to update and store away the clustering results efficiently. Empirical results prove that this algorithm has good quality while cost surprisingly little time. The problem of finding arbitrary-shaped clusters is an interesting future work.
 The work was partially supported by IRGP grant #71-4823 from the Michigan State University.

