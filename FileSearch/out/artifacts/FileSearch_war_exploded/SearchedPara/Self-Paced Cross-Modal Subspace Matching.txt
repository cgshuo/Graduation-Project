 Cross-modal matching methods match data from different modalities according to their similarities. Most existing methods utilize label information to reduce the semantic gap between different modalities. However, it is usu-ally time-consuming to manually label large-scale data. This paper proposes a Self-Paced Cross-Modal Subspace Matching (SCSM) method for unsupervised multimodal data. We assume that multimodal data are pair-wised and from several semantic groups, which form hard pair-wised constraints and soft semantic group constraints respectively. Then, we formulate the unsupervised cross-modal matching problem as a non-convex joint feature learning and data grouping problem. Self-paced learning, which learns samples from  X  X asy X  to  X  X omplex X , is further introduced to refine the grouping result. Moreover, a multimodal graph is con-structed to preserve the relationship of both inter-and intra-modality similarity. An alternating minimization method is employed to minimize the non-convex optimization problem, followed by the discussion on its convergence analysis and computational complexity. Experimental results on four multimodal databases show that SCSM outperforms state-of-the-art cross-modal subspace learning methods. Cross-Modal Matching; Heterogeneous Data; Unsupervised Subspace Learning; Self-Paced Learning
Nowadays, multimodal data spring up as the popularity of social network on the Internet. People would like to upload personalized content through various forms such as text, image, audio and video simultaneously, this phenomenon urgently pushes the need for heterogeneous content retrieval. Compared with unimodal retrieval, cross-modal retrieval exploiting heterogeneous feature representations to discover Corresponding Author.
 optimal heterogeneous content for a given user query, is more and more frequently utilized. Although search engines also provide such services, e.g., text-image search, they mainly rely on homogeneous contents associated with heterogeneous contents within the same web pages or tweets. The main challenge in cross-modal matching is how to bridge the semantic gap between heterogeneous modalities (e.g., different distributions, different dimensionalities). In this literature, some promising approaches [31, 37, 44] have been developed to alleviate this gap. Probability based learning algorithms [28, 16] along with subspace based learning algorithms [12, 29, 37, 18, 22] are even more favored and widely applied for cross-modal learning applications, e.g., image annotation and cross-language information retrieval.
Topic models, such as Latent Dirichlet Allocation (LDA) [6], have proved effective at describing the underlying topics in one single modality. Following researches [5, 28] focused on correlating topics among different modalities, and extended it to the multi-modal case. While [5] assumed a one-to-one correspondence between the topics of each modality, and [28] assumed a regression module between two sets of topics instead of one-to-one correspondence, and achieved better performance with more freedom allowed. [16] further defined a Markov random field to capture the relationships between different topic sets in every modality. Besides traditional topic models, [35] adopted deep Boltzmann machines to learn a generative model, and [27] proposed an effective nonparametric Bayesian framework based on the Indian Buffet Process (IBP) for integrating multimodal data in a latent space. Besides, some recent work [1, 19] exploited deep models for cross-modal matching. And [38] further investigated the impact of deep features on cross-modal retrieval.

In contrast, subspace based cross-modal methods seem attractive due to its efficiency. They aimed to learn a latent common subspace to make all modalities of data to be close to each other. In this literature, Canonical Correlation Analysis (CCA) [12] is the most popular one to obtain such a common space. CCA tries to find two linear projections to maximally preserve the mutual correlations among multimodal data. Owing to its simplicity and effectiveness, CCA has been widely applied to the cross-modal retrieval [7], face recognition [21] and word embedding [8, 9]. Another popular method is the Partial Least Squares (PLS) [31] that learns orthogonal score vectors by maximizing the covariance between different multimodal data. Although both CCA and PLS are able to solve cross-modal subspace matching, the valuable label information has not been fully utilized to reduce the semantic gap between text and image modalities.

Since label information potentially reduces the semantic gap between the low-level image features and high-level document descriptions, supervised methods have drawn considerable attentions in cross-modal subspace learning. A generalized multi-view framework [33] was presented to learn a discriminative common space. Built on CCA, [11] proposed to incorporate the third view (label information) to capture the high-level semantics. To select relevant and discriminative features simultaneously, [37] developed a coupled linear regression framework. Moreover, the authors of [13] seek the common structure hidden in heterogeneous modalities via the pairwise constraint. Hashing methods have been also used successfully in multimodal problem [24, 39] and cross-modal problem [40, 43, 41]. While [40] utilized discriminative coupled dictionaries to learn better hash functions, [43] exploited matrix factorization to learn the hash functions. Despite the improvement brought by these supervised cross-modal methods, they will suffer from unlabeled data exceedingly.

To address these above problems, we propose a self-paced joint learning framework (as shown in Figure 1) for the unsupervised cross-modal matching problem by considering feature learning and data grouping simultaneously, which result in a non-convex objective. First, we assume that multimodal data are pair-wised and from several semantic groups, which form hard pair-wised constraints and soft semantic group constraints respectively. For these hard pair-wised constraints, two linear projections are learned to map different modalities into a common space respectively. For the soft semantic group constraints, self-paced learning strategy [20, 42] is applied to refine the grouping results so that we can train a model from  X  X asy X  pairs to  X  X omplex X  pairs. Second, the projection regularization is employed for feature learning. A graph-based regularization term preserves the relationship of both inter-and intra-modality similarities. Finally, we present an alternating algorithm to cope with this optimization problem. Extensive experimental results on four benchmark databases demonstrate that our approach is highly effective.
The contributions of this paper are summarized as follows: 1) We propose a general framework for unsupervised cross-modal subspace learning that can handle hard pair-wised constraints and soft semantic group constraints simultaneously. A joint feature learning and data grouping formulation is accordingly developed and results in a non-convex problem. 2) Self-paced learning is used to alleviate the inaccurate estimation of soft semantic group so that we can gradually include sample pair sequences from  X  X asy X  to  X  X omplex X . Multimodal graph, preserving the inter-modality and intra-modality similarity, is then utilized to better explore unsupervised multimodal data. 3) An alternating minimization method is put forward to efficiently minimize the proposed non-convex optimization problem. Experimental results validate that our method outperforms state-of-the-art unsupervised subspace learning methodsorevenbetterthansomesupervisedones.
 The remainder of the paper is organized as follows. In Section 2, we will illustrate the details of our SCSM method. Experimental results are presented in Section 3. Finally, the conclusion is summarized in Section 4.
In this section, we will present our SCSM in details and use two modalities for example in this paper. Note that SCSM can be easily extended to cover the case for more than two modalities.
We start with a brief introduction to some notations. For a matrix M  X  R n  X  m ,its i -th row, j -th column are denoted by m i , m j respectively, and M i,j lies in the i -th row and j -th column. The Frobenius norm of the matrix M is defined as || M || F = n i =1 || m i || 2 2 , and the trace of the square matrix M is defined as Tr ( M )= i M i,i .
Assume that we have two sets of features from different modalities (e.g., image and text), X a =[ x a 1 , x a 2 ,..., x dimensionality of the i -th modality, n is the amount of training image-text pairs. And each pair { x a i , x b i } same underlying content and belongs to the same class, i.e., the hard pair-wised constraint. However, the concrete label of each pair is unknown here.

The goal of cross-modal matching is to obtain two subspaces S p = U T p X p  X  R c  X  n ,p  X  X  a, b } with the same dimensionality, where U p  X  R d p  X  c ,p  X  X  a, b } denote two projection matrices for these two modalities X p ,p  X  X  a, b respectively. Cross-modal matching tasks usually include: 1) using texts to match the related images, and 2) using images to match the related texts.

Here, we mainly focus on the case that one retrieved result is a good matching only when it shares the same semantic label with the given user query.
Curriculum learning [4] and self-paced learning [20] have been attracting increasing attention in machine learning, computer vision and multimedia analysis fields. The philosophy under these concepts is to simulate the learning process of humans/animals, i.e., humans/animals generally start with learning easier aspects of a learning task, and then gradually take more complex examples into consideration [25]. Instead of using the aforementioned heuristic strategies in curriculum learning, self-paced learning automatically includes training samples from  X  X asy X  to  X  X omplex X  in a purely self-paced way.

Given a training dataset D = { ( x i ,y i ) } n i =1 ,inwhich { x i ,y i } denotes the i -th observed sample and its label, then let L ( y i ,g ( x i ,w )) be the loss function, w is the model parameter inside the decision function g . Generally, the objective of self-paced learning consists of two parts, i.e., a weighted loss term on all samples and a general self-paced regularizer imposed on sample weights, is expressed as: where  X  is the age parameter for controlling the learning rate, and f ( v, X  ) is the self-paced regularizer. A formal definition is given in [17, 42]. This strategy, as supported by empirical evaluation, has proved helpful in alleviating the local optimum problem in non-convex optimization [3].
In Figure 1, multimodal data are firstly pair-wised, further supposed to come from several latent semantic groups, which form hard pair-wised constraints and soft semantic group constraints respectively.

Latent Discriminative Subspace Learning For the cross-modal problem, a CCA-like objective function ex-pressed as where  X (  X  ) is a regularizer imposed on the projection matri-ces, is exploited frequently to discover the optimal common subspace. Then we consider the following objective, as a relaxed candidate for that in Eq.(2), owing to the basic inequality A  X  C 2 F + B  X  C 2 F  X  A  X  B 2 F 2 .
Since multimodal data are separated into several groups in the latent subspace, we further strict the latent variable Y lying in the discrete space { 0 , 1 } c  X  n , revealing the group/cluster membership, where c is the amount of latent groups. Obviously, 1 c Y = 1 n , where 1 c and 1 n denote the constant vectors of all one, of dimensions c and n respectively. Let  X ( U a , U b )=  X  U a 2 F + U b 2 F as [2], we can write the multimodal feature learning term as follows: where each X p is the feature representation matrix, U p is the corresponding projection matrix, and U p 2 F is a regularizer imposed on project matrices to avoid trivial solutions.
Cluster indicator Y is a discrete variable, which easily involves the optimization method in a local optimum. Fortunately, the learning process from  X  X asy X  to  X  X omplex X  proposed in self-paced learning can effectively avoid the local optimum. Thus we incorporate self-paced learning model in Eq.(1) into Eq.(4), resulting in the following form: where the i -th loss of sample is defined as i = || ( U T y ) || 2 F above.

Multimodal Locality Preserving The term in Eq.(4) mainly focuses on the similarities among pair-wised hetero-geneous data. Local similarity in each modality may vanish in the process of feature learning. We then take locality preserving into consideration, which has also proven effective in [14], the projected features in the common subspace R C should inherit the similar local structure to it in these two original feature spaces. Then for each projected modality, we try to minimize the following objective function: where Z p = U T p X p are the projected feature matrices, and W ij is the intra-similarity between the i -th and the j -th sample. We adopt the Gaussian kernel function d ( x i p , x e where N r (  X  )isthesetof r -nearest samples. The total intra-similarity preserving term in Eq.(6) for each modality can be formulated as: where L p = D p  X  W p is the Laplacian matrix and D p is a diagonal matrix, where D p i,i = j W p i,j .
Moreover, a novel similarity preserving term is introduced to maximize the similarity between data from one identical latent cluster. Given the cluster indicator Y instead of the missing semantic labels in [36], W ab = W ba = Y T Y can be seen as the inter-similarity matrix. Apparently, only paired data from the same cluster are treated as similar pair, the similarity of coupled data from different clusters equals to 0, otherwise. We can define a inter-similarity preserving term akin to the intra-similarity term above, expressed as: Then we introduce a new joint similarity matrix as below: Combining these two locality-preserving terms L intra and L inter together, we further obtain the following term: where L pq is the corresponding block of the Laplacian matrix L ,and  X  is a trade-off parameter to balance the intra-and inter-similarity preserving terms.

Associating discriminative subspace learning with local similarity preserving together, we obtain the overall objec-tive function for SCSM:
In this subsection, a fast iterative optimization algorithm is developed to find the optimal solution of Eq.(12). Regarding the self-paced regualrizer f (  X  ), we adopt the same strategy utilized in [20]: f ( v ; k )=  X  1 k i v i , where v denotes the weights imposed on the loss term, which is a binary vector. When age parameter k is given in each iteration, the current indicator variable v i can be defined for each sample x i as:
Besides, we rewrite Eq.(12) into the following form:
Since the matrix Y  X  X  0 , 1 } c  X  n is limited to discrete values, i.e., a non-convex set, Eq.(14) is a non-convex problem. We employ an alternating minimization algorithm for Eq.(14), the involved parameters include U a , U b , v and Y . 1) Solve U a , U b when Y , v are fixed . Optimizing the objective function(14) is equal to minimizing the following problem: where V is a diagonal matrix whose entries correspond to the elements in v . Differentiating the objective function in Eq.(15) with respect to U p and setting it to zero, we have the following equation: Then, the optimal solution of Eq.(16) can be computed via solving the above linear system problem. 2) Solve Y when U a , U b , v are fixed . Optimizing the objective function in Eq.(14) is equal to optimizing the following problem: min
It is challenging to directly minimize the above objective function w.r.t Y due to the discrete constraints, resulting in a NP hard problem. Inspired by [34], we can optimize Y column by column, i.e., optimize one column of Y with all the other columns fixed. So we can iteratively learn one column at one time. Then Eq.(17) is equivalent to: where E = U T a X a , F = U T b X b , G = U T a X a VV T , H = U
The objective in Eq.(18) can be further attributed to minimizing two following general cases, i.e., Tr ( AY T YB and Tr ( CY T ). Let y be the i -thcolumnof Y , i =1 ,  X  X  X  and Y the matrix Y of excluding y where y is one of all n samples. Similarly, denote by a the i -thcolumnof A , A the matrix of A excluding a , b the i -th column of B ,and B the matrix of B excluding b and c thei-thcolumnof C and C the matrix of C excluding c . Then we have the following form:
Tr ( AY T YB T )= Tr (( ay T + A Y T )( yb T + Y B T )) Here Tr ( ay T yb T )= Tr ( ab T )= const . Similarly, we have: Tr ( CY T )= Tr ( cy T )+ Tr ( C Y T )= const + y T c . (20) According to Eq.(19) and Eq.(20), we can formulate Eq.(18) as follows: where e , f , g , h are the i -thcolumnof E , F , G , H and respectively.

Thus, this subproblem can be easily solved as: where m =2 Y V T v +  X  Y F T e +  X  Y E T f  X  g  X  h , and h( m ) returns the index of the minimum value of m . After 2  X  3 inner iterations, we can obtain a optimal complete Y . 3) Solve v when U a , U b , Y are fixed .Thelossofeach sample can be directly computed, thus we can obtain v by Eq.(13).
 Algorithm 1 Self-Paced Cross-Modal Subspace Matching (SCSM) Input: The matrices of unlabeled data X p  X  R d p  X  n ,p  X  Output: The projection matrices U p  X  R d p  X  c ,p  X  X  a, b 1: Initialize Y by K-means clustering, and compute the 2: repeat 3: Compute v t according to Eq.(13); 4: Compute U t p ,p  X  X  a, b } by solving the linear system 5: Compute Y t according to Eq.(21) and Eq.(22); 6: Compute W t according to Eq.(10); 7: t = t +1; 8: until Convergence;
Algorithm 1 summarizes the alternate minimization procedure to optimize Eq.(14). Firstly, the label information Y is obtained by K-means clustering on the text modality and the multimodal Laplacian matrix is constructed in Step 1; Then, Step 3  X  6 is an alternate procedure of iteratively updating each of U { a, b } , v , Y one by one in a loop. Here we try to discuss the time complexity of the proposed SCSM. For each outer iteration, we need O ( nd 2 1 + nd 2 solve U a and U b , O ( nc 2 )tosolve Y ,and O ( ncd 1 + ncd solving v . Thus, the overall time complexity is O ( nd 2 for SCSM, where c is the dimension of the common subspace and d is the larger original dimensionality for each modality.
In this section, we comprehensively compare SCSM and other existing cross-modal approaches on four benchmark datasets, the Pascal VOC [15, 10], the Wikipedia [29], and the LabelMe [32, 26] dataset, which are widely adopted for cross-modal matching tasks.
There are several widely used evaluation criteria for cross-modal subspace matching algorithms [29, 33, 37]. During the testing phase, multimodal data are mapped into a common subspace via the learned projection matrices for each modality. Then in the subspace, we take one type of modality as a query set to match another type of modality. Like [29, 37, 18], the cosine distance is utilized to measure the similarity of projected features. The simple goal is that given a query from one modality, we can return the top k closest matches in another modality via subspace learning.
The mean average precision (MAP) metric is a classical performance evaluation criterion in the information retrieval circle. The higher MAP indicates the better performance. Besides the MAP, we also exploit the precision-scope curve [30] and precision-recall curve [29] to intensively evaluate the effectiveness of different methods, where the scope K is specified by the number of the top-ranked texts/images presented to users.
 With regards to baseline methods, CCA [12], PLS [31] and GMBLM [33] are three unsupervised methods, which merely use the hard pair-wised information to learn a common latent subspace. In contrast, supervised methods such as CDFE [23], GMLDA [33], GMMFA [33], LCFS [37] and recent JFSSL [36] all take the semantic class information into account, and they are also compared here. Despite the improved performance, semantic labels are usually expensive and time-consuming to obtain for supervised methods. To further study the effect of initial clustering result, we further assume the clustering result as pseudo semantic labels, and attain the matching performance of above supervised methods, referred as UCDFE, UGMLDA and UGMMFA.
The Pascal VOC dataset [15] is a commonly used dataset, it consists of 9,963 image-text pairs from 20 categories, including 5,011 training and 4,952 testing image-text pairs. Since some of the images have multiple labels, we select the images with only one label [33, 37]. As a result, we obtain 2808 training and 2841 testing data samples that correspond to 20 categories. The image and text features are 512-dimensional Gist [26] features and 399-dimensional word frequency features, respectively.

We first use the Principal Component Analysis (PCA) to remove the redundancy in features. 95% information energy is preserved by the PCA before evaluation. The MAP scores of the cross-modal retrieval results are shown in Table 1.
From Table 1, we can observe that the proposed SCSM achieves the best performance on both the image-to-text and text-to-image matching tasks. Obviously, supervised methods (i.e., CDFE, GMLDA and GMMFA) outperform their unsupervised versions by 10%, and earn bigger advantages over CCA and PLS. This is because supervised methods rely on semantic labels to reduce the semantic gap of different modalities, but unsupervised methods only use pair-wised information. However, our unsupervised method not only surpasses the unsupervised methods, but outperforms several supervised methods, achieving the state-of-the-art performance.

Figure 2 shows the corresponding precision-recall curves and precision-scope curves. The scope, i.e., the top K retrieved items, varies from K = 50 to 1000. It can be observed from Figure 2 that SCSM consistently outperforms other unsupervised methods for both Image query vs. Text database and Text query vs. Image database, regardless of precision-scope curves and precision-recall curves. Note that GMBLM only performs inferior to SCSM, due to its consideration of intra-similarity.

The Wiki image-text dataset [29] generated from Wikipedia articles, is a fundamental dataset for cross-modal matching. It is composed of 2,866 image-text pairs from 10 semantic classes, with image features being 128-dimensional SIFT feature and the text feature being 10-dimensional Latent Dirichlet allocation (LDA) feature. We split it into a training set of 1,300 pairs and a testing set of 1,566 pairs in the experiment [37]. We directly carry out the experiment with the provided datasets owing to the low dimensional image and text features. The MAP scores obtained by SCSM and other approaches are shown in Table 1.

From Table 1, we can see that SCSM achieves similar results to GMLDA and GMMFA, and is just inferior to JFSSL. However, it continues to be the best algorithm for unsupervised cross-modal subspace learning. The reason for these results is that the dimension of the features in this database is generally low so that the feature learning hardly takes effect. In the next subsection, we will show our improved performances with the help of better feature representations.

The corresponding precision-recall curves (a) and precision-scope curves (b) are also plotted for both forms of cross-modal retrieval tasks in Figure 3. We observe that, for an image query, SCSM obtains similar and even inferior performance, while for the text query, it performs the best among these unsupervised algorithms.
The Wiki++ dataset [36] is built on the famous Wiki dataset [29]. They share all but feature representations. 4,096-dimensional Convolutional Neural Network (CNN) features are extracted for images by Caffe 1 , and 5,000-dimensional BOW feature vectors are learned based on the basic tf-idf features. We also split the entire dataset into a 1,300 training set and a 1,566 testing set. Specifically, PCA is also performed on the features, and 95% information energy preserved for both views.

From Table 1, it is obvious that the proposed SCSM algorithm achieves the best performance with average MAP 40.2% among unsupervised methods. Even though SCSM performs a little lower than JFSSL, it still can beat other supervised methods such as GMLDA and LCFS. Compared with performances obtained in the Wiki dataset, all methods achieve better results when using CNN visual features in spite of the image query or the text query. This is because the CNN features have proven effective for image feature representation. In the high and sparse dimensional feature setting, SCSM shrinks the distance with the best-performing JFSSL from 2.2% to 1%, offers the great potentials when better features are provided.

SCSM obtains the best performance in terms of precision-recall curves for both image query and text query, as shown in Figure 4(a). From the precision-scope curves in Figure 4(b), SCSM discovers more good matches in the top K documents than its several counterparts.

To further study the difference between SCSM and other methods (here we ignore the unsupervised versions of supervised methods due to the space limit) in Figure 6. From the figure, we can find that SCSM takes the first space for 5 classes, all these classes are easily distinguished. For the rest 5 classes, SCSM is a bit lower than supervised methods but higher than CCA, PLS and GMBLM. It can be concluded that SCSM can achieve a comprehensively better performance among unsupervised methods.
 Methods Matching Images GMLDA GMBLM
The LabelMe Outdoor dataset [32, 18] consists of 2,688 fully annotated outdoor images from 8 categories, i.e.,  X  X oast X ,  X  X orest X ,  X  X ighway X ,  X  X nside city X ,  X  X ountain X ,  X  X pen country X  and  X  X all building X . For the text modality, we generate the object account vector via the LabelMe 2 toolbox. We randomly select 200 samples from each category for training, resulting in 1,600 image-text pairs for the training set and the remaining 1,086 3 image-text pairs for the testing set. There are total 789 unique words with frequencies varying from one time to more than 2000 times. We select the word frequencies that are more than one time. As a result, the image and text features are 512-dimensional Gist features and 470-dimensional word frequency features, respectively.

From Table 1, SCSM obtains the best average score 65.6% among the unsupervised methods, however, it performs worse than some supervised methods. It may be because the word frequency features contain massive noises, such as misspelling and duplicated words, make it hard for unsupervised methods to do feature learning without semantic labels. However, the advanced performances from the Wiki to the Wiki++ database indicate that SCSM can probably approach the supervised methods with deep features for the LabelMe database.
 Regarding the precision-recall and precision-scope curves, SCSM still obtains the best performances in both image query and text query tasks. The gap between SCSM and second-best performing UCDFE is apparent, thus we can conclude that SCSM is distinguished, even other algorithms share the same initial pseudo semantic labels.

It is obvious to prove that the employed alternative minimization strategy can converge to a local optimum. However, under the self-paced framework, our learning algo-rithm is hard to guarantee the global convergence. On the other hand, the experimental results in Figure 8 on all four datasets can demonstrate the objective, p  X  X  a,b } || ( U iteration number increases.

With regards to the parameters  X  and  X  in SCSM, we conduct an experiment to study the influence of different choices. Note that in our experiment,  X  is set as 1 while  X  varies with the training data size n and feature length on parameter sensitivity shows that SCSM is very robust to model parameters which can achieve stable and superior performance under a wide range of parameter values.
For fair comparisons, we tune the parameter of subspace dimensionality for subspace based cross-modal approaches, and adopt the best-performing dimensionality for compar-ison. For LCFS, JFSSL and our SCSM, the subspace dimensionality is fixed as the number of semantic classes (e.g., c = 20 for the Pascal VOC dataset). Besides, the width parameter  X  forGaussiankernelinEq.(7)isfixedas 1 for following experiments.
Experimental results show that, our SCSM not only surpasses the unsupervised methods (e.g., CCA, GMBLM), but also outperforms some supervised methods (e.g., CDFE, GMLDA, and LCFS), in the Pascal VOC and Wiki++ datasets. As an unsupervised method, the reasons for the betterperformanceofourSCSMmaylieintwofold.

First, images often contain several semantic concepts, potentially belonging to several semantic groups, their labels may be inaccurate for some challenging datasets. Hence the grouping results may be a complementary for the missing label information when feature representations are enough powerful. Second, the pseudo group label obtained by canonical clustering methods is inaccurate. Since clustering is a non-convex problem, we introduce self-paced learning to avoid the local minima and refine the grouping results. Besides, the projection matrices are computed by two regression-like analyses on the pseudo label, and the multimodal graph preserving the inter-and intra-similarities is also constructed.

Moreover, we observe that all methods, especially SCSM, perform better in the Wiki++ dataset. Because the deep features in the Wiki++ dataset are high-dimensional and discriminative, SCSM can more effectively learn features, and estimate the latent semantic group more accurately. However, our method in the Wiki and LabelMe dataset is inferior to the supervised state-of-the-art method proposed by [36], much effort still needs to be taken to improve the performance when the features contain massive noises.
In this paper, we have proposed a novel unsupervised method for cross-modal subspace matching. We have introduced hard pair-wised constraints and soft semantic group constraints for multi-modal data, which are poten-tially effective for unsupervised learning. A joint feature learning and data grouping formulation has been accordingly developed. For an accurate estimation of the semantic group, self-paced learning has been incorporated into the non-convex loss. Moreover, multimodal graph is included to preserve the inter-and intra-modality similarity. To minimize this joint learning problem, we have presented an alternating minimization solution. Experimental results on four multimodal databases demonstrate that the effec-tiveness of the proposed method for unsupervised multi-modal data. For future work, the proposed method will be extended for multi-label cross-modal matching problem. This work was supported by the National Basic Research Program of China (Grant No. 2012CB316300), the Youth Innovation Promotion Association of the Chinese Academy of Sciences (CAS) (Grant No. 2015190) and the National Natural Science Foundation of China (Grant No. 61473289). Jian Liang and Zhihang Li contributed equally to this work and should be considered co-first authors.
