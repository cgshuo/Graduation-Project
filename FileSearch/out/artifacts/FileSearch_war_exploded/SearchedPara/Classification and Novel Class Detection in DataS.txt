 Data stream classification is more challenging than classifying static data because of several unique properties of data streams. First, data streams are assumed to have infinite length , which makes it impractical to store and use all the historical data for training. Therefore, traditional multi-pass learning algorithms are not directly applicable to data streams. Second, data streams observe concept-drift , which occurs when the underlying concept of the data changes over time. In or-der to address concept-drift, a classification model must continuously adapt itself to the most recent concept. Thir d, data streams also observe concept-evolution , which occurs when a novel class appears in the stream. In order to cope with concept-evolution, a cla ssification model must be able to automatically detect novel classes when they appear, before be ing trained with the labeled instances of the novel class. Finally, high speed data streams suffer from insufficient labeled data . This is because, manual labeling is bot h costly and time consuming. There-fore, the speed at which the data points are labeled lags far behind the speed at which data points arrive in the stream, leaving most of the data points in the stream as unlabeled. So, supervised classification techniques suffer from the scarcity of labeled data for learning, resulting in a poorly built classifier. Most existing data stream classification techniques address only the infinite length, and concept-drift problems [1 X 3]. Our previous work MineClass [4] addresses the concept-evolution problem in addition to the infinite length and concept-drift problems. However, it did not address the limited labeled data problem. Our current work, ActMiner, extends MineClass by addressing all the four prob-lems and providing a more realistic data stream classification framework than the state-of-the-art.

A solution to the infinite length problem is incremental learning, which re-quires a single pass over the training data. In order to cope with concept-drift, a classifier must be continuously updated to be consistent with the most recent concept. ActMiner applies a hybrid batch-incrementa l process [2, 5] to solve the infinite length and concept-drift problems. It divides the data stream into equal sized chunks and trains a classification model from each chunk. An ensemble of M such models is used to classify the unlabeled data. When a new data chunk becomes available for training, a new model is trained, and an old model from the ensemble is replaced with the new model. The victim for the replacement is chosen by evaluating the accuracy of ea ch model on the latest labeled chunk. In this way, the ensemble is kept up-to-d ate. ActMiner also solves the concept-evolution problem by automa tically detecting novel classes in the data stream. In order to detect novel class, it first identifies the test instances that are well-separated from the training data, and tag them as Raw outlier . Then raw outliers that possibly appear as a result of concep t-drift or noise are filtered out. If a sufficient number of such strongly cohesive filtered outliers (called F -outlier s) are observed, a novel class is assumed to have appeared, and the F -outlier sare classified as novel class instances. Finally, ActMiner solves the limited labeled data problem by requiring only a few sel ected instances to be labeled. It iden-tifies the instances for which the classification model has the highest expected error. This selection is done without knowing the true labels of those instances. By selecting only a few instances for labeling, it saves 90% or more labeling time and cost, than traditional approaches that require all instances to be labeled.
We have several contributions. First, we propose a framework that addresses four major challenges in data stream classification. To the best of our knowl-edge, no other existing data stream cla ssification technique addresses all these four problems in a single framework. Second, we show how to select only a few instances in the stream for labeling, and justify this selection process both the-oretically and empirically. Finally, our technique outperforms state-of-the-art data stream classification techniques using ten times or even less amount of la-beled data for training. The rest of the paper is organized as follows. Section 2 discusses the related works in data stream classification. Section 3 describes the proposed approach. Section 4 then pres ents the experiments and analyzes the results. Section 5 concludes with directions to future work.
 Related works in data stream classification can be divided into three groups: i) approaches that address the infinite length and concept-drift problems, ii) approaches that address the infinite length, concept-drift, and limited labeled data problems, and iii) approaches that addr ess the infinite length, concept-drift, and concept-evolution problems. Groups i) and ii) again can be subdivided into two subgroups: single model and ensemble classification approach.

Most of the existing techniques fall into group i). The single-model approaches in group i) apply incremental learning and adapt themselves to the most recent concept by continuously updating the current model to accommodate concept drift [1, 3, 6]. Ensemble techniques [2, 5] maintain an ensemble of models, and use ensemble voting to classify unlabeled instances. These techniques address the infinite length problem by keeping a fixed-size ensemble, and address the concept-drift problem by updating the ensemble with newer models. ActMiner also applies an ensemble classification technique. Techniques in group ii) goes one step ahead of group i) by addressing the limited labeled data problem. Some of them apply active learning [7, 8] to se lect the instances to be labeled, and some [9] apply random sampling along with semi-supervised clustering. ActMiner also applies active learning, but its dat a selection process is different from the others. Unlike other active mining techniques such as [7] that requires extra computational overhead to select the d ata, ActMiner does the selection on the fly during classification. Moreover, none of these approaches address the concept-evolution problem, but ActMiner does.

Techniques in group iii) are the most rare. An unsupervised novel concept detection technique for data streams is proposed in [10], but it is not applicable to multi-class classification. Our previous work MineClass [4] addresses the concept-evolution problem on a multi-class classification framework. It can detect the arrival of a novel class automatically, without being trained with any labeled instances of that class. However, it does not address the limited labeled data problem, and requires that all instances in the stream be labeled and available for training. ActMiner extends MineClass by requiring only a few chosen instances to be labeled, thereby reducing the labeling cost by 90% or more. In this section we discuss ActMiner in details. Before describing ActMiner, we briefly introduce MineClass, and present some definitions. 3.1 Background: Novel Class Detection with MineClass ActMiner is based on our previous work MineClass [4], which also does data stream classification and novel class detection. MineClass is an ensemble clas-sification approach, which keeps an ensemble L of M classification models, i.e., L = { L 1 ,..., L M } . First, we define the concept of novel class and existing class . Definition 1 (Existing class and Novel class). Let L be the current ensem-ble of classification models. A class c is an existing class if at least one of the models L i  X  L has been trained with class c .Otherwise, c is a novel class. The basic assumption in novel class detection lies in the following property. Property 1. Let x be an arbitrary instance belonging to a class c ,and c be any class other than c . Also, let  X  c ,q ( x ) be the q -nearest neighbors of x within class c ,and  X  c,q ( x ) be the q -nearest neighbors of x within class c . Then the mean distance from x to  X  c ,q ( x ) is less than the mean distance from x to  X  c,q ( x ) ,for any class c = c .
 In other words, property 1 states that an instance is closer to other same class instances and farther from the instances of any other class. Therefore, if a novel class arrives, the instances belonging t o that class must be closer to other novel class instances and far from any existing class instances. This is the basic idea in detecting novel class with MineClass. MineClass detects novel classes in three steps: i) creating decision boundary for a classifier during its training, ii) detect-ing and filtering outliers, and iii) computing cohesion among the outliers, and separation of the outliers from the training data.

The decision boundaries are created by clustering the training data, and saving the cluster centroids and radii as pseudopoints. Each pseudopoint represents a hypersphere in the feature space. Union of a ll the hyperspheres in a classification model constitutes the decision boundary for that model. The decision boundary for the ensemble of models is the union of the decision boundaries of each model in the ensemble. Any test instance falling outside the decision boundary of the ensemble of models is considered an outlier, called F -outlier .
 Definition 2 (F  X  outlier). A test instance is an F  X  outlier (i.e., filtered out-lier) if it is outside the decision boundary of all classifiers L i  X  L . If any test instance x is inside the decision boundary, then it can be shown that there is at least one existing class instance x , such that the mean distance from x to the existing class instances is less than the mean distance from x to the existing class instances. Therefore, according to property 1, x must be an ex-isting class instance. Any F -outlier is a potential novel class instance, because it is outside the decision boundary of the ensemble of models, and therefore, we its membership in the existing classe s cannot be guaranteed. However, only one F -outlier does not imply a novel class. We need to know whether there are enough F -outlier s that are sufficiently close to each other and far from the ex-isting class instances. This is done by computing the cohesion among F -outlier s and separation of F -outlier s from existing class instances. This is done using b min ( x ) is the mean distance from x to its q -nearest existing class instances and a ( x ) is the mean distance from x to its q -nearest F -outlier instances. A positive value indicates that x is closer to other F -outlier instances than the existing class instances. If q -NSC(x) is positive for at least qF -outlier instances, then a novel class is assumed to have arrived. This is the basic working principle of the DetectNovelClass() function in algorithm 1.
 3.2 ActMiner Algorithm ActMiner , which stands for Act ive Classifier for Data Streams with novel class Miner , performs classification and novel class detection in data streams while requiring very small amount of labeled data for training. The top level algorithm is sketched in algorithm 1.
 Algorithm 1. ActMiner
The algorithm starts with building the initial ensemble L = { L 1 , ..., L M } with the first few data chunks of the stream (line 1), and initializing the training buffer. Then a while loop (line 2) runs indefinitely until the stream is finished. Within the while loop, the latest data chunk D n is examined. Each instance x k in D n is first passed to the Classify() function, which uses the existing ensemble an F -outlier , then it is temporarily saved in a buffer buf for further inspection (line 8), otherwise, we output its predi cted class (line 9). Then we call the De-tectNovelClass() function to inspect buf to detect whether any novel class has arrived (line 11). If a novel class has arrived then the novel class instances are classified as  X  X ovel class X  (line 13). Then the class predictions of all instances in buf are sent to the output (line 15). We t hen select the instances that need to be labeled (lines 17-22). Only the instances identified as Weakly Classified In-stance (WCI) are required to be labeled by human experts, and they are saved in the training buffer with their true class labels (line 19). We will explain WCI shortly. All other instances remain as unl abeled, and they are saved in the train-ing buffer with their predicted class labels (line 20). A new model L is trained with the training buffer (line 24), and this model is used to update the existing ensemble L (line 25). Updating is done by first evaluating each model L i  X  L on L , and replacing the worst (based on accuracy) of them with L .ActMinercan be applied to any base learning algorithm in general. The only operation that needs to be specific to a learning algorithm is train and save decision boundary . 3.3 Data Selection for Labeling Unlike MineClass, ActMiner does not need all the instances in the training data to have true labels. Only those instances need to be labeled about whose class labels MineClass is the most uncertain. We call these instances as  X  X eakly classi-fied instances X  or WCIs. ActMiner finds the WCIs and presents them to the user for labeling, because the ensemble has the highest uncertainty in classifying the WCIs. In order to perform ensemble voting on an instance x j , first we initialize each v [ k ] represents a real value. Let classifier L i predicts the class label of x j to be c ,where c  X  X  1 , ..., C } . Then we increment v [ c ]by1.Let v [ max ]represent the maximum among all v [ i ]. Then the predicted class of x j is max . An instance x j is a WCI if either i) The instance has been identified as an F -outlier (see definition 2), or ii) The ratio of its majority vote to its total vote is less than the Minimum Majority Threshold (MMT), a user-defined parameter.

For condition i), consider that F -outlier s are outside the decision boundary of all the models in the ensemble. So the ensemble has the highest uncertainty in classifying them. Therefore, F -outlier s are considered as WCIs and need to be labeled. For condition ii), let us denote the ratio with Majority to Sum (M2S) ratio. Let v [ max ] be maximum in the vector V ,andlet s = C i =1 v [ i ]. There-considered to be a WCI if M2S( x j ) &lt; MMT. A lower value of M2S( x j ) indicates higher uncertainty in classifyin g that instance, and vice versa.

Next we justify the reason for labeling the WCIs of the second type, i.e., instances that have M2S( x j ) &lt; MMT. We show that the ensemble classification error is higher for the instances having lower M2S.
 Lemma 1. Let A and B be two sets of disjoint datapoints such that for any x higher than the ensemble error on B .
 Proof. Given an instance x , the posterior probability distribution of class c is p ( c | x ). Let C be the total number of classes, and c  X  X  1 , ..., C } . According to Tumer and Ghosh [11], a classifier is trained to learn a function f c ( . )that approximates this posterior probability (i.e., probability of classifying x into p ( c | x ). This is the error in addition to Bayes error and usually referred to as the added error . This error occurs either due to the bias of the learning algorithm, and/or the variance of the learned model. According to [11], the expected added error can be obtained from the following formula: Error =  X  and p (  X  c | x ), which is independent of the learned classifier.
 Let L = { L 1 , ..., L M } be an ensemble of M classifiers, where each classifier L i is trained from a data chunk. If we average the outputs of the classifiers in a M -classifier ensemble, then according to [11], the probability of the ensemble classifier L m ,and  X  avg c ( x ) is the added error of the ensemble, given by:  X  c ( x )= in the ensemble. Assuming the error variances are independent, the variance of  X  c ( x ), i.e., the error variance of the ensemble,  X  on A ,and B , respectively. Let z c ( x ) be 1 if the true class label of x is c ,and z ( x ) be 0, otherwise. Also, let f m c ( x ) be either 0 or 1. The error variance of classifier L m on A is given by [7]: ( z wise. Let x a be an arbitrary instance in A ,andlet r ( x a )bethemajority vote count of x a . Also, let us divide the classifiers into two groups. Let consider that the errors of the classifiers are independent, it is highly un-likely that majority of the classifiers will make the same mistake. Therefore, we may consider the votes in favor of the majority class to be correct. have incorrect predictions. The combined squared error (CSE) of the individual classifiers in classifying x a into class c is:
Note that CSE is the sum of the squared errors of individual classifiers in the ensemble, not the error of the ensemble itself. Also, note that each component the sum (since the prediction is wro ng). Now we may proceed as follows: M 2 S ( x a ) &lt;M 2 S ( x b )  X  r ( x a ) &lt;r ( x b ) (since the total vote = M )(4) This implies that the size of group 2 for x a is larger than that for x b .
Therefore, the CSE in classifying x a is greater than that of x b ,since each component of group 2 in CSE contributes 1 to the sum. Continuing from eqn (4),  X   X  Now, according to the Lemma statement, for any pair ( x a  X  X  ,x b  X  X  ),
M 2 S ( x a ) &lt;M 2 S ( x b ) holds, and hence, inequality (5) holds. Therefore, the mean CSE of set A must be less than the mean CSE of set B , i.e.,  X  1  X   X   X   X  2 That is, the ensemble error variance, an d hence, the ensemble error (since error variance is proportional to error) on A is higher than that of B . In this section we describe t he datasets, experimental environment, and discuss and analyze the results.
 4.1 Data Sets and Experimental Setup We use two synthetic and two real datasets for evaluation. These are: Syn-thetic data with only concept-drift (SynC) , Synthetic data with concept-drift and novel-class (SynCN) , Real data -KDDCup 99 network intrusion detection (KDD) , and Real data -Forest cover dataset from UCI repository (Forest) . Due to space limitation, we omit the details of the datasets. Details can be found in [4]. We use the following parameter settings, unless mentioned otherwise: i) K (number of pseudopoints per classifier) = 50, ii) q (minimum number of in-stances required to declare novel class) = 50, iii) L (ensemble size) = 6, iv) S (chunk size) = 2,000. v) MMT (minimum majority threshold) = 0.5. 4.2 Baseline Approach We use the same baseline techniques that were used to compare with MineClass [4]. Since to the best of our knowledge, there is no technique that can both classify and detect novel class in data streams, a combination of two baseline techniques are used in MineClass: OLINDDA [10], and Weighted Classifier Ensemble ( WCE ) [2], where the former works as novel class detector, and the latter performs classification. For eac h chunk, we first detect the novel class instances using OLINDDA . All other instances in the chunk are assumed to be in the existing classes, and they are classified using WCE .Weuse OLINDDA as the novelty detector, since it is a recen tly proposed algorithm that is shown to have outperformed other novelty det ection techniques in data streams [10].
However, OLINDDA assumes that there is only one  X  X ormal X  class, and all other classes are  X  X ovel X . So, it is not directly applicable to the multi-class nov-elty detection problem, where any combina tion of classes can be considered as the  X  X xisting X  classes. We propose two alternative solutions. First, we build parallel OLINDDA models, one for each class, which evolve simultaneously. Whenever the instances of a novel class appear, we create a new OLINDDA model for that class. A test instance is declared as novel, if all the existing class models identify this instance as novel. We will refer to this baseline method as WCE-OLINDDA PARALLEL. Second, we initially build an OLINDDA model with all the available classes. Whenever a novel class is found, the class is absorbed into the existing OLINDDA model. Thus, only one  X  X ormal X  model is maintained throughout the stream. This will be referred to as WCE-OLINDDA SINGLE. In all experiments, the ensemble size and chunk-size are kept the same for both these techniques. Besides, the same base learner is used for WCE and ActMiner. The parameter settings for OLINDDA are the same as in [4].
 In this experiment, we also use WCE-OLINDDA Parallel and WCE-OLINDDA Single for comparison, with some minor changes. In order to see the effects of limited labeled data o n WCE-OLINDDA models, we run two dif-ferent settings for WCE-OLINDDA Parallel and WCE-OLINDDA Single. First, we run WCE-OLINDDA Parallel ( WCE-OLINDDA Single) with all instances in each chunk labeled . We denote this setting as WCE-OLINDDA Parallel-Full (WCE-OLINDDA Single-Full). Second, we run WCE-OLINDDA Parallel (WCE-OLINDDA Single) with exactly the same instances labeled as were labeled by ActMiner, plus any instance identified as novel class by WCE-OLINDDA Parallel (WCE-OLINDDA Single). We denote this set-ting as WCE-OLINDDA Parallel-Partial (WCE-OLINDDA Single-Partial). We will henceforth use the acronyms AM for ActMiner, WOP f for WCE-OLINDDA Parallel-Full, WOS f for WCE-OLINDDA Single-Full, WOP p for WCE-OLINDDA Parallel-Partial, and WOS p for WCE-OLINDDA Single-Partial. 4.3 Evaluation Evaluation approach: Let F n = total novel class instances misclassified as ex-isting class, F p = total existing class instances misclassified as novel class, F e = total existing class instances misclassified (other than F p ), N c = total novel class instances in the stream, N = total instances the stream. We use the fol-lowing performance metrics to evaluate our technique: M new =%ofnovelclass instances Misclassified as existing class = F n  X  100 N instances Falsely identified as novel class = F p  X  100 N  X  N of the error metrics, it is clear that ER R is not necessarily equal to the sum of M new and F new . Also, let L p be the percentage of inst ances in the data stream required to have labels for training.

Evaluation is done as follows: we build the initial models in each method with the first init number labeled chunks with all instances in each chunk labeled. In our experiments, we set init number = 3. From the 4th chunk onward, we evaluate the performances of each method on each data point. We update the models with a new chunk whenever all wea kly classified instances (WCIs) in that chunk are labeled.
 Results: Figures 1(a 1 ),1(b 1 ) show the ERR of each baseline technique and fig-ures 1(a 2 ),1(b 2 ) show the percentage of data labeled ( L p ) corresponding to each technique on a real (Forest) and a synthetic (SynCN) dataset with decision tree. Corresponding charts for other datasets and k-NN classifier are similar, and omitted due to the space limitation. Figure 1(a 1 )showstheERRofeach technique at different str eam positions for Forest dataset. The X axis in this chart corresponds to a particular stream position, and the corresponding value at the Y axis represents the ERR upto that position. For example, at X=200, corresponding Y values represent the ERR of a technique on the first 200K in-stances in the stream. At this position, corresponding Y values (i.e., ERR) of AM, WOP f ,WOP P ,WOS f and WOS p are 7.5%, 10.8%, 56.2%, 12.3%, and 63.2%, respectively. The percent age of data required to be labeled ( L P )byeach of these techniques for the same dataset (Forest) is shown in figure 1(a 2 ). For ex-ample, at the same X position (X=200), the L P values for AM, WOP f ,WOP P , WOS f and WOS p are 8.9%, 100%, 12.7%, 100%, and 9%, respectively. Therefore, from the first 200K instances in the stream, AM required only 8.9% instances to have labels, whereas, its nearest competitor (WOP f ) required 100% instances to have labels. So, AM, using 11 times less labeled data, achieves lower ERR rates than WOP f . Note that ERR rates of other methods such as WOP p , which uses less than 100% labeled data, are much worse.

Figures 1(c 1 ),1(d 1 ) show the number of novel instances missed (i.e., misclassi-fied as existing class) by each baseline technique, and figures 1(c 2 ),1(d 2 )report the total number of novel instances encountered by each technique on the same real (Forest) and synthetic (SynCN) dat asets with decision tr ee classifier. For example, in figure 1(c 1 ), for X=200, the Y values represent the total number of novel class instances missed by each technique within the first 200K instances in the stream. The corresponding Y values for AM, WOP f ,WOP P ,WOS f and WOS p are 366, 5,317, 13,269, 12,156 and 14,407, respectively. figure 1(c 2 )shows the total number of novel instances encountered by each method at different stream positions for the same dataset. Different approaches encounter different amount of novel class instances because the ensemble of classifiers in each ap-proach evolve in different ways. Therefor e, a class may be novel for one approach, and may be existing for another approach.

Table 1 shows the summary of the evaluation. The table is split into two parts: the upper part shows the ERR and M new values, and the lower part shows the F new and L p values. For example, consider th e upper part of the table corre-sponding to the row KDD under Decision tree . This row shows the ERR and M new rates for each of the baseline techniques on KDD dataset for decision tree classifier. Here AM has the lowest ERR rate, which is 1.2%, compared to 5.8%, 64.0%, 6.7%, and 74.8% ERR rates of WOP f ,WOP p ,WOS f and WOS p ,re-spectively. Also, the M new rate of AM is much lower (1.4%) compared to any other baselines. Although WOS f and WOP f have lower ERR rates in SynC (decision tree), and Forest (k-NN), resp ectively, they use at least 20 times more labeled data than AM in those datasets, which is reported in the lower right part of the table (under L p ), and their M new rates are much higher than AM. Note that L p is determined from the WCIs, i.e., what percentage of instances are weakly classified by the ensemble. Therefore, it is different for different datasets. Some readers might find it surprising that active learning outperforms learn-ing with full labels. However, it should be noted that ActMiner outperforms other proposed techniques, not MineClass itself. MineClass, using 100% labeled instances for training, still outperform s ActMiner because ActMiner uses less la-beled instance. However, other proposed techniques (like WOP) have too strong requirement about class properties. For example, OLINDDA requires the classes to have convex shape, and assumes simila r density of each class of data. On the other hand, ActMiner does not have any such requirement. Therefore, in most real world scenarios, where classes have non-convex shape, and different classes have different data densities, ActMiner performs much better than OLINDDA in detecting novel class, even wi th much less label information.

Figure 2(left) shows the effect of increasing the minimum majority threshold (MMT) on ERR rate, and figure 2(right) shows the percentage instances labeled for different values of MMT on SynC. For AM, the ERR rate starts decreasing after MMT=0.5. This is because there is no instance for which the M2S (majority to sum) ratio is less than 0.5. So, L p remains the same (1%) for MMT=0.1 to 0.5 (see figure 2(b)), since the only instances needed to be labeled for these values of MMT are the F -outlier instances. However, when MMT=0.6, more instances needed to be labeled ( L p =3.2%) as the M2S ratio for these (3.2-1.0=) 2.2% instances are within the range [0.5,0.6). The overall ERR also reduces since more labeled instances are used for training. Sensitivity of AM to other parameters are similar to MineClass [4], and omitted here due to space limitations.
Table 2 reports the running times of AM and other baseline techniques on different datasets with decision tree. Running times with k-NN also have similar character istics. Since WOP f and WOP p have the same running times, we report only WOP f . The same is true for WOS f and WOS p .Thecolumns headed by  X  X ime (sec)/1K  X  show the average running times (train and test) in seconds per 1000 points excluding data labeling time, and the columns headed by  X  X ime (sec)/1K (including labeling time) X  show the same including data labeling time. For example, excluding the data labeling time, AM takes 1.1 seconds to process 1K instances on the KDD dataset, whereas WOP f , WOP p takes 24.0, and 0.5 seconds, respectively. In general, WOP f is much slower than AM, requiring about C times more runtime than AM. This is because WOP maintains C parallel OLINDDA models to detect novel classes. Besides, OLINDDA creates clusters using an internal buffer every time it encounters an instance that is identified as unknown , which consumes much of its running time. On the other hand, WOS f runs slightly faster than AM in three datasets. But this advantage of WOS f is undermined by its much poorer performance in classification accuracy than AM. If we consider the data labeling time, we get a more compelling picture. We consider the labeling times only for real datasets. Suppose the labeling time for each data point for the real datasets is 1 sec, although in real life, data labeling may require much longer time [12]. Out of each 1000 instances, AM requires only 33, and 65 instances to have labels for the KDD, and Forest datasets, respectively (see table 1 under L p ). Whereas WOP f and WOS f require all the 1000 instances to have labels. Therefore, the total running time of AM per 1000 instances including data labeling time is only 3.4% and 6.5% of that of WOP f and WOS f for KDD and Forest datasets, respectively. Thus, AM outperforms the baseline techniques both in classification accuracies and running times. Our approach, ActMiner, provides a more complete framework for data stream classification than existing techniques. ActMiner integrates the solutions to four major data stream classification problems: infinite length, concept-drift, concept-evolution, and limited labeled data. Most of the existing techniques address only two or three of these four problems. ActM iner reduces data labeling time and cost by requiring only a few selected instances to be labeled. Even with this limited amount of labeled data, it outperforms state-of-the-art data stream classification techniques that use ten times or more labeled data. In future, we would like to address the dynamic feature set problem and multi-label classification problems in data stream classification.

