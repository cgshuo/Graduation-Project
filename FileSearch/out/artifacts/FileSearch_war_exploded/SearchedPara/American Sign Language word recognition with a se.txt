 1. Introduction
Sign language, which is a highly visual X  X patial, linguistically complete and natural language, is the main mode of communica-tion among deaf people. However, deaf people still experience serious problems communicating with people who hear normally, almost all of whom do not understand sign language systems such as American Sign Language (ASL). This communication barrier affects deaf people X  X  lives and relationships negatively.
Deaf people usually communicate with hearing people either through interpreters or text writing. Although interpreters can facilitate communication between deaf persons and hearing persons, they are often expensive, and their involvement leads to a loss of independence and privacy. While writing is used by many deaf people to communicate with hearing people, it is very inconvenient while walking, standing at a distance, or when more than two people are involved in a conversation.

Sign language is not universal. Different countries have different sign languages, for example, American Sign Language (ASL) and
German Sign Language (GSL) have different alphabets and word sets. The similarities among signs in a sign language are created by complex body movements, i.e., using the right hand, the left hand, or both. When signs are created using both hands, the right hand is more active than the left hand. Sign language speakers also support their signs with their heads, eyes, and facial expressions.
Many researchers have been working on the recognition of various sign languages and gestures, but this research poses major difficulties due to the complexity of hand and body movements in sign language expression. Sign language recognition research can be categorized into three major classes: (i) computer-vision based, (ii) data-glove and motion-sensor based, and (iii) a combination of these two methods. Computer-vision based ASL recognition relies on image processing and feature extraction techniques for captur-ing and classifying body movements and handshapes when a deaf person makes an ASL sign. On the other hand, data-glove and motion-tracker based ASL recognition methods use a sensory glove and a motion tracker for detecting handshapes and body move-ments. The third method includes a combination of techniques from these two methods ( Oz et al., 2004 ).

Acquiring data is more difficult with the vision-based method than with the data-glove and motion-sensor based methods. Data can be collected efficiently through a 3-D vision system, which has multiple cameras and a fast frame grabber. This system requires complicated image processing methods, which demand more data and slow the recognition rate. The main advantage of this approach is that the user does not need to wear any uncomfortable devices. Additionally, facial expressions can be incorporated. In the data-glove and motion-sensor based systems, the signer has to wear a glove and sensor devices that measure the physical features of the gesture, e.g., trajectory, angles, motion, and finger bending.

In the earliest linguistic description of ASL, Wilbur (1987) used a structural linguistic framework to analyze sign formation. His purpose was to develop a national system for writing signs that contained symbols for each individual hand shape, location, and movement. Stokoe (1978) analyzed ASL formation and suggested additions to the three basic building blocks of hand shape, location, and movement. One major parameter, orientation of the palm, was suggested by Battison (1978) . These and similar ASL studies have furthered the research of other ASL recognition scientists.
In parallel to the advancements in sensor and computer technology, some successful computer-vision based sign language recognition systems have been developed. Earlier sign language recognition research appeared in the literature at the beginning of the 1990s. Charayaphan and Marble (1992) developed an image-based processing system to understand ASL using hand motions. Takahashi and Kishino (1991) used a range classifier to recognize 46 Japanese Kana manual alphabet with a VPL data glove. Their study was based on simply encoding data ranges for joint angles and hand orientation.

Since 1990, Artificial Neural Networks have been used widely for solving engineering and industrial problems. Because of the popularity of ANNs, sign language researchers have applied this algorithm to solve their problems. Kramer and Leifer developed an ASL finger spelling system using a Cyberglove with the use of a neural network for feature classification and sign recognition ( Kramer and Leifer, 1990 ; Kramer, 1996 ). Murakami and Taguchi (1991) established a recurrent neural network method to recognize 110 distinct Japanese Sign Language signs. Waldron and Kim (1995) used a neural network method to recognize 14 ASL signs using a different network for hand shape and hand orientation and position. Wysoski et al., (2002) developed an image-based ASL recognition system with a neural network for 26 static postures. Allen et al. (2003) developed an ASL finger spelling recognition system, which could recognize 24 ASL letters with a neural network. Wang et al. (2004) designed an ASL gesture recognition system with a sensory glove using an ANN, a Hidden Markov Model (HMM), and a minimum distance classifier. Oz and Leu (2007) designed an ASL recognition system based on linguistic properties with a sensory glove using an ANN.
The Hidden Markov Model, which has a well-founded mathe-matic basis and is an efficient doubly stochastic process, has been used widely in speech recognition, text recognition, and other engineering problem solving ( Rabiner, 1989 ; Takiguchi et al., 2001 ). Many ASL researchers achieved successful results using HMM. Vogler and Metaxas (1997 , 1999) used HMM for contin-uous ASL recognition with video streaming. In 1997, they were able to recognize 53 signs and a completely unconstrained sentence structure. In 1999, they were able to recognize ASL sentences with 22 signs based on ASL phonemes. Grobel and Assan (1996) used HMMs to recognize isolated signs based on computer vision with the signers wearing colored, normal gloves. Their accuracy was 91.3% for 262 signs.

There are also some Human Computer Interaction (HCI) studies based on human gestures. Lee and Xu (1996) used HMM to recognize the ASL alphabet for a human X  X obot interface. Lee et al. (2000) developed a hand gesture recognition system with human X  X omputer interaction. Stergiopolou and Papamarkos (2009) used a neural network with a shape fitting filter to recognize hand gestures.

In this study, we present an ASL word recognition system that is constructed to translate ASL signs into the corresponding English words with an ANN method. A reliable adaptive filtering system with a recurrent neural network is used to determine the duration of ASL signing. All parameters affect the accuracy of feature vector extraction. The histogram method is used to extract features from ASL signs. Based on these features, a word recogni-tion neural network is used as a classifier to convert ASL signs into English words. The developed system is capable of recognizing all 50 ASL words used in the testing. 2. System hardware and software
One of the primary means by which we physically connect to the world is through our hands. We perform most of our everyday tasks with them; however, along with our hands, we also rely on devices such as a mouse, keyboard, and joystick to work with computers and computer applications. Glove-based input devices could overcome this limitation ( Sturman and Zelter, 1994 ). Commercial devices such as the VPL data glove and the Mattel power glove have led to an explosion of research and develop-ment projects using electronic glove interfaces for computer applications with computer controlled devices. Examples of these applications include virtual reality, video games, scientific visua-lization, puppetry, and gesture-based control.

Weusearight-handCyberglove TM ( Fig. 1 ) to retrieve the joint angles for gesture features. The glove has 18 sensors, which measure the bending angles of fingers at various positions. We use 15 sensors on the glove: three sensors for the thumb, two sensors for each of the other four fingers, and four sensors between the fingers. The frequency of data collection is up to 150 Hz.
To track the position and orientation of the hand in 3-D space, the Flock of Birds s motion tracker ( Fig. 2 ) mounted on the hand and wrist is used. The receiver is located in a DC pulsed magnetic field, and the effective range is up to 8 ft around the transmitter. The measuring frequency is up to 144 Hz.

Open Inventor SDK (Software Development Kit) is used in the software development for the 3-D scene rendering. It is a high-level toolkit developed in OpenGL for graphic rendering and user interaction. We use the Microsoft s Speech SDK for the program-ming of speech synthesis. The software system is implemented using Object Oriented Programming (OOP) technology; therefore, it is easily extendable.

Fig. 3 shows the overall structure of our system. The Cyber-glove sensory glove and the Flock of Birds motion tracker are connected to the computer system with two separate RS-232 serial ports. The data stream from these devices is retrieved and segmented by a data collection program. The gesture features extracted from the raw data are sent to a decoder of the recognition system. The system produces the voice of the recog-nized ASL with a speech synthesizer. 3. Determine the duration of ASL signing
Commonly used English words are represented by signs in ASL ( Sternberg, 1994 ). The signs are expressed by hand movements over a period of time. The data collected from the Cyberglove and Flock of Birds s are input to the word recognition system.
Each time a person signs an ASL word, the data gathered consist of hand position, wrist rotation, and finger bending. Let x z be the x , y , and z coordinates stored in the i th sampling cycle. A change in hand direction from the previous cycle is indicated by ment with
D x
D y
D z  X  z i z i 1  X  1  X  The length of this vector is
D  X  q and it will give us a measure of the signing speed due to a constant sampling period.

First, all words must be identified correctly in terms of their durations of signing from the measured signals. In our case, distinguishing words requires some additional calculation due to noise and some missed movements. We first use a filter for noise reduction, and then we use a velocity network to determine whether the sign is an ASL word or not. Fig. 4 shows an example of the hand velocity profile during a typical signing movement.
During this movement, the hand velocity can increase or decrease momentarily due to momentary starting or stopping of the hand.
Hence, the use of a threshold value of velocity may not give a good solution for classification of hand movements. The ideal output for the velocity graph shown in Fig. 4 should be 1 from the time the sudden change in velocity is first seen until the time the velocity graph shows a series of low velocities. Hence, the hand velocity is filtered and then input to the velocity neural network.
The velocity neural network used is a three-stage network with two input neurons, ten hidden layer neurons, and two output neurons. The target vectors are created manually by studying the velocity graphs for different, randomly-selected signs. Twenty training samples of different signs are found to be sufficient to achieve the desired accuracy. The network is trained using the Levenberg X  X arquardt algorithm. A hyperbolic tangent sigmoid function is used in both the layers. The output of the network lies in the range of [ 1, 1], which is converted to 0 or 1.
The network continuously calculates the output using the data of filtered velocity and speed change, which are updated at 33.33 Hz. Fig. 5 , which shows the performance of the velocity
Cyberglove &amp; 3-D tracker network, demonstrates that the network produces a continuous output of 1 during the actual signing period.

If the output of the velocity network is 1, then the 15 data points from the glove and 6 data points from the motion tracker are recorded in an array. These data are collected at a frequency of 33.33 Hz. No data are recorded if the output of the velocity network is zero. When the velocity network output changes from 1 to 0, the data recording for the given signing is completed, and the feature extraction phase begins. In our study, the signing time varied between 0.15 and 1.5 s. A virtual hand model in VRML displayed on the computer screen is updated at 33.33 Hz using the data gathered from the sensory glove and motion tracker. 4. Data collection and feature extraction
The data collection and feature recognition operations consist of the following stages: first, the duration of the ASL word signing is identified; second, the data are collected and passed through a filter; and third, features are extracted from the data. As explained previously, the first filter, which is online, identifies the ASL word. The second filter, which is offline, reduces noise from the sensors X  data. Position and orientation data are filtered using the following filter: In this equation, u ( t ) is the input, y ( t ) is the output, and t is the time.

The filtered signs consist of time interval, hand shape, orienta-tion, trajectory, and hand velocity. The data of any particular sign can change each time it is signed due to variations by the signers, which cause difficulty for the word recognition algorithm design. Therefore, a constant dimension model is designed using some feature vectors, and a basic algorithm using whole word data is used. The following features are extracted. 4.1. Distance and time
The sum of D i would give a measure of the total distance covered by the sign X  X  hand movement. Every sign covers a different distance than the others. Thus, we define the feature distance as Distance  X  where n is the number of cycles for the sample. A sample of distances obtained from seven tries for the ASL word  X  X  X bortion X  X  is given in Fig. 5 . 4.2. Bounding boxes
Signs have different lengths, and they also differ in their location around the body. The hand movement for of an ASL sign fits in a rectangular box represented by ( x min , y min , z y max , z max ). We can use this bounding box as a feature vector.
The calculation formula for the bounding box is relatively simple, as evidenced in Table 1 for the ASL word  X  X  X bortion X  X . After determining the boundaries of ASL signs for the words included in our study, we normalize the data in the bounding box to be between 0 and 1. 4.3. Position histograms
The normalized trajectory is used to decrease the negative effect of the signer X  X  body position o n the accuracy of the result. The normalized trajectory is calculated using the following formulas: X Y Z
A histogram can be thought of as a discretized probability density function. Basically, the range of possible values is segmented into sub-ranges, and then the number of instances of each sub-range is counted. The histogram technique is commonly used in computer vision and pattern recognition ( Oz and Ercal, 2003a , 2003b ).
The histogram can be calculated in the following way: let d be the number of divisions we wish to segment the range into, and let h i ,0 r i r d , be the  X  X  X olumns X  X  of the histogram for each of the three coordinate axes. The following equations show how the position histogram can be calculated for the x axis: h  X  r  X  x j  X  X  Effectively, this  X  X  X ormalizes X  X  two components. The first is the length of the sequence. Because the sum term above includes a 1/ n term, and every x j must fall into exactly one h i column, the net effect is that P d 1 i  X  0 h i  X  1. The second component relates to the bounding box of the sign. The column divisions are relative to the bounding box; thus, most of the h i ,0 r i r d will not be zero. This is desirable because it essentially removes the issues of a sign X  X  size and low resolution on small signs. An alternative would be to have absolute locations, which would be nowhere as closely correlated to the information in the sign itself.

Our data range is between 30 and 30, which we divide into ten sub-ranges. Fig. 6 shows a histogram of ( x , y , z ) coordinates for an ASL word. 4.4. Orientation histogram
We can similarly apply histograms to the wrist orientation represented by a , b , and g angles. Our orientation values range between 180 and 180 1 . We can calculate the histogram while the word is being signed.

In the rotation histogram, there are 12 possible values, as shown in Fig. 7 . Although we could directly build histograms on this data, it would make the system extremely sensitive. Fig. 8 shows the hand X  X  pitch, roll, and yaw histograms for an
ASL sign. 4.5. Hand-shape histogram
The most important American Sign Language feature is hand shape, which comes from the stu dy of structural linguistics. Stokoe (1960) used a structural linguistic framework to analyze ASL sign formulation. Most of the hand shapes represent the ASL alphabet, basic numbers, and some particular words. ASL has 36 hand shapes.
We design an ANN model for hand-shape recognition using finger bending data collected from the Cyberglove. Our ANN hand-shape recognition model consists of 15 input neurons, 28 hidden neurons, and 36 output neurons. Fig. 9 shows the ASL hand shapes.
We train and test our model for both single and multiple users and find it to have an accuracy of 98%. This model is not given in the present paper because it was detailed in an earlier paper ( Oz and Leu, 2007 ). We use hand shape as a feature for sign recognition. The drawback of this system is that it uses only one glove size, which produces different data for different hand sizes for the same ASL words. Therefore, the system can only be reliable for a hand that is nearly the same size as the training hand. Fig. 10 shows the hand-shape histogram for the word  X  X  X bortion X  X . 4.6. Location histogram
One of our feature vectors is hand position, which is based on four measurement points, (a) X (d), as shown in Fig. 11 .Thespaceof hand location for signing is designated as a rectangular prism that is divided into 14 regions using four measurement points as follows. Points (a) X (c) are used for dividing a two-dimensional ( z , y ) coordinate system. Point (d) is used for the third dimension, and it is measured when the hand is parallel to the floor and the palm is facing the floor. Every signer must establish his own reference points, which can then be called up from a stored file at any time. 4.7. Velocity histogram
We also use a histogram for the magnitude of hand velocity, for which values typically range between 0 and 3. If the velocity value is greater than 3, we consider it 3. Fig. 12 shows a sample velocity histogram for the word  X  X  X bortion X  X . 5. Artificial neural network model A backpropagation algorithm is used for training the ANN model.
The basic structure and formulation of backpropagation is summar-ized here. Training a neural network involves computing weights so as to get an output response to the input within an error limit. The input and target vectors make up a training pair. The backpropaga-tion algorithm includes the following steps ( Lippman, 1987 ): 1. Select the first training pair and apply the input vector to the net. 2. Calculate the net output. 3. Compare the actual output with the corresponding target and find the error. 4. Modify the weights so as to reduce the error.

These steps are repeated until the error falls within the accepted limit. In Step 2, the output sets for test inputs are calculated. If they are nearly the same as the expected sets within the prescribed error range, then it is considered that the net has learned the problem, and the final weights are stored so that they can be reused when needed. The developed ANN has a multi-layer feedforward structure as shown in Fig. 13 . The variable definitions are given as follows ( Abulafya, 1995 ; Narendra, 1992 ): L  X  0: input layer, L  X  1: hidden layer, L  X  2: output layer, W 1,ji : weight matrix between the input layer and the hidden layer, W 2,tj : weight matrix between the hidden layer and the output layer, B 1,j values of hidden neurons, and B 2,t : bias values of output neurons.
Eq. (11) gives the output of the hidden layer: y y  X  f j  X  y NET 1 , j j  X  1 , 2 , ... , m  X  11  X 
Eq. (12) gives the output of the output layer: y y  X  f t  X  y NET 2 , t t  X  1 , 2 , ... , k  X  12  X 
Activation function: f  X  y 6. Design of the ASL word recognition system using ANN
The ASL speaking space is the hemisphere region in front of the speaker. When signs are created with both hands, the right hand is often more active than the left hand. Speakers also support their signs with their heads, eyes, and facial expressions. In the present paper, we study right-hand words only.

Some ASL words used in the training set for the ANN are given in Table 2 . When we look at the definition, we see that each word begins with a hand shape in a start position and continues with changing hand shapes and positions.
 A multi-layer ANN is designed to recognize a set of ASL words.
Although the developed system is fully flexible for any number of words, in the demonstration, the system is trained to recognize 50 words in ASL. We digitize the speaker X  X  signing with hand w angles. The extracted feature vectors of ASL words, which are produced from data collected during the signing period, are input to the network. The signing period is the time period over which the output of the velocity network is 1. At each time interval, the data from the Cyberglove and the Flock of Birds are recorded. These data consist of 15 bending angles from the Cyberglove, as well as three position and three orientation readings from the Flock of Birds. Before we produce seven feature vectors as described earlier, we pass the position data through a noise reduction filter. For every ASL sign, we use a total of 151 features: 2 for distance and time, 30 for trajectory ( x , y , z ), 36 for orientation ( a , b , g ), 36 for hand shape, 27 for motion, 14 for location, and 6 for hand velocity. The ANN consists of 151 input variables, 100 hidden neurons, and 50 output neurons. The system is designed to recognize a single word as a whole at one time. A Levenberg X  X arquardt backpropagation algorithm is used for training. The ANN is trained and tested for two different data sets, single-user data and multi-user data. The output file consists of 50 outputs, each representing one word or expression. The training set of words, which can be signed with the right hand, is selected from an ASL dictionary. An output encoding sample is shown in Table 3 .

The design of the recognition system is given in Fig. 14 .It consists of four parts: data collection, feature extraction, trained network, and output decoding.

During data collection, the velocity network triggers the data recording function. The data get recorded until the velocity network output becomes zero. The x , y , z values are passed through a filter for noise reduction. The seven main feature vectors are created from the recorded word data. The data of 151 elements are input to the ANN. The output decoding block determines the maximum output of the ANN and then checks whether the maximum output value is acceptable or not by comparing it with a pre-specified threshold value. If it is accep-table, the corresponding index of the output vector is the recognized word. If the value is not acceptable, the word to be recognized is considered to be outside of the known ASL vocabu-lary. The program allows the user to add a new word after the recognition procedure determines that this word is not in the training set. The program asks the user,  X  X  X o you want to add this word? X  X  If the user answers yes, then the new word with its corresponding signing can be added to the training set. The network is then trained to recognize the new word. Retraining the network with the new word requires a new arrangement of the model parameter, such as selecting the number of hidden neurons and indicating when the training ends. The hidden-layer neurons are determined using the following calculation: integer of hidden layer neurons  X  integer ((number of input layer neu-rons  X  number of output layer neurons)/2).
 The recognized word is displayed on the screen using the Open Inventor Interface. Some of the ASL words recognized by our system are illustrated in Fig. 15 . 7. Test results
Two ASL word recognition tests were developed, one with single-user data and the other with multi-user data. In both tests, the ASL recognition system trained with three, six, and twelve samples of data with 50 words. At the testing stage, real-time data were used. In total, 300 ASL signs (6 50) in the training set were used for the test. Both the single-user model and the multiple-user model were tested in two different ways, sequen-tially and randomly. The sequential test started with the word abortion and ended with the word beautiful . Both models were also tested with 120 signs (6 20) that were not in the training set, such as I , you , etc. Training and testing data were taken from hearing people who did not know ASL. First, signers read ASL word definitions from the dictionary and watched a video about how to sign each ASL word. After these short courses, they tried to sign the ASL words they had just learned. Therefore, data from the same person could vary. Some hand-shape network training and testing data were signed by deaf teachers from the Missouri School for the Deaf who also tested the hand-shape recognition system.

Sequential testing results were better than random testing results because, during sequential testing, every sign was exe-cuted six times; thus, the signers were actually exhibiting practice effects. During random testing, lower accuracy was normal because the signers signed different words at every test stage, making it difficult for them to determine new sign positions correctly. The testing results are given in Table 4 . In this table, Feature extraction  X  X issed X  means an unknown word. This came from the output selection procedure in which we established a threshold value for the output, and any output under this threshold was regarded as an unknown.

Both tests were conducted with 120 signs of words that were not in the training set. The test results were as good as those obtained during testing with known words. The test results are summarized in Table 5 . 8. Conclusions
The development and evaluation of an ASL word recognition system was described in this paper. The data from a Cyberglove sensory glove and a Flock of Birds s 3-D motion tracker were processed by a velocity network with noise reduction and feature extraction and by a word recognition network for the purpose of ASL recognition. Some global and local features were extracted for every ASL word. Neural networks were used to classify these feature vectors. The system was trained and tested for single and multiple users for a vocabulary of 50 ASL words. Test results show that the recognition accuracy of the system is about 90%. Besides the words in the training set, our system is capable of recognizing new ASL words without any modifications.

Some gestures in ASL require that both the right and left hands be manipulated simultaneously. Such words may be recognized by applying the proposed model but with two data gloves and more motion trackers.
 Acknowledgments This research is partially supported by a National Science Foundation award (DMI-0079404) and a Ford Foundation grant, as well as by the Intelligent Systems Center at the Missouri University of Science and Technology in the United States. References
