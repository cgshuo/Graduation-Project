 The aim of relation extraction as a subtask of information extraction is to find various predefined semantic relations between pairs of entities in text. The research of relation extraction has been advanced by the Message Understanding Conferences (MUC) [1] and the NIST Automatic Content Extraction (ACE) program (ACE, 2000-2007) [2] in Phase 2. As a subtask of Information Extraction, relation extraction can be utilized in many applications such as Question Answering and information retrieval. 
To our knowledge, no work has been done to examine the performance of the tree kernel or the shortest path dependency kernel on Chinese corpus. Since more errors exist in Chinese syntax analysis compared with English, whether these kernel meth-ods are applicable for Chinese relation extraction is uncertain. Since relation extraction task was first introduced in MUC6, many methods, such as feature-based [3, 4], tree kernel-based [11, 12, 13, 14] and composite kernel-based [5, 15, 16], have been proposed in literature. 
Feature-based methods for relation extraction employ explicitly various linguis-tic features, from lexical features, syntactic information to dependency trees and semantic knowledge in Max Entropy model [3] or SVM model [4]. The feature-based methods have achieved the state-of-art performances. However, the feature selection feature-based methods lack the ability to explore the struct u ral syntactic or depend-sight. improvements as it gives us an elegant way to explore structural features implicitly by computing the similarity between two objects via a kernel function, thus give us a good chance to explore the parsing or dependency information of the sentence where English relation extraction, from the hierarchical tree kernels [11] and [12] defined on shallow parse tree or dependency trees, shortest path dependency kernel [13] to the current convolution parse tree kernel[14,16]. Moreover, composite kernel which gen-performance further [15, 16]. Up to now, the kernel-based methods have achieved and recently exceeded the best performance of the feature-based methods for relation extraction. algorithms, for example, SVM [17] and bootstrapping [18], have been proposed in feature-based framework, and the reported results are usually alluring just on certain types of relations or on non-standard dataset. Besides, [19] proposed a novel im-proved Edit kernel for Chinese relation extraction, in which, the author improved the Chinese relation extraction. However, as to structural kernels, which have been ex-plored extensively recently for English, few work has been done up to now. kernel and the convolution parse tree kernel, we can see undoubtedly the convolution parse tree kernel has achieved better performance than the other kernel types and the achieves general performance. Therefore, in this paper, we explore different feature spaces involved in convolution kernel and im prove the original shortest path depend-dependency paths. A convolution kernel [7] aims to capture structural information in terms of substruc-tures. As a special convolution kernel, the convolution tree kernel proposed in [9] counts the number of common sub-trees as the structural similarity between two parse trees. Furthermore, [10] has proved that th e convolution tree kernel can be computed in O(|N1|*|N2|) where N1 and N2 equal to the number of nodes in two trees. Same as most of previous work on kernel-based relation extraction, we first em-ploy the parse tree segment within the entity pair (called Path-enclosed tree in [14]) in convolution kernel for entity relation extraction. Then, considering the limited parse relation instances to incorporate a verb factor in two strategies. 
The first strategy (figure 1) is to extend the cases by including the highest level of verb (the main predicate) of the sub-sentence where the entity pair occurs when there are not any verb between the entity pair. The second strategy (figure 2) is to include the nearest verb to the entity pair (similar to the dynamic span expansion method proposed in [16]). The two strategies are out of our wonder that whether the main predicate which is powerful to determine the semantics of the whole sub-sentence or tribute more to the final entity relation identification. The shortest path kernel proposed in [13] requires two shortest dependency paths to have the same length which may contribute to the low recall. To loosen the constraint, we improve the kernel by summing up the common word classes on the longest common subsequences of two shortest dependency paths other than the original short-est dependency paths. 
In implementation (figure 3), the general part-of-speech features (the italic) of the nodes is used to match the longest common subsequences while the part-of-speech longest common subsequences are utilized to compute the similarity. Same as in [13], no normalization is done in the improved shortest path dependency kernel. 
Besides, in our experiments, the dependency information is generated by the same CFG (Context Free Grammar) parser employed to generate the parse trees, so the dependency links form a tree naturally. Thus, the shortest dependency path is somewhat different from the path used in [13], the meanings slightly differ accord-ingly. There is no unified direction distribution in the original shortest dependency path. In meaning, the shortest dependency paths of Bunescu and Mooney mainly describe the predicate-argument interactions, that is, the dependency relation of the two entities as the arguments to the predicates. In contrast, we always have a parent, pointed by dependency nodes on both sides, in our shortest dependency path. In meaning, our shortest dependency paths c onvey an ordered dependency series con-necting the entity pair. Accordingly, the improved dependency path kernel will oper-ate on the two longest common subsequences belonging to the two sides from the two ends to the central parent. 5.1 Experimental Setting Data: we use the Chinese portion of ACE (Automatic Content Extraction) 2007 cor-pora from LDC to conduct our experiments. In the ACE 2007 data, the training set include 689 documents and 6900 relation instances while the testing set include 160 documents and 1977 relation instances. Specifically, there are 2030 non-nested rela-tion instances in the training set and 620 in the testing set. The ACE 2007 data defines 7 major entity types: Facility, GPE, Location, Organization, Person, Vehicle and Weapon. In this paper, we assume that the entities and their types are already known. relation instances. The data imbalance and sparseness are potential problems in ACE corpus, for example, the  X  X mployment X  subtype has 1265 positive instances while the  X  X rtifact X  subtype has only 6 instances in the training data, besides, in both the train-ing part and the testing part, the negative samples are 10 times more than the positive samples. 
Data processing: We select the Stanford syntactic parser to generate the sentence parse tree and dependency list. For we don X  X  find any appropriate POS tagger preserv-ing the Penn standard required by the above parser, we use the POS tagger internal to the parser. Besides, we utilize the PKU Chinese word segmenter. During parsing, we segment sentences which are too long to be parsed at one time. For shortest path de-pendency kernel, we construct the sentence dependency tree utilizing the output de-pendency list and extract the shortest path from it. 
Classifier: we select the LibSVM [20] as our classifier and insert into the convolu-tion tree kernel [21] and our shortest dependency path kernel based on the longest common subsequences. Besides, we adopte one vs. one strategy for the multi-class classification. The parameters are selected using 5-fold cross-validation. 
Kernel normalization: the convolution tree kernel, the linear entity kernel and its expansion occurring in all experiments are all normalized while the two shortest path dependency kernels are not. The normalizing method is: 
Evaluation Methods: we adopt Recall (R), Precision (P) and F-measure (F) stan-dards. 602 R. Huang, L. Sun, and Y. Feng 5.2 Experimental Results (1) Table 1 compares the performances of three different parse tree spans involved in the pure convolution tree kernel. We threshold the output to get best performances. We can see the Path-enclosed parse tree achieves best performance, although much lower than 74.12/54.90/63.07 (P/R/F) attained in our feature-based experiments. Es-pecially, strategy one gets the lowest F-measures which generally will involve larger infer that more noises have been introduced into the kernel computation which may counteract the benefits of involving somewhat more complete syntactic structures in the kernels. On the other hand, we are confirmed that more efforts are needed to find appropriate and effective feature spaces. (2) With the aim to examine the compleme ntary effect of the tree features and the flat features, we also experimented with two composite kernels which are combinations of the above convolution kernel with the linear entity kernel (Comp-linear) and its polynomial expansion (Comp-poly) as stated in [15]: In table 2, the F-measures show both composite kernels (with  X  set to 0.5 in Comp-linear and 0.7 in Comp-poly) advance the performances to a extent not as evi-dent as that on English dataset [15]. Especially, unlike [15, 16], the polynomial entity kernel embodying bi-gram entity information doesn X  X  improve any performance com-pared with the linear entity kernel, ruins the evaluations reversely. (3) Our original motivation to study kernel-based methods is to improve the extrac-tion performance of the non-nested relations, they have longer distances and more non-nested relations are presented in table 3. Besides, we give our previous experi-mental results using feature-based methods. The comparison shows that structural kernel X  X  performance utilizing only the parse tree information has exceeded the fea-ture-based methods on non-nested relation instances a bit. the improved version based on the longest common subsequences over real relations (4) The experimental results on the shortest dependency path kernel are somewhat disappointing. The improved kernel shows poor performance on the relation detection while the original shortest dependency path kernel has even not any relation detection ability since the trained model treats all the potential relations as positive or negative instances under different parameters. So, in table 4, we only show their multi-classifying performances on all the 1977 positive instances. We can see although our improved kernel has better performance, in general, the classification performances are both too low. When we examined the dependency path representations of relation dependencies which is presumed in [13] and we may reason that it X  X  really difficult to extract Chinese relations using this type of kernel. In this paper, we explore the effectiveness of kernel-based methods for Chinese rela-tion extraction. The experimental results show that although the current tree kernels haven X  X  achieved as good performance as the feature-based methods, it has given reasonable measures, especially it has overrun feature-based methods on non-nested relations which is difficult to deal with by just using flat features. 1. MUC (1987-1998), http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ 2. ACE (2002-2007), urlhttp://pr ojects.ldc.u penn.e du/ace/ 3. Nanda, K.: Combining lexical, syntactic and emantic features with Maximum Entropy 604 R. Huang, L. Sun, and Y. Feng 7. Haussler, D.: Convolution Kernels on Discrete Structures. Technical Report UCS-CRL-99-8. Sch X lkopf, B., Smola, A.J.: Learning with Kernels: SVM, Regularization, Optimization 9. Collins, M., Duffy, N.: Convolution Kernels for Natural Language. In: NIPS (2001) 10. Collins, M., Duffy, N.: New ranking algorithms for parsing and tagging: Kernels over dis-11. Zelenko, D., Aone, C., Richardella, A.: Kernel Methods for Relation Extraction. Journal of 12. Culotta, A., Sorensen, J.: Dependency Tree Kernel for Relation Extraction. In: ACL 2004 13. Bunescu, R.C., Mooney, R.J.: A Shortest Path Dependency Kernel for Relation Extraction. 17. Che, W.X.: Automatic Entity Relation Extraction. Journal of Chinese Information Process-18. Zhang, S.X.: Study about automatic entity relation extraction. Journal of Harbin Engineer-19. Che, W.X.: Improved-Edit-Distance Kernel for Chinese Relation Extraction. In: Dale, R., 20. LibSVM. http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 21. Moschitti. Convolution Tree kernel, 
