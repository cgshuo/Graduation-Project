 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  query formulation, retrieval models, search process. Algorithms, Experimentation, Human Factors. Salton Award Lecture, informa tion retrieval, human-computer interaction, interdisciplinary research, evaluation. Let me begin by saying how honored I am to receive the Gerard Salton Award from SIGIR. His pioneering work, including the vector space model, term weight ings, relevance feedback, and the development and evaluation of auto matic text retrieval systems, defined and shaped the field in important ways. I did not have many opportunities to interact directly with Gerry Salton, although our paths did cross in inte resting ways. Shortly after I started working at Bell Labs, I was introduced to the field of information retrieval by Michael Lesk, who had worked with Gerry at Harvard in the mid-1960s to develop the original SMART system. The last conference at which I interacted with Gerry was not SIGIR or TREC, as one might expect, but rather CHI 1995, where he spoke at panel entitled, Browsing vs. search: Can we find a synergy?  X , a theme to which I return. Following the tradition of previous recipients of the Salton Award (Gerard Salton (1983), Karen Sp X rck Jones (1988), Cyril Cleverdon (1991), William Cooper (1994), Tefko Saracevic (1997), Stephen Robertson (2000), W. Bruce Croft (2003), and C. J.  X  X eith X  van Rijsbergen (2006)), I present a pers onal reflection on information retrieval. I begin with some remarks on the problems that have motivated my research in the area for almost three decades, and the approaches that I have taken to address them. I conclude by identifying, what I believe to be, some important opportunities to con tinue to extend and improve information retrieval moving forward. Given my academic roots in mathematics and cognitive psychology, it is not surprising that my research interests are at the intersection of information retrieval and human-computer interaction. My approach to problems is more user-centric than system-centric, more empirical than theoretical, but I cross these boundaries as different stages of investigation. I do so because I believe that the success of inform ation retrieval systems depends critically on both the ability of systems to efficiently and effectively represent, match and rank objects, and the ability to understand and support individua ls in articulating their information needs and analyzing the retrieved results to solve the problem that motivated their search in the first place. My interest in information retr ieval started in the early 1980 X  X  with the observation that differen t people use a surprisingly wide variety of words to describe the same object or concept  X  things like computer commands, or keywor ds for information objects. This fundamental characteristic of human word usage set limits as to how well a simple lexical matc hing system that assigned only a few terms (no matter how carefully chosen) to each object could do in satisfying users. (It is symptomatic of the problem that we described this research as vocabulary mismatch, verbal disagreement, and statistical semant ics.) We developed, deployed and evaluated solutions that involved collecting multiple aliases for objects, and reducing the dime nsionality of the representation using techniques like Latent Semantic Indexing (LSI). Much of my subsequent research has been similarity motivated by the goal of identifying limitations of current retrieval systems, and developing new algorithms, interac tion techniques or evaluation methods to overcome them. We have made some progress by representing and leveraging richer contextual information about users, content domains, and the la rger task environments in which retrieval takes place. For example, we have modeled searchers X  interests and activities over time to personalize search results and better support people in re-fi nding information they have previously seen; identified attributes that reflect the varied relationships between objects and tightly coupled faceted browsing of these attributes with search to support more flexible access strategies; and used some simple task contexts to proactively retrieve related information. Although we are encouraged by these examples there is much to be done theoretically to represent knowledge about searchers and tasks into a consistent framework, and empirically to extend evaluation methodologies to better capture the iterative and inte ractive nature of search. Over the last ten to fifteen years, the information retrieval landscape has changed dramatically  X  it has been an amazing time to be working in this area. Fifteen years ago, Lycos went public with an early Web search engine that contained 54k pages. Today, search engines index bil lions of pages and have become the main entry point for an ever-increasing range of information, services, communications and entertainment on the Web, enterprise and desktop. Research in information retrieval provides many of the foundational algorithms and data structures that power such systems, so we have much to be proud of. Despite these tremendous accomplis hments, we still have a long way to go. Many searches are unsuccessful, and even those that succeed are often harder than they should be. Moving forward, the scale and diversity of the information, users, tasks, and search applications will continue to incr ease. We will need to extend retrieval models, evaluation me thodologies and our understanding of users to address these challenges. I present two examples of directions that I believe are of growing importance to the information retrieval community. The first area has to do with the dynamics of information and users X  interactions with it. This is perhaps most evident in social media such as blogs or wikis but is also present in more  X  X raditional X  documents and Web pages. It occurs at scales as different as the desktop and Web. How can we extend retrieval models and systems to go beyond a single, static snapshot of information? How can we model searchers in a way that captures the evolution of their information needs within a single session, and across many search episodes? The second area has to do with evaluation. Information retrieval has a long and successful traditi on of careful evaluation using shared testbeds of documents, queries, relevance assessments and outcome measures. This paradi gm has served us well for improving representations, matching and ranking algorithms, but it has limitations. Evaluations me thodologies need to be extended to handle the scale, diversity, and user interaction that characterize information systems today. For example, Web search engines, e-commerce sites, and digital libraries benefit from being able to study searchers in situ as they interact with information resources using log data. Such data provide valuable insights about what users are doing, and how well sear ch engines are meeting those needs. To be sure, there are important challenges in collecting and using interaction data (e.g., privacy of individual data, replicability of experiments, extracting signal from noisy behavioral data), but I believe that we must begin to address this to be able to make continued progress. Can we go even further and develop a kind of  X  X iving laboratory X  in which research groups can try new ideas with sear chers in situ, thus enabling controlled experiments in the wild? Information retrieval research has long played a central role in the storage, analysis and retrieval of information. As that information landscape evolves, we need to con tinue to extend our models for representing the underlying content, and our understanding of users and contexts of use. To do so will, I believe, require collaboration among researchers from different disciplines. Awards like this are not achieved alone. Over the years I have had the opportunity to work with and learn from many colleagues at Bell Labs, Bellcore, Microsoft, and the SIGIR and SIGCHI communities --Tom Landauer, George Furnas, Louis Gomez, Michael Lesk, Scott Deerwester, Don Swanson, Nick Belkin, Eric Horvitz, Stephen Robertson, Ed Cutrell and Jaime Teevan. They have all been instrumental in my understanding of how people create, find, organize and anal yze information, and in the development of new algorithms, interaction and evaluation techniques to better support these processes. 
