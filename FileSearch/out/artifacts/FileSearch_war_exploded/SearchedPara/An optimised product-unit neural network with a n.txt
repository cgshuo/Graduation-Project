 1. Introduction
Piles serve as the foundations for various important structures, such as tall buildings, offshore platforms, and wind turbines. One of the key functions of the piles is to transfer the axial load from the superstructure to the subsurface through shaft friction and end bearing. An accurate prediction of pile behaviour under loading is desirable to ensure that it does not deform beyond an acceptable level. Pile deformation under axial loading is a complicated process involving inter-related movements of both the pile and the supporting soil as the load is transferred from the pile to the supporting medium. Uncertainty about soil properties due to their spatial variability and the pile installation also adds to the complexity of the process. Numerous analytical, semi-empirical, and numerical models for pile load  X  settlement analysis are avail-able in the literature (see Table 1 ). These include elastic continuum methods ( Poulos and Davis, 1980 ), load transfer methods ( Coyle and Reese, 1966 ), and the FEM ( fi nite element method) approach ( Desai, 1974 ). The major dif fi culty in using any of the aforemen-tioned models is how to select the appropriate soil parameters. While simple soil models could yield misleading results, sophis-ticated soil models require parameters which are dif fi cult to determine.
 led to the great deal of attention given and interest in the ANN over the past two decades. Apart from being computationally simple, it is quite ef fi cient in performing a local search for an optimum solution, but severely limited in its ability to conduct a search for a global minimum.

Search algorithms: Metaheuristic gradient-free search algo-rithms are quite capable of conducting a widespread search for a global optimum. These models have been used in a wide range of applications, including neural network parameter optimisation. Particle swarm optimisation (PSO), developed by Kennedy and Eberhart (1995) , is one of the most popular gradient-free search techniques. Compared to other evolution-ary techniques, such as genetic algorithms, it is easier to implement and yields more accurate results Elbeltagia et al. (2005) . However, the major defect of metaheuristic algorithms is their inef fi ciency in searching for local minima.
Since neither the gradient-free nor the gradient-based algo-rithms are ef fi cient under all circumstances, some attempts have been made to develop hybrid algorithms in which PSO and BP complement each other. For instance, Zhang et al. (2007) and Cui et al. (2009) developed a similar algorithm, in which a PSO is initially used to train the network and BP is used later to complete the training process when no further improvement is observed in the PSO training. Although their results are in favour of hybrid training algorithms, the PSO in the hybrid algorithms tends to be underutilised: it is limited to the weight initialisation for the BP algorithm. This compromises the ability of the hybrid algorithm to explore the search space ef fi ciently.

In this paper, we propose an approach to combine PSO and BP algorithms to achieve a greater search capability. The novel of this approach is that the learning takes place without the risk of being trapped at local minima. In the proposed algorithm, the training begins with the PSO for a certain number of iterations. A number of solutions with the least errors are then selected from the PSO population and further trained for a few epochs using the Leven-berg  X  Marquardt (L  X  M) algorithm. The results of the local search by
L  X  M are then used to update the particle positions and the
PSO training is resumed. By alternating these two algorithms in the training process, the search will be more thorough and the risk of skipping the best solution is reduced. To demonstrate the ef fi ciency of the proposed learning algorithm, a higher order network is optimised using the new technique to predict the load settlement behaviour of axially loaded piles. The response of piles to loading is an extensively investigated soil  X  structure accordance with the equation  X  w n  X   X  g n ;  X  1  X  where g n is the gradient of the error function  X   X   X  : g n  X   X   X   X  w  X  ,in which  X  w n and  X  are the vector of weight adjustment and learning rate, respectively.

The Levenberg  X  Marquardt (L  X  M) algorithm is a more ef fi cient but more computationally intensive version of BP. It was devel-oped by Marquardt (1963) as an improvement on the Gauss  X  Newton method, with the aim of eliminating the numerical instability associated with the matrix inversion in the latter. The method was proposed for feed forward network training by Hagan and Menhaj (1994) . The synaptic weights are updated as follows: w where J n is the Jacobian matrix J  X  matrix with the same size as J T n J n . The parameter  X  is an always positive modi fi cation factor. The modi fi cation parameter is adjusted in accordance with the variation of the error level as the training progresses. It is increased whenever an iteration step would lead to an increased value of the error function  X   X  w  X  . When a step reduces the value of  X   X  w  X  ,  X  is reduced.

The Levenberg  X  Marquardt algorithm interpolates between steepest descent and Gauss  X  Newton methods. When  X  is large, the modi fi cation term outweighs the Jacobian term and the algorithm approximates the steepest descent method. When  X  is small, the modi fi cation term becomes negligible and the method becomes roughly the same as the Gauss  X  Newton approximation. As the L  X  M method is based on a higher degree of error function approximation, it converges much more quickly than the steepest descent back-propagation algorithm. However, similar to other gradient-based algorithms, the L  X  M algorithm has a limited ability to conduct a global search. 2.2. The PSO-based training algorithm
Particle swarm optimisation (PSO) is a class of derivative free metaheuristic search algorithms inspired by the intelligent co-operation between individual organisms in a group of animals. It was designed to simulate the co-ordination and movement dynamics in biological swarms as they search for sources of food or avoid adversaries ( Eberhart and Kennedy, 1995 ). Unlike evolu-tionary optimisation algorithms, such as genetic algorithms (GA) and evolutionary programming (EP), which are based on competi-tion and survival of the fi ttest, PSO is based on information sharing between individual members of the population. Despite the simplicity of its algorithm in comparison with GA and EP, a comparative study ( Eberhart and Kennedy, 1995 ) revealed that PSO algorithms perform better than both GA and EP. The arti fi cial swarm consists of a population of particles, each of which is described by its position (co-ordinate) and its velocity. The particle co-ordinates are a set of parameters representing a potential solution to the optimisation problem at hand. A particle aims at a better position in the search domain by moving away from its current position. The velocity of this movement is dictated by a combination of factors, including the current momentum of the particle, its past experience, and the experience of the best positioned particle in the swarm. Particles acquire experience by keeping in memory the co-ordinates of the best position they have on PSO-based neural net training ( Kennedy and Eberhart, 1995 ) that a particle swarm optimiser could train NN weights as effectively as the usual error back-propagation method. Later,
Gudise and Venayagamoorthy (2003) compared the performance of PSO and BP using simulated data (a quadratic curve). Their result indicated that PSO converges faster than back-propagation.
Results from Ren et al. (2005) also favoured PSO over BP, although the study was limited to pattern recognition. Despite its greater ability to traverse the search space in pursuit of a global optimum,
PSO, like other metaheuristic search algorithms, are inferior in local searches to gradient-based algorithms. Jumping over the best solution and converging to some point away from the global optimum is not uncommon with metaheuristic search methods, due to their lack of a focus on local minima. 2.3. PSO  X  BP hybrid algorithms In recognition of the performance of PSO in global searches and
BP algorithms in local searches, hybrid PSO  X  BP algorithms have been developed with the aim of achieving an ef fi cient global search without compromising the local search, and vise versa.
Zhang et al. (2007) developed a hybrid algorithm in which an ANN is constructed using PSO until no improvement in accuracy is obtained. Back-propagation is then used to complete the training.
From their results, a combination of BP and PSO improved the network performance over that of either BP or PSO, both in terms to stagnate. To avoid this problem, the positions of the particles that cluster at the same location are randomly reset at the end of each cycle of PSO iterations. This will not only enhance the particles  X  capacity to better explore the search space, but also will help to avoid selecting identical particles for further training using the BP algorithm. Also, to improve the ef fi ciency of the local search, all the particles whose positions have not improved since the last local (BP) search are skipped. The rationale behind taking this measure is to avoid the risk of conducting a local search using a particle that has already settled on a local minimum.
The proposed hybrid training algorithm is executed in the following steps: 1. Generate N particles with randomly initialised parameters and compute the fi tness of each particle. 2. Use the PSO algorithm to update the particle velocities and positions for a certain number of iterations. Also, update the best particle and swarm positions in each iteration. Go to Step 7 if satisfactory convergence is obtained. 3. Reset, by randomisation, the parameters of all but one of the particles that have duplicate co-ordinates at the same local optimum. 4. Arrange the particles in the order of decreasing fi tness value (i.e., increasing error). 5. Select a certain number of particles with the best fi tness value. 6. From among those selected particles, remove those with no improvement in fi tness since the last local search. Train the rest using the L  X  M algorithm. 7. If the training is satisfactory, go to step 8; else go to Step 2. 8. Terminate the algorithm and return the result.

In the present paper, a swarm population of 60 particles is used, and the best 10% of the particles are chosen for the L  X  M based local search at the end of the PSO iterations. The fl owchart in Fig. 3 illustrates the structure of the proposed algorithm. 2.4. Product-unit neural networks (PUNN)
Product-unit neural networks (PUNN) are a class of higher-order neural networks introduced by Durbin and Rumelhart (1989) , which perform neural computations by processing the products of the input they receive, instead of adding together the weighted input signals fed into the neurons. Conventional feed-forward networks (e.g., MLP and RBFN) use summation units to compute the processing signals. However, the number of proces-sing units may become too large when dealing with a highly nonlinear function involving higher order combinations of inputs. In such cases, product-unit neurons could potentially reduce the number of processing functions signi fi cantly, given their greater information capacity ( Durbin and Rumelhart, 1989 ). The mathe-matical model of a product-unit neuron is O input signal, and the synaptic weight, respectively. The basic structure of a product-unit network consists of three hidden layers: the input layer, the product-unit layer, and the summation layer ( Durbin and Rumelhart, 1989 )( Fig. 4 ). The PUNN model is similar to the HON (high-order neural network) model ( Abdelbar and Tagliarini, 1996 ) for pattern recognition and function approximation.

Despite their ability to map complex relationships with a relatively smaller number of processing units, product-unit net-works generate a more complex error surface than other types of dependent on the pile geometry and the elastic modulus of the pile material. The soil resistance depends on the properties of the soil and the characteristics of the soil  X  pile interface. The soil properties are usually estimated from in situ or laboratory tests. In this study, SPT-N values are used to represent the soil resistance. The SPT test is a popular low cost in situ test.
 The parameters k shaft and k base are the soil stiffness around the shaft and at the base, respectively, de fi ned by k
Based on Eqs. (13) and (14), the previous equation can be re-written as P  X  f 1  X  s ; k shaft ; E p ; D ; L  X  X  f 2  X  s ; k base ; D  X  :  X  15  X 
The shaft and base components of the soil stiffness are func-tions of the SPT-N values around the shaft and at the base, respectively. The SPT value around the shaft is determined by computing the weighted average value of N along the pile shaft N where the subscript n is the number of layers along the shaft of the pile segment; l i is the thickness of the i th pile segment; N i is the average value of SPT-N over the pile segment, and L is the pile length. Other factors affecting the soil stiffness are the contact area (i.e., the shaft area in the case of k shaft and the base area in the case of k base ), the soil type, and the method of installation. By replacing the soil stiffness parameters with the controlling variables in Eq. (15) , the expression for the total pile head load becomes P  X  f 1 s ; N s A s ; E p A p
The terms f 1 and f 2 are functions representing the contributions of the shaft and base resistance to the pile response. Both f 1 and f 2 are unknown functions represented by product-unit neurons. The base areas, respectively. A p is the cross-sectional area of the pile shaft. styp , instl , and pmt _ typ are parameters for the soil type, the method of pile installation, and the type of pile material, respec-tively. Due to the qualitative nature of these parameters, they have to be represented by some quantitative information, or otherwise, the neural network would not be able to handle them. For example, the parameter styp is represented by two components: the percentage of fi ne soil (silt and clay) and the plasticity of the soil (1.0 for high plasticity and 0 for low or no plasticity). In this study, both the percentage of fi ne soil and the plasticity are approximately deduced from the soil classi fi cation. A total of three nodes are dedicated to styp : one node for silt, another for clay, and another for plasticity. This parameter is represented by two nodes. Both nodes are deactivated when the pile is driven. If bored and the excavation method is dry, one of the nodes is switched on. Both nodes are switched on when a pile is bored and the excavation method is wet. This parameter enters into two input nodes. Both nodes are switched off when the pile is made up of concrete. One of the nodes is switched on if the pile is H-steel, while the other is switched on when the pile is steel pipe. More information about the network input is given in Fig. 5 . 3.3. Networks training and testing
The data was split into training and testing sets. The training set was used to teach the network, and the testing set was used to test the performance of the model. The training data constitutes 72% of the database, while the remaining 28% was earmarked for testing. The network was trained using the proposed PSO  X  BP hybrid training as described in Section 2 . For the sake of compar-ison, the network model was trained alongside the proposed algorithm, using the BP, PSO, and PSO  X  BP algorithms ( Zhang et al., 2007 ). The training data contains, to the extent possible, the widest variations in input and output patterns in the database. training and testing results for the various models are summarised in Table 4 .

The quality of the ANN learning is assessed by comparing the training data and the network output, while the network ' s prediction accuracy is determined by comparing the testing data and the network predictions. In Fig. 7 (a) and (b), the prediction results of the PUNN trained using the existing PSO  X  BP algorithm are plotted alongside the training and testing data.

Similarly, the predictions of the PUNN based on the proposed algorithm are also compared with the training and testing data in
Fig. 8 (c) and (d). It can be seen from the fi gures that the network trained using the proposed algorithm gives a better correlation with the training data than does the existing PSO  X  BP hybrid ( R 2  X  0.9623 for the new proposed algorithm and 0.9327 for the existing PSO  X  BP hybrid). This is indicative of the greater search power of the former.
Based on the R 2 values ( R 2  X  0.9611 for the proposed algorithm and 0.9266 for the existing PSO  X  BP hybrid) and N-RMSE values (N-
RMSE  X  0.04374 for the proposed algorithm and 0.06239 for the existing PSO  X  BP hybrid) in the case of testing, the network optimised with the proposed algorithm yields a better prediction than the existing PSO  X  BP algorithm. It is noticeable from the results in Table 4 that both nonhybrid algorithms (BP and PSO) underperformed in learning and prediction when compared to the hybrid algorithms. Predicted load (kN) Predicted load (kN) In the pile deformation analysis which was carried out here, the PUNN, optimised using the proposed hybrid algorithm, gives a more accurate simulation of the load  X  deformation curves than the other training algorithms. Furthermore, the predictions of the optimised PUNN model are in better agreement with pile loading tests than either those of the hyperbolic or the t  X  z methods. The results also indicate the high ability of the PUNN to use relatively simple data, such as the SPT, soil classi fi cation, pile type, and method of installation, to predict the axial load  X  deformation behaviour of piles with a reasonable accuracy.
 Acknowledgements The authors are grateful for support from The Petroleum Technology Development Fund, Nigeria, and the National Nature Science Foundation of China (Grant no. 41172252).
 References
