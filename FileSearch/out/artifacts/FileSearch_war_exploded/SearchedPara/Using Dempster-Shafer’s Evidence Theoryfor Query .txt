 The information needs of the searchers are often formulated as keyword queries. However, searchers tend to use short and incomplete queries. Consequently the original query input by the user may miss some important information, resulting in a negative impact on the retrieval effect iveness. To address this problem, query expansion (QE) has been widely used [5] [7] [15].

Pseudo-relevance feedback (PRF) is a typical query expansion method [8] [7] [19] [33]. It selects expansion terms from top-ranked documents in the first-round retrieval as feedback documents, by assuming that feedback documents are relevant and contain the key concepts related to the query. However, in practice the top ranked documents may not all be truly relevant, so that many expansion terms derived may be irreleva nt. The uncertainty of the relevance of expansion terms may cause the query drifting problem [8].

To tackle this problem, recent resea rch attempts to make use of external knowledge, such as click data, user log, Wikipedia or simple ontology data Word-Net. The analysis of the user search history (e.g., click data) may avoid the query drifting problem, but the coverage of such knowledge is limited[10]. The emer-gence of the Web and large-scale human edited knowledge (e.g., Wikipedia and Freebase) provides access to new sources of high quality term associations for query expansion [34].

In this paper, we focus on studying the use of an ontology-structured knowl-edge source -Freebase, as the source of o btaining expansion terms. Freebase employs the entity-relation model to describe the data in the knowledge base. It has over 37 million topics about real-world entities like people, places and events. We choose Freebase for three rea sons. First, it contains a large amount of knowledge, covering different aspect s of the world, such as location, peo-ple, event, military, music, film, literature and business. As an illustration, we searched the TREC queries in the Freebase, and found that 90% of the queries (we used in our experiments) have matches of some relevant knowledge. Second, unlike Wikipedia that describes human knowledge with a long detailed article, Freebase describes the human knowledge using an ontological structure, and describes each property with keywords. Therefore, the relevant concepts under-neath a query can be quickly located. T hird, different from the WordNet which mainly contains synonymy relations, Freebase not only describes entities, but also contains different properties and entity-relationships. Thus one can walk through the entity-relation graph to find deeper relationships.
 A fragment of the entity-relation graph in Freebase is shown in Figure 1. We can see that all the information about an instance (e.g., AT&amp;T) is contained in its properties (e.g., space, industry, currency). We can also find other in-stances that are related to the current one. For example, starting from AT&amp;T and walking through the Organization Domain, we can find AT&amp;T X  X  successor node - X  X acific bell X , which is also an instan ce. For a query, we can find the matched instances as well as their parent and ch ild instances up to a certain step, and treat them as different aspects that descr ibe the query. Then it is key to fuse the relevant information from different aspects. In this paper, we propose to solve the fusion problem by using the Dempster-Shafer X  X  (D-S) evidence theory [23]. D-S theory allows one to combine evidence from different sources and arrive at a degree of belief that takes into account all the available evidences. Treating different instances as different evidences for determining query expansion terms from Freebase, we redefine the Basic Probability Assignment (BPA) and Be-lief functions in the D-S theory, and define a new measure, called Certainty , to represent a term X  X  importance in the k nowledge structure. Under the frame-work of D-S theory, we also define the transitivity of the nodes in the knowledge structure. Finally, we integrate the above parameters in a single model (see Section 4.3).

The knowledge terms are then combined with the terms given by the relevance model (RM3) [17] [18]. In the combination process, we balance the knowledge term X  X  Certainty values by D-S theory and the original weights by RM3, in order to compute the final term weights in the query expansion model. Our experiments show that the selected know ledge terms are complementary to the initial query (Table 2). Combining the knowledge terms and the RM3 terms can significantly improve the retrieval effectiveness. Pseudo relevance feedback (PRF) is an effective query expansion method by reformulating the original query using terms derived from the pseudo-relevance documents [8] [7] [19] [33]. Based on the assumption that top-ranked documents are relevant to the query, several PRF algorithms are proposed, e.g., Okapi [22], relevance model [15] and mixture model [35].

Despite the large number of studies in this area, a crucial question is that the expansion terms extracted from the pseudo-relevant documents are not all use-ful [8]. It was found that one of reasons for the failure of query expansion is the lack of relevant documents in local rele vance feedback collect ion. Recently, some researches attempt to capture and utilize lexico-semantic relationships, based on the association hypothesis formulated by van Rijsbergen [25]. Early global expansion techniques [11] [12] [33] aim t o determine the strength of semantic relatedness between terms based on the ter m co-occurrence statistics. The emer-gency of hand-crafted general purpose or domain-specific ontologies also provide access to high quality term a ssociations for query expa nsion. Several approaches have been proposed to utilize external resources, such as query logs [10], Word-Net [9], Wikipedia [34], ConceptNet [13], etc. Yin et al. proposed an expansion method based on random walking through URL graph [12]. Voorhees [32] experi-mentally determined an upper bound of the WordNet based on query expansion.
Furthermore, there has b een an emerging line of research on ontology-based query expansion. Vallet et al. [3] used an ontology query language to search for relevant documents. Nagypal et al. [20] combined the use of ontology with the vector space model. Braga et al. [2] used ontologies for information extraction and filtering across multiple domains. Ozcan et al. [4] adopted ontologies to represent concepts which are expanded by using WordNet. Meij et al. [16] showed that discriminative semantic annotations of documents using domain-specific ontologies, such as MeSH, can effectiv ely improve retrieval effectiveness. Several methods which combine multiple resources have also been proposed. Mandala et al. [31] proposed a method to combine three different thesaurus types for query expansion. Bodner et al. [27] combined WordNet and co-occurrence based thesauri for query expansion. In the work of Collins-Thompson and Callan [30], a Markov chain based framework were proposed for query expansion by com-bining multiple sources of knowledge on term associations. In Cao et al. [28], term relationships from co-occurrence statistics and WordNet were used to smooth the document language model.

Dempster-Shafer (D-S) theory has been applied to IR in several previous stud-ies. In [21], D-S theory has been used to combine content and link information in Web IR, and in [24], it is used to combine textual and visual evidences for image retrieval. Theophylactou and Lalmas [14] also used D-S theory to model a compound term as a set of single terms. In our work, we employ the D-S theory to measure the scores of the terms in Freebase that are matched against a query, and to determine the weights of the e xpanded terms based on such scores. Given a query, in order to find matched instances in Freebase, we first split a query into sequences of words. For exa mple, the query  X  X ommercial satel-lite launches X  can be split into three phrases:  X  X ommercial satellite launches X ,  X  X ommercial satellite X  and  X  X atellite launches X . We search these three phrases in Freebase and get relevant instances. Note that we do not split the query to single terms, because we think that single terms will more likely match non-relevant instances, due to lack of context. For example, the query  X  X lack Monday X  de-scribes the largest one-day percentage d ecline in recorded stock market history. If we split it to  X  X lack X  and  X  X onday X , the two separate terms, individually, are irrelevant to the query. In our experiment, the minimum splitting unit is two sequential terms.

After getting the phrases from the query, we then adopt a two-round match-ing process to find candidate instances. In the first round, we will find whether a phrase extracted from query is recorded in the Freebase using exact match. If the phrase is recorded, we will get the corresponding instance and extract the notable domain from the instance. in the second round, we employ the Freebase X  X  API to search for (partially) matched instances within the notable domain. For example, for the phrase  X  X lack Monday X , in the first round matching, we get the exactly matched instance  X  X lack Monday X  and extract its notable domain  X  X vent X . In the second round, we only search the instances belonging to the  X  X vent X  domain. Through the two-round matching, we get the matched instances. The Match Score provided by the Freebase X  X  API, i s used to measure the relevance degree between a query phrase and an instance.

As shown in Fig.1, one instance X  X  prope rty may be another instance. There-fore, in addition to the two round matching strategy described above, we go deeper in the Freebase graph by walking through the path as follows: where property jk is the k  X  th property of instance j .Weusethe property jk to find the child instance k and finally we can get the property of instance k .Inour paper, we call the instances directly from the two round matching process as the first level instances and their successor instances as second level instances . 4.1 An Introduction to Dempster-Shafer Theory The Dempster-Shafer (D-S) theory is a mathematical theory of evidence [23]. It allows one to combine evidence from different sources and arrive at a degree of belief that takes into account all the available evidence. It is developed to account for the uncerta inty. Specifically, let  X  represent all possible states (or elements) under consideration. Function m: 2  X   X  [0 , 1] is called a basic proba-bility assignment ( BPA ), which assigns a probability mass to each element. It has two properties:
First, the empty set is assigned the value 0: Second, the sum of the probabilities assigned to all elements of  X  is 1:
Based on the mass assignments, the upper and lower bounds of a probability interval can be defined. This interval contains the precise probabilities of a set of elements (denoted as A ), and is bounded by two non-additive continuous measures called belief (or support) and plausibility: The belief Bel ( A ) for a set A is defined as the sum of all the masses of subsets of A : That is, Bel ( A ) gathers all the evidence directly in support of A . The plausibility Pl ( A ) is the sum of all the masses of the sets B that intersect A : It defines the upper bounds of A .

Dempster-Shafer theory corresponds to the traditional probability theory when m assigns non-zero probabilities only to individual elements. The Dempster X  X  rule of combination provides a method to fuse the evidences for an element from mul-tiple independent sources and calculate an overall belief for the element. In our paper, we use the Dempster X  X  rule to measure the Certainty of a term from differ-ent matched instances, detailed in the next subsection. 4.2 Using D-S Theory to Rank the Candidate Terms We treat different instances as differe nt sources of evidence. Our objective is to measure the certainty of a term that is contained in multiple instances. We assume the hypothesis space is compos ed of all the candidate terms extracted from Freebase, e.g., instance  X  X  X  { a, b, c, d, e, { d, e }} ,where d, e are second level instances. We use P to represent the hypothesis space.

We measure the basic probability assignment( BPA )ofasingleterm t i in P as follows: where tf k ( t i )represents t i  X  X  term frequency in the Instance I k , and we define the initial Certainty ( I k ) as the Match score of the Instance I k with respect to the query. We adopt different strategies to measure the Certainty for instances got from the two round matching process. Intuitively, if an instance contains more terms from query, the instance is more relevant to the query. Therefore we use the length of the first round matched instance to measure I 1  X  X  Certainty. For the instances got from the second round retrieval, we initialize their Certainty according to the comparison with I 1 : where FS ( I i )representsthe Match Score provided by Freebase and i =1.We use the FS ( I 1 ) as a standard value, and use FS ( I i ) FS ( I FS ( I i )and FS ( I 1 ), where FS ( I i ) &lt;FS ( I 1 ).
 For the single term, we calculate the Bel ( t i ) as follows: For the second level instance Ip i , we calculate the Bel ( Ip i ) as follows: According to the Dempster-Shafer evidence theory, the plausibility of the second level instance calculated as follows: As mentioned in Section 4.2 , the plausibility of the single term is the same as Bel .So, Pl ( t i )= Bel ( t i ).
 Fusion. Recall that, we treat different in stances as different sources of evi-dence, and Dempster X  X  rule of combination that aggregates two or more bodies of evidence (defined within the same frame of discernment) into one body of evidence. Let m j and m k be two different BPA calculated from I j and I k .Aas the common terms. we calculate the combined BPA as follows : Where B and C are two hypothesis space from two different Instances. According to the Dempster X  X  rule of combination, we fuse the term X  X  BPA from different instances. We calculate the Bel using the uniformed BPA . Transitivity. In order to measure the second le vel instance X  X  properties, we need to measure the Certainty of the second level instance, and according to the Bel and Pl , we measure the second level instance as follows: where | Ip i | indicates the length of the instance property, and | P | indicates the number of the elements in P . Therefore we get the Certainty of the second level instance which is passed through its precursor X  X  instance. After that, we can use such method to calculate Certainty of s econd level instance X  X  property. We use Certainty to represent term X  X  final score in the Freebase ( Considering Bel = Pl for single term. So, Certainty ( t i )= Bel ( t i )). 4.3 Extended Relevance Model In the framework of Relevance Model(RM), we estimate the probability distri-bution P ( w | R ), where w is an arbitrary word and R is the unknown underlying relevance model, which is usually extracted by RM based on the top-ranked documents of the initial retrieval.

Although the candidate expansion words extracted from knowledge are rele-vant to the query in Freebase, we do not know whether such knowledge words are suitable to the local corpus. In order to measure the relevance of the knowl-edge words in the local corpus, we measu re two features of knowledge candidate words. First, we will count the co-occurrence of the candidate expanded knowl-edge term and query term in a Window ( we segment the documents to small passages ). It is formulated as follows: We filter the knowledge terms using the co-occurrence feature, by removing the terms that don X  X  co-occur with the quer y terms. Second, based on the assump-tion, if a knowledge candidate term is relevant to the query, it will occur in the top-ranked feedback documents. We calculate two term distributions: 1) the term frequency in the top-ranked feedback documents, 2) the term frequency in the whole corpus. Using the method mentioned in the work of Parapar and Bar-reiro [29]. We assume that the term with a big divergence is important. Combing with the Certainty from knowledge, we measure the term score as follows: where D is the top-ranked documents, C is the whole corpus and Certainty ( t i ) is the measure of term X  X  Certainty in the Freebase. We use the log operation to smooth the distribution gap. IDF is used to represent the importance of term in the corpus. | C | represents the total amount of documents in the corpus. | j : t i  X  d j | represents the amount of documents that contain the term t i .We think that the Freebase term with a higher IDF and a bigger divergence of term distribution is more relevant to the query. We use the score computed by Eq. 11 to measure the importance of the terms. Table 2 shows the top ranked knowledge terms.
 We will combine our knowledge terms with the Relevance Model (RM3). Specifically, we formulate the new query Q new as follows: where Q ori is the original query, Q exp is the expanded query by RM, and Q kb represents the terms extr acted from Freebase. We do not involve extra parame-ters in Eq. 13, where  X  is the parameter of RM3 (i.e.,  X Q ori +(1  X   X  ) Q exp ). 5.1 Experimental Settings We use three standard TREC collections: AP8889 with topics.101-150; WSJ with topics.101-150; ROBUST04 with topics.301-450. Table 3 shows the detail of the collections. We use mean average precision (MAP) to evaluate the retrieval effec-tiveness. Lemur4.12 is used for indexing and retrieval. Documents and queries are stemmered with the Porter stemmer and the stopwords are removed through the Stopwords list.
 5.2 Baseline Models In our experiments, we select two baselines. One is the query-likelihood language model (QL), and the other is the relevance Model (RM3). RM3 is a strong baseline which is widely used, and we choose the parameters of RM3 to get its optimal result on each collection. RM3 + FB1 and RM3 + FB are the results that we didn X  X  apply D-S. RM3 + FB1 is the result that we just use the first level of the Freebase, RM3 + FB is the result th at we use both level of the Freebase. 5.3 Parameter Settings In our experiment, we adopt the Dirichlet smoothing method and set the Dirichlet prior as the default 1000. RM3 + KB is our model that combines the RM3 terms and Freebase terms using D-S theory. Both RM3 and RM3 + KB methods have parameters N, k,  X  . For number of feedback documents, we tested N from 10, 20, 30, 40, 50. We evaluated the  X  for different values: 0.1, 0.2, 0.3, 0.4, 0.5. And for the feedback terms number k , we tested :10, 20, 30, 40, 50. When N = 10, k = 50 and  X  =0 . 1, RM3 gets the optimal result. In Eq. 13, we combine knowledge candidate terms and RM3 terms, so we denote the new query model in Eq. 13 as RM3+KB. RM3+KB adopts the same parameter (i.e.,  X  )asRM3model. 5.4 Experiment Results As can be seen from the Table 4, RM3 largely outperforms QL, which demon-strates RM3 is an effective query expa nsion method on all collections. For RM3+KB, On AP8889 and WSJ, it has a significant performance improvement over the RM3 model. For ROBUST04, RM3+KB also improves the performance over RM3. RM3+KB also outperforms RM3 + FB1 and RM3 + FB in which the D-S theory is not involoved. We can see that RM3+KB performs the best on all collections.

As we have mentioned, not all of queries have the relevant knowledge in Free-base, e.g., for Topics.101-150, about 90% queries can find relevant instance in Freebase. We then report the results of the topics for which the related instances can be found in Freebase. From the result showed by Table 5, we can see that on AP8889 collection, the performance is improved by RM3 + KB about 5% over RM3, and on WSJ collection, performance is improved about 3% over RM3. For ROBUST04, the performance is also improved about 2% over RM3. These results indicate that the terms extracted from the Freebase are relevant to the query and RM3 with knowledge terms works better than using RM3 terms only. In this paper, we have explored the use of Freebase as a resource for query expansion. In our work, we adopt a two round query match in Freebase to find knowledge instances related to query. We propose to apply the Dempster-Shafer evidence theory to rank relevan t terms for the query. We then combine the extracted terms (from Freebase) wi th the expanded terms (from feedback documents) by RM3. The experiment res ult shows that terms extracted from Freebase have a significant improvement p erformance over RM 3, which is a state-of-the-art query expansion method.
 Acknowledgments. This work is funded in part by the Chinese National Pro-gram on Key Basic Research Project (973 Program, grant no. 2013CB329304 and 2014CB744604), the Natural Science Fo undation of China ( grant no. 61272265), and the European Union Framework 7 Marie-Curie International Research Staff Exchange Programme (grant no. 247590).

