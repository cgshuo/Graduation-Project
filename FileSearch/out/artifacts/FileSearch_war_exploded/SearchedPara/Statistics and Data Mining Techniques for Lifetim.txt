 Customer lifetime value (LTV)-which is a measure of the profit generating potential, or value, of a customer-is increasingly being considered a touchstone for customer relationship management. The central challenge in predicting LTV is the production of estimated customer tenures with a given service supplier, based on information contained in company databases. Classical survival analysis techniques like proportional hazards regression focus on covariate effects frequently presumed to be linear, and examination of age-wise effects can be difficult. Further, segments of customers, whose lifetimes and covariate effects can vary widely, are not necessarily easy to detect. A new neural network model for hazard prediction is used to free proportional hazards-like models from their linearity and proportionality constraints, and clustering tools are applied to identify segments of customer hazard patterns. Using the proportional hazards and neural network models in tandem, we demonstrate how data mining tools can be apt complements of the classical statistical models, and show that their combined usage overcomes many of the shortcomings of each separate tool set-resulting in a LTV tenure prediction model that is both accurate and understandable. Survival Analysis, Neural Networks, Lifetime Value, Tenure Prediction, Proportional Hazards Regression In the global competitive marketplace, businesses regardless of size are beginning to realize that one of the keys to profitable growth is establishing and nurturing a one-on-one relationship with the customer. Businesses now realize that retaining and growing existing customers is much more cost effective than focusing primarily on adding new customers. To this end, techniques for customer relationship management (CRM) are being designed, developed and implemented. These techniques should help businesses understand customer needs and spending patterns, and help develop targeted promotions which are not only better tailored for each customer, but are also more profitable to the businesses in the long run. Customer lifetime value (LTV)-which measures the profit generating potential, or value, of a customer-is increasingly being considered a touchstone for administering the CRM process in order to provide attractive benefits to, and retain, high-value customers, while maximizing profits from a business standpoint. Robust and accurate techniques for modeling LTV are essential in order to facilitate CRM via LTV. As we note in the following sections, management via LTV is not well served by the mere computation of an LTV score. A customer LTV model needs to be explicated and understood to a degree before it can be adopted to facilitate CRM. LTV is usually considered to be composed of two independent components-tenure and value. Though modeling the value (or equivalently, profit) component of LTV-which takes into account revenue, fixed and variable costs-is a challenge in itself, our experience has been that finance departments, to a large degree, dictate this aspect. In this paper, we therefore focus exclusively on modeling tenure. There are a variety of standard statistical techniques arising from survival analysis (e.g., [6]) which can be applied to tenure modeling. We look at tenure prediction using classical survival analysis and compare it with  X  X ybrid X  data mining techniques that use neural networks in conjunction with statistical techniques. We demonstrate how data mining tools can be apt complements of the classical statistical models, and show that their combined usage overcomes many of the shortcomings of each separate tool set-resulting in LTV models that are both accurate and understandable. Section 2 elaborates on the definition of LTV and discusses possible applications of LTV. Section 3 summarizes the key challenges in tenure modeling for LTV. The data used in exploring and comparing various techniques for tenure modeling is described in Section 4. Classical survival analysis for LTV tenure modeling is discussed in Section 5, while the neural network model is the subject of Section 6. Comparison of experimental results in Sections 5 and 6 show that neural network models are significantly more accurate. In Section 7, we discuss techniques to explicate and interpret the neural network. We adapt statistical methods in both developing and understanding neural network models. The final section summarizes the paper. LTV is a composite of tenure and value. The central challenge of the prediction of LTV is the production of estimated, differentiated (disaggregated) tenures for every customer with a given service supplier, based on the usage, revenue, and sales profiles contained in company databases. Tenure prediction models we develop generate, for a given customer i, a hazard curve or hazard function that indicates the probability hi(t) of cancellation at a given time t in the future. Figure 5.1 shows an example of a hazard function. A hazard curve can be converted to a survival curve or survival function, which plots the probability Si(t) of  X  X urvival X  (non-cancellation) at any time t, given that customer i was  X  X live X  (active) at time (t-l), i.e., Si(t) = Si(t -l)X[l-hi(t)] with Si (l)=l. Section 5 formally defines hazard and survival functions. Armed with a survival curve for a customer, LTV for that specific customer i is computed as: LTV = ~~=I Si (t)xvi(t) where vi(t) is the expected value of customer i at time t, and T is the maximum time period under consideration. This approach to LTV computation provides customer specific estimates (as opposed to average estimates) of total expected future (as opposed to past) profit based on customer behavior and usage patterns. In the realm of CRM, modeling customer LTV has a wide range of applications including: . Special services (e.g., premium call centers and elite . Targeting and managing unprofitable customers. . Segmenting customers, marketing, pricing and promotional . Sizing and planning for future market opportunity based on Some of these applications would use a single LTV score computed for every customer. Other applications require a separation of the tenure and value component for effective implementation, while even others would use either the tenure or value and ignore the other component. In almost all cases, business analysts who use LTV are most comfortable when the predicted LTV score and/or hazard can be explained in intuitive terms. For successful use of LTV in business applications, we posit that mere computation of an accurate LTV score or hazard is insufficient. The underlying predictive model should be transparent and easily understandable-either by design, or explicated explicitly. Transparent models (e.g., proportional hazards regression) may not be very accurate, while accurate models (e.g., neural networks) may be inscrutable. In this work we demonstrate the use of hybrid statistical and data mining techniques to build accurate and understandable models for LTV prediction. When defining LTV as a measure of customer value, there are two possible interpretations: (i) LTV to a given service provider (e.g., cellular telephone service with GTE Wireless); and (ii) potential LTV of a customer for that service, irrespective of provider (e.g., customer X  X  need or willingness to use cellular telephones). The latter interpretation is more difficult to define and compute, and less useful from a CRM perspective. We therefore exclusively use the  X  X TV to a given service provider X  interpretation. When modeling LTV using tenure and value, we use current revenue, behavior, usage and billing data for a given customer. The resulting model is assumed to be valid for a reasonable time span into the future. This may not always be true for all customers, either due to customer life changes, or due to targeted marketing efforts by the service provider. Such change is extremely difficult to model directly. We assume that the LTV models will be periodically recalibrated to capture such change. Given that LTV is defined in terms of a survival function, a distinguishing and challenging feature of tenure prediction is computing the disaggregated or differentiated hazard function for every customer. Classical statistical techniques (Section 5) like proportional hazards regression provide estimates of hazard functions that are dependent on often questionable assumptions and can yield implausible tenure estimates. If each customer can be observed from subscription to cancellation, predictive modeling techniques can be directly applied. In reality, where a large majority of customers are still currently subscribers, the data are right censored. While classical survival analysis can handle right censoring, data mining techniques like neural nets are ill adapted to directly dealing with censored data. The situation is further complicated by the fact that company databases often do not retain information on customers who have cancelled in the past. Thus, observed (and censored) lifetimes are biased by the exclusion of (relatively short-lived) customers canceling before the database X  X  observation start period. This is leji truncation, recognized by [3] and [9], which needs to be systematically addressed in order to build reliable and unbiased tenure models. Evaluating tenure models by comparing against actual cancellations is another challenge due to the prevalence of right censoring. Furthermore, the small fraction of cancellations that are observed could be a biased sample. Lastly, real world customer databases have large amounts of data (Section 4), and relatively frequent recomputation of LTV is needed to accommodate for changes in customer behavior and market dynamics. We therefore need a reasonably automated technique that can handle large amounts of data. In this paper, we apply statistics and data mining techniques to model LTV for GTE Wireless X  cellular telephone customers. GTE Wireless has a customer data warehouse containing billing, usage and demographic information. The warehouse is updated at monthly intervals with summary information by adding a new record for every active customer, and noting customers who have cancelled service. GTE Wireless offers cellular services in a large number of areasaivided into markets-scattered throughout the United States. Because of differences arising from variations in the geography, composition and market dynamics of the customer base, we build individual LTV tenure models for each market. For the purposes of LTV tenure modeling reported in this paper, we obtain a data extract from the warehouse where each customer record has about 30-40 fields including: . Ident@ution: cellular phone number, account number; . Billing: previous balance; charges for access, minutes used, . Usage: total number of calls, minutes of use for local, toll, . Subscription: number of months in service, rate plan, . Churn: a flag indicating if the customer has cancelled . Other: age, current and historical profitability, optional We use data from a single, relatively small, market containing approximately 21,500 subscribers. The data represents customer behavior summary for the month of April 1998. The chum flag in this data would indicate those customers who cancelled service during the month of April 1998. Approximately 2.5% of customers churned in that period, resulting in 97.5% of censored observations. This dataset is used in both the statistical and data mining approaches, and facilitates comparison of the various techniques. There are three classic statistical approaches for the analysis of survival data, largely distinguished by the assumptions they make about the parameters of the distribution(s) generating the observed survival times. All deal with censored observations by estimating hazard functions {hi(t)} where or the survival function {Si (t)} where Parametric survival models (see, e.g., [ 131) estimate the effects of covariates (subject variables whose values influence lifetimes, i.e., independent variables) by presuming a lifetime distribution of a known form, such as an exponential or Weibull. While popular for some applications (especially accelerated failure models), the smoothness of these postulated distributions makes them inappropriate for our data with its contract expiration date and consequent built-in hazard  X  X pikes X  (for example, see Figure 5.1.) In contrast, Kaplan-Meier methods [ 1 l] are non-parametric, providing hazard and survival functions with no assumption of a parametric lifetime distribution function. Suppose deaths occur at number of subjects at risk at time tk . (In our data situation, where we observe all subscribers active within a one-month time period, rk is the number of subjects of age k at the start of the month, and dk is the number of these age k customers dying during that month. Different data sampling methods-including all customers active within a larger range of time, for instance-require more complicated accounting for those at risk.) Then the (aggregate) hazard estimate for time tk is and the survival function is estimated by Note that this estimator cannot easily estimate the effects of covatiates on the hazard and survival functions. Subsets of customers can generate separate Kaplan-Meier estimates, but sample size considerations generally require substantial aggregation in the data, so that many customers are assigned the same hazard and survival functions, regardless of their variation on many potential covariates. This latter problem has a classic solution in proportional hazards (PH) regression [5]. This model is semi-parametric in the sense that all subjects have a common, arbitrary baseline hazard function h(t) which is related to an individual X  X  hazard function by a multiple which is a parametric function of covariates {.&amp;-Iii = 42 ,...) n;c = 42 ,..., c: covariates, and @,} p re resent parameters estimated during the PH regression. In our situation covariates include data such as various charges per month, as well as dummy variables to indicate presence of a discrete attribute: This model has two conceptual and one operational sets shortcomings. First, the form of the multiplier exp usually chosen for convenience, except in the rare instance where substantive knowledge is available. The form is, of course, reasonable as a first step, to uncover influential covariates, but its essentially linear form tends to assign extreme values to subjects with extreme covariate values. There is no mechanism to stop any component of the estimated hazard function from exceeding 1.0. Second, the presumption of proportional hazards is restrictive in that there may not be a single baseline hazard for each subject, and the form of that baseline X  X  variation may not be well modeled by the time-dependent covariates or stratification that are the traditional statistical extensions of the original PH model. An operational difficulty of PH regression lies in its de-emphasis of explicit calculation of baseline hazards. This problem is particularly acute in our situation of observing subjects over one month only of their lifetimes instead of following a cohort from birth until death/censoring. The standard packages (e.g., [ 11) do not allow the direct estimation of a baseline hazard function when time-dependent covariates are used, and our data situation described above (in which subjects are taken to be available only during the one month observation period) is construed as introducing a time-dependence. Some of these issues with PH regression are addressed by [ 121, where a parametric solution attempts to overcome some of the limitations. Fortunately, in our case, deaths are only recorded as occurring during a particular month, so there are typically many deaths that effectively occur simultaneously. In [ 171, it is shown that the PH coefficients can be estimated via a form of logistic regression. The complementary-log-log (CLL) model yields theoretically the same coefficients @,} as the PH model, and furthermore gives baseline age effects (a,} which can be translated into baseline hazard components via Direct estimation of the baseline hazard function values is very useful, both in itself, and facilitates the hazard function estimations for each individual subject. Once the hazard function hi(t) is estimated for each subject i, the individual survival function is estimated as and the median lifetime is estimated as the interpolated value of t for which $(t) = 0.5 . It is important to note that the later estimate is very sensitive to the choice of the baseline hazard function. The CLL model was fit to the data described above (Section 4). The covariates were chosen by a combination of classical variable selection techniques like backward elimination and forward selection, subject matter expert opinion and intuitive examination of coefficients. The resulting baseline hazard function is shown in Figure 5.1, By many statistical standards, the CLL model producing this hazard function fits the data well. The covariates, including the baseline hazard coefftcients, are highly significant (X*=10059.75 with 71 df, pcO.OOOl), and Somer X  X  D=O.502. However, the graph in Figure 6.lb shows the  X  X elation X  of tenure predicted by this model, and actual tenures observed from those who died during the observation period. For these data, the predicted lifetimes are quite poor. These classical statistical techniques are subject to three major problems in estimating lifetimes. First, the functional forms for the effect of the covariates must be assumed, and are typically chosen to be linear or some mild extension thereof. More sophisticated choices tend to be cumbersome and major exploration of better-fitting forms is generally manual, ad-hoc, and destructive of the significance tests which motivate the statistical approaches. Second, many of these forms work poorly with outlying values of the covariates, so it is possible that some customers with extreme covariate values may be assigned an unlikely or impossible hazard function, e.g. one in which some of its components exceed 1.0. Both of these situations are possible in the usual proportional hazards model, where for covariates {Q),c = 1,2,...,C the traditional functional form for the multiple of the baseline hazard is exp(ipcxci) for the ith customer. Third, the baseline hazard function is not easily made to vary across subsets of the customer population. This is a particularly serious defect when the object is to estimate individual customer lifetimes, rather than covariate effects as is traditional in PH analysis. Incorrect specification of a customer X  X  hazard function can seriously misestimate tenure, frequently through the misestimate of any isolated  X  X pikes. X  When applied to survival analysis, multilayer feed-forward neural networks (NN) [8]-being non-linear, universal function approximators [ lO]-can overcome the proportionality and linearity constraints imposed by classical survival analysis techniques (Section 5). with the potential for more accurate tenure models. But the large fraction of censored observations in real world data (Section 4) for LTV modeling precludes using the neural network to directly predict tenure. The actual tenure to cancellation is unknown for censored customers since they are currently active-all we know is their tenure to date, and using this instead of tenure to cancellation would be inaccurate and unsatisfactory. Ignoring censored customers, on the other hand, not only results in discarding a lot of data, but also results in a small and biased training dataset. Before describing our neural network methodology, we discuss related work to establish the context for our approach. Several researchers have explored the possibility of using neural networks (NN) for survival analysis in the context of medical prognosis. Some of this work, for example [4], formulates survival analysis as a classification (yes-no) problem by building neural networks to model decisions of the form  X  X ill the event of interest happen at time t? X  From our perspective, such a formulation is unsatisfactory, since LTV is defined in terms of a survival (or equivalently, hazard) function (Section 2). Furthermore, as brought out in Section 7, investigation of baseline hazard functions is crucial to explicating the neural network model to a business analyst. We therefore only review work where some form of a survival curve is predicted. Ravdin and Clark [18] use a multilayer feed-forward neural network with a single output unit to predict survival probabilities for breast cancer patients. Their formulation encodes time as one of the input (independent) variables and results in replicating input records for every time interval under consideration. If the survival probabilities were computed over the time period [1,7J, an uncensored input would be replicated T times with the time variable ranging from 1 to T; a censored input is replicated t times, where t is the time of the last observation. This encoding of input data results in biases that need to be corrected by selective sampling. When predicting the survival probability for a new observation, the independent variables are fed into the neural network, and the time variable set to successive values in [l,Z X  X . The T outputs from the network (one for each setting of the time variable) provides estimates of the respective SUNiVal probabilities X . De Laurentiis and Ravdin [7] use similar techniques on a synthetic dataset and explore ways to gain insights into how the generated neural networks interpret data. Hierarchical and modular neural networks for survival analysis are described in [ 151 and [ 161 respectively. One neural network is used to model survival for each time period of interest. Thus, T neural networks would be needed in order to predict survival in the period [l,TJ. Ohno-Machado and Musen [16] also describe how these independent neural networks can be combined in a systematic way to enforce the constraint that the survival function be monotonically decreasing. Street [21] uses a neural network architecture that predicts survival curves using a vector of output units, and makes appropriate use of censored observations without introducing any biases in the input data. The training vector for the output units is derived using Kaplan-Meier survival curves. For LTV tenure modeling, the time interval is dictated by the data warehouse update frequency, which, for the case of GTE Wireless, is 1 month. A reasonable maximum time period T of 36-60 months is usually necessary. Given that some of GTE Wireless X  larger markets have over half a million customers (with 30-40 attributes per customer), Ravdin and Clark X  X  [18] record replication approach will result in unnecessarily large datasets with severe performance and resource penalties. Ohno-Machado and colleagues X  [ 15,161 approach will force us to train 36-60 neural networks on large amounts of data, for every market that we need to model. These approaches do not scale very well to  X  Note that the survival probability computed here is equivalent to l-h(t) in Section 2; this is distinct from S(t), which is the cumulative survival probability. large datasets over a large number of time intervals, where the models need to be refreshed frequently with reasonable turn around times in a corporate setting. Street X  X  [21] approach, on the other hand, while scalable, suffers from the drawback that the predicted (cumulative) survival function can be non-monotonic. Our approach to harnessing multilayer feedforward neural networks for survival analysis (for LTV tenure prediction) involves predicting the hazard function for every customer. We begin with data from the data warehouse and run it through a preprocessing step where the data is readied for neural network training. The second step involves setting up and training one or more neural networks for hazard prediction. The final data post-processing is used to evaluate the performance of the neural network and compare it with classical statistical approaches. We describe this process in detail in the following sections. Customer data for LTV modeling should have, in addition to a variety of independent input attributes (Section 4), two important attributes: (i) tenure, and (ii) a censoring flag. In our data, the TENMON attribute has the customer tenure in months, and a CHURN flag indicates if the customer is still active or has cancelled. If CHURN=O, the customer is still active and TENMON indicates the number of months the customer has had service; if CHURN=l, the customer has cancelled and TENMON is his age in months at the time of cancellation. In order to model customer hazard for the period [1,7J, for every record or with the following values (for 1 5 t I T): Here, d, is the number of cancellations in time interval t; n, is the number of customers at risk, i.e., total number of customers with TENMON = t. The ratio d, /PZ, is the Kaplan-Meier [ 1 l] hazard estimate for time interval t. Intuitively, we set hazard h,(t) to 0 when a customer is active, 1 when a customer has cancelled, and to the Kaplan-Meier hazard if censored. Table 1 shows an example. This approach is similar to 1211, except that we hazard function instead of the survival function. Hazard functions do not have any monotonicity constraints, and support customer segmentation (Section 7.1) which has important ramifications from a marketing perspective. training vector for the respective neural network input case i. The remainder of the attributes, except TENMON and CHURN, serve as inputs to the neural network2. Most modem neural network packages or data mining toolsets with neural networks provide the following automated processing: . Ignore identification attributes like cellular phone number . Standardize (or normalize) continuous attributes; . Prune the number of classes for categorical attributes and These operations must be manually executed if the neural network software does not provide automated support. Finally, the dataset is split into train, test and holdout (or validation) datasets. The train and test datasets are used to train the neural network and avoid overfitting. The holdout data is used to evaluate performance of the neural network and compare it with classical statistical techniques. We use a standard feedforward neural network with one or two hidden layers and experiment with the number of hidden units to obtain the best network (see Section 6.3). The number of input units is dictated by the number of independent input attributes. The network is set up with T output units, where each output unit o, learns to estimate hazard rate h(t). The parameters of the neural network are set up to learn probability distributions [2,8]: 2 We have experimented with including TENMON. The resulting neural networks perform similarly with or without TENMON. 
Including both CHURN and THNMON is tantamount to indirectly providing the output targets as inputs, and the neural network trivially detects this correlation. Furthermore, when predicting hazards for existing customers, CHURN will always be 0. Hence CHURN should always be excluded. . The standard linear input combination function is used for . The logistic activation function q(v) = -m The relative entropy or cross entropy error function is used. Once the neural network has been trained, we use score the holdout dataset. For every observation i in the holdout dataset, the neural network outputs a predicted hazard function (hi(t)), 1 5 t I T. In the post-processing step, we convert this hazard function into a survival function si(t) = Si(t-l)x[l-hi(t)] for 1 It I Tat~d Si(l)=l. Wecompute predicted median tenure for the neural network (NNPRED) as that value of t for which Si(t) = 0.5. We score the same holdout dataset with a complementary log-log model (Section 5) built using the same training set as the neural network. We also compute predicated median tenure for the complementary log-log predictions (CLOGPRED) in an identical manner. The data described in Section 4 was used in our experiments with the neural network and classical statistical techniques. For the neural network, 40% of the data (8,600 records) was used for training, and 30% (6,450 records) each for the test and holdout datasets. We use a time period of T = 60 months3. We built several neural networks: single hidden layer networks with 2550 and 100 hidden units, and a two hidden layer network with 25 units in the first hidden layer and 10 units in the second hidden layer. After training, all four networks had very similar mean square error rates on the holdout data. We report results from the two hidden layer neural network. Figure 6.1 shows predicted median tenure (based on the survival curve) for the neural network (NNPRED) and complementary log-log model (CLOGPRED) plotted against actual tenure (TENMON) for customers in the holdout dataset who have already cancelled. Of the 6,450 customers included in the holdout dataset, 161 have already cancelled service (i.e., churned). Figure 6.1 plots predicted vs. actual tenure for these 161 customers. It is clear from the graphs that the neural network is much better at predicting tenure than the complementary log-log model. The complementary log-log tenure predictions are clustered around 20 months, and rarely exceed 30 months, resulting in grossly underestimating LTV for long-lived customers. The neural network predictions, on the other hand, have a more reasonable distribution, even for long-lived customers. Section 7 further compares neural network and complementary log-log tenure predictions. Given that neural networks do not enforce proportionality and linearity constraints, one would expect these models to be better than proportional hazards-like models. In comparisons reported in the literature [14,19], neural networks have performed comparably to proportional hazards models for medical prognosis. Tenure modeling for LTV is one of those challenging applications where the power of the neural network manifests itself in significantly better models in comparison with proportional hazards-like methods. The neural network generally performs better than the CLL model in terms of predicting tenure. But in order to better understand the characteristics of these models, we compute residuals-i.e., differences between actual tenure and the NNKLL prediction for those subjects dying during the observation period-from predictions on the holdout set. We observe how close these residuals-NNRES and CURES-are to zero, respectively, and how that distance varies for different ages of subjects. (This is important since examining only subjects who died during a one-month period necessarily produces an overabundance of short-lived observations.) Figure 7.1 shows the residuals from the two types of models. Both residuals show a systematic bias with respect to age. The CLL residuals have an average value near 0.0, 3 We have also experimented with T=36 months (3 years). The neural network performs somewhat better with the shorter time period, since this is an inherently easier problem. but the extreme ages are either overestimated (for low ages) or underestimated (for high ages). The NN residuals show much less variation than do the CLL residuals, and are substantially closer to zero for the high age groups. 
Figure 73: Hazard functions derived from clusters based on the neural network, representing the 9P percentile along the 
Figure 7.3: Mean square error for CLL (with and without 
Where does this advantage come from? The NN model is freed of the restricted form in which the covariates influence the individual hazard functions. More important, though, is the fact that this NN model generates idiosyncratic hazard functions for each subject instead of presuming a single baseline hazard functions as the 
PWCLL model must. It happens that the structure of these functions is easily described, and leads to a quantification of the relative benefit of lifting the two preceding restrictions. 
The neural network produces a T-month hazard function, i.e., a 7 X -component vector hi(l), . . . . hi(T), for each customer i. For our experiments, T=60. In order to cluster these individual hazard functions, we parameterize the shape of the hazard curve by defining a set of functions that collectively capture the geometric aspects of the curves, and were judged as likely to yield behavioral information: l Average hazard rate = Mean [hi(l), . . ., hi(T)].  X  Average hazard rate for the pre-contract expiration period . Overall slope of the hazard curve =  X ( X )i  X (l). . Initial slope of the hazard curve = h,(9)-4(U . Terminal slope of the hazard curve =  X (T)-:(*-9). . Relative size of the contract expiration (12-month)  X  X pike X  The derived attributes defined by these functions were computed for each customer, and then standardized to have a mean of 0 and a standard deviation of 1. Augmented by these additional attributes, the hazard functions were then segmented into ten clusters using k-means clustering. We chose ten clusters since we judged that to be more than necessary, and in fact, the ten output clusters included four small clusters, with less than 50 customers (out of the total of 6,450) in each. These clusters were merged with their nearest neighbors. To display each of the resulting six clusters of hazard curves, the vectors h,(l), . . ., hi(T) were subjected to a principal components analysis. This analysis revealed that at least 88% of the total variance was associated with the first principal component. Regularly spaced percentiles (5,25,50,75,95) were computed for the principal component scores for each of the six clusters, and the curves corresponding to each were plotted together. Finally, visual inspection of the six clusters revealed two clusters, both relatively small, which were similar in shape to larger clusters. These two clusters were appropriately merged with their respective nearest neighbors. Principal component analysis was repeated on the remaining four (modified) clusters, and the hazard curves corresponding to the 95th percentile component scores are displayed in Figure 7.2. We observe that within each of these four clusters, the constituent hazard functions are all very nearly multiples of each other. This conclusion derives from the results of the principal components analysis, where the first component-the average hazard rate-accounts for nearly all (88-99%) of the functions X  variation. Thus, the NN hazard functions are effectively four groups of proportional hazard models. The efficacy of this partitioning into four hazard function can be quantified by generating tenure predictions from these data based on three different models: (i) the original CLL model; (ii) a modified CLL model with separate baseline hazard functions for the four clusters (but still with the linear form of the covariate inputs); and (iii) the NN model. For each model, calculate the arithmetic mean of the squared differences between actual lifetimes and predicted lifetimes under the three models, for those who have died. This is the mean squared error (MSE) for each model. The three sums of squares are displayed in Figure 7.3. It is apparent that the generation of the four different baseline hazards is responsible for the majority of reduction in MSE. A final point of comparison of the CLL and NN models is their treatment of extreme subjects, i.e. those subjects who have large values for some or all of their covariates so that the hazard function multiplier of the PH model yield implausibly high hazard components and correspondingly short estimated lifetimes. Figure 7.4a shows the PH-estimated hazard functions (displayed as the 25th, 5Oth, 75th percentiles of hazard functions as arrayed along their first principal component) for a subset of subjects from this geographic region with lifetimes judged to be implausibly low. Note that the hazard functions necessarily have the same shape as the overall baseline hazard function, with uniformly high death probabilities. In particular, the high  X  X pike X  at 12 months is the cause of low estimated lifetimes. In contrast, Figure 7.4b shows hazard functions estimated by the NN model, and exhibiting much lower hazard rates and also a more plausible hazard shape. Histograms of the predicted lifetimes for the two models are displayed in Figure 7.5. It is apparent that the NN model generally results in lifetimes that are less extreme, presumably as a result of its use of a less rigidly defined hazard function shape. Clustering neural network generated hazard functions has resulted in segmenting customers into four groups. What are the characteristics of customers in each group? And what ramifications does this clustering have on targeted marketing and retention programs from a business standpoint? We briefly address these issues. 
In order to characterize customers in each segment, we build a decision tree with the explanatory covariates as independent attributes and the cluster number as target. Table 2 summarizes splitting rules to the most discriminatory leaf in the tree for each cluster. The table also indicates potential implications for marketing and retention efforts. This paper has described classical statistical approaches to the estimation of customer lifetimes for a common business data type, and a neural network alternative. The classical approaches provide baseline solutions for the estimation problem, provided that the issues of right censoring and left truncation are addressed. Neural network models, while inscrutable, are very accurate predictors. The neural network provides individual estimates of the hazard functions discovered to be vital in tenure estimation, and their analysis reveals a segmented proportional hazards composition that is attractive both to the statistician searching for structure in data and to the marketer searching for business meaning. In comparing and understanding these techniques, our analysis leads to tenure models based on hybrid statistical and data mining approaches that are richer in meaning and more predictive than either approach by itself. At the outset, the neural network model appears to be more accurate than the classical statistical approaches. Though this is true, we believe that the value of this work lies in the amalgamation of statistical and data mining techniques with the goal of data modeling and understanding from a business perspective. To a large degree, the success of the neural network model derives from the use of hazard functions-a construct introduced by classical statistics-as the target vector. Furthermore, the neural network model is inscrutable when it comes to deriving business insight. The information encapsulated in the neural network model is made actionable by using a combination of statistical and data mining techniques. In modeling tenure for LTV, we believe that we have instituted a hybrid statistical and data mining based analysis process where the weaknesses of one class of techniques are addressed by the strengths of the other. [l] Allison, P. D. (1995), Survival Analysis Using the SAS@ [2] Baum, E. B. and Wilczek, F. (1988),  X  X upervised Learning [3] Bolton, R. (1998),  X  X  Dynamic Model of the Duration of the [4] Burke, H. B. (1994),  X  X rtificial Neural Networks for Cancer [S] Cox, D. R. (1972),  X  X egression Models and Life Tables, X  [6] Cox, D. R. and Oakes, D. (1984), Analysis of Survival Data, [7] De Laurentiis, M. and Ravdin, P. M. (1994),  X  X  Technique [8] Haykin, S. (1994), Neural Networks: A Comprehensive [9] Helson, K. and Schmittlein, D. C. (1993),  X  X nalyzing [ lo] Homick, K., Stinchcombe, M. and White, H. (1989), 111 Kaplan, E. L. and Meier, R. (1958),  X  X onparametric [ 121 Kooperberg, C., Stone, C. and Truong, Y. (1995)  X  X azard 131 Lawless, J. E. (1982), Statistical Models and Methods for 141 Ohno-Machado, L. (1997),  X  X  Comparison of Cox 1 151 Ohno-Machado, L., Walker, M. G. and Musen, M. A. [16] Ohno-Machano, L. and Musen, M. A. (1997),  X  X odular [ 171 Prentice, R. L. and Gloeckler, L. A. (1978).  X  X egression [ 181 Ravdin, P. M. and Clark, G. M. (1992)  X  X  Practical [19] Ravdin, P. M., Clark, G. M., Hilsenbeck, S. G., Owens, M. [20] SAS Institute (1998),  X  X eural Network Node: Reference, X  [21] Street, W. N. (1998)  X  X  Neural Network Model for 
