 Event detection in social media is an important but challeng-ing problem. Most existing approaches are based on burst detection, topic modeling, or clustering techniques, which cannot naturally model the implicit heterogeneous network structure in social media. As a result, only limited informa-tion, such as terms and geographic locations, can be used. This paper presents Non-Parametric Heterogeneous Graph Scan (NPHGS), a new approach that considers the entire heterogeneous network for event detection: we first model the network as a  X  X ensor X  network, in which each node senses its  X  X eighborhood environment X  and reports an empirical p-value measuring its current level of anomalousness for each time interval (e.g., hour or day). Then, we efficiently max-imize a nonparametric scan statistic over connected sub-graphs to identify the most anomalous network clusters. Fi-nally, the event represented by each cluster is summarized with information such as type of event, geographical loca-tions, time, and participants. As a case study, we consider two applications using Twitter data, civil unrest event detec-tion and rare disease outbreak detection, and present empir-ical evaluations illustrating the effectiveness and efficiency of our proposed approach.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Non-Parametric Scan Statistics; Event Detection and Fore-casting; Social Media; Heterogeneous Graphs
Social microblogs such as Twitter and Weibo are experi-encing explosive growth, with billions of users globally shar-ing their daily observations and thoughts online. Unlike tra-ditional channels, where collection of information such as pa-tient data, crimes, and financial transactions is costly and time consuming, social media provides the vast amount of data available in real time on the internet at almost no cost. Social media also helps spread information earlier and faster than traditional media. For example, Twitter first leaked credible word of Osama bin Laden X  X  death before President Obama X  X  announcement, and there were a half million tweets (and only 800 news mentions) one hour after the event [1]. As a social  X  X ensor X  which can identify emerging patterns in sentiments and opinions, the use of social media holds great promise for detection and forecasting of significant societal events.

However, the size and complexity of social media datasets create a number of technical challenges. First, the language used in social media is highly informal, ungrammatical, and dynamic, and thus traditional natural language processing (NLP) techniques cannot be directly applied. Second, so-cial media is naturally structured as a heterogeneous graph, with entities such as user, post, geographic location, term, hashtag, and link; and relationships such as follower, friend-ship, reply, retweet, and spatial neighborhood. In addition, the attributes of each entity type could be heterogeneous as well. For example, a user may have daily attributes such as the numbers of active followers, posts, and retweets, while a tweet may have attributes such as the number of terms and the sentiment score. Finally, the size of data necessitates development of new, scalable detection methods.

This paper focuses on the problem of domain-specific event detection and forecasting, for events such as disease out-breaks, civil unrests, and financial crises. Most existing ap-proaches to event detection can be classified into three cate-gories, including burst detection, geographic topic modeling, and clustering. Burst detection -based approaches search for space-time regions where the aggregated counts of some predefined terms are abnormally high compared with the counts outside the regions [9, 10, 13]. Sakaki et al. con-sider spatial-temporal Kalman filtering, which is similar to space-time burst detection, to track the geographical tra-jectory of hot spots of tweets related to earthquakes [19]. Geographic topic model -based approaches estimate lan-guage distributions (over a predefined vocabulary) that are distinct in some geographic regions [25, 11, 8]. Cluster-ing -based approaches search for novel clusters of documents or terms using predefined similarity metrics, such as cosine similarity and social similarity for documents [22], or auto-correlations [24] and co-occurrences [20, 23] for terms. Simi-larly, [21] uses features related to text content and link infor-mation to cluster tweets. For each cluster identified, the re-lated documents may have different geographical locations, which can be combined by weighted voting [22].

We note that these domain-specific event detection ap-proaches differ in focus from general-domain event detection methods such as RW-Event [3], which attempt to distinguish events from non-event patterns (such as memes) rather than identifying events of a specific type. Such methods do not use content of tweets but only features such as temporal trends of term volume, rely on a large amount of labeled training data (as opposed to the unsupervised problem we consider here), and require extensive parameter tuning.
Each of the aforementioned methods only exploits partial information from social media that is useful for event de-tection. However, there is very limited work that is able to model the entire social media graph for event detection, due to the computational challenge of modeling the complicated heterogeneous relationships between entities and attributes, and the risk of overfitting [6]. Our approach, described be-low, incorporates three types of heterogeneity in the social media graph, including heterogeneous 1) entity types; 2) entity attributes; and 3) entity relationships. In addition, the highly informal, ungrammatical, and dynamic language used in social media motivates our use of nonparametric statistical models to provide more accurate detection and forecasting [18].
 To address the above technical challenges, we propose a Non-Parametric Heterogeneous Graph Scan (NPHGS) ap-proach for event detection and forecasting using social media data. We attempt to consider all potentially useful informa-tion in social media in a unified nonparametric statistical framework, to facilitate the early detection and accurate forecasting of societal events. Specifically, we first model a heterogeneous graph, in which: 1) each node can be of dif-ferent types, such as user, tweet, geographic location, and hashtag; 2) the relationships between nodes can be of dif-ferent types, such as retweet, reply, and follower; and 3) each node type can have different attributes, such as the numbers of tweets and users for a given geographic location; the numbers of followers, tweets, and retweets for a given user; and the number of terms and the sentiment score for a given tweet. Second, we further model the network as a  X  X en-sor X  network, in which each node senses its  X  X eighborhood environment X  and reports an empirical p-value measuring the current anomalousness levels of various neighborhood-related attributes. Third, we efficiently maximize a non-parametric scan statistic over connected subgraphs to iden-tify the most anomalous network clusters. Each cluster is returned as the indicator of an ongoing or upcoming event, and is summarized with information such as type of event, geographic locations, time, and participants. The main con-tributions of our study are summarized as follows:
The rest of this paper is organized as follows. Section 2 discusses heterogeneous graph modeling for social me-dia, and considers Twitter data as a case study. Section 3 proposes nonparametric scan statistics for heterogeneous graphs. Experiments on real Twitter datasets are presented in Section 4, and Section 5 describes future work.
A heterogeneous graph is composed of nodes, attributes, and relations that could be of multiple different types. The formal definition of a heterogeneous graph is as follows:
Definition 1 (Heterogeneous Graph). A heteroge-neous graph is defined as a directed graph G = ( V , E ,f, X  ) , where V = {V 1  X   X  X  X   X  V C } , V c refers to the set of enti-ties of type c , C refers to the total number of entity types, E  X  V  X V refers to the set of edges, f = { f 1 ...f C set of C mapping functions, f c : V  X  R D c defines a D dimensional feature vector ( f c ( v ) ) for each node v of type c ,  X  refers to a mapping function such that each e  X  E be-longs to a particular type of relation  X  ( e )  X  { 1 ...Q } , and Q refers to the number of different relation types.
Throughout the paper, we consider the detection of civil unrest events and rare disease outbreaks using Twitter data as a case study. For this application, we assume that a heterogeneous graph G = ( V , E ,f, X  ) has been extracted from the Twitter data. Additionally, we assume historical data f c ( v ( t ) ) (for t = 1 ...T ), corresponding to each fea-ture vector f c ( v ), which will be used to estimate the anoma-lousness of the current feature values. Given these data, we will return the most anomalous connected sub-graphs max S  X  X  F ( S ), where F is the nonparametric heterogeneous graph scan statistic defined below. Our goal is to identify subsets S corresponding to events of interest, as given by a separate gold standard dataset and as measured by the performance metrics defined in Section 4. The selected set of entity types includes User, Location, Term, Tweet, Link, and Hashtag. The selected entity rela-tion types and entity attributes are summarized in Figure 1 and Table 1. The sentiment (polarity) score was calculated using the python sentiment analysis package named  X  X at-tern X  [27]. The klout score [28] is an overall measure of the tweet author X  X  influence on a scale from 1 to 100. It is assumed that domain-specific content filtering has been conducted as a preprocessing step, and the majority of un-related tweets have been removed. In our study, domain-specific dictionaries for civil unrest events and hantavirus outbreaks were obtained from domain experts, and tweets that match less than three terms in the dictionary were re-moved. More complicated content filtering techniques can be applied, such as the training of a SVM classifier [19]. We note that identical pre-processing steps were applied for all methods in our experiments below, and we do not expect the relative performance of methods to be strongly dependent on this pre-processing.
 Figure 1: Entity Diagram for Twitter Data Model-ing Node Type Features Used
User #tweets, #retweets, #active-followers,
Tweet klout, sentiment, replied-by-graph-State #tweets, #active-users Term #tweets Link #tweets Hashtag #tweets
This section presents non-parametric scan statistics for heterogeneous social media graphs. Specifically, Subsec-tion 3.1 discusses the modeling of the heterogeneous graph as a  X  X ensor X  network by estimating an empirical p-value Figure 2: An Example of Twitter Heterogeneous Network for each graph node; Subsection 3.2 presents nonparamet-ric scan statistics; and Subsection 3.3 presents an efficient approximate algorithm that maximizes the nonparametric scan statistic over connected subgraphs to identify the most anomalous graph clusters, which can be regarded as indica-tors of ongoing or new events.
To deal with the heterogeneity of node attributes for dif-ferent entity types, we propose to model the heterogeneous graph as a  X  X ensor X  network. Each entity senses its local  X  X eighborhood X  in the graph and reports measurements of some predefined features. For event detection, it is neces-sary to estimate a baseline distribution for each attribute that characterizes its behavior when there is no event occur-ring. Given these baseline distributions, it is then possible to estimate an empirical p-value representing the degree of anomalousness for each node. This empirical p-value can be regarded as the signal strength of the node X  X  current attribute values as an indicator of some ongoing or newly emerging event.

In order to estimate the baseline distribution of each at-tribute for each entity, the key component is to collect a good training sample for distribution estimation. We first define an appropriate time granularity for event detection (e.g., hourly, daily, or weekly). We then collect a set of historical observations for each entity and attribute. We consider three scenarios:
For the first category of entities, it is possible to collect enough historical records for that specific entity to estimate its baseline distribution. For example, if a given user, who has used Twitter for more than a month, tweeted 100 times today, that value would be compared to her daily numbers of tweets in historical data. For the second and third cate-gories, there are not sufficient historical observations avail-able for each entity. In this situation, we consider all histor-ical records of the same entity type as the training sample, calibrated based on time since first occurrence. For example, if a given tweet is retweeted 50 times on the third day since its creation, that value would be compared to the numbers of retweets for all tweets on their third day.

Once the set of historical observations { v (1) ,...,v ( T ) defined for a given node v , we can then compute the em-pirical p-values p d ( v ) for each attribute of node v , and an overall empirical p-value p ( v ) for node v , by comparing the current and historical attribute values. Here we assume one-tailed p-values, which answer the question: under the null hypothesis that no events of interest are occurring, what is the probability that a randomly selected sample would have an observed value greater than or equal to the current ob-servation. (The proposed approach can be easily extended to incorporate two-tailed p-values as well.) For empirical p-values, we assume that the set of historical observations represent the null distribution of interest, thus testing the null hypothesis of exchangeability of past and current obser-vations. The empirical p-value of a specific feature d for a node v of type c is defined as: p d ( v ) = 1 where f c,d ( v ) refers to the d -th component of the feature vector f c ( v ). The empirical value p d ( v ) defined above can be interpreted as the proportion of historical observations f c,d ( v ) when there was no event occurring with observed values that are greater than or equal to the current observa-tion f c,d ( v ). The empirical p-value of node v is then defined as: The proposed two-stage empirical p-value p(v) has the nice theoretical property of uniformity as shown in Theorem 1:
Theorem 1 (Uniformity of p(v)). The two-stage em-pirical p-value p(v) defined by Equations (1) and (2) follows a uniform distribution on [0,1] under the assumption that the current multivariate observations for a single node are exchangeable with the reference set given the null hypothesis that no events of interest are occurring.

Proof. The assumption of exchangeability of multivari-ate observations for a single node implies that each feature X  X  observations are exchangeable with the reference set, so that the first-stage p-values are uniform on [0,1]. Moreover, the features of a node are assumed to have the same correla-tion structure as the reference set, so the minimum of the first-stage p-values is exchangeable with the corresponding minima in the reference set; and thus the second-stage p-values are uniform on [0,1].

The challenge of heterogeneity of different node types and node attributes is well addressed from the following three perspectives: First, it deals with network heterogeneity by calibrating all node types on the same scale, such that all p-values p ( v ) are drawn uniformly on [0,1] under the null hypothesis that no events of interest are occurring. Second, it allows us to consider multiple attributes for a single User, Tweet, or State node without knowing a priori which ones will be most indicative of the events of interest. Finally, it accounts for correlation between the first-stage empirical p-values p d ( v ) when computing the overall empirical p-value p ( v ).

To better understand the advantages of our proposed two-stage solution, we briefly compare it to two alternative ap-proaches. First, we could have simply used a one-stage cal-ibration process where the feature-level p-values p d passed directly into the nonparametric scan statistic de-scribed below. The nonparametric scan computes the score F ( S ) of a subgraph S as a function of the number of p-values in S which are significant at level  X  and the total number of p-values in S . However, we expect the p-values for the various features of a given node to be highly corre-lated. As a result, the one-stage calibration process would be biased toward detecting nodes with more features. For example, suppose that nodes of type 1 have 100 redundant (fully correlated) features, while nodes of type 2 have only a single feature. In this case, a node of type 1 with all 100 p-values equal to .05 would have a much higher score (given one-stage calibration) than a node of type 2 with p-value equal to .05. The two-stage calibration correctly accounts for the correlation structure and would give both nodes the same score.

A second alternative approach would be to define p ( v ) as the minimum p-value min d =1 ...D c p d ( v ) without re-calibrating the significance of p ( v ) using the historical data. Clearly this naive approach does not account for multiple hypothesis testing; it can be readily proved that this estimator does not follow a uniform distribution under the null and is biased, tending to underestimate the empirical p-value. For exam-ple, if 100 p-values for a node were independently drawn from [0,1], we would expect the minimum p-value to be less than  X  = . 05 with probability 1  X  (1  X  0 . 05) 100 = 0 . 994. Moreover, the naive approach would again be biased toward giving higher scores to nodes with more features, since the minimum p-value can only be decreased (made more signifi-cant) by adding features. Re-calibrating using the historical data in our two-stage process correctly adjusts for this bias. Finally, we note that our two-stage process is sufficiently flexible so that other p-value combination methods (such as Fisher X  X  method) could easily have been used in Equation 2 instead of the minimum p-value, while still satisfying Theo-rem 1.
As described above, we obtain a  X  X ensor X  network H = ( V , E ,p ) that is the same as the heterogeneous graph G , ex-cept that the mapping function p : V  X  [0 , 1] now defines a single empirical p-value corresponding to each node v of the heterogeneous network. Note that the mapping func-tion  X  has been removed: all relation types will be treated identically in the discussion below.

To determine which connected subgraphs are most anoma-lous, we generalize the nonparametric scan statistic [17], which extends Kulldorff X  X  spatial scan [12] and was orig-inally proposed for modeling spatial-temporal count data, to heterogeneous graphs. The nonparametric scan has also been used for anomalous pattern detection in general cate-gorical datasets [14], but we note that the present work is the first work to generalize nonparametric scan statistic to het-erogeneous graphs, requiring both the novel two-stage cali-bration procedure described above (to obtain p-values) and the novel graph scan algorithm described below (to identify subgraphs with surprisingly high numbers of low, significant p-values). Ignoring the graph constraints as in [14] leads to the identification of unconnected subsets consisting of un-related, individually anomalous nodes from different parts of the heterogeneous network, resulting in substantially re-duced detection performance.

The general form of the proposed Non-Parametric Hetero-geneous Graph Scan (NPHGS) statistic is defined as:
F ( S ) = max where S  X  V refers to a connected set of nodes, N  X  ( S ) = P v  X  S I ( p ( v )  X   X  ) is the number of p-values significant at level  X  , and N ( S ) = P v  X  S 1 is the total number of p-values in subset S . The significance level  X  can be opti-mized between 0 and some constant  X  max &lt; 1. The function  X  (  X ,N  X  ( S ) ,N ( S )) refers to a nonparametric scan statistic, i.e., a function that compares the observed number of p-values N  X  that are significant at level  X  to the expected number of significant p-values E [ N  X  ( S )] =  X N ( S ), under the null hypothesis that p-values are uniformly distributed on [0 , 1]. In this work, we explore the use of one nonparamet-ric scan statistic  X  (  X ,N  X  ( S ) ,N ( S )): the Berk-Jones (BJ) statistic [4]. The BJ statistic is defined as: where KL is the Kullback-Liebler divergence between the observed and expected proportions of p-values less than  X  : The BJ statistic can be interpreted as the log-likelihood ra-tio statistic for testing whether the empirical p-values fol-low a uniform or piecewise constant distribution. Berk and Jones [4] demonstrated that this statistic fulfills several op-timality properties and has greater power than any weighted Kolmogorov statistic.

It is important to consider a range of  X  in NPHGS, rather than a single threshold for significance. For a fixed  X  such as  X  = 0 . 05, the resulting statistic may lose the power to detect a small number of highly anomalous p-values (much smaller than 0.05) or a larger number of subtly anomalous p-values (slightly greater than 0.05). The selection of  X  is in practice slightly greater than typical significance levels predefined by users; here we use  X  max = 0 . 15.

We note that the subsets of p-values identified by our al-gorithm are affected by multiple testing on two dimensions: we maximize F ( S ) over subgraphs S and over thresholds  X   X   X  max . To adjust for multiple testing and correctly mea-sure the significance of the detected clusters, we could ap-ply a permutation test, shuffling the temporal component of the data (assuming the null hypothesis of exchangeability), identifying the maximum subgraph score for each permuted sample, and finally comparing the detected cluster score to the distribution of maximum cluster scores for the permuted samples to obtain the p-value of the detected cluster.
Based on the proposed NPHGS statistic, the detection of the most anomalous connected subgraph from V can be formalized as the following optimization problem:
It can be shown that the time cost of exact solution to the above optimization problem (5) is exponential in the total number of graph nodes |V| in the worst case. Therefore, it is necessary to develop approximate solutions. We first observe that it is possible to solve a relaxed version of this problem efficiently by removing the connectivity constraint. Note that we will use this efficient unconstrained optimization as a building block to solve the optimization problem with connectivity constraints. The relaxed problem is formalized as follows: which is equivalent to the problem: where U ( S, X  max ) refers to the union of {  X  max } and the set of distinct p-values less than  X  max in S . Because the BJ scan statistic satisfies the linear time subset scanning (LTSS) property [16], the subproblem can be solved in O ( |V| ) time, assuming that the entities v  X  V have already been sorted by priority. Specifically, the LTSS property guarantees that the only subsets S with the potential to be optimal are those consisting of the top-n highest priority nodes { v (1) ,  X  X  X  ,v ( n ) } , for some n between 1 and |V| . In this case, a lower (more significant) p-value corresponds to a higher priority. Therefore, the relaxed ver-sion (6) of the original problem (5) can be solved in the time complexity O ( |V| X | U ( V , X  max ) | + |V| log |V| ), where the ad-ditional |V| log |V| is required to sort the entities by priority. Based on the computational efficiency of the relaxed prob-lem, we propose an efficient approximate algorithm by tar-geted seeding, iterative subgraph expansion, and relaxation. The proposed algorithm, described in Algorithm 1, will re-turn a connected subgraph of the heterogeneous graph that approximately maximizes the proposed non-parametric scan statistic.

The time complexity of Algorithm 1 is dominated by the computation of the relaxed problem (6) for the updated sub-graph S  X  G in each iteration of graph expansion. This com-putation must be performed at most KCZ = KC log |V| times, each requiring O ( |V| X | U ( V , X  max ) | ) time assuming that nodes have already been sorted by priority. Here K is the number of seed entities considered for each of the C entity types, and Z is the number of iterative subgraph expansions performed for each seed entity. Computing the empirical p-values is O ( |V| T log T ), and sorting the nodes by priority is O ( |V| log |V| ). Therefore, the total computa-tional complexity equals O ( KC  X | U ( V , X  max ) | X |V| log |V| + |V| T log T ). Furthermore, we note that | U ( V , X  max ) | can be considered a constant, since only at most  X  max T distinct p-values less than  X  max will be generated by the empirical p-value estimation method described above, and thus the algorithm scales as O ( |V| log |V| ). For our civil unrest and Algorithm 1 Non-Parametric Heterogeneous Graph Scan Input: G = ( V , E ,f, X  ) Output: The most anomalous subgraph S ? Obtain  X  X ensor X  network H = ( V , E ,p ) as above;
Set  X  max = 0 . 15, K = 5, Z = log |V| , and S ? =  X  ; for ( k,c )  X  [1 ,  X  X  X  ,K ]  X  [1 ,  X  X  X  ,C ] do end for rare disease detection experiments below, we have  X  max T = ( . 15)(215)  X  32.

In addition to its computational efficiency, our proposed algorithm also has two nice theoretical properties, as follows:
Theorem 2 (Optimality). Consider the simplified case in which we fix  X  instead of allowing  X  to vary between 0 and  X  max . Let S ? = arg max S F  X  ( S ) denote the optimal con-nected subgraph. Assume that K is set sufficiently large to select some v  X  S ? as a seed entity, and Z is set greater than the diameter of S ? . If S ? satisfies the property that there is no  X  X reak-tire X  entity v  X  S ? (i.e., a node v with p-value greater than  X  and whose deletion will break the connectiv-ity of S ? ), then Algorithm 1 is guaranteed to identify the optimal connected subgraph S ? .

Proof. The BJ scan statistic satisfies three intuitive prop-erties: 1) monotonically increasing with respect to N  X  ; 2) monotonically decreasing with respect to N and  X  ; and 3) convex. Therefore, if S ? contains no  X  X reak-tire X  entities, we know that two properties hold: a) S ? consists entirely of p-values less than or equal to  X  , and b) no neighbor of S has p-value less than or equal to  X  . Property a) holds since any leaf node with p-value greater than  X  could be deleted without disconnecting S ? , increasing the score. Property b) holds since any neighbor with p-value less than or equal to  X  could be added without disconnecting S ? , increasing the score. Now, when v  X  S ? is selected as a seed entity, each successive graph expansion will add all and only those neighbor entities with p-values less than or equal to  X  . If Z is greater than the diameter of S ? , the iteration contin-ues until no remaining neighbors have p-values less than or equal to  X  , at which point S = S ? . Therefore Algorithm 1 is guaranteed to identify the optimal connected subgraph S .

Note that another popular nonparametric scan statistic, the Higher Criticism (HC) statistic [7], also satisfies the above three properties and Theorem 2 still holds. Our pre-liminary analysis (omitted due to space limitations) demon-strates that the BJ statistic outperforms the HC statistic for heterogeneous graphs and hence we focus on the BJ statistic for the remainder of our study. We also show an interesting connection to the percolation-based scan statistic defined by Arias-Castro et al. [2], which provides nice asymptotic decision-theoretical properties.
 Theorem 3 (Percolation-Based Scan Statistic).
 As in Theorem 1, we consider the simplified case with fixed  X  , and again assume that K and Z are set sufficiently large. Following [2], let F  X  ( S ) = | S | if all p-values in S are less than or equal to  X  , and 0 otherwise. Algorithm 1 is guaran-teed to find the optimal subgraph S ? = arg max S F  X  ( S ) .
Proof. As in Theorem 2, we know that two properties hold: a) S ? consists entirely of p-values less than or equal to  X  , and b) no neighbor of S ? has p-value less than or equal to  X  . Property a) holds since the inclusion of any p-value greater than  X  would reduce F  X  ( S ) to zero. Prop-erty b) holds since any neighbor with p-value less than or equal to  X  could be added without disconnecting S ? , in-creasing F  X  ( S ) by 1. The remainder of our proof proceeds identically to the proof of Theorem 2; note that we do not need the additional assumption that no  X  X reak-tire X  entities exist, since this follows directly from the definition of the percolation-based scan statistic F  X  ( S ).

Note that we have chosen to use the BJ nonparametric scan statistic because of its superior detection power, in comparison with other test statistics such as the percolation-based scan. Nevertheless, this theorem shows the applica-bility of our work to percolation-based approaches as well. The computational cost of maximizing BJ over connected subgraphs is high, but we have proposed an efficient, approx-imate algorithm to address this issue. The approximation quality has also been validated by extensive experiments on real-world datasets, as described below.
This section evaluates the effectiveness and efficiency of the proposed NPHGS approach based on comprehensive ex-periments on four countries X  Twitter data. We considered the detection and forecasting of civil unrest events such as protests and strikes, and detection of rare disease (han-tavirus) outbreaks as two case study scenarios, but the pro-posed techniques can also be directly applied to other ap-plications, such as the detection and forecasting of human rights violations and financial crises.
Datasets : We randomly collected ten percent of all the raw Twitter data from June 1, 2012 to June 30, 2013 across four countries: Argentina, Chile, Columbia, and Ecuador. Sampling was conducted at the tweet level, instead of user level. The civil unrest event labels, called Golden Stan-dard Report (GSR), were collected and confirmed from the local newspapers that are accessible from internet. The col-lected tweet volume, news sources, and number of civil un-rest events reported for each country are summarized in Ta-ble 2. An example of a labeled GSR event is: (PROVINCE =  X  X l Loa X , COUNTRY =  X  X hile X , DATE =  X 2012-05-18 X , TITLE =  X  X  large-scale march was staged by inhabitants of the northern city of Calama, considered the mining capital of Chile, who demand the allocation of more resources to cop-per mining cities X , NEWS-LINK =  X  X ttp://www.pressenza.c om/2012/05/march-of-dignity-in-mining-capital-of-chile/ X ). For rare disease outbreaks, we considered hantavirus out-breaks in Chile as a case study, because there was a spread of hantavirus outbreaks there last year that greatly threat-ened public safety and stability. The outbreak labels were reported by Chilean Ministry of Health [26] and local news reports. Specifically, there were 17 rare Hantavirus disease outbreaks in more than eight different states from January 1, 2013 to June 30, 2013. We post-processed both the civil unrest and the disease outbreak data to create binary vari-ables representing whether or not a GSR event occurred in each province (state) for each date.
 Table 2: Description of civil unrest data by country
Data Preprocessing : After we collected the raw tweets, several preprocessing steps were conducted for our proposed approach and all the comparison methods, including: 1) Vocabulary Generation : We first generated a vocab-ulary of  X  1000 terms related to civil unrests and a vocabu-lary of 25 terms related to hantavirus from domain experts; 2) Content Filtering : Only the raw tweets that match more than two terms from the vocabulary were preserved; 3) Tweet Geocoding : We implemented a geocoding li-brary for tweets based on three major rules with priorities. For each tweet, we first searched for location and landmark mentions in the tweet text, then for geotags that are avail-able if the user enabled the geocoding function in his/her phone, and finally for location information from the user X  X  profile. The first location information identified was re-turned as the geographic location of this tweet.
 Comparison Methods : We compared our proposed NPHGS approach with five existing representative methods, includ-ing Spatio-Temporal (ST) Burst Detection [13], Graph Par-tition [24], Earthquake Detection [19], Real-World (RW) Event Identification [3], and Geographic Topic Modeling [25]. The first method returns an alert for each spatio-temporal burst that is detected. The second method applies wavelet analysis to build signals for individual words, and then clus-ters the signals based on their auto-correlations using mod-ularity based graph partitioning. Each cluster is returned as an alert, with the most frequent location in the related tweets identified as the event location. The third method classifies tweets based on predefined features, and develops a probabilistic spatiotemporal model to identify the geo-graphic center and date of the event. The fourth method considers the framework of online clustering for event de-tection, but designs a new similarity function in order to capture features related to time, topical coherence, and so-cial interactions. The fifth method detects geographic topics day by day, each of which is returned as an alert.

Implementations of the first and the fifth methods were obtained from the authors, the second method was repli-cated under the authors X  instructions, and the other two methods were implemented based on the published papers [3, 19]. We strictly followed the strategies recommended by the authors in their papers to select features and estimated the related model parameters. Specifically, the parame-ters of Earthquake Detection, Graph Partition, and Spatio-Temporal Burst Detection were trained using cross valida-tion. The Twitter data from June 2012 to December 2012 were used as training data, data from January 2013 to April 2013 were considered as the test dataset for the detection and forecasting of civil unrest events, and data from Jan-uary 1, 2013 to June 30, 2013 were considered as the test dataset for the detection and forecasting of rare disease out-breaks. As an unsupervised approach, the geographic topic model has two major parameters, including the numbers of geographic regions and topics. These two parameters were predefined based on our interpretation of the data distribu-tion. Given the statistics of GSR event labels, the numbers of cities with high frequencies of civil unrest events in the four countries are mostly smaller than 20. The two param-eters were set to 20 and 2, respectively. RW-Event has a number of parameters, including the number of most fre-quent cluster terms, the parameters related to the incre-mental clustering algorithm, and the parameters related to the classification model used. In our implementation, we considered linear weighted support vector machine (SVM) to handle the issue of unbalanced class labels. We used 10-fold cross validation to identify the best combination of all the related parameters.
 Our NPHGS and Baseline Homogeneous Graph Scan Methods : Our proposed NPHGS is designed in a nonparametric statistical framework and the specification of parameters is hence relatively straightforward. Values of  X  max and the number of seed entities K were set to 0.15 and 5, respectively. We observed that performance of our method is not sensitive to the settings of these two param-eters. In addition to the above five comparison methods, we also compared our proposed NPHGS with four different homogeneous versions of NPHGS, including tweet, location, keyword, and user level homogeneous networks. In order to make fair comparisons, for each homogeneous graph, we de-fined a connection between two entities if they have direct relationships or if they share some neighbors in the hetero-geneous graph. For example, two tweets are connected if they have retweet or reply relationships, or if they are con-nected to the same geographic location or the same terms. In each case, we assume that the system alerts when de-tecting a subgraph with score F ( S ) above some threshold, allowing us to consider the tradeoffs between false positive rate and the other four performance metrics defined below by varying the alert threshold.

Performance Metrics : This study focuses on the eval-uation of both event detection and forecasting for different methods. The related performance metrics include: 1) false positive rate (FPR), 2) true positive rate (TPR) for fore-casting, 3) true positive rate for both detection and fore-casting, 4) average lead time for forecasting, and 5) average lag time for detection. For each method, the reported alerts are structured as tuples of (date, location), where  X  X ocation X  is defined at the province level.

For each gold standard event, we determine whether the method: a) Had an alert in that province from 1 to 7 days before the event (such events are considered to be  X  X uccess-fully predicted X  at the given threshold). In this case, we record the number of days of lead time for that event based on the earliest such alert; b) Did not have an alert in that province from 1 to 7 days before the event, but did have an alert in that province from 0 to 7 days after the event (such events are considered to be  X  X uccessfully detected X  at the given threshold). In this case, we record the number of days of lag time for that event based on the earliest such alert; or c) Did not have an alert in that province between 7 days before and 7 days after the event (such events are considered to be  X  X ndetected X  at the given threshold).
Based on the preceding results, we compute how many alerts were triggered that were not within the 7-day window before and after any event (this is the number of  X  X alse pos-itives X  at the given threshold). Now, as a function of the number of false positives (we scale this by time, e.g.,  X 1 FP per day X ), we can determine: 1) Proportion of gold stan-dard events that were successfully predicted; 2) Proportion of gold standard events that were successfully predicted or detected (this is one minus the proportion of undetected gold standard events); 3) Average lead time for all gold standard events: higher is better. Note that we average in a  X 0 X  lead time for each event that is not successfully predicted; and 4) Average lag time for all gold standard events: lower is better. Note that we average in a  X 0 X  lag time for each event that is successfully predicted OR is detected on the event day, and for undetected events we average in a  X 7 X  day lag time. Finally, we note that the maximum false positive rate that we consider, 1 FP per day, is non-trivial because we consider each unique combination of a province and a date as one binary variable, and thus the number of potential false positives per day could be up to the total number of provinces in a given country. Table 3 presents the comparison between the proposed NPHGS approach and five competing methods for the task of forecasting civil unrest events. All measurements were averaged over the results of the four tested countries. For NPHGS, we show the performance metrics at various false positive rates. For comparable false positive rates, NPHGS achieved much higher forecasting TPR and detection TPR than all competing methods. The average lead time of NPHGS was at least one day greater than the other methods, and av-erage lag time was consistently smaller than the other meth-ods by 1 to 2 days. Run time of NPHGS was comparable to other methods but slightly higher because it considers all node types in the heterogeneous network rather than just tweets. We note that the true positive rates of all tested methods were lower than 50%, perhaps because some GSR events did not produce strong signals in the noisy Twitter data, or because an alert was only considered  X  X orrect X  if it matched both the date and location of a GSR event.

Table 4 presents the comparison results for the task of detecting hantavirus outbreaks. The results indicate consis-tent patterns as observed in Table 3. For comparable false positive rates, NPHGS outperformed the competitive meth-ods in all the metrics. Specifically, NPHGS achieved 10% to 30% higher forecasting TPR and detection TPR than all competing methods. The lead time of NPHGS was ten per-cent to twenty percent (0.2 to 0.5 days) greater than those of competitive methods, and the lag time of NPHGS was 50% (1 to 2 days) lower than those of competing methods. By comparing Table 3 and Table 4, we observe that the true positive rate of NPHGS for forecasting alone on the Han-tavirus disease data was 10% lower than that of NPHGS on the civil unrest data. Consistently, the lead time of NPHGS positive rate (from 0 to 1 false positive per day). on the Hantavirus disease data is 1 day less than that of NPHGS on the civil unrest data. One potential interpreta-tion is that civil unrest events tend to have stronger signals in Twitter data leading up to the event, since inflamed public sentiments and emotions, as well as advance planning and organization of strikes and protests, may be visible in the Twitter data. Nevertheless, our results demonstrate that we can achieve very early detection of emerging rare disease outbreaks. Table 4 shows that at a reasonably low false pos-itive rate of 0.2 FP/day, NPHGS has detection lag time of only 1.24 days, which is better than the typical 3 to 4 days lag time using traditional public health surveillance data.
This section compares NPHGS and different versions of homogeneous graph scan methods on the civil unrest datasets, with results shown in Figure 3. We applied the same frame-work of NPHGS to homogeneous networks of different en-tity types as the baseline methods. Hence, we label NPHGS as  X  X eter X  in this case, and label the baseline methods as  X  X omo-(entity type) X . The results in Figure 3 clearly demon-strate that NPHGS consistently outperforms all the homo-geneous graph scan methods for all performance metrics. When the false positive rate was low (e.g., between 0 and 0.2 FP per day), NPHGS achieved huge (  X  30%) absolute improvements in TPR, provided two days of additional lead time for forecasting, and detected events two days earlier.
This paper presents a nonparametric approach to the prob-lem of event detection and forecasting for heterogeneous so-cial media graphs. The direct statistical modeling of hetero-geneous relationships between entities and attributes is very complicated and computationally challenging. Our work avoids this complicated modeling process by transforming the heterogeneous graph into a  X  X ensor X  network, where we convert the heterogeneous entities and attributes into empir-ical p-values (using a novel, two-stage empirical calibration procedure). We then use a novel graph scan algorithm to maximize a non-parametric scan statistic over subgraphs, enabling early detection and advance forecasting of emerg-ing societal events. In addition to evaluation of the theoret-ical properties of our method, we perform extensive exper-iments on real Twitter data. Our empirical results demon-strate that we can effectively forecast civil unrest events and achieve very early detection of rare disease outbreaks, out-performing competing methods by a substantial margin for both detection (power and timeliness) and forecasting (ac-curacy and lead time). For future work, we plan to ex-tend NPHGS to do storytelling and causality analysis, since NPHGS is able to provide rich information related to on-going or new events, such as the users, geographical loca-tions, and key terms involved. In addition, we plan to ex-tend NPHGS to a Bayesian framework so that rich domain knowledge can be naturally integrated. This work was partially supported by National Science Foundation grants IIS-0916345, IIS-0911032, and IIS-0953330. Additional support was provided by the John D. and Cather-ine T. MacArthur Foundation. [1] How fast the news spreads through social media. In [2] Ery Arias-Castro and Geoffrey R. Grimmett. Cluster [3] Hila Becker, Mor Naaman, and Luis Gravano. Beyond [4] Robert H. Berk and Douglas H. Jones. Goodness-of-fit [5] Carlos Castillo, Marcelo Mendoza, and Barbara [6] Lise Getoor. Tutorial: Representation, inference and [7] David Donoho and Jiashun Jin. Higher criticism for [8] Jacob Eisenstein, Brendan O X  X onnor, Noah A. Smith, [9] Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. [10] Jon Kleinberg. Bursty and hierarchical structure in [11] Liangjie Hong, Amr Ahmed, Siva Gurumurthy, [12] Martin Kulldorff. A spatial scan statistic.
 [13] Theodoros Lappas, Marcos R. Vieira, Dimitrios [14] Edward McFowland III, Skyler Speakman, and Daniel [15] Daniel B. Neill. An empirical comparison of spatial [16] Daniel B. Neill. Fast subset scan for spatial pattern [17] Daniel B. Neill and Jeff Lingwall. A nonparametric [18] Stanislav Nikolov and Devavrat Shah. A [19] Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. [20] Hassan Sayyadi, Matthew Hurst, and Alexey Maykov. [21] Charu C. Aggarwal and Karthik Subbian Event [22] Benjamin E. Teitler, Michael D. Lieberman, Daniele [23] Kazufumi Watanabe, Masanao Ochi, Makoto Okabe, [24] Jianshu Weng and Bu-Sung Lee. Event detection in [25] Zhijun Yin, Liangliang Cao, Jiawei Han, Chengxiang [26] http://epi.minsal.cl/vigilancia-[27] http://www.clips.ua.ac.be/pattern [28] http://en.wikipedia.org/wiki/Klout
