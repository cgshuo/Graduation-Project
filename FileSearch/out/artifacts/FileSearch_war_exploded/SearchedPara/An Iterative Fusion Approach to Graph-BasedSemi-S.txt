
Yang Wang 1 , 3 ,JianPei 2 ,XueminLin 1 , Qing Zhang 3 , 1 , and Wenjie Zhang 1 Semi-supervised learning with graphs [10] is an important and effective approach, which propagates limited label information to unlabeled data objects on a sim-ilarity graph. A similarity graph uses the set of objects as vertices, and links edges based on the similarity between objects. Edges in a similarity graph may take similarity scores as weights. After label propagation [10] or manifold rank-ing [9] in a similarity graph, the more similar two objects, the more likely they have similar labels or the similar label relevance ranking scores. This property is called local smoothness [8]. The labeled objects iteratively propagate the la-bel information or label relevance rank ing scores to unlabeled ones via graph edges until convergence, and the final lab eling result based on the label rele-vance scores should be consistent to the initial label information, which is called global consistency [8].

Often, a data object described by many features can be naturally decomposed into multiple  X  X iews X , where each view consists of a subset of features. For ex-ample, an image may have a color view and a shape view. Given a set of training data objects with multi-views, where some objects are labeled and the others are not, semi-supervised learning with graphs from multi-views triestolearnaclas-sifier by incorporating the complementary information from multi-views. More often than not, the similarities between various objects may be manifested dif-ferently by different views. In such situations, the one-combo-fits-all methods [3,5,6] may not perform well, since they use the same linear fusion from multi-views for all objects. Moreover, differen t views in such methods don X  X  collaborate with each other to achieve consistency when performing fusion process.
To tackle the problem, in this paper , we develop an iterative fusion ap-proach, called SSMF (for s emi-s upervised m etric f usion and cross-view label propagation). SSMF fuses metrics and label propagation results from multi-views iteratively until the fused metric and label propagation results converge simul-taneously. Views are weighted dynamically during the fusion process so that the adversary effect of irrelevant views can be reduced effectively. Here, the similar-ity in an irrelevant view contributes negat ively to the similarity measurement matching the ground truth. Specifically, in each iteration, there are two steps. In the semi-supervised metric fusion step, for each view we form a fused metric by combining the current metric of the view and the label propagation results from other views. Unlike the methods in [2,4] that obtain a fused metric from multi-views without label information, the metric fusion step in our method fully utilizes the label information from all views. In the label propagation step, in each view we conduct label propagation using the fused metric. This step incorporates the complemen tary information from other views rather than from a single view only. Our SSMF method iteratively conducts the two steps until convergence.

The critical idea here is that the metric fusion and cross-view label propaga-tion processes are complementary to each other. Moreover, we fuse the similarity matrix from one view and label the relevance matrix from other views to yield a cross-view based query (label) driven similarity matrix.
 Contributions . Our major contributions can be summarized as follows. 1. We develop an iterative fusion approach SSMF in this paper. SSMF fuses 2. To further improve the performance of SSMF ,weextenditto WSSMF ,a 3. Our comprehensive experiments on real image data sets show that our tech-To our best knowledge, our proposed technique is the first co-training based method for multi-view and graph-based semi-supervised learning problem. Ex-isting one-combo-fits-all methods linearly and independently combine either the metric (kernel) or the labeling propagation result from multiple views to yield a better performance than single view para digms, as introduced in section 1. Wang et al. [4] proposed a related U nsupervised based M etric F usion ( UMF for short) method. However, it fuses equal weight as suggested by UMF . Unlike the adapted UMF that fuses the pair-wise similarity metric information, which cannot utilize the graph structure to evaluate the similarity between pair-wise objects. SSMF fuses label propagation and similarity metric information interactively for each view and at each iteration, the label propagation can be regarded as a variant of graph random walk. Wang et al. [7] proposed another metric fusion technique against multi-view data via a cross-view based graph random walk approach, however, they studied the unsupervised case rather than semi-supervised learn-ing studied in this paper. In this section, we present SSMF and describe its two nice properties, namely global consistency and local smoothness [8]. We first review the prelimi-naries. Then, we discuss SSMF using two views. Last, we present the general iterative form of SSMF with multi-views. 3.1 Preliminaries Let X = { x 1 ,x 2 ,  X  X  X  ,x n } be a set of data points from M views, we construct M graphs each using a d ifferent feature. G g denotes a k -NN graph constructed on X using g -th feature. Specifically, G g is constructed by connecting every two vertices x i and x j if one is among the k nearest neighbors of the other. Here, the nearest neighbors are computed us ing Euclidean distance between the g -th feature vectors of the images. The E uclidean distance between the g -th feature of G g .Eachentry W g ( i, j )in W g represents the similarity between x i and x j according to the g -th feature vector. W g ( i, j ) is defined by a Gaussian kernel and is set to if there is an edge in G g between x i and x j .Otherwise, W g ( i, j ) is zero. D g is the diagonal matrix of G g where each element D g ( i, i ) is defined as D g ( i, i )=
Without loss of generality, assume the first m points x i ( i =1 , 2 ,...,m )are labeled points and the remaining points are unlabeled. Let the number of labels be c ,and L  X  R n  X  c be the relevance labeling matrix with L ( i, j )=1,if x i is labeled by label j , denoted by L ( x i )= j (1  X  j  X  c ), and 0 otherwise. Here, we assume each point is associated with a single class label from the label set. Similarly, let R g  X  R n  X  c be the relevance score of unlabeled point x u belonging to label j regarding the g -th view. The closed form of optimal R g is yielded by minimizing the objective function in the right hand side of Eq. (2) represents the local smoothness ,whichmeans second term in Eq. (2) represents the global consistency , which means that the final labeling matrix R g should be consistent to the initial labeling matrix L . where S g = D  X  1 2 g W g D  X  1 2 g , D g is the diagonal matrix with the i -th diagonal R g can also be regarded as the label propagation result on G g . 3.2 SSMF for Two Views Instead of directly computing the similarity metric between any pair-wise points under unsupervised scenario [4], we achieve the similarity, under semi-supervised scenario, by indirectly me asuring relevance between each point and all labels, formulated as labeling relevance matrix. As such, one can imagine that if both data points have large relevance regarding all labels, their similarity is large, otherwise, it is small. In order to learn semi-supervised metric regarding two views, we need to consider the following two challenges. That is, (1) the learned similarity metric should encode the relevance between data points and all labels. (2) the learned metric should well incorporate the complementary information from two views to achieve the consistency. Assume W [ t +1] g ( g =1 , 2) denote the metric similarity matrix for g -th view in t + 1 iterations, then we define the following semi-supervised fusion strategy: to avoid the huge difference in scale of the label relevance matrices in different views. I is identity matrix, and  X  I is incorporated to make SSMF robust to the noise. To better explain the above fusion strategy, we take Eq. (4) as an example of refining the metric for the first view by applying SSMF .
 ation t x i in the first view, which can be seen as the summation of propagation of label relevance score between x m and y -th label formulated as Rn [ t ] 2 ( m, y ), through the edge weight equivalent to similarity between x i and x m ( m = i ), formulated Q 1 ( i, m ), m = i , from the first view, and la bel relevance matrix, Rn m = i , from the second view to make the incorporation of the complementary information from two views. Following this principle, the refined W [ t +1] 1 ( i, j ) in next iteration t + 1, for the first view, is yielded by considering relevance score between all labels and both two points ( x i and x j , respectively), while effectively incorporates the complementary information from two views. Eq. (5) may be conducted similarly.
 One natural question is how to calculate R [ t ] g , and its normalized form Rn [ t ] g for each iteration t , we propose to adopt the general iterative form in the next section. 3.3 The General Iterative Form of SSMF We can get R [ t ] g ( g =1 , 2 ,...,M ) iteratively by symmetric matrix. L is the initial labeling matrix mentioned in Section 3.1.
Generalizing Eqs. (4) and (5) regarding two views, W [ t ] g may be calculated as follows for multi-views.

The iterative form of SSMF with multi-view by iteratively applying Eqs. (7) and (8) represents a novel label propagation process. Specifically, each weighted agation results inherent in Rn [ t ] j ( j = g ) from other views, as shown in Eq. (8), and hence we call the label propagation formulated as Eq. (7) as cross view label propagation .
 Now, we are ready to prove the convergence of SSMF .
 Theorem 1. The iterative form of SSMF formulated in Eq. (7) converges. It suffices to prove the convergence on one view. Following Eq. (7), we have where R [0] g = L . Apparently, since 0 &lt; X  g &lt; 1, ing to the Gershgorin circle theorem. For the second term in Eq. (9), (1  X   X  g ) L is a constant matrix for all  X  i g [ the convergence of entry H g [ i ]( l,m ). We only need to prove the convergence of series t  X  1 i =1 H g [ i ]( l,m ), where H g [ i ]( l,m )=  X  i g [ [ since [ H g [ i ]]( l,m )  X   X  i g and each item [ H g [ i ]]( l,m ) is positive. Let ( R g )  X  SSMF be the convergent label relevance matrix regarding the g -th view by interactively applying Eq. (7) (cross-view label propagation) and Eq. (8) (semi-supervised metric fusion). The final label relevance matrix regarding multi-Algorithm 1. Algorithm 1. The algorithm of SSMF 10 t = t +1;
One important issue that SSMF does not consider is that there may be some irrelevant views , and simply fusing all views using the same weight in Eq. (8) may not achieve the best overall performan ce if there are irrelevant views during the fusion process. To address this issue, we devise an effective learning method to assign a weight to each view in each fusion iteration. Consequently, we extend SSMF to WSSMF , which will be described in next section. The basic idea is to consider the labeling result of cross-view label propagation for unlabeled points in the set U in each iteration. Two views are regarded consistent if their labeling results are similar. Specifically, we denote by V [ t ] i the i -thviewiniteration t . The more consistent V [ t ] i and V [ t ] j (1  X  j = i  X  M )are, cross-view label propagation may be different at various iterations. Therefore, we calculate the weight parameter in different iterations. We define a function in terms of cross-view labeling propagation result.
 We have L ( x [ t ] u [ i ]) = max Initially, we set the label relevance score of all unlabeled points to be 0, and the weight parameter  X  [ t ] ij ( i = j )for V [ t ] j is defined as coefficient symmetric matrix in iteration t , denoted by  X  [ t ] . In iteration t ,the j -th view (1  X  j = i  X  M )issaidtobe irrelevant with respect to the i -th i -th view, we denote the set of relevant views at iteration t by Re [ t ] i .
Instead of computing global irrelevant views explicitly, for the i -th view, we only fuse the views from Re [ t ] i in iteration t , and set the correlation strength weight to be 0 for irrelevant views. Combining Eq. 11 and Eq. 8, we have the W eighted SSMF ( WSSMF for short) for multi-views, which iteratively applies Eq. 7 and Eq. 12 until convergence.

Like SSMF , WSSMF also converges, which can be immediately proved in the same manner as Theorem 1. Th erefore, the final optimal label relevance matrix views, ( R g )  X  WSSMF is the convergent label relevance matrix in the g -th view obtained using WSSMF . Based on Algorithm 1, we generate the algorithm of in line 8 with Eq. 12. 4.1 Complexity Analysis Now, we analyze the time complexity of each iteration in SSMF and WSSMF .
The cost of SSMF mainly comes from two parts: cross-view label propagation and semi-supervised metric fusion. The iterative cross-view label propagation in Line 9 of Algorithm 1 takes O ( Mn 2 c ) time, and the same time complexity holds for semi-supervised metric fusion in Lines 7-8. We remark that all the above cost is from the matrix multiplication rather than matrix inverse computation. It is well known that matrix multiplication implementation without inverse compu-tation is efficient. Similar to SSMF , WSSMF also needs O ( Mn 2 c )timeforboth metric fusion and cross-view label propagation. In addition, O ( M 2 n ) time is needed to obtain the view correlation matrix  X  in each iteration regarding M views. Therefore, the overall time complexity for WSSMF is O ( Mn 2 c )+ O ( M 2 n ). As observed in our experiments(refer to Fig 2), both SSMF and WSSMF con-verge within quite limited iterations for most cases (less than 65 times). We evaluate both SSMF and WSSMF using multi-view content based image retrieval (CBIR) and multi-label image classification on real data sets. We set the convergence threshold to 10  X  4 for all methods.

In our experiments, we compare with the following state-of-the-art multi-view graph based methods for both multi-view CBIR and multi-label image classification.  X  The multi-modality graph ( MMG )method [3], which uses multiple graph  X  The averaged distance of multiple feature based metric ( ADF )method [2],  X  The unsupervised metric fusion ( UMF )method [4], which conducts metric 5.1 Multi-view Content Based Image Retrieval (CBIR) Multi-view CBIR is a typical problem where graph based multi-view semi-supervised learning is extensively applied. Specifically, a query image is a labeled data object in our model, and the label relevance matrix R g  X  R n  X  c in Eq. (2) is reduced to a ranking score vector r g  X  R n ,and R g ( i,  X  )  X  R n is reduced to r ( i )  X  R , which represents the relevance score between x i and the query image (labeled image). L  X  R n  X  c in Eq. (2) is reduced to an n dimensional vector Y  X  R n with the i -th entry to be 1 if x
We set the number of nearest neighbors k to 20 to calculate the metric distance in Eq. (1) for all views, which is consistent with the UMF method [4]. Similar to [9], we set  X  g to 0 . 99 in Eq. (7) for all views, set  X  to 1 in both Eq. (8) and Eq. (12). All methods are tested on the COREL5K data set [1], which con-sists of 5000 images in 50 categories. Each category contains 100 images. Due to the same number of images in each category, we use the precision-scope [3] as the evaluation metric. We use HOG, color histogram, RGB-SIFT and Pyramid wavelet texture feature to construct di fferent views, most of them are utilized by MMG . For each method, we select every sample of 5000 images as the query image (labeled objects) each time, and obtain the average precision value and its statistical distribution regarding all 5000 samples, shown using 3 points (mean, +1 standard deviation, and -1 standard deviation) in Fig. 1(a).

Unsurprisingly, WSSMF outperforms the other methods in top-s average pre-cision, since it can better achieve the con sistency from multi-views than the other methods. In addition, it can effectively address the problem of irrelevant views at each iteration. SSMF is the next after WSSMF . SSMF does not handle the problem of irrelevant views. Like SSMF , UMF (1) does not consider the irrele-vant view detection, either. Moreover, (2) UMF does not fuse label propagation results during the fusion process, (3) as such UMF fails to further exploring the graph structure to improve the metric similarity like SSMF and WSSMF as discussed in section 2. Consequently, UMF is inferior to SSMF .

Both MMG and ADF perform worse than the others. MMG outperforms ADF in most cases, since MMG fully explores the graph structure for different views, and it linearly combines the independent label propagation results with different weights. ADF , however, is different from MMG . It assigns the same weight to all views in combining the label propagation results, the single graph associated with averaged metric obstructs the graph structure of original inherent individual views. However, MMG is inferior to SSMF and WSSMF , since such one-combo-fits-all late fusion method is undesirable to achieve the consistency among all views by independently fusing all the label propagation result from all views. Worse still, it cannot well handle the irrelevant views issue. Fig. 2 shows the 5-point box-plots (maximum, minimum, mean, +1 standard deviation, and -1 standard deviation) of number of iterations and running time of all queries in all methods. Both WSSMF and SSMF use more iterations on average and sot longer running time than ADF and UMF , because ADF and UMF construct only one similarity graph. Instead, WSSMF , SSMF and MMG construct multiple graphs. WSSMF and SSMF need less iterations on average to reach convergence than MMG , since the cross-view based fusion method can speed up the process of achieving consistency. However, the running time of WSSMF and SSMf is similar to that of MMG , since more matrix multiplication is performed during each iteration than MMG . 5.2 Multi-view Based Multi-label Image Classification Multi-view based multi-label image classification can be regarded as multi-view based semi-supervised learning with graphs. The Caltech-101 data set ( http:// www.vision.caltech.edu/Image_Datasets/Caltech101/ )isusedtotestmulti-label image classification. It contains 9146 images organized into 101 categories. The number of images in different categories ranges from 40 to 800. We set c = 101 and n = 9146 in the label relevance matrix R g  X  R n  X  c and L  X  R n  X  c in Eq. (2), along with k =20inEq.(1)and  X  = 1 in Eq. (12).

We use the same sample rate to draw a random sample of images from each category as labeled images. The rest of images are treated as unlabeled. Each experiment is repeated 5 times, and the average value is reported. The classifi-cation accuracy on all unlabeled images i s used to evaluate different methods. HOG, color histogram, pyramid wavelet texture feature and SIFT are used to construct different views. The results are shown in Fig. 1(b).

WSSMF outperforms the other methods. SSMF is the second best method. The results verify the advantages of our iterative fusion methods. We also observe that the difference among different methods d ecreases as the samp le rate increases, since a higher sample rate makes the problem less challenging.
 In this paper, we propose a novel iterative fusion technique for graph based semi-supervised learning from multi-views. The central idea is to fuse metrics and label propagation results from multi-views iteratively and weight views dynamically. The experimental results clearly show that our new methods outperform the state-of-the-art methods on real data sets. As future work, we will investigate how to fuse selective labeling results fr om multi-view based graphs rather than tackling all the data points including both informative and noise data points. We will also investigate active learnin g based methods for better effectiveness and efficiency.
 Acknowledgment. Jian Pei X  X  Research is supported in part by an NSERC Discovery Grant and a BCFRST NRAS Endowment Research Team Program Project. Xuemin Lin is supported by ARC DP0987557, ARC DP110102937, ARC DP120104168 and NSFC61021004. Wenjie Zhang is supported by ARC DE120102144 and DP120104168. All opi nions, findings, conclusions and recom-mendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.

