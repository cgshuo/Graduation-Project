 In the past, there have been dozens of studies on auto-matic authorship classification, and many of these studies concluded that the writing style is one of the best indica-tors for original authorship. From among the hundreds of features which were developed, syntactic features were best able to reflect an author X  X  writing style. However, due to the high computational complexity for extracting and comput-ing syntactic features, only simple variations of basic syn-tactic features such as function words, POS (Part of Speech) tags, and rewrite rules were considered. In this paper, we propose a new feature set of k -embedded-edge subtree pat-terns that holds more syntactic information than previous feature sets. We also propose a novel approach to directly mining them from a given set of syntactic trees. We show that this approach reduces the computational burden of us-ing complex syntactic structures as the feature set. Com-prehensive experiments on real-world datasets demonstrate that our approach is reliable and more accurate than previ-ous studies.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text Analysis ; H.3.3 [ Information Search and Re-trieval ]: Clustering; H.2.8 [ Database Applications ]: Data Mining
T he work was supported in part by the Blue Waters sustained-petascale computing project under the National Science Foundation (award number OCI 07-25070) and the state of Illinois, NDSEG Fellowship, NSF IIS-0905215, NSF-CCF-0905014, U.S. Air Force Office of Scientific Re-search MURI award FA9550-08-1-0265, and the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official poli-cies, either expressed or implied, of the Army Research Lab-oratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Govern-ment purposes notwithstanding any copyright notation here on.
 Algorithms, Experimentation Authorship Attribution, Text Mining, Text Categorization, Authorship Discrimination, Authorship Classification
In computational linguistics and text mining domains, there are three classical classification problems: topic clas-sification, genre classification, and authorship classification. Among these three problems, arguably the most difficult is the classification of documents in terms of their authorship (known as authorship classification , authorship attribution and/or authorship discrimination). This problem can be thought of as classifying documents based on the writing styles of the authors. This is a nontrivial problem even for humans: while a human can easily identify the topic and genre of a given document, identifying its authorship is harder. If the documents are in the same topic and genre, the task becomes much harder.

In the era of excessive electronic texts, authorship classi-fication has become more important than ever before with a wide variety of applications. Besides the early works of analyzing the disputed plays of Shakespeare (1887) [22] or anonymous documents of The Federalist Papers (1964) [24], it could also be used to identify authors of short  X  X or sale X  messages in a newsgroup [36] and even for forensic inves-tigations by identifying authorship of e-mail messages [2]. Detecting plagiarism or copyright infringement of unautho-rized reuse of source code by establishing a profile of an au-thor X  X  style is another important application of authorship classification [5].

Existing approaches to authorship classification use vari-ous methods to extract effective features, the most common of which include style markers such as function words [9, 33, 1, 12] and grammatical elements such as part of speech ( POS ) tags [3, 11, 35]. Function words are common words ( e.g. articles, prepositions, pronouns) that have little seman-tic content of their own but usually indicate a grammatical relationship or generic property. Recently, there have been several papers that claimed function words are more effec-tive than other types of style markers [33, 35, 34].
Unfortunately, research on more complex syntactic struc-tures has not been practical because of the lack of a reliable, automatic tool which retrieves syntactic structures, and be-of Eric X  X  sentences contained t . cause of the high computational cost associated with syntac-tic structure-based algorithms. Instead, several variations of POS tags [9, 11, 15] and rather simple syntactic structures like rewrite rules [3, 11, 15] have been proposed. Among them, bigram POS tags and rewrite rules showed reliable performance in various dataset configurations.

Recently, several advanced techniques have been devel-oped which greatly improved the performance of Natural Language Processing ( NLP ) tools 1 enabling reliable, highly accurate sentence parsing into a syntactic tree of POS tags. A syntactic tree is a rooted and ordered tree that is labeled with POS tags that represent the syntactic structure of a sentence. Based on the syntactic trees parsed by these tools, we propose a novel syntactic feature set of tree fragments al-lowing at most k -embedded edges (in short, a k -ee subtree). We say there is an embedded edge between two nodes if and only if they are in an ancestor-descendant relationship but not in a parent-child relationship. Compared with previ-ous feature sets that consist of parts of distinct connected subtree components, our new feature set captures the re-lationship between k +1 connected subtree components of a syntactic tree, which leads to a better representation of datasets consisting of long and complex sentences. Figure 1 gives an example of a k -ee subtree t for k = 2. Pattern t is composed of three smaller subtrees, which are connected by two embedded edges (S,NP) and (VP,PP). The differences in pattern distributions between two authors suggest that a set of k -ee subtrees can be utilized as a good feature set for authorship classification.

To reduce the number of features, we only mine a set of frequent and discriminative k -ee subtrees, which results in higher accuracy by avoiding overfitting to the training data and by not generating non-discriminative features that often degrade the performance. This task is commonly referred to as pattern-based classification. The original pattern-based classification technique employed a two-step procedure called generate-and-test which generates all frequent and closed 1 W e used Stanford Parser ( http://nlp.stanford.edu/ software/lex-parser.shtml ), but there are more tools available like N atural L anguage T ool K it (NLTK) package ( http://www.nltk.org ). candidate patterns and then selects the discriminative pat-terns among them [20]. Unfortunately, it is still intractable to use this generate-and-test methodology to get discrimina-tive patterns because there are simply too many candidate patterns.

For this reason, there have been quite a few works which directly mine discriminative patterns without generating all candidates [7, 37, 29]. Yet, these existing works cannot be directly applied to our problem setting because they require the feature values to be binary. Instead, we require numeric feature values because a (syntactic) feature can occur mul-tiple times in a document and usually the number of occur-rences implies its importance. Existing works are all based on binary-valued features and their theorems and proofs are not easily extendable to numeric-valued features. A recent work ([18]) showed that it has more gain to use numeric values than to discretize them into binary values. It also proposed a new way to directly mining discriminative nu-meric features by solving a linear programming optimization problem. But all these previous works mine top-1 pattern iteratively until the mined patterns cover the entire data. To cope with this issue, we derive an upper bound of a dis-criminative score of numeric-valued features, and develop an efficient algorithm that mines in one iteration a set of discriminative patterns to be used for classification purpose.
To validate the utility of our new feature set compared to others, for fair comparisons, we apply the same SVM classi-fication algorithm using various feature sets on several real data collections. Because of its high and reliable perfor-mance, SVM has commonly been used to compare the ef-fectiveness of feature sets [11, 35, 15]. Experimental results demonstrate the effectiveness of the proposed k -ee subtree features in comparison to the well-known existing feature sets of function words, POS tags, and rewrite rules. We demonstrate that by using k -ee subtrees as the feature set we outperform the existing feature sets by 8.23% on average and show that it is significantly better from other approaches by t-test with 95% confidence level.

In summary, the contributions of this paper are as follows:
The rest of the paper is organized as follows. Section 2 presents an overview of the related works. In Section 3, we introduce various preliminary concepts and define our new feature set of k -ee subtrees. Section 4 explains a branch-and-bound framework of discriminative k -ee subtree mining. We report experimental results in Section 5, followed by discussions in Section 6 and conclusions in Section 7.
There are two main steps involved in any authorship clas-sification algorithm: (1) the feature extraction step and (2) the classification step based on features extracted from the first step.

For the feature extraction step, since the earliest works of the authorship attribution on the plays of Shakespeare (1887) [22] and The Federalist Papers (1964) [24] that used a small number of common words such as  X  and  X ,  X  to  X  as a feature set, nearly 1,000 different feature sets have been studied includ-ing sentence length, chi-square score, lexical richness [17], vocabulary richness [8], function words [1], word n-grams [27], character n-grams [13], and rewrite rules [3] with lots of controversy on their effectiveness. Even though there is an issue of fair comparison among feature sets because previous works conducted experiments based on their own datasets with different classification methods [33, 28], function words and rewrite rules are generally considered to give reliable and good results. In [35, 34], the authors compared func-tion words with other feature sets (even combination of those feature sets) to show that function words are better than the other feature sets.

Even though many new features have been explored for authorship classification, most of the classification algorithms are simply adapted from other domains X  well-known classi-fication algorithms such as PCA [16], k -nearest neighbor, decision tree, bayesian networks [33], and SVM [9, 11, 35, 15]. In [35], a language model based authorship classification framework was proposed, which showed comparable perfor-mance to SVM . Because of its high and reliable performance, SVM has commonly been used to compare the effectiveness of feature sets [11, 35, 15], so in this paper we also use SVM for fair comparison between our new feature set and existing feature sets.

While most of the earlier works focused on binary au-thorship classification problem (classifying documents from two authors) which showed significantly good results, recent works including [2, 19, 21, 33, 35] have brought up the prob-lem of multiple authorship classification (classifying docu-ments from more than two authors). Thus, in this paper, we also show the effectiveness of our proposed k -ee subtrees feature set by solving both binary and multiple authorship classification problems.

Our proposed k -ee subtrees feature set can be considered to be a variation of tree patterns. In the data mining do-main, there have been several studies on tree pattern mining [30, 31, 4]. TreeMiner [30] is one of the pioneering frequent tree pattern mining algorithms. For tree classification, rule-based classifiers ( XRules [31]) and a decision tree based clas-sifier ( Tree 2 [4]) were proposed. To the best of our knowl-edge, mining k -ee subtrees has never been discussed before.
Previous authorship attribution approaches adopted func-tion words, POS tags, and rewrite rules as a feature set to build a classification model. Even though they achieved good accuracy, there still exists room for a more meaningful feature set to improve the performance. In this section, we describe rewrite rules which are somewhat complex syntac-tic structures that hold more syntactic information than the other two feature sets. Also, we define our new feature set of k -ee subtree patterns.
In [3], rewrite rules were considered to be building blocks of a syntactic tree, just as words are building blocks of a sentence. Here, a syntactic tree is a rooted and ordered tree which is labeled with POS tags that represents the syntactic structure of a sentence. Its interior nodes are labeled by non-terminals of the grammar, and the leaf nodes are labeled by terminals.

Compared to previous approaches that utilized function words and POS tags, rewrite rules can hold functional struc-ture information of the sentence. In linguistics, a rewrite rule is in the form of  X  X  X  Y  X  where X is a syntactic category label and Y is a sequence of such labels such that X can be replaced by Y in generating the constituent structure of a sentence. For example,  X  NP  X  DT + JJ + JJ + NN  X  means that a noun phrase ( NP ) consists of a determiner ( DT ) fol-lowed by two adjectives (JJ) and a noun ( NN ).

There is a limit when using rewrite rules as features of a classification model. First, because of the restriction that the entire rule cannot be broken into smaller parts, no sim-ilarity between rules are considered. A large number of slightly different rules are all counted as independent fea-tures. For instance, a rewrite rule  X  NP  X  DT + JJ + NN  X , missing one JJ from the above example, becomes a separate rewrite rule. Second, the expressibility of rewrite rules is limited because they must adhere to a very strict two-level tree structure, which does not allow the entire rule to be broken into smaller parts. For example, the relationships between rewrite rules are missing, which can hold more re-fined syntactic information. For these reasons, we developed a new feature set of k -ee tree patterns that are flexible and complex enough to represent the syntactic structure infor-mation of a sentence.
To overcome the drawbacks of simple syntactic feature sets used in previous approaches, we explore more complex syntactic features. Induced subtrees of a syntactic tree are one of the candidate feature sets whose features are multi-level tree fragments used to model the complex syntactic structure of a sentence. Here, we define a tree t to be an induced subtree of a tree s if there exists an identity map-ping from t to s preserving all parent-child relationships be-tween the nodes of t . Our pilot experiments showed that a small number of combinations of those induced subtrees could achieve even higher accuracy, which motivated us to define k -ee subtrees for our new feature set. Based on this Figure 2: Example of overcounting overlapped k -e e subtree occurrences motivation, we designed a new tree pattern that can capture this phenomenon.

Definition 1. We define an embedded edge e of a tree s to be a pair of two nodes with an ancestor-descendant rela-tionship. We define a k-embedded-edge subtree (shortly, k-ee subtree ) t of a tree s to be a set of induced subtrees of s that can be connected by at most k embedded edges (not with parent-child relationships) for a user specified value k .
The number of k -ee subtrees would be exponential on the number of trees and their sizes. We define a minimum sup-port  X  to ensure we only mine general common patterns that will be applicable to test data thus avoiding overfitting. We define the support of a feature t (denoted by sup ( t )) to be the total number of sentences in training data that contains t . We say t is frequent if and only if sup ( t )  X   X  for a user-specified minimum support threshold  X  .
The frequency of a pattern in a document (or a set of syn-tactic trees) is quite important in the sense that it can be a good measure to discriminate the writing styles of differ-ent authors. Well-known features like function words, and the POS tag-adapted bag-of-words approach use the num-ber of occurrences in a document as their frequency measure. However, unlike function words and POS tags, k -ee subtrees cannot simply adapt the same frequency measure because it generates overlapped occurrences, which would lead to an exaggerated frequency value. Figure 2 is an illustration of this overcounting problem. The syntactic tree S has only one A and four B s, but the number of occurrences of pattern t becomes 6. More generally, if A has n B s as its children in S , then the occurrence count of pattern t becomes O ( n 2 ). Since we allow k embedded edges for a k -ee subtree, this overcounting problem will be even more amplified.

Our observation that a document is parsed into a set of syntactic trees (of sentences) gave us an insight to define the frequency measure of a k -ee subtree in a more natural way by counting the number of syntactic trees of a document that contain the pattern.

Definition 2. We define the frequency of a k -ee subtree t in a document d (denoted by freq ( t, d )) to be the number of syntactic trees ( i.e. , parsed sentences) in d that contain t over the total number of sentences in d .

We will discuss how to mine discriminative k -ee subtree patterns in the following section (Section 4). For here, sup-pose we already have them in a set P = { t 1 , , t n } . Then, we can express a document d as a vector of their frequencies as d = ( freq ( t 1 , d ) , , freq ( t n , d )). Figure 3: Database D and its frequent k -ee subtrees
In the previous section, we introduced k -ee subtrees as a new feature set for authorship classification. These pat-terns hold more expressive syntactic information than other features and are flexible enough to consider partial match-ings of syntactic trees, but the number of k -ee subtrees is above our control. Therefore, we need to directly mine a small number of discriminative patterns not only to reduce the number of features but also to mine significant patterns which has been shown to improve classification accuracy [6]. In this section, we present a branch-and-bound framework to solve this problem.
We do not generate candidate k -ee subtrees and check for frequent attributes. Instead, we find a frequent k -ee subtree and extend it by adding a node that is guaranteed to be fre-quent in a depth-first manner, which enables several prun-ing techniques for frequent and discriminative pattern min-ing. We first introduce how to efficiently mine frequent pat-terns based on pattern-growth approach by using projected database [26, 38], and then explain pruning techniques to mine discriminative patterns.

We illustrate the procedure for pattern-growth approach as follows. First, find a size-1 frequent k -ee subtree t in the training dataset D . Second, project the postfix of each oc-currence of t in the syntactic trees of D into a new database D . A postfix of an occurrence of t in a syntactic tree s is a forest of the nodes of s appearing after the occurrence of t in a pre-order scan of s . Third, find a frequent node v in D that can be attached to the rightmost path of t that forms a k -ee subtree. Once v is frequent in D t , it ensures that the extended pattern is also frequent, so we do not need to scan the whole database D again. Note that, in this study, we consider a node v attached to t by an (induced) edge differ-ent from the one attached by an embedded edge. Fourth, recursively go back to the second step with the extended pattern for every frequent node we find. Note that the pro-jected database of a pattern t keeps shrinking as the mining process moves on and t becomes a bigger superpattern.
Example 1. Figure 3 shows an example of the pattern-growth approach to mine 0-ee subtrees from a database D of four syntactic trees when minimum support threshold is Figure 4: Binned information gain score distribution o f various feature sets 0.5. Each pattern is indexed in pattern-generation order. We first search for size-1 frequent patterns, which are t and t 6 . We choose t 1 as a starting point, and find frequent nodes that can be attached to t 1 from its projected database. We find that nodes B and C are frequent, and we extend t 1 to t 2 by adding a node B . Similar procedures are recursively performed until we mine all frequent patterns.
In previous subsections, we presented a pattern-growth method to mine frequent patterns, but the resulted patterns may still be too many. Based on the study that the patterns with high discriminative score can improve the classification performance [6], we first evaluate the discriminative power of a k -ee subtree. Note that most of the well-known dis-criminative scores ( e.g. information gain, fisher score) have upper bound on binary feature values not on numeric fea-ture values [6, 7, 29, 25]. In this subsection, we define a new discriminativeness score, binned information gain , and derive its upper bound on the numeric feature values to en-able a branch-and-bound framework to mine discriminative patterns on numeric feature values.

Definition 3. For a user specified number n , we divide range [0 , 1] of the relative sentence frequency per document of t into a partition p of equi-width n bins: p 1 = [0 , 1 p given partition p and m classes C 1 , , C m , we define the binned conditional entropy of t by and binned information Gain of t by IG ( C | X ) = H ( C )  X  H ( C | X ) where H ( C ) =  X  P m k =1 p ( C k ) log p ( C
A pattern t will have a large binned information gain score if the frequency distribution imbalance between the classes becomes bigger for each bin, which means t is significant to discriminate classes.

Figure 4 presents binned information gain score distribu-tions of various feature sets such as function words (FW), POS tags (POS), bigram POS tags (BPOS), rewrite rules (RR), and k -ee subtrees for k=0, 1, and 2 (0-ee , 1-ee , and 2-ee , respectively). We can easily see that the highest scores are mostly from k -ee subtrees, which implies that they can be more meaningful than other features  X  an assertion we later test in the experiments section.

For a tree pattern t , we denote binned information gain of t by IG ( t ) and information gain upper bound of t and its superpatterns by IG ub ( t ). Given a k -ee subtree t and a partition p , we define ( A, B, p ) to be a frequency distribution of t where A = ( A 1 , . . . , A n ) and B = ( B 1 , . . . , B A i and B i being the number of documents in class C 1 and C 2 respectively for each bin p i of a partition p . Denote ( A  X  , B  X  , p ) as a frequency distribution of a super pattern t of t . The following two lemmas describe the properties of ( A, B, p ) and ( A  X  , B  X  , p ) that will be used to prove the main theorem to derive the upper bound of binned information gain.

Lemma 1. For any k = 2 , . . . , n , the following four in-equalities hold for a k -ee subtree t and its superpattern t P P
Proof. Since t  X  is a superpattern of t , P n i = k A  X  i P in class C i . Similar proof for B i .

The following lemma shows the condition to get the upper b ound of binned information gain for a special case when only the first two bins of frequency distribution are different.
Lemma 2. For a given frequency distribution ( A, B, p ) , let ( A  X  , B  X  , p ) be a frequency distribution with A  X  x , A  X  2 = A 2  X  x ( 0  X  x  X  A 2 ) and the rest unchanged. conditional entropy when x = A 2 . Otherwise, it achieves its minimum conditional entropy when x = 0 .

Proof. Let f ( x ) be the conditional entropy of ( A  X  , B and N be the total number of documents. Then,
The following theorem describes that the binned infor-m ation gain upper bound exists and is determined by the frequency distribution of the first two bins.

Theorem 1. Given a tree pattern t , its super patterns including itself have a conditional entropy lower bound in the frequency distribution ( A  X  , B  X  , p ) of one of the following two forms: (1) A  X  1 = A 1 + A 2 , B  X  2 = P n i =2 B i , B  X  ( i = 2 , . . . , n ) and A  X  i = A i ( i = 3 , . . . , n ) (2) B A 2 = P n i =2 A i , A  X  1 = A 1 , A  X  i = 0 ( i = 2 , . . . , n ) and B ( i = 3 , . . . , n ) .

Proof. Suppose (  X  A,  X  B, p ) is a frequency distribution of a superpattern  X  t of t with minimum conditional entropy whose form is in neither cases. Denote P i =  X  A i  X  A Q ther P i &lt; P i +1 or P i +1 = 0 ( i = 1 , . . . , n  X  1). Symmet-rically, either Q i &lt; Q i +1 or Q i +1 = 0 ( i = 1 , . . . , n  X  1). Then, for all i = 2 , . . . , n , either P i = 0 or Q i = 0. ( sume P i 6 = 0 and Q i 6 = 0 for some i . Then, P i  X  1 &lt; P Q is a contradiction.) Therefore, either P 2 = 0 or Q 2 = 0. Without loss of generality, say P 2 = 0. Then, we can get another distribution (  X  A  X  ,  X  B  X  , p ) where  X  B  X  for ( i = 3 , . . . , n ), and the rest unchanged from (  X  Since its conditional entropy at each bin p i ( i = 2 , . . . , n ) becomes 0, it has smaller or the same conditional entropy with (  X  A ,  X  B , p ). By the assumption that (  X  A , imum conditional entropy, their conditional entropy are the same. By Lemma 1,  X  A  X  1  X  A 1 + A 2 and  X  B  X  1  X  B 1 since P 2 = 0). If either  X  A  X  1 &gt; A 1 + A 2 or  X  B the conditional entropy of (  X  A  X  ,  X  B  X  , p ) becomes higher than the conditional entropy of ( A  X  , B  X  , p ) in the first form of the theorem which is a contradiction to our assumption that the conditional entropy of (  X  A ,  X  B , p ) is minimum. Similar contra-diction can be derived when Q 2 = 0.
T he binned information gain measure and its upper bound described in Section 4.2 enables a branch-and-bound frame-work, and we can simply perform the feature selection proce-dure in a traditional sequential coverage way as follows ([7, 29]). First, we mine the most discriminative k -ee subtree and add it to the feature set. Second, we remove trees that contain the extracted pattern and compute binned informa-tion gain scores of the remaining patterns on the updated database. In this way, redundant patterns will have a small chance to be selected. Third, we go back to the first step un-til either the dataset becomes empty or no more patterns are mined. Once the feature selection procedure is complete, we get a small number of discriminative k -ee subtrees. Based on the feature set F of these patterns, we use the document representation described in Section 3.3 to train a classifica-tion model.

But this procedure is inefficient when many discriminative patterns need to be mined because the sequential coverage method described above is based on iteratively mining one discriminative pattern for each iteration. We observe that the object of iterative approach is to find non-repetitive dis-criminative patterns. For this purpose, previous works sim-ply applied the decision tree scheme of feature selection ei-ther (1) to a sequential coverage method to be used for SVM classification model [7, 29] or (2) to a decision tree classifi-cation model directly [10]. The difference between them is that the former recursively mines the dataset that does not contain the pattern, and the latter recursively mines both datasets containing and not containing the pattern. But both approaches need to recompute discriminativeness scores of the patterns on the updated database paying an expensive computational cost, which does not really involve removing repetitive patterns. We propose to use a modified sequential coverage method which does not recompute the binned information gain scores.
In this section, we design a novel algorithm to efficiently mine discriminative patterns in a single iteration. We com-pute the binned information gain score only once, and apply the sequential coverage method without recomputing the binned information gain scores. Moreover, we propose an efficient way of mining the discriminative patterns in one iteration.

Here, we define some terms and symbols that will be used for the rest of the section. We denote t | = s when a k -ee subtree t is contained in a tree s . We define S t = { s  X  D| t | = s } to be a set of trees in a tree dataset D that contain t . Also, we define A t = { p : k-ee subtree | X  s  X  S t , p = argmax p | = s IG ( p ) } to be a set of patterns that achieve the highest discriminative score among all patterns in some trees that contain t , and B t to be a set of arbitrary patterns from each tree of S t . We denote F to be a set of discriminative k -ee subtrees in D mined by the modified sequential coverage method.

The following lemma characterizes discriminative patterns mined by sequential coverage.
 Lemma 3. For a given tree dataset D ,
Proof. By the definition of the modified sequential cov-erage method mentioned in Section 4.3.

Lemma 3 explains that the discriminative patterns mined b y the modified sequential coverage method are indeed the most discriminative patterns for some trees of D . Based on this observation, we derive a pruning method by branch-and-bound approach in the following proposition.
 Proposition 1. (Branch-and-Bound (BB) Pruning) If IG ub ( t ) &lt; min p  X  A t IG ( p ) , then no superpattern t in F .

Proof. Since S t  X  S t  X  , IG ub ( t ) &lt; min p  X  A t IG ( p )  X  native pattern for any tree in S t  X  .

Corollary 1 . If IG ub ( t ) &lt; min p  X  B t IG ( p ) , then no su-perpattern t  X  of t is in F .
 Proof. By definition of A t , IG ub ( t ) &lt; min p  X  B t In case I G ub ( t ) = min p  X  B t IG ( p ), we also skip mining D t since any tree containing a superpattern t  X  of t will also contain another pattern that has higher or the same discrim-inative score.

Once we know an upper bound of the discriminative score of t  X  X  superpatterns, we can use the BB pruning method described in Proposition 1. Unfortunately, as alluded to earlier, this is a nontrivial task because the feature values are numeric instead of binary. In Section 4.2, we partitioned the numeric range [0 , 1] into a finite number of bins and derived the upper bound of binned information gain score by checking a constant number of cases (at most 2 cases) regardless to the number of bins.
 In the mining process, since we do not know A t , we set B t to be the set of current best patterns of S t and apply Corollary 1 as a BB pruning condition. For that reason, we maintain current best patterns for each tree.

Example 2. Consider the example from Figure 3. Sup-pose class c 1 has a document d 1 and class c 2 has a document d f rom a database D . Let the number of bins for binned in-formation gain be 3 (i.e. n = 3 ). We first mine t 1 , compute its discriminative score ( IG ( t 1 ) = 0 ) and update current B 1 ( B t 1 =  X  ) by checking t 1 . Now, B t 1 = { t 1 } . Since pattern t 2 without pruning. We compute t 2  X  X  discriminative score ( IG ( t 2 ) = 1 ), and update B t 2 = { t 1 } to be B Since IG ub ( t 2 ) = 1 = min p  X  B t IG ( p ) , we can skip generat-ing t 3 .

Following the original sequential coverage methodology mentioned in Section 4.3, when a k -ee subtree t is generated the trees containing t are removed. But in real classification tasks, we may want to generate multiple patterns to repre-sent a tree to improve accuracy. To address this issue, we use a minimum feature coverage threshold  X  introduced in [7], i.e. , a tree is removed when it is covered by at least  X  dis-criminative patterns. Lemma 3 and Proposition 1 can easily be adapted with the feature coverage parameter  X  by main-taining top- X  patterns for each tree and using  X  -th highest discriminative score as a cut-off threshold for each tree.
In summary, we proposed a branch-and-bound framework of authorship classification. During the process, the algo-rithm retains and updates the most discriminative patterns Opt ( s ) of each tree input, and at the end they become F . The basic framework is to expand the patterns from small to large sizes in pattern-growth approach. Before we expand current pattern t into a larger one, we compute the upper bound of the binned information gain of all superpatterns of t . Based on BB pruning described in Corollary 1, if the upper bound value is not greater than the current minimum Opt ( s ) from all trees ( s ) containing t , then we can safely skip exploring superpatterns of t .
In this section, we present an empirical evaluation in or-der to validate the performance of our k -ee subtree based authorship classification. We also analyze the effect of the parameters of k -ee subtree patterns presented in this paper. The experiments are designed to test the usefulness of k -ee subtrees, as a new feature set, for authorship classification.
For the following experiments, we used public data collec-tions extracted from the TREC corpus [14] and The New
From T he New York Times we collected two different types of datasets: news articles and movie reviews. For the news articles, we randomly selected two journalists from the business department, and two other journalists from the health department who were the main contributors in their h ttp://www.nytimes.com and Denise Grady and Gina Kolata from the health depart-ment. nalists in the same department are likely to write articles on the same topic and genre using similar words.
 For the movie reviews, we used four movie critics from the The New York Times . It has three main critics whom we used. We added another randomly selected critic who is one most of the movies reviewed by the critics overlapped. We assumed movie reviews of the same movie will be on the same topic and genre using similar words.

We also used news articles from the Associated Press (AP) subcollection of the public TREC corpus. The AP collection has over 200,000 documents by more than 2,380 distinct authors. We followed the same experimental configurations as previous works [33, 35] did by using the same datasets each data collection are described in Table 1. Note that the class distributions (or the number of documents per author) are mostly balanced, and in this way we do not have to consider the effect of skewed data.
To evaluate the performance, we performed multiclass classification on each data collection using SVM with lin-ear kernel. Specifically, we decomposed the multiclass prob-lem into binary problems via one-versus-one method, and paired the authors of each data collection and conducted binary classification on these pairwise datasets. For each dataset, we conducted 5-fold cross validation, and averaged the accuracy as a measure of the performance. For each fold, training data was used to mine the syntactic features and to get a classification model while test data was only used for evaluation purposes. For each training data, we used another 5-fold cross validation to determine appropriate pa-rameter values for the classification model (linear SVM). In this way, our evaluation ensured that there is no information leak from the test data for the classification task.
We used the number of occurrences of each feature as a feature value for the syntactic features except k -ee subtrees which used a new frequency measure defined in Definition 2. For the fair comparison, we used the same classifier. In [9, 35], it is shown that SVM achieves reliable performance with high accuracy for authorship classification and the choice of the SVM kernel has little or no effect on the performance.
To show how effectively our new feature set of k -ee sub-trees works, we compared the authorship classification per-formance with other syntactic features such as function words (FW), unigram POS tags (POS), bigram POS tags (BPOS), and rewrite rules (RR). As for function words, we took the list of 308 function words from [23]. We used 74 POS tags from from the stanford parser. 1,088 Bigram POS tags were identified from the leaves of syntactic trees. Rewrite rules and k -ee subtrees were generated by mining parsed sentences of syntactic POS -tagged trees.

In the table 2, we show the average sizes of feature sets for each data collection. To get the number of features of rewrite 4 T he three main critics of The New York Times are A. O. Scott, Manohla Dargis, and Stephen Holden. The other critic we used is Jeannette Catsoulis. more, David Dishneau, Don Kendall, Martin Crutsinger, and Rita Beamish. Table 2: Number of features for F W , POS , RR and k -ee feature sets Table 3: Accuracy Comparison on Different Number o f Authors and Various Data Collections rules and k -ee subtrees, we computed the average value of the number of distinct features of 5-fold training data for each feature set and dataset. As expected, rewrite rules generated much larger number of features than all the other feature sets. It is noticeable that the number of k -ee subtrees are far less than the number of bigram POS tags and rewrite rules, and sometimes even less than the number of function words. For the rest of the section, we will show that our small sized new feature set of k -ee subtrees outperforms all the other feature sets.
We first show accuracy comparison on various feature sets and then analyze the effect of the parameters of k -ee subtree approach. For the accuracy comparison with other feature sets, we conducted binary authorship classification as well as multiple authorship classification tasks. Table 3 shows the accuracies of those authorship classification tasks for various comparison feature sets and for three different data collec-tions. By default, we used the number of embedded edge k = 1, minimum support threshold  X  = 0, the number of bins n = 10, and minimum feature threshold  X  = 3 for dis-criminative k -ee subtree mining. In Tables 3 and 4, boldface denotes the best result for each dataset.
Based on the accuracy results in Table 3, our new feature set of k -ee subtrees achieved the highest performance of the comparison feature sets. Overall, most feature sets showed high accuracy on binary authorship classification tasks. But when the number of authors was increased, the performance gaps between k -ee subtree feature set and all the others be-came larger.

It is true that bigram POS tags and rewrite rules catch deeper insights of an author X  X  writing style since they are more complex and have much larger number of features than POS tags. But we conclude that a feature set of k -ee sub-trees can characterize an author X  X  writing style even better since (1) it allows even more complex syntactic structures Table 4: Accuracy Comparison on binary author-ship classification of The New York Times news ar-ticles. Two journalists Dash and Healy from the business department are denoted by B 1 and B 2 , and two journalists Grady and Kolata from the health department are denoted by H 1 and H 2 respectively.
Author Pair FW POS BPOS RR k-ee than rewrite rules as features, (2) its size is much smaller t han the feature set of bigram POS tags and rewrite rules, and (3) it achieved better accuracies. Note that the feature set of function words reliably showed reasonable accuracies as previous works mentioned [33, 35, 34]. It achieved better than POS tags and sometimes even better than bigram POS tags and rewrite rules. This is because function words have two different aspects together (syntactic and lexical) while POS tags only have a syntactic aspect. But complex syn-tactic structures can complement the lack of lexical aspect of the features, since the feature sets of rewrite rules and k -ee subtrees showed higher accuracies than function words.
On average, the feature set of k -ee subtrees improved per-formance over the other feature sets about 8.23% (overall), 6.36% (function word), 12.56% ( POS ), 8.49% (bigram POS ) and 5.50% (rewrite rule).

We also performed a significance test on the feature sets over k -ee subtrees. We used two-tailed t-test on the ac-curacy results in Table 3, and all their t values (FW:3.18, POS:5.02, BPOS: 4.49, RR: 2.69) indicated that the per-formance of k -ee subtree patterns are significantly different from (or, better than) all the others (95% confidence inter-val, threshold:2.07).

Note that we could mine k -ee subtrees even for minimum support  X  = 0, a task rarely done in previous works because too many patterns were generated from the mining process. Further discussions are described in Section 6.1. As we explained in Section 5.1, the datasets of The New York Times news articles were collected to identify the dif-ficulty of classification problem. We assumed that the jour-nalists from the same departments will be hard to classify because they might use similar terms on the same topic and genre. As expected, classification results in Table 4 show that classifying journalists from different departments was easier than journalists from same departments.

Note that the last row of Table 4 shows extremely worse performance than other cases. We manually analyzed the news articles of H 1 and H 2 , and found that their writing styles were quite informal using several quotations which made it the hardest dataset. Even for this hard task, our approach got the highest accuracy with a big gap.
In Figure 5, we analyze the role of each parameter used to mine discriminative k -ee subtrees. All experiments were conducted for binary classification of two movie critics Stephen Holden and Jeannette Catsoulis. Similar trends could be found from other datasets. For default values, we used  X  = 0 . 3, n = 10, and  X  = 3. Overall, we found that 1-ee subtree feature set showed the best performance. It could be mined with almost in a constant time even with no min-imum support threshold. But, when the number of embed-ded edges increased ( e.g. k = 2), k -ee feature set showed worse accuracies because it tended to overfit to the training data. Moreover, it took exponential time to run when mini-mum support threshold gets smaller. It is good to know that we do not need too complicated syntactic structures (with a high k ), because the computation would be too expensive to make our proposed feature set useful.

There are two parameters, n and  X  , which are related to our binned information gain score. Based on Figure 5, they did not significantly affect the running time, but somehow affected the accuracy. However, since they achieved the peak within a small range, it was not difficult to optimize their values in our experiments.
In the experiments, we could put minimum support  X  to be 0 to mine discriminative patterns regardless to any min-imum support, which is almost impossible for generate-and-test methodology that has to generate all patterns. We know that mining frequent pattern is good to avoid overfitting ef-fect, but it is hard to set a correct minimum support thresh-old value. The authors in [6] show that features with low support have low discriminative power. That is, the discrim-inative patterns we mine even with no minimum support can be considered to be frequent enough to overcome the over-fitting problem.
All current approaches using syntactic features have the same limit of utilizing imperfect natural language processing ( NLP ) tools. For example, to obtain syntactic features (in-cluding k -ee subtrees), we rely on the performance of NLP parsers. It would be a good follow-up work to apply our approach to other informal datasets such as blogs and news-groups where NLP parsers tends to show low performance. We believe our approach will still show reasonably good per-formance even on those messy datasets with informal word-ing because the frequent pattern mining will not succumb to parsing errors as shown in Table 4 for ( H 1 , H 2 ) author pair.
In this paper, we did not consider feature combinations since the main target of this paper is to introduce a new fea-ture set and its effect in authorship classification. In fact, feature combination for authorship classification itself is a different topic with some previous work [32]. Simply apply-ing fixed weights for different feature sets to classify various datasets might not achieve good results because some au-t hors have their own stylistic habits that may be captured in one feature set that is more distinctive than others [35].
In this paper, we proposed a new syntactic feature set of k -ee subtrees to classify documents based on their authorship. To mine k -ee subtrees, we developed a direct discriminative k -ee subtree mining algorithm via a branch-and-bound ap-proach. Our novel algorithm could perform a discriminative score based feature selection procedure to mine discrimina-tive patterns in one step, not iteratively. To directly mine discriminative patterns, we theoretically derived an upper bound of binned information gain score of the numeric fea-ture values.

An experimental study has been performed on public real data collections, which were purposefully chosen to contain the same topics and genres and thus use similar terms. Our k -ee subtree-based classification achieved the best results compared to other feature sets. Overall, we conclude that k -ee subtrees are meaningful features for authorship clas-sification that achieved high accuracy across various data collections.
