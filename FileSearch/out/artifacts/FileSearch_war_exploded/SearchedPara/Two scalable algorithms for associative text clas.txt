 1. Introduction Associative classification uses association rules which are mined from a transactional log database ( Li, Han, &amp; Pei, 2001; types of classifiers including Naive Bayes and Support Vector Machine (SVM) consider only word features, associative clas-sifiers can exploit combined features (for example, phrases) as well as words.

Words are elementary features in text classification. In a text collection, a document generally consists of hundreds of words. If a document is viewed as a sequence of word features, it exists in a very high-dimensional space. If phrase features are included as well as words, the dimensionality increases. A training database contains tens of thousands of documents, hence they become distributed sparsely in the document space. Therefore, raising the coverage for unseen test documents during the prediction phase is a big challenge. To match more test documents in associative classification, the length of word pattern in a rule needs to be reduced. Reducing the length of the pattern may degrade classification accuracy, although it can improve the coverage for test instances. Associative classifiers resolve this reduction in accuracy by choosing a smaller num-ber of qualified rules and combining them in a predefined manner during the prediction phase ( Antonie &amp; Za X ane, 2002; Liu
A large number of association rules can be mined by adjusting the mining parameters such as minimal support and min-imal confidence to low values. However, too many association rules can hardly be processed in a real situation because they  X  require very long computation time and very large storage space. Especially, generating high-order problem because the number of rules produced grows exponentially with the order. We propose two new algorithms to process large amount of storage space when a larger number of rules are mined to build classifiers. The other algorithm uses a new data structure to conduct the classifier building process very quickly.

When a large-scale text collection is processed, the number of generated rules may exceed 10 into a disk file due to limitations in main memory. This paper proposes a novel method of representing class association rules in a compact manner where the itemset of a rule, the antecedent part of the rule, is represented in a compact format using the information of previously generated rules. Basically, the proposed classification method requires a large number of rules while some associative classification methods produce a small number of rules from the beginning. Our classification algo-rithm ( Yoon &amp; Lee, 2008 ) can achieve a maximum performance by generating as many rules as possible, which means min-imizing the loss of the given information. The saving of 1 Gbyte in the file size cannot be negligible in real situations. To apply the mined rules to a classification task, some qualified rules should be chosen from the original generated rules. the rules are loaded into memory again and are sorted in the order of confidence and support. They keep their compact form because only confidence and support information are needed in the sorting process (uncompressed antecedent information is required in the rule-matching process). Additionally, in-memory sorting requires an extra amount of memory. As the com-pact rules themselves occupy less memory space, the sorting or classifier building process can be performed more efficiently.
The second algorithm allows the process of rule matching against training documents to be performed quickly. To store training documents and to perform rule matching, we propose a new data structure which is specially designed for our clas-sifier building process. One strong point of the second algorithm is the ability to delete documents from the structure effi-ciently as well as the matching speed. Using a naive matching algorithm takes O ( klNM ) time, where k is the average length of a rule, M is the number of the rules, l is the average length of a test document, and N the number of the training documents. Our proposing algorithm can finish the process in one order of magnitude less time compared with the original method.
Empirical results using large-scale document sets demonstrate that the proposed algorithms make associative text classifi-cation scalable to real-world problems.

This paper is structured as follows. Section 2 lists the previous studies which handled the efficiency issues related to the a new model for representing association rules compactly, and explains the algorithm for efficiently matching rules to the training documents. Section 5 shows the experimental results using the proposed algorithms and compares them with the results obtained using previous methods. Finally, Section 6 concludes the paper. 2. Related works
Liu et al. (1998) introduced CBA, a prototype of associative classification. It adopts a two-stage induction process of clas-ing examples. The rules which classify correctly at least one example are selected and the covered examples are deleted from the database. While CBA deletes the examples when they are covered once, CMAR ( Li et al., 2001 ) postpones the deletion until the examples are covered m times, which can improve the coverage of the selected rules. However, the postponement of the deletion means that the algorithm performs additional m 1 times of rule matching against the example. In the asso-m times of matching per training example, which requires a more efficient matching algorithm for an enormous number of association rules.

HARMONY ( Wang &amp; Karypis, 2005 ) keeps a rule with the highest confidence, which is assigned to each covering training instance. Without an additional pruning procedure, it can generate a small number of high-confidence rules very efficiently.
However, such a one-rule-per-example principle may work adversely when handling a database with uneven distribution of class labels, because a class with many training examples may have a high prediction score on that class label, which leads to a wrong prediction.

Frequent closed itemset model ( Pasquier, Bastide, Taouil, &amp; Lakhal, 1999 ) was used for a compact representation for asso-
Therefore, the rule mining process adopting the frequent closed itemset model can be finished within a less time and space classification method, we use all subset rules of a frequent closed rule because it is far more possible to match test docu-ments if a rule has a smaller number of items. Normally, a rule in a closed itemset format is long and compact, and cannot be used in its own in our classification method; they should be expanded into its subset rules. In our rule mining method, a rule has a compact antecedent part and does not include any other subset rules in its compact form. From the beginning, we did not consider combining several rules into one because we mine rules for the purpose of classification.
In the area of associative classification, there have not been many studies on the rule matching algorithm itself. In the document retrieval domain, an inverted index ( Zobel, Moffat, &amp; Ramamohanarao, 1998 ) is used to structure documents and match query phrases against the documents in the database. Although it provides a fast rule matching, it suffers from severe performance degradation when it deletes documents and restructures the index. We made a performance comparison between the inverted index and our method in the Experiment section. The suffix trees ( Weiner, 1973 ) lead to a linear time algorithm in query matching and index construction ( Sandhya, Lalitha, Govardhan, &amp; Anuradha, 2011 ). If we are to use the suffix tree to represent training documents, we need to sort the word items of each document according to their frequencies in the document set, so that the order of items in a rule can be kept the same as in the documents. Moreover, the suffix tree based rule matching would not be more efficient because the rule does not need to be a substring of the document in order to get a match. 3. Associative text classification 3.1. Overall flow gin by processing raw text data for rule mining. Each document is converted to a transactional record format (step 1), and this preprocessed database is mined for frequent patterns, i.e., class association rules (step 2). From that large number of generated rules, a smaller number of qualified rules are selected and combined into a final rule set for classification (step 3). When a new document is to be classified, it is converted to a pattern of words and matched to the classification rules (step 4). According to the matching score, classes are assigned to the test document (step 5). 3.2. Association rule mining
Association rules were initially devised to represent a degree of association between items in a transactional log of the transaction. Let d be the average length of training documents, and V be the vocabulary of the document set. Then,
X = X 1 X 2 X i X d denotes a document, where X i 2 V . Let Y ={ c training examples D can be written as A class association rule (CAR) is a mapping from a set of items to a class label. Let Z denote an itemset (or word pattern), Z = Z 1 Z i Z k , where Z i 2 V . Then, a CAR can be written as k is called the order of the rule, and generally k d . The left-hand side of the arrow is called the antecedent, and the right-hand side the consequent.

Definition 1. The support of a CAR is the number of training examples in which the pattern and the class label of the rule occur.
 Definition 2. The confidence of a CAR is is the support of the CAR divided by the support of its antecedent.
When mining association rules, the mining algorithm checks the support and the confidence of generated rules, and filters out the rules whose support and confidence are less than certain threshold values, minimum support ( min _ sup ) and minimum confidence ( min _ conf ). The following is an example of class association rules mined from a document collection: where 151 is the support and 0.31 is the confidence of the rule. This rule indicates that the training documents which contain both words  X  X  X ate X  X  and  X  X  X ank X  X  occur 151 times, and that 31% of them include the category  X  X  X nterest X  X . 3.3. Classifier building
Mining frequent patterns generates numerous CARs. Those rules are not used directly in the prediction phase because the ing from the highest-ranked rule, they are matched to training documents rule-by-rule. The rule-matching operations ( Fig. 2 , nested For loop) consume most of the computation time. The rule that classifies a training document correctly is chosen as a member of a final set of classification rules (line 3-b-i), and training documents classified by the rule are deleted from the database (line 3-b-ii). 4. Two efficient algorithms 4.1. Memory-saving representation of rules
There are different methods to generate association rules from a database. Our approach is based on the FP-growth meth-documents. The items of the training documents are encoded as integers and stored in a node of the FP-tree ( Fig. 3 ). The item with the largest support value is encoded as  X 1 X . A path from the root node to a terminal node denotes a training document, and the nodes in the path are linked in ascending order of their integer codes.
 Each node also contains additional information including the support of a word item and the supports of classes assigned.
The oval node in the bottom of Fig. 3 shows such information: word pattern {2,4, ... ,17} occurs 43 times in the training doc-uments; of these instances, class A occurs 20 times, class B occurs 12 times, and class C occurs nine times.
After constructing a global FP-tree for the training documents, the algorithm generates association rules while recursively constructing sub FP-trees which are conditioned on a frequent word pattern that was mined previously. In this method, the number of generated rules increases exponentially with the depth of the recursive calls. Thus, the number of generated rules plus local variables accumulated by repetitive recursive calls can become too large to be stored in main memory. The gen-erated rules are written into a disk file, but occasionally even that file can exceed the disk X  X  capacity.

In the FP-growth method, frequent patterns are output redundantly as they have identical conditioning suffix of frequent patterns mined; this redundancy contributes significantly to the memory requirement. This paper proposes a rule-encoding and -decoding algorithm that reduces this redundancy and allows a huge number of association rules to be stored very effi-ciently. Our scheme represents a class association rule as follows: where w i denotes the words in the rule, supp a the support of the antecedent, c rule. In effect, Eq. (2) represents m rules that have identical antecedent words. The confidence of the rule with class label c s / supp a . The k -order word pattern can be further compacted using our first proposed algorithm ( Fig. 4 ).
Procedure SuffixOmission() saves only non-repeating parts of antecedent words c [ ], instead the whole itemset w [].Todo this, the procedure keeps the previously-input words in buffer b [ ] whose size is fixed to the maximum allowed rule length K .
Before generating association rules, all elements of b [ ] are initialized to zero. In SuffixOmission(), before the elements of input words w [ ] are compared with the buffer elements b [ ] (line 1), they are sorted in ascending order of the item codes of the header table ( Fig. 3 ).
 Encoding results for a bit of association rules were generated from Reuters-21578 document collection ( Apt X , Damerau, &amp;
Weiss, 1994 )( Table 1 ). Rule-order parameter K was set to 4 in this case. Each input of antecedent words (left column) is generated sequentially by the FP-growth algorithm, and the right column is output by SuffixOmission(). Most antecedent patterns can be encoded as just one item, rather than as k items (except for the case  X  X 2 5 17 X  X ), and this reduction in items saves a great amount of storage. In the expression (2) , class labels c exemplary compact form would look like: 3 &gt; 288 0 144 3 94, which corresponds effectively to two class association rules. When decoding a compact representation (Eq. (2) ) into a normal representation (Eq. (1) ), procedure SuffixRestoration() is called, which is simply SuffixOmission() run in reverse.
Theorem 1. The compact representation that is output by procedure SuffixOmission() contains all information about the association rules generated by the FP-growth method.

Proof. Because FP-growth processes items in descending order of their support values, and because procedure SuffixOmis-sion() stores the compactly-encoded rules c [ ] line-by-line in a file, SuffixOmission() can conserve the order in which the word patterns of the rules are generated. When an item is conditioned to generate a composite itemset in the FP-growth algorithm, FP-growth always inspects item nodes in a bottom-up manner: the items which will be inspected always have higher ranks (smaller integer codes) than the conditioned item. Procedure SuffixRestoration() searches the internal buffer otherwise SuffixRestoration inserts c [ ] at the front (line 2) so that the words in b [ ] can be kept in the generation order of FP-growth. Therefore, the conditioned item always exists in the buffer, and the whole buffer content coincides with the output originally generated by FP-growth. h 4.2. Fast rule matching mechanism
The process of matching the rules to training documents takes a considerable time (Section 3.3 ). This time can be greatly reduced by adopting a sophisticated data structure and an algorithm that applies it. A set of training documents is rearranged into a graph structure in which the word items are indexed and linked to each other ( Fig. 5 ). First, the word items of each training document are coded using their rank-id in the header table of the initial FP-tree ( Fig. 3 ). Then, they are sorted in ascending order of the codes, and stored into a list structure. Root node TrainDoc has a link to the word list (or a training document). A word node has a word rank code and a class label of the document. After all the training documents are con-the item nodes of the document lists one-by-one. This linking process resembles that of the FP-tree construction.
To determine whether a rule can classify a document correctly, the rule must be verified to be a subset of the document. If every element of the words in a rule occurs in the document, then the rule is a subset of the document. Matching a rule with k words to a document with l words using a naive method takes O ( kl ) time in the worst case, and the total time to process M rules with respect to N training documents reaches O ( kM lN ). We propose an algorithm ( Fig. 6 ) which allows N to be over also be greatly reduced.

Algorithm ReverseRuleMatching matches a rule to N training documents. The words in the rule are sorted, then the last word of the rule is compared to the last word of a document, and the comparison proceeds to the words with higher ranks. The variable q (line 2) is a pointer that indicates the word node of the next document to which rule r will be compared. The set, and are examined in turn. The While loop (line 3-e) determines whether the word list of r is a subset of the current doc-middle of the word lists, the algorithm stops the loop prematurely (the break statements), which reduces the number of com-parisons greatly.
 cluded in the final classification rule set (line 4). Then, the documents in ClsTrDoc are examined to be deleted from the data-occurs, the number of remaining documents diminishes very quickly. Several different methods give documents a second chance of surviving the pruning process although they have been classified correctly. One accumulates the confidence score of the rules which classified the document correctly, then compares the accumulated score with a given threshold value ( h ) to determine whether the document should be deleted ( Yoon &amp; Lee, 2008 ).

Increasing the hit score threshold h reduces the rate at which documents are deleted. Although this process requires more time in the classifier building process, classification accuracy can be better than that achieved using the simple deletion mined by a sort of grid search for some interval using an extra training data. The graph structure of the training documents tree has the merit of reducing the amount of memory required for storage of words, it shows poor efficiency in deleting train-ing documents and reorganizing node links.

We tried applying another data structure to conduct rule matching fast. The training documents are stored with an in-verted index. Then, as in the document retrieval task, given the words of a rule, matching documents are returned. The rule matching algorithm in the classifier build process using this inverted index approach is very simple ( Fig. 7 ). The document a document each time it is covered by a rule. In line 4(b), when the score exceeds the threshold, the document is to be de-leted. This algorithm does not delete the document but continues the processing for the next covered document. We could have deleted the document from the training set, and then reconstruct the inverted index structure of the document set.
However, this frequent re-indexing would cause an enormous increase in computing time. We present the experimental re-sults of rule matching using the inverted index in the next section.

Before showing the experimental result, we explain the effect of the distribution of words in a document collection. Fig. 8 shows the distribution of the support of the words in Reuters-21578, which has 7193 training documents. The x -axis rep-resent the 6837 words in the training set, ordered by their support ranks. Most of the words have a support value under 100. In other words, a word which appears in a rule has a very low probability of occurring in the training documents.
The pie chart in Fig. 8 represent this well: words which occur in a training document with probability p ( w ) larger than 0.01 have only 13% share in the entire vocabulary of the training documents. Our rule matching algorithm makes a good use of this distribution. In each iteration of matching to the document set, only about a thousandth of the entire document set is considered. In addition, the volume of the document set continues to decrease as documents being hit many times are deleted from the set. Those can decrease the time complexity of rule matching, O ( kM lN ), greatly by reducing N (the number of the training documents) to about one thousandth in average. 5. Experiment 5.1. Dataset
Three multi-class test collections were used for large-scale text categorization. Reuters-21578 is a collection of articles from the Reuters newswire; the ModApte split version ( Apt X  et al., 1994 ) was used, specifically, documents in the 10 TOPICS categories. The 20 Newsgroups collection ( Lang, 1995 ) is a collection of USENET mail postings from 20 discussion groups. A much larger dataset, the Chef Moz collection from ODP 2 was used. From this dataset, we selected documents of the top 16 the scalability of our two algorithms.

We used the BOW-toolkit ( McCallum, 1996 ) to preprocess the documents. In a document text, the header part except for the title was removed. We filtered out general stop words and conducted no stemming. Our associative classification system was implemented with C++ and Perl codes executed on a Linux machine with 4 GB memory and 2.8 GHz CPU speed. Source codes of BCAR can be downloaded at http://isoft.postech.ac.kr/ ywyoon/BCAR . 5.2. Parameter selection
We conducted a simulation to investigate the relation between the number of rules and the classification accuracy in the associative classification, using the Reuters-21578 collection and setting the maximum order of generated rules to three ( Fig. 9 ). BCAR ( Yoon &amp; Lee, 2008 ) was used as classifier, and the accuracy was measured using Breakeven Point (BEP).
The number of generated rules increased as min _ sup and min _ conf decreased ( Fig. 9 a). Because our purpose is to increase the classification accuracy by assuring that as many rules as possible are matched to a test document, the minimum support and the minimum confidence thresholds were set to low values. The min _ sup and min _ conf values in this experiment ( Table 3 ) are not necessarily equal to the values which leads to the best classification accuracy, but were chosen just as to express well the effect of the storage saving.

Although using a large number of classification rules produced the best classification performance ( Fig. 9 b), it also re-quired considerable computing time and storage capacity; this disadvantage should be resolved. Increasing the value of the maximum allowed rule order is not feasible due to generation of an excessive number of rules. In this experiment, rule order is set to 6 4. 5.3. Results 5.3.1. Compact representation of rules
In order to measure the effect of representing rules in a compact form (Section 4.1 ), the sizes of rule files were compared when applying two representation methods ( Table 3 ). In the Naive coding method, the antecedent part contains the original abbreviated form as in the expression (2) . In the table, we presented the number of rules, and compared the size of the files for the two methods, varying rule orders from one to four.

The Suffix Omission method produced rule files smaller than those of Naive coding by a great amount. The compaction was smaller than produced by Naive coding, because Suffix Omission writes several class labels in one line. When rule order was four, the file size produced by Suffix Omission was about half of that produced by Naive coding. This result implies that more classification rules can be generated in the same storage space using Suffix Omission than using Naive coding. On the total processing time which is taken for encoding/decoding and file writing, there is little difference between the Naive cod-ing and the Suffix Omission algorithm.

We conducted another experiment to show the efficiency of our Suffix Omission algorithm, compared with another rule compaction method, the frequent closed itemset (FCI) model. In this experiment, we used the test subset of the Reuters-21578 collection, set min _ conf to 50%, and put no limit on the maximum rule order. Varying the min _ sup value, we examined which we used in the experiment does not include a subset-expansion code, but we implemented that part at the end of the original code for our experiment. Thus, the figures in the column under Frequent Closed contain additionally the values when a closed form of rules are expanded to all their subsets. The file size of the FCI model is five to ten times larger than that of the Suffix Omission method. In the case of the processing time, until we have min _ sup 8 starting from 12, there is three times difference at most. However, if we handle a very large volume of rules such as in the case of min _ sup 5, the dif-ference ratio exceeds ten times. The Suffix Omission can process a large scale data more efficiently than other methods. 5.3.2. Time reduction in classifier building
Computing times for building classifier using two methods, the Naive matching and the ReverseRuleMatching algorithm, were compared ( Table 5 ). Test data sets and the rule generation parameters were the same as in Table 3 . Another parameter combined with each rule order.

The computing time in Table 5 represents the total time for the classifier building process, including the time for reading rules and training documents and for sorting the rules. The rule matching procedure consumes most of the computing time. The total rule matching time increases with the number of rules to be matched, and with h . The amount of time reduction by ReverseRuleMatching is remarkable: the ratio of time required by the Naive algorithm to that required by ReverseRule-Matching (reduction ratio) ranges from 3 to nearly 274.
 h ). On the whole, ReverseRuleMatching reduced computation time more for 20 Newsgroups than for Reuters-21578. The 2 ). Thus, the words of 20 Newsgroups are more-sparsely distributed in the training database than those of Reuters. Because of this difference, 20 Newsgroups has a smaller number of documents being linked for rule matching compared to Reuters-21578, which results in more time reduction in the classifier building process.

Time reduction is most remarkable in the Chef Moz dataset. The vocabulary size of Chef Moz is about the same as that of 20 Newsgroups, but the number of training documents is six times larger than that of 20 Newsgroups. Hence, the number of rules is four times bigger than 20 Newsgroups in the case of 2nd order rules. (We did not list the result of 4th order rules because the number of the rules was so huge that they could not be generated within a reasonable time.) The time of the
Naive matching in the Chef Moz data is very long, but the time of the Reverse rule matching was very short. The time reduc-tion ratio even reaches several hundreds. This shows that the larger the dataset is the more positive effect our Reverse rule matching algorithm can acquire.

The experimental result using the document set structured by the inverted index was presented in Table 6 . The min _ sup and min _ conf was 3 and 0.06 respectively. The computing times of the inverted index method did not change as h increased, whereas the time of the Reverse rule matching increased linearly. The efficiency performance of the inverted index method was a little better than that of the Naive method due to its word index structure. However, it was still inferior compared to that of the Reverse rule matching. The reason is that the size of the training document set, N , does not decrease as the rules are matched, although the document retrieval can be processed fast; matching documents just have their scores increased, but are not deleted from the inverted-index-structured document set. 6. Conclusion
Associative classification using a large number of classification rules can show good performance when applied to a large-scale database such as a text collection. However, such a large set of generated rules can cause computational efficiency problems, including shortage of memory space for storage of rules and an excessive amount of time to process rules and doc-uments. We propose two new efficient algorithms to address those efficiency-related problems. By storing rules using a very compact representation format, a considerable amount of storage space can be saved when a huge number of association rules are processed for classifier building. By constructing a training document set as an indexed document list structure, and by matching rules to the documents indexed by the word with the lowest support, the classifier building process can be completed in a very short time, compared to the previous naive methods. If these two algorithms are applied, the use of associative classifiers becomes feasible for large-scale text classification. The proposed approach can also be applied to other types of data than text.
 Acknowledgment This research was supported by the MKE (The Ministry of Knowledge Economy), Korea, under the ITRC(Information Technology Research Center) support program (NIPA-2012-(H0301-12-3001)) supervised by the NIPA (National IT Industry Promotion Agency).
 References
