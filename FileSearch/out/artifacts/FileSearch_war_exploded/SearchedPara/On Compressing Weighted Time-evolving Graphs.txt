 Existing graph compression techniques mostly focus on static graphs. However for many practical graphs such as social networks the edge weights frequently change over time. This phenomenon raises the question of how to compress dynamic graphs while maintaining most of their intrinsic structural patterns at each time snapshot. In this paper we show that the encoding cost of a dynamic graph is proportional to the heterogeneity of a three dimensional tensor that represents the dynamic graph. We propose an effective algorithm that compresses a dynamic graph by reducing the heterogeneity of its tensor representation, and at the same time also main-tains a maximum lossy compression error at any time stamp of the dynamic graph. The bounded compression error ben-efits compressed graphs in that they retain good approxima-tions of the original edge weights, and hence properties of the original graph (such as shortest paths) are well preserved. To the best of our knowledge, this is the first work that compresses weighted dynamic graphs with bounded lossy compression error at any time snapshot of the graph. H.2.8 [ Database Applications ]: Data mining Dynamic graphs, graph compression, graph mining.
An important intrinsic property of real graphs such as social networks is that the weights of their edges tend to continuously and irregularly change with time. The irreg-ular changes of weights make the compression of dynamic graphs more challenging than that of static graphs due to the additional dimension of time. For example, the size of the publication network maintained in DBLP 1 keeps grow-ing with time as new publications are being added to the http://dblp.uni-trier.de repository. If one wants to construct a who-published-with-whom dynamic network, one would have to download a large amount of data from the server to uncover all the historical publications. An efficient method for compressing these dy-namic graphs will not only save the cost of storage on the server side, but also reduce the communication cost needed for transmitting this information on the Internet.
Existing methods for compressing static graphs generally use two types of strategies: (1) removing edges to simplify the overall graph [4,8], or (2) merging nodes that have sim-ilar properties (such as common neighbors) [5,6]. While the existing methods compress a static graph from its  X  X patial X  (nodes or edges) perspective, in this paper we propose to compress a dynamic graph from both the  X  X patial X  and the  X  X emporal X  perspectives simultaneously.

Static graphs are usually represented by their adjacency matrices. Similarly, we characterize a dynamic graph by a three dimensional (3D) tensor (i.e., a 3D array: Vertices Vertices  X  Time ), where an entry of this tensor is an edge weight at a certain time stamp. Since large dynamic graphs are mostly sparse, when using tensors to represent dynamic graphs, we only have to encode the sparse version of a tensor to encode the entire dynamic graph. The sparse version of a tensor is a set of locations and values of non-zero entries in the tensor. While the locations of non-zero entries can be efficiently encoded by (for example) run-length encoding, in this research we put our focus on reducing the cost of encoding the values of the tensor X  X  non-zero entries.
For a small example, consider a co-authorship publication network that evolves in three time periods shown in Fig. 1(a) to 1(c), where edge weights are numbers of collaborated pa-pers. The original cost of encoding this dynamic graph is shown in Fig. 1(d). For visualization purposes, in this figure we demonstrate the 3D Vertices  X  Vertices  X  Time tensor by a 2D Edges  X  Time matrix. Our target is to merge cer-tain subsets of weights across all dimensions of the tensor so that the overall encoding cost of the graph is minimized. Fig. 1(e) is an example of the re-weighted graph which has lower heterogeneity of edge weights and lower encoding cost.
Since the reduction of the encoding cost of a tensor is from the homogenization of subsets of weights, a major challenge of compressing dynamic graphs becomes how to select ap-propriate subsets of weights and unify them while maintain-ing desirable properties (such as the shortest paths) at each time snapshot. In this research we introduce the use of hier-archical clusters of edge weights to address the weight sub-set selection problem. The main contributions we make in this paper are as follows: Section 3.
We define a weighted static graph by a triple G =( V , E , w ), where V is a set of vertices (nodes), E  X  V  X  V is a set of edges, and w(e)  X  R represents the set of non-negative weights assigned to all edges e  X  E . We assign zero weights to edges that do not exist.

An essential property of a static graph is the connectivity between two nodes. The quantification of how closely two nodes are connected can be examined by the shortest path between them. We use the notation u i P =  X  u j to represent a path from u i to u j (if the path exists). The weight of the shortest path ( W SP ) between the two nodes u i and u j can be expressed as: We use the average of shortest paths over all pairs of con-nected nodes in a static graph to measure the connectivity of that graph. Denote by P the set of all shortest paths in a graph that exist (i.e., excluding paths of infinity weights defined in Eq. 1), the average shortest path weight ( AvgSP ) of a graph is defined by: Now we give the definition of a dynamic graph:
Definition 1. A time-evolving (aka dynamic) graph DG is a sequence of static graphs, DG = { G 1 , G 2 , ..., G where G t = ( V , E , w ( e, t )) ,and w ( e, t ) assigns a weight to an edge e ( e  X  E ) at time stamp t (1  X  t  X  T ) . Intuitively, when G t is represented by its adjacency matrix of size | V | X | V | , a concatenation of G 1 through G T forms a tensor of size | V | X | V | X | T | . We denote the tensor of DG by T DG .
The locations and values of non-zeros entries of T DG re-spectively indicate the end-vertices and weights of all edges in DG . As a way of discretizing edge weights, we round values of these non-zeros entries to their closest integers (if they are not originally integers). These non-zeros entries are ordered by traversing along the time dimension first and then along the two vertex dimensions of the tensor. Simi-lar to the principle of the run-length coding, the locations can be specified by the number of zeros in the tensor be-tween adjacent non-zero entries, which makes the location information simply a sequence of integers (denoted by Loc ). We concatenate Loc with the sequence of values to obtain a single string LocVal , and encode it by an entropy encoding method (e.g., arithmetic encoding [7]) as follows.
Let Int ( x ) be a bit string that encodes a positive integer x ,and L ( s ) be the length of a bit string s .Therearediffer-ent possible implementations of Int ( x ). We use the common variable length quantity (VLQ) format used in MIDI files, so L ( Int ( x )) is 8  X  log 2 ( x +1) / 7 bits. We first encode the tensor X  X  dimensions, and then encode its entries, i.e., DG = Int ( | V | ) Int ( | T | ) Meta ( LocV al ) LocV al . The string Meta ( LocV al ) contains all k unique integers x i and their frequencies freq i (1  X  i  X  k )from LocVal . Therefore L ( Meta ( LocV al )) = k i =1 L ( Int ( x i )) + L ( Int ( freq The Shannon entropy estimates the lowest number of bits required for encoding a source string [1]. The entropy of total, a dynamic graph DG in our representation requires H ( LocV al ) bits to be encoded. This graph encoding proce-dure is stated as X  X ncode( T DG ) X  X n the algorithm introduced in the next section.

Since the costs of the first and the second terms of L ( DG ) Algorithm 1 MaxErrComp Require: A dynamic graph DG = { G 1 , G 2 , ..., G T } ,andan error threshold . 1: Compute a hierarchical cluster tree ClusterTree on non-2: Initialize cutoff  X  0; 3: Compute original average shortest path weights: 4: while cutoff has not reached the root of ClusterTree 5: Increase cutoff by 1; 6: Obtain weight clusters from ClusterTree by applying 7: for each cluster of weights CL do 8: CL old  X  CL ; 9: w i  X  round(mean( CL )),  X  w i  X  CL ; 10: NewSP t  X  AvgSP ( G t ) ,  X  t  X  [1 ,T ]; 12: Restore weights: w i  X  ( CL old ) i ,  X  w i  X  CL ; 13: return Encode( T DG ); 14: end if 15: end for 16: end while 17: return Encode( T DG ); are fixed, our method aims to reduce the cost of the third term L ( Meta ) by generating fewer unique weight integers, and the fourth term H ( LocV al ) by decreasing the hetero-geneity of the bit string.

The heterogeneity of a tensor is straightforwardly mini-mized when all its non-zero entries are set to a constant value across different edges. From this perspective, the  X  X verage X  of all weights of a dynamic graph, which we call the  X  X ver-age graph X , provides a lower bound for the encoding cost of the dynamic graph. However, simply setting all weights to their average value loses the inherent variances and dynam-ics in all snapshots of the graph, which is detrimental to the analysis of the temporal behavior of graphs. Therefore, in the next section we propose an effective algorithm that finds appropriate tradeoffs between the original graph and the av-erage graph, which reduces the encoding cost and preserves desirable temporal properties simultaneously.
Our dynamic graph compressi on algorithm  X  X erges X  sub-sets of edge weights by assigning a common (average) weight to them. The subsets of weights being merged are selected using agglomerative hierarchical cluster trees [2] built on non-zero entries of the dynamic graph X  X  tensor representa-tion. We note that we only need to build the cluster tree on unique weight values (instead of all weight values), since identical weights are always clustered together without dis-tance checking. In Alg. 1 we present the MaxErrComp algo-rithm which reduces the cost of encoding a dynamic graph, and at the same time provides error-bounded changes on average shortest path weights for all time snapshots of the dynamic graph.

The MaxErrComp algorithm runs in a greedy fashion, which gradually increases the clustering cutoff value to obtain clus-ters from the hierarchical cluster tree. The cutoff value is the maximum difference allowed among the weights within each cluster. The cost of encoding the dynamic graph is reduced by averaging the weights that belong to the same cluster (line 9 in Alg. 1). Similar to the original weights, we round new weights to their nearest integers. The algorithm checks the max error of average shortest path weights among all snapshots of the dynamic graphs at each iteration. As soon as the compression error reaches the threshold ,the algorithm terminates the iterations and the weights that are changed in the last iteration roll back to their previous values (lines 11 to 14), so the compression error is always bounded by . If the dynamic graph does not change much across time or if the threshold is set to a high value, the program would run to the last line of Alg. 1 and potentially produces the average graph whose error could be lower than . It can be observed from line 11 of Alg. 1 that our Max-ErrComp algorithm can be application-generic : by changing the termination condition to other properties of graphs (such as changes to communities of vertices), one can straight-forwardly generalize MaxErrComp to preserve other types of properties of a dynamic graph.

Compared to existing methods that compress static graphs (i.e., compress one snapshot at a time in a dynamic graph), a major advantage of MaxErrComp is that it takes into account both the spatial and the temporal information of a dynamic graph, and thus the cost of encoding all dimensions of the dynamic graph X  X  tensor representation can be reduced.
We present a baseline method ( MaxErrRandom )thataver-ages random partitions of edge weights. The MaxErrRandom algorithm is similar to the MaxErrComp algorithm in terms of their terminating conditions. The difference is that instead of choosing weights from cluster trees, MaxErrRandom ran-domly selects a partition of weights, and sets all weights in each partition to their average value. The number of parti-tions used in MaxErrRandom decreases by 1 at each iteration from the total number of edges until it reaches 1, or until the terminating condition is met.
In our evaluations we include a recently proposed static-graph compression method [6], which we denote by Stat-Comp .Since StatComp does not provide a strategy for com-pressing dynamic graphs, we measure the encoding costs of a dynamic graph compressed by StatComp using the cost of encoding all of its snapshots that are separately compressed by StatComp . In this way, we can evaluate the scope for compressing temporal information of dynamic graphs.
We use two types of dynamic graphs to validate our algo-rithm. The first is a co-authorship network extracted from the DBLP bibliography. We select co-authors who have data mining publications from year 2000 to 2011. We select data mining journals and conferences from the venues whose full book or proceeding titles contain the phrases  X  X ata mining X  or X  X nowledge discovery X , and only include authors who have at least 5 publications through the 12 years period. The sec-ond data set is extracted from the Enron email network [3], which provides email connections from senior executives to other employees in the Enron company. Statistics of these two datasets are shown in Table 1.
We first evaluate our method under different settings of er-ror thresholds. Comparisons on the performance of the Max-Figure 2: Comparisons on the degree of compression, in terms of the reduction of encoding costs. Values correspond to  X  =0 X  X nthe x-axis are the encoding costs before lossy compression, and those correspond to  X  &gt; 0 X  are the encod-ing costs after compression.
 Table 2: Comparisons on the quality of compression, in terms of the error on compressed average shortest path weight, at each snapshot of the DBLP data set.
 ErrComp , MaxErrRandom and StatComp methods are shown in Fig. 2. The scenario of  X  = 0 X  on this figure represents the encoding cost of the original dynamic graph before lossy compression. We can observe that on a fixed error threshold, the cost of encoding a dynamic graph using MaxErrComp is al-ways lower than that using MaxErrRandom before they reach a common stationary point (i.e., the average graph). This phenomenon demonstrates that on the same error thresh-old, the subsets of weights selected by hierarchical cluster trees are more effective in reducing the encoding costs of dy-namic graphs. We can also observe that the encoding costs of the StatComp method are generally higher than those of the MaxErrComp and MaxErrRandom methods, and the com-lowest among the three methods.

It is also important to examine whether the temporal properties of each time snapshot of the dynamic graphs are preserved after compression. For this purpose, we change the termination condition of Alg. 1 (line 12) to a compression compare the compression error made on the average short-est path weight in every snapshot of the dynamic network, where the error is defined by | 1  X  Compressed avg. path weight As shown in Table 2, the shortest path weights preserved by MaxErrComp are the most accurate among the three meth-ods at all times. Due to space limits, in Table 2 we only present comparions of methods on the DBLP data set. Re-sults obtained from the Enron data set lead to the same conclusions to those of the DBLP data. To confirm the su-periority of MaxErrComp on compression quality, we apply paired t -tests on the errors made by the three compression methods, under the null hypothesis that their errors are not significantly different. From the low p -values in the bottom three rows of Table 2, we can confidently reject the null hy-pothesis. This suggests that under the same compression ratio, the compression quality preserved by MaxErrComp is significantly better than the other two methods.
We propose to encode a dynamic graph by encoding its tensor representation, and compress the dynamic graph by reducing the heterogeneity of the tensor. We have designed a compression algorithm that decreases the heterogeneity of the tensor entries by using hierarchical cluster trees built on the time-stamped edge weights. This algorithm is highly generic and can be easily generalized to preserve various properties of the original dynamic graph while compressing it. We have exemplified weights of shortest paths in all time snapshots of a dynamic graph as a desired property to main-tain, and with this property we have empirically tested our method on the compression of a co-authorship network and an email communication network.

In future we would like to apply our method to numerical edge weights by using differential entropy.
 This research was supported under Australian Research Coun-cil X  X  Discovery Project s funding sche me (DP110102621). [1] T. Cover and J. Thomas. Elements of Information . [2] A. Fern  X  andez and S. G  X  omez. Solving non-uniqueness in [3] B. Klimt and Y. Yang. Introducing the enron corpus. In [4] H. Maserrat and J. Pei. Neighbor query friendly [5] S. Navlakha, R. Rastogi, and N. Shrivastava. Graph [6] H. Toivonen, F. Zhou, A. Hartikainen, and A. Hinkka. [7] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic [8] F. Zhou, S. Malher, and H. Toivonen. Network
