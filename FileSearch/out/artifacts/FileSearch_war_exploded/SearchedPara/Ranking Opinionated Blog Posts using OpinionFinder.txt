 The aim of an opinion finding system is not just to retrieve relevant documents, but to also retrieve documents that ex-press an opinion towards the query target entity. In this work, we propose a way to use and integrate an opinion-identification toolkit, OpinionFinder, into the retrieval pro-cess of an Information Retrieval (IR) system, such that opin-ionated, relevant documents are retrieved in response to a query. In our experiments, we vary the number of top-ranked documents that must be parsed in response to a query, and investigate the effect on opinion retrieval per-formance and required parsing time. We find that opin-ion finding retrieval performance is improved by integrating OpinionFinder into the retrieval system, and that retrieval performance grows as more posts are parsed by Opinion-Finder. However, the benefit eventually tails off at a deep rank, suggesting that an optimal setting for the system has been achieved.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Performance, Experimentation Keywords: Opinion finding, Blogs
The advent of the opinion finding task in the TREC Blog track [2, 4] has increased research interest in the retrieval of opinionated blog posts. Two opinion identification ap-proaches appear to be effective: firstly approaches based on the presence of opinionated terms in the retrieved doc-uments, and secondly, looking for more complex subjec-tive sentences structures, as identified using machine learn-ing techniques. We propose an approach that belongs to the latter. It is based on OpinionFinder (OF), which is a freely available toolkit for identifying subjective sentences in text [8]. Two Naive Bayes classifiers are applied that dis-tinguish between subjective and objective sentences using a variety of lexical and contextual features. The classifiers have been trained using subjective and objective sentences, which have been automatically generated from a corpus of un-annotated data by rule-based classifiers [7]. In this pa-per, we propose applying OF for retrieving opinionated blog posts, and show how simple evidence from OF can be utilised for effective opinion finding performance. Moreover, as OF is a computationally expensive technique to apply, we vary the number of retrieved blog posts for which we apply it, to measure its effect on opinion finding retrieval performance. We use the Terrier IR platform for both indexing and re-trieval [5]. We index only the blog posts and their associated comments, as this is the retrieval unit of the TREC opinion finding task. Each term is stemmed using Porter X  X  stemmer, and standard English stopwords are removed. Moreover, we index each field (content, title and anchor text of incoming hyperlinks) separately, and use the PL2F field-based docu-ment weighting model. PL2F is a combination of the Di-vergence from Randomness (DFR) PL document weighting model and Normalisation 2F for weighting fields [1].
We use the 100 title-only topics from the TREC 2006 &amp; 2007 opinion finding tasks, numbered from 851 to 950. We use the 50 topics from the opinion finding task in 2006 for training, to set the parameter k of Equation (3) and the pa-rameters of PL2F [1]. In particular we use k = 100. For evaluation on the 50 topics from TREC 2007, we use opin-ion finding MAP, where the retrieved posts must not only be relevant to the query but also express an opinion about the target [2, 4].

The purpose of our experiments is to evaluate our pro-posed opinion blog post retrieval method using OF. In par-ticular, we examine to which extent the effectiveness of our proposed approach is affected by the parsing depth, i.e. the number of the top returned documents parsed by OF. On top of the retrieval baseline, i.e. the PL2F weighting model, we apply our proposed method using OF described in Sec-tion 2, with different parsing depths of the retrieved docu-ments. The parsing depths tested increase from 10 to 1200 with an interval of 10. For example, if parsing depth is 20, only the top 20 returned documents for each query are parsed using OF.

However, using OF is very computationally expensive. In-deed, the parsing of 150 documents takes approximately 1 hour of CPU time of a Pentium III 1GHz node (NB: We improved the efficiency of OF, and these improvements are now part of version 1.5). Hence to perform our experiments, many such machines were applied to parse all of the retrieved blog posts.

Figure 1 plots the parsing depth and its corresponding re-trieval performance. Parsing depth zero is equivalent to our retrieval baseline using PL2F only. From Figure 1, we ob-serve a strong correlation between the parsing depth and the resulting MAP retrieval performance -i.e. MAP increases with OF X  X  parsing depth. The linear correlation between MAP and the parsing depth is  X  =0 . 9997, which indicates an almost perfect correlation. It is also encouraging to see that our proposed approach outperforms the baseline with all the different parsing depths applied (from baseline 0.2817 to 0.3301 MAP). In contrast, for P@10, applying OF to just the top 60 ranked documents results in an increase of 22% over the baseline, while applying OF to more documents has no further effect on P@10. Applying OF to only the top 10 ranked documents results in a statistically significant increase in P@10 (Wilcoxon Signed Rank Test, p  X  0 . 05), while applying to the top 60 documents results in a signifi-cant increase in MAP.

Figure 1 also shows the parsing time in CPU hours on 1 Pentium III 1GHz processor for various parsing depths -the time taken is strictly linear. However, while a marked improvement in performance is observed for parsing only 200-400 posts, as the depth of OF increases the growth of MAP decreases, especially when compared to the constant growth in parsing time. Indeed, after about 1000 posts there is very little improvement in MAP for parsing more posts, while parsing time continues to grow linearly. Note that
