 Andr  X e F. T. Martins  X  X  afm@cs.cmu.edu M  X ario A. T. Figueiredo  X  mario.figueiredo@lx.it.pt Pedro M. Q. Aguiar ] aguiar@isr.ist.utl.pt Noah A. Smith  X  nasmith@cs.cmu.edu Eric P. Xing  X  epxing@cs.cmu.edu There has been recent interest in kernels on probabil-ity distributions, to tackle several classification prob-lems (Moreno et al. , 2003; Jebara et al. , 2004; Hein &amp; Bousquet, 2005; Lafferty &amp; Lebanon, 2005; Cuturi et al. , 2005). By mapping data points to fitted dis-tributions in a parametric family where a kernel is de-fined, a kernel is automatically induced on the original input space. In text categorization, this appears as an alternative to the Euclidean geometry inherent to the usual bag-of-words vector representations. In fact, approaches that map data to a statistical manifold, where well-motivated non-Euclidean metrics may be defined (Lafferty &amp; Lebanon, 2005), outperform SVM classifiers with linear kernels (Joachims, 1997). Some of these kernels have a natural information theoretic interpretation, creating a bridge between kernel meth-ods and information theory (Cuturi et al. , 2005; Hein &amp; Bousquet, 2005).
 We reinforce that bridge by introducing a new class of kernels rooted in nonextensive (NE) information the-ory. The Shannon and R  X enyi entropies (R  X enyi, 1961) share the extensivity property: the joint entropy of a pair of independent random variables equals the sum of the individual entropies. Abandoning this property yields the so-called NE entropies (Havrda &amp; Charv  X at, 1967; Tsallis, 1988), which have raised great interest among physicists in modeling certain phenomena ( e.g. , long-range interactions and multifractals) and as gen-eralizations of Boltzmann-Gibbs statistical mechanics (Abe, 2006). NE entropies have also been recently used in signal/image processing (Li et al. , 2006) and other areas (Gell-Mann &amp; Tsallis, 2004).
 The main contributions of this paper are:  X  Based on the new concept of q -convexity and  X  We propose a broad family of positive definite  X  We extend results of Hein and Bousquet (2005) by Our main purpose is to present new theoretical insights about kernels on measures by unifying some well-known instances into a common parametrized family. This family allows reinterpreting these kernels in light of NE information theory, a connection that to our knowledge had not been presented before. The fact that some members of this family are novel pd kernels leads us to include a set of text categorization experi-ments that illustrates their effectiveness.
 The paper is organized as follows. Sec. 2 reviews NE entropies, while Jensen differences and divergences are discussed in Sec. 3. In Sec. 4, the concepts of q -differences and q -convexity are introduced and used to define the Jensen-Tsallis q -difference. Sec. 5 presents the new family of entropic kernels. Sec. 6 reports ex-periments on text categorization and Sec. 7 presents concluding remarks and future research directions. Although, for simplicity, we focus on discrete distribu-tions on finite sets, most results are valid in arbitrary measured spaces, as shown by Martins et al. (2008). Let X denote a random variable (rv) taking values in a finite set X = { x 1 ,...,x n } according to a probability distribution P X . An entropy function is said to be extensive if it is additive over independent variables. For example, the Shannon entropy (Cover &amp; Thomas, 1991), H ( X ) ,  X  E [ln P X ], is extensive: if X and Y are independent, then H ( X,Y ) = H ( X )+ H ( Y ). Another example is the family of R  X enyi entropies (R  X enyi, 1961), parameterized by q  X  0, which includes Shannon X  X  entropy as a special case when q  X  1.
 In classic information theory, extensivity is considered desirable, and is enforced axiomatically (Khinchin, 1957), to express the idea borrowed from thermo-dynamics that  X  X ndependent systems add their en-tropies. X  In contrast, the Tsallis entropies abandon the extensivity requirement (Tsallis, 1988). These NE entropies, denoted S q ( X ), are defined as follows:
S q ( X ) ,  X  E q (ln q P X ) = called q -logarithm . It is noteworthy that when q  X  1, we get E q  X  E , ln q  X  ln, and S q  X  H ; i.e. , the family of Tsallis entropies also includes Shannon X  X  entropy. For the Tsallis family, when X and Y are independent, extensivity no longer holds; instead, we have
S q ( X,Y ) = S q ( X )+ S q ( Y )  X  ( q  X  1) S q ( X ) S q where the parameter q  X  0 is called entropic index . While statistical physics has been the main applica-tion of Tsallis entropies, some attempts have been made to produce NE generalizations of classic infor-mation theory results (Furuichi, 2006). As for the Shannon entropy, the Tsallis joint and conditional en-tropies are defined as S q ( X,Y ) ,  X  E q [ln q P XY ] and S ( X | Y ) ,  X  E q [ln q P X | Y ], respectively, and follow a chain rule S q ( X,Y ) = S q ( X ) + S q ( Y | X ). Similarly, Furuichi (2006) defines the Tsallis MI as generalizing (for q &gt; 1) Shannon X  X  MI. This NE version of the MI underlies one of the central contributions of this paper: the Jensen-Tsallis q -difference (Sec. 4). For reasons that will become clear in Sec. 5, it is conve-nient to extend the domain of Tsallis entropies to un-normalized measures, i.e. , in R n + , {  X   X  R n |  X  i  X  i 0 } , but not necessarily in the probability simplex P Tsallis entropy of a measure  X  in R n + is 1 where  X  q : R +  X  R is given by  X  q ( y ) =  X  y q ln q y = This extension does not add expressive power, since function (5) is completely determined by its values on P n  X  1 , as shown by the following proposition (the proof is straightforward).
 Proposition 1 The following denormalization for-mula holds for any c  X  0 and  X   X  R n + : where k  X  k 1 , P n i =1  X  i is the ` 1 -norm of  X  . This fact will be used in a constructive way in Sec. 5 to devise a family of pd NE entropic kernels. Jensen X  X  inequality is at the heart of many important results in information theory. Let the rv Z take val-ues on a finite set Z . Jensen X  X  inequality states that if f is a convex function defined on the convex hull of Z , then f ( E [ Z ])  X  E [ f ( Z )]. The nonnegative quan-tity E [ f ( Z )]  X  f ( E [ Z ]) is known as Jensen difference and has been studied by Burbea and Rao (1982) when  X  f is some form of generalized entropy. Here, we are interested in the case where Z  X  {  X  1 ,...,  X  m } is a random measure , where each  X  j  X  R n + , with probabil-ities  X  = (  X  1 ,..., X  m )  X  P m  X  1 . The Jensen difference induced by a (concave) generalized entropy  X  is
J
 X  (  X  1 ,...,  X  m ) ,  X  Below, we show examples of Jensen differences that have been applied in machine learning. In Sec. 4, we provide a NE generalization of the Jensen difference. Jensen-Shannon (JS) Divergence Consider a classification problem with m classes, Y  X  Y = { 1 ,...,m } , with a priori probabilities  X  = (  X  1 ,..., X  m )  X  P m  X  1 . Let p j = ( p j 1 ,...,p jn )  X  for j = 1 ,...,m , where p ji , P ( X = x i | Y = j ), be the corresponding class-conditional distributions. Letting  X  in (8) be H , the Shannon entropy, the result-ing Jensen difference J  X  H ( p 1 ,..., p m ) is known as the JS divergence of p 1 ,..., p m , with weights  X  1 ,..., X  m (Burbea &amp; Rao, 1982; Lin, 1991). In this instance of the Jensen difference, where I ( X ; Y ) = H ( X )  X  H ( X | Y ) is the MI between X and Y (Banerjee et al. , 2005).
 For m = 2 and  X  = ( 1 2 , 1 2 ), we denote the ensuing J H ( p 1 , p 2 ) as JS ( p 1 , p 2 ):
JS ( p 1 , p 2 ) = H (( p 1 + p 2 ) / 2)  X  ( H ( p 1 ) + H ( p It can be shown that that equality and is a Hilbertian metric 2 (Endres &amp; Schin-delin, 2003; Tops X e, 2000), which has motivated its use in kernel-based machine learning. Jensen-R  X enyi (JR) Divergence Let  X  = R q , which is concave for q  X  [0 , 1); then, (8) becomes We call J  X  R  X  = (1 / 2 , 1 / 2), we write J  X  R
JR q ( p 1 , p 2 ) = R q The JR divergence has been used in signal processing applications (Karakos et al. , 2007). We show in Sect. 5.3 that p JR q is also an Hilbertian metric. Jensen-Tsallis (JT) Divergence Divergences of the form (8), with  X  = S q , are known as JT diver-gences (Burbea &amp; Rao, 1982) and were recently used in image processing (Hamza, 2006). Unlike the JS di-vergence, the JT divergence lacks a MI interpretation; in Sec. 4, we introduce an alternative to the JT diver-gence, which is interpretable as a NE MI in the sense of Furuichi (2006). We now introduce Jensen q -differences , a generaliza-tion of Jensen differences. As described shortly, a spe-cial case of the Jensen q -difference is the Jensen-Tsallis q -difference , which is an NE generalization of the JS divergence, and provides the building block for the NE entropic kernels to be introduced in Sec. 5. We be-gin by introducing the concept of  X  q -convexity X , which satisfies a Jensen-type inequality.
 Definition 1 Let q  X  R and X a convex set. A func-tion f : X  X  R is q -convex if, for any x,y  X  X and  X   X  [0 , 1] , f is q -concave if  X  f is q -convex.
 Naturally, 1-convexity is the usual convexity. The next proposition states the q -Jensen inequality and is easily proved by induction, like the standard Jensen inequal-ity (Cover &amp; Thomas, 1991). It also states that the property of q -convexity gets stronger as q increases. Proposition 2 If f : X  X  R is q -convex and f  X  0 , then, for any n  X  N , x 1 ,...,x n  X  X and  X   X  P n  X  1 : Moreover, if q  X  r  X  0 , we have: Based on the q -Jensen inequality, we can now consider Jensen q -differences of the form E q [ f ( Z )]  X  f ( E [ Z ]), which are nonnegative if f is q -convex. As in Sec. 3, we focus on the scenario where Z is a random measure and  X  f =  X  is an entropy function, yielding
T The Jensen q -difference is a deformation of the Jensen 1-difference (8), in which the second expectation is re-placed by a q -expectation. We are now ready to intro-duce the class of Jensen-Tsallis q -differences. Jensen-Tsallis q -Differences Consider again the classification problem used in the description of the JS divergence, but replacing the Jensen difference with the Jensen q -difference and the Shannon entropy with the Tsallis q -entropy, i.e. , letting  X  = S q in (15). We obtain (writing T  X  q,S T where S q ( X | Y ) is the Tsallis conditional q -entropy, and I q ( X ; Y ) is the Tsallis MI (cf. (4)). Note that (16) is an NE analogue of (9), i.e. the Jensen-Tsallis q -differences are NE mutual informations.
 We call T  X  q ( p 1 ,..., p m ) the Jensen-Tsallis q -difference of p 1 ,..., p m with weights  X  1 ,..., X  m .
 When m = 2 and  X  =(1 / 2 , 1 / 2), define T q , T (1 / 2 , 1 / 2)
T q ( p 1 , p 2 ) = S q Three special cases are obtained for q  X  { 0 , 1 , 2 } :
S 0 ( p ) =  X  1 + k p k 0 ; T 0 ( p 1 , p 2 ) = 1  X  k p 1 p
S 2 ( p ) = 1  X   X  p , p  X  ; T 2 ( p 1 , p 2 ) = where k x k 0 is the number of nonzeros in x , denotes the Hadamard-Schur (elementwise) product, and  X  X  ,  X  X  is the inner product.
 The JT q -difference is an NE generalization of the JS divergence, and some of the latter X  X  properties are lost in general. Since Tsallis entropies are 1-concave, Prop. 2 guarantees q -concaveness only for q  X  1. Therefore, nonnegativity is only guaranteed for JT q -differences when q  X  1; for this reason some authors only consider this range of values (Furuichi, 2006). Moreover, unless q = 1 (the JS divergence), it is not generally true that T  X  q ( p ,..., p ) = 0 or even that T q ( p ,..., p , p can be different from p 2 , unless q = 1. In general, the minimizer is closer to either the uniform distribution (if q  X  [0 , 1)) or a degenerate distribution 3 (for q  X  (1 , 2]). For these reasons, the term  X  X ivergence X  is misleading and we use the term  X  X ifference. X  Other properties of JT q -differences (convexity, lower/upper bounds) are studied by Martins et al. (2008). Using the denormalization formula (7), we now intro-duce kernels based on the JS divergence and the JT q -difference, which allow weighting their arguments. In this section, m = 2 (kernels involve pairs of measures). 5.1. Background on Kernels We begin with some basic results on kernels (Sch  X olkopf &amp; Smola, 2002). Below, X denotes a nonempty set; R + denote the nonnegative reals, and R ++ , R + \ { 0 } . Definition 2 Let  X  : X  X  X  X  R be a symmetric function, i.e.,  X  ( y,x ) =  X  ( x,y ) , for all x,y  X  X .  X  is called a pd kernel if and only if for any integer n , x i ,...,x n  X  X and c i ,...,c n  X  R A symmetric function  X  : X  X  X  X  R is called a negative definite (nd) kernel if and only if for any integer n , x i ,...,x n  X  X and c i ,...,c n  X  R satisfying the additional constraint P i c i = 0 . In this case,  X   X  is called conditionally pd; obviously, positive definiteness implies conditional positive definiteness. Both the sets of pd and nd kernels are closed un-der pointwise sums/integrations, the former being also closed under pointwise products; moreover, both sets are closed under pointwise convergence. While pd ker-nels correspond to inner products via embedding in a Hilbert space, nd kernels that vanish on the diagonal and are positive anywhere else, correspond to squared Hilbertian distances. These facts, and the following ones, are shown by Berg et al. (1984). Proposition 3 Let  X  : X  X  X  X  R be a symmetric function, and x 0  X  X . Let  X  : X  X  X  X  R be  X  ( x,y ) =  X  ( x,x 0 ) +  X  ( y,x 0 )  X   X  ( x,y )  X   X  ( x Then,  X  is pd if and only if  X  is nd.
 Proposition 4 The function  X  : X  X  X  X  R is a nd kernel if and only if exp(  X  t X  ) is pd for all t &gt; 0 . Proposition 5 The function  X  : X  X  X  X  R + is a nd kernel if and only if ( t +  X  )  X  1 is pd for all t &gt; 0 . Proposition 6 If  X  is nd and  X  ( x,x )  X  0 , for all x  X  X , then so are  X   X  , for  X   X  [0 , 1] , and ln(1 +  X  ) . Proposition 7 If f : X  X  R satisfies f  X  0 , then, for 5.2. Jensen-Shannon and Tsallis Kernels The basic result that allows deriving pd kernels based on the JS divergence and, more generally, on the JT q -difference, is the fact that the denormalized Tsal-lis q -entropies are nd functions 4 on R n + , for q  X  [0 , 2]. Of course, this includes the denormalized Shannon en-tropy as a particular case, corresponding to q = 1. Partial proofs are given by Berg et al. (1984), Tops X e (2000), and Cuturi et al. (2005); we present here a complete proof.
 Proposition 8 For q  X  [0 , 2] , the denormalized Tsal-lis q -entropy S q is an nd function on R n + . Proof: Since nd kernels are closed under pointwise summation, it suffices to prove that  X  q (see (6)) is nd on R + . For q 6 = 1,  X  q ( y ) = ( q  X  1)  X  1 ( y  X  y q q  X  [0 , 1),  X  q equals  X   X  +  X  q times a positive constant, where  X  is the identity (  X  ( y ) = y ) on R + . Since the set of nd functions is closed under sums, we only need to show that both  X   X  and  X  q are nd, which is easily seen from the definition; besides, since  X  is nd and nonnegative, Prop. 6 implies that  X  q is also nd. For q  X  (1 , 2],  X  q equals  X   X   X  q times a positive constant. It remains to show that  X   X  q is nd for q  X  (1 , 2]; since k ( x,y ) =  X  ( x + y ) q is nd (Prop. 7), so is  X  q . For q = 1, since the set of nd functions is closed under limits,  X  ( x ) =  X  H ( x ) =  X  x ln x = lim it follows that  X  1 is nd.
 The following proposition, proved by Berg et al. (1984), will also be used below. Proposition 9 The function  X  q : R ++  X  R , defined as  X  q ( y ) = y  X  q is pd, for q  X  [0 , 1] . We now present the main contribution of this section, the family of weighted JT kernels , generalizing the JS divergence kernels in two ways: (i) they apply to un-normalized measures (equivalently, they allow weight-ing the arguments differently); (ii) they extend the MI nature of the JS divergence kernel to the NE case. Definition 3 (weighted Jensen-Tsallis kernels) The kernel e k q : ( R n + ) 2  X  R is defined as e k q (  X  1 ,  X  2 ) = e k q (  X  1 p 1 , X  2 p 2 ) where p 1 =  X  1 / X  1 and p 2 =  X  2 / X  2 are the normalized counterparts of  X  1 and  X  2 , with corresponding weights  X  , X  2  X  R + , and  X  = (  X  1 / (  X  1 +  X  2 ) , X  2 / (  X  1 +  X  The kernel k q : ( R n ++ ) 2  X  R is defined as k q (  X  1 ,  X  2 ) = k q (  X  1 p 1 , X  2 p 2 ) , S q (  X  )  X  T Recalling (16), notice S q ( Y )  X  I q ( X ; Y ) = S q ( Y | X ) can be interpreted as the Tsallis posterior conditional entropy . Hence, k q can be seen (in Bayesian classi-fication terms) as a NE expected measure of uncer-tainty in correctly identifying the class given the prior  X  = (  X  1 , X  2 ) and a random sample from the mixture distribution  X  1 p 1 +  X  2 p 2 . The more similar the two distributions are, the greater this uncertainty. Proposition 10 The kernel e k q is pd, for q  X  [0 , 2] . The kernel k q is pd, for q  X  [0 , 1] .
 Proof: With  X  1 =  X  1 p 1 and  X  2 =  X  2 p 2 and using the denormalization formula (7), we obtain e k q (  X  1 ,  X  2 ) = with  X  = S q (which is nd by Prop. 8), x =  X  1 , y =  X  2 , and x 0 = 0 (the null measure). Observe now that k of two pd kernels is a pd kernel and (Prop. 9) (  X  1 +  X  )  X  q is a pd kernel, for q  X  [0 , 1], k q is pd. As we can see, the weighted JT kernels have two in-herent properties: they are parameterized by the en-tropic index q and they allow their arguments to be unbalanced, i.e. , to have different weights  X  i . We now mention some instances of kernels where each of these degrees of freedom is suppressed.
 Weighted JS Kernel Setting q = 1, we obtain an extensive subfamily that contains unbalanced versions of the JS kernel (Hein &amp; Bousquet, 2005). Namely, we get the pd kernels: Exponentiated Weighted JS Kernel Using Prop. 4, we have that exponentiated weighted JS ker-k
EWJS (  X  1 ,  X  2 ) , exp[ tk 1 (  X  1 ,  X  2 )] is also pd for any t &gt; 0. This generalizes the exponen-tiated JS kernel k EJS ( p 1 , p 2 ) , exp [  X  tJS ( p 1 , p (Cuturi et al. , 2005).
 We now keep q  X  [0 , 2] but consider the weighted JT kernel family restricted to normalized measures, k weights (  X  1 =  X  2 = 1 / 2); note that in this case e k q and k q collapse into the same kernel, e k q ( p 1 , p 2 ) = k q ( p 1 , p 2 ) = ln q (2)  X  T q ( p Prop. 10 tells us that these kernels are pd for q  X  [0 , 2]. Remarkably, we recover three well-known particular cases for q  X  { 0 , 1 , 2 } .
 Jensen-Shannon kernel (JSK) For q = 1, we ob-tain the JS kernel, k JS : ( P n  X  1 ) 2  X  R , introduced and shown pd by Hein and Bousquet (2005).
 Boolean kernel For q = 0, we obtain the kernel k Linear kernel For q = 2, we obtain the kernel k 2 = k Summarizing, Boolean, JS, and linear kernels, are members of the much wider family of Tsallis kernels, continuously parameterized by q  X  [0 , 2]. Further-more, Tsallis kernels are a particular subfamily of the even wider set of weighted Tsallis kernels.
 A key feature of our generalization is that the kernels are defined on unnormalized measures. This is rele-vant for empirical measures ( e.g. , term counts, image histograms); instead of the usual normalization (Hein &amp; Bousquet, 2005), these empirical measures may be left unnormalized, allowing objects of different sizes to have different weights. Another possibility is the ex-plicit inclusion of weights (  X  i ): given an input set of normalized measures, each can be multiplied by an ar-bitrary (positive) weight before computing the kernel. 5.3. Other Kernels Based on Jensen Other pd kernels may be devised inspired by Jensen-R  X enyi and Jensen-Tsallis divergences (Section 3). For example, it is a direct consequence of Prop. 6 that, for q  X  [0 , 1], ( p 1 , p 2 ) 7 X  R q p 1 + p 2 2 , and therefore JR are nd kernels on ( P n  X  1 ) 2 . We can then make use of Prop. 4 to derive pd kernels via exponentiation; for example, the exponentiated Jensen-R  X enyi kernel (pd for q  X  [0 , 1] and t  X  0): However, these kernels are no longer interpretable as MIs, and arbitrary weights are not allowed. Martins et al. (2008) also show that a related family of pd ker-nels for probability measures introduced by Hein and Bousquet (2005) can be written as differences between JT-type divergences. 5.4. The Heat Kernel Approximation The diffusion kernel for statistical manifolds, recently proposed by Lafferty and Lebanon (2005), is grounded in information geometry. It models the diffusion of  X  X n-formation X  over the manifold through the heat equa-tion. Since in the case of the multinomial manifold the diffusion kernel has no closed form, the authors adopt the so-called  X  X irst-order parametrix expansion, X  which resembles the Gaussian kernel replacing the Euclidean distance by the geodesic distance induced by the Fisher information metric. The resulting heat kernel approximation is where  X  &gt; 0 and d g ( p 1 , p 2 ) = 2 arccos P i Whether k heat is pd has been an open problem (Hein et al. , 2004; Zhang et al. , 2005).
 Proposition 11 Let n  X  2 . For sufficiently large  X  , the kernel k heat is not pd.
 Proof: From Prop. 4, k heat is pd, for all  X  &gt; 0, if and only if d 2 g is nd. We provide a counterexample, using the following four points in P 2 : p 1 = (1 , 0 , 0), p 2 = (0 , 1 , 0), p 3 = (0 , 0 , 1) and p 4 = (1 / 2 , 1 / 2 , 0). The squared distance matrix [ D ij ] = [ d 2 g ( p i , p j Taking c = (  X  4 ,  X  4 , 1 , 7) we have c T D c = 2  X  2 &gt; 0, showing that D is not nd. Although p 1 , p 2 , p 3 , p 4 lie on the boundary of P 2 , continuity of d 2 g implies that it is not nd. The case n &gt; 2 follows easily, by appending zeros to the four vectors above. We illustrate the performance of the proposed NE ker-nels, in comparison with common kernels, for SVM text classification. We performed experiments in two standard datasets: Reuters-21578 and WebKB . 5 Since our objective was to evaluate the kernels, we consid-ered a simple binary classification task that tries to discriminate among the two largest categories of each dataset; this led us to the earn-vs-acq classification task for the first dataset, and stud-vs-fac (student vs. faculty webpages) in the second dataset.
 After the usual preprocessing steps of stemming and stop-word removal, we mapped text documents into probability distributions over words using the bag-of-words model and maximum likelihood estimation (which corresponds to normalizing term frequency us-ing the ` 1 -norm), which we denote by tf . We also used the tf-idf measure, which penalizes terms that occur in many documents. To weight the documents for the Tsallis kernels, we tried four strategies: uni-form weighting, word counts, square root of the word counts, and one plus the logarithm of the word counts; however, for both tasks, uniform weighting revealed the best strategy, which may be due to the fact that documents in both collections are usually short and do not differ much in size.
 As baselines, we used the linear kernel with ` 2 nor-malization, commonly used for this task, and the heat kernel approximation (28) (Lafferty &amp; Lebanon, 2005), which is known to outperform the former, al-beit not being guaranteed to be pd for an arbitrary choice of  X  (see 28), as shown above. This parame-ter and the SVM C parameter were tuned with cross-validation over the training set. The SVM-Light pack-age ( http://svmlight.joachims.org/ ) was used to solve the SVM quadratic optimization problem. Figs. 1 X 2 summarize the results. We report the per-formance of the Tsallis kernels as a function of the entropic index. For comparison, we also plot the per-formance of an instance of a Tsallis kernel with q tuned through cross-validation. For the first task, this kernel and the two baselines exhibit similar performance for both the tf and the tf-idf representations; differences are not statiscally significant. In the second task, the Tsallis kernel outperformed the ` 2 -normalized linear kernel for both representations, and the heat kernel for tf-idf ; the differences are statistically significant (using the unpaired t test at the 0 . 05 level). Regard-ing the influence of the entropic index, we observe that in both tasks, the optimum value of q is usually higher for tf-idf than for tf .
 The results on these two problems are representative of the typical relative performance of the kernels con-sidered: in almost all tested cases, both the heat ker-nel and the Tsallis kernels (for a suitable value of q ) outperform the ` 2 -normalized linear kernel; the Tsallis kernels are competitive with the heat kernel. We have introduced a new family of positive defi-nite kernels between measures, which contains some well-known kernels as particular cases. These kernels are defined on unnormalized measures, which makes them suitable for use on empirical measures ( e.g. , word counts or pixel intensity histograms), allowing objects of different sizes to be weighted differently. The family is parameterized by the entropic index, a key concept in Tsallis statistics, and includes as extreme cases the Boolean and the linear kernels. The new kernels, and the proofs of positive definiteness, are supported by the other contributions of this paper: the new concept of q -convexity, the underlying Jensen q -inequality, and the concept of Jensen-Tsallis q -difference , a nonexten-sive generalization of the Jensen-Shannon divergence. Experimentally, kernels in this family outperformed the linear kernel in the task of text classification and achieved similar results to the first-order approxima-tion of the multinomial diffusion kernel. They have the advantage, however, of being pd, which fails to happen with the latter kernel, as also shown in this paper. Future research will concern applying Jensen-Tsallis q -differences to other learning problems, like clustering, possibly exploiting the fact that they accept more than two arguments.
 The authors thank the reviewers for helpful com-ments and Guy Lebanon for fruitful discussions on the heat kernel. This work was partially supported by Funda  X c  X ao para a Ci X encia e Tecnologia (FCT), Portu-gal, grant PTDC/EEA-TEL/72572/2006. A.M. was supported by a grant from FCT through the CMU-Portugal Program and the Information and Com-munications Technologies Institute (ICTI) at CMU. N.S. was supported by NSF IIS-0713265 and DARPA HR00110110013. E.X. was supported by NSF DBI-0546594, DBI-0640543, and IIS-0713379.

