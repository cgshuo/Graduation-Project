 Chonglin Sun 1( Multi-label classification [ 22 ] has been widely used in real world applications such as text categorization [ 11 ], image annotation [ 12 , 21 ], web advertising [ 2 ] and music categorization [ 18 ]. In these applications there are usually tens or hundreds of thousands of labels, while the number is still increasing. It is of great significance to perform such tasks with high efficiency. In multi-label clas-sification, each sample can be assigned with a set of labels, which makes it a more challenging task. Many approaches have been proposed to improve its per-formance.
 The traditional approach called binary relevance (BR) [ 1 , 16 ] is to train one classifier for each label being predicted independently. Despite its low training and testing efficiency, memory usage is also a bottleneck as the number of labels becoming larger. Recently, many approaches have been proposed either to exploit hierarchical label structures or conduct dimension reduction using label corre-lations. Proposed approaches for constructing hierarchical label structures [ 4 , 6 ] are usually converted to a very complex optimization problem mainly aiming at improving testing efficiency, but the training procedure is still very slow. In this paper, we focus on conducting label space dimension reduction by incorporating label correlations.
 label space dimension reduction. The main idea of label transformation is to transform the original label set into another small set of labels that is man-vector into a k -dimensional vector ( k d ) and then training is performed on the projected vectors. However, these transformed labels are usually difficult to learn. Label selection can overcome the limitation of label transformation, which just selects a small subset of labels from the original set and uses this selected set of labels for training [ 5 , 7 ]. These approaches are based on the assumption that non-selected labels could be correctly reconstructed from the selected ones. However, previously proposed approaches on label selection are either simple random sampling which requires nondeterministic number of sampling trials or computationally expensive. And singular value decomposition (SVD) is needed to be performed in almost all of these methods.
 cation are proposed to alleviate these problems, clustering based sampling (CBS) and frequency based sampling (FBS). For CBS, labels are clustered before being selected. With the assumption that a label can faithfully construct other labels in the same cluster, we use K-means to group labels into k clusters (if k labels are to be selected), then sample one label from each cluster. This approach requires only k sampling trials if k labels are to be selected. FBS is an efficient method with high performance where only label frequency statistics is considered for label selection. Contrary to other label selection based multi-label classification approaches, FBS does not formulate label selection as a general column subset selection problem (CSSP). It is able to make better use of the property of label matrix: (1) sparse with each row containing only a few non-zero items; (2) containing values of only 0 or 1. Neither of the proposed two approaches needs to perform SVD or solve com-plex optimization problems. Experimental results on several real world multi-label datasets show that the proposed algorithms achieve state-of-the-art performance among label space dimension reduction based methods.
 tion to label selection and its related problems, and then two proposed label selection methods for multi-label classification are presented. Followed are the experimental results and analysis. Finally we conclude this paper.
 Notations: The following notations will be used in this section and the rest of this paper. n , m , d are used to denote the number of samples, the dimension of features and the number of labels respectively. X  X  IR n  X  m is denoted by v T . For a matrix A , its transpose is denoted by A Moore-Penrose pseudo-inverse, A F , A 2 denote the Frobenius norm and spec-tral norm respectively. Besides, we use A ( i ) to denote the i the j th column of A ,and A i,j to denote the j th column of i th Label selection is a very efficient class of label space dimension reduction oriented approaches for multi-label classification. By selecting a small subset of labels and performing training on these selected labels, non-selected labels can be predicted using the selected ones. Obviously, this method is based on the assumption that non-selected labels can be faithfully constructed from the selected ones. Therefore, the label selection method is the key to classification performance. In this section, we will first give a brief introduction to label selection problem. One method for label selection is to formulate it as a regularized least squares regression model [ 5 ] is the coefficient matrix with only a few non-zero rows, and  X  regularization parameters. The second term in (1) enforces joint group sparsity across the rows of W , and the third term is the traditional l the whole W . However, when the number of labels becomes large, problem (1) becomes computationally expensive.
 An alternative is to treat label selection as one column subset selection prob-lem (CSSP) [ 3 , 10 ]. For a matrix A  X  IR n  X  d , CSSP aims at finding exactly k columns so that these selected columns can approximately span A . Concretely, we expect to find a column set C with size k such that A  X  minimized. A C denotes the sub-matrix of the columns in set C of matrix A . Exact solution is impossible for its high computational complexity O ( d approximate solution such as randomized sampling [ 3 , 7 , 10 ] has been proposed to alleviate this problem. In Bi &amp; Kowk X  X  [ 7 ] work, they proposed to select k columns from A with the probability for selecting the i th column being where V k  X  IR n  X  k , is the top k right singular vectors of partial SVD performed on A . p i corresponds to the leverage score of A ( i ) on the best subspace of A . Our proposed algorithm is based on CSSP but with different calculation of p different sampling procedures.
 Similar to CSSP, the aim of the proposed algorithms is to select k columns from a given label matrix Y such that the reconstruction error is minimized. That is where C is the selected set of k columns, and Y C is the sub-matrix that contains k columns of Y . We will elaborate the proposed two column selection algorithms for multi-label classification, clustering based sampling (CBS) and frequency based sampling (FBS) in the following sections. Unlike previous proposed label selection algorithms, SVD is not needed in our proposed algorithms. 3.1 Clustering Based Sampling (CBS) The main idea of CBS is to group labels (columns of label matrix) into k clusters, and then sample one label from each cluster. In order to cluster labels, we need first to generate embeddings for each label. In this work, we represent a label vector as where L  X  IR d  X  m denotes the embedding matrix with each row denoting the vector of one label, and L ( t ) is an m -dimensional row vector which denotes the embedding of the t th label. Although this label embedding looks quite sim-ple, experiments in Sect. 4 show that CBS performs well on several real world datasets. The full procedure is described in Algorithm 1.
 Algorithm 1. Clustering Based Sampling for Multi-label Classification with a combination of sample features. Then K-means algorithm is used to group labels into k clusters. For labels with few occurrences, it is hard to obtain a good embedding for clustering. In our implementation, we just put these labels into the same cluster. Since we only sample one label from each cluster, exact k sampling trials are needed. 3.2 Frequency Based Sampling (FBS) Most of the previously proposed column selection based algorithms for multi-label classification are formulated as a general CSSP problem, they did not utilize the unique property of label matrix Y : (1) they are usually sparse with each row containing only a few non-zero items and (2) the matrix contains values of only 0 or 1. And there are usually a lot of redundant labels in multi-label classification tasks with many labels. For example, in text categorization, one text can be categorized as machine learning , it also belongs to the category of ML which is short for machine learning . And samples with label ML is often a subset of samples with label machine learning . We want labels like machine learning to be selected. Based on this fact, we propose a frequency based sampling algorithm in which each label(column) is selected with a probability where p j is the probability of the j th column being selected. Intuitively, labels with higher frequency is assigned with higher sampling probability, and is more likely to be selected. The sampling procedure is described in Algorithm 2. From Algorithm 2, it is easy to see that the probability of selecting a label is propor-tional to its occurrence frequency.
 Proposition 1. The probability of the j th column being selected p all 1  X  j  X  d ,where c is a constant ( c d ) .
 Proof. As the property of label matrix, each row has only a few non-zero terms, let c be the average number of non-zero terms in each row. The sum of all label occurrences is cn and each label (column) will occur at least once. Thus, p Proposition 2. The average sampling trials of selecting k different columns is Proof. Let p j be the probability of the j th column being selected, and T the expected number of sampling trials for sampling exactly i different columns, and C i the set of selected columns with size i . Then we have the following formula Algorithm 2. Frequency Based Sampling for Multi-label Classification Together with Proposition 1, we can obtain the lower bound of the second term From (7), we my obtain where c is a constant as described in Proposition 1. Because k d , log Typically when k =0 . 1 d , log d d  X  k =0 . 152, thus cn log 3.3 Prediction The proposed two algorithms have two different column selection procedures, but they share the same prediction procedure as [ 7 ]. On prediction, a new test sample is first applied to the k learned classifiers to obtain a k -dimensional prediction vector h . And the d-dimensional prediction vector can be constructed as y = h T Y  X  C Y . 3.4 Comparison with Other Methods The proposed approaches are mainly compared with label selection based method: ML-CSSP [ 7 ] and label space transformation based methods: PLST [ 17 ], CPLST [ 8 ] for the reported high performance. The comparison are shown in Table 1. For our CBS method, it takes O ( nmd ) in general to obtain label embeddings, however, because the label matrix Y is extremely sparse with only a few non-zero terms in each row, this complexity can be reduced to O ( nm ). And the time complexity of K-means is O ( kdm ). For our proposed FBS algorithm, only the frequency of each label is required which takes O ( nd ). In order to sample k different columns, ML-CSSP requires O ( klogk ) sampling trials, our proposed CBS only uses k trials, and FBS needs nlog d d  X  k trials. Besides, PLST and CPLST do not need to sample labels. Among these five methods, our proposed FBS is the most computationally efficient, and our CBS uses the minimum number of sampling trials. For easy comparison, the complexity of PLST and CPLST are also provided.
 In this section, experiments are conducted on a number of benchmark datasets shown in Table 2. cal500 is a dataset of human-generated musical annotations that describe 500 popular western musical tracks. Each song is annotated by a vocabulary of 174 tags [ 18 ]. corel5k is a set of PCD images. There are 371 words in total in the vocabulary and each image has 4-5 keywords [ 9 ]. delicious contains textual data of web pages along with their tags extracted from del.icio.us social bookmarking site [ 15 ]. ESPGame is a list of 100,000 images with English labels from the ESP Game. To make the training process efficient, a subset of the image dataset ESPGame is randomly selected and we pick up tags that occur at least twice within the subset. Each instance in ESPGame is represented with a 905-D feature vector 2 extracted with LIRe [ 14 ].
 The proposed two methods FBS, CBS are compared with ML-CSSP [ 7 ], PLST [ 17 ]andCPLST[ 8 ]. All the compared methods are implemented in python with linear regression as the classifier. The number of selected labels k varies from difference in relative performance as the value of k varies (But we provide the results with the variation of k on cal500 ).
 [ 7 ], and micro-averaged area under precision-recall curve (AUPRC) [ 19 ]. Squared RMSE is proportional to the commonly used Hamming loss 10-fold cross-validation is performed.
 4.1 Accuracy We compare testing accuracy of our proposed two methods with ML-CSSP [ 7 ], PLST [ 17 ],CPLST[ 8 ]. The RMSE results of the five methods on several datasets are presented in Table 3. These results are obtained using pairwise t-test with 95% confidence. Our CBS achieves the best performance on 3 of the 4 datasets, which is best among the three approaches. Our FBS also obtains the best per-formance on 2 out of the 4 datasets, and its overall performance is competitive compared to ML-CSSP, PLST and CPLST. Both FBS and CBS achieve the state-of-the-art performance. We also calculate AUPRC, and results are shown in Table 4. And the variation of RMSE with the number of selected labels k on cal500 are shown in Fig. 1.
 4.2 Sampling Trials and Encoding Time In this section, we compare sampling trials and encoding time of our CBS, FBS with ML-CSSP, PLST and CPLST.
 The sampling trials of the 5 approaches on several datasets are shown in Table 5. As the theoretical analysis shown in Sect. 3.4, CBS uses the fewest number of trials on all datasets which is equal to the number of labels being selected ( k ). Although FBS and ML-CSSP uses a bit more trials, it is still far from their bound  X  ( n log d d  X  k )and O ( k log k ) respectively. And on average, FBS uses fewer sampling trials than ML-CSSP. Besides, PLST, CPLST do not have the process of sampling.
 Table 6 shows the encoding time on several data sets. Our FBS achieves the best encoding efficiency, significantly faster than the other ones. From the result, we can see that CBS is less efficient than ML-CSSP, which is because our embedding approach leads to high dimensional embedding vectors which makes it slow for K-means, as is the case especially in the largest dataset. However, this is not a serious problem since we can embed labels into low dimensional vectors using other techniques to help accelerate the clustering. The efficiency of PLST is similar to ML-CSSP since they both need to perform SVD on label matrix. CPLST is the least efficient among the five methods because it has to perform SVD on a much larger matrix.
 4.3 Comparison of FBS and ML-CSSP Although FBS uses a strategy to calculate the probability of sampling a label which is different from ML-CSSP as defined in (5), (2) respectively, FBS achieves comparative RMSE and AUPRC as ML-CSSP. In this section, we analyze a bit about the statistical property of the distribution of label selection given by FBS and ML-CSSP. For ML-CSSP, the probability of selecting a label varies with the number of labels to be selected, whereas it remains constant for FBS. We use KL divergence to measure the similarity between them where p denotes the distribution defined in (5), and q denotes the distribution in (2). The KL divergence on several data sets are shown in Fig. 2, it is small on all four data sets. Thus, the selection procedures in these two algorithms behave alike and obtain similar accuracies. However, FBS outperforms ML-CSSP in efficiency. In this paper, we propose two efficient approaches for multi-label classification. Unlike previous proposed label selection methods, although we also formulate label selection as a column subset selection problem (CSSP), SVD is not needed in our methods. For our proposed clustering based sampling (CBS) method, only k sampling trials are needed for selecting k labels. And theoretical analysis and experimental results have demonstrated that our proposed frequency based sampling (FBS) method has the highest efficiency, and FBS is believed to make better use of the property of label matrix. Experiments on a number of real world datasets with many labels demonstrate that our proposed two algorithms achieve the state-of-the-art performance among recently proposed label space dimension reduction based multi-label classification algorithms.

