 Measurements from sensor systems typically serve as a proxy for latent variables of interest. To recover these latent variables, the parameters of the sensor system must first be determined. When pairs of measurements and their corresponding latent variables are available, fully supervised re-gression techniques may be applied to learn a mapping between latent states and measurements. In many applications, however, latent states cannot be observed and only a diffuse prior on them is available. In such cases, marginalizing over the latent variables and searching for the model parame-ters using Expectation Maximization (EM) has become a popular approach [3,9,19]. Unfortunately, such algorithms are prone to local minima and require very careful initialization in practice. Using a simple change-of-variable model, we derive an approximation algorithm for the Unsuper-vised Regression problem  X  estimating the nonlinear relationship between latent-states and their observations when no example pairs are available, when the observation function is invertible, and when the measurement noise is small. Our method is not susceptible to local minima and provides a guarantee on the quality of the recovered observation function. We identify conditions under which our estimate of the mapping is asymptotically consistent and empirically evaluate the quality of our solutions and their stability under variations of the prior. Because our algorithm takes advantage of an explicit prior on the latent variables, it recovers latent variables more accurately than manifold learning algorithms when applied to similar tasks.
 Our method may be applied to estimate the observation function in nonlinear dynamical systems by enforcing a Markovian dynamics prior over the latent states. We demonstrate this approach to nonlinear system identification by learning to track a moving object in a field of completely uncalibrated sensor nodes whose measurement functions are unknown. Given that the object moves smoothly over time, our algorithm learns a function that maps the raw measurements from the sensor network to the target X  X  location. In another experiment, we learn to track Radio Frequency ID (RFID) tags given a sequence of voltage measurements induced by the tag in a set of antennae . Given only these measurements and that the tag moves smoothly over time, we can recover a mapping from the voltages to the position of the tag. These results are surprising because no parametric sensor model is available in either scenario. We are able to recover the measurement model up to an affine transform given only raw measurement sequences and a diffuse prior on the state sequence. We assume that the set X = { x i } 1  X  X  X  N of latent variables is drawn (not necessarily iid) from a known an unknown invertible nonlinearity applied to each latent variable, y i = f 0 ( x i ) . We assume that observations, y i  X  X  D , are higher dimensional than latent variables x i  X  X  d . Computing a MAP estimate of f 0 requires marginalizing over X and maximizing over f . EM, or some other form of coordinate ascent on a Jensen bound of the likelihood, is a common way of estimating the parameters of this model, but such methods suffer from local minima.
 Because we have assumed that f 0 is invertible and that there is no observation noise, this process describes a change of variables. The true distribution p Y ( Y ) over Y can be computed in closed form using a generalization of the standard change of variables formula (see [14, thm 9.3.1] and [7, chap 11]): p
Y ( Y ) = p Y ( Y ; f 0 ) = p X ( f The determinant corrects the warping of each infinitesimal volume element around f  X  1 0 ( y i ) by ac-counting for the stretching induced by the nonlinearity. The change of variables formula immedi-ately yields a likelihood over f , circumventing the need for integrating over the latent variables. We assume f 0 diffeomorphically maps a ball in R d containing the data onto its image. In this case, there exists a function g defined on an open set containing the image of f such that g ( f ( x )) = x and  X  g  X  f = I for all x in the open set [5]. Consequently, we can substitute g for f  X  1 in (1) and, taking advantage of the identity det(  X  f 0  X  f )  X  1 = det  X  g  X  g 0 , write its log likelihood as l Y ( Y ; g ) = log p Y ( Y ; g ) = log p X ( g ( y 1 ) , . . . , g ( y N )) + For many common priors p X , the maximum likelihood g yields an asymptotically consistent es-timate of the true distribution p Y . When certain conditions on p X are met (including stationarity, ergodicity, and k th-order Markov approximability), a generalized version of the Shannon-McMillan-Breiman theorem [1] guarantees that log p Y ( Y ; g ) asymptotically converges to the relative entropy rate between the true p Y ( Y ) and p Y ( Y ; g ) . This quantity is maximized when these two distribu-tions are equal. Therefore, if the true p Y follows the change of variable model (1), the recovered g converges to the true f  X  1 0 in the sense that they both describe a change of variable from the prior distribution p X to the distribution p Y .
 Note that although our generative model assumes no observation noise, some noise in Y can be tolerated if we constrain our search over smooth functions g . This way, small perturbations in y due to observation noise produce small perturbations in g ( y ) . We constrain our search for g to a subset of smooth functions by requiring that g have a finite representation as a weighted sum of positive definite kernels k centered on observed data, g ( y ) = P i =1 c i k ( y, y i ) , with the weight vectors c i  X  X  d . Accordingly, applying g to the set of observations gives g ( Y ) = CK , where C = [ c 1  X  X  X  c N ] and K is the kernel matrix with K ij = k ( y i , y j ) . In addition,  X  g ( y ) = C  X ( y ) , where  X ( y ) is an N  X  D matrix whose i th row is  X  X  ( y i ,y )  X  X  . We tune the smoothness of g by regularizing (2) with the RKHS norm [17] of g . This norm has the form k g k 2 k = tr CKC 0 , and the regularization parameter is set to  X  For simplicity, we require p X to be a Gaussian with mean zero and inverse covariance  X  X , but we note our methods can be extended to any log-concave distribution. Substituting into (2) and adding the smoothness penalty on g , we obtain: where the vec (  X  ) operator stacks up the columns of its matrix argument into a column vector. Equation (3) is not concave in C and is likely to be hard to maximize exactly. This is because log det( A 0 A ) is not concave for A  X  X  d  X  D . Since the cost is non-concave, gradient descent methods may converge to local minima. Such local minima, in addition to the burdensome time and storage requirements, rule out descent strategies for optimizing (3).
 Our first algorithm for approximately solving this optimization problem constructs a semidefinite relaxation using a standard approach that replaces outer products of vectors with positive definite matrices. Rewrite (3) as where the kl th entry of the matrix argument of the logdet is as specified, and the matrix E ij is zero everywhere except for 1 in its ij th entry. This optimization is equivalent to subject to the additional constraint that rank( Z ) = 1 . Dropping the rank constraint yields a concave relaxation for (3). Standard interior point methods [20] or subgradient methods [2] can efficiently compute the optimal Z for this relaxed problem. A set of coefficients C can then be extracted from the top eigenvectors of the optimal Z , yielding an approximate solution to (3). Since (6) without the rank constraint is a relaxation of (3), the optimum of (6) is an upper bound on that of (3). Thus we can bound the difference in the value of the extracted solution and that of the global maximum of (3). As we will see in the following section, this method produces high quality solutions for a diverse set of learning problems.
 In practice, standard algorithms for (6) run slowly for large data sets, so we have developed an intuitive algorithm that also provides good approximations and runs much more quickly. The non-concave logdet term serves to prevent the optimal solution of (2) from collapsing to g ( y ) = 0 , since X = 0 is the most likely setting for the zero-mean Gaussian prior p X . To circumvent the non-concavity of the logdet term, we replace it with constraints requiring that the sample mean and covariance of g ( Y ) match the expected mean and covariance of the random variables X . These moment constraints prevent the optimal solution from collapsing to zero while remaining in the typical set of p X . The expected covariance of X , denoted by  X   X  X , can be computed by averaging the block diagonals of  X   X  1 X . However, the particular choice of  X   X  X only influences the final solution up to a scaling and rotation on g , so in practice, we set it to the identity matrix. We thus obtain the following optimization problem: where 1 is a column vector of 1s. This optimization problem searches for a g that transforms observations into variables that are given high probability by p X and match its stationary statistics. This is a quadratic minimization with a single quadratic constraint and, after eliminating the linear constraints with a change of variables, can be solved as a generalized eigenvalue problem [4]. Manifold learning algorithms and unsupervised nonlinear system identification algorithms solve variants of the unsupervised regression problem considered here.
 Our method provides a statistical model that augments manifold learning algorithms with a prior on latent variables. Our spectral algorithm from Section 3 reduces to a variant of KPCA [15] when X are drawn iid from a spherical Gaussian. By adopting a nearest-neighbors form for g instead of the RBF form, we obtain an algorithm that is similar to embedding step of LLE [12, chap 5]. In addition to our use of dynamics, a notable difference between our method and principal manifold methods [16] is that instead of learning a mapping from states to observations, we learn mappings from observations to states. This reduces the storage and computational requirements when processing high-dimensional data. As far as we are aware, in the manifold learning literature, only Jenkins and Mataric [6] explicitly take temporal coherency into account, by increasing the affinity of temporally adjacent points and applying Isomap [18].
 State-of-the-art nonlinear system identification techniques seek to recover all the parameters of a continuous hidden Markov chain with nonlinear state transitions and observation functions given noisy observations [3,8,9,19]. Because these models are so rich and have so many unknowns, these algorithms resort to coordinate ascent (for example, via EM), making them susceptible to local min-ima. In addition, each iteration of coordinate ascent requires some form of nonlinear smoothing over the latent variables, which is itself both computationally costly and becomes prone to local minima when the estimated observation function becomes non-invertible during the iterations. Fur-ther, because mappings from low-dimensional to high-dimensional vectors require many parameters to represent, existing approaches tend to be unsuitable for large-scale sensor network or image anal-ysis problems. Our algorithms do not have local minima and represent the more compact inverse observation function where high-dimensional observations appear only in pairwise kernel evalua-tions.
 Comparisons with a semi-supervised variant of these algorithms [13] show that weak priors on the latent variables are extremely informative and that additional labeled data is often only necessary to fix the coordinate system. The following experiments show that latent states and observation functions can be accurately and efficiently recovered up to a linear coordinate transformation given only raw measurements and a generic prior over the latent variables. We compare against various manifold learning and nonlinear system identification algorithms. We also show that our algorithm is robust to variations in the choice of the prior.
 As a measure of quality, we report the affine registration error , the average residual per data point after registering the recovered latent variables with their ground truth values using an affine trans-formation: err = min A ,b 1 N All of our experiments use a spherical Gaussian kernel. To define the Gaussian prior p X , we start with a linear Gaussian Markov chain s t = A s t  X  1 +  X  t , where A and the covariance of  X  are block diagonal and define d Markov chains that evolve independently from each other according to New-tonian motion. The latent variables x t extract the position components of s t . The inverse covariance matrix corresponding to this process can be obtained in closed form. More details and additional experiments can be found in [12].
 We begin with a low-dimensional data set to simplify visualization and comparison to systems that do not scale well with the dimensionality of the observations. Figure 1(b) shows the embed-ding of a 1500 step 2D random walk shown in Figure 1(a) into R 3 by the function f ( x, y ) = ( x, y cos(2 y ) , y sin(2 y )) . Note that the 2D walk was not generated by a linear Gaussian model, as it bounces off the edges of its bounding box. Lifted points were passed to our algorithm, which returned the 2D variables shown in Figure 1(c). The true 2D coordinates are recovered up to a scale, a flip, and some shrinking in the lower left corner. Therefore the recovered g is close the inverse of the original mapping, up to a linear transform. Figure 1(d) shows states recovered by the algo-rithm of Roweis and Ghahramani [3]. Smoothing with the recovered function simply projects the observations without unrolling the roll. The joint-max version of this algorithm took about an hour to converge on a 1Ghz Pentium III and converges only when started at solutions that are sufficiently close to the true solution. Our spectral algorithm took about 10 seconds. Isomap (Figure 1(e)) per-forms poorly on this data set due to the low sampling rate on the manifold and the fact that the true mapping f is not isometric. Including temporal neighbors into Isomap X  X  neighborhood structure (as per ST-Isomap) creates some folding, and the true underlying walk is not recovered (Figure 1(f)). KPCA (not shown) chooses a linear projection that simply eliminates the first coordinate. We found the optimal parameter settings for Isomap, KPCA, and ST-Isomap by a fine grid search over the parameter space of each algorithm.
 The upper bound on the log-likelihood returned by the relaxation (6) serves as a diagnostic on the quality of our approximations. This bound was  X  3 . 9  X  10  X  3 for this experiment. Rounding the result of the relaxation returned a g with log likelihood  X  5 . 5  X  10  X  3 . The spectral approximation (7) also returned a solution with log likelihood  X  5 . 5  X  10  X  3 , confirming our experience that these algorithms usually return similar solutions. For comparison, log-likelihood of KPCA X  X  solution was  X  1 . 69  X  10  X  2 , significantly less likely than our solutions, or the upper bound. 5.1 Learning to track in an uncalibrated sensor network We consider an artificial distributed sensor network scenario where many sensor nodes are deployed randomly in a field in order to track a moving target (Figure 2(a)). The location of the sensor nodes is unknown, and the sensors are uncalibrated, so that it is not known how the position of the target maps to the reported measurements. This situation arises when it is not feasible to calibrate each sensor prior to deployment or when variations in environmental conditions affect each sensor differently. Given only the raw measurements produced by the network from watching a smoothly moving target, we wish to learn a mapping from these measurements to the location of the target, even though no functional form for the measurement model is available. A similar problem was considered by [11], who sought to recover the location of sensor nodes using off-the-shelf manifold learning algorithms.
 Each latent state x t is the unknown position of the target at time t . The unknown function f ( x t ) gives the set of measurements y t reported by the sensor network at time t . Figure 2(b) shows the time series of measurements from observing the target. In this case, measurements were generated by having each sensor s report its true distance d s t to the target at time t and passing it through a random nonlinearity of the form  X  s exp(  X   X  s d s t ) . Note that only f , not the measurement function of each sensor, needs be invertible. This is equivalent to requiring that a memoryless mapping from measurements to positions must exist. Assuming only that the target vaguely follows linear-Gaussian dynamics, and given only the time se-ries of the raw measurements from the sensor network, our learning algorithm finds a transformation that maps observations from the sensor network to the position of the target up to a linear coordinate transform (Figure 2(c)). The recovered function g implicitly performs all the triangulation necessary for recovering the position of the target, even though the position or characteristics of the sensors were not known a priori . The bottom row of Figure 2 tests the recovered g by applying it to a new measurement set. To show that this sensor network problem is not trivial, the figure also shows the output of the mapping obtained by KPCA. 5.2 Learning to Track with the Sensetable The Sensetable is a hardware platform for tracking the position of radio frequency identification (RFID) tags. It consists of 10 antennae woven into a flat surface 30  X  30 cm. As an RFID tag moves along the flat surface, the strength of the RF signal induced by RFID tag in each antenna is reported, producing a time series of 10 numbers. We wish to learn a mapping from these 10 voltage measure-ments to the 2D position of the RFID tag. Previously, such a mapping was recovered by hand, by meticulous physical modeling of this system, followed by trial-and-error to refine these mappings; a process that took about 3 months in total [10]. We show that it is possible to recover this mapping automatically, up to an affine transformation, given only the raw time series of measurements gener-ated by moving the RFID tag by hand on the Sensetable for about 5 minutes. This is a challenging task because the relationship between the tag X  X  position and the observed measurements is highly oscillatory. (Figure 3(a)). Once it is learned, we can use the mapping to track RFID tags. This experiment serves as a real-world instantiation of the sensor network setup of the previous section in that each antenna effectively acts as an uncalibrated sensor node with an unknown and highly oscillatory measurement function.
 Figure 3(b) shows the ground truth trajectory of the RFID tag in this data set. Given only the 5 minute-long time series of raw voltage measurements, our algorithm recovered the trajectory shown in Figure 3(c). These recovered coordinates are scaled down and flipped about both axes as com-pared to the ground truth coordinates. There is also some additional shrinkage in the upper right corner, but the coordinates are otherwise recovered accurately, with an affine registration error of 1.8 cm per pixel.
 Figure 4 shows the result of LLE, KPCA, Isomap and ST-Isomap on this data set under their best parameter settings (again found by a grid search on each algorithm X  X  search space). None of these algorithms recover low-dimensional coordinates that resemble the ground truth. LLE, in addition to collapsing the coordinates to one dimension, exhibits severe folding, obtaining an affine registration Figure 3: (a) The output of the Sensetable over a six second period, while moving the tag from the left edge Figure 4: From left to right, the trajectories recovered by LLE, KPCA, Isomap, ST-Isomap. All of these error of 8.5 cm. KPCA also exhibited folding and large holes, with an affine registration error of 7.2 cm. Of these, Isomap performed the best with an affine registration error of 3.4 cm, though it exhibited some folding and a large hole in the center. Isomap with temporal coherency performed similarly, with a best affine registration error of 3.1 cm. Smoothing the output of these algorithms using the prior sometimes improves their accuracy by a few millimeters, but more often diminishes their accuracy by causing overshoots.
 To further test the mapping recovered by our algorithm, we traced various trajectories with an RFID tag and passed the resulting voltages through the recovered g . Figure 5 plots the results (after a flip about the y-axis). These shapes resemble the trajectories we traced. Noise in the recovered coordinates is due to measurement noise.
 The algorithm is robust to perturbations in p X . To demonstrate this, we generated 2000 random per-turbations of the parameters of the inverse covariance of X used to generate the Sensetable results, and evaluated the resulting affine registration error. The random perturbations were generated by scaling the components of A and the diagonal elements of the covariance of  X  over four orders of magnitude using a log uniform scaling. The affine registration error was below 3.6 cm for 38% of these 2000 perturbations. Typically, only the parameters of the kernel need to be tuned. In practice, we simply choose the kernel bandwidth parameter so that the minimum entry in K is approximately 0.1. We have shown how to recover the latent variables in a dynamical system given an approximate prior on the dynamics of these variables and observations of these states through an unknown invertible nonlinearity. The requirement that the observation function be invertible is similar to the require-ment in manifold learning algorithms that the manifold not intersect itself. Our algorithm enhances manifold learning algorithms by leveraging a prior on the latent variables. Because we search for a mapping from observations to unknown states (as opposed to from states to observations), we can devise algorithms that are stable and avoid local minima. We applied this methodology to learning to track objects given only raw measurements from sensors with no constraints on the observation model other than invertibility and smoothness.
 We are currently evaluating various ways to relax the invertibility requirement on the observation function by allowing invertibility up to a linear subspace. We are also exploring different prior models, and experimenting with ways to jointly optimize over g and the parameters of p X .
