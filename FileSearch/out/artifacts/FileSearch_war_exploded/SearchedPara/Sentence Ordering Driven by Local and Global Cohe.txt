 The canonical pipel ine of text summarization consists of topic identification , interpretation, and summary generation (Hovy, 2005). In the simple case of extraction, topic identification and interpretation are conflated to sentence selection and concerned with summary inform ativeness . In comparison, summary generation addresses summary readability and a frequently discussed generation technique is sentence ordering .
 ordering for summarization is primarily driven by coherence . For example, Barzilay et al. ( 2002 ) use lexical cohesion information to model local coherence. A statistical model by Lapata (2003) considers both lexical and syntactic features in calculating local coherence . More globally biased is Barzilay and Lee X  X  ( 2004) HMM -based content model , which models global coherence with word distribution patterns.
 lexical or topical relations, B arzilay and Lapata (2005, 2008) explicitly model local coherence with an entity grid mod el trained for optimal syntactic role transitions of entities.

Although coherence in those works is modeled in the guise of  X  lexical cohesion  X  ,  X  topic closeness  X  ,  X  content relatedness  X  , etc., few published works simultaneously accommodate coherence on the two levels: local coherence and global coherence, both of which are intriguing topics in text linguistics and psychology. For sentences, local coherence means the well -connectedness between adjacent sentences through lexical cohesion (Halliday and Hasan, 1 976) or entity repetition (Grosz et al., 1995) and global coherence is the discourse -level relation connecting remote sentences (Mann and Thompson, 1995; Kehler, 2002). An abundance of psychological evidences show that coherence on both levels is manifeste d in text comprehension (Tapiero, 2007). Accordingly, an apt sentence ordering scheme should be driven by such concerns. 
We also note that a s sentence ordering is usually discussed only in the context of multi -document summarization , factors other than co herence are also considered, such as time and source sentence position in Bollegala et al.  X  s ( 2006 )  X  X gglomerative ordering X  approach . But it remains an open question whether sentence ordering is non -trivial for single -document summarization, as it has lon g been recognized as an actual strategy taken by human summarizers (Jing , 1998; Jing and McKeown , 2000) and acknowledged early in work on sentence ordering for multi -document summarization (Barzilay et al. , 2002).

In this paper, we outline a grouping -based sentence ordering framework that is driven by the concern of local and global coherence . Summary sentences are grouped according to their conceptual relatedness before being ordered on two levels: group -level ordering and sentence -level ordering, which ca pture global coherence and local coherence in an integrated model. As a preliminary study, we applied the framework to single -document summary generation and obtained interesting results.
 stress the need to c hannel sentence ordering research to linguistic and psychological findings about text coherence ; (2) we propose a grouping -based ordering framework that integrates both local and global coherence ; (3) we find in experiments that coherence -driven sentence o rdering improves the reada bility of single -document summaries, for which sentence ordering is often considered trivial .
 techniques in previous work . Section 3 provides the details of grouping -based sentence orderin g . The preliminary experimental results are presented in S ection 4. Finally, S ection 5 concludes the whole paper and describes future work. Our ordering framework is designed to capture both local and global coherence. Globally, we identify related groups among sentences and find their relative order. Locally, we strive to keep sentence similar or related in content close to each other within one group . 2.1 Sentence Representation As summary sentences are isolated from their original co ntext, we retain the important content information by representing sentences as concept vectors. In the simplest case, the  X  concept  X  is equivalent to content word . A drawback of this practice is that it considers every content word equally contributive to the sentence content, which is not always true . F or example, in the news domain, entities realized as NPs are more important than other concepts.

To represent sentences as entity vectors, w e identify both common entities ( as the head nouns of NPs ) and name d entities. Two common entities are equivalent if their noun stems are identical or synonymous. N amed entities are usually equated by identity. But in order to improve accuracy, we also consider : 1) structu ral subsumption (one is part of anot her ); 2) hyper nymy and holonymy (the named entities are in a supero r dinate -subordinate or part -whole relation).

Now with summary sentence S i and m entities e i k e ik an d w k is the weight of e ik . We define w k = 1 if e ik is a common entity and w k = 2 if e ik is a named entity. We give double weight to named entities because of their significance to news articles. After all, a news story typically contain s events, places, or ganizations, people, etc. that denote the news theme. Other things being equal, two sentences sharing a mention of named entities are thematically closer than two sentences sharing a mention of common entities.

A lternatively, we can realize the  X  concept  X  a s  X  event  X  because events are prevalent semantic constructs that bear much of the sentence content in some domains (e.g., narratives and news reports). To represent sentences as event vectors, we can follow Zhang et al.  X  s (2010) method at the cost of more c omplexity. 2.2 Sentence Grouping To meet the global need of identifying sentence groups, we develop two grouping algorithms by applying graph -based operation and clustering. Connected Component Finding (CC)
This algorithm treats grouping sentences as finding connected components (CC) in a text graph T G = ( V , E ) , where V represents the sentences and E the sentence relations weighted by cosine similarity . E dges with weight &lt; t , a threshold, are removed because they represent poor sentence coherence .

The result a nt graph may be disconnected, in which we find all of its connected components, using depth -first search. The connected components are the groups we are looking for. Note that this method cannot guarantee that every two sentences in such a group are direct ly linked, but it does guarantee that there exists a path between every sentence pair.
 Modified K -means Clustering (MKM)
Observing that the CC method finds only coherent groups , not necessarily groups of coherent sentences , we develop a second algorithm us ing clustering. A good choice might be K -means as it is efficient and outperform s agglomerative clustering methods in NLP applications (Steibach et al., 2000), but the difficulty with the conventional K -means is the decision of K . 
Our solution is modified K -means (MKM) based on ( Wilpon and Rabiner , 1985) . Let  X  s denote cluster i by CL i and cluster similarity by Sim ( CL i = cosine . The following illustrates the algorithm .
T he above algorithm stops iterating when each cluster contains all above -threshold -similarity sentence pairs or only one sentence. Unlike CC, MKM results in more strongly connected groups , or group s of coherence sentences . 2.3 Ordering Algorithms After the sentences are grouped, ordering is to be conducted on two levels: group and sentence.

Composed of closely related sentences, groups simulate high -level textual constructs, such as  X  central event  X  ,  X  c ause  X  ,  X  effect  X  ,  X  X ackground X  , etc. for news articles, around which sentences are generated for global coherence. For an intuitive example, all sentences about  X  cause  X  should immediately precede all sentences about  X  effect  X  to achieve optimal readability. We propose two approach es to group -level ordering. 1) If the group sentences come from the same document, g roup ( G i ) order is decided by the group -representing sentence ( g i ) order ( means  X  precede  X  ) in the text . 2) G roup order is decided in a greedy fashion in order to maximize the connectedness between adjacent groups, thus enhancing local coherence . Each time a group is selected to achieve maximum similarity with the ordered groups and the first orde red group ( G 1 ) is selected to achieve maximum similarity with all the other groups. where Sim ( G , G  X  ) is the average sentence cosine similarity between G and G  X  .

Within the ordered groups, sent ence -level ordering is aimed to enhance local coherence by placing conceptually close sentences next to each other. Similarly, we propose two approaches. 1) If the sentences come from the same document, they are arranged by the text order . 2) Sentence ord er is greedily decided. Similar to the decision of group order, with ordered sentence S pi in group G p :
S Sim S S
S Sim S S
N ote that the text order is used as a common heuristic, based on the assumption that the s entences are arranged coherently in the source document, locally and globally. Currently, we have evaluate d grouping -based ordering on single -document summarization, for which text order is usually considered sufficient . But there is no theoretical proof that it leads to optimal global and local coherence that concerns us. On some occasions, e.g., a news article adopting the  X  Wall Street Journal Formula  X  ( Rich and Harper , 2007 ) where conceptually related sentences are pl aced at the beginning and the end , sentence conceptual relatedness does not necessarily correlate with spatial proximity and thus selected sentences may need to be rearranged for better readability. We are not aware of any published work that has empirical ly compared alternative ways of sentence ordering for single -document summarization. The experimental results reported below may draw some attention to this taken -for -granted issue . 3.1 Data and Method We prepared 3 datasets of 60 documents each , the first ( D4 00 ) consist ing of d ocuments of about 400 words from the Document Understanding Conference ( DUC ) 01 /02 data set s; t he second (D 1k ) consist ing of documents of about 1000 words manually selected from popular English journals such as The Wall Street Journal , Th e Washington Post , etc ; the third ( D2k ) consist ing of documents of about 2000 w ords from the DUC 01/02 data set. Then we generated 100 -word summaries for D400 and 200 -word summaries for D1k and D2k. Since sentence selection is not our focus, the 18 0 summari es were all extracts produced by a simple but robust summarizer built on term frequency and sentence position (Aone et al. , 1999).

Three human annotators were employed to each provide reference orderings for the 180 summaries and mark paragraph (of at leas t 2 sentences) boundaries, which will be used by one of the evaluation metrics described below. 
In our implementation of the grouping -based ordering, sentences are represented as entity vectors and the threshold t = ( ( ) , ) the averag e sentence similarity in a group multiplied by a coefficient empirically decided on separate held -out datasets of 20 documents for each length category . The  X  group -representing sentence  X  is the textually earliest sentence in the group. We experimented with both CC and MKM to generate sentence groups and all the proposed algorithms in 2 .3 for group -level and sentence -level orderings, resulting in 8 combinations as test orderings , each coded in the format of  X  Grouping (CC/MKM) / Group ordering (T/G) / Sentenc e ordering (T/G ) X  , where T and G represent the text order approach and the greedy selection approach respectively. For example,  X  CC/T/G  X  means grouping with CC, group ordering with text order, and sentence ordering with the greedy approach.
W e evaluated th e test orderings against the 3 reference orderings and compute the average (Madnani et al., 2007) by using 3 different metrics .
The first metric is Kendall  X  s  X  (Lapata 2003, 2006), which has been reliably used in ordering evaluations (Bollegala et al., 20 06; Madnani et al., 2007). It measures ordering difference s in terms of the number of adjacent sentence inversions necessary to convert a test ordering to the reference ordering.
In this formula, m represents the number of inver sions described above and N is the total number of sentences.

The second metric is the Average Continuity ( AC ) proposed by Bollegala et al. (2006), which captures the intuition that the quality of sentence orderings can be estimated by the number of correc tly arranged continuous sentences.
In this formula, k is the maximum number of continuous sentences,  X  is a small value in case P n = 1. P n , the proportion of continuous sentences of length n in an ordering, is define d as m /( N  X  n + 1) where m is the number of continuous sentences of length n in both the test and reference orderings and N is the total number of sentences. Following (Bollegala et al., 2006), we set k = M in (4, N ) and  X  = 0.01.
 the continuous sentences in a paragraph marked by human annotators, because paragraphs are local meaning units perceived by human readers and the order of continuous sentences in a paragraph is more strongly grounded than the order of co ntinuous sentences across paragraph boundaries. So in -paragraph sentence continuity is a better estimation for the quality of sentence orderings. This is our third metric: Paragraph -level Average Continuity ( P -AC ). H ere PP n = m  X  /( N  X  n + 1), where m  X  is the number of continuous sentences of length n in both the test ordering and a paragraph of the reference ordering. All the other parameters are as defined in AC and P . 3.2 Results The following table s show the results me asured by each metric. For comparison, we also include a  X  Baseline  X  that uses the text order. For each dataset, t wo -tailed t -test is conducted between the top scorer and all the other orderings and statistical significance ( p &lt; 0.05) is marked with *.
In general, our grouping -based ordering scheme outperforms the baseline for news articles of various lengths and statistically significant improvement can be observed on each dataset. This result casts serious doubt on the widely accepted practice of taking the text order for single -document summary generation, which is a major finding from our s tudy.

The three evaluation metrics give consistent results although they are based on different observations. The P -AC scores are much lower than their AC counterparts because of its strict paragraph constraint.

Interestingly, applying the text order poste rior to sentence grouping for group -level and sentence -level ordering leads to consistently optimal performance, as the top scorers on each dataset are almost all  X  __/T/T  X  . This suggests that the textual realization of coherence can be sought in the source document if possible , after the selected sentences are rearranged. It is in this sense that the general intuition about the text order is justified. It also suggests that tightly knit paragraphs (groups), where the sentences are closely connected, play a crucial role in creating a coherence flow. Shuffling those paragraphs may not affect the final coherence 1 . The grouping method does make a difference. While CC works best for the short and long datasets (D400 and D2k), MKM is more effective for the medium -sized dataset D1k. Whether the difference is simply due to length or linguistic/stylistic subtleties is an interesting topic for in -depth study. We have established a grouping -based ordering scheme to accommodate both local and global coherence for summary generation. Experiments on single -document summaries validate our approach and challenge the well accepted text order by the summarization community.

Nonetheless, the results do not necessarily propagate to multi -document summa rization, for which the same -document clue for ordering cannot apply directly . Adapting the proposed scheme to multi -document summary generation is the ongoing work we are engaged in. In the next step, we will experiment with alternative sentence represent ations and ordering algorithms to achieve better performance. 
We are also considering adapting more sophisticated coherence -oriented models, such as ( Soricut and Marcu , 2006; Elsner et al., 2007), to our problem so as to make more interesting comparisons possible.
 The reported work was inspired by many talks with my supervisor, Dr. Wenjie Li, who saw through this work down to every writing detail. The author is also grateful to many people for assistance. You Ouyang shared part of his sum marization work and helped with the DUC data. Dr. Li Shen, Dr. Naishi Liu, and three participants helped with the experiments. I thank them all.

The work described in this paper was partially supported by Hong Kong RGC Projects (No. PolyU 5217/07E).
