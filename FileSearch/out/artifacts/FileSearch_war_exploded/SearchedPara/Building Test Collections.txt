
While existing test collections and evaluation conference efforts may sufficiently support one X  X  research, one can easily find oneself wanting to solve problems no one else is solving yet. But how can research in IR be done (or be published!) without solid data and experiments? Not everyone can talk TREC, CLEF, INEX, or NTCIR into running a track to build a collection.

This tutorial aims to teach how to build a test collection using resources at hand, how to measure the quality of that collection, how to understand its limitations, and how to communicate them. The intended audience is advanced stu-dents who find themselves in need of a test collection, or actually in the process of building a test collection, to sup-port their own research. The goal of this tutorial is to lay out issues, procedures, pitfalls, and practical advice.
Attendees should come with a specific current need for data, and/or details on their in-progress collection building effort. The first half of the course will cover history, tech-niques, and research questions in a lecture format. The sec-ond half will be devoted to collaboratively working through attendee scenarios.

Upon completion of this tutorial, attendees will be famil-iar with the history of the test collection evaluation paradigm; understand the process of beginning from a concrete user task and abstracting that to a test collection design; under-stand different ways of establishing a document collection; understand the process of topic development; understand how to operationalize the notion of relevance, and be familiar with issues surrounding elicitation of relevance judgments; understand the pooling methodologies for sampling docu-ments for labeling, and be familiar with sampling strategies for reducing effort; be familiar with procedures for measur-ing and validating a test collection; and be familiar with current research issues in this area.
 Categories and Subject Descriptors H.3.3 [ Information Search and Retrieval ]: Misc. Keywords: test collections; experimental methods 1. Introduction to test collections : basic concepts: task, documents, topics, relevance judgments, and mea-sures; history of Cranfield paradigm. 2. Task : a task-centered approach to conceiving test col-lections; metrics as an operationalization of task suc-cess; understanding the user task and the role of the system. 3. Documents : the relationship between documents and task; naturalism vs constructivism; opportunity sam-pling and bias; distribution and sharing. 4. Topics : designing topics from a task perspective. sources for topics. exploration or topic development. extract-ing topics from logs. topics and queries. topic set size. 5. Relevance : defining relevance and utility starting from the task; obtaining labels, explicit and implicit elic-itation (highlighting); Interface considerations; inter-annotator agreement; errors; crowdsourcing for rele-vance judgments; validation, process control, quality assurance; annotator skill set. 6. Pooling : problem of scale and bias; breadth of pools, multiple systems; completeness vs. samples; methods; estimating pool depths; leave-one-out test, reusability; double pooling. 7. Analysis : bounding the score resolution; contrast-ing many different systems or variations of a system; PCA of topic:run scores, bounding of topic breadth; ANOVA, factor analysis, regression; statistical signif-icance and meaningful differences; calibration by user pilot study; per-topic analysis and failure analysis; bias. 8. Test collection diagnosis : LOO test revisited; un-judged == irrelevant; differentiating poor systems from collection bias. 9. Validation : user study; side-by-side comparison; a/b testing; interleaving. pooling as optimization; move-to-front pooling; uni-form sampling, stratified sampling, measure sampling; minimal test collections. tem adaptation; sessions, time, user adaptation; con-text, feedback; exploration and fuzzy tasks; novelty, differential relevance; fundamental limits of Cranfield.
