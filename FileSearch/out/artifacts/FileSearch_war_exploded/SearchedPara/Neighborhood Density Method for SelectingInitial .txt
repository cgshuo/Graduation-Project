 k -means clustering [1, 2] is one of the most widely used clustering methods in data mining, due to its efficiency and scalability in clustering large datasets. One well known problem of using k -means is selecting initial cluster centers for the iterative clustering process. Given a proper k , the clustering result of k -means is very sensitive to the selection of initi al cluster centers because different ini-tial centers often result in very different clusterings. In k -means clustering and other clustering methods, it is assumed that clusters distribute with certain high density in the data. Therefore, the k -means clustering process would produce a better clustering result if the initial cluster centers were taken from each high density area in the data. However, the cu rrently used initial cluster center selec-tion methods can hardly achieve this. Better selection of initial cluster centers for k -means clustering is still an interest ing research problem because of the importance of k -means clustering in real word applications [3, 4, 5, 6, 7, 8, 9]
In this paper, we propose a neighborhood density method for effectively select-ing initial cluster centers in k -means clustering. The me thod is to use the recently published Neighborhood-Based Clustering ( NBC ) algorithm [10] to search for high density neighborhoods from the data. NBC not only identifies all high density neighborhoods but also gives the central points of each neighborhood. Therefore, the neighborhood central points are used as the initial cluster centers. Since NBC determines neighborhoods based on lo cal density, clusters of different densities are taken into account. A new clustering algorithm called NK-means is developed to integrate NBC into the k -means clustering process to improve the performance of the k -means algorithm while preserving the k -means efficiency. To enhance NBC  X  X  search for dense neighborhoods, we have developed a new cell-based neighborhood search method to accelerate the search for initial clus-ter centers. A merging method is also employed to filter out insignificant initial centers to avoid too many clusters to be generated. Because the initial cluster centers are taken from the dense areas of the data, NBC enables the k -means clustering process to take less iterations to arrive at a near optimal solution, therefore, improving k -means clustering accuracy and efficiency.

We experimented NK-means with synthetic data. In comparison with the simple k-means and the refinement k-means algorithms [6], NK-means produced more accurate clustering results. It also showed a linear scalability in clustering data with varying sizes and dimensions. These results demonstrated that using the neighborhood density method to select in itial cluster centers can significantly improve the performance of the k -means clustering process.

The rest of this paper is organized as follows. Section 2 describes NBC for initial cluster center select ion, and the enhancement of NBC on search method. The merging process of insignificant initial clusters is also discussed. Section 3 defines the NK-means algorithm. Experimental results and analysis are pre-sented in Section 4. In Section 5, we s ummarize this work and point out the future work. 2.1 Search for Initial Cluster Centers with NBC The Neighborhood Based Clustering algorithm, or NBC , is a density based clus-tering method [10]. Unlike other density based methods such as DBSCAN [11], NBC finds clusters from data with respect to the local density instead of the global density. As such, it is able to discover clusters in different densities.
The locally dense neighborhood of a given point p is identified by the Neigh-borhood Density Factor ( NDF ), defined as: where kNB ( p )isthesetof p  X  X  k -nearest neighbor points, and R  X  kNB ( p )isthe set of the reverse k -nearest neighbor points of p . R  X  kNB ( p ) is defined as the set of points whose k -nearest neighborhoods contain p .Thevalueof NDF ( p ) measures the local density of the object p . Intuitively, the larger | R  X  kNB ( p ) | is, the more neighborhoods that contain p in their k -nearest neighbors, the denser p  X  X  neighborhood is. Generally speaking, NDF ( p ) &gt; 1 indicates that p is located in a dense area. NDF ( p ) &lt; 1 indicates that p is in a sparse area. If NDF ( p )=1, then p is located in an area where points are evenly distributed in space. The details of the ( NBC ) algorithm is given in [10].

Given a data set X ,wecanuse NBC to find all locally dense areas. In each dense area, we select its center as the candidate initial cluster center in k -means clustering. 2.2 Merging Candidate Clusters In the original NBC algorithm, the size of a neighborhood is specified by an input parameter. We use k nbc for this parameter here to distinguish the k parameter of the k -means algorithm. k nbc specifies the minimal number of points in a neighborhood and controls the granularity of the final clusters by NBC .If k nbc is set large, a few large clusters are found. If k nbc is set small, many small clusters will be generated.

Let { C 1 ,C 2 ,...,C i ,...,C k } be k candidate clusters generated from a sample data by NBC . Assume k is greater than the exp ected cluster number k .Each C i is defined as: where z i is the center of cluster C i and x j  X  z i 2 represents the distance between the object x j and z i . The similarity between two clusters c i and c j is calculated as
To reduce the number of candidate clusters k to the expected number k ,we can iteratively merge the two most similar clusters according to Formula (3). One merging procedure of the entire merging process is given in Table 1.
Each run of the merging procedure merge s two most similar clusters. To get the final k clusters the merging pr ocedure is repeated k  X  k times in the NK  X  means algorithm (see Table 2). Steps 1-6 of this procedure allocate the data points of the entire data set into k initial clusters. Steps 7-10 recompute k new cluster centers and radius. Steps 11-15 me rges the most similar clusters according to Formula (3). 2.3 Enhancement of NBC for Neighborhood Search To identify the dense neighborhood sindatarequirestocalculatethe NDF value for every point. This is a very time-consuming process. In NBC , a cell-based approach is adopted to facilitate the calculation of NDF and the k -nearest neighborhood search [10]. In this approach, the data space is divided into hy-percube cells of equal sides in each dimension. Search for dense neighborhoods is conducted in the cells instead of the entire space, so that the search time is reduced.

Let n be the number of points in the data set and m the number of dimensions of the data space. Given k nbc as the number of points in a neighborhood, the ideal way is to divide the data space into n/k nbc cells and each cell contains only one dense neighborhood with k nbc points. To obtain the same number of divisions in each dimension, the number of intervals in each dimension is calculated as:
Each dimension can be divided into  X  equal intervals, and the n points will be divided by each dimension into  X  subsets, { p i 1 ,p i 2 ,...,p ij ,...,p i X  } where  X   X  2. Because each dimension is equally divided, the data density in each subset p ij will be very different, depending on the distribution of the data. The problems of this approach are that it results in more cells to search because  X  m &gt;&gt; n/k nbc in high dimensional data and that it is still time consuming in searching dense neighborhoods in high density cells. To solve these two problems, we use a density-aware approach to divide the dense areas into more cells and the sparse areas into few cells. In this way, we can obtain a division with the number of cells close to n/k nbc . The search efficiency is improved significantly.
From the initial equal division of  X  m cells, we define a distribution balance factor for each dimension as: where  X  i denotes the distribution balance factor for dimension i , s ( p ij )isthe number of points in cell( i, j ), n is the number of points in the data set,  X  is a normalization factor to avoid the zero value of s ( p ij ).

After sorting the distribution balance factors for all dimensions as  X  1  X   X  2  X   X  X  X  X   X  m , we calculate the relative division rate for each dimension as: where  X  is the base number, defined as:
According to the relative division rate  X  , each dimension will be divided into  X   X   X  intervals, and the total number of cells will be:
We only select the first L dimensions to divide, which makes L i =1  X  i  X   X   X  n/k nbc and L  X  1 i =1  X  i  X   X &lt;n/k nbc . Therefore, the total number of cells will be close to n/k nbc .

Based on the above calculations, we can obtain a set of new cells based on merging the sparse cells and redividing the dense cells of the initial division. We scan all the cells to calculate the number of points in each cell. The cells with few points will be merged with adjacent cells, while the cells with too many points (more than k nbc ) will be recursively re-divided using a cell adjustment method.
Fig. 1 illustrates the process of this approach. The left figure is a data set with 1600 points. The middle figure is the initial division (first step). As the data set distributes more evenly on the vertical dimension than on the horizontal dimension, the vertical dimension is divided into 20 intervals ( k nbc = 10) while the horizontal dimension into 9 interva ls. In the readjustment (second step), the dense areas are re-divided into more cells while sparse areas are merged into fewer cells. Because the dense cells co ntain less points, the search for dense neighborhoods can be improved. Searching the k NB in sparse and wide cells will consider the neighbor cells, which is also very efficient. The NK-means clustering algorithm combines the enhanced NBC algorithm with the k -means algorithm to cluster larger high dimensional data. The en-hanced NK-means is used to select initial cluster centers from the sample data for the k -means algorithm to cluster the large data. Since the enhanced NK-means produces better initial cluster centers that are taken from the high density areas of the data, starting from these initial clusters, the k -means algorithm will produce better clustering results. Because sample data is used and the search method is enhanced, the initial cluster selection does not add too much com-putation burden to the entire clustering process but significantly improve the k -means clustering results. This has b een demonstrated by our experiments.
Table 2 gives the pseudo-code of the NK-means algorithm. Given an input data set X , a sample rate and an expected cluster number k , the algorithm first takes a sample from X . Then, the sample is fed to the enhanced NBC algorithm to produce a set of initial candidate clusters. The third step is to calculate the centers of the candidate clus ters. After the centers are calculated, they are readjusted with the entire data set X and the cluster merging process starts in Steps 4-7 (note that the merging procedure in Table 1 is executed for k  X  k times). After the merging process, a new set of initial cluster centers are obtained and used in the k -means algorithm in Step 9 as the initial cluster centers to cluster the entire data set X . In the next section, we will show the experiment results of the NK-means algorithm. We have implemented the NK-means clustering algorithm in Java and conducted experiments with synthetic data. I n these experiments, we compared NK-means with two other k-means algorithms with different initial cluster center selec-tion methods: randomk-means using the simple initial cluster center selection method [12], and Bradly X  X  refinementk-means algorithm [6]. We also conducted scalability tests of NK-means against different data sizes and dimensions.
We used Matlab to generate synthetic data sets with mixture Gaussian dis-tributions. We first carried out experiments on a two-dimension data set that contained 8,000 objects in eight inherent clusters. To test the robustness of the algorithm, we also added some noise into the data set. Fig. 2(a) shows the dis-tribution of this data set with a noise rate of 10%. The solid cycles are the real centers of the clusters.

Fig. 2(b), 2(c) and 2(d) show the clustering results from the random k-means , the refinement k-means and the NK-means respectively. The solid cycles repre-sent the inherent cluster centers while the star symbols in these figures give the initial cluster centers selected by the thr ee clustering algorithms. We can observe from the figures that the initial c luster centers selected by the NK-means are very close to the inherent cluster centers in the data. Some of the initial cluster centers selected by the random k-means , the refinement k-means were located outside of some inherent clusters. For ex ample, no initial cluster centers were selected from the two middle inherent clusters in Fig. 2(b). Because of this, the two inherent clusters were clustered into one cluster by the random k-means . Four initial cluster centers were select ed from the large inherent cluster on the upright corner of Fig. 2(b). This cluster wa s clustered into 3 clusters. Therefore, the random k-means could not recover the eight inherent clusters because of the bad selection of the initial cluster centers.

Fig. 2(c) shows that the refinement k-means could not recover the inherent clusters neither, because of the improper selection of the initial cluster centers. In this case, the two large clusters on the top were clustered into four small clusters, while the two middle small inherent clusters were clustered as one cluster. From Fig. 2(d), we can see that all eight inherent clusters were completely recovered by NK-means , due to the good selection of the initial cluster centers.
Table 3 lists the locations of real cluster centers and the final centers found by the three clustering algorithms. The final cluster centers by the three algorithms were calculated as the average values of 100 runs on the same data set. The final cluster centers by the NK-means were clearly very close to the real cluster centers, while the final cluster centers by other two algorithms were different.
To test the scalability of NK-means , we generated one data set with 600,000 normally distributed points in six clusters, and with additional 60,000 noise points. Each point is described in 10 dimensions.

Fig. 3 shows the scalability test results against the number of points and the number of dimensions in data. Fi g. 3(a) plots the running time against different numbers of points, while Fig. 3(b) is the running time against different dimensions. These results show that the running time of NK-means linearly increased with the number of points and the number of dimensions. This property indicates that NK-means is scalable to large high-dimensional data. In this paper, we have proposed a new neighborhood density method for selecting initial cluster centers for k-means clustering. We have presented the NK-means algorithm that makes use of the neighborhood-based clustering algorithm to select initial cluster centers and use the centers as input to the k -means clustering algorithm to improve the clustering performance of k -means. We have shown the experiments on both synthetic and real data to demonstrate that NK-means was superior to the other two algorithms: the random k-means and the refinement k-means .

We have also discussed the enhancement of NBC  X  X  neighborhood search method and the merging process to generate the in itial cluster centers. This enhancement enables NBC to take a larger sample which can result in better initial cluster cen-ters. The next stage is to develop a termination method in the merging process to automatically generate the e xpected number of clusters k which has been a long standing problem in k -means clustering.

