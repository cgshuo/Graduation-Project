 Discovering links between different data items in a single d ata source or across different data sources is a challenging problem fa ced by many information systems today. In particular, the recent L inking Open Data (LOD) community project has highlighted the param ount importance of establishing semantic links among web data so urces. Currently, LOD sources provide billions of RDF triples, but only millions of links between data sources. Many of these data so urces are published using tools that operate over relational data stored in a standard RDBMS. In this paper, we present a framework for discovery of semantic links from relational data. Our frame work is based on declarative specification of linkage requiremen ts by a user. We illustrate the use of our framework using several li nk dis-covery algorithms on a real world scenario. Our framework al lows data publishers to easily find and publish high-quality link s to other data sources, and therefore could significantly enhance the value of the data in the next generation of web.
 H.2.4 [ Database Management ]: Systems Algorithms Semantic link discovery, record matching, linked data, dat a man-agement
From small research groups to large organizations, there ha s been tremendous effort in the last few years in publishing data on line so that it is accessible to users. These efforts have been widel y suc-cessful across a number of domains and have resulted in a prol if-eration of online sources. In the field of biology, there were major molecular databases at the beginning of 2008, 110 more than a year earlier. In the field of medicine almost every major hos pital now has its own database of patient visits and clinical trial s. In the Linking Open Data (LOD) community project at W3C, the number of published RDF triples has grown from 500 million in May 200 7 to over 4.7 billion triples in May 2009 [6].

While publishing data is now easier than ever, attempts to es tab-lish semantic relationships between different published d ata sources has been less successful. So, from a user X  X  perspective, onl ine sources resemble islands of data (or data silos ), where each is-land may contain only part of the data necessary to satisfy hi s or her information needs. Penetrating these silos to both unde rstand their contents and understand potential semantic connecti ons is a daunting task. Consider a biologist interested in a specific gene. It is not enough for the biologist to search for the gene, even us ing a robust search that accommodates for aliases and errors in th e repre-sentation of data which are very common in web repositories. Both errors and aliases are domain specific so the biologist may ha ve to try several approximate search methods to find one best for her do-main. Furthermore, she may also want to find information abou t proteins or genetic disorders that are known to be related to this gene. Again, the search for this semantically related infor mation must be tolerant of aliasing and errors, and yet must be tailo red to the specific semantic relationships the user wishes to find.
What users need is automated support for creating referenti al links between data that reside in different sources and that are se-mantically related. Such links would provide a biologist wi th the ability to start from a gene and directly navigate to its prot ein and related genetic diseases, even through sources with no dire ct con-nection and which may use different naming conventions and d if-ferent representations for information. Of course, discov ering such links requires the use of both approximate matching (to over come syntactic representational differences and errors) and se mantic match-ing (to find specific semantic relationships). These two type s of matching must be used in concert to accommodate for the treme n-dous heterogeneity found in web repositories.

In spite of their importance, research in discovering such s eman-tic links has mainly focused on a more restricted version of t he problem, namely, on entity resolution , i.e., the identification of en-tities in disparate sources that represent the same real-world entity. Yet, the general problem investigated here considers links between entities that are not necessarily identical, although they are seman-tically related (e.g., genes related with their correspond ing proteins, medical treatments related with their corresponding clini cal trials, etc.) The importance of discovering such links is also highl ighted by the recent efforts of the LOD project, where a number of too ls and frameworks have been developed that allow the generatio n and publication of linked data from relational databases. Exam ples of such frameworks include D2RQ [7], Triplify [2] and OpenLink  X  X  lishing linked data, the number of links between existing LO D data sources is two orders of magnitude less than the number of bas e data triples. The majority of the links are either a result of exist-ing links in the relational data (e.g., links between two dat a items that are both derived from a Wikipedia page, both having the s ame URL), the result of (manual or automated) semantic annotati on [9], or a laborious implementation of a semi-automatic and domai n-specific linkage algorithm.

For typical users, this means they must experiment with a myr -iad of different link discovery methods to find one that suits their needs. In this paper, we seek to develop a generic and extensi ble framework for integrating link discovery methods. Our goal is to facilitate experimentation and help users find and combine t he link discovery methods that will work best for their application domain. To ease experimentation, we use a declarative framework tha t per-mits the interleaving of standard data manipulation operat ors with link discovery.

Our framework permits the discovery of links within and be-tween relational sources. We introduce LinQL , an extension of SQL that integrates querying with link discovery methods. Our i m-plementation includes a variety of native link discovery me thods and is extensible to additional methods, written in SQL or as user-defined functions (UDF). This permits users to interleave de clar-ative queries with interesting combinations of link discov ery re-quests. The link discovery methods may be syntactic (approx imate match or similarity functions), semantic (using ontologie s or dic-tionaries to find specific semantic relationships), or a comb ination of both.

Our first contribution is to show that by integrating ad hoc query-ing and a rich collection of link discovery methods, our fram ework supports rapid prototyping, testing and comparison of link discov-ery methods. A common way to use our framework would be to declaratively specify a portion of the data of interest (ove r which accuracy can be assessed) and to invoke one or more link disco very methods. The results can be evaluated by a user or automated t ech-nique, and the specification of the link method interactivel y refined to produce better results.

Often, link discovery algorithms are implemented using pro gram-ming languages like Java or C by third-party developers, and are au-tomatically invoked with arguments defined through the decl arative specification. For data publishers these programs act as black-boxes that sit outside the data publishing framework and whose mod ifica-tion requires the help of these developers. Our second contr ibution addresses these shortcomings by leveraging native SQL implemen-tations for a number of link discovery algorithms [12, 13]. O ur approach has several advantages including the ability to (a ) eas-ily implement this framework on existing relational data so urces with minimum effort and without any need for externally writ ten program code; (b) take advantage of the underlying DBMS opti -mizations in the query engine while evaluating the SQL implemen-tations of the link finding algorithms; and (c) use specific ef ficiency and functionality enhancements to improve the efficiency of these algorithms.

Our third contribution is to show how our declarative invoca tion of link methods permits users to tune link methods and their p er-formance. The native support for methods permits customiza tion where domain knowledge is available. We give examples where domain knowledge can be specified in the database and used to greatly enhance the performance of the discovery process.
Our final contribution is the implementation of our framewor k along with several link discovery algorithms on a commercia l database engine. We describe a case study of how our framework can be us ed to discover links over real clinical trial data draw from a nu mber of disparate web sources.

The rest of the paper is organized as follows. Section 2 intro -duces our running example, while Section 3 describes how lin ks be-tween data sources can be specified declaratively. Section 4 presents the algorithms for translating the link specifications into SQL queries. Our experimental study is described in Section 5. Section 6 h igh-lights related work and we conclude in Section 7.
Through out the paper (and in our case study in Section 5), we use an example from the health care domain drawn from a set of real-world data sources. One of our sources is a clinical tri als database which includes the sample relation in Figure 1(a). For each trial, the CT relation stores its identifier trial , the cond ition considered, the suggested inter vention, as well as the loc ation, city , and related pub lication. Another source stores patient electronic medical records (EMR) and includes a patient visit relation PV (Figure 1(b)), which stores for each patient visit its ident ifier visitid , the diag nosis, recommended prescr iption, and the location of the visit. Finally, we consider a web source extracted from DBpe dia (or Wikipedia) which stores information about drugs and dis eases and includes the DBPD and DBPG relations (Figure 1(c) and (d)) that store the name of diseases and drugs in DBpedia, respectively.
We now describe briefly some types of links data publishers ma y like to discover between these sources. For the CT and PV rela-tions, we note that the cond ition column in the CT relation is se-mantically related and can be linked to the diag nosis column in the PV relation. Such links may be useful to clinicians since they a s-sociate a patient X  X  condition with related clinical trials , and might be used to suggest alternative drugs or interventions. In Fi gure 1, patient visit  X  X ID770 X  with diagnosis  X  X halassaemia X  in th e PV relation should be linked to the trial  X  X CT00579111 X  with co ndi-tion  X  X ematologic Diseases X  since  X  X halass a emia X  is a different representation of  X  X halassemia X  and according to the NCI me di-cal thesaurus  X  X halassemia X  is a type of  X  X ematologic Diseases X . As this example illustrates, a clinician may be interested i n not only same-as relationships, but also hyponym relationships such as type-of . Similarly, note that the inter vention column in the CT relation can be linked to the prescr iption column in the PV relation. Such links can provide evidence for the relevance and effectiven ess of a drug for a particular condition. For example, both patient v isits in Figure 1(b) should link to trial  X  X CT00336362 X  (Figure 1(a) ) based on the fact that  X  X ydroxycarbamide X ,  X  X ydroxyura X  and  X  X yd rox-yurea X  all refer to the same drug.

Additional links are possible if one considers the existenc e of links between the locations of patients and the presence of c linical trials in these locations. As an example,  X  X estchester Med. Ctr X  from visit  X  X ID777 X  could link to  X  X olumbia University X  bas ed on the geographical information that both locations are in t he New York state. Another interesting link discovery scenario ar ises when a user who is interested in a particular trial, wants to find ot her related trials based on certain criteria, e.g., the similar ity of the title and authors of the trials X  corresponding publications. Obv iously, to be effective, links should be tolerant of errors and differe nces in the data, such as typos or abbreviation differences.
Data publishers often build online web-accessible views of their data. In such settings, they often want to provide links betw een their data and those in other online web sources. As an exampl e, a web source of clinical trials requires links to other web so urces related to the trials like, say, the DBpedia or YAGO sources. In our sample relations, the above example translates into finding links be-tween the cond and inter columns of CT and the name column of the DBPD and DBPG relations, respectively. The online trials data source can link the condition  X  X ematologic Diseases X  to DBp edia resource (or Wikipedia page) on  X  X lood_Disorders X , and lin k the intervention  X  X ampath X  to DBpedia resource  X  X lemtuzumab X  us-ing the semantic knowledge that  X  X ampath X  is the brand name f or the chemical name  X  X lemtuzumab X .
In this section, we introduce the LinQL declarative link specifi-cation language. We discuss the characteristics of the link finding primitives required by a flexible framework that is capable o f ef-fective link discovery in many real world scenarios, and sho w how LinQL supports such primitives. Although the linkage specificati on elements we discuss below are expressible in a number of diff erent languages and notations (e.g., RDF/XML, N3, NTriples), in o ur framework (and implementation) we chose an SQL -like syntax.
A link specification , or linkspec for short, defines the conditions that two given values must satisfy before a link can be establ ished between them. In more detail, a linkspec is defined using the g ram-mar of Figure 2 (the full grammar is omitted due to space con-straints). As shown in the figure, a CREATE LINKSPEC statement de-fines a new linkspec and accepts as parameters the name of the linkspec and the names of the relation columns whose values n eed to be linked. To create the links, our framework provides sev eral native (or built-in ) methods including synonym, hyponym, and a variety of string similarity functions (see more details on the native methods in the following sections). Such native methods can be used as such or they can be customized by setting their parame ters.
E XAMPL E 1. A common string similarity measure that has been shown to have good accuracy and efficiency is the weighted-Ja ccard measure [13]. A user can create a link specification using thi s mea-sure by setting the parameters used in the similarity comput ation. For example, she may set the threshold parameter to 0 . 5 of the q-gram to 2 , and the maximum string length to 50 , creating the following link specification
Now this link specification can be used as a join predicate in queries by any user. Notice that this specification does not i ndicate processing constraints.
A link specification can also be defined in terms of link clause expressions . Link clause expressions are boolean combinations of link clauses, where each link clause is semantically a boole an con-dition on two columns and is specified using either (a) a nativ e method; (b) a user-defined function (UDF); or (c) a previousl y de-fined linkspec.

E XAMPL E 2. Consider a setting in which a link between two values is established if a semantic relationship (e.g., syn onym or hyponym) exists between these values in an ontology. This sc enario commonly occurs in a number of domains, including healthcar e, where sources are free to use their own local vocabularies (e .g., diagnosis and drug names) as long as these vocabularies can e ven-tually be matched through a commonly accepted ontology (e.g ., the NCI thesaurus). Assume that the ontology is stored in table with concept IDs in column cid and the terms in column term
The following linkspec illustrates the power of link clause s by creating a link between two values if their corresponding va lues in an ontology are synonyms of each other. In the linkspec, th e weightedJaccard native linkspec is used to match individua l values to corresponding values in the ontology, while the synonym n ative linkspec is used to test for the synonymy of the ontology term s.
Clearly, the links between values are not necessarily one-t o-one and, in general, a value from one relation can be linked to mor e than one values from a second relation. For example, it is com mon for drugs to have more than one name. Therefore, while a drug appears as  X  X spirin X  in one relation it might appear as  X  X cetylsal-icylic acid X  or  X  X SA X  in another. When multiple such links are possible, users often want to limit the number of such links a nd only consider k results, or the top-k where ordering is possible. The LINKLIMIT essentially specifies the value of this k parameter.
E XAMPL E 3. In the previous example, while defining the mix-match linkspec, LINKLIMIT is set equal to 10, for all three link clauses. Hence, only the top 10 links are considered in each m ethod.
The previous examples consider the local version of the LINKLIMIT construct which is associated with a particular clause. The LinQL grammar also includes a global LINKLIMIT construct which is as-sociated with the whole CREATE LINKSPEC statement which can be thought of as a post-processing filter of the links returned b y the linkspec methods used in the statement.

We conclude the presentation of LinQL by introducing Boolean valued user-defined functions (UDFs). The primary differen ce be-tween using a UDF versus a native linkspec method is that the UDF allows only simple substitution-based rewriting of the query, whereas the native linkspec uses a non-trivial rewriting of the query into SQL (see next section on the translation of LinQL to SQL ). The ability to use UDFs in link specification is provided for e x-tensibility to non-SQL-based linking functions. Of course , native implementations have numerous advantages. Using native im ple-mentations, the query optimizer can optimize the processin g of link specifications yielding very efficient execution times. Fur thermore, declarative implementations of the methods within a relati onal en-gine permit great extensibility as described in the followi ng section.
E XAMPL E 4. Suppose a user writes a UDF that implements his own similarity function. This UDF, myLinkUDF(thr, delC, insC, subC) returns true only if the edit similarity the values on which it is applied is above the threshold value thr (where and subC are the costs of delete, insert and substitute operations, respectively). Then, the following linkspec can use that UD F as follows
In the previous paragraphs, we looked at how linkspecs are de -fined. We now show how linkspecs are used inside queries.
E XAMPL E 5. Suppose we want to find the tuples in the PV re-lation for which there is a link with the condition in the CT re lation, using the native weightedJaccard linkspec with default par ameter values. Then, the following query can be used.

Notice that the linkspec here is essentially defined inline. For more complex linkspecs, or for situations where the same lin kspec is used multiple times by one or more users, the query can refe r to a previously defined linkspec in a similar fashion. This is a w ay for a DBA to provide a set of specifications for methods using t he best parameter settings for different domains, making thes e meth-ods more accessible to less expert users who may not know how t o set the parameters. For example, in the query above we can use the mixmatch linkspec instead of the weightedJaccard, as follows:
In what follows, we present the currently supported native m eth-ods, including our reasons for including them in the initial imple-mentation.
String data is prone to several types of inconsistencies and errors including typos, spelling mistakes, use of abbreviations o r differ-ent conventions. Therefore, finding similar strings, or app roximate string matching (or apprioxmate join), is an important feat ure of an (online) link discovery framework. Approximate string mat ching is performed based on a similarity function sim () that quantifies the amount of closeness (as opposed to distance ) between two strings. A similarity threshold  X  is set by the user to specify that there is a link from the base record to the target record if their simil arity score, returned by function sim () , is above  X  . The right value of the threshold depends on the characteristics of the datas et, the similarity function, and the application. The user can find t he opti-mal value of the threshold for each application by trying dif ferent thresholds and manually evaluating the results.

There exists a variety of similarity functions for string da ta in the literature. The performance of a similarity function us ually de-pends on the characteristics of data, such as length of the st rings, and the type errors and inconsistencies present in the data. As stated earlier, in our framework we are interested in algori thms that are fully expressible in SQL (the benefits of which are well-known [12]). There are additional benefits of this choice for our application. Specifically, the use of SQL as an implementation lan-guage for our methods permits on-the-fly calculations of the simi-larity scores that can be enhanced dynamically to increase t he func-tionality of the matching algorithm by relying on character istics of the domain of the source and target relations.

A popular class of string similarity functions is based on to k-enization of the strings into q-grams, i.e., substrings of l ength the strings. By using q-gram tokens, we can treat strings as s ets of tokens and use a set similarity measure as the measure of simi larity between the two strings. Furthermore, q-gram generation, s torage and set similarity computation can all be done in SQL . This makes the following class of functions suitable for our framework .  X 
Size of the intersection of the two sets of q-grams, i.e., the number of common q-grams in the two strings.  X 
Jaccard similarity of the two sets of q-grams which is the size of the intersection set over the size of the union, i.e., the per centage of common q-gram tokens between the two strings.  X 
Weighted version of the above measures. The weight of each q-gram is associated with its commonality in the base (or target or both) data sources. The higher the weight of a q-gram, the mor e important the q-gram is. For example, when matching diagnos is across medical sources, q-grams for commonly occurring str ings like  X  X isorder X  or  X  X ancer X  should have low weights so that t he value of the similarity function for the strings  X  X oagulati on Disor-der X  and  X  X honation Disorder X  is small, compared to that for the strings  X  X oagulation Disorder X  and  X  X oagulation Disease X  .
There are several other string similarity measures includi ng but not limited to Edit-Similarity, Jaro, Jaro-Winkler, SoftT FIDF, Gen-eralized Edit similarity, and methods derived from relevan ce score functions for documents in information retrieval, namely C osine with tf-idf, Okapi-BM25, Language Modeling and Hidden Mark ov Models. Some of these functions can be implemented in SQL and some others can only be implemented using a UDF. However, we focus only on the above q-gram based measures based on their b et-ter accuracy and efficiency and also their flexibility for fun ctional-ity and efficiency enhancements, as discussed below.
Token Weight Assignment: To assign weights to q-gram to-kens, we use an approach inspired by the Inverse Document Fre-quency (IDF) metric in information retrieval. IDF weights reflect the commonality of tokens in documents with tokens that occur more frequently in the documents having less weight. So, by ana-lyzing (offline) the q-gram token frequency, we assign less weight to common tokens like  X  X isorder X  or  X  X ancer X  in a medical source. As a functionality enhancement, we also let the user manually spec-ify in a user-defined table the weights of some tokens. These weights override the automatically assigned (IDF-based) weights for these tokens. Manual weight-assignment is useful in applications where the user has prior knowledge about the importance of some tokens. For example, when matching diagnosis across sources, the user knows that often the use of numbers plays a more important role in the diagnosis than the name of the disease itself. So, by assign-ing a very low (or negative) weight to numbers, wrong matches between highly similar strings like  X  X ype 1 Diabetes X  and  X  X ype 2 Diabetes X  can be avoided. Similarly, when matching conditions (e.g.,  X  X iabetes X ,  X  X ancer X ) from an online source such as WebMD to their corresponding entries in, say, Wikipedia, the conditions in Wikipedia might include the term  X (disease) X  to disambiguate the disease from other terms with the same name (e.g.,  X  X ancer X  has close to seven entries in Wikipedia, in addition to the one for the disease, including one for astrology and one for the constellation). Knowing this, the user can adjust the weight of the otherwise com-mon token  X  X isease X  to increase the likelihood of a correct link.
Scalability Although, we do not address explicitly the efficiency of the string matching implementation, our similarity predicates can be used along with several existing scalable indexing and hash-ing techniques. Examples of such techniques include the indexing algorithms of [3], the Weighted Enumeration (W T E NUM ) signature generation algorithm [1] and Locality Sensitive Hashing [14].
Link discovery between values often requires the use of domain knowledge. In a number of domains, there are existing, commonly accepted, semantic knowledge bases that can be used to this end. In domains where such semantic knowledge is not available, users often manually define and maintain their own knowledge bases.
A common type of such semantic knowledge is an ontology. In the health care domain, well-known ontologies such as the NCI thesaurus are widely used and encapsulate a number of diverse relationship types between their recorded medical terms, includ-ing, synonymy, hyponymy/hypernymy, etc. Such relationship types can be conveniently represented in the relational model and (re-cursive) SQL queries can be used to test whether two values are associated with a relationship of a certain type [15]. Therefore, semantic knowledge in the form of ontologies can be seamlessly incorporated in our framework and used for the discovery of links. So, while considering links between two sources, semantic knowl-edge can be used to link a diagnosis on  X  X ineoblastoma X  to one on  X  X NET of the Pineal Gland X , since the two terms are synonyms of each other. Similarly, a diagnosis on  X  X rain Neoplasm X  can be potentially linked with both of the previous diagnoses, since the latter term is a hypernym of the former terms. No level of sophis-tication in string matching can result in links such as the ones de-scribed earlier and therefore semantic matching complements the string matching techniques described in the previous section.
In what follows, we describe the algorithm for translating a LinQL query to an SQL query, and then describe the algorithm for imple-menting each native link specification. Finally, we outline a number of efficient strategies to combine some of the specifications.
Algorithm L IN QL2SQL translates a LinQL query to a SQL query by first splitting the former query into the base query (which is in SQL ) and the link clause expression. The latter is translated into SQL by L INK C L AUSE E XPR 2SQL by iterating through the boolean combination of link clauses and generating a separate SQL query for each clause. Then, the boolean combination of link clauses is translated into a set of intersections/unions between the generated SQL queries.

L INK C L AUSE 2SQL parses a link clause to determine what type of link terminal is used. If the link terminal is a UDF, we sim-ply add an invocation of the UDF in the where clause of the SQL base query. If the link terminal is a native link, we rewrite the SQL base query using the rewrite rules associated with that particular native link. If the link terminal is a reference to a named linkspec, we retrieve the associated linkspec statement and parse the asso-ciated link method. The link method can be a UDF, native link or link clause expression. UDFs and native links are translated as described previously. Link clause expressions are translated by a recursive call to the L INK C L AUSE E XPR 2SQL sub-routine. The recursion stops when either a UDF or a native link is encountered.
The main translation logic is in the rewriting rules associated with the native links. A native link X  X  rewriting rules are specified in two parts: view definitions and link conditions. For example, the rewriting rules for the weightedJaccard native link on tab1.col1 and tab2.col2 consists of the view definitions for tab1col1weights, tab1col1sumweights, tab1col1tokenweights, tab2col2tokens, scores, scores2 , and the link conditions scores.tid1=col1 AND scores.tid2=col2 AND s.tid1=s2.tid1 AND scores2.mx=s.score .

The use of the view definition syntax is purely for readability. In practice, the SQL queries associated with the view definitions are inlined into the actual query itself (resulting in a possibly hard to read query). The WITH statement supported by some DBMS (e.g. IBM DB2) is another means for inlining some of the view defini-tions. Depending on the application, one may choose to material-ize all or part of these views using link index statements in order to speed up the query time. We present the SQL queries based on view definitions in this section and discuss briefly the cost of ma-terializing these views in the experimental results. The rest of this section describes the rewriting rules used to implement some of our native link methods.
The rewriting of the approximate string matching native lin k specification into SQL consists of three steps, namely, (a) the cre-ation of tables containing the tokenization of the strings i nto q-grams or word tokens; (b) the gathering of statistics and cal culation of token weights from the token tables; and (c) the calculati on of link scores based on the weights. In more detail: Step 1: This step can be done fully in SQL using standard string functions present in almost every DBMS. Assume a table integers exists that stores integers 1 to N (maximum allowable length of a string). The main idea is to use basic string functions SUBSTR LENGTH along with the sequence of integers in table integers create substrings of length q from the string column col1 table1 . The following SQL code shows this idea for q = 3 In practice, the string col1 is used along with UPPER() (or functions to make the search case insensitive. Also, the str ing is padded with q  X  1 occurrences of a special character not in any word (e.g.  X $ X ) at the beginning and end using the CONCAT() tion. Similarly the spaces in the string are replaced by cial characters. In case of tokenization using word tokens a similar SQL-based approach can be used. At the end of this process, th e token generation queries are declared as views, or are mater ialized Steps 2 and 3: These steps are partly native-link specific and are more easily presentable through an example. In what follows , we use the weighted Jaccard specification as an example.
 the following LinQL specification: This specification is translated into the following SQL quer ies. Initially, two queries calculate the IDF weights for the tok ens and the auxiliary views/tables needed for the final score calcul ation: Then, the next query returns the links along with their final s cores:
Assume that the synonym and hyponym data are stored in two ta-bles synonym and hyponym with columns src and tgt . The column src contains concept IDs of the terms, and the column tgt the terms. This is a common approach in storing semantic know l-edge, used in NCI thesaurus and Wordnet X  X  synsets for exampl e. Alternatively, this data could be stored in a table thesaurus an additional column rel that stores the type of the relationship, or it could even be stored in XML. In the case of XML, synonym hyponym can be views defined in a hybrid XML relational DBMS such as DB2. For brevity, we limit our discussion in this pape r to semantic knowledge stored as relational data, although our frame-work is easily extensible to other formats. We show the detai ls of the SQL implementation of the synonym and hyponym native lin k specifications in the following two examples.
 following query written using LinQL .
 This query is rewritten to: following query written using LinQL .
 This query is rewritten to:
Note that the hyponym depth is by default set to 2, which could be customized to any other value.
The main goal of this section is twofold. First, we illustrat e the flexibility of our framework by applying it in a variety of lin kage scenarios. Second, we use these scenarios to justify our cho ices in terms of functionality for the various components in our f rame-work. We build our scenarios around an online database of cli nical trials published on ClinicalTrials.gov. This database is a registry of federally and privately supported clinical trials condu cted in re-search centers all around the world. It contains detailed in formation about the trials, including information about the conditio ns associ-ated with the trials, their eligibility criteria and locati ons.
The clinical trials database used in our experiments contai ns ap-proximately 61,920 trials. Originally, the database was in XML format. Using the functionality of DB2 as a hybrid relationa l-XML DBMS, we stored all the data in relational tables. Other data sets that we used in our experiments for linkage include a databas e of patient visits or Electronic Medical Records (EMR) and DBpe dia (Wikipedia) entries about diseases and drugs. We also used t he National Cancer Institute (NCI) X  X  thesaurus as a source of s eman-tic information about medical terms. Detailed statistics o n these datasets is shown in Table 1.

Due to privacy issues associated with EMR records, our patie nt visits database is synthetic, generated using a data genera tor that resembles real EMR records in a hospital. The diagnosis and p re-scription values are randomly picked by the data generator f rom NCI terms. The data generator also creates an additional col umn with a small random string error in the diagnosis field. The er ror injected in the string resembles real errors and typos occur ring in string databases, e.g., replacing a character with an adjac ent char-acter on a keyboard, or swapping two characters or word token s.
In what follows, we describe several link discovery scenari os in-volving clinical trials. While the first scenario is describ ed in more detail (including its intermediate steps and correspondin g linkage specifications), for the other scenarios we only show the fina l re-sults and only mention changes to preceding LinQL statements.
Case 1 (Linking patient visits to trial conditions) The objec-tive here is to discover links to clinical trials that are rel ated to the conditions of certain patients. For this study, we consider 1,000 random patients from table PV , where column DIAGNOSIS stores the condition associated with a patient X  X  visit. The CT table stores the trial condition in its column CONDITION . The records matched by a simple exact matching are obtained by the SQL query:
The query returns only 33 matches, linking only 2 out of 1,000 patient visit records to matching clinical trials. This is d ue to the string errors in DIAGNOSIS values. As a next step, we try an ap-proximate string matching predicate with a low similarity t hreshold using the following linkspec and query:
Since matching a condition to the right trial is imperative here, we are rather strict (conservative) in the application of ap proxi-mate matching. For example, links from  X  X lpa Thalassemia X  ( mis-spelled record of  X  X lpha Thalassemia X ) to  X  X lpha Thalassem ia X ,  X   X  -Thalassemia X  and  X  X halassemia X  are considered correct an d we would like to find them. However,  X  X eta Thalassemia X  is consi d-ered an incorrect link. The following accuracy results were ob-tained by investigating 100 random queries using different thresh-olds:
Therefore, by choosing a high threshold 0 . 70 , 22 links are re-turned out of which 20 ( 91% ) are correct. However, by choos-ing threshold 0 . 4 , 579 links are returned (more than each patient visit), but only 231 ( 40% ) of them are correct. Given these observations, a user can choose the appropriate thres hold that works best for the specific linkage needs. For example, we cho ose threshold 0 . 55 that returns on average almost one link per visit, and has a reasonable accuracy. As a result we will have 1,102 link s to clinical trials from 335 (out of 1,000) distinct patient vis its.
The next step is to use the semantic information in NCI to im-prove the matching using the LinQL query below:
The semantic matching based only on synonyms results in 147 links to 104 distinct trials. From these, 69 links to 24 disti nct trials could not be found using exact or string matching. Repeating the above query with semantic matching based on hyponyms of dept h 2 from NCI, results in 68 additional links to 21 distinct tria ls. One reason for the relatively low number of matches based on syno nyms and hyponyms is the string errors present in the DIAGNOSIS of the PV table. This calls for using string matching combined with semantic matching. The LinQL code to do this is:
Using combined string matching and semantic matching resul ts in 173 links to 120 distinct trials, 26 more links to 16 more di s-tinct trials when compared with matching based on synonyms o nly. Depending on the results of the above steps, the user can writ e a single query for the linkage needs specific to the applicatio n. Here we choose to combine exact matching, string matching, seman tic matching based on synonyms and hyponyms, and mixed semantic matching allowing string errors. This can all be expressed u sing the query below:
The combined approach results in 1,255 links from 383 visit records to the related clinical trials. Overall, we have:
These results can help a user better understand both her data and the (combinations of) link methods that are suitable for her needs.
Case 2 (Linking prescriptions to trial interventions) Now con-sider a user who wishes to link patients who were prescribed a drug with all clinical trials that use that drug. To collect all these tria ls, the user will need the results produced from a variety of algo rithms. A sample of such results is shown in the table below. These re-sults use threshold 0 . 6 for weightedJaccard string matching, and depth 1 for hyponym matching. The table summarizes the resul ts obtained for matching 1,000 random drug prescriptions:
Notice that some methods do find the same links (that is, the to tal is less than the sum of the methods). However, the overlap is n ot that big. For this application, if the goal is to find as many po ssible matches as possible, all four of these methods add value. Case 3 (Linking trial conditions to DBpedia diseases) and Case 4 (Linking trial interventions to DBpedia drugs) In these scenarios, we are seeking links from the clinical trials X  co ndition and interventions fields to the DBpedia (or Wikipedia) disea se and drug categories, respectively. Unlike the previous cases, assume here that the user only needs to link to a single DBpedia entry per each condition and drug. This makes sense since in most cases there should be a single record in DBpedia for a single diseas e (condition) or drug intervention in the trials data. Theref ore the user uses the LINKLIMIT 1 option in the LinQL query to limit the number of matches. Then, when an exact match is found for a record, there is no need to look for approximate string or sem antic matches for that record. Considering the running times repo rted in Section 5.4, this leads to a significant performance improve ment.
The linkage specification query for matching conditions to D B-pedia diseases is as follows. The query for trial interventi ons to DBpedia drugs is similar.

Again we choose threshold 0 . 6 for string matching based on in-vestigation of the accuracy of a few random queries. The tabl e below summarizes the results for different steps of the matc hings from 1,000 condition and drug interventions:
Notice the obvious need for allowing mixed string and seman-tic matching in these two cases. The trials source, NCI thesa urus and DBpedia/Wikipedia names all use different conventions and therefore there are cases where strings do not exactly match . For example,  X  X denocarcinoma of Esophagus X  in trials matches w ith  X  X arcinoma of Esophagus X , synonym of  X  X sophageal Cancer X  i n the thesaurus which matches with  X  X sophageal_cancer X  in DB pe-dia.

Case 5 (Finding related trials) To show the flexibility of our framework, we investigate its effectiveness in a rather dif ferent sce-nario. In this case, the goal is linking trials that are relat ed to each other. Different attributes and measures can be used to iden tify tri-als that are related. In this experiment, we use the pub the trials and consider two trials related if the title and au thors of their associated publications are similar. Our trials data base stores publications associated with the trials in a single long tex t record that includes the names of the authors, title of the paper, th e confer-ence or journal and the date of the publication. Therefore, i n order to find similarity we cannot use any type of semantic informat ion about the strings. Furthermore, we are not interested in typ os and different representations of the same string here. Instead the simi-larity function should measure the amount of co-occurrence of (im-portant) words in the two strings. The following weightedJa ccard linkspec performs matching based on word tokens:
Using the linkspec over 10,000 random trials results in 2,07 4 links, whereas exact matching results in only 11 matches.
In what follows, we briefly show the effectiveness of the func -tionality enhancement we proposed based on manual definitio n of a weight-adjustment table by the user. Assume that the user d efines the following simple weight table:
We repeat the experiment for Case 1 (linking to trial conditi ons from a database of patient visits) with updated weight table s based on the above input weight adjustment table. String matching with the same settings, i.e., using weightedJaccard similarity function with threshold 0 . 6 on 1,000 random base records, results in 121 ad-ditional links out of which 91 are correct (accuracy 75%) and drops 63 of the links found with no weight adjustments out of which 1 5 were wrong matches. This means that overall, the matching ha s resulted in 58 more links (5% increase) with roughly the same ac-curacy as the case with no adjustments.

Note that we obtained these results by choosing the weight ad -justment values in the above simple table based only on our do -main knowledge, and we have not varied the values to obtain th e best results. What is more important is that we can use this me thod and leverage weight values to improve the accuracy of the lin k dis-covery, as a result of our SQL-based implementation method. The implementation of the weighted-Jaccard similarity functi on and the above customization of weights using a UDF, rather than a nat ive method, could be quite complex and inefficient.
As mentioned earlier, our focus in this paper is on the functi onal-ity of the framework and we do not address efficiency, althoug h as described in Section 3.2 several hashing and indexing techn iques can be applied to our framework to make it more efficient. How-ever, we report running times of the above examples to show th e performance of the system without any of these enhancements . We ran the experiments on a Pentium 4 3GHz HT CPU with 3GB of RAM, running Windows XP SP2. To obtain statistical significa nce, we report the average time from several runs of each experime nt.
The table below shows the running time (in seconds) for 1,000 random queries for exact, synonym and hyponym matchings wit h depth 1 and 2 for the queries presented in our case study. Noti ce these queries involve approximate joins on relatively larg e tables.
For the string matching performance, due to the nature of our data sources it is reasonable to materialize token and weigh t table views in a preprocessing step and index them for better effici ency. The time required for this preprocessing of the table column s re-lated to all the cases in our scenarios is shown in Figure 6, in clud-ing the time for tokenization, weight table generation and t he asso-ciated indexing times. As shown in this figure for all the case s the preprocessing time is relatively low. Using the preprocess ed tables in our linkage cases, the following table shows the running t ime (in seconds) per each query.

The idea of Linked Data has recently attracted a lot of atten-tion in the semantic web community. Linked Data is a method of publishing data on the web based on principles that significa ntly enhance the adaptability and usability of data, either by hu mans or machines. The notable growth of linked data sources as a pa rt of the Linking Open Data Community project is in part a result of technologies recently developed and adopted to simplify pu blishing such data sources. A wide variety of data publishing methodo logies based on generation of RDF view over relational data are wide ly used in these data sources. These methodologies are often ba sed on declarative specification of the mapping between relationa l tables and RDF triples. These frameworks include, but are not limit ed to, D2RQ and D2Rserver [7], Openlink Virtuoso and Triplify [2]. The success of these tools motivates a similarly declarative fr amework for link discovery such as ours so that not only the data sourc es can be published according to the principles of publishing l inked data, but also they can be interlinked to other existing data sources, which is another important principle of linked data.

Duplicate detection, also known as entity resolution or rec ord linkage has been the subject of extensive study in different com-munities. A recent survey [10] contains an overview of vario us techniques and algorithms used for duplicate record detect ion in databases. Many AI and machine learning techniques have bee n applied for entity resolution. The online entity resolutio n frame-work of [4] presents techniques for query-time entity resol ution specifically designed for data that contains co-occurrence and re-lational information such as bibliographic data. Another c losely related area is the work on declarative data quality and clea ning [5, 11, 13]. A distinctive feature of our framework comparing wi th all the existing techniques is our focus on discovering link s and entity matching not necessarily for cleaning or duplicate d etection purposes. Our work complements and extends work on seman-tic matching ([8, and others]) and semantic annotation and t agging (see for example, Dill et al. [9]). We provide a framework for fast prototyping and testing of semantic and syntactic matching which could exploit semantic annotations, if available. A key adv antage of our approach is allowing string matching along with seman tic matching which is crucial in many real world matching scenar -ios. Moreover, our specification language allows the definit ion of new operators, which could be a mix of several semantic and st ring matching operators.
In this paper, we presented a declarative extensible framew ork for link discovery from relational data. We proposed a simpl e spec-ification language, LinQL , along with the details of its implementa-tion. We adopted and extended existing string matching and s eman-tic matching techniques, and proposed functionality enhan cements specifically designed for our framework. We showed the effec tive-ness of our approach in several link discovery scenarios in a real world health care application. Our focus has been on develop ing efficient techniques that can handle large data sets, but als o on us-ability. We showed how a user can interactively experiment w ith and customize different link methods to better understand w hat are the most effective methods for her domain. We believe that ou r framework can significantly enhance the process of publishi ng a high-quality data source with links to other data sources on the web. A user/data publisher can use our framework to easily find the ap-propriate linkage algorithm for the specific application, a s well as the optimal value of the required parameters. Our framework com-bined with an existing popular declarative approach for gen erating linked data on the web such as [7], can lead to a quick and simpl e way of publishing an online data source with high-quality li nks. This could significantly enhance the value of the data in the n ext generation of web. [1] A. Arasu, V. Ganti, and R. Kaushik. Efficient Exact [2] S. Auer, S. Dietzold, J. Lehmann, S. Hellmann, and [3] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling Up All Pairs [4] I. Bhattacharya and L. Getoor. Query-time Entity Resolu tion. [5] A. Bilke, J. Bleiholder, C. B X hm, K. Draba, F. Naumann, an d [6] C. Bizer, T. Heath, and T. Berners-Lee. Linked Data: [7] C. Bizer and A. Seaborne. D2RQ -Treating Non-RDF [8] S. Das, E. I. Chong, G. Eadon, and J. Srinivasan. Supporti ng [9] S. Dill, N. Eiron, D. Gibson, D. Gruhl, R. Guha, A. Jhingra n, [10] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios. [11] H. Galhardas, D. Florescu, D. Shasha, E. Simon, and C.-A . [12] L. Gravano, P. G. Ipeirotis, H. V. Jagadish, N. Koudas, [13] O. Hassanzadeh. Benchmarking Declarative Approximat e [14] P. Indyk, R. Motwani, P. Raghavan, and S. Vempala.
 [15] A. Kementsietsidis, L. Lim, and M. Wang. Supporting
