 In the On Line Analytical Processing (OLAP) context, ex-ploration of huge and sparse data cubes is a tedious task which does not always lead to efficient results. In this paper, we couple OLAP with the Multiple Correspondence Analy-sis (MCA) in order to enhance visual representations of data cubes and thus, facilitate their interpretations and analysis. We also provide a quality criterion to measure the relevance of obtained representations. The criterion is based on a geometric neighborhood concept and a similarity metric be-tween cells of a data cube. Experimental results on real data proved the interest and the efficiency of our approach. H.5.2 [ Information Systems ]: Information Interfaces and Presentation, User Interfaces [Evaluation/methodology]; H.4.2 [ Information Systems ]: Information systems Applications, Types of Systems [Decision support] Algorithms, Experimentation, Performance OLAP, Data cubes, Data representation, MCA, Test-values, Homogeneity criterion
On Line Analytical Processing (OLAP) is a technology supported by most data warehousing systems [3]. It pro-vides a platform for analyzing data according to multiple dimensions and multiple hierarchical levels. Data are pre-sented in multidimensional views, commonly named data cubes. A data cube can be considered as a space represen-tation composed by a set of cells. Each cell represents a Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. precise fact associated with one or more measures and iden-tified by coordinates represented by one attribute from each dimension. OLAP provides users with visual based tools to summarize, explore and navigate into data cubes in or-der to detect interesting and relevant information. However, exploring data cubes is not always an easy task to perform. Obviously, in large cubes with sparse data, the whole anal-ysis process becomes tedious and complex.
 Figure 1: Example of different representations of a 2-dimensional data cube.

For instance, consider the cube of Figure 1 which dis-plays sales of products ( P 1 ,...,P 10 ) crossed by geographic locations of stores ( L 1 ,...,L 8 ). On the one hand, in repre-sentation 1(a), full cells (gray cells) are displayed randomly according to a lexical order of attributes  X  X lsonamed mem-bers  X  in each dimension. The way the cube is displayed does not provide an attractive representation that visually helps to interpret data. On the other hand, Figure 1(b) contains the same information as Figure 1(a). However, it displays a data representation which is visually easier to analyze. Fig-ure 1(b) gathers full cells together and separates them from empty ones. Such a representation is naturally more com-fortable and enables easy and efficient analysis. Note that representation (b) can be interactively constructed from rep-resentation (a) via some traditional OLAP operators. How-ever, this suppose that the user intuitively knows how to arrange attributes of each dimension. We propose an auto-matic identification and an arrangement of interesting facts. Our a method enables to get re levant facts expressing re-lationships and displays them in an appropriate way in or-der to enhance the exploration process independently of the cube X  X  size. In order to do so, we carry out a Multiple Cor-respondence Analysis [2] (MCA) on a data cube. Basically, MCA is a powerful describing method even for huge volumes of data. It factors categorical variables and displays data in a factorial space constructed by orthogonal system of axes which provides relevant views of data. We focus on relevant OLAP facts associated with characteristic attributes (vari-ables) provided by factorial axes. These facts are interesting since they reflect relationships and concentrate significant information. In order to ensure an appropriate represen-tation of these facts, we highlight them and arrange their attributes in the data space representation by using test-values [4]. We also propose a novel criterion to measure the homogeneity of cells X  distribution in the space representa-tion of a data cube. This criterion is based on a concept of geometric neighborhood of cells. It also takes into account a similarity metric of cells X  measures and therefore provides a scalar quantification for the homogeneity of a given data cube representation.
Our method can be directly applied on a data cube C or on a data view (a sub-cube) extracted from C .Itisupto the user to select dimensions, fix one hierarchical level per dimension and select measures in order to create a partic-ular data view to analyse. In order to lighten notations, we assume that a user has selected a data cube C ,with d dimensions ( D t ) 1  X  t  X  d , m measures ( M q ) 1  X  q  X  We also assume that the user has fixed one hierarchical level with p t categorical attributes per dimension. Let a t j the j attribute of the dimension D t and p = d t =1 p t the total number of attributes in C . For each dimension D t ,wenote { a 1 ,...,a t j ,...,a t p t } the set of its attributes.
In a first step, the aim of our method is to organize the space representation of a given data cube C by arranging the attributes of its dimensions. For each dimension D t ,we establish a new arrangement of its attributes a t j .Thisar-rangement displays multidimensional information in a more appropriate manner. In a second step, our method detects from the resulted representation relevant facts expressing interesting relationships. In order to do so, we select from each dimension D t a subset  X  t of significant attributes, also named characteristic attributes. The crossing of these par-ticular attributes allows to identify relevant cells in the cube.
We base our method on the MCA [2], which is a factorial technique that displays categorical variables in a property space and maps their associations in two or more axes. From a table of n observations and p categorical variables ( p&lt;n ), the MCA provides orthogonal axes to describe the most in-ertia of the whole data cloud. The fundamental idea is to reduce the dimensionality of the original data thanks to a reduced number of variables (factors) which are a combina-tion of the original ones. In our case, we assume the cube X  X  facts as the individuals of the MCA, the cube X  X  dimensions as its variables, and the attributes of a dimension as values of their corresponding variables. We apply the MCA on the n facts of the cube C and use its results to build test-values for the attributes a t j of the dimensions D t . We exploit these test-values to arrange attributes and detect characteristic ones in their corresponding dimensions.
Like all statistical techniques, the MCA needs a tabular representation of input data. Therefore, we can not apply it directly on a multidimensional representation. We need to convert C to a complete disjunctive table .Theconversion consists in transforming each dimension D t into a binary matrix Z t with n rows and p t columns. The i th row of Z contains ( p t  X  1) times the value 0 and one time the value 1 in the column that fits with the attribute taken by the fact i . The general term of Z t is:
By merging the d matrices Z t , we obtain a complete dis-junctive table Z =[ Z 1 ,Z 2 ,...,Z t ,...,Z d ]with n rows and p columns. It describes the d positions of the n facts of through a binary coding. In the case of a large data cube, we naturally obtain a very huge matrix Z . Once the complete disjunctive table Z is built, the MCA starts by constructing amatrix B = Z Z  X  called Burt table  X , where Z is the transposed matrix of Z . Burt table B is a ( p, p ) symmet-ric matrix which contains all the category marginal on the main diagonal and all possible cross-tables of the d dimen-sions of C in the off-diagonal. Let X be a ( p, p ) diagonal matrix which has the same diagonal elements of B and ze-ros otherwise. We construct from Z and X anewmatrix
By diagonalizing S ,weobtain( p  X  d ) diagonal elements, called eigenvalues and denoted  X   X  . Each eigenvalue  X   X  associated to a directory vector u  X  and corresponds to a factorial axis F  X  ,where Su  X  =  X   X  u  X  . An eigenvalue rep-resents the amount of inertia that reflects the relative im-portance of its axis. The first axis always explains the most inertia and has the largest eigenvalue. Usually, in a factorial analysis process, we only keep the first, two or three axes of inertia [5, 1]. In [2], Benzecri suggests that the number k of axes to keep should be fixed by user X  X  capacity to give them a meaningful interpretation. It is not because an axis has a relatively small eigenvalue that we should discard it. It can often help to make a fine point about the data.
Usually in a factorial analysis, relative contributions of variables are used to give sense to the axes. A relative con-tribution shows the percentage of inertia of a particular axis which is explained by an attribute. The largest relative con-tribution of a variable to an axis is, the more it gives sense to this axis. In our approach, we interpret a factorial axis by characteristic attributes detected through the use of the test-values proposed by Lebart et al. in [4]. In the followings, we present the theoretical principle of test-values applied to the context of our approach.
Let I ( a t j ) denotes the set of facts having a t j as attribute in the dimension D t .Wealsonote n t j =Card( I ( a t j )) = the number of facts in C having a t j as attribute (weight of a in the cube).  X  t  X j = 1 n t a j on the factorial axis F  X  ,where  X   X i is the coordinate of the facts i on F  X  .

Suppose that, under a null hypothesis H 0 ,the n t j facts are selected randomly in the set of the n facts, the mean of their coordinates in F  X  can be represented by a random test-value of the attribute a t j is:
V t  X j measures the number of standard deviations between the attribute a t j (the gravity center of the n t j facts) and the center of the factorial axis F  X  . The position of an attribute is interesting for a given axis F  X  if its cloud of facts is located in a narrow zone in the direction  X  . This zone should also be as far as possible from the center of the axis. The test-value is a criterion that quickly provides an appreciation if an attribute has a significant position on a given factorial axis or not.
In traditional representation of data cubes, attributes are usually organized according to a lexical order such as alpha-betic order for a geographic dimension or chronological order for a time dimension. In a formal way, we consider that at-tributes of a dimension D t are geometrically organized in a cube representation according to the order of indices j i.e, the attribute a t j t  X  1 precedes a t j t ,and a t j and so on (see the example of Figure 2). We propose to exploit the test-values of attributes in order to organize dif-ferently the cube X  X  facts. Especially for large and sparse cubes, this new organization displays a relevant data repre-sentation suitable for analysis. In order to do so, for each dimension, we sort its attributes a t j according to the increas-ing order of their k first test-values V t  X j on axes F  X  we obtain a new order of indices j , which provides a new arrangement of attributes a t j in each dimension D t .
In general, an attribute is considered significant for an axis if the absolute value of its test-value is higher than  X  =2. This roughly corresponds to an error threshold of 5%. In our case, for one attribute, the test of the hypothesis H 0 can in-duce a possible error. This error will inevitably be increased when performing p tests for all attributes. To minimize this accumulation of errors, we fix for each test an error threshold of 1%, which correspond to  X  =3. Wealsonotethat,when a given axis can be characterized by too much attributes according to their test-values, instead of taking them all, we can restrict the selection by only considering a percentage of the most characteristic ones. Thus, for each dimension D , we select the following set of characteristic attributes:
We propose a quality criterion of data cube representa-tions which measures the homogeneity of geometric distri-bution of cells. Attributes of a cell represent its coordinates according to dimensions of the data space representation. Let A =( a 1 j 1 ,...,a t j t ,...,a d j d ) be a cell in of the attribute taken by A in dimension D t . We assume that | A | is the value of the measure contained in A ,which is equal to NULL if A is empty. For example, in Figure 2, |
A | =5 . 7and | Y | = NULL .
 Let B =( b 1 j 1 ,...,b t j t ,...,b d j d ) be a second cell in is said neighbor of A ,noted B A ,if  X  t  X  X  1 ,...,d } ,the coordinates of B satisfy: b t j t = a t j t  X  1 or b t j t b  X  t  X  X  1 ,...,d } b t j t = a t j t , which corresponds to the situa-tion where A = B . For example, in Figure 2, the cell B is neighbor of A ( B A ). Y is also neighbor of A ( Y A ). Whereas cells S and R are not neighbors of A .
 Figure 2: A 2-dimensional example of a data cube.

The neighborhood of A ,noted N ( A ), defines the set of all cells B of C neighbors of A . For example, in Figure 2, the neighborhood of A corresponds to the set N ( A )= { F, K, L, T, E, H, B, Y } . In a formal notation:
We also define a similarity metric  X  of two cells A and B from a cube C according to the following function: where || A | X  X  B || is the absolute difference of the measures contained in A and B ,andmax( C ) (respectively, min( C )) is the maximum (respectively, the minimum) existant measure value in C . In Figure 2, where grayed cells are full and white ones are empty, max( C ) = 7, which corresponds to the cell S , and min( C )=1 . 5, which corresponds to the cell K .For
We introduce now the metric  X  defined from C to R such as  X  A  X  X  ,  X ( A )= B  X  X  ( A )  X  ( A, B ). It corresponds to the sum of the similarities of A with all its full neighbor cells. For example, in Figure 2,  X ( A )=  X  ( A, F )+  X  ( A, K )+  X  ( A, L )+  X  ( A, T )+  X  ( A, E )+  X  ( A, H )+  X  ( A, B )+  X  ( A, Y ) 1 . 64.

Therefore, we can define the crude homogeneity criterion of a data cube C as: chc ( C )=
This criterion computes the sum of similarities of every couple of full and neighbor cells in a data cube C .InFig-ure 2, the crude homogeneity criterion is computed as chc ( = X ( F )+ X ( K )+ X ( A )+ X ( S )+ X ( B )+ X ( E ) 6 . 67. Note that, the crude homogeneity criterion of a data cube touches its maximum value chc max ( C ) when all cells of C are full and have the same measure value. Therefore, we consider that chc max ( C )= A  X  X  B  X  X  ( A ) 1. Finally, we define the ho-mogeneity criterion of a data cube as follows:
The homogeneity criterion represents the quality of a mul-tidimensional data representation. This quality is rather better when full and similar cells are neighbors. Indeed, when similar cells are gathered together in specific regions of the space representation of a data cube, this cube is easier to visualize. One user can therefore directly focus his anal-ysis on these regions. Nevertheless, such a criterion can not make real sense for a single data representation. We should rather compare it to other representations of the same cube. Recall also that we aim at organizing facts of an initial data cube representation by arranging attributes in each dimen-sions. Let C ini be the initial cube representation, and be the organized one. To measure the relevance of the or-ganization provided by our method, we compute its realized gain of homogeneity:
We apply our method on a 5-dimensional cube ( d =5) that we constructed from the Census-Income Database 1 of the UCI Knowledge Discovery in Databases Archive 2 .This data set contains weighted census data extracted from the 1994 and 1995 current population surve ys conducted by the U.S. Census Bureau . The data contains demographic and employment related variables. The constructed cube con-tains 199 523 facts. One fact represents a particular profile of a sub population measured by the Wage per hour .The following table illustrates the cube X  X  dimensions.
According to a binary coding of the cube dimensions, we generate a complete disjunctive table Z =[ Z 1 ,Z 2 ,Z 3 ,Z Z contains 199 523 rows and p = 5 t =1 p t = 170 columns. By applying the MCA on Z we obtain p  X  d = 165 facto-rial axes F  X  . Each axis is associated to an eigenvalue  X  Suppose that, according to the histogram of eigenvalues, a user chooses the three first axes ( k = 3). These axes explain http://kdd.ics.uci.edu/databases/census-income/census-income.html http://kdd.ics.uci.edu/ 15 . 35% of the total inertia of the facts cloud. This contri-bution does not seem very important at a first sight. But we should note that in a case of a uniform distribution of eigenvalues, we normally get a contribution of 1 p  X  d =0 . 6% per axis, i.e. the three first axes represent an inertia already 25 times more important than a uniform distribution.
For each dimension D t of the Census-Income data cube, its attributes are sorted according to the increasing values of V j ,thenby V t 2 j ,andthenby V t 3 j . Table 1 shows the new at-tributes X  order of the Professional category dimension ( D Note that j is the index of the original alphabetic order of the attributes. This order is replaced by a new one accord-ing to the sort of test-values. In Figures 3(a) and 3(b), we can clearly see the visual effect of this new arrangement of attributes. These figures display views of data by crossing the Professional category dimension on columns ( D 2 )and the Country of birth dimension on rows ( D 5 ). Representa-tion 3(a) displays the initial view according to the alphabetic order of attributes, whereas representation 3(b) displays the same view where attributes are rather sorted according to their test-values.
 Table 1: Attribute X  X  test-values of Professional cat-egory dimension.

We emphasize that our method does not cope with com-pressing dimensions of a data cube. We do not also aim at decreasing the sparsity of a data cube. Nevertheless, we act on this sparsity and reduce its negative effect on OLAP interpretation. We rathe r arrange differently origi-nal facts within a visual effect that gathers them as well as possible in the space representation of the data cube. At a first sight, representation 3(b) is more suitable to inter-pretation than 3(a). We clearly distinguish in Figure 3(b) four dense regions of full cells. In these regions, neighbor cells are more homogeneous than in the rest of the space representation. This result is confirmed by the homogeneity criterion. Indeed, for a sparsity ratio of 63.42%, the homo-geneity criterion of the organized cube in representation 3(b) is hc ( C org )=0 . 17; whereas it measures hc ( C ini )=0 . 14 for the initial cube in representation 3(a), i.e, our method en-ables a gain of homogeneity g =17 . 19%.
According to the test of the Equation (3), for each t  X  { 1 ,..., 5 } , we select from D t the set of characteristic at-tributes for the three selected factorial axes. These charac-teristic attributes give the best semantic interpretation of factorial axes and express strong relationships for their cor-responding facts. To avoid great number of possible char-acteristic attributes per axis, we can consider, for each axis, only the first 50% of attributes having the highest abso-lute test-values. For instance, in the Professional category dimension D 2 ,theset X  2 of characteristic attributes corre-spond to grayed rows in Table 1.

In the same way, we apply the test of the Equation (3) on the other dimensions of the cube. In the representation of the Figure 3(b), we clearly see that the regions of facts corresponding to characteristic attributes of the dimensions D 2 and D 5 seem to be more interesting and denser than other regions of the data space representation. These regions contains relevant information and reflect interesting associ-ation between facts. For instance, we can easily note that industrial and physical jobs, like construction, agriculture and manufacturing are highly performed by Native Latin Americans from Ecuador, Peru, Nicaragua and Mexico for example. At the opposite, Asians people from India, Iran, Japan and China are rather specialized in commercial jobs and trades.
We have realized some experiments on the Census-Income data cube presented in section 6. The aim of these exper-iments is to appreciate the efficiency of our approach by measuring the homogeneity gain realized by our MCA-based organization on data representations with different sparsity ratios. To vary sparsity we proceeded by a random sampling on the initial dataset of the 199523 facts from the considered cube. Figure 4: Evolution of the homogeneity gain accord-ingtosparsity.

According to Figure 4, the homogeneity gain has an in-creasing general trend. Nevertheless, we should note that for low sparsity ratios, the curve is rather oscillating around the null value of the homogeneity gain. In fact, when sparsity is less than 60%, the gain does not have a constant variation. It sometimes drops to negative values. This means that our method does not bring a value added to the quality of the data representation. For dense data cubes, the employment of our method is not always significant. This is naturally due to the property of the homogeneity criterion which closely depends on the number of empty and full cells. It can also be due to the structure of the random data samples that can generate data representations already having good qualities and high homogeneity values.

Our MCA-based organization method is rather interesting for data representations with high sparsity. In Figure 4, we clearly see that curve is rapidly increasing to high positive values of gain when sparsity is greater than 60%. Actually, with high relative number of empty cells in a data cube, we have a large manoeuvre margin for concentrating similar full cells and gathering them in the space representation. This shows the vocation of using our approach in order to enhance the visual quality representation, and thus the analysis of huge and sparse data cubes.
In this paper, we introduced a MCA-based method to enhance the representation of large and sparse data cubes. This method aims at providing an assistance to the OLAP user and helps him to easily explore huge volumes of data. For a given data cube, we compute the test-values of its attributes. According to these test-values, we arrange at-tributes of each dimension and so display an appropriate representation of OLAP facts. This representation provides better property for data visualization since it gathers full cells expressing interesting relationships of data. We also identify relevant regions of facts in this data representation by detecting characteristic attributes of factorial axes. This solves the problem of high dimensionality and sparsity of data and allows the user to directly focus his exploration and data interpretation on these regions. We have also pro-posed an homogeneity criterion to measure the quality of data representations. This criterion is based on a concept of geometric neighborhood of cells. It also uses a similar-ity metric between cells. Through experiments we led on real world data, our criterion proved the efficiency of our approach for huge and sparse data cubes.

Currently, we are studying some possible extensions for this work. We consider the problem of optimizing complex-ity of our approach. We also try to involve our approach in order to take into account the issue of data updates. Fi-nally, we project to implement this approach under a Web environment that offers an interesting on line aspect and an interesting user interaction context. [1] B. Escofier and B. Leroux. Etude de trois probl` emes de [2] J.P. Benzecri. Correspondence Analysis Handbook . [3] R. Kimball. The Data Warehouse toolkit . John Wiley &amp; [4] L. Lebart, A. Morineau, and M. Piron. Statistique [5] E. Malinvaud. Data Analysis in Applied
