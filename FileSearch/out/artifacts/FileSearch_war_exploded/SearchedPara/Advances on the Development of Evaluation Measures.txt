 prehensive overview of the latest advances in the develop-ment of information retrieval evaluation measures and dis-cuss the current challenges in the area. A number of topics are covered, including background in traditional evaluation paradigm and traditional evaluation measures, evaluation measures based on user models, advanced models of user in-teraction with search engines, measures based on these mod-els, measures for novelty and diversity, and session-based measures.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance Evaluation Experimentation, Measurement evaluation, measures, user models, novelty, diversity, cas-cade model, sessions retrieve items that are relevant to the information need of an end user, is one of the most important aspects of re-trieval quality. A number of different experimental frame-works have been designed in IR to measure retrieval effec-tion comprising canned information needs and static rele-vance judgments to compute evaluation measures has so far served well IR experimentation. The availability of query logs that can demonstrate the interactions of a user with ter modeling user needs and user interaction with an engine and building measures on the top of such models. This ap-proach has led to measures that are better correlated with
This tutorial focuses on methods of measuring effective-
The primary objectives of the tutorial are to  X  describe models of user interactions with retrieval sys-tems and measures constructed on the top of such models;  X  introduce complex retrieval tasks that require the con-struction of advanced user models;  X  highlight recent advances in the field that has influ-enced the evaluation measures adopted by forums such as TREC;  X  enable attendees to use advanced evaluation measures, tailored to their own tasks;  X  identify open questions in modeling user interactions and constructing evaluation measures and motivate work in the field. 18, 19, 20, 16] 2. Basic User Model and Measures (a) Cooper X  X  Expected Search Length [7] (b) Robertson X  X  Interpretation of Average Precision [14] (c) Graded Relevance [15] 3. Cascade User Model and Measures (a) Cascade Model (b) Ranked Bias Precision [13] (c) Normalised Discounted Cumulative Gain [8, 9, 3, (d) Expected Reciprocal Rank [5] (e) Expected Browsing Utility [22] 4. Models and Measures for Novelty and Diversity (a) Information Nuggets (b) Subtopic Recall and Precision [23] (c) Intent-Aware Family [1] (d)  X  -NDCG[6] 5 . Models and Measures for Session Evaluation formation Sciences at the University of Delaware in Newark, Delaware, US. He completed his PhD in Computer Science at the University of Massachusetts Amherst in 2008. His with several Best Paper Awards at conferences such as SI-GIR, ECIR, and ICTIR. With Evangelos Kanoulas, he has been actively involved in coordination of the TREC Million Query Track and the TREC Session Track.
 Evangelos Kanoulas is a postdoctoral research scientist at Evangelos received his PhD from Northeastern University, tion retrieval evaluation in SIGIR, CIKM and ECIR. He was actively involved in coordinating the TREC Million Query and TREC Session Track.
 Emine Yilmaz is a researcher at Microsoft Research Cam-bridge. She obtained her Ph.D. from Northeastern Univer-applications of information theory, statistics and machine learning. She has published research papers extensively at major information retrieval venues such as SIGIR, CIKM and WSDM. She has also organized several workshops on Crowdsourcing and served as one of the organizers of the ICTIR Conference.
