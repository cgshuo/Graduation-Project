
This paper proposes a general boosting framework for combining multiple kernel models in the context of both classification and regression problems. Our main approach is built on the idea of gradient boosting together with a new regularization scheme and aims at reducing the cubic com-plexity of training kernel models. We focus mainly on using the proposed boosting framework to combine kernel ridge regression (KRR) models for regression tasks. Numerical experiments on four large-scale data sets have shown that boosting multiple small KRR models is superior to training a single large KRR model on both improving generalization performance and reducing computational requirements.
The emergence of kernel-based methods originated from the success of support vector machines (SVM) in pat-tern recognition [1]. Afterwards, a number of powerful kernel methods, e.g., kernel ridge regression (KRR) [2], kernel principle component analysis (KPCA) [14], kernel fisher discriminant (KFD) [10] and kernel logistic regres-sion (KLR) [23] were proposed and have shown practical relevance for both supervised and unsupervised problems. Here we just consider the supervised problem, which can be formalized as the problem of inferring a function y = f ( x ) from a given traing dataset D = { ( x When y  X  R is continuous, the problem is regression, whereas in classification problems, y is categorical (e.g., bi-nary, y  X  { X  1 , 1 } ).

Generally, kernel-based methods [15, 23] can be inter-preted as the variational problem of finding the function that minimizes the functional where V ( , ) is a loss function , H is the Reproducing Ker-nel Hilbert Spaces (RKHS) generated by the kernel k ( , ) and  X  is a parameter that trades off the two terms. The first term assesses the quality of the prediction f ( x served target y tion term and represents smoothness assumptions on f . The solution to the problem (1) was given by the well-known representer theorem [13] which shows that each minimizer f  X  H of H [ f ] has the form of Substituting f ( x ) for (1) and using the property of RKHS h k ( x i , ) , k ( x j , ) i = k ( x i , x j ) , we obtain where  X  = [  X  the so-called kernel matrix of size N  X  N and ( K ) loss functions, the problem (3) corresponds to different kernel models. For example, soft margin loss function formulation of SVM, logistic loss function results in KLR and KRR can be obtained by replacing V ( , ) with a simple squared loss 1 .

It is noted that different kernel models require different techniques to find the optimal coefficients  X  . For training KRR and KLR models, it requires solving a linear system of equations once or repeatedly. And the dual formulation of SVM is a quadratic programming (QP) problem with linear constraints. But a common drawback of all these training algorithms is that the computational cost tends to scale with the cube of the number of training examples (i.e., O ( N 3 ) ) and the memory requirements grow as the square, which is impractical for large-scale datasets. To overcome these limitations, numerous speed-up training algorithms with widely different motivations for each of them (espe-cially for SVM) have been studied extensively in the litera-ture, for example [11, 21, 3, 4, 8, 23, 9].

One general approach to scale up these kernel models to large-scale datasets is that of divide-and-conquer strat-egy. The basic idea is that the dataset is divided up into small subsets, i.e., {D j , j = 1 , ..., M } , and M sub-models are derived from the individual sets. The final prediction is generated by an ensemble model  X  f ( x ) which combine pre-dictions of all M individual models, that is where f the corresponding weight coefficient. Since the training of overall learning cost of M sub-models together with the combining effort are still below the cost of training a singl e model on the whole dataset. The difficulty of this approach is how to nicely combine the individual results if we assume that small training subsets are randomly generated from the whole dataset.

The methods for weighting individual sub-models ap-peared in the kernel learning community can be categorized approach which dynamically adjust the combining coeffi-cients involved in the composite model. For example, in [4], the authors proposed a parallel mixture of SVMs which combine their individual outputs using a trained MLP neu-ral networks. Another representative example is Bayesian Committee Machine (BCM) [20] which was proposed to scale up the training of Bayesian kernel models. It weights each sub-model by the inverse covariance of its prediction. A good performance of BCM requires that M subsets be pairwisely independent.

The second class of approaches is linearly weighting schemes which do not change the combining coefficients in the phase of testing. The naive example is simple av-eraging (AV) but it seems not suitable for combining sta-ble kernel models to our best experience. A better idea [22] is to find an optimal combination based on optimiz-ing an objective function under the constraints P and c the Adaboost technique to combine SVMs for classification problems. They claimed that making an ensemble of few SVMs, each trained on subsets between 2  X  4% of the orig-inal data set, results into algorithms whose performance is comparable to a standard SVM trained on the whole dataset.
In this paper, we propose a general framework to nicely combine multiple kernel models for both classification and regression problems. Our work is based on the ideas of gra-dient boosting attached by a new regularization scheme . In particular, we demonstrate how to combine multiple KRR models with the proposed framework. Compared to previ-ous combining methods mentioned above, our work is sim-ple to use, does not require any critical parameters and can be applied to boost different kinds of kernel-based models. More important, our boosting approach could achieve better prediction results than a single kernel model on the whole training data, also with significantly lower computational requirements.
Gradient boosting [6] is a general framework to strengthen the weak base learners. In our case, the  X  X eak-ness X  comes from learning kernel models on a subset of a given training set. Following the pioneering work by Fried-man [7], the boosting procedure can be generally viewed as a forward stagewise search for a good additive model. learner which gives the largest reduction in the loss denote d where  X  f denotes an ensemble model. The essential steps of a boosting procedure can be summarised as follows [7]: Here f  X  subset D j , f at the j -th iteration and c efficient in the ensemble model  X  f ( x ) . The loss can be any differential function which corresponds to dif-ferent boosting algorithms. The most prominent exam-ple is Adaboost which just employ the exponential loss boost with the (minus) binomial log-likelihood L ( y,  X  f ) = log(1 + exp { X  y  X  f } ) and L 2 boost with the simple squared loss L ( y,  X  f ) = 1
In contrast to the objective function (3) used in kernel models, we note that boosting framework in Figure 1 does not include a regularization term. Actually, boosting util izes an empirical method, i.e., so-called  X  X hrinkage X  in statis tics, to avoid the resulting overfit problem by a large number of base learners. This can be accomplished by replacing stage 2(b) in Figure 1 with where 0 &lt;  X   X  1 is a shrinkage factor. This can greatly im-prove the generalization performance of the algorithm [6]. The cost paid for better performance is increased computa-tion since a large number of base learners are required. This conflicts with our expected objective of reducing computa-tional cost. Another potential alternative to regularizat ion in the context of boosting is introducing a penaly term in the objective, for example, the commonly used norm-2 penalty shortcoming of this strategy lies in introducing an additio nal parameter  X  . No doubt, it will increase the computational burden of combining procedure for determining  X  .
We introduce a different regularization term which is similar to the one employed by kernel models. The idea was inspired by the sparse formulation of kernel models (3), where the  X  sparseness  X  means that some entries of the so-lution  X  are exactly zeros. Let  X  all the non-zero entries of  X  indexed by I which correponds to the set of so-called basis vectors , the sparse formulation of (3) is given by the optimization prob-lem min and the resulting sparse model has a form of where {  X  x tors, K between all the training examples and selected basis vec-tors, i.e., { ( K and K sis vectors on the kernel function, i.e., { ( K k (  X  x j ,  X  x j  X  ) , j, j  X  = 1 , ..., M } .

Now we imagine f ( x ) in (7) to be an ensemble model and now each term in the expansion is a base learner which consists of multiple kernel terms not just one as in (7), that is, where D is the number of kernel terms 3 involved in the sub-model f { a d , d = 1 , ..., D } matrices K by F and Q respectively in the context of ensemble model, evolve as and Finally, we can obtain the following objective function for combining multiple kernel models: where c = [ c vidual models built on different training subsets,  X  is a reg-ularization parameter, the components of matrices F and Q have been defined by (10) and (11), respectively. Since our ensemble objective function (12) keeps the same form as the objective (6) of one single kernel model, it is reasonabl e to set the parameter  X  =  X  . By now, we have successfully introduced a new type of regularization term for construct-ing an anti-overfit ensemble model. We name the boosting technique with this new regularization method as  X  kernel-boosting  X .

In line with the Friedman X  X  boosting framework, we propose our new general boosting framework for combin-ing kernel models, which is described in Figure 2. Note that, in stage 2(b) , we have defined  X  j = [  X  j  X  c and and these three quantities f parameters  X  j an initial base learner from a random training subset and the current residual. The key step 2(b) can be efficiently solved by any gradient-based optimization algorithm.

In the next section, we will demonstrate how to apply this framework to combine multiple KRR models in the context of regression, but the main procedure is applicable to boost other kernel models.
In this section, we apply the framework of boosting ker-nel models described in the last section to KRR models for regression tasks. We firstly review some training algorithm s to KRR on a single dataset. Secondly we discuss how to ef-ficiently implement the step 2(b) in Figure 2 when boosting KRR models.
The problem of KRR is to minimize which is equivalent to solve the linear system of equations: ( K +  X I )  X  = y , which requires O ( N 3 ) time and O ( N 2 memory. A feasible scheme to large data is to find a sparse estimate of  X  , i.e., solving min where we have changed the subscript M in (6) to D in order to highlight that M denotes the size of ensemble model and to sparse KRR (17) has a form of  X   X  such a sparse solution only cost O ( ND 2 ) time and O ( ND memory. But the trouble is how to pick up a good subset of basis vectors, which is a combinatorial problem in theory and has a crucial effect on generalization performance.
It has been shown that forward selection algorithms [17, 9, 19], which choose basis vector from all the remain-ing basis candidates one by one, could achieve good results in generalization while keeping the overall complexity to O ( ND 2 ) . The key step to forward approaches is the use of a criterion for deciding which training example should be chosen as basis vector at each iteration. A good criterion could achieve a good balance between computational cost and predictive performance. One path to achieve this is the criteria [9] with k i = [ k ( x 1 , x i ) , ..., k ( x N , x i )]  X  , k i,i which measures the score of the training instance x selected in the current iteration. This evaluating method r e-quires O ( N ) for evaluating one instance. At each iteration, if we compute this score for only  X  candidates from all the remaining instances, then the complexity for basis selec-tions is O (  X  ND ) till D basis vectors are included. Typically, the value of  X  is set to 59 [17] or the predefined number of selected basis vectors [9]. The linear scaling in N makes this selection scheme viable for large problems.

Another criterion is info-gain method [16] which is al-ways inferior to the criteria (18) for generalization on lar ge datasets [9]. But this method evaluates the score of adding one case just in O ( 1 ) time, which makes this algorithm al-most as fast as using random selection. To reduce the com-putational cost to a great extent, our boosting procedure us e this fast approach to initialize the base learner f  X  step 2(a) in Figure 2).

In the experimental section, the algorithm with the crite-ria (18) running on the whole training data will be compared empirically to our ensemble method both in computational time and generalization performance.
We are going to boost a set of regression models, so the replaced with the squared function. Correspondingly, the step 2(b) can be further detailed as
H ( c  X  j , X  j )=min where r j  X  1 = y  X  F dition for optimality of c  X   X   X  we can get After simplification of some notations, we can show that the problem (19) is equivalent to The derivative of (21) w.r.t.  X  j obtained, that is  X   X 
H (  X  X  j ) where It can be noted that the optimization problem (21) involves a quadratic function w.r.t.  X  j and thus the global optimum can be reached in at most D iterations by conjugate gradient (CG) method.

Another issue step 2(d) is how to compute the optimal weight c j from old weight c j  X  1 and new entry c general framework for boosting kernel models, we simply another better way is to update the weights of all the base learners included so far. In the context of boosting KRR models, this can be done very efficiently in O ( NM 2 ) by a recursive formula (see Appendix for details).

The final version of boosting KRR models can now be summarized as follows: 1. Initialization: 2.  X  f 3. For j = 1 : M do 4. EndFor 5. Output the ensemble model  X  f ( x ) = P M The major computational cost incurred in our boosting ap-proach is the step of optimizing the base learner (e.g. the stage 3(b) ), at each iteration. Note that evaluating the gradi-ent information  X  f complexity would be O ( NDM ) time and O ( ND ) memory. We set the size D of each base learner to 200 and the size M of ensemble model to 150 in this paper. Unlike single KRR model on the whole dataset, it costs O ( ND 2 ) time and O ( ND ) memory but where D is significantly larger than 200 especially for large-scale datasets.
In this section, we empirically demonstrate that the pro-posed boosting appoach for combining multiple KRR mod-els could achieve better generalization performance and lower computational requirements simultaneously when compared to training a single KRR with (18). The following data sets are used in the next evaluations: 1. Outaouais: The data was used in the  X  X valuating Pre-2. Kin40k: This dataset represents the forward dynamics 3. Sarcos: The task is related to learn the inverse dynam-4. Census-house: The task is to predict the median price Some properties of these data sets are shown in Table 1.
To evaluate prediction performance, we utilize normalized mean square error (NMSE) given by examples and Var( y ) is the standard deviation of training targets. The algorithms presented in this section are coded in Matlab 7.0 and all the numerical experiments are conducted on a 2G Pentium-4 machine with 512M RAM, running Windows XP.

For the first three datasets, the squared-exponential ker-nel function was used [9], k ( x i , x j ;  X  ) =  X  0 exp  X  where  X  mension and x Gaussian process regression (GPR) model can be regarded as a Bayesian interpretation of KRR [12], we can maxi-mize the marginal likelihood of GPR on a random train-ing subset (e.g., 1000 examples) for optimal settings of the hyperparameters  X  This task can be done by routines of the well-known NET-sional data, optimizing much more hyperparameters will take a very long time. So we simply use the Gaussian ker-nel exp(  X  X  x  X  x  X  k 2 / X  ) , with  X  = 1 The associated  X  parameter is tuned on a small validation dataset.

For the proposed boosting ensemble approach, we fix the D = 200 and the upper limit of ensemble model size M = 150 for all datasets considered. When implement-ing the forward selection algorithm with the criteria (18) o n the whole data, we vary the maximal allowed number of se-lected basis vectors for different datasets due to the limit of memory size. Generally, the more basis vectors chosen the better generalization performance we could achieve. The parameter  X  involved in the selection process is set to 59 order to further save the storage space for training a single KRR model.

To remove the randomness involved in the algorithms, the results reported below are averaged on 10 indepen-as a function of CPU time by learning a single model and boosted ensembles on four large-scale datasets, respec -tively. It is clear that training multiple smaller KRR model s by our proposed approach consistently achieve much bet-ter generalization performance than training a single KRR model if the same CPU time is gone. Moreover, we can note that the curves for single KRR models terminate quite earlier than ensemble models. This is because that the pro-grams cannot proceed due to an out-of-memory error at this point. Table 2 shows the maximal allowed number of se-lected basis vectors for single KRR on four datasets. If con-sidering that the setting of this number for each base learne r in boosted ensembles is just 200, we can understand why our ensembles has the advantage of requiring less memory than single large model.

We also summarize the best results of test NMSE ob-tained by single model and boosted ensembles under the parameter settings described above in Table 3. For all the datasets considered, ensembles is significantly better tha n single KRR models on generalization performance, which is dicided by a p -value threshold of 0.001 in the paired test. Table 4 reports comparisons of CPU time elapsed when
Table 2. The maximal allowed number of se-lected basis vectors for single KRR on four datasets
Figure 3. NMSE comparisons of single model and ensembles for the Outaouais dataset as a function of CPU time. single model and ensembles reach the same predictive ac-curacy. It can be seen that boosted ensembles is at least two times faster than training single KRR model. The number in parentheses reflects the number of added base learners in ensembles.

Table 3. Comparisons of best NMSE obtained by single model and ensembles.
 Data set single KRR boosted ensembles Outaouais 0.0686  X  0.0013 0.0463  X  0.0005 Kin40k 0.0280  X  0.0006 0.0176  X  0.0002 Sarcos 0.0181  X  0.0001 0.0155  X  0.0001
Census-house 0.0751  X  0.0003 0.0738  X  0.0001
We presented a general boosting framework to combine multiple kernel models where each one is initialized from an individual training subset. The proposed framework
Figure 4. NMSE comparisons of single model and ensembles for the Kin40k dataset as a function of CPU time.
Figure 5. NMSE comparisons of single model and ensembles for the Sarcos dataset as a function of CPU time. was employed to boost multiple KRR models for regres-sion tasks. The resulting ensembles is empirically com-pared to the state-of-the-art algorithm for learning a sing le KRR model on three large-scale regression problems. The results support that we still benefit a lot from ensembling multiple kernel models at least for regression even though the base learners are stable.

In the near future, we will investigate the performance of applying the proposed boosting framework to large-scale classification problems. Another interesting direction is to extend our work to Multiple Kernel Learning (MKL) [18] which aims to address the issue of model selection and het-erogeneous property involved.
Figure 6. NMSE comparisons of single model and ensembles for the Census-house dataset as a function of CPU time.

Table 4. Comparisons of CPU time when sin-gle model and ensembles reach the same ac-curacy.
 1. The gradients of f
