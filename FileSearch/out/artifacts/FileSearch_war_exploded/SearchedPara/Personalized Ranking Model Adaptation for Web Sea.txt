
Yang Song 1 , Ryen W. White 1 , Wei Chu 2 Search engines train and apply a single ranking model across all users, but searchers X  information needs are diverse and cover a broad range of topics. Hence, a single user-independent ranking model is insufficient to satisfy different users X  result preferences. Conventional personalization methods learn separate models of user interests and use those to re-rank the results from the generic model. Those methods require significant user history information to learn user preferences, have low coverage in the case of memory-based methods that learn direct associations between query-URL pairs, and have limited opportunity to markedly affect the ranking given that they only re-order top-ranked items.

In this paper, we propose a general ranking model adapta-tion framework for personalized search. Using a given user-independent ranking model trained offline and limited num-ber of adaptation queries from individual users, the frame-work quickly learns to apply a series of linear transforma-tions, e.g., scaling and shifting, over the parameters of the given global ranking model such that the adapted model can better fit each individual user X  X  search preferences. Ex-tensive experimentation based on a large set of search logs from a major commercial Web search engine confirms the effectiveness of the proposed method compared to several state-of-the-art ranking model adaptation methods. H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Retrieval Models Learning to rank, model adaptation, personalization
Search engine users X  information needs are diverse. Even for the same query, different users might express different preferences over the retrieved documents, resulting in dis-tinct ranking requirements for search results [11, 27, 29]. For example, for the seemingly unambiguous query  X  X acebook, X  some people might search for the login page of the social net-working service, i.e., www.facebook.com ; while others might be more interested about the recent news reports of the pub-lic company, stock quotes, and suchlike. However, the de-ployed ranking function in search engines is usually tuned according to general relevance judgments for a generic pop-ulation of users [2, 19]; as a result, one globally-optimized ranking model cannot satisfy such diverse search preferences. In this scenario, it is preferable for the search engine to adapt the global ranking function to accommodate each individual user X  X  result preference, i.e., personalized search.
Prior research has demonstrated that users X  aggregated clicks are informative for learning their preferences and de-veloping global search result ranking models [1, 17]. How-ever, these models only reflect the common preferences across all searchers. Adapting the global ranking model towards each individual X  X  search preferences to maximize search util-ity for each single user is more desirable. Existing person-alization methods require rich user history information to learn user preferences [24, 25] (meaning that they are slow to adapt to user interests and interest dynamics), have low cov-erage in the case of memory-based methods that learn direct associations between query-URL pairs [28], and have limited opportunity to affect the ranking given that they frequently only re-order the top-ranked items [3]. In this paper, we proposed a method that effectively learns to adapt a generic ranking algorithm on a per-user basis, and overcomes many of the aforementioned challenges faced by existing personal-ization approaches.

Beside the adapted model X  X  ranking accuracy, adaptation efficiency is also a primary consideration in this work. Ex-isting work of ranking model adaptation in information re-trieval (IR) mainly focuses on domain adaptation, e.g., from Web search to image search, where the goal of adaptation is to estimate a new ranking model for a target domain us-ing information from a related source domain [6, 12, 13, 14]. Rooted in the classifier adaptation problem in transfer learn-ing (c.f. [21]), the general assumption of domain adaptation in IR is that in the target domain there are insufficient la-beled queries to accurately estimate a ranking model, but there is adequate supervision in the source domain. There-fore, to help model learning in the target domain, the adap-tation methods need to effectively exploit supervision from the source domain. However, since most of existing methods estimate the adapted model in an offline manner, adapta-tion efficiency has received little attention in prior research. Nevertheless, in the scenario of adapting a generic ranking model for personalized search (our focus here), adaptation effic iency becomes an equally important criterion for two main reasons: 1) such an operation must be executable on the scale of all the search engine users; 2) due to the dy-namic nature of users X  search intent and the need to offer searchers a great experience quickly, search engines cannot wait weeks or even days to collect adaptation data, since by then user preferences may have already shifted or they may have switched to another search engine. Our specific emphasis on adaptation efficiency prohibits us from directly applying most of existing domain adaptation methods.
Inspired by the linear regression based model adaptation methods widely studied in automatic speech recognition (e.g., maximum likelihood linear regression [18], minimum classi-fication error linear regression [15]), we propose a general framework of ranking model adaptation, which enables rapid personalization. In particular, we assume that in a paramet-ric ranking model different users X  ranking preferences can be fully characterized by different settings of model parameters. For example, some users might prefer high authority web-sites, i.e., larger weight on the static rank score (page quality independent of query); while other users would emphasize query-term matching in documents, e.g., larger weight on retrieval-score features such as BM25. As a result, adjust-ment of the generic ranking model X  X  parameters with respect to individual user X  X  ranking preferences, e.g., click feedback, is necessary to satisfy their distinct ranking requirements. In our framework, such adjustment is achieved via linear trans-formations; and to meet the efficiency requirement for fast adaptation, we restrict such transformations to be simple and shared across features in a group-wise manner.
In this paper, based on the proposed framework for rank-ing model adaptation at the level of individual users, we aim to answer the following two research questions: 1) Under efficiency constraints, how should we effectively 2) What type of queries/users will benefit most from the
The first question is answered by the proposed ranking model adaptation framework, in which the parameters of a global ranking model are updated via a series of linear transformations, e.g., scaling and shifting, for each indi-vidual user. In the per-user basis ranking model adapta-tion scenario, the lack of adaptation data is a serious prob-lem leading to sparse observations of ranking features for each user. To alleviate the sparsity problem, transforma-tions are shared across features in a group-wise manner, such that it is possible to adapt the parameters of features that are not observed in the adaptation data. The pro-posed framework is general, and we demonstrated the de-tailed instantiation of the framework to three frequently used learning-to-rank algorithms, i.e., RankNet [4], Lamb-daRank [22] and RankSVM [16], where several important properties of the proposed adaptation framework is unveiled. To answer the second question, we collected a large set of search logs from Bing.com, and compared the proposed method against several state-of-the-art ranking model adap-tation methods. Through extensive comparisons, our pro-posed method achieved significant improvement, not only in adaptation efficiency (measured in terms of the number of queries until reaches a performant state), but also in terms of the adapted model X  X  ranking accuracy, against the baseline methods.
There are two major types of research closely related to our work in this paper, namely, ranking model adaptation and personalized search.

The major body of ranking model adaptation study in IR focuses on domain adaption, which can be categorized into three classes. One popular class is instance-based adapta-tion [6, 10, 13], which assumes certain parts of the data in the source domain can be reused for the target domain by re-weighting. Chen et al. [6] weighted the queries in the source domain by a heuristically defined utility function. In [13], Gao et al. employed a binary classifier to separate the documents in target domain from those in source domain, and then defined the importance of each source-domain doc-ument by the output of this classifier. The second category of work is feature-based [5], where a new feature represen-tation is learned for the target domain and used to transfer knowledge across domains. Chen et al. proposed CLRank in [5], which constructs a new ranking feature representation so as to reduce the distributional difference between source and target domains. The third category is model-based [12, 14], which assumes the source and target ranking models share some parameters or priors. Geng et al. [14] regularized target-domain ranking model training using a given model from the source domain. Gao et al. [12] updated the source-domain model by the training errors on the adaptation data via stochastic gradient boosting algorithm.

To the best of our knowledge, few work attempts to adapt a generic ranking model for each individual user. Under ef-ficiency constraints, both instance-based and feature-based methods are infeasible for this task, because they have to operate on numerous instances in the source domain, which is prohibitively expensive to perform for each single user. To avoid costly operation for each user, our proposed method falls in the class of model-based adaptation: we update the parameters of global ranking model for each individual user according to the observed click feedback. To alleviate the problem of sparse observation in adaptation data, transfor-mations are shared across features so that parameters of unseen features can also be effectively updated.

The task of personalized search aims at leveraging infor-mation about an individual to identify the most relevant search results for them. Mainstream of personalization tech-niques target the extraction of user-centric profiles or fea-tures, e.g., location, gender and click history, and incorpo-rating such information into the original ranking function. Teevan et al. encoded user profiles extracted from relevance feedback to re-rank the retrieved documents [26]. Dou et al. [9] performed a large-scale evaluation of several personalized search strategies, e.g., user profile based re-ranking [7], and revealed that personalization has mixed effects on the rank-ing performance. These and other personalization models (e.g., [24, 25]) use significant quantities of search history to learn interest profiles for each user, requiring sufficient data available to perform personalization effectively.
Memory-based personalization techniques learn direct as-sociations between query-URL pairs [28] (e.g., given this query, the current user consistently selects a particular URL), which can perform well given high revisitation likelihoods, but have limited query coverage. Shen et al. [23] developed a context-sensitive language model by introducing both click feedback and preceding queries for short-term personaliza-tion. However, these short-term models are specific to the current context and cannot generalize well to accommodate u ser X  X  general preferences. Once a model is learned, a com-mon strategy for the application of personalization is to re-rank the top-n results [3, 9]. This means the personalized models do not have the opportunity to promote results of low general interest (i.e., outside of the top n ), but of high interest to the current user, into the top-ranked results.
In our approach, we quickly adapt the search engine X  X  generic ranking function on a per-user basis, and therefore overcome many of these shortcomings.
Inspired by the linear regression based model adaptation methods in speech recognition [15, 18], we propose a general framework to perform ranking model adaptation. We as-sume that a global ranking model is trained based on a large user-independent training set. For each user, an adapted model is obtained by applying a set of learned linear trans-formations, e.g., scaling and shifting, to the parameters of the global model based on each individual user X  X  adaptation data, e.g., query with corresponding clicks.

In the following discussions, we first describe our gen-eral framework of ranking model adaptation, and then we take three frequently used learning to rank algorithms, i.e., RankNet [4], LambdaRank [22] and RankSVM [16], as ex-amples to demonstrate the detailed procedures of applying the proposed adaptation framework.
For a given set of queries Q u = { q u 1 , q u 2 , . . . , q u , each query q u i is associated with a list of document-label pairs { ( x u i 1 , y u i 1 ) , ( x u i 2 , y u i 2 ) , . . . , ( x a retrieved document represented by a V -dimensional vector of ranking features, and y u ij is the corresponding relevance label indicating if the document x u ij is relevant to user u (e.g., clicks). Since our focus of this work is on user-level ranking model adaptation, in the following discussions we ignore the superscript u to make the notations concise when no ambiguity is involved.

A ranking model f is defined as a mapping from a doc-ument x ij to its ranking score s ij , i.e., f : x ij  X  s that when we order the retrieved documents for query q by f , a certain ranking metric, e.g., mean average precision (MAP) or precision at k (P@k) [2], is optimized. Such rank-ing model can be manually set, or estimated by an automatic algorithm based on a collection of annotated queries [19]. In this work, we focus on linear ranking models, which can be characterized by a parametric form of linear combination of ranking features, i.e., f ( x ) = w T x , where w is the linear coefficients for the corresponding ranking features.
Denoting f s ( x ) = w s T x as the given global ranking model estimated in a user-independent manner, the adaptation of f ( x ) for each individual user is performed via the linear transformations defined by a V  X  ( V +1) dimensional matrix A , by which three linear operations, i.e., scaling, shifting and rotation, can be encoded. More precisely, where  X  w s is an augmented vector of w s , i.e.,  X  w s facilitate the shifting operation for parameter adaptation.
There are two major considerations in designing such a transformation matrix A u . First, a full transformation ma-trix has the number of O ( V 2 ) free parameters, which is re-dundant and even larger than the number of parameters needed to estimate a new ranking model for each user (i.e., O ( V )). As a result, it is infeasible for us to estimate a full transformation matrix for every user. To reduce the size of free parameters in A u , we decide to only focus on the scal-ing and shifting operations for adapting the parameters in f ( x ). This reduces the size of free parameters in A u from O ( V 2 ) to O ( V ). Second, a more important consideration is how to alleviate the problem of sparse observation of rank-ing features in the limited adaptation data. Because some advanced ranking features used in modern search engines, e.g., topic category of documents, might not be trigged in the scattered adaptation queries, one will encounter missing feature values. In order to properly update the parame-ters for unseen features during adaptation, we organize the features in groups and share the same shifting and scaling transformations to the parameters within the same group.
Based on the above considerations, we design the trans-formation matrix A u to be the following specific form, where g (  X  ) is a feature grouping function, which maps V original ranking features to K different groups, a u k and b denote the scaling and shifting operations applied to the linear coefficients w s of the source model f s ( x ) in group k . As a result, Eq (1) can be realized as,
The grouping function g (  X  ) defines the transformation shar-ing among the original ranking features. It enables the ob-servations from seen features to be propagated to unseen features within the same group during adaptation, which is critical in addressing the problem of sparsity in the limited adaptation data. However, defining the optimal grouping of ranking features is non-trivial; we postpone the discussion of constructing g (  X  ) to Section 3.4.

Once the grouping function g (  X  ) is given, another impor-tant component in our adaptation framework is the criterion to estimate the optimal transformation matrix A u . An ideal transformation should be able to adjust the generic rank-ing model to meet each individual X  X  ranking preference, i.e., maximizing the search utility for each user. In the study of learning-to-rank algorithms in IR, various types of objective functions, e.g., pairwise and listwise, have been proposed to realize the goal of optimizing ranking metrics [19]. There-fore, to make the proposed framework generally applicable, we do not restrict our adaptation objective to any specific form, but instantiate it with the objective function from the ranking algorithm we choose to adapt.

We want to emphasize that although in our framework we utilize the objective function from the ranking algorithm to be adapted as the criterion to estimate the transforma-tion matrix A u , it does not necessarily restrict the global model to being estimated by the same ranking algorithm. As long as the global model and adapted model share the same model structure, e.g., neural network structure in RankNet and linear model in RankSVM, the proposed adaptation framework is applicable.

To summarize, our general framework for ranking model a daptation can be formalized as follows, in which L ( Q u ; f u ) is the objective function defined in the ranking algorithm we choose to adapt, e.g., cross-entropy in RankNet or hinge loss in RankSVM, R ( A u ) is a regulariza-tion function defined on the transformation matrix A u ,  X  is a trade-off parameter, and w s is the linear coefficients for ranking features in the global ranking model.
RankNet [4] is a probabilistic learning-to-rank algorithm, which models the probability that a document x ij is ranked higher than x il for query q i , i.e., P ( y ij &gt; y il function is employed to map the predicted ranking scores of two documents, e.g., s ij and s il , to probability of ordering, The training objective function in RankNet is defined as the cross-entropy between the predicted pairwise ordering probabilities and the observed pairwise preferences in the training data, i.e., where  X  P ( y ij &gt; y il ) is the empirically estimated probability that x ij is ranked higher than x il .

RankNet is usually optimized via a neural network. Be-cause in each layer of a neural network, every neuron X  X  out-put is linearly combined to feed into the next layer, our adaption framework can be smoothly applied to the linear weights for each neuron (e.g., different transformation ma-trices for each neuron in the hidden layers). In order to un-derstand the effect of the proposed adaptation in RankNet, we will use a RankNet with no hidden layers for discussion, but the same procedure can be applied to general RankNet with an arbitrary number of hidden layers.

To adapt RankNet, we take the same cross-entropy func-tion defined in Eq (4) as our adaptation objective, and define the following regularization function on matrix A u , where we penalize the transformation which increases the discrepancy between the adapted model and the global model, and  X  is a parameter that controls the balance between the penalty on shifting and scaling operations.

As a result, the gradient with respect to the scaling pa-rameter a u k can be calculated as, = = where  X  x ijl is a V -dimensional vector defined as  X  x ijl x ij  X  x il . Accordingly, the gradient with respect to b u
The above gradients induce a new neural network defined over the linear transformations, where the connection among neurons is specified by the grouping function g (  X  ): the term P ( y ij &gt; y il )  X  1 in Eq (6) and Eq (7) represents the pre-diction error of the global ranking model on the adaptation data; and based on this error, the gradients specify the di-rection in which the adaptation should take. We can note that the gradients for a u k and b u k are estimated based on all the observations of ranking features in the same group; as a result, the parameters for the unseen features can also get updated, by sharing such jointly estimated transformations.
To generalize this procedure to RankNet with multiple hidden layers, we only need to replace the error term defined by P ( y ij &gt; y il )  X  1 with the corresponding back-propagation error in each hidden layer in Eq (6) and Eq (7), and the original optimization procedure for RankNet can be directly applied to the adapted problem. One thing we should note is that since one can set different number of neurons in each hidden layer, to apply the proposed adaptation in a RankNet with multiple layers, we need to specify the grouping func-tion g (  X  ) for each neuron in the hidden layers. This can be achieved via the clustering method proposed in Section 3.4.
Based on the discussion of adapting RankNet within the proposed framework, it is straightforward to adapt Lamb-daRank [22] in a similar manner. As a listwise learning-to-rank algorithm, LambdaRank modifies the error term in RankNet by adding an additional correction term and names such modified error as lambda function, where |  X  IR-Metric | is the change of any specific ranking met-ric, e.g., MAP or NDCG, given by swapping the rank posi-tions of document x ij and x il while leaving the rank posi-tions of all other documents unchanged.

Therefore, to adapt LambdaRank within our framework, we only need to replace the error function of the output layer in RankNet with the lambda function defined in Eq (8), and all the other procedures remain the same as in RankNet.
RankSVM [16] is a classic pairwise learning-to-rank algo-rithm, in which the learning problem is formalized as, where C is a trade-off parameter to control the balance be-tween model complexity and empirical hinge loss over the identified preference pairs from the training data. To adapt RankSVM, we keep the hinge loss defined in Eq (9) as our adaptation objective, and use the same regu-larization function for A u defined in Eq (5). By taking the linear transformation w u = A u  X  w s into Eq (9), we get the a dapted problem for RankSVM as,
Since the input for RankSVM training is document pairs, in the following discussion, we briefly denote  X  x ijl as  X  X  which the subscript t ranges over all the preference pairs in the adaptation set, to simplify the notations. Following the conventional derivation of RankSVM, we get the dual prob-lem of Eq (9) by introducing a set of Lagrange multipliers  X  , max where K 1 (  X  X  t ,  X  X  r ) =
By solving the above dual problem, we can get the optimal transformations as,
The effect of the proposed adaptation on RankSVM is clearly depicted in its dual form. First, as we know that the linear coefficients in front of the Lagrange multipliers  X  in Eq (11) correspond to the separation margin for each train-ing instance in SVM. In the adapted problem, the margin is rescaled according to the global model f s (  X  X  t ) X  X  prediction on the adaptation data: if the global model can well sep-arate the adaptation pair  X  X  t , i.e., f s (  X  X  t ) &gt; 0, the margin decreases, indicating this case is not crucial for adaptation; if the global model fails to correctly predict the order for this pair, i.e., f s (  X  X  t )  X  0, the margin increases, and  X  X  a more important instance in adaptation for this particu-lar user. This precisely interprets the effect of model-based adaptation: we only update the global model when it makes a mistake on the adaptation data; otherwise keep it intact. Second, the proposed linear transformations induce two new kernels in a compressed space: K 1 (  X  X  t ,  X  X  s ), corresponding to the scaling operation, defines a compound polynomial ker-nel over the ranking features projected by the global rank-ing model w s ; and K 2 (  X  X  t ,  X  X  s ), corresponding to the shift-ing operation, defines another compound polynomial kernel over the original ranking features. Both kernels work in a compressed K -dimensional space determined by the feature group mapping function g (  X  ), and are interpolated by the balance parameter  X  between the regularizations for shift-ing and scaling operations. As a result, non-linearity is in-troduced to the original linear RankSVM model, and such non-linearity helps the model to leverage the observations from seen features to the unseen ones in the same group.
In the proposed framework, a feature grouping function g (  X  ) is used to organize the ranking features so that shared transformation is performed on the parameters of features in the same group. This grouping can be given a priori according to the design of ranking features, or be determined by data-driven approaches based on a given set of queries and documents. In this work, we proposed and compared three possible ways of creating such feature groups.
The first grouping method is based on the name of ranking features. Ranking features are usually described by the way they are generated, e.g., BM25 of Body , BM25 of Title [20], such that the name of a ranking feature provides informative indication of its functionality. Given the naming scheme of features in a collection, we can manually define patterns to cluster the features into groups. We denote this grouping method Name .

The second method is based on the co-clustering algo-rithms in document analysis. Similar to [8], we first project the document-feature matrix into a lower dimensional space by singular value decomposition (SVD), and then perform k -means clustering to group the features into K clusters based on this low dimensional representation. We name this group-ing method SVD .

The third method groups features by the corresponding learned parameters in the ranking models. We first evenly split the training collection into N non-overlapping folds, and train a single ranking model, e.g., RankSVM, on each fold. Then, we create a V  X  N matrix by putting the learned parameters from those N independent models together, on which k -means clustering is applied to extract K feature groups. We name this grouping method Cross .

The Name method requires the collection to have a rea-sonable feature naming scheme; if the ranking features are arbitrarily named, e.g., named by ID, such method cannot be used. The SVD method is generally applicable since it only requires a collection of query-document pairs repre-sented by the ranking features. In the Cross method, besides a set of documents, a relevance judgment for each document with respect to a given query is also needed to estimate the grouping of features. In particular, for RankNet with mul-tiple layers, the Cross method can be used to estimate the grouping function for each neuron in the network based on the learned weights of connections.
There are four advantages of the proposed adaptation framework. First, it is a general framework for ranking model adaptation, which is applicable to a majority of ex-isting learning-to-rank algorithms [19]. Second, since the proposed adaptation framework is model-based, unlike the instance-based or feature-based adaptation methods, it does not need to operate on the numerous data from the source domain, which makes the per-user basis ranking model adap-tation feasible. Third, the same optimization technique for the original learning algorithm can be directly applied with little change, so that it does not increase the complexity of solving the adaptation problem. And in the adaptation phase, we only need to solve the optimization problem over a small amount of adaptation data, which ensures the com-putational efficiency for performing the adaptation on the scale of all search engine users. Fourth, and most impor-tantly, transformation is shared across features in the pro-posed adaptation framework. According to Eq (2), the same tra nsformation is applied onto the parameters of features in the same group, which renders several important properties in the adapted ranking models: in RankNet, the gradients for scaling (in Eq (6)) and shifting (in Eq (7)) operations are estimated based on all the observations in the same group; while in RankSVM, two new non-linear kernels are induced over the original linear function space. As a result, even though we might not observe a specific feature occurring in the adaptation data, we can still propagate the information from other features in the same group to update it properly.
In order to evaluate the proposed adaptation framework, we performed a series of experiments on large-scale search query logs sampled from Bing.com. A set of state-of-the-art ranking model adaptation methods were included as base-lines to validate the effectiveness of the proposed method.
We extracted five days X  search logs from May 27, 2012 to May 31, 2012 from Bing.com for our experiments. Dur-ing this period, a subset of users were randomly selected and all their search activities were collected, including the anonymized user ID, query string, timestamp, top 10 re-turned document lists and the corresponding clicks. The queries were ordered by their timestamp in each user, and the documents were sorted by their original order returned by the search engine under each query.

To apply the proposed adaptation method and compare with the baselines according to user click feedback, we can only use the queries with clicks. Therefore, in our experi-ment, we filtered out the queries without clicks and required each user to have at least two queries with clicks, i.e., one for adaptation and one for testing.

We also sampled a large set of manually annotated query logs from our existing data collection as the user-independent training set for adaptation. Each query-document pair in this annotation set is labeled with a five-grade relevance score, i.e., from 0 for  X  X ad X  to 4 for  X  X erfect. X  Documents in both the selected user data set and annotation data set are represented by a set of 1,830 ranking features selected from their overlapped feature set, including frequently used ranking features such as BM25, language model score and PageRank. Using the language of domain adaptation, we treat the collection of annotated queries as our source do-main and each user X  X  queries with clicks as target domain. This setting provides a good simulation for real Web search scenario, where the generic rankers in use are usually trained on offline annotated data, and thus it helps us compare the effectiveness of different ranking model adaptation methods. The basic statistics of the annotation set and selected user set are summarized in Table 1.

Preference pairs are extracted from user X  X  clicks to reflect their unique search requirements. In order to ameliorate the positional biases inherent in click data [1], we followed Joachims et al. X  X  method to extract the click preference pairs [17]. In particular, we employed two click heuristics: for a given query q with a ranked document list . . . , ( x n , y n ) 1.  X  Click  X  Skip Above  X : extract preference pair x i  X  x 2.  X  Click  X  Skip Next  X : extract preference pair x i  X  x Table 1: Statistics of annotation and user data set. An notation Set -49,782 2,320,711
I n order to avoid defining different feature grouping func-tions for different ranking algorithms we selected to adapt, e.g., in RankNet each neuron in the hidden layers needs a possibly different grouping function but in RankSVM only one grouping function is needed for the original features, we decided not to use hidden layers in the neuron networks for RankNet and LambdaRank in our experiment. As a result, the same grouping function defined on the original ranking features can be directly used in RankNet, LambdaRank and RankSVM. A LambdaRank model optimizing NDCG@10 is trained on the annotation set and used as the global ranking model for adaptation in the following experiments (denoted as Source-Only ) 1 . The trade-off parameter  X  (in Eq (3)) and  X  (in Eq (5)) in our method are selected by 5-fold cross validation on the whole user set in advance.

To quantitatively compare different adaptation methods X  performance, we employed a set of standard IR evaluation metrics: by treating all the clicked documents as relevant, we calculated Mean Average Precision (MAP), Precision at 1 (P@1), Precision at 3 (P@3) and Mean Reciprocal Rank (MRR). Definitions of these metrics can be found in [2].
The grouping of features has a substantial impact on the adaptation performance in our method, since transforma-tions will be shared for the parameters of features in the same group. Ideally, we should put parameters that need to be updated synchronously in the same group. In this ex-periment, we evaluated the three feature grouping methods, i.e., Name , SVD , and Cross , proposed in Section 3.4. For comparison purposes, we also included two trivial grouping methods: 1)  X  Full , X  which creates a group for every single feature, i.e., no transformation is shared across features; 2)  X  RND , X  which randomly allocates features into K groups.
In our data set, according to the naming scheme of fea-tures, i.e., featureType sou rce s eqID , 413 feature groups are extracted by the Name method. The two data-driven ap-proaches, SVD and Cross , were performed on the annotation set, but we have to specify the group size K for them in ad-vance. To analyze the effect of group size K in our proposed adaptation framework, we evaluated the adaptation perfor-mance of RankNet by varying the setting of K . To control the number of adaptation queries in each user, which influ-ences the adaptation performance, we selected a subset of users, where each user has at least six queries with clicks (close to the average number of queries with clicks per user in our collection), and used the first three queries for adap-tation and last three queries for testing in each user. This leads to a collection of 8,879 users with 112,069 queries.
The MAP ranking performance of adapted RankNet with different feature grouping methods is shown in Figure 1 (a). First, it is clear that a properly set K is crucial for both SVD and Cross methods. The more groups we set, the more adaptation parameters need to estimate based on the limited adaptation data; but if we set too few feature groups, the
S ince we only used a subset of annotated queries and fea-tures, the results here do not reflect the actual performance of the search engine. discriminations among the features will be lost due to inac-curate parameter updating by adaptation sharing. Besides, Figure 1 (a) also shows that the adaptation performance is less sensitive to K around its optimal value, i.e., the perfor-mance as indicated by MAP is stable in a wide range of K from 400 to 800, for both SVD and Cross .

Another observation in Figure 1 (a) is that Cross per-formed consistently better than the other grouping methods under the same setting of K . Because in the Cross method features with similar contributions (i.e., linear coefficients) to document ranking are grouped together, and they tend to update synchronously. Sharing transformations among such features is more desirable. In contrast, other grouping methods cannot exploit such relationship among the fea-tures, e.g., SVD only exploits the co-occurrence relationship between features, and thus they achieved worse results.
In order to understand the in-depth effect of feature group-ing in our adaptation framework, we computed the aver-age number of updated parameters in the adapted ranking model for each user with respect to different group size K and illustrated the results in Figure 1 (b). We can note that on average only 316 features (with a standard deviation of 214) can be observed in the adaptation data according to the result of Full method. However, because of adaptation transformation sharing across features in our framework, the number of parameters that have been actually adjusted is much larger. For example, with 800 groups, about 870 pa-rameters (with a standard deviation of 220) on average are effectively updated by the Cross method, indicating that more than 60% of updated parameters are adapted with-out actual observations. On the other hand, when K be-comes smaller, the number of updated parameters increases rapidly. Consequently, using too fewer groups forces less rel-evant features get updated by the shared transformations, which in turn degrades the overall adaptation performance.
Similar adaptation results with respect to group size K were also observed in LambdaRank and RankSVM. In the following experiments, to avoid selecting K for each individ-ual user and the variation of performance introduced by this factor, we fix K to be 800 for both SVD and Cross .
To make a thorough evaluation of the proposed adaptation method, we included several state-of-the-art ranking model adaptation methods as baselines, covering instance-based, feature-based and model-based methods, for comparisons. We describe the employed baselines briefly in text below.
TransRank [6] is an instance-based ranking model adapta-tion method, in which a utility function is defined to select the top k important queries from the source domain into target domain for model training. IW-RankSVM [13] is an-other instance-based adaptation method, which re-weights the instances in source domain by measuring its distance to the classification hyperplane between source and target do-main, and only uses those re-weighted instances from source domain for target-domain model training. CLRank [5] is a feature-based adaptation method, which constructs a new joint feature representation for both source and target do-mains to reduce the distributional difference between them.
However, it would be prohibitively expensive if we directly applied these baselines for every user, because such methods need to access all the offline training data during adapta-tion. To make these methods applicable in our application scenario, we pooled all the user X  X  adaptation data together to form a combined user collection, on which the above base-line methods are applied. In addition, we also trained a new LambdaRank model optimizing MAP on this integrated user collection as a baseline, and named it Target-Only.
RA-RankSVM [14] is a model-based adaptation method, which treats the source-domain ranking model as an addi-tional regularization for model training in the target domain. Based on RA-RankSVM, we used the same regularization idea in RankNet and LambdaRank to get the correspond-ing RA-RankNet and RA-LambdaRank baselines. Besides, without knowledge about the global model, we estimated a ranking model only based on each individual user X  X  adap-tation data, and denoted such method as Tar, e.g., Tar-RankSVM, accordingly.

All baseline methods X  hyper-parameters, e.g., trade-off pa-rameter C in RA-RankSVM, are tuned by 5-fold cross vali-dation on the full user data set in advance.
We performed the experiment on all user data in our col-lection, in which the first 50% of queries from each user are used for adaptation and the rest are used for testing.  X 
Comparison in per-user basis adaptation: we first compared the ranking performance of our proposed adapta-tion methods (under all the three grouping methods) with the model-based adaptation baseline methods, e.g., RA-Ran-kSVM, RA-RankNet and RA-LambdaRank, and the base-line methods solely depend on the adaptation data, i.e., Tar-RankSVM, Tar-RankNet and Tar-LambdaRank. These are the only baselines applicable in the scenario of per-user ba-sis ranking model adaptation. In particular, MAP metric is chosen to be optimized in the adapted LambdaRank model. T able 2: Comparison of per-user basis ranking model adaptation performance.

In Table 2, we can observe significant improvements in ranking performance from the model-based adaptation meth-ods, i.e., our methods and RA methods, against the methods solely depending on the adaptation data, i.e., Tar methods. As discussed before, sparsity is a serious problem in the per-user basis model estimation. Tar methods cannot estimate the parameters for the unseen features, and thus their rank-ing capability is limited. RA methods alleviate such defi-ciency by using the global model as back-off: for features not observed in the adaptation data, the parameters from the global model would be used. In our proposed method, besides back-off to the global model when no observation is available (as shown in Eq (5)), we also propagate the obser-vations from seen features to unseen features by transforma-tion sharing to help the model better estimate the param-eters of those unseen features. As a result, our adaptation methods, under all grouping methods, outperformed the cor-responding RA adaptation methods (all the improvements are significant with p -value &lt; 0.01 under paired t-test).
Another observation in Table 2 is that the adapted Lamb-daRank performed consistently better than the adapted Ran-kNet and RankSVM within our framework. In LambdaRank the lambda function helps the model directly optimize the IR-related metrics, e.g., MAP in our case, while RankNet and RankSVM can only minimize pairwise loss. Lamb-daRank has shown better performance than those pairwise learning-to-rank algorithms in many classical ranking tasks [22]. In our adaptation framework, such advantage of Lamb-daRank is preserved since the same lambda function and optimization procedure are applied as in the original Lamb-daRank. This demonstrates the flexibility of our adaptation framework, in which we can choose to adapt any specific ranking algorithm according to the property of the task.  X 
Comparison with integrated adaptation: according to the results in Table 2, we compared our best perform-ing method Cross -LambdaRank with the instance-based and feature-based ranking model adaptation methods, and list the results in Table 3.

First of all, we can notice that in Table 3 the global ranking model trained on the annotation set (i.e., Source-Only) did not perform well on the user testing set; while the model trained on the integrated user data improved most of the ranking metrics over 10%. This indicates ev-ident distributional difference between the generic annota-tion set and user click set. Through instance re-weighting, i.e., IW-RankSVM and TransRank, or feature construction, i.e., CLRank, all baseline adaptation methods achieved im-Table 3: Comparison of adaptation performance.
 pro ved results against the global model. For these baseline methods, since we have pooled all the users X  adaptation data together, sparsity is no longer a serious problem. However, individual user X  X  specific ranking preferences will be over-whelmed once we pooled different users X  clicks together. In our adaptation method, e.g., Cross -LambdaRank, the global model is adapted for each individual user towards maximiz-ing the search utility based on their own adaptation data. As a result, Cross -LambdaRank outperformed all these base-line adaptation methods, which are originally designed for domain adaptation, in this user-oriented evaluation.  X 
Query-/User-level improvement analysis: the re-sults shown in Table 2 and Table 3 are averaged over all the users X  testing queries. It is necessary to further investigate to what extent and what types of users/queries can benefit from the proposed adaptation method. We analyzed the de-tailed ranking results given by Cross -LambdaRank against those from the global ranking model and RA-LambdaRank, which is the best baseline according to Table 2 and Table 3. Table 4: Ranking performance gain against the global model from Cross -LambdaRank and RA-LambdaRank on repeated and non-repeated queries.

Qu ery repetition is a common phenomenon in user X  X  query log, and it is crucial for many memory-based personalization methods [9, 28]. First, we categorized the testing queries as repeated queries, if it occurred in the corresponding user X  X  adaptation query set, and the rest as non-repeated ones; and then computed the improvement of ranking performance against the global model from Cross -LambdaRank and RA-LambdaRank on these two types of testing queries. As shown in Table 4 (all the differences are significant with p -value &lt; 0.01 under paired t-test), both methods achieved notably improvements against the global model on the re-peated queries, but only Cross -LambdaRank attained im-proved results on the non-repeated queries. For the re-peated queries, both methods can simply  X  X emorize X  and  X  X romote X  the documents clicked by the user; while for the non-repeated queries, because RA-LambdaRank did not get any direct observations to adjust the relevant ranking fea-tures, it could not generalize well from the adaptation data. In Cross -LambdaRank, the unseen features can also be up-dated via transformation sharing, which affords the model a better ranking capability on those non-repeated queries.
In addition, we also applied a proprietary multi-label clas-sifier to annotate the query intent into 63 categories, e.g., navigational, commerce and etc., and found that the major improvement of our method against the global model comes from the navigational queries. In detail, comparing to the global model 44.9% navigational queries get improved MAP resu lts and only 10.2% of them become worse.  X  X owTo, X   X  X ealth X  and  X  X &amp;A X  are the major categories of queries on which our method failed to generate better ranking than the global model. We investigated such kind of queries in the user data set and found the users X  clicks are mostly for ex-ploration purposes in these kinds of informational queries (diverse clicked documents), since they might not have a clear mind of answers for these queries yet. As a result, such clicks are less reliable for updating the ranking model.
To understand what types of users can benefit from our adaptation method, we categorized the users in our collec-tion into three classes by the number of adaptation queries they have: 1) heavy user, who has more than 10 adapta-tion queries; 2) medium user, who has 5 to 10 adaptation queries; and 3) light user, who has less than 5 adaptation queries. We calculated the ranking performance gain from Cross -LambdaRank against the global model averaged over the users in these three classes in Table 5. Besides, we also included the improvement from RA-LambdaRank against the global model in the table for comparison.
 Table 5: User-level ranking performance gain over global model from Cross -LambdaRank and RA-LambdaRank.

All the differences in Table 5, except the Cross /Light/ X  X @3 with the value of -0.0021, are significant with p -value &lt; 0.01 under paired t-test. We can observe that on the heavy users, who only cover 6.8% population in our collection, Cross -LambdaRank and RA-LambdaRank achieved simi-lar improvements against the global model; while on the medium and light users, who consist 14.9% and 78.3% of the whole collection, Cross -LambdaRank achieved much more remarkably improvement than RA-LambdaRank. On the heavy users, both methods get relatively sufficient observa-tions from each user to adapt the global model; while on the medium and light users, the observations become scat-tered and many features are not observed during adaptation. RA-LambdaRank failed to adjust the unseen features prop-erly and only achieved modest improvement over the global model. By sharing transformation across features, Cross -LambdaRank better exploited the information conveyed in the limited adaptation data, and obtained better improve-ment against the global model. Besides, we also found that on the light users, both methods gave degraded P@3 results (the degradation of the Cross -LambdaRank is insignificant). We analyzed the results and found that with limited obser-vations in this group of users, the adapted models tend to overfit the original top ranked documents due to positional biases. As a result, the diversity of user preferences might not be properly captured in this type of users.
We have discussed different adaptation methods X  compu-tation complexity in Section 4.3.1, from which we concluded that the instance-based methods, e.g., TransRank and IW-RankSVM, and feature-based methods, e.g., CLRank, can-not be applied in the per-user basis adaptation scenario, due to the need to frequently access source-domain data during adaptation. Therefore, in this experiment, we will only compare with the model-based adaptation method, i.e., RA method. Besides, we also included the ranking model solely estimated on each user X  X  adaptation data as a base-line. In particular, we will only illustrate the comparison results based on LambdaRank due to its superior ranking performance shown in Section 4.3.2. (Similar results also obtained in RankNet and RankSVM, but due to space limit we cannot include them.) Since all the three methods share similar computational complexity, we evaluated their adap-tation efficiency by varying the number of adaptation queries and examining which method can adapt to a user X  X  prefer-ences with fewer number of adaptation queries. To make the results comparable across different settings of adapta-tion queries, we selected a subset of users who have at least 15 queries in total, in which we fixed the last 5 queries in each user as testing queries. This gives us a collection of 2,743 users with 42,595 queries.

First, we gradually increased the size of adaptation queries from 1 to 10 in each user, and re-estimated the adapted models every time accordingly. The relative improvement of MAP metric for all the methods against the global ranking model on the testing set are shown in Figure 2 (a).
As shown in Figure 2 (a), by leveraging knowledge from the global model, the adapted ranking models outperformed the model only estimated on the adaptation data; and, with only a small amount of adaption data, e.g., 1 or 2 queries, the adapted models can already achieve encouraging improve-ment (over 15%) against the global model. Comparing with the RA method, our proposed adaptation method achieved more rapid improvement: Cross -LambdaRank achieved 25% improvement by using three queries, while RA-LambdaRank slowly climbed to 23% improvement only after 10 queries. Such efficient adaptation credits to the transformation shar-ing across features in our framework, which helps the model better handle the sparsity problem during adaptation.
The settings in Figure 2 (a) simulate a situation in which batch update is performed, i.e., update the models for each user once we have collected sufficient adaptation data. How-ever, in a more practical setting, we cannot wait too long to collect sufficient adaptation data, so that online updating is required. In this experiment, we used the same set of users as in Figure 2 (a), but updated the ranking models for ev-ery adaptation query we collected from the user in an online manner, in which we treated the previously updated model as the base model for the next iteration of model updating. The results are shown in Figure 2 (b).

We can clearly notice the advantage of our adaptation method against the baseline methods from the online adap-tation results. Because Tar-LambdaRank cannot leverage any knowledge from the global model about the unseen fea-tures, its performance fluctuated due to the variance in the adaptation queries. Although RA-LambdaRank appeals to the global model for estimating the unseen features, in the online setting, the knowledge from global model gets dimin-ished rapidly as adaptation evolves, because it only treats the model from last iteration as regularization. As a result, its performance is worse than that in the batch mode (in Figure 2 (a)). In our method, observations in the adapta-tion data can be fully exploited via transformation sharing, so that unseen features can also be properly updated during online adaptation, which leads to consistent improvement of ranking performance in both batch and online settings.
In this work, we proposed a general ranking model adap-tation framework for personalized search. A series of learned linear transformations, e.g., scaling and shifting, were per-formed on the parameters of a generic linear ranking model in a per-user basis, such that the adapted model can better fit each individual user X  X  search result ranking preferences. By sharing transformations across features in a group-wise manner, unseen features can also be properly updated given only limited number of adaptation queries. We instantiated the proposed framework with three frequently used learning-to-rank algorithms, i.e., RankNet, LambdaRank and RankSVM, and the adaptation method achieved significant improve-ment in, not only adaptation efficiency, but also ranking performance of the adapted ranking models, against several state-of-the-art ranking model adaptation methods in exten-sive experimentation.

In our current solution, the feature grouping function and transformation matrix are estimated independently. It would be meaningful to jointly estimate the two components for better adaptation performance. Besides, the proposed linear transformation based ranking model adaptation framework opens an interesting new direction for personalization: rich signals, e.g., user-specific profiles and features, could also be included to affect the transformation in order to better reflect users X  individual search interests.
