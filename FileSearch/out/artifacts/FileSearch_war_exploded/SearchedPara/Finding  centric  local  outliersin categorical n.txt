 REGULAR PAPER Jeffrey Xu Yu  X  Weining Qian  X  Hongjun Lu  X  Aoying Zhou Abstract Outlier detection techniques are widely used in many applications such as credit-card fraud detection, monitoring criminal activities in electronic com-merce, etc. These applications attempt to identify outliers as noises, exceptions, or objects around the border. The existing density-based local outlier detection as-signs the degree to which an object is an outlier in a numerical space. In this paper, we propose a novel mutual-reinforcement-based local outlier detection approach. Instead of detecting local outliers as noise, we attempt to identify local outliers in the center, where they are similar to some clusters of objects on one hand, and are unique on the other. Our technique can be used for bank investment to identify a unique body, similar to many good competitors, in which to invest. We attempt to detect local outliers in categorical, ordinal as well as numerical data. In categori-cal data, the challenge is that there are many similar but different ways to specify relationships among the data items. Our mutual-reinforcement-based approach is stable, with similar but different user-defined relationships. Our technique can re-duce the burden for users to determine the relationships among data items, and find the explanations why the outliers are found. We conducted extensive experimental studies using real datasets.
 Keywords Data mining  X  Clustering  X  Outlier detection 1 Introduction Outlier detection is one of the four important knowledge-discovery tasks: depen-dency detection, class identification, class description and exception/outlier detec-tion [ 13 ]. The nature of outlier detection, as opposed to the other three, is to iden-tify a small percentage of objects that deviate by an amount from the majority of objects, thereby arousing suspicion that they were generated by a different mech-anism (Hawkins-outlier) [ 8 ]. The existing outlier detection techniques are cate-gorized in five categories [ 10 ]: distribution-based, depth-based, distance-based, clustering-based, and density-based. The distribution-based approaches identify outliers that deviate from a standard distribution. Depth-based approaches identify outliers by computation of k -d convex hulls in a k -dimensional space. A distance-based outlier in a dataset is an object for with p % of the objects in that dataset are more than a minimum distance away. The existing clustering-based approaches can also identify outliers as a by-product of identifying clusters. Different from the first four categories, which identify global outliers, the last density-based ap-proach is introduced to identify local outliers based on the local density of an object X  X  neighborhood in a numerical space. In this paper, we further study lo-cal outlier detection, and propose a mutual-reinforcement-based approach. In our mutual-reinforcement-based approach, the similarity between two objects is not only determined by the similarity between themselves directly but also by the similarity between themselves indirectly through other objects. To the best of our knowledge, this is the first attempt to apply mutual reinforcement [ 12 ] to outlier detection.
 proaches that are widely used in applications such as credit-card fraud detection, monitoring criminal activities in electronic commerce, video surveillance, weather prediction, and pharmaceutical research. These applications attempt to identify outliers as noise, exceptions, or objects around the border. In contrast to those applications, outlier detection is proposed for investment decision-making based on finding outstanding or distinctive outliers. For instance, when a company is looking for a university as a partner with which to collaborate, one of many possi-ble decision-making strategies is to identify a few universities which are similar to other good universities on one side, but unique (i.e., outlier) on the other. In similar circumstances, banks should not invest in a body based on whether the density of its neighborhood is much lower/higher than its neighbors neighborhood, because it is hard to explain the intention behind the density for investment purposes. In-stead, the decision should be made based on the similarities/differences, in terms of quality, efficiency, investment return, etc., which can be either categorical, or-dinal or numerical data.
 new local outlier detection problem to detect centric local outliers rather than lo-cal outliers around bounder. Second, we propose a mutual-reinforcement-based approach [ 12 ] to detect centric local outliers. In contrast to the density-based ap-proaches that identify a local outlier if the density of its neighborhood is lower than its neighbor X  X  neighborhood, our approach is to identify a local outlier if its rela-tionship with its neighbors is looser than the relationships among its neighbors X  neighborhood. Third, we propose a similarity measurement which can be easily used for categorical, ordinal and numerical data. We should recall that the concept of a density-based local outlier is based on Euclidean distance, which cannot be easily used for non-numerical data types. We study the stability of our similarity measurement. Our approach is rather stable with similar but different weighting schema. In other words, our approach will find similar outliers when using dif-ferent weighting schema. This reduces the burden imposed on users to explain the relationships between the weighting schema and outliers found using those weighting schema. Fourth, we enhance our local outlier detection for handling dense similarity graphs using an existing k -NN ( k -nearest-neighbor) approach which converts a dense similarity graph into a rather sparse similarity graph. Fi-nally, we studied the applicability of the density-based approach for finding local outliers for categorical and ordinal data using real datasets.
 ing local outlier detection approach LOF k [ 4 ], and gives our motivation of mutual reinforcement in brief. In Sect. 3 , we discuss the mutual-reinforcement-based ap-proach,  X  -LOF, and, in detail, address the similarity and differences between the mutual-reinforcement-based approach,  X  -LOF, and the density-based approach, LOF k . The properties of the mutual-reinforcement-based approach will also be discussed. Section 4 further describes a real university-ranking example using different similarity schemes (categorical, ordinal, and numerical). Section 5 dis-cusses how to support applications in multidimensional numerical space. We will report our experimental results in Sect. 6 , and discuss related work in Sect. 7 .We conclude the paper in Sect. 8 . 2 The existing density-based local outlier detection: LOF k Recently, Breunig et al. propose a density-based scheme to find local outliers in a numerical space [ 4 ]. We denote it by LOF k , to distinguish it from our local outlier approach. We introduce LOF k in brief below. Let p and o be two objects in a database D ,andlet k be a positive integer. The distance function d ( o, p ) defines the distance between o and p ,andthe k -distance( o ) gives the distance from o to its local reachability density of p for k is defined as local outlier factor of p is defined as The higher the LOF k ( p ) value is, the more likely that p is a local outlier with respect to k .The k value is controlled by a parameter MinPts , which specifies the minimum number of objects within a distance. and v i to denote the point i . Here, the Euclidean distance between points obey r 1 &lt;r 2 and d 1 &lt;d 2 . The distance between any other two points can be ignored in the following discussion. Consider LOF 4 for the point v o ,LOF 4 ( v o ) . The 4-between v o and each of its four neighbors contributes to the reachability dis-bors have the same k -distance, their contributions to the reachability distance are the same. Consider v o and its two neighbors v 1 and v 3 . Here, 4-distance( v 1 )= 4-distance( v 3 ), and therefore their contribution to lrd 4 ( v o ) is the same. From a different viewpoint, LOF k does not pay attention to the relationships between neighbors themselves. For example, the distance between v 1 and v 2 is d 1 and the distance between v 3 and v 4 is d 2 lead to the relationships between v o  X  X  neighbors. The two distance, d 1 and d 2 , are different, but they do not change LOF 4 ( v o ) .
 sideration. We explain our motivation using the example in Fig. 1 . For simplic-ity, assume that the four neighbors of v o , { v 1 ,v 2 ,v 3 ,v 4 } , are along the circum-ference. When d 1 is fixed, while the distance d 2 becomes smaller, v o is more likely to be a local outlier. In contrast, when d 2 is fixed, while the distance d 1 pose a new mutual-reinforcement-based local outlier detection approach which takes care of the relationships among neighbors. We will discuss the similarity and difference between the density-based local outlier detection (LOF k ) and our mutual-reinforcement-based local outlier detection (  X  -LOF) using simple and real examples.
 3 A mutual-reinforcement-based local outlier detection:  X  -LOF In this section, we give our mutual-reinforcement-based local outlier detection,  X  -LOF, and discuss the main similarity and difference between  X  -LOF and LOF k .  X  -LOF is designed for handling categorical data, and can handle numerical data, as shown later in this paper.
 which is a weighted connected undirected graph. Here, V is a set of vertices. We use V ( G ) to denote the set of vertices for graph G . E  X  V  X  V is a set of edges (un-ordered pair of vertices) where ( v i ,v i ) /  X  E ,and w is a set of edge labels (positive real numbers). A weight for an edge, ( o, p )  X  E , denoted by w (( o, p )) , specifies the similarity between the two vertices. The larger the value of w (( o, p ) , the higher similarity the two vertices share; we call this a similarity-based weighting scheme. One of the motivation of using similarity-based weighting is to support categorical data, ordinal data and numerical data in the same framework. On the graph G ,a is said to connect v 0 and v n . The walk that connects v 0 and v n is not necessarily unique. We call it a  X  -walk if the length of a walk is  X  . A walk is closed if v 0 = v n . A walk which is not closed is open. We note that every two vertices are connected on G by a walk due to the nature of the connected graph. We define N  X  ( p ) as a set of vertices that are connected to p within  X  -walks, for  X   X  0 , including p . Definition 1 Let p and o be two vertices. The similarity of the  X  -walk between p and o is defined as Here, s  X  ( o, p )= s  X  ( p, o ) .
 Definition 2 Let p and o be two vertices. The accumulated similarity of the  X  -walk between p and o is defined as Here, S  X  ( o, p )= S  X  ( p, o ) .
 Definition 3 The  X  local outlier factor of a vertex p , denoted by  X  -LOF, for  X &gt; 1 , is defined as where q  X  N  X  ( p ) . Note that S  X  ( p, p ) &gt; 0 when  X &gt; 1 .
 of  X  -close walks, and the numerator is the average accumulated similarity of  X  -walks between p and q , which can be either open or close. The  X  -LOF is the ratio between walks and closed walks, and captures the degree to which we call p an outlier. It is more likely that p is an outlier if  X  -LOF ( p ) is larger.  X  is also a value to measure how  X  X ocal X  it is when we identify outliers. We call this mutual reinforcement . 3.1 The algorithm The algorithm, called the  X  -Outlier-Finder, is shown in Algorithm 1. Given a graph G =( V, E, w ) ,  X  -Outlier-Finder takes three parameters, a symmetric matrix E ,an integer K and a threshold  X  . Here, E [ i, j ] contains all the weights associated with the set of edges of the graph G . The matrix value E [ i, j ]=0 if ( v i ,v j ) /  X  E . K is a parameter that controls the maximum number of walks, because the number of walks is infinite in an undirected graph.  X  is a threshold such that v i  X  V is considered as an outlier if  X  -LOF ( v i ) &gt; X  .
 3.2 Density-based versus mutual-reinforcement-based methods We will show examples to explain mutual reinforcement, in comparison with LOF k . A sample table of five columns with seven tuples is shown in Table 1 (left). Here, the TID column contains the tuple identifier, and the four columns, A, B, C and D , are user-defined categorical attributes. We construct a graph G = ( V, E, w ) , as shown in Table 1 (right). The graph, G , consists of seven vertices, is defined as the number of equal A/B/C/D values between two tuples (ver-tices). The zero weight between two vertices, v i and v j , implies ( v i ,v j ) /  X  E .The similarity defined reflects the fact that the larger a weight, the higher the similarity and ( v o ,v 2 )( v 2 ,v o ) .When  X  =3 , there are two closed  X  -walks between v o and N 6 ( v o ) is ( v o ,v 1 )( v 1 ,v 3 )( v 3 ,v 4 )( v 4 ,v 1 )( v 1 ,v 2 )( v 2 ,v o ) . for three vertices, v o (center), v 1 (close-to-center) and v 3 (border), to all seven vertices, where  X  =3 . Here, s 3 ( v o ,v o )=2 , s 3 ( v o ,v 1 ) = 1+1+1+1+1 for five ( v S s indicate that v o has higher similarity with two vertices, v 1 and v 2 , than the rest. two vertices, v 3 and v 4 ,than v o . It is important to note that the similarity between two vertices is not only determined by the similarity between themselves directly but also by the similarity between themselves indirectly through other vertices in  X  -walks. In other words, the relationship between two neighbors, v i and v j ,isalso determined by v i  X  X  neighbors X  relationship with v j and v j  X  X  neighbors with v i . seven vertices (tuples) are shown in Fig. 2 a. We further explain the idea of  X  -LOF.  X  -LOF for a vertex, v i , measures the av-erage relationship (similarity) between v i and its neighbors, in comparison with the relationship with itself from v i  X  X  neighbors X  viewpoint, within  X  -walks. When  X  X ocal X  is  X  =3 ,the  X  -LOF value for v o (the center) becomes 1.07, and is higher than the others. In this case, v o is considered as a local outlier. The explanation is that v o has many neighbors in 3-walks, its average relationship with them is 4 . 29 (=(4+7+7+3+3+3+3) / 7) , whereas the relationship from v o  X  X  neighbors X  viewpoint in 3-walks is 4 . In contrast, v 1 also has many neighbors in 3-walks, its average relationship with them is 6(= (6+8+8+7+7+3+3) / 7) ,butthe relationship from v 1  X  X  neighbors X  viewpoint in 3-walks is much higher: 8 .That is, v 1  X  X  neighbors see their relationship with v 1 as higher than v 1 sees its average relationship with them. In other words, v 1 is deep in a cluster, but v o is not. which is the number of different A/B/C/D values between v i and v j for LOF k , because A/B/C/D values are categorical data. The distance function is defined if the distance between them is small. Recall d ( v i ,v j )=4  X  w ( v i ,v j ) .The LOF k ,when k = MinPts =3 , for all seven tuples are shown in Fig. 2 b. Here, tuple v 1 is in 3-distance with four tuples: v 0 , v 2 , v 3 and v 4 , and 4-distance with two tuples: v 5 and v 6 . In contrast, tuple v 0 is in 3-distance with two tuples: v 1 and v 2 , and 4-distance with the rest. Therefore, in terms of 3-distance When MinPts = k =3 , v 1 is identified as a local outlier rather than v 0 , because LOF 3 ( v 0 ) gives a lower average density with respect to the six tuples in its 3-distance neighborhood, whereas LOF 3 ( v 1 ) gives a higher average density regarding the four tuples in its 3-distance neighborhood. However, after careful examination of Table 1 , it seems that there are no strong reasons why v 1 should be a local outlier rather than v 0 . It is worth noting that the MinPts is very sensitive in order to identify local outliers. When MinPts =3 , for this example, all LOF M inP ts values become 1 for all seven tuples. 3.3 The effect of neighbors X  neighbors We extend the seven-tuple example (Table 1 (right)) using four parameters, a , b , c and d , to control the relationships among the seven tuples, { v o ,v 1 ,v 2 ,...,v 6 } ,as shown in Fig. 3 . Figure 3 illustrates the relationship based on the length of an edge. We set the similarity between two vertices to be the reciprocal of the distance. No edge between two objects suggests non-similarity or infinite distance between two objects. Figure 3 a shows a case when b is rather small (high similarity or short distance between v 1 and v 2 ), whereas Fig. 3 b shows a case when b is rather larger (low similarity or long distance between v 1 and v 2 ).
 and vary a and d (Fig. 4 a X  X ). We can see that the impact of a and c on 3-LOF( v o ) is similar with different b and d values. When a is large (high similarity between ( v o ,v 1 ) and ( v o ,v 2 ) ), the impact of c can be ignored, and v o cannot be a local outlier. When a is small, the impact of c becomes significant. A large c value 3-LOF outlier, which is the same for LOF 2 (the density-based approach). It is worth noting that LOF 2 ignores the relationships between ( v 1 ,v 2 ) , ( v 3 ,v 4 ) and ( v 5 ,v 6 ) from v o  X  X  viewpoint. In other words, LOF 2 does not pay attention to the relationship controlled here by the parameters d and b with respect to the vertex v . Figure 4 a X  X  also explain why v o becomes an LOF 2 local outlier in terms of density.
 and (1 , 2) ,andvary b and d (Fig. 4 d X  X ). The impact of b and d on 3-LOF( v o ) is similar with different a and c values. When d becomes large (high similarity v o is more likely to be a 3-LOF local outlier. Here, note that when a =2 and c =1 (Fig. 4 d), that is, when the similarities between ( v o ,v 1 ) and ( v o ,v 2 ) are a local outlier, because its 3-LOF value is smaller than 1, with the changes of d and b in the range [1 , 3] . That is, when a is dominant, the changes of b and d do not have a significant impact on v o . On the other hand, as shown in Fig. 4 eandf, when a is not the dominant factor in comparison with c , the relationship between v  X  X  neighbors can have a significant impact on v o , unlike LOF 2 ( v o ) . When the for ( v o ,v 1 ) and ( v o ,v 2 ) , the impact of the changes of b and d on 3-LOF( v o )is significant (Fig. 4 f).
 the changes shown in these figures individually. This explains why  X  -LOF values tend to be larger when considering relationships among neighbors X  neighbors. value if a becomes small (long distance) or if c becomes large (short distance), due to the local density. However, since the 2 -distance of objects in Fig. 3 a remains unchanged, LOF 2 ( v o ) is insensitive to the changes of b and d (Fig. 3 b). How-ever, Fig. 3 bshowsthat v o is connected to two groups of objects which are not similar to one other. When b becomes smaller (low similarity or long distance), 3-LOF( v o ) increases quickly (Fig. 4 d X  X ). Therefore, from the  X  -LOF X  X  viewpoint, v in Fig. 3 b is a local outlier that cannot be found by LOF 2 .
 3-LOF( v o )andLOF 2 ( v o ) ,inFig. 5 . Here, both a and b have a value indicating similarity. When a&gt; 1 , v o cannot become a LOF 2 local outlier, because the re-ciprocal of a becomes small (short distance). LOF 2 ( v o ) values are insensitive to changes of b . On the other hand, when a becomes larger ( a  X  2 ),asmall b (low similarity between v o  X  X  two neighbors) will make v o more likely to become a lo-cal outlier. This is because v o is more like a center between two small clusters. In consequence, when a is rather small (low similarity or long distance), both 3-LOF and LOF 2 can possibly identify v o as a local outlier. When a is rather large (high similarity or short distance), LOF 2 cannot identify v o as a local outlier. On the other hand, 3-LOF may or may not be able to identify v o as a local outlier, depending on the parameter b . Therefore, we have shown that the relationships between the neighbors of v 0 is essential for the outlying nature of v 0 . This is the key difference between  X  -LOF and LOF k . 3.4 Properties of mutual-reinforcement-based local outliers In this section, we show that  X  -LOF shares the same properties proved for LOF k in [ 4 ]. In Lemma 1 ,weshowthat  X  -LOF values for objects deep in a cluster are approximately 1, which indicates that they cannot be found as local outliers. Lemma 1 Given any set of vertices V C  X  V ( G ) , and  X &gt; 1 ,let S  X  min and S  X  max be the nonzero minimum and maximum S  X  ( p, q ) for all p, q  X  V C .Let = S  X  max S  X  Then, for every vertex p  X  V C , such that ( i ) N  X  ( p )  X  V C and ( ii ) for every q  X  N  X  ( p ) , N  X  ( q )  X  V S  X  -walks, 1+ above two constraints (i) and (ii), then  X  hbox  X  LOF ( p ) is bounded. If V C is a tight cluster, then becomes very small, which forces  X  -LOF ( p ) to be close to 1. Hence, p cannot be identified as an outlier. Furthermore, for any vertex p  X  V ( G ) , we show a general upper and lower bound on  X  -LOF values. Following a similar notion in [ 4 ], we define Theorem 1 Given any vertex p  X  V ( G ) , for any 1 &lt; X  , we have Proof indirect-S  X  min ( p ) . It is obvious that the former holds. For the latter, we know by definition that p  X  N  X  ( p ) . In a similar fashion, we can show that indirect-S  X  max ( p ) .
 bors within  X  -walks are further clustered in different clusters.
 Theorem 2 Given any vertex p  X  V ( G ) , for any 1 &lt; X  . Suppose that N  X  ( p ) V = | V i | / | V ( G ) | . Let the notions direct V S min ( p ) , and indirect V i -S S max ( p ) , indirect-S Proof First we prove the left side as follows. In a similar way as shown in the proof of Lemma 1 ,wehave Therefore, Second, we prove the right-hand side in a similar fashion. Because, Then, 4  X  -LOF: stability with similar but different measures In this section, we study a real example: the 1993 NRC ranking dataset. We em-phasize the stability of  X  -LOF, which is not easily influenced by similar but dif-ferent similarity measures because the mutual reinforcement and the accumulated similarity will remain stable, even if some similarity measures vary locally. search Council X  X  study on universities granting doctorates in 41 areas in 1993. The areas are further divided into five groups, namely, Arts and Humanities , Bio-logical Sciences , Engineering , Physical Sciences and Mathematics , and Social and Behavioral . Each group contains 7 X 11 areas individually. The dataset was produced as follows. Each university granting a doctorate in an area according to faculty quality and effectiveness of program .The resulting responses were converted to scores ranging from 0.00 to 5.00 where 0 means  X  X ot sufficient for doctoral education X / X  X ot effective X  and 5 means  X  X is-tinguished X / X  X xtremely effective X  for faculty quality and effectiveness of program , respectively. Based on these scores, rankings were obtained for all doctorate-granting institutions. Newton summarized NRC ranking in 1993, 1 and studied faculty quality , due to the consideration that it is most relevant to received scores in at least 10 of the 41 areas. The raw scores are converted to an in-teger scale of 0 through 9 for faculty quality . A university will have a 0 value if it does not grant a doctorate in the corresponding area. A high value implies a high quality. As an example, we choose 10 universities from the dataset, using 23 areas in three area groups, namely, Biological Sciences ( A 1 ), Engineering ( A 2 ), and Physical Sciences and Mathematics ( A 3 ). These are summa-rized in Table 3 . We will show the results using the whole dataset later in this paper.
 The number of areas in A i is denoted by | A i | ,where | A 1 | =7 , | A 2 | =8 and |
A column A i ) for the university v k . 2 We construct three graphs: G 1 =( V 1 ,E 1 ,w 1 ) , G 2 =( V 2 ,E 2 ,w 2 ) ,and G 3 =( V 3 ,E 3 ,w 3 ) , for Table 3 . Here, each V i ,for i = (vertices) shows that the two vertices are similar (adjacent). For simplicity, we use w (( v j ,v k )) &gt; 0 to denote that there is an edge in graph G i between two vertices, v j and v k . There is no edge in graph G i between v j and v k if w i (( v j ,v k )) = 0 . In the following, we show three graphs by showing three weighting schema: w 1 (categorical weighting), w 2 (ordinal weighting) and w 3 (numerical weighting). In principle, we treat all three groups, A 1 , A 2 and A 3 , equally and individually.  X  Graph G 1 ( based on categorical weighting ). Here, we treat the score, a ij ,as  X  Graph G 2 ( based on ordinal weighting ). In contrast to in G 1 (or w 1 ), here we  X  Graph G 3 ( based on numerical weighting ). In contrast to G 1 and G 2 ( w 1 and only the graphs G 1 and G 3 in Fig. 6 .) The results are shown in Table 4 . With the two graphs, G 1 and G 2 ,when  X  =2 , Georgia Tech is found to be an outlier, because it has middle scores on most areas. When more  X  -walks are taken,  X &gt; 2 , Stanford becomes an (outstanding) outlier. In graph G 3 , Stanford becomes an (outstanding) outlier since  X  =2 . The main reasons that Stanford is found to be an outlier are as follows. Stanford has high scores in all 23 areas. It is seen as an outlier relating two groups of universities. In one group, universities such as Harvard and Princeton have high scores in the areas they grant, but they do not grant all doctorates in the three area groups. In the other group, universities grant doctorates in many areas with middle or low scores. The outliers we found are easy to explain, and are meaningful. d ( o, p ) ,for i =1 , 2 , 3 as follows: d 1 ( o, p )= w 1 w w and w 3 LOF k uses a distance. Note that the distances, d 1 , d 2 and d 3 ,are L 1 distance. We choose three MinPts ( k ) values, 3, 4 and 5, which specify the least number of objects that should be considered as a neighborhood. The results are shown in Ta b l e 5 . It is shown that the outliers found by LOF k depend highly on both the distance scheme and the sensitive global cut-off threshold MinPts . The results are not consistent among the three different distance schema. In five out of the nine sets of results, Princeton has higher LOF k values than Stanford ,and in three out of nine sets of results, Stanford has the highest LOF k value (a top local outlier). LOF k , as a density-based approach, only considers the den-sity of objects. When MinPts =3 , with the distance of either d 2 or d 3 ,the density of Stanford  X  X  neighborhood is low (low LOF 3 ( Stanford ) value) com-pared to the density of its three neighbors X  neighborhoods: Princeton , Harvard and Chicago (e.g. LOF 3 ( Princeton ) ,LOF 3 ( Harvard ) and LOF 3 ( Chicago ) ). A low density suggests that Stanford should be in the same cluster as its three neighbors. However, they should not be considered as in the same cluster, be-cause they are totally different. Table 3 shows that (a) Princeton has high scores in both Engineering and Physical Sciences , but has many zeros in Bio-logical Sciences ;(b) Harvard and Chicago have high scores in Biologi-cal Sciences and Physical Sciences , but have all zeroes in Engineering areas; and (c) Stanford has high score in most areas. Here, considering den-sity may make Stanford , which should be a local outlier, have a low density. The density-based LOF k does not reflect the relationships among objects, and, sometime, its results are hard to explain. In other words, the density does not pay enough attention to whether those neighbors are similar/different. The advantage of  X  -LOF over LOF k is that  X  -LOF considers relationships (similarity) between neighbors, rather than density.  X  -LOF finds Stanford as a local outlier over 2-walks (  X &gt; 2 ).
 tion and deviation of top outliers than LOF k . 5  X  -LOF: finding local outliers in a numerical space In this section, we show how to find local outliers in a numerical space. In Fig. 1 in [ 4 ], Breunig et al. show an example in a two-dimensional dataset containing 502 objects to illustrate how to find local outliers. However, they do not describe how the dataset is constructed in detail. In [ 21 ], Tang et al. show a similar two-dimensional dataset with 804 objects that shares great similarity with the example in [ 4 ], with detailed information. 3 We will introduce the dataset in detail, and then show how to find local outliers using  X  -LOF with a simple preprocessing step: k -NN ( k -Nearest-Neighbor). Recall that, in a numerical space, an object is con-nected to any other object with a distance. It forms a complete (dense) similarity graph. The purpose of k -NN is to reduce a dense similarity graph into a sparse similarity graph.
 two clusters, C 1 and C 2 , with an outlier o , as shown in Fig. 7 . Here, | C 1 | = 400 and | C 2 | = 403 . C 1 is covered by a square with two corner points, x 1 and x 3 , and C 2 is also covered by square with two corner points, x 2 and x 4 . The points o , x 1 , x 2 , x 3 and x 4 are on the common diagonal line of the border square. Let and dist max ( C i ) be the minimum and maximum L 2 distance between two objects dist( o, x 1 ) &lt; dist( o, x 2 ) ,dist ( o, C 2 ) = dist( o, x 2 )=dist max ( C 2 ) .  X  -LOF constructs a complete graph, G d =( V d ,E d ,w d ) with 804 vertices where w ( q, p )=1 / dist( q, p ) for ( q, p )  X  E d . When finding local outliers using  X  -LOF, it is found that we cannot find the outlier o using graph G d , simply because  X  -LOF ( p ) ,forany p  X  C 2 , is much larger than  X  -LOF ( o ) . This fact indicates that we need to turn the complete graph into a sparse similarity graph. The approach we have taken is to adopt an existing k -NN approach. The main idea behind it is a two-step approach. First, we construct a sparse similarity graph, G s =( V s ,E s ,w s ) using k -NN. An edge ( p, q )  X  E s , associated with the weight w s (( p, q )) = w (( p, q )) ,iff p is q  X  X  k -nearest-neighbor or q is p  X  X  k -nearest-neighbor. In con-trast to traditional methods, here the neighborhood relationship is required to be symmetric: if q is p  X  X  k -NN neighbor, then p is q  X  X  k -NN neighbor. The k in k -NN can be selected as the same as the value of MinPts used in LOF k . Second, we use  X  -LOF-Finder to find local outliers. The two-step approach turns out to be successful for finding local outliers in numerical spaces. We call it  X  -LOF-with-k -NN. The results are shown in Fig. 8 .  X  -LOF-with-k -NN can also identify o as an outlier. Furthermore,  X  -LOF-with-k -NN can also identify that objects in C 1 and C 2 have distinct different  X  -LOF values, whereas their LOF k values are the same.  X  -LOF suggests that the degree of o being a local outlier is higher in terms of C 1 rather than C 2 .LOF k cannot identify the relationships between o and C 1 ,and between o and C 2 because LOF k values for objects in C 1 and C 2 are the same. 6 Experimental studies In the experimental studies, we investigate several issues. First, we study the two local outlier detection approaches,  X  -LOF and LOF k on a categorical dataset (the NRC university-ranking dataset). Due to the fact that there are many different ways to measure the relationships for categorical data, we use the three similar but different sets of measures, as discussed earlier. The three issues here are: the explanation of the intention behind the two approaches for identifying top local outliers, the significance of outliers being found out, and the influence of the mea-sures on the top local outliers they can find. Second, we study the two local outlier detection approaches using a multidimensional dataset (the NHL hockey players dataset for the 2001 X 2002 regular season).  X  -LOF uses the reciprocal of the Eu-clidean distance used in LOF k . Here, the issue is whether  X  -LOF-with-k -NN can be applied to large multidimensional numerical datasets such as LOF k . 6.1 Categorical data testing We conducted our testing using the NRC university-ranking dataset introduced above. Two separated testing were conducted: using 23 areas in three groups, and 41 areas in all five groups. We mainly discuss the 23-area testing result in this section. We also conducted testing for all the 41 areas that supports the same conclusions drawn from the analysis in this section. Due to the limited space, we do not show the results for the 41-area testing. In the following experimental studies, we show the top 10 outliers with their  X  -LOF or LOF k values respectively. based weighting ( w 1 versus d 1 ), ordinal-based weighting ( w 2 vs d 2 ), and numerical-based weighting ( w 3 vs d 3 ). For our  X  -LOF approach, we use  X  =3 , 4 . For the LOF k approach, we use MinPts =10 , 30 .Thetwo MinPts are chosen as MinPts =10 implies less than 10% of the dataset size, and MinPts =30 implies about 25% of the dataset size. Both MinPts values are in the recom-mended range given in [ 4 ]. The top outliers found by  X  -LOF and LOF k under three different weighting schema, for the 23-area testing, are shown in Tables 6  X  8 respectively. 6.1.1 Intention Here, we use the categorical-based measures ( w 1 versus d 1 ) to explain the intention behind  X  -LOF and LOF k . The testing results are shown in Table 6 . The top local outliers found by  X  -LOF and LOF k are not exactly the same, as expected. LOF k gives quite high values to universities such as George Washing-ton , Indiana ,and Wyoming , whereas  X  -LOF gives high values to Wisconsin , both  X  -LOF and LOF k ; case 2, found by LOF k but not  X  -LOF; and case-3, found by  X  -LOF but not LOF k .
 denotes a cluster of universities. A solid line connecting two small circles with a weight denotes how similar the two universities are. A solid line between a small circle and a large circle indicates that the university (small circle) is similar to the cluster (large circle). We ignore the weight on such solid lines. To keep the figure clear, we only present the selected universities and the connections with highest similarity in Fig. 9 .  X  Case 1 ( found by both  X  -LOF and LOF k ). Figure 9 a shows a case in which  X  Case 2 ( found by LOF k but not  X  -LOF ). As an example, we explain the reasons  X  Case 3 ( found by  X  -LOF but not LOF k ). Wisconsin and Washington are only cal outliers. In contrast,  X  -LOF is strong at finding objects in the center of several different (weakly connected) objects in a local area as local outliers. Therefore, the local outliers found by  X  -LOF are those universities having many high scores in more than one area groups. These universities (outliers) are connected to their neighbors with similar scores in the corresponding areas. The neighbors that have different scores in other areas form a local cluster. The outlying university is usu-ally beside one or in the middle of several such local cluster(s).
 shown in Tables 7 and 8 . 6.1.2 Significance and stability In this section, we first examine the significance of the top local outliers. We de-fine a significance measure below. Let D N denote the set of LOF values, and D n denote the set of top-n LOF values, where LOF value can be either  X  -LOF or LOF k . The significance, sig ( D n ,D N ) ,isdefinedas small or the difference between the average of D n and average of D N is large. In-tuitively, the larger the significance, the better the result. A large significance value implies high significance of the top D n outliers in comparison with all objects in D showninFig. 10 ,where n =3 , 10 . The result shows that  X  -LOF has higher significance for both top-3 and top-10 local outliers than LOF k . This means that the top local outliers found by  X  -LOF are of high confidence. The results show that our method can assign top local outliers a high  X  -LOF value with high confidence. ilar but different weighting schema, we investigate the stability of  X  -LOF and LOF k for the three different weighting schema using the results shown in Ta-bles 6  X  8 . A summary is shown in Table 9 .Table 9 shows that the top local out-liers found by  X  -LOF are consistent under different schema. Most top outliers are found by all three schema, and only a few outliers are found by just one scheme. However, LOF k  X  X  results are not consistent under the three different distance def-initions. LOF 10 does not even find the same university appearing as the top three outliers in all three schema. The outliers found by  X  -LOF are insensitive to the weighting scheme selected. Therefore, it is more stable than LOF k , and is suitable for applications in which different weighting approaches are available but hard to explain. 6.2 Numerical data testing We t e s t e d  X  -LOF and LOF k on an NHL hockey players dataset. Like [ 4 ], we select three attributes from the dataset: points scored , plus  X  minus statistics and penalty-minutes .  X  -LOF uses the reciprocal of the Euclidean distance used in LOF k .
 k -NN is a preprocessing step for  X  -LOF. In this experimental study, we use the same k ( MinPts ) for both  X  -LOF-with-k -NN and LOF k ,where k = MinPts = 10, 30, 50 and 549. When k = 549 , the result of  X  -LOF-with-k -NN is the same as  X  -LOF because k = 549 allows all objects in the dataset to be considered as neighbors. In addition,  X  -LOF-with-k -NN uses different  X  values where  X   X  2 . We only show  X  =2 here. The results for  X &gt; 2 are similar. There is an informal association between  X  -LOF and LOF k for this dataset. Players with high values of points scored tend to be distinctive players. The density of those distinctive players X  neighborhood are lower than the others. Figures 11 and 12 show the  X  -LOF-with-k -NN and LOF k results, respectively.
 number ID have high values of points scored . Figure 11 a X  X  show that, with any k , the players who have high values of points scored also have higher  X  -LOF values. The top outliers always have distinctive  X  -LOF values. Furthermore, the top three outliers found by  X  -LOF using different k -NN preprocessing, k = 10, 30, 50 and 549, are almost the same. Consider the outlier found by both  X  -LOF-with-k -NN and LOF k for the same k ; its  X  -LOF value tends to be higher than its LOF k value.
 this is not local outlier detection, but a global outlier detection. When k =10 , the LOF k values are not distinct for top outliers, see Fig. 12 a. When k = 549 , only the top outlier has a distinct, high LOF k value, whereas the LOF k values for all the other players are almost the same. As shown in Fig. 12 a X  X , the top local outliers change with different k values, as expected. We carefully examined the local outliers, and cannot find a good explanation for the relationship between the parameter k ( MinPts ) and the local outliers found, although some players who have low values of points scored also have lower LOF k values. Many players who have a low value of points scored also have a similar LOF k value to those players who have a high value of points scored . Since LOF k only considers the local density, the parameter k ( MinPts ) is very sensitive. A small change in k ( MinPts ) may transform a player into a local outlier from a non-outlier, or transform a player into a non-outlier from a local outlier. The LOF k values are significantly influenced by the parameter k ( MinPts ) , which leads to an unstable ranking of the top local outliers. In addition, top outliers may not have a distinct, high LOF k value. 7 Related work Outlier detection was initially studied in the field of statistics [ 2 , 8 ]. Many tech-niques have been developed to find outliers, and these can be categorized into distribution-based [ 2 , 8 ] and depth-based [ 16 , 18 ] methods. However, recent re-search proved that these methods are not suitable for data-mining applications, since they are either ineffective and inefficient for multidimensional data, or need a priori knowledge of the distribution underlying the data [ 13 ]. Traditional clustering algorithms focus on minimizing the effect of outliers on cluster-finding analysis in these methods, so that they are only by-products of no interest. outlier detection has been studied intensively by the data-mining community are sparse within the hypersphere defined by a given radius [ 13 ]. Researchers have developed efficient algorithms for mining distance-based outliers, such as the index-based algorithm, the nested-loop algorithm, the cell-based algorithm, the cell-based algorithm for disk-resident data [ 13 ], and the partition-based algorithm [ 17 ]. However, since these algorithms are based on distance computation, they may fall down when processing categorical data or datasets with missing values. In fact, the heuristics used in these algorithms are inapplicable to high-dimensional datasets even when all the attributes are numerical and contain no missing values. based on a statistical test [ 20 ]. However, both the attributes for locating a spatial object and the attribute value associated with each spatial object are assumed to be numeric, which restricts the application of efficient algorithms to datasets with categorical attributes.
 property, namely density; this kind of method is called local outlier detection. studied for local outliers. The process of mining LOF-based outliers relies on the materialization of k nearest-neighbor search results. It has been proven that objects deep in a cluster will have an LOF value of approximately 1, which is a common property of  X  -LOF and LOF. It has also been shown that LOF-based methods tend to find relatively low-density objects, while the  X  -LOF method described herein tends to find objects similar to several totally different objects. Our analysis and experiments show that LOF k is not stable under different distance definitions, which makes the explanation of mining results difficult.
 LOF k , COF tends to identify low-density objects as outliers. COF computes den-sity using a fading method so that the shifting of local density is considered. In most experiments that we have conducted, COF finds the local outliers as well as LOF, although the outlier factors are more distinct. In contrast to LOF and COF,  X  -LOF was invented to find outliers with different neighbors.
 an object is exceptional, and whether it is dominated by other outliers. This in-formation is called the intensional knowledge of the outliers. Different strategies to scan the value lattice is analyzed and evaluated so that efficient algorithm is developed. Aggarwal and Yu argued that this approach may be expensive for high-dimensional data [ 1 ]. Therefore, they proposed a definition for outliers in low-dimensional projections and developed an evolutionary algorithm for finding outliers in projected subspaces for numerical data. They identify global outliers and tend to find low-density objects in subspaces. In contrast,  X  -LOF attempts to find local outliers whose neighbors are dissimilar.  X  -LOF can handle not only nu-merical but also categorical data.  X  -LOF X  X  result is stable in the sense that is is not greatly affected by the similarity definitions. Therefore,  X  -LOF is more suitable for real-life complex data-mining applications.
 tures in Web environments.  X  -LOF uses mutual reinforcement in a novel way. There are several main differences between  X  -LOF and HITS. First, HITS was de-veloped for processing directed graphs without weights on the edges, while  X  -LOF is designed for a weighted undirected graph. Second, HITS was developed to find global hubs and authorities, while  X  -LOF aims to find local outliers. Therefore, the  X  -LOF value is defined based on the comparison between the accumulated similarities of an object and the objects it reaches within  X  -walks. Furthermore,  X  -LOF ensures the boundary of the outlier factor. To the best of our knowledge, this is the first attempt to apply mutual reinforcement in outlier detection. attributes. It takes advantages of common neighbors, based on categorical data similarities, to define links between pairs of objects. Although this similarity is also used in  X  -LOF, the key difference is that ROCK aims to find globally sim-ilar objects as clusters, while  X  -LOF finds local outliers. Furthermore, ROCK only considers single-level link information, while  X  -LOF is based on  X  -walks, in which  X  is dynamic. 8Conclusion In this paper, we studied a novel mutual-reinforcement-based local outlier detec-tion approach,  X  -LOF. In comparison with the existing density-based local outlier detection approach, LOF k ,  X  -LOF attempts to find local outliers in the center rather than around the boarder, which are unique on one hand and are similar to some clusters of its neighbors on the other. Unlike LOF k , which is designed for handling numerical data,  X  -LOF uses similarity graphs, and is designed for categorical/ordinal as well as numerical data. However,  X  -LOF shares the idea of measuring density in a different way by measuring the radius between how many walks in a similarity graph go towards other points and how many walks come inwards (mutual reinforcement). It is important to know that  X  -LOF considers not only the similarity between an object and its neighbors but also the similarity among the object X  X  neighbors within  X  -walks. For categorical/ordinal data,  X  -LOF does not need to use a MinPts parameter. It uses K to control the maximum num-ber of walks (because the maximum number of walks in an undirected graph is infinite). In general, the larger the number of walks, the more stable the identified local outliers. For numerical data, we use the k -NN ( k -Nearest-Neighbor) method as a preprocessing step to convert a dense similarity graph into a sparse similarity graph, and then apply  X  -LOF. The k value of the k -NN method can be selected to be the same as the value MinPts . We conducted extensive experimental stud-ies using two real datasets: the 1993 NRC Ranking and the NHL hockey player dataset for the 2001 X 2002 regular season. We found that  X  -LOF can identify a few local outliers in the center with high confidence, and is stable for similar but dif-ferent user-defined similarity measures. Our technique can reduce the burden on users to determine the relationships between data items, and find the explanations why outliers are found.
 References
