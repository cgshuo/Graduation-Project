 generally categorized into feature extraction and feature selection algorithms. In the past, few works have been done to contrast and unify the two algorithm categories. In this work, we introduce a matrix trace oriented optimization framework to provide a unifying view for both feature extraction and selection algorithms. We show that the unified view of DR algorithms allows us to discover some essential relationships among many state-of-the-art DR algorithms. Inspired by these essential insights, we propose to synthesize unlimited number of novel DR algorithms by combining, mapping and integrating the state-of-the-art algorithms. We present examples of newly synthesized DR algorithms with experimental results to show the effectiveness of our automatically synthesized algorithms. role in various machine learning applications such as bioinformatics [10] etc. They are generally categorized into Feature Extraction (FE) and Feature Selection (FS) algorithms [13]. On one hand, the FE algorithms consist of linear approaches such as the Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) [7] and nonlinear approaches such as the Locally Linear Embedding (LLE) [9], Laplacian EigenMap etc. In this paper we mainly consider the linear ones as examples due to their efficiency in real world applications [12]. On the other hand, the FS algorithms can be generally categorized into Wrapper and Filtering algorithms [3]. In this work, we focus on the Filtering algorithm category due to their independency on statistical learners. and unify the FE and FS algorithm categories. Many state-of-the-art FE and FS algorithms such as LDA and Relief [5] are separately proposed by explicitly or implicitly optimizing some objective functions subject to different constraints. In this paper, we introduce a matrix trace oriented optimization framework to provide a unifying view for both FE algorithms and FS algorithms. In this unified view, we discover some essential relationships among many state-of-the-art DR algorithms, which were proposed independently in previous works. In addition, these discovered essential relationships among algorithms allow us to synthesize unlimited number of novel DR algorithms for various real world applications. FE and FS algorithms are their different feasible-solution regions in the optimization framework. For a set of DR algorithms that can be formulated as the optimization problems, if they have the same feasible-solution region, we will prove that all objective functions in this algorithm set span a linear algebraic space, in which each point is a DR algorithm's objective function. We call this space the Algorithm Space (AS). Thus the FE algorithms and FS algorithms can span two linear algebraic spaces respectively under the matrix trace oriented optimization framework. In contrast to some previous DR algorithm frameworks [14], the proposed spatial view of algorithms is interesting from two perspectives. First, it provides a unifying view for both FE and FS algorithms. Some important insights can be revealed by examining the relations between the two algorithm categories. Second, it allows new algorithms to be synthesized within and between the spaces of FE algorithms and FS algorithms. algorithm spaces under a matrix trace oriented optimization framework for the linear FE algorithms and Filter-based FS algorithms respectively. Through analysis of the two spaces, we reveal some interesting insights of the two categories of algorithms, as follows. combination of some other algorithms (See Example 1). Since the linear combination of any point in a linear algebraic space also belongs to the same space, we can synthesize novel DR algorithms through a linear combination of state-of-the-art algorithms within each AS (See Example 2). space of FE/FS to the other and show that the result of the mapping operation can be a novel FS/FE algorithm accordingly (See Examples 3 ~ 6). This mapping function view allows us to discover some essential relationship between some state-of-the-art algorithms. It also allows synthesizing novel algorithms through this kind of cross space mapping operation. machine learning algorithms for real learning tasks. However many traditional FE algorithms fail due to their high complexity. To reduce the computational cost, we propose to integrate a FE algorithm with its mapping target in another space (See Example 7). Through this cross-space integration, we can efficiently approximate the solutions to some complex DR problems. trace oriented optimization framework for FE and FS algorithms with the definition of the algorithm spaces for FE and FS algorithms respectively. We also present examples for how to combine different algorithms within a space. In Section 3, we present the algorithm mapping operations between spaces with examples. In addition, as an application instance, we propose a novel algorithm for an information retrieval (IR) task by cross-space algorithm integration. In Section 4, we present our experimental results. We summarize this paper and discuss the future works in Section 5. n instances and d features in a measurement space. DR algorithms aim to find an optimal projection f : R y = f ( x i ) R P . To finding optimal f according to some optimization criteria, many DR algorithms have been developed and formulated as constraint optimization problems. We use to describe their general form, where is a solution region, is a feasible-solution region and J stands for an objective function. In Section 2.1 and 2.2, we define two optimization frameworks for the FE algorithms and FS algorithms respectively. After that, in Section 2.3, we prove that each optimization framework can be considered as a linear algebraic space if we treat each algorithm's objective function as a point in this space. f is a linear function, i.e. y i = f ( x i )= W T x i , where W is the projection matrix. Thus, the solution region of linear FE algorithms is . A natural constraint on W is that the extracted p features should be identity matrix. We define the feasible solution region of FE algorithms as . Thus a general framework for FE algorithms can be represented by, where J FE can be the objective function of any arbitrary FE algorithms. As examples, Table 1 summarizes some state-of-the-art FE algorithms. 
PCA 
LDA 
MMC Criterion [6]. The matrix is the covariance matrix. The are between class scatter and within class scatter matrix respectively [7], where k is the number of classes and n is the size of class j in the training data. and m global sample mean and class mean respectively. From these examples we can observe that these algorithms aim to optimize the trace of different matrices. Thus we framework. from an intuitive example. Suppose that we want to select the second and the fourth features of a five dimensional vector while discarding the others. We can reduce the dimensionality of the problem by multiplying the problem vector with a matrix, FS algorithm is equivalent to finding a d by p binary matrix W, such that each column of W has only one nonzero entry and each row of W has at most one nonzero entry. Note that by exchanging the columns of W , we can select the same group of features. To guarantee the uniqueness of solutions, we further require the selected features to obey their original order. Thus the solution region and feasible region of FS algorithms can be written as and feature selection algorithms is where J FS can be the objective function of any arbitrary FS algorithms. As an example algorithm in the FS algorithms' optimization framework, Yan et al. formulated the classical Information Gain (IG) algorithm as a matrix trace oriented optimization problem [13]. In terms of IG, suppose we select a group of features T , the information gain of T is, () ( )log( ) () ( |)log( |) I GT Pk Pk PT Pk T Pk T where () probability of feature group T and (|) corresponding conditional probability. The aim is finding a group of features T among all the d original features which can maximize the information gain () I GT . Since there has one to one map between T and Information Gain (IG) based feature-selection algorithm can be formulated as, (  X 
FS,  X  FS , J IG ) in the optimization framework of feature selection algorithms. algorithms, let be a set of DR algorithms that have the same solution region and feasible-solution region. We define two computations and an abstract zero-algorithm to prove that the algorithms in set can span a linear algebraic space with each point in this space is the objective function of a dimension reduction algorithm. addition computation of two algorithms as the addition of their objective functions a i + a j = (  X  ,  X  , J i multiplication as  X  a i = (  X  ,  X  ,  X  J i ). as , where for any bounded objective function . linear space of objects, where each object is an algorithm's objective function. We call it the Algorithm Space (AS) linear combination is a DR algorithm with the form , where , i.e. . (Though conflict with the closeness of .) 
 X = =  X   X  = &gt;  X  X  X  definitions 1 and 2, we have, verified, we end the proof of Lemma 1. can be formulated as , span a linear space (FEAS) is the space spanned by linear FE algorithms algorithms. (FSAS) is the space spanned by FS algorithms and FS algorithms, which can be formulated as optimization problems. Within each algorithm space, a detailed algorithm is determined by its objective function. Note the spatial view of algorithms allows the linear computations. Through operations such as addition, we can synthesize new DR algorithms through a linear combination of existing algorithms. In the rest of this section, without loss of generality, we use the algorithm name to denote an existing algorithm to show some examples in FEAS. stands for the Orthogonal Centroid Algorithm [8]. (OCA) is also a supervised algorithm. It is a QR matrix decomposition based FE algorithm. However, the time and space cost of QR decomposition cannot meet the efficiency requirements of large-scale data processing. In the work [8], the authors pr oved that the OCA algorithm can also be formulated using the optimization framework. In other words, the solution of Orthogonal Centroid optimization problem, framework by , with Note that it is easy to prove , then, which verifies Example 1. This example shows that an algorithm can be a linear combination of other algorithms within an AS. In the same way, we can synthesize new DR algorithms by linearly combining state-of-the-art DR algorithms. combination of PCA and MMC with weights . Let and The algorithm is where Weighted MMC (WMMC) since it is a weighted version of classical MMC algorithm. and OCA are all special cases of the newly generated WMMC algorithm by choosing proper weights. In both FEAS and FSAS, there are many other interesting examples. For instance, the OCA+LDA is a variation of the classical LDA. To save space, we will not discuss more examples in this paper. The example 1 and 2 are used to show that the linear combination of existent FE or FS algorithms can be used to generate unlimited number of novel FE or FS algorithms in the algorithm space (AS) derivate from the constraint optimization framework of dimension reduction algorithms. that a major difference between FE and FS algorithms is in their different feasible-solution region . In this section, we propose a mapping functional between FEAS and FSAS, which can be used to synthesize novel FE/FS algorithms from existing FS/FE algorithms, by exchanging their feasible-solution regions. functional from FEAS to FSAS such that , According to Definition 5 we have . then . Thus is an invertible mapping functional from FEAS to FSAS. objective function of a FE algorithm in a feasible-solution region of FS algorithm space. Conversely, we can exchange FS and FE and ask the same question. We give an answer to this question in the following subsections and show that we can derive novel algorithms that can be synthesized through the mapping functional . Note that not all algorithms have closed form solution after the mapping, thus we propose approximate algorithms for sub-optimal solutions under this condition. mapped target of LDA algorithm, i.e. (LDA), is the classical Canonical Fisher Score (CFS) for feature selection. According to Theorem 1, . Thus the solution of is the matrix Let . Then each column of a binary indices of these  X 1 X  values, i.e. w i ( k )=1 iff k=k w ( k ) is the k th entry of w i . Using S i,k entry in the k th row and i th column of any matrix S , we can rewrite the objective function of LDA as Thus maximizing in H FS equals to selecting p features from d features to maximize Equation (1). This procedure can be carried out in a greedy manner with a complexity of O ( d ). Defining a feature score according to the equation, . The p features with the largest scores approximately maximize Equation (1) in a greedy manner. This is the CFS for feature selection, i.e. (LDA)=CFS. Let w i ( k )=1 if k=k and w i ( k )=0 otherwise, for all . We get matrix. This example shows that a FE algorithm can derive a FS algorithm through the mapping functional . it the Discrete MMC (DMMC). Note that . Its solution is symbols as Example 3, we have To maximize the objective function (2), we define a feature score . Since the p features with the largest scores can maximize equation (2), we can select features according to their feature scores. As a result (MMC)=DMMC is the novel DR algorithm. derivation from between-space algorithm mapping. For instance, the mapped target of OCA is the OCFS algorithm which was proposed by Yan et al. in [13] DMMC+DV=2OCFS); and the mapped target of the LPP algorithm is the Laplacian Score which was proposed by He et al in [3]. based method for feature selection [3]. It computes the features with the largest scores. In this example, we show -1 (DV) = PCA. If we reverse the derivation of Example 3, the objective 
H FS . Thus the solution of DV is subject to W H FS . By exchanging the feasible-solution region, we have -1 (DV) . Thus we have -1 (DV) = PCA and (PCA) = DV. algorithm. In this example, we introduce -1 (Relief) which is a novel FE algorithm. We additionally modify the mapped target of Relief in FEAS to seek a closed form solution. As described in [11], Relief aims to maximize the margin where and are the nearest neighbors of x i that have different and the same class labels, respectively. 
Www w = " The vector gives the weights for the d features. Relief selects p features whose weights are larger than a given threshold. Instead of solving it iteratively, we modify its solution region to , where each object in is a binary vector that indicates which of the p features are to be selected. Thus each in is a vector with p ones and ( d-p ) zeros. and otherwise. Let be the solution of maximizing (3) in , then . Note the solution of classical Relief is also a binary vector after thresholding, i.e. , then . This ends the proof. solution region for Relief directly, instead of using an iterative optimization algorithm. For simplicity, let where . Let . Maximizing the objective function (4) subject to is equivalent to selecting p features with largest scores. Conversely, . It is easy to see that there exists a unique such that . Since If maximize R ELIEF J , then allows to be maximized as well. This ends the proof. descent based strategies, we propose to modify it in order to obtain a closed-form solution by changing the distance measurement from to . Let consists of the leading eigenvectors of matrix , where and . This solution is the same as that of MMC algorithm if we replace by and . Motivated by the relationship between MMC and LDA, we can derive a variation of the objective function, Similar to LDA, the optimization of (6) can be solved by the Generalized Singular Value Decomposition (GSVD). We name it the Relief based Feature Extraction (Relief-E) algorithm. fail due to their high complexity. For instance, the complexity of PCA is O ( d 3 ) which is not suitable for the high dimensional data. However, the complexity of (PCA) is only O ( d ). Since PCA and (PCA) aim to optimize the same objective function, we propose to integrate the high complexity FE algorithm with its mapped target in FSAS to efficiently approximate the solution of it. In this example, we can compute PCA in 
R by PCA. The complexity of the two step algorithm is () Od p + . In contrast to O ( d 3 ), the complexity is greatly reduced if pd &lt;&lt; . been thoroughly studied in information retrieval research. LSI projects a term by document matrix into a lower dimensional semantic space through SVD. A bottleneck of classical LSI is its complexity, which is dimensional data for real tasks. In response, we propose a 2-step LSI algorithm which is an integration of FEAS and FSAS for LSI on high dimensional data. According to [2], the solution of covariance version LSI is the same as PCA. Though the () P CA  X  works on the high dimensional data better than PCA due to its efficiency, it cannot discover the latent semantics since it does not provide a linear combination of terms. We propose to compute a covariance version of LSI in two steps: step 1 reduces the feature scale efficiently by the () P CA algorithm; and step 2 computes the latent semantic index of terms by PCA on the reduced data. We name this Integrated algorithm as Approximated LSI (ALSI). among many state-of-the-art DR algorithms. Among the various algorithms that we discussed, the WMMC, DMMC, Relief-E and ALSI etc are all novel. In addition, under our propose algorithm framework, we can synthesize unlimited number of novel DR algorithms. Due to the space limitation, we will take one novel FE algorithm, Relief-E, one novel FS algorithm, DMMC, and one novel integrated algorithm, ALSI, as examples for experiments. We will not discuss which DR algorithm is the best since the performance of DR algorithms highly depend on the application scenario. In this paper, we use different datasets to show that each novel algorithm may have an application scenario in which it performs better than others. This confirms our motivation why we propose the algorithm space to synthesize unlimited number of novel DR algorithms for various real world applications. dataset, which is part of the UCI machine learning datasets. It has only 83 samples but 2,308 features [16]. The details of this bioinformatics dataset could also be found in (Statnikov, 2005). The experimental steps are similar to [11]. We reduce the data dimension by applying different DR algorithms. Then, we classify the reduced data by a given classifier in 10-fold cross measurement metric. In this work, we use the simple Nearest Neighbor as the classifier. To compare the FE algorithms, we project data to extremely low dimensional space, i.e. reduce to a feature set such that only 0.1% of the features remain for classification. The results are given in Table 2. The novel synthesized Relief-E can give significantly better performance compared with traditional PCA and MMC. 
Table 2 . A comparison of FE algorithms in extremely P RECISION 33.73% 36.14% 79.52% existent FS algorithms, we use the same SRBCT dataset for experiments. Instead of comparing them on a specific dimension, we show their best precision and the best dimension on which they achieve the best performance. In Table 3, it can be seen that DMMC can give the best performance by preserving a smaller number of features than other state-of-the-art FS algorithms. If we do not conduct dimension reduction, the classification precision is only 84.34%. It can be improved to 98.8% by selecting 13 features among the 2308 features through DMMC in terms of classification task. DV 93.38% 230 CFS 98.8% 43 DMMC 98.8% 13 Relie f 96.8% 184 N o Reduction 84.34% 2308 we evaluate the effectiveness of the integrated ALSI on large scale text corpus in a text retrieval task. We used the TREC2 and TREC3 ( http://trec.nist.gov/data/docs_eng.html ) , which are standard datasets used for information retrieval algorithm evaluation, as evaluation datasets. They share the same document collections but have different queries (topics) for the evaluation. There are 742,358 documents and 642,889 terms. Each dataset has 50 queries. All text documents are indexed in the vector space model through Lemur Toolkit ( http://www.lemurproject.org/ ) such that all documents are represented by numerical vectors. The evaluation of information retrieval is different from classification task, here we use the commonly used Mean Average Precision (MAP). not returning irrelevant documents to given query in the top n results.  X @ X   X  over all queries, which is defined through P@ n . The AP is given by, function indicating whether the  X   X  X  X  document is relevant or not for a given query. [1], which is a classical text retrieval algorithm without dimension reduction, and DV feature selection algorithms are used as baselines. We do not use any FE algorithms such as LSI as baseline since the large data scale makes them unusable without super computers. The experimental results are shown in Table 4. # F EATURE 642,889 20,000 20,000 after dimension reduction. Since there has no training data, DV is the only unsupervised FS algorithms discussed as examples of this paper. Thus DV is outstanding performance of ALSI in enhancing the text retrieval. Table 2, Table 3 and Table 4 have shown that Relief-E is very effective when the selected feature number is extremely small; DMMC is very efficient and can provide good performance for classification on SRBCT data. ALSI is effective for text retrieval task. This shows that the newly synthesized DR algorithms are all effective for some applications. essential relationships between FS and corresponding FE algorithms. Figure 1 gives the results for PCA, Relief and MMC in contrast to their mapped targets. We implement the experiments on Waveform dataset which is another subset of UCI dataset. The experiments configuration is the same as the classification experiments on SRBCT data. The x-axis is the number of remaining features after dimension reduction. The y-Waveform data: (i) algorithms in FEAS are significantly better than their mapped targets in FSAS if the reduced data dimension is relatively small; (ii) the best performances of FE and corresponding FS algorithms are comparable; (iii) the precision of a FS algorithm converges to its best performance as the number of performance is achieved by unsupervised PCA in Figure 1 on the Waveform data. This provides us some intuitions in selecting algorithms. (x-axis) curve on Waveform of DR Algorithms. We further defined the algorithm spaces on FE and FS algorithms respectively. Each algorithm space consists of many potential algorithms, and the two spaces can be mapped to each other, resulting in new insights which can help in analyzing different algorithms. We further give three ways to synthesize novel DR algorithms, by combining existing algorithms, mapping between algorithm spaces and by the integration of two algorithm spaces. Experiments confirm that the algorithms constructed by these methods can work well on different application datasets. In the future, we will define kernel on AS for measuring algorithm consistency. [1] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, [2] Dupret, G. (2003). Latent concepts and the number [3] He, X.F., Cai, D. &amp; Niyogi, P. (2005*). Laplacian [4] He X. F., Yan. S.C., Hu Y., Niyogi, P., Zhang H.J. [5] Kira, K., &amp; Rendell, L. A. (1992). A practical [6] Li, H. F., Jiang, T., &amp; Zhang, K. (2006). Efficient and [7] Martinez, A.M. and Kak, A.C. (2001) PCA versus LDA. [8] Park, H., Jeon, M., &amp; Rosen, J. B. (2003). Lower [9] Roweis, S.T. and Saul., L.K. (2000). Nonlinear [10] Saeys Y., Inza I., and Larranaga P. (2007). A review of [11] Sun, Y. J. &amp; Li, J. (2006). Iterative RELIEF for [12] Tenenbaum, J. B., Silva, V. D., &amp; Langford, J. C. [13] Yan, J., Zhang, B. Y., Liu, N. &amp; et al. (2006). [14] Yan, S. C., Xu, D., Zhang, B. Y. &amp; Zhang, H. J. [15] Yang, Y. M. &amp; Pedersen, J. O. (1997). A [16] Hettich, S. and Bay, S. D. (1999). The UCI KDD 
