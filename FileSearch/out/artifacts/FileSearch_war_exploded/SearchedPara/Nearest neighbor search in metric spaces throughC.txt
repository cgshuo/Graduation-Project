 1. Introduction
Traditionally, search has been applied to structured (attribute-type) data yielding records that exactly ries involving complex data types such as images, videos, time series, text documents and DNA sequences.
Similarity search is based on gradual rather than exact relevance using a distance metric that together with can be ranked according to their estimated relevance. However, experience with the current, mostly central-acceptable for the expected dimension of the problem.

Peer-to-peer (P2P) architectures seem to solve the problem of scalability, and several scalable and distributed search structures have been proposed even for the most generic case of metric space searching (see Section 2 for a survey). They mostly concentrate on the similarity range queries where the execution algorithms satisfying (1) the autonomy of updates and (2) no central coordination or flooding strategies, are easier to implement. Since no bottleneck occurs, the structures are scalable and high performance is achieved through parallel query execution on individual peers (computer nodes).
 bors algorithms are much more difficult to implement in P2P environments. The main reason is that tradi-tional (optimum) approaches are based on a priority queue with a ranking criterion, which sequentially ing are completely in contradiction with decentralization and parallelism objectives of any P2P search network.

Capitalizing on our previous work of similarity range search through MCAN ( Falchi, Gennaro, &amp; Zez-ula, 2005 ), in this article we propose and experimentally test several nearest neighbor search algorithms.
We first summarize the necessary background in Section 2 , including the related work. Then in Section 3 we define the main properties of the MCAN. Section 4 describes alternative strategies for the nearest neighbor search, while the results of experimental testing are reported in Section 5 . The paper concludes in Section 6 . 2. Background The most fundamental to our work are the Content-Addressable Network (CAN) ( Ratnasamy, Francis, of nearness. In the following, we provide the necessary background and survey relevant literature. 2.1. Content-Addressable Network (CAN)
The CAN is a distributed hash table that uses a function for mapping  X  X  X eys X  X  onto  X  X  X alues X  X  in order to sible values of K as S , we can define the mapping function G of the CAN as following: where P N is an hyper-rectangle of R N defined as with D i denoting the i th side length of the CAN structure.
 The principle of the CAN is to divide the hyper-rectangle P overlap along N 1 dimensions and are adjacent along one dimension.

The basic operation in CAN is a lookup(key) function, which returns the corresponding  X  X  X alue X  X  (e.g. the IP retrieval purposes. The search starts from an arbitrary peer of the CAN structure, and proceeds by routing a message towards its destination by simple greedy forwarding to the neighbor with coordinates closest to the destination coordinates. In general, if we divide the P N bors. Furthermore, the average routing path length is given by ( N /4) n 2.2. Metric spaces function d : D D ! R able to compute distances between any pair of objects from D . It is typically assumed the distance must satisfy the following properties: which have a distance from the query object Q 2 D at most the specified threshold (range or radius) q :
Q . We can extend this type of query to return k nearest objects that form a set K F such that j K j X  k and neighbors queries. 2.3. Pivot mapping and filtering
In this section we discuss the general pivot based strategies we used both for mapping metric space objects into vectors and for filtering during search operation in MCAN. 2.3.1. Mapping In general, the pivot-based algorithms can be viewed as a mapping F from the original metric space
M  X  X  D ; d  X  to a N -dimensional vector space. The mapping assumes a set T ={ P from the original objects as
Using triangle inequality for d in the original metric space M and then mapping is then contractive and d 1 in the derived space can be considered as a lowerbound for the original distance in the metric space M . 2.3.2. Filtering
At search time, we compute for a query object Q the query feature vector F ( Q )=( d ( Q , P for which During the evaluation of the k nearest neighbors (kNN) if at a given time we have the temporary result
O , ... , O k we can avoid the evaluation of d ( Q , O )if
In other words, the object O can be discarded if for some pivot P 2.4. Related work
Many metric-based indexing principles and index structures have been proposed, focusing on the pruning of data.
 ilarity search becomes too expensive when the stored data volume grows, because the search costs increase a distributed processing.

Restricting to multi-dimensional range and kNN queries in vector spaces, several recent works have pro-posed distributed structures for such tasks. Unfortunately, these structures are often designed for specific applications (for example spatial data) typically using vectors of low dimensionality. The vector space approach cannot be applied on many important datasets where similarities are measured by functions such as the Hausdorff distance, Jaccard X  X  coefficient, edit distance, etc.

The MAAN structure ( Cai, Frank, Chen, &amp; Szekely, 2003 ) extends the Chord protocol to support multi-attribute and range queries by means of uniform locality preserving hashing. This system expects the exact knowledge of the attribute domain distributions. Moreover, no experiments on real-life datasets are provided larity queries aside from the range search.

Ganesan, Yang, and Garcia-Molina (2004) show in the SCRAP structure a way to adapt kd-trees, which support multi-dimensional range queries, by exploiting the Chord protocol. Unfortunately, this approach becomes inefficient for more than two dimensions. In the same paper, the authors propose the MURK struc-ture that uses the space-filling curves together with CAN (improved by skip pointers). Although no real-life and cannot be applied to data that is metric by its nature.

The Mercury ( Bharambe, Agrawal, &amp; Seshan, 2004 ) provides a protocol for routing multi-attribute range-based queries. The efficiency of this protocol is measured in terms of number of hops and total number of messages for routing of three-dimensional range queries. Again, the motivation and potential application of this protocol is less general and slightly different than the ones addressed by this paper.
Wolman, 2003 ) (both improved by Aspnes, Kirsch, &amp; Krishnamurthy (2004) ) extend the concept of Distri-buted Hash Tables by the principles of locality and load balancing. These features give the opportunity to support range queries on one dimension. These approaches do not aim on more complex similarity queries.

The pSearch approach ( Tang, Xu, &amp; Dwarkadas, 2002 ) uses two traditional information retrieval algo-system based on the CAN routing protocol. This concept, defining similarity between a document and a query by means of their common terms, is only suitable for the text retrieval.
Tanin, Harwood, and Samet (2005) introduce a P2P generalization of a quadtree index. The authors pro-this structure is limited to the spatial domain .
 Banaei-Kashani and Shahabi (2004) formalize the problem of vector-based similarity search in P2P Data
Networks and propose SWAM  X  a family of small-world based access methods. This concept provides a general solution for the range and nearest neighbors search in the vector-based datasets.
Four different distributed structures have been proposed for indexing and similarity search in the metric form the metric search issue into a different problem and take advantage of some well-known solutions. 3. Principles of MCAN
The basic idea of our approach is to extend the CAN architecture in order to manage objects X of a generic information, and only distances between objects can be computed. To cope with this problem, we use the piv-
P function F () (introduced in Section 2.2) defined by Eq. (3) .

This virtual coordinate space is used to store the object O in the MCAN structure, specifically in the peer since a distance between two objects in MCAN is evaluated by means of the d ical Euclidean distance). Routing in MCAN works in the same manner as for the original CAN structure. An
MCAN peer maintains a coordinate routing table that holds the IP address and virtual coordinate zones of each of its immediate neighbors in the coordinate space. 3.1. Notation indicating the d 1 distance between the corresponding point in the coordinate space, e.g. d  X  x ; y  X  X  d  X  F  X  X  X  ; F  X  Y  X  X  . As we already explained, the MCAN is contractive, therefore d  X  x ; y  X 
We denote a peer of MCAN by the bold symbol n . Each peer n maintains its region information referred as the origin, denoted as n R v  X  X  n R v 1 ; n R v 2 ; ... ; n R v n  X  R  X  l 1 , n  X  R  X  l 2 , ... , n  X  R  X  l N . More precisely, the region n  X  R is defined as follows
The peer n also maintains the set of the neighbor peers X  information n  X  M ={ m the zone maintained by the peer n . More formally check if the query region Q intersects the zone associated with n . Note that, the range query in the d given by an hypercube of side 2 r centered in c .
 We can now introduce the formal definition of an N -dimensional MCAN structure, referred as MCAN which is composed of a set of h ( h &gt; 0) network peers { n (1) " i , j j i 6  X  j n i  X  R \ n j  X  R = ; (2) P N  X  (3) n 2 m M () 9 k j 1 6 k 6 N ;  X  n R v k  X  n R l k  X  m R v
In the definition, Point 1 states that the zones covered by the network peers do not overlap. Point 2 states that the union of the zones cover the whole MCAN N space P the condition for a network peer n to be a neighbor of m (as explained in Section 2 ). 3.2. Construction An important feature of the CAN structure is its capability to dynamically adapt to data-set size changes.
As we will see in the experimental evaluation, we are interested in preserving the scalability of the MCAN, then the number of objects each peer can maintain, we limit also the number of distance computations a peer have to evaluate during a range query computation.
 It is important to observe that in some cases we might want to use all the peers available in the network. applied in our MCAN. On the other hand, in a P2P environment, we would like to let the peers the possibility this is possible with a CAN, which even provides some fault-tolerance capabilities ( Saia, Fiat, Gribble, Karlin, &amp; Saroiu, 2002 ).
 the Incremental Selection algorithm described in Bustos et al. (2001) . This algorithm tries to maximize the average distance d 1 between two arbitrary objects in the derived space (i.e. d 3.3. Insertion
An insert operation can start in any peer of the MCAN. It starts by mapping the inserted object X to the the insertion proceeds with the greedy routing algorithm used in standard CAN structures: the inserting peer forwards the insertion operation to the neighbor peer which is closer to the point x by using the d same technique and the insert operation is forwarded again until the object X is inserted. exceeds its capacity it splits. Eventually, the object X is inserted in m or in the new allocated peer. 3.4. Split
In MCAN, we apply a balanced split, that is the resulting regions contain the same number of objects. Dur-of the metric objects is reallocated there.

If we define n 1 as the splitting peer, n 1  X  R as the old region, n split regions must satisfy the following equations:
Moreover, to respect these constrains, we create the new two regions by dividing the original one along one coordinate of the space. Therefore, the new regions, n equations: j n  X  R 0  X  l i n 1  X  R  X  l i /2 j is minimum.
 of its region. To those neighbors, which are also neighbors of n
The new peer is informed by n 1 about its neighbors that are a subset of the n discard information about the peers that no more are its neighbors. 4. Searching for nearest neighbors
Whenever we want to search for similar objects using the range search, we must specify the maximal dis-less than four edit operations between the two strings, which has a clear semantic meaning. However, a dis-sive, and the response sets might contain many not significant objects.

An alternative way to search for similar objects is to use the nearest neighbors queries. Such queries guar-object Q . Though the problem of executing k nearest neighbors queries is not new and many algorithms have readings, the distributed kNN query processing have not been systematically studied. 4.1. kNN search in MCAN
In MCAN we have developed three different strategies to perform kNN queries: parallel execution (PE), sequential execution (SE), and mixed mode execution (MME). Each of these techniques has its advantages and disadvantages which will be discussed later.
 limits k to be less than half of the MCAN peer capacity. However, in case k is greater than the number of if the region h q ; d  X  O k ; Q  X i is completely contained in the region n ( Q )  X  R (where O candidate result set of the kNN, and q  X  F  X  Q  X  ). If this is true, the kNN search correctly terminates and the k objects retrieved by n ( Q ) represent the query result of the kNN.
 propagate the kNN query execution among the peers involved. The generic behavior of these three approaches can be characterized as follows:
PE: All the peers overlapping the hypercube h q ; d  X  O k ping peers that receive the query first forward the kNN query and only then they start evaluating the query on their local data.

SE: The n ( Q ) peer only involves the most near neighbor to Q . This peer first computes locally the kNN query updating its temporary result list and only after it involves the next peer most near to Q .
MME: The n ( Q ) peer involves its neighbors that overlap the hypercube d ( O locally the kNN query updating its temporary results list and only after involves its neighbors that overlap the updated hypercube d ( O k , Q ).
 is forwarded to all peers which overlap the hypercube h q ; d  X  O nasamy, Handley, Karp, and Shenker (2001) with improvement and corrections described in Jones, Theimer,
Wang, and Wolman (2002) are used in MCAN in order to reduce the number of replicated messages. The same algorithm is also used by MME. Actually, in MME the query propagates as in the PE except for the fact that the hypercube can reduce its size during the query propagation.
 but also on their number. In practice, the algorithms MME and SE can take advantage of the partial kNN evaluation for optimizing the query by possibly reducing the number of peers which the kNN query must be forwarded to. This optimization cannot be exploited in PE and it is optimum for the SE algorithm. On the other hand while the parallelization of the kNN operation is maximum for the PE, for SE there is no par-allelization at all. The third approach (MME) represents a trade off between PE and SE.
The three algorithms also differ in terms of total number of distance computations. In fact, during the kNN distances of the actual candidate response set of the kNN. More precisely, a peer n which evaluates the kNN update the ordered list L k defined as where the object O i belong to the merged result sets of the kNN evaluated by n and by the peers which were involved before n (i.e. the peers which forwarded the operation to n ).
 any). This information is then forwarded (unmodified) to the other peers involved in the kNN query. L together with the pivots objects can be exploited by a peer in order to reduce the number of distance compu-query to their neighbors and then they proceed with the query evaluation, L trary, in the MME approach the peers can produce and forward a more accurate version of L query proceeds, with the advantage of being able to reduce both the number of peers involved and the number of distance computations with respect PE. In fact, the number of peers involved in the query and the number of distance computations both depend on the distance d ( O tation proceeds. On the other hand, in MME, the query evaluation before the forwarding reduces the degree of parallelism.

The SE algorithm takes the maximum advantage of using L k peer. For this reason, the peer that receives the forward of the kNN must know the current list of the peers involved in the SE sequence. Note that, at the end of the kNN computations performed by each peer, this list is pruned by removing the peers whose distance from Q becomes greater than the actual value of d ( O When the list is empty the operation terminates and the result is sent to the requesting peer. minimize the total computational cost. MME is somewhere in the middle.
In Figs. 1 X 3 , we sketch the algorithms of the approaches PE, MME, and SE, respectively. As can be seen in the sketches, MCAN does not make use of a coordinating peer. Any peer sends its result set to the requesting range query operation has terminated. Essentially, we maintain the list of the involved peers, which is for-warded during the query propagation. Each peer sends this information to the requester together with its results set. For this reason any involved peer must answer the requester even if its results set is empty. The application level multicast algorithm is not reported in the algorithm sketches. Summarizing, PE and MME make use of this algorithm trying to reduce multiples forwarding of the same request to the same peer. see ( Jones et al., 2002 ) for more details. 5. Performance evaluation 5.1. Datasets and measurements
For the experiments, the systems consisted of up to about 300 active peers (depending on the dataset). The
LAN communicating via the TCP and UDP protocols. Note that, we are not talking about simulation but about a real prototype.

It is important to observe that in order to evaluate the scalability performance of MCAN, in this experi-growing dataset, assuming that the greater the dataset the more are the peer employed. The objective was to demonstrate that keeping the average number of objects per peer limited as the dataset grows, the response
VEC: 45-dimensional vectors of extracted color image features. The similarity function d for comparing the and such a high-dimensional dataspace is extremely sparse.

TTL: titles and subtitles of Czech books and periodicals collected from several academic libraries. These strings are of lengths from 3 to 200 characters and are compared by the edit distance ( Levenshtein, 1965 ) on the level of individual characters. The distance distribution of this dataset is skewed.
DNA: protein symbol sequences of length 16. The sequences are compared by a weighted edit distance according to the Needleman X  X unsch algorithm ( Needleman &amp; Wunsch, 1970 ). This distance function has a very limited domain of possible values  X  the returned values are integers between 0 and 100.
Observe that none of these datasets can be efficiently indexed and searched by a standard vector data struc-ture. In the reported experiments we used three pivots for building MCAN structure (i.e. MCAN
N = 3 is an optimal solution. For more details about how the number of pivots affects the performance of the filtering please refer to ( Bustos et al., 2001 ).

All the presented performance characteristics of query processing have been taken as an average over 100 queries by randomly choosing query objects not belonging to the dataset.
 tures is in general to reduce the number of distance computations at query time. The number of distance com-other operations are negligible compared to the distance evaluation time.

Concerning the distributed environment, we use the following two characteristics to measure the computa-tional costs of a query: total distance computations  X  the sum of the number of the distance computations on all employed peers, parallel distance computations  X  the maximal number of distance computations performed in a sequential manner during the parallel query processing.

Another indicator that we monitored is the percentage of peers (with respect to the total number) that were involved by the query processing and the number of candidate results .

In order to better interpret the performance figures of the three kNN algorithms presented above, we com-as RQ. RQ works as follows: once we have obtained the results set of the kNN (evaluated using one of the three algorithms), we run a range query with radius equal to d ( Q , O be considered as lower bounds (optimal) for the other kNN algorithms. 5.2. Number of peers involved in query execution
Fig. 5 shows the average percentage of involved peers during the evaluation of kNN for increasing values of k and for the entire dataset. The performance figures as function of k are only reported for the VEC dataset (1 million objects and 260 peers), since the results of the other two datasets are very similar.
From these experiments we can see that SE is not only the best algorithm in terms of number of involved more peers and MME is not far from PE. Note also that, the number of peers grows almost linearly with k . peers involved, the more is the relevance of intermediate results updated during the forwarding of the operation.

Figs. 6 X 8 show the average percentage of involved peers as the dataset grows. For all algorithms, the per-number of peers increases, the average volume of zones maintained by the peers decreases.
Regarding the differences between the three datasets results, we can see that DNA is a much more difficult dataset to index then VEC. The most important reason is that the DNA metric function has a very limited number of discrete distance values. The TTL dataset is in the middle but not far from the DNA dataset.
In fact, the TTL and DNA distance functions are not much different even if the object are, in their meaning, completely different.
 5.3. Total number of distance computations
In Fig. 9 we report the total number of distance computations during the kNN operation for the entire dataset and various k . Even though not optimum, SE is very near to the results obtained with RQ. In fact, ation which we will study more in details later on. Regarding MME, we can see that performing kNN com-putations before forwarding the query significantly reduces the number of total distance computations.
Figs. 10 X 12 show the total distance computations as the dataset grows. Note that, in all the algorithms the total distance computations grows when we increase the dataset size. However, the most important property which is achieved through parallelism.
 5.4. Parallel cost of kNN performance figure can be considered as the parallel cost of the operation. From this experiment, it becomes operation is completed. Regarding SE and MME, the order in which the peers are visited guarantees that good results will be available to the requester before the end of the operation. However, in the PE algorithm the results are supposing to arrive sooner and almost at the same time because of the parallelism (except for n  X  Q peer), although the use of preliminarily results by the requester can make MME and SE more appealing in some scenarios.
 5.5. Candidate results
In all the algorithms, as the kNN evaluation proceeds, the peers send the partial results of their local kNN tial results candidate results .
 number of candidate results for the three algorithms as the dataset grows. The SE algorithm is near the and it does not scale well (except for the DNA and TTL datasets). On the contrary, MME is between the other two algorithms and its behavior seems growing sub-linearly.

Notice that, generally, the metric distance evaluation in the metric space is very expensive and the cost of experiments we just wanted to show that there could be problems of scalability in terms of candidate results for the PE algorithm. 6. Conclusions and future work
Many P2P applications need processing complex data for which there is no ordering and only pair-wise data and queries can be processed with a single index structure. We have concentrated on a much needed and probably the most complex form of queries, that is the nearest neighbors queries.

We have proposed three strategies for such query execution, using the MCAN similarity search structure, originally developed for processing range queries. Extensive performance evaluation on real-life data pro-cessed by our experimental system reveals the following pros and cons of individual approaches.
The experiments revealed that the SE approach is not suitable for scenarios where scalability of the computation much more bounded. Because of the fact that P2P architectures are generally used to solve the problem of scalability we think SE is, in most of the case, unusable.
 In terms of total computational cost ( Figs. 10 X 12 ) MME performs better than PE, even if not so well as the SE. Consuming less resource MME should leave more space for parallelism between independent operations.
To decide which is the best choice between MME and PE, we can also take into account the number of can-didate results. As experiments of Fig. 17 show, MME scale better than PE.

From these observations, we think that the MME approach is the best choice in general. It responds well to the demand of the scalability in terms of response time and consumes less resources than PE.
Our future work will concentrate on inter-query parallelism and approximate range and nearest neighbors algorithms, which would trade some imprecision in search results with additional improvement of performance.
 References
 1. Introduction
Traditionally, search has been applied to structured (attribute-type) data yielding records that exactly ries involving complex data types such as images, videos, time series, text and DNA sequences. Similarity forms a mathematical metric space . The obvious advantage of similarity search is that the results can be for the expected dimension of the problem.
Peer-to-Peer (P2P) architectures seem to solve the problem of scalability, and several scalable and distrib-2 for a survey). They mostly concentrate on the similarity range queries where the execution algorithms sat-ment. Since no bottleneck occurs, the structures are scalable and high performance is achieved through parallel query execution on individual peers (computer nodes).
 neighbors queries. For example, given an image, it is easier to ask for 10 most similar ones according to an
However, nearest neighbors algorithms are much more difficult to implement in P2P environments. The main tial processing are completely in contradiction with decentralization and parallelism objectives of any P2P search network.

Capitalizing on our previous work of similarity range search through MCAN ( Falchi, Gennaro, Zezula, rithms. We first summarize the necessary background in Section 2 , including the related work. Then in Sec-tion 3 we define the main properties of the MCAN. Section 4 describes alternative strategies for the nearest
Section 6 . 2. Background
The most fundamental research results to our proposal are the Content-Addressable Network (CAN) we provide the necessary background and survey relevant literature. 2.1. Content-Addressable Network (CAN)
The CAN is a distributed hash table that uses a function for mapping  X  X  X eys X  X  onto  X  X  X alues X  X  defining posi-we can define the mapping function G of the CAN as follows: where R N is an hyper-rectangular region of R N .
 The principle of the CAN is to divide the hyper-rectangular region R angular zones, each of them associated with exactly one peer of the network. The peers are responsible for adjacent zones, i.e., its neighbors.

Given a  X  X  X ey X  X , the lookup function returns coordinates of the zone into which the key belongs. This is useful for insertion, deletion, and retrieval purposes. The search starts from an arbitrary peer of the CAN structure and proceeds by routing a message towards its destination by using a simple greedy forwarding to the neighbor with coordinates closest to the destination zone. In general, if we divide the R in h zones, each peer maintains 2N neighbors. Furthermore, the average routing path length is given by the fact that there is always a value V associated with it. 2.2. Metric spaces
The mathematical metric space is a pair M X  X D ; d  X  , where D is the domain of objects and d is the distance function d : DD! R able to compute distances between any pair of objects from D . It is typically assumed
X ; Y ; Z 2D , the distance must satisfy the following properties: which have a distance from the query object Q 2D at most the specified threshold (range or radius) r : object Q . We can extend this type of query to return k nearest objects that form a set KF such that jKj X  k and queries, that is the nearest neighbors queries. 2.3. Pivot mapping and filtering
In this section, we discuss general pivoting strategies proposed in MCAN for mapping metric space objects into vectors and for filtering undesirable objects during search operations. 2.3.1. Mapping In general, the pivot-based algorithms can be viewed as a mapping F from the original metric space
M X  X D ; d  X  to an N -dimensional vector space. The mapping assumes a set f P as: new space can be evaluated as:
Since the triangle inequality for d in the original metric space M holds we have For any pair of objects, the distance in the derived space M metric space M . In this way, the mapping is contractive , i.e., d tance d in M .
 2.3.2. Filtering
At the search time, we compute for a query object Q the query feature vector F  X  Q  X  X  d ( X , Q ) for objects X that satisfy
During the evaluation of k nearest neighbors (kNN) with the temporary result X evaluation of d ( Q , X )if
In other words, the object X can be discarded if there exists a pivot P 2.4. Related work
Many metric-based indexing principles and index structures have been proposed, focusing on the pruning of search space at query time, and several comprehensive surveys describe individual approaches ( Cha  X  vez static or main memory structures and, therefore, not very suitable for large volumes of data. ilarity search becomes too expensive when the stored data volume grows, because the search costs increase a distributed processing.

Restricting to multi-dimensional range and kNN queries in vector spaces, several recent works have pro-posed distributed structures for such tasks. Unfortunately, these structures are often designed for specific applications (for example spatial data) typically using vectors of low dimensionality. The vector space approach cannot be applied on many important datasets where similarities are measured by functions such as the Hausdorff distance, Jaccard X  X  coefficient, edit distance, etc.

The MAAN structure ( Cai, Frank, Chen, &amp; Szekely, 2003 ) extends the Chord protocol to support multi-attribute and range queries by means of uniform locality preserving hashing. This system expects the exact knowledge of the attribute domain distributions. Moreover, no experiments on real-life datasets are provided larity queries accept for the range search.

Ganesan, Yang, and Garcia-Molina (2004) show in the SCRAP structure a way to adapt kd-trees, which support multi-dimensional range queries, by exploiting the Chord protocol. Unfortunately, this approach becomes inefficient for more than two dimensions. In the same paper, the authors propose the MURK struc-ture that uses the space-filling curves together with CAN (improved by skip pointers). Although no real-life and cannot be applied to data that is metric by its nature.
 The Mercury ( Bharambe et al., 2004 ) provides a protocol for routing multi-attribute range-based queries. general and slightly different from that addressed by this paper.
 locality and load balancing. These features give the opportunity to support range queries on one dimension.
These approaches do not aim on more complex similarity queries.
The pSearch approach ( Tang, Xu, &amp; Dwarkadas, 2003 ) uses two traditional information retrieval algo-tem based on the CAN routing protocol. This concept, defining similarity between a document and a query by means of their common terms, is only suitable for the text retrieval.

Tanin, Nayar, and Samet (2005) introduce a P2P generalization of a quadtree index. The authors propose this structure is limited to the spatial domain .
 Banaei-Kashani and Shahabi (2004) formalize the problem of vector-based similarity search in P2P Data
Networks and propose the SWAM-a family of small-world based access methods. This concept provides a general solution for the range and nearest neighbors search in the vector-based datasets.
Four different distributed structures have been proposed for indexing and similarity search in the metric form the metric search issue into a different problem and take advantage of some well-known solutions. Assuming similarity range queries, the pros and cons of these four structures are deeply analyzed in Batko,
Falchi, Nova  X  k, and Zezula, 2006 on several real-life datasets. 3. Principles of MCAN
The basic idea of our approach is to extend the CAN architecture so that it can manage objects F of a generic metric space M X  X D ; d  X  . However, in metric spaces it is not possible to exploit any knowledge of coordinate information, and only distances between objects can be computed. To cope with this problem, we use the pivots paradigm for mapping the objects of the metric space to an N dimensional vector space. of the function F  X  X  (introduced in the Section 2.2 ) defined by Eq. (2) .

This virtual coordinate space is used to store the object X in the MCAN structure, specifically in the peer that owns the zone in which the point F(X) belongs. Note that, the coordinate space of the MCAN is Carte-sian but the distance between two objects is evaluated by means of the d Euclidean distance. Routing in MCAN works in the same manner as for the original CAN structure. An
MCAN peer maintains a coordinate routing table that holds the IP address and virtual coordinate zones of each of its immediate neighbors in the coordinate space. 3.1. Notation
In this section, we provide necessary definitions needed for a clear presentation of our results. We use the vector in the coordinate space x 2 R N ( x  X  F  X  X  X  ), and x we have already explained, the mapping in MCAN is contractive, therefore d
We denote a peer of MCAN by the bold symbol n . Each peer n maintains its region information referred as the origin, denoted as n : R : v  X  X  n : R : v 1 ; n : R : v n : R : l 1 ; n : R : l 2 ; ... ; n : R : l N . More precisely, the region n . R is defined as follows:
The peer n also maintains the set of the neighbor peers X  information n : M f n Given an object Q 2D and a range r , we define h q ; r i as the hypercube in R 2 r .
 We can now introduce the formal definition of an N -dimensional MCAN structure, referred as MCAN which is composed of a set of h ( h &gt; 0) network peers f n (1) 8 i ; j j i 6  X  j n i : R \ n j : R  X ; (2) (3) n i 2 n j : M () 9 k j X  n i : R : v k  X  n i : R : l k that the union of the zones cover the whole MCAN N space R the condition for a network peer n i to be a neighbor of n 3.2. Construction An important feature of the CAN structure is its capability to dynamically adapt to data-set size changes.
As we will see in the experimental evaluation, we are interested in preserving the scalability of the MCAN, which means that we want to maintain a stable the response time of query execution. Since the number of objects each peer can maintain, we also limit (reduce) the number of distance computations a peer have to compute during a query evaluation.
 It is important to observe that in some cases we might want to use all the peers available in the network. ing a peer to split even if it does not exceed its storage capacity. Obviously, such methodology can also be this is possible with a CAN, which even provides some fault-tolerance capabilities ( Saia, Fiat, Gribble, Karlin, &amp; Saroiu, 2002 ).
 tion algorithm described in Bustos et al., 2001 . This algorithm tries to maximize the average distance d between two arbitrary objects in the derived space (i.e., d 3.3. Insertion
An insert operation can be initiated in any peer of the MCAN . It starts by mapping the inserted object X to peer forwards the insertion operation to the neighbor peer which is closer to the point x by using the d stored there, otherwise a neighbor peer is selected with the same technique and the insert operation is forwarded again until the object X is inserted.
 3.4. Split
In MCAN, we apply a balanced split, i.e., the resulting regions contain the same number of objects. During metric objects is then reallocated there.

If we define n 1 as the splitting peer, n 1 : R as the old region, n split regions must satisfy the following three equations:
Moreover, to respect these constrains, we create the new two regions by dividing the original one along one coordinate of the space. Therefore, the new regions, n 1 : R
Note that that we only have to choose s and n 1 : R 0 : l that divide the objects into two halves. Moreover, we choose to split along the dimension that maximizes the length of the shortest side.

After the splitting process, the peer n 1 sends a message to all its neighbors n n . The new peer is informed by n 1 about its neighbors n 2 card information about the peers that are not more its neighbors. 4. Searching for nearest neighbors
Whenever we want to search for similar objects using the range search, we must specify the maximal dis-less than four edit operations between the two strings, which has a clear semantic meaning. However, a dis-sive, and the result sets might contain many not significant objects.

An alternative way to search for similar objects is to use the nearest neighbors queries. Such queries guar-object Q . Though the problem of executing k nearest neighbors queries is not new and many algorithms have been proposed in the literature, see for example Hjaltason and Samet (1999) for many references and addi-tional readings, the distributed kNN query processing have not been systematically studied. 4.1. kNN Search in MCAN In MCAN, we have developed three different strategies to perform kNN queries: Parallel Execution (PE),
Sequential Execution (SE), and Mixed Mode Execution (MME). Each of these techniques has its advantages and disadvantages which will be discussed later.
 the algorithms could be easily modified by forwarding the kNN request to the most promising peer until the tances to Q , are the candidates for the kNN result. However, there may be other objects in different peers X  regions that are closer to the query than some of those k candidates. Nevertheless, since the MCAN space if there are other peers involved in the query, l  X  q  X  controls if the hypercube h q ; d  X  X search correctly terminates and the k objects retrieved by l  X  q  X  represent the result of the kNN query. agate the kNN query execution among the involved peers. The generic behavior of these three approaches can be characterized as follows:
MME The l  X  q  X  peer involves its neighbors that overlap the hypercube h q ; d  X  X forwarded to all peers which overlap the hypercube h q ; d  X  X nasamy, Handley, Karp, and Shenker (2001b) with improvement and corrections described in Jones, Theimer,
Wang, Wolman, and December (2002) are used in MCAN in order to reduce the number of replicated mes-sages. The same algorithm is also used by MME. Actually, in MME the query propagates as in the PE except for the fact that the hypercube can reduce its size during the query propagation.
 advantage of the partial kNN evaluation for optimizing the query by possibly reducing the number of peers which the kNN query must be forwarded to. This optimization cannot be exploited in PE and it is optimum for the SE algorithm. On the other hand, while the parallelization of the kNN operation is maximum for the SE strategies.

The three algorithms also differ in terms of the total number of distance computations. In fact, during the the kNN updates the ordered list L k defined as: where the object X i belongs to the merged result set of the kNN query evaluated both by n and by a certain number of peers (it depends on the algorithm) which have been involved.
 any). This information is then forwarded (unmodified) to the other peers involved in the kNN query. L together with the pivot objects can be exploited by a peer in order to reduce the number of distance compu-tations during the kNN evaluation exploiting Eq. (8) . It is clear that in PE approach L
On the contrary, in the MME approach the peers can produce and forward a more accurate version of L with the advantage of being able to reduce both the number of peers involved and the number of distance computations with respect to PE. In fact, the number of peers involved in the query and the number of distance computations both depend on the distance d  X  X k putation proceeds. Finally, the query evaluation before the forwarding in MME reduces the degree of parallelism.

The SE algorithm takes the maximum advantage of information stored in L whose distances (of their regions) from Q are less than or equal to d  X  X viously involved peers. This is necessary since the neighbor of a peer that is involved in the kNN needs not be a neighbor of the next peer involved in the SE sequence. Note that at the end of the kNN computa-tions (performed by each peer) this list is pruned by removing the peers whose distances from Q become is sent to the requesting peer.
In general, we can consider two aspects of the kNN operation costs: its parallelism, which is necessary for tries to minimize the total computational costs. MME is somewhere in the middle.

In Figs. 1 X 3 , we sketch the algorithms of the approaches PE, MME, and SE, respectively. As can be seen from the involved peers. Note that, in the algorithms we assume the distances between the results and the searchkNN _ local used by the previous algorithms. In Falchi et al. (2005) , we described how the requesting peers, which is forwarded during the query propagation. Each peer sends this information to the requester set is empty.
 The application level multicast algorithm is not reported in the algorithm sketches. Summarizing, PE and MME make use of this algorithm trying to reduce multiple forwarding of the same request to the same peer.
Jones et al. (2002) for more details. 5. Performance evaluation 5.1. Datasets and measurements
For the experiments, the systems consisted of up to about 300 active peers (depending on the dataset). The
LAN communicating via the TCP and UDP protocols. Note that, we are not talking about simulation but about a real prototype.

It is important to observe that in order to evaluate the scalability performance of MCAN, in this experi-
P2P scenario; however, this approach was adopted just to study the scalability of MCAN with respect to a the system remains bounded, i.e., the structure scales well. These experiments were performed by inserting objects in the network in a random order, resulting in a growing number of peers due to the splitting rule described in Section 2.3 .

VEC 45-dimensional vectors of extracted color image features. The similarity function d for comparing the
TTL titles and subtitles of Czech books and periodicals collected from several academic libraries. These
DNA protein symbol sequences of length sixteen. The sequences are compared by a weighted edit distance
Observe that none of these datasets can be efficiently indexed and searched by a standard vector data struc-ture. In the reported experiments, we used three pivots for building MCAN structure (i.e., MCAN found that N = 3 is an optimal solution. For more details about how the number of pivots affects the perfor-mance of the filtering please refer to Bustos et al. (2001) .

All the presented performance characteristics of query processing have been taken as an average over 100 queries by randomly choosing query objects not belonging to the dataset.
 tures is to reduce the number of distance computations at query time. The number of distance computations is ations are negligible compared to the distance evaluation time.

Concerning the distributed environment, we use the following two characteristics to measure the computa-tional costs of a query: Total distance computations  X  the sum of the number of distance computations on all employed peers;
Parallel distance computations  X  the maximal number of distance computations performed in a sequential manner during the parallel query processing.

Another indicator that we monitored is the percentage of peers (with respect to the total number) that were involved by the query processing and the number of candidate results .

In order to better interpret the performance figures of the three kNN algorithms presented above, we com-the three algorithms), we run a range query with radius d  X  X sidered as the lower bounds (optimal) for the other kNN algorithms. 5.2. Number of peers involved in query execution
Fig. 5 shows the average percentage of involved peers during the evaluation of kNN for increasing values of k and for the entire dataset. The performance figures as function of k are only reported for the VEC dataset (1 million objects and 260 peers), since the results of the other two datasets are very similar.
From these experiments, we can see that SE is not only the best algorithm in terms of number of involved more peers and MME is not far from PE. Note also that, the number of peers grows almost linearly with k . peers involved, the more is the relevance of intermediate results updated during the forwarding of the operation.

Figs. 6 X 8 show the average percentage of involved peers as the dataset grows. For all algorithms, the per-number of peers increases, the average volume of zones maintained by the peers decreases.
Regarding the differences between the three datasets X  results, we can see that the DNA dataset is much more difficult to index than the VEC one. The most important reason is that the DNA metric function has a very limited number of discrete distance values. The TTL dataset is in the middle but not far from the DNA data-ing, completely different. 5.3. Total number of distance computations
In Fig. 9 , we report the total number of distance computations during the kNN operation for the entire dataset and various k . Even though not optimum, SE is very near to the results obtained with RQ. In fact, ation which we will study more in details later on. Regarding MME, we can see that performing kNN com-putations before forwarding the query significantly reduces the total number of distance computations.
Figs. 10 X 12 show the total number of distance computations as the dataset size grows. Note that, in all the algorithms the total number of distance computations grows when we increase the dataset size. However, the most important property that we should expect from a P2P data structure such as the MCAN is its scalability of the search operations, which is achieved through parallelism. 5.4. Parallel cost of kNN performance figure can be considered as the parallel cost of the operation. From this experiment, it becomes from Figs. 14 X 16 we can see that the parallel cost of SE grows with the dataset size, which means that the algorithm does not scale well. On the contrary, PE scales well and it is very near to the optimum RQ. Note corresponding range radius d  X  X k ; Q  X  becomes smaller. Finally, MME, which gave better results than PE in terms of the number of involved peers and total number of distance computations, does not scale as well as the PE, but it is not very far from it.
 operation is completed. Regarding SE and MME, the order in which the peers are visited guarantees that good results will be available to the requester before the end of the operation. However, in the PE algorithm the results are supposing to arrive sooner and almost at the same time because of the parallelism (except for ing in some scenarios. 5.5. Candidate results
In all the algorithms, as the kNN evaluation proceeds, the peers send the partial results of their local kNN evaluation to the requesting peer l  X  q  X  (which is the peer which started the kNN operation). We call these partial results candidate results .
 the number of candidate results received for the three algorithms as the dataset grows. The SE algorithm is worst and it does not scale well (except for the DNA and TTL datasets). On the contrary, MME is between the other two algorithms and its behavior seems growing sub-linearly.

Notice that, generally, the metric distance evaluation in the metric space is very expensive and the cost of experiments we just wanted to show that there could be problems of scalability in terms of candidate results for the PE algorithm. 6. Conclusions and future work
Many P2P applications need processing complex data for which there is no ordering and only pair-wise and the function form the metric space, so our approach offers a high extensibility-many different forms of data and queries can be processed with a single index structure. We have concentrated on a much needed and probably the most complex form of queries: the nearest neighbors queries.

We have proposed three strategies for such query execution, using the MCAN similarity search structure, originally developed for processing range queries. Extensive performance evaluation on real-life data pro-cessed by our experimental system reveals the following pros and cons of individual approaches.
The experiments revealed that the SE approach is not suitable for scenarios where scalability of the computation much more bounded. Because of the fact that P2P architectures are generally used to solve the problem of scalability we think SE is, in most of the case, impracticable.
 In terms of total computational cost ( Figs. 10 X 12 ) MME performs better than PE, even if not so well as the SE. Consuming less resources MME should leave more space for parallelism between independent operations.
To decide which is the best choice between MME and PE, we can also take into account the number of can-didate results. As experiments of Fig. 17 show, MME scale better than PE.

From these observations, we think that the MME approach is the best choice in general. It responds well to the demand of the scalability in terms of response time and consumes less resources than PE.
An interesting direction of investigation is to generalize the kNN algorithms by parameterizing their behav-involves its neighbors whose regions overlaps h q ; a d  X  X because all the neighbors are involved before the local execution of the kNN.

Our future work will concentrate on inter-query parallelism and approximate range and nearest neighbors algorithms, which would trade some imprecision in search results with additional improvement of performance.
 References
