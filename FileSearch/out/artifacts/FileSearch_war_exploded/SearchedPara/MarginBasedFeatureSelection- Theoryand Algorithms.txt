 Ran Gilad-Bac hrach y ranb@cs.huji.a c.il Amir Navot z ana vot@cs.huji.a c.il Naftali Tishby y ; z tishby@cs.huji.a c.il The Hebrew Univ ersity, Jerusalem, Israel In many supervised learning tasks the input data is represen ted by a very large number of features, but only few of them are relev ant for predicting the label. Even state-of-art classi X cation algorithms (e.g. SVM (Cortes &amp; Vapnik, 1995)) cannot overcome the pres-ence of large number of weakly relev ant and redundan t features. This is usually attributed to \the curse of dimensionalit y" (Bellman, 1961), or to the fact that irrelev ant features decrease the signal-to-noise ratio. In addition, many algorithms become computationally intractable when the dimension is high. On the other hand once a good small set of features has been chosen even the most basic classi X ers (e.g. 1-Nearest Neigh-bor (Fix &amp; Hodges, 1951)) can achieve high perfor-mance levels. Therefore feature selection , i.e. the task of choosing a small subset of features which is su X cien t to predict the target labels well, is crucial for e X cien t learning.
 Feature selection is closely related to the more gen-eral problems of dimensionalit y reduction and e X cien t data represen tation. Many dimensionalit y reduction metho ds, like Princip al Comp onent Analysis (Jollif-fee, 1986) or Locally Linear Embedding (Roweis &amp; Saul, 2000), are in fact unsup ervised feature extrac-tion algorithms, where the obtained lower dimensions are not necessarily subsets of the original coordinates. Other metho ds, more related to supervised feature extraction, are the Information Bottlene ck (Tish by et al., 1999) and Su X cient Dimensionality Reduction (Glob erson &amp; Tishby, 2003). However, on many cases, feature selection algorithms provide a much simpler approac h as they do not require the evaluation of new complex functions of the irrelev ant features. Roughly speaking, supervised feature selection meth-ods are applied in one of two conceptual framew orks: the  X lter model and the wrapper model (Koha vi &amp; John, 1997). In the wrapp er model the selection metho d tries to directly optimize the performance of a speci X c predictor (algorithm). This may be done by estimating the predictor generalization performance (e.g. by cross validation) for the selected feature set in each step. The main drawbac k of this metho d is its computational de X ciency .
 In the  X lter model the selection is done as a prepro-cessing , without trying to optimize the performance of any speci X c predictor directly . This is usually achieved through an (ad-ho c) evaluation function using a searc h metho d in order to select a set that maximizes this function. Performing an exhaustiv e searc h is usually intractable due to the large number of initial features. Di X eren t metho ds apply a variety of searc h heuristics, such as hill climbing and genetic algorithms. One com-monly used evaluation function is the mutual informa-tion between the feature set and the labels (Quinlan, 1990). See (Guy on &amp; Elissee X , 2003) for a comprehen-sive discussion of feature selection metho dologies. In this paper we introduce the idea of measuring the qualit y of a set of features by the margin it induces. A margin (Cortes &amp; Vapnik, 1995; Schapire et al., 1998) is a geometric measure for evaluating the con X dence of a classi X er with respect to its decision. Margins already play a crucial role in curren t machine learning researc h. The novelty of this paper is the use of large margin principle for feature selection 1 .
 Throughout this paper we will use the 1-NN as the \study-case" predictor, but most of the results are rel-evant to other distance based classi X ers (e.g. LVQ (Ko-honen, 1995), SVM-RBF (Cortes &amp; Vapnik, 1995)) as well. The margin for these kind of classi X ers was pre-viously de X ned in (Crammer et al., 2002). The use of margins allows us to devise new feature selection al-gorithms as well as prove a PAC style generalization bound. The bound is on the generalization accuracy of 1-NN on a selected set of features, and guaran tees good performance for any feature selection scheme which se-lects small set of features while keeping the margin large. On the algorithmic side, we use a margin based criteria to measure the qualit y of sets of features. We presen t two new feature selection algorithms, a Greedy Feature Flip ( G- X  X p ) and an Iterativ e Searc h Margin Based Algorithm which we call Simb a , based on this criteria. The merits of these algorithms is demon-strated on a synthetic data and a face classi X cation task.
 The paper is organized as follows: Section 2 discusses margins in machine learning and presen ts our new margin based criterion for feature selection. In sec-tion 3 we presen t two new feature selection algorithms G- X  X p and Simb a and compare them to the Relief al-gorithm. A theoretical generalization analysis is pre-sented in section 4. Empirical evidence on the per-formance of these algorithms is provided in section 5, followed by concluding discussion in section 6. Margins play a crucial role in modern machine learn-ing researc h. They measure the classi X er con X dence when making its decision. Margins are used both for theoretic generalization bounds and as guidelines for algorithm design. 2.1. Two types of Margins As describ ed in (Crammer et al., 2002) there are two natural ways of de X ning the margin of an instance with respect to a classi X cation rule. The more common type, sample-mar gin , measures the distance between the instance and the decision boundary induced by the classi X er. Supp ort Vector Machines (Cortes &amp; Vapnik, 1995), for example,  X nds the separating hyper-plane with the largest sample-margin. Bartlett (1998), also discusses the distance between instances and the deci-sion boundary . He uses the sample-mar gin to deriv e generalization bounds.
 An alternativ e de X nition, the hypothesis-mar gin , re-quires the existence of a distance measure on the hy-pothesis class. The margin of an hypothesis with re-spect to an instance is the distance between the hy-pothesis and the closest hypothesis that assigns al-ternativ e label to the given instance. For example AdaBo ost (Freund &amp; Schapire, 1997) uses this type of margin with the L 1 -norm as the distance measure among hypotheses.
 Throughout this paper we will be interested in margins for 1-NN. For this special case, (Crammer et al., 2002) proved the following two results: 1. The hypothesis-margin lower bounds the sample-2. It is easy to compute the hypothesis-margin of an Therefore in the case of Nearest Neigh bor large hypothesis-margin ensures large sample-margin, and hypothesis-margin is easy to compute. 2.2. Margin Based Evaluation Function A good generalization can be guaran teed if many sam-ple points have large margin (see section 4). We in-troduce an evaluation function which assigns a score to sets of features according to the margin they in-duce. First we form ulate the margin as a function of the selected set of features.
 De X nition 1 Let P be a set of points and x be an instanc e. Let w be a weight vector over the feature set, then the margin of x is  X  where k z k w = p P i w 2 i z 2 i .
 De X nition 1 extends beyond feature selection and al-lows weight over the features. When selecting a set of features F we can use the same de X nition by identi-fying F with its indicating vector. Therefore, we use feature in F and zero otherwise.
 to introduce some normalization factor. The natural normalization is to require max w 2 i = 1, since it guar-anties that k z k w  X  k z k where the right hand side is the Euclidean norm of z .
 Now we turn to de X ne the evaluation function. The building blocks of this function are the margins of all the sample points. The margin of each instance x is calculated with respect to the sample excluding x (\lea ve-one-out margin").
 De X nition 2 Given a training set S and a weight vec-tor w , the evaluation function is: It is natural to look at the evaluation function only for weight vectors w such that max w 2 i = 1. However, for-mally , the evaluation function is well de X ned for any w and ful X lls e (  X  X  ) = j  X  j e ( w ), a fact which we make use of in the Simb a algorithm. We also use the notation e ( F ), where F is a set of features to denote e ( I F ). In this section we presen t two algorithms which at-tempts to maximize the margin based evaluation func-tion. Both algorithms can cope with multi-class prob-lems 2 . 3.1. Greedy Feature Flip Algorithm (G- X  X p) G- X  X p (algorithm 1) is a greedy searc h algorithm for maximizing e ( F ), where F is a set of features. The al-gorithm repeatedly iterates over the feature set and updates the set of chosen features. In each itera-tion it decides to remo ve or add the curren t feature Algorithm 1 Greedy Feature Flip (G- X  X p) 1. Initialize the set of chosen features to the empt y 2. for t = 1 ; 2 ; : : : to the selected set by evaluating the margin term (2) with and without this feature. This algorithm is simi-lar to the zero-temp erature Monte-Carlo (Metrop olis) metho d. It converges to a local maxim um of the eval-uation function, as each step increases its value and the number of possible feature sets is  X nite. The com-putational complexit y of one pass over all features of G- X  X p is  X   X  N 2 m 2  X  where N is the number of features and m is the number of instances. Empirically G- X  X p converges in a few iterations. In all our experimen ts it converged after less than 20 epochs, in most of the cases in less than 10 epochs. A nice prop erty of this algorithm is that it is parameter free . There is no need to tune the number of features or any type of thresh-old. 3.2. Iterativ e Searc h Margin Based Algorithm The G- X  X p algorithm presen ted in section 3.1 tries to  X nd the feature set that maximizes the margin di-rectly . Here we take another approac h. We  X rst  X nd the weight vector w that maximizes e ( w ) as de X ned in (2) and then use a threshold in order to get a feature set. Of course, it is also possible to use the weights di-rectly by using the induced distance measure instead. Since e ( w ) is smooth almost everywhere, we use gra-dient ascen t in order to maximize it. The gradien t of e ( w ) when evaluated on a sample S is: ( 5 e ( w )) i = Algorithm 2 Simba 1. initialize w = (1 ; 1 ; : : : ; 1) 2. for t = 1 : : : T 3. w  X  w 2 =  X   X  w 2  X   X  In Simb a (algorithm 2) we use a stochastic gradien t as-cent over e ( w ) while ignoring the constrain t k w 2 k 1 1, the projection on the constrain t is done only at the end (step 3). This is sound since e (  X  X  ) = j  X  j e ( w ). In each iteration we evaluate only one term in the sum in (3) and add it to the weight vector w . Note that the term  X  evaluated in step 2(c) is invarian t to scalar scaling of w (i.e.  X ( w ) =  X (  X  X  ) for any  X  &gt; 0). Therefore, since k w k increases, the relativ e e X ect of the correction term  X  decreases and the algorithm typ-ically convergence.
 The computational complexit y of Simb a is  X ( T N m ) where T is the number of iterations, N is the number of features and m is the size of the sample S . Note that when iterating over all training instances, i.e. when T = m , the complexit y is  X   X  N m 2  X  which is better than G- X  X p by a factor of N . 3.3. Comparison to Relief Relief (Kira &amp; Rendell, 1992) is a feature selection al-gorithm (see algorithm 3), which was shown to be very e X cien t for estimating features qualit y. The algorithm holds a weight vector over all features and updates this vector according to the sample points presen ted. Kira &amp; Rendell (1992) proved that under some assumptions, the expected weight is large for relev ant features and small for irrelev ant ones. They also explain how to choose the relev ance threshold  X  in a way that en-sures the probabilit y that a given irrelev ant feature will be chosen is small. Relief was extended to deal with multi-class problems, noise and missing data by Kononenk o (1994).
 Algorithm 3 RELIEF (Kira &amp; Rendell, 1992) 1. initiate the weights vector to zero: w = 0 2. for t = 1 : : : T , 3. the chosen feature set is f i j w i &gt;  X  g where  X  is a Note that the update rule in a single step of Relief is similar to the one performed by Simb a . Indeed, em-pirical evidence shows that Relief does increase the margin (see section 5). However, there is a major di X erence: Relief does not re-ev aluate the distances according to the weight vector w and thus it is infe-rior to Simb a . In particular, Relief has no mechanism for eliminating redundan t features. Simb a may also choose correlated features, but only if this contributes to the overall performance. In terms of computational complexit y, Relief and Simb a are equiv alent. In this section we use feature selection and large mar-gin principals to prove  X nite sample generalization bound for 1-Nearest Neighb or . (Cover &amp; Hart, 1967), showed that asymptotically the generalization error of 1-NN can exceed by at most a factor of 2 the gener-alization error of the Bayes optimal classi X cation rule. However, on  X nite samples nearest neigh bor can over- X t and exhibit poor performance. Indeed 1-NN will give zero training error, on almost any sample. The training error is thus too rough to provide in-formation on the generalization performance of 1-NN. We therefore need a more detailed measure in order to provide meaningful generalization bounds and this is where margins become useful. It turns out that in a sense, 1-NN is a maxim um margin algorithm. In-deed once our prop er de X nition of margin is used, i.e. sample-margin, it is easy to verify that 1-NN generates the classi X cation rule with the largest possible margin. The combination of a large margin and a small number of features provides enough evidence to obtain a use-ful bound on the generalization error. The bound we provide here is data-dep enden t (Shawe-Taylor et al., 1998; Bartlett, 1998). Therefore, the qualit y of the bound depends on our speci X c sample. It holds simul-taneously for any possible metho d to select a set of features. If an algorithm selects a small set of features with large margin, the bound guaran tees it generalizes well. This is the motiv ation for Simb a and G- X  X p . We use the following notation: De X nition 3 Let D be a distribution over X  X f X  1 g and h : X  X ! f X  1 g a classi X c ation function. We denote by er D ( h ) the gener alization error of h with respect to D : For a sample S = f ( x k ; y k ) g m k =1 2 ( X  X f X  1 g ) constant  X  &gt; 0 we de X ne the  X  -sensitive training error to be Our main result is the following theorem 3 : Theorem 1 Let D be a distribution over R N  X f X  1 g which is supported on a ball of radius R in R N . Let  X  &gt; 0 and let S be a sample of size m such that S  X  D m . With probability 1  X   X  over the random choic e of S , for any set of features F and any  X  2 (0 ; 1] Wher e h is the nearest neighb or classi X c ation rule when distanc e is measured only on the features in F and d = (64 R= X  ) j F j .
 The size of the feature space, N , appears only logarith-mically in the bound. Hence, it has a minor e X ect on the generalization error of 1-NN. On the other hand, the number of selected features, F , appears in the ex-ponen t. This is another realization of the \curse of dimensionalit y" (Bellman, 1961). See appendix A for the proof of theorem 1. We  X rst demonstrate the behavior of Simb a on a small synthetic problem. Then we test it on a task of pixel (feature) selection for discriminating between male and female face images. For the G- X  X p algorithm, we report the results obtained on some of the datasets of the NIPS-2003 feature selection challenge (Guy on &amp; Gunn, 2003). 5.1. The Xor Problem To demonstrate the qualit y of the margin based eval-uation function and the abilit y of Simb a algorithm to deal with dependen t features we use a synthetic prob-lem. The problem consisted of 1000 sample points with 10 real valued features. The target concept is a xor function over the  X rst 3 features. Hence, the  X rst 3 features are relev ant while the other features are irrelev ant. Notice that this task is a special case of parit y function learning and is considered hard for many feature selection algorithms (Guy on &amp; Elissee X , 2003). Thus for example, any algorithm which does not consider functional dependencies between features fails on this task. Figures 1 and 2 presen t the results we obtained on this problem.
 A few phenomena are apparen t in these results. The value of the margin evaluation function is highly cor-related with the angle between the weight vector and the correct feature vector (see  X gures 1 and 3). This correlation demonstrates that the margins character-ize correctly the qualit y of the weight vector. This is quite remark able since our margin evaluation func-tion can be measured empirically on the training data whereas the angle to the correct feature vector is un-known during learning.
 As suggested in section 3.3 Relief does increase the margin as well. However, Simb a outp erforms Relief signi X can tly, as shown in  X gure 2. 5.2. Face Images We applied the algorithms to the AR face database (Martinez &amp; Bena vente, 1998) which is a collection of digital images of males and females with various facial expressions, illumination conditions, and occlu-sions. We selected 1456 images and converted them to gray-scale images of 85  X  60 pixels, which are taken as our initial 5100 features. Examples of the images are shown in  X gure 4. The task we tested is classifying the male vs. the female faces.
 In order to impro ve the statistical signi X cance of the results, the dataset was partitioned indep enden tly 20 times into training data of 1000 images and test data of 456 images. For each such partitioning (split) Simb a , Relief and Infogain 4 were applied to select optimal fea-tures and the 1-NN algorithm was used to classify the test data points. We used 10 random starting points for Simb a (i.e. random permutations of the train data) and selected the result of the single run which reached the highest value of the evaluation function. The av-erage accuracy versus the number of features chosen, is presen ted in  X gure 5.
 Simb a signi X can tly outp erformed Relief and Info-gain , especially in the small number of features regime. When less than 1000 features were used Simb a achieved better generalization accuracy than both Re-lief and Infogain in more than 90% of the partitions ( X gure 6). Moreo ver, the 1000 features that Simb a selected enabled 1-NN to achieve accuracy of 92.8% which is better than the accuracy obtained with the whole feature set (91.5%). A closer look on the fea-tures selected by Simb a and Relief ( X gure 7) reveals the di X erence between the two algorithms. Relief focused on the hair-line, especially around the neck, and on other contour areas in a left-righ t symmetric fashion. This choice is suboptimal as those features are highly correlated to each other and therefore a smaller subset is su X cien t. Simb a on the other hand selected features in other informativ e facial locations but mostly on one side (left) of the face, as the other side is clearly highly correlated and does not contribute new information to this task. Moreo ver, this dataset is biased in the sense that more faces are illuminated from the right. Many of them are saturated and thus Simb a preferred the left side over the less informativ e right side. 5.3. The NIPS-03 Feature Selection Challenge We applied G- X  X p as part of our experimen ts in the NIPS-03 feature selection challenge (Guy on &amp; Gunn, 2003). It was applied on two datasets (ARCENE and MADELON) with both 1-NN and SVM-RBF classi- X ers. The obtained results were among the best sub-mitted to the challenge. SVM-RBF gave better re-sults than 1-NN, but the di X erences were minor. In the ARCENE data, the task was to distinguish be-tween cancer and normal tissues gene-expression pat-terns. Each instance was presen ted by 10,000 fea-tures and there were 200 training examples. G- X  X p selected 76 features (when run after converting the data by PCA). SVM-RBF achieved balanced error rate of 12.66% using those features (the best result of the challenge on this data set was 10.76%). MADELON was a synthetic dataset. Each instance was repre-sented by 500 features and there were 2600 training examples. G- X  X p selected only 18 features. SVM-RBF achieved 7.61% balanced error rate using these features (the best result on this dataset was 6.22%). A main advantage of our approac h is its simplic-ity. For more information see the challenge results at http://www.nipsfsc.e cs.soton.ac.uk/r esults . A margin-based criterion for measuring the qualit y of a set of features has been presen ted. Using this cri-terion we deriv ed algorithms that perform feature se-lection by searc hing for the set that maximizes it. We have also showed that the well known Relief algorithm (Kira &amp; Rendell, 1992) appro ximates a gradien t ascen t over this measure. We suggested two new metho ds for maximizing the margin based-measure, G- X  X p which does a naive local searc h, and Simb a which performs a gradien t ascen t. These are just represen tativ es of the variety of optimization techniques (searc h metho ds) which can be used. We have showed that Simb a out-performs Relief on a face classi X cation task and that it handles better correlated features. One of the main advantages of the margin based criterion is the high correlation that it exhibits with the features qualit y. This was demonstrated in  X gures 1 and 3.
 Our main theoretical result is a new rigorous bound on the  X nite sample generalization error of the 1-Nearest Neighb or algorithm. This bound depends on the mar-gin obtained following the feature selection. Several researc h directions can be further investigated. One of them is to utilize a better optimization al-gorithm for maximizing our margin-based evaluation function. The evaluation function can be altered as well. It is possible to apply non-linear functions of the margin and achieve di X eren t tradeo X s between large margin and training error and thus better stabilit y. It is also possible to apply our margin based criterion and algorithms in order to learn distance measures. Another interesting direction is to link the feature se-lection algorithms to the LVQ (Kohonen, 1995) algo-rithm. As was shown in (Crammer et al., 2002), LVQ can be viewed as a maximization of the very same margin term. But unlik e the feature selection algo-rithms presen ted here, LVQ does so by changing pro-totypes location and not the subset of the features. This way LVQ produces a simple but robust hypoth-esis. Thus, LVQ and our feature selection algorithms maximize the same margin criterion by controlling dif-feren t (dual) parameters of the problem. In that sense the two algorithms are dual. One can combine the two by optimizing the set of features and protot ypes loca-tion together. This may yield a winning combination. We begin by proving a simple lemma which shows that the class of nearest neigh bor classi X ers is a subset of the class of 1-Lipsc hitz functions. Let nn S F (  X  ) be a function such that the sign of nn S F ( x ) is the label that the nearest neigh bor rule assigns to x , while the mag-nitude is the sample-margin, i.e. the distance between x and the decision boundary .
 Lemma 1 Let F be a set of features and let S be a labeled sample. Then the for any x 1 ; x 2 2R N : where F ( x ) is the projection of x on the features in F . The proof of this lemma is straigh tforw ard and is omit-ted due to space limitations. The main tool for proving theorem 1 is the following: Theorem 2 (Bartlett, 1998) Let H be a class of real value d functions. Let S be a sample of size m gener ated i.i.d. from a distribution D over X  X f X  1 g then with probability 1  X   X  over the choic es of S , every h 2 H and every  X  2 (0 ; 1] let d = fat H (  X = 32) : We now turn to prove theorem 1: Proof (of theorem 1): Let F be a set of features such that j F j = n and let  X  &gt; 0. In order to use theorem 2 we need to compute the fat-shattering dimension of the class of nearest neigh bor classi X cation rules which use the set of features F . As we saw in lemma 1 this class is a subset of the class of 1-Lipsc hitz functions on these features. Hence we can bound the fat-shattering dimension of the class of NN rules by the dimension of Lipsc hitz functions.
 Since D is supp orted in a ball of radius R and k x k  X  k F ( x ) k , we need to calculate the fat-shattering dimen-sion of Lipsc hitz functions acting on points in R n with norm bounded by R . The fat  X  -dimension of the 1-NN functions on the features F is thus bounded by the largest  X  packing of a ball in R n with radius R , which in turn is bounded by (2 R= X  ) j F j .
 Therefore, for a  X xed set of features F we can apply to theorem 2 and use the bound on the fat-shattering dimension just calculated. Let  X  F &gt; 0 and we have according to theorem 2 with probabilit y 1  X   X  F over sample S of size m that for any  X  2 (0 ; 1] er
D (nearest-neigh bor)  X  ^ er for d = (64 R= X  ) j F j . By choosing  X  F =  X =  X  N  X  N j F j have that P F  X  [1 :::N ]  X  F =  X  and so we can apply the
