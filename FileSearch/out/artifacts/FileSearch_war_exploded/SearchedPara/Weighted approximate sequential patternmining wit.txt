 Department of Computer Science, School of Electrical and Computer Engineering, Chungbuk National University, CheongJu, South Korea Department of Electronics Engineering, KonKuk University, Seoul, South Korea 1. Introduction
Sequential pattern mining fi nds frequent subsequences as patterns in a sequence database. Suppose s 1 ,s 2 ,...,s m ,where s j is an itemset which is also called an element of the sequence, and s j  X  I .That time in an element of a sequence but it can occur multiple times in different elements of a sequence. The size | s | of a sequence is the number of elements in the sequence. The length, l(s), is the number of items in the sequence. A sequence with length l is called an l-sequence. A sequence  X  = X 1 ,X 2 ,...,X n is called a subsequence (  X   X  ) of another sequence  X  = Y 1 ,Y 2 ,...,Y m ( n m ) ,and  X  is called a super sequence of the sequence  X  if there exist an integer 1 i 1 &lt; ... &lt; i n m such that X 1  X  Y i 1 ,X 2  X  Y i 2 ,...,X n  X  Y in . A sequence database, S is a set of tuples sid, s ,where sid and s are a sequence identi fi er and a sequence respectively. The support of a sequence  X  in a sequence database (SDB) is the number of sequences in SDB that contain the sequence  X  (support if the sequence, s, is a super sequence of  X  (  X  s ). Given a support threshold, min sup, a sequence  X  is called a frequent sequential pattern in the sequence database if the support of the sequence  X  is no less than the minimum support threshold (support (  X  ) min sup). The problem of sequential pattern mining is to fi nd the complete set of all frequent super sequences. Here, the anti-monotone property [1,8] has been mainly used to prune infrequent sequential patterns. That is, if a sequential pattern is infrequent, all super patterns of the sequential pattern must be infrequent. Using this characteristic, sequential pattern mining algorithms prune infrequent sequential patterns earlier. Sequential pattern mining has become an essential task with broad applications such as analyzing Web access patterns, customer purchase data, DNA sequences and so on. Sequential patterns and items within sequential patterns have been treated uniformly, but real sequences have different importance. For this reason, weighted sequential pattern mining [23] has been suggested to fi nd important sequential patterns by considering the weights of items, itemsets, and sequences. However, there is a limitation in Noisy environment because real data can be subject to noise and measurement error and weights of items within the data can be varied according to the importance. From the above limitation, in this paper, we suggest the problem of discovering weighted approximate sequential patterns with noisy conditions where a tolerance factor is considered to relax the requirement for exact equality between weighted s upports of patterns and estimate weighted supports of patterns within the tolerance factor. Furthermore, for reducing search space, we suggest an ef fi cient way of pruning weighted approximate infrequent sequential patterns. Based on the framework, an ef fi cient sequential pattern mining algorithm called WAS (Weighted Approximate Sequential pattern mining) is proposed. To our knowledge, these works have never been presented. We analyze complexity of our approach and run extensive performance test in real datasets to illustrate the effectiveness, ef fi ciency scalability and quality of patterns in our approach.

The remainder of the paper is organized as follows. In Section 2, related work is presented. In Sec-tion 3, we investigate Weighted Approximate Sequential pattern mining and in Section 4, develop WAS (Weighted Approximate Sequential pattern mining) algorithm. Section 5 shows extensive experimental tests by comparing WAS algorithm with WSpan [23], and SPAM [3]. Finally, our conclusion is presented in Section 6. 2. Background 2.1. Sequential pattern mining: Basic examples
Table 1 shows the input sequence database SDB in our running example. Assume that a minimum support is 2. The SDB has 8 unique items, and six input sequences. A sequence a (abc) (ac) d (cf) in SDB has fi ve itemsets: a, (abc), (ac), d, (cf) where items  X  X  X  and  X  X  X  appear three times in different itemsets of the sequence. The size of a (abc) (ac) d (cf) is 5 and the length of this sequence is 9. Sequence a(bc)d is a sub sequence of a (abc) (ac) d (cf) since a  X  a, (bc)  X  (abc) and d  X  d. Additionally, the sequence &lt;a(bc)d&gt; is a frequent sequential pattern because sequences 10 and 20 contain sub sequence s = a(bc)d and the support (2) of the sequence is no less than 2. Meanwhile, minimum support (2). Based on the anti-monotone property, we can know that all super patterns of the infrequent patterns. 2.2. Related work Sequential pattern mining in data mining [9] has been extensively studied since fi rst introduced by Agrawal [1]. Since then, sequential pattern mining algorithms have been extensively developed such as constraint-based sequential pattern mining [13,17], closed sequential pattern mining [20], approximate sequence mining [10], multi-dimensional sequence pattern mining [16], sequence mining in a noisy environment [21], biological sequence mining [7,19], incremental sequence mining [5] and sequence indexing [6], sequential af fi nity pattern mining [22], closed sequential pattern mining [20] and stream pattern mining [4]. Moreover, to focus on the user X  X  interestingness, constraint based sequential pattern mining [13,17] has been suggested. In constraint-based sequential pattern mining, interesting patterns are discovered by the user X  X  point of view. In [13], various kinds of constraints are classi fi ed based on the notions of monotonicity, anti-monotonicity, succinctness and whether the constraints can be transformed into these categories if they do not belong to three categories. Among the constraints, sequential pattern mining with weight constraints [23] has been proposed in which important sequential patterns are discovered by considering both support and weight of patterns. Recently, an ef fi cient approach of mining frequent approximate sequential patterns [5] has been proposed to fi nd robust sequential pattern with noisy data. However, there is no study of fi nding weighted approximate sequential patterns. 2.3. Motivational example
In the real world, there are several applications [11,12,18] where speci fi c patterns and items within the sequential patterns have more importance or priority than other sequential patterns. For instance, in real applications, when fi nding the traversal patterns in the World Wide Web, each page has different importance and transactional patterns including the important pages have also priority. In biomedical data analysis, some genes are more signi fi cant than others in causing a particular disease, and some genes are more effective than others in fi ghting diseases. More extensions with weight constraints have been proposed such as mining mining weighted graphs [11], mining weighted association rules [18], mining weighted sub-trees or sub structures [12], mining weighted sequential patterns [23] and so on. However, in above applications, the accumulated data is so huge and the uncertainty of the processing data exists due to inherence noise and imprecision in data. Web access pages can be obsolete and lost, and they cannot be considered as the processing data. DNA data analysis is another example. The DNA data can be mutated and the exact counting can be dif fi cult. Our approaches can be utilized to fi nd important sequential patterns with uncertain data above applications. Moreover, in noisy environments, weighted approximate structures/trees/graphs/rules can be also extracted by our approach. 3. Weighted approximate sequential pattern mining with tolerance factors
In this section, we suggest a Weighted Approximate Sequential pattern mining (WAS) and show how to ef fi ciently prune the search space. 3.1. Preliminaries
A weight of an item is a non-negative real number that shows the importance of each item. Given a is formally de fi ned as To set up weights of items, an attribute value of the retail items can be used. For example, prices (pro fi ts) of items can be used as a weight factor in market basket data. However, the real values of items are not suitable for weight values because of the big variation. As discussed in [23,24], we can know that variation of items X  prices is so big that the prices cannot be directly used as weights. Therefore, the normalization process is needed which adjusts for differences among data from varying sources in order to create a common basis for comparison. Weights of items are assigned with a w weight b w according to items X  importance or priority. The weights with a w weight b w are normalized as min w weight max w and the normalized weights can be used in the mining process. Attribute values as prices of items in market basket data can be used as a weight factor and the prices of items can be normalized within a speci fi c weight range. Based on the de fi nition, items, itemsets and a sequence have their own weights, and, with the weights. General sequential pattern mining has used only supports of patterns. Meanwhile, in weight based sequential pattern mining [23], the Weighted Support (Weight (S) * Support (S)) of the sequential pattern is compared with the minimum threshold. Weighted Support (WS) of a sequential pattern is de fi ned as the resultant value of multip lying the pattern X  X  support with the weight of the pattern in which the weight of the sequence (sequential pattern) is obtained by calculating the average value of the weights in items of a sequence. A sequential pattern is called a weighted frequent sequential pattern if and only if the weighted support of the sequential pattern is no less than a minimum threshold. 3.2. Weighted approximate sequential pattern
The concept of weighted sequential pattern mining is attractive in that important sequential patterns are discovered. However, the main limitation at previous weighted sequential pattern mining is that weighted supports of sequential patterns are exactly matched with a minimum threshold. The real data is typically subject to noise and measurement error, and weights of items within the data are varied according to the importance. From the noisy environment, the small change in weights of items (items X  importance) or an approximate factor that is a fault tolerant bound to estimate weighted supports of sequential patterns by relaxing the requirement for exact equality betw een weighted supports of sequential patterns and a minimum threshold. We focus on mining robust important sequential patterns that are not in fl uenced by noisy data.
 De fi nition 3.1 Tolerance Factor,  X 
Tolerance factor,  X  is a maximum changeable fault approximate bound of sequential patterns in noisy environment. It is given as a percentage of a maximum bound in which the maximum bound among sequential patterns is the resultant value of multiplying MaxW (Maximum Weight of items) with N (number of sequences in a dataset), and it is used as a threshold.
 Weighted supports of sequential patterns S and S X  are said to be approximately identical if ( | WS (S)  X  WS (S X ) | &lt;  X  ). As a special case, we can think of a following case: ( | WS (S)  X   X  | &lt;  X  ) which means the difference between a weighted support of a sequential pattern S and a minimum threshold,  X  is less than an approximate factor,  X  . We can change the formula as follows: ( | WS (S)  X  X  X | &lt; X  ). Weighted Support (WS (S)) of a sequential pattern, S can be varied. In this case, with a fault tolerant factor, the sequential pattern may not be weighted frequent sequential pattern if the varied weighted support (WS (S)  X  X  X  ) of the sequential pattern is less than the minimum threshold (  X  ) with the noisy condition. In the above condition, the factor  X  is also called as a fault tolerance bound. If approximate weighted support ( | WS (S)  X  X  X | ) of a sequential pattern S is no less than  X  , the sequential pattern is robust in noisy environment. That is, with the noisy condition, the sequential pattern, S can have an approximate weighted support, | WS (S)  X  X  X  ) | which is still no less than a minimum threshold,  X  . Generally, in weight based sequential pattern mining, the weighted supports of weighted frequent sequential patterns are no less than the minimum support. However, at noisy environment, there are four cases where an approximate factor is applied in mining weighted approximate sequential patterns.

Case 1 ( | WS ( S )  X  X  X |  X  ) : First, in case 1, a difference ( | WS (S)  X  X  X | ) between a weighted support (WS (S)) of a sequential pattern, P, and an approximate factor (  X  )isnolessthanaminimum support (  X  ). In this case, we can know that WS (S) is always no less than  X  . In other words, if ((WS (S)  X  X  X  )  X  )thenWS(S)  X  because WS (S) (WS (S)  X  X  X  )  X  .

Case 2 ( | WS ( S )  X  X  X | &lt; X  ) : Second, in case 2, we consider that | WS (S)  X  X  X | &lt; X  .IfWS(S)is less than  X  ,WS(S)  X  X  X  is de fi nitely less than  X  and the sequential pattern, S is a weighted infrequent sequential pattern with and without noisy data so the sequential pattern can be pruned. Meanwhile, even if WS (S) is no less than  X  , the sequential pattern can be weighted infrequent in the presence of noise so a sequential pattern, S satisfying the condition ( | WS (S)  X  X  X | &lt; X  ) of the case 2 is not robust in this case in that a weighted frequent sequential pattern can become a weighted infrequent sequential pattern from noisy data.

Case 3 ( | WS ( S ) +  X  X   X  ) : Third, a weighted support of a sequential pattern, S can be increased in noisy condition, so we need to check if ((WS (S) +  X  )isnolessthan  X  or not. Although the approximate weighted support of adding a weighted support (WS (S)) of a sequential pattern, S to approximate factor (  X  )isnolessthan  X  , we cannot say that WS (S)  X  . With WS (S) less than  X  , this formula (WS (S) +  X   X  ) can be satis fi ed. Therefore, weighted infrequent sequential patterns may become weighted frequent in noisy data.
 Case 4 ( | WS ( S ) +  X  X  &lt; X  ) : Fourth, we can consider the case 4 where (WS (S) +  X  )islessthan  X  . In case 4, WS (S) must be less than  X  because WS (S) &lt; WS (S) +  X  &lt; X  , so in this case, the sequential pattern is weighted infrequent with/without an approximate factor (  X  ).

From the analysis, we know that a weighted infrequent sequential pattern can be weighted frequent with noisy environment (case 3). Of course, the reverse case is also possible and a weighted frequent sequential pattern may become weighted infrequent from the noisy data (case 2). It is essential to mine important sequential patterns that are not affected by noisy data. A sequential pattern is a robust  X  -free sequential pattern if the weighted support of the sequential pattern is always no less than a minimum threshold both with and without noi sy conditions. From above cases, a sequential pattern is robust  X  -free sequential pattern if WS (S) of the sequential pattern is no less than  X  and WS (S)  X  X   X  with noisy data when approximate weighted suppor t is varied. This condition is brie fl ythesameasWS(S)  X  X  X   X  because WS (S) and WS (S) +  X  are always no less than  X  if WS (S)  X  X  X  is no less than  X  . As a result, we can check only the condition ((WS (S)  X  X  X  )  X  ) while approximate weighted support is varied by noisy data. Based on the analysis, we de fi ne a weighted approximate frequent sequential pattern that is robust  X  -free sequential pattern with/without noisy conditions. De fi nition 3.2 Weighted Approximate Sequential Pattern ( WASP )
Given a minimum threshold,  X  , a sequential pattern S is a weighted approximate frequent sequential pattern if the sequential pattern is weighted frequent and an approximate weighted support of the sequential pattern is no less than  X  .

In noisy environment, a weighted support of a sequential pattern may be increased or decreased. To consider two cases, an approximate weighted support can be set as WS (S)  X  X  . WS (S) of a sequential pattern, S may be varied by noisy data. Thus, we can think of three types of approximate weighted supports, WS (S)  X  X  X  ,WS(S),WS(S) +  X  . If there is no fault on a sequencedatabase, WS (S) is used. If weighted support of a sequential pattern, S is increased by noisy data, the approximate weighted support of the sequential pattern is set as WS (S) +  X  . Meanwhile, WS (S)  X  X  X  is computed as an approximate weighted support if the weighted support is decreased by missing data from noisy environment. Note that the weighted approximate frequent sequential patterns found with an approximate factor,  X  X  X  is super sets of those found with an approximate factor, +  X  since | WS (S)  X  X  X | WS (S) | WS (S) +  X  X  .
 De fi nition 3.3 Approximate Weighted Support ( AWS )
An Approximate Weighted Support (AWS) of a sequential pattern is de fi ned as the resultant value of subtracting an approximate factor,  X  from Weighted Support (WS) of the sequential pattern.
Weighted approximate frequent sequential patterns found by checking the approximate weighted supports, WS (S) +  X  , may not be weighted approximate frequent sequential patterns with WS (S)  X  X  X  . We concentrate on robust sequential patterns t hat are not affected with/without noisy conditions. Therefore, a difference ( | WS (S)  X  X  X | ) between a weighted support, WS (S) of a sequential pattern, S, and an approximate factor (  X  ) is considered because | WS (S) +  X  X  is always no less than  X  if WS (S) is no less than  X  . A weighted approximate frequent sequential pattern is also called as a strong  X  -free important sequential pattern because a weighted approximate frequent sequential pattern is a robust sequential pattern that is not affected with/without noisy conditions.
 Lemma 3.1 Although a sequential pattern S is not we ighted approximate frequent, a super pattern of the sequential pattern S can be weighted approximate frequent.

From the anti-monotone property [1], if a sequential pattern, S is frequent, then, any super pattern of the sequential pattern S is a frequent sequential pattern. However, weighted approximate frequent sequential pattern mining does not satisfy the anti-monotone property. That is, a super pattern of the weighted approximate infrequent sequential pattern X can be weighted approximate frequent. Additionally, a sub pattern of the weighted approximate frequent sequential pattern Y can be weighted approximate infrequent. We prove thatweighted approximate frequentsequentialpatterns donotsatisfy anti-monotone property by showing a contradictory example. Suppose that a minimum support (  X  ) and an approximate factor,  X  are 2 and 1 respectively, and a weight list of items is: &lt;a:0.8, b:1.0, c:0.8, d:1.2, f:1.3, g:0.9, In this example, a sequential pattern &lt;bc&gt; is not a weighted approximate frequent sequential pattern because the approximate weighted support ((3* (1.0 + 0.8)/2)  X  X  X  (1) = 1.7) of the sequential pattern a weighted approximate sequential pattern whose weighted support ((3* (1.0 + 0.8 + 1.2)/3)  X  X  X  (1) = 2.0) is equal to the minimum threshold (2). Therefore, a super pattern &lt;bcd&gt; of a weighted approximate infrequent sequential pattern &lt;bc&gt; is weighted approximate frequent. As a result, the anti-monotone property cannot be used directly in weighted approximate sequential pattern mining.

The main limitation of mining weighted approximate sequential patterns is that the weighted approx-imate frequent sequential pattern mining does not satisfy anti-monotone property. Super patterns of a weighted approximate infrequent sequential pattern may be weighted approximate frequent because a weighted frequent sequential pattern with a low weight can get a high weight after adding other items with higher weights, and weighted supports within approximate factor can be changed by noisy envi-ronments. Therefore, we propose anti-monotone property of weighted maximum approximate frequent sequential pattern to reduce search space. To fi nd robust sequential patterns unaffected with/without noisy conditions, Approximate Weighted Support (AWS) is de fi ned from de fi nition 3.3. Meanwhile, Approximate Weighted Support (AWS) cannot be applied becauseit is not satisfy anti-monotone property from the Lemma 3.1. In WAS, to prune weighted approximate infrequent sequential patterns with noisy conditions earlier but maintain the anti-monotone property, Maximum Approximate Weighted Support (MAWS) is de fi ned.
 De fi nition 3.4 Maximum Approximate Weighted Support ( MAWS ) Maximum Approximate Weighted Support (MAWS) of a sequential pattern is a maximum value of Approximate Weighted Support (AWS) a sequential pattern can have with/without noisy environments which is calculated by subtracting an approximate factor,  X  from Maximum Weighted Support (MaxW * Support (S)) of the sequential pattern. Here, Maximum Weight (MaxW) is the maximum weight of items of a sequence database or a conditional database.
 Lemma 3.2 In WAS, given conditional patterns X and Y ( X  X  Y ) , ( 1 ) MAWS ( X ) is no less than MAWS ( Y ) . Additionally, ( 2 ) MAWS ( CDB ( X )) is no less than MAWS ( CDB ( Y )) . (1) From the de fi nition 3.4, Maximum Approximate Weighted Support (MAWS) of a se quential pattern, S is calculated by subtracting an approximate factor,  X  from Maximum Weighted Support (MaxW * Support (S)) of the sequential pattern. Here, Maximum Weight (MaxW) is the maximum weight of items of a sequential pattern or a database. Figure 2 shows sequential pattern extension process of our approach in which weight ascending pre fi x trees are recursively made and they are traversed by bottom up strategy.

The conditional pattern Y is grown from the conditional pre fi xY(X  X  Y) and items within these sequential patterns are sorted by weight ascending order. Additionally, the item with Maximum Weight (MaxW) is commonly included in two conditional patterns, X and Y. Thereby, MaxW of the conditional pattern Y is equal to that of the conditional pattern X. Meanwhile, a support of the sequential pattern X is no less than that of the seque ntial pattern Y because the conditional p attern X is a subset of the conditional pattern Y. As a result, | MWS (X)  X  X  X | | MWS (Y)  X  X  X | . (2) CDB (X) of a sequential pattern X is a conditional database derived with a conditional pattern X. If a conditional pattern X is a subset of a conditional pattern Y, CDB (Y) is a subset of CDB(X) since the Y-conditional database CDB (Y) is generated from the X-conditional database. Therefore, the conditional database from the conditional pattern Y is a subset of the conditional database with the conditional pattern X. From the de fi nition 3.4, MAWS (S) of a sequential pattern S is a Maximum Approximate Weighted Support ( | MWS (S)  X  X  X | ) of the sequential pattern. MAWS (CDB(S)) is a maximum approximate weighted support of items included in CDB (S) which is calculated as ( | MWS (CDB(S))  X  X  X | ). Therefore, MaxW of the CDB(Y) is no greater than that of CDB (X). As shown in Fig. 2, CDB(Y) is subset of CDB(Y) so, the items of each transaction at CDB(Y) is always included in the items of each transaction at CDB(X). Thus, a maximum support of items at CDB (X) is always no less than that of items at CDB (Y). For example, with two conditional pattern  X  X  X  and  X  X d X , we can get following CDB (Conditional DataBases) respectively: { (d,a,b,e),(d,b,e),(d,a,b,e),(d,a),(d,a,b, e) } and { (a, b, e), (b, e), (a), (a, b, e) } . Here,  X  X d X  is grown from the pre fi x  X  X  X  and MaxW is equal. Of course, the maximum support (4) of the item  X  X  X  at CDB ( X  X  X ) is always no less than that (3) of the item  X  X  X  at CDB ( X  X d X ). Finally, we can see that a maximum weighted support of items within CDB (X) is no less than a maximum weighted support of items at (CDB (Y)). In a similar way, we can know that | MWS (X)  X  X  X | | MWS (Y)  X  X  X | if X  X  Y.

To reduce search space, in WAS, during mining process, the maximum approximate weighted sup-port (instead of pattern X  X  approximate weight support) in each step is used to roughly prune weighted approximate infrequent sequential patterns.
 Lemma 3.3. Anti-monotone property of weighted maximum approximate sequential patterns.

If maximum approximate weighted support of a sequential pattern S is less than  X  , maximum approx-imate weighted support of any super pattern of the sequential pattern S is also less than  X  .Asshownin De fi nition 3.3, if a sequential pattern S is a maximum weighted approximate frequent sequential pattern, MWS (S)  X  and MAWS (S)  X  . Thus, if a sequential pattern, X is not a maximum weighted approximate frequent sequential pattern; one of following conditions is satis fi ed.
 Case 1: MWS (X) &lt; X  and MWS (X)  X  X  X   X  Case 2: MWS (X) &lt; X  and MWS (X)  X  X  X  &lt; X  Case 3: MWS (X)  X  and MWS (X)  X  X  X  &lt; X 
The case 1 does not occur because MWS (X) must be no less than  X  if maximum approximate weighted support (MWS (X)  X  X  X  ) of a sequential pattern is no less than  X  . From the case 2 and 3, two formulas: ((MWS (X) &lt;  X  and MWS (X)  X  X  X  &lt; X  ) or (MWS (X)  X  and MWS (X)  X  X  X  &lt; X  )) are combined asaformula:(MWS(X)  X  X  X  &lt; X  ) and (NOT (MWS (X)  X  )) or (MWS (X)  X  )) so the formula is fi nally (MWS (X)  X  X  X  &lt; X  ). Thereby, if a sequential pattern X is not maximum weighted approximate frequent sequential pattern, MAWS(X) of the sequential pattern is less than  X  . Then, we can prove that if MWS (X)  X  X  X  &lt; X  then, for any Y, X  X  Y, MWS (Y)  X  X  X  &lt;  X  . From lemma 3.1, if X  X  Y, then, MWS (X) MWS (Y) in our approach. Of course, (MWS (X) - X  ) (MWS (Y) - X  ) . Therefore, it is clear that if (MWS (X) - X  &lt;  X  ) then, for any Y, X  X  Y, (MWS (Y) - X  &lt;  X  ) . In other words, if a sequential pattern, X is not weighted maximum approximate frequent (MAWS (X) &lt; X  ), any super pattern of the sequential pattern X is not weighted maximum approximate frequent (MAWS (Y) &lt; X  ). As a result, anti-monotone of weighted maximum approximate frequent sequential patterns is satis fi ed.
WAS performs mining process more ef fi ciently using the anti-monotone property of maximum weight-ed approximate frequent sequential patterns. From the anti-monotone property of the weighted approx-imate frequent sequential pattern mining, if a sequential pattern is less than the minimum threshold, any super pattern of the sequential pattern is removed. However, the maximum approximate weighted support ( | support (S) * MaxW | X  X  X  ) is not exact value. Thus, in fi nal step, we should check if the sequential pattern S satis fi es the condition (WS (S) - X   X  ) to prune weighted approximate infrequent sequential patterns which satisfy the condition ((support (S) * MaxW) - X   X  ). 4. Mining weighted approximate sequential patterns 4.1. WAS algorithm On the framework, we develop the WAS algorithm to detect weighted approximate sequential patterns. A sequence database is recursively projected into a set of smaller weighted projected databases and weighted sequential patterns are grown in each weighted projected database. Given a sequence  X  = &lt;e 1 e 2 ... e n &gt; (in which each e i means a frequent element in S), a sequence  X  &lt;e 1 e 2 ... e m &gt;(m n) is called a pre fi x of the sequence  X  if (1) e i = e i for ( i m  X  1), (2) e m  X  e m and (3) all the weighted frequent items in (e m  X  e m ) are alphabetically listed after those in e m . Given a sequential pattern  X  in a sequence database,  X  -projected database (S |  X  ) is the collection of suf fi xes of sequences in S about the number of weighted sequences  X  in S |  X  .

After WAS algorithm calls the procedure WAS (WASP, &lt;  X  &gt;, 0, SDB), WAS (  X  ,L + 1, S |  X  )is called recursively after  X  projected database S | a is constructed. Recall that the maximum approximate weighted support (MWS (S)  X   X  ) is used. Therefore, in fi nal step, we should prune weighted approximate infrequent sequential patterns which satisfy this condition (MWS (S)  X   X  min sup). 4.2. Mining process
In this Section, we illustrate how WAS algorithm mines weighted approximate sequential patterns by using a pre fi x-based projection approach. Given a sequence database in Table 1, &lt;a:0.8 b:0.7, c:0.8, d:0.85, e: 0.75, f:0.9. g:0.8, h:0.85&gt;, a minimum support (  X  ) of 3 and a tolerance factor (  X  )of0.3, weighted approximate sequential patterns are mined by the pre fi x projection approach as follows. (1) Scan the sequence database once, and fi nd all the weighted frequent items in sequences. Here, a maximum approximate weighted supports ( | MWS (X)  X   X  X  ) are no less than a minimum threshold,  X  are &lt;a:5.1, b:5.1, c:5.1, d:4.2, e:3.3, f:2.4&gt; } . For instance, maximum approximate weighted support of an item &lt;a&gt; is 5.1 because Maximum Weight (MaxW) is 0.9 and | MWS (X) - X  X  is the value of subtracting an approximate factor (0.3) from maximum weighted support (0.9 * 6 = 5.4). In this case, the maximum approximate weighted support (5.1) of an item &lt;a&gt; is greater than a minimum support (3). Meanwhile, the item &lt;f&gt; is pruned whose the maximum approximate weighted support (2.7 (0.9 * 3)  X  0.3 = 2.4). is less than  X  so the item &lt;f&gt; is pruned. In this way, the items &lt;g&gt; and &lt;h&gt; are removed. (2) The complete set of weighted sequential patterns can be partitioned into following six subsets can be mined by constructing the corresponding set of weighted projected databases and mining them recursively. We only collect the sequences which have the pre fi x &lt;a&gt;. Additionally, in a sequence be considered. The sequences in sequence database containing the pre fi x &lt;a&gt; are projected with regards MaxW(Maximum Weight) of this stage is 0.85, not 0.9 because the item &lt;f&gt; with weight, 0.9 is pruned. pruning condition is applied in order to check whether it is a weighed approximate sequential pattern. once, its local items are a:1, b:2, c:4, d:2, (  X  b):3, and (  X  c):1. Here, maximum approximate weighted &lt;aa&gt; projected database returns a weighted approximate sequential pattern, &lt;aac&gt;:4.
As another example, the &lt;(ab)&gt; projected database consists of four suf fi x subsequences pre fi xed projected database once, its local items are a:1, b:2, c:4, d:3, (  X  c):1 and (  X  d):1. Maximum approximate &lt;f&gt; respectively. Recall that the maximum approximate weighted support prunes weighted approximate infrequent sequential pattern roughly while satisfying the anti-monotone property. Therefore, in fi nal step, we should prune remaining weighted approximate infrequent sequential patterns with the real weighted support ((support (S) * weight (S))  X   X  ) of the pattern S. For instance, during mining process, a sequential pattern &lt;(ab)c&gt;:4 is considered as a weighted approximate sequential pattern. However, the pattern &lt;(ab)c&gt;:4 is not weighted approximate sequential pattern because the real approximate weighted support (WS (&lt;(ab)c&gt;)  X   X  ) of the pattern is 2.767 ((((0.8 + 0.7 + 0.8)/3) * 4)  X  0.3) which is less than the minimum threshold (3). Thus, the pattern is not weighted approximate frequent. 5. Performance evaluation In this Section, we report our experimental results on the performance of WAS in comparison with SPAM [3], and WSpan [23]. SPAM is smart sequential pattern mining algorithm using bitmap represen-tation, and WSpan is a weighted sequential pattern mining algorithm. 5.1. Test environment and datasets
Our algorithm, WAS, was implemented in C++. Experiments were performed on a sparcv9 proces-sor operating at 1062 MHz, with 2048MB of memory. All experiments were performed on a Unix machine. The IBM dataset generator (http://www.almaden.ibm.com/) is used to generate synthetic sequence datasets. It accepts essential parameters such as the number of sequences (customers), the average number of itemsets (transactions) in each sequence, the average number of items (products) in each itemset, and the number of different items in the dataset. Table 2 shows parameters and their meanings in this sequential dataset generation. Different types of datasets can be generated. In our performance test, D1C10T5S8I5, D7C7T7S7I7, and D15C15T15S15I15 synthetic datasets are used. To make our experiments fair, the synthetic datasets used in the experiments are the same as those used in SPAM [3]. For example, D1C10T5S8I5 means that there are 1000 customers, average number of 10000 transactions for each customer, average number of 5000 items per transaction, average length of 8000 maximal sequences, and number of 5000 different items in this dataset. More detail information can be found in [14,15]. On the other hands, the real dataset is also tested. We report the evaluation results for Gazelle dataset. The Gazelle dataset is click stream data which is used in KDDCup-2000 and it has 29369 customers X  Web click stream data provided by Blue Martini Software Company. For each Customer, several sessi ons of Web click stream and each sess tion has multiple page views. Product pages by a customer in a session are considered as an itemset and difference sessions by one use is thought as a sequence. The Gazelle dataset 29369 sequences (customers), 35722 sessions (itemsets), and 87546 page views (products or items). Speci fi cally, there are 1423 distinct page views. 5.2. Experimental results 5.2.1. Comparison of WAS with SPAM and Wspan
In this test, we focused on the ef fi ciency of weighted approximate sequential pattern mining. Our experiment shows that WAS outperforms SPAM, and WSpan. First, we evaluated the performance on the D1C10T5S8I5 dataset. In Figs 5 and 6, weights of items are set as 0.3 X 0.5. Figure 5 compares the number of weighted sequential patterns of WAS with those of SPAM, and WSpan as the minimum support is increased.

Figure 6 shows the runtime of the algorithms under the same weight setting. In Fig. 5, we can see WAS generates fewer sequential patterns than those of SPAM, and WSpan. In Fig. 6, the runtime for fi nding weighted sequential patterns is shown in the D1C10T5S8I5 dataset. From Fig. 6, we see that WAS is much faster than other algorithms. Meanwhile, SPAM generates a huge number of sequential patterns with a minimum support of less than 5%. The main performance difference between WAS and WSpan results from considering approximate patterns. WAS can get much better performance by relaxing the requirement for exact equality between weighted supports of the pattern and the supersets of the pattern. Additionally, the performance gap between WAS and SPAM algorithms results from considering more important patterns in WAS.

Figures 7 and 8 demonstrate the results of performance test using the D7C7T7S7I7 dataset by setting normalized weights from 0.1 to 0.4. WAS outperforms SPAM, and WSpan. When a tolerance factor is larger, the performance difference becomes larger. An approximate factor in WAS is able to adjust the number of patterns by considering similarity and importance of both support and weight of patterns. For example, at higher approximate bounds, such as 2% or more, WAS performs much better than WSpan and SPAM. We see that the number of patterns for WAS is decreased as a tolerance factor is increased. WAS can also adjust the number of patterns by rese tting the minimum support. By increasing a tolerance factor, more approximately identical patterns are pruned. In Fig. 7, we can see that WAS is the fastest. WAS uses two thresholds, a minimum approximate bound and a minimum support. However, from Fig. 6, we see that the number of patterns increases quickly in WAS as a tolerance factor is decreased.
In Figs 9 and 10, we report evaluation results for D15C15T15S15I15 dataset. Normalized weights from 0.5 to 0.7 are used in this test. WAS mines weighted approximate sequential patterns with the tolerance factor. By increasing the factor, the number of sequential patterns is decreased in Fig. 9 Meanwhile, the number of WSpan is extremely increased. From Fig. 9 to Fig. 10, we can see that WAS generates fewer but important patterns. In this test, note that the effect of a minimum support with less than 58% in WSpan and SPAM is not big. For instance, the number of patterns is slightly changed with a minimum support from 58% to 60%. Meanwhile, the number of patterns with a tolerance factor, 2% is several orders of magnitude fewer than the number of patterns with a tolerance factor, 0.5% in WAS. Increasing a tolerance factor means more approximately identical patterns can be found and the subset of the patterns with the same weighted supports can be more pruned.

We run performance test with real dataset Gazelle. Figure 11 to Fig. 13 shows performance results with normalized weights, 0.1 X 0.9 and minimum supports, 0.07% X 0.09%. As a minimum support becomes lower, the number of result patterns is increased, runtime is increased, and memory usage is higher. However, WAS algorithm outperforms WSpan algorithm in all cases with the same parameter settings. When we fi xed a minimum support, we can check the effect of a tolerance factor. For instance, with the minimum support, 0.07%, WSpan algorithm generates 11,613 weighted sequential patterns. Meanwhile, WAS algorithm mines weighted approximate sequential patterns of 5,786 and 3,790 respectively. From Fig. 12, we can see that WAS is much faster than WSpan algorithm as a tolerance factor is lower. We checked the memory consumption used in WAS and WSpan algorithms. For each interval, the memory usages of WAS and WSpan are represented by the maximum usages of the interval. In Fig. 13, WAS needs lower memory than WSpan algorithm.

Figure 14 to Fig. 16 presents performance results with normalized weights of 0.2 X 0.4 and lower minimum supports, 0.03% X 0.05%. WAS outperforms WSpan. In Fig. 14, the number of sequential patterns is increased as a minimum support is decreased and in Fig. 15, a runtime of WAS is faster than that of WSpan. In Fig. 16, WAS uses less memory than WSpan as a minimum support is decreased. For example, with a minimum support of 0.03%, WSpan uses more than 328 MB but WAS algorithm needs 168MB with a tolerance factor, 0.005%. Additionally , performance of WAS algorithm becomes much better as a tolerance factor becomes higher. Note that the effect of a tolerance factor becomes bigger when a support threshold is lowered. 5.2.2. Scalability test We performed a scalability test with regard to th e number of sequences from 20K to 100K on the DxC2.5T5S4I2.5 datasets in Fig. 17 and the number of items from T2N to T10N on the D5C5Tx5S5I5 dataset in Figs 17 and 18. We set minimum supports as 0.5% and 0.3% respectively and weights are sequences to 100K sequences with the minimum support of 0.5%. From Fig. 17, we can see that WAS has much better scalability in terms of base sequences. In Fig. 18, algorithms show linear scalability with the number of items from sequences from T 2k to T10k. WAS shows th e best scalability among three algorithms. Furthermore, the performance of WAS becomes much better with the number of items of more than T8. In summary, WAS is ef fi cient and scalable in weighted sequential pattern mining. It may be not surprising that the number of patterns and the runtime are reduced but the result patterns found in WAS algorithm are robust with noisy data. 6. Conclusion
In this paper, we suggested the issue of weighted approximate frequent sequential pattern mining in which a tolerance factor was used to relax the requirement for exact equality between weighted supports of the sequential pattern and the minimum threshold. By applying the tolerance factors of the weighted supports, approximately identical patterns can be mined. In our approach, Maximum Approximate Weighted Support (MAWS) has been utilized to prune weight ed approximate infrequent patterns while satisfying anti-monotone property. Based on the framework, we developed WAS algorithm. With real and synthetic datasets, we tested performances of WAS algorithm in terms of the number of result patterns, runtime, memory usages and scalability with different number of sequences and number of items. Our performance study shows that WAS algorithm is ef fi cient and scalable than other algorithms. As future works, our approach can be applied to several applications in which noise and measurement error exists and weights of items within the data can be varied. For example, Web access pages can be obsolete and lost, and they cannot be considered as the processing data. DNA data analysis is another example. The DNA data can be mutated and the exact counting can be dif fi cult. As future works, our approaches can be applied to fi nd important frequent patterns with uncertain data in real applications. Moreover, weighted approximate sequential pattern mining can also be used in sequence indexing. Acknowledgement This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (NRF No. 2011-0005590). Moreover, this work was supported by the grant of the Korean Ministry of Education, Science and Technology (The Regional Core Research Program/Chungbuk BIT Research-Oriented University Consortium).
 References
