 The growth in size and number of longitudinal databases has lead to an increase in interest in time series data mining (TSDM) [6]. Two fundamental issues in any TSDM task are how to measure the similarity between time series and how to represent the data compactly without discarding important information. A common approach to the compression problem is to transform the data series so that the majority of the variation in the series can be captured in a small number of terms. In this paper we concentrate on fast fourier transforms (FFTs), probably the most popular transformation used in time series data mining (for example, see [1, 3, 7, 9, 10]). The basic method is to take the FFT of each se-ries, retain a fixed number of coefficients, then measure similarity between series as the Euclidean distance between the retained parameters. The objectives of this paper are to introduce an alternative distance measure based on the likeli-hood ratio statistic for testing the significance of differences between series and to demonstrate that this measure produces better results on types of data for which an FFT approach should be appropriate. We maintain that, if the prob-lem is complex enough to merit the use FFTs, then the likelihood ratio statistic will tend to give better results than Euclidean distance. Informally, this is be-cause the likelihood ratio is better able to detect consistent differences between small coefficients that may be undetected with Euclidean distance because of fluctuations in the larger fourier terms. the fourier transform represents y as a linear combination of sinusodal functions. After transformation a series is commonly compressed by retaining only the first of the FFT coefficients [1]. The Euclidean distance between the first f c coefficients of series x and y with coefficients ( p, q ) and ( r, s ) is then For complex mining problems where the use of FFT is justified, Euclidean dis-tance can give too much preference to small variations in the largest coeffi-cients, masking more complex differences in the wider spectrum. To overcome this problem we define a new distance measure, based on a test statistic for a hypothesis test of whether two series are significantly different, derived from the periodogram of a series. If series y has fourier coefficients ( p i ,q i ) then the can be thought of as an observation of an independent random variable A i with exponential density Since we have independence, the likelihood of our series is We can use the likelihood function to determine the similarity of two series by constructing a likelihood ratio hypothesis test. Assume for simplicity that the two series are the same length and have periodograms a i and b i . Assuming only to The major benefit of basing the distance on the likelihood ratio statistic is that it asymptotically follows a known distribution, and so could be used to not only measure the distance between series, but also test whether that distance is significant. d L can also be better at discriminating between series for problems where an FFT approach would seem to be appropriate: it is less influenced by small fluctuations in the larger coefficients, and better at detecting consistent variation in the smaller coefficients. A more complete description of the distance measure and the experimentation is provided in [4].
 Experiments with simulated data are designed to test two things over a class of model, M . Firstly, we test whether the likelihood ratio distance metric is better at discriminating between data from models in M . Secondly, we test whether any detected difference in discrimination effects the clustering and classification accuracy. To measure how well each distance metric discriminates, for each pair of models we measure the percentage difference, D , in the average distance of series from the same cluster to the average distance between series from different clusters. 100 random model pairs are generated to create 100 within and between distance estimates for d E and d L for different coefficient sizes. To estimate how well each measure would classify series, we form a correctness function C which is 1 if the distances between series from the same model are less than all the differences from series of different models, and 0 otherwise. 3.1 Sinusoidal and AR(1) Data We demonstrate the benefits of using d L on data from stationary order one auto-regressive models (AR(1) models), which take the form where  X  is a random variable with a standard normal distribution and  X   X  (  X  1 , 1). AR(1) models have been used extensively in TSDM research [2, 5] and are a good basis for testing how well a distance metric measures similarity based on change. For each run, two random AR(1) models X and Y were selected with  X   X  (  X  1 , 1). Figure 1 shows boxplots for the average (over 100 observations) within and between distance when the first 4 coefficients are retained. The left hand figure shows that, when using d E , although the median for within distance is lower than the median between distance, there is a large amount of overlap between the distributions. In contrast, when using d L , the largest difference in the between distance is lower than the median of the within distances, and there is clearly much greater discrimination than with Euclidean distance. The results presented in Table 1 show that d L provides a better means of discriminating between series than d E . The difference is significant at all levels. It is also inter-esting to note that the discriminatory power of d E actually decreases with the number of coefficients retained. This is caused by the fact it gives too great a weight to small fluctuations in the larger parameters, and the more coefficients retained the greater the chance of this resulting in an incorrect grouping. We also consider a class of model where an autoregressive structure discriminates the models, but where a common cyclical trend may cause a distance metric to be unable to detect differences. Let where  X  is a random variable with a standard normal distribution. The pa-rameters a, b and c control the amplitude, offset and frequency of the curves respectively. Suppose y ( t ) is an AR process as defined in Equation 3. Let the class of Sinusoidal and AR(1) models be z ( t )= x ( t )+ y ( t ) . Our objective is to detect whether the distance metrics can detect the difference in autocorellation structure even when series have the same sine wave series. Hence for a particular experiment involving two models we randomly generate a single sine model of the form given in Equation 4 and combine it with two separate AR(1) models. Table 1 shows the the average observed value of statistics C and D for both d E and d L . Although the extra cyclical trend of the sine data reduces the power of both distance metrics, d L is still better at detecting the difference between series. The following clustering experiments also show the benefit of using d L . k models are randomly selected and l series are generated from each model. Clus-ters are found using k-means and partitioning around the medoid (PAM) (both restarted 100 times at random initial data points) and the accuracy is measured against the known true clustering. The process is repeated 100 times to estimate the average clustering accuracy for compression ratios (i.e. different numbers of retained coefficients). Table 2 shows the clustering results for Piecewise Aggre-gate Approximation (PAA) (with Euclidean distance), d E and d L . Given that the difference between series is in the autocorellation structure, PAA is not a suitable transformation and hence the clusters found should simply reflect the differences in the common sinusoidal component. The results shown in Table 2 demonstrate that although there are differences in performance of the cluster-ing algorithms, the clusters formed with d L are consistently more like the true clusters than those formed using d E . 3.2 ECG Data ECG data has commonly been used in TSDM [2, 5, 6] and has the characteristic that it has a underlying cyclical trend that is not the true cause in the differences between series from different clusters. To show how d L can produce better clus-ters than d E we cluster an ECG data set first used in [5]. The data consists of 70 series of ECG measurements of patients with malignant ventricular arrhythmia (V), normal (N) or superventricular arrhythmia (S). Table 3 shows the clustering accuracy results with both d L and d E distance measures. In each cell of Table 3 the first number is the k-means accuracy and the second that achieved with PAM. The results are comparable to those reported in [5] for PAM with FFT and Euclidean distance. Although there is variation in performance of the clus-tering algorithms, in all but two cases (were the performance was equal) using d
L rather than d E gave a higher accuracy. From the dendrograms (not shown because of space restrictions, see [4])the level three clusters formed with nearest neighbour linkage demonstrate the superiority of the d L distance measure. The clusters formed by d E are (N,V,N,V,S,S,S), (V,V,S) and (V,V,S,N,C,V,N). The clusters formed by d L , (V,V,S,V,V,S,V), (N,N,N,N,S) and (S,N,S,S) are much closer to the correct classification. 3.3 Motor Current Data The simulated motor current data set used in [8] consists of 420 series of length 1500. We repeatedly randomly selected two pairs of series from different classes and measured the statistics D and C described in Section 3. The average D value (percentage difference of within class and between class distance) for d L was 82.83%, whereas with d E the average within distance was actually higher than the between difference (D=104.43%, averaged over 5000 repetitions). The number correct, C , was also higher for d L (C=850) than with d E (C= 507). The superiority of d L is also evident when accuracy is measured with a 1-nearest neighbour classifier on the first 16 coefficients. d E gave a classification accuracy of 14%, whereas d L achieved only 9.8%. These accuracies are better than those reported for a time delay neural network in [8]. In this paper we have described an alternative distance metric for use with FFTs in time series data mining. The metric, d L , is based on the likelihood ratio for testing the null hypothesis that the series are from the same process. It has the desirable property of asymptotically following a known distribution and of not being overwhelmed by small variations in the larger coefficients. We have shown this is true for simulated and real world data.

