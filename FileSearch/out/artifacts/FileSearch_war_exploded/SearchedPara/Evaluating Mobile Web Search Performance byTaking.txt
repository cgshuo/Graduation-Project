 Usage of mobile devices for Web search grows rapidly in recent years. The common tendency is that users want to receive information immediately results in incorporating rich snippets and vertical results into search engine result pages (SERPs) and in increasing of good abandonment. This arti-cle provides an offline metric for quality evaluation of mobile Web search, which takes good abandonment rate into con-sideration. The metric is the DBN click model that allows the probability to be satisfied directly on the SERP. The model parameters are estimated from the mobile search logs of a controlled experiment. The new metric outperforms traditional ERR metric in terms of the validation dataset built using a SERP degradation technique.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: Click models; evaluation; information retrieval measures; user behavior; mobile web search
The number of users that use search engines on their mo-bile devices grows rapidly due to mobile web search con-venience and the opportunity to have  X  X nformation at your fingertips X , that it brings to its users. The increase of sat-isfaction of mobile web search users became an important task in information retrieval [5]. In mobile web search rich snippets and different vertical results represent a convenient summary of information about the search results at a glance, so a user does not need to click on SERP X  X  results anymore. As the result is the growth of good abandonment rate for mobile devices. According to Li et al. [8], the potential good abandonments in mobiles is more than 50% of all abandoned queries compared with 30% for desktop searches. The com-mon tendency for good abandonment is that its rate will be continuously increasing in the coming years, since modern search engines aim not only to generate good and attractive snippets, but also to show snippets with the actual answers to satisfy the user before any further clicks become neces-sary.
 It was shown that user behaviour of conducting mobile Web search significantly differs from search behaviour on a desktop [7]. In [3] a number of click model-based offline metrics are constructed and compared with traditional of-fline metrics like ERR, DCG or Precision and online metrics. Also in [3] it was shown that metrics based on click models like Dynamic Bayesian Networks (DBN) [1] and User Brows-ing Model (UBM) [6] are better correlated with online mea-surements than traditional metrics. In order to model good abandonment we propose a new metric based on the DBN model, but allowing for the probability that the user finds the relevant information (for example, the correct answer) directly on the SERP (Section 2). To evaluate parameters of our metric we conducted a controlled user experiment on Android devices (Section 3). One of the methods for com-paring information retrieval metrics is the swap method pro-posed by Voorhees and Buckley [10] and extended by Sakai [9]. They evaluate metrics consistency comparing pairs of systems without knowing whether a system is better than another. But this method could prefer a metric inconsistent with some online experiment results. Since the agreement between offline and online evaluation is the desirable result for a new offline metric [3], we are interested whether the offline metrics decision is consistent with the online experi-ment X  X  results. Unlike Sakai and Voorhees, Buckley, we com-pare performance of the proposed metric with ERR metric [4] on pairs of systems with predefined system differences using a SERP degradation technique (Section 4). We propose a click model based on the Dynamic Bayesian Network [1], but which accounts for the probability that the user finds the correct answer directly in a snippet and which also assumes that the probability to get tired and stop search before finding some relevant information depends on the ac-tion on the last examined snippet (click or examination). The reasons to consider the probability to get the correct answer from a snippet are the following: at first, queries in-dicating good abandonment represent a large subset of all abandoned queries [8]. The second, good abandonment rate on mobiles search is significantly higher than on PC search [8]. The proposed model is described below. For a given position i of the SERP the following variables are defined: Figure 1: Distribution of relevance labels by rank for four commercial search engines The following equations describe the model:
Unlike the DBN model our model has variables sa i (5), which represent the probability of finding the correct answer in the corresponding snippet, and two variables y 1 (8) and y (9), representing the probabilities to continue the search being unsatisfied after only examining a snippet and after a click on a snippet. The inclusion of different y variables in case of click and examination is based on the assumption that navigating to a non-relevant document is more difficult on mobile device and is more irritating for a user compared with only an examination of such a document. As in the DBN model [1] we assume that the user browses SERP from the top to the bottom (1) and she always examines the first snippet of the SERP (2). There is no chance to click on a snippet without its examination (3) and no chance to be satisfied without the snippet examination (4). After the user examines a snippet i , there is a probability sa i to be satisfied by the answer from this snippet (5) and there is a certain probability ac i to be attracted by the snippet and to click on it (6). If the user finds the snippet not attractive and decides not to click on it, there is probability 1  X  y abandon search after examination (8). After the user clicks and visits a document, there is a certain probability s i she will be satisfied by this document (7). Once the user is satisfied by the document she has visited, she stops her search. If the user is not satisfied by the current result, there is certain probability 1  X  y 2 to abandon the search(9). Parameters sa i , ac i depend on snippet attractiveness and s depends on document relevance.
To build a dataset for parameters evaluation we conducted a controlled user experiment. The experiment was con-ducted in the mobile Android devices in May 2013. To Figure 2: Distribution of attractiveness labels by rank for four commercial search engines collect user data, we developed a plugin for the Dolphin browser for Androids. Ten volunteered (unpaid) partici-pants were recruited for this user study. All volunteers were undergraduate and graduate students and all had some experience with Web search on mobile devices with touch screens. In order to create a collection of mobile search tasks, we randomly sampled 200 unique queries with clicked documents from the Yandex mobile search query logs. Based on them we prepared 200 search tasks similar to mobile Web search users tasks. Below there are some examples of them: Participants had to complete these search tasks in four com-mercial search engines: Yandex, Google, Bing, Mail. Ev-ery subject had completed from 10 to 186 tasks. To begin each task the participants were presented with a task de-scription. More than one query per task was allowed and there were no restrictions on time or participants actions on search pages. The participant X  X  actions such as queries, clicks, scrolls, touches etc. were logged. Each time the par-ticipant completed the task, she was asked (through a popup window) whether or not she solved the task.

Eventually, each query was labeled as the one which led to user X  X  satisfaction or not (End+/End-). If the user is-sued more than one query to complete her task, the last query could have End+ label, and all previous queries were always labeled as End-. In total we obtained 809 queries with user actions and labels, and 530 clicks were done on the SERPs. The dataset for model parameters estimation was composed of the queries, corresponding SERP docu-ments (including organic and vertical results), users X  actions on every document (click or examination) and users X  labels to queries. For every SERP from the dataset the relevance labels were also collected. Those labels were separately col-lected for documents (relevance judgments), their snippets (attractiveness judgments). Navigating convenience on mo-bile device was taken into consideration in relevance judge-ments, i.e. relevant but hardly accessible documents were labeled less relevant. The following relevance scale from [4] with the exception of  X  X unk X  label were used:  X  X av X ,  X  X ey X ,  X  X Rel X ,  X  X el X ,  X  X on X . Figure 1 shows the distribution of relevance judgements for four commercial search engines by document ranks. To judge every snippet a subject had to answer following questions: http://dolphin-browser.com/ and test group (SERP degradation) Figure 2 shows distribution of snippet judgements for four commercial search engines by document rank. In compari-son to relevance judgements, distribution of snippet labels per document rank does not so clearly depend on the rank it-self. Interestingly, judgements  X  X nswer X  represent only 10% of all judgements.
For every SERP from the dataset we have a sequence of clicks, the user X  X  success label and documents and snippets relevance judgements. Using the assumption that browsing goes from the top to the bottom and the fact that user can-not be satisfied with the answer from the snippet labeled as  X  X o answer X , at the same time we know the meaning of all possible user action sequences on a judged SERP. By taking this fact into consideration we estimated the parameters of the model using MLE method. For example, in the case of a three-document SERP with the following snippet judge-ments:  X  X o answer X  +  X  X lick X ,  X  X nswer X  +  X  X o click X  and  X  X o answer X  +  X  X o click X , the user X  X  label  X  X nd+ X  and a click on the first document we could have only these two variants of user behavior on the SERP: Due to the lack of data for estimation we set y 1 in [1] and y 2 = 0 . 8. 8-fold cross-validation and bootstrap method were used to evaluate the rest of the parameters and the corresponding confidence limits. Using the estimated parameters of the proposed model we construct a new metric -the measure of user satisfaction Psat at rank k . P ( E i +1 = 1) = P ( E i = 1)(1  X  sa i )[(1  X  ac i ) y 1
Parameters sa i and ac i depend on the label of snippet i (6 parameters: 4 ac i for snippet labels from the Figure 2 and 2 sa i for  X  X nswer X  labels) and s i depends on the label of the document i (5 parameters for document labels from Figure 1).
To validate the new Psat metric we conduct a number of offline experiments. For this purpose we collected a metric validation dataset based on the results of online experiments. These online experiments were A/B experiments with a con-trol bucket of users served with the results generated by a high quality results and another bucket of users served with degraded snippet generation algorithm, degraded ver-tical results generation algorithm or a degraded ranking al-gorithm. These degradations were verified as to indeed de-grade search experience by different online metrics (dwell time, CTR, time to first click, measured in the scope of the above-mentioned online expeirments). For the offline valida-tion dataset we sampled 1056 SERPs of Android users from a Yandex interaction log collected in in November 2013. This set of SERPs was  X  X ontrol SERP group X  for the offline ex-periment. For this set of SERPs we have made the same verified degradations as in the online experiments and ev-ery set of degraded SERPs was a  X  X est SERP group X . The set of resulting control and test group pairs formed our of-fline validation dataset. In the dataset there are 7 snippet degradations, 5 vertical results degradations and 2 ranking algorithm degradations. For the control and all test SERP groups the same types of judgements as in Section 3 were collected. We use this dataset to compare our metric with ERR metric [3] to evaluate their ability to detect degrada-tions of different types and intensity.
In this section we describe the degradations used to form our validation dataset in more detail.

A snippet ranking algorithm generates a ranked list of candidate snippets for each document. The best snippet from this list is the snippet that is shown to user. For every search result from SERPs we collected two variants of bad snippets. First is a random snippet from the list of possible snippets for the document. Second is a snippet from the middle of candidate snippets ranked by their quality.
We have done following snippet degradations: We imitated the following document ranking degradations: Also it was shown, that users might be satisfied only by ver-tical results and end the whole search session [2], therefore removing them from SERP means its degradation. Consid-ering these facts we imitated the following degradations: We mentioned this type of snippet degradation separately, because we did not conduct the corresponding verifying on-line experiment, but we strongly believe that removing a cor-rect answer from the snippet is a degradation for user. In our SERPs we replaced snippets judged as  X  X nswer X  snippets with  X  X o answer, Click X  snippets. Group of SERPs without  X  X nswer X  were an additional test group in our dataset.
Our final dataset consists of the set of SERPs with degra-dations of different types. We choose different sample sizes to look at Psat metric performance and to compare it with ERR performance. Sample of every size was resampled 1000 times with replacement. The percentages of times when metric detects a significant difference between a pair of SERPs (initial control non-degraded SERP vs degraded SERP) are summarized in Table 1. Star (*) indicates a sig-nificant difference at level  X  = 0 . 01 between two metrics. The paired permutation test was used to evaluate the sig-nificance of differences. On sample sizes more then 200 Psat outperforms ERR, especially it is evident for the degrada-tions  X  X emove all vertical results X  and  X  X or 50% of queries remove all vertical results X . It is worth to mention that be-tween two snippets degradations, snippet replacement with a  X  X iddle X  snippet is stronger worsening (on N=1000: 99% vs 81%) than the replacement with a  X  X andom X  snippet. It seems to be true, because a random snippet has more chances to be not bad than a snippet from the middle of the snippet ranking set. For every sample size, removing correct answers from snippets is a significant degradation in 100% cases, according to Psat. The sample size of 1000 is enough to detect 100% and 80% snippet degradations with snippet from the  X  X iddle X .
Good abandonment rate will grow in the future due to the tendency to incorporate reach snippets and vertical re-sults into the SERP. In this paper, we have proposed an of-fline metric that takes into account the chance that the user finds an answer in snippet without reading the document. Our metric is able to detect both the ranking algorithm X  X  degradations and the snippet algorithm X  X  degradations bet-ter than ERR metric. In addition we want to note that our metric Psat can signal about excluding of a correct an-swer from the snippet. It is worth to mention that the rate of good abandonments is high for desktop web search too, therefore the new metric could also be evaluated for PC web search.

One of our next target is to investigate the dependence of the probability of being tired from the type of the snippet (with or without a correct answer) and the type of queries. There is an assumption that after issuing a difficult query, the user is more patient than after the easy one, because in the second case he expects to find the answer more quickly. The current model is based on the assumption of linear ex-amination (from the top to the bottom) which is very lim-iting and extending the model to let it deal with non-linear examinations could give better results.
