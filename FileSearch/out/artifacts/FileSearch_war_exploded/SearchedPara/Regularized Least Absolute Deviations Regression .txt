 widely used techniques for data analysis. However, sometimes people are not satisfied with it because of the following two limitations: 1) its results are s ensitive to outliers, so when the error terms are not normal ly distributed, especially when they have heavy-tailed distributions, linear regression often works badly; 2) its estimated coefficients tend to have high varian ce, although their bias is low. To reduce the influence of outliers, robust regression models were developed. Least absolute deviation (LAD) regression is one of them. LAD minimizes the mean absolute errors, inste ad of mean squared errors, so its results are more rob ust. To address the second limitation, shrinkage methods were proposed, which add a penalty on the size of t he coefficients. The LASSO is one of these methods and it uses the L1-norm penalty, which not only reduces th e prediction error and the variance of estimated coefficients, but also provides an automatic featur e selection function. In this paper, we propose the regularized least absolute deviation (RLAD) regress ion model, which combines the nice features of the LAD and the LASSO together. The RLAD is a regularizatio n method, whose objective function has the form of  X  X  oss + penalty. X  The  X  X oss X  is the sum of the absolute deviations and the  X  X enalty X  is the L1-norm of the coefficient vector. Furthermore, to facilitate para meter tuning, we develop an efficient algorithm which can solve the entire regularization path in one pass. Simulations with various settings are performed to demonstrate its performance. Finally, we apply the algorithm to solve the image reconstruction problem and find interesting results. widely used techniques for data analysis. Let Y be the response variable and p R x  X  be the vector of p independent predictor variables, linear regression assumes the following relationship between Y and x : where  X  is normally distributed. To estimate the coefficients, suppose n samples are obtained. Let variable and nxp R X  X  be the matrix, where each column corresponds to a predictor and each row corresponds to a sample. When  X  has normal distribution with constant variance and is independent of x , the optimal estimator of 0  X  and  X  can be obtained by minimizing the 2 norm of the residuals: (  X   X  ,  X   X  ) is also called the ordinary least square (OLS) estimator. For simplicity, let ] ,1[ X X column vector with all the components being 1. The OLS estimator is calculated from: its results are very sensitive to outliers. One way to solve this problem is to identify the outliers via diagnosis and discard them. However, when y does not have normal distribution, e.g., heavy-tailed distri bution, simply discarding outliers might not be a good solu tion. Another way to remedy this problem is to use robust methods. Robust methods minimize other functions of residuals, instead of the 2 L norm function. Least absolute deviation (LAD) regression is one of the robust methods and the LAD estimator is obtained by minimizing the 1 L norm of the residuals: Figure 1 shows the loss functions used in linear regression (in dashed line) and LAD method (in soli d line). It can be seen from the plot that outliers i n response variable have much less influence for LAD. Several studies [2, 3, 5, 7, 9] have confirmed that LAD estimator is more robust than the OLS estimator, especially when the response variable has heavy-tai led distribution and observed values for predictor vari ables are not contaminated by noise. 
Figure 1. Loss functions for linear regression estimated coefficients often have low bias but larg e variance [10]. From (2), it can be seen that to cal culate the OLS estimator the inverse matrix of X X T must be solved. Therefore, when X X T is ill-conditioned, which happens when some predictors are correlated, the OLS estimator becomes very sensitive to values in X . Shrinkage methods are proposed to address this issue. By trading off a little bias, the shrinkage methods can reduce the variance of the estimator and improv es the overall prediction accuracy. The LASSO [10] is one of the shrinkage methods and it solves the following optimization problem: Note that this is a regularization method, which ha s the form of loss + penalty . Imposing the 1 L norm penalty on the coefficient vector has the nice property tha t some coefficients can be shrunk exactly to 0 when  X  is set appropriately. Therefore, an automatic feature selection can be achieved by the LASSO and the selection process is continuous in  X  . together and propose the regularized least absolute deviation (RLAD) regression model, which solves the optimization problem below: The RLAD model inherits the nice properties from both methods and its benefits are three-fold: 1) by using the robust loss function, its results are not sensitive to outliers; 2) by using the both the variance of the RLAD estimator and the prediction error are reduced; 3) it can perform automatic feature selection, where the coefficients for irrelevant features are shrunk to zero. regularization methods, since it controls the compromise between loss and penalty . Therefore, its value must be chosen carefully, because changing  X  can often result in estimators with significantly different prediction accuracy. However, there is us ually no good way to determine the best value of beforehand, so people often need to specify a large number of  X  values and test each of them. This parameter tuning process is usually computationally expensive and it does not guarantee that the best value can be discovered. Thus, although problem (5) can be solved by calling a linear programming (LP) solver, in practice it is not efficient, because th e solver must be invoked extensively during the parameter tuning process. To facilitate parameter tuning, we develop an efficient algorithm, which can solve the entire regularization path in one pass. In other wo rds, the algorithm can give the solutions for every poss ible value of  X  .
 describes the algorithm; section 3 uses simulations to demonstrate its performance; section 4 applies the algorithm for solving the image reconstruction problem; section 5 presents the conclusions and describes the future work. equivalently transformed into: min In this problem, s is the regularization parameter (a positive constant) and it controls the size of the coefficient vector. Two problems are equivalent in the corresponding  X  &gt;0 in (5), such that two problems have the same objective value. constraints, from LP theories we know that the opti mal solutions are piecewise linear functions of s . For readers who are not familiar with LP, this point ca n also be seen from the following derivations. The ba sic idea of our algorithm is to continuously increase s and solve the solutions on this regularization path, un til the objective value can not be further decreased. Lagrangian multipliers. The Karush-Kuhn-Tucker (KKT) conditions are:  X 
L  X   X   X   X 
L  X  (9) 
L  X   X  For simplicity, we define: From (10), we can get 1 1  X   X   X  first constraint in (6) we know 0 &gt; we can get 0 =  X  . Therefore, from (10) 1 = + Similarly, rules for L i  X  can also be derived. When  X  , which conflict with (10). These derived rules are listed as follows: Theorem 1 When  X  &gt;0 and 0 | | &gt; V , t he optimal solution of problem (6) has | || | V E = .
 Proof : If  X  i L or R , then variables, Thus, we can get | || | V E  X  ; otherwise, generally speaking, (8) and (9) can not hold. there are | V |+1 variables, equations. Similarly, we can get | || | E V  X  . continuity reasons, E and V can not change immediately, so we can get the following linear sys tem: It can be seen that Therefore, when E and V are fixed, their derivatives w.r.t. s can be calculated as below: Since |E|=|V| , linear system (16) has a unique solution given that the corresponding matrix is nonsingular. With derivatives for residuals. Let ) ( represent the residual for point i , then its derivative w.r.t. s is: one of them might reach 0 as s increases. In that case, we say an event happens. For each point E i  X  , we can determine when it will hit 0 by calculating: to  X  + , which means the point moves further away can be calculated as: predictor variable in V has its coefficient reduced to zero. For each V j  X  , we calculate: it to  X  + . The step size for this event is: Therefore, the overall step size is: If 1 s  X  &lt; 2 s  X  , then we remove the corresponding point from L or R and add it into E ; otherwise ( 1 s  X  &gt; we remove the predictor variable from V . So, coefficients and residuals should be adjusted as follows: matter which event has happened, 1 | || | + = new new which conflicts with Theorem 1. Thus, we need to either remove another point from E or add a new variable into V to make them balanced. It can be determined by solving the dual variables using (8) and (9). event happens, and let new (9) become:  X   X   X   X   X   X   X  In linear system (20), there are | | new E +1 variables and 
V +1 equations. Since 1 | || | + = new new V E , there is one degree of freedom. It can be seen that derivative of i  X  w.r.t.  X  : In linear system (24), there are unique solutions f or  X   X  d is nonsingular. Note that the process of increasing s corresponds to reducing  X  , so  X  decreases throughout the algorithm. Therefore, when updating variable i  X   X  X  with  X  value. from E and added into L ; on the other hand, if new reaches 1, the point should be added into R . Therefore, Note that only one of them can be negative. We defi ne: From (9), it can be seen that for a zero-valued pre dictor j ( V j  X  ), if then j should be included in V ( Therefore, for each V j  X  , we can calculate: And we define j  X  ~  X  (26) The sign of j  X  ( V j  X  ) can be determined by: Note that i  X   X   X  X  ( new E i  X  ) and negative, so the step size  X   X  should be: If  X   X  equals to one of corresponding point will be removed from E ; if  X   X  equals to one of predictor should be added into V . After the adjustment, E and V will have the same number of elements, so we can calculate the step size for s using (16) to (19) again. The entire process is repeated until one of the following stopping criteria is met: The algorithm starts from s= 0, which corresponds to and  X  = V at this time. Let observations for the response variable in ascending order. If n is odd, let * k be the index corresponding to y . In this case, even, let 1 k and 2 k be the indices corresponding to y and them can be added into E , we arbitrarily select 1 k and let determined from residual values. For L i  X  , 1  X  = i odd, * join V , but its magnitude still keeps 0. The index and sign of this variable are determined by: At this point,  X  reduces from  X  to | | can be seen from (9). Now, we have | E |=| V |=1 and the algorithm can proceed from here. path for a simple data set. The data consist of 20 samples and 10 predictor variables, among which 3 a re true predictors. The horizontal axis represents the regularization parameter s and the vertical axis represents coefficient values. True predictors are plotted in solid lines and noise variables are plot ted in dashed lines. The vertical dotted lines indicate wh ere an event happens. From the figure, it can be seen t hat the continuous feature selection is achieved by changing s . 
Figure 2. The solutions on the regularization 
Step 1: Solve s  X  using (16) to (22); update s , with size 1 | | + k V (or 1 | | + k E ), where k
E represent set V and E at step k . So, the computational complexity at each step seems to be systems at two consecutive steps, two matrices are differed by only one row or column. Therefore by inverse updating and downdating, the computational complexity at each step can be reduced to ) | (| 2 k are on the entire regularization path for an arbitr ary problem, but our experience suggests that )) (log( np O is a reasonable estimate. Therefore, the overall computational complexity for the RLAD algorithm is and number of steps for solving the entire regularization path with various n (sample size) and p (number of predictors). We implement the algorithm in R , but since the inverse updating and downdating par t has not been implemented, the computational complexity of our code is ) ) , min( ) (log( 3 n p np O now. The experiments were conducted on a laptop wit h 1.66 GHz CPU and 1 GB memory. We repeat each setting 20 times and every time X and y are randomly generated. The figures in the following tables repr esent the average computational time based on 20 replicat es and figures in parenthesis indicate the average num ber of steps. executing programs compared with compiled languages and our code has not implemented inverse updating/downdating; however, from the following tables it can be seen that the computational time i s basically acceptable for moderately large problems. The computational time can be significantly reduced after it is coded in compiled languages with invers e updating/downdating being implemented. According to our experience, solving a problem with the same siz e using simplex method often takes more time; the interior-point method is much faster, which takes around 1/10 of the time used by the RLAD algorithm. However these methods can only give the solution fo r a single value of s , while our algorithm solves the entire regularization path, which often consists of hundre ds of solutions. For regularization method, the value of regularization parameter is always crucial to the prediction performance of the model, so the RLAD algorithm is advantageous given the solutions it provides and the time it uses. the prediction performance of our algorithm as well as its feature selection function. For simplicity, we ignore the intercept standardized. We assume the predictor vector x has multivariate normal distribution ) ,0(  X  r N and the  X  is noise. First, we generate n data points as training data and run the RLAD algorithm. Then 20,000 testin g data are generated using the same distribution and prediction errors of the obtained models can be calculated, which are measured by mean absolute deviations. Since the size for the testing data is large enough, we can treat it as the entire population an d the associated testing error as the true prediction err or. Also, since we know the true model  X  , we can calculate the optimal prediction error for comparis on purpose. Figure 3 shows the models (coefficient values) on the regularization path and prediction e rrors associated with these models. The solid curve represents the prediction errors on the testing dat a. The solid vertical line indicates where the minimal prediction error is achieved. The dashed horizontal line represents the error obtained from the true model. And the dotted curves are coefficients associated with different values of s . Therefore, it can be seen that the best model generated by the RLAD algorithm has its testing error pretty close to the optimal error. 
Figure 3. Models on the regularization path population for model selection. In practice, this c an not be the case. For real problems, we usually divide t he data into two parts, run the algorithm on one part and test the models on the other; we can also use the c ross-validation or GACV [12] approach. However, these methods mix the model generation and model selectio n together, which add uncertainty to the final result s. In other words, these methods would not allow us to te ll whether the poor prediction performance comes from bad models or the model selection process. Therefor e, our experiment setting can precisely measure the performance of the models without interference of t he model selection process. follow the multivariate normal distribution ) ,0(  X  where Thus, the first q variables are correlated and the rest are independent to each other. The response variable is calculated as  X   X  + = T x y , where The first L variables are true predictors and  X  is noise. For  X  , we use both the normal distribution ) ,0(  X  N and the double exponential distribution with parameter  X  in our experiments. The following tables show the experiment results for various settings. F or each setting, the experiment is repeated 20 times. The figures in the tables are mean values and figures i n the model and the best model obtained from the RLAD represents the number of nonzero predictors that ar e predictors which are identified by the RLAD model. double exponential noise and independent predictors vs. highly correlated predictors. From the results, we can see that the performance of the RLAD algorithm is not affected much by the noise distribution. Its performance is not affected by the existence of hig hly correlated predictors, either. Multicollinearity is a big problem for linear regression. When predictors are highly correlated, linear regression tends to gener ate erroneous p -values, so true predictors can not be correctly identified. However, nearly all the true predictors are discovered by the RLAD algorithm whe n multicollinearity problem exists. The performance i s extremely low. This situation is well known to be difficult and there has not been a good solution fo r solving it. Linear regression can not work when the re are more predictors than the sample size. However, on average the RLAD algorithm can capture 60~90% of the true predictors and its prediction error is acc eptable even when p is significantly larger than n . 
Table 6. Results for normal noise, correlated 
Table 7. Results for normal noise, correlated Table 8. Results for double exponential noise, Table 9. Results for double exponential noise, captured or transmitted. In this section, we apply the RLAD algorithm to solve the image reconstruction problem. The problem has been well studied and ther e are several categories of methods. When the distribution of the image is unknown, smoothing techniques [1,4] are often used. However, when peop le have prior knowledge about this distribution, princ ipal component analysis (PCA) method [6, 8, 11] is often preferred. In PCA method, the contaminated image is projected onto the space of principal components, which are calculated from the image database. Our experiment data come from the MPI face database, which contains 200 face images. and contaminate 20% of its pixels. For each selecte d pixel, we assume its information is completely damaged, so its color is set to white. With this se tting, the noise distribution is not normal, but heavy-tai led. As shown later, since the RLAD algorithm uses the robust loss function, its performance is not affect ed. Figure 4 shows the original image and the corrupted one. It can be seen that with 20% contaminated pixe ls the original face can hardly be recognized by visua l inspection. Figure 4. The true and the corrupted images principal components (PC). Figure 5 shows the image s for some of them. The PCs are orthogonal to each ot her and each of them can capture some characteristics f or a face. The original image will be reconstructed by summing the weighted PCs together. Since the first 20 eigenvalues are significantly larger than the rest, their corresponding PCs are used in our experiment. Figure 5. The 10 largest principal components projection and the idealistic method. In the least square projection, the corrupted image is projected to the PC space in the sense that the residual sum of squares is minimized. For the idealistic method, since we know the exact locations for noise pixels, we can remove them and project only the good pixels onto the corresponding parts of the PC space. Then the obtai ned weights are used for reconstructing the whole image . This represents the idealistic situation, because i n reality the locations for noise can not be easily determined. Therefore, the image generated by the idealistic method is often used as the golden standard for image reconstruction. From our literature revie ws, this standard can not be achieved by existing metho ds. In [11], the proposed method can arrive at the midp oint between the least square projection and the idealistic method. Figure 6 shows the results obtained by the least square projection and the idealistic approach , respectively. 
Figure 6. The reconstructed images by the represent the corrupted image and X be an n by p matrix, whose columns correspond to the p principal components. The principal components are standardized in our experiments. Inspired by [11], which uses the the penalty, we propose the following model for ima ge reconstruction: where ] [ and p , respectively. Therefore, the RLAD algorithm can solve its solutions for every value of s . components of  X  correspond to the noise pixels.  X  is used to identify the relevant PCs, where the zero-valued components of  X  eliminate the irrelevant PCs. With this setting, for each s we can obtain two reconstructed images:  X   X  y and  X   X  X + y is generated in a pixel-by-pixel fashion, where noise pixels are automatically identified and only these pixels are modified. Image  X   X  X + completely from the PCs. The RLAD algorithm is use d to solve problem (29). However, we did not solve th e entire regularization path. Once noise pixels are n ot visually observable from the output images, we stop the program. Figure 7 shows the image series on the regularization path. The horizontal axis represents the number of steps used, which has been standardized. with each other when the algorithm is checking whic h one should be added into the active set V for the next step. This is called the degeneracy problem in line ar programming. When it occurs, we have two choices: randomly picking a variable or picking one with the smallest index. Figure 7 (a) and (b) show the  X   X  y images corresponding to these two choices. It is interesting to note that they generate completely different denoising behaviors. In (a), the noise de nsity is gradually reduced until all of the noise pixels are eliminated, but before that we can not find a zone which is completely noise free. However, in (b) an interesting phenomenon can be observed, where noise pixels are removed from top to bottom, since pixels on top have smaller indices. Figure 7 (c) shows the im ages for  X   X  X + are smooth and have high quality from the beginning of the algorithm, but they do not change much as the algorithm proceeds. On the other hand, in the pixel -by-pixel reconstructed images, noise pixels are gradua lly identified and removed. At the end, both image seri es can achieve pretty good quality, which look exactly the same as the image generated by the idealistic metho d. between the reconstructed images and the true image . The two solid curves represent the MSE for image y and  X   X  X + 0 , respectively. In figure 8 (a), the upper horizontal line (dashed) indicates the performance for the least square projection and the lower horizontal line (dotted) indicates that for t he idealistic method. The  X   X  X + the golden standard, while the  X   X  y images can even exceed it. This is a surprisingly good result, beca use the golden standard can rarely be achieved by previ ous methods. parameter tuning algorithm. The performance of  X   X  y images is sensitive to the value of the regularizat ion parameter. Therefore, the best setting can be easil y missed if we use the trial-and-error approach for parameter tuning. Figure 8. Image Denoising Performance Curve the LASSO method together and propose the RLAD regression model. Its benefits are three-fold: 1) b y using the robust loss function, its results are not sensitive to outliers; 2) by using the both the variance of the RLAD estimator and the prediction error are reduced; 3) it can perform automatic feature selection, where coefficients for irrelevant features are shrunk to zero. To facilita te parameter tuning, we develop an efficient algorithm , which can solve all the solutions on the regulariza tion path. Simulations are designed to demonstrate the performance of the RLAD algorithm. The algorithm is also tested in a real-world application, the image reconstruction problem, and it achieves surprisingl y good performance. selection can not be easily performed. Since there are no assumptions for the noise distribution, asymptot ic theories are difficult to develop. Recall in our simulations, model selection is performed on the en tire population and in the image reconstruction problem it is achieved by visual inspection. Although we can perform model selection using the cross-validation method, it is a computationally expensive method. Therefore, designing a good criterion for selecting model is one of the directions for our future work. Another direction is to adopt other robust function s, such as the Huber function, as the loss function. 
