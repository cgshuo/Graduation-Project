 Contextual bandit algorithms provide principled online learning so-lutions to find optimal trade-offs between exploration and exploita-tion with companion side-information. They have been extensively used in many important practical scenarios, such as display adver-tising and content recommendation. A common practice estimates the unknown bandit parameters pertaining to each user indepen-dently. This unfortunately ignores dependency among users and thus leads to suboptimal solutions, especially for the applications that have strong social components.

In this paper, we develop a collaborative contextual bandit algo-rithm, in which the adjacency graph among users is leveraged to share context and payoffs among neighboring users while online updating. We rigorously prove an improved upper regret bound of the proposed collaborative bandit algorithm comparing to con-ventional independent bandit algorithms. Extensive experiments on both synthetic and three large-scale real-world datasets verified the improvement of our proposed algorithm against several state-of-the-art contextual bandit algorithms.
  X  Information systems  X  Recommender systems;  X  Theory of computation  X  Regret bounds;  X  Computing methodologies  X  Sequential decision making; Collaborative contextual bandits; online recommendations; rein-forcement learning
Satisfying users with personalized information plays a crucial role for online service providers to succeed in market. However, the rapid appearance of new information and new users together with the ever-changing nature of content popularity make tradi-tional recommendation approaches, e.g., collaborative filtering [7, 25], incompetent. Modern information service systems now adopt online learning solutions to adaptively find good mappings between available content and users. During online learning, the need to fo-cus on information that raises user interest and, simultaneously, the need to explore new information for globally improving user expe-rience create an explore-exploit dilemma. Significant research at-tention has been paid on multi-armed bandit algorithms [16, 19, 4, 5], which provide a principled solution of the dilemma. Intuitively, bandit algorithms designate a small amount of traffic to collect user feedback while improving their estimation qualities in realtime.
With the available side information about users or items to be presented, contextual bandits have become a reference solution [3, 10, 21, 14]. Specifically, contextual bandits assume the expected payoff is determined by a conjecture of unknown bandit param-eters and given context, which is represented as a set of features extracted from both users and recommendation candidates. Such algorithms are especially advantageous when the space of recom-mendation is large but the payoffs are interrelated. They have been successfully applied in many important applications, e.g., content recommendation [21, 6] and display advertising [11, 23].

However, a common practice in contextual bandit algorithms es-timates the unknown bandit parameters pertaining to each user in-dependently. This unfortunately ignores dependency among users. Due to the existence of social influence [13], e.g., content and opinions sharing among friends in a social network, exploiting the dependency among users raises new challenges and opportunities in personalized information services. For example, in many real-world applications, e.g., content recommendation in Facebook or Twitter, because of the mutual influence among friends and ac-quaintances, one user X  X  click decision on the recommended items might be greatly influenced by his/her peers. This indicates the knowledge gathered about the interest of a given user can be lever-aged to improve the recommendation to his/her friends, i.e., col-laborative learning. In other words, the observed payoffs from a user X  X  feedback might be a compound of his/her own preference and social influence he/she receives, e.g., social norms, conformity and compliance. As a result, propagating the knowledge collected about the preference of one user to his/her related peers can not only capitalize on additional information embedded in the depen-dency among users, which is not available in the context vectors; but also helps conquer data sparsity issue by reducing the sample complexity of preference learning (e.g., known as cold-start in rec-ommender systems [26]). Failing to recognize such information among users will inevitably lead to a suboptimal solution.
In this work, we develop a collaborative contextual bandit al-gorithm that explicitly models the underlying dependency among users. In our solution, a weighted adjacency graph is constructed, where each node represents a contextual bandit deployed for a sin-gle user and the weight on each edge indicates the influence be-tween a pair of users. Based on this dependency structure, the observed payoffs on each user are assumed to be determined by a mixture of neighboring users in the graph. We then estimate the bandit parameters over all the users in a collaborative manner: both context and received payoffs from one user are prorogated across the whole graph in the process of online updating. The proposed collaborative bandit algorithm establishes a bridge to share infor-mation among heterogenous users and thus reduce the sample com-plexity of preference learning. We rigorously prove that our collab-orative bandit algorithm achieves a remarkable reduction of upper regret bound with high probability, comparing to the linear regret with respect to the number of users if one simply runs independent bandits on them. Extensive experiment results on both simulations and large-scale real-world datasets verified the improvement of the proposed algorithm compared with several state-of-the-art contex-tual bandit algorithms. In particular, our algorithm greatly alle-viates the cold-start challenge, in which encouraging performance improvement is achieved on new users and new items.
Multi-armed bandit algorithms provide principled solutions to the explore/exploit dilemma, which exists in many real-world ap-plications, such as display advertisement selection [11, 23], recom-mender systems [21, 6], and search engine systems [24, 28]. As opposed to the traditional K -armed bandit problems [5, 4, 16, 19], feature vectors in contextual bandits are created to infer the con-ditional expected payoff of an action [3, 12, 20, 21]. The setting for contextual bandit with linear payoffs was first introduced in [3], where the expectation of payoff for each action is assumed to be a linear function of its context vector. In the follow-up research [21, 12], LinUCB is introduced to use ridge regression to compute the expected payoff of each action and corresponding confidence inter-val. Later on, generalized linear models are introduced to param-eterize bandit algorithms for non-linear payoffs [14]. Comparing to their context-free counterparts, contextual bandits have achieved superior performance in various application scenarios [21, 14].
The idea of modeling dependency among bandits has been ex-plored in prior research [8, 9, 10, 15, 17]. Studies in [2, 27] explore contextual bandits with assumptions about probabilistic dependen-cies on the product space of context and actions. Hybrid-LinUCB [21] is such an instance, which uses a hybrid linear model to share observations across users. Social network structures are explored in bandit algorithms for introducing possible dependencies [9, 15]. In [8], parallel context-free K -armed bandits are coupled by the social network structure among the users, where the observed pay-offs from neighboring nodes are shared as side-observations to help estimate individual bandits. Besides utilizing existing social net-works for modeling relatedness among bandits, there is also work automatically estimates the bandit parameters together with the de-pendency relation among them, such as clustering the bandits via the learned model parameters during online updating [15]. Some recent work incorporates collaboration among bandits via matrix factorization based collaborative filtering techniques: Kawale et al. preformed online matrix factorization based recommendation via Thompson sampling [18], and Zhao et al. studied interactive col-laborative filtering via probabilistic matrix factorization [29].
The most similar work to ours studied in this paper is the GOB.Lin algorithm introduced in [10]. GOB.Lin requires connected users in a network to have similar bandit parameters via a graph Laplacian based model regularization. As a result, GOB.Lin explicitly re-quires the learned bandit parameters across related users to be close to each other. In our algorithm, we do not have such strong assump-tion about each individual bandit, but we make explicit assumptions about the reward generation via an additive model: neighboring users X  judgements of the recommendations will be shared across, i.e., word-of-mouth, to explain the observed payoffs in different users. This gives us the flexibility in capturing the heterogeneity of preferences among different users in practice, and leads to both theoretically and empirically improved results.
We develop a contextual bandit algorithm in a collaborative en-vironment, where the adjacency graph among users is leveraged to share context and payoffs between neighboring bandits during online update. We provide rigorous proof of the resulting upper re-gret bound, which has a significant regret reduction comparing to running independent bandit algorithms on the same collection of users. In the following discussions, we will first describe the no-tations and our model assumptions about the collaborative bandit problem, then carefully illustrate our developed bandit algorithm and corresponding regret analysis.
We consider a contextual bandit problem with a finite, but possi-bly large, number of arms, which correspond to the candidate item set to be presented (e.g., articles in a content recommendation sys-tem). We denote the arm set as A and the cardinality of A as K . There are N different users in this collection. At each trial t , a learner observes a given user u t from user collection and a sub-set of arms from A , where each arm a is associated with a feature vector x a t ,u t  X  R d summarizing the information of both user u and arm a t at trial t . The learner displays an arm a t ceives the corresponding payoff r a t ,u t from the user. The goal of the learner is to update its arm-selection strategy with respect to the new observations, such that after T trials its regret with respect to the oracle arm selection strategy is minimized. In particular, the accumulated T -trial regret for all users is defined formally as, where a  X  t is the best arm to display to user u t at trial t according to the oracle strategy, r a  X  R t is the regret from all users at trial t .

In standard linear contextual bandit problems, the payoffs of each arm with respect to different users are assumed to be governed by a noisy version of an unknown linear function of the context vec-tors [3, 21]. Specifically, each user i is assumed to associate with an unknown parameter  X  i  X  R d (with k  X  i k  X  1 ), which determines a Guassian distribution N (0 , X  2 ) .  X  s are independently estimated based on the observations from each individual user. However, due to the existence of mutual influence among users, an isolated bandit can hardly explain all the observed payoffs even for a single user. For example, the context vectors fail to encode such dependency. To capitalize on the additional information embedded in the depen-dency structure among users (i.e.,  X  for different users), we propose to study contextual bandit problems in a collaborative setting.
In this collaborative environment, we place the bandit algorithms on a weighted graph G = ( V,E ) , which encodes the affinity rela-tionship among users. Specifically, each node v i  X  { V 1 in G hosts a bandit parameterized by  X  i for user i ; and the edges in E represent the affinity relation over pairs of users. This graph can be described as an N  X  N stochastic matrix W . In this matrix, each element w ij is nonnegative and proportional to the influence that user j has on user i in determining the payoffs of different arms. w ij = 0 if and only if user j has no influence on user i . W is column-wise normalized such that P N j =1 w ij i  X  { 1 ,....,N } . In this work, we assume W is time-invariant and known to the learner beforehand.

Based on the graph G , collaboration among bandits happens when determining the payoff of a particular arm with respect to a given user. To denote this, we define a d  X  N matrix  X  , which consists of parameters from all the bandits in the graph:  X  = (  X  1 ,...,  X  N ) . Accordingly, we define a context feature matrix Algorithm 1 Collaborative LinUCB 1: Inputs:  X   X  R + ,  X   X  [0 , 1] , W  X  R N  X  N 2: Initialize: A 1  X   X  I , b 1  X  0 ,  X   X  1  X  A  X  1 1 b 1 , C 3: for t = 1 to T do 4: Receive user u t 5: Observe context vectors, x a t ,u t  X  R d for  X  a  X  X  6: Take action a t = arg max a  X  X   X  X T a t ,u t vec ( 7: Observe payoff r a t ,u t 12: end for X t = ( x a t, 1 ,..., x a t,N ) , where the i th column is the context vec-tor x a t,i for arm a at trial t selected for user i . The collaboration among bandits characterized by the influence matrix W results in a new bandit parameter matrix  X   X  =  X W , which determines the payoff r a t ,u t of arm a t for user u t at trial t by, where diag u t ( X ) is the operation returning the u t -th element in the diagonal of matrix X . Eq (2) postulates our additive assumption about reward generation in this collaborative environment: the re-ward r a t ,u t is not only determined by user u t  X  X  own preference on the neighbors who have influence on u t (i.e., P j 6 = u This enables us to distinguish a user X  X  intrinsic preference of the recommended content from his/her neighbors X  influence, i.e., per-sonalization. In addition, the linear payoff assumption in our model is to simplify the discussion in this paper; and it can be relaxed via a generalized linear model [14] to deal with nonlinear rewards.
We should note that our model assumption about the collabora-tive bandits is different from that specified in the GOB.Lin model [10]. In GOB.Lin, connected users in the graph are required to have similar underlying bandit parameters, i.e., via graph Lapla-cian regularization over the learned bandit parameters. And their assumption about reward generation follows conventional contex-tual bandit settings, i.e., rewards are independent across users. In our setting, neighboring users do not have to share similar bandit parameters, but they will generate influence on their neighbors X  de-cisions. This assumption is arguably more general, and it leads to an improved upper regret bound and practical performance. Theo-retical comparison between these two algorithms will be rigorously discussed in Section 3.3.
To simplify the notations in our following discussions, we de-fine two long context feature vectors and a long bandit param-eter vector based on the vectorize operation vec (  X  ) . We define X t = vec ( X a t ) = ( x tion of context feature vectors of the chosen arm a t at trial t for all the users. And we define  X  X a t ,u t = vec (  X  X a t ,u is a special case of X a t : only the column corresponding to the user u at time t is set to x T a t ,u t , and all the other columns are set to zero. This corresponds to the situation that at trial t the learner only needs to interact with one user. Correspondingly, we define  X  = vec (  X  ) = (  X  T 1 ,  X  T 2 ,...,  X  T N ) T  X  R dN as the concatenation of bandit parameter vectors over all the users.

With the collaborative assumption about the expected payoffs defined in Eq (2), we appeal to ridge regression for estimating the unknown bandit parameter  X  for each user. In particular, we simul-taneously estimate the global bandit parameter matrix  X  for all the users as follows, b  X  = arg max where  X   X  [0 , 1] is a trade-off parameter of l 2 regularization in ridge regression.

Since the objective function defined in Eq (3) is quadratic with respect to  X  , we have a closed-form estimation of  X  as A t b t , in which where I is a dN  X  dN identity matrix.

The effect of collaboration among bandits is clearly depicted in the above estimation of  X  . Matrix A t and vector b global information shared among all the bandits in the graph. More specifically, the context vector x a t ,u t and payoff r a in user u t at trial t can be propagated through the whole graph via the relational matrix W to other users. To understand this, text vectors on every user, while the original  X  X vector with observations only at active users u t . Because of this information sharing, at certain trial t , although some users might have not generate any observation yet (i.e., cold-start), they can already start from a non-random initialization of their bandit pa-rameters  X  i . It is easy to verify that when W is an identity matrix, i.e., users have no influence among each other, the estimation of  X  degenerates to independently computing N different  X  s (since maximized when W is a uniform matrix, i.e., all the users have equivalent influence to each other. We have to emphasize that the benefit of this collaborative estimation of  X  is not to just simply compute the  X  s in an integrated manner; but because of the col-laboration among users, the estimation uncertainty of all  X  s can be quickly reduced comparing to simply running N independent ban-dit algorithms. This in turn leads to an improved regret bound. We will elaborate the effect of collaboration in online bandit learning with more theoretical justifications in Section 3.3.

The estimated bandit parameters b  X  predict the expected payoff of a particular arm for each user according to the observed context feature matrix X t . To complete an adaptive bandit algorithm, we need to design the exploration strategy for each user. Our collab-orative assumption in Eq (2) implies that r a t ,u t across users are independent given X t and W . As a result, for any  X  , i.e., the stan-dard deviation of Gaussian noise in Eq (2), the following inequality holds with probability at least 1  X   X  , | r where  X  t is a parameter in our algorithm defined in Lemma 1 of Section 3.3 and  X  is embedded in the computation of  X  t . The proof of this inequality can be found in the Appendix.

The inequality specified in Eq (6) gives us a reasonably tight up-per confidence bound (UCB) for the expected payoff of a particular arm over all the users in the graph G , from which a UCB-style arm-selection strategy can be derived. In particular, at trial t , we choose an arm for user u t according to the following arm-selection strategy,
We name this resulting algorithm as Collaborative Linear Bandit, or CoLin in short. The detailed description of CoLin is illustrated in Algorithm 1, where we use the property that vec (  X  X ( W  X  I ) vec (  X  X u t ) = ( W  X  I )  X  X t to simplify Eq (7).
Another evidence of the benefit from collaboration among users is demonstrated in Algorithm 1. When estimating the confidence interval of the expected payoff for action a t in user u t CoLin not only considers the prediction confidence from bandit u but also that from its neighboring bandits (as described by the Kro-necker product between W and I ). When W is an identity matrix, such effect disappears. Clearly, this collaborative confidence in-terval estimation will help the algorithm quickly reduce estimation uncertainty, and thus leads to the optimal solution more rapidly.
One potential issue with CoLin is its computational complexity: matrix inverse has to be performed on A t at every trial. First, be-cause of the rank one update of matrix A t (8th step in Algorithm 1), quadratic computation complexity is possible via applying the Sherman-Morrison formula. Second, we may compute A  X  1 t mini-batch manner to further reduce computation with some extra penalty in regret. We will leave this as our future research.
In this section, we provide detailed regret analysis of our pro-posed CoLin algorithm. We first prove that the estimation error of bandit parameters b  X  is upper bounded in Lemma 1.
 Lemma 1 : For any  X  &gt; 0 , with probability at least 1  X   X  , the estimation error of bandit parameters in CoLin is bounded by, k  X   X  k in which k  X   X  t  X   X   X  k A t = q (  X   X  t  X   X   X  ) T A t (  X  matrix norm induced by the positive semidefinite matrix A
Based on Lemma 1, we have the following theorem about the upper regret bound of the CoLin algorithm.
 Theorem 1 : With probability at least 1  X   X  , the cumulated regret of CoLin algorithm satisfies, in which  X  T is the upper bound of k  X   X  T  X   X   X  k A T and it can be explicitly calculated based on Lemma 1. The detailed proof of this theorem is provided in the Appendix.

As shown in Theorem 1, the graph structure plays an impor-tant role in the upper regret bound of our CoLin algorithm. Con-sider two extreme cases. First, when W is an identity matrix, i.e., no influence among users, the upper regret bound degener-ates to O ( N and uniform, i.e.,  X  i,j,w ij = 1 N , such that users have homoge-neous influence among each other, and the upper regret bound of CoLin decreases to O ( N tion, CoLin achieves an O ( single user in the graph comparing to the independent case.
Note that the our regret analysis in Theorem 1 is in a very gen-eral form, in which we did not make any assumption about the or-der or frequency that each user will be served. To illustrate the relationship between the proposed collaborative bandit algorithm and conventional independent bandit algorithms in a more intuitive way, we can make a very specific assumption about how a sequen-tial learner interacts with a set of users. Assuming all the users are evenly served by CoLin, i.e., each user interacts with the learner  X  T = T N times. When W is an identity matrix, the regret bound of CoLin degenerates to the case of running N independent LinUCB, whose upper regret bound is O ( N the regret bound reduces to O ( N O (  X  dent LinUCBs on each single user. The proof of regret bound in this special case is given in the Appendix.
 It is necessary to compare the derived upper regret bound of CoLin with that in the GOB.Lin algorithm [10], which also exploits the relatedness among a set of users. In GOB.Lin, the divergence among every pair of bandits (if connected in the graph) is measured by Euclidean distance between the learned bandit parameters. In its upper regret bound, such divergence is accumulated throughout the iterations. In extreme case where users are all connected but associate with totally distinct bandit parameters, GOB.Lin X  X  upper regret bound could be much worse than running N independent bandits, due to this additive pairwise divergence. While in our al-gorithm, such divergence is controlled by the multiplicative factor P equalities between the upper regret bound of CoLin ( R C ( T ) ) and GOB.Lin ( R G ( T ) ) always holds, 0  X  R 2 G ( T )  X  R 2 C ( T )  X  16 TN ln(1 + 2 T It is clear to notice that if there is no influence between the users in the collection, i.e., no edge in G , these two algorithms X  regret bound touches (both degenerate to N independent contextual ban-dits). Otherwise, GOB.Lin will always lead to a worse and faster growing regret bound than our CoLin algorithm.

In addition, limited by the use of graph Laplacian, GOB.Lin can only capture the binary connectivity relation among users. CoLin differentiates the strength of connections with a stochastic rela-tional graph. This makes CoLin more general when modeling the relatedness among bandits and provides a tighter upper regret bound. This effect is also empirically verified by our extensive experiments on both synthetic and real-world data sets.
We performed empirical evaluations of our CoLin algorithm against several state-of-the-art contextual bandit algorithms, including N independent LinUCB [21], hybrid LinUCB with user features [21], GOB.Lin [10], and online cluster of Bandits (CLUB) [15]. Among these algorithms, hybrid LinUCB exploits user dependency via a set of hybrid linear models over user features, GOB.Lin encodes the user dependency via graph-based regularization over the learned bandit parameters, and CLUB clusters users during online learning to enable model sharing. In addition, we also compared with sev-eral popularly used context-free bandit algorithms, including EXP3 [5], UCB1 [4] and -greedy [4]. But their performance is much worse than the contextual bandits, and thus we do not include their performance in the following discussions.

We tested all the algorithms on a synthetic data set via simula-tions, a large collection of click stream from Yahoo! Today Module dataset [21], and two real-world dataset extracted from the social bookmarking web service Delicious and music streaming service LastFM [10]. Extensive experiment comparisons confirmed the ad-vantage of our proposed CoLin algorithm against all the baselines. More importantly, comparing to the baselines that also exploit user dependencies, CoLin performs significantly better in identifying users X  preference on less popular items (items that are only ob-served among a small group of users). This validates that with the proposed collaborative learning among users, CoLin better allevi-ates the cold-start challenge comparing to the baselines.
In this experiment, we compare the bandit algorithms based on simulations and use the accumulated regret and bandit parameter estimation accuracy as the performance metrics.
In simulation, we generate N users, each of which is associated with a d -dimensional parameter vector  X   X  , i.e.,  X   X  = (  X  Each dimension of  X   X  i is drawn from a uniform distribution U (0 , 1) and normalized to k  X   X  i k = 1 .  X   X  is treated as the ground-truth bandit parameters for reward generation, and they are unknown to bandit algorithms. We then construct the golden relational stochas-tic matrix W for the graph of users by defining w ij  X   X   X  and normalize each column of W by its L1 norm. The resulting W is disclosed to the bandit algorithms. In the end, we generate a size-K action pool A . Each action a in A is associated with a d -dimensional feature vector x a , each dimension of which is drawn from U (0 , 1) . We also normalize x a by its L1 norm. To construct user features for hybrid LinUCB algorithm, we perform Principle Component Analysis (PCA) on the relational matrix W , and use the first 5 principle components to construct the user features.
To simulate the collaborative reward generation process among users, we first compute  X   X   X  =  X   X  W and then compute the payoff of action a for user i at trial t as r a t,i = diag i ( X  X  N (0 , X  2 ) . To increase the learning complexity, at each trial t , our simulator only discloses a subset of actions in A to the learning algorithms for selection, e.g., randomly select 10 actions from A without replacement. In simulation, based on the known bandit pa-rameters  X   X   X  , the optimal action a  X  t,i and the corresponding payoff r
Under this simulation setting, we compared hybrid LinUCB, N independent LinUCB, GOB.Lin, CLUB and CoLin. At each trial, the same set of actions are presented to all the algorithms; and the Gaussian noise t is fixed for all those actions at trial t . In our experiments, we fixed the feature dimension d to 5, article pool size K to 1000, and set the trade-off parameter  X  for L2 regularization to 0.2 in all the algorithms. We compared the regret of different bandit algorithms during adaptive learning. Furthermore, since the ground-truth bandit parameters are available in the simulator, we also compared the quality of learned parameters in each contextual bandit algorithm. This unveils the nature of each bandit algorithm, e.g., how accurately they can recover a user X  X  true preference.
We first set the user size N to 100 and fix the standard deviation  X  to 0.1 in the Gaussian noise for reward generation. All the con-textual bandit algorithms are executed up to 300 iterations per user in this experiment. We report the cumulated regret as defined in Eq (1) and the Euclidean distance between the learnt bandit param-eters from different algorithms and the ground-truth in Figure 1. To reduce randomness in simulation-based evaluation, we reported the mean and standard deviation of final regret from different al-gorithms after 30,000 iterations over 5 independent runs for results in all following experiments. To increase visibility, we did not plot error bars in Figure 1 (a) and (b).
 As we can find in Figure 1 (a), simply running N independent LinUCB algorithm gives us the worst regret, which is expected. Hybrid LinUCB, which exploits user dependency via a set of hy-brid linear models over user features performed better than Lin-UCB, but still much worse than CoLin. Although GOB.Lin also exploits the graph structure when estimating the bandit parameters, its assumption about the dependency among bandits is too restric-tive to well capture the information embedded in the interaction with users. We should note that in our simulation, by multiplying the relational matrix W with the ground-truth bandit parameter ma-trix  X   X  , the resulting bandit parameters  X   X   X  align with GOB.Lin X  X  assumption, i.e., neighboring bandits are similar. And  X  reward generation. Therefore, our simulation does not produce any bias against GOB.Lin. In Figure 1 (a) we did not include CLUB, whose regret grew linearly. After looking into the selected arms from CLUB, we found because of the aggregated decisions from users in the automatically constructed user clusters, CLUB always chose suboptimal arms, which led to a linearly increasing regret.
In Figure 1 (b), we compared accuracy of the learnt bandit pa-rameters from different algorithms. Because of their distinct mod-eling assumptions, LinUCB, hybrid LinUCB, CLUB and GOB.Lin cannot directly estimate  X   X  , i.e., the true bandit parameters for each user. Instead, they can only estimate  X   X   X  , which directly gov-erns the generation of observed payoffs. Only CoLin can estimate both  X   X   X  and  X   X  . As we can find in the results, CoLin gave the most accurate estimation of  X   X   X  , which partially explains its su-perior performance in regret. We also find that LinUCB actually achieved a more accurate estimation of  X   X   X  than GOB.Lin, but its regret is much worse. To understand this, we looked into the ac-tual execution of LinUCB and GOB.Lin, and found that because of the graph Laplacian regularization in GOB.Lin, it better controlled exploration in arm selection and therefore picked the optimal arm more often than LinUCB. Hyrbid LinUCB X  X  estimation of  X   X  the worst, but it is expected: hybird LinUCB uses a shared model and a personalized model to fit the observations. Comparing to CoLin X  X  estimation quality of  X   X   X  , its estimation of  X  worse. The main reason is that CoLin has to decipher  X  from the estimated  X   X  based on W , where noise is accumulated to prevent accurate estimation. Nevertheless, this result demonstrates the pos-sibility of discovering each individual user X  X  true preference from their compound feedback. This is meaningful for many practical applications, such as user modeling and social influence analysis.
We also notice that although CLUB X  X  estimated  X   X   X  as good as LinUCB X  X  (as shown in Figure 1 (b)), its regret is the worst. As we described earlier, CLUB X  X  aggregated decision at user cluster level constantly forced the algorithm to choose sub-optimal arms; but the reward generation for each arm in our sim-ulator follows that defined in Eq (2), which still provides validate information for CLUB to estimate  X   X   X  with reasonable accuracy.
In Figure 1 (c), we investigated the effect of exploration param-eter  X  t  X  X  setting in different algorithms. The last column indexed by  X  t represents the theoretical values of  X  computed from the al-gorithms X  corresponding regret analysis. As shown from results, the empirically tuned  X  yields comparable performance to the the-oretical values, and makes online computation more efficient. As a result, in all our following experiments we use a fixed  X  instead of a computed  X  t .

To further investigate the convergence property of different ban-dit algorithms, we examined the following four scenarios: 1) vari-ous user sizes N , 2) different noise level  X  , 3) a corrupted affinity matrix W , and 4) a sparse affinity matrix W , in reward genera-tion. We report the results in Table 1 to 4. Because of its poor performance, we did not include CLUB in those tables.

Firstly, in Table 1, we fixed the noise level  X  to 0.1 and varied the user size N from 40 to 200. We should note in this experiment the total number of iterations varies as every user will interact with the bandit learner 300 times. The regret in LinUCB goes linearly with respect to the number of users, since no information is shared across them. Via model sharing, hybrid LinUCB achieved some re-gret reduction compared with LinUCB; but its regret still increases linearly with the number of users. Compared with the independent bandits, we can clearly observe the regret reduction in CoLin with increasing number of users. As we discussed in Section 3.3, al-though GOB.Lin exploits the dependency among users, its regret might be even worse than running N independent LinUCBs, espe-cially when the divergence between users is large.

Secondly, in Table 2, we fixed N to 100 and varied the noise level  X  from 0.01 to 0.3. We can notice that CoLin is more robust to noise in the feedback: its regret grows much slower than all baselines. Our current regret analysis does not consider the effect of noise in reward, as long as it has a zero mean and finite variance. It would be interesting to incorporate this factor in regret analysis to provide more insight of collaborative bandit algorithms.

Thirdly, in CoLin, we have assumed the adjacency matrix W is known to the algorithm beforehand. However, in reality one might not precisely recover this matrix from noisy observations, e.g., via social network analysis. It is thus important to investigate the ro-bustness of collaborative bandit algorithms to a noisy W . We fixed the user size N to 100 and corrupted the ground-truth adjacency matrix W : add Gaussian noise N (0 , X  ) to w ij and normalize the resulting matrix. We refer to this noisy adjacency matrix as W The simulator still uses the true adjacency matrix W to compute the reward of each action for a given user; while the noisy ma-trix W 0 will be provided to the bandit algorithms, i.e., CoLin and GOB.Lin. This newly introduced Gaussian noise is different from the noise in generating the rewards as described in Eq (2).
From the cumulated regret shown in Table 3, we can find that under moderate noise level, CoLin performed much better than GOB.Lin; but CoLin is more sensitive to noise in W than GOB.Lin. Because CoLin utilizes a weighted adjacency graph to capture the dependency among users, it becomes more sensitive to the estima-tion error in W . While in GOB.Lin, because only the graph con-nectivity is used and the random noise is very unlikely to change the graph connectivity, its performance is more stable. Further theoret-ical analysis of how an inaccurate estimation of W would affect the resulting regret will be an interesting future work yet to explore.
Finally, the regret analysis of CoLin shows that its upper re-gret bound is related with the graph structure through the term P graph connectivity [10]. We designed another set of simulation experiments to verify the effect of graph structure on CoLin and GOB.Lin. In this experiment, we set the user size N to 100 and controlled the graph sparsity as follows: for each user in graph G , we only included the edges from his/her top M most influential neighbors (measured by the edge weight in W ) in the adjacency matrix, and normalized the resulting adjacency matrix to a stochas-tic matrix. No noise is added to W in this experiment (i.e.,  X  = 0 ).
As shown in Table 4, the regret of all bandit algorithms increases as W becomes sparser, i.e., less information can be shared across users. We can observe that the regret of CoLin increases faster than that in GOB.Lin, since more information becomes unavail-able to CoLin. The results empirically verified that CoLin X  X  regret bound is directly related to the graph structure described by the term P T t =1 P N j =1 w 2 u t j and GOB.Lin X  X  regret bound is only related to the graph connectivity. In this experiment, we compared our CoLin algorithm with Lin-UCB, hybrid LinUCB, GOB.Lin and CLUB on a large collection of ten days X  real traffic data from Yahoo! Today Module [21] using the unbiased offline evaluation protocol proposed in [22].
The dataset contains 45,811,883 user visits to the Today Mod-ule in a ten-day period in May 2009. For each visit, both the user and each of the 10 candidate articles are associated with a feature vector of six dimensions (including a constant bias feature), con-structed by a conjoint analysis with a bilinear model [21]. However, this data set does not contain any user identity. This forbids us to associate the observations with individual users. To address this limitation, we first clustered all users into user groups by applying K-means algorithm on the given user features. Each observation is assigned to its closest user group. The weight in the adjacency ma-trix W is estimated by the dot product between the centroids from K-means X  output, i.e., w ij  X   X  u i ,u j  X  . The CoLin and GOB.Lin algorithms are then executed over those identified user groups. For LinUCB baseline, we tested two variants: one is individual Lin-UCBs running over the identified user groups and it is denoted as M-LinUCB; another one is a uniform LinUCB shared by all the users, i.e., it does not distinguish individual users, and it is denoted as Uniform-LinUCB.

In this experiment, click-through-rate (CTR) was used to eval-uate the performance of all bandit algorithms. Average CTR is computed in every 2000 observations (not the cumulated CTR) for each algorithm as the performance metric based on the unbiased of-fline evaluation protocol proposed in [22, 21]. Following the same evaluation principle used in [21], we normalized the resulting CTR from different bandit algorithms by the logged random strategy X  X  CTR. We report the normalized CTR from different contextual ban-dit algorithms over the 160 derived user groups in Figure 2 (a).
CoLin outperformed all baselines on this real-world data set, ex-cept CLUB on the first day. Results from other user cluster sizes (40 and 80) showed consistent improvement as demonstrated in Figure 2 (a) with 160 user clusters; but due to space limit, we did not include those results on cluster size of 40 and 80. As we can find CLUB achieved the best CTR on the first day; but as some pop-ular news articles became out-of-date, CLUB cannot correctly rec-ognize their decreased popularity, and thus provided degenerated recommendations. But in CoLin, because of the collaborative pref-erence learning, it better controlled exploration-exploitation trade-off and thus timely recognized the change of items X  popularity. However, one potential limitation of CoLin algorithm is its com-putational complexity: because the dimension of global statistic matrix A t defined in Eq (4) is dN  X  dN , the running time of CoLin scales quadratically with the number of users. It makes CoLin less attractive in practical applications where the size of users is large. One potential solution is to enforce sparsity in the estimated W ma-trix such that distributed model update is possible, i.e., only share information within the connected users. The simulation study in Table 4 confirms the feasibility of this direction and we will ex-plore it in our future work.
The LastFM dataset is extracted from the music streaming ser-vice Last.fm (http://www.last.fm), and the Delicious data set is ex-tracted the social bookmark sharing service website Delicious (https: //delicious.com). These two datasets were generated by the Infor-mation Retrieval group at Universidad Autonomade Madrid for the HetRec 2011 workshop with the goal of investigating the usage of heterogeneous information in recommendation system LastFM dataset contains 1892 users and 17632 items (artists). We used the information of  X  X istened artists X  of each user to create pay-offs for bandit algorithms: if a user listened to an artist at least once, the payoff is 1, otherwise 0. The Delicious dataset contains 1861 users and 69226 items (URLs). We generated the payoffs using the information about the bookmarked URLs for each user: the payoff is 1 is the user bookmarked a particular URL, otherwise 0. Both of these two data sets contain the users X  social network graph, which makes them a perfect testbed for collaborative bandits.

Following the same settings in [10], we pre-processed these two datasets in order to fit them into the contextual bandit setting. Firstly, we used all tags associated with a single item to create a TF-IDF feature vector, which uniquely represents the item. Then we used PCA to reduce the dimensionality of the feature vectors. In both datasets, we only retained the first 25 principle components to con-struct the context vectors, i.e., the feature dimension d = 25 . We then generate the candidate arm pool as follows: we fix the size of candidate arm pool to be K = 25 ; for a particular user u , we pick one item from those nonzero payoff items for user u according to the whole observations in the dataset, and randomly pick the other 24 from those zero-payoff items for user u .

User relation graph is naturally extracted from the social network information provided by the datasets. In order to make the graph denser and make the algorithms computationally feasible, we per-formed graph-cut to cluster users into M clusters (following the same setting as in [10]). Users in the same clusters are assumed to have the same bandit parameters. In our experiments, M was set to be 50, 100, and 200. Our reported results are from the setting of M = 200 , and similar results were achieved in other settings of M. After user clustering, a weighted graph can be generated: the nodes are the clusters of the original graph; and the edges between differ-
Datasets and their full description is available at http://grouplens.org/datasets/hetrec-2011 ent clusters are weighted by the number of inter-cluster edges in the original graph. In CoLin, we also need the diagonal elements in W , which is undefined in a graph-cut based clustering algorithm. We computed the diagonal elements based on the derived regret bound of CoLin. Specifically, we first set w ij  X  c ( i,j ) , where c ( i,j ) is the number of edges between cluster i and j ; then we optimize { w
We included three variants of LinUCB, hybrid LinUCB, GOB.Lin and CLUB as our baselines. The three variants of LinUCB include: 1) LinUCB that runs independently on each user, denoted as N-LinUCB; 2) LinUCB that is shared in each user cluster, denoted as M-LinUCB (M is the number of clusters); 3) LinUCB that is shared by all the users, denoted as Uniform-LinUCB. Following the set-ting it [10], GOB.Lin also operates at the user cluster level and it takes the connectivity among clusters as input. We normalized the cumulated reward in each algorithm by a random strategy X  X  cu-mulated reward, and compute the average accumulated normalized reward in every 50 iterations. Note that user features required by hybrid LinUCB are not given in these two datasets. We construct the user features by applying PCA on CoLin X  X  adjacency matrix W and retained the first 25 principle components.
 From the results shown in Figure 2 (b) and (c), we can find that CoLin outperforms all the baselines on both Delicious and LastFM datasets. It X  X  worth noting that these two datasets are structurally different, as shown in Figure 3 (a), the popularity of items on these two data sets differs significantly: on LastFM dataset, there are a lot more popular artists whom everybody listens to than the popular websites which everyone bookmarks on Delicious dataset. Thus the highly skewed distribution of item popularity makes recommenda-tion on Delicious dataset much more challenging. Because most of items are only bookmarked by a handful of users, exploiting the re-latedness among users to propagate feedback become vital. While on LastFM since there are much more popular items that most users would like, most algorithms can easily recognize the quality of items. In order to better understand this difference, we performed detailed item-level analysis to examine the effectiveness of differ-ent algorithms on items with varied popularity. Specifically, we first ranked all the items in these two datasets in a descending order of item popularity and then examined the item-based recommen-dation precision from all the bandit algorithms, e.g., percentage of item recommendations that are accepted by the users. In order to better visualize the results, we further grouped ranked items into different batches and report the average recommendation precision over each batch in Figure 3 (b) and Figure 3 (c).

From the item-based recommendation precision results, we can clearly find that on the LastFM dataset, CoLin achieved superior performance against all the baselines in every category of items, given the popularity of items in this data set is more balanced. On Delicious dataset, CoLin achieved superior performance on top-ranked items, however, because of the skewness of item popularity distribution, less popular items are still challenging for all the com-pared algorithms.

This analysis motivates us to further analyze the user-level rec-ommendation performance of different bandit algorithms, especially to understand the effectiveness of collaboration among bandits in alleviating the cold-start problem. To quantitatively evaluate this, we first ranked the user clusters in a descending order with respect to the number of observations in it. We then selected top 50 clus-ters as group 1. From the bottom 100 user clusters, we select 50 of them who are mostly connected to the users in group 1, as re-fer to them as group 2. The first group of users is called  X  X earning bucket X  and the second group is called  X  X esting bucket X . Based on this separation of user clusters, we designed two different experi-ment settings: one is warm-start, another one is cold-start. In the warm-start setting, we first run all the algorithms on the learning bucket to estimate parameters for both group of users, such as A and b t in CoLin. However, because user group 2 does not have any observation in the learning bucket, their model parameters can only be updated via collaboration among bandits, i.e., in CoLin and GOBLin. Then with the model parameters estimated from the learning bucket, we run and evaluate different algorithms on the deployment bucket. Correspondingly, in the cold-start setting, we directly run and evaluate the bandit algorithms on the deployment bucket. It is obvious that since LinUCB assume users are indepen-dent and there is no sharing parameters among users, LinUCB X  X  performance will not change under warm-start and cold start set-ting. While in CoLin, GOB.Lin and CLUB, because of the col-laboration among users, model parameters are shared among users. In this case, user preference information learned from the learning bucket can be propagated to the deployment bucket.

We report the performance on the first 10% observations in the deployment bucket instead of the whole observations in order to better simulate the cold-start situation (i.e., all the algorithms do not have sufficient observations to confidently estimate model pa-rameters). In Figure 4 (a) and (b) we report the gap of cumulated rewards from CoLin GOB.Lin, and CLUB between warm-start set-ting and cold-start setting, normalized by rewards obtained from LinUCB. From Figure 4(a) we can notice that on Delicious dataset, although at the very beginning of the warm-start setting both GOB-Lin and CoLin performed worse than the cold-start setting, both algorithms in warm-start quickly improved and outperformed the cold-start setting. One possible explanation is that the algorithms might take the first several iterations to adapt the model propagated from user group 1 to user group 2. In particular, from Figure 4 (a), it is clear that once both algorithm are adapted, the improvement between warm-start and cold-start on CoLin is larger than that on GOB.Lin. This verified CoLin X  X  effectiveness in address the cold-start challenge. From Figure 4 (b), we can find that warm-start helps both algorithms immediately at the first several iterations on LastFM dataset. This might be caused by the flat distribution of item popularity in this data set: users in group 2 also prefer the items liked by user group 1. We should note that the larger gap from GOB.Lin than that from CoLin between warm-start and cold-start settings does not mean CoLin is worse than GOB.Lin; but it indicates the cold-start CoLin learned faster than the cold-start GOB.Lin on this data set. And the final performance of both cold-start and warm-start CoLin was better than GOB.Lin in the corre-sponding settings. We can also notice that cold-start CLUB per-formed very similarly as warm-start CLUB. It means the user clus-ters automatically constructed in CLUB does not help collaborative learning. This experiment confirms that appropriate observation sharing among users indeed helps address the cold-start problem in recommendation.

Furthermore, we performed a user-based analysis to examine how many users will benefit from the collaborative bandits set-ting. We define an improved user as the user who is served with improved recommendations from a collaborative bandit algorithm (i.e. CoLin, GOB.Lin and CLUB) than those from isolated Lin-UCBs. We report the percentage of improved users in the first 1%, 2%, 3%, 5%, and 10% observations during online learning process. Figure 4 (c) and (d) demonstrate that on all collaborative bandit al-gorithms, the warm-start setting benefits much more users than in the cold-start setting. This result further supports our motivation of this research that collaboration among bandits can help alleviate the cold-start challenge.
In this paper, we developed CoLin algorithm to study contextual bandit problem in a collaborative environment. In CoLin, context and payoffs are shared among the neighboring bandits during on-line update, and it helps reduce the preference learning complexity (i.e., requires less observations to achieve satisfactory prediction performance) and leads to reduced overall regret. We religiously proved a reduced upper regret bound comparing to independent bandit algorithms. Experimental results based on extensive simu-lations and three real-world datasets consistently confirmed the im-provement of the proposed collaborative bandit algorithm against several state-of-the-art contextual bandit algorithms.

In our current setting, we have assumed the graph structure among users is known to the algorithm beforehand. It is meaningful to study how to dynamically estimate the graph structure during on-line update with provable regret bound. The computation complex-ity in CoLin is now expensive. Utilizing the sparse graph structure to decentralize the computation is an important and interesting re-search direction. Besides, CoLin should not be only limited to lin-ear payoffs; various types of link functions and regret definitions can be explored as our future work. We thank the anonymous reviewers for their insightful comments. This paper is based upon work supported by the National Science Foundation under grant IIS-1553568.
