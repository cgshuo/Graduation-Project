 Haihui Xu 1 , Pengpeng Zhao 1( With a large amount of data produced by social services, like Twitter and Sina Weibo, much effort has been devoted to processing and understanding networked data. However, obtaining the labels for these data is usually an expensive and time consuming process [ 1 ]. One promising approach of reducing the cost of labeling is active learning [ 2 ]. The task of active learning is to determine which instances should be selected to query for labels such that a learned classifier could achieve a good performance with requiring as few labels as possible. A lot of social network systems have been widely used. These systems enable us to obtain a huge amount of networked data. The instances of networked data are connected by links. For example, users in a social network may share friend-ships. These friendships form a network. In this case, traditional active learning approaches are not appropriate, because they assumes that data instances are explicit network structure or links in the data instances. They have shown that selectively querying for labels based on data links can significantly improve clas-sification performance. According to the strategies of evaluating data instances to query for labels, existing active learning methods on networked data can be roughly divided into two categories: individual assessment or single mode based, and set assessment or batch mode based. The first category methods select one instance at a time to query for its label. The second category methods intend to select an optimal subset , by levering the inter-correlations to estimate the utility value of a set of instances. The methods belonging to the first category is inefficient to retrain or update the classification model, especially for networked data, because of information redundancy exists among the selected instances in different iterations. There are a few methods [ 4 ][ 5 ] that solve the problem in a batch mode way, but these batch mode methods are in essence in a single mode. They selected instances with a greedy algorithm, which selects one instance with the maximum utility value each time, iteratively forms a batch instances. The greedy selection can be regarded as a local optimum approach.
 request labels, such that we can build a good classification model as soon as possible. With this aim in mind, we propose a new active learning framework for networked data. We utilize the network structure as representativeness cri-teria inside a graph-based metric X  X etweenness centrality to measure instance informativeness. To capture instance correlations, we combine instance content uncertainty with links representativeness and disparity to form a matrix, where each element denotes a correlation of instances indexed by its corresponding row and column. Instead of using a greedy algorithm, we solve the problem in a global way. We transform this problem to a semi-defined programming (SDP) problem, which selects a subset of instances out of the entire unlabeled set, such that the selected subset of instances have the maximum utility value. Compared to existing methods, the contributions of this paper are threefold:  X  Instance selection in a global optimum way. Individuals with the highest  X  A new utility measure for evaluating an instance utility value. To consider the  X  A new general framework for networked data active learning. We present a The remainder of the paper is organized as follows. Section 2 reviews the related work. The preliminaries are introduced in Section 3 . The proposed approach is detailed in Section 4 , followed by experiment results in Section 5 . In Section 6 , we conclude our paper. Depending on query strategies, existing active learning methods are generally divided into three categories. The first category is based on uncertainty sampling [ 6 ].The second category is based on query-by-committee (QBC) [ 7 ], which selects the instances that is considered to be the one the committee disagrees the most. The third category is based on expected error reduction [ 3 ], which selects the instance where the expected classification error can be reduced the largest. Recent researches extend to classify related examples like networked data by exploiting relational dependencies or link information between data instances, which have been shown to be effective to improve the classification model perfor-mance under inter-related conditions [ 8 ]. Due to the success of taking the data instance dependencies (e.g., instance structure, relationship) into consideration, many graph-based active learning methods have been proposed to address the problem of classifying networked data [ 3 ][ 4 ][ 9 ][ 10 ]. We review such single mode based approaches first, and then batch mode based ones. Macskaasy et al. [ 3 ] employed graph-based metrics to assess the informativeness of data instances with an empirical risk minimization technique. Fang et al. [ 11 ] introduced a social regularization term in the instance utility measure to capture its network structure. Xia hu et al. [ 9 ] proposed an active learning method for networked texts in microblogging, which utilizes network metrics and community detection to take advantage of network information.
 These aforementioned methods belong to the single mode based approaches, which select one instance to query the oracle for its label. There exist several batch mode methods. Shi et al. [ 4 ] proposed a unified active learning framework by combining both link and content information. They defined three criteria to measure the informativeness of a set of data instances.Yang et al. [ 5 ] focused on the setting of using the probabilistic graphical model to model the networked data. They used a greedy algorithm to select one data instance with the max-imum score at a time and chose all the examples one by one. Obviously, this method essentially is a single mode active selection method. 3.1 Problem Definition In our batch mode active learning, input data are denoted as a graph G = &lt; V, E &gt; , where denotes a set of nodes (one node presents one instance in the input data) including labeled instances V L and unlabeled instances V the edges between nodes. Each node v i  X  V is described by a feature vector and a class label y i  X  Y , i.e., v i = &lt;x i ,y i &gt; . x dimensional features, and Y denotes a set of class labels,i.e., Y =[ y Each edge e ij  X  E describes some relationship between two node v example, in a citation network, the nodes are publications. The features of each node include words, and the label of a node may be one of the topics of the papers. The edges denote as the citations.
 Definition 1. Batch M ode Active Learning ( BMAL ) f or N etworked Data : Given a set of labeled instances V L  X  G with V L = { &lt;x number of unlabeled instances V U , a budget B , our goal is to actively select a subset of unlabeled instances as a whole at each iteration of active learning, and query the oracle for their labels under the budget. And this subset is the best for improving the classification model performance. To solve this problem, a general objective function for selecting instances to request labels is defined as follows. where k is the size of the selected subset at each iteration, and the total selected instances.
 instantiate the objectives function Q ( V S ) and solve it effectively. In the above formulation, the objective function can be well instantiated in different ways. This definition of active learning for networked data has been widely used in the literature [ 12 ][ 4 ][ 13 ]. We now use a correlation matrix M to formulate the function. It is formulated as a quadratic integer programming problem as follows: where X is an n -dimensional column vector and n is the size of unlabeled set V
U . The constraint k defines the size of the subset for labeling at each iteration of active learning. x i = 1 means that the instance x i is selected for labeling and x i = 0 otherwise. In the following, we elaborate how to build the correlation matrix and how to actively select an optimal subset respectively. 4.1 Correlation Matrix Construction To build a correlation matrix M  X  R n  X  n , where n is the number of instances in the unlabeled set V U , we separate elements in M into two parts. Specifically, assuming that C i,i denotes the utility value of the uncertainty and represen-tativeness of an instance x i ,and I i,j ,i = j denotes the disparity between two instances x i and x j , the correlation matrix M is constructed as follows: Note that the utility value of an instance integrates the uncertainty and the representativeness of an instance and the disparity. The detail explanations are as follows.
 Given the classification model  X  L trained on the labeled set V tainty of an instance U ( x i ) is defined as the conditional entropy of its label variable as follows: This uncertainty measure captures the utility value of the candidate instance with respect to the labeled instances. The larger the conditional entropy, the larger the uncertainty of an instance has.
 As we know, representativeness-based active learning methods choose instances which can well represent the overall distribution of unlabeled data. For the networked data, we intend to select representative nodes to capture topological patterns of the entire network. Many methods have been proposed to capture particular features of the network topology in social networks. These methods quantify the network structure with various metrics [ 14 ]. In our method, we use one of the widely used graph-based metric, Betweenness Centrality [ 15 ], to select representative nodes in a network. Betweenness is one of the most prominent measures of centrality. In a network, a node with a greater between-ness centrality has a more important role between nodes communication. The betweenness centrality is defined as follows: where  X  st is the number of shortest paths between nodes v ((s, t) denotes as a path), and  X  st ( v i ) is the number of (s, t)-paths that go through the node v i . We need to compute all-pairs shortest-paths to measure the centrality for all nodes. However, we notice that there are efficient ways for computing this [ 16 ].
 Given the uncertainty measure and the betweenness centrality measure defined above, we develop a combination framework to integrate the strengths of both content information and link information. Specifically, we combine the two measures in a general form as follows: where 0  X   X   X  1 is a tradeoff controlling parameter over the two terms . Diversity discriminates the differences between instances when taking the sample redundancy into consideration. We use the disparity[ 17 ] between each pair of instances to capture the difference between instances such that an opti-mal subset can contain instances with high disparity and low redundancy. To calculate the disparity of each pair of instances, we employ two types of distance measures, the prediction distance and the feature distance.
 a classifier on two instances. The purpose is to assess the behavioral difference between the pair of instances with respect to the classifier. We estimate the prediction distance based on their prediction probabilities. Given a classifier  X  the estimated class membership distribution of the classifier for an unlabeled instance is denoted by p x = { p ( y 1 | x,  X  L ) ,p ( y 2 of instances x i and x j , their prediction difference with respect to this classifier over all class labels Y =[ y 1 ,y 2 ,y 3 ..., y m ] is denoted by The feature distance intends to capture the disparity of a pair of instances in the feature space. Given an instance x i =[ x i 1 ,x i 2 , vector of d -dimensional features of the node v i , the feature distance between x and x j is calculated as follows: ence between instances x i and x j from different perspectives, we simultaneously the behaviors of two consider instances and their distance in the feature space. We define the final disparity between x i and x j as the product of the two dis-tances as follows: Assuming that each of the prediction distance and the feature distance assesses the instance distribution from one dimension, the product therefore assesses the joint distribution from both dimensions and is a better way of assessing the instance disparity. 4.2 Optimal Instance Subset Selection In this section, we will discuss our solution of selecting an optimal subset of instances. This problem of finding the optimal subset of instances is actually a standard 0 X 1 optimization problem, which generally is NP-hard. Our solution is based on semi-definite programming. In order to find a optimal solution, we first change the original problem to a max-cut with a size k (MC-k) problem [ 18 ]. whose objective is to partition an edge-weighted graph containing N vertices into two parts, with one of which containing k vertices, such that the total weight of edges across the cut is maximum. To transform the original problem into the MC-k problem, we change variable x i as follows: As x i  X  X  0 , 1 } , t i  X  X  X  1 , 1 } . Replacing x i in ( 2 ) using in ( 10 ), we have where t is an n -dimensional vector with values of either 1 or -1 and e is the same-sized column vector with all values being 1 . The original cardinality con-straint is rewritten as a quadratic form, where I is an identity matrix. To convet the transformed objective function in ( 11 ) and its cardinality constraints into a quadratic form, we expand the vector t =( t 1 ,t 2 , ..., t t =( t 0 ,t 1 ,t 2 , ..., t n ) with t 0 = 1 and construct a new matrix  X  as follows: Also, we can apply the same extension to the cardinality constraints and build a new constraint matrix  X   X  R ( n +1)  X  ( n +1) as follows: As a result, the original instance-selection problem in (2) is transformed into an MC-k problem as follows: To solve ( 14 ), we let T = t  X  t T , where T  X  R ( n +1) is: where  X  reprents the dot product.We integrate the constraints on a binary vari-able t i . Because t i has only two possible values (1 or -1), together with the con-straint t 0 = 1 , all the diagonal terms in T are all 1. Accordingly, the constraints t  X  X  X  1 , 1 } can be expressed as Diag ( T )= I , where I is an ( n + 1)-dimensional identity matrix. Therefore, the SDP relaxation of ( 14 ) is denoted as follows: where T 0 means that the symmetric matrix T is positive semi-definite. Fol-lowing the SDP problem formulation defined in (16), we can employ publicly available open source packages to solve this problem. In our experiments, we use a semi-definite programming algorithm [ 19 ], which is based on an interior point method to find solutions.The pseudo-code of the framework of our proposed active learning method is shown in Algorithm 1.
 Algorithm 1. BMALNeT : Batch Mode Active Learning for Networked Data With Optimal Subset Selection the unlabeled data are the remaining ones, as line 2 describes. In line 3 to 12, BMALNeT proceeds in iterations until the budget B is exhausted. In each itera-tion, we select the optimal instance subset based on correlation matrix from the unlabeled data, then update the labeled instances and the unlabeled instances. In this section, we empirically evaluate the performance of our proposed active learning strategy on three datasets, in comparison with the state-of-the-art meth-ods. 5.1 Datasets In order to validate the performance of our proposed algorithm,we conducted extensive experiments on are three real-world datasets  X  Cora,CiteSeer and WebKB[ 20 ]. In the three datasets, instances are corresponding to documents, which are represented by a set of features vectors, and the network structure of each dataset is provided by the citations between different documents. The general statistic description of each dataset is shown in Table 1 .
 5.2 Experimental Setup In the experiment, each dataset is divided into an unlabeled set and a testing set. We use LibSVM [ 21 ] to train a SVM based on the labeled set, and use it to classify the instances in the testing set. In order to evaluate the effectiveness of our proposed active learning framework, we compare it with following active learning methods: Random: this method randomly selects instances from the unlabeled set to query for labels.
 Uncertainty: this method selects the instances with the least prediction mar-gin between the two most probable class labels given by the classification model. ALFNET [ 12 ]: this method clusters the nodes of a graph into several groups, and then randomly samples nodes from each cluster.
 ActNeT [ 9 ]: this method presents two selection strategies to exploit network structure to select the most informative instances to query for their labels. To make fair comparisons, all of the methods actively select k instances to request their labels in each iteration of the active learning process. Batch mode active learning methods actively select k instances each time, and single mode active learning methods gradually select k instances for each iteration. We perform 10-fold cross-validation to obtain the average classification accuracy. 5.3 Experimental Results We use the average classification accuracy as the assessment criteria. There are two important parameters involved in our experiments: the tradeoff controlling parameter  X  in Equation (6) and the batch size k . We first set the tradeoff con-trolling parameter  X  =0 . 5, and the batch size k = 5 for each dataset. Under this setting, our experimental results are shown in Figure 1 . From Figure 1 ,wecan see that our proposed method X  X MALNeT always achieves higher classification accuracy at the fastest rate on all three datasets. It consistently performs the best on the three datasets, followed by ActNeT. ActNeT consistently performs better than ALFNET, and ALFNET consistently performs better than Uncer-tainty. Random consistently is the worst. Our proposed method performs better ActNeT and ALFNET, because it takes the redundancy of instances into consid-eration. However, both ALFNET and ActNeT do not consider the redundancy among instances. The Uncertainty method performs worse than our method, ActNeT, and ALFNET, because it does not take advantage of link information. All these methods are expected better than Random.
 A.Results under Different Tradeoff Controlling Parameters As our method has two parameters, we further investigate its performance under different parameter settings. In this section, we set the batch size to a fixed value k = 5 for each dataset, and vary the tradeoff controlling parameter  X  from 0.25 to 0.5 to 0.75. The experimental results are shown in Figure 2 . From Figure 2 ,we can see that our proposed method is slightly impacted by the tradeoff controlling parameter  X  . When  X  =0 . 25, our proposed method has the best performance on the dataset Cora and WebKB. However, it has the best performance on the dataset CiteSeer when  X  =0 . 75. We attribute this variant to the ratio of the number of instances to the number of links in a dataset. Notice that  X  is a tradeoff controlling parameter between content uncertainty and link information, which is directly related to the ratio of the number of instances and the number of links in a dataset. The ratio of the dataset CiteSeer is larger than the ratios of the dataset Cora and WebKB. That is, its link information has a relative high utility.
 B. Results under Different Batch Sizes In this section, we further investigate the performance of our proposed method under different batch sizes. Again, in our experiment, we set the tradeoff con-trolling parameter to a fixed value  X  =0 . 5 for each dataset, and vary the batch size k from 5 to 10 to 15. Our experimental results are shown in Figure 3 .From Figure 3 , we can see that the batch size has slightly impacted the performance of our proposed method. Our proposed method performs slightly better when the batch size decreases from 15 to 10 to 5 on Cora and WebKB datasets. On the CiteSeer dataset, our proposed method performs the best when the batch size is set to be 10. We attribute this to the characteristics of each dataset, such as the number of instances, and the number of links, and the number of features. However, the impact of the batch size is marginal to our proposed method. In this paper, we have proposed a new batch mode active learning method for networked data levering optimal subset selection. In our novel method, we eval-uate the value of each instance based on content uncertainty, link information and disparity. Based on the value of each instance, a correlation matrix is con-structed. Then, we transform the batch mode active learning selection problem into a semi-definite programming problem, select a subset of instances with an optimal utility value. The use of the correlation matrix and the SDP algorithm ensure that the solution of the subset is global optimal. Our experimental results on three real-world datasets show that our proposed approach outperforms the state-of-the-art methods.

