 Rapid growth in the amount of data available on social net-working sites has made information retrieval increasingly challenging for users. In this paper, we propose a collabora-tive filtering method, Combinational Collaborative F iltering (CCF), to perform personalized community recommenda-tions by considering multiple types of co-occurrences in so-cial data at the same time. This filtering method fuses se-mantic and user information, then applies a hybrid training strategy that combines Gibbs sampling and Expectation-Maximization algorithm. To handle the large-scale dataset, parallel computing is used to speed up the model training. Through an empirical study on the Orkut dataset, we show CCF to be both effective and scalable.
 H.4.m [ Information Systems Applications ]: Miscella-neous Algorithms, Experimentation Collaborative filtering, probabilistic models, personalized rec-ommendation Social networking products are flourishing. Sites such as MySpace, Facebook, and Orkut attract millions of visitors a day, approaching the traffic of Web search sites [1]. These social networking sites provide tools for individuals to es-tablish communities, to upload and share user generated content, and to interact with other users. In recent articles, users complained that they would soon require a full-time employee to manage their sizable social networks. Indeed, take Orkut as an example. Orkut enjoys 100+ million com-munities and users, with hundreds of communities created each day. A user cannot possibly view all communities to select relevant ones.

In this work, we tackle the problem of community rec-ommendation for social networking sites. Such a problem fits in the framework of collaborative filtering (CF), which offers personal recommendations (of e.g., Web sites, books, or music) based on a user X  X  profile and prior information-access patterns. What differentiates our work from prior work is that we propose a fusion method, which combines information from multiple sources. We name our method CCF for Combinational Collaborative Filtering . CCF views a community from two simultaneous perspectives: a bag of users and a bag of words . A community is viewed as a bag of participating users; and at the same time, it is viewed as a bag of words describing that community. Traditionally, these two views are independently processed. Fusing these two views provides two benefits. First, by combining bags of users with bags of words , CCF can perform personalized community recommendations, which the bags of words alone model cannot. Second, augmenting bags of users with bags of words , CCF achieves better personalized recommenda-tions than the bags of users alone model, which may suffer from information sparsity.

A practical recommendation system must be able to han-dle large-scale datasets and hence demands scalability. We devise two strategies to speed up training of CCF. First, we employ a hybrid training strategy, which combines Gibbs sampling with the Expectation-Maximization (EM) algo-rithm. Our empirical study shows that Gibbs sampling pro-vides better initialization for EM, and thus can help EM to converge to a better solution at a faster pace. Our second speedup strategy is to parallelize CCF to take advantage of the distributed computing infrastructure of modern data centers. Our scalability study on a real-world dataset of 312k active users and 109k popular communities demonstrates that our parallelization scheme on CCF is effective. CCF can achieve near-linear speedup on up to 200 distributed ma-chines, attaining 116 times speedup over the training time of using one machine. CCF is attractive for supporting large-scale personalized community recommendations, thanks to its effectiveness and scalability.
Several algorithms have been proposed to deal with either bags of words or bags of users . Specifically, Probabilistic Latent Semantic Analysis (PLSA) [7] and Latent Dirichlet Allocation (LDA) [3] model document-word co-occurrence, which is similar to the bags of words community view. Prob-abilistic Hypertext Induced Topic Selection (PHITS) [4], a variant of PLSA, models document-citation co-occurrence, which is similar to the bags of users community view. How-ever, a system that considers just bags of users cannot take advantage of content similarity between communities. A sys-tem that considers just bags of words cannot provide person-alized recommendations: all users who joined the same com-munity would receive the same set of recommendations. We propose CCF to model multiple types of data co-occurrence simultaneously. CCF X  X  main novelty is in fusing information from multiple sources to alleviate the information sparsity problem of a single source.

Several other algorithms have been proposed to model publication and email data 1 . For instance, the author-topic (AT) model [11] employs two factors in characterizing a doc-ument: the document X  X  authors and topics. Modeling both factors as variables within a Bayesian network allows the AT model to group the words used in a document corpus into semantic topics, and to determine an author X  X  topic associ-ations. For emails, the author-recipient-topic (ART) model [8] considers email recipient as an additional factor. This model can discover relevant topics from the sender-recipient structure in emails, and enjoys an improved ability to mea-sure role-similarity between users. Although these models fit publication and email data well, they cannot be used to for-mulate personalized community recommendations, whereas CCF can.
In summary, this paper makes the following three contri-butions: 1. We propose CCF to effectively fuse multiple informa-2. We devise a hybrid training method, which uses Gibbs 3. We parallelize CCF to achieve near-linear speedup on The remainder of the paper is organized as follows. In Section 2, we present CCF, including its model structure and semantics, hybrid training strategy, and parallelization scheme. In Section 3, we present our experimental results on both synthetic and Orkut datasets. We provide concluding remarks and discuss future work in Section 4.
We discuss only related model-based work since the model-based approach has been proven to be superior to the memory-based approach.
 Figure 1: (a) Graphical representation of the Community-User (C-U) model. (b) Graphical rep-resentation of the Community-Description (C-D) model. (c) Graphical representation of Combina-tional Collaborative Filtering (CCF) that combines both bag of users and bag of words information.
We start by introducing the baseline models. We then show how our CCF model combines baseline models. Sup-pose we are given a collection of co-occurrence data con-sisting of communities C = { c 1 , c 2 , ..., c N } , community de-scriptions from vocabulary D = { d 1 , d 2 , ..., d V } , and users U = { u 1 , u 2 , ..., u M } . If community c is joined by user we set n ( c, d ) = R if community c contains word d for R times; otherwise, n ( c, d ) = 0. The following models are la-tent aspect models, which associate a latent class variable z  X  Z = { z 1 , z 2 , ..., z K } .

Before modeling CCF, we first model community-user co-occurrences (C-U), shown in Figure 1(a); and community-description co-occurrences (C-D), shown in Figure 1(b). Our CCF model, shown in Figure 1(c), builds on C-U and C-D models. The shaded and unshaded variables in Figure 1 in-dicate latent and observed variables, respectively. An arrow indicates a conditional dependency between variables.
The C-U model is derived from PLSA and for community-user co-occurrence analysis. The co-occurrence data consists of a set of community-user pairs ( c, u ), which are assumed to be generated independently. The key idea is to introduce a latent class variable z to every community-user pair, so that community c and user u are rendered conditionally indepen-dent. The resulting model is a mixture model that can be written as follows: where z represents the topic for a community. For each community, a set of users is observed. To generate each user, a community is c chosen uniformly from the community set, then a topic z is selected from a distribution P ( z | c ) that is specific to the community, and finally a user u is generated by sampling from a topic-specific distribution P ( u | z ).
The second model is for community-description co-occurrence analysis. It has a similar structure to the C-U model with the joint probability written as: where z represents the topic for a community. Each com-munity X  X  interests are modeled with a mixture of topics. To generate each description word, a community c is chosen uni-formly from the community set, then a topic z is selected from a distribution P ( z | c ) that is specific to the commu-nity, and finally a word d is generated by sampling from a topic-specific distribution P ( d | z ).
 Remark : One can model C-U and C-D using LDA. Since the focus of this work is on model fusion, we defer the compari-son of PLSA vs. LDA to future work.
In the C-U model, we consider only links , i.e., the observed data can be thought of as a very sparse binary M  X  N matrix W , where W i,j = 1 indicates that user i joins (or linked to) community j , and the entry is unknown elsewhere. Thus, the C-U model captures the linkage information between communities and users, but not the community content. The C-D model learns the topic distribution for a given commu-nity, as well as topic-specific word distributions. This model can be used to estimate how similar two communities are in terms of topic distributions. Next, we introduce our CCF model, which combines both the C-U and C-D.

For the CCF model (Figure 1(c)), the joint probability distribution over community, user, and description can be written as: The CCF model represents a series of probabilistic genera-tive processes. Each community has a multinomial distribu-tion over topics, and each topic has a multinomial distribu-tion over users and descriptions, respectively.
Given the model structure, the next step is to learn model parameters. There are some standard learning algorithms, such as Gibbs sampling [6], Expectation-Maximization (EM) [5], and Gradient descent. For CCF, we propose a hybrid training strategy: We first run Gibbs sampling for a few it-erations, then switch to EM. The model trained by Gibbs sampling provides the initialization values for EM. This hy-brid strategy serves two purposes. First, EM suffers from a drawback in that it is very sensitive to initialization. A better initialization tends to allow EM to find a  X  X etter X  op-timum. Second, Gibbs sampling is too slow to be effective for large-scale datasets in high-dimensional problems [2]. A hybrid method can enjoy the advantages of Gibbs and EM. Gibbs sampling is a simple and widely applicable Markov chain Monte Carlo algorithm, which provides a simple method for obtaining parameter estimates and allows for combina-tion of estimates from several local maxima of the posterior distribution. Instead of estimating the model parameters di-rectly, we evaluate the posterior distribution on z and then use the results to infer P ( u | z ), P ( d | z ) and P ( z | c ).
For each user-word pair, the topic assignment is sampled from: where z i,j = k represents the assignment of the i th user and j th description word in a community to topic k . u i = m rep-resents the observation that the i th user is the m th user in the user corpus, and d j = n represents the observation that the j th word is the n th word in the word corpus. z  X  i,  X  j resents all topic assignments not including the i th user and the j th word. Furthermore, C UZ mk is the number of times user m is assigned to topic k , not including the current instance; C nk is the number of times word n is assigned to topic k , not including the current instance; C CZ ck is the number of times topic k has occurred in community c , not including the current instance.

We analyze the computational complexity of Gibbs sam-pling in CCF. In Gibbs sampling, one needs to compute the posterior probability for user-word pairs ( M  X  L ) within N communities, where L is the number of words in community description (Note L  X  V ). Each P ( z i,j = k | u i = m, d j = n, z  X  i,  X  j consists of K topics, and requires a constant number of arith-metic operations, resulting in O ( K  X  N  X  M  X  L ) for a single Gibbs sampling. During parameter estimation, the algo-rithm needs to keep track of a topic-user ( K  X  M ) count ma-trix, a topic-word ( K  X  V ) count matrix, and a community-topic ( N  X  K ) count matrix. From these count matrices, we can estimate the topic-user distributions P ( u m | z word distributions P ( d n | z k ) and community-topic P ( z by: where P ( u m | z k ) is the probability of containing user m in topic k , P ( d n | z k ) is the probability of using word n in topic k , and P ( z k | c c ) is the probability of topic k in community c . The estimation of parameters by Gibbs sampling replaces the random seeding in EM X  X  initialization step.
 The CCF model is parameterized by P ( z | c ), P ( u | z ), and P ( d | z ), which are estimated using the EM algorithm to fit the training corpus with community, user, and description by maximizing the log-likelihood function: n ( c, u, d ) = n ( c, u ) n ( c, d ) = Starting with the initial parameter values from Gibbs sam-pling, the EM procedure iterates between Expectation (E) step and Maximization (M) step:
We analyze the computational complexity of the E-step and the M-step. In the E-step, one needs to compute the posterior probability P ( z | c, u, d ) for M users, N communi-ties, and V words. Each P ( z | c, u, d ) consists of K values, and requires a constant number of arithmetic operations to be computed, resulting in O ( K  X  N  X  M  X  V ) operations for a single E-step. In the M-step, the posterior probabilities are accumulated to form the new estimates for P ( u | z ), P ( d | z ) and P ( z | c ). Thus, the M-step also requires O ( K  X  N  X  M  X  V ) operations. Typical values of K in our experiments range from 28 to 256. The community-user ( c, u ) and community-description ( c, d ) co-occurrences are highly sparse, where separated from the n ( c, u, d ) term in the M-step, we do not need to compute P ( z | c, u, d ) for n ( c, u, d ) = 0 in the E-step. We compute only P ( z | c, u, d ) for n ( c, u, d ) 6 = 0. This greatly reduces computational complexity. The parameter estimation using Gibbs sampling and the EM algorithm described in the previous sections can be di-vided into parallel subtasks [9].
 We distribute the computation among machines based on community IDs. Thus, each machine i only deals with a specified subset of communities c i , and aware of all users u and descriptions d . We then perform Gibbs sampling si-multaneously on each machine independently. For each ma-chine, given the current state of all but one variable z the posterior probability is sampled using Equation (4). We then calculate a number of local count variables, including C each machine, we perform a global update to merge back to a single set of count variables: C UZ mk and C DZ nk . We summa-rize the process in Algorithm 1.
 The parallel EM algorithm can be applied in a similar fash-ion. We describe the procedure below and provide the pseudo-code in Algorithm 2.
 Algorithm 1 : Parallel Gibbs Sampling of CCF Input : N  X  M community-user matrix N  X  V community-description matrix; I : number of iterations P : number of machines Output : P ( u | z ), P ( d | z ), P ( z | c )
Variables: x ic : the i th row of comm-user matrix with comm id c y ic : the i th row of comm-word matrix with comm id c for i = 0 to N  X  1 do end
Gibbs sampling initialization for iter = 0 to I  X  1 do end
We analyze the computational and communication com-plexities for both algorithms using distributed machines. As-suming that there are P machines, the computational com-plexity of each training algorithm reduces to O (( K  X  N  X  M  X  L ) /P ) (for Gibbs) and O (( K  X  N  X  M  X  V ) /P ) (for EM) since P machines share the computations simultaneously. For com-munication complexity, two variables need to be distributed among P machines for computations in next iteration: C UZ C nk in Gibbs sampling, and P ( u | z ), P ( d | z ) in EM. Thus broadcasting will take up to O ( P  X  K  X  ( M + V )).
Once we have learned the model parameters, we can in-fer three relationships using Bayesian rules, namely user-community relationship, community similarity, and user sim-ilarity. We derive these three relationships as follows: Algorithm 2 : Parallel EM algorithm of CCF
Input : N  X  M community-user matrix; N  X  V Output : P ( u | z ), P ( d | z ), P ( z | c )
Variables: x ic : the i th row of comm-user matrix with comm id c y ic : the i th row of comm-word matrix with comm id c
Load P ( u | z ), P ( d | z ), P ( z | c ) of Gibbs sampling for i = 0 to N  X  1 do end for iter = 0 to I  X  1 do end
We divided our experiments into two parts. The first part was conducted on a relatively small synthetic dataset with ground truth to evaluate the Gibbs &amp; EM hybrid training strategy. The second part was conducted on a large, real-world dataset to test out CCF X  X  performance and scalabil-ity. Our experiments were run on up to 200 machines at our distributed data centers. While not all machines are iden-tically configured, each machine is configured with a CPU faster than 2GHz and memory larger than 4GBytes.
To precisely account for the benefit of Gibbs &amp; EM over the EM-only training strategy, we used a synthetic dataset where we know the ground truth. The synthetic dataset con-sists of 5 , 000 documents with 10 topics, a vocabulary size 10 , 000, and a total of 50 , 000 , 000 word tokens. The true topic distribution over each document was pre-defined man-ually as the ground truth. We conducted the comparisons using the following two training strategies: (1) EM-only training (without Gibbs sampling as initialization) where the number of EM iterations is 10 through 70 respectively, (2) Gibbs+EM training where the number of Gibbs sam-pling iterations is 5, 10, 15 and 20, and the number of EM iterations is 10 through 70, respectively. We used Kullback-Leibler divergence (K-L divergence) to evaluate model per-formance since the K-L divergence is a good measure for the difference between the true topic distribution ( P ) and the estimated topic distribution ( Q ) defined as follows: The smaller the K-L divergence is, the better the estimated topic distribution approximates the true topic distribution.
Figure 2 compares the average K-L divergences over 10 runs. It shows that more rounds of Gibbs sampling can help EM reach a solution that enjoys a smaller K-L divergence. Since each iteration of Gibbs sampling takes longer than EM, we must also consider time . Figure 3 shows the values of K-L divergence as a function of the training time. We can make two observations. First, given a large amount of time, Figure 2: The Kullback-Leibler divergence as a func-tion of the number of iterations. both EM and the hybrid scheme can reach very low K-L di-vergence. On this dataset, when the training time exceeded 350 seconds, the value of K-L divergence approached zero for all strategies. Nevertheless, on a large dataset, we can-not afford a long training time, and the Gibbs &amp; EM hybrid strategy provides a earlier point to stop training, and hence reduces the overall training time.

The second observation is on the number of Gibbs iter-ations. As shown in both figures, running more iterations of Gibbs before handing over to EM takes longer to yield a better initial point for EM. In other words, spending more time in the Gibbs stage can save time in the EM stage. Fig-ure 3 shows that the best performance was produced by 10 iterations of Gibbs sampling before switching to EM. Find-ing the  X  X ptimal X  switching point is virtually impossible in theory. However, the figure shows that different Gibbs it-erations can all outperform the EM-only strategy to obtain a better solution early, and a reasonable number of Gibbs iterations can be obtained through an empirical process like our experiment. Moreover, the figure shows that a range of number of iterations can achieve similar K-L divergence (e.g., at time 250). This indicates that though an empirical process may not be able to pin down the  X  X ptimal X  number of iterations (because of e.g., new training data arrival), the hybrid scheme can work well on a range of Gibbs-sampling iterations.
Orkut is an extremely active community site with more than two billion page views a day world-wide. The dataset we used was collected on July 26, 2007, which contains two types of data for each community: community membership information and community description information. We re-strict our analysis to English communities only. We collected 312 , 385 users and 109 , 987 communities 2 . The number of entries in the community-user matrix, effectively, the num-ber of community-user pairs, is 35 , 932 , 001. As the density is around 0 . 001045, this matrix is extremely sparse. Figure 4(a) shows a distribution of the number of users per com-munity. About 52% of all communities have less than 100
All user data were anonymized, and user privacy is safe-guarded, as performed in [10]. Figure 3: The Kullback-Leibler divergence as a func-tion of the training time.
 Figure 4: (a) Distribution of the number of users per community, and (b) distribution of the number of description words per community. users, whereas 42% of all communities have more than 100 but less than 1 , 000 users.

For the community description data, after applying down-casing, stopword filtering, and word stemming, we obtained a vocabulary of 191 , 034 unique English words. The distri-bution of the number of description words per community is displayed in Figure 4(b). On average, there are 27 . 64 words in each community description after processing. In order to establish statistical significance of the findings, we repeated all experiments 10 times with different random seeds and parameters, such as the number of latent aspects (ranging from 28 to 256), the number of Gibbs sampling iterations (ranging from 10 to 30) and the number of EM iterations (ranging from 100 to 500). The reported results are the average performance over all runs.
 Community Recommendation: P ( c j | u i ) We use two standard measures from information retrieval to measure the recommendation effectiveness: precision and recall , defined as follows:
P recision =
Recall = Figure 5: The precision and recall as functions of the length (up to 200) of the recommendation list. Figure 6: The precision and recall as functions of the length (up to 20) of the recommendation list.
 Precision takes all recommended communities into account. It can also be evaluated at a given cut-off rank, considering only the topmost results recommended by the system. As it is possible to achieve higher recall by recommending more communities (note that a recall of 100% is trivially achieved by recommending all communities, albeit at the expense of having low precision), we limit the size of our community recommendation list to at most 200.

To evaluate the results, we randomly deleted one joined community for each user in the community-user matrix from the training data. We evaluated whether the deleted com-munity could be recommended. This evaluation is similar to leave-one-out . Figure 5 shows the precision and recall as functions of the length (up to 200) of the recommendation list for both C-U and CCF. We can see that CCF always outperforms C-U for all lengths. Figure 6 presents preci-sion and recall for the top 20 recommended communities. As both precision and recall of CCF are nearly twice higher than those of C-U, we can conclude that CCF enjoys bet-ter prediction accuracy than C-U. This is because C-U only considers community-user co-occurrence, whereas CCF con-Figure 7: The precision as a function of the number of communities a user has joined. Here, the length of the recommendation list is fixed at 20.
 Table 1: The comparison results of the three models using Normalized Mutual Information (NMI).
 siders users, communities, and descriptions. By taking into other views into consideration, the information is denser for CCF to achieve higher prediction accuracy.

Figure 7 depicts the relationship between the precision of the recommendation for a user and the number of com-munities that the user has joined. The more communities a user has joined, the better both C-U and CCF can pre-dict the user X  X  preferences. For users who joined around 100 communities, the precision is about 15% for C-U and 27% for CCF. However, for users who joined just 20 communi-ties, the precision is about 7% for C-U, and 10% for CCF. This is not surprising since it is very difficult for latent-class statistical models to generalize from sparse data. For large-scale recommendation systems, we are unlikely to ever have enough direct data with sufficient coverage to avoid sparsity. However, at the very least, we can try to incorporate indi-rect data to boost our performance, just as CCF does by using bags of words information to augment bags of users information. Remark : Because of the nature of leave-one-out, our experimental result can only show whether a joined community could be recovered. The low precision/recall re-flects this necessary, restrictive experimental setting. (This setting is necessary for objectivity purpose as we cannot obtain ground-truth of all users X  future preferences.) The key observation from this study is not the absolute preci-sion/recall values, but is the relative performance between CCF and C-U.
 Community Similarity: P ( c j | c i ) We next report the results of community similarities calcu-lated by the three models. We used community category (available at Orkut websites) as the ground-truth for clus-tering communities. We also assigned each community an estimated label for the latent aspect with the highest prob-ability value. We treated communities with the same es-timated label as members of the same community cluster . We then compared the difference between community clus-ters and categories using the Normalized Mutual Informa-tion (NMI).

NMI between two random variables CAT (category label) and CLS (cluster label) is defined as N M I ( CAT ; CLS ) = mation between CAT and CLS . The entropies H ( CAT ) and H ( CLS ) are used for normalizing the mutual informa-tion to be in the range [0 , 1]. In practice, we made use of the following formulation to estimate the N M I score [12, 13]: where n is the number of communities, n s and n t denote the numbers of community in category s and cluster t , n s,t denotes the number of community in category s as well as in cluster t . The N M I score is 1 if the clustering results perfectly match the category labels and 0 for a random par-tition. Thus, the larger this score, the better the clustering results.

Table 1 shows that CCF slightly outperforms both C-U and C-D models, which indicates the benefit of incorporating two types of information.
 User Similarity: P ( u j | u i ) An interesting application is friend suggestion: finding users similar to a given user. Using Equation (14), we can com-pute user similarity for all pairs of users. From these values, we derive a ranking of the most similar users for a given query user. Due to privacy concerns, we were not able to obtain the friend graph of each user to evaluate accuracy. Table 2 shows an example of this ranking for a given user.  X  X imilar X  users typically share a significant percentage of commonly-joined communities. For instance, the query user also joined 18 . 5% of the communities joined by the top user ranked by C-U, compared to 20 . 5% for CCF. It is encour-aging to see that CCF X  X  top ranked user has more over-lap with the query user than C-U X  X  top ranked user does. We believe that, again, incorporating the additional word co-occurrences has improved information density and hence yields higher prediction accuracy.
In analyzing runtime speedup for parallel training, we trained CCF with 20 latent aspects, 10 Gibbs sampling, and 20 EM iterations. As the size of a dataset is large, a single and P ( z | c, u, d ) X  X n its local memory, we cannot obtain the Table 3: Runtime comparisons for different number of machines.
 Figure 8: Speedup analysis for different number of machines. Figure 10: Runtime (Computation and Communi-cation) composition analysis. running time of CCF on one machine. Therefore, we use the runtime of 10 machines as the baseline and assume that 10 machines can achieve 10 times speedup. This assumption is reasonable as we will see shortly that our parallelization scheme can achieve linear speedup on up to 100 machines. Table 3 and Figure 8 report the runtime speedup of CCF us-ing up to 200 machines. The Orkut dataset enjoys a linear speedup when the number of machines is up to 100. After that, adding more machines receives diminishing returns. This result led to our examination of overheads for CCF, presented next.

No parallel algorithm can infinitely achieve linear speedup because of the Amdahl X  X  law. When the number of machines continues to increase, the communication cost starts to dom-inate the total running time. The running time consists of two main parts: computation time (Comp) and commu-nication time (Comm). Figure 9 shows how Comm over-head influences the speedup curves. We draw on the top the computation only line (Comp), which approaches the linear speedup line. The speedup deteriorates when com-munication time is accounted for (Comp + Comm). Fig-ure 10 shows the percentage of Comp and Comm in the total running time. As the number of machines increases, the communication cost also increases. When the number of machines exceeds 200, the communication time becomes even larger than the computation time.

Though the Amdahl X  X  law eventually kicks in to forbid a parallel algorithm to achieve infinite speedup, our empirical study draws two positive observations. 1. When the dataset size increases, the  X  X aturation X  point 2. The speedup that can be achieved by parallel CCF
We have introduced a generative graphical model, Com-binational Collaborative Filtering (CCF), for collaborative filtering based on both bags of words and bags of users infor-mation. CCF uses a hybrid training strategy that combines Gibbs sampling with the EM algorithm. The model trained by Gibbs sampling provides better initialization values for EM than random seeding. We also presented the paral-lel computing required to handle large-scale data sets. Ex-periments on a large Orkut data set demonstrate that our approaches successfully produce better quality recommen-dations, and accurately cluster relevant communities/users with similar semantics.

There are a couple of directions for future research. First, we would consider expanding CCF to incorporate more types of co-occurrence data. More types of co-occurrence data would help to overcome sparsity problem and make better recommendation. Second, in our analysis, the community-user pair value equals one, i.e. n ( u i , c j ) = 1 (if user u community c j ). An interesting extension would be to give this count a different value, i.e. n ( u i , c j ) = f , where f is the frequency of the user u i visiting the community c j . We are currently parallelizing LDA and will compare LDA and PLSA as the choice of our baseline algorithm in the future.
The authors would like to thank Ellen Spertus for prepar-ing the Orkut dataset and Jon Chu for helpful discussions. The first author is supported by NSF under grant II-0535085.
