 Batch evaluation techniques are widely used in information retrieval system mea-surement. Each system that is to be compared generates a ranking, or run ,for each of a set of topics, with documents included in the run and also ordered within the run on the basis of some computed textual similarity score relative to the given query. Possible similarity computations include the Okapi BM25 mechanism of Robertson et al. [ 10 ] and the language modeling techniques of Ponte and Croft [ 9 ]. Static score components such as Pagerank or other assess-ments of document quality can also be included. Those runs are then mapped to numeric effectiveness values using a set of relevance judgments and an effective-ness metric , which generates a single number as an assessment of the quality, or utility, of that run in the eyes of the user that is presumed to have inspected it. Finally, the effectiveness values are aggregated in some way across topics to get an overall performance measure which is often used, with a suitable statistical test, as a basis for answering the question  X  X s System A demonstrably better than System B? X .
 (or just ties ) in the ranking. The obvious issue is that ties admit a level of ambi-guity in the effectiveness metric values, and hence (potentially) in the outcome of a system versus system comparison, since a group of documents that all share the same computed similarity score could be presented to the user in any per-mutation that is consistent with the scores being non-increasing. Our first goal is thus to quantify the extent to which past Text Retrieval Conference (TREC) evaluation exercises have been affected by tied similarity scores, and determine whether the presence of ties may have caused ambiguity to flow through into system scores. In this part of the project we make use of a range of tie-breaking regimes, including the rules embedded in the well-known trec eval conclude that while ties have had the potential to be significantly disruptive, in practice they did not influence the outcomes of the measurements that were undertaken.
 A second related goal is to ask whether the deliberate introduction of ties might be useful in some way. For example, a range of approaches in which simi-larity scoring might be approximated or otherwise quantized have been suggested over the years including, for example the quantized document weights of Moffat et al. [ 8 ], or the impact-ordered indexes of Anh and Moffat [ 1 ]. If we allow that the retrieval system might gain tangible efficiency benefits from assigning scores with low precision to documents, then we may end up with large numbers of ties in the runs that the system generates, and being able to estimate the extent to which ties can be tolerated before there is risk of degraded system retrieval effectiveness is a key component of the approximation. In experiments using submitted TREC runs, we show that quite marked levels of approximation can be tolerated before system scores change significantly, and hence that relatively low-precision scoring can be employed if it boosts efficiency. Terminology. We suppose that the similarity scores generated for a query par-tition the document ranking  X  the run  X  X nto groups within which the documents have the same score. Let b g be the rank in the run at which the g th equi-score group commences, with, by definition, b 1 = 1; and let e g last document in that group, with b g +1 = e g + 1. That is, the g th group of tied documents spans the items [ b g ...e g ], and contains s g We further define G g to be the multiset of gain values associated with the doc-uments in the g th group, G g = { r k | b g  X  k  X  e g } , with r associated with the document at rank k ; and define t g to be the total gain asso-ciated with the g th group, t g = { r k | b g  X  k  X  e g } ten-item ranking shown in Fig. 1 , with each document given a single letter label for convenience, and with five different computed similarity scores. The second row shows a presumed relevance value for each corresponding document ( X 0 X  and  X 1 X ); and the third row lists the similarity scores that are presumed to have led to that ranking.
 If the scores are ignored and only the list of relevance values is employed, computation of (for example) the metric precision at depth k = 5 (P@5) yields a score of 2 / 5=0 . 4, because there are two  X 1 X  X  among the first five gain values. Similarly, the ranking shown has a reciprocal rank (RR) score of 1 / 3=0 . 333, since the first relevant document appears at rank k = 3. Other metrics such as average precision (AP), rank-biased precision (RBP) [ 7 ], and normalized dis-counted cumulative gain (NDCG) [ 5 ], can also be computed, based solely on that third  X  X ain X  row, without consideration of the document labels in the first row, or their scores in the second row.
 can be seen to have the same similarity score, and are part of a tied group. That means that P@5 might be either 2 / 5or3 / 5, depending on the tie-breaking rule employed to order them. Similarly, RR might be 1 / 2or1 / 3, because of the tie involving documents H and A and C (but note that there is no possible arrangement in which RR can be 1 / 4). Run Order. A range of mechanisms have evolved to deal with tied scores. The first and most obvious option is to do as has already been suggested in connection with the example shown in Fig. 1 , and that is to ignore the document scores and process the run in the order in which the documents are presented  X  in effect, pushing the responsibility for tie-breaking back to the retrieval system, whether or not it accepts it. This approach presumes that the system has employed more information than is captured in the final score, perhaps via further precision in the internal computation above and beyond what is passed to the evaluation regime, or perhaps via a secondary-key ordering process that is not part of the scores at all. However the system X  X  ordering arises, respecting the sequential presentation of documents is a plausible default way of handling tied scores. External Tie-Break Rule. A second option is to make use of some external fixed ordering criterion and use it to reorder the documents within each tied group, thereby obtaining a canonical representation for the run. For example, the documents in each group might be sorted according to their document identi-fier, or according to their length, or according to their URL or filename. As one specific example of this type of approach, the widely-used (see http://trec.nist.gov/trec eval/ ) sorts tied groups into decreasing order of document identifier before performing its various effectiveness metric computa-tions.
 Optimistic and Pessimistic Limits. A third way of handling runs with ties is to compute the best and worst scores that might arise, and then present a score range rather than a score value. The advantage of this approach is that it makes clear when scores contain potential ambiguity, in a way that mirrors the residuals of Moffat and Zobel [ 7 ], which provide guidance as to the metric weight assigned to unjudged documents. To compute an optimistic upper score bound, the t g relevant documents within the g th group are assumed to appear in the first rank positions, that is, [ b g ...b g + t g  X  computed in the usual way. Similarly, to get a pessimistic lower score bound, the t relevant documents in the group are assumed to appear as a block as deep in the run as is possible, at ranks [ e g  X  t g +1 ...e g Fig. 1 , the ordering  X  X  then A then C X  (and similarly in the other groups) is used to derive a lower bound on the score, and the ordering  X  X  then C then H X  (and so on) is used to obtain an upper bound. If a document is unjudged, then for many metrics (but notably, not for AP or NDCG) it should be assumed to be non-relevant for the purposes of establishing the lower bound, and assumed to be relevant for the purposes of establishing the upper bound.
 Averaging Across Permutations. While the worst-case bounds can be infor-mative, they are also somewhat pessimistic, and computing the average, or expected, value of the metric across all possible permutations of documents within each of the tied score groups provides a useful balance. If every permuta-tion of documents in each group is equally likely, then computing the expectation is simply the process of computing the metric for each permutation and taking their average. For a small number of small groups, this O ( approach is computationally feasible. But if there are many blocks, or if there are any large blocks, it is expensive. Fortunately, the summation over all permuta-tions telescopes for most metrics, leading to a tractable computation. McSherry and Najork [ 6 ] describe this process in detail, and present an incremental formu-lation for average precision that computes the expected score across all possible permutations of documents in each group. A similar computation can be used to compute an expected (across permutations within groups) RR score. For weighted-precision metrics such as RBP, a similar process can be adopted. The set of gain values associated with each group is summed and averaged, and then that average gain applied at each rank position, and weighted according to the decay function. For the example shown in Fig. 1 , and an RBP parameter p =0 . 5, the expected RBP0.5 score is computed as 0 . 5  X  We use these formulations for expected AP, expected RR (not to be confused with the metric ERR), and expected RBP in the experiments described in the next two sections. TREC Resources. In this section we examine the role that ties may have had on past TREC evaluations. The primary resource we make use of are the 103 runs submitted as part of the 1998 TREC7 Ad-Hoc experimentation round [ 13 ], see trec.nist.gov , and Harman [ 4 ] for a broad overview. Each run is a list of (up to) 1,000 responses from that system for each of 50 topics, with each row in the run file including fields for docnum , rank ,and score . There are thus three possible ways that each run could be interpreted:  X  by the line number ordering implicit in the presentation of the run;  X  by (increasing, or at least, non-decreasing) values in the rank field;  X  by (decreasing, or at least, non-increasing) values in the score field. Line numbers are unique within each system-topic combination, and do not admit ties, but both ranks and scores might provide ties in runs. To explore the prevalence of ties, the TREC7 Ad-Hoc runs were analyzed. Somewhat surpris-ingly, we discovered that there were 254 instances in the archived runs where scores were increasing rather than non-increasing in terms of the line ordering, and that five systems were affected by this inconsistency. The primary reason appears to be incorrect sorting of scores when exponential formatting is being used. For example, in the run bbn1 , for topic 355, the second-to-last score in the run is  X  1.37 ; and final score is  X  7.763e  X  05 . In fact, that last document X  X  correct position is some 700 locations higher, at rank 304, the rank that row was labeled with. When rank ordering was similarly checked the situation was even more confused, and 7 . 3 % of the documents in the archived runs (358,631 entries in total) were mis-ordered according to their stated ranks. That is, the supplied document ordering in the runs corresponds to neither increasing rank nor to non-decreasing score.
 sions, taking care to treat the exponential formats correctly. We used decreasing numeric score as the primary key, and then increasing rank as a secondary key. This is guaranteed to give rise to runs in which there are no score-based out-of-order items. We then counted the occurrences of score ties at the document, topic, and system level; and the occurrence of rank contradictions, where a  X  X on-tradiction X  is a pair of adjacent documents that when sorted by score have ranks that indicate the opposite ordering. Table 1 shows the results of this processing. As can be seen, 14 % of the documents in the runs have the same score as their predecessor document in that run, a fact that provides the motivation for our work here; and, of equal concern, a further 1 . 4 % of the documents cannot be placed in a manner that is consistent with both their assigned score and their assigned rank, with seven of the 103 systems affected. We can only assume that the cause of the latter issue was programming errors at the time the runs were created by the corresponding research groups. There were no ties on rank in any of the TREC7 runs.
 To ensure that the results in the remainder of the paper were not affected by programming mistakes and other experimental misunderstandings on the part of the 1998 TREC7 participants, we then took the top 80 systems, as ordered by average AP score over the 50 topics, discarding the other 23 systems from further evaluation. Similar restrictions have also been employed by other authors. Ties in TREC7. The primary evaluation metric used in TREC7 was aver-age precision, as implemented in the program trec eval (version 9.0). Working with the 80 score/rank-sorted runs, we next sought to examine the effect that the score-ties had on AP scores for systems. Figure 2 plots those systems. The horizontal axis is the trec eval score for that system, expressed as a mean AP value over the 50 topics. By inspecting the trec eval source code we were able to confirm that it (a) ignored line ordering in the input runs; (b) used exponen-tial number formats correctly when performing its sorting-by-score step; and (c) resolved score ties by reverse sorting on document number, paying no attention to the supplied rank field. The scale on the vertical axis in Fig. 2 is the AP score range measured by taking the difference between the pessimal and optimal topic scores, and then averaging across topics to get a system range. The higher up the axis a system is plotted, the greater the uncertainty in its score. Each system is plotted as a segment. The right and left ends of the segment reflect the scores that would be generated by the optimistic and pessimistic orderings for each of the tied groups; the trec eval score is shown as a circle; and the  X  X verage across permutations X  score as a triangle. The color of each point reflects the number of document ties for that system, in terms of Table 1 . The vertical axis is truncated at 10  X  6 , and the points plotted along that line have a score difference of 10  X  6 or below. At the top of the graph, many tied scores lead to wide score ranges, with the trec eval ordering being just one of them, usually not too far from the average overall. But for some systems the optimal-to-pessimal spread is wide, and as can be seen in the overlapping vertical extents, ties may have affected the relative ordering of the top few systems (AP At the bottom of the graph, only a tiny minority of systems have no tied scores at all; but for most evaluations the ties that do exist do not result in any appreciable score range, with optimal-to-pessimal ranges less than 10 topics.
 Ties in Other Years. We carried out the same analysis on several other TREC rounds, and found similar rates of tied scores in general (Table 1 ), and instances of systems with wide potential score ranges. However we found no further years in which the ordering of the top few systems might have been affected by the tie-breaking rule employed. We now consider whether the deliberate use of tied scores has a discernible effect on retrieval effectiveness.
 Score Approximation. Scoring documents using modern similarity computa-tions involves non-trivial amounts of arithmetic, especially if phrase components or term proximity components are being used. Regimes such as WAND [ 3 ] seek to minimize the number of documents scored, while still giving rise to exactly the same ranking for the top-k documents, an approach that meets the require-ments for being rank-safe to depth k . That is, the WAND process ensures that all of the documents in the first k places of the ranking are in their right posi-tions, but makes no guarantee for documents beyond depth k . This is a relatively stringent requirement, and other computation-pruning techniques might also be considered that provide more flexible trade-offs.
 document must be scored in a manner that guarantees that it is in the correct band of the ranking, where the bands are defined geometrically based on a para-meter  X &gt; 1. More precisely, let b 1 = 1, and thereafter let b g th band, for g  X  1, spans the ranks from b g to e g = b example, if  X  = 2, then the bands are [1 ... 1], [2 ... 3], [4 ... 7], and so on; and if and so on, with widths given by the Fibonacci sequence. The smaller the value of  X  , the smaller the band is that spans any given position in the ranking, and the nearer the approximate ranking is to the  X  X rue X  and exact ranking. In the limit, as  X  approaches 1, the retrieval system is obliged to place each document at its final  X  X orrect X  position; that is,  X  = 1 corresponds to a  X  X ull X  computation in which all document relationships are finalized. But when  X &gt; 1, we allow the retrieval system to economize on its computational costs and return groups of documents [ b g ...e g ], with equal scores assumed within each band. Worst-Case Bounds. It is straightforward to show that when  X &gt; 1the first group containing more than one document starts at rank v = b 1+ 1 / (  X   X  1) . That fact implies that the approximate scoring mechanism is rank-safe to depth v  X  1, and more generally, allows bounds on the imprecision in scores to be computed. For example, consider the metric reciprocal rank (RR). With the v th group the first one with multiple documents in it, the loss of score that can arise when permutation-based averaging is applied is given by where the bound arises because the worst situation is when the original run has its first relevant document at rank b v , and no other document in that group is relevant. Table 2 gives some  X  RR values; when  X   X  2, all are less than 0 . 1. It is also possible to compute worst-case differences for rank-biased precision (RBP, see Moffat and Zobel [ 7 ]). In the case of RBP, the maximum difference score difference arises when the run has a sequence of relevant documents at the start of each of its groups, followed by non-relevant documents for the rest of each group. The exact number 1  X  t g  X  ( e g + b g ) / 2 of relevant documents required in the initial run for the g th group varies according to both p (the RBP parameter) and  X  , and is chosen independently in each group to maximize the difference g th group. The overall bound on the difference,  X  RBP, is the sum of the group maximum differences. Table 2 includes  X  RBP differences for two values of the RBP parameter p . Recall-based metrics such as average precision (AP) cannot be analyzed as readily, because assuming additional documents to be relevant might decrease rather than increase the score. Experimental results showing that practice that AP has less divergence of scores than does RBP are presented in the next subsection.
 Effectiveness Score Differences in Practice. Given these worst-case bounds, the next question we ask is this: to what extent does an allowance for rank-based score imprecision affect effectiveness scores in practice? To respond to this question, we again make use of the 1998 TREC7 resources, taking the same system runs as were already examined in Sect. 3 , and for each run, mapping it to a set of equivalent banded runs based on a set of  X  values, with the documents ranked in band g in each of those runs assigned a synthetic score of 1 /g .The original system scores that were part of the TREC7 data were ignored as the grouping operation was being carried out, and original file order was used as the reference point for each run. As already detailed in Sect. 3 , 23 low-scoring systems were removed as part of the experimental methodology.
 of box-whisker elements using four different effectiveness metrics and a single representative value of  X  =1 . 4. In all cases the score difference calculated is the across-permutations computation that was illustrated in Sect. 2 when applied to the deliberately-tied rankings, subtracted from the score the same metric achieved on the original submitted ranking for that same topic. We followed standard protocols and assumed that unjudged documents were not relevant for the purposes of scoring the runs.
 process is small, and that there are nearly as many system-topic combinations that gain from the approximation process as there are that lose from it. Most RR values are unaffected (both quartiles are zero, for all of the  X  values tested), and the two deep metrics (RBP0.85 and AP) also have small inter-quartile ranges on the computed score differences. The average original metric scores across all system-topic combinations for RR, RBP0.5, RBP0.85, and AP are, respectively, 0 . 6939, 0 . 5556, 0 . 4677, and 0 . 2311; and hence the smaller AP score differences are in part a matter of relative scale. The shallow metric RBP0.5 suffers the most from the score grouping process; even so, it is only when  X &gt; 1 . 5, the first value for which ranks 2 and 3 are placed in the same group, that the differences are large. When  X   X  2 the first group always contains a single document. be regarded as being significant. To generate the table, each of the 80 systems was scored for the 50 topics using the original runs, and then re-scored using the grouped runs. The set of original topic scores was then multiplied by 0 . 99, and compared to the grouped scores, using a one-tail paired t -test. If a p value less than or equal to 0 . 05 was generated by that test, that system was counted as being one for which the grouping process degraded the system score by 1 % or less. The closer the count of such systems is to 80, the greater the confidence we can have that the grouping process will not give notably inferior system scores overall, where  X  X otably inferior X  is defined (at first) as being a 1 % degradation in measured score. Those values are shown in the left half of Table 3 , and the corresponding counts when  X  X otably inferior X  is defined as being a 3 % degra-dation are shown in the right half. The relationship between  X  and score fidelity is reflected by the decreasing numbers down each column of the table, and as  X  increases, the possible implications of changes in score also increase. When the  X  X olerable degradation limit X  was further reduced to 95 %, all 16 entries for metric and  X  were 80.
 System Comparison Sensitivity. Effectiveness measurements are also used to compare systems in a pairwise manner. In a final experiment, we explore the implications that score rounding has on the ability of metrics to differentiate between systems. The normal approach to comparing systems is to take their computed scores across a set of topics, and perform a paired t -test to explore the null hypothesis that the two systems are in fact the same. The process of carrying out the t -test generates a p value; the smaller the p value, the smaller the chance that the two systems being compared are giving the same performance on the data used. To establish significance, a threshold value  X  is employed, often  X  =0 . 05, with p  X   X  being regarded as a significant outcome.
 took the 50 topics of the TREC7 collection and the 80 runs associated with it that we have been using, and computed, for each of eleven different values of  X  , the set of p values generated for the 80  X  79 / 2 distinct system pairs. In all cases when  X &gt; 1, the averaging processes described in Sect. 2 were used; when  X  = 1, each run was processed in sorted-by-score order, and then the scores were discarded.
 to distinguish between systems using a statistical test (the discrimination ratio of the metric, see Sakai [ 11 ]), across the four metrics used in our experiments. For example, the plot in the lower-right for AP shows when  X  =1 . 0that62 . 2% of the system pairs yield  X  X ignificant at p =0 . 05 X  comparison outcomes; at negatives. The situation is similar for the other metrics, with the discrimination ratios (down to 45 % for RR) determined primrily by the effective evaluation depth, and only a small fraction of false positives and negatives. We have explored the impact of score ties on the evaluation of retrieval system effectiveness, as measured using binary relevance judgments and three estab-lished effectiveness metrics. Ties have the potential to affect system comparisons, and using TREC data, we showed that a small number of systems did indeed generate runs with very ambiguous score outcomes, but that  X  fortunately  X  the overall conclusions from those rounds of experimentation were unlikely to have been compromised. We further demonstrated that allowing a controlled group-ing of scores in runs  X  in a sense, permitting the deliberate introduction of ties  X  resulted in only small changes in the ability to compare systems. This approach represents a novel direction in which retrieval efficiency improvements might be achieved. We have not yet addressed the question of how those efficiency gains might be achieved, and a clear direction for future work is to reexamine the com-putation embedded in standard similarity scoring regimes and existing dynamic pruning heuristics, to identify and measure ways in which processing economies might accrue through the use of inexact scoring.
 Previous investigations [ 2 , 12 , 14 ] have explored the reliability and quality of the collected judgments; it may be that the pooled documents can be stratified according to the groups they appear in, and less emphasis placed on judgment quality for deeper pools, relying instead on averaging effects to preserve overall evaluation quality.

