 In this research we propose to derive new features based on data samples X  local information with the aim of improving the performance of general supervised learning algorithms. The creation of new features is inspired by the measure of average precision which is known to be a robust measure that is insensitive to the number of retrieved items in in-formation retrieval. We use the idea of average precision to weight the neighbours of an instance and show that this weighting strategy is insensitive to the number of neighbours in the locality. Information captured in the new features al-lows a general classifier to learn additional useful peripheral knowledge that are helpful in building effective classification models. We comprehensively evaluate our method on real datasets and the results show substantial improvements in the performance of classifiers including SVM, Bayesian net-works, random forest, and C4.5.
 H.2.8 [ Database Applications ]: [Data mining] Local information; average precision; classification
As one fundamental assumption made in the paradigm of lazy learning methods, local information such as nearest neighbours provide insightful information on the character-istics of data instances. The use of such local information inspires us to design a new feature creation method for tack-ling supervised learning problems with the target of improv-ing the performance of any general classifiers. Our method utilize local information of each training sample to create extra new feature space, which captures additional informa-tion that is helpful for training highly effective learners.
The values of new features of an instance i are derived from the class labels of training data instances that are nearest neighbours of i . While distance-weighted nearest neighbours are one common way of making use of the local information, we employ average precision for deriving the new feature values, which we will show advantageous than pure distance-weighted methods. We show that by calcu-lating average-precision-weighted difference between two (or more) class labels among neighbours of an input sample, we can obtain extra valuable information in association with the class label of this sample.

By using average precision to create new features for su-pervised learning tasks, our work targets to build on the connection between the community of information retrieval and that of knowledge management. Specifically, we make the following contributions in this research: 1. We propose to use data samples X  local information to 2. We define the values of the new features by using aver-3. We perform comprehensive experiments on 41 real data
The rest of the paper is structured as follows. In Section 2 we briefly review related work. In Section 3 we explain the average precision weighting algorithm, and provide our feature creation strategy. Section 4 reports experiments and Section 5 concludes this paper.
Many recent studies have made efforts to improve the per-formance on speed and accuracy for lazy learning methods such as k -nearest neighbour ( k NN) classifiers. Novel meth-ods including Neighbour Component Analysis [1], Large Mar-gin Nearest Neighbour [2] and Weighted Distance Nearest Neighbour [3] have mainly focused on the transformation of data set or the selection of k valuetoimprovetheper-formance of k -nearest neighbour method in terms of com-putational speed and accuracy. Goldberger et. al. [1] proposed Neighbourhood Component Analysis (NCA) for learning a Mahalanobis distance measure. This method as-sumes that Euclidean distances in space transformed by Ma-halanobis distance metrics can be used for calculating the probability with which a point will be correctly classified. Despite the simplicity of NCA objective function, it per-forms the same or slightly better than that of other dis-tance measures (Euclidean distance, whitening, and RCA) for k NN. Tao Yang et al. [5] suggested Weighted Distance k -Nearest Neighbours(WD k NN) method which can improve k NN method rather than NN method. Liu and Chawla [4] proposed a new weighting method for k NN to improve per-formance of k NN classifier in imbalanced dataset. Their method employs the confidence of a data instance X  X  attribute values given a class label. Zhang and Zhou [7] proposed the modified application of k NN for not single-label learning but multi-label learning. Their proposed system suggests the category vector to determine multi-label based on max-imum a posterior principle. Galvan et al. [8] introduces the extended application of lazy learning strategy. This strat-egy can be used for pre-processing of eager learning methods such as decision tree. There has been applications that used k NN as data transforming method. But existing applica-tions are for the additional function such as multilabel clas-sification [7] or require repetitive computation for building a classifier [8].

Motivation for our methods : The above approaches are mostly based on pure distance weighting mechanism (e.g., additive inverse or multiplicative inverse of Euclidean distances), which is sensitivity to subtle changes on the or-dering of neighbours as well as the selection of neighbour number k . This motivates us the design of a new weighting method that can overcome these shortcomings. Besides, we also study the application of local information for improving the information explicitly expressed in training data, which allows the performance improvements of an arbitrary gen-eral classifier.
Before we elaborate on our neighbour-weighting method, we first review the weighting strategies used in a standard k NN classifier. Given a test instance D t , the simplest k NN method employs a majority voting strategy which when un-der no weighting (NW) is computed as in the following: where l t is the expected label of D t , C denotes all class labels, and  X  ( D t ) is a set of ordered k nearest neighbours {
N 1 ,N 2 ,...,N k } of D t . N i is nearer than N j for i&lt;j . l ( i ) defines a function which tells whether the instance i is labelled as c . To incorporate distances, a weighting scheme gives more weights to the neighbour with closer distance [4]. For that reason, neighbours are weighted with the multi-plicative inverse (MI) or the additive inverse (AI)
AI: l t =argmax where dist max denotes the largest distances between D t and its neighbours.
Our approach considers the precision of classes of k -nearest neighbours according to their distances to the test instance. In the domain of information retrieval, a precision is the frac-tion of relevant items in all searched items, and the average precision (AveP) is defined as where P ( r ) is the precision at r th rank and rel ( r ) indicates whether the item at r th rank is relevant (returning value 1) or not (returning 0).

In our approach, analogical to k ranked items, there are k neighbours found from the training data which are asso-ciated with specific classes. In other words, we compute the average precision for each neighbour X  X  class label by taking its rank into consideration. In addition, in order to handle the possible influence of class imbalance (i.e., the scenarios where training data samples are dominated by a majority class), we use the expectation of each class in the overall training set to adjust the average precision values.
We define the average precision weight (APW) for the r th (1  X  r  X  k ) neighbour associated with class label c as: where ( i, c, D t ) defines the total number of neighbours which have class c between the first neighbour and the i th neigh-bour of the data instance D t : This measure has the same meaning as the number of rel-evant items. N ( D t ,j ) indicates the j th closest neighbour of test data instance D t . The following example gives an illustration on how we compute average precision to weight the neighbours.

Example 1: In the synthetic example shown in Figure 1, sample A is the test data instance which has 2 neighbours of + class followed by 3 neighbours of  X  class. Notations d1, d2, d3, d4, and d5 represent distances from the 5 neighbours to A. Following Eq. 5, the APWs of the 5th neighbour are calculated in the following way (assuming that there are 10 instances for + class and 20 instances for  X  class in the training data).

We define the values of the features as the difference be-tween APW of positive class and that of negative class. Al-gorithm 1 explains our concrete method for creating the new features in training data, with a demonstration shown in Fig-ure 1. Note that for both training and testing data, we use the original training data to find data samples X  neighbours and to create new features. In our algorithm, the value of a new feature indicates how far the instance is to a specific class in k -nearest neighbour locality. The information about how large the value is plays an important role of indicating the class distribution in the locality. Figure 1: Abstraction of k -nearest neighbours on space Algorithm 1 The feature creation algorithm When there are erratic changes in distances among data, ex-isting distance weighting approaches (such as MI and AI as in Eq. 2 and 3) can be easily affected in the accuracy of de-ciding the class, especially when the value of k changes. This is one of the reasons that distance weighting scheme is not as popular as the standard equal weighting k NN method. However, when the precision over the class of each neigh-bour is employed for calculating the weight, each class will be weighted by its rank in the ordering in addition to the nu-merical value of its distance. The total number of instances for a class reflects the expectation of that specific class. As a result, the average precision combined with distance and expectation of class weighting will be less affected by dra-matic discrepancy between distances among data. This is also the same observation on the insensitivity of average pre-cision to the number of items retrieved in the information retrieval domain. Therefore, we expect that our approach will show more stable performance compared with distance-only weighting metrics, even when the data distribution is unstable in its original feature space with respect to the number of neighbours. As we will show empirically, we ex-amine this hypothesis through practical experiments and the results show better performance than that of other methods such as majority voting and distance weighting.
Our experiments are conducted with two objectives. Firstly, we measure how much our novel weighting method APW can enhance the performance compared with other weight-ing methods such as NW (No Weighting), MI (Multiplicative Inverse), and AI (Addicative Inverse), where we use k NN as the base classifier. Secondly, we evaluate the performance improvements brought by our new features for general clas-sifiers such as Bayesian networks, C45 decision tree, random Figure 2: An illustration on the feature creation procedure using APW. Each data instance is associated with additional feature values which are based on the APW values calculated by its neighbours. forests, and SVMs (with linear kernels). Our experiments are conducted using 41 datasets from UCI [10] shown in Table 1. For data sets that have more than two classes, we keep the majority class as the positive class and com-bine the other classes as the negative class. Our proposed method is implemented in Weka [9], which also contains the other classifiers that we use in the experiments (with their default parameter settings).

We compare and analyse our APW method against exist-ing weighting methods such as NW, MI, and AI (as defined in Eq. 1 to 3 respectively) using k NN as the base learner. Figure 3 shows the performance in F-measure according to different settings of k values, where the x-axis is the number of neighbours used and the y-axis is the F-measure. Due to space limit, we only show results for 9 of the 41 data sets in experiments (the data sets not shown in the figure make the same conclusions as those shown). From the results, it is easy to see that our APW method gives more stable and robust performance compared with other methods, and is much less sensitive to changes of k.
We evaluate our feature creation strategy by examining the improvements on classification accuracy using the fol-lowing different classifiers: Bayesian networks, C4.5, Ran-dom Forest, and SVM with linear kernels. We control the values of k from 1 to 15, and inspect the performance in terms of average accuracy on 5x2 cross-validation. The sta-tistical significance test is conducted on the basis of paired t -tests.

Table 2 shows the comparisons between classifiers learned on data with new features and those learned from original data. In the table,  X  X /D/L X  indicates the total count of  X  X in X ,  X  X raw X , and  X  X ose X .  X  X in X  means significant improve-ment with 95% confidence in paired t -tests, whereas  X  X ose X  means worse performance with 95% confidence. Besides, we use two more measures to evaluate the classification im-provements:  X  X RI X  denoting average relative improvement, and  X  X AI X  denoting average accuracy improvement, calcu-lated in the following way: Figure 3: Comparisons of weighting methods. The x-axis represents the number of neighbours used, while the y-axis represents F-measure. The solid lines represent Average Precision Weighting, the dotted lines represent Multiplica-tive Inverse Distance Weighting, the cross marks represent Additive Inverse Distance Weighting, and the triangles are for No Weighting.
 where AO i is the accuracy of an original classifier and A accuracy of the classifier learned with APW features.
As shown in the Table 2, the classifiers trained on data with extra new features show better performance compared those on original features by clearly more  X  X ins X  than  X  X oses X . In many case of the classifiers, more than half of datasets are associated with substantial improvements. We can observe that in many case the performance of the classifier doesn X  X  change dramatically, which suggests that APW is stale with respect to changes of k.
In this paper we have proposed the use of average preci-sion as a highly effective weighting method to capture local information of data samples. This novel method has more stable and robust performance compared with conventional distance-based weighting methods. Importantly, we use this weighting method to create new features so that any general classifier can benefit from training on them. Experiments on real datasets illustrate that our method is less sensitive to changes of the number of neighbours, and at the same time brings forward substantially improved performance in exist-ing popular classifiers. In future, we plan to experiment the use of our method in multi-label classification problems. [1] Goldberger, J. and Roweis, S. and Hinton, G. and [2] Weinberger, K. Q and Saul, L. K, ADistancemetric [3] Jahromi, M. Z. and Parvinnia, E. and John, R. , A [4] Liu, W. and Chawla, S., Class confidence weighted k NN [5] Yang, T. and Cao, L. and Zhang, C., A novel prototype [6] Dudani, S. A, The distance-weighted k -nearest-neighbor [7] Zhang, M. and Zhou, Z.H., ML-k NN: A lazy learning [8] Galv  X  an,I.M.andValls,J.M.andGarc  X   X a, M. and Isasi, [9] Hall, M. and Frank, E. and Holmes, G. and Pfahringer, [10] Bache, K. and Lichman, M. (2013). UCI Machine
