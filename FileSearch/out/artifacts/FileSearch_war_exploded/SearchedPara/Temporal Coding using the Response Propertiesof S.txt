 that is consistent with the neural code.
 of the next spike.
 delays in order to learn desired spike times. 2.1 The Theta Neuron The theta neuron is described by the following differential equation: to effect. are two fixed points: An unstable point  X  + 2.2 Synaptic interactions The input current I is the sum of a constant current I i  X  1 ..N indexes the synapses: Synaptic currents are modeled as Diracs : I naptic neuron i , and w Figure 2: Response properties of the theta model. Curves shows the change of firing time t of a neuron receiving a Dirac current of weight w at time t . Left: For I regularly ( I different slopes. Right: Response for I equilibrium point ( I might cancel the spike if it occurs early.
 describes how the neuron converts input spike times into out put spike times. 2.3 Learning rule actual spike times t potential  X  .  X   X  spike. A small modification dw that  X  is between the unstable point  X  + spike on each of its synapses, and that all synaptic weights a re positive. Let t arrival of the action potential on synapse j . Let  X   X  after) the synaptic current: We consider the effect of a small change of weight w ordered, ie : t The partial derivative of the spiking time t In this expression, the sum expresses how a change of weight w spikes, for j &gt; i . The j th terms of this sum depend on the time elapsed between t have no a priori information on the distribution of t which yields : Note that this expression is not bounded when  X  + mostly determine the firing time t it is necessary to extend the learning rule to the case  X  + follows: 2.4 Algorithm is considered to be equal to the duration of the trial.
 sponding to all spikes is summed.
 values of the time step mean that simulations take more time. m connections ( w We use I we expect this network to perform Principal Component Analy sis (PCA) in the time domain. provided by a delayed version of the input burst (echo).
 values, and late spikes code for values close to zero.
 a centering term to the learning rule: where  X   X  IR and  X  between the firing time t after each trial: We used I Y neurons: We used  X  variance of the input. 4.1 Principal Component Analysis of a Gaussian distributio n representation of a 2D variable, that minimizes the mean-sq uared reconstruction error. The input was encoded as follows: where  X  to Figure 5: Principal Component Analysis of a 2D Gaussian distribution . The input vector was network X  X  activities. Each branch corresponds to a firing or der of the two output neurons. the approximation made in deriving the learning rule.
 branches can be viewed as the base vectors used in the compres sed representation in Y . are not allowed. 4.2 Encoding natural images with random values between 0 and 0 . 3 .
 was 0 . 11 ms /pixel.
 values have the ability to strongly delay the output spike, a nd even to cancel it. Hence, the network could be performing something similar to Nonlinear PCA [10]. neuronal dynamics.
 Figure 7: Natural image and reconstruction from spike times . The 512  X  512 image from the times the error made by PCA. neurons, where the sign of the PRC is not constant.
 Acknowledgments The author thanks Samuel McKennoch and Dominique Martinez f or helpful comments.
