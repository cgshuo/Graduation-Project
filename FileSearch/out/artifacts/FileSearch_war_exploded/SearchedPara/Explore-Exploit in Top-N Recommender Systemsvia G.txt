 We address the challenge of ranking recommendation lists based on click feedback by efficiently encoding similarities among users and among items. The key challenges are threefold: (1) combinato-rial number of lists; (2) sparse feedback and (3) context dependent recommendations. We propose the CGP RANK algorithm, which exploits prior information specified in terms of a Gaussian pro-cess kernel function, which allows to share feedback in three ways: Between positions in a list, between items, and between contexts. Under our model, we provide strong performance guarantees and empirically evaluate our algorithm on data from two large scale recommendation tasks: Yahoo! news article recommendation, and Google books. In our experiments, CGP RANK significantly out-performs state-of-the-art multi-armed bandit and learning-to-rank methods, with an 18% increase in clicks.
 H.3.3 [ Information Search and Retrieval ]: Relevance feedback; H.3.5 [ Online Information Services ]: Web-based Services Recommender systems; CGP RANK ; User feedback; Kernel meth-ods; Exploration X  X xploitation tradeoffs
Traditionally, recommender systems relied on supervised learning on a batch of data. Existing approaches like collaborative filtering, content based filtering or learning to rank techniques all try to learn a fixed optimal recommendation model given training data. These approaches fail to capture the dynamic nature of the user preferences and inventory. An ideal recommendation list for a given user/context has to take recent feedback into account.

Learning this optimal ordering leads to an  X  X xplore X  X xploit X  trade-off, where we need to gather information about the effectiveness of orderings, while at the same time maximizing conversions based
Work done while the author was a student at ETH, Z X rich and working as an intern at Google, Z X rich on the estimated data. Standard multi-arm bandit algorithms which solve such tasks either use similarity information or cannot select lists of items. On a web scale, this is a daunting task, for mainly three reasons: (1) There is an exponential number of lists to choose from; (2) the number of items available for recommendation is often large, compared to the relatively sparse click feedback; (3) optimality of the ordering depends on the context which in itself could be from a large set (e.g., user to whom the item should be recommended).

In this work, we propose an algorithm  X  CGP RANK  X  for (re-) ranking recommendations from click feedback. In order to address above challenges, it uses a positive definite kernel function to en-code prior assumptions about the similarity of items (and possible user features). This kernel is then used to share and exploit sparse feedback in multiple ways: across positions in the list (1), across items (2) and across users/contexts (3). CGP RANK navigates the exploration X  X xploitation tradeoff by predicting performance of as yet unexplored lists using nonparametric Gaussian process models, whose regularity is captured in the kernel function. Exploiting such regularity allows our algorithm to be statistically efficient, i.e., max-imize the benefit drawn from sparse observations about the users X  preferences.

We prove strong performance guarantees for our CGP RANK al-gorithm. In particular, we analyze its regret, i.e., the loss in conver-sions/clicks compared to using (hindsight) optimal orderings. We prove that under natural separability and regularity assumptions, the average regret vanishes. Further, we analyze the effect of increasing list sizes, and find a surprising theoretical result: Under some natural conditions, increasing the list size can parallelize exploration in a way to accelerate convergence, instead of an exponential slowdown. We extensively evaluate our approaches on two large datasets from real world recommendation tasks: Firstly, we consider news article recommendation, using data provided by Yahoo! 1 . Secondly, we evaluate CGP RANK on Google X  X  infrastructure, using clickstream data from Google X  X  ebooks store, demonstrating a significant im-provement over existing multi-armed bandit and learning-to-rank techniques. In particular, our approach provides an 18% lift in total clicks compared to the existing approaches. Our approach relates and builds on work in multiple areas.
Recommender systems, ranking and relevance: Popular tech-niques include collaborative filtering, matrix factorization and fre-quent item set mining [ 21 ], as well as learning to rank approaches [ 2 ]. These approaches usually estimate user X  X  preferences ( X  X x-http://webscope.sandbox.yahoo.com/ ploit X ) from a fixed training set collected a priori, and generally do not address how to dynamically collect data ( X  X xplore X ) for training in order to adapt to changing inventories and user bases. We em-pirically compare to learning to rank (LTR) approaches that learn from user interactions and demonstrate the benefit of our exploration strategy.
 Multi-arm bandits (MAB): For single item recommendation, MAB is a classical formalism for studying the explore-exploit dilemma incurred when optimizing an unknown payoff function with noisy feedback limited only to the choices made. Early ap-proaches such as -Greedy, UCB1 [ 3 ], do not exploit the similarity information between the choices, and thus fail when feedback is sparse. Modern research has addressed this challenge, under assump-tions of linear [ 17 ] and Lipschitz [ 12 ] continuous payoff functions, as well as functions with regularity explained by a kernel [ 24 , 15 ]. However, these approaches do not consider the challenges arising when selecting sets and lists.

Bandits for subset selection and ranking have been studied be-fore in both ordered and unordered subset selection settings [ 14 ]. In particular, the best subset selection under  X  X andit-feedback X  set-ting has been studied [ 11 ]. In [ 26 , 25 ], the authors study a similar problem under the setting that the feedback of a set is a submodular function of the concepts covered by the set, which allows to capture diversity, but no similarity among items. [ 23 ] considers choosing di-verse rankings exploiting item similarity for the problem of ranked document retrieval. Our approach, while similar in principle to some of these works, is the first to systematically exploit sharing of feedback among items, contexts and positions.

Top-N recommendations are an important subclass of recom-mendation problems [ 21 ]. Researchers have long expressed the importance of explore X  X xploit schemes in dynamic top-N recom-mendation problems [ 16 ] and also found deficiencies in using RMSE optimisation techniques for online recommendations [ 7 ]. We de-velop efficient techniques for managing the explore-exploit tradeoff and use appropriate regret measures to show a marked improvement over RMSE based schemes.
We have a set of items S = { s 1 ,s 2 ,...,s n } that we consider for recommendation (e.g., books, articles etc.). We model the rec-ommendation task as a sequential decision problem over T rounds, where, in each round t , we are given a subset of items S able for recommendation. To aid in the recommendation task, we are given a context z  X  X  , which, e.g., models the key (anchor) item that the user is currently considering, or possibly also containing features describing the user. While such context could be presented as a feature vector, our algorithm does not require such vectorial rep-resentation. Our task is to select an ordered list L t = [ s of b items out of S t that is recommended to the user. In response, we receive a stochastic vector of rewards, y = [ y [1] t ,...,y E derlying unknown reward function f : S  X Z  X P  X  R , such that the expected reward of recommending item s [ i ] t in context z position i  X  X  = { 1 ,...,b } is given by f ( s [ i ] t , z ness, f may model the click-through rate (CTR), and the rewards y t  X  [0 , 1] model whether the user clicks on the item in position i . The total reward in round t is thus and our goal is to maximize the expected cumulative reward Note that, in this model, we assume that the items in the list do not influence the rewards (clicks) received by other items, i.e., we do not model side effects of other items in the list. In the experiments, we do study the effect of relaxing this assumption. We further assume that the expected reward factorizes into a relevance term r ( s [ i ] t , z t ) (relevance measures the relatedness/interestingness of the item to the context which could be a query, user features or key item) and a position-dependent effect p ( i )  X  [0 , 1] . Without loss of generality, we assume that p (1) = 1 , position in the list can only decrease the expected reward.
Under these assumptions, to maximize the reward (clicks), we need to position the items in decreasing order of their true relevance r . For a fixed position i and context z t , the expected number of clicks received by an item s [ i ] t will be proportional to its relevance to the context, i.e., r ( s [ i ] t , z t ) .

The position dependence p can often be estimated effectively [ 6 ]. However, the true relevance r is a priori unknown, and must be estimated through experimentation. We face an exploration X  exploitation dilemma, where we have to choose between exploiting the information we have about the best ordering, and exploring alternate orderings, which may or may not lead to higher rewards.
Instead of maximizing rewards, in the following we equivalently wish to minimize the regret . Hereby, the instantaneous regret r in round t is given by r t = P b i =1 [ f ( s [ i ] t  X  , z list for context z t observed at round t . Our goal is to minimize the cumulative regret, R T = P T t =1 r t . In particular, we desire an algorithm such that the average regret vanishes, i.e., R T T  X  X  X  . Note that this is quite a stringent performance requirement: Vanishing regret requires that the algorithm learns the optimal (in expectation) mapping from context to recommendations.
Given the problem setup, we address the resulting challenges in this section. In Section 4.1, we show how we can share item feedback across positions. In Section 4.2, we discuss the use of statistical models for principled generalization of feedback to items/contexts that are not yet explored. Finally, the resulting exploration X  X xploitation dilemma is addressed in Section 4.3 lead-ing up to the specification of the CGP RANK algorithm. Before we describe the algorithm in Section 4.4 and provide theoretical guaran-tees in Section 4.5, we first highlight the key technical challenges: 1. There is a combinatorial number of possible ordered lists. 2. In many applications, click feedback is sparse, potentially 3. Once we have settled on a statistical model for learning about
Given a context (e.g., key-item), selecting an optimal ranked list of b recommendations is challenging due to the combinatorial num-ber of choices. In general, we may need to estimate the reward associated with each of the exponentially many rankings. How-ever, under our assumptions (1) and (2) that the reward of a list decomposes additively, and that the reward factors into a position-dependent effect independent of the item and a  X  X elevance X  effect that is position independent, the problem becomes statistically and computationally more tractable. If we know the position effects p ( i ) ,  X  i  X  { 1 ,...,b } , we can normalize the feedback received by an item across all positions that it has been shown at so far. That is, given context z t , if we observe y [ i ] t for some item s shown in position t /p ( i ) provides an unbiased estimate of r ( s , z t ) . Consequently, an unbiased estimate for the reward obtained when showing s in us to share feedback across positions.
In order to generalize feedback across items/contexts, we need to incorporate prior information about their respective similarities. We assume that this prior information is presented in terms of an arbitrary positive definite kernel function  X  : ( S  X  Z ) Hereby, for two item-context pairs ( s , z ) and ( s 0 , z  X  ( s , z ) , ( s 0 , z 0 ) represents our assumptions about how similar we expect the rewards of presenting item s in context z , as opposed to presenting item s 0 in z 0 are. A multitude of kernel functions are available for accurately capturing similarity among various types of data [ 22 ]. We detail our specific choice in Section 6. What are the consequences of choosing any particular kernel? We effec-tively assume the reward function r can be represented as a linear combination i.e., as a basis function expansion around a set of context X  X tem pairs (( s j , z j )) j . Such functions span the Reproducing Kernel Hilbert Space (RKHS) associated with kernel k , and the norm of r in that space, measures the  X  X omplexity X  (regularity) of function r . The perfor-mance of our algorithm, as analyzed in Theorem 1, will depend on this norm. Intuitively, if the kernel matches the regularity present in real data well, the norm will be small. Capturing similarity via ker-nels has important consequences: In particular, it allows interpreting the relevance function r as a sample from a Gaussian Process (GP) prior [ 20 ], with covariance (or kernel) function  X  . Consequently, one interprets the relevance as a collection of normally distributed random variables, one for each item X  X ontext pair. They are jointly distributed in a dependent manner, such that their covariances are given by the kernel: This joint distribution then allows us to make predictions about unobserved item X  X ontext pairs via inference in the GP model. In particular, suppose we have already observed feedback for t recom-mendations, i.e., obtained data pair ( s , z ) , its predictive distribution for r ( s , z ) is Gaussian, with mean and variance 2 given by  X  t ( s , z ) = k t ( s , z ) T ( K t + I )  X  1  X  y t , (3)  X  2 t ( s , z ) =  X  (( s , z ) , ( s , z ))  X  k t ( s , z ) where k t ( s , z ) = [  X  (( s 1 , z 1 ) , ( s , z )) ,..., X  (( s K t is the positive semi-definite kernel matrix such that K [  X  (( s i , z i ) , ( s j , z j ))] .

Choice of Kernels. Often, kernels over item X  X ontext pairs are naturally expressed as tensor products, where Hereby,  X  S : S 2  X  R is a kernel (similarity) among items, and  X 
Z : Z 2  X  R is a kernel (similarity) among contexts. This choice of kernel expresses our prior assumption of how smoothly the CTR changes over the item-context space.

The choice of kernel  X  S depends on the particular recommenda-tion problem. Often, similarity between items is given by a usually symmetric similarity function sim : S 2  X  R . A valid kernel func-tion however must additionally be positive definite (i.e., all resulting covariance matrices must be positive definite). Among various avail-able candidates, we use diffusion kernels, a family of kernels first introduced in [ 13 ]. The first step is to consider the items S as nodes in a weighted, undirected graph G , so that the weight w ( i,j ) of each edge ( i,j ) is given by the similarity function sim ( i,j ) . The diffusion kernel is then given as matrix-exponential K S = exp(  X  L ) of the graph Laplacian L of G . In the contextual setting, if the con-text is given as a key item, the same diffusion kernel can be used both for items and contexts. If the context is given in terms of user features,  X  Z can be chosen, e.g., as linear kernel  X  Z ( z , z or Gaussian kernel  X  Z ( z , z 0 ) = exp( || z  X  z 0 || 2 larity information is known between contexts, the diagonal kernel  X  ( z , z 0 ) = 1 [ z = z 0 ] can be used. When features are explicitly avail-able, we can use linear kernels, other kernels defined over Euclidean spaces or combinations thereof. In a special case of CGP RANK can recover the exact algorithms presented presented in [ 4 ] and [ 18 ] by choosing appropriate linear kernels. We employ CGP RANK both diffusion and linear kernels and demonstrate their performance. However, in several real world applications, features are not easily available either for the contexts or the items and the nature of CG-P
RANK allows us to use any kind of kernel that can be computed from the similarity information that is available.
How should we use the predictive model (Equations (3) and (4)) to make recommendations? One approach could be to greedily maximize the expected reward according to our current model (i.e., rank items in order of their predictive mean (3)). However, this approach ignores the predictive uncertainty (4). If our goal were to conduct experiments to most effectively reduce uncertainty about the model, we may instead consider to pick items according to the predictive variance (4). Such an approach however would incur much regret, since it would equally explore high-and low value items. Therefore, in each step, we must trade off experimentation (showing items we have not explored yet) and exploitation (showing items with high expected reward). One way to achieve this is linearly trade off the relative importance of the predictive mean and the predictive variance to score each candidate item, i.e., select the item s that maximizes, for the current context z t the surrogate objective using noise variance 1, according to our assumptions For Gaussian predictive distributions, this criterion captures an upper confidence bound (UCB), i.e., an upper bound on the relevance function that holds with a certain probability that can be controlled via the tradeoff factor  X  t, s . I.e., the respective weighting of the mean and variance is handled by an item and time dependent variable,  X  t, s . We show how to pick  X  t, s in Section 4.4 such that, with high probability, the UCB provides a valid upper bound on the true mean. At the same time, the choice is small enough so that the instantaneous regret provably decreases quickly over iterations.
In order to pick multiple items in each round, a first attempt would be to score every item s according to the selection rule (5), and select the b highest scoring items. However, given the regularity imposed by the kernel function, for a fixed context z , the highest scoring items are likely very similar. Thus, the resulting list will explore sets of highly related items together, in a possibly redundant manner.

Instead, it may be desirable to encourage diversity when selecting lists to explore. One natural, and computationally efficient way is to anticipate the reduction in uncertainty achieved by the items already selected. Looking at the predictive mean and variance  X  t Equations (3) and (4), it can be observed that, while  X  t the actual feedback  X  y t observed so far, the predictive variance  X  does not depend on previous feedback. We can utilize this insight in the following way. Suppose, in round t , we receive context z item s [1] t according to (5). Then, we update the predictive variance (4) as if we had already observed the feedback for the first item. The predictive mean is not updated (or equivalently, it is updated with its own prediction). Note that this will have the effect that the predictive variance  X  and hence the score (5)  X  for similar items is decreased. We now select the second item s [2] t according to the updated score, and proceed in this manner until the full list of b items has been selected.

After the ranked list has been selected, feedback y t = [ y is observed. According to our factorization assumption (2), each observation y [ i ] t in position i provides a noisy observation of the underlying relevance score E [ y [ i ] t ] = p ( i )  X  r ( s feed back y [ i ] t /p ( i ) as unbiased estimate of r ( s
We now describe how to compute a value for  X  t, s that allows us to prove rigorous bounds on the regret of CGP RANK . Note that in practice a more aggressive choice than this conservative prescription can lead to faster convergence. Our algorithm extends and gener-alizes the work of [ 24 , 15 , 8 ]. In these, the tradeoff parameter  X  ensures that, in each iteration, the true relevance function is con-tained within the constructed confidence bands (  X  t ( s )  X   X  with high probability. Similarly, for our problem, we compute  X  as  X  where C 0 b = 1 p ( b ) 2 max Hereby M is a bound on the RKHS norm of the reward function lected items 1 to i  X  1 in iteration  X  . Note that C t can be computed efficiently incrementally over the course of the algorithm. C pends on the maximum determinant of any (posterior) kernel matrix K t  X  1 ( L,L ) that can be constructed using at most b items paired with the current context. While computing this quantity exactly requires solving a combinatorial optimization problem, it can be approximated efficiently and accurately during each iteration by running a simple greedy algorithm (uncertainty sampling)  X  see the longer version of the paper for details. For several commonly used kernel functions (linear, Gaussian and combinations thereof),  X  can be tightly bounded by the simple expression  X  t = C log with suitable constants C,d 0 . It is this form that we use in the experiments.
 Algorithm 1 presents pseudo-code for our CGP RANK algorithm. The procedure GP-Inference (  X ,D ) takes a kernel function  X  and data set D , and returns the predictive mean and variance func-tions according to (3) and (4). Our analysis builds on and extends results of [ 15 ] for contextual GP bandit optimization (selecting individual items) and [ 8 ] for non-contextual GP bandit optimization with delayed feedback. We state our main result in the form of the following theorem and for reasons of space, reserve the details of the proof to a longer version of this paper.

T HEOREM 1. Let  X   X  (0 , 1) , M &gt; 0 , and  X  be a kernel func-tion, such that || r ||  X   X  M . In each round, choosing  X  in Equation 6 and running CGP RANK for T rounds, it holds that
Pr { R T  X 
The regret bound in Theorem 1 depends on the quantity  X  which quantifies the effective degrees of freedom of the kernel matrix K ( D , D ) that can be constructed from n context-item pairs. This quantity was analyzed in prior work [ 24 ], showing that for many common kernels (such as linear and Gaussian),  X  n only grows polylogarithmically in n . How can Theorem 1 be interpreted? It is instructive to consider the average regret per list slot, R can infer that, as long as  X  n grows only polylogarithmically in n (the common case), R
Tb where the O  X  notation hides logarithmic factors in T and b . Thus, for fixed list size b , the average regret per slot decays to 0 at an essential rate of O  X  ( 1  X  T ) . It grows linearly with the complexity M of the reward function r , and inversely proportional to the decay of the position effect p ( b ) .

How does the regret scale with list size? Since exp(  X  b ) =  X ( b ) , as the list size b increases, straightforward application of the al-gorithm will incur average regret per slot that increases with b . However, in the non-contextual case (or the case of a finite set of contexts), it is possible to slightly modify the algorithm, such that, as long as b = O (log T ) , it can be ensured that  X  bounded irrespective of b , at the cost of additional regret bounded by O (poly log( T )) . Thus, in this setting, one can achieve an average regret per slot of Algorithm 1 The CGP RANK algorithm Input: Kernel  X  , selection batch size, b
Initialize data set of observations D = {} . for t = 1 , 2 ,...,T do end for This result suggests that, perhaps surprisingly, as long as p ( b )  X  1 /  X  finding is further supported by our experimental results in Section 7.
Naively implementing Algorithm 1 can be prohibitively slow for large data sets. For general kernels, the data set size D grows with the number of observations Tb , and performing exact Bayesian inference according to Equations (3) and (4) requires solving linear systems in Tb variables.

Scaling GP Inference. Fortunately, much work has been done scaling GP inference to massive data sets [ 20 ], also in online/streaming settings [ 10 ]. Since such inference is the essential subroutine in CG-P
RANK , it can immediately benefit from these techniques. Further-more, in many practical applications (such as the recommendation tasks considered in our experiments), the kernel  X  is of bounded rank d , in which case inference only requires solving a linear sys-tem in d dimensions. Often, approximate solution is acceptable for practical performance.

Speeding up selection. In order to speed up the selection rule (5), another computational trick can dramatically accelerate performance. Note that, in order to evaluate (5) naively, the mean  X  t  X  1 and variance  X  2 t  X  1 ( s , z t ) has to be computed for each choice of s  X  S t . Inspecting Equations (3) and (4), it can be seen that com-puting (3) requires solving only one linear system, while computing (4) requires solving |S t | linear systems. By exploiting the fact that, in GPs, predictive variance must monotonically decrease, i.e.,  X  ( s , z )  X   X  2 t +1 ( s , z ) , previous estimates can be used as upper bounds. This insight allows to use priority queues to dramatically reduce the number of linear systems that need to be solved. Similar ideas have been exploited in [8].
 Delaying feedback. Instead of continuously performing updates, CGP RANK can be accelerated by reusing the same recommenda-tion multiple times, accumulating feedback and performing delayed updates. However, delaying feedback for long periods of time can incur higher regret in the intermediate period where a fixed subop-timal ordering is chosen. Hence, careful choice of the frequency depending on the problem domain and taking into consideration speed of accumulation of feedback is an important aspect of scaling up CGP RANK .

For instance, exploiting some of these techniques, in our exper-iments with the Yahoo! dataset and using linear kernels, we were able to achieve an average selection time of 0.4 millisecond per slot including updating the model based on non-delayed feedback (timed on unoptimized C++ code compiled using gnu compiler and running on a single core of a Quad Core Intel Xeon E3, 3.5GHZ machine with 32GB RAM).
We extensively evaluate CGP RANK on two real-world recom-mendation tasks. The following questions guide our experimental study: 1. Can we exploit similarity to achieve accelerated convergence? 2. Can one parallelize exploration across lists to achieve faster 3. Can improved performance be achieved by incorporating con-
Benchmarks. In our experiments, we use the following ap-proaches and compare the performance:
Data set. We first evaluate our algorithm on clickstream data made available by Yahoo! as part of the Yahoo! Webscope program [ 1 ]. Specifically, we use the R6A dataset containing a part of the user view/click log for articles displayed on the Today Module of Yahoo! during ten contiguous days. This data was collected in May 2009 and the displayed article was chosen uniformly at random from the list of available articles. This makes this dataset ideal for unbiased, offline evaluation of exploration X  X xploitation approaches. One can find detailed information on the dataset, the data collection methodology and an explanation of the unbiased offline evaluation in [ 1 , 18 , 5 ]. The dataset consists of more than 45 million lines of log. Each line consists of the following information: http://http://www.lemurproject.org/ logs. (c): Similar to 1(b). Contextual list selection task with b = 4 . For confidentiality, we only present normalized numbers. Note that CGP existing non-adaptive method and also outperforms top-b UCB1. Parameter choice. We initially chose the first 10% of the log entries as initialization data for optimizing parameters of the algorithms, training the learning-to-rank methods, and also to extract user fea-tures for clustering (see below). We used the results of this clustering for the contextual versions of CGP RANK -G and UCB1. We ignored this part of the data for all further evaluations the results reported are completely evaluated on the remaining 90% of the log entries.
The hindsight-fixed benchmark for these experiments used a weight vector obtained by solving to a linear regression problem for the click prediction task based on the entire log. The result is a single weight vector that maps any item-context pair ( s , z ) to an expected click probability.

The nature of the data set makes linear kernels the ideal choice for this task. With this choice, CGP RANK -Lin for single-item selection corresponds to the L IN UCB-H YBRID algorithm as provided in [ 18 ]. In order to use CGP RANK -G on this dataset, we require a kernel function on the articles. We decided to model the articles as nodes in a graph. The weight on the edge connecting any two articles is sim-ply the euclidean distance between their feature vectors. This choice allows us to compute a diffusion kernel on the articles. Note that computation of an appropriate diffusion kernel requires tuning of the heat parameter  X  . For our evaluations, we used the article features and their corresponding clickthrough rates to tune the parameter alpha  X  on the first 10% of the data.

For contextual versions of CGP RANK -G and UCB1, we used a simple technique of clustering the user features given in the logs, and maintaining one instance of all the evaluated algorithms per cluster (corresponding to a diagonal kernel  X  Z ). We used k -means cluster-ing on the user contexts extracted from the initialization bucket with k = 10 , picking the best solution from multiple random restarts. During the actual evaluation, in each round t , the user context given in the log line was mapped to its nearest cluster center, and z set to this cluster index.

We carry out experiments both in the contextual and non-contextual setting, and vary the size of the lists selected. For list size b = 1 , we use the actual click feedback given by the log. For b &gt; 1 , we use simulated click feedback as described above. In order to maintain consistency over the amount of feedback available, we randomly sample portions of the actual log during our simulated feedback since the rejection sampling technique used for b = 1 provides feedback once in 20 iterations on an average.
Feedback. While the goal of our work was to choose the optimal ordered list for recommendation, this dataset only contains click stream data for the choice of a single item. Hence, for the purpose of evaluating the list selection procedures, we simulated list feedback. The feedback for an article at a given position depended on the base Clickthrough rate (CTR hereafter) of the article and the bias introduced by the position. Hence, given context z , if an article s with base CTR r ( s , z ) was shown at position j with a bias of p ( j ) , then the stochastic feedback for this placement was simulated as a Bernoulli draw with click probability r ( s , z ) p ( j ) . Estimating the positional effects p ( j ) on CTR is a well studied problem. The base CTR r ( s , z ) used in the simulation was computed as the CTR predicted by the hindsight-fixed algorithm for the given ( s , z ) pair.
Data set. We carry out our second set of experiments on click-stream logs from the Google ebooks store. Here, the recommenda-tion task is, given a key book (context) the user is currently exploring, recommend a set of related books that the user may also be inter-ested in. At the time of this work, Google used metadata information about the books and also inputs from other sources to compute the ordering of the related list of books to any given key book. This is a good first approximation of true  X  X elatedness X  in the absence of any real click data. But, as we receive feedback in terms of clicks on the recommendations, we can modify the original ordering in order to reflect the tastes of the users and this new ordering represents the true  X  X elatedness" of books in the presence of a large number of clicks. In this dataset, the only context available was the current item being viewed (key item).

We evaluated our algorithm on the clicklog data of Google X  X  book store that was collected over 42 days in the beginning of 2012 . Each event in the anonymized click log data consists of two components: We estimated the unbiased position effect on the CTR using standard techniques.

Parameter choice. For each key book z , we created a graph struc-ture capturing the initial ordering given by the metadata-similarity in terms of the edge weights. Because of computational considerations, we only consider the similarity between the key book z t and all of its candidate books S t , but not the similarities between the candidate books themselves. This results in a star graph with the key book in the center. The weights on the edges are the similarity scores between the books as computed using the metadata of the books. Using the obtained related graph G , the diffusion kernel K can be computed using techniques presented in [13].

Feedback. Based on the data, we simulate feedback for each item when it was displayed in a specific related list. Note that the clicks are being aggregated over users and sessions such that we group feedback on a specific related list. Position independent base CTR models how much the users prefer seeing a related book s in the recommendation list of key item z . We define this CTR as the number of position-normalized clicks that item s got while being shown in the related list for key book z , divided by the position-normalized number of times s was shown as a related book for z .
Based on these estimates, we use offline evaluation techniques to simulate feedback for any new ordering. Since we computed the position independent feedback for each of the items in the original list and we also have the position weight terms p ( j ) , given context z , we simulate feedback for any item s with base CTR r ( s , z ) at position j with position weight p ( j ) by sampling from a Bernoulli distribution with bias r ( s , z ) p ( j ) .
Performance comparison. The results on the Yahoo! webscope dataset presented in Figure 1(a) and the results from the Google books evaluation presented in Figure 2(c) show that all versions of CGP RANK offers a consistent performance improvement over approaches that do not take item similarity into account. The abil-ity of CGP RANK to generalize feedback received from few items to a larger set of related items allows it to quickly estimate their relevance. Also, in a dynamic system where new items regularly become available for selection, this feature of CGP RANK allows it to reliably estimate relevance of new items faster. Thus, CGP is well suited to handle the cold start problem in recommendations. For the Yahoo! Webscope dataset, CGP RANK produced an overall final CTR of 0 . 0496 for the context-free setting and 0 . 0603 for the contextual setting, which compares favorably with the Ideal policy ( 0 . 0559 and 0 . 064 ). In the case of the Google books dataset, CG-P
RANK outperformed the then-existing algorithm by a margin of 18% . These findings substantiate our first hypothesis, that sharing feedback across similar items helps.

Performance without features. We decided to further test the performance of CGP RANK in settings where no explicit features are available. Instead, similarity information was provided in the form of a kernel function and the similarity between contexts was taken into account by clustering. The evaluation of this is also presented in Figures 1(a) and 1(c). While the overall performance of CGP G was outperformed by CGP RANK -Lin (CTR of 0.0574 compared to 0.0603 for single item selection), it still manages to perform well and is applicable even when explicit features are not present.
Figures 1(b) and 1(c) present the results of the list selection task with b = 4 . From the plots, it can be inferred that adaptively learning the order is better than any fixed model learnt from training data. It can also be seen that even using context in an arguably naive manner (in terms of clustering users) provides a substantial improvement over not using context. For the single article selection case with actual click feedback, there was a 14% increase in the CTR while utilizing context information, further substantiating our hypothesis that exploiting context helps.

Relaxing the independence assumption. CGP RANK assumes that items do not influence the feedback of other items within a list. This simplifies the algorithm and its analysis, but is not necessarily true in practice. Hence, we relax this assumption by clustering the articles and model the user as diversity-preferring by ensuring that at most one article from a cluster is clicked on in a round. The recommended list might still contain multiple items from the same cluster. Although the total regret is  X  5% more than in the independent case, CGP RANK still outperforms all other baselines and is better than the next best baseline by  X  10%.

Parallelizing exploration within lists. In our analysis in Sec-tion 4.5, we found that, perhaps surprisingly, increasing the list size can lead to accelerated convergence  X  at least under certain technical assumptions  X  as exploration is  X  X arallelized X  across list slots. We empirically assess this finding in Figure 2(b) which considers the per-slot average regret. In this experiment, we apply CGP on the log data, using different batch sizes b . As b increases, faster convergence is obtained in relative terms compared to the hindsight-fixed predictions.

The experiments with multiple item selection corroborate our theoretical claims that having to select multiple items is beneficial if we consider the per slot regret as long as we gather enough feedback in the lower ranked slots. From the figures, it can be seen that while average regret per slot decreases as we move from single item selection to 2 and then 4 items, there is diminishing returns when we select 8 items. This is due to the low position CTR at positions higher than 4 and also the sparse nature of feedback in the problem. As long as p ( i ) is high enough to garner enough feedback, the opportunity cost incurred by making mistakes down the order is less than that at the top of the list. This is because of the decreasing expected CTR p ( i ) as position i increases. During the actual execution, it can be noticed that CGP RANK quickly settles on the top positions while continuing to experiment with different articles down the order.
 To assess the quantitative dependence of the regret on the smallest CTR p ( b ) , we conduct an experiment varying p (2) (for b = 2 ), shown in Figure 2(a). We note that the ratio of ideal clicks to clicks garnered by CGP RANK increases as p (2) decreases. While our theoretical results suggests a much stronger dependence on p ( b ) , the effect is not as dramatic in the experiments. This is explained by the fact that our bounds are high probability bounds and in reality, the feedback is more benign.
We have developed a novel algorithm  X  CGP RANK  X  for effi-ciently reranking lists to reflect user preferences over the items displayed, taking context into account. It exploits assumptions on similarity of items and contexts as given by a positive definite kernel function, and separability assumptions on position depen-dence. In this way, CGP RANK is able to share sparse feedback across positions, items and contexts. We proved strong theoretical guarantees on its regret, indicating that, perhaps surprisingly, un-der natural assumptions, parallelization of exploration across lists can help accelerate convergence. Support for computational and statistical parallelism makes it a suitable candidate for adoption for large-scale online recommendation engines. We extensively evaluate CGP RANK on two real world recommendation tasks. On the Google ebooks recommendation task, CGP RANK achieves 18% click lift over the previous state of the art recommender system. We also showed significant improvements over state-of-the-art bandit and learning-to-rank approaches on the Yahoo! Webscope dataset. The experiments with the Yahoo! dataset demonstrates empirically our claim about being able to exploit similarity between items and context information and also shows that parallelizing exploration across the list leads to better performance compared to the single item selection problem. We believe our results present an impor-tant step towards addressing challenging, large-scale exploration X  exploitation tradeoffs in practical recommender systems. This research was supported in part by SNSF grant 200021_137971, ERC StG 307036 and a Microsoft Research Faculty Fellowship. [1] Yahoo! webscope program. [2] E. Agichtein, E. Brill, and S. Dumais. Improving web search [3] P. Auer, N. C. Bianchi, and P. Fischer. Finite-time Analysis of [4] W. Chu, L. Li, L. Reyzin, and R. E. Schapire. Contextual [5] W. Chu, S.-T. Park, T. Beaupre, N. Motgi, A. Phadke, [6] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [7] P. Cremonesi, Y. Koren, and R. Turrin. Performance of [8] T. Desautels, A. Krause, and J. Burdick. Parallelizing [9] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient [10] R. Gomes and A. Krause. Budgeted nonparametric learning [11] S. Kale, L. Reyzin, and R. E. Schapire. Non-stochastic bandit [12] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits [13] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and [14] W. M. Koolen, M. K. Warmuth, and J. Kivinen. Hedging [15] A. Krause and C. S. Ong. Contextual gaussian process bandit [16] R. Lempel. Recommendation challenges in web media [17] L. Li, W. Chu, J. Langford, and R. E. Schapire. A [18] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased offline [19] D. Metzler and W. Bruce Croft. Linear feature-based models [20] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes [21] F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor, editors. [22] B. Scholkopf and A. J. Smola. Learning with Kernels: [23] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning [24] N. Srinivas, A. Krause, S. Kakade, and M. Seeger.
 [25] M. Streeter, D. Golovin, and A. Krause. Online learning of [26] Y. Yue and C. Guestrin. Linear submodular bandits and their
