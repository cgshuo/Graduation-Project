 Web Page segmentation is a crucial step for many applica-tions in Information Retrieval, such as text classification, de-duplication and full-text search. In this paper we de-scribe a new approach to segment HTML pages, building on methods from Quantitative Linguistics and strategies bor-rowed from the area of Computer Vision. We utilize the notion of text-density as a measure to identify the individ-ual text segments of a web page, reducing the problem to solving a 1D-partitioning task. The distribution of segment-level text density seems to follow a negative hypergeometric distribution, described by Frumkina X  X  Law. Our extensive evaluation confirms the validity and quality of our approach and its applicability to the Web.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Algorithms, Experimentation Web Page Segmentation, Full-text Extraction, Template De-tection, Noise Removal
Identifying and retrieving distinct information elements from the Web has increasingly become difficult. Besides the main content (e.g., an article) modern web pages also contain a bouquet of other textual elements such as navi-gation menus, user comments, text ads, snippet previews of related documents, legal disclaimers etc. Separating (seg-menting) these distinct elements and eventually classifying them into relevant and non-relevant parts is essential for high-quality results. We consider three key application ar-eas for web page segmentation: (1) De-duplication . Identical content information may be presented using different web page layouts. (2) Content Extraction . Besides the obvious benefits for Web-based news clipping etc., removing tem-plate noise might also increase classifier performance. (3) Keyword-based Web search. A page should be regarded less relevant to the query if the matched term only occurs in a template segment.

Until now, the segmentation problem has mainly been ad-dressed by analyzing the DOM (Document Object Model) structure of an HTML page, either by rendering and visual analysis or by interpreting or learning the meaning and im-portance of tag structures in some way, both using heuristic as well as formalized, principled approaches. However, the number of possible DOM layout patterns is virtually infinite, which inescapably leads to errors when moving from train-ing data to Web-scale. The actual retrievable unit  X  namely text  X  has only partially been investigated for the purpose of web page segmentation. Whereas it has been analyzed on the level of semantics and on term-level, a low-level pattern analysis is still missing.

Our Contributions. In this paper, we try to fill this gap as follows. (1) We define an abstract block-level page segmentation model which focuses on the low-level proper-ties of text instead of DOM-structural information. (2) We concretize the abstract model. The key observation is that the number of tokens in a text fragment (or more precisely, its token density) is a valuable feature for segmentation de-cisions. This allows us to reduce the page segmentation problem to a 1D-partitioning problem. (3) We present the Block Fusion algorithm for identifying segments using the text density metric. (4) We present an empirical analysis of our algorithm and the block structure of web pages and evaluate the results, comparing with existing approaches. Organization. In Section 2 we present the related work. Section 3 covers the problem discussion. Section 4 presents our simple, yet effective algorithm. In Section 5, we present our experimental results. Section 6 concludes with an out-look to future work.
Attempts to web page segmentation consider a variety of methods from different aspects. Most commonly, the struc-ture of the web page (i.e., the DOM tree) is analyzed, in or-der to mine block-specific patterns, for example to separate and remove template elements from the actual main content. Bar-Yossef and Rajagopalan [4] identify template blocks by finding common shingles, similar to Gibson et al. [16] who also considers element frequencies for template detection. Debnath et al. compute an inverse block frequency for clas-sification [13]. In [9], Chakrabarti et al. determine the  X  X em-plateness X  of DOM nodes by regularized isotonic regression. Yi et al. simplify the DOM structure by deriving a so-called Site Style Tree which is then used for classification [26]. Vieira et al. present an approach to template removal by identifying common DOM subtrees from a sample set and re-moving these structures from the whole collection [24]. Kao et al. separate blocks of DOM subtrees by comparing the en-tropies of the contained terms [20]. Vision-based approaches add information gained after rendering the DOM, such as Cai et al. X  X  VIPS algorithm [7], Chen et al. X  X  approach to tag pattern recognition [12] as well as Baluja X  X  [3] method using decision tree learning and entropy reduction. Most recently, Chakrabarti et al. approached the webpage seg-mentation problem from a graph-theoretic perspective [10]. As shown by Cai et al. [8] and more recently by Fernandes et al. [15] the resulting segment structure can also be used for improving keyword-based search.
It is surprising how different the visual representation and the corresponding HTML document structure can be across different websites. Not only the use of different layouts con-tributes to this situation, but also the fact that there are versatile ways to model an identical layout, e.g. by varying between semantic and visual markup ( &lt;EM&gt; vs. &lt;I&gt; ), misus-ing &lt;TABLE&gt; structures for positioning non-tabular elements as well as completely neglecting HTML semantics for the layout. The latter has become very popular due to the use of CSS across most Web 2.0 websites, where tags usually are just &lt;DIV&gt; elements. This situation makes web page segmentation a non-trivial task. On Web-scale, rule-based or trained algorithms working on DOM-level are, due to the extreme heterogeneity of HTML style, susceptible to failure. On the other hand, vision-based approaches naturally have a higher complexity since the layout must be rendered (like in a browser) prior to analysis, which might be too slow to be incorporated into the Web crawling and indexing cycle.
Although we are examining the problem of web page seg-mentation from a textual perspective, there is a clear re-lationship to image segmentation from the field of Com-puter Vision: any DOM-level algorithm has to bear com-parison with image recognition approaches, which span from k-means pixel clustering over histogram mode seeking and graph-partitioning to greedy region merging strategies [22]. In fact, we can draw a parallel from Shi X  X  normalized cuts graph partitioning technique [22] to the recent work of Chak-rabarti et al. [10], for instance. An example of a graph-independent approach is Haralick X  X  and Shapiro X  X  Region Growing [22]. Region Growing essentially is a greedy merg-ing strategy; starting from one position in the image (e.g., top left corner), the grower iteratively fuses neighbored re-gions to larger ones (i.e., distinct pixels to sets of adjacent pixels). Under the assumption that the pixels are indepen-dent and uniformly distributed, the similarity of a region to another one is quantified by the deviation from the average intensity of that region  X  regions are merged if the deviation is insignificant. An application for Region Growing is text-block extraction from scanned newspaper images, where the algorithm is also known as Block Growing [2, 11].
In the field of Quantitative Linguistics, distributions of linguistic units such as words, syllables and sentences have been widely used as statistical measures to identify struc-tural patterns in plain text documents, in particular for identifying subtopics [18] as well as for discovering changes of writing style [1]  X  both can be regarded as a special form of segmentation. In this discipline, it is a generally accepted assumption that the probability of a given class x in the corresponding unit X  X  distribution solely is dependent on the probability of the neighboring lower class x  X  1 [17]:
For example, when a text is segmented into blocks of al-most the same size, it is believed that the class distribution of term frequencies (occurrence probabilities) is negative hy-pergeometric ( Frumkina X  X  law or law of text blocks ), which has been validated for various languages [6]:
Taking this into account for segmentation, an obvious strategy is to examine the statistical properties of subsequent blocks with respect to their quantitative properties. In [18], for example, Hearst presents an algorithm which discovers sequences of adjacent paragraphs that belong to a topical unit within the document; which paragraphs get assigned to a particular subtopic is decided by a neighbored-block comparison based on term-frequency and cosine similarity.
Besides such an analysis of documents on the term-level, there are further interesting quantitative properties to con-sider. The distribution of document lengths follows the well-known Zipf distribution [14]. It might be reasonable to con-sider this distribution for segmenting intra-document text portions as well. The Zipf law states that the occurrence frequency of objects of a particular class is roughly inversely proportional to the corresponding rank of the class:
Another efficient quantum is sentence length . According to Altmann [1, 5], the creation of sentences is a stochastic process which follows a rhythm based on certain synergetic properties, i.e. the sentence lengths change along with the text flow. For analyzing changes in writing style he thus recommends not to compare random samples of a document but consecutive sentences instead. He concludes that also the occurrence probability of a particular sentence length x is a function of x  X  1 (yielding a hyperpascal distribution):
Coming back to the problem of web page segmentation, it is questionable whether the particular use of one specific HTML formatting style yields better signals for finding the  X  X ight X  segmentation than another one. It is obvious that the absence of element tag information is a strong indicator for the segmental unity of a text portion. We consider such text portions atomic . Could then perhaps the sheer presence of any element tag already be a sufficiently good signal for segmentation? While there are a few tags which separate by high chance (heading tags such as &lt;H1&gt; ) and some which usually do not separate (the anchor text tag &lt;A&gt; ), the ma-jority of elements has unclear effects to segmentation. Thus, we may simply model a web page as a series of text portions (non-segmentable , atomic blocks ) interleaved by a sequence of one or more opening or closing element tags, regardless of their meaning. We call such a sequence a gap . This simpli-fies the discussion to distinguishing the gaps which separate two segments and gaps which do not. Non-separating gaps may be discarded, resulting in larger text segments ( com-pound blocks ). While we can always a priori define certain tag-based rules for this decision finding, we focus on analyz-ing the blocks X  inherent textual properties for this purpose.
Most likely, a segment gap is caused by a change in the text flow, e.g. from a list of short phrases (navigational text portions like  X  X ome X ,  X  X hat X  X  new X ,  X  X ontact us X ) over a sequence of full sentences (for the main content) back to short phrases or one-sentence blocks for the page footer (e.g.,  X  X opyright (c) 2008 by ... All rights reserved X ). This set-ting is similar to the analysis of writing style by comparing sentence lengths (see Section 3.2). Due to the lack of proper sentences in template elements, it is difficult to define  X  X en-tence X  in the web page scenario. Instead, we may substitute sentence length by text density , i.e. the number of words within a particular 2-dimensional area. Text density has been defined by Spool et al. in the field of Web Usability as the ratio between the total number of words in a block and the height of the rendered and printed block in inches [21]; a similar notion is known in Computer Vision, that is the intensity of an image region [22]. We transfer this concept to HTML text. The counterpart of a pixel in HTML is character data (the atomic text portion), an image region translates to a sequence of atomic text portions, which we also call block here. To determine a text block X  X   X  X eight X , we word-wrap its text (not its rendered representation) at a constant line width w max (in characters). The resulting block b x  X  X  density  X  ( b x ) could then be formulated as follows:
This definition of text density has the elegant property that  X  except tokenization  X  no lexical or grammatical anal-ysis needs to be performed. Given a proper wrapping width, it is supposed to serve as a discriminator between sentential text (high density) and template text (low density). We propose w max = 80. This is the traditional screen width of monospaced terminals and seems to fit the definitions of an English sentence: Assuming an average word length of 5.1 characters 1 , we can write a maximum of 80 5 . 1+1 =13 . 1 separate words (tokens) per line, which roughly covers one medium-sized sentence; obviously, the absolute maximum is 40 one-character tokens per line. It makes sense to exclude the last line of a multi-line block for the computation, since it would falsify the actual density when averaging if it is not completely filled to w max . Given the set of tokens T con-tained in the set of wrapped lines L covered by a block b we can reformulate Equation 5 as follows.
An overview of language-specific word lengths can be found at http://blogamundo.net/lab/wordlengths/
Now the density of a multi-line block is not influenced by the number of additional tokens (i.e., doubling the number of tokens leads to almost double the number of lines, which gets normalized again; see Equation 6). However, having only a few words (like  X  X ontact us X ) still leads to a much lower density value, as expected.

While our text density measure does not consider lexi-cal or grammatical properties of sentences at all, its role as a surrogate for sentence length may be well justified. Alt-mann [1] supports this by the rationale that language itself does actually not care about the existence or clear bound-aries of particular lexical or grammatical units and that such units are rather an orthographical convention of the speech community. What seems more important than a proper def-inition of  X  X entence X  is the measure of the units enclosed by the sentence (words, syllables, characters). The unit used for text density is the token , which basically is a variant of the (also diffuse) notion of  X  X ord X ; in our case it is any con-tiguous sequence of non-whitespace characters, simplified to the set of contained literals and digits.
The task of detecting block-separating gaps on a web page ultimately boils down to finding neighbored text portions with a significant change in the slope of the block-by-block text density. In Figure 1, we depict the desired segmenta-tion 2 of the CIKM 2008 welcome page ( http://cikm2008. org/ ), both visually as well as densitometrically. In the di-agram, the density of the atomic text blocks is depicted as grey bars, HTML markup is indicated as white stripes and the expected segmentation boundaries are indicated as red vertical lines. Apparently, apart from the expected spikes, the distribution of text density appears to be a fairly good signal for textual similarity as well as for identifying full-text segments (block #5).
As it turns out by the preceding discussion of the segmen-tation problem, we can essentially transfer parts from the perspective of Quantitative Linguistics as well as of Com-puter Vision to our setting. Due to Altmann X  X  findings about the length dependence of neighbored sentences within the text flow and our corresponding findings on the text density, a greedy strategy seems a plausible algorithmic approach; besides being deterministic, an at least near-optimal result is likely. If we now indeed consider text density as being in-terrelated to the notion of pixel intensity , we may consider adopting the Block Growing strategy from image process-ing to. To avoid confusion with the pixel-based methods, we call this token-based method Block Fusion . The decision when to combine (fuse) two adjacent blocks now is made by comparing them with respect to their text densities instead
Indisputably, there is no such thing like the segmentation, since segments may be considered at different granularities. Algorithm 1 The Block Fusion algorithm ( plain / smoothed ) Require: B  X  The set of (initially atomic) blocks which 1: repeat 2: loop  X  false 3: for all b i  X  B with i&gt; 1 do 4: if  X  ( b i  X  1 )=  X  ( b i +1 )  X   X  ( b i ) &lt; X  ( b i  X  1 5: Only checked for BF-smoothed 8: remove b i 9: i  X  i +1 Skip b i +1 10: loop  X  true 11: else if  X   X  ( b i  X  1 ,b i )  X   X  max then 13: remove b i  X  1 14: loop  X  true 15: end if 16: end for 17: until loop = false of pixel intensities. We may define this slope delta between two adjacent blocks x and y as:
If the slope delta is below a certain threshold  X  max ,we assume that the blocks belong to one segment and should therefore be fused.  X  X o fuse X  here means joining the lines of the two blocks x and y to a new block z , such that z spans from the first line of x to the last line of y . After this, x and y are replaced by z . As with the Block Growing strat-egy, we can iteratively continue with this operation until no pair of neighbored block exists which satisfies the threshold constraint.

In addition to that, we might also consider the follow-ing extension to this simple fusion strategy. As we can see from the example density distribution of the CIKM web page (Figure 1), there are some adjacent segments with al-the section about important dates  X  the dates are enclosed by &lt;SPAN&gt; tags, which create gaps). This may lead to high slope deltas close to 100% and therefore to less fusions than expected. We conclude that the surrounding blocks domi-nate the enclosed one. Our suggestion is to smooth these alternations by adding the following condition to the Block Fusion algorithm: if the text densities of the predecessor and successor of a block are identical and higher than its own density, all three blocks are fused. Of course, we will validate this heuristic against the plain strategy. See Al-gorithm 1 for a common representation of both strategies, BF-plain and BF-smoothed .
 The computational complexity of Block-Fusion is trivial. Assuming we have N atomic blocks on a page, the cost per iteration is c  X  ( N  X  1) comparisons ( c = 1 for BF-plain , c = 2 for BF-smoothed ) and a maximum of N  X  1 fusions per iteration occur. Because the iteration stops as soon as zero fusions occurred, the worst case that may occur is a single fusion per iteration (convergence is therefore guaran-teed). The total number of operations for a maximum of k iterations until convergence therefore is:
Two variables may influence the quality of the segmenta-tion: the threshold  X  max and the input blocks B . Regarding  X  max we believe that this threshold is not document-specific but rather depends on the average style of the document class and its inherent quantitative properties. In Section 5.2 we determine an appropriate threshold value from a random sample of web documents. According to our definition of the simple block-gap model (Section 3.3), B describes the se-quence of textual portions of the original HTML document. Whenever one or more opening or closing element tags are encountered, a new block is created, consisting of the plain text that is surrounded by markup; each block X  X  text is ini-tially word-wrapped by w max characters (the wrapping does not change in the course of fusion).

Apart from special HTML tags whose nested character elements do not contribute to the text of the page (like gard as a core feature of hypertext markup and therefore do not consider a gap before or after this tag, we do not respect the element tag X  X  semantic meaning or expected vi-sual effect  X  a &lt;H1&gt; tag produces the same type of gap as a &lt;B&gt; tag, for example. Intuitively, we could of course claim that &lt;H1&gt; does indeed have a stronger impact on segmenta-tion than a &lt;B&gt; tag, but this would again lead to heuristic, rule-based or DOM-structural approaches. For the evalu-ation, we will consider such a rule-based extension of our BF-smoothed algorithm (which we call Bf-rulebased for simplicity) which considers a set of specific gap-enforcing and gap-avoiding tags ( T ForceGap and T NoGap ). Given the set of tags T ( x, y ) between two segments x and y , to support this extension we have to change the slope delta function from  X   X  ( x, y )to X   X  ( x, y ):
We consider the following gap-enforcing tags ( T ForceGap as a good choice for the rule-based approach: H1 -H6 , UL , DL , OL , HR , TABLE , ADDRESS , HR , IMG , SCRIPT 3 (basically a subset of HTML block-level elements tags). For T NoGap ,we consider the following tags: A , B , BR , EM , FONT , I , S , SPAN , STRONG , SUB , SUP , U , TT (a subset of HTML inline element tags). When  X  max =  X  , this approach simply segments the document after every occurrence of a tag  X  T ForceGap regardless of  X   X  or T NoGap (Block Fusion has no effect in this case; we call this special variant JustRules ). Lower values of  X  max represent a trade-off between markup-based and density-based segmentation. The examination of the effects of  X  max are part of our evaluation.
To demonstrate the stability and effectivity of the density-based Block Fusion strategy, we used two standard test col-lections: Webspam UK-2007 4 and the Lyrics dataset used in [10]. Despite its name the Webspam UK-2007 collection is a good snapshot of the U.K. Web, roughly consisting of 106 million pages from 115,000 hosts. Several hosts have already been classified as spam / non-spam . From this non-spam fragment (356,437 pages) we randomly picked 111 web pages coming from 102 different websites. We manually as-sessed these documents to define a comparable segmenta-tion. These manual results were then compared against the following different clustering strategies: 1. WordWrap . Simply take all text of a page and wrap 2. TagGap . Every text portion between any tag (except 3. BF -plain, BF -smoothed and BF-rulebased. As 4. JustRules . As described in Section 4. 5. GCuts . As described in [10]. We did not implement
Occurrences of SCRIPT likely indicate a gap. http://www.yr-bcn.es/webspam/datasets/uk2007/
First of all, we need to validate the assumptions on the actual quantitative linguistic properties of textual web page content. We assume that text density as defined in Equation 6 is a surrogate for sentence length. It should therefore also yield the same characteristic distribution (or at least one which satisfies Equation 1). To derive distinct classes i from the text density quotient of neighbored blocks, we use the following assignment in accordance with Eq. 4. Of note, adjacent blocks with the same density are regarded as one block, i.e. as a contiguous  X  X entence X  which has been mistakenly separated:
We used our manually created segmentation of the 111 web pages from the Webspam-UK2007 test collection and computed the class frequencies. Then we applied the Alt-mann-Fitter 5 to automatically determine one or more pos-sible fits out of more than 200 supported discrete distribu-tions for the given input data. The most significantly fitting probability distribution is negative hypergeometric (Equa-tion 2 with K =2 . 30454, M =0 . 10989, n = 17), having  X  2 =14 . 2394, P (  X  2 )=0 . 3572, C =  X  2 / P F ( i )=0 . 0061, d.f. = 13 and is rated by the Altmann Fitter as a  X  X ery good fit X . See Figure 2 for a graphical comparison; raw results are shown in Appendix A. While this differs from the initially assumed hyperpascal distribution, the general assumption (Equation 1) still holds and seems to abide by Frumkina X  X  law. In fact, Vulanovic and K  X  ohler assume [25] that Frumk-ina X  X  law can be applied not only on term-level but to all types of linguistic units. We can show that this is at least the case for the distribution of text density quotients between adjacent blocks, coming to the conclusion that text density may indeed function as a surrogate for sentence length.
More, we also found that the distribution of the number of tokens in a segment abides by Zipf X  X  law. This has already been shown on document-level [14], and it is just consistent to also find these properties on intra-page level. We were able to fit the distribution of segment-level word lengths of our manually segmented documents to y =1 . 086  X  x  X  0 . 7028 with  X  2 = 256 . 555 and a root mean square error of 0 . 013. Figure 2: Probability Distribution of the Text Density Quotient of Adjacent Blocks (Equation 9) http://www.gabrielaltmann.de/
Metrics. In order to quantify the accuracy of the seg-mentation computed by Block Fusion, we employ the two cluster correlation metrics Adjusted Rand Index (AdjRand) and Normalized Mutual Information (NMI) used in [10]. Both metrics determine the agreement between two cluster-ing methods on a particular dataset, using a value between 0 (no agreement) to 1 (perfect agreement). The corresponding label vectors hold the information to which segment a par-ticular token belongs to. AdjRand is Hubert X  X  &amp; Arabie X  X  normalized extension to the Rand measure, which basically relates the number of agreements to the number of disagree-ments between the two given clusterings [19]. NMI measures the mutual dependence of the two solutions by relating their entropies. We use Strehl X  X  &amp; Ghosh X  X  variant [23], which is mentation should follow Zipf X  X  law on token-level, we can also measure and depict the consistency of a particular seg-mentation solution with this law. A deviation from the ex-pected distribution is regarded a segmentation failure.
Results. We assume that for each variant of Block Fu-sion there is an optimal setting for the threshold  X  max that is pre-determined by the underlying linguistic regularities. We probed Block Fusion using different settings for this threshold using our sample document set: for each candi-date threshold, we compute the average AdjRand and NMI scores retrieved by a document-level comparison of the seg-mentations. The results are shown in Figures 3 and 4; we also plot the average number of resulting blocks for each setting as a reference. For BF -plain and BF -smoothed there seems to be an optimal threshold at  X  max  X  0 . 38 for our sample document set, whereas any threshold between 0 . 3 and 0 . 4 seems reasonable. Starting with  X  max =0 . 4, the accuracy decreases and finally drops dramatically with  X  max 0 . 6. See Figures 8, 9 and 10 for the corresponding visual and densitometric representation. 6
We verified that the determined thresholds  X  max are not particularly document-specific  X  we get almost the same op-timal threshold for two random halves of the test set. For BF-rulebased the optimum is  X  max  X  0 . 6. This means that the heuristically determined gap-enforcing tags do in-deed contribute to the quality of segmentation, but the text densities do as well. The results for all applied clustering strategies are depicted in Table 1. Block Fusion clearly im-proves over WordWrap and TagGap . Interestingly, the scores of BF-plain and BF-smoothed are almost identi-cal to GCuts [10], which is a surprising achievement for a markup-agnostic approach. At last, BF-rulebased in fact outperforms any other approach. While it is close to the quality of JustRules (whose accuracy confirms the effec-tiveness of our heuristic segmentation rules for the evalu-ated dataset), it also shows that our heuristics were not per-fect and Block Fusion was able to improve them. We also examined the impact of w max to the accuracy (see Figure 5). It appears that this word-wrap boundary is stable for The short segments seen in Figure 9 could not be fused by BF-Smoothed, because the smoothening criterion  X  ( b i  X  1  X  ( b i +1 ) was not met. We heuristically found the improved criterion  X  ( b i  X  1 ) 5  X   X  ( b i +1 ) 5 which indeed fuses the segments correctly, while improving the accuracy scores only by ca. 0 . 02. We therefore consider this improvement as insignificant and omit it from the proposed solution. widths between 80 and 110. This confirms the assumption on the relation between language-specific average sentence length and line width. Theoretically, we could optimize it to w max = 90, but this would only increase accuracy by less than 0 . 01 on average.

Finally, Figure 6 shows a log-log plot of block-level to-kens counts for all considered algorithms (except GCuts ). All Block Fusion-based approaches as well as JustRules and the manual segmentation expose the typical straight line known from Zipf distributions. As expected, TagGap and WordWrap obviously do not show this behavior. This means that Block Fusion is indeed able to transform the tag-induced segmentation to a segmentation which resembles the same statistical properties as the expected ones.
Since Block Fusion is designed as an iterative algorithm, we should consider the iteration behavior in terms of aver-age accuracy error (1  X  accuracy )  X  we expect this error to monotonously decrease per iteration, just as the number of remaining blocks. Figure 7 reveals that most of the error gets removed already after the first iteration. Even though on average more blocks are fused during the following iter-ations, these fusions do not contribute to improving accu-racy. Block Fusion achieves this performance because it can fuse an arbitrary number of preceding atomic or compound blocks with similar density in one iteration (see Algorithm 1). Notably, for the used test data, the total processing time per page was only 15ms on a standard laptop.
Setup. Finally we quantify the usefulness of our segmen-tation for the purpose of near-duplicate detection. Again we compare the results from Block Fusion against [10], where the Lyrics dataset was used to evaluate the accuracy of detecting web pages with the same content but different ap-pearance. The dataset consisted of 2359 web pages song lyrics by six popular artists (ABBA, Beatles, BeeGees, Bon Jovi, Rolling Stones and Madonna), taken from the three websites absolutelyrics.com , seeklyrics.com and lyric-sondemand.com . The six artists were deliberately chosen to minimize the effect of false-positives on the evaluation caused by cover songs. As we were unable to acquire the original dataset, we crawled the three websites again using the same setup, resulting in 6982 web pages (which is likely to be a superset of the initial crawl by Chakrabarti et al.). By matching artist and title, 1082 songs have been deter-mined that appear on all three websites (i.e., on 3246 web pages). In addition to that, 3246 other web pages have been randomly chosen from the three websites (1082 for each). This setup allows a relatively clean comparison between the true-positive and true-negative rates of a de-duplication al-gorithm. To determine what a near-duplicate is and what is not, the same heuristic was used as in [10]: For each page of a pair of candidate pages, the tokens of the largest text segment are used to create 8 shingle fingerprints using the min-hash algorithm, with a window of 6 tokens. A pair of pages is regarded a near-duplicate if the pages share at least 50% of the shingles. The largest text segment simply is de-termined by counting the number of enclosed tokens; in our setup, segments containing at least 50% hyperlinked textual content are discarded since they are likely not to contain the main content despite their length.
Figure 3: Optimizing  X  max (BF-plain/smoothed) Figure 5: Impact of w max on Average Accuracy
Figure 6: Validation of Zipf Law on Block Level
Results. The resulting true positive/negative scores cor-responding to each algorithm (including a comparison to the text as a whole, FullText ) are shown in Table 3. Jus-tRules is the narrow winner with respect to finding du-plicates, but all Block Fusion variants perform equally well for detecting non-duplicates and significantly perform better than GCuts , even the simplest variant BF-plain .
Conclusions. The problem of web page segmentation can be seen from a quantitative linguistic point of view as a problem of identifying significant changes of particular sta-tistical properties within the considered text. As we demon-strate in this paper, an effective property is token-level text density , which can be derived from vision-based measures. This text density follows the same fundamental linguistic law (Frumkina X  X  Law) as many other linguistic units. In ad-dition to that, the distribution of the expected number of tokens in a segment follows Zipf X  X  law. Our new algorithm for web page segmentation, built upon the region growing strategy known in Computer Vision, performs significantly better than the state-of-the-art graph-theoretic algorithm, as our experimental evaluation on large real-world data sets demonstrates.

Outlook and Future Work. The approach presented in this paper is orthogonal to existing work and considers new and complementary aspects to solve the segmentation task. As shown by the rule-based Block Fusion hybrid, a more sophisticated combination of other strategies and the Block Fusion algorithm promises further improved segmentation quality. Since the considered linguistic properties seem to be mostly language-independent, the next logical step is to evaluate these findings on a multilingual corpus. In particu-lar, we need to discuss the influence of the wrapping parame-ter w max and threshold  X  max on different languages. Further work should also find an explanation of the discovered statis-tical behavior from a purely linguistic perspective. We also want to investigate the use of our techniques in other ar-eas of Information Retrieval, including block-level ranking, block-level link analysis and block-level classification. We would like to express our gratitude to Professor Dr. Gabriel Altmann for providing the Altmann Fitter software and for his good advice on details of discrete distribution functions used in Quantitative Linguistics. [1] Gabriel Altmann. Verteilungen der Satzl  X  angen [2] A. Antonacopoulos, B. Gatos, and D. Bridson. Page [3] Shumeet Baluja. Browsing on small screens: recasting [4] Ziv Bar-Yossef and Sridhar Rajagopalan. Template [5] Karl-Heinz Best. Quantitative Linguistics -An [6] Karl-Heinz Best. Sprachliche Einheiten in [7] Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying [8] Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying [9] Deepayan Chakrabarti, Ravi Kumar, and Kunal [10] Deepayan Chakrabarti, Ravi Kumar, and Kunal [11] Ming Chen, Xiaoqing Ding, and Jian Liang. Analysis, [12] Yu Chen, Wei-Ying Ma, and Hong-Jiang Zhang.
 [13] Sandip Debnath, Prasenjit Mitra, Nirmal Pal, and [14] Lukasz Debowski. Zipf X  X  law against the text size: a [15] David Fernandes, Edleno S. de Moura, Berthier [16] David Gibson, Kunal Punera, and Andrew Tomkins. [17] Peter Grzybek. On the systematic and system-based [18] Marti A. Hearst. Multi-paragraph segmentation of [19] Lawrence Hubert and Phipps Arabie. Comparing [20] Hung-Yu Kao, Jan-Ming Ho, and Ming-Syan Chen. [21] Jared M. Spool, Tara Scanlon, Carolyn Snyder, Will [22] George Stockman and Linda G. Shapiro. Computer [23] Alexander Strehl and Joydeep Ghosh. Cluster [24] Karane Vieira, Altigran S. da Silva, Nick Pinto, [25] Relja Vulanovic and Reinhard K  X  ohler. Quantitative [26] Lan Yi, Bing Liu, and Xiaoli Li. Eliminating noisy --ALTMANN-FITTER 2.1 --Result of fitting Input data: hist-1.dat
Distribution: Negative hypergeomet-ric (K,M,n) Sample size: 2334 Moments: M1 = 1.8106 M2 = 4.4963 M3 = 34.2793 M4 =356.4193 Best method is Method 1 of 2 Parameters: K = 2.30453585151999
M = 0.109889153462268 n=17
DF =13  X  2 = 14.2394 P(  X  2 ) = 0.3572 C = 0.0061
