 Pseudo-relevance feedback (PRF) improves search quality by expanding the query using terms from high-ranking doc-uments from an initial retrieval. Although PRF can often result in large gains in effectiveness, running two queries is time consuming, limiting its applicability. We describe a PRF method that uses corpus pre-processing to achieve query-time speeds that are near those of the original queries. Specifically, Relevance Modeling, a language modeling based PRF method, can be recast to benefit substantially from finding pairwise document relationships in advance. Using the resulting Fast Relevance Model (fastRM), we substan-tially reduce the online retrieval time and still benefit from expansion. We further explore methods for reducing the pre-processing time and storage requirements of the approach, allowing us to achieve up to a 10% increase in MAP over unexpanded retrieval, while only requiring 1% of the time of standard expansion.
 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Relevance Feedback, Retrieval Models General Terms: Algorithms, Performance Keywords: relevance model, pseudo-relevance feedback, distributed computing
Lavrenko and Croft X  X  Relevance Model (RM) [6] is a pseudo-relevance feedback method developed for the language mod-eling (LM) framework. The standard formulation of this method involves submitting an original query (LM), using the resulting ranked list to perform weighted query expan-sion, and performing a second round of retrieval (RM). The second query can consist of hundreds of terms, resulting in a slow evaluation over the collection. Table 1 illustrates this problem.
 Even for a small collection such as AP89, the original
Collection # docs terms unique Ratio (RM/LM) AP89 84.6 42.1 211.5 1.21 191.37 WSJ 173.2 81.7 243.5 1.28 160.63 Robust05 1033.4 484.2 892.2 1.38 430.88 Table 1: Collection statistics. The last column presents ratios of RM to LM effectiveness and speed: RM is more effective but much slower than LM Relevance Model (RM) is nearly 200 times slower than the Language Model (LM), while providing a 20% relative im-provement in retrieval. This tradeoff is unacceptable for any realistic setting. In this work, we reformulate RM to perform much of the computation offline, allowing for improved re-trieval times. We also investigate techniques for reducing the time and space requirements of the offline computation while avoiding a significant negative impact on retrieval perfor-mance. These improvements result in up to a 10% increase in MAP over LM while requiring only 1% of the retrieval time of the original RM. Although our experiments and dis-cussion focus on the RM, the ideas generalize to most forms of PRF and are an important step towards overcoming the inefficiency that prevents their being adopted.
We begin with the derivation made by Lavrenko and Al-lan [5], essentially requiring the cross-entropy between doc-ument M (the model) and document D (the candidate for scoring). Calculating the cross-entropy between two doc-uments lends itself nicely to the MapReduce framework, though some care must be taken to account for the smooth-ing factors involved in the second probability term. Starting from the H ( M k D ) term: An inner product over a set of documents can be performed simply by incrementing a score accumulator using the post-ing lists of an indexed collection [3, 7]. However, because of smoothing the situation is not so simple. Consider these two example documents: Substituting into Equation 1: Notice that neither posting list for the terms  X  X at X  nor  X  X treet X  will have d 2 in them. We will never be able to generate the second nor the fourth additive terms of the sum in Equa-tion 2, yet they are nonzero values, as the second part of each product is a smoothed probability, and therefore never zero. We circumvent this issue by producing the background score for every document pair, and then incrementing from that point to produce the final scores. Let us begin with Equation 1 again: meaning if the term does not occur in D but does occur in M , we still need the term X  X  contribution to the document pair. Early experiments indicated that Jelinek-Mercer smoothing is superior to Dirichlet smoothing for this method, therefore we expand our smoothed probability as follows: Let  X  ( t ) represent the background probability of t . This is the quantity (1  X   X  ) P ( t | C ) in Eq. 4: We now focus our attention on the second term to expose the smoothed probability: Substituting back into Equation 1:
This formulation has two crucial advantages over Equa-tion 1: the second sum is now over the intersection of M and D , and all of the terms are raw probabilities. The cross-product of postings will in fact recover all of the terms in the second sum for every M , D pair. At the end of scanning a document, we emit the value of the first sum as a  X  X ull term X , which now represents the background cross-entropy score of all documents with respect to M . All we need to do is carry the null term downstream to the final sums for all documents. This process only produces an extra | C | incre-ments to shuffle, a negligible increase in volume. We implemented the offline calculation using Hadoop Map-Reduce v0.20.1, and processing was performed on Yahoo! Inc. X  X  M45 cluster 1 . For retrieval, we modified a copy of Indri 2.10 2 to support merging the previously calculated scores into the ranked list. We conduct our experiments over the three collections shown in Table 1. We use the built-in Krovetz stemmer and the INQUERY stopword list during indexing. We use topics from the early ad-hoc tracks of TREC 3 as the query set for AP89 and WSJ. Robust05 represents the topics and documents from the TREC Ro-bust 2005 track. We use only the title text of each topic. For all PRF experiments, we set the number of feedback documents ( fbDocs ) to 10, and the number of feedback terms ( fbTerms ) to 100. We report Mean Average Pre-cision (MAP) and in order to gauge retrieval performance between different methods. The LM and RM methods act as the baselines for our experiments: we want LM speed with RM accuracy. We use the paired sample randomiza-tion t test as described by Smucker et al. [8] for significance testing. We use 10 million samples for each significance test. Often RM can be improved if the original query X  X  likelihood is interpolated into the score [1]. All experiments here use that variation of RM.
Although Lavrenko and Allan have shown that fastRM can be slightly slower than LM while providing much of the gain of RM [5], there are two issues that appear as the col-lection size grows: 1. The time needed to calculate the matrix is reasonable 2. The full matrix grows quadratically. The AP89 collec-In the rest of this section we present methods for addressing the issues above via approximation of the matrix.
Let A represent a fully calculated matrix of cross-entropy scores, and let  X  A be an approximation of A . The approx-imation methods used here may drop entries from A , or alter values in the cells, however the cells will never change row-rank order. We can use standard ranked list evaluation measures to inform us how  X  A impacts retrieval. However, when comparing a A and  X  A , we would like a more direct http://research.yahoo.com/node/1884 http://www.lemurproject.org/indri/ http://trec.nist.gov Table 2: Query-processing times for different num-ber of scanned columns ( c ). Time is in milliseconds. c = 0 is the Language Model run. c =  X  is using the entirety of the provided matrix. Collection is AP89. measure of how much of the original matrix is recovered by the approximation. We desire a method that assigns more importance to documents at higher ranks in a particular row. We would also like a measure that is bounded, since by def-inition the best performance we can hope for is recovering exactly the elements of the row we specify.

We draw inspiration from the NDCG measure [4] to fulfill this role, calling the resulting measure RowEval. Let A be row i in a fully calculated matrix of cross-entropy values. Let  X  A i be the corresponding row i in  X  A . Some documents do not appear in the approximate row, so if | A i | represents the number of non-empty entries in row A i , then | A i NDCG is defined over two lists of the same length, so we need to treat  X  A i as if it has the same number of entries as A . To do this we simply inject a  X  X on-relevant X  entry into  X  A i in place of every document not recovered, to create a new list  X  A 0 i . For example, if: If we define rel ( id ) to be 1 if document id is not a  X  symbol, and 0 otherwise, then RowEval@p is: RowEval @ p (  X  A i ,A i ) = Simply put, we use the non-relevant entries to make sure the actual entries in  X  A i are assigned the proper gain during computation.
Previous work [9, 2] has shown that binning retrieval scores can simultaneously increase efficiency and reduce space re-quirements while not significantly impacting retrieval perfor-mance. Binning the values would reduce the total number of cells required for each row scan; Table 2 shows retrieval times for various scan-lengths, indicating a substantial im-provement. We study two binning techniques to determine the impact on retrieval performance. variable to be the amount two scores must differ by in order to create a new bin. Given two row-wise adjacent entries in a matrix, A i,j and A i,j +1 , if | score ( A i,j )  X  score ( A then A i,j +1 is placed in the same bin as A i,j and uses the same score that A i,j is using (which itself may be a surrogate score). Otherwise we create a new bin, using score ( A i,j +1 as the score for that bin. As  X  0 , # bins  X  X  C | . Inversely, as  X  X  X  , # bins  X  1.

Stepwise binning. The step function uses two values, the binsize ( b ) and the number of bins ( n ). For example, a bin size of 100 with 10 bins means that the first 1000 doc-uments are placed into 10 bins, where the first bin contains the 100 most highly scored documents, the second bin con-tains the 100 next documents, and so on. For each bin, the highest score in a that bin is used for all of the documents contained in the bin. If the number of documents in the row is greater than the number specified by b  X  n , all of the remaining documents are placed in the last (i.e. rightmost) bin.

Table 3 shows results for several values of and stepwise binning. Both methods appear to show marked improve-ment over the run without binning (leftmost), suggesting that we could reorganize our matrix to only store the val-ues that start a bin, and delete the other scores in each bin. Notice that the stepwise method shows more variabil-ity between values, indicating higher sensitivity to parame-ter changes.
We can drop columns without dropping effectiveness [5], which implies that we can save computation time if we can only calculate the pairs we need. In order to achieve this effect, we use the following method. When processing a document, we order all of the unique terms by an impact function I . We set some threshold  X  , and only emit the first  X  unique terms from each impact-ordered document. We then form a list of document pairs based on the intersec-tions within the remaining term posting lists. Using this list we then generate the exact cross-entropy scores to form the approximate matrix. We set  X  X f-idf X  as the I function, while varying  X  . The operation behaves like a high-pass filter on the document terms, therefore we refer to this method as Highpass.

Figure 1 shows results for different settings of  X  compared to the Language Model (LM) and best fastRM (full) runs for the respective collections. All values have been scaled to reflect the increase from the lowest to the highest value for that particular axis and collection. Therefore a value of 0.5 indicates the actual value to be low + 0 . 5( high  X  low ). Figure 1 shows that for  X  = 10 and  X  = 20, the the build times are relatively short, but the relative increase in MAP is substantial. The runs where  X  = 100 took longer to con-struct than the original full calculation were due to how the cross-entropy scores are calculated from the resulting list of document pairs. It demonstrates that although high  X  value does increase the build time as expected, it does not bring increased accuracy.

Table 4 provides some insight into why the Highpass method performs well with so few terms projected. At  X  = 10, the RowEval@1K is above 0.5 for all 3 collections. However one can easily see the diminishing returns in increasing the value of  X  . This suggests that while the RowEval increases as expected, at higher values of  X  the documents for a given row are on average of lower quality (i.e. less similar). This explains the phenomenon of the  X  = 100 runs actually per-forming worse than the runs with fewer terms used. This begs the question of how much the performance is tied to the number of terms projected, as opposed to just picking the right terms to project. = 0.2223 = 0.2159 0.2216 = 0.2376 = 0.2211 = = 0.2312 = 0.2099 0.2240 = 0.2392 = 0.2085 p &lt; 0 . 05 over the WB run. Figure 1: Graph comparing increase in retrieval performance versus time to build the matrix used.  X  X  X , X  X  X , and  X  X  X  indicate AP89, WSJ, and Robust05 respectively. Labels are the value of  X  .
 Table 4: RowEval@1K of Highpass for the different values of  X  , across all collections. We have shown that we can reformulate the traditional Relevance Model to allow for much of the computation to occur offline. Additionally, we can reduce the computational requirements of calculating the matrix by accurately pre-dicting the high-quality document pairs and only produc-ing values for those entries. Results indicate that at least one method, the Highpass algorithm, shows considerable promise.

Several avenues emerge to continue this work. We plan to investigate other approximation techniques that can further improve pre-processing time and storage requirements. We would also like to broaden the scope of our work to apply to other PRF methods, with the intention of discovering principles that hold true across all PRF methods.

We believe this work demonstrates the potential of our approach towards improving efficiency of theoretically sound query expansion. Furthermore, this work represents a sig-nificant step in bridging the gap between applying statistical PRF in a lab setting versus using it in the real world.
This work was supported in part by the Center for Intel-ligent Information Retrieval, in part by NSF IIS-0910884, and in part by NSF CLUE IIS-0844226. We thank Yahoo! for the use of the M45 cluster. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.
 [1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, [2] V. N. Anh and A. Moffat. Pruned query evaluation [3] T. Elsayed, J. Lin, and D. W. Oard. Pairwise document [4] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [5] V. Lavrenko and J. Allan. Real-time query expansion in [6] V. Lavrenko and W. B. Croft. Relevance based [7] J. Lin. Brute force and indexed approaches to pairwise [8] M. D. Smucker, J. Allan, and B. Carterette. A [9] T. Strohman. Efficient Processing of Complex Features
