 Measurements are fundamental to any empirical science and, similarly, search evaluation is a vital part of information retrieval (IR). Evaluation ensures the progressive develop-ment of approaches, tools, and methods studied in this field. Apart from the scientific perspective, the evaluation ap-proaches are also important from the practical perspective. Indeed, the evaluation experiments enable commercial search engines to make data-driven decisions while developing new features and working on the quality of the user experience. Thus, it is not surprising that evaluation has gained a huge attention from the research community and such an inter-est spans almost fifty years of research [3]. The Cranfield experiments [3] evolved into the widely used offline system evaluation approach. Despite its convenience and popular-ity, the offline evaluation approach has several limitations [8]. These limitations resulted in the development and re-cent growth in popularity of the online user-based evaluation approaches such as interleaving and A/B testing [1].
There can be a considerable interplay between the user modelling, the online and the offline evaluation approaches. The online evaluation methods interpret the user X  X  behaviour observed during the evaluation experiment to infer if the tested change leads to improvements in the user satisfac-tion. In contrast, some of the modern offline evaluation approaches proposed for document retrieval are devised to predict how the users will behave once the tested search algo-rithms are deployed and whether the users will be satisfied. Only recently has the offline evaluation methods started to see their foundation in the user modelling, as highlighted by modern effectiveness metrics, such as the ERR metric [2]. Similarly, some interleaving methods have recently started to be formulated in terms of the user click models [7].
We propose to strengthen these ties between the mod-elling of the user behaviour and the evaluation, and use the user modelling approach to address a number of applica-tions in IR systems evaluation. Specifically, the statement of this thesis is that the user behaviour modelling can aid in addressing a number of applications in information re-trieval evaluation. We hypothesise that the understanding of the user searching behaviour can help to devise novel of-fline evaluation metrics, new online evaluation approaches, and improve the existing online evaluation methods. We also demonstrate how the offline evaluation methods can developed to be in agreement with the online evaluation paradigm, thus effectively bridging the gap between the of-fline and the online approaches.

In particular, based on the user modelling approach, we aim to study how the offline evaluation methods can be ex-tended to novel applications such as the query auto-completion mechanisms [5]. Our next goal is to investigate how the user model-based offline evaluation can be improved to account for non-effectiveness features, such as search efficiency [4]. We also aim to investigate how the interleaving evaluation methods can be improved based on the user behaviour mod-elling. More precisely, we study how to maximise the sen-sitivity of the interleaving methods by modelling the user behaviour [6], and how the applicability of interleaving can be extended to other domains, such as image search. Over-all, we hope that our research will contribute to improving the evaluation approaches in IR.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: search evaluation, offline evaluation, online eval-uation, user modelling
