 Health care costs are escalating. To deliver cost effective quality care, modern health systems are turning to data to predict risk and adverse events. For exam-ple, identifying patients with high risk of readmission can help hospitals to tailor suitable care packages.
 Modern electronic medical records (EMRs) offer the base on which to build prognostic systems [ 11 , 15 , 19 ]. Such inquiry necessitates modeling patient-level temporal healthcare processes. But this is challenging. The records are a mixture of the illness trajectory, and the interventions and complications. Thus medical records vary in length, are inherently episodic and irregular over time. There are long-term dependencies in the data -future illness and care may depend criti-cally on past illness and interventions. Existing methods either ignore long-term they able to model temporal irregularity [ 14 , 20 , 22 ].
 Addressing these open problems, we introduce DeepCare, a deep, dynamic neural network that reads medical records, infers illness states and predicts future outcomes. DeepCare has several layers. At the bottom, we start by modeling illness-state trajectories and healthcare processes [ 2 , 7 ] based on Long Short-Term Memory ( LSTM )[ 5 , 9 ]. LSTM is a recurrent neural network equipped with memory cells, which store previous experiences. The current medical risk states are modeled as a combination of illness memory and the current medical conditions and are moderated by past and current interventions. The illness memory is partly forgotten or consolidated through a mechanism known as forget gate. The LSTM can handle variable lengths with long dependencies making it never been used in healthcare. This may be because one major difficulty is the handling irregular time and interventions.
 and consolidation of illness through the memory. First, the forgetting and consol-idation mechanisms are time moderated. Second, interventions are modeled as a moderating factor of the current risk states and of the memory carried into the future. The resulting model is sparse and efficient where only observed records are incorporated, regardless of the irregular time spacing. At the second layer of DeepCare, episodic risk states are aggregated through a new time-decayed multi-scale pooling strategy. This allows further handling of time-modulated memory. Finally at the top layer, pooled risk states are passed through a neural network for estimating future prognosis. In short, computation steps in DeepCare can be summarized as: where x 1: n is the input sequence of admission observations, y is the outcome of interest (e.g., readmission), nnet y denotes estimate of the neural network with respect to outcome y ,and P is probabilistic model of outcomes.
 grail question  X  X hat happens next? X . In particular, we predict the next stage of disease progression and the risk of unplanned readmission for diabetic patients after a discharge from hospital. Our cohort consists of more than 12,000 patients whose data were collected from a large regional hospital in the period of 2002 to 2013. The forecasting of future events may be considerably harder than the classical classification of objects into categories due to inherent uncertainty in unseen interleaved events. We show that DeepCare is well-suited for modeling disease progression, as well as predicting future risk.
 dynamic neural network for medical prognosis. DeepCare models irregular tim-ing and interventions within LSTM  X  a powerful recurrent neural networks for sequences and (ii) Demonstrating the effectiveness of DeepCare for disease pro-gression modeling and medical risk prediction, and showing that it outperforms baselines. This section briefly reviews Long Short-Term Memory (LSTM), a recurrent neural network (RNN) for sequences. A LSTM is a sequence of units that share the same set of parameters. Each LSTM unit has a memory cell that has state c  X  R K at time t . The memory is updated through reading a new input and the previous output h t  X  1  X  R K . Then an output states on the memory c t . There are 3 sigmoid gates that control the reading, writ-ing and memory updating: input gate i t , output gate o t respectively. The gates and states are computed as follows: where  X  denotes sigmoid function,  X  denotes element-wise product, and W U The memory cell plays a crucial role in memorizing past experiences. The key is the additive memory updating in Eq. ( 5 ): if f t  X  memory is preserved. Thus memory can potentially grow overtime since new experience is stilled added through the gate i t .If f t  X  rience is updated (memoryless). An important property of additivity is that it helps to avoid a classic problem in standard recurrent neural networks known as vanishing/exploding gradients when t is large (says, greater than 10). LSTM for Sequence Labeling. The output states h t can be used to generate labels at time t as follows: for label specific parameters v l .
 LSTM for Sequence Classification. LSTM can be used for classification using a simple mean-pooling strategy over all output states coupled with a dif-ferentiable loss function. For example, in the case of binary outcome y we have: pool { h In this section we present our contribution named DeepCare for modeling illness trajectories and predicting future outcomes. As illustrated in Fig. 1 , DeepCare is a deep dynamic neural network that has three main layers. The bottom layer is built on LSTM whose memory cells are modified to handle irregular tim-ing and interventions. More specifically, the input is a sequence of admissions. Each admission t contains a set of diagnosis codes (which is then formulated as a feature vector x t  X  R M ), a set of intervention codes (which is then formu-lated as a feature vector p t ), the admission method m t  X t  X  R + between the two admission t and t  X  1. Denote by the input sequence, where u t =[ x t , p t ,m t , X t ], the LSTM computes the corre-sponding sequence of distributed illness states h 0 , h 1 The middle layer aggregates illness states through multiscale weighted pooling z =pool { h 0 , h 1 ,..., h n } , where z  X  R K  X  s for s scales.
 as P ( y | x 1: n )= P (nnet y (pool { LSTM( x 1: n ) } )). The probability P ( y depends on the nature of outputs and the choice of statistical structure. For example, for binary outcome, P ( y =1 | x 1: n ) is a logistic function; for multi-class outcome, P ( y | x 1: n ) is a softmax function; and for continuous outcome, detail. 3.1 Admission Embedding Figure 2 a illustrates the admission embedding. There are two main types of infor-mation recorded in a typical EMR: (i) diagnoses of current condition; and (ii) interventions. Diagnoses are represented using WHO X  X  ICD (International Classi-fication of Diseases) coding schemes 1 . Interventions include procedures and med-ications. The procedures are typically coded in CPT (Current Procedural Termi-nology) or ICHI (International Classification of Health Interventions) schemes. Medication names can be mapped into the ATC (Anatomical Therapeutic Chem-ical) scheme. These schemes are hierarchical and the vocabularies are of tens of thousands in size. Thus for a problem, a suitable coding level should be used for balancing between specificity and robustness.
 Codes are first embedded into a vector space of size M and embedding is learnable. Since each admission typically consists of multiple diagnoses, we aver-age all the present vectors to derive x t  X  R M . Likewise, we derive the averaged intervention vector p t  X  R M . Finally, an admission embedding is a 2 M -dim vector [ x t , p t ]. 3.2 Moderating Admission Method and Effect of Interventions There are two main types of admission: planned and unplanned. Unplanned admission refers to transfer from emergency attendance, which typically indicate higher risk. Recall from Eqs. ( 2 , 5 ) that the input gate information is updated into memory c . The gate can be modified to reflect the risk level of admission type as follows: where m t = 1 if emergency admission, m t = 2 if routine admission. the output gate is moderated by the current intervention as follows: illness. This suggests the illness forgetting is moderated by previous intervention where p t  X  1 is intervention at time step t  X  1. 3.3 Capturing Time Irregularity We introduce two mechanisms of forgetting the memory by modified the forget gate f t in Eq. 11 : Time Decay. Recall that the memory cell holds the current illness states, and the illness memory can be carried on to the future time. There are acute conditions that naturally reduce their effect through time. This suggests a simple decay where  X  t  X  1: t is the time passed between step t  X  1 and step t ,and d (  X  (0 , 1] is a decay function, i.e., it is monotonically non-increasing in time. One function we found working well is d (  X  t  X  1: t ) = [log( e +  X  2 . 718 is the base of the natural logarithm.
 Forgetting Through Parametric Time. Time decay may not capture all conditions, since some conditions can get worse, and others can be chronic. This suggests a more flexible parametric forgetting: where q  X  may have: q  X  dynamics. 3.4 Recency Attention via Multiscale Pooling Once the illness dynamics have been modeled using the memory LSTM, the next step is to aggregate the illness states to infer about the future prognosis. The simplest way is to use mean-pooling, where  X  h =pool However, this does not reflect the attention to recency in healthcare. Here we introduce a simple attention scheme that weighs recent events more than old ones:  X  h = n and  X  t : n is the elapsed time between the step t and the current step n , measured in months; m t = 1 if emergency admission, m t = 2 if routine admission. The starting time step t 0 is used to control the length of look-back in the pooling, for example,  X  t 0 : n  X  12 for one year look-back. Since diseases progress at dif-ferent rates for different patients, we employ multiple look-backs: 12 months, 24 months, and all available history. Finally, the three pooled illness states are inferring about future prognosis. 3.5 Learning Learning is carried out through minimizing cross-entropy: L = fication, y  X  X  0 , 1 } , we use logistic regression to represent P ( y where the structure inside the sigmoid is given in Eq. ( 1 ). The cross-entropy becomes: L =  X  y log  X   X  (1  X  y ) log(1  X   X  ). Despite having a complex structure, DeepCare X  X  loss function is fully differentiable, and thus can be minimized using standard back-propagation. The details are omitted due to space constraint. 4.1 Data The dataset is a diabetes cohort of more than 12,000 patients (55.5 % males, median age 73) collected in a 12 year period 2002 X 2013 from a large regional Australian hospital. Data statistics are summarized in Fig. 3 . The diagnoses are coded using ICD-10 scheme. For example, E10 is diabetes Type I, and E11 is dia-betes Type II. Procedures are coded using the ACHI (Australian Classification of Health Interventions) scheme, and medications are mapped in ATC codes. We preprocessed by removing (i) admissions with missing key information; and (ii) patients with less than 2 admissions. This leaves 7,191 patients with 53,208 admissions. To reduce the vocabulary, we collapse diagnoses that share the first 2 characters into one diagnosis. Likewise, the first digits in the procedure block are used. In total, there are 243 diagnosis, 773 procedure and 353 medication codes.
 4.2 Implementation The training, validation and test sets are created by randomly picking 2 / 3, 1 / 6, 1 / 6 data points, respectively. We vary the embedding and hidden dimensions from 5 to 50 but the results are rather robust. We report results for M =30 embedding dimensions and K = 40 hidden units. Learning is by SGD with mini-batch of 16. Learning rate starts at 0.01. After n waiting cannot find smaller training cost since the epoch with smallest training cost, the learning rate is divided by 2. At first, n waiting = 5, then updated as n min { 15 ,n waiting +2 } for each halving. Learning is terminated after n or after learning rate smaller than =0 . 0001. 4.3 Modeling Disease Progression We first verify that the recurrent memory embedded in DeepCare is a realis-tic model of disease progression . We use the bottom layer of DeepCare (Sects. 3.1  X  3.3 ) to predict the next n pred diagnoses at each discharge using Eq. ( 7 ). ease transition probabilities P d i t | d j t +1 from disease d an admission with disease subset D t , the next disease probability is estimated as Q d i ; t = 1 | D oryless Markov model by 8 . 8 % with n pred =1andby27 . 7 % with n Modeling irregular timing and interventions in DeepCare gains a further 2 % improvement. 4.4 Predicting Unplanned Readmission Next we demonstrate DeepCare on risk prediction. For each patient, a discharge is randomly chosen as prediction point, from which unplanned readmission after 12 months will be predicted. Baselines are SVM and Random Forests running on standard non-temporal features engineering using one-hop representation of diagnoses and intervention codes. Then pooling is applied to aggregate over all existing admissions for each patient. Two pooling strategies are tested: max and sum . Max-pooling is equivalent to the presence-only strategy in [ 1 ], and sum-pooling is akin to an uniform convolutional kernel in [ 20 ]. This feature engineering strategy is equivalent to zeros-forgetting  X  any risk factor occurring in the past is memorized.
 Dynamics of Forgetting. Figure 4 (left) plots the contribution of time into the forget gate. The contributions for all 40 states are computed using Q as in Eq. ( 13 ). There are two distinct patterns: decay and growing. This sug-gests that the time-based forgetting has a very small dimensionality, and we will under-parameterize time using decay only as in Eq. ( 12 ), and over-parameterize time using full parameterization as in Eq. ( 13 ). A right balance is interesting to warrant a further investigation. Figure 4 (right) shows the evolution of the forget gates through the course of illness (2000 days) for a patient.
 Prediction Results. Table 2 reports the F-scores. The best baseline (non-temporal) is Random Forests with sum pooling has a F-score of 71.4 % [Row 4]. Using LSTM with simple mean-pooling and logistic regression already improves over best non-temporal methods by a 4.5 % difference in 12-months prediction [Row 5, ref: Sect. 2 ]. Moving to deep models by using a neural network as clas-sifier helps with a gain of 5.1 % improvement [Row 6, ref: Eq. ( 1 )]. By carefully modelling the irregular timing, interventions and recency + multiscale pooling, we gain 5.7 % improvement [Row 7, ref: Sects. 3.2 , 3.3 ]. Finally, with parametric time we arrive at 79.1 % F-score, a 7.7 % improvement over the best baselines [Row 8, ref: Sects. 3.2 , 3.3 ]. Electronic medical records (EMRs) are the results of interleaving between the illness processes and care processes. Using EMRs for prediction has attracted a significant interest in recent year [ 11 , 19 ]. However, most existing methods are either based on manual feature engineering [ 15 ], simplistic extraction [ 20 ], or assuming regular timing as in dynamic Bayesian networks [ 16 ]. Irregular timing and interventions have not been adequately modeled. Nursing illness trajectory model was popularized by Strauss and Corbin [ 2 , 4 ], but the model is qualitative but imprecise in time [ 7 ]. Thus its predictive power is very limited. Capturing disease progression has been of great interest [ 10 , 14 ], and much effort has been spent on Markov models [ 8 , 22 ]. However, healthcare is inherently non-Markovian due to the long-term dependencies. For example, a routine admission with irrel-evant medical information would destroy the illness memory [ 1 ], especially for chronic conditions.
 of a large volume of data. It has achieved great successes in cognitive domains such as vision and NLP [ 12 ]. To date, deep learning approach to healthcare has irregular timing is not property modeled. We observe that there is a considerable similarity between NLP and EMR, where diagnoses and interventions play the role of nouns and modifiers, and an EMR is akin to a sentence. A major difference is the presence of precise timing in EMR, as well as the episodic nature. Our DeepCare contributes along that line.
 DeepCare is generic and it can be implemented on existing EMR systems. For that more extensive evaluations on a variety of cohorts, sites and outcomes will be necessary. This offers opportunities for domain adaptations through parameter sharing among multiple cohorts and hospitals. In this paper we have introduced DeepCare, a deep dynamic memory neural network for personalized healthcare. In particular, DeepCare supports prognosis from electronic medical records. DeepCare contributes to the healthcare model literature introducing the concept of illness memory into the nursing model of illness trajectories. To achieve precision and predictive power, DeepCare extends the classic Long Short-Term Memory by (i) parameterizing time to enable irreg-ular timing, (ii) incorporating interventions to reflect their targeted influence in the course of illness and disease progression; (iii) using multiscale pooling over time; and finally (iv) augmenting a neural network to infer about future out-comes. We have demonstrated DeepCare on predicting next disease stages and unplanned readmission among diabetic patients. The results are competitive against current state-of-the-arts. DeepCare opens up a new principled approach to predictive medicine.

