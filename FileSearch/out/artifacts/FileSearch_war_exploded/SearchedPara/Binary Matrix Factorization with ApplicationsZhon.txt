
An interesting problem in Nonnegative Matrix Factor-ization (NMF) is to factorize the matrix X which is of some specific class, for example, binary matrix. In this paper, we extend the standard NMF to Binary Matrix Factorization (BMF for short): given a binary matrix X, we want to fac-torize X into two binary matrices W , H (thus conserving the most important integer property of the objective matrix X) satisfying X  X  WH. Two algorithms are studied and com-pared. These methods rely on a fundamental boundedness property of NMF which we propose and prove. This new property also provides a natural normalization scheme that eliminates the bias of factor matrices. Experiments on both synthetic and real world datasets are conducted to show the competency and effectiveness of BMF.
Binary data have been occupying a special place in the domain of data analysis [15]. Typical applications for bi-nary data include market basket data, document-term data, Web click-stream data (users vs websites), DNA microar-ray expression profiles, or protein-protein complex interac-tion network. For instance, in market basket data, each data transaction can be represented as a binary vector where each element indicates whether or not any of the corresponding item/product was purchased. In document data, each docu-ment can be represented as a binary vector where each el-ement indicates whether a given word/term was present or not.

A distinctive characteristic of the binary data is that the features (attributes) they include have the same nature as the data they intend to account for: both are binary. This characteristic implies the symmetric association relations between the data and features. In addition, binary data are usually high dimensional. Most clustering algorithms do not work efficiently in high dimensional spaces due to the curse of dimensionality . Many feature selection techniques have been applied to reduce the dimensionality. However, the correlations among the dimensions are often specific to data locality; in other words, some data points are corre-lated with a given set of features and others are correlated with respect to different features [1]. For example, a subset of users are responsible for a subset of websites; a subset of genes are expressed under a subset of conditions. Thus for binary data clustering, one has to combine feature selection and clustering together.
Binary data matrix is a special case of non-negative ma-trix. The nonnegative matrix factorization (NMF) has been shown recently to be useful for clustering [16]. NMF can be traced back to 1970s (Notes from G. Golub) and is studied extensively by Paatero [22]. The work of Lee and Seung [13, 14] brought much attention to NMF in machine learn-ing and data mining fields. A recent theoretical analysis [4] shows the equivalence of NMF and spectral clustering and K -means clustering. Various extensions and variations of NMF have been proposed recently [5, 12, 18, 24, 27].
Despite significant research progress in NMF, few at-tempts have been made to extend the standard NMF meth-ods to binary matrices. As we discussed in Section 1.1, bi-nary data has many distinctive characteristics and we be-lieve these new aspects of binary data deserve theories and algorithms of their own. An attempt of binary matrix fac-torization (e.g., block diagonal clustering) was proposed in [15] where the input binary data matrix X is decomposed into a binary matrix W and a non-negative matrix H .In block diagonal clustering, the binary matrix W explicitly designates the cluster memberships for data points and the matrix H indicates the feature representations of each clus-ter. In this paper, we study the problem of decomposing the input binary data matrix X into two binary matrices W and H . The binary matrices W and H preserve the most important integer property of the input matrix X and they also explicitly designates the cluster memberships for data points and features. We call the decomposition as Binary Matrix Factorization, denoted by BMF.
Both the theoretical and practical aspects of BMF are studied in the paper. This rest of paper is organized as follows: Section 2 introduces the notations and gives an overview on matrix factorization methods related to BMF; Section 3 proves the boundedness property of nonnegative matrix factorizations and discusses the theoretical founda-tions for BMF; Section 4 presents penalty function and thresholding algorithms for BMF; Section 5 illustrates BMF using numerical examples; Section 6 shows experimental results on both synthetic and real world datasets; and finally Section 7 concludes.
Let the input data matrix X =( x 1 ,  X  X  X  , x n ) contain the collection of n data column vectors. Generally, we factorize X into two matrices, where X  X  p  X  n , W  X  p  X  r and H  X  r  X  n . Generally, the rank of matrices W , H is much lower than the rank of X (i.e., r min ( p , n ) ). Here we provide an overview on the matrix factorization methods related to BMF: 1. SVD: The classic matrix factorization is Principal 2. NMF: When the input data is nonnegative, and we re-3. GCM (General Clustering Model) for binary data: 4. BMF: The BMF can be written as
BMF can be derived based on variants of standard NMF algorithms. However, two problems need to be resolved first.  X  Uniqueness . For any solution W , H , we can always  X  Scale . Another related problem is the scale problem
The boundedness property of NMF discussed below is motivated by the above two problems.
In this section, we propose and prove a new fundamen-tal property of NMF. A standard decomposition of ma-trix is SVD: X = U  X  V T = U V where U = U  X  1 / 2 and V = V  X  1 / 2 typically contain mixed sign elements. NMF differs from SVD due to the absence of cancellation of plus and minus signs. But what is the fundamental signature of this absence of cancellation? Let X = WH , where X  X  p  X  n , W  X  p  X  r and H  X  r  X  n . We say the input matrix X is bounded, if 0  X  X ij  X  1. Note that binary input matrix is bounded and we can rescale a nonnegative input matrix into the bounded form. The boundedness property of NMF states: if X is bounded, then the factor matrices W, H must also be bounded, i.e., 0  X  W
We note that SVD decomposition does not has the boundedness property. In this case, even if the input data are in the range of 0  X  X ij  X  1, we can find some elements of U and V such that U &gt; 1 and V &gt; 1. For NMF, we motivate the property as following. We can write X ij =  X  r k = 1 W Due to non-negativity, no cancellation can occur. Since X for this to hold is the bounded property of elements of W and H , i.e., W ik  X  1 , H kj  X  1. We prove in Theorem 1 that this is indeed the case.
 Theorem 1 (Boundedness Theorem)
Let 0  X  X  X  1 be the input data matrix. W , H are the nonnegative matrices satisfying There exists a diagonal matrix D  X  0 such that with If X is symmetric and W = H T , then H  X  = H.
 Proof: First of all, rewrite W =( w 1 , w 2 ,  X  X  X  , w r ) , ( h where max ( w i ) , 1  X  i  X  r is the largest element of the i -th column of W and max ( h j ) , 1  X  j  X  r is the largest element of the j -th row of H .
 Note We obtain Construct D as D = D  X  1 / 2 H D 1 / 2 W , then Thus Equation (7) is proved.

Furthermore, ( Without loss of generality, assuming that then we have max ( W j )  X  max ( H j )  X  W t 1 H 1 l +  X  X  X  W tj H jl +  X  X  X  + So 0  X  W  X  ij  X  1 and 0  X  H  X  ij  X  1.
 If X is symmetric and W = H T , which implies H  X  = H .

Later we will show that we resolve the scale problem by applying the proposed rescaling in the theorem to the binary data as a special case of bounded matrices. In this way, dis-cretization can work properly because W , H are in the same scales. This is crucial for our proposed penalty function algorithm and thresholding algorithm, making them more robust. The theorem can also be regarded as the normaliza-tion of W and H . Its function can be seen more clearly from Table 2.
Given a matrix X composed of non-negative elements, the task of NMF is to factorize X into a non-negative matrix W and another non-negative matrix H such that X  X  WH . In general, the derived algorithm of NMF is as follows:  X  Randomize W and H with positive number in [0, 1].  X  For W , update H , then update W for the updated H and In this section, we extend the standard NMF to BMF: given a binary matrix X , we want to factorize X into two binary matrices W , H (thus conserving the most important integer property of the objective matrix X ) satisfying X  X  WH . This is not straightforward and two parallel methodolo-gies (e.g., penalty function algorithm and thresholding al-gorithm) have been studied and compared. we show that in this paper each of these two methods has its own advantages and disadvantages.
In terms of nonlinear programming, the problem of BMF can be represented as: which can be solved by a penalty function algorithm. The algorithm is described as follows: Step 1: Initialize  X  , W , H and  X  .
 Step 2: Normalize W , H using Theorem 1.
 Step 3: For W and H , alternately solve: Step 4: if ( H 2 ij  X  H ij ) 2 +( W 2 ij  X  W ij ) 2 &lt;  X 
In step 1, W , H are initialized with the result of the orig-inal NMF algorithm [13, 14] applied to X . In step 3, the update rule is derived as follows : First, the derivative of the cost function J ( W , H ) with respect to H is:
Let the step size which is the longest stepsize that can maintain the non-negative property of H during the iterative process, then By reversing the roles of the W and H , one can easily ob-tain the update rule of W . Similarly the update formula can be obtained when X is symmetric. The convergence of the algorithm is guaranteed as long as the minima of step 3 can be achieved.
The second method is thresholding, in other words, find-ing the best thresholds w , h for W and H respectively so that the minima of the following problem can be achieved: where the Heaviside step function is defined as and  X  ( W ) is element-wise operation:  X  ( W ) is a matrix whose ( i , j ) -th element is [  X  ( W )] ij =  X  ( W ij ) of W , H are given via the original NMF algorithm [13, 14].
As we can see,  X  ( x ) is non-smooth, so the problem is a non-smooth optimization problem. There are two imple-mentations to conquer this difficulty.  X  Discretization Method: We discretize the domain  X  Gradient Descent Method: We approximate the Heav-
In this paper, we use gradient descent method for opti-mization as the first implementation is too time-consuming. The gradient descent thresholding method can be described as follows: Step 1: Initialize w 0 , h 0 , k = 0.
 Step 2: Normalize W and H using Theorem 1.
 Step 3: Compute gradient direction g k of F ( w , h ) . Step 4: Select the step size  X  k .
 Step 5: w k + 1 = w k + 1  X   X  k g k , h k + 1 = h k  X   X  k if the stop criterion is satisfied, else end .
 In step 3, the gradient direction g k is: g ) , g ) . where W  X  =  X  ( W  X  w k ) , H  X  =  X  ( H  X  h k ) . In step 4,  X  k can be selected by minimizing F ( w k  X   X  tical, Wolfe line search method can be applied which re-quires  X  k satisfying: where d k =  X  g k and  X  ,  X  are constants, 0 &lt;  X  &lt;  X  &lt;
It can be proved that the step size  X  k is well-defined in this way, that is,  X  k exists as long as g T k d k &lt; well studied method, gradient decent, the convergence is guaranteed.
In this section, we use examples to: i) illustrate the de-tailed factorization results of BMF; ii) demonstrate the ef-fects of normalization; iii) investigate the performance of the two BMF algorithms on the input matrices with differ-ent conditions such as sparsity parameters and ranks.
First, we show the detailed results of BMF on two ex-amples where the size of X is 5  X  8 and r = 3. Note that the two input matrices have different sparsity. In the fol-lowing illustration, Derivative refers to the gradient decent thresholding method, penalty refers to the penalty function method, Max = max ij ( | V  X  W  X  H | ij ) , and J ( W , H same as that in section 4.
 Example 1: X = Derivative:
W =
H = Penalty:
W =
H = Example 2: X = Derivative:
W =
H = Penalty:
W =
H =
If we regard each column of X as a data point, then we can find that H matrices in both Derivative and Penalty pre-serve many important data characteristics of X (e.g., integer property, pair-wise distance).
In this section, we perform a set of numerical simulations to examine the performance of the two BMF algorithms on the input matrices with different conditions and to demon-strate the effects of normalization. The input matrix X is generated as follows:  X  Step 1 . Randomize X with positive number in [0, 1].  X  Step 2 . for the element X ( i , j ) &gt; p , X ( i , j
Table 1 shows the numerical results where the size of the input binary matrix X is 200  X  400. In Table 1, the den-sity parameter P is selected from { 0 . 2 , 0 . 5 , 0 . 8 NMF is a restricted form of matrix factorization. To evalu-ate the performance of NMF, we compare it with SVD using ||
X  X  X  X  || 2 as the evaluation function where X  X  = W m , via NMF and X  X  =  X  1 u 1 v T 1 +  X  2 u 2 v T 2 +  X  X  X  +  X  SVD. NMF refers to the standard NMF algorithm, BMF-penalty refers to the penalty function method, and BMF-threshold refers to the gradient decent thresholding method. Diff-W and Diff-H show the significant difference between the results of penalty method and thresholding method which indicates that the two methods are equivalently im-portant and can not be replaced by each other. From Table 1, we observe that when the input matrix X is dense (i.e., P is small), the penalty function algorithm works better than the thresholding algorithm and the thresholding algorithm is better when the input matrix X is sparse.

One useful consequence of Theorem 1 is the normaliza-tion of W , H , which eliminates the bias between W and H . This is especially true when the matrix X is sparse. Table 2
Table 2. Comparison of the normalized case and non-normalized case (in parenthesis).

Shown are percentage of nonzero elements. demonstrates the effect of normalization. P , again, refers to the sparsity parameter selected from { 0 . 2 , 0 . 5 , values in the bracket is the percentage of non-zero ele-ments in the non-normalized case, and the values outside the bracket is the percentage in the normalized case. One can observe, from Table 2, that the normalization process has effectively eliminated the bias between W and H and made the results more robust. Without normalization, the resulting matrix H is often very sparse (sometimes it even becomes zero matrix) while W is very dense. As a result, much information that should be given via H is lost and this can not be compensated by the resulting dense matrix W .
Three sets of experiments are conducted to evaluate the performance of BMF. First, Synthetic datasets is used to evaluate the effectiveness of BMF. A second set of exper-iments is performed on gene expression datasets to identify the bicluster structures. Finally BMF is applied on docu-ment datasets for document clustering. 6.1.1 Data Generation We use the method described in [23] to generate synthetic datasets. Four datasets are generated with different bicluster structures as shown in Figure 1. The main advantage of us-ing synthetic datasets is that the detailed bicluster structures are known and hence we can evaluate the performance of our BMF methods with different factors such as noise level and overlap degree systematically. In order to perform sys-tematic evaluation with a large number of experiments, the Figure 1. Bicluster Structures in Synthetic
Datasets datasets are kept small and they are of size 100  X  100. Note that the size of the datasets does not restrict the generality of the experimental results as we are focusing on the inherent structures of the input matrix [23]. 6.1.2 Results Analysis We use the match score defined in [23] to assess the biclus-tering performance. Formally, If M 1 , M 2 are two bicluster-ing sets, the match score in attribute dimension of M 1 with respect to M 2 is:
The match score in sample dimension can be defined similarly. In general, the higher the score, the better the clustering performance. The thresholding BMF algorithm is used in our experiments since the datasets are generally sparse. We compare it with four other methods, BiMax [23], ISA [11, 10], SAMBA [26],and Binary Non-Orthogonal Matrix Decomposition [19](BND for short). Note that BND is based on heuristics while BMF is based on non-linear programming. In addition, BND is sensitive to initialization of the iterative process. The first three algorithms have been reported to be the best among the six biclustering meth-ods [23]. ISA and BiMax are implemented by the software BicAT developed by [23], SAMBA is implemented by EX-PANDER [25], and BND is provided by PROXIMUS [19].
Figure 2 and Figure 3 present the results on synthetic datasets. We use match score G ( M opt , M comp ) as the stan-dard to assess the performance, where M opt is the implanted biclustering structure and M comp is the computed bicluster-ing structure. From Figure 2 and Figure 3, we observe that: i) the thresholding BMF is almost noise-independent and overlap degree-independent; ii) the thresholding BMF is al-ways the best among the four methods and can nearly iden-tify all the bicluster structures. The main reason is the abil-ity of BMF to correctly discretize original matrix. This is one of the key characteristics of BMF and is very impor-tant for identifying the exact bicluster structures. The re-sults also show that, unlike the other greedy search strategy-based algorithms, BMF is more likely to find the global op-tima. Figure 2. Performance on non-overlap case.
 BinaryNMF represents the thresholding BMF.

The top figure shows the results for constant biclusters and the bottom figures shows the results for additive biclusters.
In this section, we perform experiments to identify the bicluster structures on real world gene expression datasets. 6.2.1 Datasets Description Three gene expression datasets: AML/ALL data [2], lung cancer data [9], and Central Nervous System tumor data [2], are used in our experiments. The ALL/AML dataset includes two types of human tumor-acute myelogenous leukemia (AML, 11 samples) and acute lymphoblastic leukemia (ALL, 27 samples). Also ALL can be divided into two subtypes-ALL-T(8 samples) and ALL-B(19 sam-ples). The Central Nervous System (CNS) dataset consists of 34 samples: 10 classic medulloblastomas, 10 malignant, gliomas, 10 rhabdoids and 4 normals. The Lung Cancer
Figure 3. Performance on overlap case. Bina-ryNMF represents the thresholding BMF. The top figure shows the results for constant bi-clusters and the bottom figures shows the re-sults for additive biclusters. (LC) dataset is composed of 32 samples including malig-nant pleural mesothelioma (MPM, 16 samples) and ade-nocarcinoma (ADCA, 16 samples). The datasets and their characteristics are summarized in Table 3.

In our experiment, the genes are ranked according to their coefficient of variation (i.e., standard deviation divided by the mean) and the top genes are selected. The gene ex-pression data can be represented as a matrix X of n  X  m , the i -th row of which represents the i -th gene X  X  expression level across the m different samples. X is first discretized into binary matrix using the method described in [23]. In the resulting matrix, the element a ij denotes whether the gene i is active in the sample j or not.
 Table 3. Description of Gene Expression
Datasets 6.2.2 Result Analysis Table 4 shows the results on gene expression datasets. We compare our thresholding BMF method with SAMBA. Note that the results of BiMax and ISA are not included since they are either time-consuming for large datasets or do not yield any robust biclustering results. As one can see, the match scores of the thresholding BMF is consistently higher than those of SAMBA. Although the detailed biclus-ter structures are unknown, the results indicate that BMF is a promising model for biclustering. In addition, SAMBA generates many bicluster structures among which some are obviously meaningless as the match score in sample dimen-sion based on these structures are very low.
 Table 4. Comparison on Gene Expression
Datasets
In this section, experiments are conducted on document datasets to evaluate the performance of BMF methods. In our experiments, documents are represented using the bi-nary vector-space model where each document is a binary vector in the term space. Since the document datasets are usually sparse, so the thresholding method is used in exper-iments. We also compare our BMF algorithm with K-means and standard NMF algorithms. 6.3.1 Datasets Description We use a variety of datasets, most of which are frequently used in the data mining research. Table 5 summarizes the characteristics of the document datasets. More detailed de-scription of these datasets can be found in [17, 16]. To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored. In all our experiments, we first select the top 1000 words by mutual information with class labels. The feature selection is done with the rainbow package [20]. 6.3.2 Results Analysis The above document datasets are standard labeled corpora. We view the labels of the datasets as the objective knowl-edge on the structure of the datasets. We use accuracy as the clustering performance measure. Accuracy discovers the one-to-one relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class [16]. The experimental results are shown in Table 6.

Table 6. Clustering Accuracy. Each entry is the clustering accuracy of the column method on the corresponding row dataset.
 The results obtained by averaging 10 trials.
 From the experimental comparisons, we observe that: (i) Both NMF and BMF algorithms outperform the K-means clustering algorithm. As discussed in [4], NMF is equiva-lent to soft K-means and the soft relaxation improves clus-tering performance. (ii) The accuracy results of NMF and BMF are really close and their differences are small. On CSTR, WebKB4 and WebACE datasets, NMF is slightly better than BMF; while BMF is slightly better than NMF on Reuters and Log datasets. (iii) In general, BMF is a re-stricted form of NMF. Earlier studies [3, 7] have discussed the biclustering aspect of NMF. But the key difficulty is that one can not identify the binary relationship exactly as the resulting matrices W and H are not binary. However, BMF can explicitly identify the co-association relationships be-tween the documents and terms since W and H are binary. Hence, BMF is a competitive option for binary data clus-tering, especially when interpretability is viewed as a goal of the data analysis. (iv) Since W and H are binary, BMF offers a framework for simultaneously clustering the docu-ments and terms. The framework is able to perform implicit feature selection and provide adaptive metrics for document clustering. Both of these properties are preferable for clus-tering in high-dimensional data.
In this paper, we extend the standard NMF to BMF: given a binary input matrix X , we want to factorize X into two binary matrices W , H satisfying X  X  WH . BMF pre-serves the most important integer property of X . Two fac-torization methods: penalty function and thresholding, are proposed and studied. We also prove the boundedness the-orem to eliminate the bias of factorization and make our proposed methods more robust. Our study suggests that the penalty function method works better when the input binary matrix X is dense while the thresholding method excels oth-erwise. Experimental results show the usefulness and com-petitiveness of BMF.

Acknowledgments: This work is partially supported by the National Natural Science Foundation of China un-der grant No.10631070, and the Ministry of Science and Technology, China, under grant No.2006CB503905. Tao Li is partially supported by a IBM Faculty Research Award, NSF CAREER Award IIS-0546280 and NIH/NIGMS S06 GM008205. Chris Ding is supported in part by a University of Texas STARS Award.

