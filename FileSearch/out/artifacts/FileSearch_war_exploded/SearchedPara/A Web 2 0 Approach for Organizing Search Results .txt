 With the current growth of the World Wide Web, it is now possible to access huge amounts of data and information from different sources all over the world. Web search engines aim to help users to find appropriate data for their information needs. Although this simple method is sufficient in many cases, it may be inefficient in documents. This issue is especially annoying when the keywords that the user has chosen as the query have other more popular senses. For example, suppose that a user is using the query  X  X ava X  to get some information about Java Island. Since the  X  X rogramming language X  sense of Java is much more popular, the user will have difficulty finding results related to his information need. 
There has been limited research on organizing search results and efficiently presenting them to the user compared to the vast amount of work on ranking search results. Most work on organizing search results is focused on clustering the results and presenting them to the user in groups. Hearst [24] provided a clustering algorithm as an alternative to simple ranked list. Sanderson [12] introduced a method to automatically create a hierarchical organizatio n of concepts. Although this work is not focused on organizing search results, it can be applied to such a problem as well. Also [3, 4, 5, 6, 11, 15, 18, 19, 20, 21 and 25] introduced new techniques to cluster that cluster search results before presenting them to the user. Although all these approaches show some benefit of clustering search results, all these approaches are faced with two challenges: 1) how to identify appropriate clusters and 2) how to label the clusters properly. 
To address these two challenges, in this paper we propose to use Wikipedia to organize the search results. Specifically, our proposed method first identifies the appropriate categories using Wikipedia. It then extracts a set of training documents categories. We also create a hierarchical stru cture, so if a user needs categories to be more detailed, she can go through the hier archical structure. Since the articles in Wikipedia are written and edited manually, the extracted categories are expected to better match the ideal partitioning of documents for the user. 
Recently there has been a lot of research on using Wikipedia for different purposes. [7, 8, 9 and 15] have used different features of Wikipedia for clustering and/or cluster labeling, [2, 13, 10, 17, 22 and 23] have used Wikipedia for query processing purposes and [1 and 16] have used Wikipedia for query expansion. 
Although a lot of research has been conducted on Wikipedia for different purposes, our perspective for using Wikipedia for hierarchical search result organization seems to be new. 
In our study, experiment results show that using Wikipedia, we can build a hierarchical category structure that is more accurate than the structures constructed by the clustering methods in most cases. Also we discovered that by using Wikipedia, we can build appropriate training sets for the purpose of training the classifiers. Our proposed method is shown to be especially successful for simple queries and one word queries. 
The rest of the paper is organized as follows: We describe some of Wikipedia X  X  features and the details of our proposed method for organizing search results in section 2, discuss the experiment results in section 3 and finally conclude in section 4. Wikipedia is a free collaborative online multilingual encyclopedia. The basic unit of Usually important words in each article are linked to their corresponding pages. Each article is divided into a number of sections, and a specific concept is discussed in each section. Usually, these concepts have their own pages in Wikipedia and there is a link related topics. Usually this list is organized well and can be very helpful for extracting related categories. For example in the list of Iran related articles 3 , a number of articles about Iran are categorized into meaningful categories. Some of these categories are:  X  X conomy X ,  X  X istorical X ,  X  X olitics X  and  X  X eographic X . Another type of Wikipedia related topics, we can find articles rela ted to a specific topic categorized into meaningful categories. Each Wikipedia artic le is placed in one or more categories. topics. 
Search queries vary in semantics and user X  X  need. Similar to Xu [2] we define three types of queries: By AQ we mean a type of query that contains exactly one term which has more than one potential sense, e.g.  X  X ava X . These queries may or may not be semantically ambiguous. Usually there exists a disambiguation page for ambiguous terms. By EQ, we mean a type of query that has specific meaning and covers a narrow topic, e.g.  X  X ersian history X . This kind of query should have an article in Wikipedia with the focused on a specific entity, and there exist no article in Wikipedia with the same title. The difference between our definitions of query types and the definition propose by Xu [2] is that we count the queries about specific subjects with no corresponding Wikipedia pages as BQ. For example a query such as  X  X ersia History X  with no article in Wikipedia is treated as BQ. We organize the search results in five steps, as shown in Figure 1. 
In the first step we try to find a matching article in Wikipedia for the query. After finding the matching article we extract categories from that article. The extraction training data for each category and train a classifier for the categories. We also expand the training set to obtain better results. The expansion process is described in avoid sparse categories, in the fifth step, we expand the original query for each category to provide additional search results. The result will be a hierarchy of organized search results, with at least a minimum number of pages in each category. 2.1 Finding Wikipedia Page and Extracting Categories Although the overall algorithm is the same for all types of queries, it differs in details especially in finding a matching Wikipedia page and extracting categories. These details are described in the following sub sections.  X  Finding Wikipedia Page and Extracting Categories for Ambiguous Queries categories corresponding to different senses of the ambiguous term. This introduces two challenges: how to find all the meanings of the term, and how to categorize search results in the groups 
As stated before, Wikipedia contains disambiguation pages that cover the possible senses of a specific term. Even some people believe that Wikipedia is more suitable than WordNet for disambiguation [14]. Fo r example while WordNet contains three different senses for the word  X  X ava X , Wikipedia X  X  disambiguation page for this term contains eight possible senses. Each section in a disambiguation page refers to a specific sense of that word. Also each section contains a number of links to some title of the first section is  X  X nimals X  which contains links to articles with titles  X  X ava Pipistrelle X ,  X  X ava Shark X ,  X  X ava Sparrow X  and  X  X ava Chicken X . The title of each section perfectly describes that sense and thus we propose to use the section X  X  titles as the category names. For the query Java, the extracted groups from Wikipedia will be:  X  X ava-Animals X ,  X  X ava-Literature X ,  X  X ava-Computer Science X ,  X  X ava-Consumables X ,  X  X ava-Entertainment X ,  X  X ava-Geography X ,  X  X ava-Plants X  and  X  X ava-Transportations X . In pointing to related articles.  X  Finding Wikipedia Page and Extracting Categories for Entity Queries By EQ, we mean a type of query that has specific meaning and covers a narrow topic. Also there should exist an article about these queries in Wikipedia. When the query exactly matches the title of an article, the corresponding Wikipedia article can be group names. Usually each section contains some subsections. We use these subsections to create a hierarchical structure. To gather training data, we use links in each sections or subsections. For example for query  X  X ran X , to gather training data about the group  X  X ran-history X , we use links in history section of article with the title somehow relates to that section X  X  title, thus we expect to gather an appropriate dataset for training this way. Each section may contain a list of links, and the theme changes smoothly in each section. Thus we use uniform sampling for link selection in each section, both to avoid the large number of training articles, and to make sure that we have articles related to all parts of the section in our training data.  X  Finding Wikipedia Page and Extracting Categories for Broader Queries Since for some queries the corresponding page cannot be found easily, we try to use other features of Wikipedia for this purpose. Consider a query like  X  X ran energy X . To find the appropriate categories, we consider each query term in turn and try to find the topics, the topics are presented in a categor ized manner. Each category is considered as a group name and a classifier is trained for the categories using the linked pages for training. We then classify the original query in the extracted categories. In the above example and among the categories extracted from the list of related topics to  X  X ran X , the query will fall in the  X  X conomy X  category. Now we create a new classifier whose groups are the links in the selected category. We train this new classifier by content of linked article. Also we gather our training data for each group by extracting texts from a uniform sample of links in that article. In the above example, candidate groups are  X  X irlines of Iran X ,  X  X conomy of Iran X ,  X  X nergy in Iran X ,  X  X ranian cars X  and etc. We use existing text in each article as training data and classify the word  X  X nergy X  using Wikipedia page. Extracting the categories from this page is similar to the method presented for EQ. 
At the end of some articles there is a link to a template page related to that page. If no article matches our need in related pages we can use these template pages just like list of related topics to find the target page. 
This algorithm only works if a corresponding page in Wikipedia exists whose with more than one ambiguous term. For example it will succeed for query  X  X ran energy X  but it will not find a matching page for  X  X orth Iran X  or  X  X ava Brand Iran X  and postponed to our future works. 2.2 Expanding the Training Set and Gathering Search Results Since in some articles there may not exist enough links for training the classifier, we propose an expansion technique to expand our training set. To this end, we construct a query for each category, which is an expansion of the original query biased toward the category. We use popular information retrieval query expansion techniques for this purpose. To expand the query for each category, we use the links in the corresponding section. We gather linked documents as document set for query expansion, search the query within this document set, and then we use top ranked results to expand the query. We expand our query using lemur toolkit language modeling methods. We do expansion three times with different expansion strategies and then fuse the expansion select top five results as the biased query terms. We use these terms along the original query as our new query and submit it to a search engine. We add top 10 returned results X  content to our training set. Expanding the training set had a great influence on the classification accuracy. In our experiments we noticed that even when there are lots of links available in the original document, the expansion step makes the classification more accurate. 
Having a category structure and a training set makes it easy to categorize search results. But what if after categorizing all results we end up with some empty categories? For example when we searched  X  X ava X  in Google, there were no result change the query and collect a minimum number of results for our categories. To do so, we create a new query using our original query along with the category name and there exist some results that aren X  X  related to our target category. So we use a binary use a binary language model classifier for this purpose and train it with the collected training data. After gathering all the results we simply re-rank the results so that user can find high quality results at top. Since Wikipedia X  X  articles are created by human, we expect that our results be more accurate and suitable than any clustering algorithm. To illustrate, Figure 2 shows the clustered result returned by carrot for the query  X  X iger X  as of May 2011. As like many other queries, the query, tiger has multiple meanings, we expect the organizing engine, to cover all possible senses. But returned results by carrot do not do so. Consider that we want some information about a company with the named  X  X iger X . By current result of this clustering engine, it is difficult to find some pages related to this information need. It will be easy to find the needed information with our proposed method. Considering the query  X  X iger X , the Categories returned by our method is:  X  X oology X ,  X  X eople X ,  X  X laces X ,  X  X ehicles X ,  X  X ports X ,  X  X edia and fiction X ,  X  X ame character X ,  X  X usic X ,  X  X usiness X ,  X  X echnology and mathematics X ,  X  X ther X . 
Our method almost returns all possible meaning for that query. With these categories, it will be easier to find some information about a company named  X  X iger X . 
In order to do a more thorough evaluation of our system, we did a user study, and evaluated our method based on participants X  judgments. A screenshot of the evaluation system is shown on the left side of Figure 2 3.1 Experiment Settings In our experiments we downloaded our needed page from Wikipedia website using framework. Also we used KL-divergence method in lemur toolkit 5 for expanding query. All the coding was done with Java language. To obtain some search results for training and testing purpose, we used  X  X ahoo BOSS X  6 . Yahoo boss is a free API that allows developers to access yahoo search. We evaluate our method from two aspects. 1) Provided categories suitability. 2) Classification accuracy 
We had 9 participants in our experiments from age 22 to 28. Four of them were female and five were male. Three of them had BS degrees, two of them had MS Degree and the remaining Four Person was MS student. Also one of them had BS in Horticulture, one of them had MS in Food Science and another participant was MS student of Tourism management. The remaining six participants had BS or were MS students in computer related fields. 
Each participant has tested the system with at least 5 queries with one term, 3 ambiguous Queries and 3 queries with more than one term. In First steps designed software asked participants to provide a query term and a preferred category structure. After that System shows retrieved categories and asks users to compare retrieved categories with theirs and give our category a grade by selecting among these options: Also we asked them if there exist a category that exactly matches their seeking category. After that system shows them search results in a categorized view. 
For Evaluating the classification and re-ranking performance, System asks participants to judge top 10 results by answering a question about does this result fulfill their information needs by selecting among following options:  X  X es X ,  X  X o, But they tell the system that the search result is not related to the subject. 
After evaluating top 10 results we ask them 2 additional questions. First we ask them to search the exact query in carrot (b y selecting yahoo in carrot) and one system or both as their preferred categories. Second question was that does our category give them a hint to improve their query. They could answer this question by a simple yes/no.  X  Ambiguous queries As it is shown in Figure3 in 70.4% of questions user answered that our categories are similar or even better than their provided categories. 22.2% of answer was  X  X ood. But from different point of view X  and 7.4% answered that the categories are incomplete. Some of these queries were:  X  X aguar X ,  X  X ourist X , X  mint X  and etc. 
In table 1 we show Participant preference between our category and carrot categories. In answer to existence of us ers preferred category within the presented categories, in 56% of queries the answer was  X  X es X . Also in question about does system gave them a hint to improve their query the answers for 59% of queries were 84% of these judged results were exactly what they were looking for. 9% of judged results were categorized correctly but were not exactly what they were looking for. And the remaining 7% was not relevant to the selected category.  X  Entity Queries Participants evaluated out method by answering the questions introduced in 4.1. As it is shown in Figure3 in 75.6% of questions user answered that our categories is similar to or even better than their provided categories. 11.1% of answer was  X  X ood. But from different point of view X  and 13.3% answered that the categories are incomplete. Some of these queries were:  X  X ennis Ritchie X ,  X  X ata mining X ,  X  X lowers X ,  X  X avaScript X ,  X  X ourism X ,  X  X ata warehouse X ,  X  X oghurt X  and etc. Table 2 shows Participant preference between our categories and carrot categories. In answer to existence of users preferred category within the presented categories, in 73% of queries the answer was  X  X es X . Also in question about does system gave them a hint to improve their query the answers for 78% of queries were  X  X es X . We also judged results were exactly what they were looking for. 6% of judged results were categorized correctly but were not exactly what they were looking for. And the remaining 0.9% was not relevant to the selected category.  X  Broader Queries In 44% of questions system wasn X  X  able to provide category structure for user queries. 
The result below is about those 56% of queries which our system could find category structure for them. 
As Figure 4 shows in 73.3% of questions user answered that our categories is similar to or even better than their provided categories. 20% of answer was  X  X ood. But from different point of view X  and in 6.7% they said that the results is not relevant to their query. Some of these queries were:  X  X aldives music X ,  X  X iber made of glass X ,  X  X evolution in Egypt X ,  X  X iddle east war X  and etc. 
In table 3 we show Participant preference between our category and carrot categories. In answer to existence of us ers preferred category within the presented categories, in 53% of queries the answer was  X  X es X . Also in question about does system gave them a hint to improve their query the answers for 60% of queries were 74% of these judged results were exactly what they were looking for. 18% of judged results were categorized correctly but were not exactly what they were looking for. And the remaining 8% was not relevant to the selected category. In this paper, we proposed a search result organization method with dynamically extracted categories for each query. Our algorithm uses Wikipedia for extracting categories and gathering training data. By comparing our system X  X  result with carrot X  X , we noticed that when the query is simple, usually there is a well-formed and complete article in Wikipedia that leads to better human defined categories which automatically generate labels that does not cover all aspects of query and may not be relevant to each other. But when the query becomes much more complex, the quality of Wikipedia X  X  articles decreases and the article is no longer complete. This situation makes users to prefer carrot over our system. It shows that our proposed method still needs to be improved so that it could be able to handle more Complex Queries. 
Our proposed algorithm is still a prototype and we are planning to do some functional improvement in it. For example Wikipedia has lots of features that can help us to improve our algorithm like Wikipedia redirections and Wikipedia X  X  category structure. Another interesting feature of Wikipedia is  X  X ain article X . Using this feature will probably help constructing the hierarchy. Acknowledgments. This research is partially supported by Iran Telecommunication Research Center (ITRC). 
