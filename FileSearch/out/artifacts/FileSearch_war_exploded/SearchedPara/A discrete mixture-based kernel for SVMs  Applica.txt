 1. Introduction are shown through some experiments involving spam filtering and image database categorization. conclude the paper in Section 5. 2. A mixture model-based kernel for count data 2.1. SVMs set of NV -dimensional vectors X  X  X  ~ X 1 ; ... ; ~ X N  X  with labels y number of times word or visual feature v appears in the document or in the image, respectively. and Vapnik (1999) , for instance, for more details) determines the parameters of a decision function f  X  ( ~ is done by taking the sign of the following function: where n is the number of support vectors containing the relevant information about the classification problem, { k mize the margin between the classes (Vapnik, 1999 ), b is a bias term, U  X  dimensional input vector ~ X into another V 0 -dimensional feature space ( U  X  where tr denotes the transpose, is a symmetric positive definite kernel function. will discuss in the following section. 2.2. The MDM: a generative model for count data
Using mixture models, each vector of counts ~ X can be assumed to be generated by a linear combination of component density functions resulting in the following: where M is the number of components in the model, H ={ h j are the mixing parameters and represent the weights of each cluster j ; 0 2003; Elkan, 2006 ) where n j =( r j ,{ l jv }) are the parameters associated with the component density representing cluster j ; r j &gt; 0 ; 0 &lt; l j v &lt; 1 ; model in hand, we develop in the next subsection its corresponding Fisher kernel. 2.3. A fisher kernel for count data
The Fisher kernel was proposed initially in Jaakkola and Haussler (1999) and is computed at the estimated H on the resulting statistical manifold as follows: where U ~ X  X  H  X  denotes the Fisher score (i.e the gradient of log p  X  matrix given by (see Amari (1998) , for instance) document frequency (TF-IDF) representation for documents.
 dient of log-likelihood with respect the parameters of each component j : p where which gives us a M (2 + V )-dimensional corresponding feature space (i.e V form: ground information.
 3. The generative model estimation and selection 3.1. Parameters estimation of mixture models, the missing data are the vectors ~ Z i nents from which ~ X i arises and Z ij = 0 otherwise. By introducing the missing data Z  X  X  hood (i.e the log-likelihood if Z was observed) corresponding to our M -component mixture is given by vergence criterion is satisfied). The form of an iteration t in the EM algorithm is given in the following: (2) M-step: Update the parameter estimates according to complete-data log-likelihood given X ; H  X  t  X  and the quantity the original posterior probability and is given by Ueda and Nakano (1998) DACEMM.
 In the M-step of the DACEMM algorithm, we update the parameter estimates according to
When maximizing Eq. (13), we obtain
However, we do not obtain a closed-form solution for the n to estimate these parameters. The Newton X  X aphson method is based on the first, second and mixed derivatives of
Through Eq. (15), we can see clearly that we do not have a closed-form solution for r we use a Newton X  X aphson method: where where W 0 is the trigamma function.
 When computing the derivative w.r.t l jv , we have to take into account the fact that which does not yield to a closed-form solution. Thus, we apply Newton X  X aphson method given by the following where
Q ( H , H ( t ) , T ) and given by Then, the Hessian matrix can be written as the following: where S  X  diag  X  D 1 ; 1 ; ... ; D V 1 ; V 1 ; D v ; v  X  P ~ a T  X  1 and c  X   X  P V 1 v  X  1 1 D vv  X  1 . Then, the inverse of the matrix H ({ l 3.2. Estimation algorithm and selection of the number of clusters criterion (MMDL) proposed in Figueiredo, Leit X o, and Jain (1999) and given by where N M = M ( V + 2) is the number of parameters in our mixture model and N type criterion. Indeed, the MDL is given by By comparing Eqs. (23) and (24) , we can see that MMDL  X  M  X  X  MDL  X  M  X  X  selection is as the following:
Algorithm 1. For each candidate value of M : (2) Choose an initial estimate 2 H (0) . Set t 0 (3) Set c 0 (4) Iterate the following steps until convergence: (5) Increase s ( s s const ) (6) If s 6 1, set t t + 1, go to step 3 (7) Calculate the associated criterion MMDL ( M ) using Eq. (23) (8) Select the optimal model M * such that:
Experimentally, we have concluded that s min set to 0.04 is enough which is the same confirmation reached in Elkan the likelihood is less than a given =10 4 . 4. Experimental results 4.1. Experiments design and comparison with other kernels other kernels which are: a Fisher kernel based on finite multinomial mixture (MM), polynomial kernel
K poly  X  ~ X i ; ~ X j  X  X  X  ~ X i ~ X j  X  1  X  p , Gaussian kernel K with count data, we have taken v 2 function as a distance d sification problem. As we shall see, experimental results show that our model has significant practical potential. 4.2. Spam and text categorization commercial email from someone without a pre-existing business relationship X  (Goodman, Heckerman, &amp; Rounthwaite, ments, we have used three large publicly available datasets. The first data set is spambase by Andrew Farrugia 4 , and the third data set is 2006 ECML/PKDD Discovery Challenge
SVM light 6 (Joachims, 1999 ) package was used as an implementation of SVMs. Our results have been evaluated using some typical performance measures generally used for spam filtering problem: ticular importance in the case of spam filtering as we have mentioned previously.
In the second experiment, we apply our model for the classification of a widely used data set: the  X  X 20 newsgroups X  0.75, 0.77, 0.74, and 0.76 obtained using the polynomial, Gaussian, RBF, Sigmoid and MM kernels. 4.3. Hierarchical classification of vacation images from our database.
 counts.
 of clusters.
 erative models based kernels.

We also conducted other experiments which use directly our generative model (finite multinomial Dirichlet mixture be at least as powerful as the generative model used to develop the kernel. 5. Conclusions mixture models and SVMs. Via some applications concerning spam filtering and image databases categorization, model-molecular biology, language modeling and text compression.
 Acknowledgements Canada (NSERC), a NATEQ Nouveaux Chercheurs Grant, and a start-up grant from Concordia University. Appendix A
Here, we calculate the derivatives of log p  X  ~ X i j H  X  with respect to the mixture parameters. where
Thus,
As we have
Thus, where W is the digamma function. Finally, References
