 We demonstrate a method for collaborative ranking of future events. Previous work on recommender systems typically re-lies on feedback on a particular item, such as a movie, and generalizes this to other items or other people. In contrast, we examine a setting where no feedback exists on the partic-ular item. Because direct feedback does not exist for events that have not taken place, we recommend them based on individuals X  preferences for past events, combined collabo-ratively with other peoples X  likes and dislikes. We examine the topic of unseen item recommendation through a user study of academic (scientific) talk recommendation, where we aim to correctly estimate a ranking function for each user, predicting which talks would be of most interest to them. Then by decomposing user parameters into shared and in-dividual dimensions, we induce a similarity metric between users based on the degree to which they share these dimen-sions. We show that the collaborative ranking predictions of future events are more effective than pure content-based recommendation. Finally, to further reduce the need for ex-plicit user feedback, we suggest an active learning approach for eliciting feedback and a method for incorporating avail-able implicit user cues.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval; I.5.3 [ Pattern Recognition ]: Clustering X  Algo-rithms  X 
A major part of this work has been conducted while affili-ated with Nokia Research, Cambridge MA.  X 
A major part of this work has been conducted while affil-iated with MIT CSAIL, and during an internship at Nokia Research, Cambridge MA.
 Algorithms, Experimentation Recommendation Systems, Collaborative Filtering
Recommender systems aim to present items that are likely to be of interest to users. Such systems are widely imple-mented in industrial settings, including in e-commerce sys-tems where the goal is to recommend products to users based on their purchase, viewing, or rating history. For example, the Netflix challenge [4] was introduced to test recommen-dation methods based on historical movie ratings provided by customers.

The general recommendation setting consists of a pool of items and a pool of users, where user feedback such as rat-ings are available for varying subsets of items. The goal is to use the partial rating history to predict ratings for the remaining items that the users have yet to experience. Methods used to solve this problem can be roughly cat-egorized into content-based and collaborative filtering ap-proaches. Content-based approaches leverage feature de-scriptions of items and seek to relate user preferences to those features. Collaborative filtering methods, in contrast, rely solely on the historical record of user ratings and simi-larities across users in order to fill-in ratings for the remain-ing items. The problem is often cast as a matrix completion problem and solved using matrix factorization methods [18]. The two approaches are largely complementary.

In this paper, we consider a recommendation system in which the information items are events . Event recommenda-tion has various potential applications. Consider a location-tracking system, which records the user X  X  whereabouts within a geographical region, or inside a building [16]. Given loca-tion coordinates, it is desirable to provide users with per-sonalized location-based services. In this framework, event recommendation can bring to the user X  X  attention relevant events that take place nearby. Another motivation is assist-ing event organizers; if historical preferences about poten-tial attendees (the users) are available, recommender sys-tems can be used to predict the overall interest in the event. Such predictions can be used to guide resource management as well as to identify potential conflicts between simultane-ous events.
Our contributions are:
The paper proceeds as follows. Section 1.1 defines event recommendation as a ranking problem, and describes its modeling using a pure content-based approach. In Section 2 we outline our collaborative method for event recommenda-tion. Sections 3 and 4 describe the user study and event de-scriptions, respectively. We evaluate LowRank as compared to
RankSVM in Section 5. Section 6 outlines an active learn-ing method for eliciting relevant feedback from users that could further speed up LowRank . We review related work in Section 7. In Section 8, we conclude and discuss future work.
Informally, an event is an information item that is only valid for a short period of time. By the time one can expect user feedback on a specific event, that event is no longer relevant: an event recommendation system therefore has to recommend items for which no explicit feedback exists. This distinguishes events from other information items, like movies, for which some user feedback is directly available and continues to be useful. In general, one can approach event recommendation using existing content-based meth-ods, relating event descriptions to user preferences. How-ever, the quality of content-based recommendation is often highly dependent on the amount of user feedback available. We expect feedback about events to be relatively scarce; events are often topically diverse and new events may have low similarity to prior events. Modeling user preferences collaboratively may alleviate this data sparsity problem, by pooling together feedback from users with similar prefer-ences.

In this paper, we present a collaborative approach to event recommendation. Each user has a parameter vector that re-lates their preferences to event descriptions. This is neces-sary in order to be able to recommend yet unseen events. However, in our case, we first map event descriptions to a low-dimensional  X  X erceptual space X . Intuitively, the low-dimensional representation of events captures how events vary perceptually to users and are shared across users. The user parameters are then associated with these coordinates rather than the original feature descriptions of events. The number of parameters required to capture individual user preferences is therefore considerably smaller than in a regu-lar content-based approach.

We expect our approach, LowRank , to work best in a sce-nario where the available feedback about past events varies across users. For example, different subsets of users may have rated different past events. Such experience would help uncover the relations between the events, their low di-mensional feature representations, and the associated user preferences.

In order to evaluate our approach, we have carried out an empirical user study of event recommendation. Specif-ically, the user study, conducted at the Massachusetts In-stitute of Technology (MIT) and at Carnegie Mellon Uni-versity (CMU), concerns the recommendation of scientific talks 1 to users within their universities. The MIT group includes thirty users, who are mostly graduate students, re-search affiliates and faculty members at the Computer Sci-ence and Artificial Intelligence Laboratory (CSAIL). The CMU group includes graduate students of the computer sci-ence school. The study was designed to simulate a realistic setting, where user feedback becomes available after events take place and the task is to recommend upcoming events. We consider a ranking setting, where, in the beginning of every week, the talks scheduled for that week are ranked according to each user X  X  preferences. The data used in the study involves real seminar announcements, following their original temporal distribution.

In our experiments, we evaluate recommendation perfor-mance for several sets of future talks, given varying amounts of feedback on past talks. We evaluate content-based rec-ommendation methods using RankSVM [12]. Several sets of features are considered, including a traditional word-based model, as well as representing each talk in terms of its topic usage (see Section 4). We subsequently compare the perfor-mance of content-based ranking with our proposed LowRank method that supports collaborative ranking. The collabora-tive approach is shown to give superior performance.
Overall, our results show that it is possible to achieve good results on the task of scientific talk recommendation, which may be sufficient for practical applications. These results may be improved further by means of feature engineering.
Finally, we are interested in further reducing the amount of feedback required from each user. To this end, we sug-gest an active learning approach to this problem. While we do not give full results for active learning, our preliminary results are encouraging.

In summary, the main contribution of this paper is a framework for collaborative event recommendation. The ac-tive learning method provides a natural extension to this framework. In addition, based on the user study, we gener-ate a new dataset for event prediction. We consider a ranking problem, where, given a set of users U and known user feedback on a set of past events E P ,the goal is to generate rankings of a set of future events E F adapted to each of the users X  preferences.

We assume that user feedback is available in the form of pairwise preferences . In practice, events compete with each other on user resources, where a small subset of events can be usually attended in a given time frame. For instance, on a particular week, one may be able to attend up to two talks, due to a busy schedule; this means that in deciding which events to attend, the user should compare between alternative events taking place on that week, selecting those that agree most with his or her tastes. In order to model
We will use the words talk and seminar interchangeably. one X  X  individual ranking function, we therefore consider user inter-item preferences. Specifically, a preference pair is of the form ( e j ,e k )  X  R ( u ), implying that event e j is preferred to event e k by user u .

User feedback in the form of preference pairs can be elicited explicitly, by collecting user input on pairs of events: the user indicates which of the events is closer to his or her interests. Alternatively, if it is known that the user has at-tended (or, has been interested in attending) a subset of the events within a given time frame, then it is reasonable to conclude that those events are preferred by the user over the other events in the same time frame. More formally, we will associate every past event e j  X  E P with a time stamp t , denoting the time in which the event took place. We can split the time span in which past events E P have oc-curred into distinct reference time frames , where every time stamp t j maps to a single reference frame T i . It is assumed that all the events that take place in the same time frame T i are evaluated against each other. Let E + P ( T i ,u )denote the subset of events that the user u liked (or attended) in time period T i ,and E  X  P ( T i ,u ) be the complimentary subset that user u is known not to have liked. Then, this type of feedback can be decomposed into a set of pairs with the ex-pectation that ( e j ,e k )  X  R ( u ) whenever e j  X  E + P e  X  E  X  P ( T i ,u ). Future events are handled similarly, where the stream of incoming events is discretized into reference time frames.

This setting is similar to processing user click logs in in-formation retrieval [12], where the user is provided with a ranked list and clicks on items of interest. While, in the user click setting, users may scan only the top few items of a ranked list, we infer pairwise preference relations exhaus-tively: because the number of talks each user can pick from is relatively small (see Figure 1), we assume that all the dis-played events are observable. In general, if positive feedback corresponds to actual attendance, then the derived prefer-ences may be noisy as schedule constraints may prevent users from attending events they like. Calendar constraints can be potentially detected by dedicated applications. This is beyond the scope of this work.

Since no feedback is available for future events, recommen-dations must be based on event descriptions. Each event e is represented as a vector of m features, x j . User feedback in the form of preference pairs ( e j ,e k )  X  R ( u )meansthat announcement feature vector x j is preferred over x k .Our goal is to assign a real valued evaluation function for each future event e j  X  E F and user u . Then, given the evaluated scores, we can generate a ranked list of items per user.
We consider linear ranking functions where each event fea-ture vector x j ismappedtoascore  X   X  x j . The goal is to find parameters  X  such that the ranking function best captures past feedback from the user. In order to solve this problem for each user u individually, we can apply the RankSVM formulation [12] as follows: subject to  X  u  X  x j  X   X  u  X  x k +1  X   X  jk for all j and k such that e j  X  E + P ( T i ,u )and e k  X  E  X  P ( T i ,u ) for some past time frame T i . The slack variables  X  jk  X  0 turn the ranking constraints into soft constraints, effectively permitting us to make errors on some pairs that are difficult to capture with a linear ranking function. C is a trade-off parameter specifying how strongly we should try to respect all training constraints that can be set through cross-validation.
The RankSVM approach described above requires us to estimate an m  X  dimensional parameter vector  X  u separately for each user regardless of potential similarities across users. We hypothesize that most users base their decisions about events on a smaller number of latent  X  X erceptual X  features about the events. In order to uncover these latent feature dimensions, we introduce a k  X  m parameter matrix V , shared across all users. We use this matrix to map event descriptions x j into a k  X  dimensional subspace x j = Vx where k m .Eachuser u will subsequently estimate their k  X  dimensional parameter vector  X  u to work well as part of the transformed event descriptions  X  u T  X  x j =  X  u T Vx that users may hold diametrically opposing views about how to use the latent feature dimensions. We merely impose the constraint that they agree on what these dimensions repre-sent.

The number of parameters we need to estimate across users is now considerably smaller. If N denotes the num-ber of users, then we are estimating Nk + km parame-ters instead of Nm . The difference is substantial when k m . In fact, our method can be seen as imposing a rank k constraint on the collective parameter choices. If we stack m  X  dimensional row vectors  X  T u , u =1 ,...,N ,intoa N  X  m parameter matrix  X , then we impose the constraint that this matrix has a rank k decomposition  X  = UV ,where the rows of N  X  k parameter matrix U correspond to new k  X  dimensional user parameters  X  u , u =1 ,...,N .
The estimation problem can be defined analogously to the RankSVM formulation. The main difference is that the transformation parameters V are estimated across users. More formally, we find U and V that subject to [ UVx j ] u  X  [ UVx k ] u +1  X   X  ujk for all j , k ,and user u such that e j  X  E + P ( T i ,u )and e k  X  E  X  P ( T past time frame T i .Here  X  2 F denotes the squared Frobe-nius norm of the matrix (sum of squared entries). From the point of view of each user, for a fixed transformation parameters V , the estimation problem can be solved with RankSVM as before. Similarly, for a fixed U ,wecouldes-timate V with RankSVM across users. Our implementation (details omitted) is slightly more efficient than this alternat-ing minimization approach by instead iteratively estimating rank 1 components of UV (one column of U ,onerowof V ).
We note that if event descriptions x j are merely binary indicator vectors for event identities, i.e., having exactly one non-zero component, then our collaborative approach reduces to the typical matrix factorization approach to col-laborative filtering with the exception that the error is mea-sured in terms of ranking constraints rather than mean squared rating error.
We conducted a user study to collect user preferences on a sequence of scientific talks. Ideally, we are interested in a Figure 1: User interface for collecting forced-choice seminar preferences. scenario where a localization system tracks a person X  X  loca-tion indoors [16]; if information about ongoing events (e.g., talks), including event time and room, is maintained, then this type of location-based system can automatically detect a person X  X  presence at events over time. In the current study, however, we elicited explicit feedback from users. We exam-ined a realistic scenario: scientific seminars are announced at MIT CSAIL as well as in other institutions via a dedicated email list on a weekly basis. Similarly, in our user study, participants were presented with a list of seminars known to have taken place during a single calendar week. As illus-trated in Figure 1, the list contained the talk titles, where the content of each announcement could be displayed by clicking on its title line. An example seminar announcement is displayed in Figure 2. The list of titles for a particular week was ordered randomly so as to remove any overt biases due to display order. The participants were requested to se-lect which of the seminars included in the list they would be interested in attending. In the online form, interest in a sem-inar was indicated by marking a checkbox next to its title, and the labels for the whole list were delivered by pressing a submit button. We required that at least one talk is se-lected as relevant. In cases where the users felt they were forced to make a choice, i.e., they would prefer not to attend any of the talks, they could indicate this with a checkbox presented at the top of the screen. However, we will assume that their selection still carries information about relative ranking of the alternatives. In order to mimic time flow, and to assure that the reference time frame is weekly, the study participants were not allowed to make changes to pre-viously submitted feedback.

We collected user feedback for two sets of 15 consecutive weeks of seminar announcements using this procedure. Both sets have been originally published on the CSAIL seminar email list, where the first week sequence starts at the first week of September 2007, and the second starts at the sixth week of year 2009. The announcements included all the talks published, where duplicate announcements, as well as any posts that do not correspond to a scientific seminar, were excluded. Thirty CSAIL participants completed the study using the first dataset of seminar announcements, including mostly graduate students and research associates. In addi-tion, 56 graduate students of the computer science school at Figure 2: An example email seminar announcement.
 We drew LDA topic models from seven years worth of similar announcements.
 Carnegie Mellon University completed the study, using the second sequence of seminar announcements. Therefore, in both cases, the participants belonged to the target audience of the seminar announcements.

Table 1 shows relevant weekly statistics about the sem-inar announcements presented and the corresponding user feedback. The frequency of talks varies widely throughout each 15 week period, ranging from 2 to 21 talks on a given week. The table also includes the average number of talks judged as relevant across users per week, and the number of derived preference pairs. Overall, 8.6% of weekly responses were marked as  X  X orced X  selections, where the user found no talk as relevant to their interests. Interestingly, participants often selected approximately two talks to attend, regardless of the number shown, mirroring typical real-life attendance.
Each seminar announcement can be viewed as a document (see Figure 2). There are many ways of turning documents into fixed-length feature vectors x j . Typically, each coordi-nate of the feature vector would correspond to a word oc-currence such as term-frequency inverse document frequency (TF-IDF) weighted word count [15, 19]. The intuition be-hind this approach is that topics that users are interested in may be associated with specific terminology and therefore particular coordinates of x j . We use TF-IDF weighted word counts as a baseline feature representation.

An alternative and potentially better feature representa-tion can be obtained by identifying topics from the seminar announcements. For example, users may decide whether to attend a seminar based on the degree of overlap between their research areas and the focus of the talk. Such inferences rely on topics rather than individual words. While identi-fying topics that reflect human judgement is difficult, topic distributions inferred by methods such as Latent Dirichlet Allocation (LDA) [5] may be sufficient for recommendation purposes. Table 1: User study statistics using two different sets of 15 consecutive weeks: number of talk announce-ments per week, average number and standard de-viation (in brackets) of talks considered relevant per week, and the corresponding number and standard deviation of derived preference pairs.

LDA is a generative model over documents. The key as-sumption in the model is that each document is composed of some subset of K possible topics. The words are generated in a manner that reflects the topic composition. Specifically, to generate a document d from this model, we first draw a sample that determines the topics used in the document. The resulting sample is a distribution over topics P ( z | z =1 ,...,n . Subsequently, for each word, we first draw a topic from P ( z | d ) that the word is associated with, and then the actual word from the topic-dependent model P ( w | z ). Conversely, given a document, and the model parameters, we can infer the overall frequency of each topic used in gen-erating the words in the document. We use these topic usage frequencies as the feature vector. In other words, each coor-dinate of x j now corresponds to a frequency that a particular topic was used in generating the seminar announcement.
Learning an LDA model involves estimating the parame-ters that govern the generation of topic compositions as well as the topic-dependent word distributions from a collection of documents. For this purpose, we employ a large corpus, which includes all MIT CSAIL seminar announcement from May 2002 to July 2009. Overall, this reference corpus in-cludes about 5,000 seminar announcements. We use Gibbs sampling method [10] for learning and inference in the LDA model. (The same corpus is used to derive word and inverse document frequencies for the word-based representation de-scribed above.)
In addition to the information contained in the seminar announcements, it is possible to consider other relevant in-formation sources. For example, we could search for ab-stracts of previous publications by the seminar speaker .Since the seminar description is limited, the summaries of related publications or talk summaries may better narrow down the topics covered in the talk. Moreover, related documents would also help in case users X  interests stem partly from the speaker X  X  expertise or background. Once inferred from rele-vant abstracts or documents, speaker features can be simply appended to the feature vector.

In general, there are various other representation schemes that could be considered. For example, semi-structured meta-data, specifying details such as the speaker X  X  name and affiliation, or host details (see Figure 2), could be modeled explicitly. One could also model the user X  X  areas of interest, affiliation or social group. In this paper, however, we per-form limited feature exploration as our focus is on applying and evaluating the effect of collaborative recommendation. Others may wish to extend the model through these or other methods with their own examination of the talk and user feedback dataset.
As mentioned above, the user study was designed to mir-ror a realistic setting, where user responses to ongoing events, whether collected explicitly or implicitly by sensors, is accu-mulated over time. There are several questions that we are interested in addressing in our evaluation of the user study:
In the experiments, we divide the data collected into train and a test sets. The train set includes the first 10 weeks for which we obtained user feedback (in both user surveys). This labeled data is used as input to the prediction models, where we simulate varying lengths of the learning period by considering increasing number of weeks for which user feedback is available. The testing set includes the seminar announcements for weeks 11-15 (in both surveys), represent-ing future events. In testing, we apply the models learned to generate a ranked list for every user and for each week in the test set. The results are then evaluated against the user feedback collected. Tuning of the learning methods X  parameters is performed using the train set.

All the methods applied generate a ranked list of event entities. We evaluate performance in terms of Mean Aver-age Precision (MAP), which is a widely accepted evaluation measure in Information Retrieval [14]. To define MAP, we Figure 3: A comparison of RankSVM and LowRank with increasing training data. MAP is averaged across all users for all testing weeks. first define the precision at rank k , prec ( k ), to be the num-ber of correct entries up to rank k , divided by k  X  X .e., the precision of the list up to rank k .The non-interpolated av-erage precision of the ranking is the average of prec ( k )for each position k i that holds a correct entry: For example, consider a ranked list of items, where the items at ranks 1,2,5 are known to be correct answers, and those at ranks 3,4 are not; the non-interpolated average precision of this ranked list is (1 + 1 + 0 . 6) / 3=0 . 87. The Mean Aver-age Precision (MAP) is the average of the non-interpolated precision scores, over multiple rankings (i.e., over multiple queries).

The MAP measure is strongly correlated with query diffi-culty, reflected by the length of the ranked list and the size of the correct item set. For example, a list of two items overall, including one correct item, will have a MAP of 0.5 in the worst case; ranking a list of N items with one correct item, however, may reach a lower MAP of 1 /N , etc. For this reason, we evaluate the various methods in terms of MAP on the same set of test queries.

In our knowledge, this work is the first to present and evaluate event prediction via a user study. 2
We first evaluate the performance of learning individual models per user using RankSVM , considering several event representation schemes. Specifically, an event is represented as a topic distribution, where the number of topics K is set to 100. 3 An alternative representation considered is a TF-IDF weighted word vector. In our experiments, word in-verse document frequency (IDF) counts were obtained from
The underlying dataset is available on the first author X  X  homepage.
In tuning experiments, we explored setting the number of latent topics to K =10 , 25 , 50 , 100 , 200. While K =10led to inferior prediction results, the other value selections were found to be comparable. Figure 4: Performance of LowRank with increasing training data using two sets of parameter choices. MAP is averaged across all users for all testing weeks.
 Figure 5: LowRank with three weeks of training as compared to random seminar recommendations.
 Data shows Mean Average Precision for the full five testing weeks and individual users. the full corpus of seminar announcements available (Section 4). We apply simple feature selection, where words that ap-pear fewer than three times in the corpus are eliminated. Overall, this results in roughly 10,000 word features. Tong and Koller [19] have previously found a similar word-based representation to be effective for text classification tasks.
One of the goals of our evaluation is to gauge the impact of train set size on performance. We therefore conduct a set of experiments, where an increasing number of weeks (out of the train set available to us) are used for training. Specifically, we train recommendation models based on n weeks worth of data per user, where n ranges from one week of labeled feedback per user, to the full ten weeks worth of feedback per user available in the full train set.
In a real recommendation system, the set of users is dy-namic; in particular, users join the recommendation service at different points in time. It is also possible that only a sub-set of the existing users provide feedback during any given period (a user may be inactive. e.g., away). In the exper-iments, we therefore allow the known feedback per user to vary in time: if it is assumed that n&lt; 10 weeks worth of Figure 6: Each pair in the histogram shows a user X  X  MAP for using LowRank with both TF-IDF and LDA feature sets. The data show a strong correla-tion between predictability using either feature set. feedback are available per user, then n weeks are selected randomly for each user out of the ten weeks included in the train set. For every value of n , we train recommendation models based on a subset of the training set constructed in the described fashion. In order to eliminate sampling bias, every experiment is repeated 10 times overall. The learned models are then evaluated using the fixed test set, where we report average performance across the 10 trials.

Figure 3 shows global MAP performance over the whole test set, averaged across all users and across trials, for using TFIDF and topic feature vectors. The figure displays the results for increasing volume of feedback, starting at one week X  X  worth of user feedback and up to considering feed-back for the full ten weeks available. As a naive baseline, the figure includes the results of random ordering of the talks in the test set.

There are several trends observed in the results. First, learning is effective, as it is consistently yields results su-perior to random ordering. As one may expect, learning performance improves over time, as more user feedback on past events becomes available. However, the improvement ratio is relatively modest. We conjecture that new addi-tional topics are introduced in the seminars included in the test set, where this limits the performance of the content-based models that are learned based on past experience. With respect to the feature scheme, we find that applying RankSVM using the topic and TFIDF representation vectors yields comparable performance. Elsewhere, small improve-ments have been obtained using topic features on the task of text classification [2].
Figure 3 shows the results of applying the LowRank col-laborative approximation method for increasing volume of training data, using TFIDF and topic feature representa-tions. We fixed the regularization term to C =2,andthe rank of the parameter matrix V to k = 12. As shown, LowRank using TFIDF features yields superior performance across the full range of inspected train set size. In addi-tion, the learning curve in this case is steeper, where perfor-mance given three weeks of training data exceeds the best performance observed with the RankSVM models using eight weeks worth of training data. A shorter learning time trans-lates to an improved user experience, and overall better sys-tem utility, as new users join the system over time.
Applying LowRank using the topic features gives compa-rable performance to RankSVM . LowRank gives preferable results to RankSVM if TFIDF vectors are used. Since the TFIDF representation is much more sparse, LowRank has more direct control in this case over the useful subspace to use.

We found that a matrix rank of at least k = 8 is needed to reach good performance in the experiments. A small vari-ance was observed for higher values of k .Figure4shows the performance curve of LowRank setting the rank to k = 10 , 12 and the regularization term to C =1 , 2. As shown, the sensitivity to parameter values in this range is small.
While results have been discussed so far in a global fash-ion, a question of interest is what variance can be expected among individual users X  performance. In the experiments, we found this variance to be relatively large. Figure 5 presents the results of RankSVM using topic and TFIDF feature sets and (10 samples of) three weeks worth of user feedback for training. The figure shows the cumulative rate of MAP per-formance for an individual user for each level of MAP per-formance observed. According to the figure, for about 50% of the users, average MAP across runs was 0.55 or higher using LowRank with TFIDF features. If topic features are used, average MAP performance for about 50% of the users is 0.47 or more. Random recommendation yields lower MAP of 0.35 and above for about 50% of users. As mentioned ear-lier, MAP can vary between users due to variance of the rele-vant item set size (Table 1); in general, accuracy is expected to be higher for users who are interested in a larger number of talks. Similarly, differences in performance between users may correspond to the extent to which the user is interested in general areas, versus specialized sub-topics. In the lat-ter case, data scarcity is expected to be more pronounced. Predictions for a user who has wide interest areas are also likely to include more relevant items on average than for a user whose interest areas are narrow.
 Finally, Figure 6 gives another view of the results using LowRank with topic and TFIDF features on the full test set, for individual users. In addition to detailing perfor-mance level per user, this figure shows a strong correlation in individual user performance between the models trained with the different feature sets.
There are two problems we hope to remedy by actively eliciting feedback from users. The first problem is general data sparsity. In a working system, we expect to highlight only about 5% of all the announcements that the user re-ceives (say, in the course of one month). The goal is therefore to properly rank the top 5% amongst themselves as well as separate this set from the remaining announcements. How-ever, realistically, a single user is unlikely to provide explicit feedback for more than a few weeks worth of announcements. As a result, many of their top choices may not appear in the training data. In order to avoid this problem, it seems nec-essary to quickly focus the announcements presented to the user to be mostly in the top 5% of their ranking. For ex-ample, based on initial information from the user, an active selection approach could present to the user some predicted top 5% announcements from the overall pool of announce-ments, some 5-10% percentile announcements so as to learn to separate them from the top, and some randomly chosen announcements (or according to the normal weekly sched-ule) in order to avoid missing major areas. The quality of these predicted rankings, and therefore queries themselves, would improve with the user feedback.

The second and related issue is coverage across areas of interests. In part, this is achieved by adopting the collabora-tive formulation described earlier. We only need to identify user interests by selecting a parameter vector (direction) in the low dimensional (shared) perceptual coordinate space. The perceptual feature coordinates can be thought as repre-senting weighted subsets of topics and therefore resist spec-ifying user interests too narrowly.

Consider a fixed mapping V to the lower dimensional per-ceptual space. This mapping, shared across users, can be estimated robustly based on limited initial feedback from users. For each user u , we represent the uncertainty in their parameters  X  u with a distribution P (  X  u ), a discrete distribu-tion over a set of alternative parameters. This distribution is updated based on user feedback. We assume that the user responds to a pair of announcements in a manner that re-flects some  X  u but also allowing flipping noise. The score for a candidate pair ( x i ,x j ) is evaluated as follows. Let y ij  X  X  X  1 , 1 } represent the user response if they are pre-sented with ( x i ,x j ). We model P ( y ij | x i ,x j ;  X  P ( y ij =1 | x i ,x j ;  X  u )= (1 where is a small flipping probability. The expected re-sponse, P ( y ij | x i ,x j ), is obtained by averaging relative to the current belief about  X  u , i.e., with respect to P (  X  score for a candidate pair of announcements to be presented to the user for feedback is then the expected information gain: the entropy of the parameter choices prior to feed-back,  X  P (  X  u )log P (  X  u ) d X  u , minus the expected entropy after the feedback  X  The integrals are tractable since, by assumption, P (  X  u mass only on a set of discrete alternatives over the k  X  di-mensional unit ball (predictions are invariant to scale). The active learning method successively selects pairs with the highest information gain, updating P (  X  u ) based on each re-sponse.

In order to evaluate the active learning approach in an ac-curate fashion, we would like to conduct another user study, where users are asked to provide pairwise feedback for pref-erence pairs, selected based on their previous feedback, in an online procedure. So far, we have conducted preliminary experiments in a simulated mode, based on feedback already acquired. As before, we used the first 10 weeks of seminar announcements as possible data for training. The active learning method was initialized with 2-4 weeks of data us-ing the collaborative approach. The purpose of this step is to identify the initial shared coordinate dimensions. Subse-quently, we selected pairs of announcements from the train-ing data, separately for each user. The selections are re-stricted to pairs that are informative (user selected one but not the other). The selection of pairs was based on reducing uncertainty about the k  X  dimensional user parameters  X  u
We do not report full results in this paper. As a single example, we found that if the data of the first three weeks is considered, then three active learning pairs give better performance than training on full data individually in some cases. These are encouraging results, and we intend to in-vestigate them further.
The main focus of this work is on the general problem of event recommendation. The user study conducted, how-ever, concerns the specific application of event recommen-dation to scientific talks. Previously, researchers have con-sidered a related task of automatically recommending sci-entific papers to reviewing committee members. In most works, this problem has been approached using a content-based methods [3, 9, 20]. Dumais and Nielsen [9] compute paper-reviewer similarity, based on paper abstracts and ti-tles on one hand and relevant abstracts supplied by the re-viewers on the other hand, using latent semantic indexing (LSI). Their results are up to 40% better than a random baseline in terms of accuracy at rank 10. Interestingly, they indicate that reviewers are unable to judge their own in-terests with perfect consistency; also, the performance of the automated system was found similar to that of human judges. Basu et-al [3] evaluate the contribution of various information sources to the task of recommending papers to reviewers. They use WHIRL, a query language that accom-modates similarity metrics. In addition, they experiment with collaborative filtering, where feedback is fed by review-ers in an online fashion, and recommendations are generated based on the feedback stream using methods like KNN. Ac-cording to their results, content-based approaches yield bet-ter results than pure collaborative filtering on this task. An-other work [7] considers collaborative filtering given reviewer  X  X ids X , expressing interest or disinterest of reviewers in spe-cific papers, as available feedback. While these previous works apply collaborative filtering in  X  X raditional X  settings, based on common pools of items and users, we study a dif-ferent problem, where the sets of known items and the items for which recommendation takes place are distinct.
The problem of collaborative filtering for event recommen-dation has not received much attention, in our knowledge. A recent work proposes a hybrid content and collaborative filtering approach for event recommendation, within a fuzzy relational framework [8]. Their rationale is similar to ours, where the underlying goal is recommending future events if they are similar to past events that similar users have liked. The approach proposed is not evaluated empirically. In con-trast, in this paper we extend a more popular recommenda-tion framework to fit the settings of event recommendation; we also present a relevant user study, and make it available for the research community.

The LowRank method suggested in this paper follows a well-established matrix factorization framework for collabo-rative recommendation (see, e.g., [11,13,17]). In particular, our approach can be seen as an extension of [17] to event recommendation where, in contrast to the more common user-item rating scenario, we seek to rank future events on the basis of their word content. As a result, our problem formulation is closely related to the low-rank document re-trieval method of [1]. In order to rank documents based on their similarity to a query, the authors estimate a rank-ing function over pairs of documents based on word counts. The word-to-word parameter matrix in the ranking function is assumed to be low rank. In our case, the users are rep-resented only by their identities (user id is used as a query) while the events to be ranked are viewed as documents sim-ilarly to [1]. A number of other methods are also available for exploiting feature descriptions of items in a collaborative setting (see, e.g., [6]).
We discussed the problem of recommending items for which no previous feedback exists, focusing on the problem of fu-ture event recommendation. We introduced a low rank col-laborative approach in this setting. A user study was con-ducted to simulate a recommendation of scientific seminars on a weekly basis. Our empirical results based on this user study show that the proposed collaborative method outper-forms content-based recommendation on this problem. Since the collaborative method approach uses explicit feedback, we proposed an active learning extension to the approach that is aimed at reducing the amount of explicit feedback required from the user. We plan to evaluate this method using a dedicated user study in the future. In addition, we believe that as outdoor and indoor localization systems evolve, it will become possible to track users X  interests in items such as events directly by detecting their attendance at known events.
This research is funded by Nokia Research. [1] B. Bai, J. Weston, D. Grangier, R. Collobert, [2] S. Banerjee. Improving Text Classification Accuracy [3] C. Basu, H. Hirsh, W. W. Cohen, and [4] J. Bennett and S. Lanning. The Netflix Prize. In KDD [5] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [6] W. Chu and S.-T. Park. Personalized recommendation [7] D. Conry, Y. Koren, and N. Ramakrishnan.
 [8] C. Cornelis, X. Guo, J. Lu, and G. Zhang. A fuzzy [9] S. T. Dumais and J. Nielsen. Automating the [10] T. L. Griffiths and M. Steyvers. Finding scientific [11] T. Hofmann. Latent Semantic Models for [12] T. Joachims. Optimizing search engines using [13] Y. Koren, R. Bell, and C. Volinsky. Matrix [14] C. D. Manning, P. Raghavan, and H. Schutze.
 [15] R. J. Mooney and L. Roy. Content-based book [16] J. Park, B. Charrow, D. Curtis, J. Battat, E. Minkov, [17] J. Rennie and N. Srebro. Fast maximum margin [18] G. Strang. Linear Algebra and its Applications . [19] S. Tong and D. Koller. Support vector machine active [20] D. Yarowsky and R. Florian. Taking the load off the
