 Event originates from cognitive science, which is the basic knowledge unit for human to know the real world [1] . Zongtian Liu [2] thinks event elements are objective except for language expression. Language expression is created by human and is the customary feature, which is given to event class by human. It can change with time, but not arbi-trary. It is the product of human evolution in the long run and has its relatively con-firmed law. For the specific event classes, human described them and communicated in their naturally agreed form. The language expression changed with the description lan-guage. For the same event, there are different language expressions in different lan-guages, even there are different language expressions in the same language.
 In the text or speech, the main contents are the description about the occurrence of event. It is essential for human to express and know these contents. In order to make computers, like human, have abilities to master languages, it is the basic for them to establish a clear mapping between natural languages and event classes. The language expres sion pattern of event class is the description form of the mapping.
 The news texts in the Internet can be seen as consisting of a number of events in an orderly manner, in which each event is accompanied by semantic information such as time, environment, a ction and object etc. The language expression pattern of the event class is implicit in the texts, because people are consciously or unconsciously using them when writing articles. In the process of reading, people are constantly using and discovering thes e language expression patterns. Language expression is the language knowledge describing event. In the process of detecting event in text and describing event in speech, it will play a guiding and determining role. In the process of automat-ically writing a n article, it will play a role in guiding to produce the sentence and para-graph.
 In this paper, firstly, we gave related work about event language expression (section 2), and then, we gave the concept of event and event class ( section 3). Thirdly, we mined language expression pattern by semantic dependency parsing and frequent subtree (sec-tion 4).Finally, we detected event element by the mined language expression patterns in order to verify their validity (section 5). The research on event main ly focuses on the event detection and extraction and event relation detection and extraction. The former mainly concentrates on the MUC [3] , TDT [4] and ACE [5] , and transform event detection and extraction to classification is-HMM [10] ,CRFs [11] , ME [12, 13] and SVM [14] . The latter is just beginning, the mainly work focus on detecting and extracting specific e vent relation such as temporal relation and causal relation between events [15 -18] . The above research works are mainly word -based and feature -based. The resear ch work is less from the aspect of event language expression.
 Zongtian Liu [2] propose d a 6 -tuple event model in 2009. H e thinks the main contents of event language expression are core words, which are the appellati on about event and event objects , the description style of time and environment and the description style of collocation among event elements. He thinks each event class has its own specific language expression law, the inclusion re lation of the event class language expression is consistent with the objective inclusion relation, because the cognitive process of them is the accompanying process.
 In grammar research, the main representation is Chomsky's grammar rules [19, 20] . They argue that natural language needs to be represented in context -related grammars, and that the complexity of context -related gra mmars makes representation difficult. Because of the complexity of natural language, it is difficult and even unrealistic to find a simple and unified grammar rule to represent the description language of various events. Moreover, because of the flexibilit y of natural language, it is a vast project to write these rules entirely by hand.
 Huanjian Meng [21] proposed dependency syntax parsing -based frequent subtree method to detect event and event elements from Chinese news texts. The method pro-duced dependency parsing tr ee for each sentence describing the same event class, and produced frequent subtrees by combining Part -of -speech. Dependency syntax parsing focuses on the subject -predicate -object and some modifiers of sentence from the aspect of syntax . The method needs s ome rules to detect events and their elements. Definition 1 event [2] : We define event as a thing occurring in a certain time and envi-ronment, which some actors take par t in and show some action features. Event e can be defined with a 6 -tuple formally as formula (1) We call elements in 6 -tuple as event elements. Where A denotes an action or a set of actions in an e vent. O denotes objects involved in the event, including all participants and entities involved in the event. We name participant as actor and define two types of actors, action initiators and action recipients. T denotes time in the event. V denotes the l ocation of an event; P denotes assertion, which describes object statuses during an event occurring, including pre -condition set, mid -condition set and post -condition set. occurring. Mid -condition describes a set of object statuses when the event happening. Post -condition is a result set of object statuses after event occurring; L denotes language expressions of text -based event, it includes a Core Words Expressions (CWE) set and a Core Words Collocations (CWC) set [2] .
 Definition 2 Event class is an abstract event that represents a set of events with some common features, denoted as EC in the formula (2): C = {  X   X  1 ,  X   X  2 ,  X  ,  X   X  X  X  ,  X  } ( i  X  { A , O , T , V , P , L } , m  X  0 ) ( 2 ) Where E denotes an event set. C  X  is the set of event elements. It denotes the common features set of certain event element (element i).  X   X  X  X  denotes one of the common fea-tures of event emement i.  X   X  X  X  is also called event elements class. We did semantic dependency parsing for the sentences in CEC 2.0, and then, trans-formed the dependency semantic graphs to dependency semantic trees, finally, mined frequent subtrees and got event language expression patterns . The flow chart of mining event la nguage expression pattern is shown in Figure 1.
 4.1 CEC 2.0 CEC 2.0 is the Chinese Event Corpus, which is produced in XML language by Zongtian Liu groups of Shanghai University. It includes fire, earthquake, traffic accident, terror-ist attack and food poisoning five topics of Chinese news texts, where each text is an-notated events, event elements, event relationships and the missing e lements. We got the event class by Cilin -based clustering method proposed by Liaotao [24] . The number of annotated event elements and the gotten event classes in CEC 2.0 is shown in table 1 4.2 Semantic Dependency Parsing Semantic Dependency Parsing (SDP) is to parse semantic associations between the var-ious language units of the sentence, and the semantic association is presented in a de-pendency graph from. It is not affected by syntax, connect language units that have direct semantic association with dependent arc and mark the corresponding semantic dependency relationship. The semantic dependency relationship includes core word and dependent word. It indicates the dependency relationship between them . Semantic dependency relationsh ip is directed, and indicate core word dominate dependent word through some semantic relationship, which is the important different from dependency syntax parsing. We use LTP [2 2] to do semantic dependency parsing. 4.3 Transform Semantic Depen dency Graph to Tree Semantic dependency frequent subtree is tree structure with a high -frequency, which indicates the determinate collocation between the core word and dependent word. When an event class is mentioned, the determinate collocation that describe the event class is Semantic dependency frequent subtrees are essentially an event language expression pattern. In the semantic dependency graph, the semantic attached relationship marks some attached information, which has no effect on mining frequent subtrees. Therefore, we don X  X  consider these kinds of information. In this paper, we treated the Part -of -trigger word and the others. For example,  X + X  denotes the word appears before the trig-ger word in sentence,  X  - X  denotes the word appears after the trigger word. The trigger word is marked by D.
 The transform procedure is as follows:  X  Step1: Delete the semantic attached relationship and the leaf nodes of its connection in the semantic dependency graph  X  Step2: Words are replaced by their corresponding Parts of Speech feature in seman-tic dependency graph, and the trigger word is replaced by D.  X  Step3: Mark the position relationship between the trigger word and the others.  X  Step4: Treat the node that is directed by Root in the semantic dependency graph as the root of the new tree.  X  St ep5: Traverse the semantic dependency graph by depth first and add the traversed nodes and semantic relationships to the corresponding nodes in the new tree. 4.4 Mine f requent subtree Compared to sequences and sets, trees can better describe the interrelations hips be-tween data. Moreover, people are usually interested in some subtrees that frequently appear, which are called frequent subtrees. There are TreeMiner [25] , ESPM [26] and PETreeMiner [23] to mining frequent subtrees from the trees that have root and are or-dered. The TreeMiner [25] algorithm uses the Apriori [27] generation -test method, first generates the candidate subtrees and then tests the number of support trees to mine the f requent subtrees from the ordered trees. The number of candidate subtrees produced by this method is relatively more, and it takes more time to calculate the frequency of these candidate subtrees. ESPM algorithm doesn X  X  consider the depth information in th e process of preorder traversing, so that the trees that have different structures share the same preorder sequence, when these preorder sequence is frequent, and then further determine whether each independent tree is frequent. The ESPM algorithm reduces the generation of candidate subtrees, but the mined frequent sequences correspond to trees with different structures, which can consume a lot of time in the transformation process from frequent sequence to different tree. The PETreeMiner algorithm uses the prefix projection to check partial frequent patterns, and does not generate candidate se-quences. It adds the range information of each node to the preorder sequence of the range information of the nodes to encode the nodes in the process of projection, so that the mined sequences only correspond to one frequent subtree.
 Here we take the  X  X ollision  X  X vent class as an example to introduce the mining process of eve nt language expression pattern. Example2: Yesterday, a truck collided with a car on the outer ring road.
 Yesterday, two trucks collided with each other.
 The Part of Speech feature and semantic dependency graphs of the two sentences in example 2 are shown in figure 2 and figure 3. T he meaning of semantic dependency markers in figure 2 and figure 3 are shown in table 2 . T he semantic dependency graphs in figure 2 and figure 3 are transformed to trees, which are shown in figure 4 and figure 5 respectively. I n figure 4 , Qp denotes the quantity -phrase of dependency relationship, Quan denotes the quantity of the dependency relationship, they play a quantitative role in the number on their parent node. Here we use N + node instead of the three nodes that are connected by Qp an d Quan, the N+ node does not affect the original semantic dependency rela-tionship between node n + and node v + . The processed tree is shown in figure 6 . The process of mining frequent subtrees using the PETreeMiner [11] algorithm is as follows:  X  Step1: The preorder sequences of the trees in figure 5 and figure 6 are T0 and T1 respectively.
 T0  X  v+[ 0,3 ]  X  nt +[ 1,1 ]  X  N+[ 2,2 ]  X  D[ 3,3 ] T 1  X  v+[ 0,5 ]  X  nt +[ 1,1 ]  X  n +[ 2,2 ]  X  N+[ 3,3 ]  X  N+[ 4,4 ]  X  D[ 5,5 ] W e get 1 -item frequent notes and encode them, they are v+0  X  nt+0  X  N+0 a nd D0 .  X  Step2: for the v+0 node, there is only one v+ node in T0 and T1, and the encoded sequence database is D1: T0  X  v+[ 0,3 ] 0  X  nt +[ 1,1 ] 1  X  N+[ 2,2 ] 1  X  D[ 3,3 ] 1 T 1  X  v+[ 0,5 ] 0  X  nt +[ 1,1 ] 1  X  n +[ 2,2 ] 1  X  N+[ 3,3 ] 1  X  N+[ 4,4 ] 1  X  D[ 5,5 ] 1 The T0 and T1 are scanned from the second node respectively, and the frequent nodes are nt + 1, N + 1 and D1.  X  Step3: treat v + 0 and nt + 1 as the prefix sequence, project and encode for D1 and the proje cted database is D2: T0  X  _ N+[ 2,2 ] 10  X  D[ 3,3 ] 10 T 1  X  _ n +[ 2,2 ] 10  X  N+[ 3,3 ] 10  X  N+[ 4,4 ] 10  X  D[ 5,5 ] 10 T he frequent nodes are N+10 and D10 .  X  Step 4: treat v + 0, nt + 1 and N+10 as the prefix sequence, project and encode for D2 and the projected database is D3.
 T0  X  _ D[ 3,3 ] 100 T 1  X  _N+[ 4,4 ] 100  X  D[ 5,5 ] 100 T 1  X  _ D[ 5,5 ] 100 Because there are two N+ nodes in T1, we have to project the two N + nodes respec-tively. The frequent node is D100.  X  Step 5: treat v + 0, nt + 1, N+10 and D100 as the prefix sequence, continue to projec t and encod e .
 There is no new frequent node. Therefore, v + 0, nt + 1, N+10 and D100 is a frequent sequence. Return and treat v+ 0 and N+1 as prefix sequence, and there is frequent se-quence: v+ 0,N+1 and D10. Similarly, treat v+ 0 and D1 as prefix sequence an d there is frequent sequence: v+ 0 and D1. Treat nt+0  X  N+0 and D0 as root respectively, and the frequent subtree that includes D node is D0. The mined subtrees that include D node are shown as T1,T2 , T3 and T4 in figure 7 . I n figure 7 , we can find that T 2 , T3 , and T4 are the subtree of T1 respectively. And T3 is the subtree of T2, T4 is the subtree of T3. T he event language expression pattern, on the one hand, indicates the habits that human describe the events occurring in the real world in natural language, on the other hand, it can be also used to detect event semantic information in texts. Therefore, in this sec-tion, in order to verify the validity of the mined language expression patterns, we detect event elements in texts by using them. We get the Chinese news texts from the Internet. The topic of texts includes earthquake, fire, traffic accidents, terr orist attacks and food poisoning. The number of texts on each topic is 100. And we evaluate the results with Precision rate, Recall rate and the value of F1 -measure.
 For a sentence that describes an event in texts, firstly, we did semantic dependency parsi ng for it by LTP, and then, transform the semantic dependency graph to tree, thirdly, compare it with the minded frequent subtrees. If there is a mined frequent sub-tree with the same structure as the semantic tree, then the semantic information in the sent ence can be detected. For the case in figure 6, the T2 is the subtree of T1 , T1 is selected.
 The following describes the process of detecting event elements by the mined frequent subtrees.
 Example 3: The two cars collided The Part of Speech feature and semantic dependency graph of the sentence in example 3 is shown in figure 8 , and the transformed tree is shown in figure 9 . The transformed tree T0 in Figure 9 is compared with the mined frequent subtrees in Figure 6. It can be found that T0 and T2 have the same structure. Additionally, they are the subtrees of T1. It can be found that v1 corresponds to D and N+ corresponds to the object of event. Therefore,  X  X o llide  X  X s the action of the event, and the two cars are the objects of the event.
 We determine the minimum confidence is 0.8 and the minimum support is 0.6 through a large number of experiments. Similarly, we detect event elements in the sentence that desc ribe  X  X ollide X  event by using the mined frequent subtrees. The minimum confi-dence is fixed at 0.8, and the minimum support changed from 0.6 to 0.9 with 0.1 for each step. The results are shown in table 3.
 I t can be found that with the increase of minimum support, the number of mined fre-quent subtree s decreases gradually, and the precision rate P increases gradually, and the recall rate R decreases gradually.
 The reason is that in the process of mining frequent subtree, some sentence that are not frequent were removed with the increase in minimum support, which also the reason that the precision rate gradually increases, the recall rate gradually decreases. 
In general, although the minimum value of F1 -measure is 77.5% . There are 319 event sentences describing the "collision" event in the CEC 2.0, which includes most of language expression pattern of the "c ollision" event class . And it also indicates that the m ined frequent subtrees, to a certain extent, are valid, they can represent most of the language expression patterns for this kind of event class.

The number of sentences that describe the same event class has an effect on the detection of even elements. We still used the "collision" event class as an example. We mined frequent subtrees from CEC 2.0 with different numbers of sentences , and then detect event elements in testing data by the mined frequent subtrees. The minimum shown in table 4 .
 I t can be found that when the minimum support is 0.6, with the increase of the number of sentences, the number of the mined frequent subtrees and the precision rate are in-creasing. These indicate that these sentences that describe the same event class contain different language expression pattern. Therefore, the frequent subtrees that are mined from a large number of sentences are valid, which can objectively represent the lan-guage expression patterns.
 78.6%. This is related to the precision of semantic dependency parsing. When we minded frequent subtree and detec ted event elements, we did semantic dependency parsing for the sentences by LTP . When the sentence is long, the semantic dependency parsing is wrong, which leads to the low precision rate of the results.

On the basis dependency syntax parsing, Huanjian Meng [12] got dependency synt ax frequent subtrees from dependency syntax trees and used them to detect event elements by combining with rules. We compared our method with his. The minimum confidence is fixed at 0.8, and the minimum support is fixed at 0.6. The results are shown in tab le 5 .
 T he value of F1 -measure is 76.4% and 64.6% respectively, which indicate that our method is superior to his. The reason is that the mined frequent subtrees are semantic dependency trees by our method, but the mined frequent subtrees are syntax depend-ency trees by his method. When we detect event elements by using these mined frequent subtrees, the former is that the transformation tree matches the semantic dependency frequent subtrees directly without rules, the latter is rule -based. On the other hand, we mined the frequent subtrees from all the sentences in CEC 2.0. He mined the frequent subtrees from part of the sentences in CEC 2.0. The results of them are also influenced by the LTP. E vent language expression indicate the habits that when human communicate with each other, they describe the event occurring in the real world in natural language. For the specific event classes, human described them and communicated in their naturally agreed fo rm. The language expression patterns of the event class are implicit in the texts. We represented event language expression pattern through frequent subtrees and mined them for each event class from CEC 2.0. For the sentences that describe the same event c lass in CEC 2.0, we did semantic dependency parsing for each sentence and got the semantic dependency graphs, then transform them to semantic dependency trees, and mined frequent subtrees from these semantic dependency trees by the PETreeMiner algorithm. I n order to verify the validity of the mined frequent subtrees, we detected event elements in texts by using them. Additionally, we compared our method with another one. The value of F1 -measure is 76.4%, which indicates the mined frequent subtrees are valid . On the other hand, this indicate the mined event language expression patterns are also valid. We had mined event language expression association rules before this work. We will carry out research on detecting event and event elements and writing sentenc es auto-matically by event language expression association rules and patterns .
 The authors would like to thank the reviewers for their useful comments and sugges-tions for this paper. This work is supported by National Natural Science Foundat ion of China (61672006, 61273328, and 61305053).
 The authors claim that no conflict of interest exists in the submission of this manuscript, and the manuscript is approved by all co -authors for publication. None of the material in the paper has been published or is under consideration for publication elsewhere.
