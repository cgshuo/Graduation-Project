 ORIGINAL PAPER R. Jayadevan  X  S. R. Kolhe  X  P. M. Patil  X  U. Pal Abstract Bank cheques (checks) are still widely used all over the world for financial transactions. Huge volumes of handwritten bank cheques are processed manually every day in developing countries. In such a manual verification, user written information including date, signature, legal and cour-tesy amounts present on each cheque has to be visually veri-fied. As many countries use cheque truncation systems (CTS) nowadays, much time, effort and money can be saved if this entire process of recognition, verification and data entry is done automatically using images of cheques. An attempt is made in this paper to present the state of the art in automatic processing of handwritten cheque images. It discusses the important results reported so far in preprocessing, extraction, recognition and verification of handwritten fields on bank cheques and highlights the positive directions of research till date. The paper has a comprehensive bibliography of many references as a support for researchers working in the field of automatic bank cheque processing. The paper also con-tains some information about the products available in the market for automatic cheque processing. To the best of our knowledge, there is no survey in the area of automatic cheque processing, and there is a need of such a survey to know the state of the art.
 Keywords Bank check processing  X  Courtesy amount recognition  X  Legal amount recognition  X  Date recognition  X  Signature verification Abbreviations ANN Artificial neural network BC Bayesian classifier BN Bayesian network BPNN Back-propagation neural networks CM Co-occurrence matrix CTS Cheque truncation system DBC Differential box counting DFA Deterministic finite automation DTW Dynamic time warping ED Euclidean distance EDF Extended drop fall EER Equal error rate FAR False acceptance rate FFNN Feed-forward neural network FKNN Fuzzy K-nearest neighbour FNN Fuzzy neural network FPS Fixed point-spread FRR False rejection rate GB Global baseline GLS Grey-level space GRNN Generalized regression neural network GSC Gradient, structural and concavity HDF Hybrid drop fall HDS Hit and deflect strategy HMM Hidden Markov models HMRF Hidden Markov random field HNN Hopfield neural nets HNNC Hierarchical neural network classifier HT Hough transform ICS Image-based clearing system IQA Image quality assurance IRD Image replacement document KNN K-nearest neighbour LBP Local binary pattern LGSD Local granulometric size distributions LS-SVM Least squares support vector machines MBR Minimum bounding rectangle MD Mahalanobis distance MDC Minimum distance classifier ME Multi expert MICR Magnetic ink character recognition ML Maximum likelihood MLP multi-layer perceptron MQDF Modified quadratic discriminant function MM Mathematical morphology MMI Maximum mutual information MPR Most probable region MRS Multi resolution shape MSFC Multiple structural feature classifier MSI Model Sub-Image
MVBC Majority vote method based on Borda NN Neural network NNC Nearest neighbour classifier OCR Optical character recognition OGMM Orthogonal Gaussian mixture model PCAC Principal component analysis classifier PCC Pseudo-cepstral coefficients PF Pressure features PGM Probabilistic graphical model PNV Payee Name Verification RBF Radial basis function RBFNN Radial basis function neural network ROC Receiver operating characteristic RPBF Reference pattern based features RS Random subspaces SB Structural-based SC Symbolic classifier SDT Syntax directed translation SF Slant features SLFFNN Simple-layer feed-forward neural network SOM Self-organizing map SSE Sum-of-squared error SVM Support vector machines TB Template based TS Takagi X  X ugeno TDNN Time delay neural network
TSI Target sub image 1 Introduction Machine simulation of human reading has become a promis-ing area of research after the arrival of digital computers. The main reason for that is not only the challenge in simulating the human reading but also its utility in developing document processing systems capable of transferring data present on documents like bank cheques, commercial forms, govern-ment records and envelops into machine readable format. Paper cheques still play a big role in the non-cash transac-tions in the world even after the arrival of credit cards, debit cards and other electronic means of payment [ 1 ]. In many developing countries, the present cheque processing proce-dure requires a bank employee to read and manually enter the information present on a cheque (or its image) and also verify the entries like signature and date. As a large number of cheques have to be processed every day in a bank, an auto-matic reading system can save much of the work. Even with thesuccessachievedincharacterrecognitionoverthelastfew decades, the recognition of handwritten information and the verificationofsignaturespresentonbankchequesstillremain a challenging problem in document image analysis [ 2  X  4 ].
To save time and processing costs in clearing the cheques and to offer better customer services, many countries around the world have implemented cheque truncation systems (CTS) [or image-based clearing system (ICS)]. Instead of sending a physical cheque for clearance, the presenting bank captures the image of the paper cheque using suitable hard-ware and software. The image then will go through various clearing steps, and the transaction will be settled based on the image data. The cheque images can be black and white, grey scaleorcoloured.Blackandwhiteimagesdonotrevealallthe subtle features that are there on the cheques. Colour images increase storage and network-bandwidth requirements. So it was decided in countries like India that the electronic images of truncated cheques will be in grey-scale technology [ 5 ]. In countries like the United States, after an image of a cheque is transmitted electronically, those banks that cannot process theimageelectronicallycanprinttheimagereplacementdoc-ument (IRD), which is then processed similar to a traditional cheque [ 6 ]. In countries like India, under the cheque trun-cation system (CTS), after capturing the image, the paper cheque would be warehoused with the presenting bank. In case the beneficiary or any other connected persons require the instrument, the payee bank could issue a paper copy of the image (IRD), under its authentication. IRD has become a legally recognized replacement of the original cheque for re-presentment [ 5 ].

Legal amount, courtesy amount, date, payee details and signature are the fields to be filled by an account holder on a bank cheque as shown in Fig. 1 . (In India, the word  X  X ac X  or  X  X akh X  is used to write value equivalent to 100,000.) The signature present on the cheque ensures the authenticity of it. Banks have the freedom to customize some parts of the cheques such as the background pattern, which is generally used to personalize the cheques. They can use different col-ours and imprinted textures. Other variations that may occur among the cheques issued by different banks are different fonts, special symbols, logos, lines, etc. Images of cheques from different parts of the world can be seen in Fig. 2 .The two fields for writing the value of the cheque named legal and courtesy amounts are intended for redundancy. In case of a disagreement, the legal amount is selected. However, a disagreement between legal and courtesy amount shall be an indicator of amount alteration. The first field (legal) contains the amount written in words, and the second field (courtesy) contains the amount written in numerals. It is considered that the cheque amount recognition has to rely on both cour-tesy and legal amount recognition. The underlying principle behind this view is that the expressions of a given amount into digits and words are so different that the recognition errors on both sides are likely to be uncorrelated [ 7 ]. Automatic rec-ognition of handwritten dates present on bank cheques is also very important in application environments where cheques will not be processed prior to the dates shown. In countries like India, a cheque cannot be processed after 6months of the date written on it. Verification of the handprinted signature present on a paper cheque is inevitable as the signature car-ries the authenticity of the cheque. The main steps involved in automatic processing of a bank cheque are shown in Fig. 3 , where the first step is to obtain the image of the paper cheque using a scanner. Preprocessing and segmentation modules follow the image acquisition step. The verification and rec-ognition of different information present in the cheque are done after the extraction phase. Nowadays while processing a cheque, banks are interested to read automatically as much information as possible from the document. This may include the payee-name, payer X  X  address (if present), payer X  X  account number, name of the issuing-bank and code lines [ 8 ].
In 1997, a collection of various papers on automatic pro-cessing of bank cheques was published in the form of a book edited by Sebastiano Impedovo et al. [ 9 ]. Till date, there is no survey paper published in the area of automatic bank cheque processing as per our knowledge. It is obvious that a survey of existing techniques related to the automatic processing of bank cheques will be a great asset to the researchers working in the area. Therefore, in this paper, it is tried to highlight the issues like preprocessing, extraction and recognition of handwritten information and verification of signature.
The paper is organized as follows. Main aspects related to preprocessing like quality assurance, authentication, bi-narization, skew correction, slant correction and normaliza-tion are presented in Sect. 2 . The issues related with the extraction of different fields are discussed in Sect. 3 .The courtesy amount recognition part is divided into two subsec-tions for discussing touching numeral segmentation and digit recognition, respectively, and they are discussed in Sect. 4 . Correspondingly, the legal amount recognition part has three subsections: guideline removal, amount segmentation and word recognition, and they are presented in Sect. 5 . The date recognition techniques are discussed in Sect. 6 . Cross vali-dation of legal and courtesy amounts is required to achieve higher reliability at the system level for industrial applica-tions. The related works in this area are discussed in Sect. 7 . Various signature verification techniques are discussed in Sect. 8 . Techniques related to the recognition of payee-name field are discussed in Sect. 9 . Nowadays, cheque processing has become an industry, and Sect. 10 of the paper deals with some of the products available in the market for automatic cheque processing. It will help the readers to understand the recent trends in the cheque processing industry. Some obser-vations made during the survey and some challenges in auto-mation are also pointed out in Sect. 11 . The survey is finally concluded in the last section. 2 Preprocessing One of the challenges faced in the adoption of image-based cheque clearing is the  X  X eed X  to ensure that high-quality cheque images are transmitted for clearing through the clearing house. If the image is of low quality, then the receiving bank may not be able to process the image for clearing and may result in a cheque return. As the cost of processing  X  X heque-returns X  is multifold compared to a normal clearing, it is desirable to minimize the incidences of bad cheque-image quality.  X 3i-Infotech X  has developed an image quality assurance (IQA) validation engine, which is a standalone tool that can be used for performing IQA on any cheque image [ 10 ]. Another commercial product called  X  X 2iA check-reader X  is capable of locating and read-ing information from image replacement documents (IRD) or substitute cheques, which are used in Check 21(Cheque Clearing for the 21st Century Act) image exchange in United States [ 11 ]. Some of the image quality attributes consid-ered by A2iA check-reader are:  X  X artial image X ,  X  X xcessive image skew X ,  X  X iggyback image X ,  X  X mage too light or too dark X ,  X  X treaks and/or bands on the image X ,  X  X elow minimum image size and above maximum image size X . Silver Bullet X  X   X  X anger-IQA X  software is also used for cheque-image qual-ity assurance during the image capturing step. This allows many cheque-image quality problems to be detected at the source before truncation. The quality attributes considered include the following:  X  X ndersize image X ,  X  X versize image X ,  X  X elow minimum compressed image size X ,  X  X bove maximum compressed image size X ,  X  X xcessive document skew X ,  X  X mage too light X ,  X  X mage too dark X ,  X  X orizontal streaks present in the image X ,  X  X olded or torn document corners and edges X ,  X  X oc-ument framing error X ,  X  X xcessive spot noise in the image X ,  X  X ront-rear image dimension mismatch X  and  X  X arbon strip detected X  [ 12 ]. The following subsections discuss different techniques reported in the literature regarding the prepro-cessing of bank cheque images. 2.1 Image quality assurance Quality assurance (QA) plays a crucial role in document digitization projects by making sure that the specified qual-ity standards are reached under cost and time constraints [ 13 ]. Many algorithms based on text line distortion models [ 14  X  16 ] are proposed to deal with the nonlinear folding of documents. Sometimes, folding (warping) can be seri-ous that the contents of the document become unreadable. As the actual capturing area is usually larger than the area of interest, junk regions can be present in the forms of black/shaded borders. Fan et al. [ 17 ] combined two cropping algorithms,onebasedonlinedetectionandtheotherbasedon text region growing, to achieve robust cropping. Bourgeois et al. [ 18 ] proposed an algorithm based on morphological techniques to detect and remove line-frames. Sometimes, poor document quality makes it difficult to capture high-quality images. In large scale document digitization pro-jects, the main challenge is to automatically decide  X  X hen to apply which enhancement X . Image enhancement tech-niques may adversely affect an image X  X  quality if applied to a wrong image. Boutros in [ 20 ] proposed a prototype that can automate the image enhancement process. It is clear that the quality of image acquisition affects the later stages of doc-ument image processing. A list of general suggestions for quality control is given in [ 21 ]. It is suggested that scanning with 400dpi colour will be sufficient for the next 10years for applications where scanning is being done just for the purpose of understanding the content. 2.2 Authentication of document Garain and Halder in [ 22 ] presented a technique towards the automatic authentication of bank cheques. Support vector machines (SVMs) and neural networks (NN) were used for thesame.Theproposedmethodfirstcomputationallyextracts the security features from the document images before clas-sifying them into  X  X enuine X  and  X  X uplicate X . Security features on bank cheques are grouped into three distinct areas namely  X  X ecurity design or background artwork X ,  X  X se of color inks X  and  X  X aper and printing process X . Light fine-line printing or other security patterns that appear on the background of cheques are difficult to reproduce by ordinary printing tech-niques. Colour ink pigments and special ink types (fugitive inks, thermo chromic inks) also contribute substantially to the security of cheques. Sometimes, paper manufacturers use their own watermarks to provide additional visual protection. The printing process (technique) also provides security to documents like bank cheques. For instance, intaglio printing (used for printing bank cheques) is a special kind of off-set printing that gives the document a very high-quality look that is difficult to reproduce by scanners, colour copiers or colour laser printers. Use of MICR (magnetic ink character recog-nition) characters at bottom of the bank cheques (for code lines) is also considered as a security means.

Most banks nowadays offer  X  X ositive-pay X  services to their corporatecustomerswhichhavebeenverysuccessfuliniden-tifying counterfeit and altered cheques at the time of present-ment [ 23 ]. For cheques issued by  X  X ositive-pay X  customers, an image-enabled solution namely PNV (Payee Name Ver-ification) enhances the service by detecting the payee line alterations (if any). A tamper-proof encrypted code is printed on the cheque in the form of a bar-code. This encrypted code allows banks to prove authenticity at the point of present-ment. 2.3 Binarization Binarization of the input cheque image is the first step in most of the cheque processing systems reported so far. It is the process by which the foreground and background pixels are represented by  X 1 X  X  and  X 0 X  X  or vice versa [ 3 ]. Binarization of a grey-scale cheque image is complicated due to several causes including complex backgrounds, imprinted seal and different intensities of handwritten characters. In general, the binarization techniques can be categorized into two clas-ses: global and local thresholding. Global thresholding algo-rithms use a single threshold for the whole image, while local thresholding algorithms compute a separate threshold for each pixel based on its neighbourhood [ 24 ]. Sahoo et al. [ 25 ] comparedtheperformancesofmorethan20globalthreshold-ing algorithms using uniformity or shape measures. The com-parison showed that Otsu X  X  class separability method [ 26 ] performed best. Trier and Jain [ 27 ] evaluated the perfor-mances of 11 established local thresholding algorithms. In that evaluation, algorithms of Niblack [ 28 ], Yanowitz and Bruckstein [ 29 ], White and Rohrer [ 30 ], Trier and Taxt [ 31 ] and Parker [ 32 ] produced high recognition rates. Some well-established binarization algorithms were compared in the work reported in [ 24 ], where binarized outputs were fed into an offline handwritten recognition engine that uses pre-trainedneuralnetworksforcharacterclassification.Ascoring package is then used to automatically examine the recogni-tion results. A survey of various thresholding techniques up to 2004 is discussed in [ 33 ].

In [ 3 ], a method based on signal matching is proposed to binarize Chinese bank cheque images. The image projection function without noise is the source signal. The projection function of image binarized by an iterative threshold will match the source signal, and the threshold for which the pro-jection function matches best is considered as the optimum threshold. In [ 24 ], the binarization of a part of a Canadian cheque image that suffers from noise interference [target sub-image (TSI)] is done using information easily extracted from another noise-free part of the same image [model sub-image (MSI)]. Simple spatial features are extracted from MSI and are used as models for handwriting strokes. This model cap-tures characteristics of the writing strokes and is then used to guide the binarization of the TSI. The binarization algorithm proposed in [ 34 ] defines an initial threshold value using per-centage of the desired density of black pixels to appear in the final binarized image. After this, a correction is made using receiver operating characteristic (ROC) curves. To improve the efficiency of the algorithm, a cubic function makes a rela-tionship between the initial threshold value and the final one.
In [ 1 ], the binarization of the grey-scale image is done with a threshold value calculated dynamically based on the number of connected components in the area of courtesy amount. In [ 35 ], initially, the image is smoothed using a mean filter. The background is then eliminated through an iter-ative thresholding. In [ 36 ], gradient and Laplacian values are used to find whether an image pixel belongs to back-ground or foreground. The binarization approach proposed in [ 37 ] is based on Tsallis entropy to find the best threshold value. It also uses Histogram specification for preprocessing some images. To eliminate the background from the cheque image in [ 38 ], a stored background sample image is sub-tracted from the skew corrected test image. A binary printed information pattern is generated and is then subtracted from the background free image to generate an image contain-ing only filled information. The background residual noise is eliminated using erosion and dilation operations. To deal with broken lines on cheques, logical smearing is applied with the help of end-point co-ordinates of detected lines in [ 39 ]. In [ 40 ], the preprinted data present on Chinese bank cheques are eliminated by a subtraction process according to their colour. 2.4 Skew and slant correction Theskewoccurredwhilescanningthechequecanbedetected by finding the angle that the guidelines (baselines for writing) make with the horizontal direction ( X -axis). This approach would be convenient as almost all the cheques contain guide-lines for user inputs. Skew correction is done by simply rotat-ing the image in the opposite direction by an angle equal to the inclination of the guidelines. A comprehensive survey on different skew detection techniques is done in [ 41 ]. In [ 42 ], the skew of a cheque image is calculated by computing the histograms of pixel densities between + 5 and  X  5degrees with respect to the horizontal axis. Due to the presence of guidelines, the histogram with longest peak corresponds to the skew of the cheque image. To correct the rotation and translation occurred during the image acquisition process, a method based on projection profile [ 43 ] has been used in [ 38 ].
Slant is the deviation of handwritten strokes from the ver-tical direction ( Y -axis) due to different writing styles. It has to be detected and corrected for successful segmentation and recognition of handwritten user inputs. Different techniques for the same can be found in the surveys carried out in [ 44 ] and [ 45 ]. In [ 46 ], a chain code representation is used for calculating the slant angle of handwritten information. In [ 47 ] and [ 48 ], the average slant of a word is determined by an algorithm based on the analysis of slanted vertical histo-grams [ 49 ]. The heuristic for finding the average slant is to search for the greatest positive derivative in all the slanted histograms. The slant is then corrected through a shear trans-formation in the opposite direction. Also in [ 50 ] and [ 51 ], the slant of handwritten information is computed using the histogram of the directions of the contour pixels. 2.5 Normalization Image size normalization is a crucial preprocessing stage in the development of robust recognizers [ 52 ]. It is also rele-vant in automatic cheque processing systems as in the United States an image replacement document (IRD) is created with a reduced (70%) image of the cheque. Size-invariance is a key to any robust recognition system. The size normalization step attempts to obscure scale variations of images presented to a recognizer. It is a transformation of an input image of any arbitrary size into an output image of a fixed pre-defined size without compromising the structural details. A method for image size normalization based on multi rate filter theory is proposed in [ 52 ]. The proposed method was found better than the ratio-based normalization technique and the simple scal-ing technique. In the ratio-based normalization technique, the value of each pixel in the output image is calculated as a weighted average of the overlapping pixels in the input image. In the simple scaling normalization, a scaling-factor is first calculated by the ratio of the original image height to the new image height. If the width of the image scaled by the same ratio fits in the specified dimension, then the scaling-factor is selected for further processing. If the normalized width exceeds the specified width, another scaling-factor is applied to reduce the width further. 3 Extraction of handwritten fields After pre-processing, it is necessary to perform the extrac-tion operation of different handwritten fields prior to their recognition. In [ 53 ], for American bank cheques, the extrac-tion of legal amount includes the extraction of  X  X ollar X  and  X  X ent X  portions of the amount. This process is initiated by searching for a long horizontal line, which is usually written after the dollar part. If such a line is not present, the right side of the image is searched to find dashes and slashes to locate the cent portion. In [ 53 ], the courtesy amount recognition starts with the localization of handwritten numerical string based on the location of the courtesy amount. The removal of irrelevant objects like lines and box is based on Hough Trans-form. The dollar and cent portions of the courtesy amount are separated using characteristics such as size, shape, relative position, etc. The algorithm has about twenty simple rules to achieve this. A method based on baselines (guidelines) is used in [ 19 ] and [ 54 ] to extract the handwritten date, courtesy and legal amounts of Canadian bank cheques. The guideline for legal mount is found by analysing the lengths of the lines extracted trough edge detection. The guidelines for date and courtesy amounts are detected by using the layout informa-tion of Canadian cheques. A searching region and a bounding region are decided for each field, and the grey-scale distri-butions of the handwritten strokes related to each item are extracted by tracing the connected components.

In [ 55 ], a blank reference image is used to extract the user-entered components from a handwritten bank cheque. An inter-image morphological subtraction that consists of a GLS (grey-level space) fusion procedure and a logical sub-traction procedure (in GLS) is used in the paper to extract the handwritten fields from a cheque. In [ 42 ], three features are used to locate the courtesy amount field in French cheques: the horizontal line on which the numerals are written, the letters B.P.F, and the histograms showing the pixel densities in the upper right corner of the image. The box or line in the courtesy amount field is removed by analysing the thick-ness, direction and curvature of the line components without causing damage to the numerals. The legal amount is located using the presence of two horizontal and parallel guidelines. In [ 56 ], Hough transform is used to detect guidelines and boxes on French cheques. The letters B.P.F are used to find the beginning of the courtesy amount. To classify the hand-written and preprinted information in the area of courtesy amount, a classification is performed based on the regularity properties of machine printed text. In [ 7 ] also, the guidelines and the letters B.P.F are used to detect the legal and courtesy amounts, respectively. Guidelines are detected using a line following algorithm, and the letters B.P.F are detected by a character recognizer.

For bank cheques written in Bengali language [ 57 ], a tem-plate describing the locations of user-entered data is used to extract the areas of interest. A stored background pattern is then subtracted from these sub-images to eliminate the back-ground. In [ 1 ], the concept of minimum bounding rectangle (MBR) is used for locating the courtesy amount string. For Indian bank cheques in [ 39 ], the most probable region (MPR) for finding courtesy amount is determined using configurable rules and semantic analysis. The exact location of courtesy amount is determined by detecting the surrounding bounding box. Generally in India, the courtesy amount is located on the right half of the cheque. A box is detected by identifying the cross-section points where horizontal and vertical lines meet. Another method proposed for extraction in [ 36 ]uses fuzzy membership values, entropy, energy and aspect ratio as features, which are fed into a fuzzy neural network (FNN) for the identification of a field. For the extraction of user-entered data from Brazilian bank cheques in [ 58 ], a template is used to extract the areas of interest. For each of the resulting sub-images, the background pattern is eliminated by a sub-traction operation; the machine printed character strings are eliminated by a subtraction operation between the sub-image and a generated binary image containing the same character string.

A method is discussed in [ 59 ] for locating the cour-tesy amount block on American bank cheques. The con-nected components in the image are detected first. Then, strings are constructed on the basis of proximity and hor-izontal alignment of characters. A set of rules and heuris-tics are applied to these strings to choose the correct one. The selected string is accepted only if it passes a verification test including an attempt to recognize the currency sign. In [ 60 ], for Arabic bank cheques, two methods based on math-ematical morphology (MM) and Hough transform (HT) for the detection of lines and boxes are compared. It is found that the Hough transform-based method performs better than the mathematical morphology. For the extraction of date, signature, seal imprint and courtesy amount from Japanese bank cheques in [ 61 ], the areas of interest are initially located using a template. The filled-in items are then extracted using their colour characteristics. An extraction method based on support vector machines (SVM) is proposed in [ 62 ]for Chinese bank cheques. The bank logos are initially recog-nized using an SVM using layout information and projec-tion profile features. Cheque type characters are then rec-ognized using another SVM with projection profile feature. After knowing the bank and the cheque type, a blank tem-plate of the cheque is used to subtract it from the real cheque to get the filled-in items.

In [ 63 ], the baselines and background are eliminated by subtracting a blank template from a filled one. For each con-nected component on the cheque image, its minimal bound-ing box is found. A box receives the label of an item if it minimizes its specific distance for that item. Reconstruction of the truncated written lines is based on the assumption that a written line does not change its curvature while intersecting with graphic background lines. A method to segment uncon-strained handwritings on Iranian bank cheques based on mathematical morphology and connected component analy-sis is proposed in [ 64 ]. A local thresholding technique to sep-arate handwritten strokes from the background is discussed in [ 65 ], where baselines are extracted using mathematical morphology and with the layout description obtained by ana-lysing these extracted baselines, the connected components are detected and grouped together to extract the handwritten fields. For the identification of printed and handwritten items in [ 66 ], 2-D histogram features, typographical features and morphological features are extracted. The most suitable fea-tures are selected using a method based on the separability measure. A Bayesian classifier (BC) then uses the selected features to realize the discrimination.

In order to separate the handwritten dates into day, month and year fields in [ 67 ], the segmentation process includes a preprocessing step, a separator detection step and a seg-mentation and hypothesis step. In the preprocessing step, the image containing high noise is rejected. The machine printed information  X 19 X  is used to separate the year field. To differentiate the day and month fields if there is no delimiter between them, the following information has been used: distance-to-digit measure and contextual analysis, con-fidence value from a connected digit recognizer [ 68 ], max-imum number of runs and width feature. If delimiters like punctuations are present, some space and shape features [ 68 ] are used to recognize them. Same results are shown in [ 69 ] also. A knowledge-based segmentation scheme is employed in [ 70 ] for the segmentation of date field into day, month and year fields for English and French bank cheques. The pos-sible separator candidates are first detected using shape and spatial features. Some separator candidates are easily con-firmed or rejected based on a set of rules formed using the knowledge of writing style.

Almost all cheques have some machine printed informa-tion present on them. In [ 71 ], a set of shape-related and content-related features are used to discriminate machine printed and handwritten fields. Area and perimeter are useful in searching for regular machine printed patterns. Eccentric-ity is another measurement, which is the ratio between the lengths of their major and minor axes of the ellipse covering the pattern. An additional way to measure the elongation of a pattern is to determine its rectangularity, which is the ratio between the area of the pattern and the area of its bounding box. The nearer the result to 1, the more rectangular is the pattern. It is noted that machine printed patterns are more consistent in the variability of their grey levels. Range is another useful aspect that shows the consistency of grey lev-els. Another attribute stroke density is defined as the ratio between mass and volume, where mass is represented by the number of different grey levels in the object and volume is defined as the total number of pixels in the object. Other sta-tisticalfeaturescanalsobeused,suchaskurtosis,whichgives an idea of the degree of distribution of the grey levels in some textual objects. In [ 72 ], an algorithm is presented, which is based on the theory of hidden Markov models (HMM) to distinguish between machine printed and handwritten mate-rials. In [ 73 ], a method based on projection profile and line merge is proposed to extract handwritten patterns. A sliding window approach is described in [ 74 ] to extract signatures from cheque images. Djeziri et al. in [ 75 ] tried to tackle the problem of extracting hand-printed signatures by means of an intuitive approach which is very close to human visual perception. They defined a topological criterion specific to handwritten lines that they called filiformity. This approach is inspired by the existence cells in the human eye whose specialized task is the extraction of lines. In [ 76 ], the clas-sification of different extracted fields is done using a  X  X if-ferential box counting X  (DBC) method, which calculates the fractional dimensions of the data fields. Table 1 shows the performance details of some of the extraction techniques. The data set shows the number of cheques used for testing the techniques. 4 Courtesy amount recognition Courtesy amount on bank cheques normally appears as a string of numbers with symbols or characters representing the value of the currency type as shown in Fig. 1 . Punctua-tions like slash ( X / X ), comma ( X , X ), hyphen ( X - X ) and full stop ( X . X )alsoappearinthecourtesyamountfielddependingonthe writing style of the user. While writing courtesy amount, the digits may touch each other. As a result, the touching numer-als have to be successfully segmented before the recognition task. Figure 4 shows some handwritten courtesy amounts extracted from different handwritten cheques. Many papers are available on the topic of segmentation and recognition of handwritten numerals. A survey of segmentation techniques used for touching handwritten characters is reported in [ 77 ]. A review of various feature extraction techniques is done in [ 78 ], and all major character recognition techniques are dis-cussed in [ 79 ]. The main steps in the recognition of courtesy amounts are segmentation of touching numerals and the clas-sification of individual digits as discussed in the following subsections. 4.1 Segmentation of touching numerals Because of different writing styles, some digits in a numeral string (courtesy amount) may touch each other. Such touch-ing digits have to be separated successfully before the rec-ognition step. Some authors claim that there are two types of approaches used for segmentation of touching numerals: local and global. Cutting points between the two touching numerals are extracted in local approaches, and in global approaches, significant splitting points are detected after analysing the numeral string as a whole [ 80 ]. A segmenta-tion-based courtesy amount recognition system is presented in [ 81 ] where a two-stage segmentation module has been pro-posed: the global segmentation stage and the local segmen-tation stage. In the global segmentation stage, the courtesy amount is coarsely segmented into sub-images according to the spatial relationships of the connected components. The recognition module then verifies these sub-images, and the rejected sub-images are divided sequentially using contour analysis in the local segmentation stage. In [ 7 ], the segmenta-tion is based on a set of heuristic rules designed by analysing large number of examples. Prospective cut points are situated at the locations determined by minima of the upper contour or maxima of the lower contour. In [ 82 ], the segmentation is carried out using monotonic fuzzy valued decision functions computed by feed-forward neural networks.

In [ 83 ], the courtesy amount string is segmented into iso-lated digits using a two-level segmentation approach con-sisting of local and global approaches. At the first level, each connected component block is given as input to all the recog-nizers. Recognized blocks are removed from the input image, and the remaining blocks are fed to the second level, where a drop-falling algorithm and a contour-based algorithm are sequentially applied. After each segmentation procedure, the block is divided into two sub-blocks, and they are fed again into the digit recognition system. For segmenting the handwritten courtesy amount strings in [ 84 ], a drop-falling method and an upper-lower contour method are applied after identifying the connected components. The isolated compo-nents after segmentation are fed into a digit recognizer. If all components are recognized correctly, the segmentation is accepted. A new segmentation hypothesis is taken into account if the recognition is not completed. If the recognition fails for each and every segmentation hypothesis, the cheque will be rejected. In [ 42 ], for each connected component, the probability of representing a single character is computed using several geometric features. Large connected compo-nents are split along straight lines according to a set of crite-ria. Again, the probability of representing a single character is computed for each part of the split operation. Positions of the connected components are analysed to detect objects like comma, full stop, etc. In [ 35 ], the connected components are found by contour tracing. Connected components with a perimeter length below a specific threshold are rejected. Rec-ognizing the leftmost digit in the connected component car-ries out the segmentation of touching numerals. If successful, the remainder of the component is recognized recursively.
In [ 56 ], the segmentation is based on a pre-recognition task, which controls the digit extraction. Initially, all con-nected components are detected and small components, which do not respond to size and position, are removed. Con-nected components containing touching digits must respond to some requirements on location and morphological mea-surements. A supervisor unit capable of undoing a particular task or preventing the application of it controls the consis-tency of these operations. In [ 38 ], the segmentation process isolates each connected component and detects symbols like commas and dash-lines with their relative low heights to the adjacent numerals. To segment the touching numerals, mor-phological convolutionmasks areappliedtofindthetouching partandsubsequentlyseparatethetouchingnumerals.Forthe segmentation of courtesy amount in Chinese bank cheques a combination of dissection, holistic and recognition-based method is used in [ 40 ]. In the dissection method, the con-tour tracing is done first. The broken parts of a digit found in its maximum area are then merged together. The bigger connected components are sent to the segmentation step of recognition-based method as well as holistic method. The recognition-based method then finds the connecting strokes or ligature between digits using features like upper contour, lower contour, vertical width and vertical projection. Holis-tic method is mainly used for sub-strings containing many zeros. The most distinctive feature considered in the work for connecting zeros is the presence of loops.

In [ 85 ], connected components are found by tracing con-tours.Thedigitsplitteroperatesbyattemptingtosegmentand recognize the leftmost digit from a connected component. If successful, it tries to recursively recognize the remaining part of the string. It attempts up to 5 cuts to separate each digit in the numeral string. The segmentation and recogni-tionprocessesin[ 1 ] are performed using a feedback loop, where different segmentation attempts are evaluated until the goal is achieved. The segmentation is performed by a hybrid drop fall (HDF) algorithm [ 86 , 87 ] and an extended drop fall (EDF) algorithm [ 88 ]. In [ 89 ], the connected components are extracted using vertical projection and isolated compo-nent analysis. Then, the length estimation of connected com-ponents is carried out using syntax analysis and waveform analysis. The segmentation of connected numeral strings is finally done using a reverse  X  X ropfalling X  algorithm.
Three techniques used in [ 90 ] to segment the touching numerals are connected component extraction, upper and lower contour splitting and the hit and deflect strategy (HDS). After segmentation, an analysis module classifies the sym-bols into primitive sets, and the syntactic correctness is checked. A legal amount estimator unit decodes the hand-written word representation of the legal amount to find the number of digits in the courtesy amount. The syntactic parser output of the courtesy amount and the legal amount estimator output are decoded by an evaluation unit that determines the validity of the segmentation and re-segments if necessary. In [ 91 ], the segmentation is based on the relationship of two complementary sets of structural features called contour pro-file and skeletal points. The algorithm takes into account an over-segmentation context, and its final objective is to pro-vide the best list of hypotheses of segmentation paths without any a priori knowledge about the context. 4.2 Digit recognition The accuracy in recognizing constituent digits plays a big role in the recognition accuracy of the handwritten cour-tesy amount numeral string. After successful segmentation of individual digits from the numeral string, they have to be correctly recognized to get the value of the cheque. From the literature, it is evident that the digit recognition tech-niques can be grouped into neural network based techniques and other prominent techniques as discussed in the following subsections. 4.2.1 Neural network-based techniques A neural network consists of an interconnected group of artificial neurons, and it processes information using a con-nectionist approach. A neural network changes its structure based on external or internal information that flows through the network during the learning (training) phase [ 92 ]. The digit recognition module in [ 7 ] is based on a combination of a radial basis function (RBF) network using a concavity fea-turevectorandatimedelayneuralnetwork(TDNN).AnRBF is a type of neural network having three layers, and a TDNN is a multiplayer perceptron. Individual digits are recognized in [ 83 ] based on a multi-expert approach consisting of pat-tern matching, crossing line, histogram formation, contour slope, region enhanced loci, structural-based matching and neural network. The A2iA system in [ 42 ] uses a number of features and different classification algorithms. Mainly three types of features are considered in the work. The first type of features includes height, aspect ratio, profile and num-ber of intersections with horizontal and vertical lines. The second type of features includes the structural features like loops, ascenders, strokes in different directions, concavities and convexities. Features of the third type depend on the position of the digit relative to the text line. The classifiers used are as follows: Bayes classifier (BC); template-based (TB) classifier and neural network (NN)-based classifier.
The set of features used for digit recognition in [ 38 ]is divided into two groups. The first group of features con-tains holes and their relative locations, number of intersec-tions with the principal and secondary axis and crossing sequences. The second group of feature includes the dis-tribution of foreground pixels and the relative positions of intersecting strokes. The classification procedure in [ 38 ]is done in two stages. The first stage is a rule-based classifi-cation, and the second stage is a neural classification based on Hopfield neural nets (HNN). In [ 40 ], three classifiers are developed for digit recognition. They are hierarchical neu-ral network classifier (HNNC), multiple structural feature classifier (MSFC) and principal component analysis classi-fier (PCAC). The HNNC uses neural networks to recognize the digits; MSFC uses multiple structural features includ-ing stroke features and contour features. The PCAC is based on principal component analysis and employs statistical fea-tures. In [ 46 ], the courtesy amount recognition is done after the legal amount recognition. The legal amount is converted into corresponding digit string and used as a reference for the courtesy amount recognition. In [ 85 ], the digit recognition is done by extracting pixel distance features for classification by a back-propagation neural networks (BPNN).

In [ 50 ], the courtesy amount recognizer has four main components. A pre-segmentation module divides the input digit string into independent groups of digits containing one or more digits using projection-based features. Two recog-nition units then process each group: The digit detection unit recognizes those groups that contain only one isolated digit using features from contour information. The segmen-tation-free unit recognizes the remaining groups. For both the recognizers, a fully connected feed-forward multi-layer per-ceptron (MLP) is used for classification. These groups with more than one digit are further processed by searching for possible segmentation points and classifying the segmented parts. A global decision unit finally merges all results to a numeral string. The digits in [ 35 ] are recognized by a com-bination of three back-propagation neural networks (BPNN) using pixel distance features as input.

For the courtesy amount recognition of American bank cheques in [ 1 ], each isolated digit image undergoes slant cor-rection, size normalization and thickness normalization. The slant correction algorithm is based on the idea that if the pix-els of a numeral are moved horizontally through a series of slanted positions, the digit attains its minimum width when it is least slanted. The classification of digits is performed using structural analysis and multi-layer perceptrons (MLP). The recognition module has an array of three to four neural networks working in parallel. Their results are analysed by a function which can select or reject the results. In a post-pro-cessing stage, the resulting numeral string is analysed using deterministic finite automation (DFA) to verify whether the recognized value is a valid amount or not. Two MLP classi-fiers are combined into a single module for the purpose of recognition in [ 81 ]. In [ 93 ] and [ 94 ], it is proved that by com-bining MLPs, one could improve the accuracy of courtesy amount recognition. In [ 95 ], it is shown that the results can be improved further by combining one MLP with a generalized regression neural network (GRNN) and Elman back-propa-gation MLP (ELM). In countries like Brazil, the use of delim-iters is very common in the courtesy amount field. On an average, 36% of the Brazilian cheques contain delimiters. In [ 4 ], the problem of recognizing delimiters in courtesy amount fields is investigated, and a new approach is proposed which combines different MLP classifiers to perform the recogni-tion of delimiters and digits. 4.2.2 Other prominent techniques The courtesy amount recognition module in [ 56 ] is based on a combination of two classifiers based on cluster hyper planes. The first one is based on concavity features, and the second one is based on statistical and structural features. The output of the recognition module is the ordered list of classes with decreasing confidence level. The digit string recogni-tion in [ 53 ] is based on matching the input sub graph with graphs of symbol prototypes as described in [ 96 ]. The sym-bol prototype consists of the symbol graph and description of its elements in terms of geometrical characteristics of edges, mutual position of edges and nodes, etc. First, a graph of the input string is created. Then, the digit string is segmented into separate symbols. After selecting the sub graphs of the input string, each sub graph is compared with the prototype. Some transformations are applied to the sub graph during the matching process. A match is found if there is an isomor-phism between the transformed sub graph and a part of the prototype.

Before courtesy amount recognition in [ 39 ], a num-ber of symbols including currency symbol, delimiters, ter-minals and noise are removed. Recognition of numeral string is based on concurrent concatenation and recogni-tion using dynamic programming optimization. A modular system to recognize numerical amounts on Brazilian bank cheques is presented in [ 91 ]. The system uses a segmenta-tion-based recognition approach, and the recognition func-tion is based on a recognition and verification strategy. This approach consists of combining the outputs from different levels like segmentation, recognition and post-processing in a probabilistic model. A new feature set is fed to the verifier module to identify the segmenta-tion effects such us over-segmentation and under-segmen-tation. In [ 97 ], the handwritten numeral classifier is based on a combination of self-organizing map (SOM) and a fault tolerant (FT) technique based on a dynamic cipher code. In [ 98 ] and [ 99 ], for the recognition of courtesy amount on UK, US and French bank cheques, the segmenta-tion process produces several options, each option is then processed and interpreted separately. Four complemen-tary optical character recognition (OCR) systems recog-nize potential characters producing a list of class candidates with associated probability estimates. A log-linear integrator [ 100 ] combines the OCR results and outputs the final class probability estimates. Table 2 shows the performance details of some courtesy amount recognition systems with highest level of confidence. The performance is given at digit level and full amount level. From the table, it can be seen that the amount level accuracies are lower than the digit level accu-racies. 5 Legal amount recognition The legal amount is the value of a cheque written in words. As it is difficult to modify the legal amount, higher prior-ity is always given to legal amount than the corresponding courtesy amount. The recognition of legal amount present on a bank cheque is a big challenge because of the structural complexity of characters and variability of writing styles. There are mainly two types of approaches used for hand-written legal amount word recognition: analytical and global (holistic). In analytical approaches, each handwritten word of the legal amount is recognized by recognizing its constituent characters. For the same, a word is segmented into compo-nents like characters or graphemes (part of a character) first. Analytical approaches can still be divided into full character recognition methods and sub-characters recognition method. In global approaches, the entire word is considered as a sin-gle unit (pattern), and recognition is done without any sort of character-level segmentation. As the legal amount words written in English language can be case sensitive as shown in Fig. 5 , the size of the lexicon for word-level recognition can go up significantly [ 101 ]. Various techniques for the rec-ognition of handwritten words can be found in the surveys carried out in [ 44 , 45 ] and [ 102 ]. A survey of character level segmentation of handwritten words is done in [ 103 ]. Papers directly related to the processing of legal amounts and bank cheques are only considered here in the coming sub-sections. The important steps in legal amount recognition are guide-line removal, segmentation of the amount into words and the classification process as described in the following subsec-tions.
 5.1 Guideline removal Guidelines (base lines) are present on cheques for the user to write legal amount as shown in Fig. 1 . They can be of dif-ferent widths and lengths. When these guidelines are elim-inated for the recognition of handwritten data, some parts of the strokes get eliminated where they touch the guide-line. It is needed to reconstruct the eliminated parts of the strokes as they play a big role in the successful recognition of the legal amount. For guideline removal in [ 104 ], smooth strokes are detected after a thinning process, and a proce-dure based on stroke length is carried out. For the removal of guidelines from handwritten text, Dimauro et al. in [ 105 ]pro-posed a technique based on mathematical morphology with a dynamic selection of structuring element. In [ 83 ], two inde-pendent approaches are used for baseline removal. The first one is based on a controlled deletion strategy consisting of line detection, controlled thinning and baseline deletion. The second one is based on mathematical morphology and the use of a dynamic procedure. Guidelines on the cheque image in [ 46 ] are removed by the analysis of connected components using run length coding.

In [ 43 ] and [ 19 ], Sobel operator is initially used to calcu-late the gradient images. A local thresholding technique is then applied to select the candidate edge points of the guide-lines. The least square fitting and Hough transform are the two important techniques used for the detection of baselines. The baseline is first eliminated, and then both morphological and topological processing restore the lost information corre-sponding to the strokes. In [ 42 ], the guidelines are removed by analysing the thickness, direction and curvature of the line components without causing damage to the handwritten information. To restore the handwritten strokes intersecting with baseline in [ 35 ], a morphologic closing operation is applied with a suitable structuring element. But when the intensity of the baseline is very close to the intensity of the handwritten data, morphologic operations may not pre-serve the handwritten data completely. So topologic opera-tions based on edge detection and orientation information are used to detect and fill the gaps created during the line elimina-tion process. In [ 7 ], the overlapping between the handwritten strokes and guidelines is treated locally through a heuristic rule that extracts pixel runs in the guideline when the pixels are far enough from the closest stroke of the legal amount. A threshold equal to the normal thickness of the guideline is set in [ 106 ]. The thickness of the guideline is calculated by counting the black pixel run along the vertical direction. If the count exceeds the threshold, a crossing stoke is probably present and the line pixels are preserved. The baselines of Canadian bank cheques are detected and eliminated by using grey-level mathematical morphology in [ 65 ]. The informa-tion lost during baseline elimination is restored by mathe-matical morphology with dynamic kernels [ 107 ]. 5.2 Segmentation into words A handwritten legal amount has to be segmented into its constituent words for the recognition purpose. Many times, a word is further segmented into its constituent characters or pseudo characters for effective recognition. In [ 7 ], the words are separated using the empty space between them and then segmented into pseudo letters by cutting only the low concav-ity strokes. The recognition module handles the ambiguities between intra letter spaces and intra word spaces. In [ 42 ], the legal amount is segmented into parts called graphemes (parts of characters), which may result in over segmentation. The graphemes are then grouped into words from left to right based on word segmentation probabilities.

In [ 46 ], the legal amount is segmented into words by con-sidering the variation of spacing between the characters as a function. The concavities, the centre of mass informa-tion of the characters and bounding boxes were used for the same. For handwritten Italian legal amounts in [ 83 ], the mean length and standard deviation in terms of the number of local minima in the vertical direction are used to carry out the segmentation process. The system proposed by Dimau-ro et al. [ 108 ] for the recognition of legal amounts on Ital-ian bank cheques uses contextual knowledge (derived from courtesy amount) for the segmentation of legal amount into words. For segmenting American legal amount into words in [ 90 ], projection profile is used to measure the linear den-sity. Points are chosen based on the regions with zero linear density. The connected components are then extracted using an 8-connectivity grid. The average spacing between con-nected components acts as a threshold for word boundaries. The final segmentation is performed using average word length.

In [ 106 ], the segmentation of Chinese legal amount into characters is carried out by finding the gaps in the vertical projection profile of the amount. The segmentation process is an optimization process that minimizes the variance of the widths of the segments as the Chinese characters occupy similar widths. The segmentation of French legal amount sentences into words is done in [ 47 ] using a combination of explicit and implicit segmentation. In explicit segmentation, the words are separated easily by computing the horizontal distance between handwritten connected components. In the implicit approach, the recognition takes place as the amount is scanned from the beginning till the end of the sentence (legal amount) is reached. 5.3 Word recognition process The accuracy in recognizing constituent words and charac-ters plays a big role in the recognition accuracy of the legal amount string. After successful separation of words from the legal amount string, they have to be correctly recognized to understand the value of the cheque. From the literature, it is evident that the word recognition techniques can be grouped into HMM-based techniques and other techniques as dis-cussed in the following subsections. 5.3.1 HMM-based methods A hidden Markov model (HMM) is a statistical Markov model in which the system being modelled is assumed to be a Markov process with unobserved (hidden) states. Even though the states are not visible in an HMM, the output that is dependent on a state is visible. Each state has a proba-bility distribution over its possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states [ 109 ]. The word recognition in [ 7 ] is based on extraction of features from pseudoletters.Themainfeaturesconsideredareclosedloops, upper and lower extensions. The system performs two types of extraction. The first is based on a pseudo-character and the second is based on a full character. In both cases, the features are combined into word recognition scores trough HMMs of the words. The word recognition in [ 42 ] for French cheques is done using two complementary recognition chains each com-puting a list of word candidates along with probabilities. The first is an analytical approach based on HMM, and the second is a holistic approach based on structural features and Bayes classifier (BC). For the automated reading of German bank and postal cheques, in [ 50 ], each word of the legal amount is normalized by scaling in both the directions. An estima-tion of word length in terms of horizontal line crossings is used as a scale factor in x direction. The scale factor along the y direction depends on the distance between upper and lower baselines. A sliding window is used to scan the word from left to right using a sliding window to compute the fea-ture vector. Each sliding window is further divided into small sub-windows, and for each sub-window, the number of black pixels is counted. The legal amount models are integrated in a hierarchical network of HMMs where each path represents a valid amount.

In [ 110 ], each English word is scanned from left to right by a sliding window for feature extraction, where the features are based on the word contours. The feature vectors are rep-resented by a set of observation symbols (code words) called a codebook. Each word is then represented with a left-right HMM model with the number of states being a function of the average length of the observation sequences for that class. The training procedures used are maximum likelihood (ML) and maximum mutual information (MMI). The legal amount recognition unit in [ 47 ] for French cheques is a combination of two approaches. The first is based on global features like the relative position of the ascenders, descenders and loops within a handwritten word with a (K-nearest neighbour) KNN classification engine. The second is based on a HMM that uses feature set based on the orientation of contour points along with their distance to the baselines. A hidden Mar-kov model (HMM)-based word recognition algorithm for the recognition of legal amounts from French bank cheques is presented in [ 111 ]. Initially, the words are segmented into graphemes. Then, geometric features are extracted from the graphemes, and the feature vectors are quantized in a sequence of symbols for each word. An HMM-NN hybrid model computes the likelihood of all the word classes.
There are two complementary word recognizers in [ 98 , 99 ] for UK, US and French cheques. The first one is based on holistic features and the second one uses hidden Markov models [ 112 ]. Word recognition results are combined and presented as a list of word class candidates with associated probabilities. In [ 113 ], a system is developed for the rec-ognition of the handwritten legal amount on Brazilian bank cheques. The recognizer, based on HMM, does a global word analysis; therefore, it does not carry out an explicit segmenta-tion of words into characters or pseudo characters. This set of features is based on the representation of concavities and con-vexities. A methodology of legal amount recognition based on word segmentation hypotheses is introduced in [ 114 ]. The word-level segmentation hypotheses are derived as per the grapheme-level segmentation results of legal amount. Hybrid schemes of HMM-MLP classifiers are also introduced in the same paper for producing the ordered legal word recognition results with reliable decision values. These values can be employed for getting an optimal word segmentation path of over-segmentation hypotheses as well as a rejection criterion of word recognition result. 5.3.2 Other prominent techniques The result of courtesy amount recognition module is used as a priori information for legal amount recognition written in Italian language in [ 83 ]. Three different recognition methods for word recognition are combined in the system: analytical, global and mixed. The fully analytical approach is based on the assumption that each character consists of a sequence of basic strokes: cusps, circles, humps, oriented lines, semi circles, etc. The global approach is based on a graph descrip-tion of handwritten words. The mixed approach combines an analytical method for the analysis of singularities and global method for the analysis of regular patterns in a word. The results obtained by legal and courtesy amount recogni-tion modules are evaluated by an amount validation module, which accepts the most probable amount, if the level of con-fidence is higher than a specific threshold. In [ 53 ], the dollar part of the American legal amount field is recognized using a holistic approach based on feature sets having informa-tion about vertical and horizontal extremums (point located farthest from the middle). Feature transitions based on the similarity between different types of features are employed to measure the similarity between an input and a prototype. Probable points of segmentation into words and letters are also taken into account for best matching. Dynamic program-ming based on Levenshtein distance is also used to match the input phrase representation with all the lexicon entry repre-sentations. In [ 46 ], the word recognition module is based on dynamic programming. The algorithm compares the fea-ture vector of a word (combination of segments) with the reference feature vectors and finds the best match. The rec-ognition results at word level are arranged in a lattice, and it is traversed to generate meaningful amounts.

In [ 115 ] and [ 116 ], the classification system consists of a symbolic classifier (SC) and a neural network (NN) classifier. The symbolic classifier has to detect the ascenders, descend-ers and notable letters. A letter is called  X  X otable X  if it has an ascender and it occurs at the leftmost part of the word. When the symbolic classifier is unable to classify a word, the neural classifier is then used to classify the word. In [ 2 ], the system uses a set of geometrical and topological features like loop, endpoints, branch-points, crossing-points, convex-points and concave-points. The proposed scheme maps each word into two strings of finite symbols based on the spatial distribution of these features. An indexing (hashing) scheme is used on these strings to organize a lexicon. For an unknown word, the system uses the same indexing scheme to find a set of three words having highest similarity. A verification pro-cess is then carried out to find the best match based on seven invariant moments and a nearest neighbour classifier (NNC). For the recognition of English legal amounts in [ 48 ]seven types of global features like ascenders, descenders, loops, an estimate of the word length, as well as vertical, horizontal and diagonal strokes. A KNN classifier is employed to clas-sify the feature vectors. In [ 117 ], the failure of NN-HMM hybrid recognizer on Chinese bank cheque legal amount rec-ognition is discussed, the main reason for the failure is the inability of the NN-HMM hybrid recognizer to handle the complexity in the training Chinese words. A spiral recog-nition methodology is proposed as a substitute in the same paper.

Global features like ascenders, descenders, loops, word length, vertical, horizontal and diagonal strokes are used to create the feature vector for word recognition in [ 35 ] and [ 85 ]. The feature vector for each word consists of 11 components, based on the relative positions of ascenders, descenders, loops, strokes as well as the number of ascend-ers, descenders and loops and the word length. The words are then classified using a nearest neighbour classifier (NNC). To recognize Chinese legal amounts in [ 106 ], best five sets of segmentation points (got from the segmentation unit) are passed to the recognition stage, where an integrated segmen-tation-recognition approach is employed. The character rec-ognizer used is a combination of a statistical and a structural recognizer.Thestructuralrecognizerusesanelasticmatching approach, and the statistical recognizer employs a threshold-based approach for the recognition of characters. The match-ing scores are entered into a segmentation lattice, which is used for subsequent parsing. A Chinese language grammar checker is used to evaluate the probable legal amounts. It employs a static transition table and a dynamic transition method for grammar checking. In [ 107 ], Chinese character set associated with the currency units is used to locate the legal amount on bank cheques. The system initially tries to identify the smallest currency units in the image. Then, it tries to locate the strings associated with each currency unit. A rule-based approach is then adopted to recognize the strings.
For the recognition of handwritten Arabic amount words in [ 118 ] holistic structural features like the numbers of descenders, ascenders, loops, one dot above, two dots above, three dots above, one dot below, two dots below and sub-words are considered. These features are then presented to three classifiers: a multi-layer neural network (NN), a K-nearest neighbour (KNN) and a fuzzy K-nearest neigh-bour (FKNN) classifier. Each classifier gives a list of three candidate words together with their confidence value. Com-bining the results consists of merging the three lists of can-didates from the three classifiers to produce a new list by confidence values by taking the sum of confidence values, if a word is present in the list of two or more classifiers. For the recognition of French amount words in [ 119 ], two structural representations of a word based on strokes (8 dif-ferent strokes) and graphemes (42 representations) are used. Each of them is analysed using a Markov model. These models are individually optimized by a rigorous choice of the order to fit the structural properties of the observed data.

For American bank cheques in [ 120 ] and [ 121 ], the legal amount recognition is based on spotting words from the legal word lexicon by using confusion matrices. The recognized words are then parsed to obtain a correct syntax. The words are segmented using a disjoint box approach where the num-ber of segments is greater than the actual number of constit-uent characters. Segments are merged into characters using a dynamic programming approach with a modified quadratic discriminantfunction(MQDF).In[ 122 ],hiddenMarkovran-dom field (HMRF) is used for the recognition of legal amount fields of Amharic (Ethiopia) bank cheques. The features are based on the surrounding information of each pixel and pro-jection profiles. Contextual information based on the syntac-tical structure of Amharic cheques is also used to enhance the results. In [ 123 ], a hybrid approach for legal amount recognition on Italian bank cheques is presented. It makes use of the consideration that a legal amount can be treated as a series of  X  X ore X  groups of words separated by appro-priate  X  X eparator X  words. An analytical approach is used to achieve amount segmentation into  X  X ore X  groups of words, which are then recognized according to a global graph based approach. Lexical and syntactical a-priori knowledge of the domain of application is used for both amount segmentation and recognition. The recognition of legal amount words in [ 108 ] is accomplished using a simple matching procedure. This is based on the consideration that a word consists of a periodical signal that forms the regular part of the word and non-periodical signals that are robust in discriminating the word. Table 3 shows the performance details of some legal amount recognition systems with highest level of con-fidence. In the table, the results are summarized based on full amount recognition score as well word wise recognition score.
 6 Date recognition Automatic recognition of handwritten dates present on bank cheques is also very important in application environments, where cheques cannot be processed prior to the dates shown. In countries like India, a cheque cannot be processed after 6months of the date written on it. A date field can contain only numerals or a mixture of alphabetic letters (for Month) and numerals (for Day and Year). Punctuations and suffixes are also present in the date field as shown in Fig. 6 . Month can be written either before or after Day. Punctuations like slash ( X / X ), comma ( X , X ) and hyphen ( X - X ) can be used to identify the end of one field. Some users write suffixes such as  X  X h X ,  X  X t X ,  X  X d X  and  X  X d X  (specific to English language) after  X  X ay X . In [ 124 ], date processing of bank cheques is considered as the most difficult target in cheque processing because of the worst segmentation and recognition performance.

For date recognition in [ 85 ] and [ 35 ], the information is segmented into different fields, and appropriate recognition system(s) is applied to each field. The year field being always numeric is processed first. The technique used for courtesy amount is repeated for year recognition. The day and month fields are located using the space (gap) information between the connected components and by recognizing the special symbols like punctuations. The width of each field is used to determine whether they are numeric or alphabetic. The numeric fields are passed to the digit recognizer based on a NN classifier, which is used for the courtesy amount. Alpha-betic fields are processed by the word recognizer based on K-nearest neighbour classifier (used for the legal amount pro-cessing), which has been trained with lexicon of month words and their abbreviations.

In [ 125 ], the recognition of month name in a Brazilian cheque is based on a combination of holistic and analytical approaches with a single explicit segmentation technique to provide a grapheme sequence for the Hidden Markov Models of each recognizer. To improve the results in [ 125 ], a feature set based on concavity analysis is used. For the recognition of handwritten month words, features based on concavity anal-ysis and global features were used to improve the discrimi-nation among several writing styles in [ 126 ]. A word image is represented by two feature vectors of equal length to feed the HMMs. Another system mentioned in [ 127 ] deals with English month word recognition by combining a Multi-Layer Perceptron (MLP) classifier and an HMM classifier. Some improvements and modifications are reported to recognize both French and English month words in [ 128 ]. An effec-tive conditional combination topology is presented to com-bine two MLP classifiers and one HMM classifier, and a new modified product fusion rule is also proposed in the same. In [ 70 ], a complete date recognition system is developed in which the month name recognizer of [ 128 ] is used. For the year and day fields, the digit recognizer of [ 129 ] (for pro-cessing the courtesy amount) is used.

In [ 130 ], for the classification of hand written month words on Brazilian bank cheques, two architectures of artificial neural networks (ANN) are evaluated. The perfor-mances of conventional and class-modular MLP architec-tures are compared. Using global features (holistic approach) like perceptual features and characteristics based on concav-ities/convexities, it has been found that the class-modular architecture is superior to the conventional MLP architec-ture. The system described in [ 131 ] first segments a date image into sub-fields through the recognition process based on an HMM-based approach. Then, the three date sub-fields are processed by the system (day, month and year). A neu-ral approach has been adopted to work with strings of digits, and a Markovian strategy is employed to recognize and verify words. A concept of meta-classes of digits is also introduced, to reduce the lexicon size of the day and year and to improve the precision of their segmentation and recognition. Table 4 shows the performance details of some date recognition sys-tems. 7 Validation of legal and courtesy amounts In a complete cheque value recognition system, the consistency between the results provided by the courtesy and legal amount recognition units has to be verified. It is a com-mon practice to reject a bank cheque for which the courtesy amount recognition unit and the legal amount recognition unit indicate different quantities [ 132 ]. In some cases, both the recognition units may supply a list of  X  X  X  candidates hav-ing high level of confidence. In [ 83 ], the ranked lists of candi-dates supplied by the courtesy and legal amount recognition units are combined using a majority vote method based on Borda count function (MVBC). In [ 42 ], two methods are proposed for taking the final decision. The first method is based on a set of rejection rules, and the second is based on a neural network (NN). In [ 7 ], the final decision is taken by computing the weighted sum of the scores provided by the courtesy and legal amount recognition modules. In [ 53 ], the confidence levels of the probable answers are corrected using mutual properties of courtesy and legal amount recognizers. In [ 46 ], the number of digits in the probable legal amounts is calculated with the help of flags saved during the parsing of legal amount. This in combination with the confidence values of the first two choices of the courtesy amount recognizer is used to take the final decision.
 In [ 133 ] and [ 50 ], a new technique for error correction on Swiss postal cheques is proposed in which the legal amount is converted into a digit string using syntax directed translation (SDT). Once the result of the legal amount recognizer has been translated into the corresponding digit string, it can be easily compared to the result of the courtesy amount recog-nizer. Thus, discrepancies between the legal and the courtesy amount can be detected and correctly located at the level of individual words in the legal amount and digits in the cour-tesy amount. Table 5 shows the performance details of some systems capable of cross-validating the outputs of legal and courtesy amount recognizers. 8 Signature verification Verification of hand-printed signature present on a paper cheque is inevitable as it carries the authenticity of the cheque. Automatic verification of signatures is essential because of the difficulty in distinguishing genuine signatures from skilled forgeries on the basis of visual evaluation. Such techniques can also be applied to verify the authentication of contracts, identity cards, formal agreements, administrative forms, acknowledgements, etc [ 134 ]. Hence, static (off-line) signature verification has become a field that attracts more and more researchers [ 135 ]. So many techniques have been proposed in the recent past towards the offline verification of signatures. Impedovo and Pirlo in [ 134 ] and [ 136 ]dida detailed survey on the techniques available up to 2007 for the same. Some discussions on the prominent techniques as well as the latest advancements (after 2007) related to the detection skilled forgeries are included in the following sub-sections.Skilledforgeriesareproducedwithcloseimitations. It is very hard to visually differentiate a genuine signature and its skilled forgery as shown in Fig. 7 . The Table 6 shows the performance details in terms of false acceptance rate (FAR), false rejection rate (FRR) and equal error rate (EER) of some of the techniques discussed in this paper. As many of the researchers have used their own databases, it is not fare to compare the methods based on their performance alone. The following sub-sections discuss some of the prominent tech-niques reported in the literature for the verification of offline signatures. 8.1 Neural network based techniques Bajaj and Chaudhury [ 137 ] used different types of global features like envelop and projection profiles for offline signa-ture verification. The classification was done by feed-forward neural network (FFNN) classifiers, and the decisions were combined using a simple-layer feed-forward neural network (SLFFNN). Cordella et al. [ 138 ] proposed a hybrid multi-expert (ME) scheme based on cascaded two-stage classifiers for offline signature verification. The scheme used contour-based features and grey-level features in the first and second stages, respectively. The classification was performed using MLP in each stage. Dimauro et al. [ 139 ] used Fourier trans-form for extracting projection-based, slant-based and geo-metric-based features and Granlund descriptors for offline signature verification. The proposed ME system combined a holistic approach based on a Euclidean distance (ED) clas-sifier, a structural-based (SB) classifier and an ANN-based classifier. A voting strategy then combines the results from the three classifiers to reach the final decision.
Huang and Yan [ 140 ] proposed a scheme based on geometric features with different scales for offline signa-ture verification. Combining the decisions taken at each scale using an MLP classifier gives the similarity measure. Huang and Yan [ 141 ] also proposed another scheme where geometric and directional features were considered along with MLP and structural matching (SM) algorithms for signature verification. Santos et al. [ 142 ] used graphomet-ric features and MLP to verify offline signatures. Xiao and Leedham [ 143 ] used an MLP classifier along with direction-based grid features. High-pressure region is the feature used to detect skilled signature forgeries in the work of Sansone and Vento [ 144 ] where MLP is used for the purpose of veri-fication. Baltzakis and Papamarkos [ 145 ] proposed a system based on global, grid and texture features for offline signa-ture verification. For each one of these feature sets, a special two level Perceptron classification structure (one-class-one-network) has been implemented. The results of the first-level classifier are fed into a second-level radial base function neu-ral network (RBFNN) structure capable of taking a decision. Armand et al. [ 146 ] proposed a scheme based on radial basis function neural network (RBFNN) for signature verification using structural features. 8.2 DTW-based techniques Dynamic time warping (DTW) is dynamic programming method by which an optimal match between two given sequences (vectors) can be found under certain restrictions. The sequences (vectors) are  X  X arped X  nonlinearly to deter-mine a measure of their similarity [ 147 ]. The zero crossings of the curvature data obtained using wavelet transform are employed as features for matching signatures in the scheme proposed by Deng et al. [ 148 ]. The matching is performed using dynamic time warping (DTW). Fang et al. [ 149 ]used vertical projection profiles as features and DTW for match-ing signatures. Shanker and Rajagopalan [ 150 ] proposed a modified DTW algorithm that incorporates a stability factor for better verification of handprinted signatures. Vertical pro-jection profile of the signature is used as the feature vector for the same. Jayadevan et al. [ 151 ] considered the area formed by the matching path around the diagonal of the DTW-grid also for calculating the difference between the feature vectors of the test and stored samples. The features were extracted using discrete Radon transform in the same work. 8.3 HMM-based techniques For the verification, purpose Nel et al. in [ 152 ] extracted dynamic information like pen trajectory from the images of signatures. They assumed that a dynamic version of the static image is already available (obtained during an earlier regis-tration process). A hidden Markov model (HMM) is derived from the static image and is then matched with the dynamic version of the image. A Viterbi algorithm matches a dynamic exemplar to the HMM and determines the most likely state sequence that can be translated into the most likely pen tra-jectory. In the scheme of Justino et al. [ 153 ], simple, static and pseudodynamic features and multiple codebooks in an HMMframeworkwereusedforofflinesignatureverification. An HMM with a ring topology along with Radon transform-based features was used by Coetzer et al. in [ 154 ] and [ 155 ] for offline signature verification. A ring structured HMM is helpful in achieving rotational invariance in the verifica-tion process. Static features (related to the signature shape) and pseudo-dynamic features (related to the dynamics of the writing) were extracted locally from the signature images in the approach of Batista et al. [ 156 ]. A multiple-hypothesis principle is employed to select the most suitable solution for a given input sample from a set of different HMMs (left to right topology). 8.4 Fuzzy logic-based techniques Fuzzy logic is a form of many-valued logic derived from fuzzy set theory to deal with reasoning that is approximate rather than exact [ 157 ]. Velez at al. [ 158 ] introduced a fuzzy snake approach for offline signature verification problem by using only one training signature per person. The signature verification is performed in two stages. In the first stage, a created snake model is adjusted over the test signature image using a fuzzy approach. In the second stage, degree of similarity between the test signature and the snake model is measured using a Takagi X  X ugeno (TS) model. Madasu et al. [ 159 ] proposed a fuzzy-modelling approach for signature verification, where a well-defined fuzzification function with structural parameters was employed. The signature image was partitioned into a number of segments by a grid-based approach. For each segment, a normalized vector angle was considered as a feature. In the work of Ismail and Gad [ 160 ], handwritten Arabic signatures are recognized and verified using a grade of membership in the set of genuine samples. Fuzzy concepts are applied in the verification phase in mak-ing decisions. A set of global and local features is used for the same. Quek and Zhou [ 161 ] proposed a scheme based on fuzzy neural network (FNN) for offline signature ver-ification. Reference pattern-based features (RPBF), global baseline (GB), pressure features (PF) and slant features (SF) were considered as the features for the same. Hanmandlu et al. [ 162 ] proposed an approach based on fuzzy modelling that employs the Takagi X  X ugeno (TS) model with multiple rules for offline signature verification. Angle features extracted using a box approach were fuzzified by an exponential mem-bership function of the TS model. 8.5 SVM-based techniques A support vector machine (SVM) constructs a hyper-plane or set of hyper planes in a high or infinite dimensional space, which can be used for classification. A good separation is achieved by the hyper-plane which has the longest distance to the nearest training data points of any class [ 163 ]. Hairong et al. [ 164 ] proposed an SVM-based method for offline signa-ture verification. Static (moments, directions) and dynamic (stroke width distribution and grey distribution) features for extracted for the same. A comparison between HMM and SVM for offline signature verification was done by Justi-no et al. [ 153 ] in terms of learning and verification tasks. SVM showed better results for both the conditions. Static andpseudo-dynamicfeatureswereusedasthefeaturesforthe same. The scheme proposed by Vargas et al. [ 165 ] measures the variations in the grey level of the signature image using statistical texture features. The co-occurrence matrix (CM) and local binary pattern (LBP) are analysed and employed as features in the same paper. A support vector machine (SVM)-based model has been used for the purpose of veri-fication. Graphometric features considering the curvature of stroke segments are used in the work of Bertolini et al. [ 166 ]. An ensemble of SVM classifiers was built using a genetic algorithm, and different fitness functions were evaluated to drive the search. A dissimilarity representation is used in the approach proposed by Batista et al. [ 167 ]. The signature ver-ification is done in two stages where left to right HMMs are used in the first stage and SVMs along with random sub-spaces (RS) are used. In [ 168 ], pseudo-cepstral coefficients (PCC) are calculated to get information about the pressure distribution on a signature image. Least squares support vec-tor machines (LS-SVM) are used for the verification pur-pose. The research of Nguyen et al. [ 169 ] compared the performance of neural networks, and SVM classifiers using structural features and found that the performance of SVM was better than neural networks. Global features based on the boundary of a signature and its projections are used for enhancing the performance of SVM-based signature verifi-cation in [ 170 ]. In [ 171 ], gradient features are used along with SVM for offline signature verification. 8.6 Bayes classifier A Bayes classifier is a simple probabilistic classifier based on Bayes X  theorem (from Bayesian statistics) with strong assumptions. A naive Bayes classifier assumes that the pres-ence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature [ 172 ]. Xiao and Leedham [ 173 ] proposed a Bayesian network (BN) rep-resentation for offline handprinted signature verification. The proposed network is capable of capturing topological rela-tions among the components associated with the nodes. In the approach of Ruiz-del-Solar et al. [ 174 ], local interest points of the signature image are detected, then local descriptors are computed in the neighbourhood of these points. These descriptors are then compared using local and global match-ing procedures. Bayes classifier is used to carry out the final verification. Kalera et al. [ 175 ] proposed a scheme based on gradient, structural and concavity (GSC) features where the verification is done using Bayes classifier. 8.7 Other techniques AsetofperipheralfeaturesandaMahalanobisdistance-based (MD) classifier were considered by Fang and Tang [ 176 ]for offline signature verification. Mizukami et al. [ 177 ] proposed a Euclidean-distance-based scheme for offline signature ver-ification. The test image has to be compared with a genuine one for measuring the similarity. For offline signature veri-fication, Ramesh and Murty [ 178 ] used geometric, moment-based, envelope and wavelet features. Genetic Algorithms were used to weigh the pattern characterization capability of the individual feature components. A hybrid classification scheme is employed to reach the final decision on authen-ticity. Ueda [ 179 ] investigated pattern matching for offline signature verification. Signature strokes were thinned first for this purpose and then blurred by a fixed point-spread (FPS) function. Hairong et al. [ 180 ] proposed a probabilis-tic graphical model (PGM) capable of capturing the depen-dence and variations of signature landmark points. Chen and Srihari [ 181 ] proposed a graph matching approach for sig-nature verification using multi resolution shape (MRS) fea-tures. An approach based on visual perception is proposed by Sabourin et al. in [ 182 ] and [ 183 ] for offline signature verifi-cation. Local granulometric size distributions (LGSD) along with nearest neighbour classifier (NNC) and a minimum dis-tance classifier (MDC) were used for the same. A compo-nent-oriented approach for signature verification is proposed by Dimauro et al. in [ 132 ]. Each connected component of a signature is classified using a set of topological and spectral features. The verification step is carried out using a nearest neighbour approach in which if the number of true compo-nents in a signature is greater than a specific threshold, the signature is judged as a genuine specimen; otherwise, it is judged to be a forgery. For off-line signature verification in [ 184 ], Sabourin et al. investigated the use of shape matrices as a global shape factor, where the position of local measure-ments is taken into account. Zimmer and Ling in [ 185 ] and [ 186 ] proposed a hybrid verification system, where an online reference signature serves as a basis for extracting features from the off-line signature. A set of global and local features along with Euclidean distance (ED) classifiers were used for the verification process. 9 Recognition of payee name The payee-name field on a cheque is used to write the name of the person to whom the money is paid through that cheque. In almost all the countries, payee name (details) field is located above the legal amount field as shown in Fig. 1 .Asthelegal amount is very near to payee name field, it will be difficult to segment the two into separate fields in case of overlapping handwritten segments. The technique used for payee name recognition (word matching) in [ 124 ] is the same as that of [ 187 ] and [ 51 ]. If the payee name is machine printed, then a graph of the spotted words will be created and traversed to display a list of ranked payee names from the lexicon. It is claimed in [ 124 ] that a metadata analysis of the bank database identifies the stop-words, abbreviations and the misspelled words. To ensure accurate payee name matching, metadata tools should be provided to the banks to analyse their data on an ongoing basis.

In [ 8 ], the payee name field is located using the key phrase  X  X ay to the order of X  that always precedes the actual payee name. Then, the recognition of handwritten payee name is done using hidden Markov Models (HMMs). The extracted handwritten text is segmented into graphemes which can be smaller than characters. The graphemes after classification by a neural network (NN) form a class probability vector. The sequence of vectors is then matched with HMM mod-els of dictionary words (phrases). The best-matching word models generate the list of possible answers.

One of the difficulties in payee name recognition is the presence of aliases [ 8 ]. Aliases are different forms of a word (in particular, a name), which have identical semantic mean-ing. Some possible aliases are as follows:  X  X educed or abbre-viated form(s) of the prototype name X ,  X  X rototype name with suffix or prefix words X ,  X  X isspelled name X  and  X  X ermutated words in the name X . The presence of aliases reduces the rec-ognition rate and increases rejection rare. Two instruments named  X  X lias detector X  and  X  X lias generator X  are proposed in [ 8 ] to detect the presence of possible aliases in the name field and to overcome the difficulty. The  X  X lias detector X  evalu-ates the probability of input name being an alias. It is based on a neural network trained to distinguish aliases from non-aliases. The task of the  X  X lias generator X  is to produce the list of probable aliases of a given name. A name thesaurus is being used for the same.

The system described by Gorski in [ 8 ] has a name detec-tion process, besides the ordinary payee name recognition. Name detection is a search process, in which the system is supplied with a  X  X lack list X  of wanted people (e.g. terrorists, criminals, etc.). Then, every name recognized is matched with names from the black list and upon successful matching, the cheque goes to a special basket of items to be manually inspected. 10 Cheque processing industry During the last decade, automatic cheque processing has become an industrial problem. Some of the prominent ven-dors in the area of automatic bank cheque processing are  X  X 2iA X ,  X  X itek X ,  X  X arascript X  and  X  X oftPro X . The following subsectionsdiscusstheproductsandservicesofferedbythese vendors. 10.1 A2iA A2iA  X  X heckReader X  is capable of reading all types of writ-ing on cheques including printed, cursive or handwritten capital letters. For recognizing the  X  X ayee-name X  field, user-defined vocabularies up to thousands of entries are used. This will ensure optimal results on any types of cheques [ 188 ]. A2iA  X  X heckReader X  performs courtesy amount recognition and legal amount recognition independently. A2iA X  X  artificial intelligence and neural networks combine these results and decide on the returned values and confidence levels. A2iA  X  X heckReader X  is available in 6 languages (English, French, German, Italian, Portuguese and Spanish) and in 23 versions for countries like France, UK, Ireland, Italy, USA, Canada, Mexico, Brazil, Thailand, Hong Kong, Sweden, Netherlands, Portugal, New Caledonia, Australia, Germany, Chile, Bel-gium, Malaysia, Singapore, India and South Africa. A2iA  X  X heckReader X  yields 99% recognition accuracy for machine printed text and 96% for  X  X onstrained X  handwritten text [ 189 ].

A2iA  X  X heckReader X  has the facility to spot money laun-dering and terrorism financing. The software recognizes the name of the cheque X  X  drawer and payee and can spot fraudu-lent and money-laundering operations by validating cheques against various blacklists. A2iA  X  X heckReader X  recognizes cheques deposited at ATMs and completes the mandatory consistency controls. The software facilitates the address block printed on the cheque to be validated against the data supplied by the bank-card (Debit card) in order to verify the customer X  X  identity and provides entire cheque recognition, includingchequestockanalysis,counterfeitchequedetection and real-time acceptance at the ATM [ 189 ]. A2iA  X  X heck-Reader X  is also capable of recognizing the code line present at the bottom of the cheque. In order to comply with the U.S. Check 21 initiative, A2iA  X  X heckReader X  helps to determine the image quality, usability and validity. The presence of sig-natures can also be detected by an A2iA  X  X heckReader X . 10.2 Mitek The  X  X mageNet-Payments X  system of Mitek is capable of reading and processing personal (handwritten) and business (machine printed) cheques [ 190 ]. The system can automati-cally locate and read the date information, legal amount and courtesy amount. It is also capable of extracting names and addresses as needed. It validates the cheque image by auto-matically processing the code line present. It can assess doc-ument-image quality against the specified standards and is  X  X heck 21 X  compliant. It works with image replacement doc-uments (IRDs) as well. The recognition rate available is 85% for handwritten cheques and 95% for printed cheques. The vendor claims that the product reduces the manual data entry costs by 90%.
Mitek X  X  mobile-banking application  X  X obile Deposit X  allows consumers and business people to deposit cheques using their camera-equipped smart-phones [ 191 ]. Using their smart-phones as scanning devices, users can deposit cheques from anywhere at any time. Also, it supports the  X  X heck 21 X  industry standards for image quality. The application auto-matically corrects many different types of problems, such as photos of cheques that may be wrinkled, skewed, distorted or taken under poor lighting conditions. The other features are similar to  X  X mageNet-Payments X . 10.3 Parascript The three main products of Parascript related to automatic bank cheque processing are  X  X heckPlus X ,  X  X heckUltra X  and  X  X heckUsability X . Parascript  X  X heckPlus X  automates the rec-ognition of courtesy and legal amounts [ 192 ].  X  X heckPlus X  alsoreadsthepayeeline,codelineandchequenumberonper-sonal cheques; recognizes dates; detects signature presence; executes signature verification; and locates payer blocks on personal and business cheques. Currently, different ver-sions of the product are available for Argentinean, Austra-lian, Brazilian, Canadian, Chilean, French, Indian, Italian, Malaysian and Portuguese personal and business cheques. Parascript  X  X heckUltra X  not only reads courtesy amount, legal amount and code line on financial documents, but also analyses image integrity and detects defects in images of personal cheques, business cheques and image replace-ment documents (IRDs) [ 193 ].  X  X heckUsability X  solves the challenges of  X  X heck 21 X  by analysing personal and busi-ness cheques for scanning distortions and field entry mis-takes and provides an automatic analysis of field readability [ 194 ]. 10.4 SoftPro According to the  X 2003 Check Fraud Survey X  of the Ameri-can Bankers Association (ABA),  X  X orged Maker Signature X  and  X  X ounterfeit Items X  continue to be a favourite target of fraudsters [ 195 ]. Together, these two categories make up over 40% of cheque fraud attempts annually. The  X  X oft-pro X  group developed the  X  X raudOne X  cheque fraud solu-tion together with its technology partners. Its initial aim is to mine the digital artifacts on cheque images to uncover the fraud [ 195 ]. In co-operation with six of the US bank-ing industry X  X  leading institutions,  X  X oftpro X  defined cheque fraud prevention best-practice requirements in 2001 as part of the  X  X raudOne X  initiative. In many institutions, the product  X  X raudOne X  is successful in reducing the losses by 60% due to counterfeit and forged maker signature cheques. 11 Some observations A cheque amount processing system becomes commercially efficient only when the error rate is very low. So a cheque reader must be able to refuse to give an answer when the probability to make mistake is high. Human eyes can read a  X  X ejected X  cheque afterwards or other more advanced auto-mated approaches can be used. However, a cheque  X  X ead X  incorrectly is very difficult to deal with, in terms of costs and time involved to correct the mistake.

Ideally, automated recognition process should replace manual typing of information. In reality, this is true only to a certain extent. The reading ability of automatic reading sys-tems is still much lower than the reading abilities of human beings. However, it is definitely possible to reduce a part of manual work (typing) by automatic processing. As reported by Gorski in [ 8 ], the three main characteristics of a recog-nition system are:  X  X ead rate X (number of automatically read items / total number of items),  X  X ubstitution rate X (number of incorrectly read items / number of read items) and  X  X ecogni-tion rate X (number of correctly read items / total number of items). Commercial people prefer to speak about the  X  X ead rate X  (as it is connected with labour saving), while the sci-entific community often uses  X  X ecognition rate X , as it shows recognition ability of the system.

Sometimes, detection processes are required to find the absence of any mandatory cheque fields [ 8 ]. One of the traditional detection tasks is to find cheques without payer X  X  signature. Such cheques are not valid and require human inspection. In general, a detection process should select a set of suspicious items for human inspection. The efficiency of a detection process can be characterized by two comple-mentary terms:  X  X etection rate X  (number of detected target items / total number of target items) and  X  X uspect rate X  (num-ber of suspicious items / total number of items). Lower the suspect rate, lower will be the cost of the detection process. Higher the detection rate, higher will be the revenue from the process implementation.

From the literature, it is clear that almost all the authors have used their own databases for the purposes of testing and performance evaluation. An attempt has been made in [ 196 ] to create a database of 3000 Arabic bank cheques with data tagging. The paper also describes a validation procedure including grammars and algorithms to verify the correctness of the tagging process. Impedovo et al. in [ 197 ] presented a database that contains more than 146,000 images of isolated digits, characters, words and signatures. This database can serve as a basis for research on automatic processing of bank cheques from the countries belonging to European Union.
There is no fixed format for cheques even in a country itself. Each bank has its own standard; so the cheques of different banks differ not only in background, but also in type and position of the machine printed and handwritten information. The area of interest should be located first in those systems, which do not depend on specific cheque for-mats. Location searching is difficult for cheques with poor scan quality. Failure in location searching will present false samples to different recognition modules. A hybrid model based on a combination of an orthogonal Gaussian mixture model (OGMM) and a multi-layer perceptron (MLP) is pro-posed in [ 198 ] for locating and recognizing machine printed numeral recognition.

There are some language-specific challenges in recogniz-ing handwritten amounts and dates present on bank cheques. Recognizing Italian legal amount is tough as the entire legal amount is written as a single word without using any delim-iters [ 83 ]. Although the lexicon size of legal amount words is limited in countries like Brazil, common sub-strings present in such words can affect the performance of the recognizer [ 199 ]. Even though the vocabulary is limited in Chinese bank cheques, the grammar is very rigid [ 106 ]. Also, the number of numerals may differ in the presence or absence of a hor-izontal stroke (line). A problem may arise when the stroke appears as a short line. It is difficult to decide whether it is the presence of line or noise.

Magnetic ink character recognition (MICR) technology based on magnetic approach has widely been used for rec-ognizing the code lines (cheque number and bank branch code) present on cheques. An OCR approach is used in [ 200 ] as MICR character set uses a special type font readable by human being. Exterior character profile is used as the feature for each symbol, and sum-of-squared error (SSE) is used to measure the distance between two feature vectors for classi-fication.

There are some published works on the extraction and rec-ognition of seal and logo information from images of bank cheques. Seal imprints are extracted from images of Japanese bank cheques using histogram-based colour information in [ 201 ]. Local and global features including coordinates of out-ermost points of imprint strokes, line width of strokes, mean radius of an outer frame figure from its centroid, average line width of a seal imprint are extracted. In the verification stage, an algorithm based on local and global features of seal imprint is used first. Another algorithm uses a special cor-relation based on a global approach. The two algorithms are then combined in the multi-expert system by a voting strat-egy. The approach presented in [ 202 ] employs mathematical morphology to automatically locate and extract logos from the image of Brazilian bank cheques. Subtracting the bank cheque grey-scale image from the image resulting from the Fillhole morphological operator does the removal of artistic background from the logo information.

Contextual analysis is also critical in controlling the error rate of the entire system [ 120 ]. The contextual information for each field can be expressed as a set of specific rules. For example, by comparing the recognized legal and courtesy amount, the error rate can be further reduced. For the date field, contextual information can be the current month and year. Sometimes, the handwritten information contains bro-ken characters and symbols, which need to be repaired. Occa-sionally, the handwritten text regions may overlap each other, and it will be difficult to segment and recognize them. The processing of skewed handwritten text regions is also diffi-cult if the image is not skew free.

In case of the recognition of courtesy amount in a hand-written personal cheque, the extraction and recognition of the fractional part is difficult as some writers often include frac-tional parts on cheques [ 120 ]. Many times, the connected components make recognition almost impossible. Another difficult aspect is the way the double zero  X 00 X  is written. It is often recognized as  X 50 X  or  X 60 X  because of the connect-ing link at the top. The date segmentation and recognition task is the most difficult one as there are many formats to write a date. A density factor and a regularity measure must be computed to detect the presence of signature on a cheque image. Stamped directives present on a cheque should also be reliably detected and processed.

The common approaches employed for the extraction of user-entered information are based on the detection of base lines, which point to locations where handwritten informa-tion can be found. In other cases, the use of special symbols including currency indicators that point to the area on the image where the courtesy amount is written. One problem of eliminating the base lines is the loss of information where text and line intersect each other. Even though the lines can be successfully eliminated, it is necessary to reconstruct the handwritten information at the points where they overlap. It would be very difficult to recognize the modified (dam-aged) handwritten information if the reconstruction is not done properly.

One of the main difficulties in developing an effective cheque reading system is the high degree of variability and uncertainty in the handwritten date information. People usu-allywritethedatezonesinfreestylesthatlittleaprioriknowl-edge and few reliable rules can only be applied to identify the layout of a date pattern [ 70 ]. As there are many banks in this world and each one of them uses a specific back-ground pattern. Another difficulty is to develop an automatic binarization algorithm capable of calculating threshold value suitable for different background patterns.

In [ 98 ] and [ 99 ], it is shown that a character recognizer trained on data from a particular country can be weak in processing data from another country, even if they are cul-turally close and use same language. Structural complexity of characters and variability of writing styles are the biggest challenges in recognizing legal amounts present on a bank cheque. Sometimes, the courtesy amounts cannot be recog-nized in advance, the recognition of legal amounts needs to be conducted even without any reference values.
The advances in computer and information technology allow banks to develop automatic recognition and verifica-tion systems [ 37 ]. In the research and development of such a system, a big cheque-image database, which contains hun-dreds of cheque images, is usually used for training or test-ing. Although the colour image of a cheque contains richer information than that of its grey-scale image, it will consume more memory to process a colour cheque and more space to store it. Therefore, the main trend of the current research on automatic processing of bank cheques is based on the grey scale or binarized cheque images [ 19 ]. 12 Conclusions Automatic cheque processing is an interesting field of research from both scientific and commercial points of view. The variability of the size, structure and background of bank cheques, together with the complexity of the character rec-ognition, makes the development of universal algorithms and strategies extremely challenging for automatic bank cheque processing. The survey presented in this paper will give the reader the state of the art information in cheque processing. The performance details shown in the tables of the paper are clear indications of awareness and interest that many insti-tutions at national and international levels give to automatic bank cheque processing. It is also apparent that issue like the recognition of payee details is still to be explored fur-ther. The time required for the entire process of recognition by different methods also needs to be studied further, and it should be lesser than the time required for visual recogni-tion, typing and verification. In real conditions of a cheque reader application, the price of a recognition error is greater than the savings from a correctly recognized cheque. Thus, a cheque processing system becomes commercially efficient only when the error rate is very low. A cheque reader must be able to refuse to give an answer (or reject) when the prob-ability to make mistakes is high. The reliability [recognition rate/ (100% X  X ejection rate)] of a cheque processing system can be used to test its efficiency. As cheques are written in all major languages, almost all the major cheque processing systems reported so far have used their own databases for experimentation, so their performances cannot be compared effectively. But from the industrial point of view, the A2iA  X  X heckReader X  outperforms the others in accuracy. Major-ity of the researchers used a combination of two or more classifiers for the recognition purpose and many of them are based on hidden Markov models and neural networks. As the technology advanced, outstation cheque clearances even in developing countries can be made faster by just sending the image of the cheque to the payer X  X  bank through cheque truncation systems. The interoperability of such cheque pro-cessing systems with the existing banking software is also an area of great significance.
 References
