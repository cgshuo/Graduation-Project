 PAULO SHAKARIAN and V. S. SUBRAHMANIAN, University of Maryland There are numerous applications where we wish to draw geospatial inferences from observations. For example, criminologists [Rossmo and Rombouts 2008; Brantingham and Brantingham 2008] have found that there are spatial relationships between a se-rial killer X  X  house (the geospatial inference we wish to make), and locations where the crimes were committed (the observations). A marine archaeologist who finds parts of a wrecked ship or its cargo at various locations (the observations) is interested in de-termining where the main portion of the wreck lies (the geospatial inference). Wildlife experts might find droppings of an endangered species such as the Malayan sun bear (observations) and might want to determine where the bear X  X  den is (the geospatial inference to be made). In all these cases, we are trying to find a single location that best explains the observations (or the k locations that best explain the observations). There are two common elements in such applications.

First, there is a set O of observations of the phenomena under study. For the sake of simplicity, we assume that these observations are points where the phenomenon being studied was known to have been present. Second, there is some domain knowledge D specifying known relationships between the geospatial location we are trying to find and the observations. For instance, in the serial killer application, the domain knowledge might tell us that serial killers usually select locations for their crimes that are at least 1.2km from their homes and at most 3km from their homes. In the case of the sun bear, the domain knowledge might state that the sun bear usually prefers to have a den in a cave, while in the case of the wreck, it might be usually within a radius of 10 miles of the artifacts that have been found.

The geospatial abduction problem ( GAP for short) is the problem of finding the most likely set of locations that is compatible with the domain knowledge D and that best  X  X xplains X  the observations in O . To see why we look for a set of locations, we note that the serial killer might be using both his home and his office as launching pads for his attacks. In this case, no single location may best account for the observations. In this paper, we show that many natural problems associated with geospatial abduction are NP-Complete, which causes us to resort to approximation techniques. We then show that certain geospatial abduction problems reduce to several well-studied combi-natorial problems that have viable approximation algorithms. We implement some of the more viable approaches with heuristics suitable for geospatial abduction, and test them on a real-world data-set. The organization and main contributions of this article are as follows.  X  Section 2 formally defines geospatial abduction problems ( GAPs for short), and
Section 3 analyzes their complexity.  X  Section 4 develops a  X  X aive X  algorithm for a basic geospatial abduction problem called k -SEP and shows reductions to set-covering, dominating set, and linear-integer programming that allow well-known algorithms for these problems to be applied to GAPs .  X  Section 5 describes two greedy algorithms for k -SEP and compares them to a reduc-tion to the set-covering problem.  X  Section 6 describes our implementation and shows that our greedy algorithms outperform the set-covering reduction in a real-world application on identifying weapons caches associated with Improvised Explosive Device (IED) attacks on US troops in Iraq. We show that even if we simplify k -SEP to only cases where k -means classification algorithms work, our algorithms outperform those.  X  Section 7 compares our approach with related work. Throughout this article, we assume the existence of a finite, 2-dimensional M  X  N space S 1 for some integers M , N  X  1 called the geospatial universe (or just universe). Each point p  X  S is of the form ( x , y )where x , y are integers and 0  X  x  X  M and 0  X  y  X  N . We assume that all observations we make occur within space S .We use the space shown in Figure 1 throughout this article to illustrate the concepts we introduce. We assume that S has an associated distance function d which assigns a nonnegative distance to any two points and satisfies the usual distance axioms. 2 Definition 2.1 Observation .An observation O is any finite subset of S .
 Consider the geospatial universe shown in Figure 1. In the serial killer application, the red dots would indicate the locations of the murders, while in the shipwreck example, they would indicate the locations where artifacts were found. We wish to identify the killer X  X  location (or the sunken ship or the sun bear X  X  den).

As mentioned earlier, there are many constraints that govern where such locations might be. For instance, it is unlikely that the sunbear X  X  den (or the killer X  X  house or office) is in the water, while the sunken ship is unlikely to be on land.
Definition 2.2 Feasibility predicate .A feasibility predicate feas is a function from S to { TRUE , FALSE } .
 Thus, feas ( p )= TRUE means that point p is feasible and must be considered in the search. Figure 1 denotes infeasible places via a yellow square. Throughout this arti-cle, we assume that feas is an arbitrary, but fixed predicate. 3 Further, as feas is defined as a function over { TRUE , FALSE } , it can allow for user input based on analytical pro-cesses currently in place. For instance, in the military, analysts often create  X  X COO X  overlays where  X  X estricted terrain X  is deemed infeasible [US Army 1994]. We can also easily express feasibility predicates in a Prolog-style language; we can easily state (in the serial killer example) that point p is considered feasible if p is within R units of distance from some observation and p is not in the water. Likewise, in the case of the sun bear example, the same language might state that p is considered feasible if p is within R 1 units of distance from marks on trees, within R 2 units of scat, and if p has some landcover that would allow the bear to hide. A Prolog-style language that can express such notions of feasibility is the hybrid knowledge base paradigm [Lu et al. 1996] in which Prolog style rules can directly invoke a GIS system.

Definition 2.3 (  X ,  X  ) Explanation . Suppose O is a finite set of observations, E is a finite set of points in S ,and  X   X  0,  X &gt; 0 are some real numbers. E is said to be an (  X ,  X  ) explanation of O iff:  X  p  X  E implies that feas ( p )= TRUE , that is, all points in E are feasible and too far from some point in E .

Thus, an (  X ,  X  ) explanation is a set of points (e.g., denoting the possible locations of the home/office of the serial killer or the possible locations of the bear X  X  den). Each point must be feasible and every observation must have an analogous point in the explanation which is neither too close nor too far.

Given an (  X ,  X  )explanation E , there may be an observation o  X  O such that there are two (or more) points p 1 , p 2  X  E satisfying the conditions of the second bullet above. If
E is an explanation for O ,a partnering function  X  E is a function from O to E such partnering function  X  E . We now present a simple example of (  X ,  X  )explanations.
Example 2.1. Consider the observations in Figure 1 and suppose  X  =0 , X  =3.Then (0 (13 , 9) } is also an (0 , 3) explanation.
 The basic problem that we wish to solve in this article is the following. The Simple (  X ,  X  ) Explanation Problem (SEP) INPUT: Space S ,aset O of observations, a feasibility predicate feas , and numbers  X   X  0,  X &gt; 0.
 OUTPUT:  X  X es X  if there exists an (  X ,  X  )explanationfor O  X  X  X o X  X therwise.
A variant of this problem is the k -SEP problem which requires, in addition, that E an explanation E that is  X  X est X  according to some cost function.

Definition 2.4 Cost function  X  . A cost function  X  is a mapping from explanations to nonnegative reals.

We will assume that cost functions are designed so that the smaller the value they return, the more desirable an explanation is. Some example cost functions are given below. The simple one below merely looks at the mean distances between observations and their partners.

Example 2.2 Mean-distance . Suppose S , O , feas , X , X  are all given and suppose E is an (  X ,  X  )explanationfor O and  X  E is a partnering function. We could initially set the cost of an explanation E (with respect to this partnering function) to be: Suppose ptn ( E ) is the set of all partner functions for E in the above setting. Then we can set the cost of E as: This definition removes reliance on a single partnering function as there may be sev-eral partnering functions associated with a single explanation. We illustrate this defi-nition using our sun bear example.
 Example 2.3. Wildlife experts have found droppings and other evidence of the Malayan sun bear in a given space, S , depicted in Figure 2. Points { o 1 , o 2 , o 3 } indi-cate locations of evidence of the Malayan sun bear (we shall refer to these as set O ). around each element of O indicate the distance  X  =1 . 7km and  X  =3 . 7km. The set { p that observation o 2 can be partnered with either point. If we are looking to minimize for o 2 such that the distance is minimized.
 We now define an  X  X ptimal X  explanation as one that minimizes cost.

Definition 2.5. Suppose O is a finite set of observations, E is a finite set of points in S ,  X   X  0,  X &gt; 0 are some real numbers, and  X  is a cost function. E is said to be an optimal (  X ,  X  ) explanation iff E is an (  X ,  X  )explanationfor O and there is no other (  X ,  X  )explanation E for O such that  X  ( E ) &lt; X  ( E ).
 We present an example of optimal (  X ,  X  ) explanations in the following.

Example 2.4. Consider the sun bear from Example 2.3 whose behavior is depicted in Figure 2 (left). While { p 3 , p 6 } is a valid solution for the k -SEP problem ( k =2), it does not optimize mean distance. In this case the mean distance would be 3km. However, the solution { p 3 , p 7 } provides a mean-distance of 2 . 8km.

Suppose we are tracking a serial killer who has struck at locations O = { o 1 , o 2 } .The is depicted in Figure 2 (right). Based on historical data, we know that serial killers strikes are at least 1km away from a safe-house and at most 2km from the safe house (  X  =1,  X  = 2). Thus, for k = 2, any valid explanation of size 2 provides an optimal solution wrt mean-distance as every feasible location for a safe-house is within 2km of a crime scene.
 We are now ready to define the cost-based explanation problem.
 The Cost-based (  X ,  X  ) Explanation Problem.
 INPUT: Space S ,aset O of observations, a feasibility predicate feas , numbers  X   X  0,  X &gt; 0, a cost function  X  and a real number v&gt; 0.
 OUTPUT:  X  X es X  if there exists an (  X ,  X  )explanation E for O such that  X  ( E )  X  v , X  X o X  otherwise.

It is easy to see that standard classification problems like k -means 4 can be captured within our framework by simply assuming that  X  =0,  X &gt; max ( M , N ) 2 and that all points are feasible. In contrast, standard classification algorithms cannot take feasi-bility into account, and this is essential for the above types of applications. SEP can be easily solved in PTIME. Given a set O of observations, for each o  X  O ,let P  X  X es. X  We call this algorithm STRAIGHTFORWARD-SEP . Another algorithm would merely find the set F of all feasible points and return  X  X es X  iff for every observation o , there is at least one point p  X  F such that  X   X  d ( p , o )  X   X  .Inthiscase, F is the explanation produced, but it is a very poor explanation. In the serial killer example, F merely tells the police to search all feasible locations without trying to do anything intelligent. k -SEP allows the user to constrain the size of the explanation so that  X  X hort and sweet X  explanations that are truly meaningful are produced. The following result states that k -SEP is NP-Complete; the proof is a reduction from Geometric Covering by Discs ( GCD ) [Johnson 1982].
 T HEOREM 3.1. k-SEP is NP-Complete .

In the associated optimization problem with k -SEP,wewishtoproduceanexpla-nation of minimum cardinality. Note that minimum cardinality is a common criterion for parsimony in abduction problems [Reggia and Peng 1990]. We shall refer to this problem as MINSEP . This problem is obviously NP-hard by Theorem 3.1. We can ad-just STRAIGHTFORWARD-SEP to find a solution to MINSEP by finding the minimum hitting set of the P o  X  X .

Example 3.1. Consider the serial killer scenario in Example 2.4 and Figure 2 (right). Crime scene (observation) o 1 can be partnered with two possible safe houses { p the potential safe house located at p 2 is in both sets. Therefore, p 2 is an explana-tion for both crime scenes. As this is the only such point, we conclude that { p 2 } is the minimum-sized solution for the SEP problem. However, while it is possible for STRAIGHTFORWARD-SEP to return this set, there are no assurances it does. As we saw in Example 2.4, E = { p 1 , p 2 } is a solution to SEP, although a solution with lower cardinality ( { p 2 } ) exists. This is why we introduce the MINSEP problem.
With the complexity of k -SEP, the following corollary tells us the complexity class of the Cost-based Explanation problem. We show this reduction by simply setting the cost function  X  ( E )= | E | .
 C OROLLARY 3.1. Cost-based Explanation is NP-Complete.

As described earlier, MINSEP has the feel of a set-covering problem. Although the generalized cost-based explanation cannot be directly viewed with a similar intuition variant of the Cost-based problem that does. We introduce weighted SEP, or WT-SEP here.
 Weighted Spatial Explanation (WT-SEP) INPUT: A space S ,aset O of observations, a feasibility predicate feas , numbers  X   X  0,  X &gt; 0, a weight function c : S  X  , and a real number v&gt; 0.
 OUTPUT:  X  X es X  if there exists an (  X ,  X  )explanation E for O such that p  X  E c ( p )  X  v  X   X  X o X  otherwise.

In this case, we can easily show NP-Completeness by reduction from k -SEP, we simply set the weight for each element of S to be one, causing p  X  E c ( p ) to equal the cardinality of E .
 C OROLLARY 3.2. WT-SEP is NP-Complete .
 Cost-based explanation problems presented in this section are very general. While the complexity results hold for an arbitrary function in a general case, we also consider specific functions as well. In the following, we present the total-distance minimization explanation problem ( TD-SEP ). This is a problem where we seek to minimize the sum of distances between observations and their closest partners while imposing a restriction on cardinality.
 Total Distance Minimization Explanation Problem (TD-SEP) For space S ,let d : S  X  S  X  be the Euclidean distance between two points in S . INPUT: A space S ,aset O of observations, a feasibility predicate feas , numbers  X   X  0,  X &gt; 0, positive integer k &lt; | O | , and real number v&gt; 0.
 OUTPUT:  X  X es X  if there exists an (  X ,  X  )explanation E for O such that | E | = k and T HEOREM 3.2. TD-SEP is NP-Complete .

The NP-hardness of the TD-SEP is based on a reduction from the k -Median Prob-lem [Papadimitriou 1981]. This particular reduction (details in the appendix) also illustrates how the k -median problem is a special case of GAPs ,but k -median prob-lems cannot handle arbitrary feasibility predicates of the kind that occur in real-life geospatial reasoning. The same argument applies to k -means classifiers as well. This section presents four exact approaches to solve k -SEP and WT-SEP. First, we pro-vide an enumerative approach that exhaustively searches for an explanation. Then, we show that the problem reduces to set-cover, dominating set, and linear-integer programming. Existing algorithms for these problems can hence be used directly. Throughout this section, we shall use the symbols to represent the bound on the number of partners that can be associated with a single observation and f to repre-sent the bound on the number of observations supported by a single partner. Note that both values are bounded by  X  (  X  2  X   X  2 ), however they can be much less in practice; specifically, f is normally much smaller than . We now show correctness of NAIVE-KSEP-EXACT . This algorithm provides an exact solution to k -SEP but takes exponential time (in k ). The algorithm first identifies a set L of all elements of S that could be possible partners for O . Then, it considers all subsets of L of size less than or equal to k . It does this until it identifies one such subset as an explanation.
 P ROPOSITION 4.1. If there is a k-sized simple (  X ,  X  ) explanation for O ,then NAIVE-KSEP-EXACT returns an explanation. Ot herwise, it returns NO.
 Finally, we have the complexity of the algorithm.

P ROPOSITION 4.2. The complexity of NAIVE-KSEP-EXACT is O ( 1 ( k  X  1)! (  X  (  X  2  X 
An exact algorithm for the cost-based explanation problems follows trivially from the NAIVE-KSEP-EXACT algorithm by adding the step of computing the value for  X  for each combination. Provided this computation takes constant time, this does not affect the O ( 1 ( k  X  1)! (  X  (  X  2  X   X  2 ) | O | ) ( k +1) ) runtime of that algorithm. We now show that k -SEP polynomially reduces to an instance of the popular set-covering problem [Karp 1972] which allows us to directly apply the well-known greedy algorithm reviewed in Paschos [1997]. Set-Cover is defined as follows.
 The Set-Cover Problem (Set-Cover) INPUT: Set of elements, E and a family of subsets of E , F  X { S 1 ,..., S max } ,and positive integer k .

Through a simple modification of NAIVE-KSEP-EXACT , we can take an instance of k -SEP and produce an instance of Set-Cove r. We run the first four steps, which only takes O (  X | O | ) time by the proof of Proposition 4.2.
 T HEOREM 4.1. k-SEP polynomially reduces to Set-Cover.

Example 4.1. Consider the serial killer scenario in Example 2.4 and Figure 2 (right). Suppose we want to solve this problem as an instance of k -SEP by a reduc-tion to set-cover. We consider the set of crime-scene locations, O  X { o 1 , o 2 } as the set we wish to cover. We obtain our covers from the first four steps of NAIVE-KSEP-EXACT . Let us call the result list L . Hence, we can view the values of the elements in L as p , p The traditional approach for approximation of set-cover has a time complexity of 1997]. As f is the quantity of the largest number of observations supported by a single partner, the approximation ratio for k -SEP using a greedy-scheme after a reduction from set-cover is 1 + ln( f ). The NAIVE-KSEP-SC algorithm below leverages the above reduction to solve the k -SEP problem.

P ROPOSITION 4.3. NAIVE-KSEP-SC has a complexity of O (  X  f  X | O | 2 ) and an approximation ratio of 1+ln( f ) .

P ROPOSITION 4.4. Asolution E to NAIVE-KSEP-SC provides a partner to every observation in O if a partner exists  X  otherwise, it returns IMPOSSIBLE.

The algorithm NAIVE-KSEP-SC is a naive, straight-forward application of the O ( | E | X  |
F | X  size ) greedy approach for set-cover as presented in Paschos [1997]. We note that it avoiding the cost of iterating through all possible partners in the inner-loop. Algorithm 3 ( KSEP-TO-DOMSET )
In addition to the straightforward greedy algorithm for set-covering, there are sev-eral other algorithms that provide different time complexity/approximation ratio com-binations. However, with a reduction to the set-covering problem we must consider the result of Lund and Yannakakis [1994], which states that set-cover cannot be approxi-family F ) unless NP  X  DTIME [ n polylog n ].

A reduction to set-covering has the advantage of being straightforward. It also al-lows us to leverage the wealth of approaches developed for this well-known problem. In the next section, we show that k -SEP reduces to the dominating set problem as well. We then explore alternate approximation techniques based on this reduction. We show below that k -SEP also reduces to the well known dominating set problem (DomSet) [Garey and Johnson 1979] allowing us to potentially leverage fast algo-rithms such as the randomized-distributed approximation scheme in Jia et al. [2002]. DomSet is defined as follows.
 Dominating Set (DomSet) INPUT: Graph G =( V , E ) and positive integer K  X | V | .
 OUTPUT:  X  X es X  if there is a subset V  X  V such that | V | X  K and such that every vertex v  X  V  X  V is joined to at least one member of V by an edge in E .

As the dominating set problem relies on finding a certain set of nodes in a graph, then, unsurprisingly, our reduction algorithm, Algorithm 3, takes space S ,an observation set O , feasibility predicate feas , and numbers  X ,  X  and returns graph G O based on these arguments.

We now present an example to illustrate the relationship between a dominating set of size k in G O and a k -sized simple (  X ,  X  )explanationfor O . The following example illustrates the relationship between a k -SEP problem and DomSet.
 Example 4.2. Consider the serial killer scenario in Example 2.4, pictured in Figure 2 (right). Suppose we want to solve this problem as an instance of k -SEP by a reduction to DomSet. We want to find a 1-sized simple (  X ,  X  ) explanation (safe-house) for O (the set of crime scenes, { o 1 , o 2 } ). Suppose that after running an algorithm such as STRAIGHFORWARD-SEP , we find that { p 1 , p 2 , p 3 } are elements of S that are fea-distance of  X ,  X  from o 2 .Werun KSEP-TO-DOMSET , which creates graph G O . Refer to Figure3forthegraph.Wecanseethat { p 2 } is a 1-sized dominating sets for G O ,hence a 1-sized explanation for O .

We notice that the inner loop of KSEP-TO-DOMSET is bounded by O ( )operations and the outer loop will iterate | O | times. Thus, the complexity of KSEP-TO-DOMSET is O (  X | O | ).
 P ROPOSITION 4.5. The complexity of KSEP-TO-DOMSET is O (  X | O | ) .
 Example 4.2 should give us some intuition into why the reduction to DomSet works. We provide the formal proof in the Appendix.
 T HEOREM 4.2. k-SEP is polynomially reducible to DomSet .

The straightforward approximation scheme for DomSet is to view the problem as an instance of Set-Cover and apply a greedy algorithm. The reduction would view the set of vertices in G O as the elements, and the family of sets as each vertex and its neighbors. This results in both a greater complexity and a worse approximation ratio when compared with the reduction directly to Set-Cover.

P ROPOSITION 4.6. Solving k-SEP by a reduction to DomSet using a straight-forward greedy approach has time-complexity O ( 3  X  f  X | O | 2 ) and an approximation ratio bounded by O (1 + ln(2  X  f  X  )).

There are other algorithms to approximate DomSet [Jia et al. 2002; Kuhn and Wat-tenhofer 2003]. By leveraging Jia et al. [2002], we can obtain an improved complexity while retaining the same approximation ratio as the greedy approach.

P ROPOSITION 4.7. Solving k-SEP by a reduction to DomSet using the distributed, randomized algorithm presented in Jia et al. [2002] has a time complexity O (  X | O | + ln(2  X   X | O | )  X  ln(2  X   X  f )) with high probability and approximation ratio of O (1 + ln(2  X  f  X  )).

Hence, although a reduction to dominating set generally gives us a worse approx-imation guarantee, we can (theoretically) outperform set-cover with the randomized algorithm for dominating set in terms of complexity. Given an instance of k -SEP, we show how to create a set of integer constraints that if solved, will yield a solution to the problem.
 Definition 4.1 OPT-KSEP-IPC .The k -SEP integer programming constraints ( OPT-KSEP-IPC ) require the following information, obtained in O ( | O | X   X  (  X  2  X   X  2 )time:  X  X et L be the set of all possible partners generated in the first four steps of NAIVE-
KSEP-EXACT . partner of the i th observation (this is also generated in the first four steps of NAIVE-KSEP-EXACT ).
 For each p j  X  L ,let x j  X  X  0 , 1 } . x j =1iff p j is in E .
 Then KSEP-IPC consists of the following: Minimize p (1)  X  o i  X  O , p (2)  X  p j  X  L , x j  X  X  0 , 1 } (for the relaxed linear program: x j  X  1)
P ROPOSITION 4.8. OPT-KSEP-IPC consists of O ( | O |  X  (  X  2  X   X  2 )) variables and O ( | O | X   X  (  X  2  X   X  2 )) constraints.
 P ROPOSITION 4.9. For a given instance of the optimization version k-SEP, if OPT-KSEP-IPC is solved, then p
Example 4.3. Consider the serial killer scenario in Example 2.4, pictured in Fig-ure 2 (right). Suppose we want to solve this problem as an instance of MINSEP .We would set up the constraints as follows: where x 1 , x 2 , x 3  X  X  0 , 1 } Obviously, setting x 1 =0 , x 2 =1 , x 3 = 0 provides an optimal solution. Hence, as x 2 is the only nonzero variable, p 2 is the explanation for the crime scenes.

A solution to the constraints OPT-KSEP-IPC can be approximated using the well-known  X  X ounding X  technique [Hochbaum 1982; Vazirani 2004] that relaxes constraints. We present an OPT-KSEP-IPC using rounding.

P ROPOSITION 4.10. NAIVE-KSEP-ROUND returns an explanation for O that is within a factor from optimal, where is the maximum number of possible partners associated with any observation.

There are several things to note about this approach. First, it can be easily adapted to many of the weighted variants -such as WT-SEP. Second, we note that the rounding algorithm is not a randomized rounding algorithm  X  which often produces a solution rithm guarantees that all of the observations will be covered (if an explanation exists). Finally, this approach allows us to leverage numerous software packages for solving linear and linear-integer programs. In this section, we introduce a greedy approximation scheme for the optimization ver-sion of k -SEP that has a lower time-complexity than NAIVE-KSEP-SC but still main-tains the same approximation ratio. Our GREEDY-KSEP-OPT1 algorithm runs in lin-ear time with respect to O . The key intuition is that NAIVE-KSEP-SC iterates through O (  X | O | ) possible partners in line 2. Our algorithm first randomly picks an observa-tion and then greedily selects a partner for it. This results in the greedy step iterating through only O ( )partners.

Example 5.1. Consider the sun bear from Example 2.3 and Figure 2. After initializ-ing the necessary data structures in lines 1 X 3, GREEDY-KSEP-OPT1 iterates through the observations in O where the associated position in O is TRUE . Suppose the algo-rithm picks o 1 first. It now accesses the list pointed to from OBS [ o 1 ]. This gives us selection outlined in line 2 of NAIVE-KSEP-SC , the algorithm iterates through these points, visiting the list of observations associated with each one in the matrix array M .
First, the algorithm accesses the list pointed to by M [ p 1 ]. Figure 4 (left) shows the observations considered when p 1 is selected. As there is only one observation in list M [ p 1 ] whose associated Boolean in O is TRUE , the variable cur size is set to 1 (see line 2 of NAIVE-KSEP-SC ). cur ptr is then set to M [ p 1 ].
 Now we consider the next element, p 2 . Figure 4 (right) shows the list pointed to by M [ p 2 ]. As M [ p 2 ] points to more observations whose associated O Boolean is TRUE , we update cur size to 2 and cur ptr to M [ p 2 ].

The algorithm then iterates through p 3 and p 4 , but finds they do not offer more observations than p 2 .Hence, p 2 is added to the solution set ( E ). The algorithm updates the array of Booleans, O and sets O [ o 1 ]and O [ o 2 ]to FALSE (depicted by X X  X  over those observations in subsequent figures). numObs is decremented by 2.

Now, we enter the second iteration of line 4. The only element for the algorithm to { p As OBS [ o 2 ]= FALSE ,itonlyconsiders o 3 when computing this size .
 When the algorithm finishes its consideration of all the elements pointed to by ners to more available observations than p 6 (in our implementation of this algorithm, we use a coin-flip to break ties among partners with the same number of observations). GREEDY-KSEP-OPT1 then adds p 6 to E and terminates. The final solution returned, { p
P ROPOSITION 5.1 C OMPLEXITY OF GREEDY-KSEP-OPT1 . GREEDY-KSEP-OPT1 has a complexity of O (  X  f  X | O | ) and an approximation ratio of 1+ln( f ) .
P ROPOSITION 5.2. GREEDY-KSEP-OPT1 returns a | E | -sized (  X ,  X  ) explanation for O .
 GREEDY-KSEP-OPT1 returns IMPOSSIBLE if there is no explanation for O .

We can bound the approximation ratio for GREEDY-KSEP-OPT1 by O (1 + ln( f )), as it is still essentially a greedy algorithm for a covering problem. The main difference between GREEDY-KSEP-OPT1 is the way it greedily chooses covers (partners). This algorithm randomly picks an uncovered observation in each loop and then greedily chooses a cover that covers that observation. Improving the accuracy of this algo-rithm (in practice) is tied directly to the selection criteria used to pick observations, which is random in GREEDY-KSEP-OPT1 . In Section 5.2 we develop an algorithm that  X  X martly X  picks observations with a dynamic ranking scheme while maintaining a time complexity lower than the stan dard set-covering approach. GREEDY-KSEP-OPT1 randomly selects observations although subsequent partner se-lection was greedy. It is easy to implement an a-priori ranking of observations based on something like the maximum number of other observations which share a partner with it. Such a ranking could be implemented at the start of GREEDY-KSEP-OPT1 with no effect on complexity, but the ranking would be static and may lose its meaning after several iterations of the algorithm. We co uld also implement a dynamic ranking. We present a version of GREEDY-KSEP-OPT1 that we call GREEDY-KSEP-OPT2 that picks maintains the usual approximation ratio of 1 + ln( f ) for greedy algorithms. Our key intuition was to use a Fibonacci heap [Fredman and Tarjan 1987]. With such a data structure, we can update the rankings of obs ervations at constant amortized cost per observation being updated. The most expensive operation is to remove an observation from the heap, which costs an amortized O (ln( | O | )), however as we can never remove more than | O | items from the heap, this cost is most likely dominated by the cost of the rest of the algorithm, which is more expensive than GREEDY-KSEP-OPT1 by a factor of f .Recallthat f is the bound on the number of observations supported by a single partner and is often very small in practice.

In order to leverage the Fibonacci heap, there are some restrictions on how the ranking can be implemented. First, the heap puts an element with the minimal key on top, and can only decrease the key of elements; an element in the heap can never have its key increased. Additionally, there is a need for some auxiliary data structures as searching for an element in the heap is very expensive. Fortunately, the k -SEP problem is amenable to these type of data structures.

We based the key (ranking) on a simple heuristic for each observation. The key for a given observation o is the number of unique observations that share a partner with o . As we are extracting the minimum-keyed observation, we are taking the observation that has the  X  X east in common X  with the other observations. The intuition of choosing an observation with  X  X ess in common X  with other observations ensures that outliers get covered with larger covers. Meanwhile, elements with a higher rank in this scheme are covered last, which may lead to a more efficient cover. In Section 6 we show ex-perimentally that this heuristic was viable for the data-set we considered -providing more accurate results than the reduction from set-covering.
 Example 5.2. The basic intuition behind GREEDY-KSEP-OPT2 is similar to GREEDY-KSEP-OPT1 in that it iterates through the observations and greedily chooses a partner. The main difference is that it ranks the observations instead of just ran-domly selecting them. Consider the sun bear from Example 2.3 whose behavior is depicted in Figure 2. In Example 5.1, we used GREEDY-KSEP-OPT1 to solve the as-sociated k -SEP problem for this situation. We shall discuss how GREEDY-KSEP-OPT2 differs.
 The first main difference is that the algo rithm assigns a rank to each observation o i , called key i , which is also the key used in the Fibonacci heap. This is done in the loop at line 4. It not only calculates key i for each observation, but it also records the elements  X  X elated X  to it in the array REL OBS . Note that a  X  X elated X  observation needs only to share a partner with a given observation. Not all related observations need to have the same partner. For the sun bear scenario, we show the keys and related observations in Table I.

As the key values are the same for all elements of O , let X  X  assume the algorithm first considers o 1 as in Example 5.1. As written, we would take the minimum element in the Fibonacci heap (a constant time operation). We would then consider the partners for o 1 which would result in the greedy selection of p 2 ,(justasin GREEDY-KSEP-OPT1 and NAIVE-KSEP-SC . Also notice we retain the array of Booleans, O as well as the array of lists, M to help us with these operations.).

Now the issue arises that we must update the keys for the remaining observations, as well as remove obse rvations covered by p 2 . As we maintain REL OBS and O ,the procedure quickly iterates through the elements covered by p 2 : o 1 and o 2 .Figure6 shows the status of the observations at this point.

We remove o 1 from the heap, and set O [ o 1 ]to FALSE . This prevents us from con-sidering it in the future. We now iterate through each o in the list pointed to by REL OBS [ o 1 ]where O [ o ]is TRUE and decrease the key of each by one. As per table I, REL OBS [ o 1 ]= { o 1 , o 2 } .As O [ o 1 ]= FALSE we do nothing. As O [ o 2 ]= TRUE ,wede-crease the key of the associated node in the Fibonacci heap. The array QUICK LOOK ensures we can access that element in constan t time. Figure 6 (left) graphically depicts this action.

Next, we consider the other element covered by partner p 2 : o 2 .Afterremovingthis element from the heap and setting O [ o 2 ]to FALSE , we can easily see that there does not exist any o  X  REL OBS [ o 2 ]where O [ o ]= TRUE . Hence, we can proceed to pick a new minimum observation from the heap, which is o 3 in this case. The greedy selection proceeds (resulting in the choice of p 6 ), followed by the update procedure (which simply removes the node associated with o 3 from the heap and sets O [ o 3 ]= FALSE ). As there arenomoreelementsintheheap, GREEDY-KSEP-OPT2 returns the solution { p 2 , p 6 } .
T HEOREM 5.1 C OMPLEXITY OF GREEDY-KSEP-OPT2 . GREEDY-KSEP-OPT2 has acomplexityofO (  X  f 2  X | O | + | O | X  ln( | O | )) and an approximation ratio of 1+ln( f ) .
P ROPOSITION 5.3. GREEDY-KSEP-OPT2 returns a | E | -sized (  X ,  X  ) explanation for O .
 GREEDY-KSEP-OPT2 returns IMPOSSIBLE if there is no explanation for O . In this section, we show that our geospatial abduction framework and algorithms are viable in solving real-world geospatial abduction problems. Using a real-world dataset consisting of counterinsurgency informat ion from Iraq, we were able to accurately lo-cate insurgent weapons cache sites (partners) given previous attacks (observations) and some additional data (used for feas and  X ,  X  ). This validates our primary research goal for the experiments; to show that geospatial abduction can be used to solve prob-lems in the real world.

We considered the naive set-covering approach along with GREEDY-KSEP-OPT1 and GREEDY-KSEP-OPT2 , which according to our analytical results, had the best approximation ratios and time-complexities. We implemented these algorithms in 4000 lines of Java code, running on a Lenovo T400 ThinkPad laptop running Vista with an Intel Core 2 Duo T9400 2.53 GHz processor and 4.0 GB of RAM. Our SCARE (Social-Cultural Abductive Reasoning Engine) system [Shakarian et al. 2009] enabled us to carry out tests on real-world data. This data includes 21 months of Improvised Explosive Device or IED attacks in Baghdad 5 (a 25x27 km region)  X  these constitute our observations. It also included information on locations of caches associated with those attacks discovered by U.S. forces. The locations of the caches constitute the (  X ,  X  ) explanation we want to learn. We used data from the International Medical Corps to define feasibility predicates that took the following factors into account: (i) the ethnic makeup of neighborhoods in Baghdad; specifically, Sunni locations were deemed infea-sible for cache locations, (ii) the locations of U.S. bases in Baghdad were also considered infeasible and (iii) bodies of water were also deemed infeasible. We also separately ran tests on that part of the above data focused on Sadr City (a 7x7 km district in Baghdad) alone. On both these regions, we overlaid a grid whose cells were 100m x 100m each  X  about the size of a standard US city block. All timings were averaged over 100 runs.
We split the data into 2 parts; the first 7 months of data was used as a  X  X raining X  set and the next 14 months of data was used for experimental evaluation. We used the following simple algorithm, FIND-BOUNDS , to determine the  X ,  X  values. We set  X  max to 2.5 km. We leave more advanced procedures for learning these parameters to future work. Such parameters could also come from an expert.

Accuracy. Our primary goal in the experiments was to determine if the geospatial ab-duction framework and algorithms could provide viable results in a real-world setting.  X  X ccuracy X  in this section refers to two aspects -size of the solution, and the distance tothenearestactualcachesite. Thedistancetonearestcachesitewasmeasuredby taking the straight-line Euclidean distance to the nearest cache site that was found after the first attack supported by the projected cache site. We used the raw coordi-nate for the actual cache in the data set, not the position closest to the nearest point in the 100 m resolution grid that we overl aid on the areas. The accuracy results are summarized in Tables II, III.
Overall, GREEDY-KSEP-OPT2 consistently found the smallest solution X  X f cardi-nality 14 for Baghdad and 6 for Sadr City X  X n all 100 trials. For Baghdad, the other two algorithms both found a solution of size 14, but both averaged a higher solution. For Sadr City, GREEDY-KSEP-OPT1 often did find a solution of 6 caches while NAIVE-KSEP-SC only found solutions of size 8. Additionally, in both tests, the solution sizes for GREEDY-KSEP-OPT1 varied more than the other two algorithms. Moreover, the HSD for both Baghdad and Sadr City indicated significant difference between all pairs of algorithms wrt solution size.

Of the partners in a given solution, we also recorded the number of partners less than 0 . 5km away from an actual cache. For Baghdad, NAIVE-KSEP-SC performed site. Although this result for Baghdad is significant based on an analysis of variance (ANOVA) and honest significant differences (HSD) ( p -value of 2 . 3  X  10  X  9 ), we also note that the greatest difference among averages was still less than one partner. This same result for Sadr City, however, tells a different story. For this test, NAIVE-KSEP-SC performed poorly with regard to the other two algorithms, finding only 3 partners meeting these criteria for each of the 100 trials. GREEDY-KSEP-OPT2 performed very well in this aspect (for Sadr City). It averaged over 5 partners less than 0 . 5kmfroman actual cache. Further, for Sadr City, all partners found by GREEDY-KSEP-OPT2 were within 600 m of an actual cache site. The ANOVA ( p -value of 2 . 2  X  10  X  16 )andHSDof partners less than 0 . 5 km from an actual cache for the Sadr City trials indicate that these results are significant.

Our primary metric of accuracy was ave rage distance to actual cache. In this regard, GREEDY-KSEP-OPT2 performed the best. It obtained an average distance of 0 . 72km for Baghdad and 0 . 35km for Sadr City. This number was 40m less for Baghdad and 100m less for Sadr City when compared to GREEDY-KSEP-OPT1 ,whoseaverage distance varied widely among the trials. With regard to this metric, NAIVE-KSEP-SC performed the worst, particularly in Sadr City, where it predicted caches over twice as far from actual caches as GREEDY-KSEP-OPT2 (on average). For both Baghdad and Sadr City, the simple ANOVA yielded a p -value of 2 . 2  X  10  X  16 , which suggests with a 99% probability that there is a difference among the algorithms. Also, for both areas, Tukey X  X  HSD indicates significant difference between each pair-wise comparison of algorithms.

Algorithm Runtimes. Table IV shows the runtimes of our algorithms. In order to val-(ANOVA) and Tukey X  X  Honest Significant Difference test (HSD) for pair-wise compar-isons [Freedman et al. 2007]. An ANOVA for the Baghdad runtimes gave a p -value of 2 . 2  X  10  X  16 , which suggests with well over 99% probability that GREEDY-KSEP-OPT1 is statistically faster than GREEDY-KSEP-OPT2 . The HSD for Baghdad indicates that, with regard to runtimes, all pair-wise-comparison of the three algorithms are signifi-cantly different. For Sadr City, the ANOVA gave a p -value of 4 . 9  X  10  X  3 , which suggests with a 99% probability that the algorithms differ in run-times. However, the HSD indi-cates, with an 82% probability, that there is no difference among GREEDY-KSEP-OPT1 and GREEDY-KSEP-OPT2 , while both differ significantly from NAIVE-KSEP-SC . In our implementation of all three algorithms,  X  X ies X  in greedy selection of partners this size = cur size in line 2 of NAIVE-KSEP-SC in Section 4.2. Let us re-phrase the situation as follows. Let O be the entire set of observations and O  X  O be the set of observations currently not assigned a partner. Let p be the current partner that best meets the criteria for greedy selection and p be the partner we are considering. We define P and P as subsets of O that are the observations associated with p and p re-we flip a coin. We add a simple heuristic that simply states that  X  X artners that cover more observations are preferred. X  W e change the criteria as follows.  X  X f | P  X  O | = | P  X  O | , then do the following:  X  X f | P | &gt; | P | ,pick p  X  X f | P | &gt; | P | ,pick p  X  X f | P | = | P | ,flipacoin.
 We shall refer to this as the  X  X ie-breaker X  heuristic. The result is that the solution set of partners covers more observations and hence provides a more  X  X ense X  solution.
We added this heuristic to our existing code for all three algorithms and ran each one 100 times for both the Baghdad and Sadr City areas. Unsurprisingly, as this is a constant-time operation, runtimes were not affected. However, accuracy improved in all cases. As GREEDY-KSEP-OPT2 still provided the most accurate results, the following exposition shall focus on how the heuristics affected the solution size and accuracy for this algorithm.

Because the tie-breaker heuristic only adjusts how two partners are chosen, both of which can be paired with the same uncovered observations, the size of the solution was unaffected in both the Baghdad and Sadr City trials. However, the number of predicted cache sites less than 500m from an actual site increased for both the Baghdad and Sadr City tests. For Baghdad, more trials returned solutions with 8 predictions less than 500m from an actual site than returned 7, the opposite being the case without the tie-breaker heuristic. For Sadr City, all elements of every solution set returned were less than 500m from an actual cache site. Using the well known T-Test [Freedman et al. 2007], we showed that these results are statistically significant as this test returned experiments with the tie-breaker heuristic are shown in Tables V and VI.

Summary. Those experiments demonstrate statistically that GREEDY-KSEP-OPT2 provides a viable solution, consistently producing the smaller solution sets which were closer to actual cache sites faster than the basic set-covering approach, at times approaching the faster, although less-accurate GREEDY-KSEP-OPT1 . The proximity of the elements of the solution set to actual cache sites is encouraging for real-world use. In this section we present related work of t hree different varieties. We compare GAPs to other forms of abduction, facility location, k-means clustering, and constrained clus-tering. As an aside, readers interested in a discussion of the SCARE software from the perspective of military analysis or social science can refer to Shakarian et al. [2009], where the software package was introduced. However, that work does not include any formal technical details on the framework of geospatial abduction, complexity results, or algorithm analysis.

GAPs and other forms of Abduction. Abduction [Peirce 1955] has been extensively studied in medicine [Peng and Reggia 1986; Reggia and Peng 1990], fault diagno-sis [Console et al. 1991], belief revision [Pagnucco 1996], database updates [Console et al. 1995; Kakas and Mancarella 1990] and AI planning [do Lago Pereira and de Barros 2004]. Two major existing theories of abduction include logic-based ab-duction [Eiter and Gottlob 1995] and set-covering abduction [Bylander et al. 1991]. Though none of these papers deals with spatial inference, Shanahan [1996] presents a logical formalism dealing with objects X  spatial occupancy, while Santos and Shanahan [2002] describe the construction of a qualitative spatial reasoning system based on sensor data from a mobile robot. In Santos and Shanahan [2002], sensor data are ex-plained by hypothesizing the existence of physical objects along with the dynamic re-lationships that hold between them, all with respect to a (possibly moving) viewpoint. This approach combines both space and time. Kuipers [1996] describes the Spatial Semantic Hierarchy which formalizes, the spatial context in which a robot moves. In the hierarchy, the topological level defines a map which describes the environment as a collection of places, paths, and regions, linked by topological relations such as con-nectivity, order, containment, boundary, and abstraction. Places (i.e., zero-dimensional points), paths (i.e., one dimensional subspaces, denoting for example a street in a city, possibly represented as an ordering relation on the places they contain), and boundary regions (i.e., two-dimensional subspaces of the robot environment) are created from experience represented as a sequence of views and actions. They are created by ab-duction, positing the minimal additional set of places, paths, and regions required to explain the sequence of observed views and actions.

Set-covering abduction [Bylander et al. 1991] assumes the existence of a function determining the observable effects of a set of hypotheses, and is based on inverting such function. Given a set of hypotheses H and a set of observations O ,thedomain knowledge is represented by a function e that takes as an argument a set of hypotheses and gives as a result the corresponding set of observations. Thus, for every subset of the hypotheses H  X  H , their effects are known to be e ( H ). In this case, abduction finds a set H  X  H such that O  X  e ( H ), that is, it finds a set of hypotheses H whose effects e ( H ) include all observations in O . A common assumption is that the effects of the hypotheses are independent, that is, for every H  X  H ,itholdsthat e ( H )= No spatial reasoning is done here.

Comparison with facility location. There are several important ways in which GAPs differ from facility location problems.  X  Although it is possible to specify a distance-based cost function, in a GAP problem, the distances between observations and partners are constraints (  X  and  X  in this paper) whereas facility location problems usually attempt to minimize the distance between producers and consumers.  X  In this article, GAP problems have a minimum distance between observations and partners that must be exceeded. In many respects, this requirement makes GAP problems more difficult than facility location and other computational geometry problems as the set of possible partners that cover a given observation is a non-convex ring. Further, the feasibility function ( feas ) adds non-uniform holes to such a ring. Maass [1986] addresses the complexity of non-convex covering and high-lights issues with problems such as this.
  X  The feasibility predicate, feas is not part of a facility location problem. This gives us the ability to restrict certain locations that can be partners.  X  In general, the relation between observations and partners can be viewed to be a set of constraints. In this paper, we only used  X ,  X , and feas . However, in the future, we could add additional constraints. Further, as our formalism represents space as a set of discrete points (also not typically done with facility location), we can easily specify certain properties of these points to apply such constraints (such as feas ).

Comparison with k-means clustering. A well-known and studied problem in clustering location is the k-means problem [MacQueen 1967]. This problem can be expressed as follows: k-means : INPUT: Coordinates on a plane C and natural number k OUTPUT: k disjoint sets of C , C 1 ,..., C k such that for each C i , all the mean Euclidean distance among all c  X  C i is minimized.

Clustering problems group points into clusters, associating each cluster with a cen-ter. At first glance, one may think that the points are equivalent to observations and the  X  X enters are equivalent to partners. However, this is not so. Most versions of the clustering problem seek only to arrange points in groups, with  X  X enters X  being a side effect of the algorithm. Geospatial abduction problem seeks to find partners that sup-port observations and places constraints on the location of the partners; this is a key difference from  X  X enters X  in clustering problems. Clustering algorithms cannot handle the generality of our feasibility predicate or the (  X ,  X  )constraints.

In addition to these obvious differences, we experimentally compared an implemen-tation of k-means with GREEDY-KSEP-OPT2 on the Sadr City data. Even when we ignore the obvious value of  X ,  X  and the feasibility predicate, GREEDY-KSEP-OPT2 outperforms the SimpleKMeans solver in WEKA version 3.7.0 [WEKA 2009]. Note that the exclusion of these parameters makes GREEDY-KSEP-OPT2 perform worse than it performs with these parameters; yet, it performed better than k -means in terms of accuracy. Our experiment was setup as follows.  X  We used the same area for the Sadr City tests, as the  X  value was 0 in these tests and there were virtually no non-feasible points near the observations. This allowed us to use WEKA X  X  k-means implementation  X  X ut-of-the-box X  as we did not have to implement any extra infrastructure to deal with feasibility and  X  =0.  X  X e set k = 6, the number of partners consistently found by GREEDY-KSEP-OPT2 .
Normally, we would rather have the algorithm determine this size. Note that sup-plying the algorithm with a size already determined by GREEDY-KSEP-OPT2 (and, also the smallest size of any explanation for Sadr City we found in our trials) gives an advantage to k-means . Hence, we did not compare solution sizes.  X  We clustered the observations with k-means and considered the  X  X enter X  of each cluster the cache location for the cluster.  X  We did not compare timing results, as we ran WEKA in its GUI environment.
We ran 500 iterations of the SimpleKMeans and worked with the average centers for the clusters as reported by WEKA. Multiple runs of the 500 iterations yielded the same centers.
 Average Distance Using WEKA, we obtained an average accuracy of 0 . 38 km, which is worse than GREEDY-KSEP-OPT2 (average over 100 trials, 0 . 28 km).
 Worst-Case Distance WEKA X  X  SimpleKMeans returned 2 of the 6 points with a distance of greater than 600 meters from a cache site. Without the  X  X ie-breaking X  heuristic, GREEDY-KSEP-OPT2 never reported a prediction over 600 meters from a cache site (all reported partners over 100 trials). With the heuristic, GREEDY-KSEP-OPT2 never reported a prediction over 500 meters from a cache site.
 Best-Case Distance The closest partners ever returned by GREEDY-KSEP-OPT2 (either with our without the heuristic) were 200 m away from an actual cache site (on average, the closest partner per explanation was 220 m away). WEKA X  X  SimpleKMeans did return two partners less than 200 m -each only 100 m away from an actual cache site.

These results suggest that k-means may not be the optimal method for GAP problems. Further, it does not support feasibility and  X  . The results do hold some promise for some variants of cost-based spatial explanation problems that require a k input from one of our greedy-approaches. However, even in this case, there would be modification required of the k-means algorithm to support feasibility and  X  .
Comparison with Constrained clustering. Constrained clustering Wagstaff et al. [2001] studied clustering where, in addition to the points to be clustered, there are constraints that either force two points in the same cluster (must-link) or force two points to be in different clusters (cannot-link). Later work on constrained clustering has focused on distance constraints between elements of C or distance constraints between clus-ters [Davidson and Ravi 2005]. Much of the work in this area is summarized in Basu et al. [2008].

At first glance, it may appear that spatial abduction can be expressed as a cannot-link constrained clustering problem as follows: For each o , o  X  O if  X  p  X  S s.t. o , o .

However, such a mapping cannot be guarant eed to provide a correct result. For ex-given the generality of feas . In such a case, all three observations could be incorrectly grouped into a single cluster, although it is obvious there is no single partner that supports all of them. Hence, such a mapping would not be trivial. Further, most clus-tering algorithms are not seeking to constru ctively find centers that are constrained. We leave the study of constrained clustering to solve GAP problems (i.e., an adaption of the k-means algorithm) to future work. However, it is also worth noting that solving constrained clustering problems given cannot-link constraints is NP-complete, so the application of clustering techniques to this problem does not imply a more tractable version of geospatial abduction, but rather an alternative heuristic. There are a wide variety of problems where we can make geo-located observations  X  X n the ground X  and where we want to infer a partner location. In this paper, we have presented four examples of such problems: one dealing with serial killers, another dealing with wildlife studies, and a third (perhaps more fun) application dealing with finding sunken ships. A fourth real-world application we have looked at is that of finding weapons caches associated with Improvised Explosive Device (IED) attacks in Iraq where we were able to use real world, open source data. It is clear that many other applications exist as well. For example, a bizarre (but real world) combination of two of our examples involves frequent attacks by man-eating leopards on children in certain parts of greater Bombay in India. This situation is analogous to the serial killer scenario where the leopard is the serial killer. We want to find the leopard X  X  favorite  X  X ang outs X , capture it, and solve the problem.

In this paper, we have made an attempt to formally define a class of geospatial abduction problems ( GAPs for short). We specifically made the following contributions.  X  We developed formal mathematical definitions of geospatial abduction problems, including several variants of the above problems. We conducted a detailed analysis of the complexity of these problems.  X  We developed exact algorithms for many of the problems, including a straight-forward enumeration approach ( NAIVE-KSEP-EXACT ), by showing and leveraging reductions to both the set-covering and dominating set problems, and by articulat-ing these geospatial abduction problems via integer linear programs.  X  As the complexity of most of the problems we have studied is NP-hard, we developed two greedy approximation schemes for the k -SEP problem (other than set-covering) and illustrated a scheme to quickly find a solution using randomized approaches to the dominating set problem.  X  We have implemented these algorithms and conducted experimental comparisons of the reduction to set-covering and two other greedy approaches; GREEDY-KSEP-
OPT1 and GREEDY-KSEP-OPT2 . Both of these algorithms outperformed the set-covering reduction in an experiment on the Understanding War Special Groups dataset. We also implemented a  X  X ie-breaker X  heuristic that further improved the accuracy of the algorithms.  X  We have also developed approximation schemes using relaxations of the linear-integer program for k -SEP and the cost-based variant WT-SEP.

There are many interesting directions for future work. For example, spatial abduc-tion in dimensions greater than two might be explored. A probabilistic variant might replace the feasibility predicate with a probability distribution function, or express the relationship between observations and partners as a PDF based on distance rather than rely on  X ,  X  . Also, the use of randomization in the approximation algorithms may improve results for both the greedy and linear programming approaches presented in this article.

One aspect to explore in future work is the relationship between observations and partners. k -SEP and its cost based variants only rely on  X ,  X  . However, many appli-cations may have other constraints. Perhaps there is a direction associated with each observation (as in identifying where an artillery round originated from), which would limit the locations of the partner. Another possibility is to add geographic constraints. Perhaps the observation cannot have a partner across a body of water, or beyond the edge of a cliff.
 The electronic appendix for this article can be accessed in the ACM Digital Library.
