 K-means has better scalability and higher implementation efficiency to the application of document clustering, and it can achieve good results and is superior to hierarchical isolated point in document set. Aiming at the characteristic that the cluster center of the K-means needs to be assigned, there have been some improved methods[2].

Literature [3] made use of genetic algorithm to optimize K which is the number of clustering center has been raised by the global k-means method, which adds a dynamic clustering center through making use of global searching process. Liu Yuanchao made use of the improvement of the Maximin Principle to decide the clustering number and clustering center[5]. In literature [6] the clustering center was determined by improving the CBC committee algorithm, and the shortcoming of the method is that it has a high mittee, which depresses the accuracy to some extent. Additionally, the algorithm needs to assign the number of the cluster manually, which is always a difficult task. teristic that the K-means algorithm needs to assign the initial clustering center, based on sub-graph division. After setting up the similarity matrix of the text, we select the entire document nodes as the vertexes of the graph. For the two documents whose similarity is bigger than the current similarity threshold, we connect a line between the two corresponding nodes in the graph. Thus a disconnected graph will be formed (if the threshold is too small, the graph may be a connected graph). proper, some sub-graphs exceeding a dedicat ed text number in the disconnected graph ments of sub-graphs is high; meanwhile, the similarity between the documents included in different sub-graphs is smaller than the current similarity threshold. The purpose of maximum and makes the similarity of data point in the different cluster minimum. It is feasible to take these sub-graphs as the candidate initial cluster center. 
In the changing process of the threshold, if the mutual similarities of the document of one cluster are lower in the similarity matrix, when the threshold falls to a dedicated value, a sub-graph will be formed possibly between the documents of this cluster, and then we can consider the vector center of the documents in this sub-graph as the can-didate clustering centers, which reduces the data noise. 2.1 Preprocess of Clustering During the document clustering, preprocess work is very important. The usual proc-essing steps include: segmentation, stop word removal, word frequency statistic, fea-ture selection and building vector space model. 
In the process of our experiment, we select different number of high frequency words to experiment, and find that the use of 50 high frequency words would have the model. The computing of text similarity uses cosine formula. 2.2 Algorithms The first stage: Cluster center The main process of the algorithm is, firstly, we find out all the sub-graphs of the text set and sub-graphs formed in the current threshold from these sub-graphs. We deal with them and then depress the threshold, loop until satisfying the end situation. The input of the algorithm is the storage structure of the adjacent table of N clustering documents and the threshold of similarity is  X  ,  X  . Step 1. Finding out all the sub-graphs of the text set by using of the depth graph trav-smaller than  X  , we consider that the cluster center of M i , M j doesn X  X  change, and we similarity. If the increased document number is bigger than  X  , we need to judge whether the new element is a new cluster center or not. 
For the sub-graph that doesn X  X  have mapping relationship with the old cluster center, taking the sub-graph as the candidate cluster center and turn to Step4 ; Step 5. Reducing the similarity threshold of the sub-graph division by 0.05; 
The sub-graph in the algorithm refers to the graph that the number of the interrelated sub-graph division threshold, as a new cluster center. The second stage: Clustering We conduct a k-means clustering by use of the clustering centers, which are found out in the first stage. Step 2. Assign each document to the most similar cluster according to the average of Step 3. Update the average value of the cluster; Step 4. Repeat Step 1  X  Step 3 until the cluster division does not change again; 2.3 Time Complexity Analysis of Algorithms Prior to the implementation of the algorithm, we need to build the similarity matrix of the document. The time spending is bigger, but most of the document clustering algo-similarity matrix, the storage structure of adjacent table can be built according to the dedicated threshold. The time complexity of the graph X  X  traverse algorithm is () e n +  X  in the implementation process, taking d as the feature dimension of text set and taking k as the number of cluster center. The time complexity of the second step is the time complexity of the algorithm is ( ) d k nd e n 2 + + +  X   X  The time com-plexity of the algorithm can be increased with the increasing of the dimension numbers. Consequently, when we handle the large data set, the time complexity of the algorithm is high. We conduct a series of experiments. The first group selects the different articles that have already been categorized from www.sina.com.cn . These articles are classified artificially, so it is convenient to compare the test result. There are 7 classes of docu-trance examination, sport. We take out 20 documents for experimentation from each category of these documents. The second group makes use of the classification corpus which is provided by Lee Lurong of FuDan University, selecting 6 classes from these corpus for the experiment: Energy  X  Electronics  X  Medical  X  Communication  X  Philoso-phy  X  Literature. 
The evaluation criteria of the experiment result used most commonly F-measure values [7]. Let P be average precision. Let R be average recall. F-measure is defined to be 2 RP/(R+P) . The threshold of the algorithm is selected according to the experiment experience. When  X  is smaller, there will be many density areas and the mutual similarity of these areas will be lower. We conducted several experiments by taking definite standard text experiment (N is the document number of each cluster). experiment and each k is conducted 6 groups of different experiments. We can see that analyzing the cluster that the division number is lack, it is easy to find that the docu-ment theme distributing of the cluster is too loose. When k=6, the military cluster is not found. 
We select 2  X  6 clusters from the 7 clusters to conduct experiment, and three group of experiments are conducted to every k. Corresponding to each group of experiment for method. 
From table 2, we can see that the experiment result by the method of automatically determining the cluster center is superior to the method of artificially designating the cluster center. In the first stage of the cluster, the vector center of the cluster are the combination of the documents that are divided into the sub-graph, so each cluster in the test result has a high recall rate. If the number of selected cluster center is equal to the promoted, so the accuracy of the cluster result is higher. 
The second experiment (Table 3.) is a comparison between our method in this paper and the original cluster method C. In the original method, the cluster center is generated experiment. 
From the experiment results of table 3, in most cases, this method can achieve better results. The F-measure value of the automatically determining the cluster center method is promoted by 8% than the original K-means cluster method. We analyze the reasons for several poorer test results and it is mainly because that the selected feature communication cluster. The other reason is that the length of individual document is distinguish the document are less. A method of determining the cluster center is presented, by which the potential cluster cluster center, it removes the noise data successfully. The method, which can improve the results of the cluster remarkably, is proved effective. When our method applying in amount of information contained in the text. In future, we will continue to research it deeply. 
