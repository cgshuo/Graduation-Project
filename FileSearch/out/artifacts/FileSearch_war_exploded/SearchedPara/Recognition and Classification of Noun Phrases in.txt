 It has been shown that using phrases properly in the document retrieval leads to higher retrieval e ffectiveness. In this paper, we define four types of noun phrases and present an algorithm for recognizing these phrases in queri es. The strengths of several existing tools are combined for phrase recognition. Our algorithm is tested using a set of 500 web que ries from a query log, and a set of 238 TREC queries. Experimental results show that our algorithm yields high phrase recognition accuracy. We also use a baseline noun phrase recognition algorithm to recognize phrases from the TREC queries. A documen t retrieval experiment is conducted using the TREC queries (1) without any phrases, (2) with the phrases recognized from a baseline noun phrase recognition algorithm, and (3) with the phrases recognized from our algorithm respectively. The retrieval effectiveness of (3) is better than that of (2), which is better than that of (1). This demonstrates that utilizing phras es in queries does improve the retrieval effectiveness, and be tter noun phrase recognition yields higher retrieval performance. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  dictionaries, linguistics processing . H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  query formulation .
 Algorithms, Design, Experimentation. Information Retrieval, noun phr ases, proper noun, dictionary phrase, simple phrase, complex phrase, feedback, verification. The objective of this paper is to detect various types of multi-word noun phrases in a query. In this paper, we consider the queries that are short and similar to typical web search queries. The detected noun phrases are used to interpret the original query in order to improve retrieval effectiveness. Noun phrases are classified into four categories: (1) proper noun; (2) dictionary phrase; (3) simple phrase and (4) complex phrase. A proper noun (PN) refers to the name given to a person, place, event, group or organization, etc., for example,  X  Tom Smith  X . Dictionary phrases (DP) are the noun phrases defined in dictionaries, but not proper nouns, for example,  X  computer monitor  X . Both the simple noun phrase (SNP) and complex noun phrase (CNP) are the noun phrases that are grammatically co rrect, used in the daily language, but not formally defined in di ctionaries. We require SNP to contain exactly 2 words and CNP to contain three or more words; for example  X  small car  X  is an SNP and  X  local movie theater  X  is a CNP. The reason for recognizi ng phrases in queries, and classifying them into the four types, is that noun phrases are known to be very helpful for document retrieval [1][4][16][28]. A recent paper [20] shows that proper use of these four types of phrases yields significantly higher effectiveness in document retrieval than just using the indi vidual query words. Zhou, et. al [33] reported that the use of concepts, which are equivalent to phrases, in biomedical IR also resulted in higher retrieval effectiveness than just using i ndividual query terms. Our work converts the original query from a set of words to phrases in order to improve document retrieval pe rformance. In this paper, we utilize the following tools: (i) Wikipedia [32] is a comprehensive online encyclopedia. It is (ii) WordNet [12] is an electronic dictionary. It is used to (iii) Minipar [18] is used to recognize proper nouns. (iv) Collins parser [9] is used to recognize SNP and CNP. (v) Google [14] is used to recognize some proper nouns and to We apply Wikipedia, WordNet, Minipar and Google to detect PNs and DPs in the queries. We us e a document collection to test if these PN/DPs are really used in the real world. They are discarded if not found in the documents. Then the SNPs and CNPs are searched in the queries . These two types of phrases are grammatically correct, used in daily language, but do not have entries in dictionaries. Collins parser and some statistical techniques are used to detect the SNP/CNPs. This includes detecting the shorter noun phrases embedded in longer noun phrases. For example, a query  X  Orlando travel agents  X  is a CNP.  X  X  ravel agents  X  is a SNP. Both need to be recognized. Our algorithm is tuned using a set of 400 web queries randomly selected from a web query log. It is tested on another set of web queries and a set of multi-word TREC query titles. The contributions of this paper are (i) The evaluation of various tools and their combinations for (ii) The analysis of the errors made by each tool. (iii) The development of an algorithm that combines (iv) An operational system that yields higher noun phrase (v) Experimental results show that recognizing phrases in queries The rest of this paper is organi zed as follows. Section 2 describes the PN/DP recognition algorithm. The SNP/CNP recognition is explained in Section 3. Experi mental results are reported in Section 4. Section 5 reviews the related works. Conclusions are given in Section 6. Phrases have been used in document retrieval [1][4][16][20]. Higher retrieval effectiveness over the individual word method has been reported. Lima et al [17] studied the proper noun and phrase recognition problem. They used the EM algorithm to estimate the parameters of a probabilistic context-free grammar (PCFG), given a large Web query log and a hand-written context-free grammar. The PCFG was utilized to compute the most probable parse for a query, which was then employed for phrase recognition. They studied on the company names, human names and other short Web queries, where company and human names could fit in our PN category, and other queries could fit in our SNP/CNP categories. Mihalcea and Moldovan [24] claimed that the implicit phrases were recognized but technical details were not provided. Florian, et al [13] presented a classifier-combi nation framework for named entity recognition. Four statistical classifiers are combined. Also, additional dictionary data and two more classifiers are combined. The best f-scores on the training set and the test set are 93% and 88% respectively. Florian X  X  system mostly focuses on named entity (PN) recognition, while ours also finds the ordinary phrases (SNP and CNP) because we aim to provide general phrases to document retrieval systems. Evans and Zhai [10] extracted meaningful short noun phrases from documents by using both co rpus statistics and linguistic heuristics. The performance of their system on the noun phrase recognition was not directly reported. Instead, they compared and reported the document retrieval results of (1) using and (2) not using the recognized noun phrases. The system was tested on TREC-1993 queries 51-100. The setup (1) improves the precisions at all the document le vels over (2), 13% improvement at 5-doc level, 6% at 10-doc level and 7% at 15-doc level, etc. Lin [19] only uses Minipar and WordNet for conference name recognition. We not only add anot her tool, Wikipedia, but also expand the conference name to various types of proper nouns. The following aspects of our work appear to be novel: (1) we integrate different strategies into a unified algorithm to recognize different types of noun phrases, (2 ) Minipar, Wordnet Wikipedia and a large document collection are used jointly to find different types of proper nouns. (3) Phrase ve rification is carried out at the statistical level, validating wh ether a sequence of words is commonly used as a whole in the real world. The document collection is also used to de termine which of two overlapping phrases is the desired one. The PN/DP recognition algorithm recognizes proper nouns (PN) and dictionary phrases (DP) in a query. The pseudo code is given in Figure 1. From line 2 to line 20, given an n-word query p ( w 1 w PN/DP recognition starts from p itself (n-word candidate, a string of n consecutive words in the origin al query); if failed, it searches in the (n-1)-word sub-phrases of p ; this process repeats until reaching the 2-word candidates. We call the searches among the x-word candidates the  X  X evel-x X  search. A recognized PN/DP is called a  X  X evel-x potential PN/DP X , because they still need to be verified. We do not consider the case of two words, which are not in consecutive position, forming a phrase. For example,  X  computer price  X  is not examined as a candidate given a query  X  computer monitor price  X . is_wiki_PN(p) . In line 6, function is_wiki_PN(p) uses Wikipedia to check if a phrase candidate p is a PN. A PN should be defined in a dictionary if it is well known, and should have all of its content words capitalized. When p is submitted to Wikipedia, a definition page should be returned if p is defined. We require that the first instance of p in the main body of that page have all its content words capitalized, in order to label p as a Wikipedia PN. We emphasize the  X  X ain body of the page X  because the words in the page title are usually capitalized regardless of their types. is_wiki_DP(p) is similar to is_wiki_PN(p) but the p needs not to be capitalized in the Wikipe dia text in is_wiki_DP(p). is_wn_PN(p) . In line 6, this function uses WordNet as another dictionary to check if p is a PN. If (1) WordNet recognizes p as a defined noun phrase; (2) one of the p  X  X  hypernyms in WordNet is a city, province, country, organization, geographical area, person or a syndrome; and (3) the content words of p in its WordNet definition are capitalized, p is labeled as a WordNet PN. is_wn_DP(p) recognizes p as a DP if it is defined in WordNet but not qualified as a WordNet PN. is_minipar_PN(p) . In line 7, this function uses Minipar to check if p is a PN. If Minipar gives p a  X  X N X  label, or one of the labels of  X  X erson X ,  X  X ountry X ,  X  X orpna me X ,  X  X ocation X ,  X  X orpdesig X ,  X  X name X ,  X  X name X ,  X  X ate X , whic h refer to people, location, organization, family name, given name and dates, respectively, the p is labeled as a Minipar PN. Minipar is only used to recognize PN but not for DP, because it is not a pure dictionary. Some of its decisions are made based on grammatical rules. is_Name(p) . In line 7, this function uses a list of first names, a list of last names, and several name -written patterns to detect names of persons. This is necessary because the names of the ordinary people are hardly defined in dic tionaries. In our implementation, we use the name lists from the U.S. Census Bureau, because currently we only test our algorithm using documents written in English. The patterns we adopt are  X  X irst name initial, last name X ,  X  X irst name initial, middle name initial, last name X ,  X  X irst name, middle name initial, last name X ,  X  X irst name, last name X . This function is fired if a whole query itself matches any of the patterns, while other tools can not recognize any PN/DP in this query. During the algorithm tuning, we tried to apply this function to parts of the queries. But it gave us many false positives. So we decided to only apply it to a whole query. is_doc_PN(p) . In line 7, this function detects the less-famous proper names in a query, such as less well-known people, organizations, or locations that are not indexed in dictionaries. This is necessary as is_Name( ) only detects names of persons. A phrase candidate p is searched in a document collection. If at least three instances of p are found in documents, such that (1) All non-stop words in these instances are capitalized, (2) None of the instances is a sub-string of a longer string, which has all the non-stop words in capitalized form. (3) At least one of the instances is not a sub-string of a longer proper noun. Then we say p is a PN recognized in a document collecti on. In our implementation, we submit p to Google. The actual pages of the top 10 returned documents serve as the document set. We need to emphasize that we use Google as a huge document set and use it to give essential statistics to our system when making a decision. Any document retrieval system, which can return documents that have a set of specified words appeared in the smallest text window ahead of doc uments having the set of words in larger window sizes, can be used . Using a large text corpus to get statistics is common in IR. The system in [6] use locally stored static corpus to do sta tistical computation for document retrieval. phrase_verification (p) . In line 8, this function verifies the existence of PN/DPs that are recognized by one of the above functions. It is a simplified is _doc_PN(p). To verify a potential PN p , This function looks for at least one instance of p in a document collection, such that th is instance satisfies the condition (1) and (2) as described in is_doc_P N(p). The criteria is less strict for phrase_verification(p) because p has been recognized by one of the above functions. We use th e documents returned by Google as the document collection. Fo r example, Minipar labels  X  vista window company  X  as a PN. An instance of this phrase is found in a document as  X  Vista Window Company is proud to ... X . A DP does not need to be verified by looking for instances in the documents, because of the definition of DP. pick_one_phrase(p1, p2) function . When PN/DPs overlap, two resolving processes are conducted from line 12 to 28. First, from line 13 to 18, we resolve the overlapped PN/DPs at the same level. The function pick_one_phras e(p1, p2) at line 15 picks one phrase from two partially overlapping PN/DPs p1 and p2. This function searches all the words of p1 and p2 together in a document collection, and counts the occurrences of p1 and p2 in the documents. The one w ith the larger number of count is picked. In implementation, we submit a ll the content words in p1 and p2 to Google. The top 10 returned documents form a document set. For example, in a query  X  pocket watch chains  X ,  X  pocket watch  X  is a Wikipedia DP and  X  watch chains  X  is a WordNet DP. The query  X  pocket watch chains  X  is submitted to Google together to retrieve documents.  X  Pocket watch  X  has 62 instances while  X  watch chains  X  has 39 in stances in the returned documents. So  X  pocket watch  X  is picked. At line 19, all the sub-phrases that are contained within the recognized PN/DPs are discarded. The intuition is that the words in a PN/DP should not be decomposed. From line 22 to 28, the overlappi ng problem between two PN/DPs at different levels is solved. For example, a query is  X  starlite drive in movie theatre  X , Wikipedia recognizes  X  drive in movie theatre  X  as a level-4 DP. Is_doc_PN(p) recognizes  X  starlite drive in  X  as a level-3 PN. The pick_one_phrase(p1,p2) at line 24 solves the overlapping problem. It is as sa me as the pick_one_phrase(p1, p2) at line 15.  X  Starlite drive in  X  got 26 instances and  X  drive in movie theatre  X  got 5. The former one is chosen. Line 26 is necessary because it is possible that a di scarded PN/DP candidate may still contain shorter valid PN/DPs. For example, even  X  drive in movie theatre  X  is discarded; its sub-phrase  X  movie theatre  X  can still be recognized as a valid DP. At line 29, the whole procedure ( line 2 to 28) will run again as long as there is candidate labele d as  X  X ot checked X  at line 26. Some noun phrases are not PN or DP, yet are grammatically correct and are used in the English language. If such a noun phrase contains exactly two words, we define it as a  X  X imple noun phrase X  (SNP). If it has three or more words, we define it as a  X  X omplex noun phrase X  (CNP). For example,  X  white car  X  is a noun phrase, but probably not defined in any dictionary. This type of noun phrases is also useful to improve retrieval effectiveness. We adopt Collins parser [9] to recognize them. Collins parser is a language parser with phrase structure annotation ability. Brill tagger [3] attaches part of speech (POS) tags to the query words, because Collins parser needs them. Before the parser processes a query, the query is pre-processed. If two words connected by a hyphen, they are either unchanged, merged as a single word or the hyphen is replaced by space. The one that is the most frequent one in a document set is chosen. Other punctuation marks except the apostrophes are removed. A headword (HW) of a phrase is the element that determines the syntactic function of the whole phrase. In a noun phrase, the head is the noun that refers to the sa me entity that the whole phrase refers to [11]. It plays the same grammatical role as the whole constituent [28]. For example,  X  art  X  is the HW of DP  X  performing art  X . Before actually recognizing SNP/CNP, the PN/DPs in a query are replaced by their HWs. A PN/DP is sent to Collins parser. The parser generates a parse tree. The root of the tree is the corresponding HW. The (PN/ DP, HW) mapping information is stored for future PN/DP restoration. The HW replacement is necessary because in some circumstances, the existence of the whole PN/DP may cause parsing error. The HW replacement may help the parser get the correct parse tree. Example 1. In a query "download pieces of me", "piece of me" is a PN. The whole query is a valid CNP. Without HW-replacement, Collins parser considers "download pieces" and "of me" as two phrases. With the HW-replacement, "pieces of me" is replaced by its HW "pieces". "download pi eces" is parsed as a noun phrase (download/NN pieces/NN). After restoring the "pieces of me", the whole title is correctly recognized as a noun phrase. A coordinate structure in texts involves several components that are connected by  X  X nd X  or  X  X r X . This sometimes indicates that there are implicit phrases in the text. For example, a query  X  X ain and contributing factor X  has two explicit phrases: the query itself and  X  X ontributing factor X . But there is also an implicit phrase  X  X ain factor X . This implicit phras e can not be recognized directly. Given a query with coordinate structure, we use a set of grammatical rules to find the implicit phrases. where CONJ is a coordinate phras e; CC designates either  X  X nd X  or  X  X r X ; Compi is an adjective or a noun; Comp X  is a noun phrase with at least one modifier, and Head is the headword of Comp X . For the noun phrase  X  X ain and contributing factor X , its Collins parsing structure is shown in Figure 2. The node  X  X ONJ/factor X  refers to a coordinate structur e, which includes a noun phrase, an adjective  X  X ain X , an  X  X nd X  and a noun phrase  X  X P/factor X , while the node  X  X P/factor X  in turn includes an adjective  X  X ontributing X  and a noun  X  X actor X . So the Comp1 is  X  X ain X , CC is  X  X nd X , and Comp X  is  X  X ontributing factor X , Mod is  X  X ontributing X , Head is  X  X actor X , and both m and n are 1. When this set of rules is fired, the new phrases are generated as: In the example, the new phrase  X  X ain factor X  is generated. Rule 1: NP :-CONJ NP X  Rule 2: CONJ :-[Comp i CC] n Comp n where NP is a noun phrase containing a coordinate noun phrase CONJ, and a noun or noun phrase NP X , CC refers to either  X  X r X  or  X  X nd X ; Comp i represents a component noun phrase or adjective phrase. In the noun phrase  X  X hysical or mental impairment X , NP X  is  X  X mpairment X , Comp 1 is  X  X hysical X , CC is  X  X nd X ; and Comp  X  X ental X . The parse tree is shown in Figure 3. When this set of rules is fire d, new phrases are generated as: In the above example, new simple phrases  X  X hysical impairment X  and  X  X ental impairment X  are generated. Rule 1: NP :-CONJ PP Rule 2: CONJ :-[Comp i CC] n Comp n where NP is a noun phrase which contains a coordinate noun phrase, PP is a prepositional phras e, CC designates  X  X r X  or  X  X nd X ; and Comp i represents a component noun phrase. For the noun phrase  X  X ystematic explorations a nd scientific investigations of Antarctica X , PP is  X  X f Antarctica X , Comp 1 is  X  X ystematic explorations X , CC is  X  X nd X ; and Comp 2 is  X  X cientific investigations X . When this set of rules is fired, new phrases are: In the example, phrases  X  X ystematic explorations of Antarctica X  and  X  X cientific investigations of Antarctica X  are generated. In some cases, the coordinate noun phrase itself may satisfy 2 or more set of rules, for example  X  X  main and contributing factor] in ship loss X  satisfies the rule in 6.3, while  X  X ain and contributing factor X  satisfies rule in 6.1. Thus, the new phrases are  X  X ain factor in ship loss X  and  X  X ontributing factor in ship loss X  as shown in Figure 2. After the punctuations, headwords and coordinate structure in a query are processed, the Collins pa rser is used to recognize the SNP and CNP in the query. The pseudo code of the whole algorithm is given in Figure 4. Generating Collins noun phrases . At line 6, Collins parser analyzes a modified query and returns a parse tree. The phrases, such as noun phrases, verb phras es and adjective phrases, are annotated in the tree. The noun phr ases are picked at line 7. They are labeled as the Collins NP s. In line 9, the sub-phrases of these Collins NPs are also coll ected as Collins NPs. This is to avoid missing some noun phrases that are not recognized by the parser. For example, in Figure 3, given a query  X  best compact sedan  X , the parse tree on the left is given by Collins parser. It captures the whole query as a noun phrase, but does not capture the embedded SNP  X  compact sedan  X . The correct parsing is given on the right hand side of Figure 5. The adjective  X  compact  X  modifies the noun  X  sedan  X . The adjective  X  best  X  modifies the noun phrase  X  compact sedan  X . Figure 5: Collins parser fails to generate correct parse tree The reason why we want to obtain an embedded simple phrase, such as  X  compact sedan  X  from a complex phrase such as  X  best compact sedan  X , is that a relevant document may not contain the complex phrase but may contain the simple phrase. Such a document can still have a similarity allocated to the simple phrase, which is part of the similarity allocated to the complex phrase. Verify Collins phrase . From line 10 to l5, if a Collins phrase t is verified by verify_collinsP(t), it becomes a  X  X erified Collins phrase X . The idea is that: The noun phrases in the parse tree are grammatically correct. But the parser can not tell if the phrases are meaningful in the real world text. A phrase to be verified must (1) not intersect with a recognized PN/DP, or (2) be a phrase that contains a recognized PN/DP and additional words. For example,  X  X pider Man tickets X  contains a PN  X  X pider Man X  and an additional word  X  X ickets X . If a potential noun phrase partially overlaps with a PN/DP, we prefer the PN/DP. For example, the query  X  blood pressure leve l X  has two shorter phrases of  X  blood pressure  X  and  X  pressure level  X . The former is a DP. So  X  pressure level  X  is discarded and will not go for verification. When a two-word phrase p is fed to verify_collinsP(t), the function searches p in a document collection to examine the existence of p . If an instance of p is found, such that this instance plus its boundary words is not a sub-phrase of the query, p becomes a verified Collins phrase. For example, to verify  X  tourist should not have the word  X  free  X  before it. If p has three or more words, it could be written in various ways. For example,  X  colin farrell wallpaper  X  can be written as  X  wallpaper of colin farrell  X . We verify these phrases as follows: (2) Otherwise, we look for a narrow text window in the In our implementation, a phrase p is submitted to Google. The top 20 retrieved documents are used as the document collection. Solving the phrase-overlapping problem . From 16 to 28, the problem, in which two verified Collins phrases overlap, is resolved. From line 17 to 21, tw o overlapped phrases having the same number of words are handled. pick_one_phrase(t1, t2) at line 18 counts the occurrences of t1 and t2 respectively as described in Section 2, except that it does not care about the capitalization of the words. The one with a higher count is preferred. From line 22 to 27, th e cross-level overlapping problem is solved. We adopt a lower level priority strategy: if two verified Collins phrases from different levels overlap, the one at the lower level (has fewer words) is preferred, because during the tuning we found this is better than picking the higher level phrase. The cross-level overlapping solving process does not include the original query (|Q|-level) since the original query overlaps with all of its sub-phrases. From line 29 to 32, a verified Collins phrase is labeled as SNP if it has 2 words. Otherwise it is la beled as a CNP. We use Example 2 as an illustration. Example 2 : Collins parser labeled the query  X  X ony dvd handycam X  as noun phrase. Its sub-phrases  X  X ony dvd X  and  X  X vd handycam X  were also added as Collins NPs. All three pass the verification_collinsP(t).  X  X ony dvd X  and  X  X vd handycam X  were examined by pick_one_phrase( ). In the context of  X  X ony dvd handycam X ,  X  X ony dvd X  not followed by  X  X andycam X  was found 1 time, while  X  X vd handycam X  not following  X  X ony X  was found 12 times. Thus  X  X vd handycam X  was chosen. No cross-level overlap solving was fired. The algorithm stopped. We tune our algorithm using a set of 400 multi-word web queries, randomly selected from a search engine company X  X  web query log, which has more than 170 t housand queries. The algorithm is then tested using another set of web queries from the same query log, and a set of TREC (Text REtrieval Conference) queries. Finally, we apply the recognized phrases from TREC queries to TREC document retrieval tasks, comparing the retrieval effectiveness of the IR system when (1) not utilizing phrases in the queries at all, (2) using phrases recognized by a baseline phrase recognition algorithm, and (3) using the phrases recognized by our algorithm. The results of the noun phrase recognition experiments are repor ted in recall, precision and f-score [29]. For a phrase type T, the precision (P) is the number of the correctly identified T phrases by our algorithm divided by the total number of the identified T phrases by our algorithm. Recall (R) is the number of the correc tly identified T phrases by our algorithm divided by the total number of the T phrases in the golden standard. The F-score is de fined as 2PR/(P+R). The results of the document retrieval experiments are reported as Mean Average Precision (MAP) and Geometric Mean Average Precision (GMAP) scores[31]. We randomly selected another 500 multi-word queries from the same query log used for selecti ng the tuning set. These two sets do not overlap. These 500 queries contain 205 2-word queries, 163 3-word queries, 86 4-word queries, 30 5-word queries and 16 6-word queries. Each of three graduate students labeled all of the PNs, DPs, SNPs and CNPs in the queries. Disagreements were solved by majority voting. These labeled phrases were set as the golden standard. Table 1 shows the noun phrase recognition performance of our algorithm using this 500-web-query set. In the PN row, three major reasons for the errors are: First, the web queries contain many less well-known proper names such as names of people, small companies and organizations. They are not defined in dictionaries. There are also not enough number of instances in the documents for them to be verified. This affects the recall. The second reason is the informal writing of the PNs, such as incomplete name and unofficial names. For example, a query  X  zenon z2  X  refers to a TV show  X  Zenon: the Zequel  X . The query used an unofficial name  X  z2  X . This incorrect title can not be verified. The third reason for the e rrors in PN is the capitalization of the words in the documents. In the procedure of is_doc_PN(p), we require the content words of a potential PN be capitalized in the documents. In some cases, non-PN phrases also have all of their content words capitalized to emphasize them. They satisify the procedure is_doc_PN(p), b ecoming the false positive PNs, which affects the precision. For example,  X  Return Policy  X  is such a phrase because some companies emphasize it as an important issue. The cases involving un-offi cial names and the emphases by capitalizing certain words require further study. 
Table 1: Scores of our algor ithm on the 500-web-query set PN 263 258 243 0.9240 0.9419 0.9328 DP 102 103 102 1 0.9903 0.9951 The DP row has one false positive case. The query is  X  young models  X  that refers to young persons posing for purpose of art or fashion. Wikipedia has an entry  X  young model  X  that is about a mathematical model. In the SNP row, an error type is that is_doc_PN( ) falsely recognizes some SNPs as PNs. This lowers the recall. The  X  return policy  X  is such an example. Another error type is that some PNs are recognized as SNPs. The th ird error type is that the pick_one_phrase( ) function made wrong choices. Both the second and the third error types lower the precision. In the CNP row, the major error type is that some CNPs partially overlap with recognized SNPs, wh ile we adopt a SNP-has-higher-priority strategy, the CNPs are di scarded. This affects the recall. In order to analyze the impact of each individual tool on our algorithm, we test the performances of these tools individually. Tables 2 to 5 show the performan ce of the individual tools in PN, DP, SNP and CNP recognition tasks re spectively. In tables 2 to 5, the  X  X ull X  lines refer to the corresponding data in Table 1. In Table 2, 5 tools are tested in the PN recognition respectively. Minipar got low recall (0.2000). Mi nipar uses grammatical rules to parse text, which needs context information for correct parsing. But the web queries are too short to provide enough contexts. Table2. Scores of individual tools on PN recognition task Name list 29 26 0.0981 0.8966 0.1769 The recall of WordNet alone is low (0.2548) because it only recognizes PNs that are defined in its database. Given that many of the PNs do not have entries in WordNet, they are missed. Wikipedia is an open dictionary. Its open editing architecture makes its data updated with the current affairs of the world. That X  X  why it got a much higher recall value (0.6844) than the non-open tools of Minipar, WordNet and Name List. The name list tool got the lowest recall (0.0981) but it is expected, since it only recognizes peopl e names. Its 0.8966 precision demonstrates its effectiveness. Is_doc_PN(p) alone gets high reca ll as Wikipedia does, actually the highest (.6868) among the 5 tools. Since we use the document set returned from Google, the docum ents are also kept up to date. These 5 tools all got reasonable precisions (0.81 to 0.89). Wikipedia has the highest pr ecision 0.9231 because it has high quality contents. The results show that a single tool usually has a low recall. One single tool is not enough to recognize most of the PNs in the web queries, since th e web queries cover very wide topics. Different tools must be us ed together to achieve desirable result. Table 3. Scores of individual tools on DP recognition task In Table 3, two individual tools are tested in the DP recognition task. The recall of WordNet is low (0.3922) due to the relatively small amount of term definitions it has, comparing to the various topics in the web queries. The precision of WordNet is also low (0.6452). In many cases, WordNet does not recognize a valid PN/DP. But a sub-phrase of th is unrecognized PN/DP is still a valid DP. This sub-phrase is recognized, becoming a false positive. For example,  X  brookdale community college  X  is a PN not recognized by WordNet. But  X  community college  X  is recognized. Wikipedia alone gets recall of 0. 9510 in DP recognition, which is due to its open editing architecture and large data collection. Its precision is 0.9327 because it has the same situation as that of WordNet: a sub-phrase of an unrecognized PN/DP is incorrectly recognized as a DP. Table 4. Scores of individual tools on SNP recognition task Collins0 : Collins phrase (baseline) Collins1 : Collins 0 + sub-phrase Collins2 : Collins 1 + verify_collinsP Full : Collins 2 + overlap resolving Table 4 shows the performances of the individual tools on SNP recognition. The Collins0 row uses Collins parser alone as a baseline. The phrases labeled directly by the parser are the results of Collins0. The sub-phrases of these directly labeled Collins phrases are not included (Line 9 Figure 4). The verify_collinsP(p) (Line 14 Figure 4) and the overlapping solving technique (Line 16-28 Figure 4) are not applied. We only remove the phrases that overlap with recognized PN /DPs. The result shows the effectiveness of the Collins parser alone. The 0.8136 precision is acceptable. The 0.5749 recall is low. This shows that the Collins parser alone is not enough for SNP recognition. The Collins1 is Collins0 plus usi ng the sub-phrases of the Collins phrases in Collins0. The direct output of the parser misses many correct phrases. We add the sub-phrases of these Collins phrases as additional SNP candidates. This will bring in many incorrect phrases so that the precision could be harmed. But we want to find if these additional sub-phras es can improve the recall. The incorrect and redundant phrases will be removed in  X  X ollins2 X  and  X  X ull X  rows. We see the recall of  X  X ollins1 X  increases substantially from 0.5749 to 0.9401. The additional sub-phrases do work. Unfortunately, the precision drops from 0.8136 to 0.6461. But the f-score is still improved from 0.6737 to 0.7659. Collins2 is Collins1 plus verify_collinsP( ), which tests if a phrase is actually used in the real wo rld. This verification removes 5 (243 to 238) incorrect phrases. Th e precision increases a little. It gets the same number of correct phrases so the recall is not changed.  X  X ull X  is the full algorithm configur ation. It is the Collins2 plus the overlapping problem solving step. This step aims to further remove the incorrect phrases introduced from the parser and the sub-phrases. Comparing to Collin s2, the precision increases from 0.6597 to 0.8142, which is the highest precision among the 4 configurations. The recall drops to a still acceptable degree of 0.8922 from 0.9401. The 0.8514 f-score is the highest among the four. Thus the overlapping solving technique is necessary. Table 5. Scores of individual tools on CNP recognition task Table 5 shows the CNP recognition results of the 4 configurations defined in Table 4. Collins0 is still the baseline. In Collins1, we still see that the additional sub-phrases help increase the recall (0.8151 to 0.9795) but harm the precision (0.9261 down to 0.7647). In Collins2, the verify_collinsP( ) does not help the performance but even decreases it a little. The full configuration greatly improves the precision (0.7604 to 0.9403) at a small cost of the recall (0.9349 down to 0.8630). And the full configuration has the highest f-score among the four. The behaviors of CNP recognition are the same as those in SNP recognition. We also use a set of TREC queries to test our algorithm. There are 249 queries from the ad-hoc tracks of TREC-6, 7, 8, and the robust tracks of TREC-12 and 13, 238 of which are multi-word queries. These 238 queries contain 70 2-word queries, 143 3-word queries, 23 4-word queries and 2 5-word queries. Three graduate students double-checked and labeled the PNs, DPs, SNPs and CNPs in these 238 queries as the golden standard . These TREC queries are not new. Some TREC related documents , which describe these queries, can be found in the top retrieved documents from Google when searching these queries. These doc uments should not be used to prove the existence of the phras es. So when we use Google to collect documents, we set the restriction that the returned documents must not contain the terms such as  X  X REC X ,  X  X uery X  and  X  X hrase X . This is done by adding  X -TREC X ,  X -query X  and  X -phrase X  as query restrictions when searching in Google. The overall performance of our syst em on the TREC query set is reported in Table 6. The only error in the PN row is a falsely recognized PN from is_doc_PN(p). The only error in the DPs is due to a query  X  food and drug laws  X , where the golden standard indicates  X  food and drug  X  and the entire query is a CNP. Our system recognizes " drug laws " as a DP. The errors in the SNP row are mainly caused by the incorrect pick_one_phrase( ) results. In the CNP row, some CNPs are missed because they can not be verified in the phrase_verification(p). 
Table 6. Scores of our algorit hm on the 238-TREC-query set DP 110 111 110 1 0.9910 0.9955 SNP 99 102 90 0.9091 0.8824 0.8955 CNP 159 146 146 0.9182 1 0.9574 We tested the performance of individual tools as we did in Section 4.1. The results are s hown in Table 7 through Table 10. Table 7. Scores of individual tools on PN recognition task Name list 0 0 0 0 0 In Table 7, the  X  X ame list X  tool did not recognize anything, because there are 3 people X  X  names in the PNs, all of which are foreign names. None of the  X  X ir st name, last name X  is found by is_name( ). Minipar only finds 2 correct PNs. This again shows that lacking context greatly affects its performance. WordNet got high precision but low recall. This is similar to its performance in Table 2, because of limited number of definitions. Is_doc_PN( ) X  X  performance in the TREC data set is worse than its performance in the web data set as shown in Table 2. A major error type is that a PN is not recognized but its s ub-phrase is incorrectly recognized as PN. Wikipedia had a perfect score because the PNs in TREC queries are rather well known. Table 8. Scores of individual tools on DP recognition task In Table 8, two individual com ponents are used to recognize DP respectively. WordNet X  X  low recall value means that its performance is still affected by the limited entries. Wikipedia X  X  performance is good and stable as it does in the PN recognition experiment. Table 9. Scores of individual tools on SNP recognition task Table 9 shows the performances of the individual tools on SNP recognition in the TREC query set. The four configurations are the same as those defined in Table 4. The pattern of the performances change is similar to that in Table 4. The pure Collins parser has acceptable precision but low recall in Collins0 row. Additional sub-phrases boost the recall but also damage the precision in Collins1. Verify_collinsP( ) improves precision a little in Collins2. At last, the full algorithm increases the precision at a small cost of the recall. The full algorithm still has the best performance. Table 10. Scores of individual tools on CNP recognition task Table 10 shows the CNP recognition results using the TREC query set. The 0.9560 recall in baseline Collins0 is very high, because the TREC queries have simpler gra mmar structures. They mainly consist of nouns, while the web que ries contain many verbs and prepositions. Other than this, the performances of the four configurations still follow the same pattern as those in Table 5. The full configuration obtains 0.9182 recall and 100% precision. This experiment is to test whet her obtaining more correct phrases yields higher information retr ieval (IR) effectiveness. We conducted three document retrieva l experiments, comparing the retrieval results using phrases recognized by our algorithm in Section 4.2, to those recognized by a baseline system, and to not using phrase at all. We use the IR system by Liu [20]. This system allows both phrases and single term s in the query. The similarity between the query and a document is represented as a pair of (phrase-similarity, term-similarity ). The phrase-similarity of a document is defined as the su m of the idf (inverse document frequency) weights of the phrases in common between the document and the query. If a document does not have the recognized phrase, its phrase-sim is 0. The term-sim ilarity is the usual term similarity between the query and the document, which is computed by using Okapi fo rmula [26]. Each query term appeared in the document contributes to the term-similarity, no matter it is in a query phrase or not. The phrase-similarity has high priority than the term-similarity. Given a query, the retrieved documents are ranked in descending order of their phrase similarity values. When documents have the identical phrase similarity value, they are ranked in descending order of their term similarities. So given a query, two documents D1 and D2 have similarities (x1, y1) and (x2, y2), respectively. D1 will be ranked higher than D2 if (1) x1&gt;x2, or (2) x1=x2 and y1&gt;y2. The 249 TREC queries are from 6 resources, the ad hoc tracks of TREC 6, 7, 8 and the robust tracks of TREC 12, 13, 14. TREC 14 queries are executed on the AQUAINT data collection [31]; other 5 sets are executed on the TREC disks 4 and 5 except the Congressional Records portion [30]. We simplify our phrase recognition algorithm to a weaker  X  X ingle-tool algorithm X . It serves as a baseline phrase recognition algorithm. It utilizes just one tool to recognize one type of phrases, while our full algorithm uses multiple tools for each phrase type. In this single-tool algorithm, Wi kipedia alone recognizes the PNs and DPs, because it yields the best results in the PN/DP single-tool experiments. The Collins pa rser alone recognizes SNPs and CNPs, because it is the fundamental component in the SNP/CNP part of our algorithm. The intuition is that our algorithm has better phrase recognition capability than this baseline. Better phrases should help retrieval system produce higher retrieval effectiveness. From Tables 7, 8, 9 and 10 we can see that this single-tool baseline algorithm ha s almost the same PN/DP/CNP recognition ability as the full algor ithm, and substantially worse SNP recognition ability. We conduct three experiments. (1) Feed the queries to the IR system, without recognizing any (2) Recognize the phrases in the que ries by using the  X  X ingle-(3) Recognize the phrases in the que ries by using our complete There are 11 single-term queries in the 249 queries. Their retrieval results are also included in the final results. So the difference between (1) and (2), and that between (2) and (3) are just caused by the differences of the phrases. The retrieval results are presented as mean average precision (MAP) [30] and geometric mean average precision (GMAP) [31] in Table 11. Comparing the scores of line 1 and 2 shows that all of the 6 query sets, when using the phrases from the baseline algorithm, get much higher scores than not using phrases at all (MAP gains from 17% to 54%, GMAP gains from 15% to 55%). This shows that the document retr ieval, with the recognition of the phrases, actually improves over just using single terms. Table 11 also shows that our full phrase recognition algorithm helps the retrieval achieve higher scores than the baseline phrase recognition algorithm does. The improvements are from 1.6% to 9.6% in MAP and 2.3% to over 26% in GMAP. This demonstrates that better noun phrase recognition yields better retrieval results. Table 12. Compare our results to the highest TREC 13 MAP System Old topic set New topic set Combined TREC 13 0.317 0.401 0.333 Our algorithm 0.348 0.428 0.364 Improvement 9.78% 6.73% 9.31% In TREC 13 [30], these 249 queries are used in the robust track. 200 of them from TREC 6, 7, 8 and 12 are called the  X  X ld topic set X . The other 49 are called the  X  X ew topic set X . In [30], the best MAP of the  X  X ld topic set X  is 0.317. The best MAP of the  X  X ew topic set X  is 0.401. The combined score is 0.333. We calculated the MAP scores for the old, new and the combined set for our algorithm from Table 11. Table 12 shows the comparison between our scores and the TREC 13 scores (Table 12 uses 3 digits because TREC 13 robust track scores were reported in this format [30]). The improvements of our scores over the best scores in these topic sets are 9.78%, 6.73% and 9.31% respectively. Furthermore, the 0.2931 MAP and the 0.3508 GMAP of the TREC 14 query set (Table 11) are 5.7% and 26% higher than the best corresponding scores reported in [31]. So our algorithm helps the IR system achieve higher scores than the best officially reported scores of the same query set and the document collection. Lima et al [17] studied the proper noun and phrase recognition problem. They reported 0.8786 precision and 0.9010 grammar coverage ([17] used  X  X rammar coverage X , which is an upper bound of the recall) on 100 company names; 0.7770 precision and 0.8000 grammar coverage on 100 person names; 0.7983 to 0.8200 precision and 0.9160 to 0.9560 gr ammar coverage on 200 short queries that have 1.59 words on the average, with an upper-bound of f-score at 0.8827 (denoted by Q1); and 0.8049 to 0.8139 precision and 0.7800 to 0.8520 gra mmar coverage on 200 queries that have at least 3 words with a 3.59 word average length, with an upper-bound of f-score at 0.8325 (denoted by Q2). To compare our result to theirs, we aggregate their company and person names together as a PN set, and compare it to the PN row of Table 1. We aggregate the 2-word DPs and the SNPs in Table 1 together (0.9302 recall, 0.8727 precision, 0.9006 f-score) to compare to their Q1 set. We aggregate the 3-or-more-word DPs (11 correct) and the CNPs in Table 1 together (0.8680 recall, 0.9427 precision, 0.9038 f-score) to compare to their Q2 set. The results are shown in Table 13.. Table 13. Comparison between Lima et al. and us in F-Score In this paper, noun phrases are classified into four types. We provide an algorithm that recognizes them. The algorithm is tested on a web query set and TREC query titles. High accuracies of recognition are obtained. Utilizing an up-to-date dictionary for recognizing proper names and we ll-defined phrase recognition seems to be a good method. Looking for instances in a document set is also good for less well-known proper names. Natural language parser and finding phras e instances in documents are good for recognizing SNP and CNP. Our document retrieval experiments also show that recognizing and utilizing phrases in the queries can substantially im prove retrieval effectiveness; furthermore, the quality of the phrases has a positive impact on retrieval effectiveness. The authors thank the reviewers for their helpful comments. This work is supported in part by NSF grants IIS-0738727 and IIS-0738652, and by an AOL research grant. The views of this paper are those of the authors, and do not represent those of NSF or AOL. [1] A Arampatzis, T Tsoris, C Koster, and T van der Weide. [2] D Bikel, S Miller, R Schwartz and R Weischedel. Nymble: a [3] Eric Brill. Transformation-Based Error-Driven Learning and [4] Bruce Croft, H Turtle, and D Lewis. The use of phrases and [5] J Callan and T Mitamura. Knowledge-based extraction of [6] Guihong Cao, Jian-Yun Nie, Ji ng Bai. Integrating Word [7] Nancy Chinchor. Overview of MUC-7. In Proc. of MUC. [8] Ken Chow, Robert Luk, Ka m-Fai Wong and Kui-Lam. [9] M. Collins, Head-driven statistical models for natural [10] David Evans and Chengxiang Zhai. Noun-Phrase Analysis in [11] Glossary of linguistic terms, by E Loos, S Anderson, D Day, [12] C. Fellbaum. WordNet, An electronic Lexical Database. The [13] Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong [14] Google: http://www.Google.com/apis/ [15] David Grossman and Ophir Fr ieder. Ad Hoc Information [16] S Jones and M Staveley. Phrasier: A System for Interactive [17] E Lima and J Pedersen. Phra se Recognition and Expansion [18] D Lin. PRINCIPAR -An E fficient, broad-coverage, [19] D Lin. Using collocation statistic s in information extraction. [20] S Liu, F Liu, C Yu and W Meng. An effective approach to [21] I Mani and R MacMillan. Identifying Unknown Proper [22] Christopher Manning and Hinric h Sch X tze, Foundations of [23] M Marcus, G Kim, M Marcinkiewicz, R MacIntyre, A Bies, [24] R Mihalcea and D Moldovan. An Automatic Method for [25] G Miller. WordNet: An On-line Lexical Database, Special [26] S Robertson and S Walker. Okapi/Keenbow at TREC-8. In [27] Egidio Terra and Charles Clarke. Frequency Estimates for [28] University of Glasgow, LILT project, [29] C. van Rijsbergen. Informati on Retrieval. Butterworth, 1979. [30] E Voorhees. Overview of th e TREC 2004 Robust Retrieval [31] E Voorhees. Overview of th e TREC 2005 Robust Retrieval [32] Wikipedia: http://en.wikipedia.org [33] Wei Zhou, Clement Yu, Neil Sm alheiser, Vetle Torvik and 
