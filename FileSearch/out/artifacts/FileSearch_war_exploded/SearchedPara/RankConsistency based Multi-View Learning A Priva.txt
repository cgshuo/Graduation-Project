 Complex media objects are often described by multi-view feature groups collected from diverse domains or information channels. Multi-view learning, which attempts to exploit the relationship am-ong multiple views to improve learning performance, has drawn extensive attention. It is noteworthy that in some real-world appli-cations, features of different views may come from different pri-vate data repositories, and thus, it is desired to exploit view re-lationship with data privacy preserved simultaneously. Existing multi-view learning approaches such as subspace methods and pre-fusion methods are not applicable in this scenario because they need to access the whole features, whereas late-fusion approaches could not exploit information from other views to improve the in-dividual view-specific learners. In this paper, we propose a novel multi-view learning framework which works in a hybrid fusion manner. Specifically, we convert predicted values of each view into an Accumulated Prediction Matrix (APM) with low-rank con-straint enforced jointly by the multiple views. The joint low-rank constraint enables the view-specific learner to exploit other views to help improve the performance, without accessing the features of other views. Thus, the proposed RANC framework provides a privacy-preserving way for multi-view learning. Furthermore, we consider variants of solutions to achieve rank consistency and present corresponding methods for the optimization. Empirical in-vestigations on real datasets show that the proposed method achiev-es state-of-the-art performance on various tasks.
 H.2.8 [ Database Management ]: [Database Applications  X  Data Mining]; I.2.6 [ Artificial Intelligence ]: [Learning] Algorithms, Applications, Experimentation Multi-View Learning; Rank Consistency; Privacy-Preserving c  X 
With the rapid development of the Internet, a huge number of rich media objects can be collected from various information chan-nels, e.g., nowadays a piece of news can be naturally described using text, audio, video clip and hyperlink[17][35]. In this pa-per, we focus on problems where data are gathered from multiple private information channels, i.e., features cannot be shared dur-ing processing. This problem occurs frequently in many scenar-ios, e.g., knowledge management/analysis on database with differ-ent views/tables whose access are prohibited from different users; multi-view data analysis when cooperation-competition relation-ship exists among enterprises or researchers. In a more concrete case of image analysis, researchers can extract their own features of images as well as get the retrieval results from Google. Never-theless they cannot obtain the image features used by Google, i.e., the features of researchers or Google are information from different private channels.

In order to analyze complex information from multiple channels, multi-view learning has attracted extensive attention [21] [24] [34]. Various multi-view learning approaches can be classified into four groups: multi-view subspace learning algorithms aim at obtaining a common subspace shared by multiple views and then learn models in that shared subspace [13] [27]; pre-fusion methods like multiple kernel learning [11] mainly fuse feature information by weighted combination of kernels produced separately on each view; late fu-sion methods combine outputs of the models constructed from dif-ferent view-specific features but leave the classifiers training phase unimproved [36]; disagreement-based methods focus on how to use unlabeled data to enhance the performance of learners via the com-patibility between two views [5] [31] [39].

The aforementioned methods have achieved great success in var-ious tasks, except scenarios with cooperation competition where access to multi-view features is restricted. Subspace style and pre-fusion approaches have to interact with view features, so they could not protect the privacy of different information channels. Late fu-sion methods only build a combined model based on outputs from each view, and therefore, the information from other information channels could not help improve learning ability of view-specific classifiers. Most existing disagreement-based multi-view learning approaches, such as co-training [5], rely on strong assumptions like redundant and independent views; thou-gh recent theoretical studies [32] [33] disclosed that weaker as-sumption is sufficient, new effective algorithms are still in design.
In this paper, we propose a novel framework working in a hybrid fusion manner. This framework, RANC (RANk Consistency), can be easily applied to the cooperation-competition multi-view learn-ing scenarios, because each view-specific learner is able to exploit the information from other views to improve the performance with-out accessing features of these views; in other words, the RANC framework provides a way to privacy-preserving multi-view learn-ing. The defined rank consistency is a criterion for seeking consis-tent predictions on multiple views. We formulate it by introducing an Accumulated Prediction Matrix (APM) which is stacked by pre-dicted values/labels of each view. It is notable that in the ideal case, the predicted results of multi-view models should be consistent on each view so the rank of APM should be equal to C  X  1 , where C is the number of classes. However, the rank of APM is usually larger than C  X  1 in practice for the potential inconsistency among multi-view predictions. Practically, lower rank of APM implies more view consistency. Fig. 1 is an illustration of APM on a multi-view dataset Reuters, which gives a spectrum plot on singular values of APM. The x-axis are singular values sorted in descending order. The spectrum is long-tail distributed started from the ( C singular value, which implies low rank property of APM. There are two obvious knee points (KP) in Fig. 1. KP 1 may be caused by the ambiguities among C classes in Reuters. Note that KP 1 vanished for separable problems. KP 2 at C  X  1 clearly reveals the rank consistency property in multi-view problems. Motivated by this observation from Fig. 1, RANC reduces the long tail compo-nents of APM to leverage predictions close to the ideal case, and apparently is naturally designed for applications with two or more views. Since those properties mentioned above can be estimated on the outputs from each view instead of directly using their features, the data privacy can be maintained to a greatest extent.
In our proposed framework, rank consistency criterion can be transformed into a rank regularizer term. Specifically, we use the truncated nuclear norm [15] to model it which can be incorporated with many different losses. Solving this framework leads to en-hance the classification ability of view-specific predictor. RANC constructs a hybrid fusion paradigm combining advantages of both pre-fusion and late-fusion methods. In this paper, we demonstrate this framework with square loss in detail and the implementation can be optimized effectively with both Proximal Gradient (PG) and Alternative Direction Method of Multipliers (ADMM) techniques. Furthermore, the paper also presents an accelerated version with rank-one update which can also get satisfying results. We empir-ically validate the effectiveness of our framework and our model achieves significantly better performance on various tasks. The main contributions of this paper can be summarized as follows: Section 2 gives the related work. The main proposed framework together with detailed solutions is presented in section 3. In section 4, empirical investigations on real datasets are discussed. Finally, we conclude in section 5.
How to exploit relationship among multiple views is fundamen-tal to multi-view learning approaches. As mentioned above, there are 4 categories of multi-view learning methods, and their strategies Figure 1: A typical singular value spectrum plot of APM on Reuters (5 views and 6 classes). APM is obtained with linear SVM on each view separately. of exploiting view relationship are of diversity to improve perfor-mance. Subspace approaches seek correlations between views, and subsequent tasks like classification will be performed in the learned subspace. E.g., CCA [13] finds a linear projection for each of two views and generates a shared subspace where the correlation be-tween two views is maximized. Some others, such as MvDA [18] and MvLPP [27], are of similar intuitions. Pre-fusion methods con-sider fusing features or feature derivations (e.g., kernels [11], dis-tances [37]) before the training phase. It is typical that they rep-resent the features as multiple kernel matrices and then combine them in a kernel space. Multiple kernel learning, one of the widely used pre-fusion methods, learns a linear [9] or nonlinear [10] com-bination of kernels for classification. These two types of methods, however, both require access to features of all views.

Late fusion strategies like RLF [36], aim at fusing the predicted values of each view while leaving multi-view features untouched. Nevertheless, the fusion process is not involved in classifier training phase, which makes late fusion not helpful in enhancing learners of each view. Moreover, it is required to retrain the whole late fusion model when the predictions on any of the views are changed, and this makes late fusion methods expensive and lack of flexibility.
Disagreement-based methods, such as Co-Training [5], focus on exploiting unlabeled data in semi-supervised learning scenarios. Traditional Co-Training enhances the performance of learners via the compatibility between views, but can hardly deal with multiple views [39].

By comprehensively considering the advantages and drawbacks of existing multi-view learning approaches, the proposed frame-work RANC defines a new criterion named rank consistency to characterize the consistency of predictions among multiple views. Rank consistency is implemented with matrix rank minimization regularizer. Rank minimization for a matrix is usually used in multi-class [12] or multi-label [22] problem, where different labels (output) are related to uniform input space (single input). In multi-label learning, rank based regularization is often used for reflecting the correlation between labels. However, in multi-view settings, classifiers are functional mappings which stretches across different views (input) and a single output. Consequently, it is required to de-fine the rank consistency regularizer with a different data structure, namely, the accumulated prediction matrix (APM). Formulated on prediction matrix APM, RANC can protect features of individual view from being accessed by learners built on other views. Besides, RANC is fully incorporated in training of individual classifier on each view to refining the classifiers with updated predictions.
In multi-view learning, an instance is characterized by multiple groups of features while they are only with one unified label. With-out loss of generality, we suppose there are K views and each view has n instances, where the first l of n instances are labeled, and the rest n  X  l are unlabeled. The i th instance x i can be represented as a collection of view-specific vectors x i;k  X  R d k , where d dimension of the k th view. For labeled examples, instance x label y i  X  { 0 , 1 } in binary classification problem. In multi-class cases with C classes, the label y i for instance x i is expanded to a vector with C elements, where y i;j = 1 indicates the i th instance is with label j , otherwise, y i;j = 0 . The whole data for k th view can be expressed as X k = [ x  X  1 ;k ; x  X  2 ;k ;  X   X   X  ; x  X  corresponding labels can be expressed as Y  X  { 0 , 1 } l  X  C sifier on each view is denoted as f k : x i;k  X  ^ y , where x k , ^ y  X  R C , and loss of instance i on view k is defined as  X  ( f k ( x i;k ) , y i ) . Then the prediction of X k on the k th view is combined into F k = [ f k ( x 1 ;k )  X  ; f k ( x 2 ;k )  X  R n  X  C , and predictions from all views can be stacked into an Ac-cumulated Prediction Matrix (APM), which can be defined as F = [ F
Ideal multi-view classifiers f k , k = 1 ,  X   X   X  , K , give identical outputs for a binary classification problem and consequently make the rank of APM F equal to one. Yet in practice, the rank of APM could not be exactly equal to one. As Fig. 1 shows, in a six-class problem, the singular values of APM in descending order reveal an  X  X xponential like X  decay with a long tail in the right part of Fig. 1. In particular, the second knee point appears at the 5 th sin-gular value. This is obviously consistent with our assumption that in practical cases, the rank of APM should be equal to the free-dom degree of class number C  X  1 . This phenomenon implies the predictions from all views tend to be with low rank when multi-view classifiers are not exactly identical. To induce consistency among multiple classifiers in multi-view learning, the rank consis-tency therefore can be defined as:
Definition 1. Rank consistency for predictions on multiple views on a certain data collection is an operator RC (  X  ) : R n  X  CK  X  R which defined on the APM F , and we define RC ( F ) = rank( F ) . We can summarize two fundamental properties of the defined rank consistency operator as follows:
Property 1. RC ( F ) reflects the prediction compatibility among views. A large value of RC ( F ) implies imperfection of prediction consistency, and a small value indicates predictions from all views are aligned well. ( Qualitative property )
Property 2. The expected rank consistency is C  X  1 , which is the freedom degree of label assignments in all views for concerned dataset. In particular, for a binary problem it is equal to 1 . ( Quan-titative property )
Rank consistency can be easily used as a regularizer in a learn-ing framework, which is helpful to achieve compatible and consis-tent predictions upon all views and can generate better classifier on each view. In next subsections, we first propose the whole RANC framework based on the defined rank consistency. After imple-menting concrete classifiers for each view, we show the proposed framework with rank consistency as a regularizer can be effectively solved with different techniques.
The key to the proposed method is the use of the rank consis-tency , which boosts performance of view-specific learner by seek-ing for prediction consistency. Benefitted from the rank consistency as a regularizer, we can bridge the maximization of label consis-tency among multiple views and the part of the classification task together. We define the RANC framework as: There are K views and F is the APM. The first term L k depicts the objective functions according to the property of the k th view. Fur-thermore, L i and L j can be different while the predictor on each view is self-adaptive. The second term, RC (  X  ) , is the rank consis-tency operator on APM, which leverages the prediction consistency to enhance learner on each view.  X  &gt; 0 is a balance parameter reflects the weights between view-specific objective and rank con-sistency regularizer.

Specifically, objective function L k on the k th view in Eq. 1 is generally with the form of a regularized empirical loss: here r (  X  ) is the regularizer for view-specific classifier.  X  &gt; 0 is a scalar coefficient to balance the weights of the two terms. Here the loss term  X  (  X  ) can take several forms, e.g., square loss or hinge loss for both linear and nonlinear problems. Eq. 1 indicates the classi-fier in  X  (  X  ) and the prediction results F k for instances are connected, which provides the possibilities of refining predictions with rank consistency by optimizing them simultaneously. To simplify the discussion, here we use the regularized square loss as the basic ob-jective function for each view, with linear classifiers W The feasible domain of F k is D . Constraint F 1 ;  X  X  X  ;l the prediction on labeled data the same as the ground truth to avoid collapsing of predictions, where F 1 ;  X  X  X  ;l k is the first l rows of the prediction matrix F k . In addition, it constrains predicted values into the same range as true labels by 0  X  F k  X  1 to avoid trivial solutions. In Eq. 2, b k  X  R C is the bias for current predictor. By centralizing both instances and predictions, the objective can be rewritten as follows: H = I  X  1 n 11  X  is the centralization matrix, where I is the identity matrix and 1 is a vector with all the elements equal to one. Without loss of generality, we can assume the data matrix X is centralized so that HX k = X k . Furthermore, by taking derivative of Eq. 3 w.r.t. W k and setting it to zero, we have: Combining Eq. 3 and Eq. 4, we simplify the rank consistency fram-ework in Eq. 1 into a form only relying on the predicted values F of each view, i.e., the APM F , as follows: min here Tr(  X  ) is the matrix trace operator.
Rank norm minimization is NP-hard and nuclear norm (or trace norm) [7] usually acts as a convex surrogate. For a matrix X R m  X  n , we assume its singular values  X  i , i = 1 ,  X   X   X  are ordered from large to small. The nuclear norm is defined as  X 
X  X   X  = various scenarios where rank norm minimization is required [12].
However, the quantitative property of rank consistency indicates consistent predictions of learners constructed on each view respec-tively always have C  X  1 freedom degree on sufficient large dataset. Blindly minimizing the rank of APM will break the natural struc-ture of multi-view predictions and may lead to degeneration of clas-sification performance. Therefore, a directional optimization ap-proach, which can conduct the RC( F ) until converging to C during the minimization procedure, is desired in our task. Inspired by [15], we use truncated nuclear norm as a surrogate function of the RC(  X  ) operator:
Definition 2. Given a matrix X  X  R m  X  n , the truncated nuclear norm  X  X  X  r is defined as the sum of min ( m, n )  X  r minimum singular values, i.e.,  X  X  X  r =
Different from traditional nuclear norm minimization with all singular values preserved, truncated nuclear norm minimizes sin-gular values with first r largest ones unchanged, which is more close to the true rank definition. If  X  X  X  r = 0 , there are only r non-zero singular values for X , and this explicitly indicates rank of X is less than or equals to r . Practically, in order to impel the RC( F ) directional to the freedom degree of the APM, it is clear to set r = C  X  1 in multi-view learning scenarios.

The truncated nuclear norm can be formulated as the equivalent form by the following theorem [15]:
T HEOREM 1. Given a matrix X  X  R m  X  n and any non-negative integer r ( r  X  min ( m, n )) , for any matrix A  X  R r  X  m R r  X  n such that AA  X  = I r , BB  X  = I r , where I r  X  R r  X  r identity matrix. Truncated nuclear norm can be reformulated as: If the singular value decomposition of matrix X is X = U V where is the diagonal matrix of singular values sorted in de-scending order and U  X  R m  X  n , V  X  R n  X  n . The optimal so-lution for the trace term in the above equation has a closed form solution: A = ( u 1 , u 2 ,  X   X   X  , u r )  X  and B = ( v 1 corresponds to the first r columns of left and right singular vectors. With Theorem 1, we can reformulate our objective function as: Because of the non-convexity of truncated nuclear norm, alternative approaches can be utilized for the optimization. A simple solution to Eq. 6 is alternating descent method. We can fix F and optimize A , B via SVD on F first, and then fix A and B to optimize F . When A and B are fixed, the subproblem is convex. The whole procedure is summarized in Algorithm 1.

In step 2, A and B can be obtained by SVD on F , which are the left and right singular vectors corresponding to the maximum C Algorithm 1 The pseudo code of RANC Requir e: training instances X k for each view, parameters  X  and 1: while True do 2: Use F to solve A and B as Theorem 1; 3: Solve F with fixed A and B , i.e., 4: if  X  F t +1  X  F t  X  F  X   X  then 5: Break; 6: end if 7: end while 8: Solve W k from F using Eq. 4. 9: return W k , classifier for each view. singular values. As the number of actually required singular vectors is rather small, partial SVD can be used for more of efficient [3]. The most computational cost step, however, is the subproblem for solving F in Eq. 7. We will give a detailed investigation on em-ploying Accelerate Proximal Gradient Descent Method (APG) [2] and Alternative Direction Method of Multipliers (ADMM) [6] for solving this subproblem in the following subsections.

It is noteworthy that the whole training procedure enhances the view-specific classifier only based on alternative updates of the pre-diction matrix F k and its singular vectors. After each round of updating F k , updated view-specific prediction will be passed back to corresponding learner of each view for model refinements. So RANC restricts interactions among multiple views within predic-tions without the access to original features of other views.
Note that when A and B are fixed, the problem is composed of two convex parts, i.e., a smooth loss term P 1 ( F ) and a non-smooth trace norm P 2 ( F ) : APG is suitable for solving Eq. 8 [16], which optimizes on a lin-earized approximation version of the original problem. In the t th iteration, if we denote the current optimization variable as F we can linearize the smooth part P 1 (  X  ) at F t as: Q ( F ) = P 1 ( F t ) + Tr(  X  X  X  P 1 ( F t ) , F  X  F t  X  ) where  X  P 1 ( F t ) = 2[ E 1 F t 1 , E 2 F t 2 ,  X   X   X  , E E k = H  X  X k ( X is the Lipschitz coefficient, which can be estimated by line search strategy [2]. Minimizing Q ( F ) w.r.t. F is equivalent to solving: F = arg min Algorithm 2 The pseudo code for solving Eq. 7 with APG Requir e:  X  ,  X  1 = 1 , initialize Z 1 and F 1 using true label matrix; 1: while Stop criterion doesn X  X  meet do 2: Line search for best step-size L 3: ^ F t = D L ( Z t ) , F t = Proj D ( ^ F t ) 6: end while 7: return F k .
 APG updates using the optimal solution in Eq. 9 at each iteration. Given the following theorem [7] about the proximal operator for nuclear norm: T HEOREM 2. For each  X   X  0 and Y  X  R m  X  n , we have Her e, D ( Y ) is a matrix shrinkage operator for matrix Y , which can be calculated by SVD of Y . If SVD of Y is Y = U V T , then we can solve Eq. 9 in a closed form: Note that the SVD approach in proximal projection is also time efficient, since it will only be applied to a thin matrix. The compu-tation of F acquires the gradient of P 1 , blocks of which are con-structed using view feature information. Note that the k blocks are view-independent, we can distribute updated view-specific predic-tion in updated F k back to each view and compute the E k the corresponding view. Then only K computation results with the same size of the prediction matrix F k are returned, through which feature privacy is maintained. After we get ^ F from Eq. 9, the fea-sible F can be obtained by projecting ^ F into the D as in [8], which can be denoted as Proj D ( ^ F ). As a consequence, we have the RANC framework with APG in Algorithm 2.
Considering the diversity between representation among differ-ent views, the prediction of classifier on each view may has its own bias. So it is more reasonable to learn an optimal bias for each view, combined with which the last prediction matrix among views can be more comparable. It is notable that the introduced biases here are different from the classifier bias on each view. Let b be the biases vector, where each element is the individual bias for the corresponding view. Together with learned optimal biases, we therefore can assume low rank property for the biased APM. Then the subproblem of Eq. 7 can be further formulated with defined E
To solve the problem of Eq. 10, an equality constraint U = F 1b  X  is further introduced and the problem becomes: which can be solved with augmented Lagrange dual form by max-imizing the dual variable :  X  &gt; 0 is a scalar for the augmented term. The problem in Eq. 11 is similar to those problems which can be solved with ADMM [6]. However, here we have three blocks of variables in Eq. 11, namely blocks with U , b and F , which is with very different properties rather than the ordinary ADMM problems. Using ADMM directly can hardly get converged [29], consequently we employ a vari-ant augmented Lagrange dual optimization technique [14] for our problem, which is a splitting variant of ADMM. After letting Q = 1b  X  , we can solve an optimal candidate of Q t +1 by taking deriva-tive of Eq. 11 w.r.t. Q and set it to zero, thus we have: Then the dual variable can be updated as: With the updated , the remaining variable U can be updated with an added proximal term: min According to Theorem 2, the optimal value has closed solution: Similarly , the last block of variable F updates iteratively with: where the scalar  X  &gt; 2 [14] and ^ F k has a closed form solution: the subscript k of (  X  X F t  X  t + 1 2 ) means the k th block correspond-ing to the k th view. Similarly, each prediction ^ F t +1 within its own view, i.e., each view receives the temporary result (  X  X F t  X  t + 1 2 ) k and combines its feature transformation E compute ^ F t +1 k in their own view respectively. The view indepen-dence of Eq. 15 ensures no interaction among views, which pro-tects the data privacy. After obtained ^ F t +1 k , we also need to project it into feasible domain as in last subsection by F t +1 = Proj Update iterations for U and F are separated and consequently can be implemented in a parallel paradigm. After that a renewal of the dual variable should be carried out: Algorithm 3 gives the sketch of this procedure. Following [14] [29], the whole procedure can be proved to be converged.
RANC can be solved with APG or ADMM effectively together with truncated nuclear norm regularizer. In this section, an acceler-ated variation of RANC denoted as RANC 1 is proposed to restrict the rank of APM with rank-one update. Recall that in the ideal Algorithm 3 The pseudo code for solving Eq. 7 with ADMM Requir e:  X  ,  X  , initialize F using true label matrix; 1: while Stop criterion doesn X  X  meet do 2: Solve Q as Eq. 12 3: Update as Eq. 13 4: Solve U as Eq. 14 5: Solve ^ F by Eq. 15 and F = Proj D ( ^ F ) 6: Update as Eq. 16 7: end while 8: return F k . case, APM in C -class problem should be with rank C  X  1 , i.e., for binary problems, rank of APM should be restrained to one, which provides facilitation for designing efficient rank-one approximation approach to RANC.

To simplify the discussion, we demonstrate the implementation in binary case where each view only gets a vector prediction value output, i.e., F k  X  R n  X  1 and the APM F  X  R n  X  K . For multi-class problem, the classification can be carried out with one-vs-rest strategies. The rank-one acceleration is brought forward based on the following property: rank( X ) = 1 , where X  X  R m  X  n only if there are two vectors u  X  R m  X  1 and v  X  R n  X  1 be decomposed into the outer product of u and v , i.e., X = uv
Consequently, we can reduce the rank consistency of APM to one for binary case simply by using two vectors to approximate APM, i.e., we define RC ( F ) =  X  F  X  uv  X   X  2 F , where the error is estimated using Frobenius norm. Note that there are diversities between different views, we add biases b  X  R K for view predictors to further facilitate the rank reduction of APM, i.e., we can redefine RC ( F ) =  X  F  X  1b  X   X  uv  X   X  2 F . Unique solution of u and v can be obtained by restricting u and v orthogonal. We therefore can reformulate the original problem in Eq. 5 as follows: The rank-one approximation formulation in Eq. 17 can be solved by alternative optimization. We first fix F for solving u , v and b , i.e., solving the following problem: Bias term b , which aims at finding an optimal mean for individ-ual prediction, can be solved as [23], i.e., b can be solved by b = F  X  1 / n , and then we can solve u and v with eigenvalue decomposition:
It is notable that in Eq. 18, only the eigenvector of the largest eigenvalue is needed, which can alleviate the computation burden greatly [3]. In the second step, with the fixed approximate rank-one matrix, we can update F in a closed form. In particular, with the fixed rank-one approximation, we can update the view-specific predictors as: where ( uv  X  + 1b  X  ) k gives the k th column of matrix uv Each ^ F k can be updated on each view separately using a similar strategy as aforementioned. F = Proj D ( ^ F ) is executed to project Algorithm 4 The pseudo code of RANC 1 Requir e:  X  ,  X  , initialize F using true label matrix; 1: while Stop criterion doesn X  X  meet do 2: Solve u and v as in Eq. 18 and Eq. 19 3: Update F using Eq. 19 and F = Proj D ( ^ F ) 4: end while 5: return F k .
 T able 1: Brief dataset description. Datasets with two views and more than two views are separated with a horizontal line. ^ F into the feasible domain. The above procedure should be iterated until convergence, which can be summarized in Algorithm 4.
We conduct extensive experiments on 15 real-world datasets with multiple views. We first give the general configurations. Then we comprehensively demonstrate effectiveness of our proposed frame-work (with three variants of solutions) in comparison with the state-of-the-art multi-view learning methods.
Data used in experiments are consisted of two-view and multi-ple views (more than 2 views) datasets. Description sketches of datasets are summarized in Table 1. The Course dataset [5] de-scribes web pages and the goal is to predict whether the given web page is a course page or not. The Citeseer dataset [26] is origi-nally made of 4 views, i.e., content, inbound, outbound, cites, on the same documents. We follow [4] to choose the content and cites view in our experiment. In the content view, the documents are characterized by 3703 words. The Cora dataset [26] has the same structure as Citeseer. Following [4] the content view and the cites view are used in our experiment as well. The WebKB dataset [26] contains webpages collected from four universities: Cornell, Texas, Wisconsin and Washington which have 5 categories, i.e., student, project, course, stuff and faculty. Data in WebKB are described with two views: content and citation. We treat WebKB in 4 separate datasets grouped by universities. The Advertise dataset [20] [40] has 5 views, i.e., caption and alt features in html description to-gether with base url, destination url and image url. Each exam-ple describes an image on the web, and the task of the dataset is to determine whether a given image may be an advertisement. The NewsGroup dataset [4] is of 6 groups extracted from the 20-Newsgroup dataset, i.e., M2, M5, M10, NG1, NG2, NG3. Every group contains 10 sample sets, and we choose the first set for all 6 groups in our experiment. There are 3 views in this dataset, which are made by different preprocessing methods for texts, namely us-ing Partitioning Around Medoids, Supervised Mutual Information and Unsupervised Mutual Information [4]. In our experiments, we will denote these types of data as News-M2, News-M5, News-M10, News-NG1, News-NG2 and News-NG3. The Reuters dataset [4] is built from the Reuters RCV1/RCV2 Multilingual test collection, multi-view information is created from different languages, i.e., English, French, German, Italian and Spanish [4].

We run each method 30 times for 15 datasets. 70% of the data are randomly picked up for training and the remaining are for test. In the training set, we randomly choose 30% as the labeled data, and the left 70% as unlabeled ones. Parameters are selected by 5CV from { 10  X  5 , 10  X  4 ,  X   X   X  , 10 5 } in the first split and fixed.
Since RANC leverages advantages of above four different types of multi-view learning paradigms, we should compare it with ap-proaches from these four approaches.
RANC framework is first compared with fusion approaches since it is a hybrid fusion method with advantages of pre-fusion and late fusion. In detail, we compare with 5 multiple kernel learning (MKL) methods in pre-fusion and the state-of-the-art late fusion method RLF (Robust Late Fusion method) [36]. The MKL meth-ods are Centered Alignment-Based MKL algorithms [9], original SOCP formulated MKL algorithm from [1], Simple MKL method proposed by [25], Group Lasso-based MKL method from [19] and Localized MKL algorithm [10], which are denoted as CABMKL, MKL, SimpleMKL, GLMKL, LMKL in the following contexts and tables respectively. In RLF, we use the best tuned classifier with least square loss as initialized predictor [36]. Furthermore, the en-semble of least square classifier (LS) is listed as a baseline. WNH method [30] which combines all views data together and then uses l 2 ; 1 -norm to perform view selection is also listed as a baseline.
It is notable that fusion methods can output only one classifica-tion result for multi-view data, so we compare the integrated result (mean accuracy and std.) of RANC with them. Win/tie/lose counts with t -test at significance level 95% are also recorded in table 2, where the highest accuracy on each dataset is bolded. Probability voting is used for RANC to obtain the final fusion results. Three variants of RANC solutions in subsection 3.3, 3.4 and 3.5 are de-noted as RANC PG , RANC ADM and RANC 1 respectively.

In Table 2, it can be clearly found that RANC gets better re-sults on most datasets. The RANC ADM returns more stable re-sults. RANC 1 , the accelerated solver, can also outperform all com-pared methods from the win/tie/lose counts on most datasets. As to the statistical test results, the RANC framework outperforms fusion methods in most cases, which validates the superiority of RANC. In general, RANC PG achieves better results on two-view datasets while RANC ADM performs better on datasets with more than two views. This may be due to the bias term introduced in RANC works when the number of views is large.

To investigate the efficiency of RANC 1 , we conduct more exper-iments on a linux cluster with 2.53GHz 12 cores and 48Gb memory. The average training time costs (in seconds) of all RANC series methods, late fusion method RLF and a baseline method WNH are recorded in Table 3. Six datasets are picked up for this time costs test. Table 3 evidently verifies the efficiency of RANC 1 65.95, 80.41, 161.52 times faster than RANC PG , RANC ADM and WNH respectively in average. It is noteworthy that for those compared methods, training time of base classifiers build on each view is not included in RLF, and the implementation of WNH is only with 10 trials. In other words, the superiority of RANC speed can be further enlarged in a fair play.
Subspace multi-view learning approaches can provide classifica-tion results on individual view. In this section, we compare RANC with multi-view learning method in subspace learning paradigms. WNH [30] is also listed since it can provide predictions on each sin-gle view. In this part of experiments, multi-view Linear Discrim-inant Analysis, multi-view Canonical Correlation Analysis, multi-RANC ADM represent for solving RANC with PGM or ADMM. RANC corresponds to the acceleration method.
 Table 3: Average training time on 6 datasets (in seconds). Late fu-sion and baseline methods are compared. Pre-fusion methods, how-ever, highly depend on classifiers invoked, so are not compared. vie w Locality Preserving Projections, multi-view Marginal Fisher Analysis (which are denoted as MV-LDA, MV-CCA, MV-LPP and MV-MFA) [27] are compared.

The Cumulative Accuracies Plots (CAP) on each view are shown in Fig. 2 for each compared method. In a CAP, there are K lines. The bottom line in each CAP gives the average accuracy on the first view. While the k th line provide the cumulative accuracy from the first view to the k th view, i.e., the cumulative accura-cies equal to the summation of accuracies on all previous views. Therefore, the top line gives the overall accuracies of all views and the gap (marked with different style of shadows) between any ad-jacent lines describes the classification accuracy of different views. From the results in Fig. 2, RANC achieves best performance on most datasets, and RANC ADM appears more stable than RANC and RANC 1 . Especially on some text datasets such as News-NG2 or Reuters, all of RANC method are significantly better than other compared methods.
It is notable that during the training phase, multi-view learning approaches in fusion style and subspace learning style require to repeatedly access to the features from all views, thus those ap-proaches are actually inappropriate for multi-view scenarios with private information channels. RANC and disagreement-based ap-proaches, however, have the ability of boosting performance only with the predictions rather than directly reading the original fea-tures in other views.

We compare RANC with multi-view disagreement-based meth-ods. Since most methods of this type are only applicable to two-view scenario, here comparisons on only two-view data are made in this section. Disagreement-based approaches can provide results on each view as well as an ensemble of two view X  X  results. In this ex-periment, we conduct comparisons with classical Co-Training [5], CoTrade (Confident Co-Training with data editing) [38] and Co-Lap (Co-Regularized Laplacian SVM) [28]. The detailed results of KCCA (Kernel CCA) [13] are also reported. In KCCA, RBF kernel is used with default parameters.
 The detailed results are listed in Table 4 where the classical Co-Training is denoted as CoTrain for short. From Table 4, it can be clearly found that RANC series methods achieve the best perfor-mance either on view-specific predictions or the final ensemble re-sults. The t -test is also performed at 95% significance level, which shows the significant superiorities of RANC framework.

Although both RANC framework and disagreement-based meth-ods can preserve the privacy of different information channels, the interactions of predictions are required in the training stage for both types of methods, which increases the chance of feature exposure from channels. Therefore this type of interactions should also be restricted during the training. To investigate the number of inter-actions during training, we conduct more experiments on the con-vergence of RANC 1 and Co-Training with results shown in Fig. 3. Due to the page limits, Fig. 3 gives the convergence plot on two datasets of two-views, i.e., Course and Texas, in one run. From the Fig. 3, it is notable that RANC 1 converges rapidly on these datasets, i.e., 5 iterations on Course and Texas for the final result. RANC PG and RANC ADM represent for the PGM or ADMM solutions to RANC. RANC Figure 3: Classification performance vs. number of interactions. RANC 1 V1 and V2 denotes for the changes of classification perfor-mance on the first and second view respectively. The same naming scheme is applied to CoTrain.
 As to individual views, RANC 1 gets converged after 4 iterations on Course and 12 iterations on Texas. While for Co-Training, the results are unstable in iterations. Furthermore, the performance of RANC 1 is superior to Co-Training to a great extent. It is notewor-thy that in each iteration only predictions rather than raw features are exchanged, which preserves the data privacy.
This paper presents a novel multi-view privacy-preserving frame-work RANC (RANk Consistency multi-view learning) to boost per-formance of the predictor constructed on each view by exploiting the relationship among features from multiple private channels. In this scenario, information of one view cannot be shared with oth-ers X . We put forward the rank consistency defined on the accumu-lated prediction matrix (APM) via stacking multi-view predictions, and integrate the rank consistency in a regularizer for improving the classification performance. Properties of RANC suggest em-ploying truncated nuclear norm to control the APM rank into an appropriate range. In our framework, view-specific learner can be enhanced without access to features of other views, therefore the data privacy is well-preserved. Three effective solutions for RANC are provided together including an accelerated variant. Extensive experiments in comparison with the state-of-the-art multi-view ap-proaches are conducted on real datasets, which demonstrates the superiority of RANC in handling multi-view data. Incorporating with different loss functions in RANC framework will be further investigated, and theoretical studies on effects of RANC in multi-view scenario where feature importance varies will also be carried out in future.
 The authors want to thank reviewers for helpful comments. This work was supported by 973 Program (2014CB340501) and NSFC (61273301, 61333014).
