 This paper examines a multi-stage retrieval architecture con-sisting of a candidate generation stage, a feature extraction stage, and a reranking stage using machine-learned models. Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs with three can-didate generation approaches: postings intersection with SvS, conjunctive query evaluation with Wand , and disjunctive query evaluation with Wand . We find no significant dif-ferences in end-to-end effectiveness as measured by NDCG between conjunctive and disjunctive Wand , but conjunc-tive query evaluation is substantially faster. Postings in-tersection with SvS, while fast, yields substantially lower end-to-end effectiveness, suggesting that document and term frequencies remain important in the initial ranking stage. These findings show that conjunctive Wand is the best over-all candidate generation strategy of those we examined. Categories and Subject Descriptors : H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation Keywords: query evaluation; postings intersection
One possible architecture for web retrieval breaks docu-ment ranking into three stages: candidate generation, fea-ture extraction, and document reranking. There are two main reasons for this multi-stage design. First, there is a general consensus that learning to rank provides the best so-lution to document ranking [13, 12]. As it is difficult to apply machine-learned models over the entire collection, in prac-tice a candidate list of potentially-relevant documents is first generated. Thus, learning to rank is actually a reranking problem (hence the first and third stages). Second, separat-ing candidate generation from feature extraction has the ad-vantage of providing better control over cost/quality trade-offs. For example, term proximity features are significantly more costly to compute than unigram features; therefore, by decoupling the first two stages in the architecture, systems can exploit  X  X heap X  features to generate candidates quickly and only compute  X  X xpensive X  term proximity features when necessary X  X hus decreasing overall query evaluation latency.
Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs in the candidate generation stage, comparing postings intersection with SvS, conjunctive query evaluation with Wand , and disjunctive query evaluation with Wand . Previous work has suggested that conjunctive query evaluation yields early precision re-sults that are at least as good as disjunctive query evalu-ation, but is much faster. We experimentally confirm this observation, and additionally show that postings intersec-tion (with results sorted by spam scores) yields substantially worse output within the same framework.

The contribution of this work is an empirical evaluation of common candidate generation algorithms in a multi-stage retrieval architecture. By fixing the feature generation and reranking stages, we are able to isolate the end-to-end effec-tiveness and efficiency implications of different algorithms.
We begin with a more precise specification of our three-stage architecture, illustrated in Figure 1. The input to the candidate generation stage is a query Q and the output is a list of k document ids { d 1 ,d 2 ,...d k } . In principle, this can be considered a sorted list, but that detail is unimportant here. These document ids serve as input to the feature extraction stage, which returns a list of k feature vectors { f 1 , f each corresponding to a candidate document. These serve as input to the third document reranking stage, which typically applies a machine-learned model to produce a final ranking.
This multi-stage retrieval architecture has recently been explored by many researchers. Some have examined the en-tire pipeline; for example, Macdonald et al. [14] assessed the impact of variables such as the number of candidate documents and the objective metric to use when training the learning-to-rank model (however, they did not explore effectiveness/efficiency tradeoffs with candidate generation). Others have specifically looked at candidate generation, e.g., postings intersection on multi-core architectures [17], dy-namic pruning [18], and approximate techniques [3]. There has been some work focused on the feature extraction stage, exploring how to best represent positional information; two studies have independently come to the conclusion that it is advantageous to store document positions in a document vector representation distinct from the inverted index [1, 2]. Finally, most work on learning to rank [13, 12] assumes such Figure 1: Illustration of a multi-stage retrieval archi-tecture with distinct candidate generation, feature extraction, and document reranking stages. an architecture, because the input to the machine-learned model is a set of feature vectors. Occasionally, this fact is made explicit: for example, Cambazoglu et al. [6] experi-mented with reranking 200 candidate documents to produce the final ranked list of 20 results.

There are two general approaches to candidate genera-tion: conjunctive and disjunctive query processing. In the first, only documents that contain all query terms are con-sidered, whereas in the second, any document with at least one query term is potentially retrievable. One popular algo-rithm is Wand [4], which can operate in either conjunctive or disjunctive mode with respect to a particular scoring model (e.g., BM25). Wand uses a pivot-based pointer movement strategy to avoid needlessly evaluating documents that can-not be in the final top k ranking. An alternative approach to candidate generation involves simple postings intersection, without reference to a particular scoring model. Culpepper and Moffat [8] demonstrated SvS to be the best algorithm for accomplishing this. Note that since SvS works by it-eratively intersecting the current result set with the next shortest postings list, it must exhaustively compute the in-tersection set. In this paper we explore both Wand and SvS for candidate generation.

It is a well-known fact that disjunctive query processing is much slower than conjunctive processing. However, it is unclear what impact output quality at the candidate genera-tion stage has on end-to-end effectiveness. Although Broder et al. [4] found conjunctive processing to yield higher early precision, the results were not in the context of learning-to-rank experiments. We hypothesize that end-to-end ef-fectiveness is relatively insensitive to candidate generation quality, due to an emphasis on early precision in most web search tasks today X  X e simply need to make sure there are enough relevant documents for the machine-learned model to identify. Thus, there is an interesting possibility that con-junctive query processing might lead to both good and fast results. Our paper explores this hypothesis.
We examined four approaches to candidate generation, outlined below. For each, we varied the number of candi-dates generated k , where k  X  { 100 , 250 , 500 , 1000 } . SvS Spam . Since SvS must compute the entire intersection set, to obtain k documents, we sort the intersection results by a static prior and return the top k . 1 SvS BM25 . In this approach, we first compute the full inter-section set, and then in a second pass we compute BM25 scores for every document in the intersection set. After the second pass, the top k documents are returned.
 Wand Con . We evaluate the query in conjunctive mode and return the top k documents based on BM25.
 Wand Dis . We evaluate the query in disjunctive mode and return the top k documents based on BM25.
 All algorithms were implemented in C and an equal amount of time was spent optimizing each to ensure a fair compari-son. All conditions used the same (non-positional) inverted index (even though SvS Spam does not need access to tf  X  X ) and for SvS BM25 we modified the accumulators to hold the tf  X  X  extracted from the postings for the second-pass scoring. We assumed that all index structures are retained in memory, so query evaluation never involves hitting disk. Experiments were performed on a server running Red Hat Linux, with dual Intel Xeon  X  X estmere X  quad-core processors (E5620 2.4GHz) and 128GB RAM.
We performed experiments on the ClueWeb09 collection, a best-first web crawl from early 2009. Our experiments used only the first English segment, which has 50 million docu-ments (247GB compressed). The Waterloo spam scores [7] were used as the static priors for SvS reranking.

For evaluation, we used three different sets of queries: first, the TREC 2005 terabyte track  X  X fficiency X  queries (50,000 queries total). 2 Since there are no relevance judgments for these queries, they were used solely for efficiency experi-ments. Second, a set of 100,000 queries sampled randomly from the AOL query log [16]. Our sample retains the query length distribution of the original dataset. Similar to the TREC 2005 terabyte track queries, we used these queries only to evaluate efficiency.

Finally, we used the TREC web track topics from 2009 X  2011, 150 in total. These topics comprise a complete test collection in that we have relevance judgments, but there are too few queries for meaningful efficiency experiments. We performed five fold cross validation, using three folds for training, one for validation, and one for testing.

We collected two figures of merit: the first was end-to-end effectiveness in terms of NDCG. For efficiency, we measured query latency of the candidate generation stage, defined as the elapsed time between the moment a query is presented to the system and the time when top k candidates are retrieved and forwarded to the feature extraction stage. To capture variance, we repeated runs five times.
We used a standard suite of features very similar to those described in previous work [15, 18]. They consist of ba-sic information retrieval scores (e.g., language modeling and BM25 scores), term proximity features (exact phrases, or-dered windows, unordered windows), query-independent fea-tures (e.g., PageRank, content quality score [7], etc.). Rele-vant features were computed across multiple fields: the entire document, anchor text, as well as title fields. In total, there are 91 features. 3
For the third stage reranking model, we used two different learning-to-rank techniques. In the first, we trained a lin-ear model using the greedy feature selection approach [15]. The model is iteratively constructed by adding features, one at a time, according to a greedy selection criterion. During each iteration, the feature that provides the biggest gain in effectiveness (as measured by NDCG [11]) after being added to the existing model is selected. This yields a sequence of one-dimensional optimizations that can easily be solved us-ing line search techniques. The algorithm stops when the difference in NDCG between successive iterations drops be-low a given threshold (10  X  4 ). This training procedure is simple, fast, and fairly effective.

Separately, we learned a LambdaMART model [5] using the open-source jforests implementation 4 [10]. To tune pa-rameters, we used grid search as suggested by the authors and selected the parameter setting that results in the largest gain in NDCG on the validation set: max number of leaves (9), feature and data sub-sampling (0.3), minimum observa-tions per leaf (0.75), and the learning rate (0.05).
Table 1 summarizes NDCG at different rank cutoffs and no cutoff for all combinations of candidate generation and reranking models. The first horizontal block of the table labeled  X  X pam X  refers to SvS Spam ,  X  X on. X  indicates SvS BM25 or
Wand Con (as both algorithms produce the same results), and  X  X is. X  shows the use of Wand Dis . Within each block of the table,  X  X M25 X  indicates reranking candidates with BM25, which serves as the baseline (note this only alters the postings intersection candidate results; in the other cases, this is equivalent to a reranker that does nothing);  X  X inear X  is the linear model learned using Metzler X  X  greedy-feature selection method; and  X   X  -MART X  is LambdaMART. Each column shows the number of candidate documents retrieved.
Candidates generated using SvS Spam yield very low end-to-end effectiveness. This shows that combining postings intersection and with a static prior does not provide a suf-ficiently discriminative signal to include relevant documents in the top k , where k is small relative to the size of the collection. The quality of the static prior is not to blame here, as the Waterloo spam scores have been demonstrated to be very helpful in reranking [7]. Of course, as we increase the size of k , the results of SvS Spam will approach that of the other candidate generation algorithms, but at the cost of more time spent performing feature extraction. It is clear Number of documents Figure 2: Number of rel, non-rel, and unjudged doc-uments in the top 20 for different candidate sizes using W AND Dis (averaged across folds). that both term frequencies and document frequencies cannot be ignored in candidate generation.

On the other hand, Wand Con (and SvS BM25 , which pro-duces exactly the same results) yields NDCG scores that are statistically indistinguishable from those produced by Wand Dis (with no cutoff, Wand Dis yields slightly higher scores, but the differences are not statistically significant). In other words, with BM25 scoring, conjunctive candidate generation and disjunctive candidate generation are equally good from the end-to-end effectiveness perspective. This finding is consistent with the results of Broder et al. [4] (but in a learning-to-rank context).

Increasing k improves NDCG at all rank cutoffs for can-didates obtained by SvS Spam . We observe the same trend for NDCG without cutoff in all settings. This is expected since larger k translates into higher recall at the candidate generation stage, i.e., more relevant documents are available to the reranker. However, with conjunctive and disjunctive query processing we see a decline in NDCG at rank cut-offs 5 and 20 as we increase the number of candidate doc-uments; this trend is consistent for both the linear model and LambdaMART. To examine this effect in a bit more detail, we computed the number of relevant, non-relevant, and unjudged documents in the top 20 obtained by rerank-ing Wand Dis . Figure 2 shows these statistics for different values of k . We see that as k increases, more unjudged doc-uments find their way to the top 20, while both the number of relevant and non-relevant documents decreases. Thus, this trend appears to be an artifact of the test collection and not a property of the candidate generation algorithms. Time (ms) Time (ms)
Figure 3 shows per-query latency of the candidate gener-ation stage for different values of k . On average, SvS is the fastest since the ranking function is query indepen-dent: score computation boils down to table lookups of spam scores. We see that SvS BM25 is on par with Wand Con for larger k values. SvS BM25 retrieves the full intersection set and computes scores for every document in the set regard-less of k ; therefore, increasing k has no impact on latency. On the other hand, Wand Con slows with increasing k (but is faster with a small k ). Conjunctive query evaluation with Wand can only terminate when a postings list is fully con-sumed. However, this termination is mostly independent of k ; the only factor affected by k is the set of heap operations performed during query evaluation. Finally, Wand Dis is not only the slowest overall, but latency grows with larger values of k faster than with conjunctive evaluation.
Our experiments show that conjunctive Wand is the best candidate generation strategy of those examined: in terms of end-to-end effectiveness, it is statistically distinguishable from disjunctive query evaluation using Wand , but much faster in query evaluation. Note that the  X  X lock-max X  opti-mization to Wand proposed by Ding and Suel [9] does not affect this conclusion X  X lthough the optimization increases disjunctive query evaluation speed, it remains slower than conjunctive processing. In addition, we also show that post-ings intersection with static priors yields very poor end-to-end effectiveness, at least with the size of the candidate sets we examined. This suggests that postings intersection, while an interesting algorithmic challenge since it represents the more general problem of intersecting two lists of sorted in-tegers, is not particularly useful by itself if one X  X  goal is to build effective and efficient search systems.
This work has been supported by NSF under awards IIS-0916043, IIS-1144034, and IIS-1218043. Any opinions, find-ings, or conclusions are the authors X  and do not necessarily reflect those of the sponsor. The first author X  X  deepest grat-itude goes to Katherine, for her invaluable encouragement and wholehearted support. The second author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob.
