 We consider the problem of building compact, unsupervised representations of large, high-dimensional, non-negative data using sparse coding and dictionary learning schemes, with an emphasis on executing the algorithm in a Map-Reduce environment. The proposed algorithms may be seen as par-allel optimization procedures for constructing sparse non-negative factorizations of large, sparse matrices. Our ap-proach alternates between a parallel sparse coding phase im-plemented using greedy or convex ( l 1 ) regularized risk min-imization procedures, and a sequential dictionary learning phase where we solve a set of l 0 optimization problems ex-actly. These two-fold sparsity constraints lead to better sta-tistical performance on text analysis tasks and at the same time make it possible to implement each iteration in a single Map-Reduce job. We detail our implementations and opti-mizations that lead to the ability to factor matrices with more than 100 million rows and billions of non-zero entries in just a few hours on a small commodity cluster. I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation
Let us assume that we are given a collection of N data points or signals in a high-dimensional space R D : x i  X  R
D , 1  X  i  X  N . Let h j  X  R D , 1  X  j  X  K denote a dic-tionary of K  X  X toms X  which we collect as rows of the matrix H = ( h 1 . . . h K ) T  X  R K  X  D . Given a suitable dictionary H , the goal of sparse coding is to represent datapoints ap-proximately as sparse linear combinations of atoms in the dictionary, i.e., x i  X  P K j =1 w ij h j = H T w i , where the co-efficient vector w i  X  R K typically only has a few non-zero elements. Of course, the effectiveness of sparse coding in lending a high-quality compact representation to the entire dataset depends on whether the data distribution concen-trates around lower-dimensional subspaces, and crucially, on whether the choice of the dictionary appropriately captures this structure.

Spread over several decades, the quest for an appropri-ate dictionary that is well-suited to a class of signals (e.g. natural images) has generated a vast body of work in Signal Processing and Computational Harmonic Analysis, encapsu-lating the development of Fourier, DCT, Wavelets and other bases, and matured into the establishment of impactful cod-ing standards. Broadly speaking, in these efforts, dictionary design centers around pre-specified mathematical functions that are justified by optimality proofs and error bounds for the class of signals they attempt to model.

An alternative to the aforementioned approach to mod-eling is learning a dictionary [9] from the data itself. Led by the emergence of the web, we are increasingly encounter-ing new sources of data that more often than not are com-pletely unstructured, very high-dimensional, and expectedly voluminous. Dictionary learning now becomes appealing, as large web-based repositories essentially provide unrestricted samples of natural signals, thereby allowing one to bypass the limitations of analytical formulations that are applica-ble to restricted signal classes. However, to make dictionary learning feasible, we are confronted with the formidable chal-lenge of scaling associated algorithms to web-scale datasets.
Fortunately, the growing ubiquity of multi-core and clus-ter systems potentially offers a solution. To make parallel systems more accessible to machine learning researchers and practitioners, recent years have seen the development and adoption of the Map-Reduce (MR) programming model [7]. Map-Reduce programs, when deployed using engines like Hadoop, can be executed on large clusters in a fault toler-ant fashion. Higher-level declarative [11] and imperative [10] programming frameworks to ease the implementation of learn-ing algorithms on top of Map-Reduce have also been de-veloped. This paper delves into the problem of carefully designing and implementing large-scale sparse coding and dictionary learning methods that afford efficient paralleliza-tions on distributed computing environments like Hadoop.
We focus on text-like domains that generate very high-dimensional, non-negative, and sparse data matrices. Our framework may also be viewed as providing sparse Non-negative Matrix Factorizations (NMF) [14, 12, 16]. Sparse NMFs and dictionary learning on document collections has recently emerged [13] as an appealing alternative to popular Bayesian topic models such as the Latent Dirichlet Alloca-tion [4]. In very recent work, [1] present the first theoreti-c al results, under certain assumptions, on provably correct NMF algorithms. For various extensions and applications of NMFs, see [6].

Our contributions may be highlighted as follows: (1) We describe highly efficient parallel algorithms for sparse cod-ing and dictionary learning. These algorithms are imple-mented using multithreading on a multi-core system and the Map-Reduce programming model on a cluster-system. A hallmark of the method is that it is possible to imple-ment each iteration of the optimization algorithm using a single Map-Reduce job. We note several recent efforts [18, 15, 17] on scaling up related algorithms. (2) Our algorithms alternate between a distributed sparse coding phase and a sequential in-memory dictionary learning phase. For sparse coding, we organize non-negative variants of both Orthogo-nal Matching Pursuit (OMP) and Lasso [9, 5] around very efficient and light-weight updates. Our work lends a bet-ter understanding of the strengths and weaknesses of greedy versus convex methods in the context of parallel sparse non-negative matrix factorizations, a natural comparison that we have not seen reported elsewhere in the literature. (3) In contrast to existing Sparse NMF methods, we also impose hard sparsity constraints on the dictionary atoms. We ob-serve that the related subproblem in coordinate descent can be solved exactly. For textual problems, dictionary sparsity imposes the natural statistical prior that meaningful top-ics have support on only a small, distinctive set of words. As a complimentary computational benefit, these dictionary sparsity constraints also enable fast in-memory updates of dictionary atoms. (4) We present empirical results on both small and large problems on multi-core and cluster systems. We show that datasets with more than 6 billion non-zero entries -amongst the largest demonstrations of such tech-niques -can be factored in a matter of hours on a moderately sized cluster, exhibiting near linear scaling with respect to data size, number of cores, and dictionary size.
We are given N datapoints, { x i } N i =1 , x i  X  R D , and H = ( h 1 . . . h K ) T  X  R K  X  D denotes the dictionary matrix com-prising of K atoms which are optimization variables in the dictionary learning problem. We denote the associated sparse coding variables by { w i } N i =1 , w i  X  R K and collect them as rows of the matrix W = ( w 1 . . . w N ) T . We will use w denote the j th element of w i . We pose the following class of joint optimization problems over W and H , subject to the following constraints: where i runs from 1 to N , j runs from 1 to K , p = { 0 , 1 } , q = 0, and  X ,  X ,  X  are real-valued parameters.

The first term in the objective function above, Eqn 1, measures the squared reconstruction error. It is known to be non-convex in W and H . The second term is a regular-izer that enforces the learnt dictionary to be close, in Frobe-nius norm, to a prior H 0 , which may be available e.g., if models are being learnt periodically over time (if H 0 is not available,  X  may be set to 0). Equations 2,4 impose non-negativity constraints. The constraint on k w k p in Eqn 3 implements sparse coding by imposing a bound  X  on a spar-sity inducing norm ball: p = 0 casts this constraint in terms of the l 0 pseudo-norm, i.e., the number of non-zeros in w should not exceed  X  , while p = 1 uses the l 1 ball of radius  X  which is a convex set. Equation 5 imposes similar sparsity constraints on the dictionary elements. We mainly restrict our implementation to hard sparsity constraints, i.e., q = 0, and bound the number of non-zeros per atom exactly by  X  . As we outline in section 4, the associated rank-one subprob-lem can actually be solved exactly despite the intractability of working with l 0 norm in general. This allows us to apri-ori control the memory requirements for maintaining H as a sparse matrix and turn its updates across iterations into very cheap sequential operations. Our approach therefore enforces double sparsity in both the factors. Finally, for van-ishing  X  and sufficiently large values of  X ,  X  , it is easy to see that the reconstruction error is invariant under the trans-formation H T w i = H T QQ  X  1 w i for any invertible K  X  K scaling matrix Q , making the minimization ill-posed. In particular, for clustering applications, it is common to as-sign x i to the cluster arg max j w ij , which can be arbitrarily changed by rescaling. We therefore require dictionary ele-ments to have unit l 2 norm; other constraint sets which are convex can also be instead used [16].

Our optimization strategy is cyclic Block Coordinate De-scent (BCD) [3]. Each iteration of the algorithm cycles over the variables W and h 1 . . . h K , optimizing a single variable at a time while holding others fixed. It is easy to see that since the objective function in Eqn. 1 is separable in { w i.e. rows of W , they can be optimized in parallel. We call this the Parallel Sparse Coding phase of the algorithm: parallel for i = 1 . . . N : In the next section, we outline non-negative variations of Orthogonal Matching Pursuit and Lasso to solve these sub-problems for p = 0 and p = 1 respectively.

In the Dictionary Learning phase of BCD, we cycle sequentially over h 1 . . . h K , i.e., rows of H , keeping W fixed and solve the resulting subproblems. Due to hard spar-sity constraints, as well as appropriate aggregations from the sparse coding phase, we can guarantee that dictionary variables can be conveniently held in memory as a sparse matrix H , and manipulated very fast with efficient and ex-act sequential updates as outlined in section 4. We declare convergence if the overall mean reconstruction error fails to improve significantly relative to its previous value.
In the most general non-convex optimization setting, limit points of a BCD process may not be stationary points of the objective function. Even for a stationary point, apriori theo-retical guarantees concerning model quality typically cannot be established. In practice though, such models nonetheless often yield good solutions that turn out to provide valuable empirical insight. We next describe the details of our ap-proach.
Wi thout non-negativity constraints, for a fixed H , Eqn 7 is the classic least-squares sparse regression model, for which greedy orthogonal matching pursuit (OMP) and Lasso are two very well-studied and widely successful algorithmic frame-works [9, 5], with intriguingly similar theoretical guarantees concerning correctness.

Since the dictionary H is also varying in our setting, the contrast between greedy and convex methods becomes par-ticularly interesting for the following reasons. In each it-eration, OMP greedily selects the atom that best helps in reducing the current reconstruction residual. All the atoms are then reoptimized. Lasso, on the other hand, requires numerical optimization procedures, e.g. FISTA [2], that can handle composite objectives with a smooth and a non-differentiable component. Due to its simplicity, OMP tends to be faster than Lasso. The per iteration complexity of OMP involves greedy selection of an atom from upto K can-didates, and solving a least squares system for refitting. For Lasso, it involves taking the gradient of the objective and adapting the line search stepsize. OMP, by design, converges in no more than  X  iterations while Lasso (using FISTA) con-verges to an -optimal solution in O ( 1  X  ) iterations. One advantage of OMP is that it imposes a predictable sparsity structure on W and hence we know exact memory require-ments upfront. However, in the context of the full BCD algorithm, OMP-based sparse coding is not guaranteed to converge (using currently available theoretical results) with-out making assumptions on the dictionary at every iteration. For sparse coding with Lasso (assuming q = 1 or  X  = D , con-vex uniqueness constraints and exact minimization), well-known convergence results [3] imply that the full BCD al-gorithm will show monotonic decrease and converge to a stationary point. In practice though, OMP may still offer consistent descent to a good solution. We also point out that across BCD iterations, Lasso can utilize warm starts from previously found solutions, while OMP essentially needs to start from scratch. In a distributed setting, it is furthermore worthwhile to investigate if OMP and Lasso offer different load balancing characteristics.
 These differences motivate us to study both OMP and Lasso for sparse coding and dictionary learning, and bench-mark them in a parallel environment. We next derive very efficient implementations of non-negative versions of these algorithms. For simplicity in the exposition below, we as-sume that the data points are normalized to unit l 2 norm, i.e., k x i k 2 = 1.
We are interested in solving arg min w  X  0 , k w k R ( w ) = k x  X  H T w k 2 2 denotes the reconstruction error. We will express both Non-negative OMP (NOMP), and also Non-negative Lasso(NLASSO) below, compactly in terms of the following, where s may be interpreted as the vector of cosine similar-ities between signals and dictionary atoms (since both dic-tionary elements and data points are unit normalized) and S is the (small) K  X  K matrix of inter-atom similarities. For example note that the objective function can be written as,
Let A  X  { 1 . . . K } denote the support of a candidate solution w , i.e., A = { i : w i &gt; 0 } . NOMP starts with w = 0 , A = {} and builds the solution by greedily adding variables that help reduce the current residual, r = x  X  H
T w = x  X  P plement of A . Let j  X  A c be a candidate variable for in-clusion. Its quality is measured how much, acting alone, it can help reduce the current residual. Since k h j k 2 we can easily see the following: arg min w arg min w of w j is w j = max( r T h j , 0). Furthermore, note that r x h j  X  ( Hh j ) T w = u j , where u was defined in Eqn 9. Therefore the OMP selection criteria, quite simply, evalu-ates to ( u j  X  u j + ) 2 for a candidate variable j , where we use the notation u j + = max( u j , 0). Equivalently, the re-duction in residual norm offered is u 2 j +  X  2 u j u j + can maximize over. As in unconstrained OMP, we now di-gest the chosen variable into A and then reoptimize over it with cylic coordinate descent. Given inputs s , S ,  X  and convergence parameters ,  X  , the entire algorithm, can be organized around very simple and efficient update rules as follows, The coordinate descent procedure for refitting is guaran-teed to converge using standard BCD convergence results [3]. Given inputs x and H , NOMP offers monotonic decrease in the reconstruction error of x . In the dictionary learning set-ting, however, what we cannot guarantee is global decrease with respect to the previously found dictionary. Nonethe-less, as we report in section 6, in hundreds of experiments on real world data, we always saw stable descent behavior.
We rewrite the NLASSO problem, Eqn 7 with p = 1, equivalently as: which is a composite objective comprising of a smooth term R ( w ) and a non-smooth indicator function for the simplex defined as  X  ( w ) = 0 if w  X  0 , k w k 1  X   X  and  X  otherwise. We use an accelerated proximal method, FISTA [2], to han-dle such an objective function. In proximal methods, the idea is to linearize the smooth component, R , around the current iterate (say w t ), and minimize min where  X  R ( w ) = s  X  Sw (recall the notation from Eqn. 8). The third term above, called proximal term, keeps the up-date in a neighborhood of the current iterate w t where R is close to its linear approximation. L &gt; 0 is a parameter, which should essentially be an upper bound on the Lipschitz constant of  X  R and is typically set with a linesearch. We can rewrite this problem as, w ? = min w  X  R K 1 2 k w  X  u k 2 w here u = w t  X  1 L  X  R ( w t ) . The above minimization func-tional (mapping u to w ? ) is also called the proximal opera-tor . For our purposes, it reduces to the projection operator to the convex set w  X  0 , k w k 1  X   X  . An efficient linear time algorithm for such a projection onto the simplex is given in [8]. The rest of the FISTA details remain unchanged; we point the reader to [2] for more details. It is easy to see that we can also equivalently solve the more familiar penal-ized form of Lasso: arg min w R ( w ) +  X  0 P j w j +  X  0 some one-to-one correspondence between  X  0 and  X  and with  X  simply being an indicator function for the non-negative orthant w  X  0. In this case, the associated prox operator our dictionary and data normalization, Lasso screening tests proposed very recently [19] can be immediately adapted to the non-negative case and applied to discard elements of w upfront that are guaranteed to be zero-valued.
We now discuss the dictionary learning phase where we update H . Our strategy for this phase is reminiscent of the Hierarchical Alternating Least Squares (HALS) algo-rithm [6] which solves rank-one minimization problems to se-quentially update both the rows of H as well as the columns of W (while we perform parallel row updates for W by in-voking the sparse solvers developed in Section 3). Let us denote the data matrix as X = ( x 1 . . . x N ) T  X  R
N  X  D and let v k be the k th column of W , i.e., W = ( v 1 . . . v K ). Let R k = X  X  P i : i 6 = k v i h T i be the residual matrix excluding the k th atom. We denote the constraint set for learning dictionaries as
The BCD subproblem of optimizing h k keeping other atoms fixed becomes a rank-one matrix approximation problem, where k A k fro denotes the Frobenius norm of matrix A . It is easy to see that this reduces to a projection problem, h k = arg min h  X  X  k h  X  q k k 2 2 , where the vector q k is given by, Note from the numerator that the residual term and the prior compete for contributing to q k .
 Efficient computation of Residual matrix times a vector: Since R k is dense, we want to avoid computing it explicitly and instead use the following, The second term, P i 6 = k h i ( v T i v k ), can be expressed com-pactly and efficiently as follows: Let G = W T W , which is a small K  X  K matrix, and let g = G k , its k th column. Set g k = 0. Then its easy to see that H T g = P i 6 = k h i ( v Therefore, we have, Projection to the set C of Unit Norm Non-negative Sparse Vectors: We now consider the following projection problem: h ? = arg min h  X  X  k h  X  q k 2 2 . First, let h be any  X  -sparse vector in R D and let I ( h ) be its support. Let i be a permutation of the integer set 1  X  i  X  D such that q , . . . q i D is in sorted order and define I ? = { i 1 , . . . , i where s is the largest integer such that s  X   X  and q i 1 &gt; . . . &gt; q i s &gt; 0. Now its easy to see that, arg min Therefore, the exact solution to this problem given by, where I ?c denotes the complement of I ? . Note that if I turns out to be empty while running dictionary learning phase in BCD, h ? I ? is undefined, and in this case, we do not update the previous value of that variable. However, we never observed this to happen in practice.

Hence, all we need to solve the projection problem ex-actly is to find upto the top- X  non-negative values of q and normalize them to unit norm. We also note that in the dictionary learning phase we require the summary statis-tics: G = W T W  X  R K  X  K and columns of the matrix X
We briefly recap the developments so far and provide an outline for this section. Our parallelization strategy hinges on two observations: (a) the objective function is separable in the sparse coding variables (rows of W ) which therefore can be optimized in an embarrassingly parallel fashion, and (b) by enforcing hard sparsity on the dictionary elements, we turn H into an object that can be manipulated very efficiently in memory. Our implementation is carefully or-ganized around efficient matrix computations and one-pass aggregation of summary statistics. We begin by describing our data structures (summarized in Table 1) and then out-line our multicore implementation which requires that the data matrix should fit in memory. We relax this assumption in our cluster implementation sketched in Figure 1 which runs each iteration of the algorithm by making a single pass over the data (i.e., one MapReduce job). Note that our ap-proach generalizes to other large-scale matrix factorization problems where separability, sparsity and in-memory com-putation can be similarly exploited.
 Efficient Matrix Computations: Table 1 records various m atrices, their storage schemes, computation strategy and whether they are materialized in memory in our single-node multicore and Hadoop-based cluster implementation. We use compressed row storage for holding the read-only static sparse matrix X . For H , and W if it is materialized, we use a dynamic sparse matrix data structure which essentially is an array of pointers to the non-zero indices and values for each row, allowing sparsity structure of rows to change across the iterations. Note that sparsity  X  X ompresses X  the bigger dimension ( D ) for H as opposed to the smaller di-mension ( K ) for W , making it reasonable to manipulate H in-memory. Also note that the maximum memory require-ment for W can be bounded by O ( N  X  ) for NOMP, while NLASSO does not offer similar predictability beforehand. Table 1: Matrices: Storage and Materialization
Recall that in the parallel sparse coding phase, for NOMP a nd NLASSO, we can carry out computations using s = Hx and S = HH T (see Eqn. 8). The matrix HX T need not be materialized as its columns  X  which are the s vec-tors  X  can be computed on the fly, used, and then imme-diately discarded. The matrix S is a dense symmetric ma-trix whose lower half is stored in the standard lower-packed storage format, and computed by K sparse matrix-vector products: Hh 1 . . . Hh K . Alternatively, matrix-vector prod-ucts against S may be computed in time O ( K X  ) implicitly as Sv = H ( H T v ). Likewise, note that dictionary learning phase requires G = W T W and X T W . It is profitable to express these matrices as the aggregation of rank-one ma-These matrices are the summary statistics we need for dic-tionary learning. Hence, a single pass over the data during the parallel sparse coding phase enables us to perform these aggregations, and prepare these matrices for the dictionary learning phase. Furthermore, in the aggregation above, we exploit the sparsity of x i and w i by updating only the non-zero cells. Finally, in the projection step of the dictionary learning phase, Eqn 15, we use priority queues for finding top  X  non-negative elements in time O ( D log(  X  )). Single-Node In-memory Multicore Implementation: We implemented parallel sparse coding using multithread-i ng to utilize parallelism on a single multicore machine. The data matrix X is held in memory in its entirety. It is di-vided into as many blocks as the number of threads, and for each block, a single thread runs NOMP or NLASSO sequen-tially on the datapoints in that block. Also held in mem-ory are the K  X  K dense symmetric matrices, S = HH T and G = W T W which we assume are small. Assuming  X  = O ( K ), H also similarly requires O ( K 2 ) storage and is held in memory. To maximize the data sizes that our implementation can accommodate, we considered two dif-ferent execution plans: (a) Plan 1 : The first plan does not materialize the matrix W , but explicitly maintains the dense D  X  K matrix X T W . As each invocation of NOMP or NLASSO completes, the associated sparse coding vector w i is used to update the summary statistics matrices, W T W and X T W , and then discarded, since everything needed for dictionary learning is contained in these summary statistics. When DK N  X  , this leaves more room to accommodate larger datasizes in memory. However, not materializing W means that NLASSO cannot exploit warm-starts from the sparse coding solutions found with respect to the previous dictionary. (b) Plan 2 : In an alternate plan, we materialize W instead which consumes lesser memory if N  X  DK . We then serve the columns of X T W , i.e., the vectors X T in Eqn 14, by performing a sparse matrix-vector product on the fly. However, this requires extracting a column from W which is stored in a row-major dynamic sparse format. To make column extraction efficient, we utilize the fact that the indices of non-zero entries for each row are held in sorted or-der, and the associated value arrays conform to this order. Hence, we can keep a record of where the i th column was found for each row, and simply advance this pointer when the ( i + 1) th column is required. Thus, all columns can be served efficiently with one pass over W . Two benefits of this plan are: (i) the matrix-vector product of X T against the columns of W can be parallelized and (ii) NLASSO can use warm-starts. Thus, in this plan, both sparse cod-ing and dictionary learning phases benefit from parallelism. To concretize the preceding discussion with some numbers: on a multicore machine with 16-GB RAM, accounting for the matrix storage given in Table 1, both plans can han-dle around 10 million documents assuming sparsity of 100 words,  X  = 5 ,  X  = 1000, and 1000 topics need to be learnt. If 15000 topics need to learnt, the first plan can only han-dle about 2 million documents while the second plan can still manage 9 million by not materializing the dense D  X  K matrix X T W .

Together, these considerations outlined above, lead to a highly efficient single-node implementation that forms the basis for the cluster version described next.
 Cluster Implementation: Unlike the multi-core setting, o ur cluster implementation does not make the assumption that the data matrix X fits in main memory. Rather, X is stored in a distributed file system and our implementa-tion makes one or more passes over it in parallel. Likewise, only small blocks of the large dense matrix X T W are ever materialized at a time and written to disk in parallel in ev-ery iteration. We continue making the assumption that the small matrices W T W , HH T and the highly sparse matrix H , can be held in-memory on both the control and worker nodes. While there are many choices for the framework one could use to parallelize algorithms, we work with the Map-Reduce paradigm, which fits well with the embarrassingly parallel nature of the sparse coding phase.

The Map-Reduce (MR) programming model was designed to simplify the processing of large files on a parallel sys-tem through user-defined Map and Reduce primitives. A MR job consists of two phases: a Map phase and a Reduce phase. During the Map phase, the user-defined Map prim-itive transforms the input data into (key, value) pairs in parallel. These pairs are stored and then sorted by the sys-tem so as to accumulate all values for each key. During the Reduce phase, the user-defined Reduce primitive is invoked on each unique key with a list of all the values for that key; usually, this phase is used to perform aggregations. Finally, the results are output in the form of (key, value) pairs. Each key can be processed in parallel during the Reduce phase. H adoop ( http://hadoop.apache.org/ ), an open-source im-plementation of the MR programming model, has emerged as a vastly popular platform for parallelization in industry and academia. A user can perform parallel computations by submitting one or more MR jobs to Hadoop. One of the key advantages of Hadoop is that it is capable of running on large commodity clusters and recovering from both data as well as compute node failures.

Hadoop X  X  implementation of the MR programming model targets executions where the input, intermediate (shuffle), and output datasets do not fit in aggregate main mem-ory. Each MR job has to scan the input and intermediate datasets, which is time consuming. Furthermore, each job adds significantly to execution time in the form of startup costs. For these reasons, implementations that require the fewest number of MR jobs are ideal. Our cluster implemen-tation below is driven by this goal.

Figure 1 presents the overall flow for the MR implemen-tation of our algorithms. The execution proceeds in two phases, the preprocessing phase and the learning phase. The preprocessing phase is a one-time step whose objective is to re-organize the data for better parallel execution of the BCD algorithm. The learning phase is iterative and is coordinated by a control node that first spawns NOMP or NLASSO MR jobs on the worker nodes, then runs sequential dictionary learning and finally monitors the global mean reconstruc-tion error to decide on convergence.

The preprocessing step consists of randomization and block-ing. The goal of this step is to transform the data set into a set of  X  X locks X  such that each block is a row-wise parti-tion of the input matrix and all the blocks are roughly of the same size and sparsity, implying roughly equal units of work. We block the data set for the following reasons. First, during the learning phase, if one were to provide the input matrix to the Mappers one row at a time, majority of the time spent would go into performing I/O and not actual computation. To amortize the cost of performing I/O, we want to process a block of rows at a time. Second, ensur-ing these blocks are roughly of the same size and sparsity allows for a load balanced execution. Finally, blocking the input matrix amortizes the cost of writing out blocks of the X
T W matrix during the learning phase by allowing one to partially aggregate this matrix for each input block, directly in memory. During the pre-processing step, the Mapper as-signs each row to a random block and emits the block id as a key and row as a value. The Reducer then assimilates each block and writes it out to the file system. At the end of the pre-processing phase, we expect to have row-wise blocks of the input matrix that are roughly of same size and sparsity.
The subsequent learning phase proceeds iteratively with each iteration consisting of a parallel sparse coding phase and a sequential dictionary learning phase. The parallel sparse coding phase is implemented using a single Map-Reduce job . To explain its steps, we need the following nota-tion: let X r denote the r th block of the X and R be the num-ber of rows that were assigned to a block by the preprocess-ing phase. We refer to R as the row blocking parameter . A Mapper for the parallel sparse coding phase accepts X r and runs NOMP/NLASSO to obtain the associated sparse cod-ing matrix W r which can be materialized in memory since the blocks are small. We use the notation W r,i to denote its i th column. For large D, K , the local summary statistics matrix X rT W r cannot be materialized and written out in one go. Rather, the Mapper computes a set of D  X  C matri-matrix for j = 1 . . . d K C e . C i s the column blocking parame-ter of our implementation. The Mapper emits the block id j as a key and column block Q r,j as value. The Reducer then sums up these blocks over the row-blocks: Q j = P r Q r,j which gives the j th block of C columns of the global sum-mary statistics matrix X T W . This mechanism to write out column blocks of the local summary statistics matrices (and not all columns) has the following benefits: First, we can constrain the size of the aggregations that need to take place on the Reducers. Second, we get parallelism by having each Reducer perform a complete aggregation for a portion of the columns. In addition to producing the summary statistics matrix, the MR job also writes out the aggregated recon-struction error and W T W matrix. The output of the MR job is delivered to the sequential dictionary learning phase, at the end of which we check for convergence and proceed to the next iteration if necessary. Note that each mapper can writeout its associated W r as well to help jumpstart NLASSO in the next iteration.
We begin with small scale experiments on two classic doc-ument collections, the 20 newsgroups collection ( N = 19228, D = 18607 and K = 20) and TDT news corpus ( N = 9394, D = 19528 and K = 30), that come with manually gen-erated labels with respect to which quality measurements may be made. We then report scalability experiments for our multicore and Hadoop implementations on much larger datasets (New York times and an extended Pubmed Cor-pora) where extrinsic evaluation cannot be done due to the high cost of labeling.
 Statistical Performance on Small Datasets: We explore t he effect of double sparsity on the statistical performance of the non-negative sparse coding. We varied dictionary spar-sity, covering 5% to 100% of the full data dimensionality. The coding sparsity was varied from 0 . 05 to 1 . 0 interpreted as the fraction of the full dictionary size K t hat NOMP is allowed to use for coding. For NLASSO, the same fractions were used literally as the  X  values. Empirically, in this range, NLASSO gave similar degrees of sparsity as NOMP. For each choice of  X ,  X  , we ran the learning algorithms 10 times with different initialization and averaged the following: (a) the clustering quality in terms of normalized mutual informa-tion (NMI) obtained by assigning cluster arg max j w ij to document i , (b) the classification accuracy returned by run-ning linear logistic regression on 5% labeled samples after unsupervised sparse coding, to emulate a semi-supervised learning scenario, and (c) the final objective values attained at convergence. Each choice of  X  was associated with the mean sparsity of the final W across different random ini-tializations.

Figure 2: Results on 20NG (top), TDT2 (bottom)
In Figure 2, we plot NMI, Classification accuracy of Logis-t ic Regression and Objective values as a function of percent-age of non-zeros in W for different degrees of percentage of non-zeros in H , for both NOMP and NLASSO. We make the following observations: (1) Our Sparse NMF (both NOMP and NLASSO) demonstrates substantially better clustering and classification performance than NMF which corresponds to the rightmost point on the curves for  X  = 100%. Further-more, we never observed an increase in objective function with NOMP in these experiments; all runs smoothly con-verged. (2) Interestingly, we find that on both datasets, NOMP gives substantially better reconstruction error at sim-ilar sparsity levels than NLASSO. However, the induced sparse representation does not translate into better clus-tering performance or classification accuracy. This might seem counter-intuitive at first, but can be reasoned as fol-lows. The NOMP hypothesis space is larger since it does not restrict the magnitude of the weights as NLASSO does. This leads to better mean reconstruction error rates at sim-ilar sparsity levels. Indeed, reconstruction error alone does not reflect model utility for these tasks since we know that SVD can do even better without non-negativity and spar-sity constraints. The presence of additional weight shrink-age in NLASSO appears to act as an effective regularizer that leads to better statistical performance. (3) We see that the  X  = 10% curve performs as well or better than the  X  = 100% curve. This implies a 10-fold storage reduc-tion for H strongly supporting our choice to maintain it in memory as a light-weight sparse matrix. (4) In Table 2, as a sanity check, we compare clustering performance against popular alternatives. The parameters are  X  = 0 . 05 (to be interpreted as fraction of K for NOMP) and  X  = 0 . 1 D . We
Figure 3: Multicore Performance: NYTimes data see that Sparse NMF with NLASSO performs quite compet-i tively.
 Multicore Performance: We next consider computational p erformance on a commodity hexacore machine with 4GB RAM. For these experiments, we work with the popular NY times corpus (available from UCI Machine Learning Repos-itory) comprising of N = 299752 documents indexed over a vocabulary of D = 102660 words, with about 70-million non-zero entries in the document-term matrix. We set  X  = 1000,  X  = 5 for NOMP and 5 for NLASSO and ran our algo-rithms for K = 100 while increasing the number of cores from 1 to 6. The total time to convergence for both the execution plans is shown in Figure 3 (left). We make the following observations. The use of 6-cores offers the follow-ing speedups over a single core: 5.2x for NOMP (plan 1), 4.3x for NOMP (plan 2), 4.7x for both NLASSO (plan 1) and NLASSO (plan 2). Thus, we see near-linear speedups with increasing number of cores. NOMP prefers plan 1 because it cannot utilize warm starts and because dictionary learning is extremely fast (about 0 . 5 seconds for updating 100 topics per iteration) since it only needs to perform a top- X  oper-ation followed by simple normalization step. NLASSO, on the other hand, prefers plan 2 since warm starts allow it to converge faster. As shown in Figure 3 (right), its iterations start to take lesser and lesser time. The dictionary learning phase for plan 2 takes about 35 seconds in each iteration for 1 core, which reduces to about 13 seconds with 6 cores, due to parallel matrix-vector products as described in section 5. Hadoop Scaleout: These experiments were performed on I BM X  X  Nadal cluster, which has 48 cores distributed across 12 nodes with 4 GB of RAM per core. All the nodes were running RHEL 5 (kernel 2.6.18-164.el5) with IBM Java v1.6.0 and Hadoop 0.20.2. We measured execution time of the Hadoop implementation on an extended PubMed dataset (also available at UCI repository) containing 106 million documents, 141K words, with more than 6 Billion non-zeros in the document term matrix, as we varied various algorithm and hardware parameters. For these experiments, the row blocking parameter R = 100K rows, the column blocking parameter C = 10 columns,  X  = 5 and  X  = 1000, unless otherwise noted. (a) Varying number of cores: Figures 4 and 5 present execution time for an iteration of NOMP, an iteration of NLASSO, and the blocking phase that is common to both algorithms, for different dataset sizes, as we vary the number of cores from 16 to 48. Speedup for the blocking phase (that is common to both algorithms) varies from 1.53x for 8.2M r ows to 2.81x for 106M rows, the maximum possible speedup here being 3x (as we only vary the number of cores from 16 to 48). For 8.2M rows, the speedup is roughly half that of the maximum possible speedup as the data set is small and the system is under utilized. For 106M rows, all cores take part in I/O and the blocking procedure, improving scalability. The blocking phase is thus very efficient and is expected to continue to scale well to larger datasets.

As for the performance of an iteration of NOMP, the speedup is roughly 1.85x for 8.2M rows and 2.7x for 106M rows when K = 100 and 2.1x for 8.2M rows and 3.54x for 106M rows when K = 200. As for the performance of an iteration of NLASSO, the speedup is roughly 1.81x for 8.2M rows and 2.75x for 106M rows when K = 100 and 1.86x for 8.2M rows and 3.02x for 106M rows when K = 200. Keeping K fixed, speedup improves with increasing number of rows as the execution changes from being I/O bound and under utilizing the system to being compute bound. Furthermore, keeping the number of rows fixed, speedup improves as we increase K as there is more computation on offer to bet-ter utilize the available parallelism. We observe super linear speedup in some cases likely due to improvements in caching behavior as we increase the number of cores. Given that the execution is compute-bound on large datasets, we expect the implementation to continue to scale on larger number of cores provided the dataset is sufficiently large to make the execution compute-bound.

Figure 4: Varying Number of Cores -100 Topics
Figure 5: Varying Number of Cores -200 Topics (b) Varying row block size: F igure 6 presents execution times for an iteration of NOMP, an iteration of NLASSO, and the blocking phase that is common to both algorithms, for different values of the row blocking parameter R , as we vary the number of rows in the dataset, keeping number of cores fixed at 48. The performance of the blocking phase im-proves marginally (from 496 to 373 for 8.2M rows and from 3392 to 3382 for 106M rows) as we increase the size of each block due to the fact that the system needs to sort a fewer keys with fewer blocks. As the performance of the blocking phase is not adversely affected by increasing data sizes, we expect the blocking phase to scale well to larger datasets. However, the performance of an iteration of NOMP and NLASSO varies significantly with varying R . For NOMP, seconds per iteration varies from 200 to 122 for 8.2M rows and from 2099 to 1032 for 106M rows. For NLASSO, seconds per iterations varies from 266 to 180 for 8.2M rows and from 2650 to 1624 for 106M rows. R = 200K can improve perfor-mance by nearly 2x when compared to R = 50K. Given that performance of the blocking phase is relatively unaffected by the parameter R , and that the performance of the learning phase is heavily dependent on the value of R , picking the largest possible value for R that allows for an in-memory execution will likely provide the best performance. (c) Varying number of topics: F igure 7 presents ex-ecution times for an iteration of NOMP, an iteration of NLASSO, for different number of topics K , as we vary the data size, keeping number of cores fixed at 48. For NOMP, at 8.2M rows, execution time increases by 2.76x, and at 106M rows, execution time increases by 4.69x, when go-ing from K = 100 to 400. The execution under utilizes the system at 8.2M rows and hence execution time grows sub-linearly with K . However, at 106M rows, execution time grows near-linearly as the execution is compute bound. Overall, execution time for NOMP grows near-linearly with K . For NLASSO, at 8.2M rows, execution time increases by 5.81x, and at 106M rows, execution time increases by 6.22x, when going from K = 100 to 400. Thus, execution time for NLASSO grows super-linearly with K . We can explain these observations as follows. The per iteration cost of NOMP is linear in K , involving search over K atoms, and the total number of iterations is atmost  X  . For NLASSO, the gradi-ent computation involves the dense matrix-vector product Sw whose complexity depends on the sparsity of w , but for dense iterates is O ( K 2 ). For large K , linear scaling behavior can be recovered for NLASSO by not materializing S , but rather computing Sw using two sparse matrix-vector prod-ucts H ( H T w ) which has O ( K X  ) cost. Convergence results from [2] show that NLASSO, implemented using FISTA, will take about O ( q 1 ) iterations to return an optimal solution, independent of K .
 (d) Varying column block size: F igure 8 presents ex-ecution times for an iteration of NOMP, an iteration of NLASSO, for different number of topics K (100,400) and column blocking parameter C (5,10,20), as we vary the data size, keeping number of cores fixed at 48. For both NOMP and NLASSO, we observe an interesting trend. When K = 100, C = 5 results in the least execution time as it allows one to leverage a larger number of cores in the Reduce phase of the MR job. However, K = 400, C = 5 results in the largest execution time as it results in an excessively larger number of Reduce tasks, while C = 20 results in the least execu-tion time. These indicate that one must carefully select the p arameter C based on the cluster configuration and desired number of topics.
 (e) Varying coding sparsity: F igure 9 presents execution times for an iteration of NOMP, an iteration of NLASSO, for K = 400, as we vary the data size, keeping number of cores fixed at 48. For NOMP, varying  X  from 5 to 20 in-creases execution time by roughly 1.82x for 106M rows. For NLASSO, varying  X  from 0.1 to 1 increases execution time by roughly 1.67x for 106M rows. Thus, for both algorithms, execution time grows sub-linearly with coding sparsity. The per iteration cost of NLASSO is independent of  X  and as outlined in (c), it has linear dependence on  X  . While the re-fitting step of NOMP has superlinear dependence on  X  , its values are such a small fraction of K that this dependence is  X  X ashed out X .
 Comparison to related work: On the extended Pubmed corpus with over 106-million rows and more than 6 bil-lion non-zero entries, with K = 100 and parameters re-ported in this section, our fastest algorithms running on 12 nodes (with 48 cores) are expected to converge and com-plete execution in less than 10 hours. To the best of our knowledge, this is the largest demonstration of sparse cod-ing/dictionary learning algorithms reported to date. Com-paring this performance with other related published work has several caveats due to major differences in convergence properties of algorithms benchmarked, actual cluster config-uration, characteristics of datasets used, as well as innumer-able other lower-level implementation details. In particular, we are not aware of any publically available Hadoop imple-mentation of Sparse NMF and related dictionary learning algorithms to compare against. Nonetheless, to put our per-formance into perspective, we collect some published results in this section. [15] report running the standard NMF [14] algorithms on a proprietary dataset with around 4 . 38-billion non-zeros with K = 10, on a Hadoop cluster with unspeci-fied configuration. They report 7-hours per iteration each re-quiring multiple Map-Reduce jobs, which appears to be sig-nificantly less efficient than our implementation. [18] paral-lelize LDA and report about 4 . 1 hours on 8 . 2 million Pubmed documents, with K = 2000, on a non-dedicated Hadoop cluster with 50 nodes. [17] provide a different MPI-based parallel LDA implementation and report 10-hours for the same dataset on 1024 processors.

Acknowledgments: We thank Prabhanjan Kambadur for several insightful conversations. This research is contin-uing through participation in the Social Media in Strategic Communication (SMISC) program sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) un-der Agreement Number W911NF-12-C-0028. [1] S. Arora, R. Ge, R. Kannan, and A. Moitra.
 [2] A. Beck and M. Teboulle. Gradient-based algorithms [3] D. Bertsekas. Non-linear Programming . Athena [4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. [5] Peter Buhlmann and Sara Van De Geer. Statistics for [6] A. Cichocki, R. Zdunek, A. H. Phan, and S. Amari. [7] J. Dean and S. Ghemawat. Mapreduce: Simplified [8] J. Duchi, S. S. Schwartz, Y. Singer, and T. Chandra. [9] M. Elad. Sparse and Redundant Representations . [10] A. Ghoting, P. Kambadur, E. Pednault, and [11] A. Ghoting, R. Krishnamurthy, E. Pednault, [12] P. Hoyer. Non-negative matrix factorization with [13] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. [14] D. D. Lee and H. S. Seung. Learning the parts of [15] C. Liu, H. Yang, J. Fan, L. He, and Y. Wang. [16] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online [17] D. Newman, A. Ascuncion, P. Smyth, and M. Welling. [18] A. Smola and S. Narayanamurthy. An architecture for [19] Z. J. Xiang, H. Xu, and P. J. Ramadge. Learning
