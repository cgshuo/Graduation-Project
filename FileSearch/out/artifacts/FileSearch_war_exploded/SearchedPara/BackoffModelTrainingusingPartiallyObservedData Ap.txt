 Discourse patterns in natural con versations and meetings are well kno wn to pro vide interesting and useful information about human con versational be-havior . The y thus attract research from man y dif fer -ent and benecial perspecti ves. Dialog acts (D As) (Searle, 1969), which reect the functions that ut-terances serv e in a discourse, are one type of such patterns. Detecting and understanding dialog act patterns can pro vide benet to systems such as au-tomatic speech recognition (ASR) (Stolck e et al., 1998), machine dialog translation (Lee et al., 1998), and general natural language processing (NLP) (Ju-rafsk y et al., 1997b; He and Young, 2003). DA pat-tern recognition is an instance of  X tagging.  X  Man y dif ferent techniques have been quite successful in this endea vor, including hidden Mark ov models (Ju-rafsk y et al., 1997a; Stolck e et al., 1998), seman-tic classication trees and polygrams (Mast et al., 1996), maximum entrop y models (Ang et al., 2005), and other language models (Reithinger et al., 1996; Reithinger and Klesen, 1997). Lik e other tagging tasks, DA recognition can also be achie ved using conditional random elds (Laf ferty et al., 2001; Sut-ton et al., 2004) and general discriminati ve model-ing on structured outputs (Bartlett et al., 2004). In man y sequential data analysis tasks (speech, lan-guage, or DN A sequence analysis), standard dy-namic Bayesian netw orks (DBNs) (Murph y, 2002) have sho wn great exibility and are widely used. In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized back-off procedures (Bilmes and Kirchhof f, 2003).
Most DA classication procedures assume that within a sentence of a particular x ed DA type, there is a x ed word distrib ution over the entire sen-tence. Similar to (Ma et al., 2000) (and see cita-tions therein), we have found, howe ver, that intra-sentence discourse patterns are inherently dynamic. Moreo ver, the patterns are specic to each type of DA, meaning a sentence will go through a DA-specic sequence of sub-D A phases or  X states.  X  A generati ve description of this phenomena is that a DA is rst chosen, and then words are generated according to both the DA and to the relati ve posi-tion of the word in that sentence. For example, a  X statement X  (one type of DA) can consist of a sub-ject (noun phrase), verb phrase, and object (noun phrase). This particular sequence might be dif ferent for a dif ferent DA (e.g., a  X back-channel X ). Our be-lief is that explicitly modeling these internal states can help a DA-classication system in con versa-tional meetings or dialogs.

In this work, we describe an approach that is moti vated by several aspects of the typical DA-classication procedure. First, it is rare to have sub-DAs labeled in training data, and indeed this is true of the corpus (Shriber g et al., 2004) that we use. Therefore, some form of unsupervised clustering or pre-shallo w-parsing of sub-D As must be performed. In such a model, these sub-D As are essentially un-kno wn hidden variables that ideally could be trained with an expectation-maximization (EM) procedure. Second, when training models of language, it is nec-essary to emplo y some form of smoothing method-ology since otherwise data-sparseness would render standard maximum-lik elihood trained models use-less. Third, discrete conditional probability distri-butions formed using back off models that have been smoothed (particularly using modied Kneser -Ne y (Chen and Goodman, 1998)) have been extremely successful in man y language modeling tasks. Train-ing back off models, howe ver, requires that all data is observ ed so that data counts can be formed. In-deed, our DA-specic word models (implemented via back off) will also need to condition on the cur -rent sub-D A, which at training time is unkno wn. We therefore have developed a procedure that al-lows us to train generalized back off models (Bilmes and Kirchhof f, 2003), even when some or all of the variables involv ed in the model are hidden . We thus call our models hidden bac koff model s (HBMs). Our method is indeed a form of embedded EM training (Mor gan and Bourlard, 1990), and more generally is a specic form of EM (Neal and Hinton, 1998). Our approach is similar to (Ma et al., 2000), except our underlying language models are back off-based and thus retain the benets of adv anced smoothing methods, and we utilize both a normal and a back off EM step as will be seen. We moreo ver wrap up the abo ve ideas in the frame work of dynamic Bayesian netw orks, which are used to represent and train all of our models.

We evaluate our methods on the ICSI meeting recorder dialog act (MRD A) (Shriber g et al., 2004) corpus, and nd that our novel hidden back off model can signicantly impro ve dialog tagging accurac y. With a dif ferent number of hidden states for each DA, a relati ve reduction in tagging error rate as much as 6.1% can be achie ved. Our best HBM result sho ws an accurac y that impro ves on the best kno wn (to our kno wledge) result on this corpora which is one that uses acoustic prosody as a feature. We have moreo ver developed our own prosody model and while we have not been able to usefully emplo y both prosody and the HBM technique together , our HBM is competiti ve in this case as well. Furthermore, our results sho w the effecti veness of our embedded EM procedure, as we demonstrate that it increases train-ing log lik elihoods, while simultaneously reducing error rate.

Section 2 briey summarizes our baseline DBN-based models for DA tagging tasks. In Section 3, we introduce our HBMs. Section 4 contains experi-mental evaluations on the MRD A corpus and nally Section 5 concludes. Dynamic Bayesian netw orks (DBNs) (Murph y, 2002) are widely used in sequential data analysis such as automatic speech recognition (ASR) and DN A sequencing analysis (Durbin et al., 1999). A hidden Mark ov model (HMM) for DA tagging as in (Stolck e et al., 1998) is one such instance.
Figure 1 sho ws a generati ve DBN model that will be tak en as our baseline. This DBN sho ws a pro-logue (the rst time slice of the model), an epilogue (the last slice), and a chunk that is repeated suf -ciently to t the entire data stream. In this case, the data stream consists of the words of a meet-ing con versation, where indi viduals within the meet-ing (hopefully) tak e turns speaking. In our model, the entire meeting con versation, and all turns of all Figure 1: Baseline generati ve DBN for DA tagging. speak ers, are strung together into a single stream rather than treating each turn in the meeting indi-vidually . This approach has the benet that we are able to inte grate a temporal DA-to-D A model (such as a DA bigram).

In all our models, to simplify we assume that the sentence change information is kno wn (as is com-mon with this corpus (Shriber g et al., 2004)). We next describe Figure 1 in detail. Normally , the sen-tence chang e variable is not set, so that we are within a sentence (or a particular DA). When a sentence change does not occur , the DA stays the same from slice to slice. During this time, we use a DA-specic language model (implemented via a back off strat-egy) to score the words within the current DA. When a sentence change event does occur , a new DA is predicted based on the DA from the pre vious sentence (using a DA bigram). At the beginning of a sentence, rather than conditioning on the last word of the pre vious sentence, we condition on the special start of sentence &lt;s&gt; tok en, as sho wn in the gure by having a special parent that is used only when sentence chang e is true. Lastly , at the very beginning of a meeting, a special start of DA tok en is used.
The joint probability under this baseline model is written as follo ws: where W = f w is the DA sequence, d tence, and w in the meeting.

Because all variables are observ ed when training our baseline, we use the SRILM toolkit (Stolck e, 2002), modied Kneser -Ne y smoothing (Chen and Goodman, 1998), and factored extensions (Bilmes and Kirchhof f, 2003). In evaluations, the Viterbi al-gorithm (Viterbi, 1967) can be used to nd the best DA sequence path from the words of the meeting according to the joint distrib ution in Equation (1). When analyzing discourse patterns, it can be seen that sentences with dif ferent DAs usually have dif-ferent internal structures. Accordingly , in this work we do not assume sentences for each dialog act have the same hidden state patterns. For instance (and as mentioned abo ve), a statement can consist of a noun follo wed by a verb phase.

A problem, howe ver, is that sub-D As are not an-notated in our training corpus. While clustering and annotation of these phrases is already a widely de-veloped research topic (Pieraccini and Le vin, 1991; Lee et al., 1997; Gildea and Jurafsk y, 2002), in our approach we use an EM algorithm to learn these hid-den sub-D As in a data-dri ven fashion. Pictorially , we add a layer of hidden states to our baseline DBN as illustrated in Figure 2. Figure 2: Hidden back off model for DA tagging. Under this model, the joint probability is: where S = f s is the hidden state at the i -th position of the k -th sen-tence, and other variables are the same as before. Similar to our baseline model, the DA bigram P ( d k j d k gram. Moreo ver, if the states f s during training, the word prediction probability trained accordingly . The hidden state sequence is unkno wn, howe ver, and thus cannot be used to pro-duce a standard back off model. What we desire is an ability to utilize a back off model (to mitig ate data sparseness effects) while simultaneously retaining the state as a hidden (rather than an observ ed) vari-able, and also have a procedure that trains the entire model to impro ve overall model lik elihood.
Expectation-maximization (EM) algorithms are well-kno wn to be able to train models with hidden states. Furthermore, standard adv anced smoothing methods such as modied Kneser -Ne y smoothing (Chen and Goodman, 1998) utilize inte ger counts (rather than fractional ones), and the y moreo ver need  X meta X  counts (or counts of counts). There-fore, in order to train this model, we propose an embedded training algorithm that cycles between a standard EM training procedure (to train the hidden state distrib ution), and a stage where the most lik ely hidden states (and their counts and meta counts) are used externally to train a back off model. This pro-cedure can be described in detail as follo ws: Input : W  X  meeting word sequence Input : D  X  DA sequence
Output : P ( s
Output : P ( w randomly generate a sequence S ; back off train P ( w while not  X con ver ged X  do end
In the algorithm, the input contains words and a DA for each sentence in the meeting. The out-put is the corresponding conditional probability ta-ble (CPT) for hidden state transitions, and a back-off model for word prediction. Because we train the back off model when some of the variables are hid-den, we call the result a hidden bac koff model . While we have seen embedded Viterbi training used in the past for simultaneously training heterogeneous mod-els (e.g., Mark ov chains and Neural Netw orks (Mor -gan and Bourlard, 1990)), this is the rst instance of training back off-models that involv e hidden vari-ables that we are aware of.

While embedded Viterbi estimation is not guar -anteed to have the same con vergence (or x ed-point under con vergence) as normal EM (Lember and Kolo ydenk o, 2004), we nd empirically this to be the case (see examples belo w). Moreo ver, our algo-rithm can easily be modied so that instead of tak-ing a Viterbi alignment in step 5, we instead use a set of random samples generated under the current model. In this case, it can be sho wn using a law-of-lar ge numbers argument that having suf cient sam-ples guarantees the algorithm will con verge (we will investig ate this modication in future work).
Of course, when decoding with such a model, a con ventional Viterbi algorithm can still be used to calculate the best DA sequence. We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRD A) corpus (Shriber g et al., 2004). MRD A is a rich data set that contains 75 natural meetings on dif ferent topics with each meeting involving about 6 participants. DA an-notations from ICSI were based on a pre vious ap-proach in (Jurafsk y et al., 1997b) with some adapta-tion for meetings in a number of ways described in (Bhag at et al., 2003). Each DA contains a main tag, several optional special tags and an optional  X disrup-tion X  form. The total number of distinct DAs in the corpus is as lar ge as 1260. In order to mak e the prob-lem comparable to other work (Ang et al., 2005), a DA tag sub-set is used in our experiments that con-tains back channels (b), place holders (h), questions (q), statements (s), and disruptions (x). In our eval-uations, among the entire 75 con versations, 51 are used as the training set, 11 are used as the develop-ment set, 11 are used as test set, and the remaining 3 are not used. For each experiment, we used a ge-netic algorithm to search for the best factored lan-guage model structure on the development set and we report the best results.

Our baseline system is the generati ve model sho wn in Figure 1 and uses a back off implementa-tion of the word model, and is optimized on the de-velopment set. We use the SRILM toolkit with ex-tensions (Bilmes and Kirchhof f, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding. Our baseline system has an error rate of 19.7% on the test set, which is comparable to other approaches on the same task (Ang et al., 2005). 4.1 Same number of states for all DAs To compare against our baseline, we use HBMs in the model sho wn in Figure 2. To train, we follo wed Algorithm 1 as described before and as is here de-tailed in Figure 3.
 Figure 3: Embedded training: llh = log lik elihood
In this implementation, an upper triangular ma-trix (with self-transitions along the diagonal) is used for the hidden state transition probability table so that sub-D A states only propag ate in one direction. When initializing the hidden state sequence of a DA, we expanded the states uniformly along the sen-tence. This initial alignment is then used for HBM training. In the word models used in our experi-ments, the back off path rst drops pre vious words, then does a parallel back off to hidden state and DA using a mean combination strate gy.

The HBM thus obtained was then fed into the main loop of our embedded EM algorithm. The training was considered to have  X con verged X  if ei-ther it exceeded 10 iterations (which never hap-pened) or the relati ve log lik elihood change was less than 0.2%. Within each embedded iteration, three EM epochs were used. After each EM iteration, a Viterbi alignment was performed thus obtaining what we expect to be a better hidden state alignment. This updated alignment, was then used to train a new HBM. The newly generated model was then fed back into the embedded training loop until it con-verged. After the procedure met our con vergence criteria, an additional ve EM epochs were carried out in order to pro vide a good hidden state transi-tion probability table. Finally , after Viterbi align-ment and text generation was performed, the word HBM was trained from the best state sequence.
To evaluate our hidden back off model, the Viterbi algorithm was used to nd the best DA sequence ac-cording to test data, and the tagging error rates were calculated. In our rst experiment, an equal num-ber of hidden states for all DAs were used in each model. The effect of this number on the accurac y of DA tagging is sho wn in Table 1.
 Table 1: HBMs, dif ferent numbers of hidden states.
For the baseline system, the back off path rst drops dialog act , and for the HBMs, all back off paths drop hidden state rst and drop DA sec-ond. From Table 1 we see that with two hidden states for every DA the system can reduce the tagging error rate by more than 5% relati ve. As a comparison, in (Ang et al., 2005), where conditional maximum en-trop y models (which are conditionally trained) are used, the error rate is 18.8% when using both word and acoustic prosody features, and and 20.5% with-out prosody . When the number of hidden states in-creases to 3, the impro vement decreases even though it is still (very slightly) better than the baseline. We belie ve the reasons are as follo ws: First, assuming dif ferent DAs have the same number of hidden states may not be appropriate. For example, back chan-nels usually have shorter sentences and are constant in discourse pattern over a DA. On the other hand, questions and statements typically have longer , and more comple x, discourse structures. Second, even under the same DA, the structure and inherent length of sentence can vary . For example,  X yes X  can also be a statement even though it has only one word. There-fore, one-w ord statements need completely dif fer -ent hidden state patterns than those in subject-v erb-object lik e statements  X  having one monolithic 3-state model for statements might be inappropriate. This issue is discussed further in Section 4.4. 4.2 Differ ent states for differ ent DAs In order to mitig ate the rst problem described abo ve, we allo w dif ferent numbers of hidden states for each DA. This, howe ver, leads to a combinato-rial explosion of possibilities if done in a na  X  ve fash-ion. Therefore, we attempted only a small number of combinations based on the statistics of numbers of words in each DA given in Table 2.

Table 2 sho ws the mean and median number of words per sentence for each DA as well as the stan-dard deviation. Also, the last column pro vides the p value according to tting the length histogram to a geometric distrib ution (1 p ) n p . As we expected, back channels (b) and place holders (h) tend to have shorter sentences while questions (q) and statements (s) have longer ones. From this analysis, we use fewer states for (b) and (h) and more states for (q) and (s). For disruptions (x), the standard deviation of number of words histogram is relati vely high com-pared with (b) and (h), so we also used more hidden states in this case. In our experimental results belo w, we used one state for (b) and (h), and various num-bers of hidden states for other DAs. Tagging error rates are sho wn in Table 3.

From Table 3, we see that using dif ferent num-bers of hidden states for dif ferent DAs can produce better models. Among all the experiments we per -Table 3: Number of hidden states for dif ferent DAs. formed, the best case is given by three states for (q), two states for (s) and (x), and one state for (b) and (h). This combination gives 6.1% relati ve reduction of error rate from the baseline. 4.3 Effect of embedded EM training Incorporating back off smoothing procedures into Bayesian netw orks (and hidden variable training in particular) can sho w benets for any data domain where smoothing is necessary . To understand the properties of our algorithm a bit better , after each training iteration using a partially trained model, we calculated both the log lik elihood of the training set and the tagging error rate of the test data. Figure 4 sho ws these results using the best conguration from the pre vious section (three states for (q), two for (s)/(x) and one for (b)/(h)). This example is typical of the con vergence we see of Algorithm 1, which empirically suggests that our procedure may be sim-ilar to a generalized EM (Neal and Hinton, 1998). ) Figure 4: Embedded EM training performance.
We nd that the log lik elihood after each EM training is strictly increasing, suggesting that our embedded EM algorithm for hidden back off models is impro ving the overall joint lik elihood of the train-ing data according to the model. This strict increase of lik elihood combined with the fact that Viterbi training does not have the same theoretical con ver-gence guarantees as does normal EM indicates that more detailed theoretical analysis of this algorithm used with these particular models is desirable.
From the gure we also see that both the log lik elihood and tagging error rate  X con verge X  af-ter around four iterations of embedded training. This quick con vergence indicates that our embedded training procedure is effecti ve. The leveling of the error rates after several iterations sho ws that model over-tting appears not to be an issue presumably due to the smoothed embedded back off models. 4.4 Discussion and Err or Analysis A lar ge portion of our tagging errors are due to con-fusing the DA of short sentences such as  X yeah X , and  X right X . The sentence,  X yeah X  can either be a back channel or an afrmati ve statement. There are also cases where  X yeah? X  is a question. These types of confusions are dif cult to remo ve in the prosody-less frame work but there are several possibilities. First, we can allo w the use of a  X fork and join X  tran-sition matrix, where we fork to each DA-specic condition (e.g., short or long) and join thereafter . Alternati vely , hidden Mark ov chain structuring al-gorithms or conte xt (i.e., conditioning the number of sub-D As on the pre vious DA) might be helpful. Finding a proper number of hidden states for each DA is also challenging. In our preliminary work, we simply explored dif ferent combinations using sim-ple statistics of the data. A systematic procedure would be more benecial. In this work, we also did not perform any hidden state tying within dif-ferent DAs. In practice, some states in statements should be able to be benecially tied with other states within questions. Our results sho w that having three states for all DAs is not as good as two states for all. But with tying, more states might be more successfully used. 4.5 Inuence of Pr osody Cues It has been sho wn that prosody cues pro vide use-ful information in DA tagging tasks (Shriber g et al., 1998; Ang et al., 2005). We also incorporated prosody features in our models. We used ESPS get f0 based on RAPT algorithm (Talkin, 1995) to get F normalization is performed. For each word, a linear regression is carried on the normalized F We quantize the slope values into 20 bins and treat those as prosody features associated with each word. After adding the prosody features, the simple gener -ative model as sho wn in Figure 5 gives 18.4% error rate, which is 6.6% impro vement over our baseline. There is no statistical dif ference between the best performance of this prosody model and the earlier best HBM. This implies that the HBM can obtain as good performance as a prosody-based model but without using prosody . Figure 5: Generati ve prosody model for DA tagging.
The next obvious step is to combine an HBM with the prosody information. Strangely , even after ex-perimenting with man y dif ferent models (including ones where prosody depends on DA; prosody de-pends on DA and the hidden state; prosody depends on DA, hidden state, and word; and man y varia-tions thereof), we were unsuccessful in obtaining a complementary benet when using both prosody and an HBM. One hypothesis is that our prosody features are at the word-le vel (rather than at the DA level). Another problem might be the small size of the MRD A corpus relati ve to the model comple xity . Yet a third hypothesis is that the errors corrected by both methods are the same  X  indeed, we have ver-ied that the corrected errors overlap by more than 50%. We plan further investig ations in future work. In this work, we introduced a training method for hidden bac koff models (HBMs) to solv e a problem in DA tagging where smoothed back off models in-volving training-time hidden variables are useful. We tested this procedure in the conte xt of dynamic Bayesian netw orks. Dif ferent hidden states were used to model dif ferent positions in a DA. According to empirical evaluations, our embedded EM algo-rithm effecti vely increases log lik elihood on training data and reduces DA tagging error rate on test data. If dif ferent numbers of hidden states are used for dif-ferent DAs, we nd that our prosody-independent HBM reduces the tagging error rate by 6.1% rela-tive to the baseline, a result that impro ves upon pre-viously reported work that uses prosody , and that is comparable to our own new result that also incorpo-rates prosody . We have not yet been able to combine the benets of both an HBM and prosody informa-tion. This material is based upon work supported by the National Science Foundation under Grant No. IIS-0121396.

