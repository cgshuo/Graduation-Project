
With the rapid evolution of internet applications, people all over the world are sharing pictures, videos and audios online, and thus, content-based analysis is often demanded. Test efficiency is crucial to the success of online information processing. One obstacle to high-speed testing is the time cost of feature extraction for test objects, particularly for objects with complex representation such as images, videos and audios. In this paper, we study the problem of reducing test time cost by extracting cheap but sufficient features. We propose the T EFE (Time-Efficient Feature Extraction) ap-proach, which balances between the test accuracy and test time cost by extracting a proper subset of features for each test object. In the implementation, T EFE trains a sequence of support vector machines and classifies each test object cascadingly. Empirical study shows that T EFE is time effi-cient while holding a classification accuracy close to that of using all features. It also shows that the test time is linearly adjustable in T EFE .
As a revolution of Internet usage, Web 2.0 applications encourage every Internet user to create, collaborate, com-municate through the Web, such as Wikipedia, Facebook, YouTube, Picasa, and uncountable number of blog sites and forums. Web 2.0 has achieved great success in exchange and share of information within communities. Considering that the number of Internet users has exceeded 1.2 billion, a challenge here is that too much information is published every day, particularly in the form of images, videos and au-dios, to perform an efficient retrieval. Thus, in order to meet the information requirement of users, effective and efficient information retrieval techniques are urgently needed.
A key to the success of online information processing is a high speed of testing. In other words, results should be present to user as fast as possible. Previous research generally assume that the features of objects have already been extracted, and they try to achieve high-speed testing by selecting a learning algorithm to use, which is fast in making prediction, from a pool of learning algorithms that can be used to model the concerned objects. While the fea-ture extraction cost is neglected. However, when an appli-cation involves complicate objects, such as images, audios and videos, it is often expensive in time cost to extract ef-fective features from their raw appearances, which is one of the main obstacles to high-speed testing.

One direction towards a low test time cost is to de-sign fast feature extraction approaches. Researchers have proposed many approaches to accelerate feature extraction process, such as fast image texture extraction algorithms [8, 12], direct extraction from image encodes [3], and par-allel acceleration approach [4]. It is challenging to invent even faster features with much discriminative power.
Another direction of reducing test time cost is to reduce the features extracted from test objects. In many cases, a few features of an instance are sufficient to judge the la-bel of that instance with high confidence. We call such instances as easy instances, and as the opposite, instances which require a lot of features for a correct classification are called as hard instances. Obviously it is not necessary to extract all features of easy instances, and thus the time of extracting expensive features of easy instances can be saved. On the surface this is a feature selection problem, but traditional feature selection approaches select the same subset of features for all instances. As the consequence, for easy instances there are still redundant features, while for hard instances some discriminative features are missed. A better way is to spend different efforts (time costs) on differ-ent instances, such that the overall test time cost is reduced in maximum subject to a condition that the classification accuracy is still acceptable.

In addition to reducing test time cost, a good approach should also be adjustable in the tradeoff between classifi-cation accuracy and feature extraction time cost. This is because that different users may have different preferences, e.g.., some users may want a higher accuracy while some other users may prefer a higher speed and tolerate a rela-tively lower accuracy. A configurable approach would give user a better flexibility.

In summary, the desired approach should be able to  X  Extract test-efficient features: For each test instance,  X  Make accurate classification: Given a time budget for  X  Adapt to time budget: The approach is able to adapt
In this paper, we propose the T EFE (Time-Efficient Fea-ture Extraction) approach for the above purposes. Unlike traditional approaches which extract all features, T EFE tries to extract only a portion of features for test objects. When a test object is received, T EFE first extracts features with the lowest costs. At this moment, if it is confident enough to predict the label of the object, a classification is made; otherwise, more features are extracted until the confidence reaches a pre-set threshold, or the time budget is over, or no more features are available. This strategy saves time of extracting expensive features for test objects which are easy to classify using simple features. The approach is flexible for user to express his/er preference on the tradeoff between the time cost and classification accuracy through setting the confidence threshold. Empirical study shows that T EFE is able to achieve a classification accuracy close to existing methods using a far less time cost.

The rest of this paper is organized as follows. Section 2 introduces some related work. Section 3 proposes the T EFE approach, followed by empirical evaluation in Section 4. Fi-nally, Section 5 concludes.
Traditional machine learning and data mining research assume that every misclassification causes the same cost. In real-world tasks, however, different misclassifications may cause different costs. So, cost-sensitive learning [6, 22] has attracted much attention. In addition to misclassifica-tion cost, various types of costs may be encountered [19], among which the test cost has been studied by many re-searchers. Roughly speaking, a test cost is the expense for doing a  X  X est X  to get a feature value. For example, in med-ical diagnosis, each feature is the result of a medical test (such as blood-test), and the price of the medical test is the test cost of the corresponding feature. The balance between the quality of the trained model and the cost of acquir-ing missing values in training set has been studied in [15]. The VOILA method [1] tries to select tests by exploiting the dependencies between missing features and information shared among different feature subsets. The PAS algorithm [17] considers both the acquisition cost and misclassifica-tion cost based on a decision tree. The csNB method [2] estimates the expected utility of a feature value in testing, and compares it with the cost for feature test. The SBT (Se-quential Batch Test) algorithm [16] considers delayed test costs. Note that since test costs are mostly financial costs, these studies usually do not care time costs in estimating the utility of different tests, and thus we need new methods for the purpose of test-efficient feature extraction.

Feature selection [7] tries to remove irrelevant and/or redundant features to improve learning performance. Tra-ditional feature selection techniques roughly fall into two categories, i.e., filter methods and wrapper methods. Fil-ter methods use learner-irrelevant measurements to evalu-ate and select features, such as information gain and Re-lief [13]. Wrapper methods involve the final learner in the feature selection process, such as using the accuracy as the evaluation criterion for the goodness of features. Our work is different from traditional feature selection. First, we as-sume that different features are with different time costs, while traditional feature selection does not consider the dif-ferences in time costs. Second, we will use different fea-tures for different instances, while traditional feature selec-tion will use the same subset of features for all instances. The most important difference is that, traditional feature se-lection assumes that there exist irrelevant and/or redundant features, while we do not. We assume that if considering the whole test set, all the features are useful; while if con-sidering a specific test instance, maybe using a portion of features is sufficient. When a data set contains irrelevant and/or redundant features, we can either run T EFE directly or do feature selection at first and then run T EFE .
In [5], the JIT method is proposed and applied to spam detection. This method is a variant of ID3 decision tree. It estimates the utility of a feature using the linear combina-tion of mutual information and time cost. Similar to other decision tree algorithms, JIT stops growing the tree when a node is pure or there is no more feature for a further par-tition. This method is suitable for categorical features. To deal with continuous features in our concerned applications, JIT has to resort to discretization. It is well-known that some useful information may be lost during discretization, which may influence the final accuracy. A special test-cost sensitive learning method, i.e., minimal cost decision tree [14], also suffers from the above weakness of JIT although it uses a different strategy to partition the tree nodes.
Given a test object, ideally there exists a small subset of features which is sufficient for a correct classification. How-ever, before any feature is extracted, we know nothing about the test object. An approach that meets our goal can take a greedy search strategy, such that it extracts one feature for the test object at each step, and then makes a decision on whether to extract one more feature or to make classifica-tion based on the extracted feature(s). Going to the next step to extract one more feature will take more time cost, but making classification will take the risk of misclassifica-tion caused by insufficient features extracted. The time cost of making classification should also be taken into account as a part of test time cost. Ideally, an algorithm should take the best choice in each step to minimize the overall expected test time cost, and to achieve an acceptable accuracy.
Figure 1 illustrates the greedy strategy. An oval indicates extracting a feature and making a choice between extracting one more feature and making the classification, where the subscripts indexes the extracted features. A diamond repre-sents a base classifier, where the subscripts indicate features used by this classifier. Each time when a new feature is ex-tracted, the strategy either makes classification based on all the extracted features if the confidence for classification is high enough, or extracts one more feature.

There are two difficulties in implementing the above ap-proach. First, there are many choices for the next feature to be extracted. Deciding which one to extract is not an easy task. Second, if the tree is fully grown, the number of base classifiers is exponential in the number of features. This will make the process infeasible.

One possible solution to overcome the difficulties is to prune the tree. There is an additional benefit of pruning, i.e., reducing the risk of overfitting. Similar to common decision trees, the deep nodes may contain only a few in-stances and it is not reliable to make further choices based on so limited information. Therefore, T EFE adopts a de-generated tree structure, as illustrated by the right part of Figure 1. Here, the feature extraction order is fixed, and the number of base classifiers equals to the number of features. Note that all the base classifiers can be generated from a training set, and they are ready to use before receiving the test objects.

In the training stage, T EFE ranks all features in an as-cending list according to their feature extraction time costs, and then trains a sequence of base classifiers. The j -th clas-sifier takes the first j features in the rank list. In other words, the j -th classifier is trained on the values of the first tures of the training set.

In the testing stage, T EFE extracts features along the rank list. Each time it obtains a feature value, it feeds all known feature values to the corresponding classifier. If the confi-dence reaches the threshold set by user, T EFE will take the prediction made by the classifier as the classification result; otherwise, T EFE will extract the next feature in the rank list, until no more features can be extracted. The threshold set by user is a parameter, which gives user the flexibility of balancing the classification accuracy and test time cost ac-cording to his/er favor. ranking does not take the discriminability of features into account, since the discriminability of a feature is often hard to be measured, and when the time costs of the features span in a wide range, the factor of time cost will dominate the ranking. If there are many irrelevant and/or redundant fea-tures, feature selection can be executed at first. This will be discussed in Section 4.5.

Denote m as the total number of features, X as the train-ing instances, y as their label vector, and n as the size of the training set. T EFE will train m base classifiers, each using one more feature than its predecessor. Here we use support vector machines with linear kernel for the base classifiers. The m SVMs, presented as { SVM j = ( w j , b j ) } m to be obtained through Eq. 1, where X ( j ) instance with only the first j features.
Note that SVM j +1 is trained on the same training ex-amples as SVM j except that one more feature is involved. Based on this recognition, we find that it is not necessary to train every SVM from scratch, and one SVM can serve as the starting point for its successor. Therefore, we have a fast training process by solving the entire solution path of the SVM sequence.

The training process consists of m phases. In the first phase, SVM 1 is trained on the first feature. It is easy to get the solution analytically and we do not elaborate it here. In each following phase, say, the j -th, we solve the optimiza-tion problem in Eq. 2, where w j is the j -th component of corresponding to the newly added feature, and S is a vari-able constraining w j . Eq. 2 is modified from the original QP problem of SVM with an additional inequality constraint.
At the beginning of the j -th phase, S is set to 0, and the optimal solution to Eq. 2 is w = ( w j  X  1 , 0) and b = b where ( w j  X  1 , b j  X  1 ) is the predecessor SVM. As we relax-ing S , the optimal solution w and b will change and form a path. In the end, when S is relaxed to a value which does not constrain w j , Eq. 2 becomes the optimization problem for SVM j and the optimal solution w and b are exactly so-lution to SVM j . Such a process connects one SVM with its successor. Repeating this process, all solutions of the SVM sequence can be obtained when the path grows from j = 1 to j = m . The detail is described as follows.

In the j -th phase, the path of w and b starts from the solution of SVM j  X  1 and ends at the solution of SVM j . This path shows a piece-wise linear manner [10], that is, the path is composed of joint lines. On each line segment of the path, w , b and the Lagrange multiplier  X  are all linear in When a line ends, we only need to calculate the direction of the next line, until the phase ends. The direction of these joint lines and their ending points can be derived from Eq. 2.
Divide the training instances X into three sets, X E , X L and X R . The index sets E , L and R are defined as where  X  i is the multiplier of X ( j ) of instances corresponding to support vectors. L consists of indices of instances inside the margin. R consists of indices of instances outside the margin.
 The Lagrange primal problem of Eq. 2 is C
Setting the derivatives to zero gives along with a KKT condition (only related ones are listed)
For convenience of calculation, differentiate Eqs. 4 X 7 with respect to  X  and get y E ( X ( j ) E d w + db ) = 0 , (11) where w ( j  X  1) denotes the first j  X  1 dimensions of while w j denotes the j -th dimension of w . By Eq. 5, w is calculated to be equal to P sign ( P  X  and d X  R and d X  L are zero, so they do not appear in this equation array. By solving Eqs. 8 X 11, we can get d w , db and d X  , the direction of the next line.

At the beginning of each phase, w j = 0 , so  X  = |
P the path grows, E will change when an instance i goes out of E (  X  i reaches 0 or 1) or an instance i goes into ( 1  X  y ( X ( j ) i w + b ) becomes 0). When this happens, the equation array does not hold any more and new directions need to be calculated with the updated E . This is the very point where the current line ends and the next one starts. When  X  decreases to 0, S does not constrain w j and the Lagrange primal problem is the same as that of SVM j , and w and b becomes the solution of SVM j . Thus, the current phase ends.

When the m -th phase ends, all features have been in-volved and a sequence of m SVMs has been obtained. Fig-ure 2 summarizes the whole training procedure.

Figure 3 illustrates the starting part of the solution path, where four features are involved. Each vertical line marks the end of a phase and the beginning of the next. The cross-points of w , b and a vertical line is a solution of an SVM. A new component of w is added at every vertical line.
The training process is very efficient, because in each phase when a new feature is added, it can make good use
Figure 2. Pseudo-code of the training proce-dure of the result of the phase before. Another merit of this pro-cess is its high precision since it solves the problem ana-lytically and the KKT condition is strictly satisfied at every step. Similar to [10], this method can be extended to use nonlinear kernels.
In the testing stage, T EFE tries to avoid the extraction of unnecessary features. The criterion of necessity is the classification confidence of the classifiers. Only when the confidence is lower than the threshold set by user, T goes to extract one more feature.

The confidence of SVM on a test instance can be mea-sured by the distance between the concerned instance and the decision boundary. The larger the distance, the more
Figure 4. Pseudo-code of the testing proce-dure confident the classification. Here, a threshold thr is used to control the sanity check of the confidence. Upon receiving a new test instance, the first feature is extracted and then the first SVM is fired. If the distance is larger than the current classification is regarded as the result; otherwise the next feature is extracted and the next SVM is invoked. Generally, for the i -th SVM, if Eq. 12 is satisfied, a clas-sification is made without extracting any more features or invoking any more classifiers.
Here the threshold thr provides the user a chance to con-trol the tradeoff between test time cost and classification ac-curacy. The higher the threshold value, generally the higher the final classification accuracy and the more the features to be extracted. In many tasks the user may want to control the performance directly instead of through a parameter. How-ever, the test accuracy is generally hard to foresee, and a direct control may be meaningless. For example, the user targets to 95% accuracy but the best achievable accuracy is only 90%. So, the soft-control on the performance accom-plished by setting the parameter thr may be a better choice.
In real tasks, it is often the case that some features are ex-tracted together. For example, image color histograms have several dimensions which are calculated together. We call such features as a feature group, and in the T EFE approach a feature group is regarded as a single feature.

Figure 4 summarizes the testing procedure. When a test instance comes, its first feature is extracted and fed to SVM 1 . If the classification margin is larger than threshold thr , the test instance is labeled with the output of SVM otherwise, the next feature is extracted and SVM 2 is fired. This process is repeated until no more feature is available, and in such case, the test instance is labeled by the last SVM.
 For multi-class problems, one-vs-all decomposition can
Table 1. Image features in our empirical study be used, and the difference only lies in that the confidence and prediction are given by multiple SVMs instead of a single SVM. For C classes we will train C sequences of SVMs, each sequence for one class. In the testing stage, when a new feature value is acquired, the extracted features will be fed to the corresponding SVMs in the C sequences. Here the stop criterion is that one of the C SVMs classifies the instance as positive with a confidence higher than the threshold thr , while the other SVMs classify the instance as negative; or no more feature is available. The remaining part is as same as that for two-class setting. Our empirical study involves several image applications. We employ several basic image feature extraction methods including color features and texture features, as tabulated in Table 1 where the running time per image (with size 384  X  256) of each method is also shown. All the time of executions are recorded on a machine with Pentium CPU 2.80GHz. These methods will generate sixty features in 28 groups for each image, where features in the same group are extracted together. Taking histogram of hue for exam-ple, when the histogram is calculated, all its 16 dimensions are obtained, or none can be calculated. The features of RGB values of sampling points, hue histogram, direction of Tamura features and Haralick features are grouped and must be calculated together. All the other features can be calculated one by one, among which one single feature is deemed as a group, thus there are 28 feature groups.
We compare T EFE with three test-cost sensitive algo-rithms, i.e., JIT [5], csNB [2] algorithm and csDT [14]. In addition, we also evaluate the performance of training
Figure 5. Average test time cost per image versus accuracy on scene classification SVM based on all features as a baseline. Since csNB has no parameters, its time cost and accuracy in classification is fixed. The JIT algorithm balances time cost and mutual in-formation through its parameter  X  when splitting nodes on features. If  X  is set to 0, only mutual information is consid-ered, and if  X  is set to 1, only time cost is considered. For JIT, we try different settings for its parameter  X  .
The dataset for scene classification is from [21], which contains 1,000 images of ten categories, and each cate-gory containing 100 images. Details of the data set can be found in [21]. Ten-fold cross-validation is executed on this dataset. For SVM and T EFE , the parameter C is set to 1.
By setting several different values for threshold thr , we get a curve of average test time cost per image versus ac-curacy for T EFE , as shown in Figure 5. By changing the parameter  X  , we also get a curve for JIT. The performance of each of the other three methods, i.e., csNB, csDT and SVM with all features (abbreviated as SVM), is shown as a marked point in Figure 5.

Figure 5 shows that T EFE has achieved a better perfor-mance than JIT, since 68.8% is the highest accuracy JIT can achieve when its parameter  X  is set to zero and the time cost is not taken into account. It is observed that JIT cannot uti-lize all features even when its parameter  X  is set to zero. In other words, the user cannot expect to utilize more features to achieve a better performance.

Compared with csNB, T EFE uses far less time to achieve the same accuracy. The accuracy of csNB is 74% with test-ing time 14.6 seconds per image. The test time cost is much more than that of T EFE , even much more than the time of extracting all features. This is because that the csNB method estimates the expected utility for every un-
Figure 6. Example images classified using different features. Left: Correctly classified using color features; Middle: Correctly clas-sified using color and texture features; Right:
Incorrectly classified even when all features are used used feature in every round, which costs even more time than extracting the feature. Moreover, csNB does not pro-vide the flexibility of controlling the tradeoff between test time cost and accuracy, and it will stop even when more features available.

Figure 5 also shows that T EFE is superior to csDT. The test time cost of csDT is very small, but it suffers from a very low accuracy (53%) that is not acceptable. This method does not provide flexibility of controlling the bal-ance between test time cost and accuracy. Both csDT and JIT use tree-based classifiers, and therefore they suffer from similar deficiencies.

By comparing T EFE and SVM using all features, it can be found that when all features are extracted, their perfor-mances are the same. This is not strange because the classi-fication made at the last round of T EFE is exactly made by a SVM using all features. In such case, the test time cost of T
EFE increases by 0.002 second, which indicates that T EFE spends a little bit extra time. By decreasing the threshold of T EFE to a relatively small value, for example thr = 4 the test time cost of T EFE reduces to 0.763 second while the accuracy drops for only 4%, from 85% to 81%.

We also make an inner investigation of T EFE . We set the threshold thr to 3.6, which leads to an acceptable accuracy of 80.1%. In the first round, only the RGB values of four points are engaged in classification, and 36 of 100 images of the fourth class is labeled out and only 5 images from other classes are wrongly classified as this class. In this dataset, the fourth class dinosaur (an example is the top left image in Figure 6), is easy to separate from other classes, even by simply sampling four points from the center area of an image. This fact supports our motivation that simple features are sufficient for easy instances.

Table 2. Number of images classified in dif-ferent round
Table 2 shows the number of images labeled in different rounds. In the first eleven rounds, only color information is used, and then in the following rounds, texture features are involved. With color information only, 347 images are labeled, 83% of which are correct. This shows that color in-formation alone can provide sufficient information for many instances, considering that the overall acceptable accuracy is 80.1%. The left column of Figure 6 shows two examples. Later, when texture features are involved, 349 more images are labeled. Two examples of such images are shown in the middle column of Figure 6. Finally, the most difficult 304 images go to the last round, among which 217 are correctly classified, with a relatively low accuracy. Two examples of such images are shown in the right column of Figure 6. Overall, we can conclude that T EFE has accomplished the task to extract simple features for easy instances and more features for difficult instances.
The second task is on data set obtained from Google im-age search. We selected 12 keywords and got 12 retrieval results. For each keyword, images of the first 200 results were downloaded. Note that the downloaded images are the resized images instead of the original images, so most of these images are less than 150  X  150 pixels. Images of each result are labeled as positive and negative, by considering whether they are related or not to its keyword. Images of this dataset are mostly smaller and more diverse than those in the scene classification dataset; this makes the classifica-tion task more difficult.

For images of every keyword, it is a binary classification problem and we use ten-fold cross-validation to evaluate the compared algorithms. The parameter C is set to 1 for SVM and T EFE . Results on the twelve sub-datasets are aggre-gated and shown in Figure 7.

Figure 7 shows that if all features are extracted, T EFE takes 0.18 second per image, but actually it can achieve a comparable performance using only 0.08 second. It is ob-vious that T EFE is superior to csDT and csNB. csNB costs 4.84 seconds per image, where a large portion of the time cost is spent to estimate the utility of features; but even with such a large time cost, its accuracy is only 62%. This might due to that its estimated utility is not very reliable. csDT is
Figure 7. Average test time cost per image versus accuracy on Google images better than csNB, but it is clear that with the same time, the accuracy of T EFE is better than that of csDT; and to achieve the same accuracy, T EFE uses a smaller time cost. Compar-ing T EFE and JIT it can be found that, by using the same test time cost, the accuracy of T EFE is much higher than that of JIT. Moreover, the best accuracy of JIT is 69.75% and it could not improve the performance further although there are more features that can be used, while T EFE is able to use more features to improve the performance further.
Synthetic images, such as icons, maps, charts, etc., be-come more and more popular on the Internet. Fast and accu-rate classification of these images is increasingly important. The dataset used here is from [20]. The original dataset con-tains images from five classes and some of their labels are missing. We selected images from two classes,  X  X ap X  and  X  X rtwork X . The first class has 261 images while the second one has 256 images.

Results on this data set are shown in Figure 8. Similar to the results in previous subsections, the performance of T
EFE is apparently superior to the compared methods.
As mentioned before, feature selection can be applied when the data has irrelevant and/or redundant features. Here we use the scene classification dataset for experiments, with 67% instances for training and the remaining 33% for test-ing.
 We first evaluate the traditional feature selection method. First, all 60 features are evaluated by Relief [13] and ranked by their merits on the training set. Then, every time the last feature in the rank is removed, a SVM is trained on the re-
Figure 8. Average test time cost per image versus accuracy on synthetic images
Figure 9. Average test time cost per image versus accuracy of T EFE with feature selec-tion maining features. The performance curve is shown as  X  X e-lief X  in Figure 9. Then, we evaluate the performance of T
EFE combined with feature selection. Here T EFE is ap-plied to the data sets after the Relief feature selection pro-cedure. We select 60 (i.e., no selection), 47, and 38 features according to the Relief rank, and then on the selected fea-tures we run T EFE . The performances are shown in Fig-ure 9.

From Figure 9 we can find that T EFE is beyond what have been done by traditional feature selection. The accu-racy of T EFE without feature selection (i.e., T EFE with 60 features Figure 9) is always better than Relief. Moreover, in most part T EFE achieves the same accuracy of Relif with a much smaller time cost. This partially dues to the fact that Relif has thrown away some features which are helpful for classifying some specific images but the discriminability is weak if considering the whole dataset. Moreover, when the
Figure 10. Threshold thr versus average test time cost time budget is smaller, the difference between T EFE Relif becomes more significant. For example, when the time budget is set to 0.8 second per image, the accuracy of T EFE reduces a little but the accuracy of Relife reduces about 10%.

It is noteworthy that by combining with a good feature selection process, T EFE is able to achieve a better perfor-mance than directly working on the original features (see T
EFE with 36 features in Figure 9). However, the feature selection process has to be used carefully, because inappro-priate feature selection will degenerate the performance (see T
EFE with 47 features in Figure 9).
Different users may have different requirements, and even the same user may have different requirements under different situations. So, it is desirable that the balance be-tween test time cost and classification accuracy can be con-trolled by the user. The adjustable parameter thr in T EFE provides such a flexibility to users.

Figure 10 plots the relationship between thr and test time cost on two data sets. It can be found that the relation-ship is almost linear. An initial explanation to this linear property is that, when thr increases by  X  thr , a portion of instances p which were originally classified in each round, except the last round, are postponed to the next round. So, the time cost increased by p  X  P m of the round and the slope is p  X  P m
This linear adjustability is a nice property of T EFE . For example, the linear relationship may be estimated by evalu-ating two sample thr values on the training set. Then, when the user allocates a test time budget, the system can estimate what is probably an appropriate thr for achieving the best accuracy within that time budget.
In many online applications, test time cost is a very im-portant issue. Roughly, test time cost consists of two parts, i.e., time cost for feature extraction and time cost of making prediction. Most state-of-the-art high performance classi-fiers spend a small time cost in making prediction. Now, to reduce test time cost, more effort should be devoted to reducing feature extraction time cost.

In this paper, we propose the idea of extracting only  X  X heap but sufficient X  features for test instances. For easy instance, instead of extracting all features, we try to extract only a few features which are with relatively small feature extraction time cost, but can provide sufficient for a correct classification.
 To achieve this goal, we propose the T EFE approach. T
EFE first orders the features according to their feature ex-traction time costs. For test instances, it extracts features one by one, and when the classification confidence based on the extracted features is high enough, the test instance will be classified and no more features will be extracted. A sequence of support vector machines are generated from training set, and there is an efficient solution to obtain those SVMs through regularization path. Experiment on three im-age applications show that the proposed T EFE method is superior to compared methods. It achieves acceptable high accuracy with a small test time cost, and provides the user with a flexibility to balance the test time cost and classifi-cation accuracy according to the user X  X  favor. Moreover, it is possible to combine T EFE with feature selection methods to achieve even a better performance.
 The current work can be extended in several directions. It is interesting to design a better method to rank features for our purpose. It is also interesting to incorporate ensemble methods into the framework to improve the exploitation of the extracted features.

This research was supported by the National NSFC (60505013, 60635030), JiangsuSF (BK2008018), FANEDD (200343), 863 Program (2007AA01Z169), and MSRA IST Program.

