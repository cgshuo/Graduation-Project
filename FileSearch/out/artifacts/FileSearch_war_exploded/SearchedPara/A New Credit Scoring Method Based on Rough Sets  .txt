 credit scoring model. Those models include traditional statistical models (e.g.: logistic trees [11, 12] and neural network models [3]). All these models are widely used. But they didn X  X  all process the original sample data when they were used to build the credit scoring model. It is necessary and very important to preprocess the original sample data to eliminate redundant data and noise data, etc. In this paper, we finish the process by using decision tree method. in section 3. The design and generating of our model will be illustrated in section 4. In addresses the conclusion and discusses the possible future research work. 2.1 The Information System An information system can be represented as follows: information function such that a V a x f  X  ) , ( , for any A a  X  and U x  X  . 2.2 Indiscernibility Relation be defined as follows: 
There is an indiscernibility relation ) ( P INP , for arbitrary attribute subset A P  X  : set P . 2.3 Reduction of Concept and the Core attributes; meanwhile, we must maintain the primary areas of the information system. This problem refers to two basic concepts: reduction and core [9, 13, 14]. 2.4 Discernibility Matrix cernibility matrix can be used to complete attributes reduction. Definition 1. [13, 14]: Given an information system S , of objects, Discernibility matrix is denoted by M(S) , whose elements are as follows: The ID3 [11] is a famous algorithm to construct a decision tree. And the C4.5 [12] is the extended version of the ID3. The C4.5 mainly contains two phases: generating an initial decision tree and pruning the initial decision tree. 3.1 Generating Decision Tree criterion gain refers to the concept entropy in information theory [6, 10]. 3.2 Pruning Decision Tree specific technique to estimate the prediction error rate. This technique is called pessi-mistic error pruning [11]. 4.1 Design theory and decision tree theory. Firstly, our model preprocesses the sample data by using bring many benefits. Rough set can not only remove redundant data but also simplify the dimension of input information space by discovering the relation among all data. credit scoring classifier. The details will be presented in next section. 4.2 Reduction of Attributes reduction of attributes . We process the redundant attributes by using the algorithm in Wang and Pei X  X  paper [14] (denoted by WPA in this paper). In this paper we improved run-time. follows: Theorem 1. [1]: a b a a  X   X   X  ) ( . We will not prove the Theorem 1 because it is easy to prove it. Algorithm 1. Compute all reductions of attributes. Input : Information system f V A U S , , , = ; Output : All reductions of attributes of the information system f V A U S , , , = ; Procedure : Output : M DNF Procedure: 2. if  X  = M return  X  ; 6. = M DNF reduction of R ; return M DNF ; and the definition of the DNF are in [14]. The step 3 with notation  X * X  is our optimiza-tion step in algorithm 1, and other steps is the same as the WPA. the Algorithm 1. The value of K will be greatly decreased, because the size |M| of the set M is greatly decreased. And the rank of K is 4, so the Algorithm 1 will evidently attribute will be given in the section 5. 4.3 Generating Classifier After reduction of attributes, we got a new sample data set. We can randomly select a sample. We build the credit scoring model by using the Algorithm C4.5. 5.1 Efficiency of Reduction of Attributes lected nine databases from the database of UCI machine learning. The experiment was completed on the same PC (Intel-Celeron, 2.4GHz, 256MB RAM  X  WinXP Profes-sional). We obtained the same results by the two methods. And the experimental results on the running time are reported at the Table 1: paring with WPA on the running time. It shows that it is effective that we improved the WPA. The analysis about time complexity of the Algorithm 1 is validated by the result. 5.2 Prediction Accuracy Analysis The two databases of in our experiments are from the UCI Machine Learning Re-pository [8]: German Credit database and Australian Credit database. For the German Credit, there are in all 1000 instances which contain 700 good credit instances and 300 bad credit instances, and each instance consis ts of 20 predictive attributes and 1 class predictive attributes and 1 class attribute. 
We respectively test the two databases by using the rough set &amp; C4.5 method (de-noted by RSC) and the single C4.5 with two different ratios. The two ratios are 7:3 and 8:2 between the size of train sample and the size of test sample. We had 20 experiments by using RSC and the single C4.5 for each database. The process of choice train sample choosing train sample. The experimental results are respectively reported at the Table 3 and at the Table 4.

According to the Table 3, only 12 of all 20 predictive attributes in RSC were used to improved with the radio 7:3 and with 8:2. The average prediction accuracy is heightened about 6%  X  7%. The RSC method had a good performance for the German Credit da-tabase. From the Table 4, we can find that the RSC has also a good performance for the Australian Credit database. 
By comparing with the single C4.5, we conclude that the RSC method have a good dimension of the decision table (the original sample), but also enhanced the prediction accuracy of the credit scoring model. Reduction of attribute before building model is building the credit scoring model. However, we must notice the index min prediction accuracy in the Table 3 and Table 4. The min prediction accuracy is low in the RSC method, which indicates that the stability of RSC is not good. We think that it is caused by the algorithm C4.5 by theoretical analysis. We can get the worst in stance, and the prediction accuracy is just 0%. Suppose the 700 train instances are all from the 700 good instances and the 300 test instances are all from the 300 bad instances, which is possible though the probability is very tiny. If it happened, the generated decision tree would just have one node, and the class of the node is good . So we would get the 100% error rate when we test all bad instances in the test experiment. Hence, we can conclude that the prediction accuracy of sample. 5.3 Comparing with Other Methods Because those methods on credit scoring problem in many papers often use the k-fold cross validation to complete experiment, we tested again the prediction accuracy of our model by using the 10-fold cross validation by using the RSC method to compare with other methods. The result is the average of the accuracy determined for each of the 10 periments with the 10-fold cross validation (10x10-CV). The results are reported at the Table 5.
 programming and SVM-based are successfully applied to build the credit scoring will be compared with these methods for the German Credit database and Australian Credit database. 
We will compare RSC with the single C4.5, BPN (Back-propagation Neural Net-work), GP (Genetic Programming) [6], SVM+GA (Support Vector Machine +Genetic Algorithm) [2].The results of the two databases are summarized in the Table 6 by using RSC, single C4.5, BPN, GP, and SVM+GA. Where, the results of BP, GP, and SVM+GA are from the paper [2]. independent test sets by the same 10-fold cross validation proced ure. The GP specific parameters for the set two credit datasets areas follows: population size is 250, repro-duction rate is 0.2, crossover rate is 0.7, mutation rate is 0.08, and maximum number of configurations are tested, in which14-32-1 and 24-43-1 respectively for the Australian data and German data are selected to obtain better results. Additionally, the learning chooses their default settings. 
On the basis of the results of Table 6, we can conclude that the RSC method in our study outperforms to other methods for Australian Credit database and German Credit scoring model in this paper. derliness and quickly make decision. The credit scoring model based on rough set and decision tree in this paper fully exhibits the advantages of rough set and decision tree. tribute in this paper is a very important and effective instrument to improve the pre-diction accuracy. Reducing the redundant attributes not only avoids the harmful data to curacy than the single C4.5, BP, GP, and SVM+GA on the benchmarks. The RSC method is effective and successful on the credit scoring problem in this paper. How-combining with the Boosting Algorithm or Bagging Algorithm to get higher accuracy,. The paper is supported by the National Nature Science Foundation of China (Grant no. A0710023) and academician start-up fund (Grant No. X01109) and 985 information technology fund (Grant No. 0000-X07204) in Xiamen University. 
