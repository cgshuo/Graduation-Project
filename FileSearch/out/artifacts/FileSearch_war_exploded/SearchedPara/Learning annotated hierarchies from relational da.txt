 Researchers in AI and cognitive science [1, 7] have proposed that hierarchies are useful for rep-resenting and reasoning about the objects in many real-worl d domains. One of the reasons that hierarchies are valuable is that they compactly specify cat egories at many levels of resolution, each node representing the category of objects at the leaves belo w the node. Consider, for example, the simple hierarchy shown in Figure 1a, which picks out five cate gories relevant to a typical university department: employees, staff, faculty, professors, and as sistant professors.
 Suppose that we are given a large data set describing the feat ures of these employees and the in-teractions among these employees. Each of the five categorie s will account for some aspects of the data, but different categories will be needed for unders tanding different features and relations.  X  X aculty, X  for example, is the single most useful category f or describing the employees that publish papers (Figure 1b), but three categories may be needed to des cribe the social interactions among the employees (Figure 1c). In order to understand the structure of the department, it is important not only to understand the hierarchical organization of the emp loyees, but to understand which levels in the hierarchy are appropriate for describing each feature a nd each relation. Suppose, then, that an annotated hierarchy is a hierarchy along with a specification of the categories in the hierarchy that are relevant to each feature and relation.
 The idea of an annotated hierarchy is one of the oldest propos als in cognitive science, and researchers including Collins and Quillian [1] and Keil [7] have argued t hat semantic knowledge is organized into representations with this form. Previous treatments o f annotated hierarchies, however, usually suffer from two limitations. First, annotated hierarchies are usually hand-engineered, and there are few proposals describing how they might be learned from d ata. Second, annotated hierarchies typically capture knowledge only about the features of obje cts: relations between objects are rarely considered. We address both problems by defining a generativ e model for objects, features, relations, and hierarchies, and showing how it can be used to recover an a nnotated hierarchy from raw data. Our generative model for feature data assumes that the objec ts are located at the leaves of a rooted tree, and that each feature is generated from a partition of t he objects  X  X onsistent X  with the hierarchy. A tree-consistent partition (henceforth, t-c partition) of the objects is a partition of the objects into disjoint categories, i.e. each class in the partition is exa ctly the set of leaves descending from some node in the tree. Therefore, a t-c partition can be uniquely e ncoded as the set of these nodes whose leaf descendants comprise the classes (Figure 1a,b). The si mplest t-c partition is the singleton set containing the root node, which places all objects into a sin gle class. The most complex t-c partition is the set of all leaves, which assigns each object to its own c lass. We assume that the features of objects in different classes are independent, but that obje cts in the same class tend to have similar features. Therefore, finding the categories in the tree most relevant to a feature can be formalized as finding the simplest t-c partition that best accounts for t he distribution of the feature (Figure 1b). We define an annotated hierarchy as a hierarchy together with a t-c partition for each feature. Although most discussions of annotated hierarchies focus o n features, much of the data available to human learners comes in the form of relations. Understand ing the structure of social groups, for Like the feature case, our generative model for relational d ata assumes that each (binary) relation is generated from a t-c partition of the set of all pairs of objects . Each class in a t-c partition now corresponds to a pair of categories (i.e. pair of nodes) (Fig ure 1c), and we assume that all pairs in a given class tend to take similar values. As in the feature cas e, finding the categories in the tree most tion of the relation. The t-c partition for each relation can be viewed as an additional annotation of the tree. The final piece of our generative model is a prior ove r rooted trees representing hierarchies. Roughly speaking, the best hierarchy will then be the one tha t provides the best categories with which to summarize all the features and relations.
 Like other methods for discovering structure in data, our ap proach may be useful both as a tool for data analysis and as a model of human learning. After desc ribing our approach, we apply it to several data sets inspired by problems faced by human learne rs. Our first analysis suggests that the model recovers coherent domains given objects and features from several domains (animals, foods, tools and vehicles). Next we show that the model discovers in terpretable structure in kinship data, and in data representing relationships between ontologica l kinds. Our approach is organized around a generative model for feat ure data and relational data. For sim-plicity, we present our model for feature and relational dat a separately, focusing on the case where we have a single binary feature or a single binary relation. A fter presenting our generative model, we describe how it can be used to recover annotated hierarchi es from data.
 We begin with the case of a single binary feature and define a jo int distribution over three entities: a rooted, weighted, binary tree T with O objects at the leaves; a t-c partition of the objects; and feature observations, d . For a feature, a t-c partition  X  is a set of nodes { n each object is a descendant of exactly one node in  X  . We will identify each node with the category of objects descending from it. We denote the data for all obje cts in the category n as d leaf (single object category), then d partitions associated with the hierarchy are represented a nd each class in each partition is labeled with the corresponding category.
 The joint distribution P ( T, w,  X , d |  X ,  X  Consider now the case where we have a single binary relation d efined over all ordered pairs of objects { ( o binary tree; a t-c partition of ordered pairs of objects; and observed, relational data represented as a matrix D where D Given a pair of categories ( n is an object in the category n t-c partition,  X  , is a set of pairs of categories { ( n pair of objects ( o To help visualize these 2D t-c partitions, we can reorder the columns and rows of the matrix D according to an in-order traversal of the binary tree T . Each t-c partition now splits the matrix into contiguous, rectangular blocks (see Figure 1c, where e ach rectangular block is labeled with its category pair). Assuming we have already generated a rooted , weighted binary tree, we now specify the generative process for a single binary relation (c.f. st eps iii through v in the feature case): conditionally independent given the weighted tree T . 2.1 Inference Given observations of features and relations, we can use the generative model to ask various ques-tions about the latent hierarchy and its annotations. We sta rt by determining the posterior distribution on the weighted tree topologies, ( T, w ) , given data D = ( { d ( f ) } F F features and R relations and hyperparameters  X  and  X  = ( {  X  P ( T, w | D,  X ,  X  )  X  P ( T ) P ( w | T,  X  ) P ( D | T, w,  X  ) But P ( d ( f ) | T, w,  X  t-c partitions induced by the stochastic function  X  and P ( d ( f ) |  X ,  X  the partition, marginalizing over the feature probabiliti es,  X  dent, P ( d ( f ) |  X ,  X  marginal likelihood for d ( f ) sets, M an exponential number of t-c partitions, we present an effici ent dynamic program for calculating T First observe that, for all objects (i.e. leaf nodes) o , T ancestor of n is in  X  . With probability  X  ( w contribution to T Now the possible partitions of the objects in category n are every t-c partition of the objects below n paired with every t-c partition below n For the relational case, we describe a dynamic program T the probability of all relations between objects in n  X  m , conditioned on the tree, having marginalized out the t-c partitions and relation probabilities. Let M marginal likelihood of the relations in n  X  m . For relations, M n and m are both leaves, then T
T r ( n, m ) =  X  ( w n )  X  ( w m ) M r ( n, m ) The above dynamic programs have linear and quadratic comple xity in the number of objects, re-spectively. Because we can efficiently compute the posterio r density of a weighted tree, we can search for the maximum a posteriori (MAP) weighted tree. Conditioned on the MAP tree, we can efficiently compute the MAP t-c partition for each feature an d relation. We find the MAP tree first, rather than jointly optimizing for both the topology and par titions, because marginalizing over the t-c partitions produces more robust trees; marginalizatio n has a (Bayesian)  X  X ccam X  X  razor X  effect and helps avoid overfitting. MAP t-c partitions can be comput ed by a straightforward modification of the above dynamic programs, replacing sums with max opera tions and maintaining a list of nodes representing the MAP t-c partition at each node in the tree.
 We chose to implement global search by building a Markov chai n Monte Carlo (MCMC) algorithm with the posterior as the stationary distribution and keepi ng track of the best tree as the chain mixes. For all the results in this paper, we fixed the hyperparameter s of all beta distributions to  X  = 0 . 5 (i.e. the asymptotically least informative prior) and repo rt the (empirical) MAP tree and MAP t-c partitions conditioned on the tree. The MCMC algorithm sear ches for the MAP tree by cycling through three Metropolis-Hastings (MH) moves adapted from [14]: The first two moves suffice to make the chain ergodic; subtree s wapping is included to improve mixing. The first and last moves are symmetric. We initialize d the chain on a random tree with weights set to one, ran the chain for approximately one milli on iterations and assessed convergence by comparing separate chains started from multiple random i nitial states. 2.2 Related Work There are several methods that discover hierarchical struc ture in feature data. Hierarchical clustering [4] has been successfully used for analyzing both biologica l data [18] and psychological data, but cannot learn the annotated hierarchies that we consider. Ba yesian hierarchical clustering (BHC) [6] clustering model, but lacks any notion of annotations. It is possible that a BHC-inspired algorithm could be derived to find approximate MAP annotated hierarchi es. Our model for feature data is most closely related to methods for Bayesian phylogenetics [14] . These methods typically assume that features are generated directly by a stochastic process ove r a tree. Our model adds an intervening layer of abstraction by assuming that partitions are genera ted by a stochastic process over a tree, and that features are generated from these partitions. By intro ducing a partition for each feature, we gain the ability to annotate a hierarchy with the levels most rele vant to each feature.
 There are several methods for discovering hierarchical str ucture in relational data [5, 13], but none of these methods provides a general purpose solution to the p roblem we consider. Most of these methods take a single relation as input, and assume that the h ierarchy captures an underlying com-munity structure: in other words, objects that are often pai red in the input are assumed to lie nearby in the tree. Our approach handles multiple relations simult aneously, and allows a more flexible map-ping between each relation and the underlying hierarchy. Di fferent relations may depend on very different regions of the hierarchy, and some relations may e stablish connections between categories that are quite distant in the tree (see Figure 4).
 Many non-hierarchical methods for relational clustering h ave also been developed [10, 16, 17]. One family of approaches is based on the stochastic blockmodel [ 15], of which the Infinite Relational Model (IRM) [9] is perhaps the most flexible. The IRM handles m ultiple relations simultaneously, and does not assume that each relation has underlying commun ity structure. The IRM, however, does not discover hierarchical structure; instead it parti tions the objects into a set of non-overlapping categories. Our relational model is an extension of the bloc kmodel that discovers a nested set of categories as well as which categories are useful for unders tanding each relation in the data set. We applied our model to three problems inspired by tasks that human learners are required to solve. Our first application used data collected in a feature-listi ng task by Cree and McRae [2]. Participants in this task listed the features that came to mind when they th ought about a given object: when asked to think about a lemon, for example, subjects listed feature s like  X  X ellow, X   X  X our, X  and  X  X rows on trees. X  1 We analyzed a subset of the full data set including 60 common o bjects and the 100 features most commonly listed for these objects. The 60 objects are sh own in Figure 2, and were chosen to represent four domains: animals, food, vehicles and tools.
 Figure 2 shows the MAP tree identified by our algorithm. The mo del discovers the four domains as well as superordinate categories (e.g.  X  X iving things X , including fruits, vegetables, and animals) and subordinate categories (e.g.  X  X heeled vehicles X ). Fig ure 2 also shows MAP partitions for 10 representative features. The model discovers that some fea tures are associated only with certain parts of the tree:  X  X s juicy X  is associated with the fruits, a nd  X  X s metal X  is associated with the man-made items. Discovering domains is a fundamental cognitive problem that may be solved early in development [11], but that is ignored by many cognitive mo dels, which consider only carefully chosen data from a single domain (e.g. data including only an imals and only biological features). By organizing the 60 objects into domains and identifying a sub set of features that are associated with each domain, our model begins to suggest how infants may pars e their environment into coherent domains of objects and features.
 Our second application explores the acquisition of ontolog ical knowledge, a problem that has been previously discussed by Keil [7]. We demonstrate that our mo del discovers a simple biomedical ontology given data from the Unified Medical Language System (UMLS) [12]. The full data set in-cludes 135 entities and 49 binary relations, where the entit ies are ontological categories like  X  X ign or Symptom X ,  X  X ell X , and  X  X isease or Syndrome, X  and the relati ons include verbs like causes , analyzes and affects . We applied our model to a subset of the data including the 30 e ntities shown in Figure 3. The MAP tree is an ontology that captures several natural gro upings, including a category for  X  X iving things X  (plant, bird, animal and mammal), a category for  X  X h emical substances X  (amino acid, lipid, antibiotic, enzyme etc.) and a category for abnormalities. The MAP partitions for each relation identify the relevant categories in the tree relatively cle anly: the model discovers, for example, that relation causes , since neither of these categories can cause anything (acco rding to the data set). This distinction, however, is relevant to the second place of causes : substances can cause abnormalities and dysfunctions, but cannot cause  X  X iving things X . Note that the MAP partitions for causes and analyzes are rather different: one of the reasons why discovering sep arate t-c partitions for each relation is important is that different relations can depen d on very different parts of an ontology. Our third application is inspired by the problem children fa ce when learning the kinship structure of their social group. This problem is especially acute for c hildren growing up in Australian tribes, which have kinship systems that are more complicated in many ways than Western kinship systems, but which nevertheless display some striking regularities . We focus here on data from the Alyawarra tribe [3]. Denham [3] collected a large data set by asking 104 tribe members to provide kinship terms for each other. Twenty-six different terms were mentioned i n total, and four of them are represented in Figure 4. More than one kinship term may describe the relat ionship between a pair of individuals  X  since the data set includes only one term per pair, some of th e zeros in each matrix represent missing data rather than relationships that do not hold. For simplicity, however, we assume that relationships that were never mentioned do not exist.
 The Alyawarra tribe is divided into four kinship sections, a nd these sections are fundamental to the social structure of the tribe. Each individual, for inst ance, is permitted only to marry individuals from one of the other sections. Whether a kinship term applie s between a pair of individuals depends on their sections, ages and genders [3, 8]. We analyzed a subs et of the full data set including 64 individuals chosen to equally represent all four sections, both genders, and people young and old. The MAP tree divides the individuals perfectly according to kinship section, and discovers additional structure within each section. Group three, for example, is split by age and then by gender. The MAP partitions for each relation indicate that different relat ions depend very differently on the structure of the tree. Adiadya refers to a younger member of one X  X  own kinship section. The M AP partition for this relation contains fine-level structure only along t he diagonal, indicating that the model has discovered that the term only applies between individuals f rom the same kinship section. Umbaidya can be used only between members of sections 1 and 3, and membe rs of sections 2 and 4. Again the MAP partition indicates that the model has discovered th is structure. In some places the MAP partitions appears to overfit the data: the partition for Umbaidya , for example, appears to capture some of the noise in this relation. This result may reflect the fact that our generative process is not quite right for these data: in particular, it does not captur e the idea that some of the zeroes in each relation represent missing data. We developed a probabilistic model that assumes that featur es and relations are generated over an annotated hierarchy, and showed how this model can be used to recover annotated hierarchies from raw data. Three applications of the model suggested that it i s able to recover interpretable structure in real-world data, and may help to explain the computationa l principles which allow human learners to acquire hierarchical representations of real-world dom ains.
 Our approach opens up several avenues for future work. A hier archy specifies a set of categories, and annotations indicate which of these categories are impo rtant for understanding specific features and relations. A natural extension is to learn sets of catego ries that possess other kinds of structure, such as factorial structure [17]. For example, the kinship d ata we analyzed may be well described by three sets of overlapping categories where each individu al belongs to a kinship section, a gender, and an age group. We have already extended the model to handle continuous data and can imag-ine other extensions, including higher-order relations, m ultiple trees, and relations between distinct sets of objects (e.g. given information, say, about the book -buying habits of a set of customers, this extension of our model could discover a hierarchical repres entation of the customers and a hierar-chical representation of the books, and discover the catego ries of books that tend to be preferred by different kinds of customers). We are also actively explori ng variants of our model that permit accu-rate online approximations for inference; e.g., by placing an exchangeable prior over tree structures based on a Polya-urn scheme, we can derive an efficient partic le filter.
 We have shown that formalizing the intuition behind annotat ed hierarchies in terms of a prior on trees and partitions and a noise-robust likelihood enabled us to discover interesting structure in real-world data. We expect a fruitful area of research going forwa rd will involve similar marriages be-tween intuitions about structured representation from classical AI and cognitive science and modern inferential machinery from Bayesian statistics and machine learning.

