 Team formation has been long recognized as a natural way to acquire a diverse pool of useful skills, by combining ex-perts with complementary talents. This allows organizations to effectively complete beneficial projects from different do-mains, while also helping individual experts position them-selves and succeed in highly competitive job markets. Here, we assume a collection of projects P , where each project re-quires a certain set of skills, and yields a different benefit upon completion. We are further presented with a pool of experts X , where each expert has his own skillset and com-pensation demands. Then, we study the problem of hiring a cluster of experts T  X  X , so that the overall compensation (cost) does not exceed a given budget B , and the total ben-efit of the projects that this team can collectively cover is maximized. We refer to this as the ClusterHire problem. Our work presents a detailed analysis of the computational complexity and hardness of approximation of the problem, as well as heuristic, yet effective, algorithms for solving it in practice. We demonstrate the efficacy of our approaches through experiments on real datasets of experts, and demon-strate their advantage over intuitive baselines. We also ex-plore additional variants of the fundamental problem formu-lation, in order to account for constraints and considerations that emerge in realistic cluster-hiring scenarios. All variants considered in this paper have immediate applications in the cluster hiring process, as it emerges in the context of differ-ent organizational settings.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Team Formation; Online Marketplaces
When searching for a group of experts to hire, organiza-tions and decision-makers aim to find the most cost-effective team that can accomplish their goals. The hiring process in-cludes allocating an available budget toward the recruitment of a set of experts from a collection of candidates, in order to form a team that has all the required expertise to perform a large number of profitable projects.

As an example, consider the recruitment of a cluster of college professors by a university. In this setting, the uni-versity has to hire individuals that allow it to maximize its academic benefit, given the current opportunities and trends in terms of research and funding options. Given the avail-able budget, the goal is to recruit a cluster of professors that can collectively provide the expertise required to capitalize on such opportunities and maximize the university X  X  bene-fit. The benefit can be measured in terms of the number of publications or citations, the amount of dollars at grants, or any other function that the university wants to optimize.
In an industry setting, cluster hiring scenarios are even more prevalent. Perhaps the most characteristic example is a typical startup company, where the founders need to se-lect a team of experts with the combined expertise required to capitalize on the different opportunities that have been identified within the market that the company targets.
Another relevant setting comes from the domain of online labor markets, such as oDesk ( www.odesk.com ), Freelancer ( www.freelancer.com ), and Guru ( www.guru.com ). In these online portals, employers hire freelancers with various skills to work remotely on different types of projects. In the early stages of this model, freelancers registered and worked inde-pendently. However, as the competition grew, experts real-ized that it is in their best interest to form  X  X ives X , known as agencies, with people of complementary skillsets [11]. This allows them to diversify their talent pool, and go after a larger number of more profitable projects. This trend, has also been recognized by major companies in this area, such as oDesk.com , which are already offering team-hiring ser-vices to their enterprise customers 1 .

In this paper, we formalize the cluster hiring problem as follows: we assume a pool of n experts X , where each ex-pert i is associated with the set of skills X i that he masters. Additionally, we assume a set of m projects P , where each project P  X  P is also associated with a set of skills; these are the skills that are required for the project to be com-pleted. Finally, every project is associated with a profit F ( P ) and every expert is associated with a cost C ( X https://www.odesk.com/info/enterprise/ which corresponds to X  X  X  compensation. Given a budget B , our goal is to form a team of experts T  X  X such that P
X  X  X  C ( X )  X  B and the sum of the profits of the projects that T can perform is maximized. In the basic version of the problem, we assume that T can perform a project P only if for every required skill in P , there exists at least one worker in T that has this skill. We call this problem the ClusterHire problem.

In addition to this fundamental definition, we consider variants of the ClusterHire problem that take into con-sideration (i) a probabilistic version of project profits, based on how likely it is for the team to actually acquire and com-plete the project, and (ii) constraints on the usage of the experts with respect to how many projects they are willing and able o participate in simultaneously.
 Contributions: To the best of our knowledge we are the first to define and study the ClusterHire problem and its variants. More specifically, we show that this problem is NP-hard to solve and even approximate. We then proceed to design effective heuristics and demonstrate their efficacy in practical settings. For our experiments we use data from Freelancer and Guru. These are two of the largest players in the rapidly growing area of online labor , which has been identified as a clear candidate for the cluster hire setting [11]. In addition to experimentally evaluating our algorithms and investigating the characteristics of their solutions, we also provide an extensive data-analytic and visual study of these datasets that provides a much deeper understanding of the nature and dynamics of expert communities.
 Roadmap: The rest of the paper is organized as follows: we review the related work in Section 2. In Section 3, we de-fine the ClusterHire problem and study its computational complexity. Our algorithms for the problem are described in Section 4, and in Section 5 we provide a thorough analy-sis of our datasets. An extensive experimental evaluation of our methods is given in Section 6. We conclude the paper in Section 7.
To the best of our knowledge we are the first to introduce and study the ClusterHire problem and its variants. How-ever, our work is clearly related to other recently-studied team formation problems as well as literature on studying team dynamics and predicting the performance of teams, as well as work on inferring the abilities of the team members based on their team performance. We give an overview of this body of related work below.

Our previous work [14] was the first to introduce the prob-lem of team formation in the context of social networks. Given a pool of experts and a set of skills that needed to be covered, the goal was to select a team of experts that can collectively cover all required skills, while ensuring ef-ficient communication between the team X  X  members. Over the last years, this work has been extended in multiple di-rections. For example, there exists recent work that focuses on incorporating different definitions of communication costs between experts [1, 6, 8, 13, 15, 19]. Others take into con-sideration different levels of users X  abilities and capacity to participate in different projects [18]. Finally, more recently, the online version of the problem was introduced where the goal is to create multiple teams that can address multiple projects that arrive in an online fashion [3]. The common characteristic of all the above works is that they assume a network of experts and therefore, all these formulations have a graph-theoretic component. Our work does not assume the existence of a graph and therefore our setting raises differ-ent computational questions than the ones addressed in the above papers.
 Probably the most related to ours is the recent work by Anagnostopoulos et al. [2]. This paper considers a pool of experts, with each expert associated with a set of skills, and a collection of projects arriving one at a time in an online fashion. Each project is characterized by the set of skills required for its completion. In the version of their problem that is most related to ours, Anagnostopoulos et al. aim to create one team for each project, such that over time, the maximum number of teams that each expert partici-pates in is minimized. There are two significant differences between their setting and ours. First, our goal is to cre-ate a single team that can address many projects, while the goal of Anagnostopoulos et. al. is to choose a single team per project. Second, we do not assume that projects arrive online: in our setting the set of projects is known apriori. Therefore, again, the computational issues we face and ad-dress here are distinct.

In the area of online gaming and robot team formation, the problem of inferring the performance of a teams of play-ers has been investigated at length [17, 16]. Most recently Liemhetcharat and Velose [17] explore a version of the prob-lem in which each expert has a non-deterministic capability with respect to each project. The expected performance of the team is then evaluated based on the synergy of the par-ticipating experts, rather than their coverage of skills. Other relevant works [5, 7] evaluate team performance from an anthropological/sociological perspective. We consider these complementary to our computational approach.

Recently Gionis et al. [10] have proposed a combinato-rial method for computing the skillset of individual experts, based on their participation in successful teams. Their prob-lem setting can be thought as inverse to ours, since our goal is to compute teams based on the skillsets of experts.
In this section, we provide the notation we will use through-out the paper, and present the formal definitions and com-plexity results for different versions of our problem.
Throughout the discussion we will assume that there is a set of ` skills S , a set of n experts X and a set of m projects P . Each expert X  X  X  , is represented by a subset of skills, i.e., X  X  S ; these are the skills that the expert possesses. Similarly, every project P  X  P is characterized by the set of skills that are required in order for the project to be completed (i.e., P  X  S ).

In addition to the above, we have a profit function ( F ), such that for every P  X  X  , F ( P ) gives the (expected) profit that completing P with incur. Similarly, for every expert X , function C ( X ) gives the cost of hiring a particular expert.
For a team of experts T  X  X , we say that team T has a skill s if there exist at least one expert X  X  X  , such that X has skill s , i.e., s  X  X . For a project P  X  P , we say that team T covers P if T (as a team) has all the skills required for P . Clearly, a team of experts may cover more than one projects; in fact the more such projects being covered by a single team the better the team. That is, we define the coverage of team T to be the set of distinct projects that the team can cover. That is,
Given the projects that a team T can cover, we define the profit of the team to be the summation of the profits of the projects that T covers. That is,
In addition, every team incurs a certain cost, computed as the sum of the costs of its members. That is, Dollar profit: In real applications, there is typically a pre-specified gain (in terms of dollars) that the completion of a project will yield for the team (or for the organization that has hired the team). This dollar amount can thus naturally serve as the profit of that project, which we refer to as F Competition-based profit: The above dollar-based as-signment of profits to projects does not consider the uncer-tainty that is involved in the process of trying to acquire and complete a project. Consider the following example: we are presented with two projects P 1 and P 2 that are worth the same dollar amount. However, assume that P 1 requires a set of  X  X undane X  skills, i.e., skills that are very popular among the pool experts, while P 2 requires a rare skill s  X  . Clearly, there is larger competition for project P 1 , since there are many possible teams that can contribute the required skills. On the other hand, the competition for P 2 is smaller due to the rare skill that it requires. Therefore, a team that has s in its combined skillset has a higher probability of being assigned project P 2 , if it chooses to pursue it.

This competitive setting is simply one of the alternative instantiations of uncertainty, which is present in all the ap-plications we consider. Alternatively, one could consider the probability of completing a project even after it has been secured, given the deadlines and other constraints placed by the employer. Our framework is compatible with any method that computes the probability of success for each project. For our own experiments, we compute the dollar-based profit of a project P as follows: where s  X  ( P ) is the rarest skill among the skills required for P and freq ( s  X  ( P )) is the number of experts that actually possess this skill. While we experimented with other alter-natives, including the median and average frequency of the skills required by a project, we found that using the mini-mum frequency yielded the most intuitive results.

Throughout the paper, we use the generic notation ( F ) to refer to projects X  profits and only use F d and F c when we need to explicitly compare them.
Given the above notation, we can now define the main problem addressed in this paper as follows:
Problem 1 (ClusterHire). Given a set of projects P , a pool of experts X , cost and profit functions C () and F () , respectively, and a budget B  X  R + find T  X  X such that F ( Cov ( T )) is maximized and C ( T )  X  B .

From the computational point of view, we have the fol-lowing results for the ClusterHire problem.

Lemma 1. The decision version of the ClusterHire prob-lem is NP-complete.

Proof. For our proof, we will consider a simplified ver-sion of the problem where P consists of a single project P and F ( P ) = 1. Moreover, C ( X ) = 1 for every expert X  X  X . In the decision version of this simplified instance of the ClusterHire problem the question is if there exist a team of K experts that covers project P .

Now, we will reduce the decision version of the SetCover problem [9] to this simplified version of the ClusterHire problem. In the classical SetCover problem there is a uni-verse of items U and a set of sets Q such that for every Q  X  Q , Q  X  U . The question in the decision version of the problem is whether there exist L sets from Q , forming Q such that  X  Q  X  X  0 Q i = U .

Clearly, if we map every set Q i  X  Q from SetCover into an expert X i  X  X of ClusterHire , the two problems become identical. That is, there exists a solution of size L in the SetCover problem if and only if there exists a solution of cost L in the ClusterHire problem.

Lemma 2. The ClusterHire problem is NP-hard to ap-proximate.

Proof. The proof of the above lemma uses the same sim-plified decision version of the ClusterHire problem used in the proof of Lemma 1, as well as some intuition we gained from that proof.

More specifically, we will prove the lemma by contradic-tion. That is, assume that there exists an  X  -factor approx-imation algorithm for this simplified version of the terHire problem such that if T A is the solution of this algorithm and T  X  is the optimal solution, we have that: F ( T A )  X   X F ( T  X  ).

Now observe that such an algorithm can be used to de-cide whether there exists a solution consisting of K ex-perts that could perform the project P of the Cluster-Hire problem. However, as we showed above, the decision version of this problem is NP-complete. Therefore, such an  X  -approximation algorithm cannot exist.
 The t-ClusterHire problem: Observe that the Clus-terHire problem as defined in Problem 1 allows for solu-tions where a single expert X  X  X  can use a particular skill in multiple projects in Cov ( T ). In practice, however, this setting can lead to an expert being overworked, especially if he is one of the few members (or even the sole member) in the team who possesses a frequently required skill.
In order to avoid such shortcomings, we propose a vari-ant of ClusterHire that places an upper bound t ( X,s ) on the number of projects for which an expert X can uti-lize a skill s . By allowing for a different threshold for each expert-skill combination, this formulation is a natural fit for scenarios where certain skills are much easier to apply than others, especially for specific experts. For example, while it is very difficult for a software engineer to be the  X  X evel-opment leader X  for more than one project, the same person can use his  X  X oftware consulting X  expertise to assist in nu-merous ongoing efforts. In the same consulting setting, it is easier for a more experienced engineer to participate in more projects.
 Computing Cov t ( T ) : In practice, such a threshold corre-sponds to a different definition of the coverage of projects by teams. We refer to this alternative definition as t -coverage and we define it below. If use Cov t ( T ) to denote the set projects that are t -covered by team T , then this set has the following characteristic: such that for all s  X  S , The constraints encoded by the inequalities in Equation (5) are essentially the threshold constraints imposed by the ex-perts.

We observe that given a team T , computing Cov t ( T ) as described by Equations 4 and 5 is also an NP-hard problem. s and experts X , then the problem described by the above equation is identical to the SetPacking problem [9].
In this section, we describe our algorithms for the basic version of the ClusterHire problem. We then demonstrate how to adapt these algorithms to solve t-ClusterHire . The ExpertGreedy algorithm: The ExpertGreedy algo-rithm is a greedy algorithm on the space of experts. That is, it greedily picks experts  X  one at a time  X  so that the budget constraint is satisfied while at the same time the F objective is maximized. Specifically, the algorithm starts with an empty team and at iteration i , it forms T expert X , picked at iteration ( i + 1) needs to be among the qualified candidates Q , where: while X maximizes: Note that that the set of qualified candidates Q is different at every iteration. The set consists of all the remaining experts that can be added to the current solution without violating the budget constraint B . In case of a tie between candidate experts, the algorithm picks an expert at random.
 Algorithm 1 The ExpertGreedy algorithm.
 1: T =  X  , b = 0, Q = X 2: while b &lt; B and Q6 =  X  do 3: Q = { X | /  X  X  and C ( T  X  X  X } )  X  B } 5: T = T  X  X  X } 6: b = b + C ( X ) 7: return T The pseudocode of ExpertGreedy is shown in Algorithm 1. The worst-case running time of each iteration of Expert-Greedy is O ( nm` ). However, careful implementation and bookkeeping that takes into consideration the sparsity of the data allow for much better running times in practice. The ProjectGreedy algorithm: In contrast to Expert-Greedy , which picks experts greedily, ProjectGreedy oper-ates by greedily selecting the projects to be covered  X  one a a time  X  and then finding the best set of experts that can cover the selected project.
 Algorithm 2 The ProjectGreedy algorithm.
 1: T =  X  , b = 0 2: while b &lt; B and P 6 =  X  do 3: P : set of projects not covered by T 4: for P  X  X  do 5: X P : experts from X required to cover P 6: if C ( X P ) + b  X  B then 7: P = P \{ P } 9: T = T  X  X  P 10: b = b + C ( X P ) 11: return T
Assume that, at iteration i , ProjectGreedy has formed a team T i . Then, at iteration ( i + 1), the algorithm picks a project P that is not covered by the skills in team T i . The selection of P is done so that, if X P is the set of additional experts required to cover P , the ratio is maximized and the budget constraint, i.e., C ( T i  X  X  P B is satisfied. Similar to ExpertGreedy , any ties are broken arbitrarily.
 The pseudocode of ProjectGreedy is shown in Algorithm 2. We draw attention to line 5 of this pseudocode. This step finds a subset of experts, X P , who together with the current members of T can collectively cover the skills of project P . Clearly, the formation of X P needs to be budget-efficient. Thus, this step involves solving an instance of the weighted set-cover problem for the skills of P which are not currently covered by T . For this, we use the standard greedy approx-imation algorithm for weighted set cover. Thus, a different set-cover problem needs to be solved for each one of the candidate projects.

If I is the number of iterations of the outer while loop and T G the running time required for finding X P (i.e., the running time of the greedy algorithm for weighted-set cover on the space of experts), the worst-case running time of ProjectGreedy is O ( ImT G ). Next, we introduce a method that reduces the number of candidate projects m and con-sequently leads, in practice, to smaller running times. The CliqueGreedy algorithm: By greedily selecting a sin-gle project at every iteration, the ProjectGreedy algorithm is forced to repeatedly solve an instance of the weighted set cover problem. At the same time, by evaluating the profit-cost ratio of each project independently, the algorithm fails to identify sets of projects that require similar or even near-identical skillsets, which could lead to even higher ratios if selected together.
 Motivated by these observations, we design the Clique-Greedy algorithm. CliqueGreedy can be thought of as an extension of ProjectGreedy , which operates on groups of projects, rather than individual projects. Essentially, a group can be viewed as a larger project that requires the union of the skills required by the projects in the group. Next, we describe a 2-step method for grouping projects.

In the first step, we consider the grouping benefit for each pair of projects. Given two project P 1 and P 2 , we consider them compatible if the profit-to-cost ratio for both projects increases by at least a factor of (1 +  X  ) if they are merged. Formally, if for i = 1 , 2, and then P 1 and P 2 are in the same group if the following com-patibility condition holds:
In the second step, we compute the maximal cliques in the graph that has a node for every project and edges between every two compatible projects. The computed cliques then serve as the groups that are considered by CliqueGreedy .
Note that a clique is maximal if it is not included in any other possible clique. This allows to avoid trivial candidates and limit the number of cliques that need to be considered. In practice, even the enumeration of all maximal cliques is possible. In our experiments, we use the efficient implemen-tation of the algorithm proposed by Bron and Kerbosch [4] that is included in the NetworkX library [12].

As we saw in our experiments, the computational time of the grouping phase is dominated by the first step, since checking the compatibility condition for a pair of projects requires solving yet another set-cover problem. Therefore, checking the compatibility between all m 2 pairs of projects requires solving as many set-cover problems. In order to address this, we effectively eliminate a large number of such pair-wise checks by making the following observations:
Observation 1. Any two projects P 1 and P 2 are not com-patible if P 1  X  P 2 =  X  .

Observation 2. Any two projects P 1 and P 2 are not com-patible if Both these observations are direct consequences of the com-patibility condition and can be evaluated in constant time given that C ( P i ) is computed for every project  X  which is re-quired anyway even in the simple version of ProjectGreedy . In practice, we have observed that these two pruning mech-anisms are extremely effective, as they promptly dismiss many project pairs as incompatible.

Another factor that affects the running time of the group-ing phase is the value of the parameter  X  . This has a direct effect on the applicability of the second pruning criterion, as well as on the density (and thus the clique computation) of the resulting project graph. Larger values of  X  lead to sparser graphs and reduce the number of cliques.

In our experiments, we found that the value of  X  is dataset dependent, but tuning this parameter is easy if the dataset characteristics are studied appropriately.
 Algorithms for t-ClusterHire : All three algorithms we designed for t-ClusterHire , i.e., ExpertGreedy , Project-Greedy and CliqueGreedy can be modified to take into con-sideration the threshold on the number of times every expert can use each skill, as stipulated by the definition of the ClusterHire problem. In all cases, the greedy heuristic is modified to maximize the marginal gain at the profit level, while satisfying the threshold constraints. Considering the pseudocode for ExpertGreedy given by Algorithm 1, the only required modification is to alter line 4 so that it computes the t -coverage of the teams, and thus use Cov t instead of simple Cov . Finally, the ProjectGreedy for this version is similarly obtained by modifying line 8 of Algorithm 2 to compute the t -coverages instead of the simple coverages.
In our experiments, we used data collected from two large online labor markets: guru.com and freelancer.com . We refer to these datasets as Guru and Freelancer respectively. The business model for labor markets: Both websites follow the same business model: employer post a description of a project that needs to be completed, including the re-quired skills and the monetary reward that they are willing to pay. Experts with various skillsets and salary demands apply for each project, and are evaluated by the employer. Data analytics: From each website, we collect the follow-ing artifacts: (i) the set of skills and the salary demands (in dollars per hour) for each expert, and (ii) the set of required skills and the monetary reward (in dollars) for each posted project. For the Guru dataset, we collected data on 6 473 experts and 1 764 projects. For the Freelancer dataset, we collected data on 1 763 experts and 721 projects.
 Project analytics: Figures 1 and 2 provide some descrip-tive analytics on the projects from Freelancer and Guru datasets, respectively.

Figures 1(a) and 2(a) display the distribution of the size of the skillset required for the projects in Freelancer and Guru respectively. From Figure 2(a), we observe that while the majority of projects in Guru require up to 10 skills, larger projects of 30 skills or more are also posted. For Freelancer the distribution is different. This is due to the fact in freelancer.com employers are only allowed to spec-ify at most 5 skills per project; this fact clearly manifests itself in Figure 1(a).

Figures 1(b) and 2(b) show the distribution of project profits for Freelancer and Guru , respectively, under the F profit function (amount of dollars). Similarly, figures 1(c) and 2(c) show the distribution of project profits under the F c (expected amount of dollars, based on competition). The x -axis holds the profits, sorted from lowest to highest, and the y -axis the number of projects associated with each profit value. For both figures, both axes are in a logarithmic scale.
For the F d scheme, we observe only 9 distinct profit values for Guru (due to the quantization of profits made by the website), which also follow a distribution that resembles a power law (given the almost straight line and the log-log scale). On the other hand, for Freelancer , we observe a much higher variance in the distribution of profits.
As anticipated, the F c scheme introduces much larger vari-ance in the profit distribution for both datasets, with no clearly identifiable distribution shape. This is reasonable, distr. distr. since this function is based on the rarity of the required skills. which can vary a lot more across projects. Expert analytics: Figure 3 shows different analytics for the experts from Freelancer and Guru . Figures 3(a) and 3(b) show the distribution of the skillset size (i.e., number of skills) of the experts in each dataset: the x -axis corre-sponds to the size of skillset and the y -axis to the number of experts that have skillsets of that size. For Guru , Fig-ure 3(b), this distribution is a power-law distribution, with most users having less than 20 skills. On the other hand, the majority of experts on Freelancer have five skills, while the remaining skillsets are almost uniformly distributed over 1,2,3, and 4 skills. This difference is explained by the skill verification mechanism that is in place by freelancer.com where an expert can declare any number of skills, however on each expert X  X  only at most 6 most verified skills are dis-played; a skill of an expert gets a verification every time the expert participates in a project that utilizes this skill. Such a mechanism is absent from guru.com (at least at the time the data was collected), and thus the distribution of skillset sizes is different.

Finally, Figures 3(c) and 3(d) show the distribution of salaries for Freelancer and Guru experts respectively. The x -axis holds the salaries, sorted from lowest to highest, and the y -axis the number of experts with a given salary. Both figures display a  X  X ower law X -like distribution, with the ma-jority of experts asking for an hourly salary of at most 50. Visualization: In an attempt to gain a deeper understand-ing of the two datasets in the context of our problem, we display the similarity graphs for experts in Figures 4 and 6. Expert graph: In the graphs in Figure 4, nodes corre-spond to experts. There is an edge between two experts if their Jaccard similarity, computed on their skillsets, is higher than 0.7. For the graph we removed all nodes with (a) Expert skillset size distr. for Freelancer (c) Expert salary distr. for
Freelancer Figure 3: Expert analytics for Freelancer and Guru datasets. degree less than 2. 2 The figure demonstrates that the la-tent similarity structure among experts differs dramatically in the two datasets For Freelancer (Figure 4(a)) we observe a number of dense neighborhoods of different sizes, repre-senting clusters of similar experts. At the same time, the graph is well-connected, with several edges often bridging the observed neighborhoods. On the other hand, for Guru , (Figure 4(b)), we observe a single large dense component, as well as several smaller components that are not connected to each other. This difference stems from the different na-
Experimenting with lower and higher values of the thresh-olds had the expected results of producing larger and smaller cliques, respectively, for both datasets. Figure 4: Expert graphs: Similarity graphs for Guru and Freelancer experts. ture of the two websites; guru.com hosts a diverse spec-trum of experts with skillsets in different domains ranging from IT to legal services, financial consulting etc. On the other hand, the majority of the experts in freelancer.com are technology-oriented professionals who are more likely to have overlapping skillsets.
 Project graphs: In the graphs in Figure 6, nodes corre-spond to projects. The graphs were constructed as follows: there is an edge between two projects if the compatibility condition described in Equation (7) is satisfied. We visual-ize the compatibility graphs for both the F functions: F (the dollar amount attached to each project) and F expected dollar amount, given the competition a team has to face in the process of acquiring the project). Figures 6(a) and 6(b) display the graphs for both functions for the Free-lancer dataset for  X  = 0 . 2. Figures 6(c) and 6(d) display the same graphs for Guru and  X  = 0 . 7.

Comparing the graphs across datasets, we observe the same trends found in the expert graphs: Freelancer includes a number of distinct (but still connected) neighborhoods. On the other hand, the Guru graphs are dominated by 1-2 large components which correspond to experts from disci-plines with more dominant representation on the website.
The figures also reveal some interesting facts when con-sidered in the contexts of the two different profit functions. For Guru , introducing the competition factor in the prof-itability of a project leads to a sparser graph. As we saw in Figures 2(a) and 1(a), the average project on Guru requires a significantly higher number of skills than the average Free-lancer project. This makes the occurrence of rare skills more likely, which has a direct effect on the competitive-driven F function: as these competitive projects emerge, they are less likely to improve their profit-to-cost ratio by being grouped with other projects, leading to less edges in the graph. On the other hand, the effects of competition on the Freelancer project-graph are less prevalent, which is likely to be due to the lower diversity of skillsets required by each project (limited to at most 5).
In our experiments we study the relative performance of our algorithms for the Freelancer and the Guru datasets, which we extensively analyzed in the previous section. Figure 5: Performance of different algorithms for the Clus-terHire problem ( Freelancer dataset).
First, we focus on the evaluation of our algorithms for the ClusterHire problem, i.e., ExpertGreedy , Project-Greedy and CliqueGreedy . To do so, we report the overall profit achieved by each algorithm, for increasing values of the available budget, i.e., B  X  X  10 , 20 , 50 , 100 , 200 , 500 , 1000 } . The SmartRandom baseline: As an additional baseline, we also evaluate an iterative randomized algorithm, which we refer to as SmartRandom . SmartRandom is essentially a ran-dom version of ProjectGreedy and at each iteration it se-lects a random project, and then proceeds to hire the cheap-est set of experts who can fully cover this project. This set of experts is again identified by the greedy algorithm for set cover. In order to ensure that SmartRandom exhausts the available budget, only projects that can be covered using the current budget are considered on every iteration. The algorithm then terminates when no such projects exist. Al-though we use SmartRandom as our baseline, it really makes much more educated guesses than a naive random algorithm that forms random teams of experts.
 Results for the Freelancer dataset: Figures 5(a) and 5(b) show the performance of different algorithms on the Free-lancer dataset, in terms of the dollar ( F d ) and competition-based ( F c ) profit functions respectively. The y -axis shows the profit achieved by each algorithm, and the x -axis holds the budget that was used to hire experts.

Starting with the results for the dollar profit model, shown in Figure 5(a), we observe that, given a budget of 500$, ev-ery algorithm except SmartRandom achieves a profit around 32 K $, while SmartRandom reaches a value of 29 K $. The fact that all algorithms perform well implies that the dataset con-tains many profitable projects and many low-cost experts who can accomplish these projects. We also observe that SmartRandom performs similar to other algorithms under a limited budget (i.e., 0  X  B  X  200), which is reasonable since all algorithms are restricted to a limited set of projects that can actually be covered given the budget.

Under the competition-based profit model the results are different, as illustrated by Figure 5(b). What we observe in this case is that both ProjectGreedy and CliqueGreedy start to diverge and outperform other methods for budgets above 200$. To understand why ExpertGreedy is not as ef-ficient as before, one has to remember that many projects with high dollar profit values will no longer be profitable once the competition of the market is taken into considera-tion in the evaluation of the profit. The effect of this func-tion is that it changes the space of projects so that there are less projects that are both profitable and cost-effective. This gives ProjectGreedy and CliqueGreedy an advantage over ExpertGreedy . This is because the former evaluate and search for profitable projects (or groups of projects) as op-posed to ExpertGreedy that searches for individual workers who alone can only do projects with small profits.
 Results for the Guru dataset: To check the consistency of our results, we repeated the same experiment for both profit functions for the Guru dataset. The results are shown in Fig-ure 7. Similar to our observations for Freelancer dataset, we observe that, while all algorithms perform well in the dollar-based setting, ExpertGreedy falls short once the notion of competition is introduced. In fact, the gap between Expert-Greedy and the two algorithms based on project selection, i.e., ProjectGreedy and CliqueGreedy , is even greater than the one observed for Freelancer . A similar trend can be ob-served for SmartRandom : the algorithm is again consistently outperformed for both profit functions, with the difference in profit being significantly greater for this dataset, espe-cially under the competition-based profit scheme. As re-Figure 7: Performance of different algorithms for Cluster-Hire problem ( Guru dataset). vealed in the project analytics presented in Figures 1 and 2, the competitive-based profit function results in a much higher variance in the profits of the available projects. This has a negative effect on SmartRandom and ExpertGreedy , which do not take into consideration the profitability of the projects when they make their selections. This negative ef-fect is even stronger for the Guru dataset, since the competi-tive profit function dramatically increases the number of less profitable projects. In fact, as can be seen by Figures 2(b) and 2(c), the F c function introduces very large number of projects with a profit less than 100$, while no such projects existed for the simple dollar F d function.
 The characteristics of the solutions: In order to gain a deeper understanding of the results, we compute the average profit of the projects covered by the teams reported by the different algorithms for fixed budget B = 500$. The results are summarized in Table 1. The main message of this table is that the average profit of the projects that can be cov-ered by the solutions of ProjectGreedy and CliqueGreedy is higher than the corresponding average profit achieved by ExpertGreedy only in the case where profit is computed by the competition-based profit function. This observation is true for both our datasets. As we have already explained, the reason for that is that ProjectGreedy and CliqueGreedy are able to identify profitable projects and pick them even if these projects cannot be performed by a single worker. On the other hand, even in the presence of competition, Ex-pertGreedy cannot ignore projects that can be performed by single workers, which are also projects that typically require common skills.
 Table 1: Average profit of covered projects for B = 500$ Performance of CliqueGreedy : Figures 5 and 7 show that the CliqueGreedy algorithm performs only slightly better in terms of profit, when compared to ProjectGreedy . There-fore, a natural question to ask is whether there are any bene-fits that this algorithm has to offer. Our answer to this ques-tion is the following: although CliqueGreedy offers smalls gains in terms of profit, there are datasets for which it offers significant computational speedups. We provide evidence for this statement in Table 2.

The table reports the number of candidates that need to be evaluated by CliqueGreedy and ProjectGreedy for both datasets. The number of candidates per iteration that an algorithm has to evaluate is an important measure of its running time. This is because for every candidate project (or group of projects) the algorithm picks, it needs to solve a set cover problem for this candidate.

The results in Table 2 show that CliqueGreedy consis-tently evaluates less candidates during its computation, es-pecially for the Freelancer dataset. This essentially means that for this dataset about 1/3 of the candidates are elimi-nated and thus 1/3 less set cover computations need to be made by CliqueGreedy . Therefore, from the computational point of view for the Freelancer CliqueGreedy is beneficial since it offers a significant speedup while giving solutions with (approximately) the same profit. On the other hand, for the Guru dataset CliqueGreedy does not appear to of-fer significant computational savings. All these results were computed for  X  = 0 . 2 for Freelancer and  X  = 0 . 7 for Guru .
A visual analysis of the available projects in a dataset, such as the one we presented in Figure 6, can be used prior to running the algorithms, to evaluate if the underlying graph structures justifies the use of CliqueGreedy . For example, if multiple dense neighborhoods (which are likely to include cliques) can be identified, then the algorithm can indeed lead to significant computational savings. This is indeed the case for the Freelancer dataset and thus the savings.
In this section, we evaluate the performance of for our methods for the t-ClusterHire problem. For this we use the same experimental setup and evaluation methodology as in the previous section. For t-ClusterHire , we need to set the value of the threshold on the number of times that a user is willing to utilize each one of his skills. We set this value to t = 3 for all users and all skills. However, our experi-ments suggest that despite the fact that the actual profits change for different thresholds, the relative performance of algorithms is independent of this threshold.

Figure 8 shows the profit achieved by the different algo-rithms for both datasets and for both our profit models, i.e., dollar profit and competition-based profit. Most of the observations we made in the previous section are true here as well. More specifically, Figures 8(a) and 8(c) show how different algorithms perform under the dollar-based profit model for Freelancer and Guru datasets respectively. Simi-lar to our previous results for ClusterHire , we can see that all algorithms perform significantly better than SmartRan-dom . However, unlike our previous results, we can see that the performance of ExpertGreedy is significantly lower than the performance of both ProjectGreedy and CliqueGreedy algorithms. This is due to the utilization constraint which limits the number of cost-effective projects. That is, a set of projects with many overlapping skills are profitable in the ClusterHire problem since few cheap experts can cover all of the at once, while in the ClusterHire the profit one can get from this overlapping projects is bounded due to the utility constraint. Therefore, ExpertGreedy , that essentially prefers cheap experts looses its power because these experts cannot be used over and over again for multiple projects.
Figures 8(b) and 8(d) show how different algorithms per-form under the competition-based profit model. As dis-cussed in the previous section, adjusting the profits based on the competition in the market leads to significantly smaller number of cost-effective jobs. This gives both Project-Greedy and CliqueGreedy an advantage as they effectively search for good projects. On the other hand, ExpertGreedy does not perform well because due to its myopic nature se-lects cheap experts that can only do  X  X rivial X  projects.
Overall, for the t-ClusterHire problem ProjectGreedy and CliqueGreedy are consistently and significantly better than ExpertGreedy . Despite the computational speedups achieved by CliqueGreedy for this problem, the profit it ob-tained in this case is slightly less than the profit achieved by ProjectGreedy . The reason for the slight degradation of the profit achieved by CliqueGreedy can be explained as fol-lows: once projects are grouped into clusters, they need to be picked together. However, given the utilization constraints, the set of workers that can satisfy a group of projects may end up being expensive and hence less profitable. In this case, one has to find the balance between computational ef-ficiency and profit that one wants to achieve.
In this paper, we proposed formalizations and algorithmic solutions for the ClusterHire problem, where the goal is to hire a profit-maximizing team of experts with the ability to complete multiple projects, given a fixed budget. This problem repeatedly emerges in organizational settings, and it has become prevalent due to the establishment of the col-laboration paradigm in online labor markets. We provided a detailed analysis of the computational complexity and hard-ness of approximation of the problem, and presented algo-rithms that take into consideration the unique nature of ex-pertise data. Our methodology was evaluated on data from two of large players in the domain of online labor. This research was supported by: NSF CAREER #1253393, NSF grants: CNS #1017529, III #1218437, IIS #1320542 and gifts from Microsoft and Hariri Institute of Computing. [1] A. An, M. Kargar, and M. ZiHayat. Finding affordable [2] A. Anagnostopoulos, L. Becchetti, C. Castillo, [3] A. Anagnostopoulos, L. Becchetti, C. Castillo, [4] C. Bron and J. Kerbosch. Algorithm 457: Finding all [5] S.-J. Chen and L. Lin. Modeling team member [6] C. Dorn and S. Dustdar. Composing near-optimal [7] E. L. Fitzpatrick and R. G. Askin. Forming effective [8] A. Gajewar and A. D. Sarma. Multi-skill collaborative [9] M. Garey and D. Johnson. Computers and [10] A. Gionis, T. Lappas, and E. Terzi. Estimating entity [11] R. Greenwald. Freelancers find it pays to team up. [12] A. A. Hagberg, D. A. Schult, and P. J. Swart. [13] M. Kargar and A. An. Discovering top-k teams of [14] T. Lappas, K. Liu, and E. Terzi. Finding a team of [15] C.-T. Li and M.-K. Shan. Team formation for [16] S. Liemhetcharat and M. Veloso. Forming an effective [17] S. Liemhetcharat and M. Veloso. Weighted synergy [18] A. Majumder, S. Datta, and K. V. M. Naidu.
 [19] M. Sozio and A. Gionis. The community-search
