 From business people to the everyday person, e-mail plays an increasingly central role in a modern lifestyle. With this shift, e-mail users desire im-proved tools to help process, search, and organize the information present in their ever-expanding in-boxes. A system that ranks e-mails according to the likelihood of containing  X  X o-do X  or action-items can alleviate a user X  X  time burden and is the subject of ongoing research throughout the literature.

In particular, an e-mail user may not always pro-cess all e-mails, but even when one does, some emails are likely to be of greater response urgency than others. These messages often contain action-items. Thus, while importance and urgency are not equal to action-item content, an effective action-item detection system can form one prominent subcom-ponent in a larger prioritization system.

Action-item detection differs from standard text classification in two important ways. First, the user is interested both in detecting whether an email contains action-items and in locating exactly where these action-item requests are contained within the email body. Second, action-item detection attempts to recover the sender X  X  intent  X  whether she means to elicit response or action on the part of the receiver.
In this paper, we focus on the primary problem of presenting e-mails in a ranked order according to their likelihood of containing an action-item. Since action-items typically consist of a short text span  X  a phrase, sentence, or small passage  X  supervised input to a learning system can either come at the document-level where an e-mail is labeled yes/no as to whether it contains an action-item or at the sentence-level where each span that is an action-item is explicitly identified. Then, a corresponding document-level classifier or aggregated predictions from a sentence-level classifier can be used to esti-mate the overall likelihood for the e-mail.

Rather than commit to either view, we use a com-bination technique to capture the information each viewpoint has to offer on the current example. The STRIVE approach (Bennett et al. , 2005) has been shown to provide robust combinations of heteroge-neous models for standard topic classification by capturing areas of high and low reliability via the use of reliability indicators.

However, using STRIVE in order to produce im-proved rankings has not been previously studied. Furthermore, while they introduce some reliabil-ity indicators that are general for text classification problems as well as ones specifically tied to na  X   X ve Bayes models, they do not address other classifica-tion models. We introduce a series of reliability in-dicators connected to areas of high/low reliability in k
NN, SVMs, and decision trees to allow the combi-nation model to include such factors as the sparse-ness of training example neighbors around the cur-rent example being classified. In addition, we pro-vide a more formal motivation for the role these vari-ables play in the resulting metaclassification model.
Empirical evidence demonstrates that the result-ing approach yields a context-sensitive combination model that improves the quality of rankings gener-ated as well as reducing the variance of the ranking quality across training splits. In contrast to related combination work, we focus on improving rankings through the use of a metaclass-ification framework. In addition, rather than sim-ply focusing on combining models from different classification algorithms, we also examine combin-ing models that have different views , in that both the qualitative nature of the labeled data and the applica-tion of the learned base models differ. Furthermore, we improve upon work on context-sensitive com-bination by introducing reliability indicators which model the sensitivity of a classifier X  X  output around the current prediction point. Finally, we focus on the application of these methods to action-item data  X  a growing area of interest which has been demon-strated to behave differently than more standard text classification problems ( e.g. topic ) in the literature (Bennett and Carbonell, 2005). 2.1 Action-Item Detection There are three basic problems for action-item de-tection. (1) Document detection : Classify an e-mail as to whether or not it contains an action-item. (2) Document ranking : Rank the e-mails such that all e-mail containing action-items occur as high as pos-sible in the ranking. (3) Sentence detection : Classify each sentence in an e-mail as to whether or not it is an action-item.
 Here we focus on the document ranking problem. Improving the overall ranking not only helps users find e-mails with action-items quicker (Bennett and Carbonell, 2005) but can decrease response times and help ensure that key e-mails are not overlooked.
Since a typical user will eventually process all received mail, we assume that producing a quality ranking will more directly measure the impact on the user than accuracy or F1. Therefore, we focus on ROC curves and area under the curve (AUC) since both reflect the quality of the ranking produced. 2.2 Combining Classifiers with Metaclassifiers One of the most common approaches to classi-fier combination is stacking (Wolpert, 1992). In this approach, a metaclassifier observes a past his-tory of classifier predictions to learn how to weight the classifiers according to their demonstrated ac-curacies and interactions. To build the history, cross-validation over the training set is used to ob-tain predictions from each base classifier. Next, a metalevel representation of the training set is con-structed where each example consists of the class label and the predictions of the base classifiers. Fi-nally, a metaclassifier is trained on the metalevel rep-resentation to learn a model of how to combine the base classifiers.

However, it might be useful to augment the his-tory with information other than the predicted prob-abilities. For example, during peer review, reviewers typically provide both a 1 -5 acceptance rating and a 1 -5 confidence. The first of these is related to an es-timate of class membership, P ( X  X ccept 00 | paper) , but the second is closer to a measure of expertise or a self-assessment of the reviewer X  X  reliability on an example-by-example basis.

Automatically deriving such self-assessments for classification algorithms is non-trivial. The St acked R eliability I ndicator V ariable E nsemble framework, or STRIVE , demonstrates how to extend stacking by incorporating such self-assessments as a layer of re-liability indicators and introduces a candidate set of functions (Bennett et al. , 2005).
 The STRIVE architecture is depicted in Figure 2. From left to right: (1) a bag-of-words representation of the document is extracted and used by the base classifiers to predict class probabilities; (2) reliabil-ity indicator functions use the predicted probabili-ties and the features of the document to characterize whether this document falls within the  X  X xpertise X  of the classifiers; (3) a metalevel classifier uses the base classifier predictions and the reliability indica-tors to make a more reliable combined prediction.
From the perspective of improving action-item rankings, we are interested in whether stacking or striving can improve the quality of rankings. How-ever, we hypothesize that striving will perform better since it can learn a model that varies the combination rule based on the current example and thus, better capture when a particular classifier at the document-level or sentence-level, bag-of-words or n -gram rep-resentation, etc. will produce a reliable prediction. 2.3 Formally Motivating Reliability Indicators While STRIVE has been shown to provide robust combination for topic classification, a formal moti-vation is lacking for the type of reliability indicators that are the most useful in classifier combination.
Assume we restrict our choice of metaclassifier to a linear model. One natural choice is to rank the e-mails according to the estimated posterior proba-bility,  X  P (class = action item | x ) , but in a linear combination framework it is actually more conve-nient to work with the estimated log-odds or logit transform which is monotone in the posterior,  X   X  =
Now, consider applying a metaclassifier to a sin-gle base classifier. Given only a classifier X  X  probabil-ity estimates, a metaclassifier cannot improve on the estimates if they are well-calibrated (DeGroot and Fienberg, 1986). Thus a metaclassifier applied to a single base classifier corresponds to recalibration (Kahn, 2004).

Assume each of the n base models gives an un-calibrated log-odds estimate  X   X  bination model would have the form  X   X   X  ( x ) = W 0 ( x )+ P ple dependent weight functions that the combination model learns. The obvious implication is that our reliability indicators can be informed by the optimal values for the weighting functions.

We can determine the optimal weights in a sim-plified case with a single base classifier by assuming we are given  X  X rue X  log-odds values,  X  , and a family of distributions  X  encodes what is local to x by giving the probability of drawing a point z near to x . We use  X  instead of  X  example dependent nature of the weights, we can drop x from the weight functions. To find weights that minimize the squared difference between the true log-odds and the estimated log-odds in the  X  vicinity of x , we can solve a standard regression problem, argmin Under the assumption VAR where  X  and  X  are the stdev and correlation co-efficient under  X  , respectively. The first parame-ter is a measure of calibration that addresses the question,  X  X ow far off on average is the estimated log-odds from the true log-odds in the local con-text? X  The second is a measure of correlation,  X  X ow closely does the estimated log-odds vary with the true log-odds? X  Note that the second parameter de-pends on the local sensitivity of the base classifier, log-odds, we can introduce local density models to estimate the local sensitivity of the model.
In particular, we introduce a series of relia-bility indicators by first defining a  X  distribu-tion and either computing VAR the closely related terms VAR E an example as features for a linear metaclassifier. Thus we use a context-dependent bias term but leave the more general model for future work. 2.4 Model-Based Reliability Indicators As discussed in Section 2.3, we wish to define local distributions in order to compute the local sensitivity and similar terms for the base classification models. To do so, we define local distributions that have the same  X  X lavor X  as the base classification model.
First, consider the k NN classifier. Since we are concerned with how the decision function would change as we move locally around the current pre-diction point, it is natural to consider a set of shifts defined by the k neighbors. In particular, let d note the document that has been shifted by a factor  X  toward the i th neighbor, i.e. d We use the largest  X  to the new point is the original document, i.e. the boundary of the Voronoi cell (see Figure 3). Clearly,  X  using a simple bisection algorithm. Now, let  X  be a uniform point-mass distribution over the shifted points and  X   X 
Given this definition of  X  , it is now straight-forward to compute the k NN based reliabil-ity indicators: E
Similarly, we define variables for the SVM class-ifier by considering a document X  X  locality in terms of nearby support vectors from the set of support vectors, V . To determine  X  of the closest support vector in V to d . Let be half the distance to the nearest point in V , i.e. = min v  X  X  k v  X  d k . Then  X  i = shift vectors are all rescaled to have the same length. Now, we must define a probability for the shift. We use a simple exponential based on and the rela-tive distance from the document to the support vec-tor defining this shift. Let d exp(  X  X  v i  X  d k + 2 ) and P V i =1 P  X  ( d i ) = 1 . 2 Given this definition of  X  , we compute the SVM based reliability indicators: E  X   X 
Space prevents us from presenting all the deriva-tions here. However, we also define decision-tree based variables where the locality distribution gives high probability to documents that would land in nearby leaves. For a multinomial na  X   X ve Bayes model (NB), we define a distribution of documents iden-tical to the prediction document except having an occurrence of a single feature deleted. For the multivariate Bernoulli na  X   X ve Bayes (MBNB) model that models feature presence/absence, we use a distribution over all documents that has one pres-ence/absence bit flipped from the prediction docu-ment. It is interesting to note that the variables from the na  X   X ve Bayes models can be shown to be equiva-lent to variables introduced by Bennett et al. (2005)  X  although those were derived in a different fashion by analyzing the weight a single feature carries with respect to the overall prediction.

Furthermore, from this starting point, we go on to define similar variables of possible interest. Includ-ing the two for each model described here, we define 10 k NN variables, 5 SVM variables, 2 decision-tree variables, 6 NB model based variables, and 6 MBNB variables. We describe these variables as well as im-plementation details and computational complexity results in (Bennett, 2006). 3.1 Data Our corpus consists of e-mails obtained from vol-unteers at an educational institution and covers subjects such as: organizing a research work-shop, arranging for job-candidate interviews, pub-lishing proceedings, and talk announcements. Af-ter eliminating duplicate e-mails, the corpus con-tains 744 messages with a total of 6301 automat-ically segmented sentences. A human panel la-beled each phrase or sentence that contained an explicit request for information or action. 416 e-mails have no action-items and 328 e-mails con-tain action-items. Additional information such as annotator agreement, distribution of message length, etc. can be found in (Bennett and Car-bonell, 2005). An anonymized corpus is available at 3.2 Feature Representation We use two types of feature representation: a bag-of-words representation which uses all unigram to-kens as the feature pool; and a bag-of-n -grams where n includes all n -grams where n  X  4 . For both representations at both the document-level and sentence-level, we used only the top 300 features by the chi-squared statistic. 3.3 Document-Level Classifiers k NN We used a s -cut variant of k NN common in text classification (Yang, 1999) and a tfidf-weighting of the terms with a distance-weighted vote of the neighbors to compute the output. k was set to be 2( d log 2 N e + 1) where N is the number of training odds estimate of being an action-item is:  X   X  SVM We used a linear SVM as implemented in the SVM light package v6.01 (Joachims, 1999) with a tfidf feature representation and L2-norm. All de-fault settings were used. SVM X  X  margin score, P  X  i y i K ( x i , x j ) , has been shown to empirically behave like an uncalibrated log-odds estimate (Platt, 1999).
 Decision Trees For the decision-tree implementation, we used the WinMine toolkit and refer to this as Dnet below (Mi-crosoft Corporation, 2001). Dnet builds decision trees using a Bayesian machine learning algorithm (Chickering et al. , 1997; Heckerman et al. , 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Na  X   X ve Bayes We use a multinomial na  X   X ve Bayes (NB) and a mul-tivariate Bernoulli na  X   X ve Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifi-ers, we smoothed word and class probabilities us-ing a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. Since these are probabilistic, they issue log-odds estimates directly. 3.4 Sentence-Level Classifiers Each e-mail is automatically segmented into sen-tences using RASP (Carroll, 2002). Since the cor-pus has fine grained labels, we can train classifiers to classify a sentence. Each classifier in Section 3.3 is also used to learn a sentence classifier. However, we then must make a document-level prediction.
In order to produce a ranking score, the con-fidence that the document contains an action-item is:  X   X  ( d ) = where s is a sentence in document d ,  X  is the class-ifier X  X  1/0 prediction,  X   X  is the score the classifier as-signs as its confidence that  X  ( s ) = 1 , and n ( d ) is the greater of 1 and the number of (unigram) to-kens in the document. In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold. When no sentence is predicted pos-itive, the document score is the maximum sentence score normalized by length. The length normaliza-tion compensates for the fact that we are more likely to emit a false positive the longer a document is. 3.5 Stacking To examine the hypothesis that the reliability in-dicators provide utility beyond the information present in the output of the 20 base classifiers ( 2 representations  X  2 views  X  5 classifiers ), we con-struct a linear stacking model which uses only the base classifier outputs and no reliability indicators as a baseline. For the implementation, we use SVM light with default settings. The inputs to this classifier are normalized to have zero mean and a scaled variance. 3.6 Striving Since we are constructing base classifiers for both the bag-of-words and bag-of-n -grams representa-tions, this gives 58 reliability indicators from com-puting the variables in Section 2.4 for the document-level classifiers ( 58 = 2  X  [6 + 6 + 10 + 5 + 2] ).
Although the model-based indicators are defined for each sentence prediction, to use them at the document-level we must somehow combine the re-liability indicators over each sentence. The simplest method is to average each classifier-based indicator across the sentences in the document. We do so and thus obtain another 58 reliability indicators.
Furthermore, our model might benefit from some of the structure a sentence-level classifier offers when combining document predictions. Analogous to the sensitivity of each base model, we can con-sider such indicators as the mean and standard de-viation of the classifier confidences across the sen-tences within a document. For each sentence-level base classifier, these become two more indicators which we can benefit from when combining docu-ment predictions. This introduces 20 more variables ( 20 = 2 representations  X  2  X  5 classifiers ).
Finally, we include the 2 basic voting statistic reliability-indicators ( PercentPredictingPositive and PercentAgreeWBest ) that Bennett et al. (2005) found useful for topic classification. This yields a total of 138 reliability-indicators ( 138 = 58 + 20 + 58 + 2 ). With the 20 classifier outputs, there are a total of 158 input features for striving to handle.

As with stacking, we use SVM light with default settings and normalize the inputs to this classifier to have zero mean and a scaled variance. 3.7 Performance Measures We wish to improve the rankings of the e-mails in the inbox such that action-item e-mails occur higher in the inbox. Therefore, we use the area under the curve (AUC) of an ROC curve as a measure of rank-ing performance. AUC is a measure of overall model and ranking quality that has gained wider adoption recently and is equivalent to the Mann-Whitney-Wilcoxon sum of ranks test (Hanley and McNeil, 1982). To put improvement in perspective, we can write our relative reduction in residual area (RRA) best AUC performer (bRRA), and relative to perfect dynamic selection performance, (dRRA), which as-sumes we could accurately dynamically choose the best classifier per cross-validation run.

The F1 measure is the harmonic mean of preci-sion and recall and is common throughout text class-ification (Yang and Liu, 1999). Although we are not concerned with F1 performance here, some users of the system might be interested in improving rank-ing while having negligible negative effect on F1. Therefore, we examine F1 to ensure that an improve-ment in ranking will not come at the cost of a statis-tically significant decrease in F1. 3.8 Experimental Methodology To evaluate performance of the combination sys-tems, we perform 10 -fold cross-validation and com-pute the average performance. For significance tests, we use a two-tailed t-test (Yang and Liu, 1999) to compare the values obtained during each cross-validation fold with a p -value of 0 . 05 .

We examine two hypotheses: Stacking will out-perform all of the base classifiers; Striving will out-perform all the base classifiers and stacking. 3.9 Results &amp; Discussion Table 1 presents the summary of results. The best performer in each column is in bold. If a combi-nation method statistically significantly outperforms all base classifiers, it is underlined. Table 1: Base classifier and combiner performance
Now, we turn to the issue of whether combination improves the ranking of the documents. Examining the results in Table 1, we see that STRIVE statistically significantly beats every other classifier according to AUC. Stacking outperforms the base classifiers with respect to AUC but not statistically significantly.
Examining F1, we see that neither combination method outperforms the best base classifier, NB (sent,ngram) . If we examine the hypothesis of whether this base classifier significantly outperforms either combination method, the hypothesis is re-jected. Thus, STRIVE improves the overall ranking with a negligible effect on F1.

Finally, we compare the ROC curves of striving, stacking, and two of the most competitive base class-ifiers in Figure 4. We see that striving loses by a slight amount to stacking early in the curve but still beats the base classifiers. Later in the curve, it dom-inates all the classifiers. If we examine the curves using error bars, we see that the variance of STRIVE drops faster than the other classifiers as we move fur-ther along the x -axis. Thus, STRIVE  X  X  ranking quality varies less with changes to the training set. Several researchers have considered text classifi-cation tasks similar to action-item detection. Co-hen et al. (2004) describe an ontology of  X  X peech acts X , such as  X  X ropose a Meeting X , and attempt to predict when an e-mail contains one of these speech acts. Corston-Oliver et al. (2004) con-sider detecting items in e-mail to  X  X ut on a To-Do List X  using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demon-strated that sentence-level classifiers typically out-perform document-level classifiers on this problem and examined the underlying reasons why this was the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the system.

In terms of classifier combination, a wide variety of work has been done in the arena. The STRIVE metaclassification approach (Bennett et al. , 2005) extended Wolpert X  X  stacking framework (Wolpert, 1992) to use reliability indicators. In recent work, Lee et al. (2006) derive variance estimates for na  X   X ve Bayes and tree-augmented na  X   X ve Bayes and use them in the combination model. Our work comple-ments theirs by laying groundwork for how to com-pute variance estimates for models such as k NN that have no obvious probabilistic component. While there are many interesting directions for fu-ture work, the most interesting is to directly integrate the sensitivity and calibration quantities derived into the more general model discussed in Section 2.3.
In this paper, we took an existing approach to context-dependent combination, STRIVE , that used many ad hoc reliability indicators and derived a formal motivation for classifier model-based local sensitivity indicators. These new reliability indi-cators are efficiently computable, and the resulting combination outperformed a vast array of alterna-tive base classifiers for ranking in an action-item de-tection task. Furthermore, the combination results yielded a more robust performance relative to varia-tion in the training sets. Finally, we demonstrated that the STRIVE method could be successfully ap-plied to ranking.

