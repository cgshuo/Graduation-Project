 @cs.ust.hk
In this paper we show that evaluating machine trans-lation by assessing the translation accuracy of each argu-ment in the semantic role framework correlates with hu-man judgment on translation adequacy as well as HTER, at a significantly lower labor cost. The correlation of this new metric, MEANT, with human judgment is far supe-rior to BLEU and other automatic n-gram based evalua-tion metrics.

We argue that BLEU (Papineni et al. , 2002) and other automaticn-grambasedMTevaluationmetricsdonotad-equately capture the similarity in meaning between the machinetranslationandthereferencetranslation X  X hich, ultimately, is essential for MT output to be useful. N-gram based metrics assume that  X  X ood X  translations tend to share the same lexical choices as the reference trans-lations. While BLEU score performs well in captur-ing the translation fluency, Callison-Burch et al. (2006) and Koehn and Monz (2006) report cases where BLEU strongly disagree with human judgment on translation quality. The underlying reason is that lexical similarity does not adequately reflect the similarity in meaning. As MT systems improve, the shortcomings of the n-gram based evaluation metrics are becoming more apparent. State-of-the-art MT systems are often able to output flu-ent translations that are nearly grammatical and contain roughly the correct words, but still fail to express mean-ing that is close to the input.

Atthesametime,althoughHTER(Snover et al. ,2006) is more adequacy-oriented, it is only employed in very large scale MT system evaluation instead of day-to-day research activities. The underlying reason is that it re-quires rigorously trained human experts to make difficult combinatorial decisions on the minimal number of edits soas tomake theMT outputconvey thesame meaningas thereferencetranslation X  X highlylabor-intensive,costly process that bottlenecks the evaluation cycle.
Instead, with MEANT, we adopt at the outset the principle that a good translation is one that is useful , in the sense that human readers may successfully un-derstand at least the basic event structure X  X  who did what to whom , when , where and why  X  (Pradhan et al. , 2004) X  X epresentingthecentralmeaningofthesourceut-terances. Itistruethatlimitedtasksmightexistforwhich inadequate translations are still useful. But for meaning-ful tasks, generally speaking, for a translation to be use-ful, atleastthebasiceventstructuremustbecorrectlyun-derstood. Therefore, our objective is to evaluate trans-lation utility : from a user X  X  point of view, how well is the most essential semantic information being captured by machine translation systems? In this paper, we detail the methodology that underlies MEANT, which extends and implements preliminary di-rectionsproposedin(LoandWu,2010a)and(LoandWu, 2010b). We present the results of evaluating translation utility by measuring the accuracy within a semantic role labeling(SRL)framework. Weshowempiricallythatour proposed SRL based evaluation metric, which uses un-trained monolingual humans to annotate semantic frames inMToutput,correlateswithhumanadequacyjudgments as well as HTER, and far better than BLEU and other commonly used metrics. Finally, we show that replacing the human semantic role labelers with an automatic shal-low semantic parser in our proposed metric yields an ap-proximation that is about 80% as closely correlated with human judgment as HTER, at an even lower cost X  X nd is still far better correlated than n-gram based evaluation metrics. Lexical similarity based metrics BLEU (Papineni et al. , 2002) is the most widely used MT evaluation met-ric despite the fact that a number of large scale meta-evaluations (Callison-Burch et al. , 2006; Koehn and Monz, 2006) report cases where it strongly disagree with human judgment on translation accuracy. Other lexi-cal similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie,2005),PER(Tillmann etal. ,1997),CDER(Leusch et al. , 2006) and WER (Nie  X  en et al. , 2000), also per-form well in capturing translation fluency, but share the sameproblemthatalthoughevaluationwiththesemetrics can be done very quickly at low cost, their underlying as-sumption X  X hata X  X ood X  X ranslationisonethatsharesthe same lexical choices as the reference translation X  X s not justified semantically. Lexical similarity does not ade-quatelyreflectsimilarityinmeaning. State-of-the-artMT systems are often able to output translations containing roughly the correct words, yet expressing meaning that is not close to that of the input.

Wearguethatatranslationmetricthatreflectsmeaning similarity is better based on similarity in semantic struc-ture, rather than simply flat lexical similarity. HTER (non-automatic) Despite the fact that Human-targeted Translation Edit Rate (HTER) as proposed by Snover et al. (2006) shows a high correlation with human judgment on translation adequacy, it is not widely used in day-to-day machine translation evaluation because of its high labor cost. HTER not only requires human experts to understand the meaning expressed in both the refer-ence translation and the machine translation, but also re-quires them to propose the minimum number of edits to the MT output such that the post-edited MT output con-veys the same meaning as the reference translation. Re-quiring such heavy manual decision making greatly in-creases the cost of evaluation, bottlenecking the evalua-tion cycle.

To reduce the cost of evaluation, we aim to reduce any human decisions in the evaluation cycle to be as simple as possible, such that even untrained humans can quickly complete the evaluation. The human decisions should alsobedefinedina waythatcan becloselyapproximated by automatic methods, so that similar objective functions might potentially be used for tuning in MT system devel-opment cycles.
 Task based metrics (non-automatic) Voss and Tate (2006) proposed a task-based approach to MT evaluation that is in some ways similar in spirit to ours, but rather than evaluating how well people understand the mean-ing as a whole conveyed by a sentence translation, they measuredtherecallwithwhichhumanscanextract one of the who , when , or where elements from MT output X  X nd without attaching them to any predicate or frame. A largenumberofhumansubjectswereinstructedtoextract only one particular type of wh -item from each sentence. They evaluated only whether the role fillers were cor-rectlyidentified,withoutcheckingwhethertheroleswere appropriately attached to the correct predicate. Also, the actor, experiencer, and patient were all conflated into the undistinguished who role, while other crucial elements, like the action, purpose, manner, were ignored.
Instead, we argue, evaluating meaning similarity should be done by evaluating the semantic structure as a whole: (a) all core semantic roles should be checked, and (b) not only should we evaluate the presence of se-mantic role fillers in isolation, but also their relations to the frames X  predicates.
 Syntax based metrics Unlike Voss and Tate, Liu and Gildea (2005) proposed a structural approach, but it was based on syntactic rather than semantic structure, and fo-cused on checking the correctness of the role structure without checking the correctness of the role fillers . Their subtreemetric(STM)andheadwordchainmetric(HWC) addressthefailureofBLEUtoevaluatetranslation gram-maticality ; however, the problem remains that a gram-matical translation can achieve a high syntax-based score evenifcontainsmeaningerrorsarisingfromconfusionof semantic roles.

STM was the first proposed metric to incorporate syn-tacticfeaturesinMTevaluation,andSTMunderliesmost other recently proposed syntactic MT evaluation met-rics, for example the evaluation metric based on lexical-functional grammar of Owczarzak et al. (2008). STM is a precision-based metric that measures what fraction of subtree structures are shared between the parse trees of 221 machine translations and reference translations (averag-ing over subtrees up to some depth threshold). Unlike VossandTate, however,STMdoesnotcheckwhetherthe role fillers are correctly translated.

HWC is similar, but is based on dependency trees con-taining lexical as well as syntactic information. HWC measures what fraction of headword chains (a sequence of words corresponding to a path in the dependency tree) also appear in the reference dependency tree. This can be seen as a similarity measure on n-grams of dependency chains. Note that the HWC X  X  notion of lexical similarity still requires exact word match.

Although STM-like syntax-based metrics are an im-provement over flat lexical similarity metrics like BLEU, they are still more fluency-oriented than adequacy-oriented. Similarity of syntactic rather than semantic structurestillinadequatelyreflectsmeaningpreservation. Moreover, properly measuring translation utility requires verifying whether role fillers have been correctly trans-lated X  X erifying only the abstract structures fails to pe-nalize when role fillers are confused.
 Semantic roles as features in aggregate metrics Gim  X  enez and M ` arquez (2007, 2008) introduced ULC, an automatic MT evaluation metric that aggregates many typesoffeatures,includingseveralshallowsemanticsim-ilarity features: semantic role overlapping, semantic role matching,andsemanticstructureoverlapping. UnlikeLiu andGildea(2007)whousediscriminativetrainingtotune the weight on each feature, ULC uses uniform weights. Although the metric shows an improved correlation with humanjudgmentoftranslationquality(Callison-Burch et al. , 2007; Gim  X  enez and M ` arquez, 2007; Callison-Burch et al. , 2008; Gim  X  enez and M ` arquez, 2008), it is not com-monlyusedinlarge-scaleMTevaluationcampaigns, per-haps due to its high time cost and/or the difficulty of in-terpreting its score because of its highly complex combi-nation of many heterogenous types of features.
Specifically,notethatthefeaturebasedrepresentations of semantic roles used in these aggregate metrics do not actually capture the structural predicate-argument rela-tions.  X  X emantic structure overlapping X  can be seen as the shallow semantic version of STM: it only measures the similarity of the tree structure of the semantic roles, without considering the lexical realization.  X  X emantic role overlapping X  calculates the degree of lexical overlap between semantic roles of the same type in the machine translationanditsreferencetranslation,usingsimplebag-of-wordscounting;thisisthenaggregatedintoanaverage over all semantic role types.  X  X emantic role matching X  is just like  X  X emantic role overlapping X , except that bag-of-words degree of similarity is replaced (rather harshly) by a boolean indicating whether the role fillers are an ex-act string match. It is important to note that  X  X emantic role overlapping X  and  X  X emantic role matching X  both use flatfeaturebasedrepresentationswhichdonotcapturethe structural relations in semantic frames, i.e., the predicate-argument relations.

Like system combination approaches, ULC is a vastly more complex aggregate metric compared to widely used metrics like BLEU or STM. We believe it is important to retain a focus on developing simpler metrics which not only correlate well with human adequacy judgments, but nevertheless still directly provide representational transparency via simple, clear, and transparent scoring schemes that are (a) easily human readable to support er-ror analysis, and (b) potentially directly usable for auto-matic credit/blame assignment in tuning tree-structured SMT systems. We also believe that to provide a foun-dation for better design of efficient automated metrics, making use of humans for annotating semantic roles and judging the role translation accuracy in MT output is an essentialstepthatshouldnotbebypassed,inordertoade-quately understand the upper bounds of such techniques.
We agree with Przybocki et al. (2010), who observe in the NIST MetricsMaTr 2008 report that  X  X uman [ade-quacy] assessments only pertain to the translations evalu-ated, and are of no use even to updated translations from the same systems X . Instead, we aim for MT evaluation metricsthatprovidefine-grainedscoresinawaythatalso directlyreflectsinterpretableinsightsonthestrengthsand weaknesses of MT systems rather than simply replicating human assessments.
A good translation is one from which human readers maysuccessfullyunderstandatleastthebasiceventstruc-ture X  X  who did what to whom , when , where and why  X  (Pradhan et al. , 2004) X  X hich represents the most essen-tial meaning of the source utterances.

MEANT measures this as follows. First, semantic role labeling is performed (either manually or automatically) on both the reference translation and the machine transla-tion. The semantic frame structures thus obtained for the MToutputarecomparedtothoseinthereferencetransla-tions, frame by frame, argument by argument. The frame translation accuracy is a weighted sum of the number of correctly translated arguments. Conceptually, MEANT is defined in terms of f-score, with respect to the preci-sion/recall for sentence translation accuracy as calculated byaveragingthetranslationaccuracyforallframesinthe MT output across the number of frames in the MT out-put/reference translations. Details are given below. 3.1 Annotating semantic frames
In designing a semantic MT evaluation metric, one im-portant issue that should be addressed is how to evaluate the similarity of meaning objectively and systematically 222 using fine-grained measures. We adopted the Propbank SRLstylepredicate-argumentframework,whichcaptures thebasiceventstructureinasentenceinawaythatclearly indicatesmanystrengthsandweaknessesofMT.Figure1 showsthereferencetranslationwithreconstructedseman-tic frames in Propbank format and the corresponding MT output with reconstructed semantic frames by minimal trained human annotators. 3.2 Comparing semantic frames
After annotating the semantic frames, we must deter-minethetranslationaccuracyforeachsemanticrolefiller in the reference and machine translations. Although ulti-mately it would be nice to do this automatically, it is es-sentialtofirstunderstandextremelywelltheupperbound ofaccuracyforMTevaluationviasemanticframetheory. Thus, instead of resorting to excessively permissive bag-of-words matching or excessively restrictive exact string matching,fortheexperimentsreportedhereweemployed a group of human judges to evaluate the correctness of each role filler translation between the reference and ma-chine translations.

In order to facilitate a finer-grained measurement of utility, the human judges were not only allowed to mark each role filler translation as  X  X orrect X  or  X  X ncorrect X , but also  X  X artial X . Translations of role fillers are judged  X  X or-rect X  X ftheyexpressthesamemeaningasthatoftherefer-encetranslations(ortheoriginalsourceinput,inthebilin-guals experiment discussed later). Translations may also bejudged X  X artial X  X fonlypartofthemeaningiscorrectly translated. Extra meaning in a role filler is not penalized unless it belongs in another role. We also assume that a wronglytranslatedpredicatemeansthattheentireseman-tic frame is incorrect; therefore, the  X  X orrect X  and  X  X ar-tial X  X rgumentcountsarecollectedonlyiftheirassociated predicate is correctly translated in the first place.
Table 1 shows an example of SRL annotation of MT1 inFigure1byoneoftheannotators,alongwiththehuman judgment on translation accuracy of each argument. The predicate ceased inthereferencetranslationdidnotmatch with any predicate annotated in MT1, while the predicate resumed matched with the predicate resume annotated in MT1. All arguments of the untranslated ceased are auto-matically considered incorrect (with no need to consider each argument individually), under our assumption that a wronglytranslatedpredicatecausestheentireeventframe to be considered mistranslated. The ARGM-TMP argu-almost two months , in the reference translation is partially translated to ARGM-TMP argument, So far , nearly two months ,inMT1. SimilardecisionsaremadefortheARG1 argument and the other ARGM-TMP argument; now in the reference translation is missing in MT1. 3.3 Quantifying semantic frame match To quantify the above in a summary metric, we define MEANTintermsofanf-scorethatbalancestheprecision and recall analysis of the comparative matrices collected from the human judges, as follows.
 SRL REF MT1 Decision PRED (Action) ceased  X  no match PRED (Action) resumed resume match ARGM-TMP (Temporal) now  X  incorrect C fractional counts of correctly or partially translated se-manticframesintheMToutputandthereference,respec-tively, which can be viewed as the true positive for pre-cision and recall of the whole semantic structure in one source utterence. Therefore, the SRL based MT evalua-tionmetricisequivalenttothef-score,i.e.,thetranslation accuracy for the whole predicate-argument structure.
Note that w pred , w j and w partial are the weights for the matched predicate, arguments of type j , and partial trans-lations. These weights can be viewed as the importance ofmeaningpreservationforeachdifferentcategoryofse-mantic roles, and the penalty for partial translations. We will describe below how these weights are estimated.
If all the reconstructed semantic frames in the MT out-put are completely identical to those annotated in the ref-erence translation, and all the arguments in the recon-structed frames express the same meaning as the corre-spondingargumentsinthereferencetranslations,thenthe f-score will be equal to 1.

For instance, consider MT1 in Figure 1. The number of frames in MT1 and the reference translation are 1 and 2, respectively. The total number of participants (includ-ing both predicates and arguments) of the resume frame in both MT1 and the reference translation is 4 (one pred-icate and three arguments), with 2 of the arguments (one ARG1/experiencer and one ARGM-TMP/temporal) only partiallytranslated. Assumingfornowthatthemetricag-gregates ten types of semantic roles with uniform weight for each role (optimization of weights will be discussed later), then w pred = w j = 0 . 1 , and so C precision and are both zero while P precision and P recall are both 0.5. If we furtherassumethat w partial = 0 . 5 ,thenprecisonandrecall are 0.25 and 0.125 respectively. Thus the f-score for this example is 0.17.
 Both human and semi-automatic variants of the MEANT translation evaluation metric were meta-evaluated, as described next. 4.1 Evaluation Corpus We leverage work from Phase 2.5 of the DARPA GALE program in which both a subset of the Chinese source sentences, as well as their English reference, are being annotated with semantic role labels in Propbank style. The corpus also includes three participating state-of-the-art MT systems X  output. For present purposes, we randomly drew 40 sentences from the newswire genre of thecorpustoformameta-evaluationcorpus. Tomaintain a controlled environment for experiments and consistent comparison,theevaluationcorpusisfixedthroughoutthis work. 4.2 Correlation with human judgements on We followed the benchmark assessment procedure in WMT and NIST MetricsMaTr (Callison-Burch et al. , 2008, 2010), assessing the performance of the proposed evaluationmetricatthesentencelevelusingrankingpref-erenceconsistency, whichalsoknownasKendall X  X  X rank correlation coefficient, to evaluate the correlation of the proposedmetricwithhumanjudgmentsontranslationad-equacyranking. Ahighervaluefor X indicatesmoresimi-laritytotherankingbytheevaluationmetrictothehuman judgment. The range of possible values of correlation co-efficient is [-1,1], where 1 means the systems are ranked Label Event Label Event Agent who Location where Action did Purpose why Experiencer what Manner how Patient whom Degree or Extent how
Temporal when Other adverbial arg. how in the same order as the human judgment and -1 means the systems are ranked in the reverse order as the human judgment.
The first experiment aims to provide a more concrete understanding of one of the key questions as to the upper bounds of the proposed evaluation metric: how well can human annotators perform in reconstructing the semantic frames in MT output? This is important since MT out-put is still not close to perfectly grammatical for a good syntactic parsing X  X pplying automatic shallow semantic parsers, which are trained on grammatical input and valid syntactic parse trees, on MT output may significantly un-derestimate translation utility. 5.1 Experimental setup
We thus introduce HMEANT, a variant of MEANT based on the idea that semantic role labeling can be sim-plified into a task that is easy and fast even for untrained humans. The human annotators are given only very sim-ple instructions of less than half a page, along with two examples. Table 2 shows the list of labels annotators are requested to annotate, where the semantic role labeling instructions are given in the intuitive terms of  X  X ho did what to whom, when, where, why and how X . To facili-tate the inter-annotator agreement experiments discussed later, each sentence is independently assigned to at least two annotators.

After calculating the SRL scores based on the confu-sion matrix collected from the annotation and evaluation, weestimatetheweightsusinggridsearchtooptimizecor-relation with human adequacy judgments. 5.2 Results: Correlation with human judgement
Table 3 shows results indicating that HMEANT corre-lateswithhumanjudgmentonadequacyaswellasHTER does(0.432),andisfarsuperiortoBLEU(0.198)orother surface-oriented metrics.

Inspection of the cross validation results shown in Ta-ble 4 indicates that the estimated weights are not over-fitting. Recall that the weights used in HMEANT are globally estimated (by grid search) using the evaluation corpus. To analyze stability, the corpus is also parti-tioned randomly into four folds of equal size. For each fold, another grid search is also run. R HMEANT is the rank at which the Kendall X  X  correlation for HMEANT is found, if the Kendall X  X  correlations for all points in the grid search space are sorted. Many similar weight-vectors produce the same Kendall X  X  correlation score, so  X  X istinct R  X  shows how many distinct Kendall X  X  corre-lation scores exist in each case X  X etween 16 and 29. HMEANT X  X  weight settings always produce Kendall X  X  correlation scores among the top 5, regardless of which fold is chosen, indicating good stability of HMEANT X  X  weight-vector.
 Next, Kendall X  X   X  correlation scores are shown for HMEANT on each fold. They vary from 0.33 to 0.48, andareatleastasstableasthoseshownforHTER,where  X  varies from 0.30 to 0.59.

Finally,  X  CV showsKendall X  X correlationsiftheweight-vectorisinsteadsubjectedtofullcross-validationtraining and testing, again demonstrating good stability. In fact, thecorrelationsforthetrainingsetinthreeofthefolds(0, 2, and 3) are identical to those for HMEANT. 5.3 Results: Cost of evaluating
The time needed for training non-expert humans to carryoutourannotationprotocolissignificantlylessthan HTER and gold standard Propbank annotation. The half-page instructions given to annotators required only be-tween 5 to 15 minutes for all annotators, including time 225 for asking questions if necessary. Aside from providing two annotated examples, no further training was given.
Similarly, the time needed for running the evaluation metric is also significantly less than HTER X  X nder at most5minutespersentence, evenfornon-experthumans using no computer-assisted UI tools. The average time used for annotating each sentence was lower bounded by 2 minutes and upper bounded by 3 minutes, and the time used for determing the translation accuracy of role fillers averaged under 2 minutes.
 Note that these figures are for unskilled non-experts. Thesetimestendtodiminishsignificantlyafterannotators acquire experience.
We now show that using monolingual annotators is es-sentially just as effective as using more expensive bilin-gual annotators. We study the cost/benefit trade-off of using human annotators from different language back-grounds for the proposed evaluation metric, and compare whether providing the original source text helps. Note that this experiment focuses on the SRL annotation step, ratherthanthejudgmentsofrolefillerparaphrasingaccu-racy, because the latter is only a simple three-way deci-sion between  X  X orrect X ,  X  X artial X , and  X  X ncorrect X  that is farlesssensitivetotheannotators X  X anguagebackgrounds. MT output is typically poor. Therefore, readers of MT output often guess the original meaning in the source input using their own language background knowledge. Readers X  language background thus affects their under-standing of the translation, which could affect the accu-racyofcapturingthekeysemanticrolesinthetranslation. 6.1 Experimental Setup
Both English monolinguals and Chinese-English bilin-guals (Chinese as first language and English as second language) were employed to annotate the semantic roles. For bilinguals, we also experimented with the difference in guessing constraints by optionally providing the origi-nal source input together with the translation. Therefore, there are three variations in the experiment setup: mono-linguals seeing translation output only; bilinguals seeing translation output only; and bilinguals seeing both input and output.

Theaimhereistodoaroughsanitycheckontheeffect ofthevariationoflanguagebackgroundoftheannotators; thus for these experiments we have not run the weight es-timationstepafterSRLbasedf-scorecalculation. Instead, we simply assigned a uniform weight to all the seman-tic elements, and evaluated the variation under the same weight settings. (The correlation scores reported in this sectionarethusexpected tobelowerthan thatreportedin the last section.) 6.2 Results
Table 5 of our results shows that using more expen-sive bilinguals for SRL annotation instead of monolin-guals improves the correlation only slightly. The cor-relation coefficient of the SRL based evaluation metric driven by bilingual human annotators (0.351) is slightly better than that driven by monolingual human annotators (0.315); however, using bilinguals in the evaluation pro-cess is more costly than using monolinguals.

The results show that even allowing the bilinguals to see the input as well as the translation output for SRL annotation does not help the correlation. The correlation coefficient of the SRL based evaluation metric driven by bilingual human annotators who see also the source in-put sentences is 0.315 which is the same as that driven by monolingual human annotators. We find that the correla-tion coefficient of the proposed with human judgment on adequacy drops when bilinguals are shown to the source input sentence during annotation. Error analyses lead us to believe that annotators will drop some parts of the meaning in the translations when trying to align them to the source input.

This suggests that HMEANT requires only monolin-gual English annotators, who can be employed at low cost.
One of the concerns of the proposed metric is that, given only minimal training on the task, humans would annotate the semantic roles so inconsistently as to reduce the reliability of the evaluation metric. Inter-annotator agreement (IAA) measures the consistency of human in performingtheannotationtask. AhighIAAsuggeststhat the annotation is consistent and the evaluation results are reliable and reproducible.

To obtain a clear analysis on where any inconsistency might lie, we measured IAA in two steps: role identifica-tion and role classification. 7.1 Experimental setup Role identification Since annotators are not consistent in handling articles or punctuation at the beginning or the end of the annotated arguments, the agreement of se-mantic role identification is counted over the matching of
Experiments REF MT bilinguals working on output only 76% 72% monolinguals working on output only 93% 75% bilinguals working on input-output 75% 73%
Experiments Ref MT bilinguals working on output only 69% 65% monolinguals working on output only 88% 70% bilinguals working on input-output 70% 69% word span in the annotated role fillers with a tolerance of  X 1 word in mismatch. The inter-annotator agreement rate (IAA) on the role identification task is calculated as follows. A 1 and A 2 denotethenumberofannotatedpred-icates and arguments by annotator 1 and annotator 2 re-spectively. M span denotes the number of annotated pred-icates and arguments with matching word span between annotators.
 Role classification The agreement of classified roles is counted over the matching of the semantic role labels withintwoalignedwordspans. TheIAAontheroleclas-sification task is calculated as follows. M label denotes the number of annotated predicates and arguments with matching role label between annotators.
 7.2 Results
The high inter-annotator agreement suggests that the annotation instructions provided to the annotators are in general sufficient and the evaluation is repeatable and could be automated in the future. Table 6 and 7 show the annotators reconstructed the semantic frames quite con-sistently, even they were given only simple and minimal training.

We have noticed that the agreement on role identifica-tion is higher than that on role classification. This sug-gests that there are role confusion errors among the an-notators. We expect a slightly more detailed instructions and explanations on different roles will further improve the IAA on role classification.

The results also show that monolinguals seeing output only have the highest IAA in semantic frame reconstruc-tion. Data analyses lead us to believe the monolinguals are the most constrained group in the experiments. The monolingual annotators can only guess the meaning in the MT output using their English language knowledge. Therefore, they all understand the translation almost the same way, even if the translation is incorrect.
On the other hand, bilinguals seeing both the input and output discover the mistranslated portions, and often un-consciously try to compensate by re-interpreting the MT output with information not necessarily appearing in the translation, in order to better annotate what they think it should have conveyed. Since there are many degrees of freedom in this sort of compensatory re-interpretation, this group achieved a lower IAA than the monolinguals.
Bilingualsseeingonlyoutputappeartotakethisevena stepfurther: confrontedwithapoortranslation,theyoften unconsciously try to guess what the original input might have been. Consequently, they agree the least, because they have the most freedom in applying their own knowl-edge of the unseen input language, when compensating for poor translations.
In the previous experiment, we showed that the pro-posed evaluation metric driven by human semantic role annotators performed as well as HTER. It is now worth asking a deeper question: can we further reduce the la-borcostofMEANTbyusingautomaticshallowsemantic parsing instead of humans for semantic role labeling?
Notethatthisexperimentfocusesonunderstandingthe cost/benefit trade-off for the semantic frame reconstruc-tion step. For SRL annotation, we replace humans with automatic shallow semantic parsing. We decouple this from the ternary judgments of role filler accuracy, which are still made by humans. However, we believe the eval-uation of role filler accuracy will also be automatable. 8.1 Experimental setup
We performed three variations of the experiments to assess the performance degradation from the automatic approximation of semantic frame reconstruction in each translation (reference translation and MT output): we ap-plied automatic shallow semantic parsing on the MT out-put only; on the reference translation only; and on both reference translation and MT output. For the semantic 227 parser, we used ASSERT (Pradhan et al. , 2004) which achieves roughly 87% semantic role labeling accuracy. 8.2 Results
Table 8 shows that the proposed SRL based evaluation metric correlates slightly worse than HTER with a much lower labor cost. The correlation with human judgment on adequacy of the fully automated SRL annotation ver-sion,i.e.,applyingASSERTonboththereferencetransla-tionandtheMToutput,oftheSRLbasedevaluationmet-ric is about 80% of that of HTER. The results also show that the correlation with human judgment on adequacy of either one side of translation using automatic SRL is in the 85% to 95% range of that HTER.
We have presented MEANT, a novel semantic MT evaluation metric that assesses the translation accuracy via Propbank-style semantic predicates, roles, and fillers. MEANT provides an intuitive picture on how much in-formation is correctly translated in the MT output.
MEANTcanberunusinginexpensiveuntrainedmono-linguals and yet correlates with human judgments on ad-equacy as well as HTER with a lower labor cost. In con-trast to HTER, which requires rigorous training of human expertstofindaminimumeditofthetranslation(anexpo-nentially large search space), MEANT requires untrained humans to make well-defined, bounded decisions on an-notating semantic roles and judging translation correct-ness. The process by which MEANT reconstructs the se-mantic frames in a translation and then judges translation correctness of the role fillers conceptually models how humans read and understand translation output.
We also showed that using automatic shallow seman-tic parser to further reduce the labor cost of the pro-posed metric successfully approximates roughly 80% of the correlation with human judgment on adequacy. The results suggest future potential for a fully automatic vari-ant of MEANT that could out-perform current automatic MT evaluation metrics and still perform near the level of HTER.

Numerousintriguingquestionsarisefromthiswork. A furtherinvestigationintothecorrelationofeachofthein-dividual roles to human adequacy judgments is detailed elsewhere, along with additional improvements to the MEANT family of metrics (Lo and Wu, 2011). Another interesting investigation would then be to similarly repli-catethisanalysisoftheimpactofeachindividualrole,but using automatically rather than manually labeled seman-tic roles, in order to ascertain whether the more difficult semantic roles for automatic semantic parsers might also correspondtothelessimportantaspectsofend-to-endMT utility.

This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract Nos. HR0011-06-C-0022 and HR0011-06-C-0023 and by the Hong Kong Research Grants Council (RGC) research grants GRF621008, GRF612806, DAG03/04.EG09, RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency.
 Satanjeev Banerjee and Alon Lavie. METEOR: An Au-tomatic Metric for MT Evaluation with Improved Cor-relation with Human Judgments. In Proceedings of the 43th Annual Meeting of the Association of Computa-tional Linguistics (ACL-05) , pages 65 X 72, 2005. ChrisCallison-Burch,MilesOsborne,andPhilippKoehn.
Re-evaluating the role of BLEU in Machine Transla-tion Research. In Proceedings of the 13th Conference oftheEuropeanChapteroftheAssociationforCompu-tational Linguistics (EACL-06) , pages 249 X 256, 2006. Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. (Meta-) evalua-tionof Machine Translation. In Proceedings of the 2nd
Workshop on Statistical Machine Translation , pages 136 X 158, 2007.
 Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. Further Meta-evaluation of Machine Translation. In Proceedings of the 3rd Workshop on Statistical Machine Translation , pages 70 X 106, 2008.
 Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Pryzbocki, and Omar Zaidan. 228
Findingsofthe2010JointWorkshoponStatisticalMa-chineTranslationandMetricsforMachineTranslation. In Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR , pages 17 X 53, Uppsala, Sweden, 15-16 July 2010.
 G.Doddington. AutomaticEvaluationofMachineTrans-lation Quality using N-gram Co-occurrence Statistics.
In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT-02) , pages 138 X 145, San Francisco, CA, USA, 2002. Mor-gan Kaufmann Publishers Inc.
 Jes  X  us Gim  X  enez and Llu  X  is M ` arquez. Linguistic Features for Automatic Evaluation of Heterogenous MT Sys-tems. In Proceedings of the 2nd Workshop on Sta-tistical Machine Translation , pages 256 X 264, Prague,
Czech Republic, June 2007. Association for Computa-tional Linguistics.
 Jes  X  us Gim  X  enez and Llu  X  is M ` arquez. A Smorgasbord of
Features for Automatic MT Evaluation. In Proceed-ings of the 3rd Workshop on Statistical Machine Trans-lation , pages 195 X 198, Columbus, OH, June 2008. As-sociation for Computational Linguistics.
 Philipp Koehn and Christof Monz. Manual and Auto-matic Evaluation of Machine Translation between Eu-ropean Languages. In Proceedings of the Workshop on Statistical Machine Translation , pages 102 X 121, 2006. GregorLeusch,NicolaUeffing,andHermannNey. CDer: Efficient MT Evaluation Using Block Movements. In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-tics (EACL-06) , 2006.
 Ding Liu and Daniel Gildea. Syntactic Features for Eval-uation of Machine Translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-tion , page 25, 2005.
 Ding Liu and Daniel Gildea. Source-Language Fea-tures and Maximum Correlation Training for Machine Translation Evaluation. In Proceedings of the 2007
Conference of the North American Chapter of the As-sociation of Computational Linguistics (NAACL-07) , 2007.
 Chi-kiu Lo and Dekai Wu. Evaluating machine transla-tionutilityviasemanticrolelabels. In Seventh Interna-tional Conference on Language Resources and Eval-uation (LREC-2010) , pages 2873 X 2877, Malta, May 2010.
 Chi-kiu Lo and Dekai Wu. Semantic vs. syntactic vs. n-gram structure for machine translation evaluation. In Dekai Wu, editor, Proceedings of SSST-4, Fourth
Workshop on Syntax and Structure in Statistical Trans-lation (at COLING 2010) , pages 52 X 60, Beijing, Aug 2010.
 Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In 22nd International Joint Conference on Artificial In-telligence (IJCAI-11) , Barcelona, Jul 2011. To appear. Sonja Nie  X  en, Franz Josef Och, Gregor Leusch, and Her-mannNey. AEvaluationToolforMachineTranslation:
FastEvaluationforMTResearch.In Proceedingsofthe 2nd International Conference on Language Resources and Evaluation (LREC-2000) , 2000.
 KarolinaOwczarzak,JosefvanGenabith,andAndyWay.
Evaluating machine translation with LFG dependen-cies. Machine Translation , 21:95 X 119, 2008.
 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: A Method for Automatic Evaluation ofMachineTranslation. In Proceedingsofthe40thAn-nualMeetingoftheAssociationforComputationalLin-guistics (ACL-02) , pages 311 X 318, 2002.
 SameerPradhan, WayneWard, KadriHacioglu, JamesH. Martin, and Dan Jurafsky. Shallow Semantic Parsing
Using Support Vector Machines. In Proceedings of the 2004 Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL-04) , 2004. Mark Przybocki, Kay Peterson, S  X  ebastien Bronsart, and GregorySanders. TheNIST2008MetricsforMachine
Translation Challenge -Overview, Methodology, Met-rics, and Results. Machine Tr , 23:71 X 103, 2010. Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul. A Study of Trans-lation Edit Rate with Targeted Human Annotation. In
Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06) , pages 223 X 231, 2006.
 Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf. Accelerated
DP Based Search For Statistical Translation. In Pro-ceedings of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH-97) , 1997.
 Clare R. Voss and Calandra R. Tate. Task-based Evalua-tion of Machine Translation (MT) Engines: Measuring
HowWellPeopleExtractWho,When,Where-TypeEl-ements in MT Output. In Proceedings of the 11th An-nual Conference of the European Association for Ma-chine Translation (EAMT-2006) ,pages203 X 212,Oslo,
Norway, June 2006.
