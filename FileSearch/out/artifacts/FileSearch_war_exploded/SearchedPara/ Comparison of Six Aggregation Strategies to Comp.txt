 The decision to grant trust in virtual societies is often an evidence based process. The evidence for such decision derives from a diverse set, where mutual relati onships and contradictions might occur. This paper compares and evaluates six aggregation strategies to compute users X  trustworthiness. Our evaluation performed over a large online-community, shows how a rule-based strategy based on an a rgumentation semantic outperforms strategies where mutual relations hips among evidence are ignored. I.2.11 [Artificial Intelligence] : Distributed Artificial Intelligence  X  intelligent agents, multiagent systems. Algorithms, Experimentation, Human Factors Computational Trust, Web Comm unities, Argumentation Theory A computational trust model is a multi-staged process that produces a level of trustworthiness for a (digital) trustee entity. This paper focuses on the critical stage of aggregating the various pieces of evidence into a final trust value for the trustee. The set of evidence used in a trust-base d decision is various. In theory, any application data can be inte rpreted as a digital footprint that entities left behind and that can reveal information about their trustworthiness. For instance, the trust model of Longo [4] uses temporal factors as evidence, such as longevity, persistency, and regularity. Cognitive models such as Castelfranchi [17] intend trust a combination of four basic beliefs, namely competence, fulfillment, disposition, dependence. Finally, even a baseline trust model usually includes a combination of recommendation values and evidence collected from direct experience. This paper tests six techniques to aggregate different sets of evidence into a final trust value. The model selected as the test ground comes from authors X  previous works in trust computation. Anyway, it is important to notice how the strategies used are generic and applicable to any trust model. The strategies used are: an averaging approach, a linear aggregation, the weakest link (modeling a pessimistic attitude of the trustier ), the accrual of reason (modeling an optimistic attitude), a variance-based strategy and an argumentation-based strategy . Each strategy is described in the next section. A substantial di fference among the strategies is that the argumentation-based one considers the mutual relationships and contradictions that might occur among different pieces of evidence, while the other five do not. The comparison of the six strategies can be seen as an evaluation of the impact of introducing extra knowledge about evidence X  X  relationships in the computation. In fact, the six strategies can be ordered by the amount of knowledge used, as table 1 displays. Strategy Knowledge Used Average No knowledge about mutual relationship is used Linear Combination Weakest Link The minimum evidence value is selected Accrual The maximum evidence value is selected Variance-based The consensus among the evidence is considered Argumentation Mutual relationships and contradictions among In the next section we describe th e six strategies, in section 4 we present our semantic and sectio n 5 present our evaluation. The general shape of the model is depicted in figure 1.We assume that a trust value is the result of a computation that has to aggregate a set of evidence, called trust arguments. Their quantification is given a priori and outside our problem space Each argument is assumed to have a strength in [-1,1] where 1 is the highest positive trust value, 0 neutral and -1 the most negative. Each argument has an associated second value, the value of plausibility, which quantifies the va lidity of each argument in the context. For instance, Alice can have a very high strength for reputation , but the use of reputation could be a totally invalid trust argument since, it is known that in the specific context reputation values are strongly biased. The plausibility value thus acts as a weight for the strength of the corresponding argument. For each trustee entity there is a vector of couples (strength, plausibility), a couple for each argument. Our problem is how to aggregate the couples  X   X   X   X ,  X   X  into a single trust value The first strategy is the pure aggregation, where each element is equally weighted ( p is a constant). This is the only suitable approach in conditions of co mplete ignorance. Therefore: The second strategy is a weighted aggregation, to be used when information on the relative st rength of arguments is known: The third strategy is the weakest link principle, where the minimum value is used as the fi nal aggregation. The strategy models a pessimistic trustier that required all the argument to be high in order to grant trust. The fourth strategy  X  called the accrual of reasons -uses as the aggregation value the maximum among  X   X   X   X   X   X  , and it models an optimistic trustier that requir es only one positive argument to grant trust and neglect all the others. The fifth strategy is a variance-based strategy. For each user, this strategy modifies the value of the weighted aggregation by reducing it proportionally to the st andard deviation of the set of trust arguments of that user. The id ea (as in [5]) is that trustworthy members should have consistent arguments, while members with contradictory arguments should be trust less. The method is able to quantify contradictions among arguments (using  X  ), but it does not try to solve them as an argumentation approach does. The formula is: where  X   X   X   X  is the percentile in [0..1] of  X  of a particular user over the population of all users. Th erefore users are penalized proportionally to the value of their  X  compared to the others. Finally, the argumentation strategy is visualized in fig 3. The potential relationships among argumen ts have been explicated. In an argumentation scenario evidences are not isolated objects but they potentially conflict or supp ort each other. Argumentation X  X  goal is to identify which argumen ts are invalidated and which are strengthen, reaching more sustai nable conclusions. After having defined the links among the different arguments (the argumentation network), an argum entation semantic describes how the value of arguments is modified by other arguments (function of support/attack), how arguments are aggregated afterwards (function of accrual), and how conclusions are computed (function of conclusion). Argumentation is computed in the following way. First, a defeas ible semantic (described in the next section) is applied accord ing to the mutual relationships identified among the arguments. As a result of the argumentation process, the value of p i may vary. The final argumented value is: Note how s i are unchanged since in our particular inference network (figure 2), evidence does not attach/support directly the strength of other evidence, but rather their plausibility, as next section explain. Note also that the new p X  i could be null for arguments that are completely defeated. A semantic, as already anticipated, requires to define: a function of conclusion f c , that defines the strength of a defeasible conclusion given a premise and a plausibility link, a function of attack and support , defining how the strength or plausibility of argument changes in presence of defeaters or supporters, and the accrual of reasons, that defines how to compute the strength of a conclusions based on n independent arguments of strengh S argument X  strength is assumed to be in [-1,1], plausibility in [0,1]. Function of Conclusion Given a premise A of strength S a and plausibility P of the conclusion (see [3] for justification) is: Function of Attack and Support These two functions define how the strength of a reasoning link P ac changes if attacked by an argument of plausibility P strength S b . Note again how S a is not involved in the process. We used the following function of support: With analogous argument, the functio n of attack is the following (with no activation rule, since an at tacker can never be neglected): Accrual of Reasons In our simple inference network (figure 1 and 2), the only accrual of reasons is how the computa tion of the final trust value, represented by formula (1) of previous section. (attack/support) among the trust ev idence large enough to test the impact of argumentation. Our argumentation rules are implemented as follows: A mutual relationship defines if an argument attacks/supports another. It has the form Arg1 a ttacks/support arg2 . Each argument is required to have the proper positive or negative strength, and each relationship has a plausibility value attached. Let X  X  analyze some of the identified rules AR 1 states that longevity is a stronger argument when the trustee is also active, while a low degree of activity invalidates a high degree of longevity ( AR 2 ). With the same notation: Stability is stronger evidence if the entity is active ( AR even a high degree of stability is undermined if the entity is not active , since it could be out of business rather than complete. A lack of pluralism weakens high activity, since it undermines its validity and controllability: A lack of longevity attacks also a high level of activity. It is a common used metric that spammers are likely to create a large amount of activity for short time and suddenly disappear: The trust model used, described in [3], computes trust as a combination of various piece of evidence. Each piece of evidence is derived by matching some gene ric trust patterns, whose validity is justified by sociological theory of trust, over the application data. Table 2 shows th e trust pattern used by the method with a brief explanation. We selected this model since it represents a good testing ground due to the richness of evidence used in the computation, but we remind how the strategies we evaluate in this paper are generic and applicable to any trust models. Trust Arguments Longevity Trust an entity because of the time spent Persistency Trust entity because of the Regularity Stability Trust stable entities Degree of Activity Degree of Activity as an indicator of Pluralism Trust entities or objects that are the Pertinence Pertinence as an ingredient of trust Similarity Trust entities similar to trustworthy ones Std Compliance Trust entities compliant to a standard Reputation Trust entities with high reputation Past-Outcomes Trust entities that did well in the past Connectivity Trust entities well-connected in the Accessibility/ Visibility Authorship Trust entities or objects on the base of We evaluated the strategies over the large online community FinanzaOnline.it , a community of traders with almost 80 000 registered users and about 9 million messages. Aim of our previous experiment was the computation of a level of trustworthiness for each forum me mber. We valida ted our results against an explicit poll we promoted within the forum administrators, asking forum me mbers to identify trustworthy entities. The anonymous poll received almost 1 500 answers. The results of the poll showed a clear consensus about the most trustworthy entities. According to the results, we divided members in ordered tiers. The first tier cont ains the 10 most trustworthiness entities, covering the 78.5% of the preferences, the second the members from 11 to 50 positions (89% of the overall preferences). A trust com putation is successful if it recognizes tier 1 and tier 2 members as trustworthy. 
In order to compare our strategies we quantify the distance of their computed values to the poll X  s results. First, we ordered users according to their poll X  X  position. Therefore, member n is the n most trustworthy member. For each strategy i we computed a trust value for all the members and we defined function R i that assigns to the n th member its position in the strategy X  X  rank. We defined: E(n) measures the average error generated by each strategy for the set of top-n members. E(10) and E(50) are of particular relevance. Trust Arguments The following trust arguments where mapped over the available elements of FinanzaOnline forum. Further details in [12]. Longevity , mapped using the registration date and the date of last post, with high plausibility. Persistency and Regularity were mapped using the action of writing a post and its timestamp. Activity , mapped quantifying the following actions: posting a message (compulsory), opening a discussion, opening a poll and adding attachments to messages (optional). Pertinence , computed by quantifying, fo r each user, the amount of activity (post, attachments) pertinent to the domain of trading and by discarding out-of-topic contributions. Connectivity. The scheme requires a PageRank-like computation using the action of quoting a message as indicator of links between entities. Then, a network of references is built, revealing the central entities and the peripheral ones. Reputation. An internal reputation system is available. Since the values produced are highly positively biased, i.e. suspiciously aggregated around very high valu es and many members dismiss its reliability the plausibility of the argument is set to low (0.3) . Accessibility. Accessibility was mapped over profile information of each member: contact information ( email, skype ,..), picture, biographical information, visits received to the personal homepage. Due to the low availability of the data and the fact that they are not verified, the plausibility in the context is set to low. Table 3 show results of our previ ous experiment on a set of 5015 member. For each trust argument, it shows the value of E(10) and E(50) , and the plausibility value for each argument (see [12] for details). The table is the input of our evaluation; these data has now to be aggregated into a final trust value. Table 4 shows the value of E(10), E(50)  X  average error on the top 10 or 50 members -, S(10) and S(50)  X  the standard deviation of the error  X  and the Max error in each of the two cases. First, we notice how the argum entation strategy outperforms all the others, while interestingly the strategy enhanced by the standard deviation also outperforms the other, even if the gap is not statistically significant compared to the averaging and linear combination. The results show an overall very high degree of precision that the introduction of defeasibility makes more efficient. E(50) is now reduced from 53.53 to 40.7 positions, with an error decreased of about 30% in respect to the best second case, and by about 37% in respect to the basic aggregation. We notice how the optimistic Max strategy does not perform well, as expected: when only the best argum ent is used to grant trust, average users with at least a singl e very high trust argument  X  typically high longevity  X  will sc ore high deteriorating the trust ranking. On the contrary, the pessimistic Min strategy underperforms the aggregation ones but not significantly, with a 12% loss of precision over E(50) . A reasonable explanation is that, since we are focusing on the t op-tier users, they usually have quite high level for each indicator, which guarantees a reasonable global trust value. We notice how in any case also these members have internal contradictions, as the variance-enhanced strategy spotted and the argumentation strate gy tried to solve. In general, an interesting trend is the fact that more gain is achieved for E(50) and S(50) rather than E(10) and S(10) . The very top-level entities require less argumentation, due to their high level in each indicator and the presence of less internal contradictions. In order to statistically valida te our results, we performed a significance tests between the to p two strategies. Our problem is to compare the statistical difference between two ranks of the same population. We tested it using the Wilcoxon Signed-Rank Test, that gave a Z crit of -1.84 (sigma equal to 14.28), showing statistical significance. By analysing results in detail, we note how argumentation gives more consiste ncy, mainly by reducing the impact of entities with high but not regular activity (rule AR applied to 267 members), by reducing the ranking of entities with high activity but low pertinence ( AR 24 , applied to 197 members), by excluding old but inactive or medium-active entities, ( AR applied to 1447 members) We now investigate the sensitivity of the system X  X  results to different plausibility values. How critical is the selection of a plausibility value? In our discussion this is given apriori , results of a (subjective) domain expert choice or it could be the output of a learning process, or empirical probabilities. We wonder how the quality of prediction is affected by a different plausibility value assignment. We considered the value of plausibility chosen for the four principal trust schemes ( time, activity, connectivity, competence) forming the 4-tuple P 0 =(1,0.9,0.6,0.75). We then perturbed the values around P 0 by +/-0.15, testing the results obtained. The 16 different combinations are displayed in graph 1. The value displayed are the normalized value of E(50) , where a value of 1 is assigned to our original tuple P 0 . The values obtained are ranging from 0.985 to 1.061, with an average of 1.032. This means that around P 0 the system has similar behavior, with an average variation of 3.2% and a maximum of 6.1%. We conclude how a perturbation of size 0.3 does not affect statistically the validity of the results. aggregate evidence in a trust computation. Our evaluation showed how a set of argumentation rules defined over the available evidence, extracted from a gene ric knowledge-based model of trust, can improve the quality of the predictions by 30%. Future works evaluate other potential bene fits that argumentation can add to trust computation: a fastest-to-react value and a more scrutable and justifiable system for humans. [1] L. Longo at al. Temporal Factors to Evaluate Trustwhortiness of Virtual Identities. IEEE Securecomm 2007, Nice, France [2] J. Pollock. Defeasible Reasoning with Variable Degrees of 
Justification. Artificial Intell. , N. 133, pages 233-282, 2001 [3] P. Dondio. Trust as a form of Defeasible Reasoning. PHD 
Tehsis, Computer Science Department, Trinity College Dublin [4] www.finanzaonline.it Last accessed on the 12 August 2008. [5] J. Sabater, C. Sierra. REGRET: A reputation model for gregarious societies. 4 th Workshop on Fraud and Trust in 
Agent Societies, Montreal, Canada. pp. 61-69, 2001 [6] C. Castelfranchi, R. Falcone. Trust is much more than subjective probability: 32nd HICSS. Hawaii, 2000. 
