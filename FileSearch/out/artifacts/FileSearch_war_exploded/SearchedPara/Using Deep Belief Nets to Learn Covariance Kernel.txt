 Gaussian processes (GP X  X ) are a widely used method for Bayes ian non-linear non-parametric re-encodes prior knowledge of the smoothness of the underlying process that is being modeled. Be-areas of machine learning.
 unlabeled data is of no use. Given a set of i.i.d. labeled input vectors X associated target labels { y model p ( y the input data X = [ X a mixture P none of the existing approaches have proved to be particular ly successful. of X that makes explicit a lot of the higher-order structure in th e input data. discovered by the DBN. For a regression task, we are given a data set D of i . i . d . labeled input vectors X their corresponding target labels { y regression model: we are modeling, so that a-priori p ( f | X covariance matrix, whose entries are specified by the covari ance function K Integrating out the function values f , the marginal log-likelihood takes form: which can then be maximized with respect to the parameters  X  and  X  . Given a new test point x value y where k prior p ( y distribution over the latent function f which is then used to produce a probabilistic prediction: we approximate the non-Gaussian posterior p ( f | X agation [12]. For more thorough reviews and implementation details refer to [13, 16]. In this section we describe an unsupervised way of learning a DBN model of the input data X = [ X images of face patches and word-count vectors of documents. 3.1 Modeling Real-valued Data x where g ( x ) = 1 / (1+exp(  X  x )) is the logistic function, w input i and feature j ,  X  2 over visible vector x is: where E ( x , h ) is an energy term: E ( x , h ) = P eter updates required to perform gradient ascent in the log-likelihood is obtained from Eq. 9: where  X  is the learning rate, z distribution and &lt;  X  &gt; To circumvent the difficulty of computing &lt;  X  &gt; The expectation &lt; z is computed as z real-valued data. Then Eq. 8 is used again to activate the fea tures and compute &lt;z  X  i = 1 simplified version of Eq. 11. 3.2 Modeling Count Data with the Constrained Poisson Model data x and a conditional Bernoulli distribution for modeling  X  X id den X  topic features h : p ( x i = n | h ) = Pois n, where Pois n,  X  = e  X   X   X  n /n ! , w j , for word i , and b deals appropriately with documents of different lengths. The gradient of the log-likelihood function is: 3.3 Greedy Recursive Learning of Deep Belief Nets now describe an efficient way to learn additional layers of bi nary features. is part of a multilayer directed model.
 We could initialize the new RBM model by simply using the exis ting learned model but with the for the hidden units.
 features in the layer below.  X  = {  X ,  X  } and W as: where K gradients: A B
A 100 22 . 24 28 . 57 17 . 94 18 . 37 15 . 28 15 . 01 18 . 13 (10) 16 . 47 (10)
B 100 26 . 94 28 . 32 23 . 15 19 . 42 19 . 75 18 . 59 25 . 91 (10) 19 . 27 (20) can be used to slightly refine those features rather than to cr eate them. two different classes of news story based on the vector of wor d counts in each story. 5.1 Extracting the Orientation of a Face Patch of figure 2 shows randomly sampled examples from the training and test data. For training on the Olivetti face patches we used the 784-100 0-1000-1000 architecture shown in training of a DBN model. The 2.8 million parameters of the DBN model may seem excessive for training data and we penalize the squared weights. After the DBN has been pretrained on the unlabeled data, a GP m odel was fitted to the labeled data using the top-level features of the DBN model as inputs. We call this model GP-DBNgreedy . These derivatives can be backpropagated through the DBN to a llow discriminative fine-tuning of all four types of GP model on varying amounts of labeled data. The results show that both GP-DBNgreedy and GP-DBNfine significantly outperform a regular GP model. Indeed, GP-DBNfine with only 100 labeled training cases outperforms GPstandar d with 1000. than GPstandard on non-occluded data.
 We have also experimented with using a Gaussian kernel with A RD hyper-parameters, which is a common practice when the input vectors are high-dimensiona l: where D is the diagonal matrix with D length-scale parameter for each dimension. ARD hyper-para meters were optimized by maximizing the marginal log-likelihood of Eq. 3. Table 1 shows that ARD h yper-parameters do not improve GPstandard, but they do slightly improve GP-DBNfine and they strongly improve GP-DBNgreedy and GPpca when there are 500 or 1000 labeled training cases.
 We suspect that the GP-DBNfine model does not benefit as much fr om the ARD hyper-parameters features. 5.2 Extracting the Magnitude Represented by a Handwritten D igit and Discriminating being used for greedily pretraining the DBN model. Table 2, p anel A, shows that GP-DBNfine and GP-DBNgreedy perform considerably better than GPstandard both with and without ARD hyper-The performance of GP-DBNgreedy demonstrates that the gree dily learned feature representation tasks, even though these tasks are unknown when the DBN is bei ng trained. 5.3 Classifying News Stories The Reuters Corpus Volume II is an archive of 804,414 newswir e stories The corpus covers four major groups: Corporate/Industrial, Economics, Governme nt/Social, and Markets. The data was major group. The available data was already in a convenient, preprocessed format, where common stopwords were removed and all the remaining words were stem med. We only made use of the 2000 as a vector containing 2000 word counts. No other preprocess ing was done. Economics groups. As expected, GP-DBNfine and GP-DBNgreedy work better than GPstandard. relevant to the classification task. on comparing our method to other kernel-based semi-supervi sed learning algorithms [4, 19]. Acknowledgments We thank Radford Neal for many helpful suggestions. This res earch was supported by NSERC, CFI and OTI. GEH is a fellow of CIAR and holds a CRC chair.

 kernel in advance and only adapt a few hyper-parame ters. They do not learn a complicated task-specific kerne l. This is wasteful if there is a lot of unlabeled data and only a little labeled data: On one of the regression tasks we use to compare methods it gives: fixed kernel: 16.3% error dataset and then use the features in the deepest la yer to train a Gaussian Process (GP) on the labeled data. This reduces the error: greedily learned kernel: 11.2% error the features in every layer of the DBN. This produc es 
