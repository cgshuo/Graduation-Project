 The classic Set Cover problem requires selecting a mini-mum size subset A  X  F from a family of finite subsets F of U such that the elements covered by A are the ones covered by F . It naturally occurs in many settings in web search, web mining and web advertising. The greedy algorithm that iteratively selects a set in F that covers the most uncov-ered elements, yields an optimum (1+ln |U| )-approximation but is inherently sequential. In this work we give the first MapReduce Set Cover algorithm that scales to problem sizes of  X  1 trillion elements and runs in log p  X  iterations for a nearly optimum approximation ratio of p ln  X , where  X  is the cardinality of the largest set in F .

A web crawler is a system for bulk downloading of web pages. Given a set of seed URLs, the crawler downloads and extracts the hyperlinks embedded in them and schedules the crawling of the pages addressed by those hyperlinks for a subsequent iteration. While the average page out-degree is  X  50, the crawled corpus grows at a much smaller rate, implying a significant outlink overlap. Using our MapRe-duce Set Cover heuristic as a building block, we present the first large-scale seed generation algorithm that scales to  X  20 billion nodes and discovers new pages at a rate  X  4 x faster than that obtained by prior art heuristics.
Set Cover is one of the 21 classic combinatorial prob-lems shown by Karp to be NP-complete in 1972 [1]. Given a collection F of subsets of a finite set U , the objective is to obtain a minimum subset A  X  F such that every element in U belongs to at least one set in A . It is also one of the first problems to be approximated [2] with an approximation ratio of 1 + ln n , where n = |U| . This is shown to be op-timum as no polynomial-time (1  X  o (1)) ln n approximation algorithm can exist unless NP has slightly superpolynomial time algorithms [3]. Set k  X  Cover is a variation that re-quires each element in U to be covered at least k times, while Weighted Set Cover is a variation where each set has an c associated weight and the objective is to minimize the sum of the weights of the sets in A .

Johnson X  X  greedy approximation algorithm [2] selects at each step the set that covers the most elements that have not been covered by previous sets and is therefore inherently se-quential. In [4] Berger, Rompel and Shor study the problem in a parallel setting and give an NC O (log n )-approximation algorithm. They closely approximate the greedy algorithm by bucketing set cardinalities by factors of p &gt; 1 and pro-cessing sets within a bucket in parallel. Their techniques lead to an O (log 5 M )-depth, p ln n -approximation random-ized algorithm on a PRAM, where M = P S  X  X  | S | . In [5] Blelloch, Peng and Tangwongsan improve upon [4] by ob-taining a work-optimum, O (log 3 M )-depth, p ln n approxi-mation algorithm.

In [6] Chierichetti, Kumar and Tomkins, inspired by the theoretical results of [4], provide the first MapReduce-based algorithm for Max k -Cover , a related problem where the objective is to cover as many elements in U with at most k sets. Their algorithm closely follows the proof of [4] and requires O ( poly (  X  ) log 3 mn ) MapReduce steps to achieve an (1  X  1 /e  X   X  )-approximation randomized algorithm, where m = |F| . Such an algorithm may be an improvement upon the sequential greedy algorithm, but is still not practical for problem instances where M  X  1 , 000 , 000 , 000 , 000 as in such cases it would require an impractically large (  X  100 , 000) number of MapReduce steps. In fact their largest dataset has an input size of M = 72 million and is comprised of only 14 . 2 million sets.

In [7] Cormode, Karloff and Wirth present a secondary storage-friendly sequential greedy algorithm for Set Cover . They also adopt the bucketing heuristic of [4] and forgo sam-pling within each bucket opting instead for a sequential scan of its elements. However during each such scan their al-gorithm requires moving sets arbitrarily between buckets, which are stored on disk. Such an approach cannot scale to 1 trillion item instances. Furthermore only single-node results are presented, while their largest instance consists of only M = 5 , 267 , 656 elements.

In this paper we present Set Cover algorithms that scale to problem instances of size  X  10 12 and use them to generate seed sets for web crawling, which is one of the important problems addressed by web crawlers.
 A web crawler is a system for batch downloading of web pages. Web crawlers have various applications, the most prominent of which are web search engines, web data min-ing and web monitoring. There are currently over 1,500 a ctive crawlers [8]. By some early estimates, crawlers con-sume up to 40% of the Internet X  X  text web bandwidth [9]. At an abstract level, the web crawling algorithm is straightfor-ward; given a set of seed URLs a crawler places them in the Frontier structure, from which it iteratively selects URLs, downloads them, extracts the hyperlinks contained in them and adds the new URLs to the Frontier. Web crawling has many engineering challenges, such as scalability, adhering to politeness policies and managing latency, and algorithmic challenges, such as content selection, spam and crawl traps detection, Frontier URLs scheduling, and seed set creation.
Web crawlers are almost as old as the web itself, be-ginning with Matthew Gray X  X  World Wide Web Wanderer [10] shortly after the launch of NCSA Mosaic, the WWW Worm [11], the RBSE spider [12], WebCrawler [13] and MOMspider [14]. The undocumented crawlers of the first-generation search engines (including Lycos, Infoseek, Ex-cite, AltaVista, and HotBot) followed. The first work that addressed scalability issues was Mike Burner X  X  description of the Internet Archive crawler [15]. Details of Google X  X  first-generation crawler were given by Page and Brin in [16]. Heydon and Najork provided details on the Mercator [17,18], the first documented scalable design which was used in many web research projects [19 X 23] and adopted by AltaVista in 2001. Another early distributed design was Shkapenyuk and Suel X  X  Polybot web crawler [24]. The IBM WebFountain crawler [25] represented another industrial-strength, fully distributed design. UbiCrawler [26] was the first scalable distributed web crawler that used consistent hashing, allow-ing for incremental scalability and graceful degradation in the presence of failures. In [27] Lee, Leonard, Wang, and Loguinov shared their experience in designing IRLbot, a single-server web crawler that crawled at a sustained rate of 1,789 pages/sec, downloaded 6.3 billion pages and dis-covered 41 billion unique nodes in a span of 41 days. Her-itrix [28], the crawler currently used by the Internet Archive, and Nutch [29] are two of the most popular open-source crawlers.

The dominant search engines only crawl and index a small fraction of all exposed web pages [30]. The problem of scheduling URLs to be downloaded is therefore crucial. The scheduling policy balances two major goals; (1) coverage, measured either against some prior corpus or via some met-ric on the collected pages, and (2) freshness, measured via the X  X taleness X  X f pages compared to their live versions. Crawl-ing is either a batch or, more commonly, an incremental process that never terminates. Olston and Najork present a thorough survey on scheduling policies in [31].
 In [32], Broder, Kumar, Maghoul, Raghavan, Rajagopalan, Stata, Tomkins and Wiener performed an insightful study on the structure of the web. Their findings on web connec-tivity suggest the existence of five major components; (1) a central strongly connected (SCC) component (28%), (2) a component IN without inlinks from SCC whose outlinks reach SCC (22%), (3) a component OUT without outlinks to SCC that can be reached from SCC (22%), (4) dentril components leaving IN, or entering OUT (22%), and (5) disconnected components (6%.) Their study implies that there exist many node pairs n i , n j such that n j is not reach-able from n i , or only reachable after following hundreds of outlinks. Therefore seed URLs should be selected carefully and multiple seeds may be necessary to ensure good cover-age [31]. Ntoulas et al. [33] and Dasgupta at al. [34] studied the creation and retirement of pages and links and found that it is possible to discover 90% of new pages by monitor-ing links spawned from a small, well-chosen set of old pages, while discovering the remaining 10% requires substantially more effort. Therefore (1) it is possible to deduce a rela-tively small set of seed URLs that discovers a substantial part of the web, and (2) selecting a random set may leave a significant part of the web undiscovered.

Zheng, Dmitriev and Giles were the first to systematically study the creation of seed lists for web crawling [35]. They proposed a graph-based framework for crawler seed selec-tion, and presented several algorithms within that frame-work. Evaluation on real web data showed significant im-provements over simpler heuristic seed selection approaches. However, many of their algorithms involve calculating the number of nodes reachable within a few hops of a given node and thus, are not scalable. Their experimental results were drawn on a dataset that included 2000 web sites of more than 100 pages each.
 In the MapReduce programming paradigm, the basic unit of information is a ( key, value ) tuple [36,37]. The input to a MapReduce algorithm is a set of ( key, value ) tuples while operations on a set of tuples occur in three phases: the map, shuffle and the reduce phase, which we describe hereafter. In the map phase the mapper accepts as input a sequence of tuples and outputs any number of new tuples for each input tuple. Each map operation is stateless which allows for easy parallelization as different inputs for the map can be processed by different computation nodes. During the shuffle phase, all of the values that are associated with an individual key are sent to the same node. The shuffle phase is transparent to the programmer. During the reduce phase the reducer aggregates all of the values associated with a single key k and outputs a multiset of tuples whose key is k . Map and reduce phases are serialized: the reduce phase can only start after all maps have terminated. While each reducer operating on a single key executes sequentially, in-sofar as the MapReduce paradigm is concerned, reducers operating on different keys can be parallelized. A program in the MapReduce paradigm can comprise many rounds of map/reduce phases executed in a pipelined fashion.
MapReduce is a powerful computational model that has proved successful in enabling large-scale web data mining. Many matrix-based algorithms, such as PageRank [38], have been successfully implemented in the MapReduce model. The authors of [6] suggest three major requirements for effi-cient MapReduce computations: 1) the number of iterations is at most polylogarithmic in the input size; 2) the output of the map or reduce step should remain linear in the input size. Also, the map and reduce steps should run in time linear in their input sizes; 3) the map/reduce steps should use constant or logarithmic amount of memory.
 In this work we develop: (1) an efficient in-memory Set Cover heuristic algorithm that does not require auxiliary data structures of O ( M ) memory footprint and is guaran-teed to complete in O ( m log m log p  X  + p p  X  1 M ) steps while providing a ( p ln  X  + 1)-approximation ratio; (2) a highly scalable MapReduce algorithm for Set Cover that exe-cutes in log p  X  iterations while providing a ( p ln  X  + 1)-approximation ratio; (3) a Layered Set Cover algorithm for generating web crawling seed sets that exploits our MapRe-duce heuristic and generates seeds that discover new nodes at a significantly faster rate.
 The rest of this work is organized as follows: Section 2 pro-vides the necessary definitions. Section 3 presents our main results: an efficient in-memory and a highly scalable MapRe-duce heuristic algorithm for Set Cover . Section 4 describes our algorithm for generating seed sets for web crawling. Sec-tion 5 provides preliminary experimental results. Section 6 concludes this work.
Let U = { 1 , . . . , n } be a finite set of n elements and F a family of subsets of U . Let m = |F| and M = P S  X  X  | S | . Let w : F  X  R be a weight function on the sets in F .
Definition 2.1. The k -coverage cov k ( A ) of a family of sets A  X  F is cov k ( A ) = { x | P S  X  X  1 S ( x )  X  k } . where 1 S ( x ) is the indicator function of set S . Definition 2.2. The coverage cov ( A ) of a family of sets A  X  F is cov ( A ) = cov 1 ( A ) =  X  S  X  X  S .

Definition 2.3 (Min Set Cover). A  X  F is a mini-mum set cover if cov ( A ) = cov ( F ) and
Definition 2.4 (Max-k -Cover). A  X  F is a max-k -
Definition 2.5 (Min Set k -Cover). A  X  F is a min-imum k -set cover if cov k ( A ) = cov k ( F ) and
Definition 2.6 (Weighted Set Cover). A  X  F is a weighted set cover if cov ( A ) = cov ( F ) and
We begin by describing the classic Greedy set cover heuris-tic. We identify performance bottlenecks as well as scal-ability issues. We then proceed to describe fGreedy , an efficient in-memory heuristic that guarantees the same ap-proximation as Greedy . We further refine fGreedy to rfGreedy by limiting the amount of access allowed to F and analyze the impact on the approximation guarantee. Equipped with these results, we present pGreedy , a MapRe-duce Set Cover approximation algorithm designed to scale to 10 12 input size and spGreedy , a simplification over pGreedy that maintains a similar approximation guarantee. Algorithm 1 G reedy Set Cover approximation Require: a family F of subsets of U = { 1 , . . . , n } 1: A =  X  3: while C 6 =  X  do 4: S = arg max S  X  X  | S  X  C | 5: A = A  X  { S } 6: C = C \ S 7: return A 3.1 G reedy
Johnson X  X  classic Greedy approximation algorithm is shown in Algorithm 1. At each iteration, the algorithm selects the set that has the largest overlap with the set of currently un-covered elements. After each selection S , all elements that have been covered by S are removed from the remaining sets. This operation is typically performed by maintaining an inverted index, mapping each element to the sets it is contained into. This auxiliary structure is as large as the problem input itself. Upon selecting a new set S on line 4 in Algorithm 1, the new ordering of the sets needs to be computed. This is an expensive operation as a particular element may be contained in a large number of sets. Also, updating the position of a particular set within the ordering maybe not be useful as this set may never be selected by the algorithm. Moreover, the ordering itself requires additional space, albeit O ( m ) instead of O ( M ). 3.2 fGreedy
In this section we present an in-memory heuristic that achieves the approximation ratio of Greedy but does not require constructing an inverted index. Even though it re-quires the construction of a heap of m elements, the space required for this is as much as the space required for main-taining an ordering on the sets in Algorithm 1. The algo-rithm first computes the cardinalities of all the sets in the input and builds a max-heap of pointers to the sets, or-dered by their cardinalities. The algorithm also maintains a set of uncovered elements, initialized to U , in the form of a bit-vector. At each iteration, the set on the top of the heap is examined. The current number of uncovered ele-ments within it, and subsequently, its position in the heap, is updated. If it remains the top element in the heap af-ter updating, then it is selected as part of the solution. In Lemma 3.1 we show that fGreedy indeed achieves the same approximation ratio as Greedy .
 Lemma 3.1. fGreedy yields a ln  X  + 1 approximation
Proof. We will show that fGreedy makes the same set choices as Greedy . A set T is selected as part of the solu-tion only on line 9 in algorithm 2. At that point, the number of new elements contributed by T with respect to the cur-rent coverage C is accurate, since a decrease key o peration has been applied on line 6 that has repositioned T in the heap based on its new key | T  X  C | . It is also the maximum cardinality set in the heap H . Any prior set selection would have reduced the coverage of remaining sets in H , therefore T is indeed the set which maximizes T  X  C . Let us now as-sume that T should be placed in A but the algorithm does not select it. In this case, the heap-down operation on line 6 has pushed the set down from the root of H . However, as T is indeed part of the Greedy solution, all sets T  X  that Algorithm 2 f Greedy Set Cover approximation Require: a family F of subsets S j of U = { 1 , . . . , n } 1: A =  X  3: H = max-heap on the sets S j ordered by | S j | 4: while C 6 =  X  do 5: T = find max( H ) 6 : decrease key( H , T , | T  X  C | ) 7 : if T == find max( H ) t hen 8: delete max( H ) 9 : A = A  X  { T } 10: C = C \ T 11: else 12: T = T  X  C 13: return A will be pushed lower than T in subsequent iterations of the algorithm without being selected. 3.3 r fGreedy
The fGreedy heuristic does not require the construction of an inverted index thus almost halving the memory re-quirements. It also avoids updating the relative ordering of all the sets at each iteration, opting for correcting it only when necessary. Its performance is dependent on whether a set is examined multiple times during the execution of the algorithm on average. In this section we present rfGreedy , a refinement on fGreedy that bounds the number of times each specific set can be discovered during the execution of the algorithm, at the cost of selecting a slightly less optimal set at each iteration.

Given an approximation factor q , the algorithm selects the top element T in the heap to be placed in the result, even if it would no longer maintain its position at the top of the heap, so long as the uncovered elements T  X  C are at least a fraction 1 /q of the uncovered elements at the time T was last examined. In Lemma 3.2, we show that rfGreedy parses the input at most 2 q  X  1 q  X  1 t imes. In Lemma 3.3, we show that it is an O ( m log m log  X  + q q  X  1 M ) -time algorithm. Finally, in Lemma 3.4 we show that rfGreedy is a ( q ln  X  + 1)-approximation algorithm.

Lemma 3.2. rfGreedy accesses the input M at most
Proof. Constructing the heap on line 3 in algorithm 3 requires computing the initial cardinalities of all the sets in F , contributing M to the total access of the problem input. Let us now upper bound the number of accesses of each individual set S  X  F . S can only be accessed when it is at the top of H . At this point, either S is selected to be placed in A or its cardinality has been reduced to 1 /q of the one computed the last time it was placed or moved in the heap. will be accessed. Aggregating over all sets, the total element accesses is upper bounded by q q  X  1 M .

Lemma 3 .3. rfGreedy completes in O ( m log m log q  X +
Proof. Each set will be examined in the heap at most log q  X  times while each heap-down operation costs O (log m ). Algorithm 3 r fGreedy Set Cover approximation Require: a family F of subsets S j of U = { 1 , . . . , n } Require: q &gt; 1 1: A =  X  3: H = max-heap on the sets S j ordered by | S j | 4: while C 6 =  X  do 5: T = find max( H ) 6 : s = | T | 7: s  X  = | T  X  C | 8: if qs  X   X  s then 9: delete max( H ) 1 0: A = A  X  { T } 11: C = C \ T 12: else 13: decrease key( H , T , s  X  ) 1 4: T = T  X  C 15: return A By Lemma 3.2, a total of O ( q q  X  1 M ) probes to the input will be performed.

Lemma 3 .4. rfGreedy is a ( q ln  X  + 1) -approximation algorithm Proof. Let us consider the sequence of sets of elements remaining to be covered, obtained after each selection of rfGreedy on line 10. After the j -th selection, there remain | C j | elements to be covered.

The optimum solution contains OP T sets. Therefore be-fore the j -th selection, there exists a set that contains at least | C j | /OP T elements. Since Greedy selects the largest set, its selection will contain at least that many elements.
Furthermore, rfGreedy selects a set that contains at least 1 /q of the elements of the set that would have been selected by Greedy . Therefore, rfGreedy selects a set that covers at least | C j | qOP T e lements.
 We will first compute the number of selections t that rf-Greedy makes until at most OP T elements remain to be covered. It holds:
The inequality holds because f ( x ) = (1  X  1 /x ) x is a mono-tonically increasing function whose limit is 1 /e . Then:
This holds because the optimum solution needs to contain at least n/  X  sets for it to be a cover of U .

Once the number of remaining elements to be covered is reduced to OP T , each subsequent selection will cover at Algorithm 4 p Greedy Set Cover approximation Require: a family F of subsets S j of U = { 1 , . . . , n } Require: a sequence of partition points p 1 &lt; . . . &lt; p Require: q &gt; 1 1: A =  X  3: for L = k.. 1 do 4: S L = { S  X  F | | S  X  C |  X  p L } 5: H = max-heap on the sets S  X  S L ordered by | S  X  C | 6: while C 6 =  X  and S L 6 =  X  do 7: T = find max( H ) 8 : s = | T | 9: s  X  = | T  X  C | 10: if qs  X   X  s then 11: delete max( H ) 1 2: S L = S L \{ T } 13: A = A  X  { T } 14: C = C \ T 15: else if s  X   X  p L then 16: decrease key( H , T , s  X  ) 1 7: T = T  X  C 18: else 19: delete max( H ) 2 0: S L = S L \{ T } 21: return A least one additional element. Therefore T OT  X  t + OP T , which implies that: This completes the proof. 3.4 p Greedy
While rfGreedy provides an efficient in-memory heuris-tic, the focus of this work is set cover instances that are many times larger than what can fit in a single-server mem-ory. rfGreedy provides an essential building block for our very large scale Set Cover algorithm. While it minimizes the memory requirements for auxiliary data structures, it still expects the input to be present in memory. In this section we present pGreedy , a MapReduce algorithm that uses rfGreedy as its basic building block. We draw our inspiration from [4, 6, 7], in particular adopting a variation of the out-degree bucketing approach in order to split the input into chunks that can be handled by rfGreedy . The algorithm is presented in Algorithm 4. We split the input according to a sequence of k points p 1 &lt; . . . &lt; p tition [1 ,  X  X  into ranges [ p 1 , p 2 ) , . . . , [ p k  X  1 input sets whose current out-degree is within a particular range are processed by rfGreedy . Ranges are processed a more general bucketing schema than p j = p j  X  1 as found in [4,6,7], as this allows us to fine-tune the overall execution time of the algorithm by selecting appropriate buckets that balance the execution times of each individual node. We also note that our algorithm selects sets differently from [6] and [7] in a non-trivial way. Specifically, it is possible for a set to be selected (on line 13 in Algorithm 4) at iteration L = j , even if the number of new elements it covers is smaller than the lower bound p j for the particular bucket. Thus, the Algorithm 5 s pGreedy Set Cover approximation Require: a family F of subsets S j of U = { 1 , . . . , n } Require: a sequence of partition points p 1 &lt; . . . &lt; p 1: A =  X  3: for L = k.. 1 do 4: S L = { S  X  F | | S  X  C |  X  p L } 5: while C 6 =  X  and S L 6 =  X  do 6: pick a set T  X  S L 7: S L = S L \{ T } 8: if | T  X  C |  X  p L then 9: A = A  X  { T } 10: C = C \ T 11: return A algorithm tends to treat each set within the bucket similarly, w hile in previous approaches sets whose cardinalities were closer to the lower bound of their respective buckets would tend to be placed more easily at a lower bucket. Using rf-Greedy as a building block also allows us to separate the bucketing from the approximation guarantee. In our algo-rithm, bucketing is present such that it splits the input into chunks that can fit in memory (space trade-off), while the approximation ratio of the overall algorithm is still dictated by rfGreedy (time trade-off.) In Lemma 3.5 we show that pGreedy maintains the same approximation ratio as rf-Greedy .

Lemma 3.5. pGreedy is a ( q ln  X +1) -approximation al-gorithm when q  X  min j p j p
P roof. The proof follows that of lemma 3.4. We note that since the heap H allows the examination of the sets in S
L in non-increasing order, the arguments of lemma 3.4 still hold even though H does not contain all sets of F . 3.5 s pGreedy
In this section we present spGreedy , a further simplifi-cation of our MapReduce algorithm. We observe that, if c an be achieved without the overhead of a max-heap. This is because any set selected during the processing of range [ p timum set that could have been selected at that point. This algorithm is more similar to the algorithms in [4,6] but dif-fers from the one in [7]. In the latter, sets are placed in buckets that are stored on disk. When a set is rejected (as in our case happens on line 8 in Algorithm 5,) it is moved to a lower bucket. In our approach, we simply ignore the set altogether, as it will be placed at the appropriate parti-tion of [1 ,  X  X  during the next MapReduce cycle (specifically, during the next iteration of L .) In Lemma 3.6, we bound the approximation ratio of spGreedy based on its parti-tion points. If the partition points are selected as powers of a factor p , Theorem 3.7 formally states the algorithm X  X  approximation ratio and MapReduce iterations.

Lemma 3.6. spGreedy is a ( p ln  X  + 1) -approximation algorithm where p = max j p j p
P roof. The proof follows that of lemma 3.4. We note that although there is no heap to maintain an order on the Algorithm 6 L ayered Set Cover Require: a family F of subsets of U = { 1 , . . . , n } Require: a subset L 0  X  F Require: a depth D 1: A =  X  2: C = F 3: compute families L 1 , .., L D by Breadth-First traversal 4: for I = D.. 1 do 5: SC I = L I  X  C 6: for J = ( I  X  1) .. 0 do 7: compute Set Cover SC J of SC J +1 8: A = A  X  SC 0 9: remove all covered sets from C 10: return A sets, if a set S i s selected on line 6 in algorithm 5 at iter-ation L = j , then there does not exist a set S  X  for which | S ous iteration.

Theorem 3 .7. Let p j = p j  X  1 . Then spGreedy is a ( p ln  X +1) -approximation algorithm that completes in log MapReduce iterations.
We now describe implementation details for algorithms pGreedy and spGreedy , which use the Hadoop Pipes API and are implemented in C++11. Set elements are repre-sented as integers from a contiguous range [1 ..m ]. A com-mon building block that is utilized by both the mappers and the reducers is the representation of a cover. Cover C is represented by a bit-vector which is stored as a binary file BV in HDFS. The value of the i -th bit in BV denotes the presence of element i in C .
 Each iteration of the algorithms corresponds to a different MapReduce job. At the beginning of each iteration, BV is copied in parallel onto the local storage of all map nodes. During the map phase, BV is read-only memory-mapped. The map phase is a filter on the sets, allowing only those sets to pass that contain a specific number of undiscovered elements.

During the reduce phase, BV is copied in memory and is updated as per the algorithms X  specifications. The reducer emits a sequence of set IDs and updates the memory copy of BV to reflect the updates on C . Upon termination, BV is stored in HDFS, ready to be redistributed to all the map-pers in the next iteration. BV is comprised of m bits, and therefore is comparatively small. For instance, a 10 10 sets input only requires 2.25GB of space for BV .
A natural variation of Set Cover requires each element in U to be covered by more than a specific number k &gt; 1 of sets. We observe that our results extend to this variation. However, in order to support the multicover semantics, each bit in BV needs to be replaced with a modulo-k counter which requires  X  log( k + 1)  X  bits. For small values of k this may still yield a practical solution.
Our results can be extended to the Weighted Set Cover variation (with a caveat: all ln  X  terms would be relaxed to Figure 1: Pictorial description of the Layered Set C over algorithm. (a) during the first phase, a set of  X  X ood X  nodes is obtained via simple filter-based heuristics as L 0 . Subsequent layers contain the nodes obtained via a Breadth-First traversal; (b) during the second phase, the resulting seed set is incrementally constructed via 3 sub-phases, denoted with black, purple and green color respectively. ln m .) This requires the additional transfer of the weight of each set during the map phases. As a minor optimization, we note that if a total ordering on the weights can be im-posed, then an alternative to transferring extra information would be to encode the set X  X  weight via its position in BV .
In this section we describe how our Set Cover heuris-tics can be used to calculate web crawling seed sets. As the authors of [35] note, a natural formulation for the prob-lem of obtaining seed sets from a web corpus is Max k -Cover , where each set input S j corresponds to a node n j in the graph, and contains as elements all nodes that can be reached from n j by a D -level deep breadth-first traversal. This approach however, is applicable to very small datasets and is impossible to implement at scale, as the input quickly blows up to an unmanageable size where each set includes many millions of elements.
 Instead, we propose an alternative formulation based on Set Cover . Given a corpus IN , a subset L 0 of IN based on some quality criteria and a depth D , we perform a D -level deep Breadth-First traversal, thus obtaining node sets L , L 2 , .., L D . In the context of set covering, a node n corresponds to a set that includes all nodes that are pointed to by the outlinks of n j . We then obtain a set cover SC of L D as a subset of L D  X  1 , followed by a set cover SC of SC D  X  1 as a subset of L D  X  2 . Eventually we obtain SC which is part of the final seed set. If a D -depth Breadth-First traversal were to be executed from SC 0 , at least all nodes in L D would be discovered. We then remove all covered nodes and repeat the algorithm. Since all nodes at level D have been covered, we only traverse the graph until depth D  X  1. We continue in this fashion until all nodes in L 1 have been covered. The Layered Set Cover algorithm is depicted in Figure 1 and formally presented in Algorithm 6.
We explore the performance of our MapReduce Set Cover heuristic as well as our Layered Set Cover heuristic. We Figure 2: Performance of spGreedy as a function of a pproximation factor p . The resulting cover sizes for p = 1 . 2 and p = 2 are 7,655,425,042 and 7,421,503,788 respectively. draw our results on a 1,500-node cluster whose nodes com-prise 2x Intel E5-2620 processors and 64GB of memory (see Table 1.) Our dataset is a large subset of the crawled web. It includes 20.6 Billion pages and approximately 918 Billion outlinks. The average out-degree is 44.63 while the maxi-mum out-degree is limited to 5,000. The disk footprint of the graph is 12TB, stored in 1,000 part files on HDFS (see Table 2.)
Subsequently, we examine the sensitivity of the algorithm on the approximation parameter p by obtaining covers for the whole graph for p = 1 . 2 and p = 2. We show the runtime performance of the algorithm for the same parameters.
Following that, we describe the first experiment that ex-ploits our Set Cover heuristic. We show that it is possible to start from an extremely small subset of the graph (11M pages) and discover the majority of the graph within two BFS hops.

We then examine the performance of a seed list gener-ated by our Layered Set Cover heuristic, by comparing it against a high-quality, user-generated seed list. We show that the Set Cover -based seed list yields 3 . 84 times more pages, 4 hops away from the seed list.

Finally, we report that our Layered Set Cover -based seed list resulted in a significant lift of the median of a PageRank-based metric by 2 . 32 x over all pages discovered.
In Figure 2 we present the cardinality of the resulting covers of the spGreedy algorithm as a function of the ap-CPUs per Node 2x Intel(R) Xeon(R) CPU E5-2620
Memory per Node 64GB Figure 3: Relative execution times of spGreedy for p = 1 . 2 and p = 2 . The algorithm completes all iter-ations in 1150 and 3006 minutes respectively. proximation factor p , for p = 1 . 2 and p = 2. For a value of p that tends to 1, the algorithm will perform exactly as Greedy , thereby allowing us to draw comparisons with the classic in-memory heuristic even though it is not possible to execute it for inputs of this size. We observe that the result-ing cover sizes are almost identical (they are within 2% of each other.) This result shows that the algorithm is stable for different small values of p .
In Figure 3 we present the execution times of all the in-dividual iterations of spGreedy for p = 1 . 2 and p = 2. We observe that the performance of the algorithm is significantly better for p = 1 . 2, a fact that may seem counter-intuitive at first, as the approximation guarantee is tighter in this case. However this difference is easily explained: As p tends to 1, the aggregate output of the mappers is smaller and thus, the single reducer is presented with a smaller amount of infor-mation to process. Moreover, the more tuples the reducer is called to process, the higher the chances that subsequent tuples will be rejected as they fail to pass the bucket mem-bership test. From an engineering perspective, the optimum scenario occurs when the time spent to complete all map-pers is equal to the time spend on the single reducer, on average across all iterations. This equality is well approxi-mated when p = 1 . 2. We note that with our heuristic, it is indeed possible to cover a significant subset of the web graph within only 19 hours. Moreover, our algorithm scales essen-tially linearly with the input graph size and can be applied to larger graphs as well.
We ran Algorithm 6 on the input graph for D = 4. A total of 10 executions of spGreedy were performed. We used the seed set result A as a starting point for a depth-4 Breadth-First traversal. Similarly, we performed a depth-4 traversal starting from a manually-curated, high-quality seed set of similar size to A . We present the results in Figure 4. We observe a significant increase of the frontier size as [5] G. E. Blelloch, R. Peng, and K. Tangwongsan, [6] F. Chierichetti, R. Kumar, and A. Tomkins, [7] G. Cormode, H. Karloff, and A. Wirth,  X  X et Cover [8]  X  X ist of spiders and crawlers. X  [9] S. Bal and R. Nath,  X  X iltering the Web Pages that are [10] M. Gray,  X  X nternet growth and statistics: Credits and [11] O. A. McBryan,  X  X envl and wwww: Tools for taming [12] D. Eichmann,  X  X he rbse spider-balancing effective [13] B. Pinkerton,  X  X inding what people want: Experiences [14] R. T. Fielding,  X  X aintaining distributed hypertext [15] M. Burner,  X  X rawling towards eternity: Building an [16] S. Brin and L. Page,  X  X he anatomy of a large-scale [17] A. Heydon and M. Najork,  X  X ercator: A scalable, [18] M. Najork and A. Heydon, High-performance web [19] A. Z. Broder, M. Najork, and J. L. Wiener,  X  X fficient [20] D. Fetterly, M. Manasse, M. Najork, and J. Wiener, [21] M. R. Henzinger, A. Heydon, M. Mitzenmacher, and [22] M. R. Henzinger, A. Heydon, M. Mitzenmacher, and [23] M. Najork and J. L. Wiener,  X  X readth-first crawling [24] V. Shkapenyuk and T. Suel,  X  X esign and [25] J. Edwards, K. McCurley, and J. Tomlin,  X  X n [26] P. Boldi, B. Codenotti, M. Santini, and S. Vigna, [27] H.-T. Lee, D. Leonard, X. Wang, and D. Loguinov, [28] G. Mohr, M. Stack, I. Rnitovic, D. Avery, and [29] R. Khare, D. Cutting, K. Sitaker, and A. Rifkin, [30] Z. Bar-Yossef and M. Gurevich,  X  X andom sampling [31] C. Olston and M. Najork,  X  X eb crawling, X  [32] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, [33] A. Ntoulas, J. Cho, and C. Olston,  X  X hat X  X  new on [34] A. Dasgupta, A. Ghosh, R. Kumar, C. Olston, [35] S. Zheng, P. Dmitriev, and C. L. Giles,  X  X raph-based [36] J. Dean and S. Ghemawat,  X  X apreduce: Simplified [37] H. Karloff, S. Suri, and S. Vassilvitskii,  X  X  model of [38] L. Page, S. Brin, R. Motwani, and T. Winograd,  X  X he
