 . Consider the general SVM classier model in which, given n training examples f ( x i ;y i ) g n primal problem consists of solving the follo wing problem: where l denotes a loss function over labels y i 2f +1 ; 1 g and the outputs o i on the training set. The machine' s output o for any example x is given as o = w ( x ) b = P n the i are the dual variables, b is the threshold parameter and, as usual, computations involving are handled using the kernel function: k ( x;z ) = ( x ) ( z ) . For example, the Gaussian kernel is given by The regularization parameter C and kernel parameters such as comprise the vector h of hyperpa-rameters in the model. h is usually chosen by optimizing a validation measure (such as the k -fold cross validation error) on a grid of values (e.g. a uniform grid in the (log C; log ) space). Such a grid search is usually expensi ve. Particularly , when n is lar ge, this search is so time-consuming that one usually resorts to either def ault hyperparameter values or crude search strate gies. The prob-lem becomes more acute when there are more than two hyperparameters. For example, for feature weighting/selection purposes one may wish to use the follo wing ARD-Gaussian kernel: where t = weight on the t th feature. In such cases, a grid based search is ruled out. In Figure 1 (see section 5) we sho w contour plots of performance of an SVM on the log C log plane for a real-world binary classication problem. These plots sho w that learning performance beha ves  X nicely X  as a function of hyperparameters. Intuiti vely , as C and are varied one expects the SVM to smoothly transition from pro viding undertting solutions to overtting solutions. Given that this phenomenon seems to occur routinely on real-w orld learning tasks, a very appealing and principled alternati ve to grid search is to consider a dif ferentiable version of the performance validation function and invoke non-linear gradient-based optimization techniques for adapting hyperparameters. Such an approach requires the computation of the gradient of the validation function with respect to h . Chapelle et al. (2002) give a number of possibilities for such an approach. One of their most promis-ing methods is to use a dif ferentiable version of the lea ve-one-out (LOO) error . A major disadv an-tage of this method is that it requires the expensi ve computation and storage of the inverse of a kernel sub-matrix corresponding to the support vectors. It is worth noting that, even if, on some lar ge scale problems, the support vector set is of a manageable size at the optimal hyperparameters, the corresponding set can be lar ge when the hyperparameter vector is away from the optimal; on man y problems, such a far-of f region in the hyperparameter space is usually tra versed during the adaptation process! We highlight the contrib utions of this paper . (1) We consider dif ferentiable versions of validation-set-based objecti ve functions for model selec-tion (such as k -fold error) and give an efcient method for computing the gradient of this function with respect to h . Our method does not require the computation of the inverse of a lar ge kernel sub-matrix. Instead, it only needs a single linear system of equations to be solv ed, which can be done either by decomposition or conjug ate-gradient techniques. In essence, the cost of computing the gradient with respect to h is about the same, and usually much lesser than the cost of solving (1) for a given h . (2) Our method is applicable to a wide range of validation objecti ve functions and SVM models that may involv e man y hyperparameters. For example, a variety of loss functions can be used together with multiclass classication, regression, structured output or semi-supervised SVM algorithms. (3) Lar ge-scale empirical results sho w that with BFGS optimization, just trying about 10-20 hy-perparameter points leads to the determination of optimal hyperparameters. Moreo ver, even as compared to a ne grid search, the gradient procedure pro vides a more precise placement of hy-perparameters leading to better generalization performance. The benet in terms of efcienc y over the grid approach is evident even with just two hyperparameters. We also sho w the usefulness of our method for tuning more than two hyperparameters when optimizing validation functions such as the F measure and weighted error rate. This is particularly useful for imbalanced problems. This paper is organized as follo ws: In section 2, we discuss the general class of SVM models to which our method can be applied. In section 3, we describe our frame work and pro vide the details of the gradient computation for general validation functions. In section 4, we discuss how to develop dif ferentiable versions of several common performance validation functions. Empirical results are presented in section 5. We conclude this paper in section 6. Due to space limitations, several details have been omitted but can be found in the technical report (K eerthi et al. (2006)). In this section, we discuss the assumptions required for our method to be applicable. Consider SVM classication models of the form in (1). We assume that the kernel function k is a continuously dif fer entiable function of h . Three commonly used SVM loss functions are: (1) hinge loss; (2) squared hinge loss; and (3) squared loss. In each of these cases, the solution of (1) is obtained by computing the vector that solv es a dual problem. The solution usually leads to a linear system relating and b : where P and q are, in general, functions of h . We mak e the follo wing assumption: Locally around h (at whic h we are inter ested in calculating the gradient of the validation function to be dened soon) P and q are continuously dif fer entiable functions of h . We write down P and q for the hinge loss function and discuss the validity of the abo ve assumption. Details for other loss functions are similar .
 , c , u , y c , y u , e c , e u , uc , uu etc be appropriately dened vectors and matrices. Then (4) is given by If the partitions I 0 , I c and I u do not change locally around a given h then assumption 2 holds. Generically , this happens for almost all h .
 The modied Huber loss function can also be used, though the deri vation of (4) for it is more comple x than for the three loss functions mentioned abo ve. Recently , weighted hinge loss with asymmetric mar gins (Grandv alet et al., 2005) has been explored for treating imbalanced problems. and C i = C , m i = m if y i = 1 . Because C + and C are present, the hyperparameter C in (1) can be omitted. The SVM model with weighted hinge loss has four extra hyperparameters, C + , C , m + and m , apart from the kernel hyperparameters. Our methods in this paper allo w the possibility of efciently tuning all these parameters together with kernel parameters.
 The method described in this paper is not special to classication models only . It extends to a wide class of kernel methods for which the optimality conditions for minimizing a training objecti ve function can be expressed as a linear system (4) in a continuously dif ferentiable manner 1 . These include man y models for multiclass classication, regression, structured output and semi-supervised learning (see Keerthi et al. (2006)). Suppose that for the purpose of hyperparameter tuning, we are given a validation scheme involving a small number of (training set, validation set) partitions, such as: (1) using a single validation set, (2) k -fold cross validation, or (3) averaging over k randomly chosen (training set, validation set) partitions. Our method applies to any of these three schemes. To keep notations simple, we explain the ideas only for scheme (1) and expand on the other schemes towards the end of this section. Note that throughout the hyperparameter optimization process, the training-v alidation splits are x ed. Let f ~ x l ; ~ y l g ~ n an element of a validation set with an element of the training set. The output on the l th validation example is ~ o l = P where is a vector containing and b , and l is a vector containing y i ~ K li , i = 1 ;:::;n and 1 as the last element (corresponding to b ). Let us suppose that the model selection problem is formulated as a non-linear optimization problem: next section, we will outline the construction of such functions for criteria lik e error rate, F measure etc. We now discuss the computation of r h f . Let denote a generic parameter in h and let us represent partial deri vative of some quantity , say v , with respect to as _ v . Before writing down expressions for _ f , let us discuss how to get _ . Dif ferentiating (4) with respect to gives No w let us write down _ f . where _ ~ o l is obtained by dif ferentiating (6): The computation of _ in (8) is the most expensi ve step, mainly because it requires P 1 . Note that, for hinge loss, P 1 can be computed in a some what cheaper way: only a matrix of the dimension of
I u needs to be inverted. Ev en then, in lar ge scale problems the dimension of the matrix to be inverted can become so lar ge that even storing it may be a problem; even when lar ge storage is possible, the inverse can be very expensi ve. Most times, the effecti ve rank of P is much smaller than its dimension. Thus, instead of computing P 1 in (8), we can instead solv e for _ approximately using decomposition methods or iterati ve methods such as conjug ate-gradients. This can impro ve efcienc y as well as tak e care of memory issues by storing P only partially and computing the remaining parts of P as and when needed. Since the right-hand-side vector ( _ q _ P ) in (11) changes for each dif ferent with respect to which we are dif ferentiating, we need to solv e (11) for each element of h . If the number of elements of h is not small (say , we want to use (3) with the MNIST dataset which has more than 700 features) then, even with (11), the computations can still remain very expensi ve.
 We now give a simple trick that sho ws that if the gradient calculations are re-or ganized, then ob-taining the solution of just a single linear system suf ces for computing the full gradient of f with i.e., Using (10) and plugging the expression for _ from (8) into (9) gives where d is the solution of The beauty of the reor ganization in (13) is that d is the same for all variables in h about which the dif ferentiation is being done. Thus (14) needs to be solv ed only once. In concurrent work (See ger, 2006) has used a similar idea for kernel logistic regression.
 As a word of caution, note that P may not be symmetric. See, e.g., the P arising from (5) for the hinge loss case. Also, the parts corresponding to zero components should be omitted from calculations and the special structure of P should be utilized,e.g., for hinge loss when computing _ P the parts of _ P corresponding to 0 (see (5)) can be ignored. The linear system in the abo ve equation can be efciently solv ed using conjug ate gradient techniques.
 The sequence of steps for the computation of the full gradient of f with respect to h is as follo ws. First compute l from (12). For various choices of validation function, we outline this computation in the next section. Then solv e (14) for d . Then, for each use (13) to get all the deri vatives of f . The computation of _ P has to be performed for each hyperparameter separately . In problems with man y hyperparameters, this is the most expensi ve part of the gradient computation. Note that in some cases, e.g., = C , _ P is immediately obtained. For = or t , when using (2,3), one can cache pairwise distance computations while computing the kernel matrix. We have found (see section 5) that the cost of computing the gradient of f with respect to h to be usually much less than the cost of solving (1) and then obtaining f .
 We can also emplo y the abo ve ideas in a validation scheme where one uses k training-v alidation splits (e.g in k -fold cross-v alidation). In this case, for each partition one obtains the linear system (4), corresponding validation outputs (6) and the linear system in (14). The gradient is simply computed by summing over the k partitions, i.e., _ f = P k the quantities P;q;d etc associated with the k th partition.
 The model selection problem (7) may now be solv ed using, e.g., Quasi-Ne wton methods such as BFGS which only require function value and gradient at a hyperparameter setting. In particular , reaching the minimizer of f too closely is not important. In our implementations we terminate 10 3 j f ( h k ) j , where h k +1 and h k are consecuti ve iterates in the optimization process. A general concern with descent methods is the presence of local minima. In section 5, we mak e some encouraging empirical observ ations in this regard, e.g., local minima problems did not occur for the C; tuning task; for several other tasks, starting points that work surprisingly well could be easily obtained. We consider validation functions that are general functions of the confusion matrix, of the form evaluates to 1 if the l th example is correctly classied and 0 otherwise. Then, tp and fp can be written as tp = P examples in the positi ve and negative classes. The most commonly used validation function is err or rate .
 Err or rate ( er ) is simply the percentage of incorrect predictions, i.e., er = (~ n + tp + fp ) = ~ n . For classication problems with imbalanced classes it is usual to consider either weighted err or rate or a function of precision and recall such as the F measur e .
 Weighted Err or rate ( wer ) is given by wer = (~ n + tp + fp ) = (~ n + + ~ n ) , where is the ratio of the cost of misclassications of the negative class to that of the positi ve class. F measur e ( F ) is the harmonic mean of precision and recall: F = 2 tp= (~ n + + tp + fp ) Alternati vely , one may want to maximize precision under a recall constraint, or maximize the area under the ROC Curve or maximize the precision-r ecall break even point . See Keerthi et al. (2006) for a discussion on how to treat these cases.
 It is common practice to evaluate measures lik e precision, recall and F measure while varying the threshold on the real-v alued classier output, i.e., at any given threshold 0 , tp and fp can be redened in terms of the follo wing, For imbalanced problems one may wish to maximize a score such as the F measure over all values of 0 . In such cases, it is appropriate to incorporate 0 as an additional hyperparameter that needs to be tuned. Such bias-shifting is particularly also useful as a compensation mechanism for the mismatch between training objecti ve function and validation function; often one uses an SVM as the underlying classier even though it is not explicitly trained to minimize the validation function that the practitioner truly cares about. In section 5, we mak e some empirical observ ations related to this point.
 The validation functions discussed abo ve are based on discrete counts. In order to use gradient-based methods smooth functions of h are needed. To develop smooth versions of validation functions, we dene ~ s l , which is a sigmoidal approximation to ~ u l (15) of the follo wing form: where 1 &gt; 0 is a sigmoidal scale factor . In gener al, 0 ; 1 may be functions of the validation out-puts. (As discussed abo ve, one may alternati vely wish to treat 0 as an additional hyperparameter .) The scale factor 1 inuences how closely ~ s l approximates the step function ~ u l and hence controls the degree of smoothness in building the sigmoidal approximation. As the hyperparameter space is probed, the magnitude of the outputs can vary quite a bit. 1 tak es the scale of the outputs into account. Belo w we discuss various methods to set 0 ; 1 .
 We build a dif ferentiable version of such a function by simply replacing ~ u l by ~ s l . Thus, we have f = f (~ s 1 ::: ~ s ~ n ) . The value of l (12) is given by: Figure 1: Performance contours for IJCNN with 2000 training points. The sequence of points gen-erated by Gr ad are sho wn by (best is in red). The point chosen by Grid is sho wn by in red. ( @f=@ ~ s l ) = ( @f=@tp )( @tp=@ ~ s l ) + ( @f=@fp )( @fp=@ ~ s l ) .
 We now discuss three methods to compute the sigmoidal parameters 0 ; 1 . For each of these methods the partial deri vatives of 0 ; 1 with respect to ~ o l can be obtained (K eerthi et al. (2006)) and used for computing (17).
 Dir ect Method. Here, we simply set, 0 = 0 , 1 = t= , where denotes standard deviation of the outputs f ~ o l g and t is a constant which is heuristically set to some x ed value in order to well-approximate the step function. In our implementation we use t = 10 .
 Hyper parameter Bias Method. Here, we treat 0 as a hyperparameter and set 1 as abo ve. Minimization Method. In this method, we obtain 0 ; 1 by performing sigmoidal tting based on unconstrained minimization of some smooth criterion N , i.e., ( 0 ; 1 ) = argmin Lik elihood : N = N nll = P proposed in Chapelle et al. (2002). The probabilistic err or rate : per = P are suitable validation functions which go well with the choice N = N nll . We demonstrate the effecti veness of our method on several binary classication problems. The SVM model with hinge loss was used. SVM training was done using the SMO algorithm. Five fold cross validation was used to form the validation functions. Four datasets were used: Adult , IJCNN , Vehicle and Splice . The rst three were tak en from http://www .csie .ntu.edu.tw/ X cjlin/libsvmtools/datasets/ and Splice was tak en from http://ida.r st.fr aunhofer .de/ X r aetsc h/ . The number of examples/features in these datasets are: Adult : 32561/123; IJCNN : 141691/22; Vehicle : 98528/100; and Splice : 3175/60. For each dataset, training sets of dif ferent sizes were chosen in a class-wise stratied fashion; the remaining examples formed the test set.
 The Gaussian kernel (2) and the ARD-Gaussian kernel (3) were used. For ( C; ) tuning with the Gaussian Kernel, we also tried the popular Grid over a 15 15 grid of values. For C; tuning with the gradient method, the starting point C = = 1 was used.
 Comparison of validation functions. Figure 1 sho ws the contours of the smoothed validation error rate and the actual test error rate for the IJCNN dataset with 2000 training examples on the (log C; log ) plane. Grid and Gr ad respecti vely denote the grid and the gradient methods applied to the ( C; ) tuning task. We used f = er smoothed with the dir ect method for Gr ad . It can be seen that the contours are quite similar . We also generated corresponding contours (omitted) for f = per and f = N nll (see end of section 4) and found that the validation er with the dir ect method better represents the test error rate. Figure 1 also sho ws that the gradient method very quickly plunges into the high-performance region in the ( C; ) space.
 Comparison of Grid and Grad methods. For various training set sizes of IJCNN , in Table 1, we compare the speed and generalization performance of Grid and Gr ad , Clearly Gr ad is much more efcient than Grid . The good speed impro vement is seen even at small training set sizes. Although the efcienc y of Grid can be impro ved in certain ways (say , by performing a crude search follo wed by a rened search, by avoiding unnecessary exploration of dif cult regions in the hyperparame-ter space etc) Gr ad determines the optimal hyperparameters more precisely . Table 2 compares Grid and Gr ad on Adult and Vehicle datasets for various training sizes. Though the generalization performance of the two methods are close, Grid is much slo wer .
 Table 1: Comparison of Grid , Gr ad &amp; Gr ad-ARD on IJCNN &amp; Splice . nf= number of hyperparam-eter vectors tried. (For Grid , nf= 225.) cpu= cpu time in minutes. erate= % test error rate. are as in Table 1. For Vehicle and n trg =16000, Grid was discontinued after 5 days of computation. Featur e Weighting Experiments. To study the effecti veness of our gradient-based approach when man y hyperparameters are present, we use the ARD-Gaussian kernel in (3) and tune C together with all the t 's. As before, we used f = er smoothed with the dir ect method . The solution for Gaussian kernel was seeded as the starting point for the optimization. Results are reported in Table 1 as Gr ad-ARD where cpu denotes the extra time for this optimization. We see that Gr ad-ARD achie ves signicant impro vements in generalization performance over Gr ad without increasing the computational cost by much even though a lar ge number of hyperparameters are being tuned. Maximizing F-measur e by thr eshold adjustment. In section 4 we mentioned about the possi-ble value of threshold adjustment when the validation/test function of interest is a quantity that is dif ferent from error rate. We now illustrate this by taking the Adult dataset, with F measur e . The size of the training set is 2000. Gaussian kernel (2) was used. We implemented two methods: (1) we set 0 = 0 and tuned only C and ; (2) we tuned the three hyperparameters C , and 0 . We ran the methods on ten dif ferent random training set/test set splits. Without 0 , the mean (standard deviation) of F measure values on 5-fold cross validation and on the test set were: 0.6385 (0.0062) and 0.6363 (0.0081). With 0 , the corresponding values impro ved: 0.6635 (0.0095) and 0.6641 (0.0044). Clearly , the use of 0 yields a very signicant impro vement on the F-measure. The ability to easily include the threshold as an extra hyperparameter is a very useful adv antage for our method. Optimizing weighted err or rate in imbalanced problems. In imbalanced problems where the proportion of examples in the positi ve class is small, one usually minimizes weighted error rate wer (see section 4) with a small value of . One can think of four possible methods in which, apart from the kernel parameter and threshold 0 (we used the Hyperpar ameter bias method for smoothening), we include other parameters by considering sub-cases of the weighted hinge loss model (see section 2)  X  (1) Usual SVM: Set m + = m = 1 , C + = C , C = C and tune C . (2) Set m + = m = 1 , C + = C , C = C and tune C . (3) Set m + = m = 1 and tune C + and C treating them as independent parameters. (4) Use the full Weighted Hing e loss model and tune C + , C , m + and m . To compare the performance of these methods we took the IJCNN dataset, randomly choosing 2000 training examples and keeping the remaining examples as the test set. Ten such random splits were tried. We tak e = 0 : 01 . The top half of Table 3 reports weighted error rates associated with validation and test. The weighted hinge loss model performs best. Table 3: Mean (standard deviation) of weighted ( = 0 : 01 ) error rate values on the IJCNN dataset. The presence of the threshold parameter 0 is important for the rst three methods. The bottom half of Table 3 gives the performance statistics of the methods when threshold is not tuned. Interestingly , for the weighted hinge loss method, tuning of threshold has little effect. Grandv alet et al. (2005) also mak e the observ ation that this method appropriately sets the threshold on its own. Cost Br eak-up. In the gradient-based solution process, each step of the optimization requires the evaluation of f and r h f . In doing this, there are three steps that tak e up the bulk of the computa-tional cost: (1) training using the SMO algorithm; (2) the solution of the linear system in (14); and (3) the remaining computations associated with the gradient, of which the computation of _ P in (13) is the major part. We studied the relati ve break-up of the costs for the IJCNN dataset (training set sizes ranging from 2000 to 32000), for solution by Gr ad and Gr ad-ARD methods. On an average, the cost of solution by SMO forms 85 to 95% of the total computational time. Thus, the gradient computation is very cheap. We also found that the _ P cost of Gr ad-ARD doesn' t become lar ge in spite of the fact that 23 hyperparameters are tuned there. This is mainly due to the efcient reusage of terms in the ARD-Gaussian calculations that we mentioned in section 4. The main contrib ution of this paper is a fast method of computing the gradient of a validation func-tion with respect to hyperparameters for a range of SVM models; together with a nonlinear optimiza-tion technique it can be used to efciently determine the optimal values of man y hyperparameters. Ev en in models with just two hyperparameters our approach is faster and offers a more precise hy-perparameter placement than the Grid approach. Our approach is particularly of great value for lar ge scale problems. The ability to tune man y hyperparameters easily should be used with care. On a text classication problem involving man y thousands of features we placed an independent feature weight for each feature and optimized all these weights (together with C ) only to nd severe overtting taking place. So, for a given problem it is important to choose the set of hyperparameters carefully , in accordance with the richness of the training set.
 S. S. Keerthi, V. Sindhw ani and O. Chapelle. An efcient method for gradient-based adaptation of hyperparameters in SVM models. Technical Report, 2006.
 O. Chapelle, V. Vapnik, O. Bousquet and S. Mukherjee. Choosing multiple parameters for support vector machines. Mac hine Learning , 46:131 X 159, 2002.
 Y. Grandv alet, J. Mari  X  ethoz and S. Bengio. A probabilistic interpretation of SVMs with an applica-tion to unbalanced classication. NIPS, 2005.
 J. Platt. Probabilities for support vector machines. In Advances in Lar ge Mar gin Classier s . MIT Press, Cambridge, Massachusetts, 1999.
 M. See ger . Cross validation optimization for structured Hessian kernel methods. Tech. Report, MPI
