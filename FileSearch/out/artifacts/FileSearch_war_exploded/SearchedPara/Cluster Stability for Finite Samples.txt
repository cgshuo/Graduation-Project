 Clustering is one of the most common tools of unsupervised data analysis. Despite its widespread use and an immense amount of literature, distressingly little is known about its theoretical founda-tions [14]. In this paper, we focus on sample based clustering, where it is assumed that the data to be clustered are actually a sample from some underlying distribution.
 A major problem in such a setting is assessing cluster validity. In other words, we might wish to know whether the clustering we have found actually corresponds to a meaningful clustering of the underlying distribution, and is not just an artifact of the sampling process. This problem relates to the issue of model selection, such as determining the number of clusters in the data or tuning parameters of the clustering algorithm. In the past few years, cluster stability has received growing attention algorithm is repeatedly applied over independent samples, resulting in  X  X imilar X  clusterings, then these clusterings are statistically significant. Based on this idea, several cluster validity methods have been proposed (see [9] and references therein), and were shown to be relatively successful for various data sets in practice.
 However, in recent work, it was proven that under mild conditions, stability is asymptotically fully determined by the behavior of the objective function which the clustering algorithm attempts to optimize. In particular, the existence of a unique optimal solution for some model choice implies stability as sample size increase to infinity. This will happen regardless of the model fit to the data. From this, it was concluded that stability is not a well-suited tool for model selection in clustering. This left open, however, the question of why stability is observed to be useful in practice. In this paper, we attempt to explain why stability measures should have much wider relevance than measure of generalization, in a learning-theoretic sense. When we have a  X  X ood X  model, which is stable over independent samples, then inferring its fit to the underlying distribution should be easy. In other words, stability should  X  X ork X  because stable models generalize better, and models which generalize better should fit the underlying distribution better. We emphasize that this idea in itself is not novel, appearing explicitly and under various guises in many aspects of machine learning. The novelty in this paper lies mainly in the predictions that are drawn from it for clustering stability. The viewpoint above places emphasis on the nature of stability for finite samples. Since generaliza-tion is meaningless when the sample is infinite, it should come as no surprise that stability displays similar behavior. On finite samples, the generalization uncertainty is virtually always strictly posi-tive, with different model choices leading to different convergence rates towards zero for increasing sample size. Based on the link between stability and generalization, we predict that on realistic data, all risk-minimizing models asymptotically become stable, but the rates of convergence to this ultimate stability differ. In other words, an appropriate scaling of the stability measures will make them independent of the actual sample size used. Using this intuition, we characterize and prove a mild set of conditions, applicable in principle to a wide class of clustering settings, which ensure the relevance of cluster stability for arbitrarily large sample sizes. We then prove that the stability measure used in previous work to show negative asymptotic results on stability, actually allows us to discern the  X  X orrect X  model, regardless of how large is the sample, for a certain simple setting. Our results are further validated by some experiments on synthetic and real world data. We assume that the data sample to be clustered, S = { x 1 , .., x m } , is produced by sampling instances i.i.d from an underlying distribution D , supported on a subset X of R n . A clustering C D for some D  X  X is a function from D  X  D to { 0 , 1 } , defining an equivalence relation on D with a finite number of equivalence classes (namely, C D ( x i , x j ) = 1 if x i and x j belong to the same cluster, and 0 otherwise). For a clustering C X of the instance space, and a finite sample S , let C X | S denote the functional restriction of C X on S  X  S .
 A clustering algorithm A is a function from any finite sample S  X  X , to some clustering C X of has some user-defined parameters  X  . In particular, A k denotes the algorithm A with the number of clusters chosen to be k .
 Following [2], we define the stability of a clustering algorithm A on finite samples of size m as: where S 1 and S 2 are samples of size m , drawn i.i.d from D , and d D is some  X  X issimilarity X  function between clusterings of X , to be specified later.
 Let ` denote a loss function from any clustering C S of a finite set S  X  X  to [0 , 1] . ` may or may not correspond to the objective function the clustering algorithm attempts to optimize, and may involve a global quality measure rather than some average over individual instances. For a fixed sample size, we say that ` obeys the bounded differences property (see [11]), if for any clustering C S it holds that instance of S by any other instance from X , and clustering it arbitrarily.
 A hypothesis class H is defined as some set of clusterings of X . The empirical risk of a clustering C X  X  H on a sample S of size m is ` ( C X | S ) . The expected risk of C X , with respect to samples S of size m , will be defined as E S ` ( C X | S ) . The problem of generalization is how to estimate the expected risk, based on the empirical data. The relationship between generalization and various notions of stability is long known, but has been dealt with mostly in a supervised learning setting (see [3][5] [8] and references therein). In the context of unsupervised data clustering, several papers have explored the relevance of statistical stability and generalization, separately and together (such as [1][4][14][12]). However, there are not many theoretical results quantitatively characterizing the relationship between the two in this setting. The aim of this section is to informally motivate our approach, of viewing stability and generalization in clustering as closely related.
 Relating the two is very natural in a Bayesian setting, where clustering stability implies an  X  X nsur-prising X  posterior given a prior, which is based on clustering another sample. Under this paradigm, we might consider  X  X oft clustering X  algorithms which return a distribution over a measurable hypoth-esis class H , rather than a specific clustering. This distribution typically reflects the likelihood of a clustering hypothesis, given the data and prior assumptions. Extending our notation, we have that for any sample S , A ( S ) is now a distribution over H . The empirical risk of such a distribution, with respect to sample S 0 , is defined as ` ( A ( S ) | S 0 ) = E C In this setting, consider for example the following simple procedure to derive a clustering hypothesis distribution, as well as a generalization bound: Given a sample of size 2 m drawn i.i.d from D , we Then we have the following: Theorem 1. For the procedure defined above, assume ` obeys the bounded differences property with parameter 1 /m . Define the clustering distance d D ( P , Q ) in Eq. (1), between two distributions P ,
Q over the hypothesis class H , as the Kullback-Leibler divergence D KL [ Q||P ] 2 . Then for a fixed confidence parameter  X   X  (0 , 1) , it holds with probability at least 1  X   X  over the draw of samples S 1 and S 2 of size m , that The theorem is a straightforward variant of the PAC-Bayesian theorem [10]. Since the loss function is not necessarily an empirical average, we need to utilize McDiarmid X  X  bound for random variables with bounded differences, instead of Hoeffding X  X  bound. Other than that, the proof is identical, and is therefore ommited.
 This theorem implies that the more stable is the Bayesian algorithm, the tighter the expected gen-eralization bounds we can achieve. In fact, the  X  X xpected X  magnitude of the high-probability bound we will get (over drawing S 1 and S 2 and performing the procedure described above) is: E Note that the only model-dependent quantity in the expression above is stab ( A, D , m ) . Therefore, closely related to minimizing stab ( A, D , m ) . In general, the generalization bound might converge to 0 as m  X   X  , but this is immaterial for the purpose of model selection. The important factor is the relative values of the measure, over different choices of the algorithm parameters  X  . In other words, the important quantity is the relative convergence rates of this bound for different choices of  X  , governed by stab ( A, D , m ) .
 This informal discussion only exemplifies the relationship between generalization and stability, since the setting and the definition of d D here differs from the one we will focus on later in the paper. Although these ideas can be generalized, they go beyond the scope of this paper, and we leave it for future work. From now on, following [2], we will define the clustering distance function d D of Eq. (1) as: In other words, the clustering distance is the probability that two independently drawn instances from D will be in the same cluster under one clustering, and in different clusters under another clustering.
 rithm X  X  objective function, to which the algorithm converges for asymptotically large samples, then stab ( A, D , m ) converges to 0 as m  X   X  , regardless of the parameters of A . From this, it was concluded that using stability as a tool for cluster validity is problematic, since for large enough samples it would always be approximately zero, for any algorithm parameters chosen.
 However, using the intuition gleaned from the results of the previous section, the different conver-gence rates of the stability measure (for different algorithm parameters) should be more important than their absolute values or the sample size. The key technical result needed to substantiate this intuition is the following theorem: Theorem 2. Let X, Y be two random variables bounded in [0 , 1] , and with strictly positive ex-Y , . . . , Y m be m identical independent copies of X and Y respectively, define  X  X = 1 m and  X  Y = 1 The importance of this theorem becomes apparent when  X  X ,  X  Y are taken to be empirical estimators of stab ( A, D , m ) for two different algorithm parameter sets  X  ,  X  0 . For example, suppose that ac-cording to our stability measure (see Eq. (1)), a cluster model with k clusters is more stable than a These stability measures might be arbitrarily close to zero. Assume that with high probability over d involves an expectation over the unknown distribution D (see Eq. (2)). However, we can estimate them by drawing another sample S 3 of m instance pairs, and computing a sample mean to estimate different convergence rates ( c  X  0 . 01) , which are slower than  X (1 /m ) , then we can discern which number of clusters is more stable, with a high probability which actually improves as m increases. Therefore, we can use Thm. 2 as a guideline for when a stability estimator might be useful for arbi-trarily large sample sizes. Namely, we need to show it is an expected value of some random variable, with at least slightly different convergence rates for different model selections, and with at least some of them dominating  X (1 /m ) . We would expect these conditions to hold under quite general settings, since most stability measures are based on empirically estimating the mean of some ran-dom variable. Moreover, a central-limit theorem argument leads us to expect an asymptotic form of  X (1 / for the theorem to apply. The difficult step, however, is showing that the differing convergence rates can be detected empirically, without knowledge of D . In the example above, this reduces to show-indeed differ by some constant ratio independent of m .
 Proof of Thm. 2. Using a relative entropy variant of Hoeffding X  X  bound [7], we have that for any 1 &gt; b &gt; 0 and 1 / E [ Y ] &gt; a &gt; 1 , it holds that: By substituting the bound D KL [ p || q ]  X  ( p  X  q ) 2 / 2 max { p, q } in the two inequalities, we get: which hold whenever 1 &gt; b &gt; 0 and a &gt; 1 . Let b = 1  X  (1  X  E [ Y ] / E [ X ]) 2 / 2 , and a = Eq. (3), and to both sides of Eq. (4), and after some algebra, we get: As a result, by the union bound, we have that Pr(  X  X  X   X  Y ) is at most the sum of the r.h.s of the last two inequalities, hence proving the theorem.
 As a proof of concept, we show that for a certain setting, the stability measure used by [2], as defined above, is meaningful for arbitrarily large sample sizes, even when this measure converges to zero for any choice of the required number of clusters. The result is a simple counter-example to the claim that this phenomenon makes cluster stability a problematic tool.
 The setting we analyze is a mixture distribution of three well-separated unequal Gaussians in R , to discern whether the data contain 2 , 3 or 4 clusters. We prove that with high probability, this empirical estimation process will discern k = 3 as much more stable than both k = 2 and k = 4 (by an amount depending on the separation between the Gaussians). The result is robust enough to hold even if in addition one performs normalization procedures to account for the fact that higher number of clusters entail more degrees of freedom for the clustering algorithm (see [9]). We emphasize that the simplicity of this setting is merely for the sake of analytical convenience. The proof itself relies on a general and intuitive characteristic of what constitutes a  X  X rong X  model (namely, having cluster boundaries in areas of high density), rather than any specific feature of this setting. We are currently working on generalizing this result, using a more involved analysis.
 The next two lemmas, however, show that the stability measure for k = 3 (the  X  X orrect X  model order) is smaller than the other two, by a substantial ratio independent of m , and that this will be discerned, technical, and appear in the supplementary material to this paper.
 Lemma 1. For some  X  &gt; 0 , let D be a Gaussian mixture distribution on R , with density function Assume  X   X  1 , so that the Gaussians are well separated. Let A k be a centroid-based cluster-ing algorithm, which is given a sample and required number of clusters k , and returns a set of k centroids, minimizing the k-means objective function (sum of squared Euclidean distances between each instance and its nearest centroid). Then the following holds, with o (1) signifying factors which converge to 0 as m  X  X  X  : Lemma 2. For the setting described in Lemma 1, it holds that over the draw of indepen-dent sample pairs ( S 1 , S 2 ) , ( S 0 d and d D ( A 3 ( S 1 ) , A 3 ( S 2 )) , is larger than 2 with probability of at least: It should be noted that the asymptotic notation is merely to get rid of second-order terms, and is not an essential feature. Also, the constants are by no means the tightest possible. With these lemmas, we can prove that a direct estimation of stab ( A, D , m ) , based on a random sample, allows us to discern the more stable model with high probability, for arbitrarily large sample sizes. Theorem 3. For the setting described in Lemma 1, define the following unbiased estimator  X   X  k, 4 m of size m , m and 2 m respectively. Estimate d D ( A k ( S 1 ) , A k ( S 2 )) by computing  X   X  Proof. Using Lemma 2, we have that: Denoting the event above as B , and assuming it does not occur, we have that the estimators  X   X  expected value of  X   X  3 , 4 m is at least twice smaller than the expected values of the other two. More-have that: Combining Eq. (5) and Eq. (6) yield the required result. In order to further substantiate our analysis above, some experiments were run on synthetic and real world data, with the goal of performing model selection over the number of clusters k . Our first experiment simulated the setting discussed in section 4 (see figure 1). We tested 3 different Gaussian mixture distributions (with  X  = 5 , 7 , 8 ), and sample sizes m ranging from 2 5 to 2 22 . For 4, using the k -means algorithm, and repeated this procedure over 1000 trials. Our results show that although these empirical estimators converge towards zero, their convergence rates differ, with approximately constant ratios between them. Scaling the graphs by  X  m results in approximately constant and differing stability measures for each  X  . Moreover, the failure rate does not increase with sample size, and decreases rapidly to negligible size as the Gaussians become more well separated -exactly in line with Thm. 3. Notice that although in the previous section we assumed a large separation between the Gaussians for analytical convenience, good results are obtained even when this separation is quite small.
 For the other experiments, we used the stability-based cluster validation algorithm proposed in [9], which was found to compare favorably with similar algorithms, and has the desirable property of Figure 1: Empirical validation of results in section 4. In each row, the leftmost sub-figure is the 1000 trials), as a function of the sample size, and on the right is the failure rate as a function of the sample size (percentage of trials where  X   X  3 was not the smallest of the three). Figure 2: Performance of stability based algorithm in [9] on 3 data sets. In each row, the leftmost sub-figure is a sample representing the distribution, the middle sub-figure is a log-log plot of the computed stability indices (averaged over 100 trials), and on the right is the failure rate (in detecting the most stable model over repeated trials). In the phoneme data set, the algorithm selects 3 clusters as the most stable models, since the vowels tend to group into a single cluster. The  X  X ailures X  are all due to trials when k = 4 was deemed more stable. with higher stability. The synthetic data sets selected (see figure 2) were a mixture of 5 Gaussians, and segmented 2 rings. We also experimented on the Phoneme data set [6], which consists of 4 , 500 log-periodograms of 5 phonemes uttered by English speakers, to which we applied PCA projection on 3 principal components as a pre-processing step. The advantage of this data set is its clear low-dimensional representation relative to its size, allowing us to get nearer to the asymptotic convergence rates of the stability measures. All experiments used the k -means algorithm, except for the ring data set which used the spectral clustering algorithm proposed in [13].
 Complementing our theoretical analysis, the experiments clearly demonstrate that regardless of the actual stability measures per fixed sample size, they seem to eventually follow roughly constant and differing convergence rates, with no substantial degradation in performance. In other words, when stability works well for small sample sizes, it should also work at least as well for larger sample sizes. The universal asymptotic convergence to zero does not seem to be a problem in that regard. from previous work, cluster stability does not necessarily degrade with increasing sample size. This prediction is substantiated both theoretically and empirically.
 The results also provide some guidelines (via Thm. 2) for when a stability measure might be relevant for arbitrarily large sample size, despite asymptotic universal stability. They also suggest that by appropriate scaling, stability measures would become insensitive to the actual sample size used. These guidelines do not presume a specific clustering framework. However, we have proven their fulfillment rigorously only for a certain stability measure and clustering setting. The proof can be generalized in principle, but only at the cost of a more involved analysis. We are currently working on deriving more general theorems on when these guidelines apply.
 Acknowledgements : This work has been partially supported by the NATO SfP Programme and the PASCAL Network of excellence.

