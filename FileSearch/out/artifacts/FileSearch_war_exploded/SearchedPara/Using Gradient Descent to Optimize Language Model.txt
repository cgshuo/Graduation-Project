 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Theory, Experimentation Keywords: Language modeling, parameter estimation, gra-dient descent.
Most information retrieval models have some set of tun-able parameters. It is often the case that the parameters of such models are either set to default values or tuned in a supervised or unsupervised manner. Historically, many re-trieval models have been tuned to maximize some underlying retrieval metric, such as mean average precision. However, as the number of parameters increases, the direct maximiza-tion techniques become computationally expensive. For this reason, there has been a growing interest in both the infor-mation retrieval and machine learning communities to de-velop parameter estimation techniques that scale well.
Since most information retrieval metrics are non-smooth with respect to model parameters, the machine learning techniques have focused on maximizing or minimizing some surrogate function that attempts to mimic or be highly cor-related with retrieval metrics. Standard optimization tech-niques can then easily be applied to the surrogate function in order to estimate approximate parameters.

There have been, however, few studies from an informa-tion retrieval perspective into how well such surrogate func-tions compare to the direct search approach. Recent work has investigated how the effectiveness of using BM25F pa-rameters estimated by minimizing the RankNet cost func-tion correlates with the effectiveness of an approach that directly maximizes NDCG [2]. The results showed that RankNet acts as a good surrogate and produces reasonable effectiveness when compared to a more computationally ex-pensive direct search technique.

Following up on this work, we wish to explore how to use the RankNet cost function to optimize language modeling smoothing parameters. In addition, we explore how well the parameters learned using RankNet compare to those found by a direct search technique for various metrics that rely on binary relevance judgments, such as mean average precision, binary preference, and precision at 10.

In this section we describe, in general terms, how to esti-mate parameters using direct search and the RankNet cost function.
Given a model with one or more parameters, a set of rel-evance judgments, and a retrieval metric, it is straightfor-ward to implement various algorithms that directly attempt to find the parameter setting that maximizes or minimizes themetric. Oneofthemostna  X   X ve approaches is to search for the global optimum via a brute force search over the en-tire parameter space. Depending on whether the parameter space is bounded and the number of parameters, this may be computationally infeasible.

Techniques such as coordinate ascent (and its variants) may be more efficient, but are not guaranteed to find a global optimum. These techniques are forced to estimate deriva-tives via finite differences or perform line searches, since an-alytical derivates cannot be computed in general.
Rather than directly search within the original, non-smooth retrieval metric space, we may instead optimize a surrogate function. In our work, we choose the RankNet cost function as our surrogate [1, 2]. The RankNet cost function is defined over pairwise preferences. That is, for a given query Q ,we define the set R Q , such that ( d 1 ,d 2 )  X  X  Q implies that doc-ument d 1 should be ranked higher than document d 2 . Given a set of binary relevance judgments, it is easy to construct R
Q by taking the cross product of relevant and non-relevant documents. There are other ways to construct R Q , as well, although we found that we achieved the best results by using all of the pairwise preferences we had available to us.
Once we have defined our pairwise preferences, the RankNet cost is computed according to: where Q is the set of queries, Y = g ( Q ; d 2 )  X  g ( Q ; d g ( Q ; d )isthescoreofdocument d with respect to query Q using the current parameter setting.

In order to minimize C , we perform coordinate descent, which requires us to compute gradients with respect to our model parameters. Given some model parameter  X  ,weap-ply the chain rule in order to compute the gradient of C with respect to  X  as:
Using the RankNet cost function, it is easy to see that  X C is computed as:
Therefore, the final piece that needs to be computed de-pends on the underlying scoring function. In order to analyt-ically compute the partial, we must be able to differentiate our scoring function with respect to each parameter. It is then straightforward to compute  X Y  X  X  according to:
Note that the RankNet cost function does not depend on any specific retrieval metric. Therefore, RankNet will always learn the same parameters for a given set of pairwise preferences, regardless of the metric we wish to optimize for.
We wish to estimate the parameters for the language mod-eling query likelihood ranking function using two-stage smooth-ing [3]. The two-stage smoothing estimate, which has two parameters, generalizes both Dirichlet and Jelinek-Mercer smoothing, and therefore makes for a good general pur-pose language modeling ranking function. A document D is scored against query Q under this model as follows: where  X  and  X  are the smoothing parameters that we wish to estimate. Note that we rank according to the log query likelihood in order to simplify the mathematical derivations.
The partial derivates of the scoring function, with respect to  X  and  X  , are computed as follows:
In this section we evaluate how the effectivness of language modeling parameters learned by minimizing the RankNet cost function compare to the effectiveness of parameters learned by using direct search. Experiments are carried out on four standard TREC ad hoc retrieval test collections, in-cluding three newswire data sets (AP  X 88 X - X 90, WSJ  X 87- X 92, and the 2004 Robust Track data set), and a large web data set (wt10g). For training purposes, each set of topics is split into a training and test set.

The results are given in Table 1. As the table shows, the outcomes for MAP and BPREF are the same, with direct BPREF Table 1: Test set effectiveness for parameters esti-mated using direct search and RankNet. Optimal effectivness values are also provided as an upper bound. Effectiveness is measured in terms of mean average precision, binary preference, and precision at 10. Italicized values indicate statistically signifi-cant improvements over direct search. Bold values indicate significant improvements over RankNet. search being significantly better than RankNet on the WSJ and ROBUST data sets. In addition, RankNet is signif-icantly worse than optimal for every data set except AP, whereas direct search only differs significantly from optimal for WSJ and ROBUST. The results for P@10 show that RankNet is only significantly worse than direct search for WSJ. Indeed, RankNet appears to be more stable for P@10 than MAP and BPREF.

These results indicate that RankNet is never significantly better than direct search for estimating the two-stage lan-guage modeling parameters. While RankNet often does pro-duce reasonable effectiveness, it is considerably less consis-tent than direct search. This is likely the result of the cost function minimizing a surrogate function that only tends to be correlated with actual retrieval metrics. As pointed out by Taylor et al., it may be possible to improve the consis-tency of RankNet by measuring the retrieval metric on some validation set and using that as a stopping criteria [2].
Therefore, when computationally feasible, direct search still seems to be the most appropriate method for estimating parameters. However, RankNet acts as a good surrogate that scales better and often produces reasonable results. This work was supported in part by the CIIR, in part by NSF grant #CNS-0454018, in part by ARDA and NSF grant #CCF-0205575, and in part by Microsoft Live Labs. Any opinions, findings and conclusions or recommendations ex-pressed in this material are those of the author(s) and do not necessarily reflect those of the sponsor.
