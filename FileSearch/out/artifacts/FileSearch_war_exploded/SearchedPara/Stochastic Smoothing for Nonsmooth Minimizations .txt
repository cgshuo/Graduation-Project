 Hua Ouyang houyang@cc.gatech.edu Alexander Gray agray@cc.gatech.edu College of Computing, Georgia Institute of Technology Nonsmoothness is a central issue in machine learn-ing computation, as many important methods min-imize nonsmooth convex functions. For example, using the nonsmooth hinge loss yields sparse sup-port vector machines; regressors can be made ro-bust to outliers by using the nonsmooth absolute loss other than the squared loss; the l 1-norm is widely used in sparse reconstructions. In spite of the at-tractive properties, nonsmooth functions are theoreti-cally more difficult to optimize than smooth functions ( Nemirovski &amp; Yudin , 1983 ). In this paper we focus on minimizing nonsmooth functions where the func-tions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning).
 Smoothness and strong-convexity are typically certifi-cates of the existence of fast global solvers. Nesterov X  X  deterministic smoothing method ( Nesterov , 2005b ) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods ( Nesterov , 2004 ) can be applied. It tions. If a nonsmooth function is strongly convex, this rate can be improved to O (1 /t 2 ) using the excessive gap technique ( Nesterov , 2005a ).
 In this paper, we extend Nesterov X  X  smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions. Combin-ing this with a stochastic version of the optimal gradi-ent descent method, we introduce and analyze a new algorithm named A ccelerated N onsmooth S tochastic G radient D escent ( ANSGD ), for a class of functions that include the popular ML methods of interest. To our knowledge ANSGD is the first stochastic first-order algorithm that can achieve the optimal O (1 /t ) rate for minimizing nonsmooth loss func-tions without Polyak X  X  averaging ( Polyak &amp; Juditsky , 1992 ). In comparison, the classic SGD converges in O (ln t/t ) for nonsmooth strongly convex functions ( Shalev-Shwartz et al. , 2007 ), and is usually not ro-bust ( Nemirovski et al. , 2009 ). Even with Polyak X  X  averaging ( Bach &amp; Moulines , 2011 ; Xu , 2011 ), there are cases where SGD X  X  convergence rate still can not be faster than O (ln t/t ) ( Shamir , 2011 ). Numerical experiments on real-world datasets also indicate that ANSGD converges much faster in comparing with these state-of-the-art algorithms.
 A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization ( Duchi et al. , 2011 ). This work achieves similar iter-ation complexities as ours, in a parallel computation scenario.
 In machine learning, many problems can be cast as minimizing a composition of a loss function and a reg-ularization term. Before proceeding to the algorithm, we first describe a different setting of  X  X omposite min-imizations X  that we will pursue in this paper, along with our notations and assumptions. 1.1. A Different  X  X omposite Setting X  In the classic black-box setting of first-order stochastic algorithms ( Nemirovski et al. , 2009 ), the structure of the objective function min x { f ( x ) = E f ( x , ) :  X  P } is unknown. In each iteration t , an algorithm can only access the first-order stochastic oracle and obtain a subgradient f  X  ( x , t ). The basic assumption is that f ( x ) = E f  X  ( x , ) for any x , where the random vector is from a fixed distribution P .
 The composite setting (also known as splitting box model. It was proposed to exploit the structure of objective functions. Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities ( Daubechies et al. , 2004 ; Beck &amp; Teboulle , 2009 ; Nesterov , 2007a ). Stochas-tic variants have also been proposed recently ( Lan , Hu et al. , 2009 ; Xiao , 2010 ). A stochastic compos-ite function  X ( x ) := f ( x ) + g ( x ) is the sum of a smooth stochastic convex function f ( x ) = E f ( x , ) and a nonsmooth (but simple and deterministic) func-tion g (). To minimize  X , previous work construct the model iteratively:  X  X  X  f ( x t , t ) , x  X  x t  X  + 1 mal function (typically a Bregman divergence) and  X  t is a stepsize.
 A successful application of the composite idea typi-cally relies on the assumption that the above model is nuclear norm, it is straightforward to obtain the mini-mum in analytic forms. However, this assumption does not hold for many other applications in machine learn-ing, where many loss functions (not the regularization term, here the nonsmooth g () becomes the nonsmooth loss function) are nonsmooth, and do not enjoy sepa-rability properties ( Wright et al. , 2009 ). This includes important examples such as hinge loss, absolute loss, and  X  -insensitive loss.
 In this paper, we tackle this problem by studying a new where loss function f () is convex and nonsmooth, while g () is convex and L g -Lipschitz smooth: g ( x )  X  in this paper we focus on unconstrained minimiza-tions. Without loss of generality, we assume that both f () and g () are stochastic: f ( x ) = E f ( x , ) and g ( x ) = E g ( x , ), where has distribution P . If either one is deterministic, its is then dropped. To make our algorithm and analysis more general, we assume that g () is  X  -strongly convex:  X  x , y , g ( x ) convex, one can simply take  X  = 0.
 The main idea of our algorithm again stems from ex-ploiting the structures of f () and g (). In Section 2 we propose to form a smooth stochastic approxima-tion of f (), such that the optimal methods ( Nesterov , 2004 ) can be applied to attain optimal convergence rates. The convergence of our proposed algorithm is analyzed in Section 3 , and a batch-to-online conver-sion is also proposed. Two popular machine learning problems are chosen as our examples in Section 4 , and numerical evaluations are presented in Section 5 . All proofs are provided in a longer version of this paper 1 . 2.1. Stochastic Smoothing Method An important breakthrough in nonsmooth minimiza-tion was made by Nesterov in a series of works ( Nesterov , 2005b ; a ; 2007b ). By exploiting function structures, Nesterov shows that in many applications, minimizing a well-structured nonsmooth function f ( x ) can be formulated as an equivalent saddle-point form where u  X  R m , U  X  R m is a convex set, A is a linear operator mapping R D  X  R m and Q ( u ) is a continuous convex function. Inserting a non-negative  X  -strongly convex function  X  ( u ) in ( 1 ) one obtains a smooth ap-proximation of the original nonsmooth function where  X &gt; 0 is a fixed smoothness parameter which is crucial in the convergence analysis. The key property of this approximation is: Lemma 1. ( Nesterov , 2005b )(Theorem 1) Function  X  f ( x , X  ) is convex and continuously differentiable, and its gradient is Lipschitz continuous with constant L Nesterov X  X  smoothing method was originally proposed for deterministic optimization. A major drawback of this method is that the number of iterations N must be known beforehand, such that the algorithm can set a proper smoothness parameter  X  = O sure convergence. This makes it unsuitable for algo-rithms that runs forever, or whose number of itera-tions is not known. Following his work we propose to extend this smoothing method to stochastic opti-mization. Our stochastic smoothing differs from the deterministic one in the operator A and smoothness parameter  X  , where both will be time-varying. We assume that the nonsmooth part f ( x , ) of the stochastic composite function  X () is well structured, i.e. for a specific realization t , it has an equivalent form like the max function in ( 1 ): where A t is a stochastic linear operator associated with t . We construct a smooth approximation of this function as:  X  f ( x , t , X  t ) := max where  X  t is a time-varying smoothness parameter only associated with iteration index t , and is independent of t . Function  X  () is non-negative and  X  -strongly con-vex. Due to Lemma 1 ,  X  f ( x , t , X  t ) is  X  A t  X  2 smooth. It follows that Lemma 2.  X  x , y ,t , E  X  f ( x , , X  t )  X  E  X  f ( y , , X  E  X  X  X   X  f ( y , , X  We have the following observation about our compos-ite objective  X (), which relates the reduction of the original and approximated function values.
 Lemma 3. For any x , x t ,t ,
 X ( x t )  X   X ( x )  X  E where D U := max u  X  X   X  ( u ) . 2.2. Accelerated Nonsmooth SGD ( ANSGD ) We are now ready to present our algorithm ANSGD (Algorithm 1 ). This stochastic algorithm is obtained by applying Nesterov X  X  optimal method to our smooth surrogate function, and thus has a similar form to that of his original deterministic method ( Nesterov , 2004 )(p.78). However, our convergence analysis is more straightforward, and does not rely on the con-cept of estimate sequences. Hence it is easier to iden-tify proper series  X  t , X  t , X  t and  X  t that are crucial in achieving fast rates of convergence. These series will be determined in our main results (Thm. 1 and 2 ). Algorithm 1 Accelerated Nonsmooth Stochastic Gra-dient Descent (ANSGD) INPUT: series  X  t , X  t , X  t  X  0 and 0  X   X  t  X  1;
OUTPUT: x t +1 ; 0. Initialize x 0 and v 0 ; for t = 0 , 1 , 2 ,... do 2.  X  f t +1 ( x )  X  max end for To clarify our presentation, we use Table 1 to list some notations that will be used throughout the paper.  X   X  f Our convergence rates are based on the following main lemma, which bounds the progressive reduction  X  t of the smoothed function value. Actually Line 1, 3, and 4 of Alg. 1 are also derived from the proof of this lemma. Lemma 4. Let  X  t be monotonically decreasing. Apply-ing algorithm ANSGD to nonsmooth composite func-tion  X () , we have  X  x and  X  t  X  0 ,  X  t pq + where p :=  X   X  t +1 ( y t )  X  and q :=  X  X  X   X  f t +1 ( y t ) +  X  3.1. How to Choose Stepsizes  X  t In the RHS of ( 7 ), nonnegative scalars p,q  X  0 are data-dependent, and could be arbitrarily large. Hence we need to set proper stepsizes  X  t such that the last two terms in ( 7 ) are non-positive. One might conjecture that: there exist a series c t  X  0 such that It is easy to verify that if we take  X  t = t + retain a tight bound, we take Taking expectation on both sides of ( 7 ) and noticing that E due to Jensen X  X  inequality, we have Lemma 5.  X  x and  X  t  X  0 ,
E + The optimal convergence rates of our algorithm differs according to the fact of  X  (positive or not). They are presented separately in the following two subsections, where the choices of  X  t , X  t , X  t will also be determined. 3.2. Optimal Rates for Composite When  X  = 0, g () is only convex and L g -Lipschitz smooth, but not assumed to be strongly convex. Theorem 1. Take  X  t = 2 t +2 ,  X  t +1 =  X  t ,  X  t = L g  X  stant. We have  X  x and  X  t  X  0 , E [ X ( x t +1 )  X   X ( x )] ( t + 2) 2 In this result, the variance bound is optimal up to a constant factor ( Agarwal et al. , 2012 ). The domi-nating factor is still due to the stochasticity, but not affected by the nonsmoothness of f (). Taking the pa-rameter  X  =  X /D 0 , this last term becomes 2 This bound is better than that of stochastic gradi-ent descent or stochastic dual averaging ( Dekel et al. , 2010 ) for minimizing L -Lipschitz smooth functions, whose rate is O function g (), our bound is of the same order as it, keeping in mind that our rate is for nonsmooth mini-mizations. This fact underscores the potential of using stochastic optimal methods for nonsmooth functions. The diminishing smoothness parameter  X  t = 2 t +2 indi-cates that initially a smoother approximation is pre-ferred, such that the solution does not change wildly due to the nonsmoothness and stochasticity. Eventu-ally the approximated function should be closer and closer to the original nonsmooth function, such that the optimality can be reached. Some concrete exam-ples are given in Fig. 1 .
 The E  X  A  X  2 in our bound is a theoretical constant. In Sec. 4 we demonstrate a sampling method, and it turns out to work quite well in estimating E  X  A  X  2 . 3.3. Nearly Optimal Rates for Strongly When  X &gt; 0, g () is strongly convex, and the conver-gence rate of ANSGD can be improved to O (1 /t ). Theorem 2. Take  X  t = 2 t +1 ,  X  t +1 =  X  t ,  X  t = L g  X  We have  X  x and  X  t  X  0 , E [ X ( x t +1 )  X   X ( x )]  X  where Note that C is the smallest iteration index for which one can retain 1 /t 2 rates for the E  X  A  X  2 part ( B ). Without any knowledge about L g ,  X  and E  X  A  X  2 , one can set a parameter  X  and take  X  t = L g  X  t + 2 serve that one can take  X  fairly large (of O ( E  X  A  X  2 )), meaning that C can be very small (O(1)), and B is O ( 1 t 2 ) for all t . In this sense, strongly convex ANSGD is almost parameter-free. Without the O (1 /t ) rate of D
U , all terms in our bound are optimal. This is why our rate is called  X  X early X  optimal. In practice, D U is usually small, and it will be dominated by the last 3.4. Batch-to-Online Conversion The performance of an online learning (online convex minimization) algorithm is typically measured by re-gret , which can be expressed as where x  X  t := arg min x ing theory literature, many approaches are proposed which use online learning algorithms for batch learn-ing (stochastic optimization), called  X  X nline-to-batch X  (O-to-B) conversions. For convex functions, many of these approaches employ an  X  X veraged X  solution as the final solution.
 On the contrary, we show that stochastic optimization algorithms can also be used directly for online learning. This  X  X atch-to-online X  (B-to-O) conversion is almost free of any additional effort: under i.i.d. assumptions of data, one can use any stochastic optimization algo-rithm for online learning.
 Proposition 1. For any t  X  0 , E  X  where x  X  := arg min x  X ( x ) and x  X  t := arg min x When  X () is convex, the second term in ( 16 ) can be bounded by applying standard results in uni-form convergence (e.g. ( Boucheron et al. , 2005 )):  X  summing up the RHS of ( 11 ), we can obtain an O (  X  vex, the second term in ( 16 ) can be bounded using ( Shalev-Shwartz et al. , 2009 ):  X ( x  X  t , i +1 ) = O (ln t ). Together with summing up the RHS of ( 13 ), an O (ln t ) regret bound is achieved. The O (  X  Using our proposed ANSGD for online learning by B-to-O achieves the same (optimal) regret bounds as state-of-the-art algorithms designated for online learn-ing. However, using O-to-B, one can only retain an O (ln t/t ) rate of convergence for stochastic strongly convex optimization. From this perspective, O-to-B is inferior to B-to-O. The sub-optimality of O-to-B is also discussed in ( Hazan &amp; Kale , 2011 ). In this section, two nonsmooth functions are given as examples. We will show how these functions can be stochastically approximated, and how to calculate pa-rameters used in our algorithm. 4.1. Hinge Loss SVM Classification Hinge loss is a convex surrogate of the 0  X  1 loss. De-note a sample-label pair as := { s ,l }  X  P , where s  X  R D and l  X  R . Hinge loss can be expressed as f hinge ( x ) := max for SVM classifiers where the objective is min  X ( x ) = min E f hinge ( x ) + 2  X  x  X  2 . Note that the regulariza-tion term g ( x ) = 2  X  x  X  2 is  X  -strongly convex, hence according to Thm. 2 , ANSGD enjoys O (1 / (  X t )) rates. the smooth stochastic approximation of hinge loss is This maximization is simple enough such that we can obtain an equivalent smooth representation:  X  f Several examples of  X  f hinge with varying  X  t are plotted in Fig. 1 (left) in comparing with the hinge loss. Here u is a scalar, hence it is straightforward to calculate E  X  A  X  2 , which will be used to generate se-quences  X  t . In binary classification, suppose l  X  { 1 ,  X  1 } . Using definition ( 3 ), one only needs to cal-small subset of k random samples s i (e.g. k = 100), and calculate the sample average of the squared norms  X  estimate of E  X  A  X  2 . 4.2. Absolute Loss Robust Regression Absolute loss is an alternative to the popular squared loss for robust regressions ( Hastie et al. , 2009 ). Us-ing same notations as Sec. 4.1 it can be expressed as f abs ( x ) := smooth stochastic approximation can be expressed as  X  f Solving this maximization wrt u we obtain an equiva-lent form:  X  f abs ( x , t , X  t ) =  X   X   X   X   X  This approximation looks similar to the well-studied Huber loss ( Huber , 1964 ), though they are different. Actually they share the same form only when  X  t = 0 . 5 (green curve in Fig. 1 Right). The parameter E  X  A  X  2 can be estimated in a similar way as in Sec. 4.1 . In this section, five publicly available datasets from various application domains will be used to evaluate the efficiency of ANSGD . Datasets  X  X vmguide1 X ,  X  X eal-sim X ,  X  X cv1 X  and  X  X lpha X  are for binary classifications, and  X  X balone X  is for robust regressions. 2 Following our examples in Sec. 4 , we will evaluate our algorithm using approximated hinge loss for classifica-tions, and approximated absolute loss for regressions. Exact hinge and absolute losses will be used for sub-gradient descent algorithms that we will compare with, as described in the following section. All losses are squared-l 2-norm-regularized. The regularization pa-rameter  X  is shown on each figure. When assuming strong-convexity, we take  X  =  X  . 5.1. Algorithms for Comparison and We compare ANSGD with three state-of-the-art algo-rithms. Each algorithm has a data-dependent tun-ing parameter, denoted by  X  (although they have dif-ferent physical meanings). The best values of  X  are found based on a tuning subset of samples. Note that when assuming strong-convexity, our ANSGD is almost parameter-free. As discussed after Thm. 2 , our experiments indicate that the optimal  X  is taken such that E  X  A  X  2  X   X  1, meaning that one can simply take  X  SGD . The classic stochastic approximation ( Robbins &amp; Monro , 1951 ) is adopted: x t +1  X  x  X   X  only assuming convexity (  X  = 0), we use stepsize  X  the stepsize used in SGD2 ( Bottou ):  X  t = 1 ( t + X ) . Averaged SGD . This is algorithmically the same as SGD , except that the averaged result  X  x := 1 t used for testing. We follow the stepsizes suggested by the recent work on the non-asymptotic analysis of SGD that Polyak X  X  averaging combining with proper step-sizes yield optimal rates. When only assuming convex-ity, we use stepsizes  X  t =  X   X  When assuming strong convexity, the stepsize is taken AC-SA . This approach ( Lan , 2010 ; Lan &amp; Ghadimi , 2011 ) is interesting to compare because like ANSGD , it is another way of obtaining a stochastic algorithm based on Nesterov X  X  optimal method, begging the question of whether it has similar behavior. Theoret-ically, according to Prop.8 and 9 in ( Lan &amp; Ghadimi , 2011 ), the bound for the nonsmooth part is of O (1 / for  X  = 0 and O (1 /t ) for  X &gt; 0. In comparison, our nonsmooth part converges in O (1 /t ) for  X  = 0 and O (1 /t 2 ) for  X &gt; 0. Numerically we observe that di-rectly applying AC-SA to nonsmooth functions results in inferior performances. 5.2. Results Due to the stochasticity of all the algorithms, for each setting of the experiments, we run the program for 10 times, and plot the mean and standard deviation of the results using error bars.
 In the first set of experiments, we compare ANSGD with two subgradient-based algorithms SGD and Aver-aged SGD . Classification results are shown in Fig. 2 , 3 , 4 and 5 , and regression results are shown in Fig. 6 . In each figure, the left column is for algorithms without strongly convex assumptions, while in the right col-umn the algorithms assume strong-convexity and take  X  =  X  . For classification results, we plot function val-ues over the testing set in the first row, and plot testing accuracies in the second row.
 It is clear that in all these experiments, ANSGD  X  X  func-tion values converges consistently faster than the other two SGD algorithms. In non-strongly convex experi-ments, it converges significantly faster than SGD and its averaged version. In strongly convex experiments, it still out performs, and is more robust than strongly convex SGD . Averaged SGD performs well in strongly convex settings, in terms of prediction accuracies, al-though its errors are still higher than ANSGD in the first three datasets. The only exception is in  X  X lpha X  (Fig. 5 ), where Averaged SGD retains higher function values than ANSGD , but its accuracies are contradic-torily higher in early stages. The reason might be that the inexact solution serves as an additional regulariza-tion factor, which cannot be predicted by the analysis of convergence rates.
 In the second set of experiments, we compare ANSGD with AC-SA and its strongly convex version. Results are in Fig. 7 , 8 and 9 . In all experiments our ANSGD significantly outperforms AC-SA, and is much more stable. These experiments confirm the theoretically better rates discussed in Sec. 5.1 . We introduce a different composite setting for non-smooth functions. Under this setting we propose a stochastic smoothing method and a novel stochastic algorithm ANSGD . Convergence analysis show that it achieves (nearly) optimal rates under both convex and strongly convex assumptions. We also propose a  X  X atch-to-Online X  conversion for online learning, and show that optimal regrets can be obtained.
 We will extend our method to constrained minimiza-tions, as well as cases when the approximated function  X  f () is not easily obtained by maximizing u . Nesterov X  X  excessive gap technique has the  X  X rue X  optimal 1 /t 2 bound, and we will investigate the possibility of in-tegrating it in our algorithm. Exploiting links with statistical learning theories may also be promising.
