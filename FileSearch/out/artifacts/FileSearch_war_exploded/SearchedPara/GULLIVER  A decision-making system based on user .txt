 1. Introduction
Virtual reality can provide, in comparison with classical training, many advantages ( Amokrane et al., 2008 ). In the case of navigation, training in virtual environments allows to easily modify environmental conditions (wind, current, etc.), which have impacts environmental conditions. Anothe r advantage of training in virtual reality is the strong coupling between the user and the virtual environment. The virtual world must answer to user's actions in a credible way, so that the user experiences presence in the virtual world thanks to a sensorimotor coupling. This encourages a percep-tual learning ( Waterworth and Waterworth, 2000 ).

We use an informed virtual environment (IVE: environment including knowledge-based models and providing an action/percep-tion coupling) for fl uvial navigation training. Contrary to standard adaptive feedback according to learner interactions. The purpose is based on learner observation. This brings heterogeneous data for making decision. As a common formal framework to express these data with their uncertainty, we use the Dempster  X  Shafer (DS) the utility of uncertainty-based models for decision-making in informed virtual environments. The scienti fi ccontributionofthe paper is a decision-making system based on user observation for an adaptive training in informed virtual environments.
 existing training systems with adaptive feedback and an overview of inference systems for decision-making under uncertainty are presented. Section 3 provides some useful formulas and notations about DS theory, in the context of reasoning with ENCs (Evidential
Network with Conditional belief functions). Next, Section 4 describes our GULLIVER system of adaptive training in IVE based on the inference in ENC. Section 5 presents an experiment of the system with a discussion about the results. 2. Related work 2.1. Decision-making in training systems for adaptive feedback ners. For example, TRUST ( Mellet d'Huart, 2004 ), a truck-driving training simulator, allows the trainer to add explanations/indica-tions which will appear in situ during the simulation to help the learners. In this case, the guidance is automatic but the system is non-adaptive. There is no decision-making system which allows for the choice of different levels of feedbacks. The explanations are prede fi ned and will appear regardless of how the learners are driving. Therefore, they will not be appropriate for every learner; experienced learners will receive too much assistance while we believe that adaptive training systems are necessary and are very much worth their additional complexity. An example of such an adaptive system is PEGASE ( Buche et al., 2010 ), a generic intelligent system for virtual reality learning, which can guide learners according to their errors, their pro fi le and domain knowl-edge. Decision-making in matters regarding feedback guidance is based system with different priorities for each rule. The trainer plays a central role in this process: the rule priorities are computed according to the discretion of the trainer, and the system suggests several kinds of feedbacks which must then be validated by the trainer. The system learns from the trainer's choices and is expected to be able to replace him/her after repeated use. Such a training system is more complex to build than a non-adaptive system but the advantage is that it can properly account for the user. Moreover, it allows for a more generic approach; when changes in the training scenario occur, the choice in feedback does not need to be manually recon because it is based on a decision-making system and not explicitly de fi ned by the trainer. Other training systems with adaptive feedback exist ( Weevers et al., 2003; Lopez-Garate et al., 2008 ) and demonstrate qualities similar to those of PEGASE, i.e. propos-ing adaptive feedback according to information about the learner (e.g. mistakes made) and domain knowledge.

These existing systems all indicate that the most important requirement for decision-making in training is consideration for the learners' errors. The error detection system depends on the type of the task in the training: procedural or non-procedural.
For example, the PEGASE system can only be used for learning procedural tasks, because it requires prior knowledge of the actions that must be completed so that errors may be detected when the user deviates from the procedure. This is also the case for other training systems ( Ferrero et al., 2005; Gerbaud et al., no normalized deterministic procedure that a navigator may follow. Hence, we use a system for learning non-procedural tasks.
Such systems are more complex to build but are appropriate for training of complex tasks. For example, in TELEOS ( Mufti-
Alchawafa, 2008 ), which is a learning environment for orthopaedic surgery, the learners build their knowledge by interacting with the system. The environment gives a learning situation to the learner in order to allow him/her to take decisions to acquire more experience. Therefore, the system must respond pertinently to the learner's actions so as to encourage him/her to think about his/her actions. An in fl uence diagram, based on a Bayesian network, is used for decision-making in this system. In our case, we use a non-procedural approach based on enaction ( Varela, 1979 ). Enactive systems do not have prede fi ned rules about what behavior may be successful and the learner is free to choose how to interact with it. This creates a strong coupling between the user and the system.
With fl uvial navigation, we do not have a normalized procedure of appropriate actions that must be completed; instead, we can only know if an action is wrong. Errors are mainly detected according to a predictive model (the future position of the ship determined by a physics engine) and simple navigational rules. For example, if the future position of the ship indicates that it will navigate under a is an error. Because a predictive model is used for error detection, there exists uncertainty which must be taken into account by the decision-making system. In the case of procedural task learning, there is no uncertainty in error detection because the system knows exactly what the learner has to do step-by-step and each deviation from the procedure is an error. In our case, if it is detected that the ship will crash into a bridge in 2 min, then the learner might be in the process of making a mistake. The shorter the delay before the collision is, the more certain the system can be that the learner is making this mistake. In addition, physiolo-gical sensors are used to detect learner's state (for example the stress level with a heart rate variability sensor), which gives uncertain data about the user's state due to the sensor reliability and uncertainty of data interpretation. 2.2. Inference systems for decision-making under uncertainty
As explained in the previous section, the decision-making system must take uncertainty into account. All uncertain data about learner observation have to be expressed in a common formal framework so as to allow decision-making. We base ourselves on the Dempster-Shafer (DS) theory ( Shafer, 1976 ). Unlike the theory of probability, the DS theory allows for explicit modeling of ignorance, which is useful in our case since we can have incomplete data about the actual situation of the learner. For example, some sensors can be defective or unused (e.g. every learning platform does not have all sensors) and thus some data about the learner will be incomplete. To represent in fl uences between variables (e.g. variables about learner's errors and about possible kinds of feedback to avoid these errors) and to reason on these variables, directed graphs are widely used ( Ben Yaghlane and Mellouli, 2008 ). In the case of probabilistic inference, Bayesian networks (BNs) are used ( Pearl, 1988 ). With belief functions, the equivalent network is called Evidential Network with Conditional belief functions (ENCs) ( Smets, 1993; Xu and Smets, 1994 ). ENCs have been generalized by DEVN (Directed EVidential Network with conditional belief functions) to have n-ary relations between variables (ENCs are limited to binary relations) ( Ben Yaghlane et al., 2003 ). In our case, relations between variables are only binary because it is easier to specify ( Smets, 1993 ) and to update, so we use an ENC in our system. Moreover, to be more intuitive, we represent knowledge by using conditional belief functions unlike joint belief functions as in valuation networks ( Shenoy, 1994 ), since it is more natural and easier to express knowledge in a causal form ( Pearl, 1988 ). Contrary to ENC, BNs need experimental data to be initialized (to compute conditional probabilities) ( Hu et to A and B, whereas in an ENC one has to specify the belief of C conditionally to A and the belief of C conditionally to B (and then apply a combination rule to fuse these two results). When the number of variables increases, conditional probabilities in BN cannot be simply speci fi ed or updated by hand. 3. Decision-making with conditional belief functions
In this section, some de fi nitions and notations in the Transfer-able Belief Model (TBM) ( Smets and Kennes, 1994 ) are brie presented.
 De fi nition 1. Let  X  be a fi nite set called the frame of discernment.  X  is the domain relative to the variable X . A basic belief  X 
The belief mass m  X  S  X  A  X  represents the belief of the source S in the fact  X   X  A A  X  , where  X  is the real state of the system observed ( Ramasso et al., 2007 ). 3.1. Combination rules
Data fusion is used to enhance decision-making. To combine heterogeneous data coming from several sources using bba, combination rules are used. Two of them are presented in the next two de fi nitions.

De fi nition 2. Let two distinct bba m  X  same frame of discernment  X  . The sources S 1 and S 2 are supposed to be reliable and distinct ( Smets, 2007 ). The TBM conjunctive rule of combination (CRC) of m  X  S 1 and m  X  S 2 is de fi ned as follows: 8 A D  X  ; m  X  S 1 S 2  X  A  X  X  X  m  X  S 1 m  X  S 2  X  X  A  X  X   X 
With the CRC, the resulting bba can have not a null mass on the empty set, which represents the con fl ict degree of the belief sources. If these belief sources are distinct and at least one of them is reliable (without being able to quantify the reliability and knowing which source is reliable), the disjunctive rule of combi-nation (DRC) must be used ( Smets, 1993 ).

De fi nition 3. Let two distinct bba m S 1 and m S 2 de fi ned on the same frame of discernment  X  . The sources S 1 and S 2 are supposed to be distinct and one of them is reliable. The disjunctive rule of combination (DRC) of m S 1 and m S 2 is de fi ned as follows: 8 A D  X  ; m  X  S 1 S 2  X  A  X  X  X  m  X  S 1 m  X  S 2  X  X  A  X  X   X  3.2. Discounting
Applying the DRC on bba, when belief sources are not reliable, produces decreasingly informative bba ( Ramasso et al., 2007 ). When it is possible to quantify the reliability of a source S , the
CRC can be used after applying a discounting ( Mercier et al., 2008 ) on the bba coming from S .

De fi nition 4. The Shafer discounting of a bba m  X  source S which has a reliability of 1  X  is de fi ned as follows ( Shafer, 1976 ):  X  m  X   X  m  X  ( 3.3. Conditioning
To represent knowledge about in fl uences between variables, conditional beliefs are used.

De fi nition 5. Let m  X  be a bba about the frame of discernment .

The conditional bba given B D  X  is de fi ned by the following unnormalized rule of conditioning ( Smets, 1993 ): m
 X   X  A j B  X  X  theorem ( Smets, 1993 ) to compute the belief of A with the knowledge of A given B and an a priori on B .

Theorem 1. Suppose there exists some a priori belief m  X  0 from the belief induced by the conditional bba m  X   X  : j  X 
Smets (1993) m  X   X   X   X  X   X  belief given sets ( Smets, 1993 ), the following formula, applying
DRC on a conditional bba, can be used in (6) : m  X   X   X  j  X   X  X  3.4. Decision-making with pignistic probabilities ( Smets, 2005 ) of a bba.
 De fi nition 6. The pignistic probability function BetP f m of the bba m  X  is de fi ned as follows ( Smets, 2005 ): BetP f m  X  g X   X   X  X   X  the highest pignistic probability ( Ramasso et al., 2007 ). 3.5. Knowledge representation by an evidential network with conditional belief functions where ( Smets (1993); Xu and Smets (1994); Ben Yaghlane et al. (2003); Ben Yaghlane and Mellouli (2008) )
The model of evidential network with conditional belief func-tions can be seen as a representation of Ben Yaghlane et al. (2003) :
A knowledge base : the nodes and arcs of the directed acyclic graph qualitatively represent the in fl uences between the vari-ables. These in fl uences are also quantitatively represented by the conditional bba associated with each arc. The knowledge base is a priori known.

A fact base : the root nodes represent the new observation introduced in the network. The belief of these variables is propagated in the network. 4. Inference in ENC: the GULLIVER system for an adaptive training in informed virtual environments
The GULLIVER system (GUiding by visuaLization metaphors for fl uviaL navigation training in Informed Virtual EnviRonments) is a decision-making system which interprets data coming from user observation to infer the best feedback to display. Knowledge is represented in an ENC which is also used to propagate belief for decision-making.

The global system operates as follows. The fl uvial navigation simulator SimNav computes the position, the direction and the speed of the barge controlled by the learner thanks to the boat controls associated ( Fig. 2 ). From these data, the position of the barge is updated in the IVE (Informed Virtual Environment).
Actions (boat movements) and events (collisions, etc.) are transmitted by the IVE to the user's activity detection module. Information about the learner's gestures is also transmitted to this module which is in charge of detecting the mistakes and the risks made by the learner. The learner's state (stress level, cognitive load, etc.) is also recognized using data coming from physiological sensors. For example, we use a heart rate sensor to determine the learner's stress level ( Fig. 3 ).

From the learner's state, his/her mistakes and the risks taken, the decision-making module based on an ENC activates the appropriate level of assistance to guide the learner. This module can also decide to trigger events. For example, if the learner does not make mistakes and feels at ease, the environment will increase in complexity by adding some dangers, for instance fl oating objects to avoid or a thick fog.

In addition to the learner's state, his/her mistakes and the risks level (novice, experienced, etc.). If the learner is a novice, the guiding system must adapt to a cognitive speed compatible with the learner's perception and comprehension speed to avoid a cognitive overload ( Bottecchia, 2010 ).
 Fig. 4 shows an example of two levels of feedbacks that the GULLIVER system can choose to display. The two screenshots represent the same place on a river but for two learners with a different level. On the left screenshot, the system detected that the learner made several errors and did not feel at ease. Therefore, several aids were triggered by the system to help the learner in their navigation. When this learner will be more comfortable with fl uvial navigation, some aids will disappear. In this case, if the learner makes many errors after removing some aids, they will automatically reappear. On the right screenshot, the system detected no navigation errors and the learner seemed to be at User X  X  state tracking User X  X  gestures tracking ease. Hence, the system did not display some aids but some dif fi culties: a thick fog and dead trunks to avoid. 4.1. Initialization of the ENC: new beliefs deduced from user observation AnextractoftheENCusedinGULLIVERsystemispresentedin This allows initialization of the ENC before propagating information.
For example, the belief on the variable stress is determined by a heart rate sensor located on the user's ear. Indeed, it was demon-in bba as a function of the heart rate was determined intuitively according to the user's heart rate at rest (60 bpm).

As the heart rate sensor has a reliability of 80%, a discounting is applied on the bba. For example, if the heart rate is 105 bpm, then the stress bba is 8 &gt; &lt; &gt; : 8 &gt; &lt; &gt; : computed as a function of the future position of the boat. The more imminent the collision is, the more certain the collision is.
The computation of the bridge _ collision bba as a function of the time to collision was determined intuitively ( Fig. 7 ). The future position of the boat is determined up to 90 s in the future. If there because the physical engine can reliably compute the future position of the boat. For example, if the boat will collide a bridge in 18 s, then the bridge _ collision bba is 8 &gt; &lt; &gt; : 4.2. Information propagation in ENC beliefs must be propagated to the decision nodes ( Fig. 5 )to determine what assistance might be useful considering the cur-rent learner's situation. An arc between two variables represents an in fl uence of the parent variable on its child. These in are represented by conditional bba, which are the translation of simple rules. The simplicity is a choice made so that the system can be modi fi ed by fl uvial navigation trainers. For example, we have the rules: next bridge The two  X  50%  X  represent the degree of relevance of the rules. translated by Table 1 .

With Table 1 and the stress bba computed in the previous section, the belief can be propagated to the node  X  hide the next bridge  X  by using the formula (6) . We obtain m stress  X  useful  X  X  0 : 2 m stress  X  notuseful  X  X  0 m 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :
Similarly, we have the rule  X  if the boat is about to collide with a about to collide with a bridge, we cannot conclude anything on the usefulness of hiding the next bridge. This is translated by Table 2 .
With Table 2 and the bridge _ collision bba computed in the previous section, the belief can be propagated to the node the next bridge  X  and we obtain m m m 8 &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; :
In the case of  X  hide the next bridge  X  , there are two parent nodes which bring beliefs ( Fig. 5 )  X  stress  X  and  X  bridge collision resulting beliefs must be combined by using the CRC (formula (2) ) to obtain the fi nal belief on  X  hide the next bridge  X  . We obtain m m m 8 &gt; &lt; &gt; :
In order to classify the aids by order of priority for decision-making, the pignistic probability of the usefulness of each aid/ event (the terminal nodes in Fig. 5 ) is computed (formula (8) ). For example with  X  to hide the next bridge  X  , we obtain 4.3. Choosing the best level of feedback: a constraint satisfaction problem
These kinds of feedbacks (visual and audio aids, events which add complexity the situation, etc.) with the highest pignistic probability are very likely to be displayed, but a fi nal necessary. Indeed, such a kind of feedback must be moderated in order to respect some constraints. For example, displayed feedback should not cause a cognitive overload of the user (displaying of too much assistance at the same time), they have to be mutually compatible (for example it is not possible to simultaneously trigger the rules  X  hide the next bridge  X  and  X  highlight bridge must have a high pignistic probability of usefulness, etc. Some constraints, called strong constraints, de fi nitely have to be respected (for example, these chosen kinds of feedbacks must have a high pignistic probability of usefulness) while others, called weak constraints, can be discarded (although the best solution must respect as many of them as possible). This is a constraint satisfaction problem and it is solved by enumerating every possibility and then by computing a score for each possibility.
A great score is obtained by a solution which respects most of the weak constraints. The enumeration and the evaluation of every possibility are not always possible in a short time (the computa-feedbacks and each one must be activated or not). In this case, a genetic algorithm is used to compute a good solution within 1 s. This delay, determined empirically, is suf fi cient to have a good refresh of the guiding.

In the genetic algorithm, each individual (candidate solution) is feedback. 0 means the kind of feedback is not activated and 1 means it is activated. The population is initialized randomly except some bits that are fi xed by applying the simple strong constraints (for example, some kinds of feedback can be manually set as mandatory by the user). The population is evaluated by applying the con-eliminated. A score is incremented for each weak constraint respected. The individual with the highest score of all generations is the solution chosen. The selection, crossover and mutation operations used in this genetic algorithm are standard. They have not been optimized yet, but this is not very important because a good solution is acceptable, even if it is not the best.
In the experiment detailed in the next section, the genetic algorithm is not used because there are not a lot of available kinds of feedbacks so that the enumeration and the evaluation of every candidate solution can be always computed within less than 1 s. 5. Experimental evaluation 5.1. Objectives
Our work includes user evaluations which use simulated training sessions for inland waterway navigation in the informed virtual environment. Our goal is to demonstrate its value and impact as a pedagogical tool that can guide and coach learners for inland waterway navigation, allowing them to better predict and control the trajectory of the river barge, detect and understand errors, etc.
We hypothesize that training sessions with systems that provide adaptive guidance based on the needs of the user will be more effective as a pedagogical tool than systems with non-adaptive guidance. This increase in performance can be measured quantitatively (e.g., reduction in time required for learning) and qualitatively (e.g., better understanding of the errors that were committed).
 5.2. Context
We make a distinction between guiding the learners and correcting the errors of the learners ( Zachary, 1986 ). A learner is considered to be a cognitive subject actively engaged in the construction of his or her own knowledge ( Fernandes et al., 2003 ). Therefore, the most important characteristic of these training sessions is to augment, rather than encumber, the ability of the learners to actively build their own knowledge. The visual and auditory aids that are given to learners with our system are designed with this in mind. They should rarely provide direct, explicit instruction or replace the need for reasoning and re tion in the learning process.

For example, if a learner is preparing to pass under a bridge archway which is prohibited, the system could call attention to preting the sign and choosing the right bridge archway still remain learning strategy, which is not based on the repetition of the same  X  learning by doing  X  ( Gibbs, 1988; McLaughlin and Rogers, 2010 ), and fundamentally requires that a learner makes an effort in understanding a subject. 5.3. Apparatus
With these previous assumptions in mind, our objective is to validate the informed interaction proposed by the GULLIVER system. In order to do so, the user evaluations are conducted with a system which uses a custom-built hardware interface ( Fig. 3 ), allowing the user to control the river barge (with engine and rudder controls built into the dashboard); a hear-rate sensor, for the purpose of measuring the stress of the learner (used in the
GULLIVER module); an immersive stereoscopic display, which requires that the user wear stereoscopic glasses; a head tracking camera, for recovering the position and orientation of the user's head (allowing the system to understand the general area of the screen that is occupying the user's attention); and several dis-tributed computers which are executing the SimNav simulation, the GULLIVER decision module, the 3D graphics engine, and the input devices on the river barge control.

The system will collect data on the user in real-time so that not only it may evaluate the user throughout the training session, but or expert). These data also allow the system to determine what form of assistance and visual aids are the most effective for the user of the current session with regard to the user's reaction. 5.4. Experimental protocol
This experiment is based on the hypothesis that a training session with guidance that adapts to the behavior of the learner can be more ef fi cient (requiring less time for learning) and more effective (providing a better understanding of errors) than a traditional training session, without adaptive guidance (a size-fi ts-all  X  training session).

The experimental protocol used in the experiments is de fi in Table 3 . We choose to sample subjects from two different populations (P1, P2) so that we may perform both a between-groups and a within-subjects comparative analysis. Each group will perform one of the two tests, one with adaptive guidance and one without. We also designed two different routes in order to reduce the effects of learning by practice (c.f. Fig. 8 ). The is short and relatively easy, with only one bridge, so that learners can become acquainted with the system and waterway navigation.
This route is used as a training session in the context of this experimental design. The second route becomes progressively more dif fi cult and contains four bridges. This route is used in the two experimental conditions. 5.4.1. Population average age of 18.9. 65% were males and 35% were females. Every participant was completely novice with regard to ocean or inland waterway navigation. 5.4.2. Methodology unpaired Student's t-tests as our method of statistical analysis.
This allows us to compare two samples with a high number of subjects. This fi rst requires that we test the variance of the
Student's t-tests, so that we may choose if we should conduct our t-tests with equal or unequal variance. This analysis is done between each experimental condition and population, so that we may determine if the difference between route durations is statistically signi fi cant, with a level of error of 5%. This will allow us to evaluate the ef fi ciency of adaptive and non-adaptive training. 5.4.3. Data gathering objective data. The objective data are stored and calculated by the
GULLIVER decision module. These are the list of actions with controls), the heart rate of each user, the number and duration of aids that are selected by the system and provided to participants, the errors or illegal manoeuvres with regard to navigation rules (infractions of speed limits and one-way passages), and the overall duration of the route.
 separates the results into two different categories with regard to the experimental conditions. This is instrumental in allowing us to evaluate the experimental conditions separately and to determine if there is a statistically signi fi cant difference between adaptive and non-adaptive training.
 based on a scale of 20 points. A better score signi fi es a better execution of the planned route. The 20 points are divided by category and presented in Table 4 .
 data that are
The rate of strong guiding aids (i.e. those that provide a high level of assistance) denotes the percentage of time that the following aids were in execution by the system:
Subjective questionnaires were also distributed to the partici-pants at the end of the test. Each questionnaire contains 21 question 5-point Likert scale questions and 2 open questions, which provide insight regarding the impressions and opinions of the users.

These subjective data are cross-referenced with the objective data to allow us to corroborate and to validate the participant's impressions. The subjective data that are compared between populations in the experimental design are the responsiveness of the system regarding the actions of the user, the degree to which the user felt adequately guided by the system, and the dif the route that was taken. For the analysis of these results between populations, we used the same approach as with the objective data: we conducted a test of variance and performed a student t -test to determine if the differences were statistically signi 5.5. Results
After the objective data are gathered, a score is calculated (as previously described) for each participant for each test which measures the pro fi ciency of the route they executed, such as the motor control, the average of the change in angle produced by the rudder, as well as the average change in angle observed in the trajectory of the river barge (zigzag index), the traf tions in navigation (speed limit infractions, forbidden bridge archways), and the percentage of visual and auditory aids provided by the decision module.
 collected to present the results of our analysis, which is centered around the user's pro fi ciency in predicting and controlling the river barge's manoeuvres, and the impact that the adaptive guidance has on the effectiveness of the training session. 5.5.1. Increased capacity for predicting and controlling river barge manoeuvres the river barge while manoeuvring can be analyzed using much of the data from the motor and rudder controls of the river barges, as well as the amount of aid received. The capacity for predicting and controlling river-barge manoeuvres is calculated with the zigzag index. Zigzagging is commonly attributed to the beginners of navigation. It occurs because people who are unfamiliar with the navigation of the river barges will have a tendency to over-compensate their manoeuvres, resulting in many corrective manipulations of the rudder and motor controls.
 signi fi cant differences between and within the two population groups (c.f. Methodology). The bar charts in each fi gure show the average score calculated by the GULLIVER module for the corre-test, The lighter-colored bars represent the training sessions with adaptive guidance, while the darker colored bars represent the training sessions with non-adaptive guidance.
 difference: the participants who underwent a training session with adaptive guidance (lighter-colored bars) used the rudder less in the second training session than they did in the fi rst training session, resulting in a 10% decrease in the zigzag index. In the experimental condition with adaptive guidance, we observed that participants increased their capacity to predict and to control the river barge between the two tests with adaptive guidance. On the other hand, in the experimental conditions with non-adaptive guidance, there was no statistically signi fi cant difference between the fi rst and second tests. 5.5.2. Adaptive guidance and evolution of trainees through the tests
Another interesting result that complements the previous one concerns the performance of the trainee, and the adaptability of the GULLIVER decision module according to the precise needs of each user (presented in Fig. 10 ).

GULLIVER allows us to observe the rate of guiding aids (the percentage of time that the aids were in execution by the system) that each participant receives throughout the training session. In
Fig. 10 , we focus on the strong aids, which go beyond simple indicators and serve to explicitly guide the users. These include the future position of the boat, the map of the route that must be taken, and the highlighting in red the forbidden bridge archways.
In the second test, the participants in the training sessions with adaptive guidance required less assistance in the second run than in the fi rst run, causing a 48% decrease in the rate of aids chosen by the system. Because this decrease was not accompanied by a change in the performance scores, it does not have a negative impact on the results of learning. We suppose that trainees are therefore able to anticipate the navigation since they succeed in the test with 48% less aids. This would be a good argument for adaptive training. Participants construct their representation of the world, constructing knowledge when feedback is provided in real time. They can understand the navigation and then predict what will happen in a given situation. 5.6. Discussion
The key challenge in this experiment on 60 novice participants was to isolate the effect of adaptive feedback in a virtual environment for fl uvial navigation training. Ruder manipulation is a way to measure the anticipation in navigation. The amount of displayed aids is another criteria related to the autonomy of the trainee.

When considering the fi rst principal result i.e. the increase in the participant's capacity to control and manoeuvre the river barge, we believe that the GULLIVER decision module activates the feedback display correctly when a participant needs help and when a participant does not ( Riegler, 2001 ), because the module gives less assistance to participants who demonstrate better rudder control. This is a strong argument to af fi rm that the GULLIVER decision module serves as a valuable pedagogical tool. The result of this pedagogical approach is that the trainee becomes more autonomous and more pro fi cient. This increase in pro ciency is supported by the fi ner control of the rudder.
The second main result, i.e. 48% decrease in the need for visual aids, is extremely positive. We assume that trainees are able to extract from their experience in the virtual environment a knowl-edge about their errors, and that they react with a better con-sciousness when navigating for the second time, compared to the non-adaptive system. This is a strong con fi rmation that adaptive feedback gives a better consciousness of errors to the trainee. Moreover, a priori, some elements lead us to believe that GULLIVER decision module helps the learner gain con fi dence in their actions. According to the results of the subjective question-adaptive aids than with the non-adaptive aids. This is especially important when considering the rate of aids that was provided in each experimental condition ( Fig. 10 ). Indeed, the population that received the higher level of assistance found the route to be more complex. Our hypothesis is that they had less con fi dence in the actions that they were choosing because they never had any feedback regarding their behavior. This could potentially give them the impression that they could not perform at the same level without these aids. Conversely, the participants who received less aid found the route to be easier, which indicates a higher level of con fi dence in their navigation skills
The second point that reinforces this hypothesis is the fact that both populations believe that they received a suf fi cient amount of assistance, while in fact, one population, with the non-adaptive guidance, received much more assistance than the other. With this result in mind, we can question the importance of giving the user the entirety of the available visual and auditory aids, when a system that adapts to the behavior of the user and selects the most We believe that it is probably better to not give learners all the possible aids so that they do not become dependent on this help, especially considering that in real scenarios, participants will have no such navigational assistance.
 6. Conclusion
In this work a decision-making module, GULLIVER, has been designed and evaluated for a fl uvial-navigation virtual training system. GULLIVER determines the most appropriate level of feed-back to display for learner guiding. The module is based on a decision-making model integrating uncertain data coming from observation of the learner by the system. Metaphors of visualiza-tion are displayed to the user in an immersive virtual reality platform as well as audio feedback. This platform allows the tracking of the user actions on boat controls, his/her heart rate and his/her head movement through real time capture. User beliefs which are then propagated in an ENC (evidential network with conditional belief functions) for decision-making about the best guiding to display to the learner. The main advantage of our system is that it can be easily updated by non-expert in computer science thanks to the use of conditional beliefs translated from simple rules. The system can be intuitively initialized by hand and does not need experimental data as for Bayesian networks by example. Another advantage is the genericity of our system which can be applied for other applications which need to display adaptive feedback to the user.

GULLIVER was evaluated as a self-learning system for inland waterway navigation on 60 novice participants. The experiment was based on a navigation case repetition. The results show that the adaptive guidance of the system allows for a more effective training than sessions with non-adaptive guidance in a population of learners who are novice in navigation. Two major results can particularly be highlighted: (i) the increase in the prediction and the control of the river barge after a training with the adaptive feedback, measured through the rudder use and (ii) the drop of 48% less aids when the participants were navigating a second time with the adaptive system, suggesting a better con fi dence in their was particularly well suited for our needs, but we would have bene fi ted from having a delay of several days between the two test runs, which is the future experiment that we plan.

As perspective, we also plan to enhance the system so that it will be auto-adaptive: the system will provide auto-regulation, throughout its use, for the rule relevancies (translated in condi-tional beliefs) and the constraint satisfaction system. Another perspective is to extend this system for other applications like automotive, railroad or aeronautics driving assistance in augmen-ted reality visualization.
 Acknowledgments
This work has been funded by the European Union and Picardie region under the OSE project (fOrmation for fl uvial tranSport with an informed virtual Environment). Europe is committed in Picardie with the European Regional Development Fund. We also deeply thank Alvaro Rodriguez Martin and the enthusiastic students from the University of Technology of Compi X gne for their active support for the experiment.
 References
