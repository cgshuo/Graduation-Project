 Online social networks have become important channels for users to share content with their connections and diffuse information. Al-though much work has been done to identify socially influential users, the problem of finding  X  X eputable X  sharers, who share good content , has received relatively little attention. Availability of such reputation scores can be useful for various applications like rec-ommending people to follow, procuring high quality content in a scalable way, creating a content reputation economy to incentivize high quality sharing, and many more. To estimate sharer reputation, it is intuitive to leverage data that records how recipients respond (through clicking, liking, etc.) to content items shared by a sharer. However, such data is usually biased  X  it has a selection bias since the shared items can only be seen and responded to by users con-nected to the sharer in most social networks, and it has a response bias since the response is usually influenced by the relationship be-tween the sharer and the recipient (which may not indicate whether the shared content is good). To correct for such biases, we propose to utilize an additional data source that provides unbiased goodness estimates for a small set of shared items, and calibrate biased social data through a novel multi-level hierarchical model that describes how the unbiased data and biased data are jointly generated accord-ing to sharer reputation scores. The unbiased data also provides the ground truth for quantitative evaluation of different methods. Ex-periments based on such ground-truth data show that our proposed model significantly outperforms existing methods that estimate so-cial influence using biased social data.
 Categories and Subject Descriptors: H.2.8 [Database Manage-ment] : Database Applications  X  Data mining General Terms: Algorithms, experimentation.
 Keywords: Sharer reputation, Influential users
Social networks have made it seamless to share content among connected users, or simply called connections . A large fraction of publishers allow easy sharing of content on major social networks like Facebook, LinkedIn, Twitter and others by instrumenting their pages with social network  X  X uttons X . Usually these content sharing events, which we shall refer to as shares , are broadcasted to user X  X  first degree connections on the social network. The recipients can respond to the broadcasted content through clicks or social gestures like re-shares, likes and comments. Such social gestures may fur-ther propagate content to the connections of the respondents. For example, the network update stream (NUS) on the homepage of LinkedIn (shown in Figure 1) provides each user with a stream of content items shared by her connections (among other types of up-dates), and she can respond to a shared item by clicking the item to see the content, clicking the  X  X hare X  or  X  X ike X  button to broadcast the item further to her connections, or commenting about the item. We call such data social response data .
 Sharer reputation and item attractiveness. We study the prob-lem of estimating sharer reputation for different content topics. In-tuitively, the reputation of a sharer for a topic represents her ability of sharing  X  X ood X  content on that topic. More precisely, we define sharer reputation as the propensity of a random user interested in the topic (not necessarily connected to the sharer) to respond posi-tively to a typical content item shared by the sharer (e.g., through a click, share or like). For ease of exposition, we call the propensity of a random user to respond positively to a content item the attrac-tiveness of the item. Examples of applications that can leverage such sharer reputation scores are as follows.  X  Recommending reputable sharers for users to follow (e.g., [16]):  X  Selecting high quality content curators (e.g., [12]): Reputable  X  Providing features for item ranking: Items shared by reputable Biases in social response data. Estimating unbiased user reputa-tion scores from social response data described above is difficult. While such data allows us to measure how positive the recipients respond to the items shared by a sharer, it is inherently biased.  X  Selection bias : The content shared by a user is usually only  X  Response bias : The recipients of a shared item know the sharer X  X  Prior work. Most work on user reputation or influence in social networks does not try to correct for biases in social data [25, 28, 22, 21]. One common approach is to construct an influence graph among users based on how information propagates (e.g., response to shared items), and then apply methods such as PageRank [4] or HITS [17] to identify influential nodes in the graph. It should be clear that such an influence graph is directly affected by the selec-tion bias and the response bias. Thus, influential notes tend to result from high network in-degree, or high concentration of activities in a sharer X  X  neighborhood, instead of the attractiveness of shared items. Our experiments (Section 4) also show that such an approach fails to estimate sharer reputation. See Section 2 for a discussion of other related work. Bias removal of social data has been studied in question-answering [26, 6] and comment rating [7] environments, where unbiased scores on a small sample of data were provided by human editors. In this paper, we study a different problem setting (i.e., content sharing) and develop a data-driven approach which does not require human supervision.
 Our approach. We propose to supplement biased social response data with some unbiased user action data that quantifies the attrac-tiveness of a sample of shared items, and estimate unbiased reputa-tion scores by a novel multi-level hierarchical model that describes how the unbiased data and biased data are jointly generated. The unbiased user action data records how users respond (e.g., click) to a sample of shared items that they saw in a way that satisfies the following two important properties:  X  Random user : Those shared items should be seen by a ran- X  Hidden identity : When a user decides to take an action (e.g., Availability of unbiased data. We argue that most social network sites can obtain unbiased user action data through active experi-mentation. Most social network sites either have or can create a content recommendation module on their pages. For example, the LinkedIn Today (LT) module as shown in Figure 1 is such a mod-ule that recommends top content items to users, where items can be recommended to all users and sharer identities are not displayed. One way to collect unbiased data is to randomly display a sam-ple of shared items to a random subset of users and record their responses. We note that, even without active randomization, data directly collected from such a module has much less selection and response biases as long as the recommended items cover a set of typical shared items. We also note that the amount of unbiased data does not need to be large, as long as it covers a set of typical shared items. Reasonable performance can be achieved with a few thousands of typical shared items in our experience.
 Figure 1: Snapshot of LinkedIn homepage. LinkedIn Today (LT) module is in the red solid box and Network Update Stream (NUS) module is in the green dashed box.
 Modeling challenge. Note that, in a sense, the unbiased user action data is  X  X ggregated X  since a user X  X  response to an item in such data does not provide a reputation assessment for a particular sharer, but an aggregated assessment for all of the sharers of that item. On the other hand, the biased social response data is  X  X isaggregated X  since each response is an assessment for an individual sharer. The main challenge is how to jointly model unbiased aggregated user response on a small set of items and a large amount of biased dis-aggregated user response on all shared items to obtain unbiased reputation scores.
 Contributions. We study a novel problem of estimating unbiased sharer reputation scores by combining biased social response data and unbiased user action data through a novel multi-level hierar-chical model. Our model combines information by coupling the latent reputation score in the unbiased data with the latent reputa-tion score in the biased data through a linear regression with sharer specific coefficients. To improve model performance with small amounts of unbiased data, we propose a novel co-sharing Markov random field prior for the reputation scores in the unbiased data. We provide rigorous evaluation of our method through ground truth provided by the unbiased data. We show that our method provides significant improvement compared to existing methods that esti-mate social influence through biased network response data.
Estimating the importance of individual users in terms of infor-mation diffusion [23] has been studied under the topic of finding influential users who significantly affect their neighbors (connec-tions). Following the seminal theory on the  X  X nfluentials X  by Katz and Lazarsfeld [14, 29], the problem of identifying influential users has been studied in several different contexts. Before discussing how our work is different from work on user influence, we first note that influential users usually refer to those who affect their neigh-bors [20], while reputable users in our definition refer to those who receive good response from an anonymous audience.

One of the well studied framework for finding influential users is the influence maximization problem [15] where one chooses a seed set of nodes so that the information from the seed set has maximum reach in the network. While we aim at estimating the reputation (some latent property) for all nodes, the influence maxi-mization problem focuses on picking a small number of important nodes. Also, the methodologies for influence maximization have been illustrated mostly with synthetic data, whereas we use unbi-ased ground-truth data for both training and testing.

Another line of research that has a long history is related to rank-ing the nodes in a network. Examples of such methods include PageRank [4] or HITS [17] that identify important nodes from the network structure [13, 18]. Early studies use just network infor-mation to estimate users X  influence [30], same as PageRank. Re-cently, as researchers realized that diffusion process may be differ-ent from the network structure [5], ground-truth diffusion data as well as network information are used to identify influential users in information diffusion [25]. However, we note that the ground-truth diffusion data may still be heavily influenced by the underlying net-work structure. For example, if two users are not connected in the network, the current data-driven approaches assume that the two users fail to infect each other in any case. Our method can avoid this problem by calibration against network independent data (re-sponses from anonymous crowd). Furthermore, our evaluation is quantitative as we have ground-truth data for the users X  reputation, while the previous methods used manual inspection [22] or quali-tative analysis [25].

Another weakly related line of research are empirical studies that analyze the relationship between the reputation and the behavior of users. Question answering sites adopt reputation scoring system where users who give the right answer earn reputation scores as rewards. [2, 27, 6] studied how reputation scores of a user would affect the quality of answers by the user. Our method to estimate latent reputation could also be helpful in designing more effective reputation scoring systems.
In this section, we describe our novel method for estimating sharer reputation through joint modeling of unbiased aggregated re-sponse (in the unbiased user action data) and biased disaggregated response (in the biased social response data).
We begin with a precise definition of user reputation that we study in this paper.
 Sharer reputation. We define the reputation score of sharer s for topic k as the propensity that a randomly selected user who is  X  X n-terested in X  topic k but not necessarily connected to s would take a positive  X  X ction X  on a random content item shared by sharer s . Typically, a user is interested in multiple topics and for simplicity we assume a user X  X  interest affinity to various topics is known. If this is not the case, methods for identifying users X  topical interests (e.g., [24, 10, 19]) can be applied before estimating user reputation. Based on the data available to us, we use click as our main action. Note that other kinds of actions can be handled in a similar manner.
Our motivation for defining reputation as above is the follow-ing: (1) It makes an explicit connection to a common type of ob-jective in content recommendation, i.e., to maximize user actions on the recommended items. For example, recommending highly scored sharers based on this definition of reputation would maxi-mize clicks by respondents. (2) Reputation scores for sharers with  X  X ufficient X  amount of  X  X nbiased X  data can be computed accurately and provides valuable ground-truth data to evaluate the accuracy of different methods. We will discuss data sufficiency and unbiased-ness in more details later.

We now provide a more detailed description of the two kinds of data we use to estimate reputation scores.
 Biased social response data. On social networking sites, users share items that in turn propagate through their connections to other users. When a recipient sees a shared item together with the sharer X  X  identity, she may respond to the item through clicking, liking, re-sharing, etc., or just ignore it depending on factors like item quality, whether the sharer is a close friend, etc. In our experiments, we use log data collected from the Network Update Steam (NUS) module on the homepage of LinkedIn.

Let z sij  X  { 0 , 1 } indicate user i  X  X  response to item j shared by sharer s . For simplicity, we do not distinguish different types of positive response like clicks, likes, re-shares, and just look at whether there is any positive response. This response data can po-tentially be used to quantify the reputation of the sharer. How-ever, selection and response biases (as defined and discussed in Section 1) need to be corrected.
 Unbiased user action data. To help remove biases in social re-sponse data, we can collect some unbiased user action data in ad-dition to the biased social response data. Similar to social response data, unbiased user action data records users X  response to the items that they saw. However, it needs to satisfy the random user and hidden identity properties defined in Section 1.

Although the unbiased user action data does not have response bias and minimal selection bias, sharing information in some form has to be available to allow estimation of sharer reputation scores. Here, although identities of sharers is not known to the respon-dents, the system knows the set of sharers for each item, it opens the door to combine this information with biased social response data to estimate sharer reputation scores.

Unbiased user action data can be collected through active ex-periments on the site or approximated by log data from some item recommendation application. In our experiments, we take the lat-ter approach. Specifically, we use log data from the LinkedIn To-day (LT) module on the homepage of LinkedIn. We note that this dataset has no response bias and items can be recommended to all users (not just the sharer X  X  connections), but a different kind of slight selection bias may exists since items are not recommended to users randomly  X  the serving scheme uses some notion of ar-ticle popularity. However, because article popularity is typicaly estimated through an explore/exploit algorithm that also ensures some degree of randomization [1], selection bias is weak. See Sec-tion 4 for details of our data. As discussed in Section 1, this kind of nearly unbiased user action data is not unique to LinkedIn and can be made available for many social network sites.

Let y ij  X  { 0 , 1 } indicate user i  X  X  response on item j without knowing who shared item j . In our dataset, each click is a positive action. Other types of actions can also be used. Let J s set of items shared by sharer s and S j denote the set of sharers who shared item j . It is important to note that although such user action data does not have (or is less affected by) the biases found in the social response data, sharing information is available at an  X  X ggregated X  resolution which poses additional challenges:  X  Such unbiased data is usually sparse . For example, in our  X  The attribution of credit for a user action on an item to a sharer Problem definition. Given a set { z sij } of social response data and a set { y ij } of unbiased user action data together with a user interest vector  X  i for each user i , a feature vector x i for each user i and a feature vector w si for each pair of connected users s and i , the goal is to estimate unbiased reputation score  X  sk of each sharer s on each topic k , which intuitively represents the the propensity that a random user who is interested in topic k would take a positive action on a random content item shared by user s .
We now develop our generative model for estimating topic-specific user reputation scores (  X  sk , the reputation of sharer s on topic k ), which are unknown latent factors to be learned from data. The model fitting algorithm will be described in Section 3.3 User action model. For the unbiased user action data, we assume the mean of the binary response y ij for user i on item j is a func-tion of user i  X  X  interest vector  X  ik for different topics k , and the attractiveness p jk of item j for users interested in different topics k . More specifically, where  X  ( x ) = 1 / (1 + e  X  x ) is the sigmoid function and b is a bias term to be learned from data. Here, p jk is a latent factor associ-ated with (item j , topic k ), also to be learned from data. We note that the interest vector  X  ik is assumed to be given or has been ex-tracted from the user X  X  profile and content consumption patterns in a separate process before our modeling.
 Aggregation of user reputation. We connect attractiveness of items to user reputation through modeling the attractiveness p of item j for users interested in different topics k as the average of reputation scores  X  sk of the sharers s  X  X  j of item j ; i.e., where  X  1 is a tuning parameter which represents the strength of a-priori belief that the attractiveness of an item is connected to the reputation scores of the sharers of the item . We note that from the generative point of view, we aggregate user reputation to generate item attractiveness. However, from the estimation point of view, we need to disaggregate attractiveness to obtain user reputation. Co-sharing random-field prior. Recall that the unbiased user ac-tion data is sparse. In particular, it is quite likely that the number of items j in the unbiased data is much smaller than the number of sharers s . This is in fact the case in our LT dataset and poses a challenge when we disaggregate attractiveness of a small number of items to obtain reputation scores for a large number of sharers. One common approach is to shrink all reputation scores to 0 (in-tuitively, when we lack data to estimate a score, make sure it is close to a neutral value 0 and does not overfit the data). This kind of shrinkage does not work well in our scenario (see Section 4.2 for experimental results) since we lack data for most of our shar-ers and hence a large fraction of them will obtain reputation scores close to 0. Hence, we leverage co-sharing data and propose a novel co-sharing Markov random-field prior to regularize the  X  sk The basic idea of our co-sharing random field prior is as follows. In the absence of observed user actions, we assume the reputation score of sharer s is similar to the reputation scores of other users who share the same items as sharer s ; this leads to the following Markov random field prior. (  X  sk |{  X  tk : all sharer t 6 = s } )  X  Normal distribution with To understand this co-sharing random-field prior, note that the neigh-bors of each sharer s are all other sharers who have shared at least one common item. We also see that the weight on a neighbor de-pends not only on the number of co-shared items, but also on the total sharers per common item. If co-sharing happens to an item with many other sharers, the weight is discounted.

We now look at the formulation from a technical perspective; we begin with the prior mean that performs double averaging. For a given item j shared by s , we average the reputation scores  X  other users t who also share item j . Then, we average over all the items shared by s . Note that the actual averaging in the equation is a bit special. For a given item j , instead of dividing the sum of  X  by |S j | X  1 (which is the number of elements in the sum), we di-vide the sum by |S j | and correct this bias using P j  X  X  in the denominator. We choose this kind of averaging because it provides a very clean prior probability density function (see Equa-tion 4). We shall denote the co-sharing Markov random field prior
P ROPOSITION 1. The joint log-prior distribution log(Pr( { p jk } , {  X  sk } ) = log(Pr( { p jk }|{  X  sk } )  X  Pr CSH-MRF ( {  X  is The proof follows by completing squares and performing routine algebraic computations. From the above formula, we can clearly see the roles of tuning parameters  X  1 and  X  2 when we estimate p and  X  sk by maximizing the log-posterior function.  X  1 the strength of interaction between item attractiveness p reputation  X  sk , while  X  2 specifies how strongly we want to shrink user reputation toward 0.

If we have a large amount of unbiased user action data that cov-ers items shared by all users, the above model alone would be suf-ficient. However, unbiased user action data is usually sparse. In particular, it usually covers a small number of items and a small fraction of sharers. Thus, we need to leverage biased social re-sponse data but correct for both the selection and response biases through proper modeling.
 Social response model. In the biased social response data, we as-sume that each response z sij represents whether user i would re-spond positively to item j shared by sharer s , and it is modeled as a function of user i  X  X  interest vector  X  ik and the  X  X ncalibrated reputation score X   X  sk of sharer s on different topics k ; i.e., z sij  X  Bernoulli ( probability =  X  ( P k  X  ik  X  sk +  X  0 where x si is a feature vector including features that can potentially explain the bias between users s and i ,  X  is a vector of regression coefficients to be learned from data, and  X  sk is a latent factor also to be learned from data. Here, we call  X  sk uncalibrated reputation because user behavior in social response data can be quite different from that in the unbiased user action data from which we obtain the unbiased reputation  X  sk .
 Regression-based calibration. We model the relationship between  X  sk and  X  sk through a linear regression, where the regression co-efficients depend on user features; i.e., where x s is a feature vector of sharer s , and  X  k and  X  of topic-specific regression coefficients to be learned from data. Here, we calibrate  X  sk through a linear function and predict the Figure 2: Graphical Representation of our model (variance compo-nents are not shown) Symbol Description
Observation z sij User i  X  X  response to item j shared by sharer s y ij User i  X  X  response on item j
J s The set of items shared by sharer s
S j The set of sharers who shared item j  X  ik User i  X  X  interest in topic k x s Feature vector for sharer s x si Feature vector between sharer s and user i
Variables to be learned  X  sk Unbiased reputation score for sharer s on topic k  X  sk Uncalibrated reputation score for sharer s on topic k p jk Item j  X  X  attractiveness in topic k  X  k ,  X  k Topic-specific regression coefficients  X  Regression coefficients for a bias term for z sij b Bias for y ij unbiased reputation  X  sk by regressing on  X  sk . In this regression,  X  x s is the slope and  X  0 k x s is the intercept. Different users can have different slopes and intercepts depending on their user fea-tures. This provides more flexibility and leads to better perfor-mance.
 Summary. Figure 2 shows the graphical representation of our model. We also summarize our notations in Table 1. We denote the response in unbiased and biased context as Y and Z respec-tively. Representing all unknown parameters as  X  , we summarize our model below.
Although we specify our model using a probabilistic framework, parameter estimation is performed using an optimization approach for the sake of scalability. Our goal is to find the mode of pos-terior distribution Pr ( X  | Y , Z ) . We obtain this by maximizing log Pr( X  | Y , Z ) , which is a non-convex problem but one can ob-tain the mode by using a coordinate ascent approach. We use coor-dinate ascent since the set of conditional maximizations we iterate are standard regression problems and could be solved through read-ily available software.

We have to find the maximum of = argmax  X  log Pr( Y,Z |  X ) + log Pr( X ) = argmax  X  log Pr( Y |  X ) + log Pr( Z |  X ) + log Pr( X ) where the last equality comes from the conditional independence between Y and Z given  X  . Our model specifies Pr( X ) as follows: log Pr( X ) = log Pr( { p jk }|{  X  sk } ) + log Pr CSH-MRF ( {  X  We can express log Pr( X ) as follows: log Pr( X ) =  X   X  1 2 P s P j  X  X 
We develop a coordinate ascent approach to solve Equation 7 ef-ficiently. We note that if we are to solve for one set of variables ( e.g. ,  X  sk s or p jk s) with other sets fixed, then each subproblem becomes a regression problem with regularization. First, we con-sider fitting p and the corresponding bias b by solving the following problem: argmax As  X  ik s are fixed, Y is a logistic function of p jk s and b , and thus this problem is logistic regression with gaussian priors which can be solved efficiently [11].

Second, we aim to fit  X  sk s with other variables fixed. Instead of solving the above problem for all  X  sk s, we can optimize each  X  one at a time by solving the following subproblem for each  X  argmax  X  which is a standard linear regression problem.

Third, we update  X  sk s by solving: From Equation 6, we can show Pr(  X  sk |  X  sk ,  X  k ,  X  k ) is a Gaussian fore, the problem of updating  X  sk s is a logistic regression with Gaussian priors.

Fourth, we update  X  k and  X  k by fitting linear regression for each topic k as given by
We iterate the four steps described above until the likelihood converges. In our experiments, our method converges within less than 10 iterations. After learning the parameters, we output the estimates for user s  X  X  reputation on topic k as the conditional ex-sharers (users who never shared on unbiased context), the estimated reputation score is simply given by (  X   X  0 k x s )  X   X  sk In this section, we illustrate our method using LinkedIn datasets. We compare it with two versions of PageRank and a few special cases of our model. We note that availability of unbiased data gives us the ground truth to quantify the performance of various methods. Dataset description. LinkedIn is the largest professional network in the world with more than 200 million members as of Decem-ber 31, 2012. The unbiased user action data is obtained from the LinkedIn Today module (LT), while the biased (but granular) social response data is obtained from user interactions with the network update stream (NUS). Figure 1 shows a snapshot of the homepage of LinkedIn and the two modules. The data used in our analysis was collected during a four-month period from May 2012 to Au-gust 2012. All unique user identifiers were anonymized.
 LinkedIn Today (LT) dataset. A large part of our LT data comes from a random sample of users, to whom LT randomly recom-mends top algorithmically picked articles that match users X  profiles. During the data collection period, LinkedIn Today recommends 3 items (which are articles shared by some sharers) to each user visit. When a user responds to a recommended item in LT, she does not know who shared the item and, thus, can respond in a more unbi-ased way. Also, LT recommends items to all users (instead of only users connected to the items X  sharers) with randomization; thus, user selection bias is weak. To mitigate bias due to displaying same items to a user multiple times, our response y ij for user i on article j is only confined to first view events; i.e., the first time an article is seen by the user. We also only consider users who clicked at least once during the four-month period.
 Network Update Stream (NUS) dataset. On LinkedIn, an item (article) shared by any sharer s propagates to the connections of s through NUS; the connections of s can see such a sharing event with the sharer X  X  identity in their NUS modules. The connections can respond by clicking the item or ignore the sharing event. Simi-lar to the LT data, we only consider responses to first view events; i.e., z sij represents whether user i clicked item j shared by sharer s when user i saw item j in her NUS module for the first time. Users X  topical interest. In our experiments, the topical interest  X  ik of a user i is obtained through the industries ( e.g. , Internet, Fi-nance, Semiconductor, Communication, and so on) that the user follows or belongs to. On LinkedIn, every user belongs to at least one industry, which is indicated in their profile and can also be inferred through the user X  X  current company. A user can also  X  X ol-low X  a number of industries to obtain article recommendations re-lated to those industries even if she does not belong to them. Using industries to define topics is natural for LinkedIn since it focuses on professional news. Thus, we seek to estimate sharer reputation scores for each industry. Analysis with other kinds of topics can be done similarly. In fact, our model can also be extended to work when  X  ik s are latent variables to be estimated from data by adding another layer to the hierarchical model, which is future work. Proxies for ground-truth sharer reputation. As a proxy to mea-sure reputation of sharer s on topic k , we use the average LT CTR (Click-Through Rate using the unbiased LT dataset) of items shared by sharer s based on response by viewers of LT interested in topic k . More specifically, for each (sharer s , topic k ) pair, we count the numbers of views and clicks by viewers interested in topic k on the set of items shared by s in the unbiased LT data. Then, average LT CTR = number of clicks / number of views.
 Evaluation Metric. To evaluate the performance of a model, we split our data into a training set and a test set (the splitting method will be described later), and then train the model using the training set and evaluate the method using the test set. To reduce noise, we compute evaluation metrics based on a set of  X  X est sharers X  who satisfy the following two conditions: (1) A test sharer must have at least 10 shared items in the unbiased LT data in the test set; oth-erwise his/her average LT CTR (our proxy for ground-truth repu-tation score) would be noisy since it is based on small counts. (2) A test sharer must also have at least 100 view events in the NUS data; otherwise, given the low click rate in NUS, she may receive too few clicks to exploit the NUS data for estimating her reputation due to noise induced by small counts.

Since our goal is to rank sharers based on reputation, we use two ranking-based evaluation metrics. We note that because a ranking-based metric does not depend on the scale of the predicted repu-tation scores, it is particularly useful for comparing methods that generate scores on different scales. For example, scores from our model and PageRank scores can be on very different scales, but what we focus on is the ranking of sharers. Intuitively, a good method assigns higher scores to sharers who share better items.  X  Kendall X  X   X  : To determine whether the ranking of sharers  X  CTR of top k sharers : The top k sharers identified by a better To evaluate a model, we first compute the two metrics on the test set for each individual topic (i.e., industry) and then average across topics. It is important to note that the main focus of this paper is on estimating sharer reputation scores. Thus, improving model accu-racy in terms of predicting individual users X  response (e.g., click) to individual items is not the main goal. Because of this, we do not consider metrics that measure click prediction accuracy per event.
In this subsection, we consider splitting data into a training set and a test set by time. In particular, we use the data in May for training, and the next 3 months for testing. We estimate sharers X  reputation scores using the training data, and then evaluate the esti-mated scores based on average LT CTR computed using data in the test period. Notice that our test set is much larger than the training set. This is done to reduce the variance of the evaluation metric. Baseline Methods. Many recent methods for finding influential users in social networks are based on PageRank (e.g., [30, 25, 28]). Thus, we consider PageRank as the main baseline to compare to. We implemented two versions of PageRank [4] for computing sharer reputation scores: PageRank-Rate and PageRank-Volume . These two methods are based on the intuition that users who share good items would necessarily get more responses from their con-nections. Let N V ( i,s ) be the number of items that are shared by sharer s and viewed by her connection i , and let N R ( i,s ) be the number of items that are shared by sharer s and responded (clicked or liked or re-shared) by her connection i . For example, if sharer s shared 11 items and her connection i saw 10 of them and responded to 3, then we have N V ( i,s ) = 10 and N R ( i,s ) = 3 . Each method defines the edge weight w ( i,s ) between each (recipient i , sharer s ) pair based on N V ( i,s ) and N R ( i,s ) in its own way, and then computes the PageRank scores [4] for all of the sharers. Both methods normalize the edge weights so that the sum of out-going edge weights is one:  X  w ( i,s ) = w ( i,s ) P putes the score f ( s ) of sharer s by solving the following equation: where  X  is a given constant and N is the number of users. We experimented with a number of  X  values, but only report the per-formance of  X  = 0 . 2 following [25]. We note that other values of  X  do not improve the performance. To estimate reputation scores for industry k , we take an induced subgraph of users who follow or belong to industry k . Then, the PageRank scores on the induced subgraph (either by PageRank-Rate or PageRank-Volume ) provides reputation scores for that industry.

We note that our PageRank methods follow the same approach as [30, 28]. We also tried another version of PageRank proposed by S X ez-Trumper et al. [25] in our early experiments. However, its performance is worse than PageRank-Rate or PageRank-Volume and gives negative correlations probably because it tends to identify abusive users. So, we drop it from our experiments.
 Variations of our model. To measure the effect of adding differ-ent components to our hierarchical model, we consider two sim-plified versions of Full Model: Model (only Y) and Model (only Z). Model (only Z) assumes that the reputation of sharer s on topic k is its uncalibrated score  X  sk estimated based only on biased so-cial response data. Specifically, Model (only Z) estimates  X  ing only the NUS response data z sij (Equation 12) and then sets  X  sk =  X  sk . Model (only Y) estimates  X  sk using only unbiased user action data (the LT data) y ij without using the NUS social response data at all. Specifically, Model (only Y) first estimates p jk by solving Equation 10 and then estimates  X  sk through Equa-tion 11. Note that Model (only Y) cannot estimate the reputation scores of  X  X old-start X  sharers, who do not share any items recom-mended by LinkedIn Today during the training period.
 Comparing overall performance of different models. The first row of Table 2 shows the performance of each model. Our Full Model achieves the highest rank correlation, which indicates that appropriate combination of biased social response (NUS data Z ) with unbiased user action (LT data Y ) improves the accuracy. Model (only Y) achieves the second highest rank correlation and Model (only Z) is the third. This is expected since user reputation is pri-marily defined based on unbiased LT data. For sharers with a suffi-cient amount of unbiased LT data, the biased NUS data is not really needed. However, for sharers who do not have many shared items in LT data, NUS data helps. Note that the PageRank baselines per-form poorly  X  they show slight negative rank correlations.
As discussed in Section 3, the main purpose of using biased so-cial response data (NUS data Z ) is to estimates the reputation of cold-start users who do not share any item (or share very few items) in the unbiased user action data (LT data Y ). To measure the per-formance of the methods in the cold-start scenario, we limit our focus on  X  X old-start test sharers X  who did not share any item in the unbiased LT data. To reduce the variance of the performance metric, we increase the number of the cold-start test sharers by ran-domly picking 50 % of test sharers and removing them from the unbiased training data. The second row of Table 2 shows the perfor-mance of different methods for these cold-start test sharers. Recall that Model (only Y) cannot estimate reputation scores for cold-start sharers. Full Model outperforms all the other methods with a sig-nificant margin. It achieves rank correlation 0 . 1124 , a 241% lift over the second best method, Model (only Z).

Figure 3 shows the CTR of top k sharers (relative to the CTR of all shared items) for different models as a function k for the cold-start scenario. First, note that all curves would eventually converge to 1.0 when k is the total number of sharers since, by definition, the CTR of all sharers equals the average CTR of all shared items. Second, note that our Full Model significantly and uniformly out-performs all other methods. Also note that the other three curves, which compute reputation purely on biased social response data, are all below 1.0 when k is small. This suggests that users who receive highest social response seem to exhibit some undesired be-havior (e.g., LinkedIn Open Networkers whose goals are connect-ing to as many people as possible, forming a large densely con-nected component with a large number of activities).
 Breaking down by industries. We examine how the performance of different models varies depending on the topics (industries). Due to space limitation, we only show the results of Kendall X  X   X  . Fig-ure 4 shows box plots for the distributions of Kendall X  X   X  across industries for the all user scenario and the cold-start scenario. Note that this box plot does not represent statistical significance, but shows the variation of rank correlation across industries. In Fig-ure 4a, Full Model achieves higher  X  in most industries; the 75% quantile of Full Model is higher than the median  X  of the best baseline (Model (only Y)). On the other hand, the PageRank-based baselines and Model (only Y) perform poorly regardless of indus-tries. In Figure 4b, we see a larger gap in performance than the gap in Figure 4a. The minimum value of Full Model is higher than the median value of the best baseline (Model (only Z)).

Figure 5 shows finer breakdown of the performance of Full Model compared to the best baseline for the top 20 industries for the two scenarios, where the y -axis  X   X  is the difference between  X  of Full Model and  X  of the best baseline, for each industry (positive values mean Full Model outperform), and the bar width is proportional to the number of test users in the industry. In Figure 5a, Full Model outperforms in 14 (70% of all) industries and more importantly, the industries where Full Model cannot outperform have smaller num-bers of sharers  X  the average number of test sharers in industries where Full Model is inferior is 56% of the number of the test users in the rest of the industries. In Figure 5b, Full Model is superior in most industries  X  it outperforms the best baseline method in 18 (90% of all) industries. Different amounts of unbiased data. We first study the effect of using different amounts of unbiased user action data (the LT data) in model training. To control the amount of unbiased data, we cre-ate a series of datasets in the following way: Instead of splitting by time (as in Section 4.1), here we split by items. Specifically, we consider all of the items that appear in the unbiased LT data and randomly select 40% of the items to be the  X  X est items X  and call the rest 60%  X  X raining items X . We set aside all of the events on Figure 3: CTR of top k sharers for different models as a function of k , normalized by the average CTR of all items Figure 4: Box plots of the distributions of Kendall X  X   X  for the top 20 industries. R: PageRank-Rate , V: PageRank-Volume , Y: Model (only Y), Z: Model (only Z), M: Full Model. the test items to form the test set. The remaining events form the  X  X nitial training set X . Because our main focus is on how to estimate sharer reputation for sharers without unbiased data (cold-start), we randomly select 50% sharers who satisfy the two test sharer con-ditions (defined when we introduce evaluation metrics) to form the set of test sharers. We then remove all the events related to the test sharers from the initial training set to form the  X 100% training set X , which consists of events on 16,545 items. To simulate dif-ferent amounts of unbiased data, we randomly sample P % of the items that appear in the  X 100% training set X  and select only training events on the sampled items to form a  X  P % training set X . We note that model performance numbers in this subsection is not compa-rable to those in the previous subsection due to different ways of splitting data into training and test sets.

Figure 6a shows Kendall X  X   X  rank correlation as a function of the percentage of unbiased training data used to train our model. As can be seen, the unbiased data can be reduced by 50% without sig-nificantly affecting model performance. From 50% to 5%, model performance degrades roughly in a log-linear manner (notice that x-axis is log scaled). Figure 5: Improvement in Kendall X  X   X  for each industry by Full Model compared to the best baseline. The bar width is proportional to the number of test users in the industry Co-sharing random field prior vs. zero-mean prior. Recall that in our model, we put a co-sharing random field prior over  X  understand the benefit of using co-sharing random field prior, we compare our model with this prior to the model that replaces the co-sharing random field prior with a commonly used alternative prior, a zero-mean prior; i.e., Figure 6b shows the lift in Kendall X  X   X  rank correlation of our model with the co-sharing random field prior over the model with the zero-mean prior as a function of the percentage of unbiased training data, where lift is defined as (  X  CSH-MRF  X   X  zero-mean As can be seen, the smaller the data size, the higher the lift.
Our experiments provide interesting insights which we summa-rize below. Figure 6: Effect of different amounts of unbiased data (x-axis is log scaled)  X  Classical methods of estimating social influence like PageR- X  Our method of combining information from both sources is  X  It is interesting to see the role the co-sharing Markov random
We proposed a novel multi-level hierarchical model that esti-mates unbiased reputation scores by augmenting social response data that has selection and response bias with aggregated data from an unbiased context. Our method provides significant improvement over existing methods to estimate social influence. We show that it is indeed possible to correct for the biases in social response data by benchmarking it with reasonable amount of data from an unbiased context. Furthermore, the co-sharing Markov random field prior helps in providing reliable estimates even with small amount of un-biased data. Our research has opened up avenues for future work. While we illustrated our method with only two sources of data, one can generalize the method to multiple  X  X isaligned X  data sources; some of them may be unbiased but available at different levels of aggregation, while others could be more granular but biased. For instance, one can obtain aggregated unbiased data from different kinds of recommendation modules. It is even possible to obtain some measures of article quality through offsite reading and shar-ing data obtained from external publishers that are instrumented with  X  X ocial share buttons X . Developing a principled framework to combine multiple misaligned social response data available at different levels of aggregations with different kinds of biases is a challenging problem we plan to address in the future.
