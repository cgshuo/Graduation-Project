 Yuting Liu  X  Tie-Yan Liu  X  Bin Gao  X  Zhiming Ma  X  Hang Li Abstract This paper is concerned with a framework to compute the importance of webpages by using real browsing behaviors of Web users. In contrast, many previous approaches like PageRank compute page importance through the use of the hyperlink graph of the Web. Recently, people have realized that the hyperlink graph is incomplete and inaccurate as a data source for determining page importance, and proposed using the compute page importance from user behavior data (which covers some previous works as special cases). First, we use a stochastic process to model the browsing behaviors of Web users. According to the analysis on hundreds of millions of real records of user behaviors, we justify that the process is actually a continuous-time time-homogeneous Markov pro-importance. Second, we propose a number of ways to estimate parameters of the stochastic process from real data, which result in a group of algorithms for page importance com-proposed algorithms can outperform the baseline methods such as PageRank and Trust-Rank in several tasks, demonstrating the advantage of using our proposed framework. Keywords User browsing process Continuous-time time-homogeneous Markov process Staying time BrowseRank 1 Introduction Page importance is a key factor for Web search, because for contemporary search engines, crawling, indexing, and ranking are usually guided by this measure. Conventionally, the perlink graph of the Web. These approaches take the link from one webpage to another as an endorsement of the linking page, and assume that the more links pointed to a page, the more likely it is important. Two typical examples are PageRank (Brin et al. 1998 , Page et al. 1999 ) and HITS (Kleinberg 1998 ). The link analysis algorithms have been suc-cessfully applied to Web search.

Recently people (Eirinaki and Vazirgiannis 2005 , Liu et al. 2008 , Oztekin et al. 2003 ) have realized the limitations of the link analysis approaches. First, the hyperlink graph is an perlinks can be easily added or deleted by the Web content creators. For example, it is a common technique to spam search engines by purposely creating a large number of hy-perlinks to the target webpage (e.g., link farm and link exchange) (Gyongyi et al. 2005 ). Second, there are some unrealistic assumptions on the user behaviors in these link analysis methods. For example, in HITS and PageRank, it is assumed that users select the next page to visit from the outlinks of the current page in a uniformly random manner. For another pages. These are, however, clearly inconsistent with the browsing behaviors of real users.
To tackle these problems with the link analysis methods, a fundamental approach is to leverage the browsing behaviors of real users. This is because it is the users who determine which pages to visit, which hyperlinks to click, and which pages to spend much time on. This is exactly the motivation of this paper and some previous works (Liu et al. 2008 ). In this work, we propose a formal framework to compute page importance from user behavior data.
First, we use a stochastic process to model the browsing behaviors of Web users, which classified into two categories: reading (or staying on) a webpage or jumping from one page to another. The stochastic process thus contains the staying on a state and the transition from one state to another. We then show that such a process is actually a continuous-time time-homogeneous Markov process, and its stationary probability distribution can be used as the measure of page importance. Furthermore, we prove that this stationary distribution is determined by two factors, the visiting frequency and the length of staying time. In other words, the more visits to a page made by the users and the longer time spent by the users on it, the more likely it is important. This is consistent with our intuition.

Second, we propose a number of ways to estimate the parameters of the stochastic bution of an associated embedded Markov chain of the original process, which encodes the transition information. For each step, we adopt various estimators. In total eight different algorithms are developed by combining different estimators in the two steps. Note that the algorithm proposed in Liu et al. 2008 is one of the eight algorithms. In this regard, we say our proposed framework can contain the work in Liu et al. 2008 as its special case.
Then, we conduct comprehensive experiments to study the effectiveness of different estimators, and thus different algorithms we proposed. We also compare the proposed algorithms with baselines like PageRank (Brin et al. 1998 , Page et al. 1999 ) and TrustRank (Gyo  X  ngyi et al. 2004 ). We performed the experiments at both page level and website level. For the page-level experiment, we mainly tested the contributions of different page important algorithms to relevance ranking. For the website-level experiments, we mainly tested the use of page importance in authoritative page finding and web spam filtering. Our experimental results have shown that the proposed algorithms are very effective in these computation; (2) the comparison between user behavior data and the conventional hy-perlink graph; (3) the reason why the proposed algorithms outperform the baselines.
The rest of the paper is organized as follows. Section 2 describes the user browsing Experimental results are reported in Section 4 . Section 5 introduces some related works. Conclusion and future work are given in Section 6 . 2 User browsing process In order to precisely model the behaviors of real Web users, we first perform some analysis browsing process. After that we will show that the stationary distribution of this process can be an effective measure of the page importance. 2.1 Motivation The Web might be the richest information source in the world, and people are fulfilling their information needs by browsing the Web. According to our observations on a large amount of user behavior data, users X  browsing behavior can be classified into two types: reading a webpage, or jumping from one page to another.

Reading is an important means for users to get the information contained in a webpage, and it usually takes a certain period of time. When reading, some users may quickly scan the headlines, others may read the content very carefully. As a result, the time spent on the same page by different users can have a large variance.

When a user cannot find the desired information on the current page, or she/he wants to find some other interesting things, she/he may choose to jump to another page by clicking a hyperlink on the current page. She/he can also choose to directly input a new URL in the address bar of the browser. 1 The jump from one page to another by hyperlink click can be regarded as a kind of endorsement to the destination page. 2 The jump by means of direct URL input can be regarded as a kind of preferential reset (as opposite to the random reset in conventional link analysis algorithms).

When the user comes to a new page, she/he will repeat the loop of reading and jumping, until her/his information need is satisfied or she/he gives up.
Based on the above observations, we can make the following assumptions, which are intuitively easy to understand. 1. Assumption 1 2. Assumption 2 2.2 Mathematical formulation for user browsing process We use the following stochastic process to describe the browsing behaviors of the Web users. Suppose there is a Web surfer browsing the Web. We use X s to denote the webpage that the surfer visits at time s C 0. Then the user browsing process can be represented as a continuous-time discrete-state stochastic process X = { X s , s C 0} (Berger 1993 ), where the state space of X is the set of all webpages (we suppose there are N webpages in total).
Considering the two assumptions that we obtained from the observations on real data, we can attain the following properties of process X . 2.2.1 Markov property According to Assumption 1 in Section 2.1 , the process X is a continuous-time Markov process (Anderson 1991 ): given a previous state X s , the transitions occurring afterwards are independent of the states earlier than X s . Mathematically, for any time series ? ? [ t C s C u C 0, we have probability is dependent on not only the source and destination states, but also the starting and ending time of the transition. 2.2.2 Time-homogeneity According to Assumption 2 in Section 2.1 , the browsing behavior is independent of the homogeneity (Berger 1993 ). Mathematically, for any time points ? ? [ t C s C 0,
The above equation indicates that the transition probability is only determined by the length of the transition period, but not by the starting and ending time of the transition. Hereafter, for simplicity, we use p ij ( t ) to denote the transition probability from page i to probability matrix can be written as P  X  t  X  X  p ij  X  t  X  N N : which means that after a Web surfer spends a period of time t on page i , she/he can only do the following things: transits to another page j = i , or keeps staying at page i . In other words, the user browsing process is conservative (Anderson 1991 ).

Based on the above analysis, the user browsing process X is a continuous-time time-homogeneous Markov process . As we have discussed in Section 2.1 , there are two types of user browsing behaviors: reading a page and jumping from one page to another. Now let us whether effective page importance can be computed from this process.

First, in order to relate the properties of process X to the two types of user behaviors, we need to introduce the concept of Q-Process. Actually, it has been a common practice to study a continuous-time time-homogeneous Markov process with its (one-to-one) corre-sponding Q-Process, when the state space is finite (Wang et al. 1992 ).
 Definition 1 Suppose X is a continuous-time time-homogeneous Markov process as derivative of p ij ( t )at t = 0. The matrix ( q ij ) N 9 N is called a Q-matrix.
The Q-matrix is also referred to as the transition rate matrix, because q ij , i = j can be explained as the rate at which process X enters state j from state i , and -q ii as the rate at which process X leaves state i . Considering that the process must enter a state (denoted as j ) when it leaves state i , we have q ii  X  conservative.

Owing to the conservativity of the process, it is easy to prove that -? \ q ii \ 0, 0 \ q ij \ ? , and Furthermore, according to the one-to-one correspondence between Q and P ( t ), the original process X is sometimes referred to as a Q-Process.

Based on the definition of Q-matrix, it has been proved that process X satisfies the following two properties (Wang et al. 1992 ).
 Property 1 Suppose X is a Q-process. Then for any state i and any time interval r [ 0, we have
This property indicates that the staying time on page i is governed by an exponential staying time on page i is 1 q page i is. Note that this property distinguishes the Q-Process from a discrete-time Markov process, in which the length of the staying time is a constant for all the states. Property 2 Suppose X is a Q-process. Then for any two states i, j and i = j,
This property indicates that the probability of the transition from state i to state j directly is determined solely by these two states, regardless of the staying time on page i .
Based on the above discussions, we can see that it is quite reasonable to define the user browsing process as a continuous-time time-homogeneous Markov process. First, the Q-Process can be decomposed into two parts, the staying on a state and the transition from one state to another, which just correspond to the two types of user behaviors. Second, in the Q-Process, the length of the staying time is governed by an exponential distribution, whose parameter is only determined by the leaving rate of the page (i.e., -q ii ). Accord-ingly, the period of time that a real user spends on a page is mainly determined by the page independent of the staying time. Also, when a user wants to jump from the current page to another page, the choice of the target page is usually determined by the importance of the pages, but irrelevant to the length of the staying time on the current page. 2.3 Page importance computation based on user browsing process In this subsection, we discuss how to compute page importance based on the aforemen-tioned user browsing process.

At first glance, it is easy to perform the task due to the following reason. As we know, ducible 3 and aperiodic, 4 there exists a unique stationary probability distribution p = ( p 1 , p ,..., p N ) (Berger 1993 ), which is independent of time interval t :
The i th element p i stands for the ratio of the time spent on page i over the whole period p use it as a measure of page importance.

However, in practice it is very difficult to directly compute this distribution p , distribution p .

One feasible (and widely-used) solution to the above challenge is to leverage the correspondence between P ( t ) and matrix Q . In this solution, the embedded Markov chain of the Q-matrix, defined as below (Stewart 1994 ), is used. Definition 2 Suppose X is a Q-Process with transition matrix P ( t ). Then a discrete-time Markov chain Y = { Y n , n C 0} with transition matrix ~ P is called an embedded Markov following condition: where q ij , i , j = 1,..., N are the elements in the Q-matrix.

By comparing Eq. 5 with 3 , we find that process Y just records the direct transition information in process X . Then the stationary probability distribution of process Y , denoted as ~ p , corresponds to the mean visiting frequencies of all the webpages.
 With the help of EMC, we can compute the page importance p in the following way. Proposition 1 Suppose X is an irreducible Q-process, and Y is the EMC derived from its tributions of processes X and Y respectively. Then we have
The above proposition shows that we obtain the stationary probability distribution p of process X by normalizing the product of ~ p and the reciprocal of the diagonal elements of original transition probability matrix P ( t ).

As discussed before, ~ p corresponds to the mean visiting frequencies of all the webpages, conclusion that the page importance p is determined by two factors: visiting frequency and Similarly, the longer the staying time on a page is, the more important it is. Therefore, in order to compute effective page importance, it turns out that we should develop effective methods to estimate ~ p and 1 q 3 Parameter estimation In this section we discuss how to estimate the parameters of the user browsing process, so as to effectively compute page importance.

As analyzed in the previous section, parameter q ii is related to the length of the staying information from the user behavior data. After that, some estimation models can be applied. 3.1 Information extraction from user behavior data Many Web service applications can log the user browsing behaviors under agreements with the users, at the same time of assisting users in their accesses to the Web. Usually, the Web service log takes the following form. First a user ID is used to represent the user. The user ID is randomly created, and does not contain any privacy information of the user. Second, all the behaviors of the user in a period of time are stored as records, each represented by a triple of \ URL, TIME, TYPE [ and sorted in the chronological order (see Table 1 for examples). Here, URL denotes the URL of the webpage visited by the user, TIME denotes the time stamp of the visit, and TYPE indicates whether the visit is by a direct URL input (INPUT) or by a hyperlink click (CLICK).

Since the amount of the user behavior data collected by Web service application is usually very large, one needs to perform some pre-processing to extract useful information from it. We list some important pre-processing in Table 2 . Since some of them have been explained in (Liu et al. 2008 ) in detail, we just highlight the differences here. We actually modify the session segmentation and staying time extraction, and add a new step named counting. The reason for introducing the counting process is as follows. For each session, the last URL corresponds to the page whose hyperlinks will not be clicked by the user any more. In other words, the user will perform preferential reset on such a page. The resetting number may be useful for the estimation of the visiting frequency.

By aggregating the information extracted from the records, we are able to build a user number, a resetting number, a preferential reset probability, and a set of observations of staying time as its metadata. Each directed edge in the graph represents the transition between two vertices along hyperlink, associated with the number of transitions as its weight. In other words, the user browsing graph is a weighted directed graph with vertices containing metadata and edges containing weights. We denoted it as G  X  ; W ; C ; R ; Z ; c [ , where V  X  denote the vertices, the numbers of transitions, the visiting numbers, the resetting numbers, use N to denote the total number of webpages in the user browsing graph, and m i is the number of observations of staying time for page i . 3.2 Estimation of q ii theoretically governed by an exponential distribution parameterized by -q ii . When given a the unknown parameter q ii of the random variable T i based on these observations. We can choose to perform the task by using many methods like maximum likelihood estimation (MLE). According to our experiments, however, the estimation is non-trivial. This is because the observed staying time do not strictly follow the exponential distribution.
Here is an illustration. For each page in the user behavior data, we have a number of observations of the staying time. Some pages may have a large number of observations, while some other pages may have just a few, depending on the number of visits by users. We randomly select a webpage from those pages with sufficient observations (i.e., with more than one million observations of the staying time), and plot the distribution of these observations in a log-linear coordinate (See Fig. 1 a 5 ).

From the figure, we can see that the curve does not correspond to an exact exponential distribution, because the log-linear plot of an exponential distribution should be a straight line. The major difference is that there are significantly smaller numbers of short staying time in the observations. As for this phenomenon, we hypothesize that the observations of staying time from the user behavior data are noisy, due to the following reasons. (1) The factors will affect the staying time. (2) The recorded staying time will be longer than the leaves the browser window open.

Therefore, it would be necessary to consider the noise in the parameter estimation. To model, we ignore the noise, and directly use the classical MLE method for the estimation. In the second model, we use a de-noise method which cleans the observed samples before estimation. 3.2.1 MLE model Maximum likelihood estimation is a popular statistical method for parameter estimation. function as follows,
Then the maximum likelihood estimator of q ii is computed through the following optimization.
Here, Z i  X  1 m to approximate the real expectation of T i which is equal to 1 q 3.2.2 Additive noise model 2008 ) can be used to represent the observations and to conduct an unbiased and consistent estimation of parameter q ii .

We regard the observed staying time Z i as being composed of the real staying time T i and the noise U .

Here T i is governed by an exponential distribution, and U is governed by a Chi-square distribution with k degrees of freedom, denoted as Chi ( k ). The reasons of using the Chi-square distribution are as follows. First, the Chi-square distribution is supported within [0, Second, we plot the curve for the combination of the Chi-square distribution with k = 4 appropriate. (a) (b)
With the additive noise model, we estimate the staying time as below. Let the mean and and variance of variable U are k and 2 k . According to Property 2, the mean and variance of variable T i are 1 q 1995 )
If we can estimate the parameters l i and r i 2 , there are many ways to compute the r 2 can be straightforward, since the sample mean Z i and sample variance S 2 unbiased and consistent estimators for l i and r 2 i (Sorenson 1980 ). Given m i observations on Z i denoted as z 1 i ; z 2 i ; ... ; z m i i , we have
As mentioned above, one can solve the group of Eqs. 10 so as to get the estimates of parameters q ii and k as follows:
In practice, however, due to data sparsity, the observed samples do not necessarily satisfy the above equations in a strict manner. To tackle this challenge, one can relax the equation group 12 to the following optimization problem, in which the objective function represents the difference between parameter k calculated from the two equations in 12 ,
Optimize problem ( 13 ) can be solved by many optimization methods such as gradient descent (Boyd et al. 2003 ). In this way, we can get the estimate of q ii efficiently. 3.3 Estimation of ~ p process. First, the stationary distribution of a discrete-time Markov process characterizes quency. Second, as we know, the stationary distribution ~ p of a discrete-time Markov process is the principal eigenvector of its transition matrix. Therefore we also can estimate all the elements in the transition matrix first and then compute the distribution by the power method (Golub et al. 1996 ). We call the second approach an indirect approach.
In this subsection, we will introduce four models to estimate ~ p of the EMC. The first model belongs to the direct approach, and the other three models belong to the indirect approach. 3.3.1 Direct model In the user browsing graph G , the set C contains the number of visits of each vertex. By the law of large number (Rice 1995 ), we can take the visiting frequency as a cursory estimator for ~ p : 3.3.2 Indirect model 1 From the user browsing graph, we can obtain the real transition information between any transitions between two pages i and j to estimate the corresponding transition probability ~ p : primitive), we adopt the same smoothing trick as in the PageRank algorithm. Accordingly, the transition probability can be updated as follows,
With the smoothing, 7 process Y will have a unique stationary probability distribution, and one can safely use power method to calculate it. 3.3.3 Indirect model 2 In Indirect Model 1, we adopt the same smoothing method as that in the PageRank algorithm, which assumes that users randomly choose any webpage to restart when they do not want to browse along hyperlinks. Actually, in Section 3.1 we have discussed that the preferential reset probability in the user browsing graph can better describe the real users X  reset probability to replace the uniform reset probability in Indirect Model 1. Accordingly, we obtain the following new estimator for the transition probability,
One may have the concern whether process Y is still irreducible with the use of the Theorem 1. Because of this, we can still safely apply the power method to calculate the corresponding stationary distribution ~ p . 3.3.4 Indirect model 3 Indirect Models 1 and 2 estimate the transition based on w ij . In this way, only the number included. This may lead to some inaccurate estimation, as shown below.

Suppose we have the following observations for page i : it has been visited for 100 all the other 98 times, users conduct resets (i.e., users terminate their browsing sessions at page i and start new sessions). If we only use the observed transitions for the resolved by the smoothing factor, because there are actually 98% users conducting reset.
 To tackle the problem, we propose using two quantities, of the transition probability. The first quantity is the frequency of real transitions from page i to other pages by hyperlink click, and the second one is the frequency of performing reset at page i . Furthermore, we assume that when performing reset, users will follow the probabilities as below.
It can be proved that with Eq. 17 , the corresponding EMC is irreducible (see Theorem 1), and thus a unique stationary probability distribution p exists. Therefore, one can safely use the power method (Golub et al. 1996 ) to calculate it in an efficient manner. Theorem 1 Suppose X is a Q-process and Y is its EMC. If the entries in the transition primitive, accordingly, process Y and X are irreducible.
 Proof Please refer to the proof in Liu et al. 2008 . 3.4 BrowseRank algorithms With the aforementioned models for parameter estimations, we can construct eight algo-these algorithms BrowseRank. To differentiate them, we use the notation BR( x , y )to denote the BrowseRank algorithm with model x to estimate q ii and model y to estimate ~ p . We summarize these algorithms in Table 3 . It is easy to find that BR(A,InD3) is exactly the algorithm proposed in Liu et al. 2008 . In this sense, we say that our proposed framework can cover the work in Liu et al. 2008 as its special case. 4 Experimental results We conducted two types of experiments to verify the effectiveness of the BrowseRank algorithms and thus the proposed framework. The first type of experiment was conducted at the webpage-level, in order to test the contribution of an algorithm to relevance ranking. The second type was conducted at the website-level, to test the performance of an algo-rithm in finding important websites and depressing spam sites. 4.1 Webpage-level experiments 4.1.1 Ranking in web search In Web search engines, the retrieved webpages for a given query are often ranked based on lists given by these two factors can be used to generate the final ranked list (Baeza-Yates et al. 1999 ): where 0 B h B 1 is the combination coefficient.

We used this method in our experiments, and used BM25 (Robertson 1997 ) as the relevance model for ranking. 4.1.2 Dataset and baselines We used a user behavior dataset, obtained from a commercial search engine. All possible privacy information was rigorously filtered out and the data was pre-processed according to the steps listed in Section 3.1 . There are in total over three-billion records, and among them there are about one-billion unique URLs. We also obtained a hyperlink graph containing these one-billion webpages from the same commercial search engine, and computed baselines based on it.
 In addition, we obtained a dataset for relevance ranking also from the search engine. The dataset contains 7500 queries and their associated webpages. The queries were randomly sampled from the query log, and the pages were randomly selected from the top 1000 results in the click-through log for each query. The BM25 scores of these webpages were also provided (when counting the inverse document frequency, the were asked to independently judge whether the page is relevant to the query. Then these judgments were aggregated by simple voting to determine the final relevance of the page to the query.
 We implemented two baselines for comparison. One is PageRank , and the other is UPR , which runs the PageRank algorithm on the user behavior data as described in Eirinaki et al. 2005 . The first baseline was selected to show the effectiveness of the user behavior data, and the second was used to show the effectiveness of the proposed models. 4.1.3 Evaluation measures We used three evaluation measures in our experiments, Precision (Baeza-Yates et al. 1999 ), Mean Average Precision (MAP) (Baeza-Yates et al. 1999 ), and Normalized Dis-are as follows.

Suppose the query collection is S , and | S | means the total number of queries. For some query q [ S , there are N q webpages associated with it. Among these webpages, N rel are relevant, and N irrel are irrelevant. 4.1.3.1 Precision where N rel ( n ) denotes the number of relevant webpages ranked in the top n positions for a query. 4.1.3.2 MAP pos ( n ) = 1; otherwise, pos ( n ) = 0. 4.1.3.3 NDCG There are four steps to compute NDCG for a ranked list of webpages: (1) Compute the gain of each webpage; (2) Discount the gain of each webpage by its position; (3) Cumulate the discounted gain of the list; (4) Normalize the discounted cumulative gain of the list.
 pages), and the normalization constant M n is chosen so that the perfect list gets a NDCG score of 1. 4.1.4 Results and discussions We compared the performances on relevance ranking for different BrowseRank algo-rithms, and also for PageRank and UPR. The experimental results are shown as below. 4.1.4.1 Comparison among different BrowseRank algorithms The experimental results about the comparison among the eight BrowseRank algorithms are presented in Figs. 2 , 3 , and 4 .

In these figures, when h = 1, the page importance will not affect the final ranking result, and therefore all the algorithms have the same performance. For other values of h , we can see that BR(A,InD3) has the best performance in terms of all the evaluation measures. In addition, we also find that the comparisons among different BrowseRank algorithms are robust to various measures. Roughly speaking, their performances can be ordered as follows:
Based on these results, we can come to the following conclusions. 1. When comparing BR(A,y) with BR(M,y) for any y 2f D ; InD 1 ; InD 2 ; InD 3 g ; we can 2. From the comparison between Direct Model and the Indirect Models, we can obtain 3. Note that BR(M,D) actually ranks pages according to the ratio of the time spent on 4.1.4.2 Comparison between BrowseRank and baselines Based on the above discussions, we find that BR(A,InD3) is the best among the eight BrowseRank algorithms, and BR(M,D) is the reference to distinguish different estimators. Therefore, in the following experiments, we just use them as the representatives of the BrowseRank algorithms in the comparisons with the baselines. The corresponding results are presented in Figs. 5 , 6 , and 7 .
From the figures, we can see that with the help of page importance, the performance of relevance ranking can be improved. Furthermore, the two BrowseRank algorithms con-measures. And the UPR algorithm has better performance than PageRank, which agrees with the observations in Eirinaki et al. 2005 .

We also conducted t -tests at a confidence level of 95% for the above observations. In terms of MAP, the improvement of BR(A,InD3) over PageRank is statistically significant with a p -value of 0.00218. In terms of P@3, P@5, NDCG@3, and NDCG@5, the 7.37 9 10 -8 and 3.11 9 10 -6 , respectively. In terms of MAP, P@3, NDCG@3, and NDCG@5, the improvements of BR(A,InD3) over UPR are also statistically significant with p -values of 0.0328, 0.0206, 8.72 9 10 -4 and 0.00455, respectively. The only exception is P@5, which p -value is 0.1107 ( [ 0.05) indicating that the improvement is not significant.
As for the above experimental results, we have the following discussions. 1. The algorithms using the user behavior data (i.e., BrowseRank and UPR) outperform 2. The performance of PageRank and UPR are worse than that of BR(M,D). This seems to 4.2 Website-level experiments 4.2.1 Dataset and baselines In the second experiment, we still used the same user behavior data. The difference was that we did not distinguish webpages in the same website when running the BrowseRank algorithms. That is, we ignored the transitions between the pages within the same website and also aggregated the transitions from (or to) the pages in the same website. The website-level user browsing graph consists of about 6-million vertices and 60-million edges.
We implemented three baselines, PageRank, TrustRank and UPR, for the website-level experiments. As for these algorithms, we also aggregated the corresponding page impor-tance to the website level. We chose BR(A,InD3) as the representative of the BrowseRank algorithms to compare with these baselines. 4.2.2 Top-20 websites We listed the top-20 websites ranked by using the four algorithms in Table 4 . From this table, we have the following observations.

First, BR(A,InD3) tends to give high ranks to Web 2.0 websites (marked in bold) such as myspace.com , youtube.com , and facebook.com . The main reasons are that Web users visit these websites with high frequencies and often spend long time on them, even if they do not have as many inlinks as Web 1.0 websites like adobe.com and apple.com do.
Second, some websites like adobe.com are ranked very high by PageRank. One reason is that adobe.com has a huge number of inlinks for Acrobat Reader and Flash Player downloads. However, Web users do not really visit such websites very frequently and they should not be regarded more important than the websites on which users spend much more time (like myspace.com and facebook.com ).

Third, the ranking results produced by TrustRank are similar to PageRank. The dif-ference is that the well-known websites are ranked higher by TrustRank, mainly because these websites are likely to be included or pointed to by websites in the seed set.
Fourth, UPR also gets better ranking results than PageRank and TrustRank. This also indicates that the user behavior data is more reliable and effective than hyperlink graph as a data source to calculate page importance.

In summary, BR(A,InD3) seems to better represent users X  preferences than the baselines. 8 4.2.3 Spam fighting We randomly sampled 10,000 websites from the 6-million websites and asked human experts to make spam judgments on them. As a result, 2,714 websites are labeled as spam and the rest are labeled as non-spam.
 We used the spam bucket distribution to evaluate the performances of the algorithms. Given an algorithm, we sorted the 6-million websites in descending order of the scores that the algorithm produces. Then we put these sorted websites into 15 buckets. The experiment websites over buckets for PageRank, TrustRank, UPR and BR(A,InD3) are listed in Table 5 .

We can see that the number of spam websites in the top buckets of BR(A,InD3) is smaller than those of the baselines. Take the top three buckets as examples. BR(A,InD3) returns 12, and UPR returns 10. And we also found that BR(A,InD3) can successfully push a lot of spam websites to the tail buckets. UPR also pushes many spam websites to the tail buckets, however, the number is smaller than that of BR(A,InD3). In this regard, we say that BR(A,InD3) is more effective in spam fighting than PageRank, TrustRank and UPR. Furthermore, UPR is better than PageRank and TrustRank, and TrustRank outperforms PageRank. These results are consistent with the results obtained in previous works (Gyo  X  ngyi et al. 2004 ).

The possible explanations to the above experimental findings are as follows: 1. Creating fraudulent hyperlinks, which can hurt PageRank, cannot hurt BR(A,InD3) so 2. The performance of TrustRank can be affected by the selection of the seed set. For 3. Click fraud, which can hurt UPR, cannot hurt BR(A,InD3) so much, because 4. It should be noted that there are always unknown spam techniques that can spam a 5 Related work Many algorithms have been developed to compute page importance, and roughly they can graph, which we call link analysis; the second introduces extra information into the cal-culation process, such as users X  behaviors. 5.1 Methods based on link analysis The link analysis algorithms have been extensively studied in the literature. Representative methods include PageRank (Brin et al. 1998 , Page et al. 1999 ), and HITS (Kleinberg 1998 ). The basic idea of PageRank is as follows. If many important pages link to a page on the link graph, then the page is also likely to be important, and the importance information can be propagated along the hyperlinks. A discrete-time Markov chain which simulates a Web surfer X  X  random walk on the hyperlink graph is defined and page importance is calculated as the stationary probability distribution of the Markov chain. HITS is based on the notions of hub and authority to model the two aspects of importance of a webpage. A hub page is the one from which many pages are linking to, while an authority page is the authorities and vice versa. Previous study has shown that HITS performs comparably with PageRank (Amento et al. 2000 ).
 In addition to PageRank and HITS, many other algorithms have also been proposed. Some of these methods focus on the speed up of the computation of PageRank and HITS (Haveliwala 1999 , McSherry 2005 ), while some others focus on the refinement and enrichment of PageRank and HITS algorithms. Examples include Topic-sensitive Page-Rank (Haveliwala 2002 ) and query-dependent PageRank (Richardson et al. 2002 ). The basic idea of these two algorithms is to introduce topics into the page importance model, and to assume that the endorsement from a page with the same topic is larger than that from a page with a different topic. Some other example algorithms modify the  X  X erson-alized vector X  (Haveliwala et al. 2003 ), change the  X  X amping factor X  (Boldi et al. 2005 ), or introduce different weights to inter-domain and intra-domain links (Langville et al. 2004 ). Besides, there are also studies on theoretical issues of PageRank algorithm (Bianchini et al. related works.

Link analysis algorithms that are robust against link spam have also been proposed recently. For example, TrustRank (Gyo  X  ngyi et al. 2004 ) is a link analysis technique which takes into consideration the reliability of webpages when calculating their importance. In TrustRank, a set of reliable pages are first identified as seed pages. Then the trust of the seed pages is propagated to other pages along hyperlinks. Since the propagation starts from reliable pages, TrustRank can be more spam-resistant than PageRank. 5.2 Methods using users behavior data the usage graph which is obtained from Web logs to adjust the transition weights between two pages. The weights of existing hyperlinks may be adjusted, and in the mean time, some new edges may be created according to the usage data. After that they in that they only trust the usage data, and wipe off the Web structure information. Their main contribution is that they point out that real user behavior data is very important for page importance calculation.

Liu et al. ( 2008 ) also propose an algorithm to compute page importance only based on the user behavior data. Differ from the previous works, they not only use the real transition information, but also use the staying time information. Furthermore, they build a continuous-time Markov process model to combine these two kinds of information. Their work can be regarded as a special case as what we have proposed in this paper.
 6 Conclusion and future work In this paper, we have pointed out that the user behavior data is a more reliable data source for computing page importance than hyperlink graph. We have then proposed a framework to compute effective page importance from user behavior data. In the framework, a sto-chastic process, named the user browsing process, is used to model the behavior data. We have shown that this process is actually continuous-time time-homogeneous Markov process. Consequently, we obtain a set of algorithms. Our experiments have shown that the proposed algorithm can outperform PageRank and other baselines in several Web search tasks, indicating the advantages of the proposed framework.

In the future, we plan to further investigate the following issues. 1. User behavior data tends to be very sparse. The use of user behavior data can lead to 2. We model the user behavior data using a continuous-time time-homogeneous Markov 3. The content information and other metadata are not used in the proposed algorithms. 4. The user browsing process can be used for tasks even beyond the calculation of page References
