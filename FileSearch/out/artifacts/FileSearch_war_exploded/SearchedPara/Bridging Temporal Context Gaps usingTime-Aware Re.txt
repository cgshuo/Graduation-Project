 Understanding a text, which was written some time ago, can be compared to translating a text from another language. Complete interpretation requires a mapping, in this case, a kind of time-travel translation between present context knowledge and context knowledge at time of text creation. In this paper, we study time-aware re-contextualization , the challenging problem of retrieving concise and complement-ing information in order to bridge this temporal context gap. We propose an approach based on learning to rank tech-niques using sentence-level context information extracted from Wikipedia. The employed ranking combines relevance, complementarity and time-awareness. The effectiveness of the approach is evaluated by contextualizing articles from a news archive collection using more than 7,000 manually judged relevance pairs. To this end, we show that our ap-proach is able to retrieve a significant number of relevant context information for a given news article.

Reading a current news article about your own country typically is straightforward. Things get worse if the article is, for example, from the 60s or the 70s as it can be found in news archives such as the New York Times Archive 1 . We are especially interested in time-aware re-contextualization settings, where explicit context information is required for bridging the gap between the situation at the time of content creation and the situation at the time of content digestion. This includes changes in background knowledge, the societal and political situation, language, technology, and simply the forgetting of the original knowledge about the context. Figure 1: Camel advertisement (left) and contextu-alization information taken from Wikipedia (right).
The importance of time-aware re-contextualization is well illustrated by the advertisement poster from the 1950s in Figure 1. From today X  X  perspective it is more than surprising that it would be actually doctors, who recommend smoking. It can, however, be understood from the context information at the right side of Figure 1, which has been extracted from the Wikipedia article on tobacco advertising.

Dealing with content from former times is not restricted to expert users such as journalists, historians or researchers. With the growing age of the Web, general Web users are in-creasingly confronted with content, which has been created at different time points in the past, assuming knowledge of the context at the respective time for its interpretation.
Basic forms of contextualization have already been sug-gested in early works such as [4, 10, 11]. The Wikify! sys-tem [10], for example, enables an automated linkage of con-cept mentions with Wikipedia pages. Pure linkage to the Wikipedia article is, however, not sufficient for the re-con-textualization task we are targeting. First, Wikipedia pages on popular concepts, events and entities tend to contain large amounts of content, while the concrete aspect of the text to be contextualized might be covered only marginally or not at all; and relevant information might be distributed over various articles. Furthermore, the crucial temporal as-pect such as considering the situation with respect to smok-ing in the 1950s is also missing in pure linking approaches.
Time-aware re-contextualization, that is, the association of an information item i (such as a phrase in a text) with ad-ditional context information c i for easing its understanding is a challenging task. Several subgoals of the information search process have to be combined with each other: (1) c has to be relevant for i , (2) c i has to complement the infor-mation already available in i and the surrounding document, (3) c i has to consider the time of creation (or reference) of i , and (4) the set of collected context information for i should be concise avoiding to overload the user.
Our main contributions are: (1) framing the problem of time-aware re-contextualization; (2) a learning-to-rank ap-proach combining temporal aspects with the idea of comple-mentarity; and (3) its evaluation with over 7,000 relevance judgments using two real-world document collections.
As discussed in Section 1, our work goes beyond the pure linkage of entity and concept mentions with Wikipedia pages as it is described in [4, 10, 11].

In the area of temporal search, previous work [2] has shown that leveraging the time dimension in ranking can improve the retrieval effectiveness for temporal queries. Re-trieving and processing external information to be added to documents has been widely studied in the recent years. In [6], for example, news articles are enriched with related predictions retrieved from other documents in the same col-lection. In [3], the authors present a topic modeling ap-proach which jointly exploits news articles and Twitter for event summarization. In order to generate a representative but not redundant summary of an event, complementarity between tweets and news article sentences is assessed by considering both their similarity and their difference. In contrast to those approaches, our work on time-aware re-contextualization adds another dimension to the contextu-alization task, namely time. We are not looking for more information on the current context, but try to re-construct the original context of a document.

From an application perspective, our work is also related to computational history, which refers to the application of data analysis and mining techniques in support of history research. In this field, various methods have been developed including methods for speeding up search and analyses [5], methods for investigating linguistic and cultural trends [9], as well as methods for analyzing collective memory and the perception based on news article references to the past [1].
Given a document d with creation date t d and a source of background information C (or a context source ), we de-fine time-aware re-contextualization as the process of recon-structing the relevant part of the original context of doc-ument d at time t d by retrieving information from C that helps in interpreting d .

In more detail, time-aware re-contextualization can be re-garded as a combination of an annotation problem with a retrieval and ranking problem. The annotation problem con-sists of identifying a set of (possibly related) contextualiza-tion hooks together with temporal references. These are the parts of d that require contextualization. Contextualiza-tion hooks can, for example, be entity mentions, concept mentions, implicit topics, or phrases. The retrieval problem consists of identifying context units within a context source C that are candidates for contextualizing context hooks. C can for example be an ontology composed from statements about instances as context units or a document collection composed from sentences as context units. Finally, there is a need for ranking contextualization candidates in a way that top-k results of the ranking form a concise, useful and diverse set of contextualization units.
We use a textual collection of background knowledge as context source, and augmented sentences as context units. An augmented sentence c is a tuple c : = ( s c ,T c ,E c ,p where s c is the text of the sentence augmented with the text of its previous and following sentences, T c is a set of tempo-ral expressions present in s c , E c is a set of entities mentioned in s c , and p c is the title of the document the sentence be-longs to. We augment sentences with their neighbors under the assumption that a single sentence usually does not con-tain sufficient information for being understood in isolation. In the rest of this paper, we will use the terms augmented sentences and sentences interchangeably.

Given a document d to be contextualized, we construct a set of queries from its textual content, and retrieve sentences (or context units) using the constructed queries, and re-rank them using a ranking model. Particularly, we address the ranking problem by defining and investigating different fea-tures for achieving effective time-aware re-contextualization. For this reason, we introduced some simplifications that al-lowed us to easily formulate queries and retrieve sentences to re-rank, leaving the investigation of how to automati-cally identify what requires additional information as a fu-ture work. Next, we will describe the query formulation and present the features used for learning a ranking model.
To retrieve context units for a given document d , a set of queries has to be formulated capturing the parts of d that re-quire re-contextualization for being understood. Identifying such parts is not a trivial task, since they might depend on the user X  X  background knowledge as well as on the publica-tion date of the document. To gain insights, we conducted a preliminary study with a group of human evaluators asking them to annotate the parts of a set of older news articles that require additional information. Over a total of 221 an-notations, 37% represented entities, 32% concepts and top-ics, 16% terms, and 15% short phrases. This means that employing entity and topic extraction tool alone is not suf-ficient, especially because not all the detected entities and topics would require re-contextualization. Even assuming to use these tools, there is still the problem of how to combine different hooks, that are logically related, in a single query.
For these reasons, we decided to simplify the query for-mulation step by asking evaluators to build queries for a given document: given a document d , a set Q d of queries is built by manually selecting and combining words within the document. The publication date t d is not included in the query, since this might prevent the system to retrieve useful sentences whose temporal expression set T c is empty. The temporal dimension will be exploited in the re-ranking model (Section 4.2). For each query q within the set Q d the top-k sentences are retrieved from the context source. The retrieved results are stored in different ranked list C d,q
We propose different features for ranking a set of contex-tualization candidates C d,q , including 4 classes of features that are used to estimate the usefulness of a sentence in reconstructing the original context of a document d .
Temporal Similarity. The first class of feature is aimed at capturing temporal similarity, or measuring how close the temporal expressions in a sentence c are to the creation time t d of a document d . The intuition behind this is that if c contains temporal expressions that are close to t d , it is more likely to contain information referring to the situation at time t d . In order to do that, we employ a time-decay function TSU [6, 7], which is computed as: where  X  and  X  are constants, 0 &lt;  X  &lt; 1 and  X  &gt; 0 , and  X  is a unit of time distance. The value of this function decreases exponentially with respect to the time distance between t 1 and t 2 . Given a sentence c and a document d , we compute the maximum and the average temporal sim-
Term Similarity. Using only a temporal similarity is not sufficient for re-contextualizing documents because not all information temporally close to the creation date of the document is relevant to it. For this reason, we include the original tf-idf score assigned by the search engine to each retrieved sentence as a learning feature.

Complementarity. In some cases, a sentence can be re-lated to a document without adding any useful information for understanding, e.g. the information might be already present in the document. Thus, we consider the comple-mentarity of sentences to the document, which has been introduced in [3] for summarization tasks. Given two texts s and s 2 , a complementarity metric compl is computed as: compl ( s 1 ,s 2 ) = ence between s 1 and s 2 respectively. We compute text-based and entity-based complementarities between a sentence c and a document d as: compl t ( s c ,s d ) and compl e ( E use Jaccard Index to compute similarity and difference be-tween two amounts of text s 1 and s 2 . Before computing these measures, text and entities have been preprocessed with stop-words removal, tokenization and stemming.
Sentence-Based Features. We consider 3 additional features based on information present in sentences. First, we measure how much the title field p c of a sentence is men-tioned in a document as title = matches k p is the number of words of the title that are present in the document, and k p c k is the number of words that form the title. Second, we consider the length of sentences as feature. Finally, we compute the number of entity mentions within
Ranking Model. In order to jointly consider the pro-posed features in a unique ranking model, we resort to the learning to rank paradigm [8]. In our experiments, we em-ploy different learning to rank algorithms, namely AdaRank, RankBoost, RankNet, ListNet, and LambdaMART. How-ever, the best performing algorithms are AdaRank and Rank-Boost. For this reason, we will report and discuss the results only for these two ranking algorithms.
In this section, we describe our experimental settings, present the results followed by a detailed discussion.
Document Collections. In our experiments, we used the New York Times Annotated Corpus, which contains 1.8 million documents from January 1987 to June 2007, as the document collection to be re-contextualized. We employed the Wikipedia dump of February 2013 as a context source, and used Stanford CoreNLP parser 2 for tokenization, sen-tence splitting, entity annotation, and temporal expression extraction. After splitting Wikipedia pages in sentences, the text of every sentence has been augmented with the text of its previous and following sentences, according to Section 4; entities and temporal expressions have been then extracted from the augmented text. We used Apache Solr 3 to index the augmented sentences, obtaining more than 70 millions of indexed sentences. Entities, temporal expressions, and the title of the Wikipedia page containing the augmented sentence where stored in separate fields.

Query Documents. In our evaluation, we manually se-lected 30 news articles to be contextualized from different topics (politics, science, education, sport, and wars). In par-ticular, we chose news articles from the earlier years of the collection because of their higher need for time-aware re-contextualization. For each article, we then constructed a set of queries, as described in Section 4.1, to retrieve rele-vant sentences from the Wikipedia index. We exploited only the lead paragraph of articles because it provides a concise summary of the most important topics in the article.
Relevance Assessment. Since there is no gold stan-dard for the time-aware re-contextualization task, we built a ground truth by retrieving top-k sentences for each formu-lated query by using Solr default similarity scoring function with k = 100 . In more detail, we asked human assessors to evaluate query/sentence pairs using 4 levels of relevance: 3 for excellent (very relevant context), 2 for good (relevant context), 1 for fair (related context), 0 for bad (non-relevant context). The human relevance assessment took into ac-count the requirement for providing additional information which complements the information in the lead paragraph of a given query article. More precisely, a retrieved sentence is relevant context, if it is relevant and if it provides contex-tual information not already present in the article. Finally, we considered a pair ( q , s ) as relevant if its relevance score is greater than 1, otherwise it is regarded as irrelevant . In total, we evaluated 7,390 query/sentence pairs 4 , which are used for training the learning to rank algorithms.
Parameter Settings. We used the learning-to-rank im-plementation of RankLib 5 . For AdaRank, we set a training iteration to 100, the tolerance between consecutive learning rounds to 2  X  10  X  3 , and the maximum number of consecutive feature selection to 3. For RankBoost, we set the number of rounds to 100, and the number of threshold candidates to 5. We chose these parameter settings because they achieved the best performance. The ranking models were trained via 5-fold cross validation. For TSU, we set  X  = 0 . 25 ,  X  = 0 . 5 , and  X  = 2 y , where y is the number of years.
We evaluated our approach for re-contextualization com-pared to Solr default ranking ( tf-idf ), which is considered as baseline. Different ranking models have been trained by combining the features described in Section 4 in different ways, showing how the individual features contribute to the task. We report the results for both learning to rank algo-rithm employed.

In our task, we focus on top-precision performance metrics instead of recall-based metrics: we assume that users are interested in few very useful re-contextualizing sentences, and that a great number of false positives would annoy the user during the reading. Thus, we will measure the ranking performances through the precision at 1, 3, 5, and 10 (P@1, P@3, P@5, and P@10), as well as Mean Average Precision (MAP). The reported performances are average values over the 5 folds that we created to train the models.

In Table 1, we report the results obtained by using dif-ferent feature sets in the learning model. The symbol * indicates statistically improvement over the baseline using t-test with significant at p &lt; 0 . 05 . The model consider-ing the whole set of features ( all in the table) achieves the best performances for each of the evaluation criteria and algorithms. It reaches precision values from 0.601 (P@10, RankBoost) to 0.774 (P@1, RankBoost), with improvements between 28.7% and 45.5% over the baseline. Also TSU and compl , i.e., the models considering temporal and comple-mentarity features, respectively, outperform tf  X  idf . TSU performs better than compl under most of the evaluation criteria, confirming that the temporal dimension alone gives significant insights in estimating sentence relevance. For the sake of completeness, we also report results obtained with RankBoost when considering individual features separately in the model. These are complementarity based on entities ( compl e ), complementarity based on text ( compl t ), maximal temporal similarity TSU max and average temporal similar-ity TSU avg . We do not report detailed behaviors for each sentence-based feature within sent  X  based because they did not provide relevant results when considered alone.
In order to further investigate the impact of the individual features for the task of time-aware re-contextualization, we also evaluated the effect of excluding -in turn-each class of features (temporal similarity, complementarity, tf-idf, sentence-based) from the training process. The results are reported in Table 2. For both learning algorithms, we can clearly ob-serve that the greatest decrease of performances occurs when the temporal features are excluded ( no _ TSU ). A perfor-mance decrease can be also noticed for the no _ compl model, but it is less pronounced and in some cases it is comparable with the decrease of no _ tf  X  idf . This behavior suggests that the temporal aspect has a higher influence than the complementarity for deciding if a sentence is relevant for re-contextualization. However, further investigations and experiments are envisioned to assess the role of complemen-tarity in time-aware re-contextualization. In facts, our com-plementarity measure is based on assessing similarity and difference between text via a term-based metric (Jaccard Index), which cannot capture similarity and differences be-tween content at a semantic level. Therefore, we plan to experiment with semantic approaches (e.g. topic modeling) for better assessing complementarity between two texts.
In this paper, we have presented an approach for time-aware re-contextualization of texts, in our case news articles, using Wikipedia as the contextualization source. The exper-iments showed that contextualization sentences can be iden-tified with considerable precision, when combining temporal Table 1: Overall performance of different features. Table 2: Impact of removing features from learning. features with the idea of complementarity in re-ranking rel-evant search results for the purpose of re-contextualization.
As future work, we plan to further develop the approach for time-aware re-contextualization including the considera-tion of the semantic level in computing complementarity and the development of approaches for automatically identifying context hooks and the queries deduced from them.
 Acknowledgments The work was partially funded by the
