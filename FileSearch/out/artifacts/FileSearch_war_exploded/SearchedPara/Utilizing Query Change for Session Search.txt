 Session search is the Information Retrieval (IR) task that performs document retrieval for a search session. During a session, a user constantly modifies queries in order to find relevant documents that fulfill the information need. This paper proposes a novel query change retrieval model (QCM), which utilizes syntactic editing changes between adjacent queries as well as the relationship between query change and previously retrieved documents to enhance session search. We propose to model session search as a Markov Decision Process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent X  X  actions are query changes that we observe and the search agent X  X  actions are proposed in this paper. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and 2012.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Session search; query change model; retrieval model
Session search is the Information Retrieval (IR) task that retrieves documents for a search session [4, 8, 13, 14, 15, 25, 32]. During a search session, a user keeps modifying queries in order to find relevant documents that fulfill his/her infor-mation needs. In session search, many factors, such as rel-evance feedback, clicked data, changes in queries, and user intentions, are intertwined together and make it a quite chal-lenging IR task. TREC (Text REtrieval Conference) 2010-2012 Session tracks [18, 19, 20] studied session search with a focus on the  X  X urrent query X  task, which retrieves relevant documents for the current/last query in a session based on previous queries and interactions. Table 1 shows examples from the TREC 2012 Session track. 1 All examples mentioned in this paper are from TREC 2012. For simplicity, we use  X  X x X  to refer to a TREC 2012 session where x is the session identification number.

From Table 1, we notice that queries change constantly in a session. The patterns of query changes include general to specific ( pocono mountains  X  pocono mountains park ), specific to general ( france world cup 98 reaction  X  france world cup 98 ), drifting from one to another ( pocono moun-tains park  X  pocono mountains shopping ), or slightly differ-ent expressions for the same information need ( glass blowing science  X  scientific glass blowing ). These changes vary and sometimes even look random ( gun homicides australia  X  martin bryant port arthur massacre ), which increases the difficulty of understanding user intention. However, since query changes are made after the user examines search re-sults, we believe that query change is an important form of feedback . We hence propose to study and utilize query changes to facilitate better session search.

One approach to handle query change is to classify them based on various types of explorations [20], such as specifica-tion, generalization, drifting, or slight change, then perform retrieval. Another approach is mapping queries into seman-tic graphical representations, such as ontologies [7] or query flow graphs developed from query logs [2], then studying how queries move in the graphs. However, ontology map-ping is challenging [17], which may introduce inaccurate in-termediate results and hurt the search accuracy. Moreover, relying on large scale query logs may not be applicable due to lack of such data. Therefore, although these approaches have been applied to IR tasks such as query reformulation [3] and query suggestion [2, 30], they have yet to be directly applied to session search. It is therefore necessary to explore new solutions to utilize query change for session search. We propose to model session search as a Markov Decision Process (MDP) [16, 28], which is applicable to many human decision processes. MDP models a state space and an action space for all agents participating in the process. Actions from the agents influence the environment/states and the environment/states influence the agents X  subsequent actions based on certain policies. A transition model between states indicates the dynamics of the entire system. In our MDP, queries are modeled as states. Previous queries that the user wrote influence the search results; the search results again influence the user X  X  decision of the next query. This interaction continues until the search stops.

As illustrated in Figure 1, we consider two agents in this entire process: the user agent and the search engine agent. The user agent X  X  actions are mainly human actions that are able to change search results, such as adding and deleting query terms, i.e. query change . Clicking is a human action; however, it does not explicitly impact the retrieval. There-fore, it is not considered as a user action here. Query change is the only form of user action in this paper. Based on the user actions, we design corresponding policies for the search engine agent; which is the main focus of this paper.
It is difficult to interpret the user intent [5, 31] behind query change. For instance, for a query change from Kuro-sawa to Kurosawa wife (s38), there is no indication about  X  X ife X  in the search results returned for the first query. How-ever, Kurosawa X  X  wife is actually among the information needs provided to the user by NIST X  X  topic descriptions. Our experience with TREC Session tracks suggests that informa-tion needs and previous search results are two main factors that influence query change. However, knowing information needs before search could not easily be achieved. This pa-per focuses on utilizing evidence found in previous search results and the relationship between previous search results and query change to improve session search.

In this paper, we summarize various types of query changes based on potential user intents into user agent policies. We further propose corresponding policies for the search engine agent and model them in the query change retrieval model (QCM), a novel reinforcement learning [16] inspired frame-work. The relevance of a document to the current query is recursively calculated as the reward beginning from the starting query and continuing until the current query. This research is perhaps the first to employ reinforcement learn-ing to tackle session search. Our experiments demonstrate that the proposed approach is highly effective and outper-forms the best performing TREC 2011 and 2012 session search systems.

The remainder of this paper is organized as follows. Sec-tion 2 analyzes query change and summarizes policies for the user agent. Section 3 proposes policies for the search en-gine agent. Section 4 elaborates the query change retrieval model. Section 5 discusses how to handle duplicated queries. Section 6 evaluates our approach, followed by a discussion in Section 7. Section 8 presents the related work and Section 9 concludes the paper.
 Table 2: Evidence that query change  X  q appears in previous search results D i  X  1 .
We define a search session S = {Q , D , C} as a combination of a series of queries Q = { q 1 ,...,q i ,...,q n } , retrieved docu-ment sets D = { D 1 ,...,D i ,...,D n } , and clicked information C = { C 1 ,..,C i ,...,C n } , where n is the number of queries in the session (i.e., the session length) and i indexes the queries. In TREC 2010-2012 Session tracks, each retrieved document set D i contains the top 10 retrieval results d i 1 ,...,d in decreasing relevance for q i . Each clicked data C i contains the user-clicked documents, clicking order, and dwell time. For instance, for s6 q 6 , pocono mountains camelbeach hotel (Table 1), C 6 tells us that the user clicked the 4 th ranked search result, followed by the 2 nd , with dwell time 15 seconds and 17 seconds, respectively.

TREC 2010-2012 Session Tracks aim to retrieve a list of documents for the current query, i.e. the last query q n a session, ordered in decreasing relevance. Without loss of specificity, we assume that any query between q 1 to q n could be the last query. We therefore study the problem of retriev-ing relevant documents for q i , given all previous queries q to q i  X  1 , previous retrieval results D 1 to D i  X  1 , and previous clicked data C 1 to C i  X  1 .

We define query change  X  q i as the syntactic editing changes between two adjacent queries q i  X  1 and q i : q can be written as a combination of the shared portion between q i and q i  X  1 and query change: q i = ( q i  X  q
The query change  X  q i comes from two sources. First, the added terms, which we call positive  X  q , are new terms that the user adds to the previous query. Second, the removed terms, which we call negative  X  q , are terms that the user deletes from the previous query. For example, in Table 1 s37,  X  X S X  and  X  X olicy X  are the added terms; while in s28,  X  X tock X  and  X  X arket X  are the removed terms.

We call the common terms shared by two adjacent queries theme terms since they often represent the main topic of a session. For example, in Table 1 s37 the theme terms are  X  X erck lobby X . 2
We thus decompose a query into three parts as theme terms , added terms , and removed terms and write it as: q = ( q i  X  q i  X  1 )+(+ X  q i )  X  (  X   X  q i ) = q theme +(+ X  q where q theme are the theme terms, + X  q i and  X   X  q i represent added terms and removed terms, respectively.

Our observations suggest that documents that have been examined by the user factor in deciding the next query change. We therefore propose the following important as-sumption between  X  q i , the query change between adjacent
We perform K-stemming to all query terms. For instance,  X  X obbists X  and  X  X obbying X  are both stemmed to  X  X obby X . queries q i and q i  X  1 , and D i  X  1 , the search results for q
The assumption basically says that previous search results decide query change . In fact, previous search results D could influence query change  X  q i in quite complex ways. For instance, the added terms in s37 (Table 1) q 1 to q 2 , are  X  X S X  and  X  X olicy X . D 1 contains several mentions of  X  X olicy X , such as  X  A lobbyist who until 2004 worked as senior policy advisor to Canadian Prime Minister Stephen Harper was hired last month by Merck  X . However, these  X  X olicy X -related mentions are about  X  X anada policy X  whereas the user adds  X  X S policy X  in q 2 . This suggests that the user might have been inspired by  X  X olicy X  in D 1 , however he preferred the policy in US , not in Canada . Therefore, instead of simply cutting and pasting identical terms from D i  X  1 , the user creates related terms to add for the next search.

In another example, s28 (Table 1) q 1 ,  X  X tock X  and  X  X arket X  are frequent terms that are similar to stopwords. Docu-ments in D 1 are hence all about them and totally ignore the theme terms  X  X rance world cup 98. X  In q 2 , the user removes  X  X tock market X  to boost rankings for documents about the theme terms. In this case, removing terms is not only about generalization, but also about document re-ranking.
To provide a convincing foundation for our approach, we look for evidence to support our assumption. We investigate whether  X  q i (at least) appears in D i  X  1 . Table 2 shows how often theme terms, added terms, and removed terms are present in D i  X  1 for both TREC 2011 and 2012 datasets. Around 90% of the time theme terms occur in D i  X  1 most removed terms ( &gt; 60%) appear in D i  X  1 . 3 Added terms are new terms for the previous query q i  X  1 ; we thus expect to see few occurrences of added terms in D i  X  1 . Surprisingly, however, more than a third of them appear in D i  X  1 suggests that it is quite probable that previous search results motivate the subsequent query change.

Table 3 summarizes various types of query changes into possible policies for the user agent. This table mainly serves as a guide for us to design the policies for the search engine agent. We do not perform a thorough user study to validate this table. However, we believe that it is a good representa-tive of various search scenarios and can help design a good session search agent.

Along two dimensions, Table 3 summarizes the user agent X  X  actions and possible policies. The dimensions are whether a previous query term t  X  q i  X  1 appears in previous search
A third of query terms that do not appear in D i  X  1 removed by the user. results D i  X  1 (the left most column) and whether the user likes D i  X  1 and the occurrence of t in D i  X  1 (the top most row). Combinations of the two dimensions yield 4 main cases (as in a contingency table) and 8 sub-cases. For each case, we identify four items: a rough guess of user intention, the user X  X  actual action, an example, and the semantic ex-ploration type for this action. For example, query change in s6 q 8  X  q 9 , pocono mountains chateau resort attractions  X  pocono mountains chateau resort getting to can be inter-preted as the following. Previous query term  X  X ttractions X  appears in D i  X  1 and the user likes the returned documents D i  X  1 . One possibility is that he is satisfied with what he reads and moves to the next information need. Therefore, the user removes  X  X ttraction X  and adds new terms  X  X etting to X  as the new query focus. This is a drift in search focus. (case 2 in Table 3)
We further group the cases in Table 3 by types of user actions, i.e., query change, and summarize them into:  X  Theme terms ( q theme ), terms that appear in both q and q i . In fact, they often appear in many queries in a session. It implies a strong preference for those terms from the user. If they appear in D i  X  1 , it shows that the user favors them since the user issues them again in q i . If they do not appear in D i  X  1 , the user still favors towards them and insists to include them in the new query. This corresponds to t in cases 1 and 3 in Table 3.  X  Added terms (+ X  q ), terms that appear only in q i , not in q i  X  1 . They indicate specification, destination of drifting, or destination of slight change. If they appear in D i  X  1 for the sake of novelty [14], they will not be favored in D
If they do not appear in D i  X  1 , which means that they are novel and the user favors them now. This corresponds to t 0 in cases 1, 2, 6, 7, and 8, and t 00 in case 4 in Table 3.  X  Removed terms (  X   X  q ), terms that appear only in q i  X  1 not in q i . They indicate generalization, source of drift-ing, and source of slight change. If they appear in D i  X  1 removing them means that the user observes them and dislikes them. If they do not appear in D i  X  1 , the user still dislikes the terms since they are not in q i anyway. This corresponds to t in cases 2, 5, 6, 7, and 8 in Table 3.
The search engine agent observes query change from the user agent and takes corresponding actions. For each type of query change, theme terms , added terms , and removed terms , we propose to adjust the term weights accordingly for better retrieval accuracy. The search engine agent X  X  action include Table 4: Search engine agent X  X  policy. Actions are adjust-ments on the term weights.  X  means increasing,  X  means decreasing, and  X  means keeping the original term weight. q + X  q Y  X   X  X olicy X  in s37, q 1  X  q 2 N  X   X  X S X  in s37, q  X   X  q Y  X   X  X eaction X  in s28, q 2  X  q 3 N  X   X  X egislation X  in s32, q increasing , decreasing , and maintaining the term weights. Based on the observed query change as well as whether the query terms appeared in the previous search results D i  X  1 we can sense whether the user will favor the query terms in the current run of search. Table 4 illustrates the policies that we propose for the search engine agent.

As shown in Section 2, theme terms q theme often appear in many queries in a session and there is a strong preference for them. Thus, we propose to increase the weights of theme terms no matter whether they appeared in D i  X  1 or not (rows 1 and 2 in Table 4). In the latter case, if a theme term was not found in D i  X  1 (top retrieval results), it is likely that the documents containing them were ranked low. Therefore, the weights of theme terms need to be raised to boost the rank-ings of those documents (row 2 in Table 4). However, since theme terms are topic words in a session, they could appear like stopwords within the session. To avoid biasing too much towards them, we lower their term weights proportionally to their numbers of occurrences in D i  X  1 .

For added terms + X  q , if they occurred in previous search results D i  X  1 , we propose to decrease their term weights for the sake of novelty [14]. For example, in s5 q  X  X ocono mountains X   X   X  X ocono mountains park X , the added term  X  X ark X  appeared in a document in D 5 . If we use the original weight of  X  X ark X , this document might still be ranked high in D 2 and the user may dislike it since he read it before. We hence decrease added terms X  weights if they are in D i  X  1 (row 3 in Table 4). On the other hand, if the added terms did not occur in D i  X  1 , they are the new search focus and we increase their term weights (row 4 in Table 4). In an in-teresting case (s37 q 1  X  q 2 ), part of + X  q ,  X  X olicy X , occurred in D 1 whereas the other part,  X  X S X , did not. To respect the user X  X  preference, we increase the weight of  X  X S X  while de-creasing that of  X  X olicy X  to penalize documents about other  X  X olices X  including  X  X anada policy X .

For removed terms  X   X  q , if they appeared in D i  X  1 , their term weights are decreased since the user dislikes them by deleting them (row 5 in Table 4). For example, in s28 q 2 q ,  X  X eaction X  existed in D 2 and is removed in q 3 . However, if the removed terms are not in D i  X  1 , we do not change their weights since they are already removed from q i by the user (row 6 in Table 4).

In the sections below, we follow policies proposed for the search engine agent as shown in Table 4 and incorporate them into a novel query change retrieval model (QCM).
Markov Decision Process (MDP) [16, 28] models a state space S and an action space A . Its states S = { s 1 ,s 2 change from one to another according to a transition model T = P ( s i +1 | s i ,a i ), which models the dynamics of the entire system. A policy  X  ( s ) = a indicates that at a state s , what are the actions a can be taken by the agent. In session search, we employ queries as states. Particularly, we denote q as state, T as the transition model P ( q i | q i  X  1 as documents, and A as actions. Actions include keeping, adding, and removing query terms for the user agent and increasing, decreasing, and maintaining the term weights for the search engine agent.
 In a MDP, each state is associated with a reward function R that indicates possible positive reward or negative loss that a state and an action may result. In session search, we consider the reward function to be the relevance function.
Reinforcement learning [16] offers general solutions to MDP and seeks for the best policy for an agent. Each policy has a value associated with the policy and denoted as V which is the expected long-term reward starting from state s and continuing with policy  X  from then on. In a MDP, it is believed that a future reward is not worth quite as much as a current reward and thus a discount factor  X   X  (0 , 1) is applied to future rewards. By considering the discount fac-tor, the value function starting from s 0 for a policy  X  can be written as: V  X  ( s 0 ) = E  X  [ R ( s 0 ) +  X R ( s 1 ) +  X  E [ P  X  t =0  X  t R ( s i )]. The Bellman equation [16] describes the optimal value V  X  for a state s in the long run and is often used to obtain the best value for a MDP: where s 0 is the next state after s , V  X  ( s ) and V  X  ( s optimal values for s and s 0 .

For session search, we observe that the influence of previ-ous queries and previous search results to the current queries, becomes weaker and weaker. The user X  X  desire for novel doc-uments also supports this argument. We hence propose to employ the reinforcement learning model backwards. That is, instead of discounting the future rewards, we discount the past rewards, i.e. the relevant documents that appeared in the previous search results.

We propose the query change retrieval model (QCM) as the following. We consider the task of retrieving relevant documents for q i as ranking documents based on the re-ward, i.e., how relevant it is to q i . Inspired by the Bellman equation, we model the relevance of a document d to the current query q i as: Score ( q i ,d ) = P ( q i | d )+  X  X which recursively calculates the reward starting from q 1 continues with the search engine agent X  X  policy until q i (0 , 1) is the discount factor, max D i  X  1 P ( q i  X  1 maximum of the past rewards, P ( q i | d ) is the current reward, and P ( q i | q i  X  1 ,D i  X  1 ,a ) is the query transition model.
The first component in Eq.1, P ( q i | d ), measures the rel-evance between q i and a document d that is under evalu-ation. This component can be estimated by the Bayesian belief network model [27]: P ( q i | d ) = 1  X  Q t  X  q where P ( t | d ) is calculated by the multinomial query genera-tion language model with Dirichlet smoothing [33]: P ( t | d ) = rences of term t in document d , P ( t | C ) calculates the prob-ability that t appears in corpus C based on Maximum Like-lihood Estimation (MLE), | d | is the document length, and  X  is the Dirichlet smoothing parameter (set to 5000).
The remaining challenges of calculating Eq.1 include max-timating the transition model P ( q i | q i  X  1 ,D i  X  1 ,a ). They are described in Section 4.1 and Section 4.2, respectively.
When considering the past/future rewards, MDP uses only the optimal (the maximum possible) values from those past /future rewards. This is reflected in max D i  X  1 P ( q i  X  1 as part of Eq. 1.

Prior research [10, 22] suggests that Satisfying (SAT) clicks, i.e., clicked documents with dwell time longer than 30 sec-onds [10, 22], are probably the only ones that are effective at predicting user behaviors and relevance judgments. Since the user also skims snippets in search interactions, in this work, we consider both the top 10 returned snippets and SAT clicks as effective previous search results and denote
To obtain an maximum reward from all possible reward query q i  X  1 and all previous search results d i  X  1  X  D propose to generate a maximum rewarding document , de-noted as d  X  i  X  1 . We further propose that the candidates for the d  X  i  X  1 should only be selected from the effective previ-ous search results D e i  X  1 . We define d  X  i  X  1 as the document(s) that is the most relevant to q i  X  1 . To discover d  X  i  X  1 rank all the documents (either a snippet or a document) d d P ( t | d i  X  1 ) is calculated by MLE: P ( t | d i  X  1 ) = ) is the number of occurrences of term t in document d i  X  1 | d i  X  1 | is the document length. We do not apply smoothing here since P ( t | d i  X  1 ) can be zero, i.e., t /  X  D e rely on this property in later calculation.

After ranking documents d i  X  1 in D e i  X  1 , we generate d by the following options: (1) using the document with the largest P ( q i  X  1 | d i  X  1 ), (2) concatenating the top k documents all documents in D e i  X  1 . Experiments show that option (1) works the best and we use this setting throughout the paper. For notation simplicity, we use D i  X  1 from now on to denote effective previous search results.
The transition model indicated in Eq. 1 is P a P ( q i | q D i  X  1 ,a ). It includes the probabilities of query transitions under various actions. We incorporate polices designed in Table 4 to calculate it.

Search engine agent performs actions based on user agent X  X  actions. We need to identify user X  X  actions, i.e. query change  X  q before search engine takes actions. Particularly, we rec-ognize  X  q by the following procedure. First, we generate q theme based on the Longest Common Subsequence (LCS) [11] in both q i  X  1 and q i . A subsequence is a sequence that appears in two strings in the same relative order but is not necessarily continuous. The LCS can be the common prefix or the common suffix of the two queries; it can also consist of several discontinuous common parts from the two queries. Take s6 q 6  X  q 7 as an example: q 6 = X  X ocono mountains camelbeach hotel X , q 7 = X  X ocono mountains chateau resort X , q theme = LCS( q 6 , q 7 ) =  X  X ocono mountains X . Next, we rec-ognize added terms + X  q and removed terms  X   X  q . Gener-ally, the terms that occur in the current query but not in the previous query constitute + X  q ; while the terms occur in the previous query but not in the current query constitute  X   X  q . In the above example,  X   X  q 7 =  X  X amelbeach hotel X , and + X  q 7 =  X  X hateau resort X .

The search engine actions are decreasing , increasing , and maintaining term weights. According to Table 4 rows 3 and 5, we decrease a term X  X  weight if the query change, either + X  q or  X   X  q , occurred in the effective previous search re-sults D i  X  1 . We propose to deduct term t  X  X  weight by P ( t | d ), i.e. t  X  X  default contribution to the relevance score between q and the document under evaluation (denoted as d ). Fur-thermore, since t already occurred in D i  X  1 , for the sake of novelty, we deduct more weight that is proportional to t  X  X  frequency in D i  X  1 such that the more frequently t occurred in D i  X  1 , the more heavily t  X  X  weight is deducted from the current query q i and d . We formulate this weight deduction for a term t  X  + X  q or t  X  X  X   X  q as: where d  X  i  X  1 denotes the maximum rewarded document, d is the document under evaluation, and P ( t | d ) is calculated by MLE. We apply the log function to avoid numeric underflow.
We notice that Eq. 2 has an interesting connection with the Kullback-Leibler divergence (KL divergence) [33]: where KLD t  X  d  X  t to the KL divergence between two documents X  language models  X  d  X  tween  X  d  X  to D i  X  1 , and the less deduction to the relevance score. In this sense, Eq. 2 models novelty for the added terms and the removed terms during a query transition.

According to Table 4 row 4, we increase a term X  X  weight if it is an added term and did not occur in D i  X  1 . We propose to raise the term weight proportional to its inverse document frequency ( idf ). This is to make sure that while increasing a preferred term X  X  weight, we avoid increasing its weight too much if it is a common term in many documents. We formulate this weight increase for a novel added term t ( t  X  + X  q and t /  X  D i  X  1 ) as: where idf ( t ) is the inverse document frequency of t in Corpus C and P ( t | d ) is calculated by MLE. Note that this term weight adjustment is in a form of tf-idf .

The increasing in term weights also applies to theme terms, which corresponds to rows 1 and 2 in Table 4. Theme terms repeatedly appear in a session, which implies the impor-tance of them. Similar to the novel added terms, we should avoid increasing their weights too much. We could discount the increment proportional to idf . However, theme terms are topical/common terms within a session, not necessarily common terms in the entire corpus. Therefore, idf may not be applicable here. We hence employ the negation of the number of occurrences of t in previous maximum rewarding document, 1  X  P ( t | d  X  i  X  1 ), to substitute idf . We formulate this weight increase for a theme term t  X  q theme as: where d  X  i  X  1 denotes the maximum rewarded document and P ( t | d ) is calculated by MLE.

For removed terms that did not appear in D i  X  1 (Table 4 row 6), the search agent does not change their term weights.
By considering all possible cases for the transition model as defined in Eq. 1, the relevance score between the current query q i and a document d is represented as below: Score ( q i ,d ) = log P ( q i | d ) +  X  X  X   X  X  X   X  X where  X  ,  X  , , and  X  are parameters for each types of actions. Note that we apply different parameters  X  and  X  on + X  q and  X   X  q , since added terms and removed terms may affect the retrieval differently. We report the parameter selection in Section 6. It is worth noting that Eq. 6 is valid only when i &gt; 1. When i = 1, there is no previous result for q 1 . We thus use as a base case. P ( q 1 | d ) is calculated by Eq. 4.
Using Eq. 7 as the base case for the recursive function described in Eq. 1, we obtain the overall document relevance score Score session ( q n ,d ) for a session that starts at q ends at q n by considering all queries in the session: = Score ( q n ,d ) +  X  [ Score ( q n  X  1 ,d ) +  X Score session = where q 1 ,q 2 ,  X  X  X  ,q n are in the same session, and  X   X  (0 , 1) is the discount factor. Eq. 8 provides a form of aggregation over the relevance functions of all the queries in a session. Duplicated queries sometimes occur in a search session. Prior work shows that removing duplicated queries could effectively boost the search accuracy [8, 19]. Duplicated queries often occur when a user is frustrated by irrelevant documents in search results and comes back to one of the previous queries for a fresh start. For example, in s6 (Ta-ble 1), q 2 and q 4 are duplicates and both search for pocono mountains pennsylvania hotels . The query between them is q : pocono mountains pennsylvania things to do . It suggests that the user might dislike the search results for q 3 and he returns to q 2 to search again ( q 2 = q 4 ).

To detect query duplicates, we first remove punctuations and white spaces in queries, then apply stemming on them. Table 5: Dataset statistics for TREC 2011 and 2012 Session. Next we determine exact string matches between every query pair. The exactly matched query pairs are identified as du-plicated queries.

Since the user may dislike the queries and their corre-sponding search results between two duplicated queries, we propose to eliminate from the MDP the undesired queries and their interactions. We achieve this by setting the dis-count factor to zero for any interaction between two dupli-cated queries as well as that for the earlier query in the two. The new discount factor  X  0 can be calculated as: where  X  i is the original discount factor for the i th query,  X  is the updated discount factor for the i th query after de-duplication.

For the above example s6, the effects from q 2 and q 3 on the session are eliminated. The entire session is now equivalent to q 1 ,q 4 ,q 5 ,...,q 11 . The evaluation datasets are from TREC 2011 and 2012 Session tracks [18, 19]. Table 5 lists the statistics about these two datasets. Each search session includes several queries and the corresponding search results. The users (NIST assessors) were given a topic description about infor-mation needs before they searched. For example, s85 (Table 1) are related to topic 43  X  X hen is scientific glass blowing used? What are the purposes? What organizations do sci-entific glass blowing? X  Multiple sessions can relate to the same topic. The search engine used to create the sessions was Yahoo! BOSS. The top 10 returned documents were shown to the users and they clicked documents that were interesting to them and interacted with the system. We use TREC X  X  official ground truth and official evaluation metrics nDCG@10 and MAP.

The corpus used in this evaluation is ClueWeb09 Cate-gory B collection (CatB). 4 CatB contains the first 50 mil-lion English pages crawled from the Web during January to February 2009. We filter out the spam documents by removing documents whose WateQCMoo X  X   X  X roupX X  spam ranking scores [6] are less than 70.

We compare the following systems in this evaluation:  X  Lemur : Directly submitting the current query q n (with punctuations removed) to the Lemur search engine [21] (language modeling + Dirichlet smoothing) and obtain the returned documents.  X  TREC best : The top TREC system as reported by NIST [13, 14]. It adopts a query generation model with rele-vance feedback and handles document novelty. CatB was used in their TREC submissions. This system is used as the baseline system in this evaluation .  X  Nugget : Another top TREC 2012 session search system groups semantically coherent query terms as nuggets and http://lemurproject.org/clueweb09/. Table 6: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  X  at p &lt; 0 . 05 level. Table 7: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2011 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  X  at p &lt; 0 . 05 level. creates structured Lemur queries [8]. We re-implement and apply it on both TREC 2011 and 2012.  X  TREC median : The median TREC system as reported by
NIST [18, 19].  X  QCM : The proposed query change retrieval model.  X  QCM + De-Duplicate (Dup) : The proposed query change retrieval model with duplicated queries removed.
Table 6 and Table 7 demonstrate search accuracy for all systems under comparison for TREC 2012 and TREC 2011, respectively. The evaluation metrics are nDCG@10 and MAP, the same as in the official TREC evaluations. TREC best serves as the baseline.

Table 6 shows that the proposed QCM approach outper-forms the best TREC 2012 system on nDCG@10 by 4.1%, which is statistically significant (one sided t -test, p = 0 . 05). The search accuracy is further improved by 0.46% through removing the duplicated queries. The experimental results strongly suggest that our approach is highly effective.
Table 7 shows that for TREC 2011, our approach again outperforms the baseline by a statistically significant 7.24% (one sided t -test, p = 0 . 05) and achieves a further improve-ment of 9.34% by the QCM+Dup approach. For TREC 2011, the performance gain by performing de-dup is 2.1%, which is bigger than that for TREC 2012 (0.46%). The reason is probably because that TREC 2012 only has 5 du-plicated queries while TREC 2011 has 16 (shown in Table 5). However, the best approach for TREC 2011 is the nugget approach, which is slightly better than QCM+Dup.

Table 5 illustrates the dataset differences between TREC 2011 and 2012. These differences may affect search accuracy. The average number of sessions per topic is 2.04 in 2012, that is more than that in 2011 (1.23). Moreover, on average, TREC 2012 sessions contain less queries per session (3.03) than 2011 (3.68). As a result, the shorter sessions in 2012 may make the search task more difficult than 2011 since less information are provided by previous interactions. Another difference is that 2012 sessions have fewer (sometimes even none) relevant documents than 2011 sessions in CatB ground truth. It unavoidably hurts the performance for any retrieval system. Generally, we observe lower search accuracy in 2012 (Table 6) than in 2011 (Table 7).
We investigate good values for parameters in Eq. 6. A supervised learning-to-rank method should be able to find the optimal values for those parameters. However, in this paper, we take a step-by-step parameter tuning procedure and leave the supervised learning method as future work.
We add each component, i.e., theme terms, added terms, and removed terms, one by one into Eq. 6. The tuning is performed for QCM only and the parameters are shared between QCM and QCM+Dup.

First, we plot nDCG@10 against  X  while setting other parameters to 0 (Figure 2(a)).  X  represents the parameter for theme terms.  X  ranges over [1.1, 2.5] by an interval of 0.1. We notice that nDCG@10 reaches its maximum at  X  = 2 . 2. We find 2 other local maximums at 1.6 and 1.2 for  X  .
Next, we fix  X  to the above values and plot nDCG@10 against  X  (Figure 2(b)).  X  is the parameter for added terms that appeared in effective previous search results; we call them old added terms.  X  ranges over [1.0, 2.4] by an interval of 0.2. We choose the top 2 local values from each curve and pick 6 combinations for (  X , X  ) as indicated in Figure 2(c).
Then, we fix (  X , X  ) and plot nDCG@10 against (Figure 2(c)). is the parameter for added terms that did not appear in effective previous search results; we call them novel added terms. ranges over [0.05, 0.1] by an interval of 0.01. All the curves show similar trends and reach the highest nDCG@10 at around 0.07. We hence fix to 0.07.

Finally, we plot nDCG@10 against  X  (Figure 2(d)) with the parameter combinations that we discover eerlier. Even-tually, nDCG@10 reaches its peak 0.3353 at  X  = 2 . 2 , X  = 1 . 8 , = 0 . 07, and  X  = 0 . 4. We apply this set of parameters to both QCM and QCM+Dup.

As we can see,  X  ,  X  , and  X  are much larger than . This range of [1 , 10], while in Eq. 2 and Eq. 5, P ( t | d Figure 3: Discount factor  X  . Figure 4: Error types. Table 9: nDCG@10 for various aggregation schemes.  X  p is 0.4 in PvC.  X  is 0.92 in QCM and QCM+Dup. TREC 2012 best serves as the baseline. A significant improvement over the baseline is indicated with a  X  at p &lt; 0 . 05 level. in the range of [0,0.1]. Therefore, the values of are two magnitudes less than that for the other parameters. Among  X  ,  X  , and  X  , we find that  X  and  X  are larger than  X  , which implies that theme terms and added terms may play more important roles in session search than removed terms.
QCM proposes an effective way to aggregate all queries in a session as in Eq.8. We compare how effective it is to prior query aggregation methods. A query aggregation scheme can be represented as: Score ( session,d ) = P n i =1  X  i where Score ( q i ,d ) is the relevance scoring function of d and q and  X  i is the query weight for q i . [8] proposed several aggregation schemes for TREC 2012 Session track. The schemes are: uniform (all queries are equally weighted), previous vs. current (known as PvC; all previous queries are discounted by  X  p , while the current query uses a complementary and higher coefficient (1  X   X  p and distance-based (previous queries are discounted based on a reciprocal function of queries X  positions in the session).
We express various query aggregation schemes in terms of the discount factor  X  in order to compare them with our approach. From Table 8, we find that QCM degenerates to uniform when  X  = 1. Previous queries in PvC and Distance-based schemes are also discounted as they are in QCM, but with different decay functions.

The search accuracy for different aggregation schemes are compared in Table 9. QCM performs the best for both TREC 2011 and 2012. The PvC scheme is the second best scheme, which confirms what is reported in [8]. The Distance-based scheme gives the worst performance.

We explore the best discount factor  X  for QCM over (0 , 1) by an interval of 0.02. Figure 3 illustrates the relationship between nDCG@10 and  X  . nDCG@10 climbs to its peak 0.3368 when  X  = 0 . 92. The result suggests that a good discount factor  X  is very close to 1, implying that previous queries contribute to the overall search accuracy nearly the same as the last query. It suggests that in QCM, a discount between two adjacent queries should be mild.
A main contribution of our approach is that we treat a search session as a continuous process by studying changes among query transitions and modeling the dynamics in the entire session. Through the reinforcement learning style framework, our system provides the best aggregation scheme for all queries in a session (Table 9). This allows us to better handle sessions that demonstrate evolution and exploration in nature than most existing systems do. On the contrary, for sessions that are clear in search goals and lack of a ex-ploratory nature, the advantage of our system over other systems looks less significant.

This can be seen in Table 10, which illustrates the search accuracy for the TREC best, Nugget, and our system for various classes of sessions. The TREC best is used as the baseline and we also show the percentile improvement over it in Table 10. TREC 2012 sessions were created by consider-ing and hence can be classified into two facets: search target (factual or intellectual) and goal quality (specific/good or amorphous/ill) [19]. Table 10 shows that QCM works very well for all classes of sessions. Specifically, QCM works even better, i.e. outperforms the TREC best even more signifi-cantly, for sessions that search for intellectual targets as well as sessions that search with amorphous goals. In our opin-ion, this is due to that intellectual tasks produce new ideas or new findings (e.g. learn about a topic or make decision based on the information collected so far) while searching. Both intellectual and amorphous sessions rely more on pre-vious search results. Thus, users reformulate queries based more on what they have retrieved, not the vague informa-tion need. This is a scenario where our approach is good at since we employ previous search results to guide the search engine X  X  action. For specific and factual sessions, users are clearer in search goals, query changes may come less from the previous search results. In summary, our good performance on both intellectual task and amorphous task is consistent with our efforts of modeling query changes.

Moreover, we benefit from term-level manipulation in var-ious aspects in our system. The first aspect is novelty. Both the TREC best system and our system handle novelty in a session. The TREC best system only deals with novelty at the document level. They consider documents that have been examined by the user in a previous interaction not novel and the rest are novel [14]. That is, they determine novelty purely based on document identification number, not the actual content. Through studying whether query terms appeared in previous search results, our approach evaluates and models novelty at the term level (or concept level), which we believe better represents the evolving informa-tion needs in a session. The second aspect is query han-dling. The Nugget approach [8] treats queries at the phrase level and formulates structured queries based on phrase-like nuggets . The approach achieves good performance, espe-cially for TREC 2011. However, due to complexity in nat-ural language, nugget detection is sensitive to dataset and the approach X  X  performance is not quite as stable as ours on different datasets.

Lastly, our system benefits from trusting the user. Our ap-proach does not use too much materials from other resources such as anchor texts, meta data, or click orders, as many other approaches do [8, 26]. We believe that the most direct and valuable feedback is the next query that the user enters. In this work, we manage to capture the query change and investigate the reasons behind it. We use ourselves as users to summarize possible human users X  reasoning and actions. More detailed analysis about user intent might be useful for researchers to understand web users, however, it might be overwhelming (too fine-grained or too much semantics) for a search engine that essentially only counts words. Our system retrieves nothing for 22 out of 98 sessions in TREC 2012. To analyze the reason for the poor performance for those sessions, we study their topic descriptions, queries, and ground truth documents. We summarize the types of errors as  X  X wo theme concepts X ,  X  X ll query X ,  X  X ew relevant documents X , and others. Figure 4 shows how many sessions that we fail to retrieve under each error type.

We call the first type of errors  X  X wo theme concepts X . It comes from a type of session where the information need cover more than one concepts. For instance, s17 and s18 share the the same topic  X ... To what extent can decisions and policies of the Indian government be credited with these wins? X . Queries in s17 and s18 ask about both concepts  X  X n-dian politics X  and  X  X iss universe X . Unfortunately, very few relevant documents about both theme concepts exist in the corpus. The retrieved documents are about either concept, but none is about both. Eight sessions belong to this type. As future work, we can improve our system by incorporating structures in queries, and enable more sophisticated opera-tors such as Boolean and proximity search.

The second type of errors is  X  X ll query X , where in such sessions, queries themselves are ill formulated and do not well-represent the information needs indicated in the given topic. A common mistake is that the user misses some sub-information need. For example, the topic for s16 is:  X ... you want to reduce the use of air conditioning in your house ... you could protect the roof being overly hot due to sun ex-posure... Find information of ... how it could be done. X  A good query for this topic should include roof and air condi-tioning . However, the queries that the user issued for s60,  X  X educe airconditioning X  and  X  X ttic insulation air condition-ing costs X , do not mention roof at all. Because of this ill query formulation, our system yields no relevant documents for s60. On the other hand, for s59, which shares the same information need with s60, our system achieves a nDCG@10 of 0.48 simply because s59 queries  X  X ool roof X . It suggests that ill queries mislead the search engine and yield poor re-trieval performance. Four sessions belong to this type. As future work, we will explore effective query suggestion by studying sessions that share the same topic.
 The third type of errors is  X  X oo few relevant documents X . For sessions with too few relevant documents in the ground truth, our system do not perform well. In total 2,573 rele-vant documents exist in CatB for all 48 TREC 2012 topics; on average 53.6 relevant documents per topic. However, topics 10, 45, 47 and 48, each has no more than 2 relevant documents and topic 47 (s92 to s95) has no relevant docu-ment in CatB (Table 5). This problem could be reduced if we index the entire ClubWeb09 CatA collection.

Figure 4 also indicates in which classes of sessions these errors lie. We find that all  X  X wo theme concept X  errors be-long to sessions created with amorphous goals while all  X  X oo few relevant documents X  errors belong to those with specific goals. Moreover,  X  X ll queries X  tend to occur more in sessions with amorphous goals. Note that  X  X ll query X  and  X  X ew rel-evant documents X  are errors due to either the user or the data. There might not be much room for our system to im-prove over them. However,  X  X wo theme concepts X  is where our system can certainly make further improvements. Session search is a challenging IR task [4, 8, 13, 14, 25, 32]. Existing approaches investigate session search from various aspects such as semantic meanings of search tasks [23], doc-ument novelty [14], and phrase structure in queries [8]. The best TREC system [13, 14] employs an adaptive browsing model by considering both relevance and novelty; however it does not demonstrate improvement by handling novelty. In this paper, we successfully model query and document nov-elty by investigating the relationship between query change and previous search results. Moreover, our analysis on query change does not require knowledge of semantic types for the sessions as [23] proposed.

Our proposed work is perhaps the most similar to the problem of query formulation [1, 9, 12, 24] and query sug-gestion [29]. [12] showed that certain query changes such as adding/removing words, word substitution, acronym expan-sion, and spelling correction are more likely to cause clicks, especially on higher ranked results. The finding is generally consistent with our view of query change. However, their work only emphasized on understanding of query changes, without showing how to apply it to help session search. [24] examined the relationship between task types and how users change queries. They classified query changes by semantic types: Generalization, Specialization, Word Substitution, Repeat, and New. Similar to [12], however, [24] stopped at understanding query changes and didn X  X  apply their findings to help session search. This probably makes us the first to utilize query changes in actual retrieval. [1] derived query-flow graph, a graph representation of user query behavior, from user query logs. The approach detected query chains in the graph and recommended queries based on maximum weights, random walk, or just the previous query. Other mining approaches [1, 29] identify the importance of query change in sessions; however, they require the luxury of large user query logs.

This research is perhaps the first to employ reinforcement learning to solve the Markov Decision Process demonstrated in session search. Reinforcement learning is complex and dif-ficult to solve. Its solutions include model-based approaches and model-free approaches [16]. The former learn the transi-tion model and the reward function for every possible states and actions and mainly employ MLE to estimate the model parameters. Others also use matrix inversion or linear pro-gramming to solve the Bellman equation. It works well when state spaces are small. However, in our case, the state space is large since we use natural language queries as the states; hence we could not easily apply model-based approaches in practice. In this work, we effectively reduce the search space by summarizing users X  and search engine X  X  actions into a few types and employ a model-free approach to learn value func-tions directly.
This paper presents a novel session search approach (QCM) by utilizing query change and modeling the dynamic of the entire session as a Markov Decision Process. We assume that query change is an important form of feedback. Based on this assumption, through studying editing changes be-tween adjacent queries, and their relationship with previous retrieved documents, we propose corresponding search en-gine actions to handle individual term weights for both the query and the document. In a reinforcement learning in-spired framework, we incorporate various ingredients present in session search, such as query changes, satisfactory clicks, desire for document novelty, and duplicated queries. The proposed framework provides a theoretically sound and gen-eral foundation that allows more novel features to be incor-porated. Experiments on both TREC 2011 and 2012 Ses-sion tracks show that our approach is highly effective and outperforms the best session search systems in TREC. This research is perhaps the first to employ reinforcement learn-ing in session search. Our MDP view of modeling session search can potentially benefit a wide range of IR tasks. This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations ex-pressed in this paper are of the authors, and do not neces-sarily reflect those of the sponsor.
