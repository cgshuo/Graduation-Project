 The profusion of digitally available images has nat-urally led to an interest in the field of automatic im-age annotation and retrieval. A number of studies attempt to associate image regions with the corre-sponding concepts. In (Duygulu et al., 2002), for example, the problem of annotation is treated as a translation from a set of image segments (or blobs) to a set of words. Modeling the association between blobs and words for the purpose of automated an-notation has also been proposed by (Barnard et al., 2003; Jeon et al., 2003).

A recurring hindrance that appears in studies aim-ing at automatic image region annotation is the lack of an appropriate dataset. All of the above studies use the Corel image dataset that consists of 60,000 images annotated with 3 to 5 keywords. The need for an image dataset with annotated image regions has been recognized by many researchers. For ex-ample, Russell et al (2008) have developed a tool and a general purpose image database designed to delineate and annotate objects within image scenes.
The need for an image dataset with annotated ob-ject boundaries appears to be especially pertinent in the biomedical field. Organizing and using for re-search the available medical imaging data proved to be a challenge and a goal of the ongoing research. Rubin et al (2008), for example, propose an ontol-ogy and annotation tool for semantic annotation of image regions in radiology.

However, creating a dataset of image regions manually annotated and delineated by domain ex-perts, is a costly enterprise. Any attempts to auto-mate or semi-automate the process would be of a substantial value.

This work proposes an approach towards auto-matic annotation of regions of interest in images used in scientific publications. Publications abun-dant in image data are an untapped source of an-notated image data. Due to publication standards, meaningful image captions are almost always pro-vided within scientific articles. In addition, image Regions of Interest (ROIs) are commonly referred to within the image caption. Such ROIs are also com-monly delineated with some kind of an overlay that helps locating the ROI. This is especially true for hard to interpret scientific images such as radiology images. ROIs are also described in terms of location within the image, or by the presence of a particular color. Identifying ROI mentions within image cap-tions and visual clues pinpointing the ROI within the image would be the first step in building an object delineated and annotated image dataset. The goal of this research is to locate visually salient image region characteristics in the text surrounding scientific images that could be used to facilitate the delineation of the image object boundaries. This task could be broken down into two related subtasks -1) locating and classifying textual clues for visu-ally salient ROI features (Image Markers), and 2) lo-cating the corresponding ROI text mentions (Image Marker Referents). Table 1 gives a classification of Image Markers including examples of Image Mark-ers and Image Marker Referents. Figure 1 shows the frequency of Image Marker occurrences. Cohen et al (2003) attempt to identify what they refer to as  X  X mage pointers X  within captions in biomedical publications. The image pointers of in-terest are, for example, image panel labels, or letters and abbreviations used as an overlay within the im-age, similar to the Overlay Labels described in Table 1. They developed a set of hand-crafted rules, and a learning method involving Boosted Wrapper Induc-tion on a dataset consisting of biomedical articles related to fluorescence microscope images.

Deschacht and Moens (2007) analyze text sur-rounding images in news articles trying to identify persons and objects in the text that appear in the corresponding image. They start by extracting per-sons X  names and visual objects using Named Entity Recognition (NER) tools. Next, they measure the  X  X alience X  of the extracted named entities within the text with the assumption that more salient named en-tities in the text will also be present in the accompa-nying image.

Davis et al (2003) develop a NER tool to iden-tify references to a single art object (for example a specific building within an image) in text related to art images for the purpose of automatic cataloging of images. They take a semi-supervised approach to locating the named entities of interest by first provid-ing an authoritative list of art objects of interest and then seeking to match variants of the seed named en-tities in related text. 4.1 Dataset The chosen date-set contains more than 60,000 images together with their as-sociated captions from three online life and earth sciences jour-nals 1 . 400 randomly selected image cap-tions were manually annotated by a single annotator with their Image Markers and Image Marker Referents and used for testing and for cross-validation respectively in the two methods described below. 4.2 Rule Based Approach First, we developed a two-stage rule-based, boot-strapping algorithm for locating the image markers and their coreferents from unannotated data. The al-gorithm is based on the observation that textual im-age markers commonly appear in parentheses and are usually closely related semantic concepts. Thus the seed for the algorithm consists of: 1. The predominant syntactic pattern -parenthe-ses, as in  X  X ooking of the soft palate (arrow) X  . This pattern could easily be captured by a regular expres-sion and doesn X  X  require sentence parsing. 2. A dozen seed phrases (e.g  X  X eft X  ,  X  X ircle X  ,  X  X s-terisk X  ,  X  X lue X  ) identified by initially annotating a small subset of the data (20 captions). Wordnet was used to look up and prepare a list of their corre-sponding inherited hypernyms. This hypernym list contains concepts such as  X  X  spatially limited lo-cation X  ,  X  X  two-dimensional shape X  ,  X  X  written or printed symbol X  ,  X  X  visual attribute of things that results from the light they emit or transmit or re-flect X  . Best results were achieved when inherited hy-pernyms up to the third parent were used.

In the first stage of the algorithm, all image cap-tions were searched for parenthesized expressions that share the seed hypernyms. This step of the al-gorithm will result in high precision, but a low re-call since image markers do not necessarily appear in parentheses. To increase recall, in stage 2 a full text search was performed for the stemmed versions of the expressions identified in stage 1.

A baseline measure was also computed for the identification of the Image Marker Referents using a simple heuristic -the coreferent of the Image Marker is usually the closest Noun Phrase (NP). In the case of parenthesized image markers, it is the closest NP to the left of the image marker; in the case of non-parenthesized image markers, the referent is usually the complement of the verb; and in the case of pas-sive voice, the NP preceding the verb phrase. The Stanford parser was used to parse the sentences.
Table 2 summarizes the results validated against the annotated dataset (excluding the 20 captions used to identify the seed phrases). It appears that the relatively low accuracy for Image Marker Referent identification was mostly due to parsing errors since the syntactic structure of the image caption texts is quite distinct from the Penn Treebank dataset used for training the Stanford parser. 4.3 Support Vector Machines Next we explored the possibility of improving the rule-based method results by applying a machine learning technique on the set of annotated data. Sup-port Vector Machines (SVM) (Vapnik, 2000) was the approach taken because it is a state-of-the-art classification approach proven to perform well on many NLP tasks.

In our approach, each sentence was tokenized, and tokens were classified as Beginning, Inside, or Outside an Image Marker type or Image Marker Ref-erent. Image Marker Referents are not related to Im-age Markers and creating a classifier trained on this task is planned as future work. SVM classifiers were trained for each of these categories, and combined via  X  X ne-vs-all X  classification (the category of the classifier with the largest output was selected). Fea-tures of the surrounding context are used as shown in Table 3 and Table 4.

Table 5 summarizes the results of a 10-fold cross-validation. SVM performed well overall for iden-tifying Image Markers, Location being the hardest because of higher variability of expressing ROI posi-tion. Image Marker Referents are harder to classify, as deeper syntactic knowledge is necessary. Idiosyn-cratic syntactic structures in image captions pose a problem for the general-purpose trained Stanford parser and performance is hindered by the accuracy of computing Dependency Path feature. We explored the feasibility of determining the con-tent of ROIs in images from scientific publications using image captions. We developed a two-stage rule-based approach that utilizes WordNet to find ROI pointers (Image Markers) and their referents. We also explored a supervised machine learning ap-proach. Both approaches are promising. The rule-based approach seeded with a small manually an-notated set resulted in 78.7% precision and 68.1% recall for Image Markers recognition. The SVM ap-proach (which requires a greater annotation effort) outperformed the rule based approach (p=93.6%, r=87.7%). Future plans include training SVMs on the results of the rule-based annotation. Further work is also needed in improving Image Marker Referent identification and co-reference resolution. We also plan to involve two annotators in order to collect a more robust dataset based on inter-annotator agreement.

