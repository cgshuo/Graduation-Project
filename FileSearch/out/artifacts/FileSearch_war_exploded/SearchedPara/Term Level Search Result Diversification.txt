 Current approaches for search result diversification have been categorized as either implicit or explicit . The implicit ap-proach assumes each document represents its own topic, and promotes diversity by selecting documents for different top-ics based on the difference of their vocabulary. On the other hand, the explicit approach models the set of query top-ics, or aspects. While the former approach is generally less effective, the latter usually depends on a manually created description of the query aspects, the automatic construction of which has proven difficult. This paper introduces a new approach: term-level diversification. Instead of modeling the set of query aspects, which are typically represented as coherent groups of terms, our approach uses terms without the grouping. Our results on the ClueWeb collection show that the grouping of topic terms provides very little benefit to diversification compared to simply using the terms them-selves. Consequently, we demonstrate that term-level di-versification, with topic terms identified automatically from the search results using a simple greedy algorithm, signif-icantly outperforms methods that attempt to create a full topic structure for diversification.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models Algorithms, Measurement, Performance, Experimentation. Search result diversification, term level, topic level.
Search result diversification has been studied as a task of re-ordering an initial ranking of documents retrieved for a query. The goal is to produce a more diverse ranked list with respect to some set of topics or aspects associated with this query. Existing approaches to diversification have been classified as either implicit or explicit [30]. The implicit ap-proach includes MMR [4] and its probabilistic variants [31]. These techniques do not assume any explicit representation of the underlying topics for a query. Instead, they assume each document represents its own topic. As a result, di-versity is achieved by iterating over the input ranking and selecting documents based on the difference of their vocab-ulary, as measured by document similarity. These methods are generally less effective [1, 30, 18] as there are no guar-antees that the topics covered by the resulting documents correspond to query aspects.

The explicit approach, on the other hand, models the set of query aspects and select documents for each of them. This includes algorithms such as IA-Select [1], xQuAD [30] and Proportionality Model [18]. The success of these meth-ods, however, has been observed mostly with descriptions of query aspects that have been created manually, either as a concise list of topics [30, 18], a larger taxonomy from which the query topics can be inferred [1], or a list of topics obtained directly from commercial search engines [30, 18].
Generating the query aspects or topic descriptions auto-matically, on the other hand, is not as well understood. Al-though there have been a number of attempts to do this [5, 27, 17], only more recent techniques that build topic descrip-tions by combining information from several sources have been shown to be effective on web copora [19, 20].
In the literature, a query topic or aspect is usually iden-tified as a single phrase or unit. More generally, a topic is a coherent group of what we call topic terms. Fig. 1 shows an example TREC query: joints (topic 82) with two top-ics: treat joint pain and woodwork joint type . These topics contain five topic terms: treat , joint , pain , woodwork and type . The question that we address in this paper is whether diversification with respect to these topics benefits from the additional structure or grouping of terms or would diversifi-cation using the topic terms directly be just as effective?
We investigate the problem of term level diversification empirically. Instead of modeling the set of query aspects, each of which is a coherent group of terms, this approach directly models these terms without their topical grouping. Thus, it still explicitly models the user intents, but it uses a weaker representation of them. Our experiments on the ClueWeb collection using two existing diversification frame-works [30, 18] confirm that discarding the topic structure does not result in any significant loss in diversification effec-Figure 1: T wo different levels for diversification: topic tiveness. Therefore, instead of trying to recover the topics for a query, we only need to identify a set of terms that cover most of the query topics. This is, in fact, the main task for multi-document summarization (e.g., [29, 23, 24]).
Consequently, we propose to use a simple greedy algo-rithm from the document summarization literature for iden-tifying topic terms for diversification from the initial rank-ing of documents [23, 24]. Our results show that this simple method significantly outperforms many existing approaches for estimating the full topic structure from the same data on a wide ranges of both relevance and diversity measures. To the best of our knowledge, our method is the first one that can provide statistically significant improvement over standard relevance-based retrieval models in both relevance and diversity measures, without relying on any external data source or manually created topic set.

In summary, the main contribution of this paper is term level diversification. It simplifies the current topic level ap-proach by taking as input a set of terms as opposed to a set of topic descriptions. This is important since automatic topic generation has proven challenging. We show that our approach with terms generated automatically, using a sum-marization technique [23, 24], significantly outperforms its topic level counterpart with topics generated using existing methods. When ground-truth query topics are available, our approach remains comparable to the topic level alternative.
In the next section, we briefly mention related work. Sec-tion 3 presents the current topic level diversification frame-works, which will also be used for term diversification. Sec-tion 4 describes in more detail the notion of term level di-versification as well as our algorithm for identifying topic terms. Section 5 and 6 contains the experimental setup and results, as well as analysis and discussions. Finally, Section 7 concludes.
Search result diversification has been studied as the task of retrieving documents covering multiple possible topics, aspects, or interpretations of a query. Existing work can be categorized using two orthogonal criteria: their representa-tion of these topics and their notion of diversity. Query Topic Representation. Proposed techniques are usually classified as either implicit or explicit . The implicit approach, in fact, does not assume any of such representa-tion. Instead, it assumes each document has its own topic. It promotes diversity by selecting documents that are differ-ent to one another in terms of vocabulary, as captured by document similarity such as cosine [4] or Pearson X  X  correla-tion [28] between the document vectors and KL divergence between their language models [31]. As the selected docu-ments do not necessary cover any of the query topics, this approach often fails to provide consistent improvement over standard relevance-based retrieval model on large web cor-pora [19, 18].

The explicit approach, on the other hand, models the set of query aspects and returns documents for each of them [5, 1, 30, 17]. Our term level diversification scheme belongs to this second category. The difference is, instead of modeling the set of query topics, each of which is a group of terms, we model these terms directly without their grouping structure. As we will show later on, the grouping provides very little benefit to diversification compared to the presence of the topic terms. This effectively reduces the task of finding a set of topics into finding a simple set of terms.

The success of the explicit approach, in fact, has been ob-served primarily with query topics that are either created manually (e.g. TREC subtopic descriptions [30, 18] or a larger predefined taxonomy [1]) or obtained directly from related queries provided by commercial search engines [30, 18]. Generating these aspects automatically, on the other hand, is not as well understood. For example, while cluster-ing queries from logs [27] or anchor text and ngrams from the web [17] can produce interesting looking clusters of text, their effectiveness for diversification has yet been confirmed. Topics extracted from clustered documents, either determin-istically or probabilistically via topic modeling [5], were only evaluated on a very small collection. In addition, their ef-fectiveness is concluded to be only comparable to MMR [4], the canonical technique from the implicit approach [5]. Only more recent work [19, 20] has achieved some success, but they generally build topic descriptions by combining infor-mations from several sources of data.

Instead of trying to generate a set of topics for a query, we apply a simple greedy algorithm [23, 24] to extract a diverse set of topic terms automatically from the input ranking. We then evaluate and compare their effectiveness for diversifi-cation (term level) with topics generated using some of the subtopic mining techniques mentioned above that utilize the same data (topic level) [5].
 Notion of Diversity. There are two notions of diversity in the current literature: diversity by redundancy and by proportionality . The concepts of redundancy and novelty are based on the cascade user model which assumes users will scan the result list from top to bottom [14]. Therefore, documents at any position in the result list that provide the same information as those at earlier ranks are considered re-dundant. Similarly, novel documents are those that provide new information. A ranking is more diverse if it contains less redundancy, or equivalently, more novelty. Common to these techniques [4, 31, 7, 1, 5, 28, 30, 32, 18] are the greedy framework which sequentially selects documents with mini-mal redundancy, the measure of which is where they differ. For example, MMR [4] (implicit) measures redundancy of a document by its cosine similarity to the documents selected previously. IA-Select [1] and xQuAD [30] (explicit) measures how much it covers the query topics that have not been well covered by those chosen earlier.

On the other hand, a proportional ranking of documents with respect to a topic popularity distribution is a ranking in which the number of documents on each topic is propor-tional to its popularity [18]. By this definition, perfectly proportional search results would naturally be diverse. The main algorithm in this class is PM-2 [18], which selects doc-uments in a similar greedy fashion, except that it maximizes p roportionality using the Sainte-Lagu  X  e formula.
In this paper, we compare term level diversification to the topic level counterpart using both frameworks. In par-ticular, we choose xQuAD (redundancy-based) and PM-2 (proportionality-based) simply because they have been demon-strated to be effective on the ClueWeb collection, which we also use to conduct experiments.
In this section, we first formally describe the problem of diversification at the topic level. Then we will present the two frameworks for diversification in the current literature: redundancy-based and proportionality-based diversification. These frameworks will later be used for term diversification.
Let q indicate a user query and T = { t 1 , t 2 , ..., t n the set of topics for q . Let W = { w 1 , w 2 , ..., w n the weights for each of the topics t i  X  T . These weights can be interpreted as the importance [30] or popularity [18] depending on the diversification techniques. In addition, let R = { d 1 , d 2 , ..., d m } indicate a ranked list of documents initially retrieved for q and P ( d | t ) denote some probabilistic estimate of d  X  X  relevance to a topic t . The task of topic level diversification is to select a subset of R using { T, W, P ( d | t ) } to form a diverse ranked list S of size k .

It is worth noting that the type of topics T = { t 1 , t 2 will determine the relevance measure P ( d | t ). For example, if T is a set of short textual descriptions (e.g. queries), P ( d | t ) is often the relevance score of d to t given by some retrieval models [30, 18].
This framework promotes diverse rankings of documents by penalizing redundancy at every rank. It does so by greed-ily selecting documents in R to put into S . At each step, it selects the document that is most different to those previ-ously selected (thus minimizing redundancy), while remains relevant to the query q : where D ( d j , S ) is a measure of novelty, which indicates the difference between the candidate document d j and each of the documents in S . Different choices of D ( d j , S ) corre-spond to different instantiations of this framework [1, 5, 30]. In this paper, we choose xQuAD [30] simply because it has proven effective on several TREC Web Track query sets [30, 18], which we use to carry out our evaluation. Our find-ings, nevertheless, should apply to all techniques within this framework. xQuAD measures the difference between documents by the topics they cover. It defines p i to be the  X  X ortion X  of the topic t i that has not been covered by documents in S : Higher p i indicates that most of the documents in S are not relevant to t i . As such, t i is less substantially covered and it should have higher  X  X riority X  in getting more documents. With this, D ( d j , S ) is calculated as follows: which means the novelty of a document is its ability to cover the topics that need covering (i.e. higher p i ) weighted by the importance of the topics w i .
The main algorithm in this class is the proportionality model PM-2 [18]. It is a probabilistic adaptation of the Sainte-Lagu  X  e method for assigning seats to members of com-peting political parties such that the number of seats for each party is proportional to the votes they receive. PM-2 starts with a ranked list S with k empty seats. For each of these seats, it computes the quotient qt i for each topic t i following the Sainte-Lagu  X  e formula: According the the Sainte-Lagu  X  e method, this seat should be awarded to the topic with the largest quotient in order to best maintain the proportionality of the list. Therefore, PM-2 assigns the current seat to the topic t i  X  with the largest quotient. The document to fill this seat is the one that is not only relevant to t i  X  but to other topics as well: d = arg max After the document d  X  is selected, PM-2 increases the  X  X or-tion X  of seats occupied by each of the topics t i by its nor-malized relevance to d  X  : T his process repeats until we get k documents for S or we are out of candidate documents. The order in which each document is put into S determines its ranking.
Diversification at the term level is very similar to the topic t . Instead of diversifying R using the set of topics T = { t 1 , t 2 , ..., t n } , we propose to perform diversification using T t as a topic.

Let us reuse the example query provided in Fig. 1 earlier to illustrate this. Instead of diversifying the initial ranking for the query joints with respect to two of its topics: treat joint pain and woodwork joint type , we propose to perform diversification with respect to its topic terms: treat , joint , pain , woodwork and type .

We will now compare diversification at the topic level to its term level counterpart at an intuitive level, using both frameworks, to provide some justification for why one can expect similar performance from these two paradigms. This is based on the assumption that if a document is more rel-evant to one topic than another, it is also more relevant to the terms associated with this topic than any of the terms from the other topic. In other words, if a document is more r elevant to treat joint pain than it is to woodwork joint type , we assume that it is also more relevant to treat and pain than it is to woordwork and type . Since both topics have the term joint , we will ignore it for the ease of explanation.
Let us first explain using the xQuAD X  X  framework. At either the topic or term level, it follows from Eq. (1) that the first selected document d 1 is the one that is most relevant to the user query. Let us assume this document d 1 is more relevant to treat joint pain than it is to woodwork joint type . At the second step at the topic level, woodwork joint type will have higher  X  X riority X  (higher value for p ) to get documents and thus, xQuAD will favor documents on this topic. At the same time at the term level, d 1 should be also more relevant to treat and pain than it is to woodwork and type (because of our assumption). Therefore, woodwork and type will have higher  X  X riority X  than either treat or pain . It follows that, if the topic level system is able to find a document d  X  for its relevance to woodwork joint type , this same document should also emerge at the term level. Once this document is selected, the  X  X riority X  of woodwork and type decreases as does that of woodwork joint type . As the algorithm proceeds, the two approaches may select different documents due to the different numbers of  X  X opics X , each of which is down-weighted by a different amount (by Eq.(2)). Nevertheless, the general idea still applies, that if a document is selected for its relevance to any particular topic, that same document should be at least a highly potential candidate at the term level due to its relevance to the corresponding terms.
Similarly, in the framework of PM-2, let us also assume the first document d 1 is more relevant to treat joint pain than it is to woodwork joint type at both levels. As a result, the  X  X ortion X  of seats occupied by treat and pain is also higher than that of both woodwork and type , just as treat joint pain will have a higher portion than woodwork joint type . At the second step, woodwork joint type should be assigned a higher quotient by the Sainte-Lagu  X  e formula, which again indicates woodwork joint type has higher  X  X riority X  similarly to what happens in the xQuAD framework.
As mentioned earlier, diversification frameworks assume { T, W, P ( d | t ) } as inputs and the choice of T will determine P ( d | t ). An obvious choice for P ( d | t ) for term level diver-sification is P ( t k i | d ), the probability that the document d generates the topic term t k i . This is, however, highly prob-lematic. At the term level, in addition to those true query topics which have become latent, there are also  X  X alse X  la-tent topics formed by the wrong combinations of terms. In the context where we identify topic terms for a query au-tomatically, some of them might be generic and ineffective. As the number of bad terms increases, the number of  X  X alse X  topics will grow exponentially. Combined with the fact that there are likely many non-relevant documents in the baseline ranking, term diversification under the effects of these X  X alse X  topics might end up promoting non-relevant documents.
Assuming any document that is relevant to a true query topic should be relevant to the query itself, we propose to expand each topic term with the query. Let { q 1 , q 2 , ..., q be the set of terms of the query q . P ( d | t k i ) is estimated as follows: w hich is essentially the query likelihood model for ranking d by the query length to avoid biased towards shorter terms (i.e. terms can include both unigrams or phrases). In the case where all terms have the same length, the normalization is certainly not necessary.

Note that the inclusion of the query is not the only mech-anism to keep non-relevant documents under control. In-terpolating P ( t k i | d ) with P ( q | d ) as has been done in the redundancy-based framework (Eq. (1)) is another possibil-ity. We do not do so because we do not want to introduce more parameters into the framework. In principle, any com-bination of P ( t k i | d ) and P ( q | d ) should be applicable. Since P ( q | d ) can be obtained directly from the baseline rankings, this does not increase computational complexity.
We now present DSPApprox , the topic term extraction al-gorithm proposed by Lawrie and Croft [23, 24] for hierarchi-cal multi-document summarization. The goal of the algo-rithm is to select from a collection of documents a small set of highly representative terms that best summarize them. This algorithm is applied hierarchically, resulting in an hi-erarchical topic structure.

Since we only need a single diverse set of topic terms, we only apply the algorithm once on the initial ranking of documents R = { d 1 , d 2 , ..., d m } retrieved for the query q . The algorithm first identifies a set of vocabulary from these documents, from which it forms a set of more specific topic terms . It then measures for these terms their topicality and how well they predict the occurrences of other terms. Fi-nally, it greedily selects a subset of topic terms, aiming to maximize both their topicality and their coverage of the vo-cabulary.
 Vocabulary Identification. We consider as vocabulary all terms that (1) appear in at least two documents, (2) have at least two characters and (3) are not numbers. In our experi-ments, we test two types of terms: unigrams and phrases . We use a very simple method for phrase extraction. We scan through terms in each document and at each position, we se-lect the longest sequence of terms that matches a wikipedia title as a phrase.
 Topic Terms Identification. All vocabulary terms that co-occur with any of the query terms within a proximity window of size w is selected as topic terms.
 Topicality and Predictiveness. Topicality of a term measures how informative it is at describing the set of doc-uments. To compute topicality, a relevance model P R ( t | q ) [25] is first estimated from the initial set of documents R : where P ( t | d ) is the probability that d i generates the term t and P ( d i | q ) is relevance of d i to the query. The topicality TP ( t ) of a term t is estimated as its contribution to the KL divergence between this relevance model and the language model for the entire retrieval collection: I t is equivalently t  X  X  contribution to the clarity score of the query q [16].
Predictiveness, on the other hand, measures how much the o ccurrence of a term predicts the occurrences of others. Let P ( t | v ) indicate the probability that a term t occurs within a window of size w of another term v and C t indicate the set all such v . The predictiveness of t is estimated as follows: w here Z is the hierarchy level specific normalization factor. In our case, we set it to the size of the vocabulary. Greedy Algorithm. Pseudo-code for this algorithm is pre-sented as Algorithm 1. It iteratively selects terms from the candidate topic term set T . The utility of each term is the product of its topicality and predictiveness. At each step, the algorithm selects the topic term t  X   X  T with maximum utility. Then, it decreases the predictiveness of other topic terms that predict the same vocabulary. This makes sure topic terms that cover the uncovered part of the vocabulary will emerge for selection in the next iteration. The algorithm stops once the utility of all candidate topic terms reaches 0, indicating that all vocabulary has been covered. Some ex-ample topic terms (both unigrams and phrases) generated by DSPApprox for the query joints are provided in Table 1. Algorithm 1 DSPApprox f or identifying topic terms. 7: DTT : the output diverse set of topic terms 8: PREDV : vocabulary that has been predicted by DTT 9: DTT  X  X  X  10: PREDV  X  X  X  16: for all v  X  pred \ PREDV do 17: for all t i  X  T do 19: end for 20: end for 21: PREDV = PREDV  X  pred 22: end while Q uery and Retrieval Collection . Our query set consists of the 147 queries with relevance judgments from three years of the TREC Web Track X  X  diversity task (2009 [10], 2010 [11] and 2011 [12]). Our evaluation is done on the ClueWeb09 million web pages in English. This collection is stemmed using the Krovetz stemmer [22]. Stopword removal is only performed on the query using a small stopword list. Baseline Retrieval Model . We use the standard query-initial retrieval run. This run serves not only as a means to provide a set of documents for the diversification systems but also as a baseline to verify their usefulness.
Spam filtering is known to be an important component of web retrieval [2]. In addition, documents with too few stop-words are found to have poor readability [21, 26]. Therefore, we incorporate both of these into our baseline ranking. We use the spam filtering technique described by Cormack et al. [13], which assigns a  X  X pamminess X  percentile S ( d ) to each document d in the collection. Let  X  ( d ) be the stopword to non-stopwords ratio in d and p ( d | q ) indicate the score the retrieval model assigns to the document d . Following Ben-dersky et al. [2], the final score of d is given by: Diversification Frameworks . We compare term level di-versification to topic level diversification using both xQuAD [30] and Proportionality Model (PM-2) [18], which we have described in Section 3. While xQuAD obtains diversity by penalizing redundancy at every position in the ranked list, PM-2 does so by promoting proportionality at every rank. Evaluation Metric . We report our evaluation results using several standard metrics that have been used in the official evaluation of the diversity tasks at TREC [11, 12]:  X  -NDCG [8], ERR-IA (a variant of ERR [6]) and NRBP [9]. These metrics penalize redundancy at each position in the ranked list based on how much of that information the user has al-ready seen from documents at earlier ranks. In addition, we also report our results using Precision-IA [1] and subtopic recall, which indicate respectively the precision across all topics of the query and how many of those topics are covered in the search results. All of these measures are computed using the top 20 documents retrieved by each model to be consistent with official TREC evaluation. Statistically sig-nificant differences are measured using two-tailed t-test with p-value &lt; 0.05.

Most diversification mechanisms are evaluated using only diversity measures [1, 30, 18]. It is unclear if diversity is achieved at a cost to relevance. Therefore, in addition to all diversity measures above, we also report our results us-ing two standard relevance-based metrics for web retrieval: NDCG and ERR, which are also evaluated at the top 20 documents.
 Parameter Settings . All of the diversification approaches under evaluation are applied on the top K retrieved docu-ments. We set K = 50 to be consistent with existing re-search which found that both xQuAD and PM-2 achieve their highest performance at K = 50 [18]. Consequently, all topic and term extraction techniques will also operate on these top 50 documents.

Each topic and term extraction technique, as we will show later, has several free parameters that require tuning. xQuAD and PM-2 also have one parameter  X  to tune. To enforce fair comparison, all parameters are selected via 3-fold cross validation.

We consider for  X  values in the range of [0 . 05 , 1 . 0] with a increment of 0 . 05. Value ranges for parameters of the topic and term extraction methods will be presented in their respective sections.
We first compare the term level diversification approach to the topic level approach using the set of true topics as-sociated with each query (TREC  X  X ub-topics X ). A topic is a coherent group of terms. These topics represent the or-acle grouping of the oracle topic terms. By comparing the diversification effectiveness of this set of topics (topic level diversification) with that of the corresponding set of unigram topic terms (term level diversification), we can separate the benefit diversification algorithms get from the grouping with the benefit they get from the presence of topic terms.
In addition, related queries provided by commercial search engines have been demonstrated to be very effective for di-versification [30, 18]. These queries too can be considered good underlying topics for the original query. As a result, we also compare the two diversification paradigms using this topic set. It is worth noting that the search engine provides no suggestions for three of the queries in our set. The query set for this experiment only contains 144 queries (out of 147).
Similar to existing work [18], the document-topic rele-vance function P ( d | t ) for topic level diversification is im-plemented as the query-likelihood score for d with respect to t (each topic t is treated as a query). In particular, let t = { t 1 i , t 2 i , .., t n i } indicates the set of terms for the topic t . P ( d | t i ) is computed using the geometric mean to avoid biased towards shorter topics: F or term level diversification, P ( d | t ) is calculated as de-scribed in Section 4.2.

Table 2 compares term diversification to topic diversifi-cation using both topic sets and both diversification frame-works. The first thing to notice is that both topic and term diversification, using both PM-2 and xQuAD, significantly outperform the baseline in all metrics. This is certainly un-surprising since we are using the oracle data. Nevertheless, it confirms the effectiveness of both of these frameworks at providing relevant and diverse results.

What is more interesting from Table 2 is that the set of topic terms maintains a highly comparable level of perfor-mance to the topic structures. There are no statistically significant differences in all cases. These results are con-sistent across different diversification techniques and topic sets. This suggests that existing diversification frameworks are capable of returning relevant documents for topics with-out the explicit topical grouping.

We notice, however, that some of the query topics are dif-ferent to the query itself by only one term. For example, topics for the query  X  X outh africa X  include  X  X istory of south africa X  and  X  X aps of south africa X  . Both of these topics have only one key term, which is  X  X istory X  and  X  X aps X  respec-tively. It is possible that term level diversification is com-petitive with the topic level alternative because of queries like this.

To investigate this issue, we use the notion of key term to indicate the number of non-stopword terms in a query topic that are different to the query text. To quantify the impact the number of key terms has on our approach, we plot the number of topics where each approach is able to provide at least one relevant document against the number of key terms for these topics. In addition, we also plot the actual number of relevant documents retrieved for each topic (on log scale) against the number of key terms it contains. These plots are presented by Fig. 2 (a) and (b) respectively. Note that we only show the plots for PM-2 because the analysis with xQuAD is very similar.

Fig. 2 reveals that not only is our approach comparable with its topic counterpart on topics with a single key term, it also remains competitive consistently across different num-bers of key terms. In particular, term level diversification has a slight advantage with topics that have only one single key term. With topics that have two and three key terms, although the topic level systems perform better, the differ-ence is very small. The two approaches become comparable with larger numbers of key terms. Given that the term level systems do not need the topical structure, this very slight performance loss seems reasonable.

In summary, our experiments with the X  X racle topics X  X how that the benefits for diversification of grouping topic terms are minimal compared to just using the terms themselves. The existing frameworks, PM-2 and xQuAD to be specific, can perform topical diversification at the term level. To-gether, these findings indicate that term level diversification is worth pursuing.
We now evaluate the effectiveness of DSPApprox for auto-matically extracting topic terms, considering unigrams and phrases separately, from the initial ranking of documents. The total number of unigrams and phrases the algorithm returns are approximately 100 and 500 respectively. Since using too many terms is inefficient and unlikely to be ef-fective, we use a parameter T to control the number of terms used for diversification. The second parameter is w , which determines the size of the window in which (1) a term has to co-occur with at least one query term in order to be considered a candidate topic term, and (2) predic-tion boundary: a term cannot predict terms that are more than w words away. We consider w  X  { 20 , 30 , 40 , 50 } and T  X  X  5 , 10 , 20 , 40 , 60 , 80 , 100 } . Baseline 1. Our first baseline for comparison, first pro-posed by Carterette and Chandar [5], estimates topic mod-els using LDA [3] from the documents and uses the resulting clusters for diversification. This model only has one param-eter, which is the number of latent topics c  X  [2 .. 10]. The relevance between a document to a topic is provided by the LDA framework. We use the multi-threaded implementa-Baseline 2. Our second baseline technique, also proposed by Carterette and Chandar [5], applies k-nearest neighbor (KNN) first to cluster the documents. After that, it esti-mates a relevance model [25] from each of the clusters and use it as a topic model. Its parameters include k  X  [2 , 10] and T  X  X  5 , 10 , 20 } , which are the number of neighbors and the number of top terms from the relevance model to be used as topic description respectively. The topic descrip-tion is treated as an Indri weighted query. The relevance between a document and this topic is obtained directly via Indri X  X  output relevance score.
 Baseline 3. MMR [4] has become a canonical baseline in the diversity literature [31, 30, 18]. Though it does not ex-plicitly model topics, it fits into the class of algorithms that relies solely on the set of documents. The framework of MMR is very similar to the one presented in Eq. (1). The novelty component D ( d, S ), which indicates the different be-tween a document d and those previously selected in S , is ag-gregated over its difference to each of the document d j  X  S . The difference between two documents is implemented based on the cosine similarity. We experimented with three aggre-gation functions: max , min and average and report results with max since it is the most effective.
Table 3 presents the results comparing the systems men-tioned above. All of their parameters are determined using 3-fold cross validation. The letters Q, M, L, K indicate sta-tistically significant differences (p-value &lt; 0 .05) to query-likelihood, MMR, LDA and KNN respectively. Among the three baseline techniques for topic generation, MMR X  X  per-formance is the most similar to the baseline. It is interesting to see that while LDA has the best results in S-Recall, it per-forms poorly on all other measures, regardless of the diver-sification techniques. This can be explained by the fact that an LDA topic is a distribution over the entire vocabulary of the initial set of documents. Thus, each topic has a higher chance of matching its documents, but at the same time it also matches several non-relevant documents. Overall, KNN is the only technique among the three baselines that can provide some improvement over query-likelihood (with PM-2). Nevertheless, KNN has a trade-off between relevance and diversity: while KNN topics used by PM-2 help most of the diversity measures, it hurts relevance. On the other hand, the topics it generates, when used by xQuAD, helps relevance but hurt most of the diversity measures. Overall, the difference between these baselines and query-likelihood is mostly not statistically significant.

In contrast, both the unigrams and phrases generated us-ing DSPApprox when used by PM-2 substantially outper-form all other systems under comparison on many mea-sures. Statistically significant differences are observed in many cases. In fact, it is the only system that optimizes for diversity measures yet outperforms query-likelihood in both relevance measures. Between unigrams and phrases, the for-mer appears to be slightly more robust by improving more queries and hurting fewer, but the latter manages to retrieve more relevant results. In addition, their performance with xQuAD is still slightly higher than all three baselines on most precision-based measures.

The limited performance of our method when using xQuAD can be explained by xQuAD X  X  vulnerability to large numbers of topics. Let us revisit Eq. (3) and assume that at the k -th step, the topic t i has the highest  X  X riority X  p i . We can rewrite Eq. (3) with respect to t i as follows: There is an implicit uncontrolled trade-off here between the relevance of a document to t i , the topic with the highest priority, and its relevance to other lower priority topics. As the size of T increases, it becomes possible that a document d  X  that is relevant to many t j will be selected even though xQuAD should be selecting documents for t i . This is cer-tainly not a big problem for topic level diversification since the number of topics is relatively small. At the term level, however, our algorithm generates hundreds of terms, many of which can be very generic. As such, some non-relevant documents can appear randomly relevant to many of such terms, dominating the topic term with the highest priority. Note that PM-2 has the same trade-off as xQuAD (Eq. (4)). The difference is that it is controlled by the parameter  X  . If a topic term set is too noisy, cross-validation should be able to specify a larger value for  X  to put more emphasis on the topic with the highest priority .
We focus our analysis of DSPApprox results using PM-2. As can be seen from Table 3, although DSPApprox has slightly lower subtopic recall compared to query-likelihood ( QL ), the difference is not significant. Our investigation sug-gests that not only do DSPApprox and QL cover about the same number of topics, they cover almost the same set of topics (97% overlap). This high percentage of overlap sug-gests that the terms generated by DSPApprox are biased to-wards topics covered by the top ranked documents in the initial ranking.

We believe the cause of this bias is the way DSPApprox computes topicality. We observe that the topicality of a term is relatively proportional to the probability that it is given by the relevance model [25] estimated from the top 50 documents. This model usually assigns higher probabilities to frequent terms from higher ranked documents since they are assumed more relevant. If a document at a very low position covers topics that are different from those at early ranks, chances are their topic terms do not appear in these documents with high frequency. Therefore, their chance to be included in the resulting set of terms is relatively small, causing these topics to be excluded from the coverage of the final set. This is the main reason why subtopic recall was not improved.
Regardless, DSPApprox s till manages to outperform QL in both  X  -NDCG and Precision-IA. This indicates that while both of them have the same topic coverage, DSPApprox re-trieves more relevant documents for these topics as well as provides better ranking for them. More quantitative analy-sis on this is provided in Table 4. WIN and LOSS indicate the set of queries where DSPApprox helps and hurts  X  -NDCG compared to QL .  X  X  denotes the performance difference in  X  -NDCG. S.Rec  X  indicates the subset of WIN where S-Recall is also improved and REST indicates the remaining of the set. Similarly, S.Rec  X  indicates the subset of LOSS where S-Recall is also lower and REST indicates the remaining. It can be seen that the increase of S-Recall contributes very little to the overall improvement on  X  -NDCG (the S.Rec  X  sets). At the same time, they are also responsible for some performance loss (the S.Rec  X  sets). On the other hand, a significant chunk of improvement is observed on the sets of queries that cover no more topics than QL (the REST sets). The analyses above suggest that the terms provided by DSPApprox , though unable to recover additional topics due to the bias issue, correctly represent most of those covered by QL . Consequently, they help surface more documents on theses topics, significantly improving  X  -NDCG.

It is worth noting that, diversification with both unigrams and phrases provided by DSPApprox also significantly im-proves the relevance of the results (NDCG). Our approach, in fact, turns out to be very similar to pseudo-relevance feed-back. The difference is that traditional relevance feedback uses the extracted terms to update the query model to re-trieve new documents. Our approach, on the other hand, only attempts to re-order the input ranking, pushing more relevant documents to earlier ranks. As such, diversification can be considered a precision-driven framework for relevance feedback.
As mentioned earlier, our topic term identification algo-rithm has two parameters. T controls how many of the top output terms to use for diversification and w determines how many words away can a term predict as well as how far can a topic term be from the query term. The best parameter values selected for DSPApprox (using 3-fold cross-validation) with unigrams is { T = 40 , w = 20 } and the best values for phrases are { T = 80 , w = 40 } .

We first vary T from 10 to 200 and and keep w = 20 for unigrams and w = 40 for phrases. Note that the line for unigrams stops at T = 100 since our algorithms generates at most 100 unigram terms. Fig. 3 shows that regardless of the value of T , their is always some improvement. The set of unigrams, in particular, is very robust: it provides sub-stantial improvement for most of T  X  X  values. As for phrases, two few (e.g. less than 50) or too many (more than 100) terms result in very minor improvement.

We then vary w from 20 to 50 and keep T constant. Fig. 3 shows the sensativity of this parameter. Similarly, improve-ment is observed at every value.
This paper introduces a new approach to topical diver-sification: diversification at the term level. Existing work models a set of aspects for a query, where each aspect is a coherent group of terms [30, 18]. Instead, we propose to model the topic terms directly. Our experiments, using both TREC subtopics and related queries provided by a commer-cial search engine, show that the two approaches achieve highly comparable results in all diversity and relevance mea-sures. It indicates that the topical grouping provides little benefit to diversification compared to the presence of the terms themselves. The reason for this is that if a document is selected by the topic level system for its relevance to some particular topics, it is often relevant to the corresponding topic terms as well. Thus, this document also appears as a highly potential candidate to term level system. Term level diversification, in fact, works in the same principles as the topic counterpart.

This effectively reduces the task of finding a set of query topics, which has proven difficult, into finding a simple set of terms. Consequently, we propose to use a simple greedy algorithm from the literature of multi-document summariza-tion [23, 24] to identify a diverse set of topic terms (unigrams and phrases). Our results demonstrate that, diversification using these terms significantly outperforms its topic level al-ternative with automatically extracted topics, as well as the standard relevance-based retrieval models on various diver-sity and relevance measures.

For future work, we will consider applying DSPApprox on not only the initial retrieved documents but also on exter-nal data such as Wikipedia and anchor text collections. We believe this will help alleviate the current bias issue, improv-ing sub-topic recall. In addition, note that DSPApprox itself is a term diversification algorithm: it selects a set of terms that best cover the vocabulary. It is worth examining the possibility of replacing it with techniques such as PM-2.
This work was supported in part by the Center for In-telligent Information Retrieval and in part under subcon-tract #19-000208 from SRI International, prime contractor to DARPA contract #HR0011-12-C-0016. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
