 This paper introduces a general framework for the use of translation probabilities in cross-language information re-trieval based on the notion that information retrieval fun-damentally requires matching what the searcher means with what the author of a document meant. That perspective yields a computational formulation that provides a natu-ral way of combining what have been known as query and document translation. Two well-recognized techniques are shown to be a special case of this model under restrictive as-sumptions. Cross-language search results are reported that are statistically indistinguishable from strong monolingual baselines for both French and Chinese documents.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation, Measurement Cross-Language IR, Statistical translation
Information retrieval systems seek to identify documents in which authors chose their words to express the same meanings that the searcher intended as they choose their query terms. Cross-Language Information Retrieval (CLIR) deals with the special case of this problem in which the doc-uments and the queries are expressed using words in differ-ent languages. Direct matching of terms between the query and a document would generally fail, so the usual approach has been to translate in one direction or the other so that the query and the document are expressed using terms in the same language; direct term matching techniques can then be employed. Both directions have weaknesses: the limited context available in (typically) short queries adds uncertainty to query translation, and computational costs can limit the extent to which context can be exploited when translating large document collections. Nevertheless, these have proven to be practical approaches; systems that make effective use of translation probabilities learned from parallel corpora can achieve retrieval effectiveness measures similar to those achieved by comparable monolingual systems.
Query translation achieves the information retrieval sys-tem X  X  goal by approximating what would have happened if the searcher actually had expressed their query in the doc-ument language. Document translation takes the opposite tack, approximating what would have happened if the au-thors had written in the query language. McCarley found that merging ranked lists generated using query translation and document translation yielded improved mean average precision over that achieved by either approach alone [11], which suggests that bidirectional techniques are worth ex-ploring. In this paper, we return to first principles to derive an approach to CLIR that is motivated by cross-language meaning matching. This framework turns out to be quite flexible, accommodating alternative computational approx-imations to meaning and subsuming existing approaches to query and document translation as special cases. Moreover, the approach is also effective, repeatedly outperforming the best previously published query translation technique. The remainder of the paper is organized as follows. In Section 2, we review previous work on CLIR using query translation, document translation, and merged result sets. Section 3 then introduces our meaning matching model and explains how some previously known CLIR techniques can be viewed as restricted implementations of meaning match-ing. Section 4 then describes the design of an experiment in which three variants of meaning matching are compared to strong monolingual and CLIR baselines. The results pre-sented in section 5 illustrate the effect of exploiting alterna-tive language resources in the meaning matching framework, showing that the use of bidirectional translation knowledge and similarity-based synonymy can yield statistically signif-icant improvements in mean average precision over previ-ously known query translation techniques. Section 6 then concludes the paper with a discussion of the implications of the meaning matching model for future work on CLIR.
In order to create broadly useful systems that are com-putationally tractable, it is common in information retrieval generally, and in CLIR in particular, to treat terms inde-pendently. Research on CLIR has therefore focused on three main questions: (1) which terms should be translated?; (2) what possible translations can be found for those terms?; and (3) how should that translation knowledge be used? Our focus in this paper is on the third of those questions. In this section, we review prior work on the question of how a known set of translations should be used.  X  X ranslation X  is actually somewhat of a misnomer, since the most effective approaches map term statistics, rather than the terms themselves, from one language to another. Three basic statistics are used in information retrieval sys-tems that use a  X  X ag of words X  representation of queries and documents: the number of occurrences of a term in a docu-ment (Term Frequency, or TF), the number of terms in the document (Length, or L), and the number of documents in which a term appears (Document Frequency, or DF). Gen-erally, documents in which the query terms have a high TF (after length normalization) are preferred, and highly selec-tive query terms (i.e., those with a low DF) are given extra weight in that computation.

When no translation probabilities are known, Pirkola X  X   X  X tructured queries X  have been repeatedly shown to be among the most effective known approaches when several plausible translations are known for some query terms [15]. The basic idea behind Pirkola X  X  method is to treat multiple transla-tion alternatives as if they were all instances of the query term. Specifically, the TF of a query term with regard to a document is computed as the summation of the TF of each of its translation alternatives that are found in that document, and its DF in the collection is computed as the number of documents in which at least one of its transla-tion alternatives appears. Both the TF and DF can be pre-computed for each possible query term at indexing time [12], but query-time implementations are more common in exper-imental settings. The DF computation is expensive at query time, so Kwok later proposed a simplification that upper bounds Pirkola X  X  DF with no noticeable adverse effect on retrieval effectiveness [8]. With the simplified computation, the DF of a query term is estimated as the sum of the DF of each of its translation alternatives.

Darwish later extended Kwok X  X  formulation to handle the case in which translation probabilities are available by weight-ing the TF and DF computations, an approach he called probabilistic structured queries (PSQ) [4].
 where p ( f i | e ) is the estimated probability that e would be properly translated to f i . Similar approaches have also been used in a language modeling framework, often with-out explicitly modeling DF (e.g., [7, 9, 20]). Translation probabilities can be estimated from corpus statistics (using translation-equivalent  X  X arallel X  texts), directly from dic-tionaries (when presentation order encodes relative likeli-hood of general usage), or from the distribution of an at-tested translation in multiple sources of translation knowl-edge. Darwish found that Pirkola X  X  structured queries yielded declining retrieval effectiveness with increasing numbers of translation alternatives, but that the incorporation of trans-lation probabilities in PSQ tended to mitigate that effect.
McCarley was the first to try bidirectional translation, merging a ranked list generated using query translation with another ranked list generated using document translation [11]. He found that the merged result yielded statistically signif-icant improvements in mean average precision when com-pared to either query or document translation alone, and similar improvements have since been obtained by others (e.g. [2, 5]). Our  X  X eaning matching X  model, introduced in the next section, can be viewed as an effort to build on that insight by more directly incorporating bidirectional trans-lation evidence into the retrieval model. Boughanem et al. took an initial step in the direction that we explore, using bidirectional ( X  X ound trip X ) translation to filter out poten-tially problematic translations that were attested in only one direction, but without incorporating translation proba-bilities [1]. In the next section, we derive a general approach to meaning matching and then propose a range of compu-tational implementations.
In this section, we derive an overarching framework for matching meanings between queries and documents and a range of computational implementations that incorporate different sources of evidence.
IR can be viewed as a task of matching the meaning in-tended in a query with the meaning expressed in each docu-ment. The term independence assumption allows us to score each document based on matches between the meaning of each query term with the meaning of each document term. Of course, in human languages different terms may share the same meaning. In monolingual IR it is common to treat words that share a common stem as if they expressed the same meaning, and some automated and interactive query expansion techniques can also be cast in this framework. The key insight between what we call meaning matching is to apply that same perspective directly to CLIR.
 The basic formulae are a straightforward generalization of Darwish X  X  PSQ technique with one important difference: no translation direction is specified. Instead, for each word e in query language E , we simply assume that a set of terms f ( i =1 , 2 ,...,n )indocumentlanguage F is known, each of which shares the searcher X  X  intended meaning for term e with some probability p ( e  X  f i )( i =1 , 2 ,...,n )respec-tively. Any uncertainty about the searcher X  X  meaning for e is reflected in these statistics, the computation of which is described in subsequent parts of this section. If we see a translation f i appearing one time in document d k ,wecan therefore treat this as our having seen query term e occur-ring p ( e  X  f i ) times in that document. If term f i occurs TF ( f i ,d k ) times, our estimate of the total  X  X ccurrence X  of query term e as estimated from the occurrences of document term f i will be p ( e  X  f i )  X  TF ( f i ,d k ). Applying the usual term independence assumption on the document side and considering all the terms in document d k that might share a common meaning with query term e ,weget: Figure 1: Illustrating the effect of overlapping bidi-rectional translations.

Turning our attention to the DF, if document d k contains aterm f i that might share a meaning with e ,wecantreatthe document as possibly  X  X ontaining X  e . Indeed, if every term that shares a meaning with e is found in that document, the meaning of e is sure to have been intended by the author of that document and the contribution of that document to the DF computation should be 1. If only some of the terms that share a common meaning with e appear in a document, we adopt a frequentist interpretation and increment the DF by the sum of the probabilities for each unique term that might share a common meaning with e . We then assume that terms are used independently in different documents and estimate the DF of query term e in the collection as:
Document length normalization is unaffected by this pro-cess because it can be performed using only document-language term statistics.

The comparison to Darwish X  X  PSQ (Equations 1 and 2) is direct; PSQ is simply a unidirectional special case of mean-ing matching. The opposite direction, using p ( e | f i )rather than p ( f i | e ) seems at least equally well (and perhaps bet-ter) motivated, but the fundamental insight behind meaning matching is that there is no need to commit to one transla-tion direction or the other.
To model how term meaning is matched across languages, consider a case in which two English query terms and three French document terms share subsets of four different mean-ings (see Figure 1). At this point we treat  X  X eaning X  as an abstract concept; a computational model of meaning is in-troduced in the next section. In this example, the query term e 2 has the same meaning as the document term f 2 if and only if e 2 and f 2 both express meaning m 2 or mean-ing m 3 . If we assume that the searcher X  X  choice of meaning for e 2 is independent of the author X  X  choice of meaning for f , we can compute probability distributions for those two events. Generalizing to any pair of words e and f : where:
Note that despite our notation, p ( e  X  f ) values are not actually probabilities but rather products of probabilities. For example, if all possible meanings of every term were equally likely, then i p ( e 1  X  f i )=0 . 75 while i p ( e f )=0 . 67. This would have the undesirable effect of giv-ing more weight to some query terms than others, so we renormalize the values so that n i =1 p ( e  X  f i )is1forevery query term e . This yields something that we can treat as if it were a probability distribution, although we retain the notation throughout as a reminder of the process by which the values were produced.

It can be useful to threshold these probabilities in some way because low probability events are generally not mod-eled well. We therefore compute the cumulative distribution function for every e and apply a fixed threshold (selected from a grid of values), which we called Cumulative Proba-bility Threshold (CPT), to select the matches that will be used. This is done by ranking the translations in decreas-ing order of their normalized probabilities, then iteratively selecting translations top-down until the cumulative proba-bility of the selected translations is first reached or exceeds the threshold. A threshold of 0 thus corresponds to using the single most probable translation (a well-studied base-line) and a threshold of 1 corresponds to use of all transla-tion alternatives. The p ( e  X  f ) are again normalized after the threshold is applied.
Further development of meaning matching requires a com-putational model of meaning in which meaning represen-tations are aligned across languages. We chose  X  X ynsets, X  sets of synonymous terms, as a simple computational model of meaning. Cross-language syset alignments are available from some sources, most notably EuroWordNet. We call meaning matching implemented in that way Full Aggregated Meaning Matching (FAMM). For cases in which aligned synsets do not already exist, we decompose the problem into (1) mapping words across languages, (2) mapping words in each language into monolingual synsets, (3) aggregating the word-to-word mappings to produce word-to-synset mappings, and (4) aligning the resulting synsets.
 We could obtain evidence for monolingual synonymy in English from WordNet, but similar resources are available for only a small number of relatively resource-rich languages. We therefore explored one of the several possible sources of statistical evidence for synonymy. Because statistical word-to-word translation models were available for use in our CLIR experiments, we elected to find candidate synonyms by looking for words in the same language that were linked by a common translation. For example, to find document-langauge synonyms, we computed: where p ( f j  X  f ) refers to the probability of f j synonym of f . Of course, that results in a proliferation of poorly estimated low probability events. We therefore arbitrarily suppressed any candidate synonyms for which p ( f j  X  f ) &lt; 0 . 1. Alternatively, we could use statistical translation in only one direction (e.g., e i p ( e i | f ) to derive statistical synonyms. Other ways of constructing statistical synonym sets are also possible (e.g., distributional similarity in monolingual corpora), but recent work on word sense disambiguation suggests that translation usage can provide a strong basis for identifying synonyms [16].
Statistical word-to-word translation has been well stud-ied, and a number of effective implementations are avail-able (e.g., [13]). To derive a word-to-synset mapping model from a statistical word-to-word translation model, we aggre-gated multiple translation alternatives based on synsets in the target language. Since some translations might appear in more than one synset, we needed some way of assigning their translation probability across those synsets. We used a simple greedy method, iteratively assigning each transla-tion to the synset that would yield the greatest aggregate probability. Specifically, the algorithm worked as follows: 1. Compute the aggregate probability that e maps to each 2. Select Synset s j with the largest aggregate probability,
Figure 2 illustrates the greedy method of aggregating syn-onymous translation alternatives into synsets by an exam-ple. In that example, four translations of word e are grouped into two synsets s 2 and s 3 : s 2 contains three of the four translation with p ( s 2 | e )=0 . 8, while s 3 contains only the other one translation with p ( s 3 | e )=0 . 2. Thus, probabilis-tic mapping of words in one language to synsets in another language is achieved.

The selected synsets then form a word-to-synset map-ping for e . The same computation can be performed in the other direction. Because greedy aggregation results in unique mappings, at most one alignment can exist in which a query term e maps to a document-language synset s d that contains f and document term f maps to a query-language synset s q that contains e . As a result, the summation in Equation 5 will be unused. We call the resulting technique Derived Aggregated Meaning Matching (DAMM).

The incorporation of aggregation is a distinguishing char-acteristic of the meaning matching model, so we wanted to isolate the effect of aggregation for a contrastive analysis. If we simply assume that each term encodes a unique meaning, we get p ( e  X  f )= p ( e | f )  X  p ( f | e ). We call this Individual Meaning Matching (IMM). Similarly, we can isolate the ef-fect of bidirectional translation knowledge by further assum-ing uniform translation probabilities in one direction. For example, assuming a uniform distribution for p ( e | f ) across all f yields (after normalization) p ( e  X  f )= p ( f | e ), which is exactly the formulation of PSQ. If uniform translation probabilities are assumed in both directions p ( e  X  f )be-comes a constant factor. In this case, PSQ is simplified as Pirkola X  X  structured queries. In the next section we describe Test collection from CLEF X 01-03 TREC-5,6 Query language English English
Document language Fr en ch Chinese #ofsearchtopics 151 54 #ofdocuments 87,191 139,801 Avg. # of rel docs per topic 23 95 experiments to compare the relative effectiveness of PSQ, IMM, DAMM, and FAMM.
To evaluate the effectiveness of the proposed meaning matching model for CLIR, we conducted two sets of exper-iments: one on retrieving French news stories with English queries and the other on retrieving Chinese news stories with English queries. This section describes the experiment setup for the study, including the selection of the test collection and IR system, and training translation models, inducing statistical synonyms
Table 1 shows the statistics of the two test collections used in our experiments. For English-French CLIR, we ac-cumulated the French test collections created by the Cross-Language Evaluation Forum (CLEF) in 2001, 2002 and 2003 into a single collection. 1 We stripped accents from the docu-ment collection and removed French terms contained on the stopword list provided with the open source Snowball stem-mer. 2 We then created a document index based on stemmed French terms. We formulated TD queries with words from the title and description filed in the search topics. For En-glish queries, we performed pre-translation stopword-removal using an English stopword list provided with Inquery. For French queries, we performed accent-removal, stopword-removal, and stemming using the same tools that we used for pro-cessing the document collection. The French queries serve to establish a useful upper baseline for CLIR effectiveness.
For English-Chinese CLIR, we accumulated search topics from TREC-5 and TREC-6, which used the same Chinese document collection. That gives us a total of 54 topics. The Chinese documents, originally encoded in GB code, were converted into UTF-8 using the uconv codeset con-version tool and then segmented into individual words us-ing the LDC Chinese segmenter. 3 The resulting document collection was then converted into hexadecimal format that guards against character handling problems [10]. We also formulated TD queries. For Chinese queries, we performed codeset conversion and segmentation in the same way that the Chinese documents were processed. For English queries, we again removed stopwords using the Inquery stopword list.
All our experiments were run using the Perl Search Engine (PSE), a document retrieval system based on Okapi BM25 weights that already implements PSQ. We obtained PSE from the University of Maryland and modified it to imple-ment other variants of cross-language meaning matching. In
The 9 of the 160 accumulated topics that do not have rel-evant French documents were removed from the collection. http://snowball.tartarus.org/ http://www.ldc.upenn.edu/Projects/Chinese/segmenter /mansegment.perl Table 2: Corpus statistics and model iterations for training translation models. the Okapi BM25 formula [17], We used k 1 =1 . 2, b =0 . 75, and k 3 = 7 as has been commonly used.
Table 2 describes the process that we used to train our statistical translation models. For both language pairs, we derived word-to-word translation models in both directions using the freely available GIZA++ toolkit [13]. 4 For French, we trained the translation models with the Europarl parallel corpus [6]. For Chinese, we combined corpora from multiple sources including the Foreign Broadcast Information Service (FBIS) corpus, HK News and HK Law, UN corpus, and Sinorama, the same corpora also used by Chiang et al [3]. We stripped accents from the French documents, segmented the Chinese documents with the same version of LDC seg-menter that was used for indexing, and filtered out implau-sible sentence alignments by eliminating sentence pairs with a token ratio either smaller than 0.2 or larger than 5.
For both language pairs, we ran GIZA++ twice, with ei-ther of the two languages as the source language respec-tively. When training translation models for the English-French pair, we started with 5 HMM iterations, followed by 10 IBM Model 1 iterations, and ending with 5 IBM Model 4 iterations. The net result of this process was two trans-lation tables, one from English words to French words and the other from French words to English words. All nonzero values produced by GIZA++ were retained in each table. We ran our Chinese-English experiments after the English-French experiments with the goal of confirming our results using a different language pair, so we made a few changes to reduce computational costs. Model 4 seeks to achieve bet-ter alignments by modeling systematic position variations; that is an expensive step not commonly done for CLIR ex-periments. We therefore omitted Model 4 for the English-Chinese pair. We ran 10 IBM Model 1 iterations followed by 5 HMM iterations. A comparison of results using lexi-cons from before and after the 5 HMM iterations indicated no noticeable difference between the two conditions, so in this paper we report Chinese-English results only for the 10 IBM Model 1 iterations. Finally, we observed in our English-French experiments that working with a large number of low probability translations yielded both lower effectiveness and greater computational costs, so we imposed a cumulative probability threshold of 0.99 on the model for each trans-lation direction before creating bidirectional models for our English-Chinese experiments. In this section, we report our experiment results for both English-French CLIR and English-Chinese CLIR. We present http://www-i6.informatik.rwth-aachen.de/Colleagues/och/software/GIZA++.html Figure 3: Comparison with the top 5 official CLEF runs. the results in three parts: (1) establishing a strong upper baseline using French queries, (2) establishing a strong lower baseline using known CLIR techniques with English queries, and (3) comparing the retrieval effectiveness of the meaning matching model with those baselines. We show that mean-ing matching that combines bidirectional translation and statistical synonymy knowledge achieved results that were statistically indistinguishable from the upper (monolingual) baseline and significantly better than the lower (CLIR) base-line for CLIR with both language pairs.
Although not strictly an upper bound (because of expan-sion effects), it is quite common in CLIR evaluation to com-pare the effectiveness of a CLIR system with a monolingual baseline. We obtained monolingual baselines for each lan-guage pair by retrieving documents with TD queries for-mulatedfromsearchtopicsthatareexpressedinthesame language as the documents.

To get a better idea of the effectiveness of our monolin-gual baselines, we compared them with published top re-sults gained from experiments with the same test collec-tions. For the English-French CLIR experiments, we com-puted the mean average precision (MAP) over 50 queries formulated from the CLEF 2001 topic set (Topics 41-90). Figure 3 shows the MAP of the top five official monolingual French runs from CLEF 2001. Our baseline (BASE in the figure) achieved a MAP of 0.470, which is above the aver-age (0.460) of those top five runs but lower than the top threeruns. WenoticedthebestCLEF2001runtweaked the stopword list and stemming, and, in particular, used query expansion based on blind relevance feedback [18]. To facilitate comparison, we also expanded our original French queries with the top 20 words selected from the top 10 re-trieved documents based on Okapi weights, weighting the added words with a coefficient of 0.1. This resulted in a monolingual MAP of 0.501 (BASE-BRF in Figure 3) that closely matched the best official run in CLEF 2001 mono-lingual French retrieval. This suggests that our monolingual baseline is strong. With a goal to study the relative effec-tiveness of the meaning matching model, we want to avoid masking those effects by other factors. Therefore, blind rel-evance feedback was not used in the remaining runs.
For the monolingual baseline in the English-Chinese CLIR experiments, we computed results for the same 19 TREC-5 Figure 4: Comparison of meaning matching with monolingual baseline and PSQ for English-French CLIR queries for which results had been reported at the TREC-5 conference. We obtained a MAP of 0.280, which was at the median of the 15 automatic official runs submitted to TREC-5. 5 Most of those runs across which the median was computed used longer queries (all words from the title, de-scription, and narrative field), however, whereas we used only the title and description fields for all of our experi-ments (in both language pairs). Moreover, as had been the case for French we did no automatic query expansion. We therefore feel that our monolingual baseline for Chinese is a reasonable one.
A major motivation for us to develop the cross-language meaning matching model is to improve CLIR effectiveness over a strong CLIR baseline. We chose probabilistic struc-tured queries (PSQ) as our CLIR baseline because among vector space techniques for CLIR it presently yields the best retrieval effectiveness. Direct comparison to techniques based on language modeling would be more difficult to in-terpret because vector space and language modeling handle issues such as smoothing and DF differently.

Figure 4 shows the relative English-French CLIR effective-ness as compared to the monolingual French baseline. We ran CLIR and computed MAP at different Cumulative Prob-ability Thresholds (CPT). What is shown at each point in the figure is the monolingual percentage of the CLIR MAP. Overall, English-French CLIR was very effective, achieving at least 90% of monolingual MAP when translation alterna-tives with very low probability were excluded. In addition, the baseline PSQ technique exhibited the same decline in MAP near the tail of the translation probability distribu-tion (i.e., at high cumulative probability thresholds) that Darwish and Oard reported [4]. The best MAP of PSQ was obtained at a CPT of 0.5, which is near 95% of monolingual effectiveness. However, the difference is still statistically sig-nificant by a Wilcoxon signed rank test (at p&lt; 0 . 05).
In the English-Chinese case, PSQ with multiple transla-tions was always better than with the one-best translation (corresponding to the CPT of 0) before the cumulative prob-ability reached 0.99, which is where the best PSQ was ob-tained. However, MAP of the best PSQ was just about 82% http://trec.nist.gov/pubs/trec5/t5 proceedings.html Figure 5: Comparison of meaning matching with monolingual baseline and PSQ for English-Chinese CLIR of monolingual MAP, and was significantly lower.

In the English-Chinese CLIR experiments, CLIR MAP did not tail off because we excluded translations after the cumulative probability reached 0.99.
Also shown in Figure 4 and Figure 5 are cross-language meaning matching based on bidirectional translation and synonym aggregation. The effectiveness of English-French CLIR based on IMM, which uses bidirectional translation but without synonymy knowledge, showed monotonic in-crease before CPT reaches 0.9. The highest MAP (0.376 at a CPT of 0.9) is about 97% of monolingual MAP, which is statistically indistinguishable from either the best PSQ or the monolingual baseline. For English-Chinese CLIR, the effectiveness of IMM showed similar pattern of changes. As far as comparison is concerned, the best IMM (at a CPT of 0.99) is about 90% of monolingual MAP, which is sig-nificantly better than the best PSQ while still worse than monolingual baseline.

The monotonic increase of MAP at low and medium CPT regions seems to indicate some advantage of using bidirec-tional translation knowledge over unidirectional translation knowledge. Essentially this is because using bidirectional translation knowledge can both eliminate some spurious trans-lation alternatives that are otherwise included in unidirec-tional translation and gives better estimation of meaning matching probability. However, such effects are limited, especially when many low probability translations are in-cluded. In fact, after a CPT of 0.9 in English-French CLIR, IMM decreased faster than PSQ, showing combining bidirec-tional translation knowledge may have included more low-probability translations than using unidirectional translation knowledge. A statistical translation model can in princi-ple translate any word into any other word appearing in any aligned sentence, and low probability events are natu-rally not very well modeled. We show below that synonymy knowledge can partially offset the negative effect due to the inclusion of too many low-probability translations.
When bidirectional translation knowledge is combined with statistical synonymy knowledge, which is the case of derived aggregated meaning matching (DAMM), the best DAMM was significantly better than the best PSQ for both English-French CLIR (with 6% relative improvement) and English-Figure 6: Query-by-query comparison of the best DAMM and the best PSQ for English-French CLIR.
 Chinese CLIR (with 19% relative improvement), achieving cross-language MAP comparable to monolingual baselines in both cases. However, in both cases, the best DAMM was statistically indistinguishable from the best IMM. Putting these findings together with the above comparisons of IMM with PSQ and monolingual retrieval, it is reasonable to say that both bidirectional translation knowledge and synonymy knowledge can help, and combining them can help more.
For English-French CLIR, full aggregated meaning match-ing (FAMM) with aligned synsets obtained from EuroWord-Net reached only about 30% of monolingual MAP, which is significantly worse than any of the meaning matching tech-niques we tried. We found that many high-probability trans-lations contained in the GIZA++ translation tables were not covered by the aligned synsets, and our implementation of FAMM therefore treated their probabilities as zero. This is clearly undesirable, and future work on compensating for limited word coverage of aligned synsets is needed. Overall, aggregation had little effect at low CPT values. This is mainly because the number of translation alterna-tives included at low CPT values was very small (in most cases there was just one translation selected). Generally, the more translations involved, the larger effect aggregation is likely to have. Therefore, at high CPT values where more translations are included, aggregation tends to have more effect on meaning matching.

Although a Wilcoxon signed rank test shows DAMM sig-nificantly outperformed PSQ when the CPT threshold was adjusted most favorably for each, we want to further in-vestigate what actually happened through query-by-query comparison. We plot the non-interpolated average precision (AP) difference for each query between the best DAMM and the best PSQ in the English-French CLIR experiments (see Figure 6). Among the 151 queries, 67 had higher AP with DAMM, 48 had higher AP with PSQ, and the remaining 36 were the same  X  revealing the difference between them was not due to a small set of topics. Same comparison of the best DAMM and the best PSQ in the English-Chinese CLIR ex-periments confirmed this finding. There are other variants of cross-language meaning matching, depending on transla-tion in which direction is used and synonymy knowledge in which language is used. For example, a  X  X robabilistic Doc-ument Translation X  (PDT) technique which uses document translation knowledge in a similar way as PSQ can be devel-oped; synonymy knowledge in the target language can also be used when only unidirectional translation is considered. We did run experiments for both language pairs and found PDT was at least as effective as PSQ, but adding statisti-cal synonymy knowledge to unidirectional translation could hurt CLIR performance. The latter finding suggests the ne-cessity of combining bidirectional translation with synonymy knowledge. We also compared our meaning matching tech-nique, which basically multiplies translation probabilities, with an earlier approach in which an arithmetic mean was used [20]. Both techniques used bidirectional translation statistics more effectively than unidirectional probabilities. We found, however, when synonym aggregation was used, meaning matching was the more effective technique. De-tailed cross-language meaning matching variants and their experimental evaluation can be found in [19].

We want to point out that the interpretation of the sta-tistical significance tests in our study should be cautious. We compared the optimal effectiveness of different mean-ing matching variants, which is usually achieved at different CPT levels. In an operational system, however, it is hard to tune the parameter without pre-existing knowledge of rele-vance. Therefore, our findings should only be interpreted as the meaning matching technique could potentially outper-form one of the best known query translation techniques.
This paper introduced a general framework for the use of translation probabilities in CLIR. We started with one of the most fundamental issues in IR, the question of how to match what the searcher means with what the document author meant. That naturally pointed us to the direction of translating both queries and documents, or more precisely, using translation knowledge in both directions. Differen-tial polysemy makes statistical translation models by nature asymmetric, and selection of either direction alone would be counterintuitive when matching meanings is the goal. From that key insight, we developed a computational formalism that integrated knowledge about translation and synonymy into a unified model using techniques similar to those previ-ously developed for the probabilistic structured query tech-nique. We then showed that the probabilistic structured query method is a special case of our meaning matching model when only query translation knowledge is used.
Our experiments with an English-French test collection for which a large number of topics are available showed that CLIR using bidirectional translation knowledge together with statistical synonymy significantly outperformed CLIR in which only unidirectional translation knowledge was exploited, achiev-ing CLIR effectiveness comparable to monolingual effective-ness under similar conditions. Despite the big differences between the two language pairs, our experiments on English-Chinese CLIR consistently confirmed these findings, show-ing the proposed cross-language meaning matching tech-nique is not only effective, but also robust. The importance of the technique and the study lies in it introduces a novel and effective way of using statistical translation knowledge for searching information across language boundaries.
Several things should be considered for improving the pro-posed model. First, studies in statistical MT have showed that translation based on learned phrases (or  X  X lignment templates X ) can be more accurate than translation based solely on individual words [14]. A natural next step would therefore be to integrate phrase translation into our meaning matching model. Second, we only tried the greedy method of aggregation. The method assigns each translation al-ternative to only one synset. It may also be worth test-ing other techniques that assign each translation alternative to multiple synsets with some weighting factor, e.g., based on information such as orthographic similarity between the translation and words in each synset. Next, an obvious lim-itation of our current implementation of meaning matching is its reliance on sentence-aligned parallel corpus, which is necessary for training statistical translation models. Now that our experiments have shown that meaning matching based on bidirectional translation knowledge is quite robust with respect to noisy translations, it might be interesting to see how it performs with translation knowledge obtained from comparable corpora. Finally, decisions for some pa-rameter settings in our study were somewhat arbitrary, e.g., synonyms were cut off at the probability of 0.1, and selec-tions and iterations of IBM Models in statistical MT training were also quite limited. In the future, we plan to explore a broader spectrum of parameter settings, which will hope-fully provide us a better and more complete understanding of the cross-language meaning matching framework. The authors would like to thank James Mayfield, Philip Resnik, Vedat Diker, Dagobert Soergel, Jimmy Lin, and all the members of the Computational Linguistics and Infor-mation Processing Laboratory at the University of Mary-land Institute for Advanced Computer Studies for their valu-ablecomments. Thisworkhasbeensupportedinpartby DARPA contract N661010028910 (TIDES) and HR0011-06-2-0001 (GALE). [1] M. Boughanem, C. Chrisment, and N. Nassr.
 [2] Martin Braschler. Combination approaches for [3] David Chiang, Adam Lopez, Nitin Madnani, Christof [4] Kareem Darwish and Douglas W. Oard. Probabilistic [5] In-Su Kang, Seung-Hoon Na, and Jong-Hyeok Lee. [6] Philipp Koehn. Europarl: A multilingual corpus for [7] Wessel Kraaij. Variations on Language Modeling on [8] K. L. Kwok. Exploiting a chinese-english bilingual [9] Victor Lavrenko and W. Bruce Croft. Relevance-based [10] Gina-Anne Levow and Douglas W. Oard. Evaluating [11] J. Scott McCarley. Should we translate the documents [12] Douglas W. Oard and Funda Ertunc.
 [13] F.J.OchandH.Ney.Improvedstatisticalalignment [14] Franz Josef Och and Hermann Ney. The alignment [15] Ari Pirkola. The effects of query structure and [16] Philip Resnik and David Yarowsky. Distinguishing [17] S. E. Robertson and Karen Sparck-Jones. Simple [18] Jacques Savoy. Report on CLEF-2001 experiments: [19] Jianqiang Wang. Matching Meaning for [20] Jinxi Xu and Ralph Weischedel. TREC-9 cross-lingual
