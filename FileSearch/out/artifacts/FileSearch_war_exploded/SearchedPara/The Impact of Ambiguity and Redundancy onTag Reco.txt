 Collaborative tagging applications have become a popular t ool al-lowing Internet users to manage online resources with tags. Most collaborative tagging applications permit unsupervised t agging re-sulting in tag ambiguity in which a single tag has many differ ent meanings and tag redundancy in which several tags have the sa me meaning. Common metrics for evaluating tag recommenders ma y overestimate the utility of ambiguous tags or ignore the app ropri-ateness of redundant tags. Ambiguity and redundancy may eve n burden the user with additional effort by requiring them to c larify an annotation or forcing them to distinguish between highly related items. In this paper we demonstrate that ambiguity and redun dancy impede the evaluation and performance of tag recommenders. Five tag recommendation strategies based on popularity, collab orative filtering and link analysis are explored. We use a cluster-ba sed ap-proach to define ambiguity and redundancy and provide extens ive evaluation on three real world datasets.
 H.3 [ Information Storageand Retrieval ]: H.3.3 Information Search and Retrieval X  Search process ;H.2 [ Database Management ]: H.2.8 Database application X  Data mining Experimentation, Performance Folksonomies, ambiguity, redundancy, recommender system s
Collaborative tagging applications, also known as folkson omies [18], have emerged as a powerful trend allowing Internet use rs to annotate, share and explore online resources. Delicious 1 delicious.com users as they bookmark URLs. Citeulike 2 enable researchers to manage scholarly references. Bibsonomy 3 allows users to tag both. Still other collaborative tagging systemsspecialize in mu sic, photos and blogs.

At the core of collaborative tagging systems is the post: a us er describes a resource with a set of tags. Taken in isolation, a n in-dividual post allows a user to organize web resources for lat er use: resources can be easily sorted, aggregated and retrieved. T aken as a whole the sum of many posts results in a complex network of in -terrelated users, resources and tags that is useful in its ow n right or for data mining techniques that support the user X  X  activiti es.
Despite the many benefits offered by folksonomies, they also present unique challenges. Most collaborative tagging app lications allow the user to describe a resource with any tag they choose . As a result they contain numerous ambiguous and redundant tags . An ambiguous tag has multiple meanings:  X  X pple X  may refer to th e fruit or the company. Redundant tags share a common meaning:  X  X merica X  and  X  X SA X  confer the same idea.

These tags make it difficult to judge the effectiveness of tag rec-ommendation algorithms which suggest tags for a user during the annotation process. Standard evaluation approacheswill o ften view recommended ambiguous tags as hits when they appear in a hold -out set even if the underlying meaning of the recommended tag was different than the context in which it appears in the hold out set. Such recommendations, therefore, while appearing to b e effec-tive in the evaluation process, in reality may mislead users as they search for relevant resources.

Redundancy can hamper the effort to judge recommendations as well, but from the opposite perspective. A recommended ta g may be counted as a miss even though it is synonymous to a tag in the holdout set. An example may be when the holdout set for a test user contains the tag  X  X ava" while the recommendation set contains  X  X ava." Therefore, redundancy may mask the true ef fec-tiveness of the recommendation algorithm: while from the us er X  X  perspective  X  X ava" is a good recommendation, in the evaluat ion process it would appear as incorrect.

Our goal in this work is to determine the impact of these two ph e-nomena on the effectiveness of tag recommendation. We emplo y a cluster-based approach to define and measure ambiguity and re-dundancy. We cluster both resources and tags into highly coh esive partitions based on co-occurrence. A tag is considered ambi gu-ous if several resources from different clusters have been a nnotated with it. Tags from the same tag cluster are considered redund ant. We chose the cluster-based approach over a variety of semant ic and linguistic approaches because it provides a more general an d lan-citeulike.org bibsonomy.org guage independent method for defining ambiguity and redunda ncy. We define metrics, based on the resulting clusters, for measu ring the degree of ambiguity for a tag, and the level of redundancy for pairs of tags. We provide extensive evaluation on three r eal world Folksonomies to determine the impact of ambiguity and re-dundancy across several common tag recommendation algorit hms as well as across data sets.

The rest of this paper is organized as follows. In Section 2 we de-tail previous work related to tag recommendation in Folkson omies and efforts in identifying and quantifying ambiguity and re dun-dancy. In Section 3 we briefly describe our clustering approa ch and then detail our measures for ambiguity and redundancy. S ec-tion 4 reviews five tag recommendation strategies employed i n our evaluation. Our methodology, datasets and experimental re sults are offered in Section 5. Finally, we conclude the paper with dir ections for future work.
The term  X  X olksonomy X  was first coined in [21], a portmanteau of  X  X olk X  and  X  X ntology X . Folksonomies permit users to anno tate online resources with tags. These tags can serve multiple fu nc-tions: convey ownership ( X  X on X ), describe the resource ( X  X  rticle X ), describe the characteristics of the resource ( X  X olksonomi es X ), pro-vide subjective commentary ( X  X ool X ), or help organize the r esource ( X  X oread X ) [8].

A folksonomy can be described as a four-tuple: a set of users, U ; a set of resources, R ; a set of tags, T ; and a set of annotations, A . We denote the data in the folksonomy as D and define it as: D =  X  U, R, T, A  X  . The annotations, A , are represented as a set of triples containing a user, tag and resource defined as: A  X  { X  u, r, t  X  : u  X  U, r  X  R, t  X  T } .

A folksonomy can therefore be viewed as a tripartite hyper-g raph [19] with users, tags, and resources represented as nodes an d the annotations represented as hyper-edges connecting one use r, one tag and one resource.

Many authors have attempted to exploit this data structure i n or-der to recommend resources, tags or even other users. In [10] the authors proposed an adaptation of link analysis to the folks onomy data structure for resource recommendation. They have call ed this technique Folkrank since it computes a Pagerank [3] vector from the tripartite graph. In [11] Folkrank is used for tag recomm en-dation. While it suffers from extreme computational costs, it has proven to be one of the most effective tag recommenders; as su ch we have included it in our evaluation.

In [17, 11] traditional collaborative filtering algorithms were extended for tag recommendation in folksonomies. In [15] th e authors personalized tag recommendation by considering th e user profile as well as the tags most often applied to the resource b eing annotated. Because of its simplicity, popularity and effec tiveness we analyze this technique as well.

Ambiguity is a well known problem in information retrieval a nd has been identified as a problem in folksonomies as early as [1 8]. WordNet has been used to identify ambiguous tags and disam-biguate them by using synonyms [14]. Folksonomy searches ar e expanded with ontologies in [12] to solve the ambiguity in ta g-ging systems. Clustering was used to measure ambiguity in [2 4]. They focus on the network analysis techniques to discover cl usters of nodes in networks. In [13] multi-dimensional scaling is u sed for co-word clustering to visualize the relationships between tags. We also presume that tags and resources can be aggregate into ti ghtly related clusters.

Entropy as a measure of tag ambiguity has been proposed in [25, 23]. They used a probabilistic generative model for dat a co-occurrence to determine ambiguous tags. The tagging space i s as-sume to cover different categories and the tag membership of each category is estimated via the EM algorithm. Our measure of am -biguity uses a similar approach. We cluster resources and us e the distribution of tags across the clusters to measure their am biguity.
Redundancy in a folksonomy is due largely to the ability of us ers to tag resources irrespective of a strict taxonomy. In [8] tw o types of redundancy are identified: structural and synonymic. In [ 22] structural redundancy is explained by stemming to remove su ffixes, removing stop words, comparing tags for differences of only one character or identifying compound tags. Synonymic redunda ncy is evaluated in [1] by using WordNet to determine synonyms. Clu s-tering has also been utilized to identify redundancy. In [6, 7] ag-glomerative clustering is used to identify similar tags. Si nce stem-ming and lexical databases are ill suited to deal with the inh erent chaos found in tags we rely on clustering to aggregated tags i nto single topic clusters.
In this paper we utilize a cluster based definition of ambigui ty and redundancy. Resources are modeled as a vector over the se t of tags. In calculating the vector weights, a variety of measur es can be used: recency, adjacency, or frequency. In this work we re ly on frequency. The term frequency for a resource tag pair is the number of times the resource has been annotated with the tag. We defin e tf as: tf(r,t) = |{ a =  X  u, r, t  X   X  A : u  X  U }| .

Several techniques exist to calculate the similarity betwe en vec-tors such as Jaccard similarity, Pearson correlation, or co sine simi-larity. We rely on cosine similarity [20]. Similarity betwe en tags, modeled as a vector over the resource space, can be defined ana lo-gously.

While the approach for measuring ambiguity and redundancy i s independent from any specific clustering method, because of its speed, simplicity and popularity we rely on K-Means cluster ing [5, 16]. Resources are randomly partitioned into k initial sets. Ascer-taining the ideal value k is a difficult problem. In this work we use both subjective evidence (visually inspecting the cluster ing) and Hubbert X  X  correlation with distance matrix [4] defined as: where M = n ( n  X  1) / 2 , the numberof tag pairs in the folksonomy; C ( t i , t j ) is 1 if the two tags are in the same cluster; and S ( t is the similarity between the two tags.

Several values of k are evaluated using h ( k ) , and k is chosen such that tightly connected tag clusters are not further sep arated for larger values of k .
Ambiguous tags have multiple meanings. A tag may have differ -ent word senses;  X  X pple X  can refer to the company or to the fru it. Names may also result in ambiguity;  X  X aris X  might mean the ci ty or the celebrity. Subjective tags such as  X  X ool X  can result in a mbiguity since different users have contradictory notions of what co nstitutes cool. Finally, overly vague tags such as  X  X ool X  can mean gard ening implements to some or software packages to others.

Ambiguous tags can impede users as they navigate the system or burden the user with unwanted recommendations. At a syste ms level ambiguous tags can introduce erroneous features into the user profile. While recommenders are often judged by their abilit y to predict items occurring in a holdout set, the quality of a tag recom-mender may be underestimated by traditional metrics if it ro utinely passes up these tags in order to recommend tags with greater i n-formation value. Moreover, evaluation metrics may overval ue a recommender that proposes ambiguous tags despite their lac k of specificity.

We define a tag as ambiguous if it has been applied to several re -sources from among different resource clusters. Assuming t hat the clustering algorithm has effectively aggregated similar r esources and separated resources with little similarity, then a tag w hich has been annotated to resources of many clusters can be assumed t o be more ambiguous than a tag which has been narrowly applied.
Given a cluster of resources, c  X  C r , we may then define the cluster frequency, cf , of a tag as the sum of the term frequencies over the resources in the cluster:
We may then adapt entropy in order to compute the ambiguity of a tag, a ( t ) , as: where f ( t ) is the frequency of the tag in the folksonomy.
The entropy of a tag reflects its ambiguity by revealing wheth er it is distributed across many resource clusters or if it is co nfined to only a few clusters. We may further define the ambiguity of a recommendation set as the average ambiguity of its tags.
Because users may annotate resources with any tag they choos e, folksonomies are laden with redundant tags that share a comm on meaning. Syntactic variance such as  X  X logs X  or  X  X logging X  c an cause redundancy. Case ( X  X ava X  or  X  X ava X ), spelling ( X  X ray  X  or  X  X rey X ), and multilinguism ( X  X hoto X  or  X  X oto X ) may also res ult in redundancy. The use of non-alphanumeric characters, abb re-viations, acronyms and deliberate idiosyncratic tagging a re other sources of redundancy.

Redundant tags make it difficult to the judge the quality of re c-ommendations. Traditionally a holdout set is employed to ra te an algorithm X  X  effectiveness. However if the utility metric d efines suc-cess as the ability to exactly match the holdout set, the usef ulnessof the recommender can be undervalued. Redundant tags that cle arly reflect the user X  X  intent in the holdout set should be counted as matches; this more accurately reflects the benefit of the reco mmen-dation to the user.

For example, consider the case in which  X  X ecSys X  is in the hol d-out set and  X  X ec_sys X  is suggested. Standard evaluation tec hniques would count this as a miss, no better than if it had recommende d a completely irrelevant tag. Instead, if the two tags are know n to be redundant, the redundancy aware metric would count the sugg ested tag as a hit.

In order to detect redundancy we rely on clusters of tags. Two tags are considered redundant if they are members of the same clus-ter. When evaluating tag recommenders we use recall and prec ision as well as a redundancy aware versions that rely on clusters o f tags.
Recall is a common metric of recommendation algorithms that measures coverage. It measures the percentageof items in th e hold-out set, T h , that appear in the recommendation set T r . It is defined as: r = ( | T h  X  T r | ) / | T h | . Precision is another common metric that measures specificity and is defined as: p = ( | T h  X  T r
Redundancy aware recall and precision assume a set of tag clu s-ters. A recommended tag is considered a hit if it or a tag in its cluster also appears in the holdout set: where R ( i, T h ) is defined as 1 if i or one of its redundant tags appears in T h , and 0 otherwise. Redundancy aware precision is similarly defined:
As in standard recall and precision, the redundancy aware me t-rics will fall between 0 and 1.

In order to calculate the redundancy of a recommendation set , each tag-tag pair in the set is evaluated as to whether or not t hey appear in the same cluster: where N is the number of tag pairs in T r and C ( t i , t two tags share a cluster, and 0 otherwise.
Here we review several common recommendation techniques which we employ in our evaluation. Recommendation in folk-sonomies may include the suggestion of tags during the annot ation process, new resources to users as they navigate the system o r even other users that share common interests. In this paper we foc us on tag recommendation. We consider techniques based on popula rity, collaborative filtering and linkage analysis.

In traditional recommendation algorithms the input is ofte n a user, u , and the output is a set of items, I . Tag recommendation in folksonomies differs in that the input is both a user, u , and a resource, r . The output remains a set of items, in this case a rec-ommended set of tags, T r . For each recommendation approach we define  X  ( u, r, t ) to be the relevance of tag, t , for the user-resource pair. A recommender will return the top n tags with the highest  X  .
Perhaps the simplest recommendation strategy is merely to r ec-ommend the most commonly used tags in the folksonomy. Alter-natively, given a user-resource pair a recommender may igno re the user and recommend the most popular tags for that particular re-source. This strategy is strictly resource dependent and do es not take into account the tagging habits of the user. We define  X  for resource based popularity, pop r , recommendations as:
In a similar fashion a recommender may ignore the resource an d recommend the most popular tags for that particular user. Wh ile such an algorithm would include tags frequently applied by t he user, it does not consider the resource information and may r ec-ommend tags irrelevant to the current resource. We define  X  for user based popularity, pop u , recommendations as:
User Based K -Nearest Neighbor is a commonly used recom-mendation algorithm in Information Retrieval that can be mo dified for tag recommendation in folksonomies. Traditionally it fi nds a set of users similar to a query user. From these neighbors a se t of recommended items is constructed.

We can modify this approach by ignoring neighborsthat have n ot tagged the query resource. Once a neighborhood of similar us ers has been discovered, the algorithm considers only those tag s that have been applied to the query resource and calculates a rele vance for each tag,  X  , as the average similarity of the neighbors that have applied the tag. Thus the algorithm is resource driven throu gh both the selection of neighbors and the selection of tags. Still i t remains user driven in that neighbors are determined through a user m odel.
Users may be modeled in a myriad of ways; here we model each user as a vector over the set of tags. We call this method KN N Alternatively we may model each user as a vector of resources . We call this method KN N ur .

We again rely on term frequency to calculate the weights in the vector using either tag counts or resource counts and use cos ine similarity to define the similarity, sim ( u 1 , u 2 ) , between users.
Folkrank was proposed in [10]. It computes a Pagerank vector from the tripartite graph of the folksonomy. This graph is ge nerated by regarding U  X  R  X  T as the set of vertices. Edges are defined by the three two-dimensional projections of the hypergraph .
If we regard the adjacency matrix of this graph, W , (normalized to be column-stochastic), a damping factor, d , and a preference vec-tor, p , then we iteratively compute the Pagerank vector, w , in the usual manner: w = dAw + (1  X  d ) p .
 However due to the symmetry inherent in the graph, this basic Pagerank may focus to heavily on the most popular elements. T he Folkrank vector is taken as a difference between two computa tions of Pagerank: one with and one without a preference vector. Ta g recommendations are generated by biasing the preference ve ctor towards the query user and resource [11]. These elements are given a substantial weight while all other elements have uniforml y small weights.

We include this method as a benchmark as it has been shown to be an effective method of generating tag recommendations. H ow-ever, it imposes steep computational costs.
In this section we discuss our datasets. We then discuss our e x-perimental methodology. Since the clustering of tags and re sources is a fundamental step in our ambiguity and redundancy metric s we provide both empirical and anecdotal evidence of the qualit y of the clusters. We then provide two separate collections of exper iments: the first on ambiguity, the second on redundancy. We have chosen three datasets for our experiments: Deliciou s, Bibsonomy and Citeulike. In order to reduce noise and focus o n the denser portion of the dataset a P -core was taken such that each user, resource and tag appear in at least p posts as in [2, 11]. Table 1 shows the distribution of the datasets.
 Delicious is a popular Web site in which users annotate URLs. On 10/19/2008, 198 of the most popular tags were taken from th e Table 1: The number of users, resources, tags, posts and an-notations for the datasets before and after ambiguous tags a re removed. user interface. For each of these tags the 2,000 most recent a nnota-tions including the contributors of the annotations were co llected. This resulted in 99,864 distinct usernames. For each user th e social network was explored recursively resulting in a total of 524 ,790 usernames.

From 10/20/2008 to 12/15/2008 the complete profiles of all us ers were collected. Each user profile consisted of a collection o f posts including the resource, tags and date of the original bookma rk. The top 100 most prolific users were visually inspected; twelve w ere removed from the data because their post count was many order s of magnitude larger than other users and were suspectedto be Web-bots. Due to memory and time constraints, 10% of the user profi les was randomly selected. A p-core of 20 was taken from this data set for experiments.

The Bibsonomy dataset was gathered on 1/1/2009 encompassin g the entire system. This data set has been made available onli ne by the system administrators [9]. They have pre-processed the data to remove anomalies. A 5 -core was taken to reduce noise and increase density.

Citeulike is used by researchers to manage and discover scho l-arly references. The dataset is available to download. On 2/ 17/2009 the most recent snapshot was taken. The data contains anonym ous user ids and posts for each user including resources, the dat e and time of the posting and the tags applied to the resource. A P -core of 5 was calculated.
We employ the leave one post out methodology as described in [10]. One post from each user was placed in the testing set con sist-ing of a user, u , a resource, r , and all the tags the user has applied to that resource. These tags, T h , are analogous to the holdout set commonly used in Information Retrieval evaluation. The rem ain-ing posts are used to generate the recommendation models.
The tag recommendation algorithms accepts the user-resour ce pair and returns an ordered set of recommended tags, T r . From the holdout set and recommendationset utility metrics were cal culated. For each metric the average value was calculated across all t est cases.
The clustering algorithm is independent of the ambiguity an d re-dundancy measures. Still, we assume that tags and resources can be aggregated into coherent clusters representing distinc t topic ar-eas. Support for that assumption is given in Table 2 where fou r tag clusters are presented.

Cluster 1 represents the idea of  X  X ecommendation X , while cl us-ter 2 represents New York 4 . Other clusters show clearly recog-nizable categories. Clusters can capture misspellings, al ternative spellings or multilinguism such as in cluster 3:  X  X olksonom y X  ver-sus  X  X olksonomies. X  They can also capture tags that share a s imilar concept space:  X  X ecommendation X  and  X  X ollaborativefilter ing. X 
Cluster 4, on the other hand, shows how clustering can be ef-fected by ambiguity. Two similar yet distinct senses of clus ter-ing have been aggregated: clustering algorithms and comput ational clusters 5 .

Visual examination of two resource clusters also shows the e f-fectiveness of aggregating similar items. In the example re sources are clearly related either to album artwork or to maps.

Despite the apparent ability of the clustering approach to a ggre-gate items into well defined cohesive clusters, an objective mea-sure is still needed to select k . In Figure 1 Hubert X  X  correlation with distance matrix is calculated for several values of k for tag and resource clusters in Delicious. When k is approximately 2500 we observe a knee in the progression. We have interpreted thi s to overheardinnewyork.com is a popular web blog. terracotta is an opensource project for computational clus ters Figure 1: Evaluation of k for resource and tag clusters in Deli-cious using Hubert X  X  correlation with distance matrix.
 Figure 2: The distribution of ambiguity in Bibsonomy. Popu-larity versus ambiguity in Bibsonomy. mean that tightly focused clusters are not broken up for larg er val-ues of k , and have consequently selected this value. In order to preserve space we do not here report all experiments for k on dif-ferent datasets and for resource and tag clusters, but we obs erve a similar trend for all experiments and a have chosen k accordingly. For Bibsonomy we have chosen 500 for k in both tag and resource clusters and in Citeulike we have chosen 750 and 500 respecti vely.
Ambiguity can cause standard utility metrics to overestima te the effectiveness of tag recommenders by rewarding ambiguous r ec-ommendations even as they contribute to noise, confound the user experience, clutter the user profile and impose additional e ffort on the user. The effectiveness of the recommenders may even be u n-derestimated when they avoid such tags, penalizing them for not suggesting the ambiguous tags found in the holdout set.

In Figure 2 we show the distribution of Ambiguity for all 1651 tags in the Bibsonomy dataset. A few tags are very ambiguous while the majority have a relatively low level of ambiguity. Figure 2 also shows the correlation between popularity and ambigui ty. A very popular tag is likely to also be ambiguous perhaps becau se it can be applied in many different contexts, though ambiguous tags are not necessarily popular demonstrating that popularity alone is not a good predictor of ambiguity. The Delicious and Citeuli ke datasets show nearly identical trends.

Table 3 lists the top ten ambiguous tags in Delicious.  X  X ook-marks X  and  X  X mported X  seem to be system tags that have been ap -plied during an import operation and thus have been applied t o sev-eral resources regardless of context. Subjective tags such as  X  X ool, X   X  X nteresting, X  and  X  X seful X  are ambiguous because differe nt peo-ple find alternative subjects interesting. Consequently, s ubjective tags can be applied to any resource cluster.  X  X ools X  and  X  X oo ls X  are ambiguous because they are overly vague, perhaps meaning ha nd tools to some and data mining tools to others. The tag  X  X oread  X  is a functional tag and is ambiguous because users mark items to read across disparate topics. Five of the top ten ambiguous t ags in Delicious also appear in the top tags for Bibsonomy. This is l ikely due to the fact that both systems allow the user to annotate UR Ls and therefore share common a domain, though Bibsonomy users Table 3: Top 10 ambiguous tags in Delicious along with their ambiguity score and frequency. also annotate scholarly articles. The most ambiguous tags i n Citeu-like appear to be overly vague tags such as  X  X odel, X   X  X heory,  X  and  X  X oftware. X  Within the scope of scientific publication thes e tags are quite indeterminate.

In order to measure the impact of ambiguoustags across the fo lk-sonomies we remove the top one, two and three percent of ambig u-ous tags from the datasets. Table 1 shows the impact of the dat asets with the ambiguous tags removed. Rarely is a user or resource completely removed through this process. Moreover the numb er of posts (a user, resource and all tags applied to that resource) is reduced marginally while the number of triples (user-resou rce-tag tuples) is dramatically reduced. This shows that while user s rou-tinely apply ambiguous tags, they rarely annotate a resourc e with ambiguous tags alone.

To ascertain the impact of ambiguity on the tag recommendati on strategies we perform the five recommendation techniques wi th the original datasets as well as datasetsgenerated by removing ambigu-ous tags. For the k -nearest neighbor recommendation strategy we tuned k on the original datasets. For Delicious we found 10 to be optimal for both KN N ur and KN N ut . In Bibsonomy we used 5 for both techniques, and in Citeulike we used 5 and 10 respect ively. For Folkrank we verified the optimal value of 0.3 for d as in [11].
We then measure the effectiveness of the tag recommendation strategies with recall and precision as shown in Figure 3. Th e left most column in the figure shows the impact of removing ambigu-ous tags on the most popular by resource strategy, pop r . Since pop r recommends tags based upon their popularity for a particula r resource, the recommendedtags tend to be quite specificin de scrib-ing the resource. The removal of ambiguous tags from the data set, therefore will either result in little changein recommenda tioneffec-tiveness or (in the case of broad folksonomies, such as Delic ious) may actually reduce the noise and result in higher recommend ation accuracy.

We observe the opposite effect with the most popular by user technique, pop u applied to the Delicious dataset. In this approach, the recommended tags (those that the user has often used acro ss resources) are much more likely to be inheretly ambiguous. A user for example that often annotates resources with  X  X oread X , c an be presumed to continue this behavior regardless of the charac teristics of the resource. Therefore, removing such ambiguoustags fr om the data may actually result in reduced precision (from the pers pective of the evaluation methodology). Bibsonomy shows similar re sults while Citeulike reveals little change. The behaviorin Cite ulike may be due to the fact that users tag a much more spefcific set of re-sources (scientific articles) based on their interest in spe cific areas. Thus, most popular tags by a user tend to be less ambiguous tha n in a broad folksonomy such as Delcious.

These two examples demonstrate the difficulty in evaluating and comparing the true effectiveness of tag recommendation str ategies. Recommendationby pop r is often contextually appropriate but am-biguous tags in the user X  X  holdout set masks the true value of this technique. On the other hand, ambiguous tags cause recall an d pre-cision to overrate the quality of pop u : the recommended tags offer little utility and does not provide the user with new or conte xtually significant options.
As would be expected, more robust recommenders such as K -nearest neighbor are more resilient to ambiguity. As shown i n Table 1 the number of resources changes very little despite the rem oval of ambiguous tags. Consequently when users are modeled as ve c-tors over the set of resources such as in KN N ur we see very little change in the performance across the reduced datasets.

However, in KN N ut the inclusion of ambiguous tags may ob-fuscate the the user model and imply false similarities amon g users. The Delicious experiment shows how ambiguous tags may have muddied the user profile. Removing the ambiguous tags from th e data, and thus the holdout set, the recommender is able to foc us on more context oriented annotations. In contrast, the Bibson omy and Citeulike experiments reveal little changein KN N ut when remov-ing ambiguous tags, likely becausethe average ambiguity of tags in a post is far less in Bibsonomy and Citeulike (2.24 and 1.91, r espec-tively) compared to Delicious (3.78). Here we see that ambig uity not only plays a role when comparing recommendation techniq ues but when comparing datasets as well.

Folkrank behaves in a similar manner. In Delicious, where am -biguity is more common, the removal of ambiguous tags makes i t easier to model users, in this case through linkage analysis . Exper-iments on Citeulike and Bibsonomy show little change.

Looking across the datasets rather than over the recommenda tion techniques reveals additional insights. In general Citeul ike exhibits little change regardless of the removal of ambiguous tags. T his is not surprising since it has the lowest average ambiguity ove r its posts (1.91), most likely because it is focused on scholarly journals and its members have an added incentive to organize their res ources with more focused tags.

Delicious members on the other hand are often casual users th at annotate a broad range of topics. Consequently it has the lar gest average ambiguity among posts (3.78). We therefore witness dra-matic changes in this dataset. Bibsonomy, a collaborative t agging system that includes both URLs and journal articles, has an a verage ambiguity over posts of 2.24, falling between the other two.
Tag redundancy may also mask the true effectiveness of the re c-ommendation algorithm: while from the user perspective a ta g may be a good recommendation, in the evaluation process it might ap-pear as ineffective if it is very similar but not identical to the hold-out tag. We can measure the impact of redundancy by comparing recommendation accuracy in the standard evaluation framew ork to the redundacy-aware framework defined in Section 3.2 .

Figure 4 shows the most popular by resource recommendation strategy, pop r , on the Delicious dataset and the Folkrank, f olk , algorithm on the Citeulike dataset. Both techniques are eva luated with standard recall and precision as well as the redundancy -aware metrics that rely on clusters of tags. In both cases we have ob served that redundant tags have been suggested, but were ignored by the standard methods.

For simplicity we define dif r to be the average difference be-tween standard recall and redundancy-aware recall across r ecom-mendation sets of size 1 through 10. In the precision case, dif is defined similarly. The results are provided in Table 4. In a ll strategies and datasets we observed a similar trend, but to d ifferent degrees. We found the smallest improvement in pop u due to the fact that recommended tags are drawn from the target user X  X  o wn profile, and because users tend to focus on a single tag from a r e-dundant cluster rather than employing several redundant va riations.
The greatest difference is observed in knn ur suggesting that many of the standard evaluation techniques may underestima te its true effectiveness from the user perspective. The other rec ommen-Figure 4: pop r on Delicious and f olk on Citeulike. Recall is reported on the x-axis, precision on the y-axis. The lower line shows recall-precision for recommendation sets of siz e 1 through 10. The upper line shows the redundancy aware ver-sions.
 Table 4: The difference in standard recall and precision and their redundancy aware counterparts. dation strategies also show a marked difference in dif r and dif suggesting that many recommenders have difficultly penetra ting the noise created by redundant tags.

We also observed differences among the redundancy of recom-mendation sets as defined in Equation 7. Table 5 shows that pop suggests far more redundant tags that the other techniques. This is likely due to the fact that it solely focuses on the resource w hose representation includes redundant variants of tags associ ated by many users to that resource. On the other hand pop u , which looks only at the user, generates little redundancy. The other tec hniques possess moderate redundancy.

The quality of a recommender may be overvalued if the inclu-sion of several redundant tags imposes additional effort on the user. Users may be required to judge the nuancesbetween similar ta gs or randomly choose one of several tags. Moreover they may be for ced to hunt for a previously used tag from a collection of synomym s or similar tags.
Ambiguity can give a false impression of success when the rec -ommended tags offer little utility. Such recommendations w hile appearing to be effective in the evaluation process may misl ead users as they search for relevant resources. Recommenders t hat avoid ambiguous tags may be penalized by standard utility me trics for not promoting such tags. More robust algorithms such as K -nearest neighbor and Folkrank weather ambiguity better tha n their simpler counterparts. However when users are modeled with t ags, ambiguity can pollute the user profile and impede the perform ance of the recommender. We have also discovered that ambiguity p lays a more significant role in folksonomies that include a broad s ubject domain. Recommendersfor Citeulike need be little concerne d with ambiguity, while those for Delicious must be wary.

Redundancy can hamper the effort to judge recommendations as well. Tags synonymous to those in the holdout set are treat ed as misses even when they serve the user X  X  need. The quality of a recommendation set may also be negatively effected by redun dancy Table 5: Average ambiguity and redundancy for five strategie s on a recommendation set of 10. when the inclusion of several redundant tags imposes additi onal effort on the user. We have discovered that our five recommend ers produce varying amounts of redundancy.

In sum, we have demonstrated that tag ambiguity and redun-dancy hinders the evaluation and utility of tag recommender s in folksonomies. Future work will explore ambiguity and redun dancy aware recommendation strategies.
This work was supported in part by the National Science Foun-dation Cyber Trust program under Grant IIS-0430303 and a gra nt from the Department of Education, Graduate Assistancein th e Area of National Need, P200A070536. [1] A. Almeida, B. Sotomayor, J. Abaitua, and D. L X pez-de [2] V. Batagelj and M. Zaver X nik. Generalized cores. Arxiv [3] S. Brin and L. Page. The anatomy of a large-scale [4] M. Brun, C. Sima, J. Hua, J. Lowey, B. Carroll, E. Suh, and [5] E. Durkheim, F. Alcan, and M. Morente. De la division du [6] J. Gemmell, A. Shepitsen, B. Mobasher, and R. Burke. [7] J. Gemmell, A. Shepitsen, B. Mobasher, and R. Burke. [8] S. Golder and B. Huberman. The Structure of Collaborativ e [9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme.
 [10] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme.
 [11] R. Jaschke, L. Marinho, A. Hotho, L. Schmidt-Thieme, an d [12] S. T. Jeff Z. Pan and E. Thomas. Reducing ambiguity in [13] M. E. I. Kipp and G. D. Campbell. Patterns and [14] S.-S. Lee and H.-S. Yong. Component based approach to [15] M. Lipczak. Tag Recommendation for Folksonomies [16] S. Lloyd. Least squares quantization in PCM. IEEE [17] L. Marinho and L. Schmidt-Thieme. Collaborative Tag [18] A. Mathes. Folksonomies-Cooperative Classification a nd [19] P. Mika. Ontologies are us: A unified model of social [20] C. Van Rijsbergen. Information Retrieval .
 [21] T. Vander Wal. Folksonomy definition and wikipedia. [22] J. Vig, S. Sen, and J. Riedl. Tagsplanations: explainin g [23] X. Wu, L. Zhang, and Y. Yu. Exploring social annotations for [24] C. Yeung, N. Gibbins, and N. Shadbolt. Tag meaning [25] L. Zhang, X. Wu, and Y. Yu. Emergent semantics from
