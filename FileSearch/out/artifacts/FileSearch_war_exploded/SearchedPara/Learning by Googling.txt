 The goal of giving a well-defined meaning to information is currently shared by endeavors such as the Semantic Web as well as by current trends within Knowledge Management. They all depend on the large-scale formalization of knowl-edge and on the availability of formal metadata about infor-mation resources. However, the question how to provide the necessary formal metadata in an effective and efficient way is still not solved to a satisfactory extent. Certainly, the most effective way to provide such metadata as well as formalized knowledge is to let humans encode them directly into the system, but this is neither efficient nor feasible. Further-more, as current social studies show, individual knowledge is often less powerful than the collective knowledge of a cer-tain community.
 As a potential way out of the knowledge acquisition bottle-neck , we present a novel methodology that acquires collec-API. In particular, we present PANKOW, a concrete in-stantiation of this methodology which is evaluated in two experiments: one with the aim of classifying novel instances with regard to an existing ontology and one with the aim of learning sub-/superconcept relations. The goal of giving a well-defined meaning to information is currently shared by different research communities. This goal is based on the assumption that, once information has a well-defined meaning, it can be (i) searched and retrieved more effectively, (ii) shared between different parties and (iii) used to derive implicit or new knowledge via certain in-ference mechanisms. This vision is shared in particular by the Semantic Web [5], by current trends within Knowledge Management [22] as well as by knowledge-based information systems in general. As has been argued in almost every work dealing with knowledge acquisition, any information system relying on background knowledge suffers from the so called knowledge acquisition bottleneck , i.e. the difficulty of encod-ing knowledge into a system in a declarative fashion. So far, it seems that the most effective  X  though certainly not most efficient  X  way of dealing with this problem is to let a group of knowledge engineers model the required world knowledge from scratch.
 Inspired by current social studies as [43], in which it is ar-gued that collective knowledge is much more powerful than individual knowledge, we present in this paper a new par-adigm of dealing with the above mentioned bottleneck. In very general terms our paradigm is based on the idea that collective knowledge is gathered as a first step and then as a second step presented to a knowledge engineer who can thus effectively and efficiently customize this collective knowledge with regard to the specific context of interest. In this model, the purpose of general knowledge is to compensate the po-tential lack of knowledge of an individual with respect to a certain topic, while the role of the individual is to filter the collective knowledge with regard to a specific context. This abstract model with the purpose of overcoming the knowledge acquisition bottleneck is for example instanti-ated by our PANKOW (Pattern-based Annotation through Knowledge on the Web) methodology [13]. PANKOW was originally conceived to support a web-page annotator in the task of assigning the instances appearing in the page to the appropriate concept in a given ontology in line with the CREAM framework [28]. In particular, PANKOW generates instances of lexico-syntactic patterns indicating a certain se-mantic or ontological relation and counts their occurrences tistical distribution of instances of these patterns then con-stitutes the collective knowledge which is taken into account by the annotator to decide with which concept to annotate the instance in the particular context. Figure 1 for example shows a dialog in which the user is presented with the top 5 suggestions from the collective knowledge about how to an-notate the instance Niger , i.e. as a river, as a country, etc. The advantage of such an approach combining collective and individual knowledge to overcome the knowledge-acquisition bottleneck seems thus obvious: even if the individual has never heard about the instance in question, together with the collective knowledge and the local context in which the instance appears, he might get a fairly accurate idea of the concept it belongs to.
 The remainder of this article is further structured as fol-lows: in Section 2 we describe PANKOW in more detail and present our lexico-syntactic pattern library. We also discuss the application of PANKOW in the annotation scenario de-scribed above as well as to learning sub-/superconcept re-lations. In section 3 we present results of an evaluation of PANKOW with respect to both tasks. Finally, before con-cluding, we discuss some related work.
 Figure 1: PANKOW within an annotation scenario (inter-active mode) PANKOW is based on the idea that certain lexico-syntactic patterns matched in texts convey a specific semantic re-lation. Pioneering research in this line was conducted by Hearst [29] who defined a collection of patterns indicating sub-/superconcept relations. An example of such a pattern used by Hearst is the following: where NP stands for a noun phrase. If such a pattern is matched in a text, according to Hearst we could derive that for all 0 &lt; i  X  n hyponym(lemma( NP i ), lemma ( NP 0 )) where lemma(NP) represents the lemma 2 of the concatena-tion of each open-class 3 word in NP. 4 For example, from the sentence  X  Such injuries as bruises, wounds and broken bones...  X  we could derive the relations: hyponym(bruise,injury), hyponym(wound,injury) and hyponym(broken bone,injury). Moreover, PANKOW also builds upon the idea that such patterns as described above can not only be matched in a corpus, but also in the World Wide Web as in [13], [17], [18] or [36].
 For this purpose, PANKOW generates pattern instances out of pattern schemes and counts the hits of these pattern in-stances on the web. For each instance or concept of interest, we thus yield the number of times it is related to other en-tities in the specific way indicated by the pattern schema, thus yielding a statistical  X  X ingerprint X  for this object with respect to a given semantic relation. In what follows, we first describe the process from a general point of view. Then, in Section 2.2 we describe the patterns we use and finally we formally define what a statistical fingerprint is and how it can be used.
From a linguistic point of view, a term t 1 is a hyponym of a term t 2 if we can say  X  a t 1 is a t 2  X . Correspondingly, t then a hypernym of t 1 .
The lemma of a word is its base or normal form, i.e. cats -cat , drove -drive , etc.
In contrast to closed-class words which belong to a class of words with a constant extension (examples are prepositions, determiners, ...), open-classes are evolving classes whose ex-tension constantly changes.
Hearst doesn X  X  explicitly talk about lemmatization, but it is clear from her examples that lemmatization should be performed.
 Figure 2: PANKOW within an annotation scenario (auto-matic mode) In this paper we slightly abstract from the process of PANKOW as described in [13]. In fact, the general process consists of three steps: Input: a set of entities (instances or concepts) to be classi-Step 1: The system iterates through the set of entities to Result 1: Set of pattern instances Result 2: the counts for each pattern instance Step 3: The system sums up the query results to a total Result: The statistical fingerprint for each entity, i.e. the The statistical fingerprint then represents the collective knowl-edge about the potential concepts an instance could belong to or about the potential superconcepts of a certain concept. Given the tasks of (i) classifying instances with regard to an ontology or (ii) finding an appropriate superconcept for a new concept, a knowledge engineer could be presented with the most relevant view of a statistical fingerprint in order to take a final decision.
 Figure 2 depicts an example of how PANKOW can be em-ployed in an annotation scenario. An important question here is how to find potential new instances in web pages. Though this is not directly the topic of this paper, a few words on this issue seem appropriate. In order to find can-didate new instances or concepts in a web page to be anno-tated, we first extract the textual content of the web pages and then run a part-of-speech tagger 5 over the page to as-sign each token its corresponding syntactic category. Then certain regular expressions defined over these tags and the corresponding tokens allow to find candidate instances or concepts. The main heuristic in finding instances consists of finding sequences of capitalized words tagged as proper nouns. Thus, in the abstract of this paper our method would find Semantic Web , Knowledge Management , World Wide Web , Google API and PANKOW as potential instances to be annotated. In order to find concepts in web pages, we in-terpret each sequence of lower case words tagged as common nouns as potential concepts. Of course, the patterns we use exploit other heuristics, but a detailed description of these is out of the scope of this paper and in general the issue of how to find candidate instances or concepts is orthogonal to the aim of the approach described in this paper.
 Now given an unknown instance or concept on a certain web page, patterns respectively indicating an instance-of or subconcept relation are instantiated for the new instance or concept and each concept in the target ontology. Finally, given the statistical fingerprint of the instance or concept, we follow a principle of disambiguation by maximal evidence thus assigning the instance or concept to that concept in the target ontology with the highest number of hits in the statistical fingerprint. Figure 2 illustrates the principle of disambiguation by maximal evidence within an annotation scenario. Instead, a user can also be involved in the process and for example asked to select a concept out of the top 5 elements of the statistical fingerprint. Figure 1 depicts an annotation scenario in which the user is asked to choose one concept out of the top-5 view of the statistical fingerprint. In the following we describe the patterns we exploit and give a corresponding example. The first four patterns have been used by Hearst to iden-tify isa -relationships between the concepts referred by two words in the text. However, they can also be used to spot instance-of -relations. In fact, in PANKOW they are used as indicating subclass as well as instance-of relations, depend-ing on whether the entity to be classified is an instance or a concept. Correspondingly, we formulate our patterns using the variable  X  &lt; I &gt;  X  to refer to the name of an instance and the variable  X  &lt; C &gt;  X  to refer to the name of a concept from the given ontology.
 The patterns reused from Hearst are: HEARST1: &lt; C &gt; s such as &lt; I | C 0 &gt; HEARST2: such &lt; C &gt; s as &lt; I | C 0 &gt;
HEARST3: &lt; C &gt; s, (especially | including) &lt; I | C
HEARST4: &lt; I | C 0 &gt; (and | or) other &lt; C &gt; s
A part-of-speech tagger assigns syntactic cate-gories to words. We use the QTag tagger in http://web.bham.ac.uk/o.mason/software/tagger/.
 Depending on whether we are attempting to classify an in-stance or a concept, we would then either derive: instance-of(I,C) or subconcept(C X ,C). The above patterns would match the following expressions: continents such as Asia (HEARST1) vehicles such as cars (HEARST1) such continents as Africa (HEARST2) such cars as cabriolets (HEARST2) presidents, especially George Washington (HEARST3) vehicles, especially motor-bikes (HEARST3) the Eiffel Tower and other sights in Paris (HEARST4) motor-bikes and other two-wheeled vehicles (HEARST4) The next patterns are about definites, i.e. noun phrases in-troduced by the definite determiner  X  the  X . Frequently, defi-nites actually refer to some entity previously mentioned in the text. In this sense, a phrase like  X  the hotel  X  does not stand for itself, but it points as a so-called anaphora to a unique hotel occurring in the preceding text. Nevertheless, it has also been shown that in common texts more than 50% of all definite expressions are non-referring [38], i.e. they exhibit sufficient descriptive content to enable the reader to uniquely determine the entity referred to from the global context. For example, the definite description  X  the Hilton hotel  X  has sufficient descriptive power to uniquely pick-out the corresponding real-world entity for most readers. One may deduce that  X  Hilton  X  is the name of the real-world entity of type Hotel to which the above expression refers. Consequently, we apply the following two patterns to cate-gorize an instance by definite expressions: DEFINITE1: the &lt; I &gt; &lt; C &gt; DEFINITE2: the &lt; C &gt; &lt; I &gt; The first and the second pattern would for example match the expressions  X  the Hilton hotel  X  and  X  the hotel Hilton  X , re-spectively. It is important to mention that these patterns are in our approach only used to categorize instances into the ontology, but not concepts. The following pattern makes use of the fact that certain en-tities appearing in a text are further described in terms of an apposition as in  X  Excelsior, a hotel in the centre of Nancy  X . The pattern capturing this intuition looks as follows: APPOSITION: &lt; I | C 0 &gt; , a &lt; C &gt; The probably most explicit way of expressing that a certain entity is an instance or a subconcept of a certain concept is by the verb  X  to be  X  in a copula 6 construction as for example in  X  The Excelsior is a nice hotel in the center of Nancy  X . Here X  X  the general pattern:
COPULA: &lt; I | C 0 &gt; is a &lt; C &gt;
A copula is an intransitive verb which links a subject to an object, an adjective or a constituent denoting a property of the subject.
 Having defined these patterns, one could match these pat-terns in a corpus and propose the corresponding relations. However, it is well known that the above patterns are rare and thus one will need a sufficiently big corpus to find a significant number of matches.
 Thus, PANKOW resorts to the biggest corpus available: the World Wide Web. In fact, several researchers have shown that using the Web as a corpus is an effective way of ad-dressing the typical data sparseness problem one encoun-ters when working with corpora (compare [26], [32], [36], [40]). Actually, we subscribe to the principal idea by Mark-ert et al. [36] of exploiting the Google TM API. As in their approach, rather than actually downloading web pages for further processing, we just take the number of web pages in which a certain pattern appears as an indicator for the strength of the pattern.
 Given a candidate entity we want to classify with regard to an existing ontology, we instantiate the above patterns with each concept from the given ontology. For each pattern documents that contain it. The function  X  X ount X  models this query. Thereby, E , C and P stand for the set of all entities to be classified, for the concepts from a given ontology and for a set of pattern schema, respectively. Thus, count( e, c, p ) returns the number of hits of pattern the pattern schema p instantiated with the entity e and the concept c . Further we define the sum over all the patterns conveying a certain relation r : where P r is the set of pattern schemes denoting a certain relation r .
 Now we formally define the statistical fingerprint of an entity e with respect to a relation r and a set of concepts C : Further, instead of considering the complete statistical fin-gerprints, we consider views of these such as defined by the following formulas. The first formula defines a view of the statistical fingerprint which only contains the concept with maximal number of hits. 7 SF max ( e, r, C ) := { ( c, n ) | c := argmax c 0  X  C count Further, we extend this to consider the top-m concepts with maximal count: SF m ( e, r, C ) := { ( c, n ) | C = { c 1 , c 2 , ..., c if m  X | C | .
 Finally, we also consider a view only taking into account those concepts having hits over a certain threshold  X  :
We assume that argmax breaks ties randomly in this con-text.

SF  X  ( e, r, C ) := { ( c, n ) | count r ( e, c )  X   X   X  n = count We can now combine these views by set operations. For example, we yield the set of the m top concepts having hits over a threshold  X  as follows: As an example of such a view, consider the visualization of the SF 6 view of the statistical fingerprint for Niger with re-gard to the instance-of relation in Figure 3. It is interesting to observe that the most prominent concept for Niger seems to be river , directly followed by country and further by state, coast, region and area . We have evaluated PANKOW with respect to two tasks: the task of finding the appropriate ontological concept for a given instance, and the task of finding sub/superconcept relations. For our instance classification experiment, we asked 2 sub-jects to annotate 30 texts with destination descriptions from http://www.lonelyplanet.com/destinations . They used a pruned version of the tourism ontology developed within the GET-ESS project [42]. We manually pruned this ontology by removing concepts which did not appear in the above pages in order to facilitate the annotation process. The original ontology consisted of 1043 concepts, while the pruned one consisted of 682. The subjects were told to annotate in-stances in the web page with the appropriate concept from the ontology. In what follows, we will refer to these subjects as A and B. Subject A actually produced 436 categoriza-tions and subject B produced 392. There were 277 proper nouns (referred to by I in the following; | I | = 277) that were annotated by both subjects. For these 277 proper nouns, they used 59 different concepts (henceforth constituting our set of concepts C ). The categorial agreement on these 277 proper nouns as measured by the Kappa statistic (cf. [11]) was 63.48%, which allows to conclude that the classifica-tion task is overall well defined. In the following, we only consider the common instances in I for our evaluation. To evaluate our approach, we compare the answers of our system with the following reference standards: Now as answers S max, X  of the system we consider the fol-lowing set: S As evaluation measures, we use the well-known P(recision), R(ecall) and F 1 -Measures to evaluate our system against Standard A and Standard B . P, R and F 1 are defined as follows (for y  X  X  A, B } , the two standards): R Furthermore, in our experiments we will always average the results for both annotators as given by the following formu-las: To get an upper bound for the task we are looking at, we also calculated the F 1 -Measure of Standard A measured against Standard B and the other way round and got F 1 =62.09% as average. This value thus represents an upper bound for any system attempting to find the correct class for an unknown instance. Table 1 shows the top 60 SF max ( i, instance-of , C ) values for different instances i . While some classifications are defi-nitely spurious, it can be seen in general that the results are quite reasonable. Figure 4 shows the precision, recall and F 1 -Measure values for different thresholds  X  within the interval [0..1000], averaged over both reference standards: Standard A and Standard B . Obviously, the precision in-creases roughly proportionally to the threshold  X  , while the recall and F 1 -Measure values decrease. It can be observed that P=R=F at  X  = 0. The best F 1 ,avg -Measure was 28.24% at a threshold of  X  = 60 and the best Recall ( R avg ) was 24.9% at a threshold of  X  = 0.
 In a second version of the experiment, instead of merely choosing the concept with maximal count with respect to the statistical fingerprint, we considered the top 5 concepts, i.e. the view SF 5 , X  = SF 5  X  SF  X  and considered the answer Instance Concept # Google TM Matches Atlantic city 1520837 Bahamas island 649166 USA country 582275 Connecticut state 302814 Caribbean sea 227279 Mediterranean sea 212284 South Africa town 178146 Canada country 176783 Guatemala city 174439 Africa region 131063 Australia country 128607 France country 125863 Germany country 124421 Easter island 96585 St Lawrence river 65095 Commonwealth state 49692 New Zealand island 40711 Adriatic sea 39726 Netherlands country 37926 St John church 34021 Belgium country 33847 San Juan island 31994 Mayotte island 31540 EU country 28035 UNESCO organization 27739 Austria group 24266 Greece island 23021 Malawi lake 21081 Israel country 19732 Perth street 17880 Luxembourg city 16393 Nigeria state 15650 St Croix river 14952 Nakuru lake 14840 Kenya country 14382 Benin city 14126 Cape Town city 13768 St Thomas church 13554 Niger river 13091 Christmas Day day 12088 Ghana country 10398 Crete island 9902 Antarctic continent 9270 Zimbabwe country 9224 Central America region 8863 Reykjavik island 8381 Greenland sea 8043 Cow town 7964 Expo area 7481 Ibiza island 6788 Albania country 6327 Honduras country 6143 Iceland country 6135 Nicaragua country 5801 Yugoslavia country 5677 El Salvador country 5154 Senegal river 5139 Mallorca island 4859 Nairobi city 4725 Cameroon country 4611 Rust park 4541 Figure 4: Precision, Recall and F 1 -Measure for S max, X  over threshold  X  (instance classification) Figure 5: Precision, Recall and F 1 -Measure and Recall for S 5 , X  over threshold  X  (instance classification) as correct if the annotator X  X  answer was in this view. The results in terms of the same measures are given in figure 5. The qualitative behaviour of the three measures is simi-lar as in the first experiment, but obviously the results are much better. The best F 1 -Measure of 51.64% was reached at a threshold of  X  = 50, corresponding to a Precision of 66.01% and a recall of 42.42%. Concluding, these results mean that in 66% of the cases the correct concept for an in-stance is among the top 5 suggestions and on the other hand for more than 40% of the relevant instances the system is able to suggest 5 concepts, one of which is the correct one. This is certainly a very satisfactory result and a good proof that using our PANKOW methodology to gather collective knowledge in form of statistical fingerprints and presenting certain views of these to a user would drastically help to reduce the time taken to annotate a given web page. As a second experiment, we attempted to reproduce the sub-/superconcept relations of a given ontology. In partic-ular, we considered the tourism ontology which was man-ually constructed by an ontology engineer in the context of the comparison study described in [35]. Furthermore, as this ontology was specified in German, we translated it into English. The ontology consisted of 289 concepts, from which we removed a few abstract concepts such as par-tially material thing , or geometric concept thus yielding 272 concepts with 225 direct is-a relations and 636 transitive (direct + non-direct) is-a relations between them. For our evaluation we take into account the set of transitive rela-tions. As in the first experiment, we evaluated PANKOW in terms of Precision, Recall and F 1 -Measure. In contrast to the above experiment, we merely compared to one reference standard, i.e. the ontology described above. The answers of the system are now defined as follows:
S The reference standard is given by the following set O : where  X  C is the partial order representing the concept hier-archy of the reference ontology.
 Now, Precision, Recall and F 1 -Measure are defined as fol-lows: Figure 6 shows the results of the sub-/superconcept extrac-tion in terms of Precision, Recall and F 1 -Measure. In this case the best F 1 -Measure was F 1 =18.25% and was reached at threshold  X  = 0, corresponding to a precision of P=21.74% and a recall of R=15.73%. This was also the overall best re-call. Thus, the results seem to be not as good as in the above experiment. This is probably due to the fact that concept labels are much more ambiguous than instance la-bels. When considering again the top 5 best suggestions of the system, the results increase as shown in figure 7. The best F 1 -Measure in this second version of the experiment was F=52.33% at  X  = 0; the precision was P=62.32% and the re-call R=45.10%. These results are also impressive and again corroborate the claim that our approach is a very promising step towards overcoming the knowledge acquisition bottle-neck. Our experiments have shown that the results of our system are within a range in which they can not be used automat-ically without any human interaction. However, we have also shown that when operating in an interactive mode in which a user is presented with the top 5 suggestions, our sys-tem performs very well obtaining F-Measures over 50% on such non-trivial classification tasks. However, we compare our approach from a quantitative point of view with systems performing the task of assigning instances to the correspond-ing concept automatically. In the computational linguistics community this task is known as  X  X amed Entity Recogni-tion and Classification X  (NERC). This task received special Figure 6: Precision, Recall and F 1 -Measure for S max, X  over threshold  X  (sub-/superconcept extraction) Figure 7: Precision, Recall and F 1 -Measure for S 5 , X  over threshold  X  (sub-/superconcept extraction) attention as a subtask within the framework of the Mes-sage Understanding Conferences (MUC) ([30]) which aimed at evaluating information extraction systems on a shared task. The named entity recognition task comprised three categories: PERSON, LOCATION and ORGANIZATION and systems typically achieved F-Measures well above 90%. However, this task is certainly much simpler than the ones in [3] where 1200 WordNet synsets are considered, [27] which consider 325 concepts, [23] taking into account 8 and [20] considering from 2-8 depending on the document in ques-tion. These systems are described in detail in the following section 4. Table 2 gives an overview of these systems, in par-ticular showing the number of classes considered, the type of text preprocessing needed as well as the recall or accu-racy on the task. It is important to mention here that as in our case the set of instances annotated by the system is equal to the instances annotated by the human subject, the recall corresponds to the accuracy results reported by the other systems. It can be concluded from the table that the performance of our system, given the number of classes considered and the fact that no text processing methods are needed, seems indeed reasonable compared to systems per-forming a related task. It is however important to emphasize that as the number of classes considered is not the same, the systems are not directly comparable. Traditionally, supervised information extraction techniques have been applied to facilitate the creation of metadata on the basis of textual input. Several learning techniques have been applied to induce extraction rules from a labeled set of training examples. Kushmerick et al. for example de-veloped a technique called Boosted Wrapper Induction [24]. Califf and Mooney [9] use ILP-based bottom-up rule induc-tion techniques, while Soderland [41] uses a top-down rule induction algorithm applying a hill-climbing approach. Re-cently, Ciravegna [15] developed a novel algorithm called LP 2 . However, due to the fact that all these systems ex-ploit regularities in the induction of extraction rules, their application for information extraction from the Web seems limited. Furthermore, the cost for using such systems re-mains extremely high as one needs to provide a considerable amount of training examples.
 Concerning the task of learning the correct class or on-tological concept for an unknown entity, there was quite a lot of related work within the framework of the above mentioned Message Understanding Conferences. However, the challenge of categorizing into 3 classes is quite modest when compared against the challenge of categorizing into 59 classes as in our approach. We thus focus on the discussion of approaches tackling a classification into a larger number of concepts such as [3], [20], [23] and [27].
 Hahn and Schnattinger [27] create a hypothesis space when encountering an unknown word in a text for each concept that the word could belong to. These initial hypothesis spaces are then iteratively refined on the basis of evidence extracted from the linguistic context the unknown word ap-pears in. In their approach, evidence is formalized in the form of quality labels attached to each hypothesis space. At the end the hypothesis space with maximal evidence with regard to the qualification calculus used is chosen as the correct ontological concept for the word in question. The re-sults of the different version of Hahn et al X  X  system (compare [27]) in terms of accuracy can be found in Table 2. Their approach is very related to ours and in fact they use sim-ilar patterns to identify instances from the text. However, the approaches cannot be directly compared. On the one hand they tackle categorization into an even larger number of concepts than we do and hence our task would be eas-ier. On the other hand they evaluate their approach under clean room conditions as they assume accurately identified syntactic and semantic relationships and an elaborate ontol-ogy structure, while our evaluation is based on very noisy real-world input  X  rendering our task harder than theirs. Alfonseca and Manandhar [3] have also addressed the prob-lem of assigning the correct ontological class to unknown words. Their system is based on the distributional hypoth-esis, i.e. that words are similar to the extent to which they share linguistic contexts. In this line, they adopt a vector-space model and exploit certain syntactic dependencies as features of the vector representing a certain word. The un-known word is then assigned to the category correspond-ing to the most similar vector. The best result measured against a reference standard (strict evaluation mode as they call it) was achieved using only verb/object dependencies as features (compare Table 2). Their results seem thus lower compared to our system, but they are also considering a much larger number of concepts, i.e. 1200.
 Fleischmann and Hovy [23] address the classification of named entities into fine-grained categories. In particular, they cat-egorize named entities denoting persons into the following 8 categories: athlete , politician/government , clergy , busi-nessperson , entertainer/ artist , lawyer , doctor/scientist , po-lice . Given this categorization task, they present an exper-iment in which they examine 5 different Machine Learning algorithms: C4.5, a feed-forward neural network, k-nearest Neighbors, a Support Vector Machine and a Naive Bayes classifier. As features for the classifiers they make use of the frequencies of certain N-grams preceding and following the instance in question as well as topic signature features which are complemented with synonymy and hypernym in-formation from WordNet. They report a best result of an accuracy of 70.4% when using the C4.5 decision tree clas-sifer. Fleischman and Hovy X  X  results are certainly very high in comparison to ours  X  and also to the ones of Hahn et al. [27] and Alfonseca et al. [3]  X  but on the other hand though they address a harder task than the MUC Named Entity Task, they are still quite away from the number of categories we consider here.
 Evans [20] derives similar statistical fingerprints as consid-ters named entities on the basis of these fingerprints as fea-tures in order to derive a class topology from the document in question. He uses a bottom-up hierarchical clustering algorithm for this purpose. His approach differs from the others discussed here in that it is totally unsupervised with-out even the set of categories being given. Thus, the entities are classified with respect to different sets of categories de-pending on the document considered. Overall, he reports 41.41% of correctly classified entities, considering from 2 to 8 classes.
 In the field of ontology learning, researchers have been us-ing on the one hand unsupervised context-based approaches. Maedche et al. [34] for example use a k-nearest neighbours approach to classify an unknown concept into an existing ontology. Caraballo [10], Faure et al. [21] as well as Bisson et al. [6] use bottom-up hierarchical clustering techniques to learn concept hierarchies. Cimiano et al. [14] present an approach based on Formal Concept Analysis and compare it to hierarchical agglomerative clustering and Bi-Section-KMeans as an instance of a partitional algorithm. The prob-lem of these approaches seems certainly that the quality of the automatically acquired ontologies seems low.
 On the other hand, there is quite a lot of work related to the use of linguistic patterns to discover certain ontological re-lations from text. Hearst X  X  [29] seminal work had the aim of discovering taxonomic relations from electronic dictionaries. The precision of the isa -relations learned is 61/106 (57.55%) when measured against WordNet as gold standard. Hearst X  X  idea has been reapplied by different researchers with either slight variations in the patterns used [31], in very specific domains [2], to acquire knowledge for anaphora resolution [37], or to discover other kinds of semantic relations such as part-of relations [12] or causation relations [25]. Instead of matching these patterns in a large text collection, some researchers have recently turned to the Web to match these patterns such as in [13], [17], [36]. Some researchers have also used the World Wide Web for question answering purposes such as in [4], [33] or [39], for discovering synonyms [44] or to avoid data sparseness problems [1; 26; 32]. Especially interesting in our context is the work in [17], which aim is to acquire instances for a given concept. In particular, Etzioni et al. present results on the task of ac-quiring instances of cities, countries, US states, films and actors. In contrast to our approach, they actually down-load the pages and match the patterns locally instead of generating patterns and counting their hits, thus creating less network traffic than with our approach. Interestingly, they also make use of a Bayesian classifier in order to decide weather an instance belongs to a certain concept or not. Re-cently, they have also considered learning new patterns by a rule induction process [19]. Though our approaches are definitely related, the aims are to some extent orthogonal. While we aim at classifying a given concept or instance, Et-zioni et al. aim at learning the extension of certain concepts for use within a search engine which  X  X nows it all X . Brin [8] presents a bootstrapping approach in which the sys-tem starts with a few patterns, and then tries to induce new patterns using the results of the application of the seed pat-terns as training dataset. This is also the general idea under-lying the Armadillo system [16], which exploits redundancy in the World Wide Web to induce such extraction rules. Before concluding this section on related work it seems im-portant to mention that any approach exploiting the Web to discover redundancies or overcome data sparseness faces inherent limits. Brewster et al. [7] for example have argued that in the Web a lot of information remains implicit in the head of web page creators, forming part of their background knowledge and never expressed in an explicit way. This in-herent problem of a non-technical nature seems difficult to overcome and gets certainly more important the more tech-nical the domain of consideration becomes. We have proposed a new methodology to overcome the knowl-edge acquisition bottleneck . The core of this methodology is a two-stage process in which first collective knowledge about certain items is collected and then presented to a knowledge engineer to be applied in a specific application context. We have also presented a concrete instantiation of this method-ology, PANKOW, in which collective knowledge is acquired by matching specific lexico-syntactic patterns in the World Wide Web, leading to the creation of so called statistical fingerprints. These are presented to the knowledge engineer in the form of certain snapshots or views to support him in the creation of metadata and knowledge. Further, we have presented an evaluation of PANKOW with respect to two tasks, one consisting in the classification of instances with regard to an existing ontology and one with the aim of finding the appropriate superconcept for a given concept. In both tasks the results are very promising, especially the ones for the interaction mode in which the knowledge engi-neer gets presented the top-5 best predictions of the system, which clearly corroborates the practical usefulness of our two-stage methodology.
 In our methodology, collective knowledge may be however dominated by a context different from the one in which the given entity to be classified appears. By always classifying entities with respect to the concept with maximal number of hits in the statistical fingerprint, we are thus actually cre-ating a bias towards senses which are predominant in the Web. In future work we will address this issue by attempt-ing to provide more context-based classifications by taking into account the similarity between the page to be anno-tated and the page in which the pattern was matched, thus hopefully increasing the accuracy of our approach. More-over, ambiguity is handled only implicitly through the fact that the statistical fingerprint contains all the concepts the entity could possibly be classified with. However, a more explicit and systematic treatment of ambiguity also taking into account the fact that the relation between words and concepts is not one-to-one is certainly desirable. Further, we will also tackle the issue of scalability. Finally, instead API, we will examine the possibility of actually download-ing the abstracts of the pages and processing them offline, thus considerably reducing network traffic.
 [1] E. Agirre, O. Ansa, E. Hovy, and D. Martinez. Enrich-[2] K. Ahmad, M. Tariq, B. Vrusias, and C. Handy. [3] E. Alfonseca and S. Manandhar. Extending a lexical [4] M. Banko, E. Brill, S. Dumais, and J. Lin. AskMSR: [5] T. Berners-Lee, J. Hendler, and O. Lassila. The Seman-[6] G. Bisson, C. Nedellec, and L. Canamero. Designing [7] C. Brewster, F. Ciravegna, and Y. Wilks. Background [8] Sergey Brin. Extracting patterns and relations from the [9] M.E. Califf and R.J. Mooney. Bottom-up relational [10] S.A. Caraballo. Automatic construction of a hypernym-[11] J. Carletta. Asessing agreement on classification [12] E. Charniak and M. Berland. Finding parts in very [13] P. Cimiano, S. Handschuh, and S. Staab. Towards the [14] P. Cimiano, A. Hotho, and S. Staab. Comparing con-[15] F. Ciravegna. Adaptive information extraction from [16] F. Ciravegna, A. Dingli, D. Guthrie, and Y. Wilks. Inte-[17] H. Cui, M.-Y. Kan, and T.-S. Chua. Unsupervised [18] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A.-M. [19] O. Etzioni, M. Cafarella, D. Downey, A-M. Popescu, [20] R. Evans. A framework for named entity recognition in [21] D. Faure and C. Nedellec. A corpus-based concep-[22] Dieter Fensel. Ontologies: A Silver Bullet for Knowl-[23] M. Fleischman and E. Hovy. Fine grained classification [24] F. Freitag and N. Kushmerick. Boosted Wrapper In-[25] R. Girju and M. Moldovan. Text mining for causal rela-[26] G. Grefenstette. The WWW as a resource for example-[27] U. Hahn and K. Schnattinger. Towards text knowledge [28] S. Handschuh and S. Staab. CREAM -Creating [29] M.A. Hearst. Automatic acquisition of hyponyms from [30] L. Hirschman and N. Chinchor. Muc-7 named entity [31] L.M. Iwanska, N. Mata, and K. Kruger. Fully automatic [32] F. Keller, M. Lapata, and O. Ourioupina. Using the [33] C. T. Kwok, O. Etzioni, and Daniel S. Weld. Scaling [34] A. Maedche, V. Pekar, and S. Staab. Ontology learn-[35] A. Maedche and S. Staab. Measuring similarity between [36] K. Markert, N. Modjeska, and M. Nissim. Using the [37] M. Poesio, T. Ishikawa, S. Schulte im Walde, and [38] M. Poesio and R. Vieira. A corpus-based investigation [39] D.R. Radev, H. Qi, Z. Zheng, S. Blair-Goldensohn, [40] P. Resnik and N. Smith. The web as a parallel corpus. [41] Stephen Soderland. Learning information extraction [42] S. Staab, C. Braun, I. Bruder, A. D  X usterh  X oft, A. Heuer, [43] J. Surowiecki. The Wisdom of Crowds: Why the Many [44] P.D. Turney. Mining the web for synonyms: PMI-IR
