 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process. H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  User profiles and alert services. Keywords Expert finding, information retrieval, expertise modeling, community-based Web information service, digital reference, information sy stems, language models There have been a growing numbe r of Web information services that bring together a network of self-declared  X  X xperts X  to answer other people X  X  questions. This starte d as digital reference services such as the Mad Scientist Network 1 , but has now become a popular part of several Web search services, including Google Answers 2 and All Experts 3 . One such service, called Wondir free, publicly available, live question and answer engine that connects people with questions to people with answers. People using such services are like a community  X  anyone can ask, anyone can answer, and everyone can share, since all of the questions and answers are public and searchable immediately. We refer to this type of servi ces as community-based question-answering (QA) services. There are hundreds of questions asked each day but some portion of them may not be answered or there may be a lag between the time when a question is asked and when it is answered. To get fast, releva nt answers, the key is getting the right question in front of the right person. The goal of our work is to investigate how the expertise of users, or  X  X xperts X , can be captured, and when combined with state-of-the-art information retrieval techniques whether the system is able to identify the group of  X  X xperts X  who are likely to provide answers to given questions. The expert finding problem has been explored in the research communities of Digital Reference [1,2] and Knowledge Management [3]. Our work is di fferent from previous efforts in that we focus on automatically finding experts in an open-domain community-based QA service, a nd the expert finding task is evaluated on large-scale, real data. http://www.madsci.org http://answers.google.com/answers/ http://www.allexperts.com http://www.wondir.com The data for this study comes from the log of the questions and answers submitted to the Wondir QA service between Oct. 2, 2002 and Feb. 15, 2005. We created a pool of 852,316 QA pairs, and derived 5 data sets each having a different requirement on the minimum number of questions each user must be associated with For example, the set D5 is all the QA pairs in the pool that are associated with users who answered at least 5 questions. For each data set, we created a test set by randomly selecting one question for each expert included in that set. The remaining questions and corresponding answers form the traini ng set. Questions in the test set are queries for experiments. The QA pairs in the training set are used to create different expert profiles. Relevance judgments are generated by taking the users who actually answered the questions in the test set. Statistics of the data sets are given in table 1. We cast the expert finding problem as an IR problem. Given a question, we define an  X  X xpert  X  as a person who has answered similar questions in the past in the system. The expertise of a person is characterized using a profile that has been derived from the previously answered questi ons. The given question can be viewed as query and the expert profiles can be viewed as documents. These profiles are ranked using language models which are representative of state-of-the-art information retrieval techniques. More specifically, the la nguage models we used in this work are: the query likelihood mode l [4], the relevance model [5], and the cluster-based language m odel [6]. People whose profiles are ranked higher are considered more likely to be experts for answering the given question. Depending on the text that is used, expert profiles can be built from: 1) all previously answered questions by a user, both question and answer texts (i.e.  X  X ll QA pairs X  in table 2); 2) all previously answered questions, question texts only (i.e.  X  X ll Qs X ); 3) one of the previously answered questions , both question and answer texts (i.e.  X  X ingle QA pair X ); Or, 4) one of the previously answered questions, question text only (i.e.  X  X  ingle Q X ). Note that by using 3) and 4) we could have multiple profiles for each expert  X  they can be viewed as different versi ons of the profile. At the time of retrieval, a language model is computed for each version, and experts are ranked based on the score of their best profile version. In all experiments, both the queri es and documents are stemmed, and stopwords are removed. Th e Mean Reciprocal Rank (MRR) measure [7] is used for evaluation. An expert is associated with a question if he/she provided an answer. The first set of experiments inve stigates how well experts can be ranked when each of the four different profile configurations is used. The query likelihood mode l is applied to produce the ranking. Results are given in table 2. We observe that, on each data set, for runs with profile s considering single questions, the performance is very similar between using QA pair and Q only. For runs with profiles considering all previously answered questions, using Qs only gives better performance than using QA pairs, with an average of 6.1% difference in MRR score. The results of using  X  X ll Qs X  are compar able to those of using  X  X ingle QA pair X  or  X  X ingle Q X . In general, performance tends to go up when the requirement on the minimum number of questions each expert should have answered in the past drops. The best-performing single run is on the D2 set with the  X  X ingle Q X  profile configuration, which has a M RR score of 0.13. The profile configuration  X  X ll Qs X  seems to give the most consistent performance across different data se ts. The next set of experiments compares the performance of diffe rent language models in ranking experts. The results are shown in table 3. All three models can rank the true answerer within rank 9 (with over 0.11 in MRR score). The performances of QL and CBDM are comparable and they are both better than that of RM. Across all data sets, the performance of all three langua ge models improves as the minimum number of questions each expert must be associated with decreases. The best performance is achieved on the D2 set. At first sight, the MRR scores ar e not as high as some of those reported in the TREC QA track. Considering the task at hand, however, we feel that the results obtained in these experiments are very reasonable, because ranking experts is very different from ranking answers in a typical QA system. For example, in the TREC QA track, there are straightforward correct answers for most test questions, and the number of correct answers to each question is typically small. In the expert finding task that we discussed in this paper, however , there is no such thing as a  X  X orrect X  expert. All we know is who actually answered a question, but not who possesses th e knowledge for that question. Therefore the relevance judgment se t that considers relevant only the true answerers of a question suffers from serious incompleteness as there are possibly many experts that possess the knowledge about a given topic but only a very small number of them actually answered the question. We started investigating a possible solution to this problem, which is to boost the relevance judgment set by exploiting hierarchical clustering methods to group experts based on their profiles. We have experimented with state-of-the-art information retrieval (IR) techniques and different ways of building profiles for finding experts in an open-domain community-based QA service. Language models have been chosen as representative of state-of-the-art IR techniques in this wo rk but other retrieval techniques can also be applied. Among the four different profile configurations, the one that cons iders all previously answered questions with question texts only seems to give the most consistent performance across diffe rent data sets. Results have shown that reasonable performance for ranking experts can be achieved when language models are combined with this type of profiles. For future work, we plan to carry out more experiments with boosted relevance judgment se t and possibly other techniques to expand expert profiles. This work was supported in part by the Center for Intelligent Information Retrieval, in part by Advanced Research and Development Activity and NSF grant #CCF-0205575, and in part by NSF grant number DUE-0226144. Table 2. Results for using th e query likelihood (QL) model to rank experts. Evaluation measure is MRR. Table 3. Results for different retrieval models. Expert profiles are  X  X ll Qs X . Evaluation measure is MRR. 
