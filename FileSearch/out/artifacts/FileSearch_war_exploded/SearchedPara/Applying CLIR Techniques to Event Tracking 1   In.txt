 well-defined queries. However, the problem becomes tougher when we need to follow the gradual evolution of events through time. This is the goal of event track-ing[1][13]; given an event-driven topic, described implicitly as one or two news sto-ries, we need to recognize which subsequent news stories describe the same evolving and changing event. Notice that, although the task resembles adaptive filtering[8], it is more difficult since the availability of human relevance feedback cannot be assumed. Cross-lingual event tracking (CLET) needs to handle tracking tasks over multiple challenge is finding the most effective way of bridging the language gap between new stories and events/topics. 
One popular and effective approach is translating multilingual test documents to a preferred language, and treating the problem as a monolingual task[3]. Whether this the volume of data to be processed.  X  If only a small number of pre-selected information sources need to be monitored for event tracking, and if those sources collectively produce a few thousands of multi-lingual documents daily, then translation of all the documents may be affordable.  X  If the information we need is scattered over many sources on the Internet, possi-bly in many languages, sometimes with restricted access, and typically buried in large volumes of irrelevant documents, then the translate-everything approach is unlikely to scale or be cost effective, given the higher computational cost of machine translation over CLIR methods. These methods require only limited translation of selected train-ing documents, and each document is treated as a bag of words in the translation. 
Using the online news on the Web as an example, there are at least 4,500 news sites online (according to Google News indexes), producing hundreds of thousands multilingual documents daily and on the fly is a very demanding proposition for cur-rent machine translation, to our knowledge. Even if this were computationally translated documents are not relevant to the user X  X  interest in event tracking. 
Based on the above concerns, we propose a new, more cost-effective approach that queries when downloading data from news sources we can: 1). Limit and focus the input to our system, and 2). Maximize the usefulness of the downloaded stories when a limit is imposed by the news sources. Many query expansion techniques have been successfully applied to improve cross-lingual information retrieval[14][9]. However, the applicability of these methods to CLET has not been studied. In this paper, we are performance. In particular, we are focusing on the English-Chinese cross-lingual tracking task, for which benchmark evaluation data and results are available[7]. 
A unique challenge specific to event tracking is adapting to the evolving event pro-file in the absence of human relevance feedback. Using unsupervised learning or pseudo-relevance feedback in an effective fashion is a crucial, as attested to by the use workshop LIMSI had the best performance on the provided benchmark dataset (de-noted by LWAdapt and described in section 2.1 and [4]). While results obtained using LWAdapt are good, we believe the method has several drawbacks outlined in reason or technique responsible for the good results. Our proposed weighted adapta-tion technique (denoted by NWAdapt and described in section 2.2) is greatly simpli-formance advantage NWAdapt has over fixed weight adaptation and LWAdapt. world setting. The user might be interested in recent events and is willing to provide one or two stories about that event, but not constant online feedback. Example events number (such as 4) of previously identified past stories that define the event. Note that relevance feedback through the entire process of selecting test documents. 
Our tracking system is an improved version of those described in[13][11]. We ap-proach the tracking problem as a supervised learning problem, starting with the stan-dard Rocchio formula for text classification: where ) , (  X  D c examples), and R D S positive centroid. and the centroid: A binary decision is obtai ned by thresholding on r . 
Unlike text classification where training data is more abundant, for event tracking we have to rely on extremely limited training examples. Naturally, the class prototype trained from the small initial training set is not very accurate. In particular, it cannot main issues that the adaptation mechanism needs to address: 1. Deciding whether a story is on topic and should be added to the centroid. 2. Choosing a method for adjusting the centroid once a story has been identified as 
We use pseudo-relevance feedback as a solution to (1): the story ( d ) is added to the centroid ( C ) as long as it has a score S(d,C) that is higher than an adaptation threshold th . Adapting this threshold is an interesting problem; however, in this paper we are focusing on (2). 
To address question (2), we define the new centroid to be: where c  X  used to adapt the centroid;  X  is the weight given to the vector adaptation). However, intuitively, different stories should have different weights; clear, however, what these weights should be. By addressing this problem, LIMSI had the best tracking system on TDT2001 benchmark evaluation. We briefly describe their weighted adaptation method in the next section, followed by our approach. 2.1 LWAdapt: LIMSI X  X  Weighted Adaptation LIMSI developed a novel approach to compute the variable adaptation weight. The similarity between a story and a topic is the normalized log likelihood ratio between the topic model and a general English model[4]. The similarity score S(d,C) is good performance of the LIMSI X  X  system in TDT 2001. However, it has several prob-lems: 1. A large amount of retrospective data is needed to ge t a reliable probabilistic mapping function is applied to. 
Recall that news-story events are typically short-lasting, and a retrospective collec-performing adaptation mechanism that does not suffer from these drawbacks. 2.2 NWAdapt: Normalized Weighted Adaptation In this section, we outline a new weight ed adaptation algorithm (NWAdapt), which is simpler and more effective than LIMSI X  X  approach. The basic idea of our approach is to directly use the cosine similarity scores generated by the tracking system (see for-mula (2)) as the adaptation weight s. The cosine score reflects the similarity between each new and the prototype (centroid) of a topic/event. To ensure that the weights are non-negative, we rescale the cosine scores linearly as follows: makes the weights more intuitive since they now fall in the more familiar probability score is higher than the threshold, the new document is used to update the topic proto-type (formula (3)). 
While this approach may appear to be si mplistic, several reasons make it worth in-vestigating: 1. It generalizes LIMSI X  X  approach (i.e., using system-generated confi-dence scores in PRF adaptation) by examining another kind of confidence scores  X  the cosine similarity. 2. If it works well, it will provide a strong baseline for future inves-tigations on weighted adaptation methods because cosine similarity is simple, easy to implement, and well-understood in IR research and applications. scores of documents with respect to each particular topic, not averaged over topics. 
As Section 3 shows, the simpler method has the advantage of being slightly more effective, in addition to avoiding LWAdapt X  X  drawbacks. 2.3 Cross-Lingual Components Our cross-lingual event tracking approach involves translating a few sample training documents instead of the large test data set. Therefore, the cross-lingual component is an integral part of our tracking system instead of a preprocessing step. The tracking process can be divided into several steps: 1. Topic Expansion (PRF, optional) 2. Sam-pling: Choosing training stories to translate. 3. Sample translations: a. using a diction-ary (DICT). b. using the CL-PRF technique (below). 4. Segmentation (for Chinese): a. Phrase-based. b. Bigram-based. 5. Ad aptation (described in section 2.1 and 2.2). Pseudo-relevance Feedback. Pseudo-relevance feedback (PRF) is a mechanism for query (or, in our case, topic) expansion. Originally developed for monolingual re-trieval, it uses the initial query to retrieve a few top ranking documents, assumes those documents to be relevant (i.e.,  X  X seudo-relevant X ), and then uses them to expand the original query. Let q sion, d documents. The new query is defined as: 
The adaptation of PRF to cross-lingual retrieval (CL-PRF) is to find the top-form the corresponding query in the target language[12]. Let q  X  query in the target language, d language corresponding document; the updated query/topic is defined to be as follow: In our experiments, the positive examples for each topic are the queries. Sampling Strategy. Our sampling strategy is simply using temporal proximity to the translate a story or not. An average of 120 stories per topic are translated, as opposed to a potentially infinite number of stories to translate with the conve ntional approach. These training stories are the only ones used as training data. 
This sampling strategy was chosen due to its mix of convenience, speed and poten-containing the on-topic examples as the sample to translate; therefore, the size of the analyzing each story to decide whether to translate it or not can approach the transla-tion cost itself, thereby defeating the purpose of sampling. The potentia l effectiveness comes from the fact that these files can be richer in positive examples and borderline effectiveness of the approach when compared to another sampling strategy remains to translation itself. 
Note that our approach is flexible: the temporal proximity window can be ex-panded to allow more stories to be translated, if time allows. Tuning this parameter is this approach to dealing with many data sources, since the fact that some sources are faster/more prolific than others needs to be taken into account. Segmentation. Segmentation is particularly problematic when translation is involved: BBN X  X  research shows that among the total 25% words which cannot be translated from Chinese into English, 5% result from a segmentation error[3]. In our research, mentation errors and unrecognized named entities. 
In our experiments, we used both phrase-based segmentation and bigram-based segmentation to separate the terms. For ph rase-based segmentation we reconstructed a phrase dictionary from segmented Chinese data provided by the LDC. To segment our Chinese text, we used a longest string matching algorithm. For bigram-based segmentation, we simply used all two consecutive Chinese characters as tokens. Sec-tion 3.5.1 compares the tracking effectiveness of these two alternatives. This section presents our experimental setup and results. Section 3.1 and 3.2 present the data and performance measures used; S ection 3.3 discusses previously published results. The following describe our experiments, which can be grouped as follows: 1. Mixed language event tracking: Here, the topics as well as the stories are either in English, or translated into English from Mandarin by SYSTRAN. We present these recent TDT benchmark evaluations. 2. CLET based on test document translation: This is similar to (1) in that it uses the English stories and it establishes a baseline for (3) under the same evaluation condi-tions. 3. CLET based on translating a few training stories per event: This is the approach we promote in this paper. 3.1 Tools and Data We chose the TDT-3 corpus as our experimental corpus in order to make our results comparable to results published in TDT evaluations[6]. The corpus includes both English and Mandarin news stories. SYSTRAN translations for all Mandarin stories are also provided. Details are as follows: 1. Mixed language event tracking: We used the TE=mul,eng evaluation condition in TDT 2001[7]. This uses the newest publicly available human judgments and allows us to compare our results with benchmark results released by NIST. 2. CLET based on test document translation: We used the TE=man,eng evaluation condition in TDT 1999 (topics are English, news stories are in Mandarin with a SYSTRAN translation provided by NIST). We also provided a dictionary translation using a 111K entries English-Chinese wordlist provided by LDC. In order to compare the results with (3), we use the same experimental conditions. 3. CLET based on translating sampled training stories per event: We used the TE=man,nat evaluation condition in TDT 1999 (topics are in English, documents are Mandarin native stories). In order to mode l the real test setting, we kept all the Man-darin native data in TDT3 as a test set. For the training phase, we used sampled trans-lated English documents, including the 4 positive examples. There are 59 events. For expansion phase we used the first six months of 1998 TDT3 English news stories. For provided by LDC . For Chinese phrase segmentation we reconstructed a phrase dic-tionary from segmented Chinese data provided by the LDC. 3.2 Evaluation Measures event tracking used in TDT benchmark evaluations[7]. Each story is assigned a label of YES/NO for each of the topics. If the system assigns a YES to a story labeled NO beled YES, it commits a miss error. The performance measures ( costs ) are defined as: C ( ratio of the number of miss errors to the number of the YES stories in the stream; P fa is the ratio of the number of false alarm errors to the total number of NO stories. The normalized cost minimum of two trivial systems (Simply assigns  X  X es X  labels or  X  X o X  labels without examining the stories). To compare costs between two tracking approaches, we used the Cost Reduction Ratio (  X  ): where the cost reduction ratio by using approach2 instead of approach1. 
We also use the Detection-Error Tradeoff (DET) curve[5] to show how the thresh-old would affect the trade-off between the miss and false alarm rates. 3.3 Mixed Language Event Tracking Results In the mixed language event tracking task, LIMSI was the best in the benchmark evaluation in TDT2001. The cost using Nt=4 (4 positive instances per topic) is C ) ( = 0.1415, as released by NIST. In order to compare our new weighted and normalized adaptation (NWAdapt) with LIMSI X  X  weighted adaptation (LWAdapt) and our old adaptation approach (FWA-dapt), we implemented LIMSI X  X  approach in our system. We trained LIMSI X  X  confi-dence transformation and all the parameters on TDT1999 dry-run conditions and applied the adaptation weight and parameters for the TDT 2001 task. 
Table 1 shows four results: our system performance without adaptation, our system performance with FWAdapt, with LWAdapt and with NWAdapt. Note that our LWA-dapt implementation performed as well as the results reported by NIST. NWAapt reduced the cost more than FWAdapt and LMAdapt when compared to the no adapta-tion alternative. 3.4 CLET Using Test Document Translation We experimented with the conventional approach to CLET (test document transla-tion) by using both SYSTRAN translations provided by LDC and dictionary transla-tion after topic expansion. For dictionary translation, we used a simple bilingual dic-performs worse than the SYSTRAN translation (0.1336 vs. 0.1745), but this experi-ment is useful in order to provide a fair comparison with topic translation, which uses the same dictionary. We opted for using a dictionary in our experiments because SYSTRAN is a more costly solution that is also less likely to be available for an arbi-trary language. Additionally, while SYSTRAN is better than a dictionary on news stories, this is not necessarily true in a domain with technical vocabulary. 
Seg-menta-tion Phrase No No Phrase Bigram No No Bigram Phrase No FWAdapt Phrase+ Phrase No LWAdapt Phrase+ Phrase No NWAdapt Phrase+ Phrase Yes No Phrase+TE Bigram Yes No Bigram+TE 
Phrase Yes FWAdapt Phrase+TE 
Phrase Yes LWAdapt Phrase+TE 
Phrase Yes NWAdapt Phrase+TE 3.5 CLET Using Training Sample Translation We explored both DICT and CL-PRF as CLIR methods targeted towards bridging the language gap. CL-PRF performed only slightly better than DICT, probably because of Table 2 summarizes the different parameter values and their label subsequently used in the result. Refer to Section 2.3 for the detailed description of each factor. Topic Expansion and Segmentation. Table 3 compares the different approaches to topic expansion and Chinese segmentatio n. As expected, topic expansion does im-prove the cost significantly (  X  =41%). 
Additionally, using bigrams as linguistic units significantly improves performance over using phrase segmentation. Due to the overlapping nature of bigram segmenta-tion, the segmented text contains more information but also more noise when com-pared to phrase segmentation. This creates an effect similar to query expansion and is more likely to contain the  X  X rue X  meaning unit. Our experiments show that, in spite of the added noise, the added information improves the tracking performance. Using both topic expansion and bigram segmenta tion yields a 50% relative cost reduction. Adaptation Approaches for CLET. In Section 2, we mentioned that adaptation is a useful approach to improve the tracking system performance. Here, we compare the (FWAdapt), LIMSI weighted adaptation (LWAdapt) and our simplified normalized weighted adaptation (NWAdapt). For fixed adaptation, we chose the best result obtained. We trained LIMSI X  s mapping approach on the TDT2 data set. Table 4 shows that, while fixed weight adaptation leads to an insignificant improvement over no adaptation, LWAdapt and NWAdapt do significantly better, with our simplified approach being slightly better. 
As expected, combining all three favorable approaches yields the best result so far: the cost is 0.2413, with a 52.6% cost reduction with respect to the baseline. Our sim-plified adaptation performed better than LWAdapt in all parameter combinations. Translating Test Documents vs. Sampled Training Documents. Using SYSTRAN to translate the testing documents and performing the equivalent of monolingual event tracking is the best approach with regard to minimizing tracking cost. However, while avoiding the expense of translating all stories. The approach we have proposed is to translate small training documents per topic, using temporal proximity to positive instances as a sampling criterion.
 system. Topic expansion was also used when translating the test set, to facilitate a fair comparison. The tracking cost is reduced by 27% when the entire test set is translated tively low volume, with few languages represented and comes from a source that does the amount of data, its linguistic variety, or its access limitations makes translating all documents impossible or difficult, the increased cost is an acceptable tradeoff. In this paper we have proposed a more practical approach to cross-lingual event track-ing (CLET): translating a small sample of the training documents, instead of translat-ing the entire test set. The latter approach could be prohibitively expensive when the  X  X est set X  is expanded to include the entire Web. In order to implement this approach, we have examined the applicability and performance of several cross-lingual informa-tion retrieval techniques to CLET. In addition, we have presented a significantly sim-plified event tracking adaptati on strategy, which is more reliable and better perform-ing than its previously introduced counterpart. 
Overall, these strategies (in particular pre-translation topic expansion and bigram segmentation) reduce the cross-lingual tracking cost by more than 50% when com-expansion has been repeatedly shown to improve cross-lingual information re-trieval[10], but has not been previously used as a translation aid in true cross-lingual way of bridging the language gap in true cross-lingual event tracking. In this work we focused mostly on expanding topics by using pseudo-relevance feed-back and bigram segmentation. In our future work we plan to concentrate on improv-ing the translation accuracy. Potential methods include better machine translation techniques, named entity tracking, and investigating various sampling strategies. 
