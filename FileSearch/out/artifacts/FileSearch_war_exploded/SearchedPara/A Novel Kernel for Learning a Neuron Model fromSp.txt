 Neurons are the predominant component of the nervous system and understanding them is a ma-jor challenge in modern neuroscience research [1]. Many neuron models have been proposed to understand the dynamics of individual and populations of neurons. Although these models vary in complexity, at a fundamental level they are mechanisms which transform input spike trains into an output spike train. This view has found expression in the Quantitative Single-Neuron Modeling competition where submitted models compete on how accurately they can predict the output spike train of a biological neuron given an input current [2]. Since the vast majority of neurons receive input from chemical synapses [3], a stricter stipulation would be to predict output spikes based on input spike trains at the various synapses of the neuron. There are advantages to this variation of the problem: complicated subthreshold fluctuations in the membrane potential need not be modeled, since models are now judged strictly on the basis of their performance at predicting the timing of output spikes. Models now have the liberty to focus on threshold crossings at the expense of be-ing inaccurate in the subthreshold regime. Not only does the model better represent the functional complexity of the input/output transformation of a neuron, comparisons to the real neuron can be conducted in a non-invasive manner.
 In this paper we learn a Spike Response Model 0 (SRM 0 )[4] approximation of a neuron by only considering the timing of all afferent (incoming) and efferent (outgoing) spikes of the neuron over a bounded past. We begin by formulating the problem in a classification based supervised learning framework where spike train data is labeled according to whether the neuron is about to spike, or has recently spiked. We demonstrate that optimizing the model to properly classify this labeled data naturally leads to a quadratic programming problem when combined with an appropriate represen-tation of the model via a dictionary of functions. We then derive a novel kernel on spike trains which is computed from a dictionary of post-synaptic potential (PSP) and after-hyperpolarizing potential (AHP) like functions. Finally, experimental results are presented to demonstrate the efficacy of the approach. For a complementary approach to learning a neuron model from spike train data, see [5]. An SRM 0 model was chosen for several reasons. First, SRM 0 has been shown to be fairly versatile and accurate at modeling biological neurons [6]. Second, SRM 0 is a relatively simple neuron model, and therefore is likely to display better generalizability on unseen input. Finally, the disparity be-tween the learned neuron model and the actual neuron could shed light on the various operational modes of biological neurons. It is conceivable that the learned SRM 0 model accurately predicts the behavior of the neuron a majority of the time. However, there could be states, bursting for example, where the prediction diverges. In such a case, the neuron can be seen as operating in two differ-ent modes, one SRM 0 like, and the other not. Multiple models could then be learned to model the neuron in its various operational modes. It has been shown, that if one assumes a neuron to be a finite precision device with fading memory and a refractory period, then the membrane potential of the neuron, P , can be modeled as a function of the timing of the neuron X  X  afferent and efferent spikes which have occurred within a bounded past [7]. Spikes that have aged past this bound, denoted by  X  , are considered to have a negligible effect on the present value of P . We denote the arrival times of spikes at synapse j using the vector t in an  X  window of time. t 0 represents the output spike train of the neuron and vectors t 1 ... t m represent spike trains on the input synapses. t j i represents the time that has elapsed since that spike was generated or received by the neuron. Spikes are only considered if they occurred within  X  time. We can then formalize the membrane potential function P : R N  X  R , where N = P m j =0 N j . of the neuron. The neuron generates a spike when P ( t 0 ,..., t m ) =  X  and dP/dt  X  0 , where  X  is the threshold of the neuron. For notational simplicity, we define the spike configuration, s  X  R N , which represents the timing of all afferent and efferent spikes within the window of length  X  . s is the vector of vectors, s =  X  t 0 ,..., t m  X  . The neuron generates a spike when P ( s ) =  X  , dP/dt  X  0 . As discussed in Section 1, we shall learn an SRM 0 approximation of the neuron. The SRM 0 model uses a bounded past history as described above to calculate the present membrane potential of the neuron. The present membrane potential  X  P is calculated as shown in Equation 1.  X  models the effect of a past generated spike, the AHP. j represents the response of the neuron to a presynaptic spike at synapse j , the PSP. u rest is the resting membrane potential. At any given time, the neuron generates a spike if the membrane potential crosses the threshold from below (i.e.,  X  P ( s ) =  X  , d  X  P/dt  X  0 ). In order to learn an SRM 0 approximation of a neuron in a non-invasive manner, we pose a super-vised learning classification problem which labels the given spike train data according to whether the neuron is about to spike or has recently spiked. We denote the former S  X  and the latter S + . This old spike configurations (  X  P ( s )  X   X  ), which leads to the classification problem shown in Equation 2. It should be noted that the true membrane potential function, P , is a feasible solution to this problem since P ( s ) &lt;  X   X  s  X  X   X  and P ( s )  X   X   X  s  X  X  + .
 To generate training data which belong to S + and S  X  , we provide the spike configurations which illustrated in Figure 1(a). The spike train at the instant the neuron generated a spike is shown by the solid lines. We shift the spike window infinitesimally into the past (future) to produce a spike configuration s  X  X   X  ( S + ) , shown by the up (down) arrows. Notice that the spike which is currently generated in the output spike train, t 0 , emphasized by the dashed circle, is not included in either spike configuration s . The reason it is not included in s  X  X   X  is that it simply has not been generated at that point in time. The reason it is not included in s  X  X  + is twofold. First, the spike would induce an AHP effect which would cause the membrane potential to fall below the threshold. Second, if it were included, this would cause the classifier to only consider whether or not that particular spike existed when classifying a given spike configuration as a member of S + or S  X  . If it did exist, it would belong to S + , and if it did not exist it would belong to S  X  . Although this method would work well for the training data, it would not generalize to unseen live spike train data. Figure 1: Figure (a) depicts the spike configurations used in the classification problem. Figure (b) shows the REEF for a fixed value of t = 1 s and variable  X  and  X  values. Figure (c) portrays the form of cross sections of the REEF as a function of t for different values of  X  and  X  . Producing a hypersurface which can separate the supra-threshold spike configurations from the sub-As discussed above, if we could map a given spike configuration s to its corresponding membrane potential P ( s ) , then the classification problem is trivial. Although we do not have access to the membrane potential function, we can use a linear combination of functions from a dictionary to reproduce an approximation to the membrane potential function P . The choice of the dictionary is crucial. By choosing a dictionary which is tailored to the form of typical PSP and AHP functions, we increase the likelihood of successfully modeling the given neuron.
 The SRM 0 model is an additively separable model [8], that is, the membrane potential is a sum feature lends itself well to modeling the membrane potential using a linear combination of dictionary elements. The dictionary used here was one derived from a function used by MacGregor and Lewis for neuron modeling [9]. It consists of functions (parametrized by  X  and  X  ) of the form We call this the reciprocal exponential  X  exponential function (REEF) dictionary. Figures1(b) and (c) present the dictionary for various cross sections of t ,  X  and  X  . We would like to combine members of the chosen dictionary of functions to construct an approxima-tion of the membrane potential function, P , which will yield a solution to the classification problem posed in Equation 2. We shall first discuss how this can be achieved in a discrete setting, where we combine a finite number of  X  and  X  parametrized dictionary functions to model P . Following this we will discuss a continuous formulation, in which we combine elements drawn from an infinite continuous range of  X  and  X  parametrized dictionary functions to model P . In the context of the continuous formulation, we will prove a specific instance of the Representer theorem which was first shown by Kimeldorf and Wahba [10]. The Representer theorem shows that the optimal solution to the posed classification problem must lie in the span of the data points which were used to train the classifier. In the discrete and continuous formulation, we will first model the effect of a single spike for simplicity. We will conclude this section by extending the continuous formulation to the case of multiple spikes on a single synapse, and the case of multiple spikes on multiple synapses. 4.1 Discrete Formulation In the discrete formulation, we wish to approximate the membrane potential function using a linear combination of a finite, predefined set of functions from the REEF dictionary. Focusing on the single spike case, our goal is to model the effect of a single spike on the membrane potential. We parametrized REEF functions as shown in Equation 4. f t (  X , X  ) = 1  X   X  exp(  X   X /t )  X  exp(  X  t/ X  ) is now a univariate function over t for fixed values of  X  and  X  . A specific set of parameter settings the effect of the spike on the membrane potential. Inserting Equation 4 into Equation 2 yields a quadratic optimization problem on the mixing coefficients  X  i,j  X  X . The major disadvantage of the discrete formulation is that for any given neuron, the optimal value set of the  X   X  X  and  X   X  X  is unlikely to be known beforehand. While one can argue that the approximation  X  P can be improved by increasing M and N , as the number of functions increases, so does the dimensionality of the feature space. Since M and N can be increased independent of the size of the training dataset, the procedure is susceptible to over-fitting. To resolve this issue, we shift to a continuous formulation of the problem, which by virtue of the Representer theorem does not suffer from the rising feature space dimensionality issue. The dimensionality of the feature space is now controlled by the span of the training dataset. 4.2 Continuous formulation In the continuous formulation, we consider L 2 , the Hilbert space of square integrable functions on the domain {  X , X  }  X  [0 ,  X  ) 2 . We are concerned with finding a threshold dependent classification function  X  P , such that  X  P ( t )  X   X  + 1 when the spike t  X  X  + and  X  P ( t )  X   X   X  1 when t  X  X   X  . This function is defined in Equation 5. In this formulation, the mixing function ,  X  (  X , X  ) , is by definition a member of L 2 . Therefore, if f the domain variables in f t (  X , X  ) and  X  (  X , X  ) and refer to them as f t and  X  . 4.2.1 Proof Therefore  X  f t ,f t  X  = t  X  t ( t + t ) 2 = 1 4 &lt;  X  X  X  t  X  [ ,  X  ) for some &gt; 0 . We must note here that by defining the membrane potential function in this manner, we have formu-lated a problem which yields a solution which is different from the solution to the discrete problem. Since the delta function centered at any arbitrary point (  X   X  , X   X  ) does not belong to L 2 , the mixing function  X  cannot be made up of a linear combination of these delta functions, as is the case in the discrete formulation. In addition, we are not working with a reproducing kernel Hilbert space since we are considering L 2 . However, our definition in Equation 5 defines the  X  X oint evaluation X  of our membrane potential function.
 Since  X  P ( t ) is defined using the standard inner product in L 2 with respect to particular members of L 2 , we can reformulate the classification problem in Equation 2 as shown in Equation 8. Here M is the number of data points, m = 1 ...M , and y m is the corresponding classification for spike time t m (that is, y m = +1 if t m  X  X  + and y m =  X  1 if t m  X  X   X  ).
 We can now use a specific instance of the Representer theorem [10] to show that the optimal solution for  X  to the optimization problem specified in Equation 8 can be expressed as  X  = P M k =1  X  k f t k . optimization problem, which is a standard quadratic programming problem. 4.2.2 Representer Theorem For some  X  1 , X  2 ,... X  M  X  R , the solution to Equation 8 can be written in the form Proof We consider the subspace of L 2 spanned by the REEF functions evaluated at the times of the given training data points (span { f t k : 1  X  k  X  M } ). We then consider the projection  X  k of  X  on this subspace. By noting  X  =  X  k +  X   X  and rewriting Equation 8 in its Lagrangian form, we are left with Equation 10. However, by the definition of  X   X  ,  X   X   X  ,f t k  X  = 0 , which then simplifies the summation term of Equation 10 to only depend upon  X  k as shown in Equation 11.
 In addition, by considering the relation shown in Equation 12, we find that the first term is minimized when  X  =  X  k . Hence, the optimal solution to Equation 8 will lie in the aforementioned subspace and therefore have the form of Equation 9. 4.2.3 Dual Representation We can now substitute the form of the optimal solution shown in Equation 9 back into the original equivalent to Equation 14. The resultant quadratic programming problem is solvable given that we have access to the positive definite matrix K , which was derived in Section 4.2.1 and is shown in Equation 15.
 4.3 Single Synapse We are now in a position to extend the framework to multiple spikes on a single synapse. Since additively separable [8] and that each spike X  X  effect on the membrane potential for the given synapse is identical . Introducing the latter assumption is the core contribution of this section. We first define the threshold dependent classification function for a single spike in a manner identical to that of the single spike formulation shown in Equation 5. This will be the  X  X tereotyped X  effect that a spike arriving at this synapse has on the membrane potential. Note that the AHP effect of the output spike train can be modeled seamlessly (as a virtual synapse) in this framework. 4.3.1 Primal Problem We now consider the additive effects of multiple spikes arriving at a synapse. We define the vector t m =  X  t m their spike times. Note that we have abused notation. Instead of the superscript repeatedly referring to the synapse in question, it now refers to the data point. The primal optimization problem, defined in Equation 16, is equivalent to Equation 17.
 The Representer theorem states that the optimal  X  must lie in span { P N k i =1 f t k omit the formal proof since it follows along the lines of the previous case. Therefore, the optimal  X  to Equation 17 will be of the form 4.3.2 Dual Problem Substituting back Equation 18 yields the dual problem Equation 19, which can be solved given the positive definite kernel in Equation 20.
 Min.
 4.4 Multiple Synapses exception that spikes arriving at different synapses could have different effects on the membrane potential, depending on the strength/type of the synaptic junction. Therefore, we keep the effects of each synapse on the membrane potential separate by assigning each synapse its own  X  function. 4.4.1 Primal Problem Since each synapse and the output has its own  X  function, this simply adds another summation term over the S synapses and the output (indexed by 0). The primal optimization problem is defined in Equation 21 which is equivalent to Equation 22. S is the number of synapses, N m,s is the number synapse of the m th data point.
 The Representer theorem states that the optimal  X  s for the s th synapses must lie in and therefore, the optimal  X  s to Equation 22 will be of the form 4.4.2 Dual Problem Substituting Equation 23 into Equation 22 yields the dual problem shown in Equation 24 which can be solved given access to the positive definite kernel defined in Equation 25.
 4.5 Summary With the above kernels we are able to formulate quadratic programming problems which can be solved with SVM light [11]. The choice of the dictionary used to derive the kernel is critical to the success of this technique. A dictionary of functions tailored to the forms of PSPs and AHPs will perform better than a more general class of functions. The properties of the REEF dictionary which make it suitable for this problem are its exponential decay as well as its additive separability [8]. This explains why a Gaussian radial basis function (GRBF) does not work well for this problem. The GRBF kernel is not additive. A slight variation of the GRBF which takes the sum of Gaussian however it did not perform well when applied to more complicated neurons. To test the kernel we learned SRM 0 neurons with increasing levels of complexity. We first con-sidered a simplistic neuron which only received spikes on a single synapse. We then increased the complexity of the neuron, by introducing AHP effects as well as different types (excitatory and in-hibitory) of afferent synapses with varying synaptic weights. The PSP effect was modeled via the classical alpha function [PSP ( t ) = C  X  t  X  exp(  X  t/ X  ) ] while the AHP effect was modeled by an exponential function[AHP ( t ) = K  X  exp(  X  t/ X  ) ]. Although we learned neurons with varying com-plexity, for want of space, we discuss here the case of a single neuron that received input spike trains from 4 excitatory synapses and 1 inhibitory synapse to mimic the ratio of connections observed in the cortex [12]. The stereotyped PSP for the excitatory and inhibitory synapses differed in their rise and fall times. The parameters for the stereotyped PSP were set as follows. For the excitatory PSP, C = 0 . 1 and  X  = 10 , where t is in units of milliseconds. For the inhibitory PSP, C =  X  0 . 39 and  X  = 5 . For the AHP, K =  X  16 . 667 and  X  = 2 .
 We first trained the classifier using 100,000 seconds of spike train data. Only the spike configurations occurring at fixed differentials before and after the neuron emitted a spike were considered. The in-put spike trains were generated using an inhomogeneous Poisson process, where the rate was varied sinusoidally around the intended mean spike rate in order to produce a more general set of training data. This resulted in 1,647,249 training data points, however only 10,681 of them were used in the solution as support vectors. After training, we tested our model using 100 seconds of unseen data. All spike configurations were considered when testing, regardless of temporal proximity to Figure 2: Figure (a) shows histograms of the difference in time between the actual and predicted spike time by the learned model. Figure (b) shows the various PSP approximations (gray) in com-parison to the PSP functions used by the neuron (black). Figure (c) depicts the AHP approximation (gray) and the AHP function used by the neuron (black).
 They were 0 . 9947 , 0 . 9532 and 0 . 9948 respectively. We also calculated a histogram of how close the spike predictions were. For every spike produced by the neuron, we determined the temporal proximity of the closest spike time predicted by the model. We then histogrammed this data. Figure 2(a) shows two histograms depicting these calculations. The larger histogram contains predictions with time differences varying between 0 and 70 ms, with a bin size of 1 ms while the inlaid his-togram ranges from 0 to 10 ms and has a bin size of 0.1 ms. Both use a logarithmic scale on the y-axis. From the histograms, we see that the vast majority of spikes were predicted correctly (with a temporal proximity of 0 ms) and that out of the mispredicted spike times, the temporal proximity of all predicted spikes fell within 70 ms of the actual spike time.
 In Figures 2(b) and 2(c) we display a comparison of the approximated PSP and AHP versus the true PSP and AHP. To calculate the classification model X  X  approximated PSP we artificially send a single spike across each input synapse. We artificially generate a spike to produce the AHP approximation. By considering the distance of the single spike data point from the classifier X  X  margin as the spike ages, we can get a scaled and translated version of the PSP and AHP. The figures show these ap-proximations scaled and translated back appropriately. In Figure 2(b) we show the approximations of the PSPs for the input synapses. The approximations are shown in gray; the true PSPs are shown in black. The different line styles are representative of the different synapses and therefore have varying synaptic weights. A similar image for the AHP is shown in Figure 2(c). We note that there are small differences between the approximated and the true functions. If the PSP and AHP ap-proximations were exact, we would have seen perfect classification results. However, as with most machine learning techniques, the quality of the solution is limited by the training data given. In this paper we have developed a classification framework which uses a novel kernel derived from a REEF dictionary to produce an SRM 0 approximation of a neuron. The technique used is non-invasive in the sense that it only requires the timing of afferent and efferent spikes within a certain bounded past. The REEF dictionary was chosen due to its similarity to PSP and AHP functions used in a neuron model proposed by MacGregor and Lewis [9].
 By producing an SRM 0 approximation, which is additively separable [8], we produce a model which increased generalizability to unseen input. The simplicity of the SRM 0 model has the potential to allow us to observe deviations between the model and the neuron, which can lead to insights on the various behavioral modes of neurons.
 Acknowledgments This work was supported by a National Science Foundation grant (NSF IIS-0902230) to A.B. References [1] R. Jolivet, A. Roth, F. Sch  X  urmann, W. Gerstner, and W. Senn. Special issue on quantitative [2] W. Gerstner and R. Naud. How Good Are Neuron Models? Science , 326(5951):379 X 380, [3] W. Gerstner and W. Kistler. Spiking Neuron Models: An Introduction . Cambridge University [4] R. Jolivet, T.J. Lewis, and W. Gerstner. The spike response model: a framework to pre-[5] L. Paninski, J.W. Pillow, and E.P. Simoncelli. Maximum likelihood estimation of a stochastic [6] R. Jolivet, T.J. Lewis, and W. Gerstner. Generalized integrate-and-fire models of neuronal [7] A. Banerjee. On the phase-space dynamics of systems of spiking neurons. I: Model and exper-[8] Tadeusz Stanisz. Functions with separated variables. Master X  X  thesis, Zeszyty Naukowe Uni-[9] R.J. MacGregor and E.R. Lewis. Neural Modeling . Plenum Press, New York, 1977. [10] G. Kimeldorf and G. Wahba. Some results on Tchebycheffian spline functions. Journal of [11] T. Joachims. Making large-scale support vector machine learning practical. In Advances in [12] E.M. Izhikevich. Simple model of spiking neurons. IEEE Transactions on Neural Networks ,
