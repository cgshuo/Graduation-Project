 Combining the output from multiple retrieval sources over the same document collection is of great importance to a number of retrieval tasks such as multimedia retrieval, web retrieval and meta-search. To merge retrieval sources adap-tively according to query topics, we propose a series of new approaches called probabilistic latent queryanalysis (pLQA), which can associate non-identical combination weights with latent classes underlying the query space. Compared with previous query independent and query-class based combina-tion methods, the proposed approaches have the advantage of being able to discover latent query classes automatically without using prior humanknowledge, to assign onequeryto a mixture of query classes, and to determine the number of query classes under a model selection principle. Experimen-tal results on two retrieval tasks, i.e., multimedia retrieval and meta-search, demonstrate that the proposed methods can uncover sensible latent classes from training data, and can achieve considerable performance gains.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Algorithms, Performance, Theory Keywords: Retrieval, Query Class, Fusion, Learning Anumber of information retrieval tasks utilize retrieval outputsprovidedbymultiple knowledge sources for thesame document collection. For example, web retrieval [7] consid-ers a single web page as a document retrievable based on sev-eral types of information, such as titles, anchor texts, main body texts as well as its linking relation to other pages. Mul-timedia retrieval [20, 1, 8] attempts to retrieve documents containing information extracted from multiple modalities, such as the speech transcription text, the low-level visual features of the images and a set of semantic concepts au-tomatically detected in the video clip. Meta-search [15, 10, 11] starts with the ranked lists generated from differ-Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. ent search engines on a single document collection. In all scenarios, retrieval systems usually benefit from combining various retrieval sources by exploiting complementary infor-mation from them.

Retrieval source combination has been examined by a sig-nificant body of previous work. Most current approaches fall into the category of query-independent methods, which adopt the same combination strategy for every query. Well-knownexamplesare theCombSUM/CombMNZ methods[15] in meta-search. However, these methods might have limited effectiveness because optimal combination strategies often vary considerably for different query topics. To address this, recent work has led to query-class dependent combination approaches, which map the query space into one of the pre-defined query classes and learn the best combination strat-egy for each class from training data. For example, in order to improve web document retrieval, Kang et al. [7] classified queries into three categories where each category had its spe-cific combination strategy. In multimedia retrieval, query-class combination methods using manually defined query classes [20, 1, 8] have been superior to query-independent combination in a number of standard video search evalua-tions [16]. Voorhees et al. [17] proposed a query clustering method for distributed IR, which groups the queries based on the number of common documents retrieved and creates centroids by averaging the query vectors inside the clusters. Recent advances in query difficulty classification [21] can also be viewed as quite similar. In these studies, each query is first classified into one of two groups based on retrieval  X  X ifficulty X  and then coupled with corresponding merging schemes based on classification results.

Despite recent successes, query-class combination meth-ods still have plenty of room for improvement. One major issue is that query classes usually need to be defined using expert domain knowledge. This manual design can work well when only a few query classes are needed, but it will become difficult for tens or hundreds of query classes, where each query in a class has to share similar characteristics and thus a similar combination strategy. If the query classes are not carefully defined, a large learning error from both the query-class assignment and the combination parameter estimation might result. Furthermore, current query-class methods do not allow mixtures of query classes, but at times such a mixture treatment could be helpful. For instance, the query X  X inding Bill Clinton in front of US flags X  should be as-sociated with both a  X  X erson X  class and a  X  X amed Object X  class rather than only one of these. Finally, determining the number of query classes remains an unanswered problem in these methods due to the nature of manual design. Some previous approaches [17, 8] can discover query classes by us-ing clustering techniques. However, these approaches typi-cally separate the processes of query class categorization and combination optimization into two sequential steps without being jointly optimized. Moreover, it is not straightforward for general clustering methods to handle mixtures of query classes in a principled manner.

Based on these considerations, it is desirable to develop a data-driven probabilistic combination approach that al-lows query classes and their corresponding combination pa-rameters to be automatically discovered from the training data itself, rather than handcrafted using human knowledge. Therefore, we propose a new combination approach called probabilistic latent query analysis (pLQA) to merge multi-ple retrieval sources based on statistical latent-class mod-els. The proposed approaches have advantages over query-independent and query-class combination methods in sev-eral ways: (1) they unify combination weight optimization and query-class categorization into a single discriminative learning framework; (2) they are able to automatically dis-cover the latent query classes directly from training data; (3) they can handle mixtures of query classes in one query and (4) they can determine the number of query classes with an statistical model selection principle. Experiments are conducted on two retrieval applications, i.e., multimedia retrieval on the TRECVID X 02- X 05 collections [16] and meta-search on the TREC-8 collection [18]. The results show that the proposed approaches can uncover sensible latent classes from training data, and also demonstrate higher effective-ness in combining multiple retrieval sources.
In this section, we present a discriminative combination framework that all of our proposed approaches are built on. Let us begin by introducing the basic notations and termi-nologies used in this work. The term document refers to the basic unit of retrieval throughout this paper. Aquery col-lection Q contains a set of queries { q 1 , ..., q t , ..., q q can have either a set of keywords, a detailed text de-scriptions and possibly image, audio, and video query ex-amples. Asearch collection D contains a set of documents { d 1 , ..., d j , ..., d M D } .Let y  X  X  X  1 , 1 } indicate if document D is relevant or irrelevant to query Q . For each query q and document d , we can generate a bag of ranking features from N retrieval sources, denoted as f i ( d, q ). Our goal is to generate an improved ranked list by combining f i ( d, q ).
Conventional relevance-based probabilistic models [4] rank documents by sorting the conditional probability that each document would be judged relevant to the given query, i.e., P ( y =1 | D, Q ) or denoted as P ( y + | D, Q ). Most well-known text retrieval models, such as the BIR model [13], proceed by inverting the position of y and D based on the Bayes rule and estimating the generative probabilities of docu-ment D in the relevant and irrelevant documents. How-ever, the underlying model assumptions of these approaches such as the term independency could be invalid in practice. In contrast, discriminative models can directly model the classification boundary and typically make fewer model as-sumptions. They have been applied in many domains of text processing such as text classification and information extrac-tion. Moreover, different retrieval sources usually provide heterogeneous types of outputs for combination including both query-dependent features and query-independent fea-tures. Generative models might face difficulties in the man-ual design of different model distributions for these outputs. Nallapati [12] has shown that in presence of heterogenous features, discriminative models are superior to generative models in a home-page finding task. Moreover, since the number of retrieval sources is usually much smaller than the number of text keywords, it allows a discriminative model to estimate the parameters more robustly given the same amount of the training data.

Taking these factors into account, we decided to utilize discriminative models to combine multiple retrieval sources. Formally, we model the posterior probability of therelevance as a logistic function on a linear combination of ranking features, i.e., where  X  ( x )=1 / (1 + e  X  x ) is the standard logistic function and  X  is the combination parameter for the output of i th ranking features f i ( D,Q ). This logistic regression model presented in Eqn(1), a.k.a. the maximum entropy model, summarizes our basic retrieval source combination frame-work. It naturally provides a probabilistic interpretation for the retrieval outputs. Once the parameters are estimated, documents can be presented to users in descending order of P ( y + | D, Q ), or equivalently by the weighted sum of retrieval outputs solved using Newton X  X  method described in [2].
Theaforementioned discriminative retrieval framework pro-vides a principled platform to support the task of retrieval source combination. However, in its current form, the model still has a problem in that the combination parameters  X  i are completely independent of the queries. In other words, the model will associate every possible query with the same set of combination parameters no matter what types of in-formation needs users are expressing. To improve upon this, we study a more flexible combination approach called prob-abilistic latent query analysis (pLQA). It aims to determine the combination weights based on the latent mixing struc-ture of the query space that can be automatically discovered under a statistical latent class model. In the rest of this sec-tion, we first discuss the basic form of the pLQ Amethod and its parameter estimation strategies. To deal with un-seen queries outside the training collection, we then extend pLQ Ato its adaptive version and kernel version.
It would be ideal if we could learn specific combination parameters for every possible query. However, given the virtually infinite number of query topics, it is impractical to learn the combination weights on a per query basis because we cannot collect enough training data individually. We need to come up with a trade-off to balance the difficulties of collecting training data and the ability to capture theidio-syncracy of the query space. To achieve this, we make the Figure 1: Graphical model representations for (a) the following assumptions in our models: (1) the entire query space can be described by a finite number of query classes, where queries from each class share the same combination function; (2) the query description can be used to indicate which class a query belongs to. Under the first assumption, the basic probabilistic retrieval model expressed in Eqn(1) can be naturally extended to a finite mixture of conditional probabilistic models. Formally, we can introduce a multino-mial latent query class variable z to indicate which mixture the combination function is drawn from. Based on the sec-ond assumption, the choice of z is solely depending on the query Q . Putting all these together, we have the joint prob-ability of relevance y and latent variable z as, where  X  is the parameter for multinomial distributions,  X  is the combination parameter for query classes. The mixture component P ( y + | Q, D, z ;  X  ) corresponds to a single logis-tic regression model and the mixing proportion P ( z | Q ;  X  ) controls the switches among different classes based on the query-dependent parameters  X  Q . By marginalizing out the hidden variables z , the corresponding mixture model can be written as, ( M z is the number of query classes) P ( y + | Q, D ;  X ,  X  )= In the following discussions, we refer the model presented in Eqn(3) to as the basic pLQA ( BpLQA ) model where each la-tent query class represents a group of similar queries sharing the same combination weights. Note that when the number of latent variables is reduced to 1, BpLQ Adegrades to the case where retrieval source combination is not relevant to queries, i.e., a query-independent combination approach.
Figure 1(ab) compare the probabilistic graphical model representations of BpLQ Aand the query-class combination methods, where one of the their major differences is the semantic of the mixing proportions P ( z | Q,  X  ). In the query-class method, query classes have to be derived from man-ually defined generation rules before the learning process. However, query classes in the BpLQ Amodel are expressed as latent variables and thus can be estimated directly from training data together with the combination weight estima-tion. Moreover, they also differ in the way how they asso-ciate queries with query classes. By analogy, the query-class method can be viewed as a hard version of  X  X uery catego-rization X  which associates each query with one single query class,andmeanwhileBpLQAcanbeviewedasasoftver-sion of  X  X uery categorization X  which leads to a probabilistic membership assignment of queries to latent query classes.
Because of the representation differences, BpLQ Acan ex-ploit the following advantages over the query-class combi-nation method: (1) it can automatically discover the query classes from training data rather than by manual defini-tion, (2) it offers probabilistic semantics for the latent query classes and thus allows mixing multiple query types for a single query, (3) it can discover the number of query types in a principled way, (4) it can address the insufficient data problem caused by ill-defined query classes that have few positive examples in the training set and (5) it unifies the combination weight optimization and query class categoriza-tion into a single learning framework.

The parameters in BpLQ Acan be estimated by maximiz-ing its incomplete data log-likelihood. Atypical approach to achieve this is to use the Expectation-Maximization (EM) algorithm [3], which can obtain a local optimum of log-likelihood by iterating an E-step and an M-step until con-vergence. The E-step of the BpLQ Amodel can be derived by computing the expectation of z , h tj ( z )= P ( z | Q t ,D j )= where  X  zt = P ( z | Q t ;  X  ) is the probability of choosing hidden query classes z given query q m and  X  zi is the weights for f under the class z . By optimizing the auxiliary Q-function, we can derive the following M-step update rules,  X  zi =argmax  X   X  When the log-likelihood converges to a local optimum, the estimated parameters can be plugged back into the BpLQA model to describe the underlying query space structure and show how the training queries are organized.

Thechoiceofmixturenumber K might depend on the amount and the distribution sparseness of the training data. As the number of latent variables grows, the family of de-cision functions represented in BpLQ Awill become less re-stricted and thus lead to lower bias in the estimated mod-els, but meanwhile it would suffer from a higher variance due to an increased model complexity. Based on the bias-variance trade-off, we can determine the number of query classes that are sufficient to capture the variations in the query space with the given amount of training data. To be more concrete, the number of query classes can be ob-tained by maximizing the sum of the log-likelihood and some model selection criteria such as Akaike Information Crite-ria(AIC), Bayesian Information Criteria(BIC) and so forth. In our work, we choose BIC [14] as the selection criterion. It quantifies the relative goodness-of-fit of statistical models by maximizing the formula of BIC =2 l (  X ,  X  )  X  k log( n ) , where k is the number of parameters to optimize and n is the number of training data.
Discovering the underlying structure of query space by itself is not sufficient to handle the retrieval source com-bination, because a practical combination model should be able to predict combination parameters for unseen queries outside the training collection. Unfortunately, BpLQ Acan-not easily generalize the multinomial parameters  X  to any of these unseen queries, because each parameter  X   X  t in BpLQA specifically corresponds to the t th training query. Since it is impossible to enumerate all possible queries in the training set, we need to come up with a solution to predict the mix-ing proportions P ( z | Q t ;  X  ) of any unseen queries that do not belong to the training collection. To generalize the parame-ters to new documents, Hofmann [6] suggested a  X  X old-in X  process for the latent class model by re-learning all training documents with the new document to generate an updated parameter estimation. However, the  X  X old-in X  process by plugging in new queries and re-estimating the entire model is not reasonable in our task, because it requires a long time to process and more importantly, we do not have any rele-vance judgment for new queries to learn from.

To address this problem, we propose an adaptive approach aiming at parameterizing the mixing proportion P ( z | Q t using a specific set of features directly extracted from query topics, or called query features, that are able to capture important characteristics of users X  information need. For-mally, we can represent each query as a bag of query features { q 1 , ...q L } . The mixing proportions P ( z k | Q ;  X  )canthenbe modeled using a soft-max function 1 Z exp( Z = the exponential function to be a probability distribution. By substituting the mixing proportion back into Eqn(3), previ-ous BpLQ Amodel can be rewritten as, P ( y + | Q, D )= 1 Note that, since  X  zl is associated with each query feature instead of each training query, this modification allows the estimated  X  zl to be applied in any unseen queries as long as they can be formulated as vectors of query features. In the following discussions, we refer the model expressed in Eqn(4) to as the adaptive pLQA ( ApLQA )model.

For this model, the EM algorithm can be derived simi-larly except the mixing proportions need to be substituted with Eqn(3.2). Therefore, the E-step computes the posterior probability of latent variable z given Q t and D j as follows, In the M-step, we have the same update rule for updating  X  zi . For updating  X  ,wehave  X  zl =argmax  X  where Z t is the normalization factor for query Q t , i.e., Z z exp( gradient descent method. Note that, this step is actually fitting a multi-class logistic regression model with query fea-tures as inputs. In more detail, it is looking for the most similar logistic regression model w.r.t. the posterior proba-bilities of z for query Q t .

The only remaining issue for defining the ApLQA model is to design a set of predictive query features. There are two useful principles to guide the design of suitable query features: 1) they should be able to be automatically gen-erated from query descriptions or the statistics of ranking features, and 2) they should be predictive to estimate which latent classes the query belong to. For example, we can consider the presence/absence of specific person names in query topics, and the mean retrieval scores of each retrieval source as query features. Note that, in contrast to creat-ing query classes that must be mutually exclusive, defining query features is much more flexible, eliminating the need to partition the query space into non-overlapping regions. Moreover, the number of query features can be much larger than the number of query classes with the same amount of training data.
By introducing explicit query features into the combina-tion function, ApLQA can handle unseen queries that do not appear in the training data. However, the assumption of linear query feature combination in ApLQA might not be the best choice in general because the number of query fea-tures is often limited. Also, there exists some useful query information that cannot be described by explicit query fea-ture representation. For example, the edit distance between two queries is a helpful hint for combination but it cannot be easily represented as a explicit query feature. Therefore, we develop an extension of the ApLQA model called the kernel pLQA ( KpLQA ) model that lends itself to the use of implicit feature space via Mercer kernels based on the representer theorem [9]. This extension is also motivated by a significant body of recent work that has demonstrated kernel methods are effective in a variety of applications.
In more detail, the kernel representation allows simple learning algorithms to construct a complex decision bound-ary by projecting the original input space to a high di-mensional feature space, even infinitely dimensional in some cases. This seemingly computationally intensive task can be easily achieved through a positive definite reproducing ker-nel K and the well-known  X  X ernel trick X . To begin, let us rewrite the sum term function f z ( Q ) with respect to the query Q , P ( y + | Q, D )  X  Since the loss function above only depends on the value of f at the data points { f z ( Q ) } , according to the representer theorem, the minimizer f ( x ) admits a representation of the form f z ( Q )= training queries,  X  zk is the kernel fusion parameter for z and Q ,and K (  X  ,  X  ) is a Mercer kernel on the query space which has to be positive definite. With this kernel representation, we can derive the corresponding log-likelihood function by substituting original mixing proportion term P ( z | Q t )tobe, ApLQ Ais a special case of KpLQ Aif each element of K is chosen to be the inner product between the query features of two queries. However, the flexibility of kernel selection Table 1: Labels of the video collections and statistics. has offered more powers to the KpLQ Amodel. For exam-ple, the kernel function can have different forms such as the polynomial kernel K ( u, v )=( u  X  v +1) p and the Radial Basis Function (RBF) kernel K ( u, v )=exp(  X   X  u  X  v 2 ). The lat-ter one has the ability to project the query features into an infinite dimensional feature spaces. Moreover, we can trans-form the distance metric between queries (e.g., edit distance between queries) into the implicit feature space in form of a Mercer kernel, instead of designing explicit features for each query. The optimization of KpLQ Ais similar as that of ApLQA except for changing the mixing proportion term in the E-step and M-step to the kernelized one.
In this section, we present the experimental results on two retrieval applications based on the retrieval source combina-tion, i.e., multimedia retrieval and meta-search.
Our experiments are designed based on the guidelines of the manual retrieval task in the TREC video retrieval eval-uation(TRECVID), which requires an automatic video re-trieval system to search relevant documents without any human feedback. In this task, the retrieval units are video shots defined by a common shot boundary reference. The proposed combination algorithms are evaluated using the queries andthevideocollections officially providedbyTREC  X 02- X 05 [16]. For TREC X 03- X 05, each of these video collec-tions is split into a development set and a search set chrono-logically by source. The development sets are used as the training pool to develop automatic multimedia retrieval al-gorithms and the search sets mainly serve as the testbeds for evaluating the performances of retrieval systems. For each query topic, the relevance judgment on search sets was pro-vided officially by NIST. The relevance judgment on devel-opment sets was collaboratively collected by several human annotators using the Informedia client [5]. Moreover, we manually designed 40 additional queries and collected the ground truth on the TREC X 04 development set in order to demonstrate ApLQA is able to learn from arbitrary queries that do not belong to the testing set 1 . We couple them with the TREC X 04- X 05 query topics to construct an external training corpus with 88 queries. Table 1 lists the labels of video collections and their query/document statistics.
As building blocks for multimedia retrieval, we generated a number of ranking features on each video document, in-cluding 14 high-level semantic features learned from devel-opment data (face, anchor, commercial, studio, graphics,
These queries bring some (but insignificant) performance improvement in our retrieval experiments, as can be verified by comparing performance between BpLQA and ApLQA. weather, sports, outdoor, person, crowd, road, car, building, motion), and 5 uni-modal retrieval experts (text retrieval, face recognition, image-based retrieval based on color, tex-ture and edge histograms). The detailed descriptions on the feature generation can be found in [5]. In an attempt to incorporate the ranking information in the learning process, we weighted the positive data stronger to balance the pos-itive/negative data distribution and meanwhile shifted the median value of each feature to zero [19]. To initialize the EM algorithm in pLQA, we set the mixing parameters  X  z to some random numbers and estimated the combination parameters  X  z from M z individual queries, which are au-tomatically selected using a simple heuristic similar to the maximal margin relevance.

In order to implement the ApLQA and KpLQA model, we also designed the following binary query features, i.e., if the query topic contains 1) specific person names, 2) specific object names, 3) more than two noun phrases, 4) words re-lated to people/crowd, 5) words related to sports, 6) words related to vehicle, 7) words related to motion, 8) similar image examples w.r.t. color or texture, 9) image examples with faces and finally a feature indicates 10) if the text re-trieval module finds more than 100 documents. All these query features can be automatically detected from the query description through some manually defined rules plus nat-ural language processing and image processing techniques. For example, the first three query features can be obtained by the methods of named entity extraction, NP tagging and shallow parsing, of which the details can be found in our pre-vious work [20] 2 . The features of sports, vehicle and people can be detected using a manually defined vocabulary with a limited set of words. The eighth and ninth query features can be automatically generated by existing image processing and face detection techniques. Finally the last query feature can be simply obtained from text retrieval statistics. Note that our current implementation requires some query fea-tures to be generated from manually defined rules. It is an interesting direction to explore fully automatic approaches to extract query features but we leave it as our future work.
To illustrate the ability of pLQ Ato discover meaningful latent query classes, Figure 2 shows five latent query classes that are automatically learned from the BpLQ Amodel on the TREC X 05 development data. In this figure, all of the TREC X 05 queries are listed except those without any train-ing data in the development set. The blocks on the right show the scale of mixing proportions p ( z | Q )ofeachquery Q w.r.t. each query class z . These queries are organized into five groups based on the class IDs of their maximal mixing proportion in all query classes, i.e., argmax z p ( z | Q ).
It is interesting to examine whether the query grouping suggested by BpLQ Aare sensible. Among these five groups, the first one mainly contain all the  X  X amed person X  queries in the training data and one query related to person appear-ance,  X  X eeting with a large table X . This group of queries usually has a high retrieval performance when using the text features and prefers the existence of person faces, while content-based image retrieval is not effective for them. The second group consists of three queries related to sport events
This work [20] also provides evidence that above query fea-tures can be predicted with an accuracy above 93%, which is sufficient to support the following retrieval process. Figure 2: We organize TREC X 05 queries into five groups including  X  X asketball X ,  X  X occer X  and  X  X ennis X . They often rely on both text retrieval and image retrieval results, be-cause these sport scenes in the news broadcast usually share common transcript words and image background. The last three groups all contain queries related to object finding, however, they have many distinctions in the combination strategies. In the third group, the queries tend to search for objects that have similar visual appearances without any apparent motions. The case of the fourth group is quite dif-ferent. They are mainly looking for the objects in the out-doors scene such as  X  X oad X  and  X  X ilitary vehicle X . Finally, the fifth group seems to be a general group that contains all remaining queries. The queries in this group search for ob-jects without visual similarities and thus place a high weight on the text retrieval since text retrieval is usually the most reliable retrieval component in general.

As can be found, most of the discovered latent query classes are consistent with the observations made by many previous studies [20, 1], such as the classes of named per-son, objects and sports. Meanwhile, BpLQ Ais also able to suggest some query classes that have not been suggested before such as the class of  X  X utdoor objects X . This analysis shows that BpLQ Acan achieve a reasonable query classifi-cation through a completely data-driven statistical learning approach instead of a manual handcrafting procedure.
Finally, it is also helpful to analyze the query mixing pat-terns learned from BpLQA. For example, the query  X  X mar Karami X  can be described by a mixture of the first and the second query classes. It is not obvious why the second query class is related at first sight. But after a further analysis, we found that Karami always showed up with a similar back-ground when he was meeting foreigners and hence visual ap-pearance turned out to be a helpful clue to find him. Such a mixture treatment provides more flexibilities in retrieval modeling and offers deeper understandings on the queries.
Next we present the multimedia retrieval results on all Figure 3: The negative training log-likelihood of the search sets using the proposed models and parameters learned from the development set. Since BpLQ Acannot generalize its parameters to unseen queries, we use the de-velopment set on the same year as the search set to estimate its parameters. For example, the parameters estimated on the set t 03 d is reused in the search set t 03 s .ForApLQA and KpLQA, there is no such a constraint and therefore the parameters are estimated with a larger training set t 04 dx .
As discussed before, we can obtain the number of query classes by optimizing the regularized log-likelihood with an additional BIC term. Figure 3 plots the curves of the neg-ative log-likelihood and their regularized counterparts on three development sets (i.e., t03d, t04d and t05d) against the number of query classes. It can be observed that when the number of query classes grows, the learning curves be-come consistently lower and lower until they asymptotically reach saturated levels. Occasionally the curves even slightly raise at theend, indicating the training data seem to beover-fitted in the case of a large number of query classes. Based on the statistical model selection principle, the number of queryclasses for each training set can bedetermined byseek-ing the lowest point on the regularized log-likelihood. Given the ;earning curves, we can find that the optimal numbers of latent classes turn out to be 4, 4 and 6 for the collections of t03d, t04d, t05d respectively.

Table 2(a) lists a detailed comparison between BpLQA with the estimated numberof query classes and several base-line methods including text retrieval (Text), query indepen-dent (QInd) and query-class combination (QClass) methods with five classes defined in [20]. QClass has shown to be one of the best overall retrieval systems in the past TRECVID evaluations. The parameters in all baseline methods were learned using the same training sets as BpLQA. All the results results are reported in terms of the mean average precision(MAP) up to 1000 documents, precision at top 30, 100 documents and recall at top 1000 documents. To ana-lyze the results in more detail, we also grouped the queries in each collection and reported their MAPs in five different categories, i.e., named person, special object, general object, sports and general queries. As can be observed from the re-sults, QClass is usually superior to both QInd and Text. On average, it brings a roughly 3% absolute improvement (or 30% relative improvement) over the text retrieval. As compared to QClass, BpLQ Aachieves another 2% boost in terms of MAP without any manual tuning on the queryclass definitions. As it results, the differences between BpLQA and Text/QInd become statistically significant in two out of three collections. The major performance growth factor for BpLQ Acan be traced to a higher precision on the top documents, although the recalls between all these methods do not vary a lot. By comparing MAPs with respect to each query type, we find that BpLQ Abenefits most from the Person and Special Object type queries, as well as the General Object type in t 05 s . Thisisbecausethesequery types have their information needs clearly defined and thus they are able to be improved by making better use of the training data.

Table 2(b) compares text retrieval and query-class com-bination (QClass-x) with ApLQA, KpLQA using the RBF kernel with  X  =0 . 01 (KpLQA-R),KpLQAusing the polyno-mial kernel with p = 3 (KpLQA-P). All the parameters are estimated from the external training set t 04 dx .EachpLQA model is learned with six query classes based on the regu-larized likelihood estimation. The experimental settings are similar to those presented above except that the results of t 02 s are evaluated in addition. Note that, QClass-x in this table are learned on a larger training set and thus it can out-perform its counterparts in the Table 2(a), which shows the importance of sufficient training data. Despite this change, both ApLQ Aand KpLQ Acan still outperform QClass-x by a margin of 1-2% (10-20% relatively) w.r.t. MAP. Simi-larly, the increase in precision is the main advantage brought by the pLQ Amodels. Among these models, KpLQ Ashow some advantages over the ApLQA model in 3 out of 4 col-lections, although the difference between them is not sig-nificant. However, we believe KpLQ Ahas more opportuni-ties to be improved because it is flexible to incorporate the distance-metric-type features and we will explore this choice in the future.
Our next series of experiments are designed based on the application of meta-search that combines multiple search en-gines on a single text collection. The TREC-8 collection [18] is used as our testbed which contains 50 query topics and around 2GB worth of documents. Each topic consists of both a short topic title and a long topic description. From the submitted outputs provided by all the participants, we extracted the top five manual retrieval systems and top five automatic retrieval systems as inputs of the meta search system. Their system codes are READWARE2, orcl99man, 8manex, CL99XTopt, iit99ma1, pir9Attd, att99atde, ibms99a, ok8amxc, and fub99td respectively. Each system has at best return 1000 documents for each query. The relevance judgment was officially provided by NIST using a pooling method. We adopt the sum normalization scheme [11] which normalizes the sum of scores from each submission runs to be one and shifts minimum to be zero. Table 3: The comparison of meta-search rerformance
We also extracted the following query features for learning the ApLQA model: length of the query title, appearance of named entities in the query and the score ratio between the first ranked document and 50th ranked document for each of the ten systems. Together with a constant term, we generated a total of 13 query features for each query. These features are designed in an attempt to capture information about query difficulties and generalities.

In the following, we examine the performance of various meta-search algorithms that combine all of the retrieval sys-tems. The retrieval performance is averaged over the last 25 queries in terms of MAP, precision at top 30, 100 docu-ments and recall at top 1000 documents. We evaluated the best underlying retrieval system(BOU) in addition to four meta-search strategies, including CombSUM, CombMNZ, query independent combination (QInd) and ApLQA. For those algorithms that require parameter estimation(QInd and ApLQA), we use the first 25 queries as the training data. As shown in Table 3, CombSUM and CombMNZ can improve upon BOU by a margin of around 10% MAP. Between them, the performance of CombMNZ is slightly worse than that of CombSUM. With aid of the training set, QInd that uses flexible wights is superior to CombSUM that fixes equal weights for every retrieval system. Finally, by in-troducing the query features and allowing the combination weights vary across different queries, ApLQA offers an addi-tional 2% improvement over QInd w.r.t. MAP. This advan-tage mainly comes from the extra combination flexibilities provided by ApLQA.
In this paper, we propose a series of new combination ap-proaches called probabilistic latent query analysis (pLQA) to merge multiple retrieval sources, which unifies the com-bination weight optimization and query class categoriza-tion into a discriminative learning framework. Three pLQA models have been discussed which evolve from a basic ver-sion(BpLQA)to anadaptiveversion (ApLQA)thatoperates on the query feature space and a kernel version (KpLQA) that builds on a Mercer kernel representation. In contrast to the typical query-independent and query-class combina-tion methods, pLQAcan automatically discoverlatent query classes from the training data rather than relying on manu-ally defined query classes. Also, it can associate one query with a mixture of query classes and thus non-identical com-bination weights. Finally, based on statistical model selec-tion principles, we can obtain the optimal number of query classes by maximizing the regularized likelihood. Our ex-periments in two large-scale retrieval applications, i.e., mul-timedia retrieval and meta-search on the TREC collections, demonstrate the superiority of the proposed methods which can achieve significant gains in average precision over the query-independent/query-class combination methods. We expect that future investigation on designing better query features for ApLQA and introducing some distance-metric-type kernels to KpLQ Acould result in a further improve-ment on the performance of retrieval source combination. This work was supported in part by the Advanced Research and Development Activity (ARDA) under contract number H98230-04-C-0406 and NBCHC040037, and by the National Science Foundation under Grant No. IIS-0535056.
