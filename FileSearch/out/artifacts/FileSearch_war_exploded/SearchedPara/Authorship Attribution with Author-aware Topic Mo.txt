 Authorship attribution (AA) has attracted much at-tention due to its many applications in, e.g., com-puter forensics, criminal law, military intelligence, and humanities research (Stamatatos, 2009). The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classifi-cation problem). While most of the early work on AA focused on formal texts with only a few pos-sible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic mod-els have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed that methods based on Latent Dirichlet Allocation (LDA)  X  a popular topic model by Blei et al. (2003)  X  yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap.

In addition to being the first (to the best of our knowledge) to apply Rosen-Zvi et al. X  X  (2004) Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which ad-dresses AT X  X  limitations in the context of AA. We show that DADT outperforms AT, LDA, and linear support vector machines on AA with many authors. Background. Our definition of DADT is motivated by the observation that when authors write texts on the same issue, specific words must be used (e.g., texts about LDA are likely to contain the words  X  X opic X  and  X  X rior X ), while other words vary in fre-quency according to author style. Also, texts by the same author share similar style markers, indepen-dently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T ( D ) document topics and T ( A ) author topics .

Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The clos-est work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA of multi-authored texts (DMR does not use disjoint topic sets). We use AT rather than DMR, since we found that AT outperforms DMR in AA of single-authored texts, which are the focus of this paper. The Model. Figure 1 shows DADT X  X  graphical rep-resentation, with document-related parameters on the left (the LDA component), and author-related parameters on the right (the AT component). We de-fine the model for single-authored texts, but it can be easily extended to multi-authored texts.

The generative process for DADT is described be-low. We use D and C to denote the Dirichlet and categorical distributions respectively, and A , D and V to denote the number of authors, documents, and unique vocabulary words respectively. In addition, we mark each step as coming from either L DA or A T , or as new in D ADT .
 Global level:
L. For each document topic t , draw a word dis-
A. For each author topic t , draw a word distribu-
A. For each author a , draw the author topic dis-
D. Draw a distribution over authors  X   X  D (  X  ) , Document level: For each document d :
L. Draw d  X  X  topic distribution  X  ( D ) d  X  D  X  ( D ) , D. Draw d  X  X  author a d  X  X  (  X  ) .

D. Draw d  X  X  topic ratio  X  d  X  Beta  X  ( A ) , X  ( D ) , Word level: For each word index i in document d : D. Draw di  X  X  topic indicator y di  X  Bernoulli(  X  d ) . L. If y di = 0 , draw a document topic z di  X 
A. If y di = 1 , draw an author topic z di  X  X   X  ( A ) a DADT versus AT. DADT might seem similar to AT with  X  X ictitious X  authors, as described by Rosen-Zvi et al. (2010) (i.e., AT trained with an additional unique  X  X ictitious X  author for each document, allow-ing it to adapt to individual documents and not only to authors). However, there are several key differ-ences between DADT and AT.

First, in DADT author topics are disjoint from document topics , with different priors for each topic set. Thus, the number of author topics can be differ-ent from the number of document topics, enabling us to vary the number of author topics according to the number of authors in the corpus.

Second, DADT places different priors on the word distributions for author topics and document topics (  X  ( A ) and  X  ( D ) respectively). Stopwords are known to be strong indicators of authorship (Kop-pel et al., 2009), and DADT allows us to use this knowledge by assigning higher weights to the ele-ments of  X  ( A ) that correspond to stopwords than to such elements in  X  ( D ) .

Third, DADT learns the ratio between document words and author words on a per-document basis, and makes it possible to specify a prior belief of what this ratio should be. We found that specify-ing a prior belief that about 80% of each document is composed of author words yielded better results than using AT X  X  approach, which evenly splits each document into author and document words.

Fourth, DADT defines the process that generates authors . This allows us to consider the number of texts by each author when performing AA. This also enables the potential use of DADT in a semi-supervised setting by training on unlabelled texts, which we plan to explore in the future. We experimented with the following AA methods, using token frequency features, which are good pre-dictors of authorship (Koppel et al., 2009). Baseline: Support Vector Machines (SVMs). Koppel et al. (2009) showed that SVMs yield good AA performance. We use linear SVMs in a one-versus-all setup, as implemented in LIBLIN-EAR (Fan et al., 2008), reporting results obtained with the best cost parameter values.
 Baseline: LDA + Hellinger (LDA-H). This ap-proach uses the Hellinger distances of topic dis-tributions to assign test texts to the closest author. In (Seroussi et al., 2011), we experimented with two variants: (1) each author X  X  texts are concatenated be-fore building the LDA model; and (2) no concate-nation is performed. We found that the latter ap-proach performs poorly in cases with many candi-date authors. Hence, we use only the former ap-proach in this paper. Note that when dealing with single-authored texts, concatenating each author X  X  texts yields an LDA model that is equivalent to AT. AT. Given an inferred AT model (Rosen-Zvi et al., 2004), we calculate the probability of the test text words for each author a , assuming it was written by a , and return the most probable author. We do not know of any other studies that used AT in this man-ner for single-authored AA. We expect this method to outperform LDA-H as it employs AT directly, rather than relying on an external distance measure. AT-FA. Same as AT, but built with an additional unique  X  X ictitious X  author for each document. DADT. Given our DADT model, we assume that the test text was written by a  X  X ew X  author, and infer this author X  X  topic distribution, the author/document topic ratio, and the document topic distribution. We then calculate the probability of each author given the model X  X  parameters, the test text words, and the inferred author/document topic ratio and document topic distribution. The most probable author is re-turned. We use this method to avoid inferring the document-dependent parameters separately for each author, which is infeasible when many authors ex-ist. A version that marginalises over these parame-ters will be explored in future work. We compare the performance of the methods on two publicly-available datasets: (1) PAN X 11 : emails with 72 authors (Argamon and Juola, 2011); and (2) Blog: blogs with 19,320 authors (Schler et al., 2006). These datasets represent realistic scenar-ios of AA of user-generated texts with many can-didate authors. For example, Chaski (2005) notes a case where an employee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email. Experimental Setup. Experiments on the PAN X 11 dataset followed the setup of the PAN X 11 competi-tion (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the pa-rameters according to the results on the given valida-tion subset, and ran the tuned methods on the given testing subset. In the Blog experiments, we used ten-fold cross validation as in (Seroussi et al., 2011).
We used collapsed Gibbs sampling to train all the topic models (Griffiths and Steyvers, 2004), run-ning 4 chains with a burn-in of 1,000 iterations. In the PAN X 11 experiments, we retained 8 samples per chain with spacing of 100 iterations. In the Blog experiments, we retained 1 sample per chain due to runtime constraints. Since we cannot average topic distribution estimates obtained from training sam-ples due to topic exchangeability (Steyvers and Grif-fiths, 2007), we averaged the distances and probabil-ities calculated from the retained samples. For test text sampling, we used a burn-in of 100 iterations and averaged the parameter estimates over the next 100 iterations in a similar manner to Rosen-Zvi et al. (2010). We found that these settings yield stable results across different random seed values.
We found that the number of topics has a larger impact on accuracy than other configurable pa-rameters. Hence, we used symmetric topic pri-ors, setting all the elements of  X  ( D ) and  X  ( A ) to min { 0 . 1 , 5 /T ( D ) } and min { 0 . 1 , 5 /T ( A ) tively. 1 For all models, we set  X  w = 0 . 01 for each word w as the base measure for the prior of words in topics. Since DADT allows us to encode our prior knowledge that stopword use is indicative of author-ship, we set  X  ( D ) w = 0 . 01  X  and  X  ( A ) w = 0 . 01 + for all w , where w is a stopword. 2 We set = 0 . 009 , which improved accuracy by up to one percentage point over using = 0 . Finally, we set  X  ( A ) = 4 . 889 and  X  ( D ) = 1 . 222 for DADT. This encodes our prior belief that 0 . 8  X  0 . 15 of each document is com-posed of author words. We found that this yields better results than an uninformed uniform prior of  X  ( A ) =  X  ( D ) = 1 (Seroussi et al., 2012). In addition, we set  X  a = 1 for each author a , yielding smoothed estimates for the corpus distribution of authors  X  .
To fairly compare the topic-based methods, we used the same overall number of topics for all the topic models. We present only the results obtained with the best topic settings: 100 for PAN X 11 and 400 for Blog, with DADT X  X  author/document topic splits being 90/10 for PAN X 11, and 390/10 for Blog. These splits allow DADT to de-noise the author represen-tations by allocating document words to a relatively small number of document topics. It is worth not-ing that AT can be seen as an extreme version of DADT, where all the topics are author topics. A fu-ture extension is to learn the topic balance automat-ically, e.g., in a similar manner to Teh et al. X  X  (2006) method of inferring the number of topics in LDA. Results. Table 1 shows the results of our experi-ments in terms of classification accuracy (i.e., the percentage of test texts correctly attributed to their author). The PAN X 11 results are shown for the val-idation and testing subsets, and the Blog results are shown for a subset containing the 1,000 most prolific authors and for the full dataset of 19,320 authors.
Our DADT model yielded the best results in all cases (the differences between DADT and the other methods are statistically significant according to a paired two-tailed t-test with p &lt; 0 . 05 ). We attribute DADT X  X  superior performance to the de-noising ef-fect of the disjoint topic sets, which appear to yield author representations of higher predictive quality than those of the other models.
 As expected, AT significantly outperformed LDA-H. On the other hand, AT-FA performed much worse than all the other methods on PAN X 11, prob-ably because of the inherent noisiness in using the same topics to model both authors and documents. Hence, we did not run AT-FA on the Blog dataset.
DADT X  X  PAN X 11 testing result is close to the third-best accuracy from the PAN X 11 competi-tion (Argamon and Juola, 2011). However, to the best of our knowledge, DADT obtained the best accuracy for a fully-supervised method that uses only unigram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest accu-racy (65.8%), assumed that all the test texts are given to the classifier at the same time, and used this additional information with a semi-supervised method; while Kern et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies respectively, used var-ious feature types (e.g., features obtained from parse trees). Further, preprocessing differences make it hard to compare the methods on a level playing field. Nonetheless, we note that extending DADT to enable semi-supervised classification and additional feature types are promising future work directions.
While all the methods yielded relatively low accu-racies on Blog due to its size, topic-based methods were more strongly affected than SVM by the transi-tion from the 1,000 author subset to the full dataset. This is probably because topic-based methods use a single model, making them more sensitive to corpus size than SVM X  X  one-versus-all setup that uses one model per author. Notably, an oracle that chooses the correct answer between SVM and DADT when they disagree yields an accuracy of 37.15% on the full dataset, suggesting it is worthwhile to explore ensembles that combine the outputs of SVM and DADT (we tried using DADT topics as additional SVM features, but this did not outperform DADT). This paper demonstrated the utility of using author-aware topic models for AA: AT outperformed LDA, and our DADT model outperformed LDA, AT and SVMs in cases with noisy texts and many authors. We hope that these results will inspire further re-search into the application of topic models to AA. This research was supported in part by Australian Research Council grant LP0883416. We thank Mark Carman for fruitful discussions on topic modelling.
 Authorship attribution (AA) has attracted much at-tention due to its many applications in, e.g., com-puter forensics, criminal law, military intelligence, and humanities research (Stamatatos, 2009). The traditional problem, which is the focus of our work, is to attribute test texts of unknown authorship to one of a set of known authors, whose training texts are supplied in advance (i.e., a supervised classifi-cation problem). While most of the early work on AA focused on formal texts with only a few pos-sible authors, researchers have recently turned their attention to informal texts and tens to thousands of authors (Koppel et al., 2011). In parallel, topic mod-els have gained popularity as a means of analysing such large text corpora (Blei, 2012). In (Seroussi et al., 2011), we showed that methods based on Latent Dirichlet Allocation (LDA)  X  a popular topic model by Blei et al. (2003)  X  yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap.

In addition to being the first (to the best of our knowledge) to apply Rosen-Zvi et al. X  X  (2004) Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which ad-dresses AT X  X  limitations in the context of AA. We show that DADT outperforms AT, LDA, and linear support vector machines on AA with many authors. Background. Our definition of DADT is motivated by the observation that when authors write texts on the same issue, specific words must be used (e.g., texts about LDA are likely to contain the words  X  X opic X  and  X  X rior X ), while other words vary in fre-quency according to author style. Also, texts by the same author share similar style markers, indepen-dently of content (Koppel et al., 2009). DADT aims to separate document words from author words by generating them from two disjoint topic sets of T ( D ) document topics and T ( A ) author topics .

Lacoste-Julien et al. (2008) and Ramage et al. (2009) (among others) also used disjoint topic sets to represent document labels, and Chemudugunta et al. (2006) separated corpus-level topics from document-specific words. However, we are unaware of any applications of these ideas to AA. The clos-est work we know of is by Mimno and McCallum (2008), whose DMR model outperformed AT in AA of multi-authored texts (DMR does not use disjoint topic sets). We use AT rather than DMR, since we found that AT outperforms DMR in AA of single-authored texts, which are the focus of this paper. The Model. Figure 1 shows DADT X  X  graphical rep-resentation, with document-related parameters on the left (the LDA component), and author-related parameters on the right (the AT component). We de-fine the model for single-authored texts, but it can be easily extended to multi-authored texts.

The generative process for DADT is described be-low. We use D and C to denote the Dirichlet and categorical distributions respectively, and A , D and V to denote the number of authors, documents, and unique vocabulary words respectively. In addition, we mark each step as coming from either L DA or A T , or as new in D ADT .
 Global level:
L. For each document topic t , draw a word dis-
A. For each author topic t , draw a word distribu-
A. For each author a , draw the author topic dis-
D. Draw a distribution over authors  X   X  D (  X  ) , Document level: For each document d :
L. Draw d  X  X  topic distribution  X  ( D ) d  X  D  X  ( D ) , D. Draw d  X  X  author a d  X  X  (  X  ) .

D. Draw d  X  X  topic ratio  X  d  X  Beta  X  ( A ) , X  ( D ) , Word level: For each word index i in document d : D. Draw di  X  X  topic indicator y di  X  Bernoulli(  X  d ) . L. If y di = 0 , draw a document topic z di  X 
A. If y di = 1 , draw an author topic z di  X  X   X  ( A ) a DADT versus AT. DADT might seem similar to AT with  X  X ictitious X  authors, as described by Rosen-Zvi et al. (2010) (i.e., AT trained with an additional unique  X  X ictitious X  author for each document, allow-ing it to adapt to individual documents and not only to authors). However, there are several key differ-ences between DADT and AT.

First, in DADT author topics are disjoint from document topics , with different priors for each topic set. Thus, the number of author topics can be differ-ent from the number of document topics, enabling us to vary the number of author topics according to the number of authors in the corpus.

Second, DADT places different priors on the word distributions for author topics and document topics (  X  ( A ) and  X  ( D ) respectively). Stopwords are known to be strong indicators of authorship (Kop-pel et al., 2009), and DADT allows us to use this knowledge by assigning higher weights to the ele-ments of  X  ( A ) that correspond to stopwords than to such elements in  X  ( D ) .

Third, DADT learns the ratio between document words and author words on a per-document basis, and makes it possible to specify a prior belief of what this ratio should be. We found that specify-ing a prior belief that about 80% of each document is composed of author words yielded better results than using AT X  X  approach, which evenly splits each document into author and document words.

Fourth, DADT defines the process that generates authors . This allows us to consider the number of texts by each author when performing AA. This also enables the potential use of DADT in a semi-supervised setting by training on unlabelled texts, which we plan to explore in the future. We experimented with the following AA methods, using token frequency features, which are good pre-dictors of authorship (Koppel et al., 2009). Baseline: Support Vector Machines (SVMs). Koppel et al. (2009) showed that SVMs yield good AA performance. We use linear SVMs in a one-versus-all setup, as implemented in LIBLIN-EAR (Fan et al., 2008), reporting results obtained with the best cost parameter values.
 Baseline: LDA + Hellinger (LDA-H). This ap-proach uses the Hellinger distances of topic dis-tributions to assign test texts to the closest author. In (Seroussi et al., 2011), we experimented with two variants: (1) each author X  X  texts are concatenated be-fore building the LDA model; and (2) no concate-nation is performed. We found that the latter ap-proach performs poorly in cases with many candi-date authors. Hence, we use only the former ap-proach in this paper. Note that when dealing with single-authored texts, concatenating each author X  X  texts yields an LDA model that is equivalent to AT. AT. Given an inferred AT model (Rosen-Zvi et al., 2004), we calculate the probability of the test text words for each author a , assuming it was written by a , and return the most probable author. We do not know of any other studies that used AT in this man-ner for single-authored AA. We expect this method to outperform LDA-H as it employs AT directly, rather than relying on an external distance measure. AT-FA. Same as AT, but built with an additional unique  X  X ictitious X  author for each document. DADT. Given our DADT model, we assume that the test text was written by a  X  X ew X  author, and infer this author X  X  topic distribution, the author/document topic ratio, and the document topic distribution. We then calculate the probability of each author given the model X  X  parameters, the test text words, and the inferred author/document topic ratio and document topic distribution. The most probable author is re-turned. We use this method to avoid inferring the document-dependent parameters separately for each author, which is infeasible when many authors ex-ist. A version that marginalises over these parame-ters will be explored in future work. We compare the performance of the methods on two publicly-available datasets: (1) PAN X 11 : emails with 72 authors (Argamon and Juola, 2011); and (2) Blog: blogs with 19,320 authors (Schler et al., 2006). These datasets represent realistic scenar-ios of AA of user-generated texts with many can-didate authors. For example, Chaski (2005) notes a case where an employee who was terminated for sending a racist email claimed that any person with access to his computer could have sent the email. Experimental Setup. Experiments on the PAN X 11 dataset followed the setup of the PAN X 11 competi-tion (Argamon and Juola, 2011): We trained all the methods on the given training subset, tuned the pa-rameters according to the results on the given valida-tion subset, and ran the tuned methods on the given testing subset. In the Blog experiments, we used ten-fold cross validation as in (Seroussi et al., 2011).
We used collapsed Gibbs sampling to train all the topic models (Griffiths and Steyvers, 2004), run-ning 4 chains with a burn-in of 1,000 iterations. In the PAN X 11 experiments, we retained 8 samples per chain with spacing of 100 iterations. In the Blog experiments, we retained 1 sample per chain due to runtime constraints. Since we cannot average topic distribution estimates obtained from training sam-ples due to topic exchangeability (Steyvers and Grif-fiths, 2007), we averaged the distances and probabil-ities calculated from the retained samples. For test text sampling, we used a burn-in of 100 iterations and averaged the parameter estimates over the next 100 iterations in a similar manner to Rosen-Zvi et al. (2010). We found that these settings yield stable results across different random seed values.
We found that the number of topics has a larger impact on accuracy than other configurable pa-rameters. Hence, we used symmetric topic pri-ors, setting all the elements of  X  ( D ) and  X  ( A ) to min { 0 . 1 , 5 /T ( D ) } and min { 0 . 1 , 5 /T ( A ) tively. 1 For all models, we set  X  w = 0 . 01 for each word w as the base measure for the prior of words in topics. Since DADT allows us to encode our prior knowledge that stopword use is indicative of author-ship, we set  X  ( D ) w = 0 . 01  X  and  X  ( A ) w = 0 . 01 + for all w , where w is a stopword. 2 We set = 0 . 009 , which improved accuracy by up to one percentage point over using = 0 . Finally, we set  X  ( A ) = 4 . 889 and  X  ( D ) = 1 . 222 for DADT. This encodes our prior belief that 0 . 8  X  0 . 15 of each document is com-posed of author words. We found that this yields better results than an uninformed uniform prior of  X  ( A ) =  X  ( D ) = 1 (Seroussi et al., 2012). In addition, we set  X  a = 1 for each author a , yielding smoothed estimates for the corpus distribution of authors  X  .
To fairly compare the topic-based methods, we used the same overall number of topics for all the topic models. We present only the results obtained with the best topic settings: 100 for PAN X 11 and 400 for Blog, with DADT X  X  author/document topic splits being 90/10 for PAN X 11, and 390/10 for Blog. These splits allow DADT to de-noise the author represen-tations by allocating document words to a relatively small number of document topics. It is worth not-ing that AT can be seen as an extreme version of DADT, where all the topics are author topics. A fu-ture extension is to learn the topic balance automat-ically, e.g., in a similar manner to Teh et al. X  X  (2006) method of inferring the number of topics in LDA. Results. Table 1 shows the results of our experi-ments in terms of classification accuracy (i.e., the percentage of test texts correctly attributed to their author). The PAN X 11 results are shown for the val-idation and testing subsets, and the Blog results are shown for a subset containing the 1,000 most prolific authors and for the full dataset of 19,320 authors.
Our DADT model yielded the best results in all cases (the differences between DADT and the other methods are statistically significant according to a paired two-tailed t-test with p &lt; 0 . 05 ). We attribute DADT X  X  superior performance to the de-noising ef-fect of the disjoint topic sets, which appear to yield author representations of higher predictive quality than those of the other models.
 As expected, AT significantly outperformed LDA-H. On the other hand, AT-FA performed much worse than all the other methods on PAN X 11, prob-ably because of the inherent noisiness in using the same topics to model both authors and documents. Hence, we did not run AT-FA on the Blog dataset.
DADT X  X  PAN X 11 testing result is close to the third-best accuracy from the PAN X 11 competi-tion (Argamon and Juola, 2011). However, to the best of our knowledge, DADT obtained the best accuracy for a fully-supervised method that uses only unigram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest accu-racy (65.8%), assumed that all the test texts are given to the classifier at the same time, and used this additional information with a semi-supervised method; while Kern et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies respectively, used var-ious feature types (e.g., features obtained from parse trees). Further, preprocessing differences make it hard to compare the methods on a level playing field. Nonetheless, we note that extending DADT to enable semi-supervised classification and additional feature types are promising future work directions.
While all the methods yielded relatively low accu-racies on Blog due to its size, topic-based methods were more strongly affected than SVM by the transi-tion from the 1,000 author subset to the full dataset. This is probably because topic-based methods use a single model, making them more sensitive to corpus size than SVM X  X  one-versus-all setup that uses one model per author. Notably, an oracle that chooses the correct answer between SVM and DADT when they disagree yields an accuracy of 37.15% on the full dataset, suggesting it is worthwhile to explore ensembles that combine the outputs of SVM and DADT (we tried using DADT topics as additional SVM features, but this did not outperform DADT). This paper demonstrated the utility of using author-aware topic models for AA: AT outperformed LDA, and our DADT model outperformed LDA, AT and SVMs in cases with noisy texts and many authors. We hope that these results will inspire further re-search into the application of topic models to AA. This research was supported in part by Australian Research Council grant LP0883416. We thank Mark Carman for fruitful discussions on topic modelling.
