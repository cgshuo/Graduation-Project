 We consider the problem of document indexing and representa-tion. Recently, Locality Preserving Indexing (LPI) was proposed for learning a compact document subspace. Different from Latent Semantic Indexing which is optimal in the sense of global Euclid-ean structure, LPI is optimal in the sense of local manifold struc-ture. However, LPI is extremely sensitive to the number of dimen-sions. This makes it difficult to estimate the intrinsic dimensional-ity, while inaccurately estimated dimensionality would drastically degrade its performance. One reason leading to this problem is that LPI is non-orthogonal. Non-orthogonality distorts the metric structure of the document space. In this paper, we propose a new algorithm called Orthogonal LPI. Orthogonal LPI iteratively com-putes the mutually orthogonal basis functions which respect the lo-cal geometrical structure. Moreover, our empirical study shows that OLPI can have more locality preserving power than LPI. We compare the new algorithm to LSI and LPI. Extensive experimental results show that Orthogonal LPI obtains better performance than both LSI and LPI. More crucially, it is insensitive to the number of dimensions, which makes it an efficient data preprocessing method for text clustering, classification, retrieval, etc.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods Algorithms, Measurement, Performance, Experimentation, Theory Orthogonal Locality Preserving Indexing, Locality Preserving In-dexing, Document Representation and Indexing, Similarity Mea-sure, Dimensionality Reduction, Vector Space Model Copyright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00.
There are two fundamental problems in document processing: how to represent the documents and how to evaluate their similar-ity. If we denote by document space the set of all the documents, different indexing algorithms see different structures of the docu-ment space. The Vector Space Model (VSM) might be one of the most popular model for document representation. Each document is represented as a bag of words . Correspondingly, the document space is associated with a Euclidean structure and the inner product (or, cosine similarity) is used as the standard similarity measure for documents. Unfortunately, VSM suffers from some problems such as synonymy and polysemy .

Data representation is fundamentally related to the problem of manifold learning [4][18][20] which is an emerging research area. Given a set of high-dimensional data points, manifold learning tech-niques aim at discovering the geometric properties of the data space, such as its Euclidean embedding [4][18][20], intrinsic dimension-ality [13], connected components [21], homology [16], etc. Par-ticularly, learning representation is closely related to the embed-ding problem, while clustering can be thought of as finding con-nected components. Finding an Euclidean embedding of the doc-ument space is the primary focus of our work in this paper. Mani-fold learning techniques can be classified into linear and non-linear techniques. For document processing, we are especially interested in linear techniques due to the consideration of computational com-plexity. However, our algorithm presented in this paper can be eas-ily extended to nonlinear case. The typical linear techniques for document representation include Latent Semantic Indexing [7], It-erative Residual Rescaling [1][2], Non-negative Matrix Factoriza-tion [22], and Locality Preserving Indexing [11].

LSI is originally motivated to deal with the problem of syn-onymy and polysemy . The mathematics behind LSI is the Singu-lar Value Decomposition (SVD). The basis functions obtained by SVD are the eigenvectors of the matrix XX T , where X is the term-document matrix. It would be important to note that LSI is different from Principal Component Analysis (PCA) in that XX T is gener-ally not the data covariance matrix. In fact, this occurs only when the documents has a zero mean. One of the main advantages of LSI is that its basis functions are orthogonal. Therefore, the metric structure in the LSI subspace can be well preserved. LSI received a lot of attentions during these years and many variants of LSI have been proposed [12][14][19].

LSI is optimal in the sense of preserving the global geometric structure of the document space (inner product). However, it might not be optimal in the sense of discrimination. Specifically, LSI might not be optimal in separating documents with different topics. Recently, LPI is proposed to discover the discriminant structure of the document space. It has shown that it can have more discrimina-tive power than LSI. A reasonable assumption behind LPI is that, close inputs should have similar topics. The detailed discriminant analysis of LPI can be found in [11]. Different from LSI, LPI is non-orthogonal. Therefore, it can not preserve the metric struc-ture of the document space and suffers from the problem of dimen-sionality estimation. In fact, inaccurate estimation of the intrinsic dimensionality of the document space would drastically degrade LPI X  X  performance. In the worst case, it can even produce worse performance than in the original representation space.
 In this paper, we propose a new algorithm called Orthogonal Locality Preserving Indexing . Orthogonal LPI is fundamentally based on LPI. It shares the same locality preserving character as LPI, but at the same time it requires the basis functions to be or-thogonal. Orthogonal basis functions preserve the metric structure of the document space. In fact, if we use all the dimensions ob-tained by Orthogonal LPI, the projective map is simply a rotation map which does not distort the metric structure. Therefore, the performance of Orthogonal LPI is not sensitive to the number of dimensions. While for LPI, since it does not preserve the metric structure, its performance can be much worse than that in the origi-nal document space if the dimensionality is inaccurately estimated. Moreover, our empirical study shows that Orthogonal LPI can have more locality preserving power than LPI. Since it has been shown that the locality preserving power is directly related to the discrim-inating power [11], the Orthogonal LPI is expected to have more discriminating power than LPI.

The rest of the paper is organized as follows: in Section 2, we give a brief review of LSI and LPI. Section 3 introduces our al-gorithm. We provide a theoretical justification of our algorithm in Section 4. Extensive experimental results on document similarity, local structure of document space and clustering are presented in Section 5. Finally, we provide some concluding remarks and sug-gestions for future work in Section 6.
LSI is one of the most popular algorithms for document index-ing. It is fundamentally based on SVD (Singular Value Decompo-sition). Given a set of documents { x 1 , , x n } X  R m , they can be represented as a term-document matrix X = [ x 1 , x 2 , , Suppose the rank of X is r , LSI decompose the X using SVD as follow: Where  X  = diag (  X  1 , ,  X  r ) and  X  1  X   X  2  X  X  X   X  r are the singular values of X , U = [ a 1 , , a r ] and a i is called left singu-lar vectors, V = [ v 1 , , v r ] and v i is called right singular vectors. LSI use the first k vectors in U as the transformation matrix to em-bed the original document into a k dimensional subspace. It can be easily checked that the column vectors of U are the eigenvectors of features and at the same time the reconstruction error can be min-imized. Let a be the transformation vector and y i = a T x objective function of LSI can be stated below: with the constraint onal. It would be important to note that XX T becomes the data covariance matrix if the data points have a zero mean, i.e. where e = (1 , , 1) . In such a case, LSI is identical to Principal Component Analysis [9]. More details on theoretical interpreta-tions of LSI using SVD can refer to [3][8][17].

Different from LSI which aims to extract the most representa-tive features, LPI aims to extract the most discriminative features. Given a similarity matrix S , LPI can be obtained by solving the following minimization problem: with the constraint where L = D  X  S is the graph Laplacian [6] and D ii = D ii measures the local density around x i . LPI constructs the simi-larity matrix S as:
S Thus, the objective function in LPI incurs a heavy penalty if neigh-boring points x i and x j are mapped far apart. Therefore, minimiz-ing it is an attempt to ensure that if x i and x j are  X  X lose X  then functions of LPI are the eigenvectors associated with the smallest eigenvalues of the following generalized eigen-problem: LPI, thus, the basis functions of LPI can also be regarded as the eigenvectors of the matrix ( XDX T )  X  1 XLX T associated with the smallest eigenvalues. Since ( XDX T )  X  1 XLX T is not symmetric in general, the basis functions of LPI are non-orthogonal.
Once the eigenvectors are computed, let A k = [ a 1 , , a the transformation matrix. Thus, the Euclidean distance between two data points in the reduced space can be computed as follows: If
A is an orthogonal matrix, AA T = I and the metric structure is preserved.
In this Section, we introduce a novel algorithm for document indexing and representation, called Orthogonal LPI. The theoretical justifications of our algorithm will be presented in Section 4.
In the document analysis and processing problems one is often confronted with the fact that the dimension of the document vec-tor ( m ) is much larger than the number of documents ( n lem, we can apply PCA to project the documents into a subspace without losing any information and the matrix XDX T becomes non-singular.

The algorithmic procedure of OLPI is stated below: 1. PCA Projection : We project the document set x i into the 2. Constructing the adjacency graph : Let G denote a graph 3. Choosing the weights : If node i and j are connected, put 4. Computing the Orthogonal Locality Preserving Projec-5. OLPI Embedding : Let W OLP I = [ a 1 , , a l ] , the embed-
In this section, we provide theoretical justifications of our pro-posed algorithm. We begin with the following definition: Preserving Function f is defined as follows: The Locality Preserving Function f ( a ) evaluates the locality pre-serving power of the projective map a . Directly minimizing this function will lead to the original LPI algorithm. Our OLPI algo-rithm tries to find a set of orthogonal basis vectors which minimizes the locality preserving function.

Thus the objective function of OLPI is: with the constraint a , we can always normalize it such that a T XDX T a = 1 , and the above minimization problem is equivalent to minimizing the value Note that, the above normalization is only for simplifying the com-putation. Once we get the optimal solutions, we can re-normalize them to get a othonormal basis vectors.

It is easy to check that a 1 is the eigenvector of the generalized eigen-problem: associated with the smallest eigenvalue. Since XDX T is non-singular, a 1 is the eigenvector of the matrix ( XDX T )  X  associated with the smallest eigenvalue.

In order to get the k -th basis vector, we minimize the following objective function: with the constraints:
We can use the Lagrange multipliers to transform the above ob-jective function to include all the constraints The optimization is performed by setting the partial derivative of C Comparing to (1),  X  exactly represents the expression to be mini-mized. We define: Using this simplified notation, the previous set of k  X  1 can be represented in a single matrix relationship thus  X  Let us now multiply the left side of (2) by ( XDX T )  X  1 This can be expressed using matrix notation as With equation (4), we obtain As shown in (3),  X  is just the criterion to be minimized, thus a the eigenvector of associated with the smallest eigenvalue of M ( k ) .

Finally, we get the optimal orthogonal basis vectors. The orthog-onal basis of OLPI preserves the metric structure of the document space.

Recall in LPI [11], the basis vectors of LPI is the first tors associated with the smallest eigenvalues of the eigen-problem: Thus, the basis vectors satisfy the following constraint: The transformation of LPI is non-orthogonal. Actually, it is orthogonal. Both LPI and OLPI try to preserve the local geometric structure. They find the basis vectors by minimizing the Locality Preserving Function: f ( a ) reflects the locality preserving power of the projective map a .
In LPI, based on the Rayleigh Quotient format of the eigen-problem (Eqn. 5) [10], the value of f ( a ) is exactly the eigenvalue of Eqn. (5) corresponding to eigenvector a . Therefore, the eigen-values of LPI reflect the locality preserving power of LPI. In OLPI, as we show in Eqn. (3), the eigenvalues of OLPI also reflect its lo-cality preserving power. This observation motivates us to compare the eigenvalues of LPI and OLPI.

Fig. 1 shows the eigenvalues of LPI and OLPI. The data set used for this study is the document set  X  X ir X  in Table 2 (please see Sec-tion 5.2.1 for details). As can be seen, the eigenvalues of OLPI is consistently smaller than those of LPI, which indicates that OLPI can have more locality preserving power than LPI. We also did ex-periments on the other 29 document sets in Table 2 and get the similar results.

Since it has been shown in [11] that the locality preserving power is directly related to the discriminating power, we expect that the OLPI based applications on document processing can obtain better performance than those based on LPI.
In this section, several experiments on TDT2 data corpus were performed to show the effectiveness of our proposed algorithm. We compared our proposed algorithm Orthogonal LPI with LSI and LPI. of 1998 and taken from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI) and 2 television programs (CNN, ABC). It consists of 11201 on-topic documents which are classified into 96 semantic categories. In this experiment, those documents appearing in two or more categories were removed, and only the largest 20 categories were kept, thus leaving us with 8741 documents in total as described in table 1. Each document is rep-resented as a term-frequency vector. We simply removed the stop http://www.nist.gov/speech/tests/tdt/tdt98/index.html Table 1: 20 semantic categories from TDT2 used in our experi-ments word and no further preprocessing was done. Each document vec-tor is normalized to 1 and the Euclidean distance is used as the distance measure.
From the &lt; title &gt; field of 300 TREC ad hoc topics (topic 251 we chose 30 keywords that appear in our data collection with high-est frequencies, say, q i ( i = 1 , 2 , , 30) . For each keyword let D i denote the set of the documents containing q i . Let D 1  X  X  D 30 . Finally, we get 30 document subsets and each subset contains multiple topics. Note that, these subsets are not necessarily disjoint. The numbers of documents of these 30 docu-ment subsets ranged from 256 to 805 with an average of 507, and the number of topics ranged from 6 to 20 with an average of 16.7 (Table 2). The reason for generating such 30 document subsets is to split the data collection into small subsets so that we can compare our algorithm to LSI and LPI on each subset. In fact, the key-words can be thought of as queries in information retrieval. Thus, the comparison can be thought of as being performed on different queries [11].

The accuracy of similarity measure plays a crucial role in most of the information processing tasks, such as document clustering, classification, retrieval, etc. In this subsection, we evaluate the ac-curacy of similarity measure using three different indexing algo-rithms, i.e. OLPI, LPI and LSI. The similarity measure we used is the cosine similarity.

For the original document set D , we compute its lower dimen-sional representations D OLP I , D LP I and D LSI by using OLPI, Figure 2: The average precisions of LSI, LPI and OLPI vary with the dimensionality reduction rate. The optimal perfor-mances obtained by both LPI and OLPI are better than LSI. Also, OLPI is less sensitive to the dimensionality reduction rate than LPI.
 LPI and LSI respectively. Similarly, D OLP I consists of 30 subsets, D D
LSI, 30 . We take the number of nearest neighbors for the OLPI and LPI algorithm to be 7.
 For each document subset D i (or, D OLP I,i , D LP I,i , D we evaluate the similarity measure between the documents in Intuitively, we expect that similarity should be higher for any doc-ument pair related to the same topic (intra-topic pair) than for any pair related to different topics (cross-topic pair). Therefore, we adopted the average precision used in TREC, regarding an intra-topic pair as a relevant document and the similarity value as the ranking score. Specifically, we denote by p i the document pair which has the i -th highest similarity value among all pairs of doc-uments in the document set D i . For each intra-topic pair precision is evaluated as follows: The average of the precision values over all intra-topic pairs in was computed as the average precision of D i . Note that, the defin-ition of precision we used here is the same as that used in [1][11].
The experimental results on similarity are reported in this sub-section. We compared OLPI (corresponding to document set D to LPI (corresponding to document set D LP I ), LSI (corresponding to document set D LSI ) and the original document representation (corresponding to document set D as baseline algorithm). In gen-eral, the performance of OLPI, LPI and LSI varies with the number of dimensions. We compared their results on different dimensions. Figure 2 shows the average precision over 30 document sets (Ta-ble 2) with different dimensionality reduction rates. In Figure 2(a), the rate ranges from 0.3% to 100%. In Figure 2(b), the rate ranges from 0.3% to 10%. Figure 2(b) provide us with a better view of the performance changes when the dimensionality is small.
 As can be seen from Figure 2, the best performances of both LPI and OLPI are better than baseline. However, the LPI is very sensitive to the dimensionality, which makes the dimensionality es-timation extremely crucial in LPI. When the dimensionality is in-accurately estimated, the performance of LPI can be much worse than the baseline. The orthogonal basis functions of OLPI preserve the metric structure of the document space. Moreover, it has more locality preserving power (or, discriminating power) than LPI, as we have shown in Figure 1. Also, it can be seen that, both LPI and OLPI outperform LSI when the dimensionality is small.

By using LPI or OLPI, we can obtain an extremely low dimen-sional representation for documents, which might facilitate some real world applications such as clustering, classification and re-trieval. And the insensitivity to the dimension makes OLPI more applicable than LPI. As pointed in [11], LPI is an unsupervised approximation to LDA algorithm which is supervised. Orthogonal LPI share the sim-ilar objective function with LPI. It is intrinsically similar with LPI. Thus Orthogonal LPI also has discriminating power. Meanwhile, the orthogonal basis in OLPI make it less sensitive to the reduced dimensionality.
 In many cases, the data points (documents) may lack of labels. Specifically, for each data point, we do not know to what specific topic it is related to. However, it might be possible to discover the discriminant structure hidden in the data points. In other words, it might be possible to know if two data points are related to the same topic. In the context of learning theory, it is often assumed that if two points x 1 , x 2 are close in the intrinsic geometry of the data space, then they are related to the same topic [5]. In this section, we evaluate the discriminating power of OLPI, LPI and LSI. The dataset we used here is the same as that used in Section 5.2.1.
For each document subset, we project the documents into a sub-space by using OLPI, LPI, LSI and baseline algorithm. For the baseline algorithm, we simply use SVD to remove those compo-nents corresponding to zero eigenvalue. In other words, the base-line algorithm preserves inner product and there is no information loss, while the dimensionality is reduced. Let n denote the number of data points in the subset and c denote the number of semantic classes contained in this subset. For each semantic class, let denote the number of data points in the i -th class. Let x j sample in the i -th semantic class. For each data point x j p nearest neighbors in the subspace. Among these p i data points, Correspondingly, the average precision can be computed:
As before, the accuracy varies with the dimensionality reduc-tion rates as shown in Figure 3. The rate in Figure 3(a) ranges from 0.3% to 100% and the rate in Figure 3(b) ranges from 0.3% to 10%. As can be seen, the optimal performances obtained by both LPI and OLPI are better than LSI. Also, OLPI is less sensitive to the dimen-sionality reduction rate than LPI. Therefore, OLPI can work more stably than LPI in the real world applications, such as document clustering.
Document clustering is one of most crucial techniques to orga-nize the documents in an unsupervised manner. In this subsection, we investigate the use of indexing algorithms for document cluster-ing.

We chose K-means as our clustering algorithm and compared four methods. These four methods are listed below: Figure 3: The average accuracies of LSI, LPI and OLPI vary with the dimensionality reduction rate. The optimal perfor-mances obtained by both LPI and OLPI are better than LSI. Also, OLPI is less sensitive to the dimensionality reduction rate than LPI. Note that, the two methods LPI and OLPI need to construct a graph on the documents. In this experiment, we used the same graph for these two methods and the parameter p (number of nearest neigh-bors) was set to 7.
The clustering result is evaluated by comparing the obtained la-bel of each document with that provided by the document corpus. Two metrics, the accuracy ( AC ) and the normalized mutual in-formation metric ( M I ) are used to measure the clustering perfor-mance [22]. Given a document x i , let r i and s i be the obtained cluster label and the label provided by the corpus, respectively. The AC is defined as follows: where n is the total number of documents and  X  ( x, y ) is the delta function that equals one if x = y and equals zero otherwise, and map( r i ) is the permutation mapping function that maps each clus-ter label r i to the equivalent label from the data corpus. The best mapping can be found by using the Kuhn-Munkres algorithm [15].
Let C denote the set of clusters obtained from the ground truth and C  X  obtained from our algorithm. Their mutual information metric M I ( C, C  X  ) is defined as follows: where p ( c i ) and p ( c  X  trarily selected from the corpus belongs to the clusters c respectively, and p ( c i , c  X  ily selected document belongs to the clusters c i as well as the same time. In our experiments, we use the normalized mutual information M I as follows: where H ( C ) and H ( C  X  ) are the entropies of C and C  X  M I = 1 if the two sets of clusters are identical, and M I = 0 if the two sets are independent.
The evaluations were conducted with different number of clus-ters, ranging from 2 to 10. For each given cluster number tests were conducted on different randomly chosen categories, and the average performance was computed over these 50 tests. For each test, K-means algorithm was applied 10 times with different start points and the best result in terms of the objective function of K-means was recorded.

Figure 4 shows the average accuracy and average mutual infor-mation for different number of classes (different k ). Both LPI and OLPI reach their best performance at very low dimensionality. Af-ter the optimal dimension, the performance of LPI decreases dras-tically. For OLPI, its performance fluctuates slightly and is always above the performance of the baseline. For LSI, the clustering per-formance does not outperform the baseline.
We summarize the experiments below: 1. The low dimensionality of the document subspace obtained 2. The discriminating power and orthogonal basis functions are
We have proposed a new algorithm for document indexing and representation, called Orthogonal Locality Preserving Indexing. The new algorithm combines the advantages of both Latent Semantic Indexing and Locality Preserving Indexing. As shown in our ex-periment results, Orthogonal LPI can have as much discriminative power as the standard LPI, while it does not suffer from the prob-lem of dimensionality estimation.

Several questions remain unclear and will be investigated in our future work: 1. In most of previous work on document indexing, it is as-2. Orthogonal LPI is linear, but it can be also performed in re-[1] R. Ando. Latent semantic space: Iterative scaling improves [2] R. Ando and L. Lee. Iterative residual rescaling: An analysis [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Latent [4] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral [5] M. Belkin, P. Niyogi, and V. Sindhwani. On maniold [6] F. R. K. Chung. Spectral Graph Theory , volume 92 of [7] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [8] C. H. Ding. A similarity-based probability model for latent [9] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern [10] G. H. Golub and C. F. V. Loan. Matrix computations . Johns [11] X. He, D. Cai, H. Liu, and W.-Y. Ma. Locality preserving [12] T. Hofmann. Probabilistic latent semantic indexing. In [13] B. Kegl. Intrinsic dimension estimation using packing [14] E. Kokiopoulou and Y. Saad. Polynomial filtering in latent [15] L. Lovasz and M. Plummer. Matching Theory . Akad  X  e [16] P. Niyogi, S. Smale, and S. Weinberger. Finding the [17] C. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala. [18] S. Roweis and L. Saul. Nonlinear dimensionality reduction [19] C. Tang, S. Dwarkadas, and Z. Xu. On scaling latent [20] J. Tenenbaum, V. de Silva, and J. Langford. A global [21] U. von Luxburg, O. Bousquet, and M. Belkin. Limits of [22] W. Xu, X. Liu, and Y. Gong. Document clustering based on
