 Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maxi-mizing the minimum margin , i.e., the smallest distance from the instances to the classification boundary. Recent theo-retical results, however, disclosed that maximizing the min-imum margin does not necessarily lead to better general-ization performances, and instead, the margin distribution has been proven to be more crucial. In this paper, we pro-pose the Large margin Distribution Machine (LDM), which tries to achieve a better generalization performance by opti-mizing the margin distribution. We characterize the margin distribution by the first-and second-order statistics, i.e., the margin mean and variance. The LDM is a general learning approach which can be used in any place where SVM can be applied, and its superiority is verified both theoretically and empirically in this paper.
 I.2.6 [ Arti cial Intelligence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology X  classifier design and evaluation ; H.2.8 [ Database Management ]: Database Ap-plications X  Data mining Margin distribution; minimum margin; classification
Support Vector Machine (SVM) [5, 26] has always been one of the most successful learning algorithms. The basic idea is to identify a classification boundary having a large margin for all the training examples, and the resultant opti-mization can be accomplished by a quadratic programming (QP) problem. Although SVMs have a long history of lit-eratures, there are still great efforts [16, 6, 25, 14, 8] on improving SVMs.

It is well known that SVM can be viewed as a learning ap-proach trying to maximize over training examples the min-imum margin , i.e., the smallest distance from the examples to the classification boundary, and the margin theory [26] provided a good support to the generalization performance of SVM. It is noteworthy that the margin theory not only plays an important role for SVMs, but also has been ex-tended to interpret the good generalization of many other learning approaches, such as AdaBoost [10], a major repre-sentative of ensemble methods [31]. Specifically, Schapire et al. [21] first suggested margin theory to explain the phe-nomenon that AdaBoost seems resistant to overfitting; soon after, Breiman [4] indicated that the minimum margin is crucial and developed a boosting-style algorithm, Arc-gv, which is able to maximize the minimum margin but with a poor generalization performance. Later, Reyzin et al. [20] found that although Arc-gv tends to produce larger min-imum margin, it suffers from a poor margin distribution; they conjectured that the margin distribution, rather than the minimum margin, is more crucial to the generalization performance. Such a conjecture has been theoretically s-tudied [27, 11], and it was recently proven by Gao and Zhou [11]. Moreover, it was disclosed that rather than simply con-sidering a single-point margin, both the margin mean and variance are important [11]. All these theoretical studies, however, focused on boosting-style algorithms, whereas the influence of the margin distribution for SVMs in practice has not been well exploited.
 In this paper, we propose the Large margin Distribution Machine (LDM), which tries to achieve strong generalization performance by optimizing the margin distribution. Inspired by the recent theoretical result [11], we characterize the mar-gin distribution by the first-and second-order statistics, and try to maximize the margin mean and minimize the margin variance simultaneously. For optimization, we propose a d-ual coordinate descent method for kernel LDM, and propose an averaged stochastic gradient descent (ASGD) method for large scale linear kernel LDM. Comprehensive experiments on twenty regular scale data sets and twelve large scale data sets show the superiority of LDM to SVM and many state-of-the-art methods, verifying that the margin distribution is more crucial for SVM-style learning approaches than mini-mum margin.

The rest of this paper is organized as follows. Section 2 introduces some preliminaries. Section 3 presents the LDM. Section 4 reports on our experiments. Section 5 discusses about some related works. Finally, Section 6 concludes.
We denote by X  X  R d the instance space and Y = { +1 ,  X  1 the label set. Let D be an unknown (underlying) distribu-tion over X  X Y . A training set of size m is drawn identically and independently (i.i.d.) according to the distribution D . Our goal is to learn a function which is used to predict the labels for future unseen instances.
For SVMs, f is regarded as a linear model, i.e., f ( x ) = w  X   X  ( x ) where w is a linear predictor,  X  ( x ) is a feature map-ping of x induced by a kernel k , i.e., k ( x i , x j ) =  X  ( x According to [5, 26], the margin of instance ( x i , y i ) is for-mulated as From [7], it is shown that in separable cases where the train-ing examples can be separated with the zero error, SVM with hard-margin (or Hard-margin SVM), is regarded as the maximization of the minimum margin {
In non-separable cases where the training examples cannot be separated with the zero error, SVM with soft-margin (or Soft-margin SVM) is posed, where = [  X  1 , . . . ,  X  m ]  X  measure the losses of instances, and C is a trading-off parameter. There exists a constant  X  C such that (2) can be equivalently reformulated as, where  X  0 is a relaxed minimum margin, and  X  C is the trading-off parameter. Note that  X  0 indeed characterizes the top-p minimum margin [11]; hence, SVMs (with both hard-margin and soft-margin) consider only a single-point margin and have not exploited the whole margin distribution.
In this section, we first formulate the margin distribution, and then present the optimization algorithms and the theo-retical guarantee.
The two most straightforward statistics for characteriz-ing the margin distribution are the first-and second-order statistics, that is, the mean and the variance of the mar-gin. Formally, denote X as the matrix whose i -th column is  X  ( x i ), i.e., X = [  X  ( x 1 ) . . .  X  ( x m )], y = [ y a column vector, and Y is a m  X  m diagonal matrix with y , . . . , y m as the diagonal elements. According to the defi-nition in (1), the margin mean is and the margin variance is Inspired by the recent theoretical result [11], LDM attempt-s to maximize the margin mean and minimize the margin variance simultaneously.

We first consider a simpler scenario, i.e., the separable cases where the training examples can be separated with the zero error. In these cases, the maximization of the margin mean and the minimization of the margin variance leads to the following hard-margin LDM, where  X  1 and  X  2 are the parameters for trading-off the mar-gin variance, the margin mean and the model complexity. It X  X  evident that the hard-margin LDM subsumes the hard-margin SVM when  X  1 and  X  2 equal 0.

For the non-separable cases, similar to soft-margin SVM, the soft-margin LDM leads to Similarly, soft-margin LDM subsumes the soft-margin SVM if  X  1 and  X  2 both equal 0. Because the soft-margin SVM often performs much better than the hard-margin one, in the following we will focus on soft-margin LDM and if without clarification, LDM is referred to the soft-margin LDM.
We in this section first present a dual coordinate descen-t method for kernel LDM, and then present an average s-tochastic gradient descent (ASGD) method for large scale linear kernel LDM.
By substituting (3)-(4), (5) leads to the following quadrat-ic programming problem, min s.t. y i w  X   X  ( x i )  X  1  X   X  i , (6) is often intractable due to the high or infinite dimen-sionality of  X  (  X  ). Fortunately, inspired by the representer th eorem in [22], the following theorem states that the opti-mal solution for (6) can be spanned by {  X  ( x i ) , 1  X  i
Theorem 1. The optimal solution w  X  for problem (6) admits a representation of the form where = [  X  1 , . . . ,  X  m ]  X  are the coefficients.
Proof. w  X  can be decomposed into a part that lives in the span of  X  ( x i ) and an orthogonal part, i.e., for some = [  X  1 , . . . ,  X  m ]  X  and v satisfying  X  ( x for all j , i.e., X  X  v = 0 . Note that so the second and the third terms of (6) are independent of v ; further note that the constraint is also independent of v , thus the last terms of (6) is also independent of v .
As for the first term of (6), since X  X  v = 0 , consequently we get with equality occurring if and only if v = 0 .

So, setting v = 0 does not affect the second, the third and the last term while strictly reduces the first term of (6). Hence, w  X  for problem (6) admits a representation of the form (7).
 Acc ording to Theorem 1, we have where G = X  X  X is the kernel matrix. Let G : i denote the i -th column of G , then (6) can be cast as where Q = 4  X  1 ( m G  X  G  X  ( Gy )( Gy )  X  ) /m 2 + G and p =  X   X  2 Gy /m . By introducing the lagrange multipliers = [  X  second constraints respectively, the Lagrangian of (8) leads to
L ( , , , ) = 1 By setting the partial derivations of { , } to zero, we have Al gorithm 1 Kernel LDM Inp ut: Data set X ,  X  1 ,  X  2 , C Output:
Initialize = 0 , = 2 m Q  X  1 Gy , A = Q  X  1 GY , h ii = while not converge do end while By substituting (10) and (11) into (9), the dual 1 of (8) can be cast as: where H = Y GQ  X  1 GY , Q  X  1 refers to the inverse matrix of Q and e stands for the all-one vector. Due to the simple decoupled box constraint and the convex quadratic objec-tive function, as suggested by [29], (12) can be efficiently solved by the dual coordinate descent method. In dual co-ordinate descent method [13], one of the variables is selected to minimize while the other variables are kept as constants at each iteration, and a closed-form solution can be achieved at each iteration. Specifically, to minimize  X  i by keeping the other  X  j  X  = i  X  X  as constants, one needs to solve the following subproblem, where e i denotes the vector with 1 in the i -th coordinate and 0 X  X  elsewhere. Let H = [ h ij ] i;j =1 ;:::;m , we have where [  X  f ( )] i is the i -th component of the gradient Note that f ( ) is independent of t and thus can be dropped. Considering that f ( + t e i ) is a simple quadratic function of t , and further note the box constraint 0  X   X  i  X  C , the minimizer of (13) leads to a closed-form solution, Algorithm 1 summarizes the pseudo-code of kernel LDM.
For prediction, according to (10), one can obtain the co-efficients from the optimal  X  as
H ere we omit constants without influence on optimization. He nce for testing instance z , its label can be obtained by
In section 3.2.1, the proposed method can efficiently deal with kernel LDM. However, the inherent computational cost for the kernel matrix in kernel LDM takes O ( m 2 ) time, which might be computational prohibitive for large scale problems. To make LDM more useful, in the following, we present a fast linear kernel LDM for large scale problems by adopting the average stochastic gradient descent (ASGD) method [19].

For linear kernel LDM, (5) can be reformulated as the following form, min tor.

For large scale problems, computing the gradient of (14) is expensive because its computation involves all the train-ing examples. Stochastic gradient descent (SGD) works by computing a noisy unbiased estimation of the gradient via sampling a subset of the training examples. Theoretical-ly, when the objective is convex, it can be shown that in expectation, SGD converges to the global optimal solution [15, 3]. During the past decade, SGD has been applied to various machine learning problems and achieved promising performances [30, 23, 2, 24].

The following theorem presents an approach to obtain an unbiased estimation of the gradient  X  g ( w ).

Theorem 2. If two examples ( x i , y i ) and ( x j , y j ) are sam-pled from training set randomly, then where I  X  { i | y i w  X  x i &lt; 1 } is an unbiased estimation of  X  g ( w ) .

Proof. Note that the gradient of g ( w ) is where Q = 4  X  1 ( m XX  X   X  Xy ( Xy )  X  ) /m 2 + I and p =  X   X  2 Xy /m . Further note that According to the linearity of expectation, the independence Al gorithm 2 Large Scale Kernel LDM Inp ut: Data set X ,  X  1 ,  X  2 , C Output:  X  w
Initialize u = 0 , T = 5; for t = 1 , . . . T m do end for b etween x i and x j , and with (16), we have It is shown that  X  g ( w , x i , x j ) is a noisy unbiased gradient of g ( w ).

Wit h Theorem 2, the stochastic gradient update can be formed as where  X  t is a suitably chosen step-size parameter in the t -th iteration.

In practice, we use averaged stochastic gradient descent (ASGD) which is more robust than SGD [28]. At each it-eration, besides performing the normal stochastic gradient update (17), we also compute wh ere t 0 determines when we engage the averaging process. This average can be computed efficiently using a recursive formula: where  X  t = 1 / max { 1 , t  X  t 0 } .

Algorithm 2 summarizes the pseudo-code of large scale kernel LDM. In this section, we study the statistical property of LDM. Specifically, we derive a bound on the expectation of error for LDM according to the leave-one-out cross-validation es-timate, which is an unbiased estimate of the probability of test error.

Here we only consider the linear case (14) for simplicity, however, the results are also applicable to any other feature ma pping  X  . Following the same steps in Section 3.2.1, one can have the dual problem of (14), i.e., where H = Y X  X  Q  X  1 XY , Q = 4 1 m 2 ( m X X  X   X  Xy ( Xy ) I , e stands for the all-one vector and Q  X  1 refers to the in-verse matrix of Q .

Theorem 3. Let denote the optimal solution of (18), and E [ R ( )] be the expectation of the probability of error, then we have where I 1  X  { i | 0 &lt;  X  i &lt; C } , I 2  X  { i |  X  i = C h = max { diag { H }} .

Proof. Suppose and the corresponding solution for the linear kernel LDM are w  X  and w i , respectively.
 As shown in [17], the leave-one-out procedure. Note that if  X   X  i = 0, ( x i will always be classified correctly in the leave-one-out proce-dure according to the KKT conditions. So for any misclassi-fied example ( x i , y i ), we only need to consider the following two cases: 1) 0 &lt;  X   X  i &lt; C , according to the definition in (20), we have where e i denotes a vector with 1 in the i -th coordinate and 0 X  X  elsewhere. We can find that, the left-hand side of (22) (23) is equal to  X   X  i 2 h ii / 2. So by combining (22) and (23), we have Further note that y i x  X  i w i &lt; 0, rearranging the above we can obtain 1  X   X   X  i h ii . 2)  X   X  i = C , all these examples will be misclassified in the leave-one-out procedure.
 So we have where I 1  X  { i | 0 &lt;  X   X  i &lt; C } , I 2  X  { i |  X   X  h = max { h ii , i = 1 , . . . , m } . Take expectation on both side and with (21), we get that (19) holds.
In this section, we empirically evaluate the effectiveness of LDM on a broad range of data sets. We first introduce the experimental settings in Section 4.1, and then compare LDM with SVM and three state-of-the-art approaches 2 in Section 4.2 and Section 4.3. In addition, we also study the cumulative margin distribution produced by LDM and SVM in Section 4.4. The computational cost and parameter in-fluence are presented in Section 4.5 and Section 4.6, respec-tively.
We evaluate the effectiveness of our proposed LDMs on twenty regular scale data sets and twelve large scale data sets, including both UCI data sets and real-world data sets like KDD2010 3 . Table 1 summarizes the statistics of these data sets. The data set size is ranged from 106 to more than 8,000,000, and the dimensionality is ranged from 2 to more than 20,000,000, covering a broad range of properties. All features are normalized into the interval [0 , 1]. For each data set, half of examples are randomly selected as the training data, and the remaining examples are used as the testing data. For regular scale data sets, both linear and RBF ker-nels are evaluated. Experiments are repeated for 30 times with random data partitions, and the average accuracies as well as the standard deviations are recorded. For large s-cale data sets, linear kernel is evaluated. Experiments are repeated for 10 times with random data partitions, and the average accuracies (with standard deviations) are recorded.
LDMs are compared with standard SVMs which ignore the margin distribution, and three state-of-the-art method-s, that is, Margin Distribution Optimization (MDO) [12], Maximal Average Margin for Classifiers (MAMC) [18] and Kernel Method for the direct Optimization of the Margin Distribution (KM-OMD) [1]. For SVM, KM-OMD and LD-M, the regularization parameter C is selected by 5-fold cross validation from [10 , 50 , 100]. For MDO, the parameters are set as the recommended parameters in [12]. For LDM, the regularization parameters  X  1 ,  X  2 are selected by 5-fold cross and t 0 are set with the same setup in [28], and T is fixed to 5. The width of the RBF kernel for SVM, MAMC, KM-OMD and LDM are selected by 5-fold cross validation from the set instances. All selections are performed on training sets.
Tables 2 and 3 summarize the results on twenty regular scale data sets. As can be seen, the overall performance of LDM is superior or highly competitive to SVM and oth-er compared methods. Specifically, for linear kernel, LD-M performs significantly better than SVM, MDO, MAMC, KM-OMD on 12, 9, 17 and 10 over 20 data sets, respec-tively, and achieves the best accuracy on 13 data sets; for RBF kernel, LDM performs significantly better than SVM, MAMC, KM-OMD on 10, 18 and 15 over 20 data sets, re-spectively, and achieves the best accuracy on 15 data sets. MDO is not compared since it is specified for the linear kernel. In addition, as can be seen, in comparing with s-tandard SVM which does not consider margin distribution,
Th ese approaches will be briefly introduced in Section 5. https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp th e win/tie/loss counts show that LDM is always better or comparable, never worse than SVM.
Table 4 summarizes the results on twelve large scale data sets. KM-OMD did not return results on all data sets and MDO did not return results on KDD2010 in 48 hours due to the high computational cost. As can be seen, the overal-l performance of LDM is superior or highly competitive to SVM and other compared methods. Specifically, LDM per-forms significantly better than SVM, MDO, MAMC on 6, 7 and 12 over 12 data sets, respectively, and achieves the best accuracy on 8 data sets. In addition, the win/tie/loss counts show that LDM is always better or comparable, never worse than SVM.
Figure 1 plots the cumulative margin distribution of SVM and LDM on some representative regular scale data sets. The curves for other data sets are similar. The point where a curve and the x -axis crosses is the corresponding mini-mum margin. As can be seen, LDM usually has a little bit smaller minimum margin than SVM, whereas the LD-M curve generally lies on the right side, showing that the not have results since it is speci ed for the linear kernel. and MDO did not return results on some data sets in 48 hours. ma rgin distribution of LDM is generally better than that of SVM. In other words, for most examples, LDM generally produce a larger margin than SVM.
We compare the time cost of LDM and SVM on the twelve large scale data sets. All the experiments are performed with MATLAB 2012b on a machine with 8  X  2.60 GHz CPUs and 16GB main memory. The average CPU time (in seconds) on each data set is shown in Figure 2. We denote SVM imple-mented by the LIBLINEAR [9] package as SVM l and SVM implemented by ASGD 4 as SVM a , respectively. It can be seen that, both SVM a and LDM are much faster than SVM l , owing to the use of ASGD. LDM is just slightly slower than SVM a on three data sets (news20, real-sim and skin) but highly competitive with SVM a on the other nine data sets. Note that both SVM l and SVM a are very fast implementa-h ttp://leon.bottou.org/projects/sgd tions of SVMs; this shows that LDM is also computationally efficient. LDM has three regularization parameters, i.e.,  X  1 ,  X  2 and C . In previous empirical studies, they are set according to cross validation. Figure 3 further studies the influence of them on some representative regular scale data sets by fixing other parameters. Specifically, Figure 3(a) shows the influence of  X  1 on the accuracy by varying it from 2  X  8 2 2 while fixing  X  2 and C as the value suggested by the cross validation described in Section 4.1. Figure 3(b) and Figure 3(c) are obtained in the same way. It can be seen that, the performance of LDM is not very sensitive to the setting of the parameters, making LDM even more attractive in practice.
There are a few studies considered margin distribution in SVM-like algorithms [12, 18, 1]. Garg et al. [12] pro-posed the Margin Distribution Optimization (MDO) algo-rithm which minimizes the sum of the cost of each instance, where the cost is a function which assigns larger values to instances with smaller margins. MDO can be viewed as a method of optimizing weighted margin combination, where the weights are related to the margins. The objective func-tion optimized by MDO, however, is non-convex, and thus, it may get stuck in local minima. In addition, MDO can on-ly be used for linear kernel. As our experiments in Section 4 disclosed, the performance of MDO is inferior to LDM.
Pelckmans et al. [18] proposed the Maximal Average Mar-gin for Classifiers (MAMC) and it can be viewed as a spe-cial case of LDM assuming that the margin variance is zero. MAMC has a closed-form solution, however, it will degener-ate to a trivial solution when the classes are not with equal sizes. Our experiments in Section 4 showed that LDM is clearly superior to MAMC.
 Aiolli et al. [1] proposed a Kernel Method for the direct Optimization of the Margin Distribution (KM-OMD) from a game theoretical perspective. Similar to MDO, this method also directly optimizes a weighted combination of margin-s over the training data, ignoring the influence of margin variances. Besides, this method considers hard-margin only, which may be another reason why it behaves worse than our method. It is noteworthy that the computational cost pro-hibits KM-OMD to be applied to large scale data, as shown in Table 4.
Support vector machines work by maximizing the mini-mum margin. Recent theoretical results suggested that the margin distribution, rather than a single-point margin such as the minimum margin, is more crucial to the generaliza-tion performance. In this paper, we propose the large mar-gin distribution machine (LDM) which tries to optimize the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously. The LDM is a general learning approach which can be used in any place where SVM can be applied. Comprehensive experiments on twenty regular scale data sets and twelve large scale data sets validate the superiority of LDM to SVMs and many state-of-the-art methods. In the future it will be interest-ing to generalize the idea of LDM to regression and other learning settings.
The authors want to thank anonymous reviewers for help-ful comments and suggestions. This research was supported by the National Science Foundation of China (61333014) and the National Key Basic Research Program of China (2014CB340501). [1] F. Aiolli, G. San Martino, and A. Sperduti. A kernel [2] A. Bordes, L. Bottou, and P. Gallinari. Sgd-qn: [3] L. Bottou. Large-scale machine learning with [4] L. Breiman. Prediction games and arcing classifiers. [5] C. Cortes and V. Vapnik. Support-vector networks. [6] A. Cotter, S. Shalev-shwartz, and N. Srebro. Learning [7] N. Cristianini and J. Shawe-Taylor. An Introduction to [8] H. Do and K. Alexandre. Convex formulations of [9] R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, [10] Y. Freund and R. E. Schapire. A decision-theoretic [11] W. Gao and Z.-H. Zhou. On the doubt about margin [12] A. Garg and D. Roth. Margin distribution and [13] C. J. Hsieh, K. W. Chang, C. J. Lin, S. S. Keerthi, [14] C. Jose, P. Goyal, P. Aggrwal, and M. Varma. Local [1 5] H. J. Kushner and G. G. Yin. Stochastic [16] S. Lacoste-julien, M. Jaggi, M. Schmidt, and [17] A. Luntz and V. Brailovsky. On estimation of [18] K. Pelckmans, J. Suykens, and B. D. Moor. A risk [19] B. T. Polyak and A. B. Juditsky. Acceleration of [20] L. Reyzin and R. E. Schapire. How boosting the [21] R. E. Schapire, Y. Freund, P. L. Bartlett, and W. S. [22] B. Sch  X  olkopf and A. Smola. Learning with kernels: [23] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [24] O. Shamir and T. Zhang. Stochastic gradient descent [25] M. Takac, A. Bijral, P. Richtarik, and N. Srebro. [26] V. Vapnik. The Nature of Statistical Learning Theory . [27] L. W. Wang, M. Sugiyama, C. Yang, Z.-H. Zhou, and [28] W. Xu. Towards optimal one pass large scale learning [29] G. X. Yuan, C. H. Ho, and C. J. Lin. Recent advances [30] T. Zhang. Solving large scale linear prediction [31] Z.-H. Zhou. Ensemble Methods: Foundations and
