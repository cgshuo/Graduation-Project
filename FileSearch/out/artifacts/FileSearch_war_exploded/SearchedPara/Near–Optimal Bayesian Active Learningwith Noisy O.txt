 How should we perform experiments to determine the most accurate scientific theory among com-peting candidates, or choose among expensive medical procedures to accurately determine a patient X  X  condition, or select which labels to obtain in order to determine the hypothesis that minimizes general-ization error? In all these applications, we have to sequentially select among a set of noisy, expensive observations (outcomes of experiments, medical tests, expert labels) in order to determine which hy-pothesis (theory, diagnosis, classifier) is most accurate. This fundamental problem has been studied in One way to formalize such active learning problems is Bayesian experimental design [ 6 ], where one assumes a prior on the hypotheses, as well as probabilistic assumptions on the outcomes of tests. The goal then is to determine the correct hypothesis while minimizing the cost of the experimentation. Un-fortunately, finding this optimal policy is not just NP-hard, but also hard to approximate [ 5 ]. Several heuristic approaches have been proposed that perform well in some applications, but do not carry theo-retical guarantees (e.g., [ 18 ]). In the case where observations are noise-free 1 , a simple algorithm, gen-eralized binary search 2 (GBS) run on a modified prior, is guaranteed to be competitive with the optimal policy; the expected number of queries is a factor of O (log n ) (where n is the number of hypotheses) more than that of the optimal policy [15], which matches lower bounds up to constant factors [5]. The important case of noisy observations, however, as present in most applications, is much less well understood. While there are some recent positive results in understanding the label complexity of noisy active learning [ 19 , 1 ], to our knowledge, so far there are no algorithms that are provably competitive with the optimal sequential policy, except in very restricted settings [ 16 ]. In this paper, we introduce a general formulation of Bayesian active learning with noisy observations that we call the Equivalence Class Determination problem. We show that, perhaps surprisingly, generalized binary search performs poorly in this setting, as do greedily (myopically) maximizing the information gain (measured w.r.t. the distribution on equivalence classes) or the decision-theoretic value of information. This motivates us to introduce a novel active learning criterion, and use it to develop a greedy active learning algorithm called the E quivalence C lass E dge C utting algorithm ( EC 2 ), whose expected cost is competitive to that of the optimal policy. Our key insight is that our new objective function satisfies adaptive submodularity [ 9 ], a natural diminishing returns property that generalizes the classical notion of submodularity to adaptive policies. Our results also allow us to relax the common assumption that the outcomes of the tests are conditionally independent given the true hypothesis. We also develop the Eff icient E dge C utting appro X imate objec tive algorithm ( E FF ECX TIVE ), an efficient approximation to EC 2 , and evaluate it on a Bayesian experimental design problem intended to tease apart competing theories on how people make decisions under uncertainty, including Expected Value [ 22 ], Prospect Theory [ 14 ], Mean-Variance-Skewness [ 12 ] and Constant Relative Risk Aversion [ 20 ]. In our experiments, E FF ECX TIVE typically outperforms existing experimental design criteria such as information gain, uncertainty sampling, GBS, and decision-theoretic value of information. Our results from human subject experiments further reveal that E FF ECX TIVE can be used as a real-time tool to classify people according to the economic theory that best describes their behaviour in financial decision-making, and reveal some interesting heterogeneity in the population. In the Bayesian active learning problem, we would like to distinguish among a given set of hypotheses H = { h 1 ,...,h n } by performing tests from a set T = { 1 ,...,N } of possible tests. Running test t incurs a cost of c ( t ) and produces an outcome from a finite set of outcomes X = { 1 , 2 ,...,` } . We let H denote the random variable which equals the true hypothesis, and model the outcome of each test t by a random variable X t taking values in X . We denote the observed outcome of test t by x t . We further suppose we have a prior distribution P modeling our assumptions on the joint probability P ( H,X 1 ,...,X N ) over the hypotheses and test outcomes. In the noiseless case, we assume that the outcome of each test is deterministic given the true hypothesis, i.e., for each h  X  X  , P ( X 1 ,...,X N | H = h ) is a deterministic distribution. Thus, each hypothesis h is associated with a particular vector of test outcomes. We assume, w.l.o.g., that no two hypotheses lead to the same outcomes for all tests. Thus, if we perform all tests, we can uniquely determine the true hypothesis. However in most applications we will wish to avoid performing every possible test, as this is prohibitively expensive. Our goal is to find an adaptive policy for running tests that allows us to determine the value of H while minimizing the cost of the tests performed. Formally, a policy  X  (also called a conditional plan) is a partial mapping  X  from partial observation vectors x A to tests, specifying which test to run next (or whether we should stop testing) for any observation vector x A . Hereby, x A  X  X  A is a vector of outcomes indexed by a set of tests A X  X  that we have performed so far 3 (e.g., the set of labeled examples in active learning, or outcomes of a set of medical tests that we ran). After having made observations x A , we can rule out inconsistent hypotheses. We denote the set of hypotheses consistent with event  X  (often called the version space associated with  X  ) by V ( X ) := { h  X  X  : P ( h |  X ) &gt; 0 } . We call a policy feasible if it is guaranteed to uniquely determine the correct hypothesis. That is, upon termination with observation x A , it must hold that |V ( x A ) | = 1 . We can define the expected cost of a policy  X  by where T (  X ,h )  X  X  is the set of tests run by policy  X  in case H = h . Our goal is to find a feasible policy  X   X  of minimum expected cost, i.e., A policy  X  can be naturally represented as a decision tree T  X  , and thus problem (2.1) is often called the Optimal Decision Tree (ODT) problem.
 Hence, various heuristics are employed to solve the Optimal Decision Tree problem and its variants. Two of the most popular heuristics are to select tests greedily to maximize the information gain (IG) conditioned on previous test outcomes, and generalized binary search (GBS). Both heuristics are greedy, and after having made observations x A will select where Alg  X  { IG , GBS } . Here,  X  IG ( t | x A ) := H ( X T | x A )  X  E x marginal information gain measured with respect to the Shannon entropy H ( X ) := E x [  X  log 2 P ( x )] , and  X  GBS ( t | x A ) := P ( V ( x A ))  X  P x  X  X  P ( X t = x | x A ) P ( V ( x A ,X t = x )) is the expected reduction in version space probability mass. Thus, both heuristics greedily chooses the test that maximizes the benefit-cost ratio, measured with respect to their particular benefit functions. They stop after running a set of tests A such that |V ( x A ) | = 1 , i.e., once the true hypothesis has been uniquely determined.
 It turns out that for the (noiseless) Optimal Decision Tree problem, these two heuristics are equivalent [ 23 ], as can be proved using the chain rule of entropy. Interestingly, despite its myopic nature GBS has been shown [ 15 , 7 , 11 , 9 ] to obtain near-optimal expected cost: the strongest known bound is c (  X  GBS )  X  c (  X   X  ) (ln(1 /p min ) + 1) where p min := min h  X  X  P ( h ) . Let x S ( h ) be the unique vector x S  X  X S such that P ( x S | h ) = 1 . The result above is proved by exploiting the fact that f GBS ( S,h ) := 1  X  P ( V ( x S ( h ))) + P ( h ) is adaptive submodular and strongly adaptively monotone [ 9 ]. Call x A a subvector of x B if A  X  B and P ( x B | x A ) &gt; 0 . In this case we write x
A  X  x B . A function f : 2 T  X H is called adaptive submodular w.r.t. a distribution P , if for any x
A  X  x B and any test t it holds that  X  ( t | x A )  X   X  ( t | x B ) , where Thus, f is adaptive submodular if the expected marginal benefits  X  ( t | x A ) of adding a new test t can only decrease as we gather more observations. f is called strongly adaptively monotone w.r.t. P if, informally,  X  X bservations never hurt X  with respect to the expected reward. Formally, for all A , all t /  X  X  , and all x  X  X  we require E H [ f ( A ,H ) | x A ]  X  E H [ f ( A X  X  t } ,H ) | x A ,X t = x ] . The performance guarantee for GBS follows from the following general result about the greedy algorithm for adaptive submodular functions (applied with Q = 1 and  X  = p min ): Theorem 1 (Theorem 10 of [ 9 ] with  X  = 1 ) . Suppose f : 2 T  X H X  R  X  0 is adaptive submodular and strongly adaptively monotone with respect to P and there exists Q such that f ( T ,h ) = Q for all h . Let  X  be any value such that f ( S,h ) &gt; Q  X   X  implies f ( S,h ) = Q for all sets S and hypotheses h . Then for self X  X ertifying instances the adaptive greedy policy  X  satisfies c (  X  )  X  c (  X   X  ) ln Q  X  + 1 . The technical requirement that instances be self X  X ertifying means that the policy will have proof that it has obtained the maximum possible objective value, Q , immediately upon doing so. It is not difficult to show that this is the case with the instances we consider in this paper. We refer the interested reader to [9] for more detail.
 In the following sections, we will use the concept of adaptive submodularity to provide the first approximation guarantees for Bayesian active learning with noisy observations. We now wish to consider the Bayesian active learning problem where tests can have noisy outcomes. Our general strategy is to reduce the problem of noisy observations to the noiseless setting. To gain intuition, consider a simple model where tests have binary outcomes, and we know that the outcome of exactly one test, chosen uniformly at random unbeknown to us, is flipped. If any pair of hypotheses h 6 = h 0 differs by the outcome of at least three tests, we can still uniquely determine the correct hypothesis after running all tests. In this case we can reduce the noisy active learning problem to the noiseless setting by, for each hypothesis, creating N  X  X oisy X  copies, each obtained by flipping the outcome of one of the N tests. The modified prior P 0 would then assign mass P 0 ( h 0 ) = P ( h ) /N to each noisy copy h 0 of h . The conditional distribution P 0 ( X T | h 0 ) is still deterministic (obtained by flipping the outcome of one of the tests). Thus, each hypothesis h i in the original problem is now associated with a set H i of hypotheses in the modified problem instance. However, instead of selecting tests to determine which noisy copy has been realized, we only care which set H i is realized. The Equivalence Class Determination problem (ECD). More generally, we introduce the Equivalence Class Determination problem 4 , where our set of hypotheses H is partitioned into a set of m equivalence classes {H 1 ,..., H m } so that H = U m i =1 H i , and the goal is to determine which class H i the true hypothesis lies in. Formally, upon termination with observations x A we require that V ( x A )  X  X  i for some i . As with the ODT problem, the goal is to minimize the expected cost of the tests, where the expectation is taken over the true hypothesis sampled from P . In  X  4, we will show how the Equivalence Class Determination problem arises naturally from Bayesian experimental design problems in probabilistic models.
 Given the fact that GBS performs near-optimally on the Optimal Decision Tree problem, a natural ap-proach to solving ECD would be to run GBS until the termination condition is met. Unfortunately, and perhaps surprisingly, GBS can perform very poorly on the ECD problem. Consider an instance with a uniform prior over n hypotheses, h 1 ,...,h n , and two equivalence classes H 1 := { h i : 1  X  i &lt; n } 1 [ X ] is the indicator variable for event  X  . In this case, the optimal policy only needs to select test n , however GBS may select tests 1 , 2 ,...,n in order until running test t , where H = h t is the true hypothesis. Given our uniform prior, it takes n/ 2 tests in expectation until this happens, so that GBS pays, in expectation, n/ 2 times the optimal expected cost in this instance.
 The poor performance of GBS in this instance may be attributed to its lack of consideration for the equivalence classes. Another natural heuristic would be to run the greedy information gain policy, only with the entropy measured with respect to the probability distribution on equivalence classes rather than hypotheses. Call this policy  X  IG . It is clearly aware of the equivalence classes, as it adaptively and myopically selects tests to reduce the uncertainty of the realized class, measured w.r.t. the Shannon entropy. However, we can show there are instances in which it pays  X ( n/ log( n )) times the optimal cost, even under a uniform prior. See the long version of this paper [10] for details. The EC 2 algorithm. The reason why GBS fails is because reducing the version space mass does not necessarily facilitate differentiation among the classes H i . The reason why  X  IG fails is that there are complementarities among tests; a set of tests can be far better than the sum of its parts. Thus, we would like to optimize an objective function that encourages differentiation among classes, but lacks complementarities. We adopt a very elegant idea from Dasgupta [ 8 ], and define weighted edges be-tween hypotheses that we aim to distinguish between. However, instead of introducing edges between arbitrary pairs of hypotheses (as done in [ 8 ]), we only introduce edges between hypotheses in different classes. Tests will allow us to cut edges inconsistent with their outcomes, and we aim to eliminate all inconsistent edges while minimizing the expected cost incurred. We now formalize this intuition. (unordered) pairs of hypotheses belonging to distinct classes. These are the edges that must be cut , by which we mean for any edge { h,h 0 } X  X  , at least one hypothesis in { h,h 0 } must be ruled out (i.e., eliminated from the version space). Hence, a test t run under true hypothesis h is said to cut edges E weight function w : E  X  R  X  0 by w ( { h,h 0 } ) := P ( h )  X  P ( h 0 ) . We extend the weight function to an additive (modular) function on sets of edges in the natural manner, i.e., w ( E 0 ) := P e  X  X  0 w ( e ) . The objective f EC that we will greedily maximize is then defined as the weight of the edges cut (EC): The key insight that allows us to prove approximation guarantees for f EC is that f EC shares the same beneficial properties that make f GBS amenable to efficient greedy optimization. The proof of this fact, as stated in Proposition 2, can be found in the long version of this paper [10].
 Proposition 2. The objective f EC is strongly adaptively monotone and adaptively submodular. Based on the objective f EC , we can calculate the marginal benefits for test t upon observations x A as We call the adaptive policy  X  EC that, after observing x A , greedily selects test t  X  arg max t  X  EC ( t | x A ) /c ( t ) , the EC 2 algorithm (for equivalence class edge cutting ). Figure 1: (a) An instance of Equivalence Class Determination with binary test outcomes, shown with the set of Note that these instances are self X  X ertifying, because we obtain maximum objective value if and only if the version space lies within an equivalence class, and the policy can certify this condition when it holds. So we can apply Theorem 1 to show EC 2 obtains a ln( Q/ X  ) + 1 approximation to Equivalence Class Determination. Hereby, Q = w ( E ) = 1  X  P i ( P ( h  X  H i )) 2  X  1 is the total weight of all edges that need to be cut, and  X  = min e  X  X  w ( e )  X  p 2 min is a bound on the minimum weight among all edges. We have the following result: Theorem 3. Suppose P ( h ) is rational for all h  X  H . For the adaptive greedy policy  X  EC imple-mented by EC 2 it holds that where p min := min h  X  X  P ( h ) is the minimum prior probability of any hypothesis, and  X   X  is the optimal policy for the Equivalence Class Determination problem.
 In the case of unit cost tests, we can apply a technique of Kosaraju et al. [ 15 ], originally developed for the GBS algorithm, to improve the approximation guarantee to O (log n ) by applying EC 2 with a modified prior distribution. We defer details to the full version of this paper. We now address the case of noisy observations, using ideas from  X  3. With noisy observations, the conditional distribution P ( X 1 ,...,X N | h ) is no longer deterministic. We model the noise using an additional random variable  X  . Fig. 1(b) depicts the underlying graphical model. The vector of test outcomes x T is assumed to be an arbitrary, deterministic function x T : H X  supp( X )  X  X  N ; hence X
T | h is distributed as x T ( h,  X  h ) where  X  h is distributed as P (  X  | h ) . For example, there might be up to s = | supp( X ) | ways any particular disease could manifest itself, with different patients with the same disease suffering from different symptoms.
 In cases where it is always possible to identify the true hypothesis, i.e., x T ( h, X  ) 6 = x T ( h 0 , X  0 ) for all h 6 = h 0 and all  X , X  0  X  supp( X ) , we can reduce the problem to Equivalence Class De-termination with hypotheses { x T ( h, X  ) : h  X  X  , X   X  supp( X ) } and equivalence classes H i := { x
T ( h i , X  ) :  X   X  supp( X ) } for all i . Then Theorem 3 immediately yields that the approximation factor of EC 2 is at most 2 ln (1 / min h, X  P ( h, X  )) + 1 , where the minimum is taken over all ( h, X  ) in the support of P . In the unit cost case, running EC 2 with a modified prior ` a la Kosaraju et al. [ 15 ] allows us to obtain an O (log |H| + log | supp( X ) | ) approximation factor. Note this model allows us to incorporate noise with complex correlations.
 However, a major challenge when dealing with noisy observations is that it is not always possible to distinguish distinct hypotheses. Even after we have run all tests, there will generally still be uncertainty about the true hypothesis, i.e., the posterior distribution P ( H | x T ) obtained using Bayes X  rule may still assign non-zero probability to more than one hypothesis. If so, uniquely determining the true hypothesis is not possible. Instead, we imagine that there is a set D of possible decisions we may make after (adaptively) selecting a set of tests to perform and we must choose one (e.g., we must decide how to treat the medical patient, which scientific theory to adopt, or which classifier to use, given our observations). Thus our goal is to gather data to make effective decisions [ 13 ]. Formally, for any decision d  X  X  we take, and each realized hypothesis h , we incur some loss ` ( d,h ) . Decision theory recommends, after observing x A , to choose the decision d  X  that minimizes the risk , i.e., the expected loss, namely d  X   X  arg min d E H [ ` ( d,H ) | x A ] . A natural goal in Bayesian active learning is thus to adaptively pick observations, until we are guaranteed to make the same decision (and thus incur the same expected loss) that we would have made had we run all tests. Thus, we can reduce the noisy Bayesian active learning problem to the ECD problem by defining the equivalence classes over all test outcomes that lead to the same minimum risk decision. Hence, for each decision d  X  X  , we define If multiple decisions minimize the risk for a particular x T , we break ties arbitrarily. Identifying the best decision d  X  X  then amounts to identifying which equivalence class H d contains the realized vector of outcomes, which is an instance of ECD.
 One common approach to this problem is to myopically pick tests maximizing the decision-theoretic value of information (VoI):  X  VoI ( t | x A ) := min d E H [ ` ( d,H ) | x A ]  X  E loss of the best decision due to the observation of x t . However, we can show there are instances in which such a policy pays  X ( n/ log( n )) times the optimal cost, even under a uniform prior on ( h, X  ) and with | supp( X ) | = 2 . See the long version of this paper [ 10 ] for details. In contrast, on such instances EC 2 obtains an O (log n ) approximation. More generally, we have the following result for EC 2 as an immediate consequence of Theorem 3.
 Theorem 4. Fix hypotheses H , tests T with costs c ( t ) and outcomes in X , decision set D , and loss function ` . Fix a prior P ( H,  X ) and a function x T : H X  supp( X )  X  X N which define the probabilistic noise model. Let c (  X  ) denote the expected cost of  X  incurs to find the best decision, i.e., to identify which equivalence class H d the outcome vector x T belongs to. Let  X   X  denote the policy minimizing c (  X  ) , and let  X  EC denote the adaptive policy implemented by EC 2 . Then it holds that where p 0 min := min h  X  X  { P ( h, X  ) : P ( h, X  ) &gt; 0 } .
 If all tests have unit cost, by using a modified prior [ 15 ] the approximation factor can be improved to O (log |H| + log | supp( X ) | ) as in the case of Theorem 3.
 The E FF ECX TIVE algorithm. For some noise models,  X  may have exponentially X  X arge support. In this case reducing Bayesian active learning with noise to Equivalence Class Determination results in instances with exponentially-large equivalence classes. This makes running EC 2 on them challenging, since explicitly keeping track of the equivalence classes is impractical. To overcome this challenge, we develop E FF ECX TIVE , a particularly efficient algorithm which approximates EC 2 .
 For clarity, we only consider the 0  X  1 loss, i.e., our goal is to find the most likely hypothesis (MAP consider the weight of edges between distinct equivalence classes H i and H j : w ( H i  X H j ) = X In general, P ( X T  X  X  i ) can be estimated to arbitrary accuracy using a rejection sampling approach with bounded sample complexity. We defer details to the full version of the paper. Here, we focus on the case where, upon observing all tests, the hypothesis is uniquely determined, i.e., P ( H | x T ) is deterministic for all x T in the support of P . In this case, it holds that P ( X T  X  X  i ) = P ( H = h i ) . Thus, the total weight is This insight motivates us to use the objective function which is the expected reduction in weight from the prior to the posterior distribution. Note that the weight of a distribution 1  X  P i P ( h i ) 2 is a monotonically increasing function of the R  X  enyi entropy (of order 2), which is  X  1 2 log P i P ( h i ) 2 . Thus the objective  X  Eff can be interpreted as a (non-standard) information gain in terms of the (exponentiated) R  X  enyi entropy. In our experiments, we show that this criterion performs well in comparison to existing experimental design criteria, including the classical Shannon information gain. Computing  X  Eff ( t | x A ) requires us to perform one inference task for each outcome x of X t , and O ( n ) computations to calculate the weight for each outcome. We call the algorithm that greedily optimizes  X  Eff the E FF ECX TIVE algorithm (since it uses an Efficient Edge Cutting approXimate objective), and present pseudocode in Algorithm 1. Input : Set of hypotheses H ; Set of tests T ; prior distribution P ; function f . begin end Algorithm 1 : The E FF ECX TIVE algorithm using the Efficient Edge Cutting approXimate objective. Several economic theories make claims to explain how people make decisions when the payoffs are uncertain. Here we use human subject experiments to compare four key theories proposed in literature. The uncertainty of the payoff in a given situation is represented by a lottery L , which is simply a random variable with a range of payoffs L := { ` 1 ,...,` k } . For our purposes, a payoff is an integer denoting how many dollars you receive (or lose, if the payoff is negative). Fix lottery L , and let p i := P [ L = ` i ] . The four theories posit distinct utility functions, with agents prefer-ring larger utility lotteries. Three of the theories have associated parameters. The Expected Value theory [ 22 ] posits simply U EV ( L ) = E [ L ] , and has no parameters. Prospect theory [ 14 ] posits U
PT ( L ) = P i f ( ` i ) w ( p i ) for nonlinear functions f ( ` i ) = ` ` &lt; 0 , and w ( p i ) = e  X  (log(1 /p i ))  X  [ 21 ]. The parameters  X  PT = {  X , X , X  } represent risk aversion, loss aversion and probability weighing factor respectively. For portfolio optimization problems, finan-cial economists have used value functions that give weights to different moments of the lottery [ 12 ]: U
MV S ( L ) = w  X   X   X  w  X   X  + w  X   X  , where  X  MV S = { w  X  ,w  X  ,w  X  } are the weights for the mean, stan-dard deviation and standardized skewness of the lottery respectively. In Constant Relative Risk Aver-sion theory [ 20 ], there is a parameter  X  CRRA = a representing the level of risk aversion, and the util-distinguish which of the four theories best explains the subject X  X  responses. Here a test t is a pair of lotteries, ( L t 1 ,L t 2 ) . Based on the theory that represents behaviour, one of the lotteries would be preferred to the other, denoted by a binary response x t  X  X  1 , 2 } . The possible payoffs were fixed to L = { X  10 , 0 , 10 } (in dollars), and the distribution ( p 1 ,p 2 ,p 3 ) over the payoffs was varied, where p obtained the set of possible tests.
 We compare six algorithms: E FF ECX TIVE , greedily maximizing Information Gain (IG), Value of Information (VOI), Uncertainty Sampling 5 (US), Generalized Binary Search (GBS), and tests selected at Random. We evaluated the ability of the algorithms to recover the true model based on simulated responses. We chose parameter values for the theories such that they made distinct predictions and were consistent with the values proposed in literature [ 14 ]. We drew 1000 samples of the true model and fixed the parameters of the model to some canonical values,  X  PT = { 0 . 9 , 2 . 2 , 0 . 9 } ,  X  MV S = performance of the 6 methods, in terms of the accuracy of recovering the true model with the number of tests. We find that US, GBS and VOI perform significantly worse than Random in the presence of noise. E FF ECX TIVE outperforms InfoGain significantly, which outperforms Random. Figure 2: (a) Accuracy of identifying the true model with fixed parameters, (b) Accuracy using a grid of We also considered uncertainty in the values of the parameters, by setting  X  from 0.85-0.95,  X  from 2.1-2.3,  X  from 0.9-1; w  X  from 0.8-1.0, w  X  from 0.2-0.3, w  X  from 0.2-0.3; and a from 0.9-1.0, all with 3 values per parameter. We generated 500 random samples by first randomly sampling a model and then randomly sampling parameter values. E FF ECX TIVE and InfoGain outperformed Random significantly, Fig. 2(b), although InfoGain did marginally better among the two. The increased parameter range potentially poses model identifiability issues, and violates some of the assumptions behind E FF ECX TIVE , decreasing its performance to the level of InfoGain.
 After obtaining informed consent according to a protocol approved by the Institutional Review Board of Caltech, we tested 11 human subjects to determine which model fit their behaviour best. Laboratory experiments have been used previously to distinguish economic theories, [ 4 ], and here we used a real-time, dynamically optimized experiment that required fewer tests. Subjects were presented 30 tests using E FF ECX TIVE . To incentivise the subjects, one of these tests was picked at random, and subjects received payment based the outcome of their chosen lottery. The behavior of most subjects (7 out of 10) was best described by EV. This is not unexpected given the high quantitative abilities of the subjects. We also found heterogeneity in classification: One subject got classified as MVS, as identified by violations of stochastic dominance in the last few choices. 2 subjects were best described by prospect theory since they exhibited a high degree of loss aversion and risk aversion. One subject was also classified as a CRRA-type (log-utility maximizer). Figure 2(c) shows the probability of the classified model with number of tests. Although we need a larger sample to make significant claims of the validity of different economic theories, our preliminary results indicate that subject types can be identified and there is heterogeneity in the population. They also serve as an example of the benefits of using real-time dynamic experimental design to collect data on human behavior. In this paper, we considered the problem of adaptively selecting which noisy tests to perform in order to identify an unknown hypothesis sampled from a known prior distribution. We studied the Equiva-lence Class Determination problem as a means to reduce the case of noisy observations to the classic, noiseless case. We introduced EC 2 , an adaptive greedy algorithm that is guaranteed to choose the same hypothesis as if it had observed the outcome of all tests, and incurs near-minimal expected cost among all policies with this guarantee. This is in contrast to popular heuristics that are greedy w.r.t. version space mass reduction, information gain or value of information, all of which we show can be very far from optimal. EC 2 works by greedily optimizing an objective tailored to differentiate between sets of observations that lead to different decisions. Our bounds rely on the fact that this objective func-tion is adaptive submodular. We also develop E FF ECX TIVE , a practical algorithm based on EC 2 , that can be applied to arbitrary probabilistic models in which efficient exact inference is possible. We apply E
FF ECX TIVE to a Bayesian experimental design problem, and our results indicate its effectiveness in comparison to existing algorithms. We believe that our results provide an interesting direction towards providing a theoretical foundation for practical active learning and experimental design problems. Acknowledgments. This research was partially supported by ONR grant N00014-09-1-1044, NSF grant
