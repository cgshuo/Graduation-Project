 An important problem of matrix completion/approximation based on Matrix Factorization (MF) algorithms is the exis-tence of multiple global optima ; this problem is especially serious when the matrix is sparse , which is common in real-world applications such as personalized recommender sys-tems. In this work, we clarify data sparsity by bounding the solution space of MF algorithms. We present the conditions that an MF algorithm should satisfy for reliable completion of the unobservables, and we further propose to augment current MF algorithms with extra constraints constructed by compressive sampling on the unobserved values, which is well-motivated by the theoretical analysis. Model learn-ing and optimal solution searching is conducted in a prop-erly reduced solution space to achieve more accurate and efficient rating prediction performances. We implemented the proposed algorithms in the Map-Reduce framework, and comprehensive experimental results on Yelp and Dianping datasets verified the effectiveness and efficiency of the aug-mented matrix factorization algorithms.
 H.3.3 [ Information Storage and Retrieval ]: Information Filtering; F.2.1 [ Numerical Algorithms and Problems ]: Computation on Matrices; G1.6 [ Mathematics of Com-puting ]: Optimization Matrix Factorization; Collaborative Filtering; Recommender Systems; Compressed Sensing; Optimization
Matrix Factorization (MF) techniques have achieved sig-nificant success in many real-world applications, such as Collaborative Filtering (CF)-based recommender systems, where MF is conducted on partially observed user-item rat-ing matrices, and the results are thus used to predict the unobserved ratings (i.e., the missing entries). A number of commonly known MF algorithms have been proposed and extensively investigated, for example, the Singular Value De-composition (SVD), Nonnegative Matrix Factorization (NMF), Probabilistic Matrix Factorization (PMF), etc.

The important advantages of fast iterations and flexible modeling and being amenable to parallelization make MF widely used in real-world systems. However, despite such empirical success, MF approaches have mostly been used as a heuristic with little solid theoretical analysis other than the guarantees of convergence to the local minima [10]. In fact, the performance of most MF algorithms relies heavily on the sparsity and the underlying structure of the matrices.
The existence of multiple global/local optima leads to se-rious problems when MF is used to predict the unobserv-ables. Consider conducting regularized NMF on a simple Block Diagonal Form (BDF) [28, 29, 27] structured matrix in the diagonal blocks X 11 and X 22 , and the ratings in off-diagonals X 12 and X 21 are all unobserved. Suppose the NMF algorithm gives the global optimal solution that X is factorized as X  X  UV 0 = U 1 U and V are split in the same pattern according to X ; then we have X 11  X  U 1 V 0 1 and X 22  X  U 2 V 0 2 . Now we rearrange the columns in U 1 and V 1 in the same order, which gives and  X  V 1 . One can see that we have  X  U 1  X  V 0 1 = U 1 the newly constructed factorization X  X  h  X  U 1 U a global optimal solution, as it gives the same predicted rat-ings on the observed values and the same penalty on the reg-ularization terms. However, it could give completely differ-ent predictions on the off-diagonal areas, as  X  U 1 V 0 2 would not be equal to U 1 V 0 2 and U 2 V 0 1 , after the columns of U 1 and V 1 have been rearranged.

This example demonstrates the multiple global optima prob-lem, where an MF algorithm  X  X ails X  on BDF matrices, as it gives completely different predictions on the unobserved val-ues, while achieving the same minima on the objective loss function. This contradicts the reason for using MF to pre-dict unobservables because we assume the preferences of the users to be  X  X redictable X  from their historical choices. As we will show later, this problem arises not only on BDF ma-trices but also on many matrices where the observed values are sparse or improperly distributed; thus, the performance of the algorithm cannot be properly constrained.

This problem is rarely noticed or investigated in the re-search community, most likely because most of the public benchmark datasets, such as MovieLens, Netflix and Yahoo! Music, have been preprocessed by removing those users who have rated less than a specific number of items. Such pre-processing makes a dataset biased from the original rating distribution, and thus the insufficiency of the ratings does not pose as severe a problem as it could be.

Recently, the release of the more  X  X ractical X  Yelp rating dataset 1 exposed the problem directly. By permuting the rows and columns of the rating matrix, it can be rearranged into a BDF structure with 53 diagonal blocks, with a dom-inating block and 52 scattered blocks, which is shown in Fig.1(a). The corresponding bipartite graph of the matrix is highly disconnected, and these scattered blocks correspond to the 52 connected components, as shown in Fig.1(b). Ac-cording to the analysis above, the presence of one single scattered block increases the global optimal solution space by O ( r !), where r is the number of latent factors used in an MF algorithm, which is typically assigned between 50 to 100. The algorithm could converge to any of the global optimal solutions, although they provide very different predictions on the unobserved values.

The essential reason that MF algorithms fail in such cases is that they only make constraints on the observed values in a matrix, without any constraints on the predictions of the unobserved values. In this work, we indicate with theoretical analysis that, for an MF algorithm to avoid the multiple global optima problem, and thus to recover the unobserved values properly, the following two basic conditions should be satisfied: 1. The number of constraints should be up to the order 2. The distribution of the constraints should be nearly
A single observed value can be viewed as a constraint in MF algorithms; however, the number of observed values could be far from the above requirement, and they would not necessarily be nearly isometrically distributed. In this work, we treat the MF as a subspace fitting problem and analyze the difference between the solution space and the ground truth. We propose to augment MF algorithms with extra constraints constructed from the unobserved values, which are selected according to some specific distributions. In this way, our MF model satisfies the above two conditions, and the algorithm can find a proper solution in a reduced solution space. Experimental results verify the effect of our method in improving the prediction accuracy, stability, con-vergence rate and computational efficiency.

The paper is structured as follows: In section 2, we in-troduce some of the related work; In sections 3 and 4 we give some preliminaries and conduct theoretical analysis of the solution space, which form the basis of this work, and, afterwards, we present our method and algorithms; the ex-perimental results are shown in section 5; finally, the work is discussed in section 6 and concluded in section 7.
Latent factor models based on Matrix Factorization (MF) techniques have long been an important research direction in Collaborative Filtering (CF)-based recommendations [13, 26]. Recently, the MF approaches have gained great popu-larity, as they usually outperform traditional methods, and (a) Yelp dataset Matrix (b) Yelp dataset Graph Figure 1: Structures of Yelp dataset. In the left is the exampled structure of the rating matrix, and in the right is the real structure of the scattered blocks. have achieved state-of-the-art performance [24]. A variety of MF algorithms have been investigated in different CF set-tings, for example, Principle Component Analysis (PCA) [1], Singular Value Decomposition (SVD) [13], Non-negative Matrix Factorization (NMF) [14], Max-Margin Matrix Fac-torization (MMMF) [23, 17], and Probabilistic Matrix Fac-torization (PMF) [19, 18].

However, despite such empirical success, MF approaches have mostly been used as heuristics and have little solid the-oretical analysis other than the guarantees of convergence to local minima. The most recent work concerning the theoret-ical properties of MF algorithms is given in [10, 25], which investigate the optimization algorithms for MF and their stability with adversarial noise in terms of prediction accu-racy. However, they do not touch upon the topic of how to overcome the multiple global optimal problem.
 This problem is closely related to the research of matrix Compressed Sensing (CS) [8, 4], which can be viewed as a generalized form of matrix completion or matrix factoriza-tion in that a constraint is not restricted to a single observed value but instead the linear equations of the observations. According to the mathematical relationships, the CS prob-lem is formulated as a rank minimization problem in [3] and further formulated as a convex optimization problem based on nuclear norm minimization [6, 5]. Later, [21] investigated the uniqueness of low-rank matrix completion problems with the basic tools of rigidity theory. However, the success of CS relies on the assumption that the constraints are sufficient and isometrically distributed, which can hardly be satisfied in the real-world scenarios of CF.

In the effort to tackle this problem, recent work has fo-cused on the idea of reformulating current MF algorithms to fit the real distributions of data. [20] attempted to con-duct CF on non-uniformly sampled matrices using a prop-erly weighted version of nuclear-norm regularizers, and [15] proposed a graph theoretic approach for matrix completion under more realistic power-law distributed samples. [12] ex-plored the relationships between matrix factorization and combinatorial algebraic theory. To speed up the process of rank minimization, [16] proposed the Singular Value Projec-tion (SVP) algorithm for matrix completion with affine con-straints. However, these approaches make tight assumptions on the distributions of data, which restricts their application in practical systems and scenarios. Instead of the traditional approach of reformulating the algorithms, we attempt to re-sample the data to alleviate the problem of multiple global optima, which brings about the advantages of both higher prediction accuracy and the ability to conveniently integrate the approach into many MF algorithms.
Definition 1. Matrix Factorization (MF) algorithms can be generally defined by the constrained optimization problem: where X  X  R m  X  n is the approximation matrix, U  X  R m  X  r and V  X  R n  X  r are the decision variables, and R is the reg-ularization term. A : R m  X  n  X  R p is a linear map defining a set of p linear constraints on X , where each of the con-and b  X  R p is the corresponding vector of the b i  X  X .
This definition of MF is equal to the frequently used reg-ularized optimization form in terms of Lagrange multipliers. Let  X  be the Lagrange multiplier for the linear constraint A ( X )  X  b = 0, then Eq.(1) could be reformulated as: min where  X  = 1 /  X  is the regularization coefficient. Eq.(2) is a special case of the unified view of MF given in [22]: where D W ( X,  X  X ) is the loss between the approximation X and the original matrix  X  X . However, Eq.(1) and Eq.(2) are general enough to represent the objective function of most MF algorithms. For example, SVD takes the objective function k W ( X  X   X  X ) k 2 F +  X  ( k U k 2 F + k V k 2 is the Frobenius norm, and W is an indication matrix such that W ij = 1 if  X  X ij is observed, and 0 otherwise.
Proposition 1. Let X  X  R m  X  n and rank( X )  X  r , then there exist matrices U  X  R m  X  r and V  X  R n  X  r , such that X = UV 0 .

Proof. This can be easily proven by the singular value decomposition of X , which is X = M  X  N 0 = ( M and by setting U = M
Proposition 1 guarantees that we can always find an ac-curate factorization for X using r factors if the rank of X is no more than r , which allows us to bypass the explicit fac-torization of X and use the following definition to conduct matrix factorization.

Definition 2. Low Rank Matrix Factorization (LRMF):
The most frequently used choice for the regularization term in Eq.(1) is the Frobenius norm regularizer, where R ( U,V ) = k U k 2 F + k V k 2 F , and in Eq.(4) is the nuclear norm R ( X ) = k X k  X  , where nuclear norm k X k  X  is defined as the sum of the singular values of X . The following proposition guarantees their equivalence in terms of low rank matrix factorization.

Proposition 2. [23] Consider the factorization of X in an unlimited dimension factorization space; then the nuclear norm of X can be represented as: k X k  X  = min
In this work, we leverage the Low Rank Matrix Factor-ization (LRMF) in Definition 2, as well as the minimization of nuclear norm k X k  X  primarily to analyze the properties of the solution space of Eq.(4), based on which we propose our augmented matrix factorization framework.
It is important to notice that most MF algorithms for collaborative filtering only consider the observed values and require the predictions to be close to the corresponding ob-servations, which means that the measurement matrix A i = e e 0 k  X  R m  X  n in Eq.(1) and Eq.(4) has a non-zero value at only a single element corresponding to one of the observed values, and thus the number of constraints p is equal to the number of observations.

As we will show in the following parts, such a measure-ment set A is usually insufficient to guarantee a global opti-mal solution. However, the measurement set A could have not been restricted to such single-valued constraints. In this section, we analyze the solution space of the LRMF problem and further propose the Augmented Matrix Factorization (AMF) framework for more accurate rating prediction.
We bound the solution space of the LRMF problem with the nuclear norm regularizer by analyzing the properties of the linear map A .

Consider solving the constrained optimization problem of LRMF in Eq.(4) with the nuclear norm regularization R ( X ) = k X k  X  using the method of Lagrange multipliers; this yields the global optimal solution X  X  , such that A ( X b . We then define the adjoint problem as follows:
We have X  X  as one of the exact global optimal solutions for Eq.(6). Suppose the global optimal solutions of this ad-joint problem is generally denoted by X , and the residual matrix between X and the known optimal solution X  X  is de-noted as R = X  X   X  X . We then investigate this residual error under the linear map A by analyzing kA ( R ) k 2 2 , whose ideal value would be zero because A ( X  X   X  X ) = A ( X  X  )  X  X  ( X ) = b  X  b  X  = 0. More specifically, we investigate the appropriate properties that A should satisfy, which guarantees we will search for the global optimal solution in a properly reduced solution space.
In this section, we describe the characteristics of an im-portant class of linear maps, which are those that satisfy the restricted isometry property , and we bound kA ( R ) k 2 the condition that the linear map A satisfies this property.
Definition 3. [10] A linear map A : R m  X  n  X  R p is said to satisfy the Restricted Isometry Property (RIP), with the RIP constant  X  r , if, for all of the matrices X  X  rank( X )  X  r , the following holds: Lemma 1. According to the definition of the Restricted Isometry Property, we have  X  r  X   X  r 0 for r  X  r 0 .
This definition of the RIP is a generalization to matrices from sparse vectors in [9]. The following theorem shows that the solution space of Eq.(6) can be properly bounded given that A satisfies the RIP.

Theorem 1. Suppose that A : R m  X  n  X  R p is isometri-cally restricted, and R = X  X   X  X is the residual matrix in Eq. (6) ; then there exists a matrix R 0 whose rank satisfies rank( R 0 )  X  2 r , such that kA ( R ) k 2 2 = O ( r X  2 r
Theorem 1 indicates that, when the linear map A is iso-metrically restricted, the solution space of Eq.(6) reduces along with the deceasing of the RIP constant  X  . Specially, when  X  is small enough to be close to zero, there would be one single global optimal solution for the adjoint problem. The proof of Theorem 1 requires the following lemmas. Lemma 2. Let A,B be matrices of the same dimensions. If AB 0 = 0 and A 0 B = 0 , then k A + B k  X  = k A k  X  + k B k
Lemma 3. [3] For any matrices A,B  X  R m  X  n of the same dimensions, there exist matrices B 1 and B 2 , such that: (1) B = B 1 + B 2 ; (2) rank( B 1 )  X  2 rank( A ) ; (3) AB 0 2 A B 2 = 0 ; (4)  X  B 1 ,B 2  X  = 0 .
 Proof. Let the singular value decomposition of A be A = U [  X  0 0 0 ] V 0 , and let  X  B = U 0 BV . Partition ing to the SVD of A as  X  B = h  X  B 11  X  B 12  X  U satisfy the above four conditions.

Lemma 4. For any matrix X such that rank( X )  X  r , we have k X k F  X k X k  X   X 
Proof of Theorem 1. By applying Lemma 3 to the ma-trices X and R , there exist matrices R 0 and R c such that R = R 0 + R c , rank( R 0 )  X  2 r , and XR 0 c = 0 and X 0 By the optimality of X  X  , we have k X k  X   X  k X  X  k follows that: where  X  1 follows from the triangle inequality and  X  2 follows from Lemma 2. It can be further derived from Eq.(8) that k R
Let R c = U diag(  X  ) V 0 be the SVD of R c , where the sin-gular values in the diagonal matrix diag(  X  ) are sorted in descending order. Now we partition R c into a sum of ma-trices R 1 ,R 2 ,  X  X  X  . For each i  X  1, define the index set I { 2 r ( i  X  1) + 1 ,  X  X  X  , 2 ri } , and define R i = U I i
By this construction method, we have rank( R i )  X  2 r for i  X  1, and that: which further implies that:
With Eq.(10) and the relationship k R 0 k  X   X k R c k  X  , we can sum up the following bound: where the last step follows from Lemma 4 and the fact that rank( R 0 )  X  2 r . As for k R 1 k 2 F , we have the following:
Now we can wrap up the proof to give the following bound for kA ( R ) k 2 2 : kA ( R ) k 2 2 = kA ( R 0 ) + A ( R 1 ) + X where  X  1 follows from the triangle inequality,  X  2 follows from the RIP of A and the fact that rank( R i )  X  2 r for i  X  0, and  X  3 follows from Eq.(11) and Eq.(12).
In real-world applications, such as CF-based recommender systems, the linear map A is usually viewed to be gener-ated from some (perhaps unknown) random distributions. For example, it is observed that most real-world datasets in practical recommender systems usually exhibit power-law distributed samples [15, 20]. Unfortunately, the linear map A corresponding to the original power-law distributed ob-servations hardly ever satisfies the RIP property, which, ac-cording to Theorem 1, makes the solution space of MF al-gorithms poorly bounded and further leads to the multiple global optimal problem.

Consequently, we focus on the task of augmenting the original linear map A by resampling the unobserved entries in order to add extra constraints on the MF algorithm and bound the solution space properly. As a result, we need to investigate the necessary properties that A should satisfy under the circumstance of conducting entry sampling. The following definition describes the isometric property in terms of distributions in a probabilistic view.

Definition 4. Let A be a random variable that takes val-ues in linear maps from R m  X  n to R p . We say that A satisfies the Nearly Isometric Property (NIP) if, for all X  X  R m  X  n and, for all 0 &lt;  X  &lt; 1 , P |kA ( X ) k 2 2  X  X  X k 2 F | X   X  k X k 2 F  X  2 exp  X  p In order for a random linear map to be nearly isometric, Definition 4 requires two essential ingredients. First, it must be isometric in expectation, which is indicated by Eq.(14); and second, the probability of large distortions of length must be exponentially small, as implied by Eq.(15).
Several frequently used linear maps satisfy the NIP in real applications. For easier explanation of such types of linear maps, we introduce the following matrix representation of a linear map: Definition 5. Given X  X  R m  X  n , linear map A : R m  X  n  X  R , and a set of p constraints A ( X ) = b , where each con-straint is of the form b i =  X  X,A i  X  = tr( A 0 i X ) . Let d = mn and x = vec( X ) be the vectorization of X . Then, we define the constraint matrix A = [vec( A 1 ) vec( A 2 )  X  X  X  vec( A R p  X  d so that the linear constraints A ( X ) = b can be equally reformulated as Ax = b .
 Generally, it is proven in [7, 11] that, whenever the entries A ij are independently and identically distributed with zero mean and finite fourth moment, the maximum singular value of the constraint matrix A is almost surely 1 + p d/p for d that is sufficiently large, and thus the matrix A and the corresponding linear map A satisfy the NIP.

Specifically, we give some frequently used examples of distributions that satisfy the NIP, which are considered in the experiments primarily for augmenting MF algorithms. The linear map A satisfies the NIP when A ij is sampled independently and identically from the Gaussian distribu-tion A ij  X  X  (0 , 1 p ). The entries sampled from a symmetric Bernoulli distribution also meet the requirements: P ( A ij q p ) = P ( A ij =  X  bution that takes zeros into account is also frequently used due to its convenience in reducing the number of parame-ters, which is P ( A ij = q 1 2 p X  ) = P ( A ij =  X  q 1 2 p X  P ( A ij = 0) = 1  X  2  X  (0 &lt;  X  &lt; 1 2 ) [3, 2].
The following theorem characterizes the family of linear maps that satisfy the NIP regarding the upper bound of the isometric constant.
 Theorem 2. [3] Given fixed 0 &lt;  X  &lt; 1 , and let A : R m  X  n  X  R p be a linear map that satisfies the NIP; then, for every 1  X  r  X  min( m,n ) , there exist positive constants c 0 and c 1 , depending only on  X  , such that, with probabil-ity of at least 1  X  exp(  X  c 1 p ) , we have  X  r  X   X  whenever p  X  c 0 r ( m + n ) log( mn ) .
 The readers might refer to [3] for a proof of this theorem. The theorem indicates that, given A is nearly isometrically distributed, the upper bound of the isometric constant  X  can be closely bounded almost surely, as long as the number of linear constraints p of A is sufficiently large at the order of O ( r ( m + n ) log( mn )). In consideration of Theorem 1, we further arrive at the following corollary.

Corollary 1. Suppose that the maximum rank r allowed in the LRMF problem defined by Eq. (4) and Eq. (6) satisfies r  X  min( m,n ) / 2 , and that A : R m  X  n  X  R p is a linear map with the NIP. Then, kA ( R ) k 2 2 reduces almost surely with the increase of p , whenever p = O ( r ( m + n ) log( mn )) , where R = X  X   X  X is the residual matrix.

Proof. According to the definition of LRMF problem, we have rank( X )  X  r and rank( X  X  )  X  r ; as a result, rank( R ) = rank( X  X   X  X )  X  2 r  X  min( m,n ), and the corollary can now be deduced from Theorem 2.

This corollary indicates that, under the assumption that the linear constraints are isometrically distributed, the so-lution space of the LRMF problem reduces with the in-crease of the number of linear constraints p , as long as r  X  min( m,n ) / 2, which means that the rank requirement is not  X  X oo X  large. In fact, this requirement is easily satisfied in practical applications of CF, especially in the scenario of LRMF, because the rank of a matrix is usually far less than the number of rows or columns. This is because the prefer-ences of users are usually determined by a limited number of latent factors, e.g. , in most MF algorithms the number of factors is set to be a few tens, while the number of rows or columns of the matrix could be in the millions. According to the theoretical analysis, we see that, for an MF algorithm to find the appropriate global optimal solution in a properly reduced solution space, the following two basic conditions are needed:
An MF algorithm might fail if either of the two conditions is not satisfied. For a better understanding of the condi-tions, we present the following two case studies utilizing the previously noted Yelp rating dataset and BDF structured matrices.
 Case Study I. The Yelp Rating Dataset.

This dataset violates the first condition. In fact, the Yelp dataset is different from many previous public datasets, and it is in better conformity with practical situations because the previous datasets usually eliminate those users with less than 20 ratings, which relieves them of the multiple global optimal problem to a great extent. The following table lists the statistics of some frequently used datasets.
 Table 1: Statistics of some frequently used datasets, where m, n, p are the number of rows, columns and observed values, and  X  p=r(m+n) log (mn) is the re-quired number of constraints, where we use r =10 to keep the magnitude, as r is usually set to be a few tens in practical applications.
 We see that the number of constraints in the MovieLens, Netflix and Yahoo! Music datasets are all approximately to the order of the corresponding requirements, while, for the Yelp dataset, they differ by two orders of magnitude. Exper-imental findings are consistent with the theoretical analysis: previous work conducted on traditional datasets [13, 14, 23, 19] reported significant improvement in terms of prediction accuracy that MF approaches could achieve, while the ex-perimental results show that many MF algorithms perform even worse than a simple global averaging strategy on the Yelp dataset challenge.
 Case Study II. The BDF Structured Matrices.

This type of matrix violates the second condition. Note that the number of constraints in a BDF structured matrix could be extremely high. Suppose a BDF matrix has k diag-onal blocks, each of which is an n  X  n fully filled sub-matrix with n 2 observed values; then we have:
However, such a matrix would still suffer from the multiple global optimal problem, as shown by the example in the pre-vious sections of this paper. The underlying reason is that the observed values are all restricted to the diagonal blocks, which makes the constraints biased from the requirement of nearly isometric distributions.
The two basic conditions inspire us to augment MF algo-rithms by adding extra constraints to the linear map A if the requirement on the number of constraints is not satis-fied, and, at the same time, we attempt to make it nearly isometrically distributed, which allows us to reduce the solu-tion space of the LRMF problem. An MF algorithm is then conducted in this reduced solution space for a global optimal solution. Algorithm 1 shows the procedure of augmenting the constraints in the LRMF problem, followed with more detailed explanations and analyses.
 Algorithm 1: Augment ( A ,b,m,n,p,r,c 0 , X  ) 1 A  X  [vec( A 1 ) vec( A 2 )  X  X  X  vec( A p )] 0 ; 2 A 0  X  A, b 0  X  b ; 3  X  p  X  c 0 r ( m + n ) log( mn ) , d  X  mn ; 4 if p &lt;  X  p then 5  X  p  X   X  p  X  p ; 7 end 8 p  X  max( p,  X  p ); 9 for i  X  1 to p do 10 for j  X  1 to d do 11 if A ij = 0 then 12  X   X  random(0 , 1); 13 if  X  &lt;  X  then A ij  X  p 1 / 2 p X  ; 14 if  X  &gt; 1  X   X  then A ij  X  X  X  p 1 / 2 p X  ; 18 b  X  AA + 0 b 0 ; // A + 0 is the pseudoinverse of A 20 return A := { A i } p i =1 ,b,p ;
In this algorithm, we first check whether the original num-ber of constraints p has reached the requirement  X  p , and if not, the constraint matrix A and vector b are augmented by appending some zero vectors or values, respectively, to gain the required number of constraints.

These augmented constraints, as well as the original ones, are then resampled in the second stage to meet the require-ment of nearly isometric distributions. Note that we choose to use the generalized form of the symmetric Bernoulli distri-bution for constraint resampling because it keeps A ij set to zero with high probability, compared with the Gaussian dis-tribution and symmetric Bernoulli distribution, which ben-efits the computational time in practical applications. How-ever, it is important to note that the selection of distribution for resampling is not restricted to the generalized form of the symmetric Bernoulli distribution, and any distribution satisfying the NIP can be integrated into the framework of Algorithm 1.

In the last stage, the constraint vector b is reconstructed to match up with the augmented constraint matrix A by multiplying A with the estimated vector  X  x = A + 0 b 0 , where A 0 and b 0 are the constraint matrix and constraint vector before augmentation, respectively. This estimation given by the pseudoinverse of A 0 is chosen based on the least square property, namely, we have k A 0 x  X  b 0 k 2  X  k A 0  X  x  X  b all x . For an arbitrary matrix A 0 , the pseudoinverse A be calculated from the singular value decomposition of A 0 which is shown as follows:
A 0 = U  X  V 0  X  A + 0 = V  X   X  1 U 0  X   X  x = V  X   X  1 U 0 b
The pseudoinverse is chosen also for the purpose of keep-ing the generality of the algorithm because we do not apply any restriction to the form of the constraint matrix A 0 in the original LRMF problem. However, this also brings about the problem of computational efficiency because the compu-tation of the SVD of a high dimensional matrix could be notably expensive in practical applications.

Fortunately, it is important to note that, in most MF settings, each of the measurement matrices A i in the linear map A has only a single non-zero, whose value is one, at the very position corresponding to an observed value in X . More formally, we have A i ( p,q ) = 1 for the i -th observed value in X , where X ( p,q ) 6 = 0. As a result, A 0 is a matrix where there is at most one non-zero in each row and each column, which gives us that A + 0 = A 0 0 , and we thus have: Furthermore, we achieve k A 0  X  x  X  b 0 k 2 2 = 0 in such cases. As a result, we need not compute the SVD of the original constraint matrix A 0 in practice, and the estimated vector  X  x is achieved by simple matrix-vector multiplications.
When Gaussian fast sampling is used based on the central limit theorem, the computational complexity of Algorithm 1 is quasilinear: O ( p log( d )) = O ( r ( m + n ) log 2 There exist several possible methods for optimizing the LRMF problem in Eq.(1) and Eq.(4), including Semi-Definite Programming (SDP), the interior point methods, projected subgradient methods and low rank parameterization [3].
The SDP solvers and the interior point methods can achieve quite accurate solutions, even to the machine precision. How-ever, they are also rather computationally expensive and could hardly be applied to the large-scale real-world datasets, where the number of rows and/or columns of a matrix could be in the millions. The projected subgradient method is also expensive as it involves the computation of the SVD of a high dimensional matrix as a core stage.

In this work, we adopt the approach of low rank parame-terization based on the method of Lagrange multipliers for model learning after the constraints have been augmented, which is shown in Eq.(2) and is a standard method for solv-ing equality constrained optimization problems. According to Proposition 2, this approach applies both to the case when the nuclear norm R ( X ) = k X k  X  is used for regular-ization and the case when the Frobenius norm R ( U,V ) = k U k 2 F + k V k 2 F is used [3].

Let X = UV 0 be the low rank parameterization of X , where U  X  R m  X  r and V  X  R n  X  r are the low rank parame-ters, and let L ( U,V ) = k U k 2 F + k V k 2 F +  X  kA ( UV the Lagrangian function. Then the partial deviations are:
In the method of multipliers, we minimize the Lagrangian function by updating the decision variables U and V al-ternately. The minimization of the Lagrangian function in terms of U and V can be conducted using any local search technique. In this work, we adopt the linear search method for the convenience of implementation, and the updating rules for U and V are: where the corresponding step sizes for U and V are:  X 
U =  X   X  V =  X 
The readers could refer to the supplementary materials 2 for the detailed derivation of Eq.(19)  X  (21). The following algorithm shows the procedure of solving the LRMF problem after augmentation, where the parameters N and  X  are used to determine when to terminate the algorithm.
 Algorithm 2: AugmentMF ( A ,b,m,n,p,r,c 0 , X ,  X  ,N, X  ) 1 A := { A i } p i =1 ,b,p  X  Augment ( A ,b,m,n,p,r,c 0 , X  ); 2 U  X  R m  X  r , V  X  R n  X  r ; //Initialize randomly 3 X  X  UV 0 , t  X  0; 4 repeat 5 X  X   X  X, t  X  t + 1; 6 Compute  X  U , X  U as in Eq.(19) and Eq.(21); 7 U  X  U +  X  U  X  U ; 8 Compute  X  V , X  V as in Eq.(19) and Eq.(21); 9 V  X  V +  X  V  X  V ; 10 X  X  UV 0 ; 11 until k X  X  X  X  k 2 F &lt;  X  or t &gt; N ; 12 return X ;
The computational complexity of Algorithm 2 under the generalized symmetric Bernoulli distribution is O ( pr ( m + n )) = O ( r 2 ( m + n ) 2 log( mn )), which is unfortunately not quasilinear like that of Algorithm 1. However, the observa-tion that both the gradients in Eq.(19) and the step sizes in Eq.(21) are summed from the p measurement matrices { A i } p i =1 makes it easy to fit into the Map-Reduce paral-lelization framework. In this work, when the augmentation factor c 0 and the number of latent factors r are given, we use d c 0 r e mapping tasks in the Map-Reduce framework, which makes the computational time comparable to that of solving the original optimization problem without augmentation.
In this section, we conduct extensive experiments to inves-tigate the performance of the proposed Augmented Matrix Factorization (AMF) framework in terms of several impor-tant evaluation aspects. We mainly focus on the following two research questions: 1. What is the performance of the framework in terms of 2. What is the performance on the matrices that are
One can see that these two research questions correspond to the two conditions required of a matrix for reliable rating prediction, which are the number and the distribution of the constraints. Through the experimentations, we would like to investigate the performance of the framework in these two different CF settings.
We chose the Yelp 3 and Dianping 4 user-item rating matrix datasets for the experiments. The Yelp rating dataset is from the Yelp dataset challenge, as has been indicated in the previous sections, and the Dianping dataset is collected from the website. Some statistical information about the datasets is shown in the table below: Dataset m (# users ) n (# items ) p (# ratings ) density Yelp 45,981 11,537 229,907 0.00043 Dianping 8,361 11,392 210,382 0.00221
The Yelp dataset consists of the user ratings of businesses that are mostly located in the city of Phoenix in the US, while the Dianping dataset consists of the ratings on restau-rants located in three of the main cities in China: Beijing, Shanghai and Guangzhou.

In the Dianping dataset, the inter-city ratings are far sparser than the inner-city ratings, e.g. , users from Bei-jing would more likely make ratings on Beijing restaurants, while it is rare for them to rate the restaurants in Shanghai or Guangzhou. This makes the Dianping dataset approxi-mately BDF structured, where each of the diagonal matrices represents a city and they are denser than the off-diagonals. We use the Yelp dataset primarily to verify the performance on matrices that violate the first condition and use the Di-anping dataset for the second condition.
The test set of the Yelp dataset is not publicly available; as a result, we conduct ten-fold cross-validation on the train-ing set for model learning and evaluation. In the Dianping dataset, we use all of the inner-city ratings and randomly se-lect 20% of the inter-city ratings each time for training, and use the remaining 80% of the inter-city ratings for testing. We also conduct the procedure of training and evaluation 10 times on the Dianping dataset.

The experiments were conducted on a 3.1GHz Linux server with 64 cores and 64GB RAM. Three popular and state-of-the-art MF algorithms were chosen for performance compar-isons, which are NMF in [14], PMF in [18] and fast MMMF in [17]. For easy comparison with the previous work, we use Root Mean Square Error (RMSE) as the evaluation metric. We set  X  = 50 for regularization and use the parameters  X  = 0 . 01 and N = 100 in Algorithm 2 to ensure conver-gence. A series of experiments were conducted to verify the performance of the AMF algorithm in terms of prediction accuracy, stability, convergence rate and computational effi-ciency.
We investigate the prediction accuracy in terms of three important parameters in the AMF algorithm: the number of latent factors r , the augmentation factor c 0 , and the sam-pling rate  X  in the generalized Bernoulli distribution.
We first investigate the prediction accuracy regarding the number of latent factors r in the low rank parameters U and V . We set c 0 = 1 and  X  = 0 . 01 in the AMF algorithm, and the regularization coefficient  X  is set to 0.06 in the NMF algorithm of [14]. For PMF in [18],  X  U and  X  V are both set to be 0.005, and the regularization constant C = 1 . 5 in the fast MMMF algorithm of [17]. These hyper parameters are chosen according to grid search-based cross-validation to achieve the best performance for each of the algorithms. We then tune the parameter r in the range of 10  X  100 with a tuning step of 10, and the experimental results are shown in the figures below. Figure 2: RMSE vs different choices of the number of latent factors r in matrix factorization algorithms.
The experimental results show that when the number of latent factors r is sufficient, better performance in terms of RMSE can be achieved in the AMF framework, compared with all of the other three frequently used MF algorithms. The best prediction accuracy and the corresponding r for each of the four algorithms are shown in Table 3.
 Table 3: The best prediction accuracy achieved by each MF algorithm on each dataset.

We see that the prediction accuracy tends to be stable with the increase of r . This is because the underlying fac-tors affecting users X  decisions are limited, which gives us rel-atively stable performance when the latent factors used are sufficient. In the following experiments, we set r = 60 for all four algorithms on both of the datasets. Applying more factors is allowed, but it is sufficient to achieve stable and satisfactory accuracies according to the experiments.
To investigate the relationship of prediction accuracy with the augmentation factor c 0 , we fix the parameters  X  = 0 . 01 and r = 60. For the Yelp dataset, we tune c 0 in the range of 0 . 5  X  1 . 5 with a tuning step of 0 . 1, and, for the Dianping dataset, c 0 is tuned from 0 . 5 to 4 with a tuning step of 0 . 5. Different ranges and tuning steps are used because we find that the optimal augmentation factor c 0 is different on different datasets. The experimental results are plotted in Figure 3. F igure 3: RMSE vs different choices of augmenta-tion factor c 0 in the AMF algorithm.

We see that the AMF framework helps to gain better pre-diction accuracy when appropriate augmentation factors are given, yet might bring negative effects if c 0 is not appropri-ately set. It is shown that RMSE decreases with the increase of c 0 at the beginning, until an optimal selection of the aug-mentation factor, and then tends to increase along with c This is mainly because that sampling noise might be in-troduced when too many augmented extra constraints are involved. Though these constraints help in reducing the so-lution space of the LRMF problem and further lead to more stable convergence in fewer iterations, they might guide the problem into a deflected optimal solution.

The best prediction accuracy on Yelp is RMSE = 1 . 231, with the corresponding augmentation factor c 0 = 1 . 2, and, on Dianping the best performance RMSE = 0 . 951 is achieved when c 0 = 2. In the following experiments, we use the best selection of c 0 for both of the two datasets, correspondingly.
We investigate the impact of the sampling rate  X  in the generalized symmetric Bernoulli distribution. In this exper-iment, we also set r = 60 on both datasets and use c 0 = 1 . 2 for Yelp, while using c 0 = 2 for Dianping. We tune the pa-rameter  X  in the range of 10  X  4  X  10  X  1 with a tuning step of timing 10, and in the range of 0 . 1  X  0 . 5 with a step of 0 . 1. The experimental results are shown in Figure 4 below. F igure 4: RMSE vs different sampling rates  X  .

Similarly, it is observed that an appropriate sampling rate is required to achieve the optimal prediction accuracy, and that, whenever  X  is too small or too large, negative effects might be introduced. Moreover, the prediction accuracy might even be worse than NMF, PMF and MMMF when the sampling rate  X  is set too high on the Yelp dataset.
To understand the underlying reason for this observation, we go back to the intuitional effect of  X  in Algorithm 1. One can see that a higher  X  makes the linear constraints denser, and thus each of the linear constraints, including the augmented ones, involves more parameters in the estimated matrix X . In the extreme case where  X  = 0 . 5, each of the linear constraints attempts to restrict each of the estimations in X . As a result, there, in fact, would be no augmentation to take advantage of if  X  is too small, while the augmented constraints would counteract or eliminate with each other and even act as noise constraints if  X  is too large.
To verify the effect on solution space reduction of the AMF framework, we investigate the AMF algorithm, as well as the three competing algorithms, in terms of the stability of the final optimal solutions that they converge to. The pa-rameters ( r,c 0 , X  ) are assigned as (60 , 1 . 2 , 0 . 01) on Yelp and (60 , 2 , 0 . 1) on Dianping, and the parameters for NMF, PMF and MMMF algorithms are the same as those in Section 5.3.1. We calculate the standard deviation  X  and the coef-ficient of variation c v of the 10 RMSE evaluation results on each of the datasets and for each of the algorithms. The experimental results are shown in Table 4.
 Table 4: The standard deviations  X  and coefficient of variations c v of the evaluation results on RMSE.
We see that the standard deviation and coefficient of vari-ation in the AMF algorithm are at least an order of mag-nitude smaller than those of the NMF, PMF and MMMF algorithms. This observation implies that the optimal solu-tions achieved in the AMF algorithm are more stable and further verifies the fact that the augmented constraints help in reducing the solution space of the LRMF problem, which is in accordance with the theoretical analysis in Section 4. The experimental results also show that the variation of RMSE is more obvious on the Dianping dataset than on the Yelp dataset. This observation is not surprising because the inter-city ratings falling into the off-diagonal areas are hardly effectively constrained, which further expands the so-lution space of the LRMF problem. As a result, different initializations of the optimization procedure might result in different optimal solutions. This experimental result is con-sistent with our solution space analysis on the BDF struc-tured matrices in the previous sections.
In this section, we experiment on the convergence rate of the AMF algorithm in order to further investigate the effect of conducting augmentation on the constraints in the LRMF problem. We use the same parameter assignments as those in Section 5.4 and record the RMSE on the training set in the model learning process for every 5 iterations, which is called an epoch, for both the AMF algorithm and the competing algorithms. The experiments were conducted 10 times, and the average RMSE is calculated on each epoch. The results are plotted in Figure 5. Figure 5: RMSE on training set vs the number of iterations in the model learning process.

We see that the training loss tends to be stable after ap-proximately 30  X  40 iterations in the AMF algorithm, while it takes 50 or more iterations for NMF and PMF. As for the MMMF algorithm, training loss tends to decrease consis-tently in the tuning range. This experimental result implies the fast convergence rate of the AMF framework, where the augmented constraints help in guiding the optimization al-gorithm to converge to an optimal solution.

The underlying reason for this observation can be ex-plained in relation to two aspects. First, the solution space itself has been reduced by incorporating the augmented con-straints, and, second, the extra constraints help in calculat-ing a more accurate and rigorous descent gradient in Eq.(19) for model learning in each iteration.
As the constraints are augmented and extra constraints are introduced in the AMF framework, the computational time is increased remarkably in the model learning process. However, the independence of the constraints makes it easy to conduct optimization in a simple Map-Reduce framework, which makes the computational time comparable to that of the NMF, PMF and MMMF algorithms. In this section, we report the computational efficiency of the AMF algorithm.
We use d c 0 r e mapping tasks in the Map step to compute the gradients  X  U ,  X  V in Eq.(19) and the step sizes  X  U in Eq.(21), and then update U and V in the Reduce step. We still choose the optimal settings of the parameters ( r, X  ), Dianping. We then tune the parameter c 0 to simulate the augmentation process and record the computational time. Parameters for the NMF, PMF and MMMF algorithms are also the same as those in Section 5.3.1. Experimental results are shown in Figure 6. F igure 6: Computational time in minutes vs the aug-mentation factor c 0 on both datasets.

The results show that the computational time of the AMF algorithm is comparable to the NMF and PMF algorithm under the Map-Reduce framework with d c 0 r e mapping tasks, where each task processes approximately ( m + n ) log( mn ) constraints, which is numerically comparable to the original number of observed values on both datasets.

One might notice that the computational time begins to rise when c 0 = 1 . 1. This is because the experiments were conducted on a machine with 64 cores and r = 60 is set for model learning; thus, when d c 0 r e is greater than 64, the mapping tasks cannot be truly and efficiently distributed among the cores. Nevertheless, the relatively stable compu-tational time when c 0  X  1 implies that the nature of par-allelization of the AMF framework makes it scale efficiently with the processing power in practical applications.
In this section, we discuss aspects of the presented work and note some of the future directions.
The relationship between augmentation and regulariza-tion, especially the parallel between augmentation and the Bayesian interpretation of regularization, is of important theoretical interest in conducting matrix completion. For example, the frequently used Frobenius norm regularizer is, actually, guiding the model learning process around a cen-tral point under the assumption of Gaussian distribution, while, in the augmentation framework, one can integrate many distributions satisfying the NIP, where the Gaussian distribution is only a special case.

The augmentation framework might also be closely related to other methods like bagging, stacking, or more generally, ensemble learning. In our understanding, the augmentation framework is better than a simple bagging method in that one can appropriately control the resampling procedure to make the constraints satisfy the NIP. However, a deeper rela-tionship therein under the background of matrix completion may bring brand new insights into this well studied problem.
Although the constraints are restricted to linear measure-ments in this work, they could be much more flexible in that multiple assumptions can be incorporated into the model by means of adding rather  X  X irect X  constraints constructed from the observed or unobserved values, instead of only incorpo-rating a single and simple Gaussian distribution assumption in the regularization approach. In addition, we can even integrate various types of external information beyond nu-merical ratings into the augmented constraints.

We will investigate the deep relationship between augmen-tation, regularization and ensemble learning both theoreti-cally and practically, as well as take more general non-linear constraints into consideration in the further work.
The problem of data sparsity leads to multiple global op-tima in matrix factorization algorithms, which further leads to unreliable predictions. In this paper, we investigated the data sparsity with solution space analysis of low rank matrix factorization algorithms, under the conditions of restricted and nearly isometric properties of the linear maps. We found that two basic requirements should be satisfied for reliable completion of matrices in real-world applications, which was verified by the case studies on several frequently used pub-lic datasets. Based on these theoretical analyses, we further designed the augmented matrix factorization framework to improve the performance of low-rank matrix factorization. Extensive experimental studies demonstrated the new AMF framework in terms of prediction accuracy, stability, conver-gence rate and computational efficiency.

