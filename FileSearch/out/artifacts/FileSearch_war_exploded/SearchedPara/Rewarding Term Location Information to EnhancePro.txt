 We investigate the e ect of rewarding terms according to their locations in documents for probabilistic information retrieval. The intuition behind our approach is that a large amount of authors would summarize their ideas in some par-ticular parts of documents. In this paper, we focus on the beginning part of documents. Several shape functions are de ned to simulate the in uence of term location informa-tion. We propose a Reward Term Retrieval model that com-bines the reward terms' information with BM25 to enhance probabilistic information retrieval performance. H.3.3 [ Information Storage &amp; Retrieval ]: Information Search &amp; Retrieval BM25-RT, In uence Shape Function
Using document semantic information is usually regarded as an e ective approach for improving Information Retrieval (IR) performance. In this paper, we propose to reward terms according to their location information in documents.
Human beings do not treat each word equally when read-ing a document. People pay more attention on certain parts of the document. This is due to the fact that many authors would summarize their ideas in some particular locations, e.g. the beginning and/or the end. It particularly ts ar-ticles that have abstract and introduction at the beginning and conclude the summary in the end. In this paper, we focus on the situation that the beginning of the documents should be rewarded during retrieval.

Many researchers have studied using semantic informa-tion in some particular domains in IR. For example, On-tology Web Language and Information Retrieval (OWLIR) system [4] is designed for Semantic Web documents, using several techniques such as information extraction, annota-tion and inference. Wang and Li [5] investigated semanti-cally annotated Wikipedia XML corpus to improve retrieval performance. Zhao and Callan [6] utilized query structure expansion to enhance structured retrieval in a question an-swering (QA) application. Gao and Toutanova [1] learned semantic models from large amounts of query-title pairs con-where p is the position that a term occurs, j D j is the length of the document, and is a parameter controlling the per-centage of the document that should be considered as the beginning part. It ranges from 0 to 1. When = 0, p is always greater than j D j . Therefore the value of each ISF is always 0. It becomes a special case that ISFs do not a ect the retrieval process. The above ISFs are normalized to 0 to 1, and their shapes are shown in Figure 1. In this gure, the document length j D j is 100 and is 0.8.

In [3], the eld information is integrated into BM25 by us-ing weighted term frequency on each eld. Here we enhance the within-document term frequency of a query term by the ISFs to reward query terms occurring closer to the begin-ning of the document. The within-document query term frequency is linearly combined with the accumulation of a term's ISF value. where ISF could adopt any of the above de ned shape func-tion. The only parameter that is a ected is k 1 , which con-trols the non-linear tf function. The weighting function of BM25 with Rewarded Terms (BM25-RT) is as follows. where N is the number of documents in the collection, n is the number of documents containing q i , tf RT is within-document term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the average docu-ment length, nq is the number of query terms, k 3 is tuning constant defaulted to be 8, K equals to k RT 1 ((1 b ) + b dl=avdl ). In this paper, we set b = 0 : 75 and k 1 = 1 : 2, which are regarded as default parameter settings in many BM25 applications [2]. We evaluate the proposed approach on three data sets: WT2G (topic 401-450), TREC8 (topic 401-450), and Blog06 (topic 851-950). The WT2G collection is a 2G size crawl of Web documents. The TREC8 contains 528,155 newswire articles from various sources, such as Financial Times (FT), the Federal Register (FR) etc., which are usually consid-ered as high-quality text data with little noise. The Blog06 collection includes 100,649 blog feeds collected over an 11 week period from December 2005 to February 2006. For all test collections used, each term is stemmed using Porter's English stemmer, and standard English stopwords are re-moved. We use the MAP and P@10 as measurements in our experiments.

Table 1 shows the result of BM25-RT on the above three data sets using Cosine ISF, Linear ISF, and Parabolic ISF. We compare BM25-RT with BM25, since BM25-RT doesn't incorporate any eld or annotation information. Signi cant improvement is observed on all the data sets.

Figure 2 shows how the parameter in ISF a ects the retrieval performance. We have discussed in Section 2 that
