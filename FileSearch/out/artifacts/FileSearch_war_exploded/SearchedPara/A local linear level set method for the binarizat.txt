 ORIGINAL PAPER David Rivest-H X nault  X  Reza Farrahi Moghaddam  X  Mohamed Cheriet Abstract Document image binarization is a difficult task, especially for complex document images. Nonuniform back-ground, stains, and variation in the intensity of the printed characters are some examples of challenging document fea-tures. In this work, binarization is accomplished by taking advantage of local probabilistic models and of a flexible active contour scheme. More specifically, local linear mod-els are used to estimate both the expected stroke and the background pixel intensities. This information is then used as the main driving force in the propagation of an active con-tour. In addition, a curvature-based force is used to control the viscosity of the contour and leads to more natural-look-ing results. The proposed implementation benefits from the level set framework, which is highly successful in other con-texts, such as medical image segmentation and road network extraction from satellite images. The validity of the proposed approach is demonstrated on both recent and historical docu-ment images of various types and languages. In addition, this method was submitted to the Document Image Binarization Contest (DIBCO X 09), at which it placed 3rd.
 Keywords Level set method  X  Local linear modeling  X  Multi-level classifiers  X  Binarization 1 Introduction Document image binarization is an essential preprocessing step in many document-related applications, such as cleanup, storage, transmission, and offline analysis and recognition. It may be defined as the process of separating the text pixels from the background. Dramatically reducing the amount of information makes subsequent treatments simpler and more efficient. However, at the same time, any error at this step has a major influence on the performance of the following steps.
Although binarization of simple document images is straightforward, the difficulty quickly increases with doc-ument complexity. A large number of document defects may interfere with the binarization process. For handwritten doc-uments, non uniformity of stroke pixel intensities, due to ink smearing and variations in pen pressure, are common problems. For printed documents, certain font shapes with very thin segments may be problematic. Also, many physical characteristics of the original media, along with digitaliza-tion artifacts, can complicate the binarization process: paper appearance and texture, stains, bleed-through effect, and var-ious other degradations caused by exposure to humidity or light. As a result of these difficulties, no simple method could emerge as a general solution for the binarization problem. In this respect, the objective of this paper is to introduce a gen-eral purpose binarization method, which performs well in a variety of situations.

Global binarization methods [ 1 , 2 ] are interesting as they are parameterless. However, it is well accepted that, in many challenging cases, no global thresholding of pixel intensities could produce an acceptable binarization [ 3 ]. More complex schemes must then be considered in order to improve the quality of the output. In this respect, recent document image binarization methods tend to take advantage of more local cues. These approaches can be roughly classified in two main categories: color/intensity-based [ 4  X  7 ] and edge-based [ 8 , 9 ]. The first category consists of methods which consider varia-tions in color or intensity, while the second includes methods in which abrupt changes across edges are the main source of classification information. The intensity-based methods can be split into two subgroups: threshold-based methods and clustering methods [ 7 ]. Niblack X  X  method [ 5 ] can be consid-ered as the first local threshold method. However, its outputs tend to be noisy in background-only regions. Sauvola and Pietikinen [ 4 ] solved this problem by introducing a general-ization of Niblack X  X  method that tries to identify the back-ground regions. Also, multi-scale approaches have been used in many works to differentiate between the text and the inter-fering patterns [ 6 , 10 ]. The main drawback of the local meth-ods is their high computational cost and theirs tendency to fail to segment the inner parts of thick characters. Grid-based modeling, which was introduced in [ 6 ], reduces the com-putational time considerably. The integral image technique has also been used to reduce the computational cost [ 11 ]. Grid-based modeling also provides an opportunity to trans-form global thresholding methods into their local versions. For example, an adaptive method based on Otsu X  X  global thresholding method has been introduced in [ 6 ]. Clustering methods, in contrast, use various features to classify pixels into two or more classes and to locate the text pixels. Sev-eral clustering methods, such as Markovian clustering [ 12 ], K-means [ 7 ], and PCA, can be used for this purpose. The main problem with these methods is the appearance of noisy pixels on the output. However, it seems that the recently intro-duced Markovian clustering method may have the ability to remove the noisy pixels [ 12 ]. The method that placed 2nd at DIBCO X 09, proposed by Fabriozi and Marcotegui [ 13 ], will also be used for comparison purposes in this work. The method was developed based on the toggle mapping morpho-logical operator toward text localization [ 14 ]. A few other successful approaches in binarization of document images are morphological operators [ 15 ], Markov Random Fields [ 16 ], local adaptive partitioning methods [ 17 ]. Despite huge diversity in the binarization methods, It is worth noting that the proposed method is the first level set-based approach in document binarization.

Edge-based methods usually use a measure of the changes across an edge, such as the gradient [ 18 ] or other partial deriv-atives, or use an edge detector, such as Canny edge detector, directly [ 8 ]. One of the most successful edge-based methods was presented in [ 19 ]. This method is of great interest, as a variation of it won first place in the first contest on docu-mentimagebinarization(DIBCO X 09)[ 9 , 13 ].Inamostrecent published work [ 9 ], the image contrast has been used to iden-tify the text edges and to estimate the local threshold values in order to binarize highly degraded historical documents images. However, edge information alone is not always suf-ficient to perform a proper binarization of many degraded document images. Because of degradation and ink fading, the edges may not be very sharp, and therefore there is a high level of variation in the gradient across the edges over the image. This is one of the reasons why the gradient is used in this work in combination with other tools. Moreover, as a new contribution, and despite a high level of possible dissimilarity in color/intensity, we use the gradient not only for locating the edges, but also to follow the ink along the strokes. Its use, made possible using local linear modeling [ 20 ], is the key concept in the recovery of cuts and weak strokes and plays a major role in higher-level processes [ 6 ].
At a higher conceptual level, different methods can be combined in order to compensate for their individual draw-backs. Along the same line, an interesting voting scheme has been proposed in [ 21 ]. Those combination methods can be seen as high-level modeling based on the basic and funda-mental methods. As our proposed method is a basic one, com-parison with the combined method has not been performed. However, it could very well serve as an input to one of those high-level methods in order to achieve a higher performance.
The complexity of the binarization problem may be slightly reduced if some prior geometrical knowledge regard-ing the document is available or can be estimated. Typically, the size of the structures of interest is related to the pen width or to the font face. By knowing the stroke width, it is eas-ier to reject small spurious objects or to fill unwanted gaps, as has been done in recent work on strongly degraded char-acters [ 22 ]. This information may also be considered in the selection of the scale used to compute local statistics.
In this paper, an innovative binarization method is intro-duced, based on the concept of stroke map extraction and optimization. The optimization is conducted using the level set framework, which offers an elegant way to integrate var-ious priors. An implementation of the method was submitted to the Document Image Binarization Contest X  X IBCO X 09 , held at the ICDAR 2009 international conference, where it placed 3rd [ 13 ]. 1 However, this is the first time that a com-prehensive description of the method is published. The local linear regional force used in the proposed model has been introduced by some of us in [ 20 ]. However, this work is based on widely different premises. Most significantly, the global process, the involved segmentation clues, and the initializa-tion method are different.

To our knowledge this work is among the first ones to take advantage of the level set method to achieve good results in document image binarization. The quality of the results has been assessed during the DIBCO X 09 contest. The level set framework is used in this work to integrate many document-related aspects. However, the level set method is not limited to the presented forces and could be used in conjunction with other functions. The local linear model, central to our proposed method, used by the level set-based steps is able to capture configurations of smoothly varying stroke intensities. In addition, a stroke gray level (SGL) model is introduced to limit the propagation of the contour to areas that are unlikely to be part of the stroke. Finally, in our initialization step, we compute an estimate of the stroke width and use it as a prior knowledge to detect the potential stroke pixels.

The paper is organized as follows. Section 2 describes the proposed method. In Sect. 3 , the experimental results, and both the qualitative and quantitative evaluations of the method, are presented. Finally, the conclusion is provided in Sect. 4 . 2 Proposed method A novel binarization algorithm is introduced in this section, which benefits from the flexibility of the level set framework to integrate various regional information into one method. Since the level set method is a local optimization method with a tendency to fall into the nearest local minimum, a stroke map-based [ 23 ] initialization is used. This results in a three-step process: (1) A stroke map is extracted from the document image to serve as an initialization for the level set method; (2) Our level set method is used in erosion mode in order to remove less likely stroke pixels from the map; and, finally, (3) The level set method is used in free displacement mode to produce the final delineation and binarization. 2.1 Problem statement A gray-scale document image is given by u ( r )  X  X  0 . 0 , where r = ( r x , r y )  X   X  R 2 . The image domain is a rectangular lattice of height H and width W .IfanRGB color image is given as input, only the luminance component is used. The conversion is then given by 2 u = 0 . 288 u R 0 . 587 u G + 0 . 114 u B . The set of foreground pixels is denoted by S  X  and the set of background pixels by B = \ S .Itis assumed that S corresponds to the pixels of interest, which in this paper are limited to strokes and printed characters. The objective of the binarization is to produce a binary image b ( r ) , where b ( r ) = Of course, the contents of the sets S and B are not known from the beginning, and therefore they must be estimated from the data. In the following, the subscripts S and B will be used to indicate relations with the set of stroke pixels or the set of background pixels. 2.2 Local linear level set framework If a rough estimation of the stroke position is available, it is possible to set the binarization problem in a curve evolution framework. 3 The idea is to represent the stroke X  X ackground interface using closed curves. Then, the curves are evolved along an artificial time variable. At each time step, forces of various kinds attract the curve toward its best position, with respect to a certain local optimum (e.g. see Fig. 2 ). However, in the proposed method and the rest of the paper, the initial map is calculated automatically. In typical cases of docu-ment image binarization, there will be many curves to repre-sent the boundaries of the multiple strokes. In such a complex case,explicittrackingofthecontourevolutionwouldbecum-bersome, and possibly impractical, due to issues related to parameterization and the merging or splitting of the curves. It is worth noting that, for the sake of illustration, the initial map in this figure is selected manually.

The level set framework makes it possible to represent a set of curves implicitly. Its formulation is defined directly on the image grid and allows for topological change to be handled naturally. In addition, it permits the use of a large class of forces to drive boundary evolution [ 24 ]. This frame-work was introduced by Osher and Sethian in 1988 [ 25 ], and its potential for active contour segmentation began to be exploited during the second half of the  X 90s [ 26  X  29 ].
While early segmentation methods mostly exploited edge information, Chan and Vese [ 30 ], and also Paragios and Der-iche [ 31 ], among others, introduced region-based active con-tour segmentation methods which take advantage of global regional information. These methods make it possible to seg-ment regions that are not delineated by strong gradients and to tackle a broader class of problems. They are also gen-erally considered to be more robust than their edge-based counterparts.

Not withstanding their quality, many region-based seg-mentation models perform poorly on images where the statis-tics of the region vary significantly from one part to another. This is often the case with ancient documents, where the tone of the background may vary due, for example, to aging, water damage, or digitalization artifacts. Also, the variation in intensity within a single pen stroke, e.g. due to a change in the ink flow rate, may be problematic for some region-based algorithms. Local statistical models for region-based segmentation were recently introduced by several authors [ 32  X  35 ]. While this model adapts to local specificity, it also extracts some information from the structure of the gray level of the local pixels. Some of us introduced in [ 33 ]a local linear model for the segmentation of smoothly varying regions.

Introduction to the level set method The basic concept of the level set framework can be explained as follows. Within the framework, a set of closed, but possibly disjoint, con-tours C on is represented by the intersection of a surface  X ( r ) :  X  R and of the zero level z = 0. By convention,  X ( r ) , called the level set function ( lsf ), is approximated as a signed distance function of C .If  X ( r i )  X  0 the pixel at r is labeled as stroke , otherwise, the pixel is labeled as back-ground . A sketch illustrating this mode of representation is shown in Fig 1 .

Starting from an initial and arbitrary guess, C 0 , the evo-lution of C subject to some driving force F acting in the contour X  X  normal direction corresponds to the evolution of  X ( r , t ) along an artificial time variable t , as defined by the following governing equation [ 24 ]:  X  X ( r , t ) with  X ( r , t = 0 ) corresponding to C 0 . The driving force F can be spatially variant and may correspond to a combination of both internal and external forces. Internal forces are com-puted from intrinsic geometrical properties of the contour, such as its volume and curvature, and external forces can come from various image features or from a priori knowl-edge. In the following, it is assumed that | X   X ( r , t ) | X  ( 2 ), since  X  is approximately a distance function. Also, all considered forces are restricted to only act on the boundaries by using a regularized Dirac function,  X   X  [ 30 ]:  X  Equation ( 2 ) can then be rewritten as:  X  X ( r , t ) Equation ( 2 ) has been called the level set equation in [ 24 ] and is the main equation in the level set method. Equation ( 4 ) is a well-known approximation of ( 2 ) introduced in [ 30 ]. Basically, both equations represent a process by which a contour is updated iteratively subject to the net force F acting on it. As it will be seen in the following sections, all the forces considered in this work result from the mini-mization of certain quantities related to either the shape or the position of the contour, with respect to the input image intensities.

Before presenting the details of the various force acting on the binarizing contours, a high-level overview of a basic local linear level set segmentation method is presented below. Figures. 2 and 3 illustrate the basic concepts.

Themethodstarts fromacertaininitialization, whichis the contours of an arbitrary binarization of the document image. In Fig. 2 , the initial binarization was manually drawn, but in the rest of this work, the initialization is estimated automat-ically (see Sect. 2.3 ). Also, as demonstrated in Sect. 3.1 ,a good result could be achieved starting from an almost random initialization in some cases.

After initialization, the iterative evolution of the contour begins. The displacement of the contour is driven by various forces that tend to minimize certain quantities, as detailed in Sects. 2.2.1  X  2.2.4 . Central to this work are the forces induced by the local linear regional models. In order to drive the level set, the parameters of two distinct local linear models are computed: a model for the strokes and a model for the back-ground. These model parameters are computed locally, at each pixel position, by fitting a bilinear model to the intensi-ties of all the pixels that are inside the contour for the stroke model and outside the contour for the background model in a certain neighborhood. As an example, the average inten-sities of the local linear model for both the stroke and the background of the final contour in Fig. 2 are presented in Fig. 3 . Once the local linear model parameters are com-puted, the contour is displaced in such a way that the error between each pixel X  X  intensity and the corresponding model (the stroke model, if the pixel is inside the contour, and the backgroundmodelotherwise)isminimized.Sincetheparam-eters of the local linear models depend on the position of the binarizing contour, their parameters must be updated when the position of the contour changes. The iterative evolution of the contour then consists of cycles of parameter updates and contour displacement. A few instances of the contour prop-agation and the final binarization result are also presented in Fig. 2 .

The local linear-induced forces are the most important in this work. However, other requirements, such as the relative smoothness of the contour and the robustness to noise, call for the inclusion of other forces. Those forces are presented and discussed in the following sections. 2.2.1 Local linear regional force The local linear regional force is the most important force in the binarization process. It is derived from the minimization of an energy function based on a probabilistic model, which tries to follow the local variations in the text intensities.
When considering a sufficiently small scale, smoothly varying stroke and background pixels can be modeled with interesting accuracy by using two local linear models: L ( r ) = a L els [ 20 ] X  X ee Fig. 3 . If the parameters of the local linear models are known or can be estimated, a segmentation can be achieved by iteratively moving the boundaries in order to maximize the model probabilities of the observed pixel. Below, the probabilistic model and the energy function are first presented. Then, the local linear force derived from the energy function is introduced.

Probabilistic modeling and energy function The binari-zation problem can be considered as a classification problem where each pixel of the document image is either labeled as stroke or background . Given a certain binarizing contour describedbyalevelsetfunction  X  ,thepixelsonthedocument image are labeled stroke if  X &gt; 0 and background otherwise. For computational purpose, a regularized Heaviside func-tion is used to produce a soft assignment. The regularized Heaviside function used in this work is defined as follows: H ( X ) = H Given this membership, a Gaussian formulation is used to compute the stroke and the background probabilities of the local linear model for a particular pixel: P ( u ( r ) | R ) = exp  X  Here, R  X  X  L S , L B } , u x and u y denote the image gradients,  X   X  R + is a balancing parameter, and g is a monotoni-cally decreasing function of the document image gradient that decreases the weight of the gradient term close to the edges. Its definition is chosen from [ 27 ]: g ( u ) = where G is a 3  X  3 Gaussian smoothing kernel with a standard deviation of 0 . 5. A sample output is presented in Fig. 3 .The various  X  e terms in ( 6 ) represent the variance of the error. Because minimizing ( 6 ) when those terms are not fixed is difficult, a simplifying assumption that they are constant and equal everywhere was made.

By using ( 6 ) and ( 5 ), the probability of a certain labeling for the whole document image can be written as follows: P ( u | R ) = This corresponds to the probability of a given contour posi-tion with respect to the stroke and background local lin-ear models. Related probabilistic models have also been presented by other authors [ 31 , 34 , 36 , 37 ]. It is worth not-ing that the model is intrinsically balanced. This is because boundary pixels, which almost consist of the same number of stroke and background pixels, are the most influencing ones in ( 8 ). This advantage of the proposed method pre-vents high-numbered background pixels from dominating over low-numbered stroke ones.

Since, from a numerical point of view, it is easier to min-imize the negative log-likelihood of ( 8 ) than to perform its direct maximization, the following expression will be used: P ( u | R ) = X  This equation defines the local linear energy. The energy level corresponding to a given contour is the minimum if the error between the document image pixel intensities and the local linear models is minimal. A hypothesis of this work is that this state will correspond to a good binarization of the document image.

Energy minimization and local linear force The minimi-zation of ( 9 ) is performed by computing its Euler X  X agrange equationsandbyfollowinganapproximatedgradientdescent procedure. The derivation of the local linear force is briefly introduced below. For more details, we refer the interested reader to [ 20 ].
Following this scheme, the force that tends to minimize ( 9 ) is derived by keeping the local linear models L S and L fixed and by computing the gradient descent equation of ( 9 ) with respect to  X  [ 20 , 34 ]: F When integrated into the level set Eq. ( 2 ), this force can drive the level set contour toward a position that is a local minimum of ( 9 ).

Since, at initialization, the parameters of the two local lin-ear models (stroke and background) are unknown, they must be estimated from the data. Also, any motion of the level set is likely to disturb the models X  parameters, so the estima-tion will need to be refined at every few iterations. Keeping  X  fixed, the parameters of the linear models are estimated at every spatial position r . Let us introduce two sets  X  and  X  B ( r ) that correspond to the current estimation of the local stroke and background pixels. Also, let K s ( r ) ={ : max ( s square neighborhood of the pixel r and of the size W . Then, the set  X  S ( r ) membership is given by the regularized Heavi-side function ( 5 ). With those sets defined, it is now possible to compute the parameters a s ( r ), b s ( r ) and c s ( r linear model L S ( r ) by using a weighted 2D linear regression, which corresponds to the optimization of ( 6 ) with respect to the parameters of L S by keeping  X  fixed. Numerically, this is achieved by solving the following system of linear equations using any classical technique:  X   X   X  = The local linear parameters of the background are com-puted in a similar way, but with  X  B ( r ) membership given by 1  X  H ( X ( r )) . Also, it is worth noting that the sums over K can be computed using convolutions.

The system of linear equations ( 11 ) is solvable in most practical situations. However, three limiting cases where the system is underdetermined are of interest. Let N Ks be the number of pixels in K s ( r ) .(1)If N Ks = 0, that it means that the position r is far from any boundary and, in this sit-uation, the values of a s , b s and c s are not of important and they can safely be set to 0. (2) If N Ks = 1, it is not possible to compute the slope coefficients b s and c s . In this case, it is assumed that a s = u , b s = 0, and c s = 0. (3) Finally, if N Ks &gt; 1, but all pixels are on the same one-pixel-wide line, it is not possible to compute the slope in the direction perpendicular to that line. In this case, it is assumed that b specific configuration.

The regional parameters a i , b i and c i depend on the level set position and their value must be updated during the level set evolution, when the position of the contour defined by  X  changes. However, since they are costly to compute and vary slowly from one iteration of ( 4 ) to another, they may be computed every few iterations without compromising the final result.

For practical purposes, it may not be desirable to rely on the local linear model if too few pixels are inside the window defined by W . Therefore, the following formulation is used in the proposed level set schemes: F where N Kb is the number of pixels in K b ( r ) , and W min a small number related to the precision of the grid. In this work, we use W min = 9, which corresponds to a small block of 3  X  3 pixels. This force cancels itself out if the number of strokes or background pixels inside the window defined by W at a certain position r is less than a predefined number. 2.2.2 Stroke gray level regional force A stroke gray level (SGL) regional force is now introduced to guide the level set evolution from a larger scale than the local linear force, thereby ensuring greater consistency over the document image.

Key to this force is to define an SGL map [ 23 ] which rep-resents the expected gray level of the stroke at the position of any pixel of the document image. In this work, a fast and effective technique to calculate the SGL map is introduced. Starting with a few stroke pixels identified by another mean,  X  S , the SGL map can be constructed by solving a heat diffu-sion problem on the image domain, where the pixels from  X  S act as constant heat sources. A more computationally effi-cient technique, which gives similar results, has been used: 1. A binary stroke indicator image, b s ( r ) , and a stroke only image, I s , are created from  X  S , as follows: b ( r ) = 1if r  X  I ( r ) = u ( r )  X  b 2. The SGL map is created by smoothing I s with a large Gaussian kernel K  X  and by normalizing this result with the smoothed binary stroke indicator image response: sgl where  X  denotes the convolution operator, and is a small positive real number. (3) Finally, from ( 13 ), it can be noted that some region of sgl ( r ) may have a value of 0, not because the average stroke value in this area is 0, but because at this location K  X   X  b s ( r ) = 0, which indicates that no stroke pixel has been involved in the computation. To circumvent this problem, values where sgl ( r )  X  are canceled, and the value of the closest pixel of the SGL for which sgl ( r )&gt; assigned by using a distance transform algorithm [ 38 ]. In this work, the Matlab function bwdist has been used for this purpose. An illustration of the result is presented in Fig. 4 .
Once the SGL map has been computed, the SGL regional force can be defined as follows: F where H w, p is given by ( 5 ). This definition makes use of the fact that the background pixel intensities will be lighter than the stroke pixel intensities, on average. If this is not the The parameter w and p are automatically set based on the intensities of the stroke and of the background pixels. Let and  X  b be the average intensity of the pixel corresponding to the stroke  X  S and the background not (  X  S ), respectively. The parameters are calculated as follows: p = Here, parameter w can be seen as a rough estimation of the standard deviation of the stroke pixel intensities. If the dis-tance between some pixel intensity and the SGL map is less than w , the SGL force is zero. The value of the force smoothly raises to 1.0 as the intensity of the pixel gets closer to that of the background. Finally, the value of the force is 1.0 if the difference between the pixel intensity and the SGL map is more than 3 w . See Fig. 5 for an illustration. 2.2.3 Length regularization force With the level set methodology, it is classical to add a curva-ture-driven force to the evolution equation. This force tends to minimize the contour length, thereby augmenting its reg-ularity. It is defined as follows [ 25 ]: F See Fig. 6 for an illustration of the effect of this force on a contour. 2.2.4 Area expansion or contraction force A simple constant force may be added to ( 4 ) to cause the stroke area enclosed by the zero level set ( { x , y }|  X ( to tend to either expand or contract. Since crossing an image edge is usually not beneficial, this force can be modulated by using the edge indicator function ( 7 ). The resulting force can be written as follows: F where D  X  X  X  1 , 1 } defines the direction. The estimated stroke area will tend to expand if D = 1 and to contract if D = X  1. See Fig. 7 for an illustration of the effect of this force on a contour. In contrast to the length regulari-zation force, the area contraction force shrinks the contour uniformly in all directions, regardless of its local properties.
As the above discussion suggests, the level set frame-work allows the simultaneous application of a large variety of forces at the stroke X  X ackground interface. Our binariza-tion algorithm involves the combination of those forces in a specific way, as described in Sect. 2.4 . 2.3 Stroke map and a priori information An initialization map is required to identify high-probabil-ity text pixels. The other pixels, which may be degraded, will be recovered by the local linear evolution of the level set function. For this purpose, we use one of the multi-level clas-sifiers [ 23 ], the stroke map (SM). Multi-level classifiers use different features to locate text pixels. Although the infor-mation at the pixel level is helpful, a major part of the image information is carried within the spatial relations. The clas-sifiers at the content level, such as the SM, the stroke profile (SP) [ 23 ], and the stroke cavity map (SCM) [ 22 ], search for this information based on the stroke-based features. In other words, these classifiers try to use the document-related nature of the images. In the case of the SM, the likelihood of hav-ing a stroke around the pixel in question is examined based on the structure of the text pixels around it. In this analysis, the average stroke width, w s [ 6 ], is used to determine the possibility of there being a stroke around the pixel. In a new kernel-based approach, on a neighborhood of size 2 w s + 1, a score is calculated based on which an SM value is assigned to the pixel. The SM can operate on different operational regimes. For the purposes of this work, which is to avoid as many false positive pixels as possible, the SM is set to an internal high-confidence operating mode. As is obvious, the SM itself needs an initialization map in order to secure pre-estimation of the text pixels. We use the grid-based Sauvola method [ 6 ] to generate a fast and adequate initialization map for the SM.

The SM classifier makes use of a rough binarization of the input image in order to estimate the score of each pixel in terms of strokes. Let us assume that a rough binariza-tion is u BW . Based on the calculated score, each pixel is assigned to strokes (1) or to background (0). Therefore, the output of the SM is a binary image. The SM score of each pixel is calculated based on the SM score matrix, a SM , k ( k , l = X  1 ,..., 1), which will be discussed later: SM = Thr SM , min catches the isolated regions that are not strokes, and is between 1 and 9. We use Thr SM , min = 1. Thr SM , differentiates between strokes and big black regions, and is between 3 and 9. We use Thr SM , max = 9. The score matrix a SM provides a measure of the presence of black pixels around the target pixel based on the average stroke width, w s . The matrix evaluates this relation on the area around the target pixel in different directions. For this pur-pose, an overlapping system of nine patches, W k , l ,isused (see Fig. 8 ): W and a SM , k , l is defined as follows: a where Thr a is a value close to 1 (0.7) and is used to ensure that the corresponding patch is full of black pixels. 2.4 Combined-level set method The complete method is now described using previously definedformulations.Itiscomposedofthreesteps:(1)initial-ization, (2) level set evolution in erosion mode for stroke map consolidation, and (3) level set evolution in free displacement mode for contour optimization and final binarization. 2.4.1 First step: stroke map extraction In this step, an initial binarization is estimated using the method described in Sect. 2.3 . At this point, the goal is to obtain a rough binarization that retains the main features of the input image. 2.4.2 Second step: stroke map erosion The purpose of this step is to remove any miss-labeled pixels from the stroke map in order to compute, in the next step, the best possible gray level stroke map.

A level set function  X  is first computed from the stroke map extracted in the previous step using a distance trans-form algorithm 4 [ 38 ]. The level set function is constructed such that region where  X &gt; 0 corresponds to the estimated stroke pixels. Then, the stroke map is eroded by updating the level set function  X  with ( 4 ) subject to F = F to propagate the interface. Here,  X  and  X  are positive real numbers that are used to balance the effects of the three forces. After propagation of the interface, a binarization can be produced by thresholding  X &gt; 0. Force F L is the main driving force in ( 20 ) and is based on local region modeling of the underlying stroke and background properties. However, since the stroke map extracted during initialization may be imprecise, an area reduction force F A is included to promote the removal of less likely stroke pixels. In order for this to work as expected, parameter D of ( 16 )issetto  X  1. Finally, force F R tends to smooth the boundary and to produce more natural-looking results.

Strategies to choose the value of the various parameters is discussed in Sect. 3 . 2.4.3 Third step: stroke map optimization In this step, the goal is to capture the weaker parts of the strokes and to produce the final binarization. After the sec-ond step has been completed, a map indicating the most likely stroke pixels is available. From this map, an SGL is computed using the method from Sect. 2.2.2 , and the level set function  X  is further refined by updating it with ( 4 ) subject to F = F until the level set function stagnates. When comparing ( 20 ) and ( 21 ), it can be noted that in ( 21 ) the area contraction force F A has been removed in favor of the SGL-based force F
G . The purpose of the latter is to provide segmentation cues based on the local average gray level of the stroke pixels and not to enforce any geometric a priori information. In this respect, F G is complementary to F L . However, two impor-tant distinctions apply: (1) the local linear model of F L different from the local average model of F G , the local linear model being more suitable in order to follow strokes with increasing or decreasing pixel intensities and (2) the SGL map of F G is computed once, during the second step , while thelocallinearmodelparametersof F L areupdatedeveryfew iterations of ( 4 ). Roughly speaking, F L drives the binariza-tion process based on the local structure, while F G prevents the contour from leaking in regions with very different inten-sities from those of the SGL map. At the end of this process, the final binarization is obtained by a simple thresholding of the level set function, i.e.  X &lt; 0. 2.4.4 Level set evolution and stopping criterion The level set-based steps described in Sects. 2.4.2 and 2.4.3 are both iterative. Indeed, after the level set function  X  been initialized to some position, it evolves based on ( 4 ) subject to the net force ( 20 )or( 21 ), for the stroke map ero-sion step or the stroke map optimization step, respectively. The iterations are terminated when the contours stagnate . This condition is defined formally as follows: If the num-ber of pixels that have changed their label did not increase after a period of N iterations, then the iterations are stopped. Figure 9 shows a typical example of the progression of the number of pixels that changed their label with respect to the number of iterations. As we can deduce from the shape of the curve, the final result is not very sensitive to the number of iterations N used to detect the stagnation of the contour. At the same time, it is important that N &gt; 1, because the typical time step  X  t in ( 4 ) will be smaller than 1, so it may take sev-eral iterations before a pixel changes its label. In this work, we use an arbitrary large number and we fixed N = 100. 2.5 Parameter selection The initialization method presented in Sect. 2.4.1 can be con-sidered as parameterless. However, the level set formulations of Sects. 2.4.2 and 2.4.3 have various parameters that must be handpicked. More specifically, in ( 20 ) of the second step, F
L has parameters ance the various forces. In the third step, the computation of the SGL map depends on the width  X  of the Gaussian kernel and ( 21 ) has the following parameters:  X  2 , W 2 , X  , and We note that the parameters W 1 and W 2 are related to the stroke width and that the parameters  X  i and  X  i are related to the document image contrast. Thus, we set W = W 1 = W free parameters is then only 6. The parameter W depends on the extend of the text blobs. Therefore, we find it useful to relate it to the estimate of the stroke width w s computed during the initialization step (for details, see Sect. 2.4.1 and Appendix A). The following relations have been used to adapt the parameters to the input image: W =  X w s .
 In addition, because parameters  X ,  X ,  X  , and  X  account for the intensity-dependent quantities, they will give more consistent results across images if the pixel intensities are normalized. In this work, a simple linear rescaling, which maps the pixel intensities to values in [ 0 . 0 , 1 . 0 ] used.

Since the process is divided into three steps, not all the parameters have to be adjusted at the same time, but only few of them. In this work, the parameters used during the stroke map erosion step (  X ,  X ,  X  , and  X  ) are first adjusted. Once this is done, the value for  X  and  X  is fixed. It is worth noting that the parameters have a rather independent behav-ior, which make their adjustment easier. For example,  X  sets the sensitivity of the method,  X  favors straight lines, and determines the robustness to noise (maybe at the expense of recall performance). The method is much less sensitive to the specific value of  X ,  X  , and  X  . 3 Experimental results and discussion The proposed algorithm was tested on document images representing both handwritten and printed characters. The robustness of the proposed method with respect to the SM initialization is first demonstrated. Then, results obtained on images with synthetic degradations are presented. Finally, results obtained on real images are shown and analyzed. 3.1 Evaluation of the sensitivity of the level set method The proposed method has been used to binarize the document image shown in Fig. 10 . In order to assess the dependency of the proposed method on the stroke map initialization, a varying amount of salt and pepper noise have been added to the computed SM. The amount of noise used has been varied from 5 to 100% by 5% step, where p % of noise corresponds to the case where p % of the document image pixels has been randomly assigned to either 1 ( stroke )of0( background ).
As can be seen from Fig. 11 , the final results are quite robust with respect to perturbation of the SM initialization. The binarization produced by the method remains unper-turbed with level of salt and pepper noise of up to 95%. The F-measure, described in Sect. 3.4 , has been used to quan-tify the result. A F-measure of 99.56% has been obtain at all levels of perturbation, except at 100% noise where a F-mea-sure of 27.81 was obtained. When a totally randomized SM is used for initialization, the stroke and background region are inverted in some regions of the image. This phenome-non has been called local twisting in [ 39 ] and is a conse-quence of the local character of the method; one part of the image does not know about the other. This problem is easily avoided by using a SM initialization that contains at least some information.

Notwithstanding the good result presented in this experi-ment, it is important to note that a good initialization can be decisive in many cases. For example, when the image to be binarized contains large areas of stain, the local linear model may not receive enough information in order to result in a good binarization of the document image. 3.2 Evaluation on images with synthetic degradation 3.2.1 Bleed-through, blurred, noisy, and low contrast In this section, results obtained on images with synthetic degradations are presented. Four scenarios are investi-gated, as presented in Table 1 . The proposed method has been applied to each degraded image, and the result-ing binarization is fed to a commercial OCR software, Fine Reader 9. 5 As the proposed method has not been designed for such relatively high level of burring and noise, a preprocessing step has been included in scenario 2 and 3. In scenario 2, the blind deconvolution algorithm [ 40 , 41 ] with a Gaussian point spread function with size parameter  X  = 1 /( D mal image derivative, as computed with the Sobel X  X  oper-ator. In scenario 3, a median filter of size w  X  w, w = 2 w width, 6 has been applied. Binarization and OCR results are shown in Figs. 12 , 13 , 14 and 15 . The OCR error rate has been quantified using the string edit distance , as defined in [ 6 ]. For each scenario, the values obtained as a function of the degree of degradation are plotted in Fig. 16 .

As can be seen from the results, the proposed method is fairly resilient to moderate level of bleed-through, blurring, and Gaussian white noise. In those three cases, the OCR performance was not linear: The computed string edit dis-tance remained very low until the degradation level reaches a critical point, which is around d = 0 . 12 for bleed-through,  X  tion of white noise. Except for the bleed-through case, we believe that those values are much higher than what is com-monly observed in practice. In the case of bleed-through, the achieved performance level seems sufficient for process-ing of most normal and historical documents. For documents that suffer from very high level of bleed-through degradation, other methods, which take into account the verso side of the document, exist and might be more appropriate [ 42 ]. Finally, the experiments demonstrate that the binarization method is independent of contrast. 3.2.2 Simulated faded ink degradation The proposed method was also applied on images with sim-ulated faded ink degradation, which mimics the fading deg-radation of historical documents. A sample image is shown in Fig. 17 . Starting from the uncorrupted image, progressive level of degradation has been added using the following pro-cedure:
First, a random set of circles with a radius of a factor of stroke width are generated. The area within the circles is converted to gray scale using a Euclidean distance trans-form. This gray map, u gray , is used to mix the input image and its estimated background. A parameter, called the deg-radation level, and denoted as L , controls the dominance of the background on the final degraded result as follows: u where EB is the estimated background. Using a ground-truth binary map, this degradation is only restricted to the text regions. The total area of the circles is fixed to 40 percent of the image area in the following experiments.

The kind of synthetic degradation that has been added is very challenging for the proposed method because it results in faded spot on the stroke. Nevertheless, as can be seen from Fig. 17 , the method is able to correctly binarize all the text up to a degradation level of 7. Also, based on the F-measure performance presented in Fig 18 , the proposed method out-performs other methods such as Otsu X  X  and Sauvola X  X . The degradations start to impact the final result at level 7 or 8, depending on the case, which we believe is quite satisfactory. 3.3 Subjective evaluation on real images We applied our algorithm to difficult document images of Latin, Arabic, and Persian handwritten text taken from vari-ous databases: the Google Book Search dataset [ 43 ], which contains scanned, resized, and flattened images of books; a Latin and Arabic manuscript dataset [ 44 ], which contains a large number of ancient documents, including Arabic manu-scripts from Tombouctou; a dataset from the national Library of Algeria; and two old manuscripts, courtesy of the Juma Al Majid Center for Cultural Heritage (Dubai), 7 which together contain about 500 pages. Images from the DIBCO X 09 contest sample set were also used. As the ground truth for most of those images is not available, binarization results achieved by using our algorithm are presented, along with those obtained by using Otsu method [ 1 ], Sauvola X  X  method [ 4 ], the PDE-based method [ 23 ], and the methods that placed first in the DIBCO X 09. Sample results can be seen in Figs. 19 , 20 and 21 . The full images are available on the Internet. 8 The authors acknowledge that the DIBCO X 09 participants did not have access to the dataset, and therefore were not able to adapt the parameters of their methods.
 A step-by-step illustration of our method is provided in Fig. 19 d X  X . As can be seen in Fig. 19 d, the method described in [ 23 ] and used in this paper for the stroke map extraction step already performs better than both Otsu X  X  and Sauvola X  X  methods. The effect of the stroke map erosion step is illus-trated in Fig. 19 e and allows for the removal of many outlier regions. Finally, the final boundary optimization allows for both a finer delineation and the recovery of weaker strokes.
In addition, figures illustrating the behavior of the pro-posed method in presence of bleed-through (Fig. 20 ), stain (Fig. 21 ), and low-quality documents ( 21 ) are provided. In all cases, our methods clearly outperform the other methods in terms of the precision of the binarization. It is worth noting that the performance of the proposed method is not high in the case of a large amount of the bleed-through. The inter-fering patterns can pass through the rough binarization step onto the stroke map and then onto the final output. In this case, it is recommended to use the recursive binarized map introduced in [ 6 ] instead of the SM. The output of the recur-sive binarization methods consists of a scatter set of black pixels with a high degree of confidence that they belong to the text pixels. We will investigate this combination of recur-sive binarization methods as the initialization and level set framework in the future. 3.4 Objective evaluation against DIBCO X 09 The binarization algorithm described in this paper was sub-mitted to the DIBCO X 09 binarization contest, the results of which were announced during the ICDAR X 09 conference [ 13 ]. It placed 3rd among 43 algorithms submitted by 35 different international teams.

The DIBCO X 09 database 9 is composed of a total of 14 images divided into two sets: 4 images were provided as sam-ples before the contest, and 10 images were used for testing. The 10 test images were not available to participants before the contest. In the two sets, half the images were of hand-written text and the rest were of printed text. The document images present Latin texts, feature a variety of font faces and script styles, and suffer from various types of degradation. 3.4.1 Performance evaluation Various objective measures can be used to evaluate the per-formance of binarization algorithms. During the DIBCO X 09 binarization contest, four measures were used to evaluate the performance of the methods: the F-measure, the peak signal-to-noise ratio, the negative-rate metric, and the mis-classification penalty metric. [ 13 ]. However, from what can be seen in [ 13 ], those values are highly correlated. There-fore, we chose to concentrate on the F-measure, because it is well-widely accepted and simple, and so is easy to interpret. The F-measure is defined as follows: F-measure = whereRecall = TP TP + FN , Precision = TP TP + FP and TP , and FN representing the number of true positive, false pos-itive and false negative values respectively. 3.4.2 Model selection The four sample images provided for the DIBCO X 09 con-test were used for model selection. Initially, a set of param-eters was handpicked by trial and error. Then, this set of parameters was refined by a heuristic optimization proce-dure. Various values for the parameters related to the sec-ond step (  X  1 , W 1 , W min1 , X  , and  X  1 ) were tested on the four sample images. For each test, only one parameter was differ-ent from those in the original parameter set. The parameter values resulting in the best average F-measure for all four images were then selected and fixed. The same procedure was repeated to find the best parameters related to the third process step (  X  2 , W 2 , W min2 , X ,w, p , and  X  2 ). 3.4.3 Results The ten images of the DIBCO X 09 test set were processed using the model found in Sect. 3.4.2 . The values of the performance measures for the individual images are pre-sented in Table 2 . Also, sample results are presented in Figs. 22 , 23 , 24 , 25 and 26 along with results obtained using the Otsu and Sauvola methods. It can be to noted that the proposed method leads to more consistent result with more continuous strokes. 3.5 OCR evaluation with printed DIBCO X 09 images A typical application of a binarization algorithm is to prepare a document image before performing OCR. In this respect, it is interesting to measure how the proposed algorithm might improve the recognition rate of a specific OCR engine. Binarized versions of the five images representing printed documents from the DIBCO X 09 test set were analyzed using an OCR engine for analysis. For this purpose, we consid-ered the performance of the commercial OCR software Fine Reader 9, 10 and opted to use it. The ground-truth text was generated manually in the form of a string of characters. All line breaks were replaced with space characters. The OCR error, defined in [ 6 ], of various methods is provided in Table 3 . Binarized images were produced using the pro-posed method, and also Otsu X  X  and Sauvola X  X  methods. The results obtained are presented in Table 3 .

As an example of the limit of the proposed method, an image from DIBCO X 09 dataset is used in Fig. 24 . The out-puts of different methods including Sauvola X  X  method and the proposed method are presented in the figure. The Sauvola X  X  method works better in this case and discovers the degraded letters. The lower performance of the proposed method is because of the initialization limit which can be improved in future. Also, the lower performance on this image is the reason for slightly lower score of the proposed method in Table 3 . 3.6 Computational cost and complexity of the method With our current implementation, the proposed method is computationally intensive. For example, the processing of the 512  X  512 pixel image from Fig. 19 with a window size of W 1 = W 2 = 51 takes 25, 253, and 348s for steps 1, 2m and 3 of the proposed method, respectively, on a single core of a 2.6GHz AMD Opteron 885 processor.

More generally, the computational cost of the initializa-tion procedure described in Sect. 2.3 is O ( n  X  m 2 ) , where n and m are the number of pixels and the size of the window, respectively. However, the complexity reduces to O ( n ) using integral image representation [ 45 ].

For the two level set steps, the computation of ( 2 ) is linear in terms of the number of pixels, n , and is then performed in time O ( n ) .Thesumsin( 11 ) computed at each pixel location are evaluated in time O ( n  X  m 2 ) . Other operations of ( 11 )are linear in n . Overall, the computational time for those steps is then O ( n ) + O ( n  X  m 2 ) + O ( k ) = O ( n  X  m 2 ) per iteration. It is also worth noting that the distance transform algorithm used to construct the level set function is linear in the number of pixels [ 38 ]. sws (a) (b)
A few strategies can be put in place to reduce the com-putational burden. Since the parameters of the local linear model vary slowly in time, it is not necessary to compute them at every iteration, thereby reducing the actual compu-tational cost. In addition, as there is no global image opera-tion, a good acceleration is expected from a narrow band level set implementation, which would reduce the complexity to approximately O ( level set method allows for GPU implementation that could further reduce the execution time [ 46 , 47 ]. 4 Conclusion and future prospects A binarization method for old and degraded document images has been developed based on local linear probabilis-tic models embedded within a level set implementation of an active contour scheme. The local linear models of the stroke and background intensities, introduced here in the context of document image binarization, are used to derive the main driving force for the displacement of the level set function. A complementary SGL model was used to restrain the progres-sion of the contour outside regions of stroke-like intensity. Other forces, traditional in the level set framework, such as the curvature and area contraction forces, have also been used. Also, a multi-level classifier, the stroke map, has been used to provide a rough initialization of the level set function.
During our experiments, the proposed three-step binari-zation process proved very convenient. Each step has its own goal and produces intermediate results that facilitate under-standing of the algorithm. The three goals are initialization, outlier removal, and final binarization, respectively. In many cases, the initial binarization will be incrementally improved during the process, and this effect is reflected in the results presented in Table 2 . It is clear that the improvement in binarization quality due to each step of the proposed method will vary depending on the setting. For example, although the improvement due to the last binarization step in Fig. 19 might not stand out visually, inspection of the enlarged areas (Fig. 19 g X  X ) reveals that this step did improve the connectiv-ity, which might be an important cue for some recognizers. In contrast, the benefit of the last step is more perceptible in Fig. 26 . Whether this benefit worth the computational invest-ment depends on the intention of the end user.

The level set methodology is an appealing integration framework because of its elegant formulation, power, and simplicity. The method has been tested on different data-sets including DIBCO X 09 dataset, with promising results. It placed 3rd among 43 methods at the DIBCO X 09 contest.
As a prospect for the future, application of the recursive binarization methods to initialize the level set framework will be considered in order to enable the method to remove the bleed-through signatures on the highly degraded document images. Also, improvement to the driving forces to make them more adaptable to the variations on the degraded doc-ument images will be considered.
 A Estimation of the average stroke width In order to estimate the average stroke width, we first com-pute the stroke width spectrum (SWS), which is a measure of the frequency of various stroke width values on the document image. In order to determine this spectrum, a kernel-based approach on a binarized version of the image is used for each given stroke width candidate,  X  w s . For each pixel on the image domain, if the ratio of text pixels in a patch of size  X  w that pixel is higher than a threshold (say 0 . 9), that pixel is countedas  X  w s . Inorder toreducethecomputational cost, inte-gral image representation [ 11 , 45 ] is used to obtain the stroke width spectrum, a sample of which is shown in Fig. 27 as blue bars. Usually,  X  w s is tested from 3 to 23 pixels. As can be seen from the spectrum, it consists of many jumps, the highest of which can be associated with the average stroke width on the input image. This behavior of the stroke width spectrum is very similar to the first ionization potential curve in inorganic chemistry [ 48 ]. However, to increase the accu-racy of this estimation, we use a model-based approach. In this model, we assume that the ideal input images contain strips of a constant width w s . For these images, it is easy to show that SWS is equal to 1 /w s for  X  w s = w s . This model isshowninFig. 27 as a yellow-dashed line. By dividing the SWS by this model, a characteristic curve for the document is obtained which reaches its highest value at the average stroke width, w s . The curve is shown in the same figure as a continuous line and is normalized to 1. An average stroke width of 11 can be estimated from the image in the figure. References
