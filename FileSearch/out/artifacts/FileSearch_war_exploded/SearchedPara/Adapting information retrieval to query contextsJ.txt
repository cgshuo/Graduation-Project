 Jing Bai, Jian-Yun Nie * 1. Introduction tion  X  (however, it talks about bus service in Sri Lanka, which is irrelevant to the information need).
The overwhelming number of documents about  X  X  Java language  X  on the Web is not the only reason that leads to the above answers are returned for the same query for whatever user and for whatever search goal.

In reality, the query submitted by the user is not the only information about the underlying information need. In most cases, query is only a very partial specification of the information need. Many other contextual factors also provide useful
Looking at the nature of user relevance judgment, we know that a document can be judged relevant by the user for several reasons. It can provide ( Zhang, Anghelescu, &amp; Yuan, 2005 ):  X  Direct evidence, which explicitly gives an answer to a user X  X  question.  X  Indirect evidence, which lets the user infer an answer to the question.  X  Contextual evidence, which provides peripheral or background information surrounding an answer.  X  Comparative evidence, which provides a basis for interpretation or inspires some answer through perceived similarity to process is basically implemented as a word matching process.

We see here a clear gap between the ultimate goal of IR and its implementations. The study described in this paper aims at developing an approach to IR that integrates some contextual factors.

As the study in Zhang et al. (2005) shows, the desired documents are highly influenced by the specific context in which document ranking function according to the query context. So, our basic task is to make the ranking function dependent on the contextual factors as much as possible.

There are various types of contextual factor for an information need. Some typical ones are as follows: 1. On the one hand, the domain of interest of the user (for the given query) provides useful hints to help interpret ambig-uous query words such as  X  X  X ava X . It also provides a set of background terms that are often used in the domain, but usually omitted in the query. For example, the term computer rarely appears in a computer-related query. We will call this type of contextual factor the domain context . 2. On the other hand, a query is submitted to a system to retrieve documents from a given collection. A collection covers a particular set of documents. They deal with some particular topics, in certain domains, for a certain period of time, etc.
The query topic can be rarely or frequently covered by documents in the collection. Topics can also be developed together with other related topics. These characteristics of the document collection also influence the way that the documents should be ranked for a query. For example, if the query topic is developed frequently with another topic in the collection, then a document on the second topic could also be relevant to the query.

Both above contextual factors provide what is called contextual evidence in Zhang et al. (2005) . They are related to the background of the query topic or peripheral events. According to Zhang et al. (2005) , documents covering these related topics can also be desired by the user. 3. In addition, the query itself can often provide useful information about the information need. For example, the fact that mation has not been fully exploited in previous IR approaches, in particular in query expansion: Usually, the expansion terms are determined from each of the original query terms independently. In this study, we put a special emphasis on this type of the query context. The terms that appear together with another term in a query implicitly help specify a par-ticular meaning for the second term. Therefore, when we try to expand a query, context terms should be taken into account in order to reduce ambiguity.

Previous studies have found many other useful contextual factors in IR such as the user X  X  task at hand, the time constraint, this problem is to extend the query description so as to create a better and more complete description of the information need in which contextual information is integrated. The query completion process is geared to add more contextual infor-mation into the query. Therefore, we use this approach as a means to implement adaptive IR.

To illustrate that query expansion is a reasonable means to implement adaptive IR and to enhance the current IR approaches, let us consider again the previous information need on  X  X  X us service in Java X . We now take into account the current search context, i.e., the user is preparing a travel. We try to determine the relevant additional terms to add into the query to make it more complete. Assume that we have determined  X  X  transportation  X  as a relevant term and expand top results become relevant. If we add more words such as  X  X  X otel X  and  X  X  X light X  related to the Travel domain, even more relevant documents are found: 12 results among the top 20 from Google deal with  X  X  bus transportation services in Java Island  X .

This example shows the potential impact of introducing more related terms into the query: they render the query less ambiguous and more representative of the appropriate context. The above process is commonly called query expansion .It precision as well, as the above example illustrates.

The proposed framework is illustrated in Fig. 1 , where the query is enhanced by different contextual factors.
The consideration of contextual factors in IR is not new. However, many previous implementations heavily rely on heu-ristic or manual setting. In this paper, we will use the language modeling framework as the basic framework to integrate contextual factors. Our implementation is more principled than most previous ones. In addition, more contextual factors can be integrated in a similar way.

The choice of the language modeling framework is due to its robustness and its capability of capture the most important guage models can achieve high retrieval effectiveness compared to other models.

Another contribution of this study is that we will show that the integration of multiple contextual factors can be per-formed in the same framework, and more contextual factors can result in higher retrieval effectiveness. This suggests a promising avenue for adaptive IR by integrating more contextual factors in the future.

In the remaining sections, we will first describe the related approaches in the literature in Section 2. Then a general con-text-dependent model is proposed in Section 3. It is built using a language modeling approach. In Section 4 the proposed approach is then tested on several TREC collections. We will show that the proposed approach can significantly improve the retrieval effectiveness. Finally, some conclusions and future work will be presented in Section 5. 2. Related work
We will review three families of approaches directly related to this study: query expansion, pseudo relevance feedback and use of user profile. 2.1. Query expansion using term relations
Query expansion is a general approach that aims to improve query expression by adding related terms to the query. The addition of new terms extends the original query so that it has a wider coverage than the original query. This method can provide a solution to the short query problem in IR. Two key questions in query expansion are: (1) which terms are related and should be added? (2) how are new terms weighted and integrated into the query? feedback .

A thesaurus contains a set of relations defined between terms. Intuitively, it is good resource for query expansion. How-the reasons are as follows:  X  Although a thesaurus such as Wordnet contains many relations validated by human experts, the coverage is far from  X  Another problem is the lack of information about the appropriate context to apply relations. For example, Wordnet con
Co-occurrence relations or association relations are another type of relation commonly used, either to complement a the-words, for example,  X  X  york ? new  X , because the term new often co-occurs with york . One should notice that such co-occur-not truly related to these latter.
 van Rijsbergen, 1983 ). In Grefenstette (1992) , it is shown that when queries are expanded using term co-occurrence infor-mation, worse system effectiveness can be obtained. Smeaton and van Rijsbergen (1983) also did not observe a noticeable improvement using co-occurrence relations for query expansion, and this was believed to be due to the limited amount of a query does not make the original query more specific or more general. Therefore, it does not bring much new information to the query. In addition, as users tend to use frequent words in their queries, the added words also tend to be frequent words, which have poor discriminative value to distinguish a document from the others.

We observe that another important reason lies in the fact that co-occurrence relations are highly ambiguous. Indeed, co-occurrence relations are usually established between single words such as  X  X  Java ? programming  X . The validity of such a relation is strongly context-dependent: the above relation is valid only in computer-related context. However, the appro-the relation is always applied and the term programming added, leading unavoidably to much noise for non-computer-related queries. Attempts have been made to select the best expansion terms considering all the query terms. For example, one may sum up the relations of the candidate expansion term to all the query terms (Qiu &amp; Frei, 1993 ). However, this does not provide a radical solution to the problem because the term relations summed up are still created between the expansion term and each of the single query terms. It is possible that an expansion term accumulates a strong sum with the query terms, yet it is inappropriate for expanding the query in the query context. For example, the term year may have relatively strong relations with the words in the query space program because it co-occurs often with the latter in docu-ments. Yet this term is an inappropriate (and useless) expansion term for the query. This example shows that the ap-proach proposed in Qiu and Frei (1993) is ineffective in filtering out common terms that are inappropriate for query expansion.

Lau, Bruza, and Song (2004) propose a logical approach to deal with this problem. They define logical relations to encode domain knowledge (i.e., relations between terms) in different contexts. For example, in Computer Science , we have to use it in practice because of the difficulty to determine the strong logic conditions required (e.g. the negated terms).
An alternative is to use domain-specific relations to expand queries in the corresponding domain. For example, we can the appropriate relations are applied. However, in many situations only general relations are available.
In this study, we propose to use context-dependent term relations that are less strict than those defined in Lau et al. in practice. More specifically, we propose to add some context words into the condition of an implication such as in { com-Java , we will conclude that programming is related only if we observe both computer and Java in a query.
This approach shares a common virtue with some existing studies: the consideration of the relationships between terms in the same context. For example, Gao, Nie, Wu, and Cao (2004) explicitly model dependencies between query terms and document are required to contain the same term dependencies. Vechtomova and Karamuftuoglu (2007) uses lexical chains as a measure of term dependency, while Metzler and Croft (2005) use term proximity instead. In Metzler and Croft (2005) , terms are determined and term dependency is considered only for these subsets.

The above family of approaches considers term dependency from a different perspective than ours: term dependency is considered between the query terms and it is used as an additional requirement to single terms for relevant documents  X  relevant document should not only contain single query terms, but the query terms should have strong dependencies as in the query. In our case, we also consider a contextual term dependency, but for the purpose of suggesting related terms for expansion. This aspect is complementary to the previous one. 2.2. Pseudo relevance feedback and collection characteristics
Pseudo relevance feedback is another common method to determine expansion terms. These terms are extracted from the top-ranked documents returned with the original query.

It is interesting to compare query expansion and pseudo relevance feedback. Both aim to expand the original query, how-tionships are created manually in a thesaurus or extracted automatically from a document collection. In the second case, the extraction of terms is circumvented within a subset of documents that are returned by the system. The term extraction pro-cess can be considered to be similar to a co-occurrence analysis within these documents, since the terms that occur fre-quently within these documents also have high co-occurrence counts with the query terms in these documents.
However, an important difference between them is that co-occurrence only reflects the relation between a term and another term, while a term extracted from the feedback documents has implicitly a relation with the entire query. Xu and Croft (1996) called the two cases global and local context analysis, respectively.

Instead of blindly using all the feedback documents, Kurland, Lee, and Domshlak (2005) try to group the feedback doc-uments into several clusters, and only some clusters are selected and used to expand the query. Liu and Croft (2004) use a similar approach, but on document: they use the document cluster corresponding to a document to enhance (smooth with) that document.
 All the above approaches, as well as those based on co-occurrence analyses, exploit implicitly collection characteristics.
The co-occurrence relations extracted from a document collection reflect the way that terms are used in the collection: they are often used together with some other terms. The implicit exploitation of collection characteristics is even stronger in pseudo relevance feedback. The subset of documents used to determine expansion terms are those that are related to the query in the given collection. These documents strongly reflect the way that the query topic is developed and described another document collection covering a different period of time, the feedback documents would more likely describe  X  X  stock exchange  X . Therefore, feedback documents implicitly reflect some collection characteristics related to the query topic. The collection characteristics extracted from the feedback documents can also be used to expand the query. 2.3. User profile and domains of interest
A user may be interested in some particular (preferred) topics. These latter can be specified by the user or deduced auto-correspond to the profile. The hypothesis made in this approach is that a user tends to be interested in the same topics. preferred topic in the user profile may be unduly favored.
 (2002) defined a set of omains using ODP directories. The domains related to a query are identified automatically according to the query and used to re-rank the retrieval results. A similar approach is used in Wei and Croft (2007) , where domain models are created using ODP categories and user queries are manually mapped to them. However, the experiments showed variable results. In some of the cases, improvements are observed whereas in other cases, no improvement or even degra-dation is observed. It remains unclear whether domain models can be effectively used in IR.

We observe that most previous studies have often limited themselves to consider only one contextual factor. In fact, various
When multiple contextual factors are considered, they can act on the query simultaneously and may interfere with each related terms suggested by considering term relations may overlap with those extracted from the feedback documents.
However, in this paper, we will take a rather simple approach: We assume that each contextual factor tries to specify an factors. 3. A language model for integrating several context factors
The influence of a contextual factor can be modeled and used in different ways. For example, one can integrate the topic query expansion approach due to the following reasons: 1. The query expansion approach is easy to implement. It is consistent with the current IR implementation and does not require a radical change of the latter. 2. The approach can be effective to orient the search to the documents in the appropriate context. This has been illustrated by the example we showed in Section 1. 3. The document re-ranking approach used in previous studies can be considered as a special case of query expansion.
Indeed, both try to define a new scoring function for documents, but this only affects the top-ranked documents for doc-ument re-ranking, while it affects all the documents in query expansion. So, query expansion is a general way to take into account additional factors for scoring.

Our goal is to create an enhanced query by integrating all the related terms suggested by different contextual factors. We will use statistical language modeling (LM) as the basic framework. As we mentioned earlier, this framework has the qual-seems to be an appropriate framework to implement our approach.
 The basic LM approach defines a ranking score according to the negative KL-divergence as follows: divergence can be roughly explained by the following principle: if we observe a large divergence between the document model and the query model, then the document is considered not to correspond well to the query. Therefore, the negative KL-divergence can be used as a ranking function.

The most basic query model is defined by Maximum Likelihood Estimation (MLE). We will denote this model for query Q by h 0 Q . On the other hand, the document model should be smoothed to avoid the zero-probability problem. One can find sev-eral smoothing methods for IR in Zhai and Lafferty (2001) . So, the traditional LM approach can be formulated as follows:
We now assume that each of the contextual factors, when acting on a query Q , will create an additional language model for Q as follows:  X  Knowledge model: This is the result of applying term relations to the query. We call this model knowledge model because the term relations that we apply here correspond to the general knowledge. However, we will distinguish two types of term relations: context-independent and context-dependent relations, as we discussed in Section 2.3.
In particular, we will advocate the utilization of the second type of relation. By applying term relations, some addi-tional terms will be suggested. These terms define a new language model h that P t 2 V P  X  t j h K Q  X  X  1. domain of interest of the query can be used to suggest useful background terms in the domain. The background terms are the common terms subsumed by the given domain and for any query in this domain. A domain model h model that specifies the common terms used in the domain of the query.  X  Collection characteristics: A document is retrieved from a collection, which can contain documents covering a certain per-considered to be the related topics in the given collection. We use a set of feedback documents to reflect the collection characteristics related to the query. A feedback model h FB
Once all theses models are constructed, the final query model can be defined by combining them as a linear mixture mod-el as follows: model and the feedback model. The parameter a i (with P i 2 X mixture weight denotes the relative contribution of the corresponding model. So, the final query model can also be seen as a weighted combination of all the component models.
 Given the above final query model, we use the following score function based on KL-divergence to score documents: We can further define the following score function according to each of the component models ( i 2 X ): Then, the final document score can be rewritten as follows:
This combination of the scores can more explicitly show the relationship with document re-ranking, which also combines additional scores to define a final score.

Let us now describe each of the component context models  X  knowledge model, domain model and feedback model. 3.1. Knowledge model
A knowledge model is constructed from a set of term relations. Let us consider the following general form of term relations: which means when we observe the terms { t j , t k , ... } together, we can conclude that term t means that these terms appear together in a query or within a window. No other constraint is imposed on the relationship between them in this paper (although stricter relations could be imposed). The traditional context-independent term rela-tion t j ? t i is a special case of the above general form.

For example, { Java , language } ? programming means that when we observe Java and language in the same query, then the term programming can be deduced. This is a weaker relation than the logical relations used in Lau et al. (2004). Indeed, the proposed type of relation follows the same principle as Yarowsky X  X  study (1995) , which tried to determine the appropriate word sense according to one relevant context word in the sentence. However, the goal here is not word sense disambigua-to distinguish between different meanings for a word). The purpose of our word sense discrimination is to determine the related terms in the given context, and not the exact meaning of the terms.

Notice that we do not consider the case that the conclusion is also a set of terms or compound terms, although this is not precluded in principle. The reason is the current LM approach uses a unigram model. For comparability to previous ap-proaches, we have restricted the experimental work in this paper to a unigram representation, but it is of course possible to extend the approach in the future to include the compound term aspect.

The condition part of the above relation can be arbitrarily long. In practice, however, we do not need to create long con-ditions. This is because:  X  In most cases of ambiguous words, the addition of one useful context word suffices to disambiguate it.  X  When the condition of a relation becomes longer, its applicability also becomes more limited.  X  The extraction of relations with a longer condition is more complex to extract in time and space. So, one should limit the size of the condition.

Therefore, we will advocate the utilization of relations whose condition contains two terms: rence relation as simple co-occurrence relation or context-independent relation.

Term relations have been used in several recent language models in IR, either for document expansion or for query expan-sion. For example, Berger and Lafferty (1999) proposed a translation model that expands the document model. This approach is further extended in Cao, Nie, and Bai (2005) to include more types of relation. The same approach can also be used to ex-grate the simple co-occurrence relation to create a new query model as follows: By assuming P co  X  t i j t j ; h co Q  X  P co  X  t i j t j  X  , we have where P co ( t i j t j ) represents the context-independent co-occurrence relation between t resents a probabilistic term distribution: only the terms with P defined as follows: where c ( t i , t j ) denotes the co-occurrence counts of two terms in text windows of fixed size.

The above approach can be directly extended to context-dependent query expansion. To distinguish from the previous co-occurrence model, we call this model knowledge model and denote it by h where  X  t j ; t k  X 2 Q means a word couple contained in the query. The probability P follows: where c ( t i , t j , t k ) denotes the co-occurrence count of three terms in text windows of fixed size. couples in the query, i.e.
 imally the whole query (similar to minimal spanning tree). However, our tests do not show any advantage using the latter approaches. So, we will use the uniform assignment in this paper.
 The document score according to the knowledge model is then defined as follows:
Notice that in the above score function, we have to look at all the terms in the vocabulary t related terms are indeed noise and it is usually better not to retain them as expansion terms. Therefore, we can keep only a subset E of strongest expansion terms ( E is set at 100 in our case). Then the score function becomes:
The number of relations determined in this way can be very large, with a theoretical upper bound of O ( j V j relations have very low probabilities and are often noise. Therefore, we further apply the following filtering criteria on relations:  X  The mutual information between the terms MI  X  t j ; t k  X  P ( t i j t j t k )&gt; d ( d = 0.0001 in our case).

By these filtering criteria, we are able to reduce considerably the number of relations. For example, on a collection of about 200 MB, with a vocabulary size of about 148 K, we only keep about 137 M such relations, which remain tractable. No-the later added terms are relatively rare terms. As rare terms can only be combined with a small number of other terms, the number of new relations generated will be small.

To see the impact of adding a context word in the relations, let us consider one of the test queries  X  X  space program  X  and compare the expansion terms they suggest. Using context-dependent relations, we can determine the following expansion terms (the related terms are in bold): { space , program } ? shuttle :0.0174 soviet :0.0146 nation:0.0124 station :0.0105 US :0.0098 man :0.0093 year:0.0082 nasa :0.0076 launch :0.0069 flight :0.0069 administration:0.0065 defense :0.0064 develop:0.0063 challenger :0.0055 billion:0.0053 america:0.0050 budget:0.0047 center:0.0046 aeronautic :0.0044 president:0.0043 base :0.0043 mission :0.0042 war :0.0041 include:0.0039 missil :0.0038 rocket :0.0038 research:0.0037 state:0.0037 astronaut :0.0037 agence:0.0037 science :0.0037 house:0.0035 star :0.0035 american:0.0034 money:0.0033 office:0.0033 increase:0.0032 spend:0.0031 explorer :0.0031 work:0.0031 reagan:0.0030 unit:0.0030 support:0.0029 fund:0.0029 time:0.0028 million:0.0028 bush:0.0027 cut:0.0027 discovery :0.0027 satellite : 0.0022 booster : 0.0022 orbit : 0.0022 ...

Using the traditional context-independent co-occurrence relations, the following sets of terms are derived: space ? shuttle :0.0140 launch :0.0091 nation:0.0078 soviet :0.0076 program :0.0075 flight :0.0065 year: 0.0064 center:0.0064 station :0.0064 nasa :0.0060 administration:0.0057 mission :0.0048 aeronautic :0.0043 astronaut :0.0042 agency:0.0041
US:0.0040 rocket :0.0040 office:0.0038 orbit :0.0036 challenger :0.0035 satellite :0.0034 system:0.0033 time:0.0032 dis-covery :0.0032 man :0.0032 plan:0.0030 defense :0.0030 million:0.0030 day:0.0030 develop:0.0029 base:0.0029 state:0.0029 president:0.0029 air:0.0028 work:0.0026 research:0.0026 earth :0.0026 test:0.0026 missil :0.0025 include:0.0024 american:0.0023 unit:0.0023 office:0.0023 report:0.0022 make:0.0021 engin:0.0021 people:0.0021 build:0.0021 crew:0.0020 ... program ? year:0.0085 state:0.0055 million:0.0054 govern:0.0047 federal:0.0040 nation:0.0040 billion:0.0037 drug:0.0036 school:0.0034 include:0.0033 house:0.0032 percent:0.0032 people:0.0031 president:0.0030 educate:0.0029 depart:0.0029 work:0.0029 bush:0.0029 time:0.0028 develop:0.0028 office:0..0028 US :0.0028 plan:0.0028 aid:0.0027 call:0.0026 report:0.0026 fund: 0.0026 service:0.0025 administration:0.0025 cut:0.0025 cost:0.0023 company:0.0023 support:0.0023 make:0.0023 television:0.0022 money:0.0022 show:0.0021 part:0.0021 space :0.0021 spend:0.0021 bud-get:0.0021 congress:0.0020 increase:0.0020 propose:0.0020 student:0.0020 health:0.0020 american:0.0020 soviet :0.0019 country:0.0019 test:0.0019 new:0.0019 ...

Combining the two sets of expansion terms, we have space program ? year:0.0074 shuttle :0.0073 nation:0.0059 launch :0.0049 soviet :0.0048 million:0.0042 state:0.0042 administra-tion:0.0041 program :0.0037 center:0.0037 station :0.0037 flight 0.0035 US :0.0034 office:0.0033 nasa :0.0032 gov-ern:0.0030 time:0.0030 president:0.0029 agency:0.0029 plan:0.0029 develop:0.0029 billion:0.0028 include:0.0028 work:0.0027 people:0.0026 house:0.0025 mission :0.0025 system:0.0025 federal:0.0024 report:0.0024 defense:0.0024 day:0.0024 percent:0.0023 bush:0.0023 test:0.0022 call:0.0022 make:0.0022 aeronautic :0.0022 astronaut :0.0021 amer-ican:0.0021 base :0.0021 school:0.0021 unit:0.0020 research:0.0020 challenger :0.0020 drug:0.0020 depart:0.0019 air:0.0019 company:0.0019 month:0.0019 ...

Comparing the above list with that of { space , program }, we can see that the context-dependent expansion terms are more related to the query: Not only the related terms are generally ranked higher, but also several new related terms (the under-with context-dependent terms is more effective. This will be confirmed by our experiments. 3.2. Extra-query context: domain of interest
We will exploit multiple topic domains of queries instead of a single user profile. A topic domain is considered to provide a background for the interpretation of a query. One can see at least two types of useful element in a domain: 1. A domain contains a set of domain-specific term relations. For example, in Computer Science , X  X  Java ? programming  X  X sa valid relation, thus can be included in computer science-related domain. terms for the domain of Environment . These terms are often implicitly assumed when a user issues a query in the Envi-ronment domain such as  X  X  waste cleanup  X .

These two types of element suggest two possible utilizations of domain: (1) using domain-specific term relations for query expansion; (2) using domain-specific terms to complement the query. In the first case, a term is considered to be related to a query if it is connected with a term or a term combination through the term relations in that domain. So, the related terms are both domain-and query-specific. In the second case, all the terms specified in the domain model are considered to be related to any query in the corresponding domain. Therefore, the expansion process is domain-specific.

The first utilization is similar to the utilization of general term relations in query expansion we just described, ex-cept that general term relations are now replaced by domain-specific relations. Intuitively, this approach seems to be a reasonable way to deal with ambiguities in query expansion. For example, one can extract co-occurrence term relations from a specific domain, and use these relations to expand queries in that domain. However, the domain-specific rela-tions can be limited in coverage: queries can also contain general terms in addition to domain-specific terms, to which domain-specific knowledge does not apply. In addition, domain-specific knowledge may overlap the general knowledge.
When the later is already disambiguated by the introduction of additional context words, they play a similar role to domain-specific knowledge. Therefore, we will focus on the utilization of domain model as a set of specific terms (although we will also test the other method). This strategy has been used in most previous studies on personalized IR.
 Let us denote the domain model for the query as h Dom Q . The score function according to the domain model is as follows: In order to use the domain models as above, we have to deal with two problems: How to construct a model for each domain? How to select the corresponding domain model for a query? 3.2.1. Defining domain models
A domain model  X  a probability distribution over terms, can be constructed using a set of documents in the domain. To do this, we assume that each domain contains a set of documents classified in it. These documents can be identified in two dif-ferent ways: (1) using some existing definition of domains; (2) define one X  X  own domains.

One can take advantages of an existing domain hierarchy such as ODP and Yahoo! Directory , which also contain a set of documents manually classified in each of the domains. The documents manually classified can be used to build domains is inappropriate for a user and the user has to define his own categories. In some cases, the user X  X  own domains can be man-ually mapped to an existing domain hierarchy. This is the approach used in Wei and Croft (2007) . However, this approach is not always feasible.

An alternative is to allow the user define his own domains and ask him to assign a domain Dom  X  Q  X  to his queries for a period of time in order to collect example documents for each domain. In this case, we can collect example documents in several ways:  X  The user can judge the relevance of the documents to a query. The relevant documents can be classified into the domain
Dom  X  Q  X  of the query and then used in the construction of the domain model. However, the approach is difficult to imple-ment as most users consider the judgment of relevance as a burden and are not willing to do it.  X  As an alternative, we can collect the documents that the user chooses to read or browse through for a query and include more feasible in practice.  X  One can also assume that the top-ranked documents are closely related to the query, thus should be classified into the domain Dom  X  Q  X  .

In our experiments, we will simulate and compare the first and the third approaches using TREC data. In the third ap-proach, we will use the top 100 retrieved documents for each query. The second approach cannot be tested in our study be-tested in the future.

Note that the third approach above is different from the feedback model: the third approach constructs a language model for the domain using the top-ranked documents of all the queries in the domain; while the feedback model is constructed for a single query. However, similar processes are used to construct both models.

Given a set of documents in a domain, the simplest approach to construct a domain model is to use MLE. However, the doc-uments in a domain do not only contain domain-specific terms. General terms in a language also occur frequently. Therefore, by MLE, both domain-specific and general terms will be mixed up. What we desire, however, is a domain model that focuses on domain-specific terms only. It is then necessary to purify the domain model so that common terms can be filtered out.
To do this, we employ expectation maximization (EM) ( Dempster, Laird, &amp; Rubin, 1997 ) algorithm to extract the specific
In this process, we assume that each document in the domain is the result of generation from both the domain-specific model (to be extracted) and the general language model (approximated by the collection model). The goal of the EM process is to extract the domain model such that the likelihood of the domain documents can be maximized.

More specifically, the likelihood of a domain document D is expressed as the following generation from a mixture of the domain model and the collection model: where tf ( t , D ) is the term frequency of t in document D and k advocated in Zhai and Lafferty (2001) , where the same process is used to extract a feedback model. The reason to set k the same as in Zhai and Lafferty (2001) : we assume that each document in the domain to be generated partly from the do-main model and partly from the collection model, which represents the noise; and the relative contributions of the two parts in the language can be attracted by the collection model and the domain model can be purified.

The EM algorithm is used to extract the domain model h Dom uments in the domain), that is:
During the EM process, the updating functions are as follows: where the superscript ( n ) means the value at step n . To start, we can assign P
The effect of the EM process can be observed in Table 1 , which shows some words in the domain model of  X  X  Environment  X  before and after EM iterations (after 12 iterations).

We can see in the left part that the probabilities of domain-specific terms are much increased, while those of the general domain much better than with the MLE model. 3.2.2. Determining query domain
Once a set of domain models has been built, the next question is to assign the appropriate domain to a new query. We can native is to determine the query domain automatically.
 Bayes, etc. In our implementation we use language modeling to classify a query. This approach is an extension to the Na X ve
Bayes approach, and has been used in Peng, Schuurmans, and Wang (2004) Bai, Nie, and Paradis (2004). We use this ap-proach because we already have the domain model constructed. So, a classification using language modeling does not re-quire much additional cost. This approach can be described as follows: Given a set of domain models, we select the closest one which has the highest score with the query, i.e.: 3.3. Extra-query context: feedback model and query X  X  collection context
Another extra-query context is collection characteristics related to the query. Many attempts have been made in IR to create query-specific profiles that reflect some collection characteristics. The most common method is based on implicit the feedback model. The process is the same as we described for the extraction of domain models. So, we will not give more details here. In our implementation, we will use the top 10 documents as pseudo feedback documents, and we will keep 100 top terms as expansion terms. 3.4. Parameter tuning
As several component models are combined to form the final query model, an additional question concerns the setting of the mixture weights. Two methods can be used to tune these parameters: with a training dataset containing queries and relevance judgments, or without relevance judgments. 3.4.1. Tuning with relevance judgments
We assume that we have a set of training queries with relevance judgments. Using such a training data, we can directly the training dataset.
 Assume a set of parameters a  X h a 0 ; a K ; a Dom ; a FB i . The query model is described as: where X = {0, K , Dom , FB }. The document score is determined as follows:
The parameter tuning problem is defined as follows:
This maximization problem cannot be solved using methods such as gradient descent because the objective function MAP  X  a  X  eters in turn while keeping the other parameters unchanged in each iteration, until reaching the maximum MAP.
It is known that this algorithm can be trapped in a local maximum. In order to avoid this situation to some extent, we repeat the line search 10 times, each from a random starting point. The best values among the 10 runs are kept. This ap-proach has been used in IR and turned out to be effective (Gao, Qi, Xia, &amp; Nie, 2005 ).

The above training process determines the values of the parameters on a training dataset. Then we assume that the same reasonable for a new dataset. These values are often close to the optimal values. So the parameters seem to be quite stable across document collections and queries. 3.5. Tuning without relevance judgments
In many cases, we do not have relevance judgments. In this situation, we can exploit the set of feedback documents as pseudo relevant documents and try to determine the parameters a such that they maximize the likelihood of the feedback documents. This principle has been widely used in IR, especially in the language modeling framework (Kurland et al., 2005; Lavrenko &amp; Croft, 2001; Zhai &amp; Lafferty, 2001 ). We follow the same principle here.
 Given a set of feedback documents FB , its log-likelihood according to the query model is defined as follows:
However, the above likelihood cannot be maximized because of the zero-probability problem. We need to smooth the query model with the collection model as follows: We will further fix the weight a C at some value (0.5) for the same reason as for k determined using EM algorithm. These update functions derived are as follows (where X 0 = {0, Dom , K , FB , C }): fast. In all, the EM process takes about 5 s in our experiments. 4. Experiments
The experiments aim to test whether the consideration of the contextual factors is beneficial, i.e., whether it produces and to use the contextual factors are feasible in practice. 4.1. Test collections
The main dataset are those from TREC Disks 1 X 3 ad hoc tracks, including topics 1 X 150. This collection is chosen because the topics 1 X 150 contain a manually specified domain for each topic. This is the only set of TREC topics with such domain information. This allows us to test and compare different methods for query domain identification. We use topics 1 X 50 as our training topics, while topics 51 X 150 are used as test topics. Thirteen domains are defined for these topics and assigned to topics. Below is an example of TREC topic (#55):
We choose to use only the title of topic as our query because this corresponds to a more realistic situation  X  user queries are usually very short.

The approaches described in the previous sections have been integrated with the Lemur toolkit. toolkit to build different component query models and to integrate them. In all experiments, terms are stemmed using the Porter stemmer and stopwords are removed.

As additional test collections, we also use the collections of TREC 7 and 8, for which no domain is manually indicated for queries. These data are mainly used to test whether we can automatically assign a domain to each query and whether this is helpful. Some statistics of the data are described in Table 2 .

Notice that some queries only contain one word: there are, respectively, 4, 5 and 3 such single-word queries in the three test query sets. For these queries, context-dependent term relations do not apply.

For each query, we will retrieve 1000 documents. These documents are used to evaluate the average precision (AvgP). In addition, we also provide the precision at 10 documents (P@10) and recall (the number of retrieved relevant documents) as additional measures. 4.2. Baseline methods
Two baseline models are used: the classical unigram model without any expansion, and the model expanded with the feedback model. This latter model is also used as a baseline because this is a common method in current IR, and it produces the state-of-the-art effectiveness.
 The second baseline method is exactly the approach that expands the original query by the feedback model.
In all the experiments, document models are smoothed by Jelinek-Mercer smoothing. This choice is made according to the are heavily expanded, they become similar to long queries. The smoothing parameters are set to maximize the effectiveness of baseline models.

Table 3 shows the retrieval effectiveness on all the collections with the two baseline methods. As found in many previous studies, the model with feedback performs much better than the basic unigram model. The differences between them are statistically significant.

In addition to the increase in average precision, we also observe that P@10 and Recall are also increased. This shows that the integration of the feedback model does not only increase recall (as one usually suggests), but also precision.
This experiment confirms the advantage of considering the feedback model often shown in previous studies. As we dis-cussed, the feedback model reflects indeed some query-related collection characteristics. Therefore, this experiment also shows the advantage of considering query-related collection characteristics. 4.3. Using knowledge model
Let us first test the integration of a knowledge model. We compare the utilization of context-dependent relations with the simple co-occurrence relations. The column Without FB is compared to the baseline model without feedback, while With FB compared to the baseline model with feedback.

We can observe in Table 4 that, without feedback model, context-independent co-occurrence relations produce some improvements, varying from 5.53% to 20.00% However, context-dependent relations can produce much larger improvements in all cases, ranging from 14.12% to 37.83%.

When the feedback model is added, we still observe some additional improvements with both types of relation, however, the improvement scales are reduced. This is not surprising because the feedback model and the knowledge model can suggest some expansion terms in common. Despite this, the improvements generated by addition of the knowledge model are still knowledge model is different and complementary.

On Recall, we also observe improvements in all the cases when the knowledge model is added. This confirms the positive impact of query expansion using knowledge model on recall.
 On P@10, in most cases, we also observe improvements when the knowledge model is added, except the case of TREC8/ process for parameter tuning tries to optimize the MAP, without considering P@10. So the optimal parameters for MAP could be different from the optimal ones for P@10.

Despite this exception, we can still conclude that the expansion with the knowledge model usually improves precision for the top-ranked documents.

We also tested context-dependent term relations with three terms in the condition part, i.e., relations of the type { t , t , t k } ? t . However, the effectiveness is lower than relations with two terms in the condition. This can be explained by cannot be found through co-occurrences. (2) There are fewer queries containing more than two terms. So, more complex relations are less applicable. (3) The addition of one context word in term relations is usually sufficient to disambiguate the meaning of query words. The addition of more context words is often unnecessary. Therefore, the relations with two terms in the condition seem to represent a good compromise, especially considering the higher time and space complexity required for longer conditions. 4.4. Using domain model In this section, we test several strategies to create and to use domain models.

Strategies for creating domain models:  X  Manual collection (C1) : With this strategy, we collect documents that are classified manually into domains. To simu-late this process, we use the relevant documents manually judged for queries in the domain to select example documents for each domain. In order to avoid bias, when we test on a query, only the relevant documents for the other queries are used to build domain models.  X  Automatic collection (C2) : This strategy simulates the situation where we do not have manually judged documents in domains. However, the user is willing to indicate the domain of his queries during the period of domain construction. To collect documents for different domains, we simply use the top-100 documents retrieved with the queries in the corre-sponding domain. This is a strategy similar to those which observe the user X  X  interactions with the system in order to perform personalized IR. Again, we exclude the current test query in the domain construction process.
Strategies for using domain models:  X  Manual classification (U1) : The domain model for a test query is determined by the user manually . This strategy can be tested on the first dataset (TREC Disks 1 X 3).  X  Automatic classification (U2) : The domain model for a test query is determined by the system automatically using query classification. This strategy can also be tested on the second and third datasets (TREC 7 and 8). 4.4.1. Comparison between methods of creation of domain models
We compare strategies C1 and C2 for domain model creation. In this series of tests, each of the queries 51 X 150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.

In Table 5 , we use manual identification of query domain for Disks 1 X 3 (U1), but automatic identification for TREC 7 and 8 (U2). For TREC 7 and 8 queries, we assume that the desired domains are those defined for Disk 1-3, and the models are cre-ated using the documents in Disk 1-3.

First, it is interesting to observe that the incorporation of domain models can generally improve retrieval effectiveness in all the cases. The improvements on Disks 1 X 3 and TREC 7 are statistically significant. However, the improvement scales are smaller than using feedback and knowledge models (see Table 4 ). The smaller improvements can be partly explained by the we can see that we only have few training queries in several domains. In addition, topics in the same domain can vary can only cover a few topics in the domain. It is expected that this problem can be partly solved when the number of docu-ments in a domain grows: the coverage of the topic domain will become better.

Second, we observe that the two methods to create domain models (C1 and C2) perform equally well. In other words, providing relevance judgments for queries or performing a manual classification does not add much advantage for the pur-pose of creating domain models. This may seem surprising. However, an analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain. By using the documents judged relevant to the queries in the domain, we can certainly filter out all the documents outside the domain. In this case, the domain model ex-ploits the truly in-domain documents. However, when we use the top-ranked documents, we should also notice that the top-ranked documents are usually related to a query of the domain to some extent. Even if some of them are irrelevant to the query, this does not mean that they are not in the corresponding domain. For example, a document about  X  X  X ew York stock exchange X  may be irrelevant to a query on  X  X  X nsider trading X , but it is certainly within the domain of the corresponding do-main  X  Economics . These irrelevant but in-domain documents can also be useful to determine the most commonly used terms in the corresponding domain. Therefore, for the purpose of domain model construction, we do not need to judge if the documents are relevant to queries in the domain. They are only required to be related to the queries. This result opens the door for a simpler method to exploit user X  X  search history without requiring relevance judgments.
Finally, we can observe that in both cases with and without feedback models, the impact of the domain model is steady  X  the improvement scales are comparable. This indicates that the effects of the feedback model and the domain model differ and are complementary. It is thus beneficial to combine them together. 4.4.2. Determining query domain automatically Here, we examine the possibility to identify the query domain automatically, once domain models have been constructed.
Table 6 shows the results with this strategy using both ways to construct domain models. We can observe that with auto-domains (see For TREC 7 and 8 queries, we assume that the desired domains are those defined for Disk 1 X 3, and the models are created using the documents in Disk 1 X 3 Table 5 ).

This shows that automatic domain identification is a feasible alternative to the manual identification. Therefore, once do-main models are constructed, the user can issue queries as usual without having to indicate their domains, and the domain model constructed previously can be automatically used to enhance these queries.

Looking at the accuracy of the automatic domain identification, however, we observe a quite low accuracy: for queries 51 X 150, only 38% of the domains determined automatically correspond to the manually identified ones. Table 7 shows more details about the correctness of automatic domain identification.
 contain similar terms. In this case, a query in one of these domains can be easily classified into a wrong domain.
However, in this situation where domains are close, a wrong domain assigned to a query is not always irrelevant and use-low usefulness of the domain models.

The above observation shows that query classification can be performed in a much more flexible way. A query can be classified into several domains. In addition, the manually defined domains can also be replaced by automatically identified clusters. Indeed, a cluster grouping similar documents dealing with similar topics can reflect the background terms equally (2005) and Liu and Croft (2004), which use document clusters to complement the query or the document representation. The utilization of clusters as query domains is an interesting aspect to study in the future. 4.4.3. Extracting term relations within domains
We mentioned that one possible approach to select more relevant expansion terms is to use domain-specific term rela-tions. In the following experiments, we will test whether it is better to exploit term relations from documents in each domain.

Again, we use the relevant documents judged for the other queries to constitute a domain. Both simple and context-dependent co-occurrence term relations are extracted from the documents in the domain only. We can then expect that these terms are more domain-specific than the general term relations extracted from the whole collection. The domain-spe-cific term relations are used to expand queries in the same way as general term relations. Table 8 shows the results using domain-specific relations.

Let us first compare the two ways to exploit a set of in-domain documents: use them to construct a domain model (as a term distribution) and using them to extract domain-specific term relations. From this table, we can see that the second method can make a larger impact on retrieval effectiveness than the former. This may show that the domain model we used that query. Some of these terms may not be related to the query, even though they are commonly used in the domain. For example, the term  X  X  X xchange X  is commonly used in the domain of Economics , but it is not necessarily a useful expansion term for the query  X  X  X irbus subsidiary X . In contrast, when we apply the relations extracted from the domain, only the terms related to the query are added. So, we can expect adding better expansion terms.

It is interesting to compare domain-specific term relations with the term relations extracted from the whole document collection. Comparing Table 8 to Table 4 on context-independent relations, we can see that the domain-specific co-occur-tion, which is highly ambiguous, it is useful to restrict its extraction within a specific domain.

However, for context-dependent term relations, the relations extracted from the whole collection are better than domain-specific ones. This can be explained by the fact that the context-dependent relations are much less ambiguous by nature.
Even if context-dependent relation is extracted from the whole collection, it runs much less danger to be applied in a wrong context. The fact that we restrict the extraction within a domain will simply reduce the coverage of the relations. Many context-dependent relations.

This result also validates our earlier hypothesis that by adding some context words into terms relations, the relations be-come less ambiguous and can be applied in the correct contexts. 4.5. Complete model
The results with the complete model are shown in Tables 9 and 10 . This model integrates all the components described in line methods. The improvements indicated in ( ) and [ ] are, respectively, over unigram model with or without feedback model.

In these experiments, the parameters are tuned using the line search method (Press, Teukolsky, Vetterling, &amp; Flannery, 1992) on queries 1 X 50. The values of the parameters are: a for the other test collections.

Our first observation is that the complete models always produce better results (AvgP) compared to the other cases where only some of the query models are used. All the improvements over both the baseline models (with or without feedback) are statistically significant. This result confirms that the integration of other contextual factors is beneficial. produced the highest improvements over the original query model. However, even with lower weights, the other models do have strong impacts on the final effectiveness. This demonstrates the benefit of integrating more contextual factors in IR.
We can also compare the complete model with others on Recall and P@10. On Disk 1 X 3 and TREC 7, both measures are increased along the increase in AvgP. The case of TREC 8 is slightly different. For the complete model, both Recall and P@10 can be slightly lower than some of the partial models. For example, the Recalls for the complete model using C1 and C2 are, respectively, 3321 and 3322, which are slightly lower than 3338 obtained with the partial model that integrates only the feedback and the context-dependent relations (Table 4). P@10 for the complete model using C2 (0.4960) is also slightly lower than the same partial model. This observation shows that recall and P@10 do not always increase when AvgP increases. As we discussed earlier, the main reason may be the fact that all our settings in these experiments try to maximize AvgP. They are not tuned to maximize Recall and P@10. For precision-oriented IR, the parameters can be set to maximize P@10. Let us summarize all the experiments presented so far in Table 11 .
 Table 11 shows the following trend on all the collections: From this, we can conclude that the more we incorporate contextual factor, the more the final model is effective. On individual contextual factors, we can also observe the following general order: 4.6. Sensibility of the mixture weights We have performed an exhaustive test on all the possible combinations of mixture weights on different test collections.
The best settings are very similar in all the cases. They are always in the following ranges: 0.1 0.1 6 a Dom 6 0.2 and 0.5 6 a FB 6 0.6. More specifically, for both manual and automatic identifications of query domain (U1 and U2), the best settings are: ing dataset to determine them. 4.7. Using unsupervised training
We also tested the utilization of EM algorithm to tune the mixture weights for each query. Tables 12 and 13 describe the results.

From the above tables, we can observe that in general, the retrieval effectiveness is slightly lower than that with the weights trained on the training data. This is not surprising because we now use an unsupervised learning. However, we still nificant. However, on the third collection, the effectiveness is lower than the baseline with feedback. This means that the addition of the knowledge model and the domain model to the feedback model leads to a decrease in retrieval effectiveness.
This decrease can be explained by the fact that we have tuned the parameters to maximize the likelihood of the feedback documents. This training process is biased toward the feedback model. It cannot result in the best parameters to maximize tion to maximize in an unsupervised learning for IR.

Overall, the comparison between the two training methods indicates that if we have a training data with relevance judg-ments, then a supervised training is preferred to an unsupervised training. Especially, we have observed that the optimal parameters are quite stable across collections. In absence of such a training data, an unsupervised training can be used as a reasonable alternative. 5. Conclusion
Traditional IR approaches usually consider the query as the only element available about user X  X  information need. In real-ity, other contextual factors exist and they help specify the information need from different perspectives.
In this paper, we consider that IR adaptation should go through the consideration of more contextual factors. A document each contextual factor is used as a means to expand the query. We showed that the query expansion approach is general, and more contextual factors can be integrated using the same principle.

In this study, we exploited several contextual factors to suggest good expansion terms for the query. We proposed to cre-ate three types of language model using context-dependent term relations, the background terms in the topic domain of the query and pseudo feedback documents that reflect the collection characteristics related to the query. The expansion of query aims to increase not only recall, but also precision. As our experiments have shown, when relevant terms are added into a query, the retrieval effectiveness is increased on all recall levels, including for the top-ranked documents in most cases.
Previous studies have also tried to add terms into queries, basically through the following approaches: query expansion using terms relation, query expansion using pseudo relevance feedback, personalization. In comparison to these existing methods, we have developed new approaches to use these contextual factors and to integrate them. These approaches con-stitute our main contributions in this study.
 Intra-query context  X  knowledge model:
We observed that in traditional approaches to query expansion using term relations, the relations are usually cre-ated between two single terms. This led to the application of inappropriate relations. The key problem that we observed is the lack of context in the relations. Therefore, we proposed the creation of context-dependent term relations, which contain more terms in the condition of a relation that help specify the correct situation to apply the relation. Our exper-to add many context terms. With only one additional term in the condition part of relations, we can obtain the best results.
 Extra-query context  X  domain model:
User profile is often created to indicate the general interests of the user. However, we observed that such a user profile that mixes up all topic domains may not be effective for new queries not related to the topic. Therefore, we proposed to model topic domains instead of creating one single user profile.

We have tested two different ways to create and two use domain models. We have shown that the example documents collected for domain model training do not need to be judged manually. Simple methods can be used, for example, by gathering the top-ranked documents for queries in the domain. We have also shown that once the domain models are constructed, queries can be automatically classified into domains.
Overall, our study aimed at the consideration of contextual factors in IR operations. We have successfully shown that this is feasible and it can produce a large impact on the retrieval effectiveness.

This work has explored several aspects of context-sensitive IR. On each of the aspects, our approaches can be further improved.

Different contextual factors are considered to be independent. They are combined using a simple interpolation. In reality, textual factors.

We have used topic domains to replace user profile. In reality, domain and user profile can be used together. For example, classified into one of the domains in the user profile. In this way, we can take advantage of domain models to apply the appropriate terms to a particular query. On the other hand, we also have a better idea about the preferred topic domains cannot be always classified in terms of topic domains only. Many preferences are not related to topic domains, such as the preferred medium, document source, and so on. Therefore, other types of user models are required for them. The approaches user preferences and behavior.

Our investigation is based on the language modeling framework. It is also limited by this. In particular, we only used uni-account for term dependencies. Traditional n -gram models do not seem to solve this problem, as (Song et al., 1999 ) showed.
A possible solution may lie in the integration of NLP techniques within the language modeling framework as in Gao et al. (2004) .

In this paper, we have considered only three contextual factors. Many other factors exist. More contextual factors can be investigated. In addition, the interactions between different contextual factors should be investigated.
Finally, we have tested our approach using TREC data, which do not contain all the characteristics of Web search. It would be interesting to investigate the impact of this approach in real Web search environment with real users. In addition, more contextual factors, such as the previous search history of the user, could also be integrated.
 References
