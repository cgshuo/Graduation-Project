 Recommendation based on user preferences is a common task for e-commerce websites. New recommendation algo-rithms are often evaluated by offline comparison to base-line algorithms such as recommending random or the most popular items. Here, we investigate how these algorithms themselves perform and compare to the operational produc-tion system in large scale online experiments in a real-world application. Specifically, we focus on recommending travel destinations at Booking.com , a major online travel site, to users searching for their preferred vacation activities. To build ranking models we use multi-criteria rating data pro-vided by previous users after their stay at a destination. We implement three methods and compare them to the current baseline in Booking.com : random, most popular, and Naive Bayes. Our general conclusion is that, in an online A/B test with live users, our Naive-Bayes based ranker increased user engagement significantly over the current online system.
This paper investigates strategies to recommended travel destinations for users who provided a list of preferred activ-ities at Booking.com , a major online travel agent. This is a complex exploratory recommendation task characterized by predicting user preferences with a limited amount of noisy information. In addition, the industrial application setting c  X  comes with specific challenges for search and recommenda-tion systems [11].

To motivate our problem set-up, we introduce a service which allows to find travel destinations based on users X  pre-ferred activities, called destination finder 1 . Consider a user who knows what activities she wants to do during her hol-idays, and is looking for travel destinations matching these activities. This process is a complex exploratory recommen-dation task in which users start by entering activities in the search box as shown in Figure 1. The destination finder service returns a ranked list of recommended destinations.
The underlying data is based on reviews from users who have booked and stayed at a hotel at some destination in the past. After their stay, users are asked to endorse the destination with activities from a set of  X  X ndorsements X . Ini-tially, the set of endorsements was extracted from users X  free-text reviews using a topic-modeling technique such as LDA [5, 14]. Nowadays, the set of endorsements consists of 256 activities such as  X  X each, X   X  X ightlife, X   X  X hopping, X  etc. These endorsements imply that a user liked a destination for particular characteristics. Two examples of the collected endorsements for two destinations,  X  X angkok X  and  X  X ondon X , are shown in Figure 2.

As an example of the multi-criteria endorsement data, consider three endorsements: e 1 =  X  X each X  , e 2 =  X  X hopping X  , and e 3 =  X  X amily Friendly X  and assume that a user u j , after visiting a destination d k (e.g.  X  X ondon X ), provides the review r ( u j , d k ) as: This means our user endorses London for  X  X hopping X  only. However, we cannot conclude that London is not  X  X amily Friendly X . Thus, in contrast to the ratings data in a tradi-tional recommender systems setup, negative user opinions are hidden. In addition, we are dealing with multi-criteria ranking data.

In contrast, in classical formulations of Recommender Sys-tems (RS), the recommendation problem relies on single rat-ings ( R ) as a mechanism of capturing user ( U ) preferences for different items ( I ). The problem of estimating unknown ratings is formalized as follows: F : U  X  I  X  R . RS based on latent factor models have been effectively used to under-http://www.booking.com/destinationfinder.html of recommended destinations (top 4 are shown). stand user interests and predict future actions [3, 4]. Such models work by projecting users and items into a lower-dimensional space, thereby grouping similar users and items together, and subsequently computing similarities between them. This approach can run into data sparsity problems, and into a continuous cold start problem when new items continuously appear.

In multi-criteria RS [1, 2, 12] the rating function has the following form: The overall rating r 0 for an item shows how well the user likes this item, while criteria ratings r 1 , . . . , r n insight and explain which aspects of the item she likes. MCRS predict the overall rating for an item based on past ratings, using both overall and individual criteria ratings, and rec-ommends to users the item with the best overall score. Ac-cording to [1], there are two basic approaches to compute the final rating prediction in the case when the overall rating is known. In our work we consider a new type of input for RS which is multi-criteria ranking data without an overall rating.
 Figure 2: The destination finder endorsement pages of London and Bangkok.

There are a number of important challenges in working on the real world application of travel recommendations.
First, it is not easy to apply RS methods in large scale industrial applications. A large scale application of an un-supervised RS is presented in [9], where the authors ap-ply topic modeling techniques to discover user preferences for items in an online store. They apply Locality Sensitive Hashing techniques to overcome performance issues when computing recommendations. We should take into account the fact that if it X  X  not fast it isn X  X  working. Due to the vol-ume of traffic, offline processing X  X one once for all users X  comes at marginal costs, but online processing X  X one sepa-rately for each user X  X an be excessively expensive. Clearly, response times have to be sub-second, but even doubling the CPU or memory footprint comes at massive costs.

Second, there is a continuous cold start problem. A large fraction of users has no prior interactions, making it impos-sible to use collaborative recommendation, or rely on his-tory for recommendations. Moreover, for travel sites, even the more active users visit only a few times a year and have volatile needs or different personas (e.g., business and leisure trips), making their personal history a noisy signal at best.
To summarize, our problem setup is the following: (1) we have a set geographical destinations such as  X  X aris X ,  X  X on-don X ,  X  X msterdam X  etc.; and (2) each destination was re-viewed by users who visited the destination using a set of endorsements. Our main goal is to increase user engage-ment with the travel recommendations as indicator of their interest in the suggested destinations.

Our main research question is: How to exploit multi-criteria rating data to rank travel destination recommen-dations? Our main contributions are: The remainder of the paper is organized as follows. In Section 2, we introduce our strategies to rank destinations recommendations. We present the results of our large-scale online A/B testing in Section 3. Finally, Section 4 concludes our work in this paper and highlights a few future directions.
In this section, we present our ranking approaches for rec-ommendations of travel destinations. We first discuss our baseline, which is the current production system of the desti-nation finder at Booking.com . Then, we discuss our first two approaches, which are relatively straightforward and mainly used for comparison: the random ranking of destinations (Section 2.2), and the list of the most popular destinations (Section 2.3). Finally, we will discuss a Naive Bayes ranking approach to exploit the multi-criteria ranking data.
We use the currently live ranking method at Booking. com  X  X  destination finder as a main baseline. We are not able to disclose the details, but the baseline is an opti-mized machine learning approach, using the same endorse-ment data plus some extra features not available to our other approaches.
 We refer further to this method as  X  X aseline X .

Next, we present two widely eployed baselines, which we use to give an impression how the baseline performs. Then we introduce an application of the Naive Bayes ranking ap-proach to multi-criteria ranking.
We retrieve all destinations that are endorsed at least for one of the activities that the user is searching for. The retrieved list of destinations is randomly permuted and is shown to users.

We refer further to this method as  X  X andom X .
A very straightforward and at the same time very strong baseline would be the method that shows to users the most popular destinations based on their preferences [6]. For ex-ample, if the user searches for the activity  X  X each X , we cal-culate the popularity rank score for a destination d i as the conditional probability: P (Beach | d i ). If the user searches for a second endorsement, e.g.  X  X ood X , the ranking score for d calculated using a Naive Bayes assumption as: P (Beach | d P (food | d i ). In general, if the users provides n endorsements, e , . . . , e n , the ranking score for d i is P ( e 1 | d i
We refer further to this method as  X  X opularity X .
As a primary ranking technique we use a Naive Bayes ap-proach. We will describe its application to the multi-criteria ranking data (presented in Equation 1) with an example. Let us again consider a user searching for  X  X each X . We need to return a ranked list of destinations. For instance, the ranking score for the destination  X  X iami X  is calculated as where P (Beach | Miami) is the probability that the destina-tion Miami gets the endorsement  X  X each X . P (Miami) de-scribes our prior knowledge about Miami. In the simplest case this prior is the ratio of the number of endorsements for Miami to the total number of endorsements in our database.
If a user uses searches for a second activity, e.g.  X  X ood X , the ranking score is calculated in the following way: If our user provides n endorsements, Equation 4 becomes a standard Naive Bayes formula.
 We refer further to this method as  X  X aive Bayes X .

To summarize, we described three strategies to rank travel destination recommendations: the random ranking, the pop-ularity based ranking, and the Naive Bayes approach. These three approaches will be compared to each other and against the industrial baseline. Next, we will present our exper-imental pipeline which involves online A/B testing at the destination finder service of Booking.com .
In this section we will describe our experimental setup and evaluation approach, and the results of the experiments. We perform experiments on users of Booking.com where an instance of the destination finder is running in order to con-duct an online evaluation. First, we will detail our online evaluation approach and used evaluation measures. Second, we will detail the experimental results.
We take advantage of a production A/B testing envi-ronment at Booking.com , which performs randomized con-trolled trials for the purpose of inferring causality. A/B test-ing randomly splits users to see either the baseline or the new variant version of the website, which allows to measure the impact of the new version directly on real users [10, 11, 15].
As our primary evaluation metric in the A/B test, we use conversion rate, which is the fraction of sessions which end with at least one clicked result [13]. As explained in the motivation, we are dealing with an exploratory task and therefore aim to increase customer engagement. An increase in conversion rate is a signal that users click on the suggested destinations and thus interact with the system.

In order to determine whether a change in conversion rate is a random statistical fluctuation or a statistically signifi-cant change, we use the G-test statistic (G-tests of goodness-of-fit). We consider the difference between the baseline and the newly proposed method significant when the G-test p-value is larger than90%.
Conversion rate is the probability for a user to click at least once, which is a common metric for user engagement. We used it as a primary evaluation metric in our experi-mentation. Table 1 shows the results of our A/B test. The production  X  X aseline X  substantially outperforms the  X  X an-dom X  ranking with respect to conversion rate, and performs slightly (but not significantly) better than the  X  X opularity X  approach. The  X  X aive Bayes X  ranker significantly increases the conversion rate by 4.4% compared to the production baseline.

We achieved this substantial increase in conversion rate with a straightforward Naive Bayes ranker. Moreover, most computations can be done offline. Thus, our model could be Table 1: Results of the destination finder online A/B testing based on the number of unique users and clickers.
 Ranker type Number of users Conversion rate G-test Baseline 9.928 25.61%  X  0.72% Random 10.079 24.46%  X  0.71% 94% Popularity 9.838 25.50%  X  0.73% 41%
Naive Bayes 9.895 26.73%  X  0.73% 93% trained on large data within reasonable time, and did not negatively impact wallclock and CPU time for the destina-tion finder web pages in the online A/B test. This is crucial for a webscale production environment [11].

To summarize, we used three approaches to rank travel recommendations. We saw that the random and popular-ity based ranking of destinations lead to a decrease in user engagement, while the Naive Bayes approach leads to a sig-nificant engagement increase.
This paper reports on large-scale experiments with four different approaches to rank travel destination recommen-dations at Booking.com , a major online travel agent. We fo-cused on a service called destination finder where users can search for suitable destination based on preferred activities. In order to build ranking models we used multi-criteria rat-ing data in the form of endorsements provided by past users after visiting a booked place.

We implemented three methods to rank travel destina-tions: Random, Most Popular, and Naive Bayes, and com-pared them to the current production baseline in Booking. com . We observed a significant increase in user engage-ment for the Naive Bayes ranking approach, as measured by the conversion rate. The simplicity of our recommenda-tion models enables us to achieve this engagement without significantly increasing online CPU and memory usage. The experiments clearly demonstrate the value of multi-criteria ranking data in a real world application. They also shows that simple algorithmic approaches trained on large data sets can have very good real-life performance [8].

We are working on a number of extension of the cur-rent work, in particular on contextual recommendation ap-proaches that take into account the context of the user and the endorser, and on ways to detect user profiles from im-plicit contextual information. Initial experiments with con-textualized recommendations show that this can lead to sig-nificant further improvements of user engagement.

Some of the authors are involved in the organization of the TREC Contextual Suggestion Track [6, 7, 16], and the use case of the destination finder is part of TREC in 2015, where similar endorsements are collected. The resulting test collection can be used to evaluate destination and venue recommendation approaches.
 This work was done while the main author was an intern at Booking.com . We thank Lukas Vermeer and Athanasios Noulas for fruitful discussions at the early stage of this work. This research has been partly supported by STW and is part of the CAPA project. 2 [1] G. Adomavicius and Y. Kwon. New recommendation [2] G. Adomavicius, N. Manouselis, and Y. Kwon. Multi-[3] D. Agarwal and B.-C. Chen. Regression-based latent [4] D. Agarwal and B.-C. Chen. flda: matrix factoriza-[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] A. Dean-Hall, C. L. Clarke, J. Kamps, P. Thomas, [7] A. Dean-Hall, C. L. Clarke, J. Kamps, P. Thomas, and [8] A. Halevy, P. Norvig, and F. Pereira. The unreasonable [9] D. J. Hu, R. Hall, and J. Attenberg. Style in the long [10] R. Kohavi, A. Deng, B. Frasca, T. Walker, Y. Xu, and [11] R. Kohavi, A. Deng, R. Longbotham, and Y. Xu. Seven [12] K. Lakiotaki, N. F. Matsatsinis, and A. Tsoukias. Mul-[13] M. Lalmas, H. O X  X rien, and E. Yom-Tov. Measuring [14] A. Noulas and M. S. Einarsen. User engagement [15] D. Tang, A. Agarwal, D. O X  X rien, and M. Meyer. Over-[16] TREC. Contextual suggestion track. Text REtrieval http://www.win.tue.nl/~mpechen/projects/capa/
