 critical issue for Web search. Web pages have a huge variation in quality relative to, for example, newswire articles. To address this problem, we propose a document quality language model approach that is incorporated into the basic query likelihood demonstrate that, on average, the new model is significantly better than the baseline (query likelihood model) in terms of precision at the top ranks. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation Document quality, prior probabilities, collection-document distance, web retrieval To achieve the goal of improving the performance of Web ad hoc retrieval by exploiting document quality information, we propose a document quality model that is incorporated into the basic query likelihood retrieval model in the form of a prior content features to estimate Web document quality. One of the two features is a novel document quality metric that was found to be helpful for identifying low quality documents. The first step of our approach depends on the identification of metrics or document features that are predictive of quality. In this paper we focus on two metrics, collection-document distance and information-to-noise ratio, the first of which is new and the second having been used with some su ccess in a previous study [1] where information-to-noise ratio is simply defined as the total number of terms in the documents after indexing divided by the raw size of the document. We now show how to compute the first metric, collection-document distance. Document Distance (CDD for short) is given by CDD P w C where P w D P w D P w C PwD PwC documents like tables or lists are unlikely to be relevant for ad hoc queries because a relevant document for the TREC ad hoc task usually explains or describes some topic using sentences with hypothesize that such low quality documents will have unusual significantly from the word usage in an average document, the quality of this document may be low. In the CDD measure, the model. The KL divergence between the collection language model and the document language model (i.e. the CDD) indicates how different these distributions are. The higher the CDD is, the more unusual the word distribution of the document is, and the more quality and low quality documents respectively. These two density estimation[3]. Figure 1: Distribution of CDD values for low and high quality documents by a na X ve Bayes classifier combining the two quality metrics mentioned above. Let D denote a document, Let H denote the of the high quality class and the low quality class respectively. By Bayes rule, we have: Pr( | )  X  X  = By assuming independence among the features, we have () ()(), , (3) fX fxfx j HL == where x 0 is the CCD metric and x 1 is the information-noise ratio.  X   X  and  X  relative frequencies in the training data(please refer to [3] for the details of our training data). To estimate f H and f L from the training data, we adopt the Kernel density estimation and choose the Gaussian Kernel. Without loss of generality, assume density function f(x) and we wish to estimate f(x) at a point x Guassian Kernel density estimator for f(x) at the point x defined as [5] () exp( )(4) fx
Finally, the probability given in Equation 2 is embedded as a prior probability in the query likelihood model. Specifically, given a query Q and a document D, let P(D|Q) be the probability is as follows: Where P( Q | D ) is the query likelihood model described in [2] and P( D=H | X ) computed by Equation 2 can be interpreted as the document prior probability that reflects prior knowledge about the relevance of the document D[4]. 701-750.The retrieval parameter settings are given in [3]. We did Table 1: Precision on the GOV2 collection.  X  X os X  means result is better than the baseline,  X  X eg X  means result is worse than the baseline,  X  X q X  means result is the same as the baseline. Bold cases means the results are statistically significant 5 docs 0.5184 0.5633 11 6 32 10 docs 0.4980 0.5306 12 7 30 15 docs 0.4653 0.5088 18 6 25 20 docs 0.4612 0.5020 19 7 23 Table 3: Precision on the WT10G collection mean the results are statistically significant.To better compare our model with the baseline query-likelihood model, all queries are divided into three types:  X  X os X ,  X  X eg X  and  X  X q X , which means our queries. The results for WT2G and WT10G are shown in table 2 field TREC topic 401-450 and 501-550 as queries. Information Retrieval and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903 and in part by NSF grant #IIS-recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. [1] X. Zhu and S. Gauch, Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web. In Proceedings of SIGIR 2000 , 288-295 , 2000. [2]F.Song and W.B. Croft. A general language model for information retrieval. In Proceedings of SIGIR 1999 , 279-280 , 1999 [3]Y.Zhou and W.B.Croft Document quality models for web adhoc retrieval Technical report IR-432, Center for Intelligent Information Retrieval, University of Massachusetts,2004 [4] W. Kraaij and T. Westerveld and D. Hiemstra, The importance of prior probabilities for entry page search, Proceedings of SIGIR 2002, 27-34, 2002. [5] T. Hastie, R. Tibshirani, J. H. Friedman .The Elements of Statistical Learning, Section 6, Kernel Method. Springer press, 2001 
