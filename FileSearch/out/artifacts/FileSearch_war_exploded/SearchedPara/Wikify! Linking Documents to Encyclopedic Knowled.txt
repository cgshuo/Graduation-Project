 This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disam-biguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be com-bined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the correspond-ing Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distin-guishable from manual annotations.
 I.2.7 [ Natural Language Processing ]: Text analysis; I.7 [ Document and Text Processing ]: Document and Text Editing Algorithms,Experimentation keyword extraction, word sense disambiguation, Wikipedia, semantic annotation
Wikipedia (http://en.wikipedia.org) is an online encyclo-pedia that has grown to become one of the largest online repositories of encyclopedic knowledge, with millions of ar-ticles available for a large number of languages. In fact, Wikipedia editions are available for more than 200 languages, with a number of entries varying from a few pages to more than one million articles per language.

One of the important attributes of Wikipedia is the abun-dance of links embedded in the body of each article con-necting the most important terms to other pages, thereby providing the users a quick way of accessing additional infor-mation. Wikipedia contributors perform these annotations by hand following a Wikipedia X  X anual of style, X  X hich gives guidelines concerning the selection of important concepts in a text, as well as the assignment of links to appropriate re-lated articles. For instance, Figure 1 shows an example of a Wikipedia page, including the definition for one of the meanings of the word  X  X lant. X  Figure 1: A sample Wikipedia page, with links to related articles.

This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disam-biguation. The paper also shows how these two methods can be combined into a system, which we refer to as Wikify!, which is able to automatically perform the annotation task following the Wikipedia guidelines. Specifically, given an input document, the Wikify! system has the ability to iden-tify the important concepts in a text (keyword extraction), and then link these concepts to the corresponding Wikipedia pages (word sense disambiguation).

There are many applications that could benefit from such a system. First, the vision of the Semantic Web is to have semantic annotations readily available inside the webpages, which will allow for a new semantically-oriented way of ac-cessing information on the Web [2]. The annotations pro-duced by the Wikify! system can be used to automatically enrich online documents with references to semantically re-lated information, which is likely to improve the Web users X  overall experience.

Second, in educational applications, it is important for students to have fast access to additional information rel-evant to the study material. The Wikify! system could serve as a convenient gateway to encyclopedic information related to assignments, lecture notes, and other teaching ma-terials, by linking important terms to the relevant pages in Wikipedia or elsewhere.

In addition, the system can also be used by the Wikipedia users, where the Wikify! system can provide support for the annotation process by suggesting keywords and links. Fi-nally, we believe that a number of text processing problems are likely to find new solutions in the rich text annotations produced by the Wikify! system. Wikipedia has already been successfully used in several natural language processing applications [1, 3, 6, 27], and we believe that the automatic Wikipedia-style annotation of documents will prove useful in a number of text processing tasks such as e.g., summa-rization, entailment, text categorization, and others.
The work closest to ours is perhaps the  X  X nstant Lookup X  feature of the Trillian instant messaging client, as well as the Microsoft Smart Tags and the Google AutoLink. How-ever, the coverage of these systems is small, and they are merely based on word or phrase lookup, without attempt-ing to perform keyword extraction or link disambiguation. A less comprehensive system is that of Drenner et al. [4] which attempts to discover movie titles in movie oriented discussion forums and link them to a movie database. More recently, the Creo and Miro systems described in [5] have expanded significantly the functionality and coverage of the Google and Microsoft interfaces, by adding personalized se-mantic hypertext that allows for a goal-oriented browsing experience. Related to some extent is also the ARIA sys-tem [14], where relevant photos are suggested based on the semantic analysis of an email content.
 In the following, we start by providing a brief overview of Wikipedia, and describe the structure and organization of this online encyclopedic resource. Next, we describe the ar-chitecture of the Wikify! system, and we show how Wikipedia can be used as a resource to support automatic keyword ex-traction and word sense disambiguation. We describe the keyword extraction and the word sense disambiguation al-gorithms, and we provide individual evaluation results as obtained on a gold-standard data set. Finally, we present the results of a survey conducted to evaluate the overall quality of the system, and conclude with a discussion of the results.
Wikipedia is a free online encyclopedia, representing the outcome of a continuous collaborative effort of a large num-ber of volunteer contributors. Virtually any Internet user can create or edit a Wikipedia webpage, and this  X  X reedom of contribution X  has a positive impact on both the quantity (fast-growing number of articles) and the quality (potential mistakes are quickly corrected within the collaborative en-vironment) of this online resource. In fact, Wikipedia was found to be similar in coverage and accuracy to Encyclope-dia Britannica [7]  X  one of the oldest encyclopedias, consid-ered a reference book for the English language, with articles typically contributed by experts.

The basic entry in Wikipedia is an article (or page ), which defines and describes an entity or an event, and consists of a hypertext document with hyperlinks to other pages within or outside Wikipedia. The role of the hyperlinks is to guide the reader to pages that provide additional information about the entities or events mentioned in an article.

Each article in Wikipedia is uniquely referenced by an identifier, which consists of one or more words separated by spaces or underscores, and occasionally a parentheti-cal explanation. For example, the article for bar with the meaning of  X  X ounter for drinks X  has the unique identifier bar (counter) . 1
The hyperlinks within Wikipedia are created using these unique identifiers, together with an anchor text that rep-resents the surface form of the hyperlink. For instance,  X  X enry Barnard, [[United States | American]] [[educational-ist]], was born in [[Hartford, Connecticut]] X  is an example of a sentence in Wikipedia containing links to the articles United States, educationalist, and Hartford, Connecticut. If the surface form and the unique identifier of an article co-incide, then the surface form can be turned directly into a hyperlink by placing double brackets around it (e.g. [[educa-tionalist]] ). Alternatively, if the surface form should be hy-perlinked to an article with a different unique identifier, e.g. link the word American to the article on United States , then a piped link is used instead, as in [[United States | American]] .
One of the implications of the large number of contribu-tors editing the Wikipedia articles is the occasional lack of consistency with respect to the unique identifier used for a certain entity. For instance, the concept of circuit (electric) is also referred to as electronic circuit , integrated circuit , electric circuit , and others. This has led to the so-called redirect pages , which consist of a redirection hyperlink from an alternative name (e.g. integrated circuit ) to the article actually containing the description of the entity (e.g. circuit (electric) ).

A structure that is particularly relevant to the work de-scribed in this paper is the disambiguation page . Disam-biguation pages are specifically created for ambiguous en-tities, and consist of links to articles defining the different meanings of the entity. The unique identifier for a disam-biguation page typically consists of the parenthetical expla-nation (disambiguation) attached to the name of the am-biguous entity, as in e.g. circuit (disambiguation) which is the unique identifier for the disambiguation page of the en-tity circuit .

In the experiments reported in this paper, we use a Wikipedia download from March 2006, with approximately 1.4 million articles, and more than 37 millions hyperlinks.
Given a text or hypertext document, we define  X  X ext wiki-fication X  X s the task of automatically extracting the most im-portant words and phrases in the document, and identifying for each such keyword the appropriate link to a Wikipedia article. This is the typical task performed by the Wikipedia users when contributing articles to the Wikipedia repository. e.g. http://en.wikipedia.org/wiki/Bar (counter)
Extract Sense Definitions from Sense Inventory The requirement is to add links for the most important con-cepts in a document, which will  X  X llow readers to easily and conveniently follow their curiosity or research to other ar-ticles. X  In general, the links represent  X  X ajor connections with the subject of another article that will help readers to
Automatic text wikification implies solutions for the two main tasks performed by a Wikipedia contributor when adding links to an article: (1) keyword extraction, and (2) link dis-ambiguation.

The first task consists of identifying those words and phrases that are considered important for the document at hand. These typically include technical terms, named entities, new terminology, as well as other concepts closely related to the content of the article  X  in general, all the words and phrases that will add to the reader X  X  experience. For instance, the Wikipedia page for  X  X ree X  includes the text  X  X  tree is a large, perennial, woody plant [...] The earliest trees were tree ferns and horsetails, which grew in forests in the Carboniferous Period. X  , where perennial , plant , tree ferns , horsetails , and Carboniferous are selected as keywords. This task is identi-fied with the problem of keyword extraction , which targets the automatic identification of important words and phrases in an input natural language text.

The second task consists of finding the correct Wikipedia article that should be linked to a candidate keyword. Here, we face the problem of link ambiguity, meaning that a phrase can be usually linked to more than one Wikipedia page, and the correct interpretation of the phrase (and correspondingly the correct link) depends on the context where it occurs. For instance, the word  X  X lant X  can be linked to different ar-ticles, depending on whether it was used with its green plant or industrial plant meaning. This task is analogous to the problem of word sense disambiguation , aiming at finding the correct sense of a word according to a given sense inventory. 2 http://en.wikipedia.org/wiki/Wikipedia:Manual of Style
We designed and implemented a system that solves the  X  X ext wikification X  problem in four steps, as illustrated in Figure 2. First, if the input document is a hypertext, we pre-process the hypertext by separating the HTML tags and the body text. In the second step, the clean text is passed to the keyword extraction module, which identifies and marks the important words and phrases in the text. The text anno-tated for keywords is then passed to the word sense disam-biguation module, which resolves the link ambiguities and completes the annotations with the correct Wikipedia arti-cle reference. Finally, when all the annotations are ready, the structure of the original hypertext document is recon-structed, and the newly added reference links are included in the text.

In the following two sections, we show how Wikipedia can be used to support the process of selecting the keywords (keyword extraction) and disambiguating the links (word sense disambiguation), and we provide an evaluation of the individual performance for each of these two tasks on a gold-standard collection of Wikipedia webpages.
The Wikipedia manual of style provides a set of guidelines for volunteer contributors on how to select the words and Although prepared for human annotators, these guidelines represent a good starting point for the requirements of an automated system, and consequently we use them to de-sign the link identification module for the Wikify! system. The main recommendations from the Wikipedia style man-ual are highlighted below: (1) Authors/annotators should provide links to articles that provide a deeper understand-ing of the topic or particular terms, such as technical terms, names, places etc. (2) Terms unrelated to the main topic Only make links that are relevant to the context and terms that have no article explaining them should not be linked. (3) Special care has to be taken in selecting the proper amount of keywords in an article  X  as too many links obstruct the readers X  ability to follow the article by drawing attention away from important links.

Since the criteria for selecting linked words in Wikipedia appear to be very similar to those used for selecting key-words in a document, we decided to address the problem of link identification by implementing techniques typically used for the task of keyword extraction.

Looking at previous work in keyword extraction (see [11] for a survey), there are both supervised and unsupervised methods that have been used in the past with a similar de-gree of success. Supervised methods generally employ ma-chine learning such as Naive Bayes [9], decision trees [28], or rule induction [10], using features such as syntactic features, syntactic patterns, and others. On the other side, unsuper-vised methods, such as e.g. the random walk based sys-tem proposed in [19], were found to achieve accuracy figures comparable to those obtained by state-of-the-art supervised methods. For our system, given that efficiency is also an important factor, we decided to implement and evaluate a set of unsupervised keyword extraction techniques.
An important observation based on the second recommen-dation from the Wikipedia guidelines is the fact that candi-date keywords should be limited to those that have a valid corresponding Wikipedia article. According to this restric-tion, we could construct a keyword vocabulary that con-tains only the Wikipedia article titles (1 . 406 . 039 such titles are included in the March 2006 version of Wikipedia), and use this controlled vocabulary to extract keyphrases. How-ever, this would greatly restrict our potential to find all the keyphrases, since the actual use of a phrase (surface form) may differ from the article title. For instance, different mor-phological forms such as e.g.  X  X issecting X  or  X  X issections X  can be linked to the same article title  X  X issection. X  If we ignore these morphological variations, we are likely to miss a good fraction of the keywords that appear in a form dif-ferent than the Wikipedia titles. To address this problem, we extended the controlled vocabulary with all the surface forms collected from all the Wikipedia articles, and subse-quently discounted all the occurrences that were used less than five times. After this process, the resulting controlled vocabulary consisted of 1 . 918 . 830 terms.
Given that we work under a controlled vocabulary set-ting, we can avoid some of the problems typically encoun-tered in keyword extraction algorithms. Most notably, all the keywords in our vocabulary are acceptable phrases, and therefore nonsense phrases such as e.g.  X  X roducts are X  will not appear as candidates. This reduces our problem to the task of finding a ranking over the candidates, reflecting their importance and relevance in the text.

We implement an unsupervised keyword extraction algo-rithm that works in two steps, namely: (1) candidate ex-traction, and (2) keyword ranking.

The candidate extraction step parses the input document and extracts all possible n-grams that are also present in the controlled vocabulary.

The ranking step assigns a numeric value to each candi-date, reflecting the likelihood that a given candidate is a valuable keyphrase. In our experiments we used three dif-ferent ranking methods, as follows:
In order to address the problem of selecting the right amount of keywords in a document, we carried out a sim-ple statistical examination of the entire Wikipedia collec-tion and found that the average percentile ratio between the number of words in an article and the number of manu-ally annotated keywords is around 6%. Consequently, in all the experiments we use this ratio to determine the number of keywords to be extracted from a document.

We experiment with two corpora to obtain the phrase counts required for the tf.idf and  X  2 measures: (a) the British Table 1: Precision (P), Recall (R) and F-measure (F) evaluations for the various keyphrase extraction methods National Corpus (BNC) and (b) the entire corpus of Wikipedia articles (excluding the articles used for testing). The per-formance of the system with BNC counts was significantly and consistently smaller than the one using Wikipedia, and therefore we only report on the latter.

For the evaluation, we created a gold standard data set consisting of a collection of manually annotated Wikipedia articles. We started by randomly selecting 100 webpages from Wikipedia. We then removed all the disambiguation pages, as well as those pages that were overlinked or under-linked (the annotators did not obey the recommendations of the manual of style and selected too many or too few keyphrases), which left us with a final test set of 85 docu-ments containing a total of 7,286 linked concepts.
We evaluate the keyword extraction methods by compar-ing the keywords automatically selected with those manually annotated in the gold standard dataset. Table 1 shows the evaluation results for each of the three keyword extraction methods. We measure the performance in terms of preci-sion, recall and F-measure, where precision is calculated as the number of correctly identified keywords divided by the total number of keywords proposed by the system; recall is defined as the number of correct keywords divided by the total number of keywords in the original document; and F-Measure is the harmonic mean of the precision and recall.
As shown in Table 1, the results for the traditional mea-sures of tf.idf and  X  2 are very close to each other, while the keyphraseness measure produces significantly higher scores. It is worth noting that precision and recall scores in key-word extraction are traditionally low. Turney [28] evaluated a supervised system on a corpus of journal articles and re-ported a precision of 29% and 15% when extracting 5 and 15 keyphrases respectively. On a different corpus contain-ing only article abstracts, the unsupervised system of [19] reported an F-measure of 36.2, surpassing by 2.3% the su-pervised system of Hulth [10].
Ambiguity is inherent to human language. In particular, word sense ambiguity is prevalent in all natural languages, with a large number of the words in any given language car-rying more than one meaning. For instance, the English noun plant can either mean green plant or factory ; similarly the French word feuille can either mean leaf or paper . The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the prob-lem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Word sense am-biguity is also present within Wikipedia, with a large num-ber of the concepts mentioned in the Wikipedia pages hav-ing more than one possible explanation (or  X  X ense X ). In the Wikipedia annotations, this ambiguity is solved through the use of links or piped links, which connect a concept to the corresponding correct Wikipedia article.

For instance, ambiguous words such as e.g. plant , bar , or chair are linked to different Wikipedia articles depending on the meaning they have in the context where they occur. Note that the links are manually created by the Wikipedia contributors, which means that they are most of the time accurate and referencing the correct article. The following represent five example sentences for the ambiguous word bar , with their corresponding Wikipedia annotations (links): In 1834, Sumner was admitted to the [[bar (law) | bar]] at the age of twenty-three, and entered private practice in Boston.
 It is danced in 3/4 time (like most waltzes), with the couple turning approx. 180 degrees every [[bar (music) | bar]] . Vehicles of this type may contain expensive audio players, televisions, video players, and [[bar (counter) | bar]] s, often with refrigerators.
 Jenga is a popular beer in the [[bar (establishment) | bar]] s of Thailand.
 This is a disturbance on the water surface of a river or estu-ary, often cause by the presence of a [[bar (landform) | bar]] or dune on the riverbed.

Interestingly, these links can be regarded as sense an-notations for the corresponding concepts, which is a prop-erty particularly valuable for the entities that are ambigu-ous. As illustrated in the example above, the ambiguity is related to the surface form of the concepts defined in Wikipedia, e.g. the word bar that can be linked to five different Wikipedia pages depending on its meaning. Note that although Wikipedia defines the so-called disambigua-tion pages, meant as a record of a word meanings, the dis-ambiguation pages do not always account for all the pos-sible surface form interpretations. For instance, there are several Wikipedia pages where the ambiguous word bar is sometimes linked to the pages corresponding to nightclub or public house , but these meanings are not listed on the disambiguation page for bar .

Regarded as a sense inventory, Wikipedia has a much larger coverage than a typical English dictionary, in partic-ular when it comes to entities (nouns). This is mainly due to the large number of named entities covered by Wikipedia (e.g. Tony Snow , Washington National Cathedral ), as well as an increasing number of multi-word expressions (e.g. mother church , effects pedal ). For instance, in the March 2006 ver-sion, we counted a total of 1.4 million entities defined in Wikipedia, referred by a total of 4.5 million unique sur-face forms (anchor texts), accounting for 5.8 million unique Wikipedia word  X  X enses X  (where a  X  X ense X  is defined as the unique combination of a surface form and a link to a Wikipedia entity definition). This is significantly larger than the num-ber of entities covered by e.g. WordNet [20], consisting of 80,000 entity definitions associated with 115,000 surface forms, accounting for 142,000 word meanings.
There are a number of different approaches that have been proposed to date for the problem of word sense disambigua-tion, see for instance the Senseval/Semeval evaluations (http://www.senseval.org). The two main research direc-tions consist of: (1) knowledge-based methods that rely ex-clusively on knowledge derived from dictionaries, e.g. [13, 16, 22], and (2) data-driven algorithms that are based on probabilities collected from large amounts of sense-annotated data, e.g. [8, 23, 24].

We implemented and evaluated two different disambigua-tion algorithms, inspired by these two main trends in word sense disambiguation research [18].

The first one is a knowledge-based approach, which relies exclusively on information drawn from the definitions pro-vided by the sense inventory. This method is inspired by the Lesk algorithm, first introduced in [13], and attempts to identify the most likely meaning for a word in a given con-text based on a measure of contextual overlap between the dictionary definitions of the ambiguous word  X  here approx-imated with the corresponding Wikipedia pages, and the context where the ambiguous word occurs (we use the cur-rent paragraph as a representation of the context). Function words and punctuation are removed prior to the matching.
For instance, given the context  X  X t is danced in 3/4 time, with the couple turning 180 degrees every bar X  , and assum-ing that  X  X ar X  could have the meanings of bar music or bar counter , we process the Wikipedia pages for the mu-sic and counter meanings, and consequently determine the sense that maximizes the overlap with the given context.
The second approach is a data-driven method that inte-grates both local and topical features into a machine learn-ing classifier [17]. For each ambiguous word, we extract a training feature vector for each of its occurrences inside a Wikipedia link, with the set of possible word senses being given by the set of possible links in Wikipedia. To model feature vectors, we use the current word and its part-of-speech, a local context of three words to the left and right of the ambiguous word, the parts-of-speech of the surround-ing words, and a global context implemented through sense-specific keywords determined as a list of at most five words occurring at least three times in the contexts defining a cer-tain word sense. This feature set is similar to the one used by [23], as well as by a number of Senseval systems. The parameters for sense-specific keyword selection were deter-mined through cross-fold validation on the training set. The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider [12].

Finally, given the orthogonality of the knowledge-based and the data-driven approaches, we also implemented a vot-ing scheme, meant to filter out the incorrect predictions by seeking agreement between the two methods. Since we no-ticed that the two methods disagree in their prediction in about 17% of the cases, we use this disagreement as an in-dication of potential errors, and consequently ignore the an-notations that lack agreement.
To evaluate the accuracy of the disambiguation algorithms, we use a gold-standard data set consisting of a collection of pages from Wikipedia, containing manual  X  X ense X  annota-tions made by the Wikipedia contributors. As mentioned before, the  X  X ense X  annotations correspond to the links in a Wikipedia page, which uniquely identify the meaning of the corresponding words. We use the same set of pages used dur-ing the keyword extraction evaluation, namely 85 Wikipedia pages containing 7,286 linked concepts.

Since the focus of this particular evaluation is on the qual-ity of the disambiguation system, we decided to detach the keyword extraction and the word sense disambiguation eval-uations, and assume that the keyword extraction stage pro-duces 100% precision and recall. This assumption helps us avoid the error propagation effect, and consequently isolate the errors that are specific to the word sense disambiguation module. An evaluation of the entire system is reported in the following section.

We therefore start with the set of keywords manually se-lected by the Wikipedia contributors within the dataset of 85 pages, and for each such keyword we use our word sense disambiguation method to automatically predict the correct  X  X ense, X  i.e. the correct link to a Wikipedia definition page.
For instance, given the context  X  X enga is a popular beer in the [[bar (establishment) | bar]]s of Thailand. X  , we will at-tempt to disambiguate the word  X  X ar, X  since it has been marked as a candidate Wikipedia concept. We therefore try to automatically predict the title of the Wikipedia page where this concept should be linked, and evaluate the quality of this prediction with respect to the gold standard annota-tion bar (establishment) .

Evaluations of word sense disambiguation systems typi-cally report on precision and recall [18], where precision is defined as the number of correctly annotated words divided by the total number of words covered by the system, and re-call is defined as the number of correct annotations divided the total number attempted by the system.

The gold standard data set includes all the words and phrases that were marked as Wikipedia links in the 85 test articles, which amount to a total of 7,286 candidate con-cepts. Out of these, about 10% were marked as  X  X nknown X   X  indicating that the corresponding surface form was not found in other annotations in Wikipedia, and therefore the system did not have any knowledge about the possible mean-ings of the given surface form. For instance, the surface form  X  X onference Championship X  is a candidate concept in one of our test pages; however, this surface form was not encoun-tered anywhere else in Wikipedia, and therefore since we do not have any sense definitions for this phrase, we mark it as  X  X nknown. X  These cases could not be covered by the system, and they account for the difference between the total num-ber of 7,286 concepts in the data set, and the  X  X ttempted X  counts listed in Table 2.

Precision, recall and F-measure figures for the three dis-ambiguation algorithms are shown in Table 2. The table also shows the performance of an unsupervised baseline al-gorithm that for each candidate concept randomly selects one of its possible senses, and the performance of the most frequent sense baseline using counts derived from Wikipedia.
Perhaps not surprising, the data-driven method outper-forms the knowledge-based method both in terms of preci-sion and recall. This is in agreement with previously pub-lished word sense disambiguation results on other sense an-notated data sets [18]. Nonetheless, the knowledge-based method proves useful due to its orthogonality with respect to the data-driven algorithm. The voting scheme combin-ing the two disambiguation methods has the lowest recall, but the highest precision. This is not surprising since this third system tagged only those instances where both systems agreed in their assigned label. We believe that this high pre-cision figure is particularly useful for the Wikify! system, as it is important to have highly precise annotations even if the trade-off is lower coverage.

Note that these evaluations are rather strict, as we give credit only to those predictions that perfectly match the gold standard labels. We thus discount a fairly large num-ber of cases where the prediction and the label have similar meaning. For instance, although the system predicted Gross domestic product, as a label for the concept  X  X DP, X  , it was discounted for not matching the gold-standard label GDP , despite the two labels being identical in meaning. There were also cases where the prediction made by the system was better than the manual label, as in e.g. the label for the concept football in the (British) context playing football , wrongly linked to Association football by the Wikipedia an-notator, and correctly labeled by the automatic system as football (soccer) .

The final disambiguation results are competitive with fig-ures recently reported in the word sense disambiguation lit-erature. For instance, the best system participating in the recent Senseval/Semeval fine-grained English all-words word sense disambiguation evaluation reported a precision and recall of 59.10%, when evaluated against WordNet senses [25]. In the coarse-grained word sense disambiguation eval-uation, which relied on a mapping from WordNet to the Ox-ford Dictionary, the best word sense disambiguation system achieved a precision and recall of 83.21% [21].
The Wikify! system brings together the capabilities of the keyword extraction and the word sense disambiguation systems under a common system that has the ability to au-tomatically X  X ikify X  X ny input document. Given a document provided by the user or the URL of a webpage, the system processes the document provided by the user, automatically identifies the important keywords in the document, disam-biguates the words and links them to the correct Wikipedia page, and finally returns and displays the  X  X ikified X  docu-ment. The interface (shown in Figure 3) allows the user to either (1) upload a local text or html file, or (2) indicate the URL of a webpage. The user also has the option to indi-cate the desired density of keywords on the page, ranging from 2% X 10% of the words in the document (default value: 6%), as well as the color to be used for the automatically generated links (default color: red). The Wikify! system is then launched, which will process the document provided by the user, automatically identify the important keywords in the document, disambiguate the words and link them to the correct Wikipedia page, and finally return and display the  X  X ikified X  document. Note that when an URL is provided, the structure of the original webpage is preserved (including images, menu bars, forms, etc.), consequently minimizing the effect of the Wikify! system on the overall look-and-feel of the webpage being processed.

In addition to the evaluations reported in the previous sections concerning the individual performance of the key-word extraction and word sense disambiguation methods, we also wanted to evaluate the overall quality of the Wikify! system. We designed a Turing-like test concerned with the quality of the annotations of the Wikify! system as com-pared to the manual annotations produced by Wikipedia contributors. In this test, human subjects were asked to dis-tinguish between manual and automatic annotations. Given a Wikipedia page, we provided the users with two versions: (a) a version containing the original concept annotations as originally found in Wikipedia, which were created by the Wikipedia contributors; and (b) a version where the an-notations were automatically produced using the Wikify! system. Very briefly, the second version was produced by first stripping all the annotations from a Wikipedia web-page, and then running the document through the Wikify! system, which automatically identified the important con-cepts in the page and the corresponding links to Wikipedia pages.

The dataset for the survey consisted of ten randomly se-lected pages from Wikipedia, which were given to 20 users with mixed professional background (graduate and under-graduate students, engineers, economists, designers). For each page, the users were asked to check out the two differ-ent versions that were provided, and indicate which version they believed was created by a human annotator. Note that the order of the two versions (human, computer) was ran-domly swapped across the ten documents, in order to avoid any bias.

Over the entire testbed of 200 data points (20 users, each evaluating 10 documents), the X  X uman X  X ersion was correctly identified only in 114 cases, leading to an overall low accu-racy figure of 57% (standard deviation of 0 . 15 across the 20 subjects).

An  X  X deal X  Turing test is represented by the case when the computer and human versions are indistinguishable, thus leading to a random choice of 50% accuracy. The small dif-ference between the accuracy of 57% achieved by the sub-jects taking the test and the ideal Turing test value of 50% suggests that the computer-generated and human-generated Wikipedia annotations are hardly distinguishable, which is an indication of the high quality of the annotations produced by the Wikify! system.
In this paper, we introduced the use of Wikipedia as a resource to support accurate algorithms for keyword extrac-tion and word sense disambiguation. We also described a system that relies on these methods to automatically link documents to encyclopedic knowledge. The Wikify! system integrates the keyword extraction algorithm that automat-ically identifies the important keywords in the input doc-ument, and the word sense disambiguation algorithm that assigns each keyword with the correct link to a Wikipedia article.

Through independent evaluations carried out for each of the two tasks, we showed that both the keyword extrac-tion and the word sense disambiguation systems produce accurate annotations, with performance figures significantly higher than competitive baselines. We also performed an overall evaluation of the Wikify! system using a Turing-like test, which showed that the output of the Wikify! system was hardly distinguishable from the manual annotations pro-duced by Wikipedia contributors.
 We believe this paper made two important contributions. First, it demonstrated the usefulness of Wikipedia as a re-source for two important tasks in document processing: key-word extraction and word sense disambiguation. While the experiments reported in this paper were carried out on En-glish, the methods can be equally well applied to other lan-guages, as Wikipedia editions are available in more than 200 languages. Second, the Wikify! system can be seen as a practical application of state-of-the-art text processing tech-niques. The Wikify! system can be a useful tool not only as a browsing aid for daily use, but also as a potential source of richer annotations for knowledge processing and information retrieval applications. [1] S. F. Adafre and M. de Rijke. Finding similar [2] T. Berners-Lee, J. Hendler, and O. Lassila. The [3] R. Bunescu and M. Pasca. Using encyclopedic [4] S. Drenner, M. Harper, D. Frankowski, J. Riedl, and [5] A. Faaborg and H. Lieberman. A goal-oriented Web [6] E. Gabrilovich and S. Markovitch. Overcoming the [7] J. Giles. Internet encyclopaedias go head to head. [8] A. Gliozzo, C. Giuliano, and C. Strapparava. Domain [9] C. Gutwin, G. Paynter, I. Witten, C. Nevill-Manning, [10] A. Hulth. Improved automatic keyword extraction [11] C. Jacquemin and D. Bourigault. Term Extraction and [12] Y. Lee and H. Ng. An empirical evaluation of [13] M. Lesk. Automatic sense disambiguation using [14] H. Lieberman and H. Liu. Adaptive linking between [15] C. D. Manning and H. Sch  X  utze. Foundations of [16] R. Mihalcea. Large vocabulary unsupervised word [17] R. Mihalcea. Using Wikipedia for automatic word [18] R. Mihalcea and P. Edmonds, editors. Proceedings of [19] R. Mihalcea and P. Tarau. TextRank  X  bringing order [20] G. Miller. Wordnet: A lexical database.
 [21] R. Navigli and M. Lapata. Graph connectivity [22] R. Navigli and P. Velardi. Structural semantic [23] H. Ng and H. Lee. Integrating multiple knowledge [24] T. Pedersen. A decision tree of bigrams is an accurate [25] S. Pradhan, E. Loper, D. Dligach, and M. Palmer. [26] G. Salton and C. Buckley. Term-weighting approaches [27] M. Strube and S. P. Ponzetto. Wikirelate! computing [28] P. Turney. Learning algorithms for keyphrase
