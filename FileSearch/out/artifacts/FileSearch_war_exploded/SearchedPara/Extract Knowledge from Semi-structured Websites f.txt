 Simplifying the key tasks of search engine users by directly retrieving to them structured knowledge according to their queries is attracting much attention from both industry and academia. A bottleneck of this challenging pr oblem is how to extract the structured knowledge from the noisy and complex Web scale websites automatically. In this pa per, we propose an unsupervised automatic wrapper inducti on algorithm, named as S calable K nowledge E xtractor from webS ites ( SKES ). SKES induces the wrapper in a divide and conquer m ode, i.e., it divides the general wrapper into several sub-wra ppers to learn from the data independently. Moreover, through employing techniques such as tag path representation of Web pa ges, SKES is verified to be efficient and noise-tolerant by the experimental results. Furthermore, based on our automa tically extracted knowledge, we also built a prototype to serve structured knowledge to end users for simplifying their key search tasks. Very positive feedbacks were received on the prototype. H.3.m [ Information Storage and Retrieval ]: Miscellaneous  X  Data Extraction, Template Induction, Search ; H.3.5 [ Information Storage and Retrieval ]: Online Information Services  X  Commercial services, Web-based services Algorithms, Performance, Experimentation. Search task simplification, stru ctured knowledge extraction, semi-structured Web sites, auto wrapper induction. User search task simplification is recognized as one important problem for search engine companies to improve users X  search experiences. However, among vari ous state-of-the-art search engines, many users with knowledge seeking tasks are still enduring from complex query-click-browsing procedures. Directly returning the structured knowledge to user queries instead of requiring them click a long list of Web pages is regarded as an effective approach for user search task simplification. As shown in Figure 1, in Bing 1 search engine, it directly triggers the side by side comparison table between athletes to users whose queries have the intent to compare them. information, which could complete the search tasks of end users, as knowledge for user search task simplification. Figure 1. An example of search task simplification in Bing. To directly return the structured knowledge for simplifying user tasks, there are two major challe nges. The first challenge is how to construct the large scale structured knowledge base, by which the search tasks of end users can be satisfied. The second challenge is how to understand the intent of user search queries for triggering the structured knowledge. In this paper, we adopt a simple solution for user query intent understanding and mainly discuss how to construct the large scale knowledge base automatically for user search task simplification. It is a challenging task to extract the structured knowledge from the semi-structured and unstructured Web automatically. Notice the fact that the structured data on the Web generally cannot be http://www.bing.com/ accessed directly since it is always encoded and displayed as semi-structured or unstructu red HTML pages. The major challenge is thus known as how to develop an effective and efficient auto wrapper induction algorithm for automatic knowledge extraction. Different research efforts have been tried for this wrapper induction problem. Among these efforts, structured data extraction from a set of detail pages, each of which displays detailed information of an entity, is recognized as a very effective way [6, 1]. However, existing algorithms still have limitations in real application such as their inability to handle noisy situations and low efficiency in Web-scale data. Motivated by this observation, in this paper we develop a scalable and noise-tolerant knowledge ex traction algorithm, which is named as SKES, from detail pages. We propose a top-down template induction method which divides the general template inducti on problem into several sub-templates X  induction problems and conquers them separately. By further employing the tag path [9 ] representation of input pages, our method is much more efficient than existing methods. The noisy situation is handled by introducing a support threshold so that the underlying templates in noi se pages will not be generated due to insufficient support. Experime ntal results show that we can achieve the knowledge extraction precision as high as 97% with high efficiency and a certain portion of noise pages can be tolerated. We also implement a prototype system for search task simplification and very positive feedbacks are collected by user study. The rest of this paper is organi zed as follows. In Section 2, we introduce the related works of Web Information Extraction. In Section 3, we introduce our algorithm SKES in detail. In Section 4, we show the experimental results to verify the effectiveness and efficiency of our proposed solution in terms of structured knowledge extraction. In addition, we use the real user study to verify the user task simplification feature based on the extracted structured knowledge. Finally, S ection 5 gives the conclusions and our future work. The key challenge of our work is much related to the field of Web Information Extraction (WIE). WI E aims to extract information with a specific structure (e.g. ontology) from the World Wide Web. Different research efforts ha ve tried to extract information from different perspectives. Considering the data type, they could be generally classified into two categories, which are unstructured data and semi-structured data re spectively. Different techniques have been applied, such as the NLP and pattern learning in unstructured data [3, 7, 2], wrapper induction and table extraction in semi-structured data [5, 10, 4] . Our work belongs to the latter category, i.e. structured data extraction from semi-structured HTML pages. Structured data extraction from detail pages are deemed as an effective way as detail pages contain much information. Detail pages in a Website are usually machine-generated from backend databases by using an underlying te mplate. Therefore, the task of structured data extraction from detail pages is to induce the underlying template (wrapper) automa tically and use this template to extract structured data. RoadRunner [6] and EXALG [1] are two methods that are fully automatic and are much related to our work. RoadRunner develops a novel matching technique to compare HTML pages of the same format and generate a template based on their similarities and di fferences. It relies on a greedy approach that starts from one page as an initial template and compare other pages one by one to induce more general template. EXALG is an improved data extr action method which handles all pages X  tokens at the same time. It detects the underlying template by two techniques: differentiating roles of tokens and equivalence classes . A template will be generated from equivalence classes by further filtering those with insufficient support and size. As it detects template in the unit of tokens and filter template with insufficient, templates that consis t of few tokens are filtered and unable to be detected by EXALG. Based on several sites X  experimental results, RoadRunner and EXALG work well in pure detail pa ges. However, the above two methods fail to handle noisy s ituations. RoadRunner generates template by comparing pages one by one and will generate incorrect template from noisy de tail pages. EXALG also neglects the noisy situation as its support threshold is to filter template parts which occur rarely instead of the noise pages. In addition, they are not efficient enough in Web scale data. The match algorithm in RoadRunner has exponential time complexity with respect to the length of input page. EXALG has a polynomial time complexity which is still not efficient enough due to its complex techniques. Compared to the two methods, our algorithm employs a divide and conquer manner which divides the general template induction problem into several sub-templates X  induction problems and conquers them separately, which makes our algorithm much more efficient. In addition, we use tag path representation of HTML pages, which can reduce the number of tags dramatically and differentiate roles of tags natu rally. A support threshold is also introduced to handle noisy situations so that the underlying templates in noise pages will not be generated due to insufficient support. The key of structured data extraction from detail pages is to induce a good template (schema) of these pages, since the induced template can be used to automatically extract corresponding data fields encoded in HTML page s. However, the problem of inducing a good (unambiguous) templa te is complicated due to the existence of missing data fields. It has been proved as a NP-complete problem [8]. Due to this limitation, we make some reasonable assumptions to make our method useful and scalable. SKES consists of three steps: (1) template induction, (2) structured data extraction, and (3) post-processing extracted data. To induce templates in a more scalable manner, we employ two techniques: (1) using tag paths to represent the original HTML pages, and (2) inducing templates in a top-down manner, which is inspired by the divide-and-conquer strategy. The first technique is a powerful presentation of HTML pages and reduces the number of tags in pages dramatically. The second technique is based on the observation that templates and da ta fields can be differentiated more easily if a more specific section can be identified. Then the general idea is to induce a template with some confident wrappers first, which splits the general temp late into several sections, and induce more specific templates in each section respectively. Each node in a HTML Dom tree (tag tree) can be identified by following a path from the root to the node. Such a path is a tag path [9]. For each HTML page, we identify those text nodes (i.e. nodes containing text information) and store their tag paths and corresponding texts. These tag paths and texts are used to represent the original page. An example is shown in Figure 2 and Figure 3, where Figure 2 is a samp led HTML code and Figure 3 is our representation. We represent HTML pages in this way because of two reasons. First is that representation in tag paths is very powerful. It naturally differentiates roles of tags in different tag paths, i.e. tags of same name in different tag pa ths can be differentiated due to the tag path representation. The second reason of our representation is that by only c onsidering text nodes X  tag paths and texts, the scale of the number of tags will reduce dramatically. Since inducing a good template is NP-complete, based on observation on real Websites pages we make two assumptions for scalable template inductions. The fi rst is that across pages of same format, the invariant parts are deemed as template, and the variant parts are deemed as encoded data fields. The second assumption is that different data fields (including templates) are displayed in the same order in one site. That is, if data field A displays in precede of data field B in one page, B should never display in precede of A in other pages. As we construct extracted data into a table format and it is hard to organize data with complex struct ures, templates of very complex structures are not induced in SKES. We only extract three kinds of data fields: (a) data fields in a single slot, (b) data fields displayed in a list pattern, and (c) data fields across multiple slots (called data section). Each data field will be organized in a column of the extracted table. Based on the assumptions and this simplification, we start to (1) identify several data sections to construct a general template (called root template) first. Then (2) in each data section, we furthermore identify more specific da ta fields and data sections to generate a more specific template. Step 2 will reapply on the new generated template until no more templates can be generated. Before describing our approach in detail, we first give some formal definitions: Definition 1 (tag path) : A tag path (tp) is a path from the root node to a text node in the DOM tree. Definition 2 (tag path text) : A tag path text (tpt) is a text node X  X  tag path and the text carried by this node. Definition 3 (occurrence vector) : An occurrence vector of a tag path (or a tag path text) is an integer vector V tp (V tpt f ], where n is the number of input detail pages, and f occurrence frequency of tp (tpt) in page i. Definition 4 (occurrence positions) : An occurrence positions of a tag path (or a tag path text) is a vector PS tp (PS tpt p ], where n is the number of input pages, and p i number where tp (tpt) occurs in page i. Definition 5 (equivalence class): An equivalence class (es) is a set of tag paths whose occurrence vectors have same elements in m pages, where m/n is larger than a user defined support threshold s. Notice that a single tag path can be an equivalence class. Based on the above definitions, we now introduce our top-down template induction method, whic h generates a more specific template in each iteration. First we identify several general data sections to construct root template by detecting from tag path texts, their corresponding occurrence vectors and occurrence positions (shown in Algorithm 1). Algorithm 1 InduceRootTemplate Input: A set of input HTML pages D n , a support threshold s Output: Induced root template RT . Steps: 01: Convert D n into pages of tag path representation DP 02: Go through DP n , and get all distinct tag path texts TPT , occurrence vectors V TPT and occurrence positions PS 03: Initialize RT to be empty. 04: for each tag path text tpt in TPT do 05: get occurrence vector V tpt for tpt from V 06: if support( V tpt ) &gt;= s * n then 07: add tpt to RT 08: end if 09: end for 10: Identify  X  X ata Section X  between tag path texts 11: return RT The support threshold s is a ratio which is set for handling noisy situations. (It is set to 0.9 in experiments.) Tag path texts whose supports are above this threshol d will be added to the root template. Algorithm 1 takes this threshold and a set of input HTML pages as input to generate root template RT . Algorithm gives more detail steps for root template induction. Given an induced root template, the problem of template induction problem is divided into sub templates X  induction in each data section of the template. For each data section, we further detect patterns of tag paths to identify more data fields and generate more specific template. Specifically, three kinds of data displayed in a list pattern, and (c) other data sections. A data field of the first kind can be identified after a tag path which occurs exactly once in most pages or in certain portions of pages. It is marked as  X #value# X  or  X #optional value# X  respectively. A data field of the second kind can be identified after an equivalence class which occurs one or more times, and among which no more other tag path exists (not nest structure) in most pages. It is marked as  X #list# X . Then more specific data sections are able to be detected since data fields of the first two kinds which have been identified split the original data section into several data sections. Then these data sections will be processed in the next iteration. Algorithm 2 gives more detailed steps for specific template induction. Algorithm 2 InduceSpecificTemplate Input: Root template RT , a set of input pages DP n Output: A specific template ST . Steps: 01: Go through DP n , get all distinct tag paths TP , occurrence vectors V TP and occurrence positions PS 02: Initialize ST = RT , template T to be empty. 03: while T != ST 04: T = ST 05: for each Data Section in T do 06: get TP d , V TPd , PS TPd from TP , V 07: Initialize SubTemplate T s to be empty 08: for each tp in TP d do 09: get V tp from V TPd , PS tp from PS 10: Template Line tl = check( tp , V tp , PS 11: Add tl to T s 12: end for 13: sort T s 14: insert T s to ST 15: end for 16: Go through PS TP , add flag  X  X ata Section X  into ST 17: end while 18: return ST Given the induced template, it is straightforward to extract structured data from pages represen ted by tag paths. We apply the template on each page to get several data fields and construct all results into a table format. We can summarize all the related information of one site in a table where each row is an entity in a detail page and each column is an attribute. Thus each cell in the table will be the attribute value of the entity in corresponding row. The number of the table X  X  column s is determined by the number of data fields in template. For those columns containing data fields of the first kind, they can be stored directly in a database since they are single values of various attributes. As for data fields of the second kind, currently we regard them as multi-valued attributes and store a list of attribute values in one cell of the table where each value is separated by a particular delimiter. Then it is straightforward to normalize the table for more conveni ent storage and retrieval. For each data section in the final template, we store them in a column as an attribute. This is reasonable as a Web site may encode a long data field across multiple te xt nodes (e.g. data field of attribute  X  X escription X ). We post-process the extracted data because of two reasons. First is that in template induction we only consider the template among text nodes. That is, template within a text node will be neglected by our method. For example, if two data fields  X 1.10 X  and  X  X ame X  in an underlying data base are encoded into a text node  X  X rice: $1.10 Category: Game X  in the page, our method will identify such text as a data field which is incorrect. This step aims to split such column in the structured data into several columns by common tags and remove common template in the prefix or suffix. The second reason is that the extracted data may rely on manually attribute labeling if au tomatic labeling method (which usually depends on a set of training data) is not available. The fewer the columns in the extracted data, the less labor consuming for manually labeling. Therefore, this step also aims to filter some unrelated columns. To detect template in each text node, we identify common tokens in each column of extracted data. If common tokens exist in a column, they will help us split the column or filter these tokens if they are common prefix or suffix. In addition, we also detect a common delimiter (e.g.  X : X ) in each column. If such a delimiter exists, it will be used to separa te the attribute and value. The column will be split to several columns which are organized by their attribute name Notice that our method may extract some varying templates as data fields because of the failure of assumption 1 (mentioned in Section 3). We filter such columns by a heuristic rule that a column will be removed if there is only one distinct data field in this column. Also we will filter those columns that contain only a small number of data fields (fewe r than 5% of the input pages). For those noise pages in the input collection, there will be no data field or very few data fields in their corresponding rows. We also filter such very sparse rows. We have crawled thousands of data intensive Websites of 3 domains (which are Mobile app lication, Movie and Restaurant) from Bing X  X  indexed pages to ex tract structured knowledge. For experimental evaluation, we choose 30 Websites of them. We compare the efficiency of our approach with that of RoadRunner, whose implementation in Java is available on the Web 2 . All the experiments are performed on an AMD Opteron 254 computer with a 2.8GHz CPU and 16G of RAM. Our algorithm is implemented in C#, which utilizes Html Agility Pack HTML DOM tree for each page. For all detail pages in each Website, we run our algorithm to get structured data in a table format. After that, we manually label attributes of each column to build knowledge base. To compare with the algorithm of RoadRunner, which is not scalable on such large data, we sample http://www.dia.uniroma3.it/db/roadRunner/ http://htmlagilitypack.codeplex.com/ hundreds of pages in each Website. On these sampled pages, both algorithms are run in the exactly same procedure and platform. www.freewarepocketpc.net 97.25% 74.59% www.pocketpc-freeware.net 99.36% 80.14% Average 97.51% 74.88% We use precision and recall as primary metrics to evaluate the effectiveness of SKES. Since our datasets are very large, to generate a complete ground truth is very hard. Thus, we randomly choose 100 pages from each Website and manually label all data fields in these pages by 6 invited editors as ground truth. To evaluate the precision of our approach, they label the extracted results of the sampled pages in each Website to get true positives and false positives. True positives are the set of data fields correctly extracted by our appro ach, and false positives are the set of data fields extracted incorrectly. Then the precision and recall for each Website can be calculated. Experimental results of our approach are shown in Table 1. As is shown in Table 1, our approach achieves very high precision and good recall on most Websites. We generate six datasets from each Website by random sampling, which contains 40, 80, 120, 160, 200, and 240 detail pages respectively. Table 2 shows the experimental result of our approach and RoadRunner, which has been averaged on the 30 Websites. We can see that the runtime of our approach increases linearly with respect to the number of pages, while the runtime of RoadRunner increases sharply. The author of RoadRunner reported in their work [6] that the AND-OR tree used in RoadRunner has in the worst case exponential size due to the need to explore different alternatives for each mismatch. Although some techniques have been proposed to lower the complexity, it seems that th e algorithm is still very time consuming in dealing with real pages. While our algorithm uses a divide-and-conquer strategy, whic h shows very high efficiency. The runtime of SKES is mainly affected by two aspects: the number of pages and the average number of tag path texts in a page. Since the variance of the number of tag path texts is not very big in different Websites, he re we evaluate the runtime of our approach in pages with diffe rent scales. SKES is applied on over 10 thousand high traffic Website s indexed by Bing to verify its efficiency. The runtimes of our approach in each Website are recorded, some of which are shown in Figure 4. As the figure shows, the runtime of SKES is ge nerally linear with respect to the number of pages. For clusters containing hundreds of thousands pages, our approach can finish in hours. Number of Pages Runtime of RoadRunner Runtime of 
Percent of Noises Precision Recall In this section we evaluate the performance of our approach in noisy situation. By adding irreleva nt pages (e.g. index pages, list pages) into the original detail pages dataset in a Website, we generate 5 more clusters which contain 2%, 4%, 6%, 8% and 10% noise pages respectively. Our approach is run on the 5 clusters, the performance of which is shown in Table 3. As is shown in Table 3, the performance of our approach only has a light degradation when the percent of noise pages is less than 8%. It demonstrates that our approach can tolerant a certain portion of noise pages. We also implement a prototype system for user search task simplification (shown in Figure 5) that utilizes the knowledge base extracted by SKES. Since the problem of how to identify the intent of user queries and trigge r related structured knowledge to users is also very difficult, in this system we employ a naive keyword based method. That is, if user queries contain words that equal to data fields in our know ledge base, we just select and return structured knowledge of entities that satisfy all the keywords. If there is no satisfied entity, a traditional page list from Bing API 4 will be returned. Notice that it is only a prototype system currently. Understanding more complex queries correctly is our future work. 
Figure 5. An example of returned structured knowledge by Is the Demo helpful for knowledge seeking? 4.36 The Demo usually returns correct table? 4.16 A number of queries can trigger the Demo to return structured table? You need not browse any pages to seek knowledge? 3.53 Is the user interface of the Demo acceptable? 4.06 Can you save a lot of time by using this Demo? 4.10 We designed a questionnaire and distributed it to 15 users who used this prototype system for a week to get their feedback and subjective evaluation of our system. In the questionnaire, we asked users some general informa tion about the prototype system. http://www.bing.com/developers Several interesting items are liste d in Table 4. These questions were answered on a scale of 5 with 5 = strongly agree and 1 = strongly disagree. As is shown in Table 4, users generally satisfied the prototype system and admitted that it was helpful for them to seek knowledge. The paper presents an algorithm SKES to automatically extract structured knowledge from semi-s tructured Websites for search task simplification. By employ ing techniques such as top-down template induction algorithm and tag path representation, SKES is shown to be quite efficient and noise-tolerant in real Websites. After running our algorithm on thousa nds of related sites, a large structured knowledge base of high quality is built on three domains. We also develop a prototype system to simplify user search task by using the built knowledge base. User study shows that it really helps users for knowledge seeking. The work was supported by MSRA Funding with No. FY11-RES-OPP-058. [1] A RASU , A. AND G ARCIA -M OLINA , H. 2003. Extracting structured data from web pages. In SIGMOD Conference . 337 X  348. [2] B ANKO , M., C AFARELLA , M. J., S ODERLAND , S., B M., AND E TZIONI , O. 2007. Open information extraction from the web. In IJCAI . 2670 X 2676. [3] B RIN , S. 1998. Extracting pattern s and relations from the world wide web. In WebDB . 172 X 183. [4] C AFARELLA , M. J., H ALEVY , A. Y., Z HANG , Y., W AND 0002, E. W. 2008. Uncovering the relational web. In WebDB . [5] C OHEN , W. W., H URST , M., AND J ENSEN , L. S. 2002. A flexible learning system for wrapping tables and lists in html documents. In Proceedings of the 11th international conference on World Wide Web . WWW  X 02. ACM, New York, NY, USA, 232 X 241. [6] C RESCENZI , V., M ECCA , G., AND M ERIALDO , P. 2001. Roadrunner: Towards automatic data extraction from large web sites. In VLDB . 109 X 118. S Unsupervised named-entity extraction from the web: An experimental study. Artif. Intell. 165, 1, 91 X 134. [8] Y ANG , G., R AMAKRISHNAN , I. V., AND K IFER , M. 2003. On the complexity of schema inference from web pages in the presence of nullable data attributes. In Proceedings of the twelfth international conference on Information and knowledge management . CIKM  X 03. ACM, New York, NY, USA, 224 X 231. [9] Z HAO , H., M ENG , W., W U , Z., R AGHAVAN , V., 2005. Fully automatic wrapper generation for search engines. In In Proceedings of the 14th international conference on World Wide Web (WWW2005). 66 X 75. [10] Z HAO , S. AND B ETZ , J. 2007. Corroborate and learn facts from the web. In proceddding of the 13 th ACM SIGKDD conference on Knowledge Discovery and Data Mining ( KDD2007) . 995 X 1003. 
