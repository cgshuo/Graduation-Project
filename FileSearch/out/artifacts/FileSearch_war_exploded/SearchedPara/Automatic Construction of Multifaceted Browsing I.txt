 Databases of text and text-annotated data constitute a signif-icant fraction of the information available in electronic form. Searching and browsing are the typical ways that users lo-cate items of interest in such databases. Interfaces that use multifaceted hierarchies represent a new powerful browsing paradigm which has been proven to be a successful comple-ment to keyword searching. Thus far, multifaceted hierarchies have been created manually or semi-automatically, making it difficult to deploy multifaceted interfaces over a large num-ber of databases. We present automatic and scalable methods for creation of multifaceted interfaces. Our methods are inte-grated with traditional relational databases and can scale well for large databases. Furthermore, we present methods for se-lecting the best portions of the generated hierarchies when the screen space is not sufficient for displaying all the hierarchy at once. We apply our technique to a range of large data sets, in-cluding annotated images, television programming schedules, and web pages. The results are promising and suggest direc-tions for future research.
 H.3.1 [ Information Storage and Retrieval ]: Content Anal-ysis and Indexing; H.4.3 [ Information Systems Applica-tions ]: Communications Applications Algorithms, Design, Experimentation, Measurement Multifaceted Hierarchies, Browsing, Hiearchy Construction, Faceted Classification, Faceted Navigation
Databases of text and text-annotated data constitute a sig-nificant fraction of the information available in electronic form. Users typically locate items of interest in such databases ei-ther by using keyword-based search or by browsing through
Copyright 2005 ACM 1-59593-140-6/05/0010 ... $ 5.00. the contents of the collection. Concept hierarchies are com-monly used to support browsing activity. Most libraries use the Dewey Decimal system to organize their holdings, allow-ing their users to navigate easily through the contents of the library. Yahoo! also uses a topic-based hierarchy to organize web sites according to their topic and allows the users to iden-tify quickly web pages of interest.

Most of the existing systems use a single hierarchy to present the contents of a collection. Early work by Pollitt [28] and more recently by Yee et al. [33] showed that multifaceted hi-erarchies, which allow users to browse across multiple dimen-sions, are superior than single, monolithic hierarchies. For example, consider the case of a user looking for images with dogs playing with children in a farm. Having multiple hier-archies, the user can browse first through the hierarchy  X  X n-imals X  and select the category  X  X ammals  X  Carnivores  X  Dogs. X  Then, having the  X  X nimal X  dimension fixed, the user can browse through the hierarchy  X  X laces X  to locate images with farms, and then browse through the hierarchy  X  X eople X  to locate images with children. Such multifaceted interfaces expose the contents of the underlying collection and can help users more quickly locate items of interest. The major short-coming of the systems that use multifaceted hierarchies is the need to: 1. Identify manually the dimensions/facets that can be used 2. Create manually the hierarchies for each dimension.
In this paper, we present techniques for automatically con-structing multifaceted hierarchies from a large collection of text or text-annotated objects. Specifically, the contributions of this paper are: 1. A technique for discovering facets that can be used to 2. An efficient hierarchy construction algorithm that lever-3. A set of methods for selecting the best portions of the 4. An extensive experimental evaluation, including three
The rest of the paper is organized as follows: Section 2 gives the necessary background. Section 3 describes our facet extraction technique and Section 4 outlines our subsumption-based hierarchy construction algorithm. Section 5 describes our algorithms for selecting the best categories from a hierar-chy. Section 6 describe the experimental setting and results. Finally, Section 7 discusses related work and Section 8 con-cludes the paper.
Although we are aware of no work on automatic construc-tion of multifaceted browsing schemes, automatic creation of subject hierarchies has been attracting interest for a long time, mainly in the form of clustering [34, 23]. Scatter/Gather [6] was a pioneering system that used clustering for browsing large document collections. Scatter/Gather demonstrated that clus-tering reveals quickly to the user the contents of the underlying collection. However, automatic clustering techniques generate clusters that are typically labeled using a set of keywords, re-sulting in category titles such as  X  battery california technology mile state recharge impact official hour cost government  X  [12]. While it is possible to understand the content of the docu-ments in the cluster from the keywords, this presentation is hardly ideal.

Sanderson and Croft [30] presented an alternative technique for generating concept hierarchies from text, based on the con-cept of subsumption . For terms x and y from a document collection, term x subsumes y ( x  X  y ) if: where P ( x | y ) is the probability that term x occurs in a doc-ument, given that term y does. Using the generated sub-sumption relations, the algorithm creates a hierarchy of terms, which has as top-level categories the general terms, and as lower-level categories terms that are more specific in nature.
While subsumption is a simple and effective technique for hierarchy construction, it has two shortcomings: (1) it requires n 2 computations of conditional probabilities, where n is the number of terms in the collection, and (2) it requires the terms to have a unique meaning (i.e., requires sense disambiguation when a term has multiple potential meaning). Sanderson and Croft sidestepped these problems by focusing only on query results and using only terms that appear more frequently in the query results than in the whole collection. 1 These heuris-tics guaranteed that the terms were not ambiguous and that only a small number of terms is used.

Unfortunately, these heuristics do not apply if we want to create hierarchies for arbitrary collections. Next, in Section 3, we show how our facet extraction technique reduces the am-biguity problem, and then, in Section 4, we show how we can improve the complexity of the hierarchy construction algo-rithm.
One of the problems that may appear during the construc-tion of a concept hierarchy is the fact that the same collection can be browsed in many different, orthogonal ways. Consider for example how a user can browse the schedule of TV pro-grams. It is possible to browse by time, by TV channel, or by title. It is also possible to browse by actor, for example, or by many other dimensions. Mixing terms that belong to dif-ferent dimensions might result in an awkward hierarchy: an actor might be classified under the term  X  X onday X  because of
On average they extracted 2350 terms for hierarchy construc-tion. a sitcom that is shown every Monday night. While it is a per-fectly valid subsumption relation, it is not a structure useful for browsing.

In this section we present our technique for extracting im-portant navigational facets from a collection. Since there are already collections that have metadata organized across differ-ent facets (e.g., the Corbis image collection  X  see Section 6.1), we can use these data to train a machine learning algorithm to classify keywords in the appropriate facets. For example, the words  X  X at X  and  X  X og X  are under the  X  X nimals X  facet, while the words  X  X ountain X  and  X  X ields X  are under  X  X opo-graphic Features. X  Therefore, it is possible to use the facet as a target class and the keywords as features, in order to as-sign keywords to the appropriate facet. Unfortunately, such a straightforward approach does not generalize. A classifier trained in this way will classify correctly only words that have been assigned to facets before. A classifier might classify cor-rectly the words  X  X at X  and  X  X og X  in the  X  X nimals X  facet, but a new word, such as  X  X heep, X  will not be assigned to any facet.
To allow our technique to generalize, we rely on the ob-servation that keywords under different facets tend to have different  X  X ypernyms. X  2 Based on this observation, we ex-pand each keyword using its hypernyms from a lexical corpus, such as WordNet [8]. After the expansion, each keyword is represented as a set of words. For example, the word  X  X at X  is represented as  X  X at, feline, carnivore, mammal, animal, liv-ing being, object, entity X . The new representation allows the classifier to generalize more easily and assign unseen words to the correct facets.

However, using hypernyms does not resolve the problem of sense disambiguation. Each word can have different meanings according to its context. Consider the word  X  X id, X  which can mean either a young person or a young goat. Before assigning this word to a facet, we have first to decide the true meaning of the word. To identify the correct sense of the word, we exploit the fact that keywords are associated with objects and each object is characterized by a set of other keywords, which provide valuable clues for the true meaning of the word. (The use of context is the basis of many techniques [21] for sense disambiguation. 3 ) For example, when the word  X  X id X  appears together with the words  X  X oat, X  and  X  X razing X  then  X  X id X  is much more likely to refer to a young goat than to a child.
Based on the above observations, we treat facet classification as a text classification problem. In text classification [20, 7], we characterize each document using a set of words; based on the presence of these words across categories, we train a classifier to assign documents to the appropriate categories. In our case, we treat each keyword as three sets of words. The first set of words contain the keyword itself, the second set contains the hypernyms of all the senses of the keyword, and the third set contains the other keywords associated with the object.
Hypernym is a linguistic term for a word whose meaning includes the meanings of other words, as the meaning of vehicle includes the meaning of car, truck, motorcycle, and so on.
We should emphasize that disambiguation for facet extrac-tion is easier than the general problem of sense disambigua-tion. First, the context keywords are of high quality, some-thing that is not always the case in natural language sentences. Second, and most important, while a word might have multiple senses, the senses are often closely related (see for example, the WordNet senses for  X  X ear X  and  X  X attle X ). While sense disambiguation is hard for such words, closely-related senses typically correspond to the same facet ( X  X eneric Thing X  for  X  X ear X  and  X  X ction, Process, or Activity X  for  X  X attle X ), elim-inating the need for disambiguation for facet extraction.
Specifically, our algorithm, for assigning the keywords into facets, performs the following steps: 1. Get a collection D of text-annotated objects. Each ob-2. For each keyword/facet pair k ij /F ij : 3. Train a document classifier over the prepared training
After creating the classifier, we can use it over a new set of annotated objects, to identify the facets that appear in the collection. After running the classifier over the keywords of the new objects, we can examine which facets appear frequently in the new data and use these facets for browsing. Empirically, we observed that facets that appear in 5% of the data can be proved useful for locating content of interest. One  X  X isadvan-tage X  of supervised learning techniques is that they cannot  X  X iscover X  new types of facets. Identifying new, previously unknown dimensions for browsing is an interesting direction for future research.

In this paper, we gathered our training data from a set of annotated images from the Corbis collection (see Section 6.1), which contained a comprehensive set of facets. 4 We report the experimental setting and the results in Section 6.
The objective of this work is to construct automatically con-cept hierarchies for browsing large collections of text or text-annotated objects. Current algorithms for concept hierarchy construction do not scale well and are inadequate for creating concept hierarchies for collections with tens of thousands of documents.
Our hierarchy construction algorithm extends the subsump-tion-based algorithm from [30], to include the notion of direc-tionality and the notion of equivalent terms . Two terms x and y might co-occur in all but a few documents: these terms, from a co-occurrence point of view, are equivalent. A naive use of subsumption, though, would consider one of them as  X  X arent X  term and the other as  X  X hild. X  The directionality property ensures that x subsumes y only when y appears in a small fraction of the documents that contain x (i.e., x is considerably  X  X ore general X  than y ). If two terms co-occur frequently, and none of them subsumes the other, we consider them  X  X quivalent X  for the purpose of hierarchy construction. Definition 4.1. [Subsumption and Equivalency]
A term x subsumes y , ( x  X  y ) if:
The terms x and y are equivalent ( x  X  y ) if:
The whole collection contains more than 3 million images, and approximately 40 facets.
We denote with  X  d (  X  d &gt; 1) the directionality threshold and with  X  s (0 &lt; X  s &lt; 1) the subsumption threshold . We define contain both terms x and y and f ( x ) is the number of objects that contain term x . 2
One of the shortcomings of subsumption is its reliance on pairwise computation of conditional probabilities. Therefore the cost of a naive method is at least O ( n 2 ) where n is the number of terms. Our facet extraction algorithm from Sec-tion 3 improves this complexity by separating the keywords into different facets. When we create separate subsumption hierarchies for each facet, then the complexity is O ( m  X  ( O ( n 2 m ), where m is the number of facets. However, even this improvement is not sufficient when we have to work with col-lections that contain tens of thousands of objects and terms. Next, we present a set of techniques that can improve substan-tially the efficiency of the hierarchy construction algorithms.
In this section we present a novel algorithm that reduces substantially the number of pairwise computations. Before describing the algorithm, we describe one property of sub-sumption:
Lemma 4.1. A term x can subsume term y , only if f ( x ) &gt;  X  document frequencies of x and y , respectively.

By exploiting this lemma, we can build hierarchies by com-puting a substantially smaller number of conditional probabil-ities, using the following algorithm: 1. Sort the terms by increasing document frequency. 2. For each term x , compute the conditional probability 3. Examine which pairs x,y satisfy the subsumption re-Since most keyword frequencies in most collections follow a Zipfian distribution, this algorithm needs only 25%-30% of the time needed by the basic subsumption algorithm, depending on the value of the thresholds. (We omit the proof due to space constraints.) In Section 6.3 we quantify in detail the benefits our the algorithm.

Furthermore, by construction, the graph generated by this algorithm is a directed acyclic graph. To construct the concept hierarchy, it is necessary to eliminate the forward-edges, i.e., edges that connect a term x with its  X  X randchildren. X  To perform this task we have to perform a topological sort on the graph, which increases the computational cost. To avoid creating edges that will be later eliminated, we can exploit the following lemma.

Lemma 4.2. If x subsumes y ( x  X  y ) and y subsumes z ( y  X  z ), then x subsumes z but the edge x  X  z will be a forward-edge and will be eliminated from the final hierarchy.
Based on this lemma, we can modify the hierarchy construc-tion algorithm to eliminate from consideration all the term pairs that form  X  X edundant X  subsumption relations (i.e., sub-sumptions that will be eliminated later). Finally, we can speed up considerably the identification of pairs of equivalent terms using the following lemma: CreateHierarchy
LocateEquivalent(Term x ) a1: foreach term y  X  T with f ( y ) &gt;f ( x ) and a2: if ( x is equivalent to y ) a3: Merge x and y ( x  X  y ) a4: Remove y from T
CreateEdges(Term t ) // Check only terms with sufficiently smaller frequency b1: foreach term y  X  T with f ( y ) &lt; min { 1 / X  d , X  s b2: if (( x subsumes y ) b3: Create the edge x  X  y b4: Mark y as covered by x b5: CreateEdges( y )
Lemma 4.3. Two terms x and y , with f ( x )  X  f ( y ) , can where f ( x ) and f ( y ) are the document frequencies of x and y , respectively.

The resulting algorithm that exploits all three lemmas is shown in Figure 1. The algorithm guarantees that the re-sulting graph is a directed acyclic graph, with no forward edges; therefore it can be used directly to browse the collec-tion. Furthermore, our strategy eliminates from consideration term pairs that a-priori cannot form subsumption or equiva-lency relations (steps a1 and b1). Additionally, the predicates in a1 and b1 are easy to implement in relational database systems, which have been specially optimized to execute such band joins. The only requirement is to keep a table with the term frequencies and build an index on the frequency field. We implemented our prototype on top of a relational data-base system (Microsoft SQL Server 2000) using mainly SQL statements for hierarchy construction. The presented algo-rithm works best when low-frequency terms (which are often highly-specific) are subsumed  X  X arly X  by other terms. This is fortunately the case for most of the cases where it is possible to extract a meaningful hierarchy from the collection.
So far, we have described the hierarchy construction algo-rithm assuming that each object comes associated with a set of keywords. For some object types (e.g., annotated images) the associated keywords are highly descriptive and can be used directly with no further steps. The keyword extraction pro-cess, though, is more complicated when the object annotation is in free text form. One approach would be to use as terms all the words that appear in the collection. Still, even after removing stopwords and filtering out infrequent words, unin-formative words like  X  X lept, X  or  X  X pproximately X  appeared in the generated hierarchies.

To eliminate such problems, we decided to keep as terms only the nouns and noun phrases that appear in the free text annotations. This choice is supported by earlier studies [11], which indicated that nouns and noun phrases are good fea-tures for constructing clusters of text documents. While pars-ing to identify nouns and noun phrases adds an additional overhead in the hierarchy construction process, it also reduces substantially the overall number of terms. This reduction re-sults in large gains of efficiency, as we have discussed in Sec-tion 4.2. Additionally, nouns and noun phrases are typically better suited for titles of categories, and this results in a better browsing experience.
Consider a user browsing a collection using a small-screen device, such as a smartphone. Typically, the screen cannot dis-play more than 5-10 categories. Thus, if a category has many children then the user cannot see all the children at once. The most prevalent solution to this problem is to paginate the re-sults, showing the  X  X est X  categories first. Ranking categories is a non-trivial problem; the lack of explicit user goals (char-acteristic of browsing) makes the problem even harder.
In this section, we propose a set of techniques for ranking the children of a category. Section 5.1 presents a frequency-based ranking, which serves as our baseline. Then, Section 5.2 presents our set-cover-based approach, which tries to maxi-mize coverage of the collection. Finally, Section 5.3 presents our merit-based approach, which ranks higher categories that expose the largest fraction of the collection with the lowest cost for the user.
This technique simply ranks categories by the number of objects that are classified under the category. Categories that contain the larger number of objects are ranked higher. This heuristic allows the user to see first categories with the great-est wealth of information. Moreover, it guarantees that low-ranked categories (that might not appear on screen) represent only a small fraction of the collection.

This ranking scheme is very easy to implement. Further-more, if the categories do not overlap (i.e., no object is clas-sified under multiple categories) then this ranking scheme is optimal in terms of collection coverage. Unfortunately, this is not always the case. When categories overlap, then the frequency-based ranking may become sub-optimal by present-ing first categories with highly overlapping content. The rank-ing scheme that we present next resolves this problem.
The objective of set-cover ranking is to maximize the num-ber of distinct objects that are accessible from the top-k ranked categories. In other words, this ranking tries to maximize the cardinality of the set o ( C 1 )  X  ...  X  o ( C k ), where C the top-k categories and o ( C i ) is the set of objects classified under the category C i .

This problem is an instance of the set-cover problem, which is a well-known NP-complete problem [5]. Consequently, the optimal ranking, which exposes the maximum fraction of the collection, would take time exponential to the number of cat-egories. For our purposes, though, the optimal solution is unnecessary: not only it is unjustifiably expensive, but it also has the unfortunate property of generating  X  X on-monotonic X  rankings. That is, the top-k categories are not necessarily a subset of the set with the top-k + 1 categories. This property can lead to non-intuitive interface behavior: after enlarging the browser to show larger parts of the hierarchy, categories that used to be highly-ranked might disappear.
To avoid both the complexity and the non-monotonicity is-sues, we decided to use a greedy algorithm for category rank-ing. The greedy algorithm is the best polynomial algorithm for approximating the set-cover problem, and runs in time lin-ear to the number of categories. Specifically, the algorithm works as follows: 1. Mark all the objects as uncovered. 2. Select category C with the largest number of uncovered 3. Mark all the objects classified under C as covered . 4. If all objects covered or ranked k categories, stop; else
Using this greedy algorithm we can quickly compute the top-k categories and ensure that the displayed categories cover a large fraction of the underlying collection. If we want to see more categories, the algorithm can easily resume and rank the remaining categories.

Both the set-cover and the frequency-based algorithm try to maximize the number of objects that are covered by the displayed, top-k categories, regardless of how easy it is to ac-cess these objects. Next, we present an alternative approach that considers the structure of the underlying hierarchy and the respective effort that the user has to put to locate items of interest.
The ranking methods presented so far focused only on cov-ering as many objects as possible. The merit-based ranking method that we present now, takes into consideration the structural properties of the sub-hierarchies under the cate-gories selected. Specifically, the merit-based method ranks higher categories that enable users to access their contents with the smallest cost, on average.

Before describing our merit-based approach, we first define the cost of locating an object n , classified in a hierarchy H ( H is a directed acyclic graph.) We assume for simplicity that n is reachable through only one path from the root of the hierarchy and denote the path as C 1  X  ...  X  C h . Category C 1 is the root node of H and C h is the leaf node that contains the object n . We treat the browsing activity of a user as a random walk on the hierarchy H . We denote with T ( C i ) the time required to reach n from the node C i . In general, the time spent to reach n consists of three components:
We can add an extra cost for browsing the contents of the wrong category and then returning. For simplicity we do not explore this direction further in this paper. Hence, the time T ( C i ) that a user spends on C i is:
T ( C i ) =  X b ( C i ) + P e ( C i )  X  T ( C i ) + (1  X  P Therefore, the time spent browsing through the whole hierar-chy H to reach n is:
Using this methodology, we can compute the average cost for accessing any object classified under the hierarchy H i rooted at category C i . It is clear from Equation 2 that hierarchies H with large number of children and long paths have large T ( H ). However, these are the categories that contain the largest num-ber of objects. To avoid unfairly penalizing such categories, we introduce the merit(C) metric for a category C : where o ( C ) is the number of distinct objects classified under C and T ( C ) is defined in Equation 2. This metric is similar to the F -measure (which favors high precision and recall) and favors categories with low cost T ( C ) and large number of objects o ( C ). It should be noted that the merit can be computed very efficiently in a bottom-up fashion.

Using the merit of each category, we can rank categories appropriately, putting first categories that have good hierarchy structures under them and provide access to a large number of objects.
In this section present the experimental evaluation of our techniques. We first describe the data sets that we used (Sec-tion 6.1). Then, in Section 6.2, we evaluate our facet ex-traction technique of Section 3; in Section 6.3, we examine the efficiency of our hierarchy construction algorithm of Sec-tion 4; and in Section 6.4 we evaluate the structural properties of the generated hierarchies. Finally, in Section 6.5 we provide further discussion, outlining possible future directions. http://www.corbis.com
We evaluated our facet extraction algorithm of Section 3 using the keywords from the Corbis data set. For classifier we used Support Vector Machines (SVM) with linear kernels.
Initially, we tested the accuracy of the classifier without us-ing the WordNet hypernyms and without using the keywords associated with the same image. The classifier, as expected, could not generalize. The accuracy (as measured by the F 1 measure) was 10%, only slightly above the accuracy of a ran-dom classifier. By adding the hypernyms, the performance im-proved considerably, reaching an average F 1 -measure of 71% (detailed results omitted due to space constraints.) This im-provement confirmed our hypothesis that hypernyms are use-ful features for allocating keywords to facets. Nonetheless, the sense ambiguity is still a problem in this case: after adding as extra features the remaining keywords from each document, the classification performance improved considerably, reaching an average F 1 -measure of 81% (see Table 1).

We also wanted to compare our method against variations of other techniques. One hypothesis was that we can create facets by picking some high-level hypernyms from WordNet, which can serve as root nodes for the corresponding facets. For example, the term  X  X nimal/fauna X  in WordNet could serve as the root node for the  X  X nimal X  facet. Subsequently, all terms that have  X  X nimal/fauna X  as a hypernym could be assigned to the  X  X nimal X  facet. (This approach would be close in spirit with the hierarchy construction algorithm in [31].) To test the accuracy of this approach, we trained RIPPER [4], a rule-based classifier, using the keywords and their hypernyms as http://www.rcn.com http://membled.com/work/apps/xmltv http://www.imdb.com http://www.dmoz.org
We kept only the text from each page by stripping the HTML tags using the  X  lynx  X  X ump  X  command.
 Table 1: The average performance of the facet ex-traction technique for each of the 14 facets in the Corbis data set. (Results obtained using 10-fold cross-validation.) speedup Figure 2: The speedup of the our hierarchy construc-tion algorithm over the basic subsumption algorithm, for different values of the subsumption threshold  X  s , for the DMOZ dataset and for  X  d = 1 . 2 . features. The average F 1 -measure in that case was close to 55%, significantly worse than the corresponding results for SVMs. The results also highlighted that some classes work well with simple, rule-based assignments of terms to facets, but there are other classes that need more elaborate classifiers. For example, for the facet GAN ( G eneric AN imals) the rule-based classifier resulted in an F 1 -measure of 93.3%, showing that simple rules work well for this facet. However, for the APA facet ( A ction, P rocess, or A ctivity) the F 1 -measure was only 35.9%, showing that simple rules do not work well for such a complicated facet.
We ran an extensive set of experiments examining the effect of the different parameters of our technique on the running time of the algorithm.

Initially, we examined the speedup obtained after eliminat-ing from consideration terms with low frequency in the col-lection (less than a threshold  X  f ). We observed that the time required for constructing the hierarchies decreased exponen-tially with the threshold  X  f . Based on these results about the quality of the hierarchies, we decided to set  X  f = 16, which resulted in efficient executions and good quality of the hierar-chies (see Section 6.4).

Then, we examined the effect of the thresholds  X  s and  X  d (see Definition 4.1 in Section 4) on the efficiency of our al-Table 2: The time required to build a concept hier-archy (in minutes) as a function of the directionality threshold  X  d (  X  s = 0 . 8 ). In parentheses the percent-age of the terms that were matched as  X  X quivalent X  during the  X  X ocateEquivalent X  step.
 Table 3: The average coverage of the collection, for the different data sets and different ranking methods. (  X  s = 0 . 8 , X  d = 1 . 2 ) gorithm. We observed that execution time decreases as the subsumption threshold  X  s becomes smaller. This was an ex-pected outcome according to Lemma 4.1: small values of  X  s disallow the formation of subsumption relations between terms with similar frequencies.

The effect of the directionality threshold  X  d is more mixed (see Table 2). Large values of  X  d force the algorithm of Fig-ure 1 to perform more comparisons to locate equivalent terms, slowing the execution of the  X  X ocateEquivalent X  procedure. However, large values of  X  d identify more  X  X quivalent X  terms, which in turn reduces considerably the number of terms used by the  X  X reateEdges X  procedure. In our data sets, we identi-fied a value of  X  d  X  1 . 2 to be a good choice, a choice reinforced by the experiments of Section 6.4.

Finally, Figure 2 shows the speed improvement of our al-gorithm over the basic subsumption algorithm, for different values of  X  s and  X  d = 1 . 2, for the DMOZ data set. For the value  X  s = 0 . 8, the algorithm runs 3 times faster than the basic subsumption algorithm. The speedups are higher for the XMLTV and Corbis datasets that benefit more from the reduction of nodes from the  X  X ocateEquivalent X  step of our algorithm of Figure 1. (See Table 2 for the statistics.)
Beyond efficiency, another important aspect of hierarchy construction is the quality of the generated hierarchies. For this purpose, we examined the structural properties of the generated hierarchies and examined how these properties can affect the browsing experience. In the future we plan to con-duct a study of the user interface using human subjects, but the results of such a study are out of the scope of this paper. (See Section 6.5 for some anecdotal results.) We examined the following properties of the generated hierarchies: Coverage : We define coverage as the fraction of the collection objects that are reachable using the hierarchy. Unreachable objects are those annotated only with terms that do not ap-pear in the hierarchy. The perfect hierarchy has coverage is equal to 1, allowing the user to reach all objects of the hier-archy. We used coverage as one of the metrics to evaluate the ranking schemes of Section 5. We show the results in Table 3 for  X  s = 0 . 8 and  X  d = 1 . 2 and after selecting 10 categories per node. (The results were similar for other values of the thresh-olds.) While all methods achieve a relatively good coverage, the set-cover method (as expected) consistently covers larger fraction of the collection compared to the other two methods. Table 4: The average cost for reaching an object us-ing hierarchies created by different ranking methods, computed using Equation 2 (  X  = 1 ,P e (  X  ) = 0 . 2 ). The merit-based method performs slightly worse than the set-cover , an expected result since set-cover is explicitly designed to optimize the coverage of the hierarchy.
 Cost : In addition to coverage, we also measured additional structural characteristics of the hierarchy, such as the average path length to find an object (shorter paths are preferable), and the average branching factor (small branching factors are preferred, since users can decide faster which category is best). Since configurations that optimize average path length tend to have larger branching factors, we decided to use the cost metric from Section 5.3 to combine meaningfully these met-rics. Table 4 summarizes the results. Merit-based hierarchies consistently perform better than the other two approaches, de-creasing by 10-50% the time needed to locate items of interest, compared to the other approaches.
 Conclusions : In general, merit-based performs very well and offers fast access to the contents of the collection. Addition-ally, merit-based rankings are efficient to implement on top of relational database systems, while the set-cover rankings typically take longer to compute.
To experience first-hand the quality of the hierarchies, we built a hierarchy browser and we used it to explore the contents of multiple collections. We also experimented with different methods for presenting the hierarchies (e.g., using RSVP tech-niques for text [9]) but analyzing the user interface is out of the scope of this paper.

Our informal results suggest that keyword extraction is vi-tal for generating good quality hierarchies. For example, for the Corbis and XMLTV data sets, the objects contained high-quality keywords that in turn helped generate high-quality hierarchies that were informative and easy to use. For the DMOZ dataset, the quality of the hierarchies depended on the quality of the textual content in each web page. Web pages with little or no quality text were difficult to charac-terize properly and, correspondingly, difficult to organize in a hierarchy. We believe that more advanced techniques for keyword indexing (e.g.,[32, 13] for text or [1, 14] for images) and standard techniques for web indexing (e.g., use of anchor text) can substantially improve the hierarchies.
Several methods have been proposed for browsing collec-tions of text or multimedia documents. Scatter/Gather [6] demonstrated that clustering can be used for browsing large collection of text documents. Multiple methods (e.g., [34, 2, 23]) have been proposed for clustering text and web data. The difficulty of meaningfully labeling the clusters led to the intro-duction of concept hierarchies: Sanderson and Croft [30] intro-duced the subsumption hierarchies and Lawrie and Croft [18] showed experimentally that subsumption hierarchies outper-form lexical hierarchies [25, 26, 27]. Lawrie et al. [17, 19] suggested a technique for identifying terms that are useful for hierarchy construction. The algorithm in [17] is similar to the algorithm used by the set-cover ranking. The main difference is the optimization goal: set-cover tries to optimize the cov-erage of the collection from the hierarchy, while the algorithm in [17] tries to maximize the (weighted) coverage of the collec-tion vocabulary.
 Kominek and Kazman [15] use the hierarchical structure of WordNet [8] to offer a hierarchy view over the topics covered in videoconference discussions. Stoica and Hearst [31] also use WordNet together with a tree-minimization algorithm to cre-ate an appropriate concept hierarchy for a collection. Barnard et al. [1] used WordNet to disambiguate the keywords associ-ated with each image, and to generate clusters of higher qual-ity. As an alternative to creating a separate hierarchy for each collection, Chaffee and Gauch [3] presented a system that uses a personalized ontology to offer a common browsing experience across collections of web pages (i.e., web sites) that organize their contents in different ways. Other, less common browsing structures were proposed (e.g., wavelet-based text visualiza-tion [24], dynamic document linking [10]) but clustering and hierarchy-based approaches continue to be the most popular interfaces for browsing.

Faceted interfaces, which use multiple, orthogonal classi-fication schemes to present the contents of a database, be-come increasingly popular. A large number of e-commerce web sites use faceted interfaces [16], based on engines provided by companies such as Endeca 12 and Mercado 13 , which expose the facets that are already defined for the products (e.g.,  X  X y price, X   X  X y genre X  and so on). Academically-developed sys-tems, such as Flamenco [33], HiBrowse [28], and OVDL [22], demonstrated the superiority of faceted interfaces over single hierarchies. Our work on automatic construction of multi-faceted interfaces contributes to this area and facilitates the deployment of faceted databases. In an orthogonal direction, Ross and Janevski [29] presented work on searching faceted databases and described an associated entity algebra and an query engine.
We presented methods for automatically constructing mul-tifaceted hierarchies, and methods for selecting the best parts of the generated hierarchies when it is not possible to fit all the categories on screen. Our experiments with real-life data sets indicate that automatic construction of multifaceted interfaces is feasible, and generates high-quality hierarchies. We are in-terested in exploring different ways of presenting the hierar-chies to expose the contents of the collection in efficient ways. Furthermore, we are interested in integrating better browsing and searching in multifaceted databases. Creating the appro-priate indexing structures to support concurrent searching and browsing is a promising direction for future research. The authors would like to thank Gavin Smyth of Microsoft Research Cambridge for his work on the implementation of the hierarchy browser. http://www.endeca.com http://www.mercado.com
