 Approximate dynamic programming methods often offer surprisingly good performance in practical problems modeled as Markov Decision Processes (MDP) [6, 2]. To achieve this performance, the parameters of the solution algorithms typically need to be carefully tuned. One such important pa-rameter of MDPs is the discount factor  X  . Discount factors are important in infinite-horizon MDPs, in which they determine how the reward is counted. The motivation for the discount factor originally comes from economic models, but has often no meaning in reinforcement learning problems. Nev-ertheless, it is commonly used to ensure that the rewards are bounded and that the Bellman operator is a contraction [8]. In this paper, we focus on the quality of the solutions obtained by approximate dynamic programming algorithms. For simplicity, we disregard the computational time, and use performance to refer to the quality of the solutions that are eventually obtained.
 In addition to regularizing the rewards, using an artificially low discount factor sometimes has a significant effect on the performance of the approximate algorithms. Specifically, we have observed a significant improvement of approximate value iteration when applied to Tetris, a common rein-forcement learning benchmark problem. The natural discount factor in Tetris is 1, since the received achieved with approximate dynamic programming algorithms are on average about 6000 lines re-moved in a single game [4, 3]. Our results, depicted in Figure 1, with approximate value iteration and standard features [1] show that setting the discount factor to  X   X  (0 . 84 , 0 . 88) gives the best expected total number of removed lines, a bit more than 20000. That is five times the performance with discount factor of  X  = 1 (about 4000). The improved performance for  X   X  (0 . 84 , 0 . 88) is sur-prising, since computing a policy for this discount factor dramatically improves the return calculated with  X  = 1 . Figure 1: Performance of approximate value iteration on Tetris with different discount factors. For evaluation of the policy with  X  = 1 ) on the 100 games, and averaged over 10 learning runs. In this paper, we study why using a lower discount factor improves the quality of the solution with regard to a higher discount factor. First, in Section 2, we define the framework for our analysis. In Section 3 we analyze the influence of the discount factor on the standard approximation error bounds [2]. Then in Section 4 we argue that, in the context of this paper, the existing approximation error bounds are loose. Though these bounds may be tightened by a lower discount factor, they are not sufficient to explain the improved performance. Finally, to explain the improved performance, we identify a specific property of Tetris in Section 5 that enables the improvement. In particular, the rewards in Tetris are received sparsely, unlike the approximation error, which makes the value function less sensitive to the discount factor than the approximation error. In this section we formalize the problem of adjusting the discount factor in approximate dynamic programming. We assume  X  -discounted infinite horizon problems, with  X  &lt; 1 . Tetris does not undiscounted infinite horizon problems with a finite total reward can be treated as discounted prob- X  -discounted problem and the undiscounted problem have the same optimal policy. We therefore treat Tetris as a discounted problem with a discount factor  X   X  &lt; 1 near one. The analysis is based on Markov decision processes, defined as follows.
 to state s 0 from state s given action a ), and r : S  X  A 7 X  R + is a (non-negative) reward function. We assume that the number of states and actions is finite, but possibly very large. For sake of sim-plicity, we also assume that the rewards are non-negative; our analysis can be extended to arbitrary rewards in a straight-forward way. We write k r k  X  to denote the maximal reward for any action and state.
 Given a Markov decision process ( S,A,P,r ) and some discount factor  X  , the objective is to find a of  X  from state s is defined as the  X  -discounted infinite horizon return: It is well known [7, 2] that this problem can be solved by computing the optimal value function v  X  , which is the fixed point of the Bellman operator Lv = max  X  r  X  +  X P  X  v . Here r  X  is the vector on S with components r ( s, X  ( s )) and P  X  is the stochastic matrix associated with a policy  X  . We consider in this paper that the MDP is solved with 1) an approximate dynamic programming value and policy iteration with existing error bounds. These methods invariably generate a sequence of approximate value functions, which we denote as  X  v  X  . Then,  X   X  is a policy greedy with regard to the approximate value function  X  v  X  .
 As we have two different discount factors, we use a subscript to denote the discount factor used in calculating the value. Let  X  be a discount factor and  X  any policy. We use v  X   X  to represent the value of policy  X  calculated with the discount factor  X  ; when  X  is the optimal policy corresponding to the discount  X  , we will simply denote its value v  X  . As mentioned above, our objective is to compare, policy derived from the approximate  X  -discount value. The following shows how this error may be decomposed in order to simplify the analysis. Most of our analysis is in terms of L  X  mainly because this is the most common measure used in the existing error bounds. Moreover, the results could be extended to L 2 norm in a rather straight-forward way without a qualitative difference in the results. Thus 0  X  v  X   X  v  X   X   X   X  v  X   X  v  X   X   X  and consequently: mation error . In other words, a bound of the loss due to using  X   X  instead of the optimal policy for discount factor  X  is the sum of the error on the optimal value function due to the change of discount and the error due to the approximation for discount  X  . In the remainder of the paper, we analyze each of these error terms. In this section, we develop a discount error bound and overview the existing approximation error bounds. We also show how these bounds motivate decreasing the discount factor in the majority of MDPs. First, we bound the discount error as follows.
 Theorem 2. The discount error due to using a discount factor  X  instead of  X  is: Proof. Let L  X  and L  X  be the Bellman operators for the corresponding discount factors. We have now: k v  X  and  X  respectively. Then we have: Finally, the bound follows from above as: Remark 3 . This bound is trivially tight, that is there exists a problem for which the bound reduces to equality. It is however also straightforward to construct a problem in which the bound is not tight. 3.1 Approximation Error Bound We now discuss the dependence of the approximation error e a (  X  ) on the discount factor  X  . Approxi-mate dynamic programming algorithms like approximate value and policy iteration build a sequence are approximate because at each iteration the value  X  v k  X  is an approximation of some target value v , which is hard to compute. The analysis of [2] (see Section 6.5.3 and Proposition 6.1 for value the optimal policy: To completely describe how Eq. (1) depends on the discount factor, we need to bound the one-step lar approximation framework used and is in general difficult to estimate, we propose to make the following assumption.
 Assumption 4. There exists  X  (0 , 1 / 2) , such that for all k , the single-step approximation error is bounded by: We consider only  X  1 / 2 because the above assumption holds with = 1 / 2 and the trivial constant approximation  X  v k  X  = k r k  X  / 2 .
 Remark 5 . Alternatively to Assumption 4, we could assume that the approximation error is constant bound is unlikely in practice. To show that, consider an MDP with two states s 0 and s 1 , and a single Assume a linear least-squares approximation with basis M = [1 / error in terms of  X  is: 1 / 2(1  X   X  ) = O (1 / (1  X   X  )) .
 If Assumption 4 holds, we see from Eq. (1) that the approximation error e a is bounded as: 3.2 Global Error Bound Using the results above, and considering that Assumption 4 holds, the cumulative error bound when using approximate dynamic programming with a discount factor  X  &lt;  X  is: An example of this error bound is shown in Figure 2: the bound is minimized for  X  ' 0 . 8 &lt;  X  . This is because the approximation error decreases rapidly in comparison with the increasing discount error. More generally, the following proposition suggests how we should choose  X  .
 Proposition 6. If the approximation factor introduced in Assumption 4 is sufficiently large, pre-factor  X  = (2 + 1)  X  p (2 + 1) 2 + (2  X  1) &lt;  X  .
 Figure 3 shows the approximation error fraction necessary to improve the performance. Notice that the fraction decreases rapidly when  X   X  1 .
 Proof. The minimum of  X  7 X  e (  X  ) can be derived analytically by taking its derivative: So we want to know when  X  7 X   X  1 / 2  X  2 + (2 + 1)  X  + 1 / 2(2  X  1) equals 0. The discriminant  X  = (2 + 1) 2 + (2  X  1) = 2 (2 + 3) is always positive. Therefore e 0 (  X  ) equals 0 for the points  X   X  = (2 + 1)  X  This means that  X   X  is a local minimum of e and  X  + a local maximum.
 we see that  X   X   X  0 . Then, the condition  X   X  &lt;  X  is satisfied if and only if: where the inequality holds after squaring, since both sides are positive. We show in this section that the bounds on the approximation error e a (  X  ) are very loose for  X   X  1 and thus the analysis above does not fully explain the improved performance. In particular, there exists a naive bound on the approximation error that is dramatically tighter than the standard bounds when  X  is close to 1.
 Lemma 7. There exists a constant c  X  R + such that for all  X  we have k v  X   X   X  v  X  k  X   X  c/ (1  X   X  ) . Proof. Let P  X  ,r  X  and  X  P,  X  r be the transition reward functions of the optimal approximate policies respectively. The functions may depend on the discount factor, but we omit that to simplify the notation. Then the approximation error is: Thus setting c = 2 max  X  k r  X  k  X  proves the lemma.
 tight. Consider even that the single-step approximation error is bounded by a constant, such that bound. Such a bound implies that: e a (  X  )  X  2  X / (1  X   X  ) 2 . From Lemma 7, this bound is loose error bounds are loose, whenever &gt; 0 . The looseness of the bound will be more apparent in problems with high discount factors. For example in the MDP formulation of Blackjack [5] the discount factor  X  = 0 . 999 , in which case the error bound may overestimate the true error by a factor up to 1 / (1  X   X  ) = 1000 .
 The looseness of the approximation error bounds may seem to contradict Example 6.4 in [2], which shows that Eq. (1) is tight. The discrepancy is because in our analysis we assume that the MDP has fixed rewards and number of states, while the example in [2] assumes that the reward depends on the discount factor and the number of states is potentially infinite. Another way to put it is to say that Example 6.4 shows that for any discount factor  X  there exists an MDP (which depends on  X  ) for which the bound Eq. (1) is tight. We, on the other hand, show that there does not exist a fixed MDP such that for all discount factor  X  the bound Eq. (1) is tight.
 Proposition 6 justifies the improved performance with a lower discount factor by a more rapid de-crease in e a with  X  than the increase in e d . The naive bound from Lemma 7 however shows that e a it may not be sufficient to offset the increase in the discount error.
 Some of the standard approximation error bound may be tightened by using a lower discount factor. For example consider the standard a-posteriori approximation error bound for the value function  X  v  X  [7] : where  X   X   X  is greedy with respect to  X  v  X  . This bound is widely used and known as Bellman error bound. The following example demonstrates that the Bellman error bound may also be loose for  X  close to 1: Assume that the current value function is the value of a policy with the transition matrix and reward P ,r 1 , while the optimal policy has the transition matrix and reward P 2 ,r 2 . The looseness of the bound is depicted in Figure 4. The approximation error bound scales with 1 (1  X   X  ) 2 , while the true this example. The intuitive reason for the looseness of the bound is that the bound treats each state as recurrent, even when is it transient.
 The global error bound may be also tightened by using a lower discount factor  X  as follows: Finding the discount factor  X  that minimizes this error is difficult, because the function may not be convex or differentiable. Thus the most practical method is a sub-gradient optimization method. The global error bound the MDP example above is depicted in Figure 5. In this section, we propose an alternative explanation for the performance improvement in Tetris that does not rely on the loose approximation error bounds. A specific property of Tetris is that the rewards are not received in every step, i.e. they are sparse. The value function, on the other hand, is approximated in every step. As a result, the return should be less sensitive to the discount factor than the approximation error. Decreasing the discount factor will thus reduce the approximation error more significantly than it increases the discount error. The following assumption formalizes this intuition.
 Assumption 8 (Sparse rewards) . There exists an integer q such that for all m  X  0 and all instantia-tions r i with non-zero probability: P m i =0 r i  X b m/q c and r i  X  X  0 , 1 } .
 and J m = { j t j = 1 , j  X  m } and let I = I  X  and J = J  X  . From the definition, these two sets satisfy that | I m | X | J m | . First we show the following lemma.
 Proof. By induction on m . The base case m = 0 is trivial. For the inductive case, consider the following two cases: 1) r m +1 = 0 . From the inductive assumption, there exists a function that maps I j  X  = max J m +1 . Then if j  X  = m + 1 then the function f : I m  X  J m can be extended by setting exists from the inductive assumption.
 In the following, let R i be the random variable representing the reward received in step i . It is possible to prove that the discount error scales with a coefficient that is lower than in Theorem 2: Theorem 10. Let  X   X   X   X   X  , let k =  X  log(1  X   X  ) / (log(  X  )  X  log(  X   X   X  )) , and let  X  = E h P k Proof. Consider  X  be the optimal policy for the discount factor  X  . Then we have: 0  X  v  X   X  v  X   X  v  X   X  v  X   X  . In the remainder of the proof, we drop the superscript  X  for simplicity, that is v  X  = v not the optimal value function. Intuitively, the proof is based on  X  X oving X  the rewards to earlier steps to obtain a regular rewards structure. A small technical problem with this approach is that moving the rewards that are close to the initial time step decreases the bound. Therefore, we treat these rewards separately within the constant  X  . First, we show that for f ( i )  X  k , we have that  X  i  X   X  i  X   X  f ( i )  X   X  f ( i ) . Let j = f ( i ) = i  X  k , for some k  X  0 . Then: with the maximization used to get a sufficient condition independent of  X  . Since the function f Then we have for j &gt; k : Because the playing board in Tetris is 10 squares wide, and each piece has 4 squares, it takes on average 2.5 moves to remove a line. Since Theorem 10 applies only to integer values of q , we use a Tetris formulation in which dropping each piece requires two steps. A proper Tetris action is taken formulation, we change the discount factor to  X  1 2 . Then the upper bound from Theorem 10 on the constant; it is independent of the new discount factor  X  .
 The sparse rewards property can now be used to motivate the performance increase, even if the approximation error is bounded by / (1  X   X  ) instead of by / (1  X   X  ) 3 (as Lemma 7 suggests). The approximation error bound will not, in most cases, satisfy the sparsity assumption, as the errors are typically distributed almost uniformly over the state space and is received in every step as a decrease in the approximation error.
 The cumulative error bounds derived above predict it is beneficial to reduce the discount factor to  X  when: The effective discount factor  X   X  in Tetris is not known, but consider for example that it is  X   X  = approximate value iteration may be expected to improve using  X   X   X   X   X  .
 We end by empirically illustrating the influence of reward sparsity in a general context. Consider a simple 1-policy, 7-state chain problem. Consider two reward instances, one with a single reward of 1, and the other with randomly generated rewards. We show the comparison of the effects of a lower discount factor of these two examples in Figure 6. The dotted line represents the global error with sparse rewards, and the solid line represents the cumulative error with dense rewards. Sparsity of rewards makes a decrease of the discount factor more interesting. We show in this paper that some common approximation error bounds may be tightened with a lower discount factor. We also identified a class of problems in which a lower discount factor is likely to increase the performance of approximate dynamic programming algorithms. In particular, these are problems in which the rewards are received relatively sparsely. We concentrated on a theoretical analysis of the influence of the discount factor, not on the specific methods which could be used to determine a discount factor. The actual dependence of the performance on the discount factor may be non-trivial, and therefore hard to predict based on simple bounds. Therefore, the most practical approach is to first predict an improving discount factor based on the theoretical predictions, and then use line search to find a discount factor that ensures good performance. This is possible since the discount factor is a single-dimensional variable with a limited range.
 The central point of our analysis is based on bounds that are in general quite loose. An important several settings (problems, approximation architecture). If such errors follow some law, it might be interesting to see whether it helps to tighten the bounds.
 Acknowledgements This work was supported in part by the Air Force Office of Scientific Research Grant
