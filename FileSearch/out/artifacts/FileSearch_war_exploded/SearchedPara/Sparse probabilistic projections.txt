 Principal component analysis (PCA) is widely used for data pre-processing, data compression and dimensionality reduction. However, PCA suffers from the fact that each principal component is a recent years, several methods for sparse PCA have been designed to find projections which retain maximal variance, while enforcing many entries of the projection matrix to be zero [20, 6]. While most of these methods are based on convex or partially convex relaxations of the sparse PCA prob-lem, [16] has looked at using the probabilistic PCA framework of [18] along with ` 1 -regularisation. Canonical correlation analysis (CCA) is also commonly used in the context for dimensionality re-duction.The goal is here to capture features that are common to several views of the same data. Recent attempts for constructing sparse CCA include [10, 19].
 by [13]. We introduce a general probabilistic model, which allows us to infer from an arbitrary number of views of the data, both a shared latent representation and individual low-dimensional representations of each one of them. Hence, the probabilistic reformulations of PCA and CCA fit this probabilistic framework. Moreover, we are interested in sparse solutions, as these are important for interpretation purposes, denoising or feature extraction. We consider a Bayesian approach to the problem. A proper probabilistic approach allows us to treat the trade-off between the modelling accuracy (of the high-dimensional observations by low-dimensional latent variables) and the degree of sparsity of the projection directions in principled way. For example, we do not need to estimate the sparse components successively, using, e.g., deflation, but we can estimate all sparse directions jointly as we are taking the uncertainty of the latent variable into account.
 pendix A, is based on automatic relevance determination (ARD) [14]. No parameter needs to be set in advance. The entries in the projection matrix which are not well determined by the data are automatically driven to zero. The second approach uses priors from the generalised hyperbolic fam-eventually leading to very sparse solutions if desired. For both approaches we derive a variational EM algorithm [15]. Shaded and unshaded nodes are respectively observed and unobserved random variables. Plates indicate repetitions. (b) Marginal prior on the individual matrix entries ( b = 1 ). We consider the graphical model shown in Figure 1(a). For each observation, we have P independent measurements x 1 ,..., x P in different measurement spaces or views . The measurement x p  X  R D p is modelled as a mix of a common (or view independent) continuous latent vector y 0  X  R Q 0 and a view dependent continuous latent vector y p  X  R Q p , such that p .
 We are interested in the case where y 0 and y p are low-dimensional vectors, i.e., Q 0 ,Q p D p for all p . We impose Gaussian priors on the latent vectors: The resulting generative model comprises a number of popular probabilistic projection techniques as PCA [18]. If there are two views, we recover probabilistic CCA [2].
 tries are zero. One way to achieve sparsity is by means of ARD-type priors [14]. In this framework, a zero-mean Gaussian prior is imposed on the entries of the weight matrices: Type II maximum likelihood leads then to a sparse solution when considering independent hyper-parameters. The updates arising in the context of probabilistic projections are given in Appendix A. Since marginalisation with respect to both the latent vectors and the weights is intractable, we apply variational EM [15]. Unfortunately, following this route does not allow us to adjust the degree of sparsity, which is important e.g. for interpretation purposes or for feature extraction. Hence, we seek a more flexible approach. In the remaining of this paper, we will assume that the defined shortly, has the form of an (infinite) weighted sum of scaled Gaussians: We will chose the prior over  X  ij in such a way that the resulting marginal prior over the correspond-ing  X  ij induces sparsity. A similar approach was followed in the context of sparse nonparametric Bayesian regression in [4, 5]. 2.1 Compact reformulation of the generative model Before discussing the approximate inference scheme, we rewrite the model in a more compact way. Let us denote the n th observation, the corresponding latent vector and the means respectively by The generative model can be reformulated as follows: where Note that we do not assume that the latent spaces are correlated as  X  = diag {  X  0 ,  X  1 ,...,  X  P } . This is consistent with the fact the common latent space is modelled independently through y 0 . Subsequently, we will also denote the matrix of the hyperparameters by  X   X  R D  X  Q , where we set (and fix)  X  ij =  X  for all  X  ij = 0 . 2.2 Sparsity inducing prior over the individual scale variables We impose an inverse Gamma prior on the scale variable  X  ij : for all i and j . The shape parameter a and the scale parameter b are non-negative. The marginal prior terms of the modified Bessel function of the third kind K  X  (  X  ) : for  X  ij 6 = 0 , and The function  X (  X  ) is the (complete) Gamma function.
 b is sufficiently large. For a more formal discussion in the context of regression we refer to [7]. It is interesting to note that for a/DQ = 1 we recover the popular Laplace prior, which is equivalent to the ` 1 -regulariser or the LASSO [17], and for a/DQ  X  0 and b  X  0 the resulting prior is fits also into the framework defined by (5). However, it corresponds to imposing a flat prior on the scale variables over the log-scale, which is a limiting case of the Gamma distribution. When imposing independent Gamma priors on the scale variables, the effective joint marginal is a product of Student-t distributions, which again is sharply peaked around zero and sparsity inducing. by EM. In other words, we view the weight matrix  X  as a matrix of parameter and estimate the entries by maximum a posteriori (MAP) learning. The other parameters are estimated by maximum likelihood (ML).
 The variational free energy is given by ential entropy. Since the Kullback-Leibler divergence (KL) is non-negative, the negative free energy is a lower bound to log-marginal likelihood: that the posteriors we will obtain in the E-step are exact.
 The variational EM finds maximum likelihood estimates for the parameters by cycling through the following two steps until convergence: the log-marginal likelihood. The convergence can be checked by monitoring the variational lower bound, which monotonically increases during the optimisation. The explicit expression of the vari-ational bound is here omitted due to a lack of space 3.1 Posterior of the latent vectors are independent. Hence, the posterior of each low-dimenstional latent vector is given by 3.2 Posterior of the scale variables The inverse Gamma distribution is not conjugate to the exponential family. However, the posterior over matrix  X  is tractable. It has the form of a product of generalised inverse Gaussian distributions (see Appendix B for a formal definition): factorised form arises from the scale variable being independent conditioned on the weights. 3.3 Update for the parameters Based on the properties of the Gaussian and the generalised inverse Gaussian, we can compute the variational lower bound, which can then be maximised. This leads to the following updates: where the required expectations are given by Note that diag { X } denotes a block-diagonal operation in (19). More importantly, since we are seek-ing a sparse projection matrix, we do not suffer from the rotational ambiguity problem as is for example the case standard probabilistic PCA. 4.1 Synthetic denoising experiments Because of identifiability issues which are subject of ongoing work, we prefer to compare various methods for sparse PCA in a denoising experiment. That is, we assume that the data were generated from sparse components plus some noise and we compare the various sparse PCA on the denoising uniformly at random M = 4 unit norm sparse vectors in P = 10 dimensions with known number S = 4 of non zero entries, then generate i.i.d. values of the random variables Z from three possible distributions (Gaussian, Laplacian, uniform), then add isotropic noise of relative standard deviation 1 / 2 . When the latent variables are Gaussian, our model exactly matches the data and our method should provide a better fit; however, we consider also situations where the model is misspecified in order to study the robustness of our probabilistic model.
 We consider our two models: SCA-1 (which uses automatic relevance determination type of sparsity priors) and SCA-2 (which uses generalised hyperbolic distribution), where we used 6 latent dimen-sions (larger than 4) and fixed hyperparameters that lead to vague priors. Those two models thus have no free parameters and we compare them to the following methods, which all have two regu-larisation parameters (rank and regularisation): DSPCA [6], the method of Zou [20] and the recent method of [16] which essentially considers a probabilistic PCA with ` 1 -penalty on the weights. In Table 1 we report mean-squared reconstruction error averaged over 10 replications. It can be seen sparse PCA methods, even when the model is misspecified. 4.2 Template attacks Power consumption and electromagnetic radiation are among the most extensively used side-channels for analysing physically observable cryptographic devices. A common belief is that the useful information for attacking a device is hidden at times where the traces (or time series) have large variance. Once the relevant samples have been identified they can be used to construct tem-plates, which can then be used to assess if a device is secure. A simple, yet very powerful approach, recently proposed by [1], is to select time samples based on PCA. Figure 2(a) shows the weight Table 1: Denoising experiment with sparse PCA (we report mean squared errors): (top) Gaussian distributed latent vectors, (middle) latent vectors generated from the uniform distribution, (bottom) latent vectors generated from the Laplace distribution. associated to each time sample by the first three principal directions found by PCA. The problem a threshold manually in order to decide whether the information leakage at time t is relevant or not. Figure 2(b) shows the weight associated to the time samples by SCA-2 when using a Laplace prior (i.e. for a/DQ = 1 ). It can be observed that one gets a much better picture of where the relevant information is. Clearly, sparse probabilitic PCA can be viewed as being more robust to spurious noise and provides a more reliable and amenable solution. matrices. Sparsity was enforced by either imposing an ARD-type prior or by means of the a Normal-Inverse Gamma prior. Although the inverse Gamma is not conjugate to the exponential family, the posterior is tractable as it is a special case of the generalised inverse Gaussian [12], which in turn is a conjugate prior to this family. Future work will include the validation of the method on a wide range of applications and in particular as a feature extracting tool.
 Acknowledgments We are grateful to the PASCAL European network of excellence for partially supporting this work. In this section, we provide the updates for achieving automatic thresholding of projection matrix entries in a probabilistic setting. We apply Tipping X  X  sparse Bayesian theory [8], which is closely related to ARD [14]. More specifically, we assume the prior over the scale variables is uniform over a log-scale, which is a limiting case of the Gamma distribution.
 variational EM. The variational free energy is given by In order to find a tractable solution, we further have to assume that the approximate posterior q has a factorised form. We can then compute the posterior of the low-dimenstional latent vectors: the weights is given by has the same form as in (20). Finally, the updates for the parameters are found by maximising the negative free energy, which corresponds to performing type II ML for the scaling variables. This yields The Generalised inverse Gaussian distribution is in the class of generalised hyperbolic distributions. It is defined as follows [12, 11]: where y &gt; 0 and K  X  (  X  ) is the modified Bessel function of the third kind 1 with index  X  . The following expectations are useful [12]: where R  X  (  X  )  X  K  X  +1 (  X  ) /K  X  (  X  ) . Inverse Gamma distribution When  X  = 0 and  X  &lt; 0 , the generalised inverse Gaussian distribution reduces to the inverse Gamma distribution: It is straightforward to verify this result by posing a =  X   X  and b =  X / 2 , and noting that for  X  &lt; 0 .

