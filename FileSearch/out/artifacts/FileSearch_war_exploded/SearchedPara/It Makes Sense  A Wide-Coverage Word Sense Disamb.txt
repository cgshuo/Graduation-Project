 Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambigu-ous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine transla-tion (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003).
In previous SensEval workshops, the supervised learning approach has proven to be the most suc-cessful WSD approach (Palmer et al., 2001; Sny-der and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on su-pervised learning methods. These systems used a set of knowledge sources drawn from sense-annotated data, and achieved significant improve-ments over the baselines.

However, developing such a system requires much effort. As a result, very few open source WSD systems are publicly available  X  the only other publicly available WSD system that we are aware of is SenseLearner (Mihalcea and Csomai, 2005). Therefore, for applications which employ WSD as a component, researchers can only make use of some baselines or unsupervised methods. An open source supervised WSD system will pro-mote the use of WSD in other applications.
 In this paper, we present an English all-words WSD system, IMS (It Makes Sense), built using a supervised learning approach. IMS is a Java im-plementation, which provides an extensible and flexible platform for researchers interested in us-ing a WSD component. Users can choose differ-ent tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector ma-chines (SVM) as the classifier and integrate mul-tiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification mod-els trained with examples collected from parallel texts, S EM C OR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996).

A previous implementation of the IMS sys-tem, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and fine-grained task, respectively. Our current IMS im-plementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks.

The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation results of IMS on SensE-val/SemEval English tasks. Finally, we conclude in Section 4. In this section, we first outline the IMS system, and introduce the default preprocessing tools, the feature types, and the machine learning method used in our implementation. Then we briefly ex-plain the collection of training data for content words. 2.1 System Architecture Figure 1 shows the system architecture of IMS. The system accepts any input text. For each con-tent word w (noun, verb, adjective, or adverb) in the input text, IMS disambiguates the sense of w and outputs a list of the senses of w , where each sense s likelihood of s sense inventory used is based on WordNet (Miller, 1990) version 1.7.1.

IMS consists of three independent modules: preprocessing, feature and instance extraction, and classification. Knowledge sources are generated from input texts in the preprocessing step. With these knowledge sources, instances together with their features are extracted in the instance and fea-ture extraction step. Then we train one classifica-tion model for each word type. The model will be used to classify test instances of the corresponding word type. 2.1.1 Preprocessing Preprocessing is the step to convert input texts into formatted information. Users can integrate differ-ent tools in this step. These tools are applied on the input texts to extract knowledge sources such as sentence boundaries, part-of-speech tags, etc. The extracted knowledge sources are stored for use in the later steps.

In IMS, preprocessing is carried out in four steps:  X  Detect the sentence boundaries in a raw input  X  Tokenize the split sentences with a tokenizer.  X  Assign POS tags to all tokens with a POS tag- X  Find the lemma form of each token with a
By default, the sentence splitter and POS tag-tence splitting and POS tagging. A Java version of Net (Miller, 1990) thesaurus, is used to find the lemma form of each token. 2.1.2 Feature and Instance Extraction After gathering the formatted information in the preprocessing step, we use an instance extractor together with a list of feature extractors to extract the instances and their associated features.
Previous research has found that combining multiple knowledge sources achieves high WSD accuracy (Ng and Lee, 1996; Lee and Ng, 2002; Decadt et al., 2004). In IMS, we follow Lee and Ng (2002) and combine three knowledge sources  X  POS Tags of Surrounding Words We use  X  Surrounding Words Surrounding words fea- X  Local Collocations We use 11 local collo-
As shown in Figure 1, we implement one fea-ture extractor for each feature type. The IMS soft-ware package is organized in such a way that users can easily specify their own feature set by im-plementing more feature extractors to exploit new features. 2.1.3 Classification In IMS, the classifier trains a model for each word type which has training data during the training process. The instances collected in the previous step are converted to the format expected by the machine learning toolkit in use. Thus, the classifi-cation step is separate from the feature extraction the default classifier of IMS, with a linear kernel and all the parameters set to their default values. Accordingly, we implement an interface to convert the instances into the LIBLINEAR feature vector format.

The utilization of other machine learning soft-ware can be achieved by implementing the corre-sponding module interfaces to them. For instance, IMS provides module interfaces to the WEKA ma-chine learning toolkit (Witten and Frank, 2005),
The trained classification models will be ap-plied to the test instances of the corresponding word types in the testing process. If a test instance word type is not seen during training, we will out-put its predefined default sense, i.e., the WordNet first sense, as the answer. Furthermore, if a word type has neither training data nor predefined de-fault sense, we will output  X  X  X , which stands for the missing sense, as the answer. 2.2 The Training Data Set for All-Words Once we have a supervised WSD system, for the users who only need WSD as a component in their applications, it is also important to provide them the classification models. The performance of a supervised WSD system greatly depends on the size of the sense-annotated training data used. To overcome the lack of sense-annotated train-ing examples, besides the training instances from the widely used sense-annotated corpus S EM C OR (Miller et al., 1994) and DSO corpus (Ng and Lee, 1996), we also follow the approach described in Chan and Ng (2005) to extract more training ex-amples from parallel texts.

The process of extracting training examples from parallel texts is as follows:  X  Collect a set of sentence-aligned parallel  X  Perform tokenization on the English texts  X  Perform Chinese word segmentation on the  X  Perform word alignment on the parallel texts  X  Assign Chinese translations to each sense of  X  Pick the occurrences of w which are aligned  X  Identify the senses of the selected occur-Finally, the English side of these selected occur-rences together with their assigned senses are used as training data.

We only extract training examples from paral-lel texts for the top 60% most frequently occur-ring polysemous content words in Brown Corpus (BC), which includes 730 nouns, 190 verbs, and 326 adjectives. For each of the top 60% nouns and adjectives, we gather a maximum of 1,000 training examples from parallel texts. For each of the top 60% verbs, we extract not more than 500 examples from parallel texts, as well as up to 500 examples from the DSO corpus. We also make use of the sense-annotated examples from S EM C OR as part of our training data for all nouns, verbs, adjectives, and 28 most frequently occurring adverbs in BC. Table 1: Statistics of the word types which have training data for WordNet 1.7.1 sense inventory
The frequencies of word types which we have training instances for WordNet sense inventory version 1.7.1 are listed in Table 1. We generated classification models with the IMS system for over 21,000 word types which we have training data. On average, each word type has 38 training in-stances. The total size of the models is about 200 megabytes. In our experiments, we evaluate our IMS system on SensEval and SemEval tasks, the benchmark data sets for WSD. The evaluation on both lexical-sample and all-words tasks measures the accuracy of our IMS system as well as the quality of the training data we have collected. 3.1 English Lexical-Sample Tasks Table 2: WSD accuracies on SensEval lexical-sample tasks
In SensEval English lexical-sample tasks, both the training and test data sets are provided. A com-mon baseline for lexical-sample task is to select the most frequent sense (MFS) in the training data as the answer.
 We evaluate IMS on the SensEval-2 and SensEval-3 English lexical-sample tasks. Table 2 compares the performance of our system to the top two systems that participated in the above tasks (Yarowsky et al., 2001; Mihalcea and Moldovan, 2001; Mihalcea et al., 2004). Evaluation results show that IMS achieves significantly better accu-racies than the MFS baseline. Comparing to the top participating systems, IMS achieves compara-ble results. 3.2 English All-Words Tasks In SensEval and SemEval English all-words tasks, no training data are provided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in S EM C OR , the Word-Net first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-words tasks.

Using the training data collected with the method described in Section 2.2, we apply our sys-tem on the SensEval-2, SensEval-3, and SemEval-2007 English all-words tasks. Similarly, we also compare the performance of our system to the top two systems that participated in the above tasks (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). The evaluation results are shown in Table 3. IMS easily beats the WNs1 baseline. It ranks first in SensEval-3 English fine-grained all-words task and SemEval-2007 English coarse-grained all-words task, and is also compet-itive in the remaining tasks. It is worth noting that because of the small test data set in SemEval-2007 English fine-grained all-words task, the dif-ferences between IMS and the best participating systems are not statistically significant.

Overall, IMS achieves good WSD accuracies on both all-words and lexical-sample tasks. The per-formance of IMS shows that it is a state-of-the-art WSD system. This paper presents IMS, an English all-words WSD system. The goal of IMS is to provide a flexible platform for supervised WSD, as well as an all-words WSD component with good perfor-mance for other applications.

The framework of IMS allows us to integrate different preprocessing tools to generate knowl-edge sources. Users can implement various fea-ture types and different machine learning methods or toolkits according to their requirements. By default, the IMS system implements three kinds of feature types and uses a linear kernel SVM as the classifier. Our evaluation on English lexical-sample tasks proves the strength of our system. With this system, we also provide a large num-ber of classification models trained with the sense-annotated training examples from S EM C OR , DSO corpus, and 6 parallel corpora, for all content words. Evaluation on English all-words tasks shows that IMS with these models achieves state-of-the-art WSD accuracies compared to the top participating systems.

As a Java-based system, IMS is platform independent. The source code of IMS and the classification models can be found on the homepage: http://nlp.comp.nus.edu. sg/software and are available for research, non-commercial use.
 This research is done for CSIDM Project No. CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) ad-ministered by the Media Development Authority (MDA) of Singapore.

