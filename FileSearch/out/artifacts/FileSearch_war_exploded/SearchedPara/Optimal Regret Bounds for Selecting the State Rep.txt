 Odalric-Ambrym Maillard 1 odalricambrym.maillard@gmail.com Phuong Nguyen nmphuong@cecs.anu.edu.au Ronald Ortner rortner@unileoben.ac.at Daniil Ryabko daniil@ryabko.net In Reinforcement Learning (RL), an agent has to learn a task through interactions with the environment. The standard RL framework models the interaction of the agent and the environment as a finite-state Markov decision process (MDP). Unfortunately, the real world is not (always) a finite-state MDP, and the learner of-ten has to find a suitable state-representation model : a function that maps histories of actions, observations, and rewards provided by the environment into a fi-nite space of states , in such a way that the resulting process on the state space is Markovian, reducing the problem to learning in a finite-state MDP. However, finding such a model is highly non-trivial. One can come up with several representation models, many of which may lead to non-Markovian dynamics. Testing which one has the MDP property one by one may be very costly or even impossible, as testing a statistical hypothesis requires a workable alternative assumption on the environment. This poses a challenging prob-lem: find a generic algorithm that, given several state-representation models only some of which result in an MDP, gets (on average) at least as much reward as an optimal policy for any of the Markovian represen-tations. Here we do not test the MDP property but propose to use models as long as they provide high enough rewards.
 Motivation. One can think of specific scenarios where the setting of several state-representation mod-els is applicable. First, these models can be discreti-sations of a continuous state space. Second, they may be discretisations of the parameter space: this sce-nario has been recently considered (Ortner &amp; Ryabko, 2012) for learning in a continuous-state MDP with Lipschitz continuous rewards and transition probabil-ities where the Lipschitz constants are unknown; the models are discretisations of the parameter space. A simple example is when the process is a second-order Markov process with discrete observations: in this case a model that maps any history to the last two obser-vations is a Markov model; a detailed illustration of such an example can be found, e.g., in Section 4 of (Hutter, 2009). More generally, one can try and ex-tract some high-level discrete features from (continu-ous, high-dimensional) observations provided by the environment. For example, the observation is a video input capturing a game board, different maps attempt to extract the (discrete) state of the game, and we assume that at least one map is correct. Some pop-ular classes of models are context trees (McCallum, 1996), which are used to capture short-term memo-ries, or probabilistic deterministic finite automata (Vi-dal et al., 2005), a very general class of models that can capture both short-term and long-term memories. Since only some of the features may exhibit Markovian dynamics and/or be relevant, we want an algorithm able to exploit whatever is Markovian and relevant for learning. For more details and further examples we refer to (Maillard et al., 2011).
 Previous work. This work falls under the framework of providing performance guarantees on the average reward of a considered algorithm. In this setting, the optimal regret of a learning algorithm in a finite-state MDP is O ( et al., 2010) and Regal.D (Bartlett &amp; Tewari, 2009). Previous work on this problem in the RL literature includes (Kearns &amp; Singh, 2002; Brafman &amp; Tennen-holtz, 2003; Strehl et al., 2006). Moreover, there is currently a big interest in finding practical state rep-resentations for the general RL problem where the environment X  X  states and model are both unknown, e.g. U-trees (McCallum, 1996), MC-AIXI-CTW (Ve-ness et al., 2011),  X MDP (Hutter, 2009), and PSRs (Singh et al., 2004). Another approach in which pos-sible models are known but need not be MDPs was considered in (Ryabko&amp; Hutter, 2008).
 For the problem considered in this paper, (Maillard et al., 2011) recently introduced the BLB algorithm that, given a finite set  X  of state-representation mod-the number of models) in respect to the optimal pol-icy associated with any model that is Markovian. BLB is based on uniform exploration of all representation models and uses the performance guarantees of UCRL2 to control the amount of time spent on non-Markov models. It also makes use of some internal function in order to guess the MDP diameter (Jaksch et al., 2010) of a Markov model, which leads to an additive term in the regret bound that may be exponential in the true diameter, which means the order T 2 / 3 is only valid for possibly very large T .
 Contribution. We propose a new algorithm called OMS (Optimistic Model Selection), that has regret of order p |  X  | T , thus establishing performance that is optimal in terms of T , without suffering from an un-favorable additive term in the bound and without compromising the dependence on |  X  | . This demon-strates that taking into consideration several possibly non-Markovian representation models does not signif-icantly degrade the performance of an algorithm, as compared to knowing in advance which model is the right one. The proposed algorithm is close in spirit to the BLB algorithm. However, instead of uniform ex-ploration it uses the principle of  X  X ptimism X  for model selection, choosing the model promising the best per-formance.
 Outline. Section 2 introduces the setting; Section 3 presents our algorithm OMS ; its performance is anal-ysed in Section 4; proofs are in Sections 5, and Sec-tion 6 concludes. Environment. For each time step t = 1 , 2 ,... , let H t := O X  ( A X R X O ) t  X  1 be the set of histories up to time t , where O is the set of observations, A is a finite set of actions and R = [0 , 1] is the set of possible rewards. We consider the problem of reinforcement learning when the learner interacts sequentially with some unknown environment: first some initial obser-vation h 1 = o 1  X  H 1 = O is provided to the learner, then at any time step t &gt; 0, the learner chooses an action a t  X  A based on the current history h t  X  H t , then receives the immediate reward r t and the next observation o t +1 from the environment. Thus, h t +1 is the concatenation of h t with ( a t ,r t ,o t +1 ). State representation models. Let  X  be a set of state-representation models. A state-representation model  X   X   X  is a function from the set of histories model  X  , the state at step t under  X  is denoted by s t, X  :=  X  ( h t ) or simply s t when  X  is clear from context. For the sake of simplicity, we assume that S  X   X  X   X  0 =  X  for  X  6 =  X  0 . Further, we set S := S  X   X   X  S  X  . A particular role will be played by state-representation models that induce a Markov decision process (MDP) . An MDP is defined as a decision process in which at any discrete time t , given action a t , the probability of immediate reward r t and next observation o t +1 , given the past history h t , only depends on the current obser-vation o t . That is, P ( o t +1 ,r t | h t a t ) = P ( o Observations in this process are called states of the en-vironment. We say that a state-representation model  X  is a Markov model of the environment, if the process ( s t, X  ,a t ,r t ) ,t  X  N is an MDP. This MDP is denoted as M (  X  ). We will always assume that such MDPs are weakly communicating , that is, for each pair of states x 1 ,x 2 there exists k  X  N and a sequence of ac-tions  X  1 ,..., X  k  X  A such that P ( s k +1 , X  = x 2 | s 1 , X  x ,a 1 =  X  1 ,...,a k =  X  k ) &gt; 0. It should be noted that there may be infinitely many state-representation models under which an environment is Markov.
 Problem description. Given a finite set  X  which includes at least one Markov model, we want to con-struct a strategy that performs as well as the algorithm that knows any Markov model  X   X   X , including its re-wards and transition probabilities. For that purpose we define for any Markov model  X   X   X  the regret of any strategy at time T , cf. (Jaksch et al., 2010; Bartlett &amp; Tewari, 2009; Maillard et al., 2011), as where r t are the rewards received when following the proposed strategy and  X  ? (  X  ) is the optimal av-erage reward in  X  , i.e.,  X  ? (  X  ) :=  X  ( M (  X  ) , X  wards received when following the optimal policy  X  ?  X  for  X  . Note that for weakly communicating MDPs the optimal average reward indeed does not depend the expected sum of rewards obtained in T steps (fol-lowing the optimal policy) at the price of an additional O (  X  High-level overview. The OMS algorithm we pro-pose (shown in detail as Algorithm 1) proceeds in j = 1 , 2 ,... . In each run j of some episode k , starting at time t = t k,j , OMS chooses a policy  X  k,j applying the optimism in face of uncertainty principle twice. First, in line 6, OMS considers for each model  X   X   X  a set of admissible MDPs M t, X  (defined via confidence intervals for the estimates so far), and computes a so-called optimistic MDP M + t (  X  )  X  M t, X  and an asso-ciated optimal policy  X  + t (  X  ) on M + t (  X  ) such that the (line 7) OMS chooses the model  X  k,j  X   X  which maxi-by a term intuitively accounting for the  X  X omplex-ity X  of the model, similar to the REGAL algorithm of (Bartlett &amp; Tewari, 2009).
 The policy  X  k,j is then executed until either (i) run j reaches the maximal length of 2 j steps (line 19), Algorithm 1 Optimistic Model Selection ( OMS ) Require: Set of models  X  0 , parameter  X   X  [0 , 1]. 1: Set t := 1, k := 0, and  X  :=  X  0 . 2: while true do 3: k := k + 1, j := 1 , sameEpisode := true 4: while sameEpisode do 6:  X   X   X   X , use EVI to compute optimistic MDP 7: Choose model  X  k,j  X   X  such that 9: sameRun := true . 10: while sameRun do 11: Choose action a t :=  X  k,j ( s t ), get reward r t , 12: Set testFail := true iff the sum of the col-13: if testFail then 14: sameRun := false , sameEpisode := false 15:  X  :=  X  \{  X  k,j } 16: if  X  =  X  then  X  :=  X  0 end if 17: else if v k ( s t ,a t ) = N t k ( s t ,a t ) then 18: sameRun := false , sameEpisode := false 19: else if ` k,j = 2 j then 20: sameRun := false , j := j + 1 21: end if 22: t := t + 1 23: end while 24: end while 25: end while (ii) episode k terminates when the number of visits in some state has been doubled (line 17), or (iii) the executed policy  X  k,j does not give sufficiently high av-erage reward (line 12). Note that OMS assumes each model to be Markov, as long as it performs well. Oth-erwise the model is eliminated (line 15).
 Details. We continue with some details of the algo-rithm. In the following, S  X  := |S  X  | denotes the number of states under model  X  , S := |S| is the total number of states, and A := |A| is the number of actions. Further,  X  :=  X / 36 t 2 is the confidence parameter for time t . Admissible models. First, the set of admissible MDPs M t, X  the algorithm considers at time t for each model  X   X   X  is defined to contain all MDPs with state space S  X  and with rewards r and transition probabili-ties p satisfying where pirical transition probabilities and mean rewards (at time t ) for taking action a at state s , and N t ( s,a ) is the number of times action a has been chosen in state s up to time t . (If a hasn X  X  been chosen in s so far, we set N t ( s,a ) to 1.) It can be shown (cf. Appendix C.1 of Jaksch et al. (2010)) that the mean rewards r and the transition probabilities p of a Markovian state-representation  X  satisfy (3) and (4) at time t for all s  X  S  X  and a  X  A , each with probability at least 1  X   X  t , making Markov models admissible with high probability.
 Extended Value Iteration. For computing a near-optimal policy  X  + t (  X  ) and a corresponding optimistic MDP M + t (  X  )  X  X  t, X  (line 6), OMS applies for each  X   X   X  extended value iteration (EVI) (Jaksch et al., 2010) with precision parameter t  X  1 / 2 . EVI computes opti-mistic approximate state values u + t, X  = ( u + t, X  ( s )) R 1994) with an additional optimization step for choos-ing the transition kernel maximizing the average re-ward. The (approximate) average reward  X  t (  X  ) in M b  X  + t (  X  ) = min n r + t ( s, X  + t (  X ,s )) where r + t and p + t are the rewards and transition prob-(Jaksch et al., 2010) that Penalization term. At time t = t k,j , we define the empirical value span of the optimistic MDP M + t (  X  ) as penalization term considered in (1) for each model  X  is given by where the constants are given by c (  X  ; t ) := 2 q 2 S  X  A log(2 S  X  S  X  At/ X  t ) + 2 q c (  X  ; t ) := 2 q 2 S  X  A log(2 S  X  At/ X  t ) . Deviation from the optimal reward. Let ` k,j := t  X  t k,j + 1, and v k,j ( s,a ) be the total number of times a has been played in s during run j in episode k (or until current time t if j is the current run). Similarly, we write v k ( s,a ) for the respective total number of visits during episode k . (Note that by the assumption S  X   X  X   X  0 =  X  for  X  6 =  X  0 , the state implicitly determines the respective model.) Then for the test (2) that de-cides whether the chosen model  X  k,j gives sufficiently high reward, we define the allowed deviation from the optimal average reward in the optimistic model for any t &gt; t k,j in run j as lob k,j ( t ) := 2 X where sp + k,j := sp ( u + t tuitively, the first two terms correspond to the esti-mation error of the transition kernel and the rewards, while the last one is due to stochasticity of the sam-pling process. We now provide the main result of this paper, an upper bound on the regret of our OMS strategy. The bound in-volves the diameter of a Markov model  X  , D (  X  ), which is defined as the expected minimum time required to reach any state starting from any other state in the MDP M (  X  ) (Jaksch et al., 2010).
 Theorem 1 Let  X  ? be an optimal model, i.e.  X  ?  X  argmax  X  ? (  X  ) |  X   X   X  ,  X  is Markovian . Then the regret  X (  X  ? ,T ) of OMS (with parameter  X  ) w.r.t.  X  ? after any T &gt; SA steps is upper bounded by with probability higher than 1  X   X  , where  X  ? :=  X  ? (  X  S ? := S  X  ? , and D ? := D (  X  ? ) .
 In particular, if for all  X   X   X  , S  X  6 B , then S 6 B |  X  | and hence with high probability Comparison with the BLB algorithm. Compared to the results obtained by (Maillard et al., 2011) the regret bound in Theorem 1 has improved dependence of T 1 / 2 (instead of T 2 / 3 ) with respect to the horizon (up to logarithmic factors). Moreover, the new bound avoids a possibly large constant for guessing the di-ameter of the MDP representation, as unlike BLB , the current algorithm does not need to know the diam-eter. These improvements were possible since unlike BLB (which uses uniform exploration over all models, and applies UCRL2 as a  X  X lack box X ) we employ opti-mistic exploration of the models, and do a more in-depth analysis of the  X  UCRL2 part X  of our algorithm. On the other hand, we lose in lesser parameters: the multiplicative term in the new bound is S ? A S
A p |  X  | B (assuming that all representations induce a model with no more than S  X  6 B states), whereas the corresponding factor in the bound of (Maillard factor state spaces is an interesting question: one may note that the algorithm actually only chooses models not much more complex (in terms of the diameter and the state space) than the best model. However, it is not easy to quantify this in terms of a concrete bound. Another interesting question is how to reuse the in-formation gained on one model for evaluation of the others. Indeed, if we are able to propagate informa-tion to all models, a log( |  X  | ) dependency as opposed to the current p |  X  | seems plausible. However, in the current formulation, a policy can be completely unin-formative for the evaluation of other policies in other models. In general, this heavily depends on the inter-nal structure of the models in  X . If all models induce state spaces that have strictly no point in common, then it seems hard or impossible to improve on p |  X  | . We also note that it is possible to replace the diameter in Theorem 1 with the span of the optimal bias vector just as for the REGAL algorithm (Bartlett &amp; Tewari, 2009) by suitably modifying the OMS algorithm. How-ever, unlike UCRL2 and OMS for which computation of optimistic model and respective (near-)optimal policy can be performed by EVI, this modified algorithm (as REGAL ) relies on finding the solution to a constraint optimization problem, efficient computation of which is still an open problem. The proof of Theorem 1 is divided into two parts. In Section 5.1, we first show that with high probability all Markovian state-representation models will collect sufficiently high reward according to the test in (2). This also means that the regret of any Markov model is not too large. This in turn is used in Section 5.2 to show that also the optimistic model employed by OMS (which is not necessarily Markov) does not lose too much with respect to an optimal policy in an arbitrary Markov model. In our proof we use analysis similar to (Jaksch et al., 2010) and (Bartlett &amp; Tewari, 2009). 5.1. Markov models pass the test in (2) Assume that  X  k,j  X   X  is a Markov model. We are going to show that  X  k,j will pass the test on the collected rewards in (2) of the algorithm at any step t w.h.p. Initial decomposition. First note that at time t when the test is performed, we have P where lected for choosing a in s from time t k,j to the current optimistic rewards of the model M + t icy  X  k,j and P + k,j the respective optimistic transition u tor given by EVI. By (5) and noting that v k,j ( s,a ) = 0 when a 6 =  X  k,j ( s ) or s /  X  X  k,j , we get ` We continue bounding each of the two terms on the right hand side of (8) separately.
 Control of the second term. Writing r ( s,a ) for the mean reward for choosing a in s (this is well-defined, since we assume the model is Markov), we have r The terms of this decomposition are controlled. That is, using that M (  X  k,j ) is an admissible model according to (4) with probability 1  X   X  t of measure concentration in Appendix C.1 of (Jaksch et al., 2010) to the quantity definition of r + k,j ( s,a ), and since N t we deduce that with probability higher than 1  X   X  t On the other hand, using again the results of measure concentration in Appendix C.1 of (Jaksch et al., 2010), and that v k,j ( s,a ) 6 N t union bound over S k,j At k,j events that with probabil-ity higher than 1  X   X  t Control of the first term. For the first term in (8), let us first notice that, since the rows of P + k,j sum to 1,
P + k,j  X  I u + k,j is invariant under a translation of the quantity h + k,j , where Then, we make use of the decomposition where P k,j denotes the transition matrix correspond-ing to the MDP M (  X  k,j ) under policy  X  k,j . Since both matrices are close to the empirical transition ma-trix b P t this expression.
 First part of the first term. Indeed, since sp + k,j = k h k,j k  X  , we have for the first term in (11), using the decomposition p + k,j (  X | s )  X  p k,j (  X | s ) = p b p centration result and the definition of p + k,j , that with probability higher than 1  X   X  t v = X 6 X 6 X Second part of the first term. The second term of (11) can be rewritten using a martingale difference et al., 2010) we set X  X  := p (  X | s  X  ,a  X  )  X  e &gt; s get v Now the sequence { X  X  } t ence sequence with | X  X  | 6 p (  X | s  X  ,a  X  )  X  e &gt; s Thus, an application of Azuma-Hoeffding X  X  inequal-ity (cf. Lemma 10 and its application in Jaksch et al. (2010)) to (13) yields with probability higher than 1  X   X  t (12) this concludes the control of the first term of (8). Putting all steps together. Combining (8), (9), (10), (11), (12), and (14), we deduce that at each time t of run j in episode k , any Markovian model  X  k,j passes the test in (2) with probability higher than 1  X  4  X  t Further, it passes all the tests in run j with probability higher than 1  X  4  X  t 5.2. Regret analysis Next, let us consider a model  X  k,j  X   X , not necessarily Markovian, that has been chosen at time t k,j . Let t + 1 be the time when one of the three stopping conditions in the algorithm (lines 12, 17, and 19) is met. Thus OMS employs the model  X  k,j between t k,j and t + 1, until a new model is chosen after the step t + 1. Noting that r  X   X  [0 , 1] and that the total length of the run is ( t + 1)  X  t k,j + 1 = ` k,j + 1 we can bound the regret  X  k,j of run j in episode k by  X  Since by assumption the test in (2) has been passed for all steps  X   X  [ t k,j ,t ], we have and we continue bounding the terms of lob k,j ( t ). Stopping criterion based on the visit counter. Since P s,a v k,j ( s,a ) = ` k,j 6 2 j , by Cauchy-Schwarz this into the definition (6) of lob k,j , we deduce from (15) that  X  Selection procedure with penalization. Now, by definition of the algorithm, for any optimal Markov model  X  ? defined in the statement of Theorem 1, when-ever M (  X  ? ) is admissible, i.e. M (  X  ? )  X  M t was not eliminated during all runs before run j in episode k , we have  X  k,j  X  pen (  X  k,j ; t k,j ) &gt; lently Noting that ` k,j 6 2 j and recalling that when M (  X  ? ) is admissible, the span of the corresponding optimistic model is less than the diameter of the true model, i.e. sp ( u + t obtain from (16), (17), and a union bound that with probability higher than The sum in (19) comes from the event that  X  ? passes all tests (and is admissible) for all runs in all episodes previous to time t k,j , and 2  X  t that  X  ? is admissible at time t k,j . We conclude in the following by summing  X  k,j over all runs and episodes. Summing over runs and episodes. Let J k be the total number of runs in episode k , and let K T be the total number of episodes up to time T . Noting that c (  X  ? ; t k,j ) 6 c (  X  ? ; T ) and c 0 (  X  ? ; t k,j ) 6 c well as using that 2 t k,j &gt; 2 j (so that 2 j +1 2  X  gives  X (  X  ? ,T ) = with probability higher than 1  X  P K T k =1 P J k j =1 4  X  where we used a union bound over all events considered in (19) for the control of all the  X  k,j terms, avoiding redundant counts (such as the admissibility of  X  ? at time t k,j ). Now, using the definition of  X  t fact that 2 t k,j &gt; 2 j , we get that 4  X  t where the last inequality follows by a series-integral comparison, using that t 7 X  t  X  2 is a decreasing func-tion. Thus, we deduce that the bound (20) is valid and it remains to bound the double sum P k P j 2 j/ 2 . From the number of runs... First note that by definition of the total number of episodes K T we have which implies also that we have the bound
X Further, by Jensen X  X  inequality we get Now, to bound the total number of runs P K T k =1 J k , us-ing Jensen X  X  inequality and (21), we deduce
X and thus it remains to deal with K T . ... to the number of episodes. First recall that an episode is terminated when either the number of visits in some state-action pair ( s,a ) has been doubled (line 17 of the algorithm) or when the test on the accumu-lated rewards has failed (line 12). We know that with probability at least 1  X   X  the optimal Markov model is not eliminated from  X , while non-Markov models fail-ing the test are deleted from  X . Therefore, with prob-ability 1  X   X  the number of episodes terminated with a model failing the test is upper bounded by |  X  | X  1. Next, let us consider the number of episodes which are ended since the number of visits in some state-action pair ( s,a ) has been doubled. Let K ( s,a ) be the number of episodes which ended after the number be the number of steps in these episodes. As it may happen that in an episode the number of vis-its is doubled in more than one state-action pair, we assume that K ( s,a ) and T ( s,a ) count only the episodes/steps where ( s,a ) is the first state-action pair for which this happens. It is easy to see that K ( s,a ) 6 the bound P s  X  X  P a  X  X  log 2 2 T ( s,a ) on the total num-ber of these episodes is maximal under the constraint P ( s,a ). This shows that the total number of episodes K T is upper bounded by with probability 1  X   X  , provided that T &gt; SA . Putting all steps together. Combin-ing (20), (22) and (23) we get  X (  X  ? ,T ) 6  X  2  X  the definition of c , c 0 , the regret of OMS is, with probability higher than 1  X   X  , bounded by  X (  X  ? ,T ) 6  X  ? + D ? SA + |  X  | log 2 2 ( 2 T SA ) + 2 D ? + 2 and we may conclude the proof with some minor sim-plifications. The first natural question about the performance guar-antees obtained is whether they are optimal. We know from the corresponding lower-bounds for learn-ing MDPs (Jaksch et al., 2010) that the dependence on T we get for OMS is indeed optimal. Among other parameters, perhaps the most important one is the number of models |  X  | ; here we conjecture that the p |  X  | dependence we obtain is optimal, but this re-mains to be proven. Other parameters are the size of the action and state spaces for each model; here we lose with respect to the precursor BLB algorithm (see the remark after Theorem 1), and thus have room for improvement. It may be possible to obtain a better dependence for OMS at the expense of more sophisti-cated analysis. Note, however, that so far there are no known algorithms for learning even a single MDP that would have known optimal dependence on all these pa-rameters.
 Another important direction for future research is infi-nite sets  X  of models; perhaps, countably infinite sets is the natural first step, with separable  X  in a suitable sense  X  continuously-parametrized general classes of models being a foreseeable extension. A problem with the latter formulation is that one would need to formal-ize the notion of a model being close to a Markovian model and quantify the resulting regret.
 This work was supported by the French National Re-search Agency (ANR-08-COSI-004 project EXPLO-RA), by the European Community X  X  Seventh Frame-work Programme (FP7/2007-2013) under grant agree-ment 270327 (CompLACS), 216886 (PASCAL2) and 306638 (SUPREL), the Nord-Pas-de-Calais Regional Council and FEDER through CPER 2007-2013, the Austrian Science Fund (FWF): J 3259-N13, the Australian Research Council Discovery Project DP120100950, and NICTA.
 Bartlett, P. L. and Tewari, A. REGAL: A regulariza-tion based algorithm for reinforcement learning in weakly-communicating MDPs. In UAI 2009, Pro-ceedings of the 25th Conference on Uncertainty in Artificial Intelligence , pp. 35 X 42, 2009.
 Brafman, R.I., and Tennenholtz, M. R-max -a gen-eral polynomial time algorithm for near-optimal re-inforcement learning. Journal of Machine Learning Research , 3:213 X 231, 2003.
 Hutter, M. Feature reinforcement learning: Part I. Unstructured MDPs. Journal of General Artificial Intelligence , 1:3 X 24, 2009.
 Jaksch, T., Ortner, R., and Auer, P. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research , 99:1563 X 1600, 2010. Kearns, M., and Singh, S. Near-optimal reinforcement learning in polynomial time. Machine Learning , 49: 209 X 232, 2002.
 Maillard, O., Munos, R., and Ryabko, D. Selecting the state-representation in reinforcement learning. In
Advances in Neural Information Processing Systems 24: 2627 X 2635, 2011.
 McCallum, R. A. Reinforcement Learning with Selec-tive Perception and Hidden State . PhD thesis, De-partment of Computer Science, U. Rochester, 1996. Ortner, R. and Ryabko, D. Online regret bounds for undiscounted continuous reinforcement learning. In
Advances in Neural Information Processing Systems 25: 1772 X 1780, 2012.
 Puterman, M. Markov Decision Processes: Discrete Stochastic Dynamic Programming . Wiley, 1994. Ryabko, D. and Hutter, H. On the possibility of learn-ing in reactive environments with arbitrary depen-dence. Theoretical Computer Science , 405:274 X 284, 2008.
 Singh, S. P., James, M. R., and Rudary, M. R. Predic-tive state representations: A new theory for mod-eling dynamical systems. In UAI  X 04, Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence , pp. 512 X 518, 2004.
 Strehl, A. L., Li, L., Wiewiora, Eric, Langford, J., and
Littman, M. L. PAC model-free reinforcement learn-ing. In Machine Learning, Proceedings of the 23rd
International Conference (ICML 2006) , pp. 881 X  888, 2006.
 Veness, J., Ng, K. S., Hutter, M., Uther, W., and Sil-ver, D. A Monte-Carlo AIXI approximation. Jour-nal of Artificial Intelligence Research , 40(1):95 X 142, 2011.
 Vidal, E., Thollard, F., Higuera, C. D. L., Casacu-berta, F., and Carrasco, R.C. Probabilistic finite-state machines. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 27(7):1013 X 
