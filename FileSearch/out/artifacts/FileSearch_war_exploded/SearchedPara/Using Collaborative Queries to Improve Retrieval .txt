 We describe a preliminary analysis of queries created by 81 users for 4 topics from the TREC Robust Track. Our goal was to explore the potential benefits of using queries created by multiple users on retrieval performance fo r difficult topics. We first examine the overlap in users X  queries and the overlap in results with respect to different queries for the same topic. We then explore the potential benefits of combining users X  queries in various ways. Our results provide some evidence that having access to multiple users X  queries can improve retrieval for individual searchers and for difficult topics. H.1.2 [ Information Storage and Retrieval ]: Models and Principles  X  User/Machine Systems  X  Human factors Performance, Human Factors Collaborative queries, polyrepresentation One of the major goals of the TREC Robust Track was to investigate topics that had perfo rmed poorly in previous TRECs [6]. Participants experimented w ith a variety of techniques; more successful techniques used external sources of information, such as Web search engine results, to expand queries that had been created using the title or description fields from the topic. Although participants were able to improve retrieval performance for some topics, performance for ot her topics remained low. We investigate an alternative appr oach to improving performance of difficult topics which makes use of queries generated by multiple users for the same topics. Belkin, et al. [1] investigated the effectiveness of multiple query representations for ten TREC topics that had been generated by ten expert online searchers and found that a progressive combination of query formulations led to a progressive improvement in results. This result can be explained, in part, by Ingwersen X  X  [2] theory of polyrepresentation in IR, which suggests that obtaining multiple representations of a single information need is a better approach to retrieval than using solitary queries. Our work is also related to the notion of collaborative queries and social search systems [5]. The idea behind these systems is that retrie val for users can be improved by incorporating results from previous users X  searches for similar topics. These systems often r ecommend previously posed queries that are believed to be similar to the current query (presumably because they are about the same topic). This approach leverages the knowledge and experiences of multiple users with similar interests to improve retrieva l performance. We adopt this approach in our investigation of retrieval for difficult topics. The queries that were analyzed in this study were collected in a previous study where 81 users were presented with four topics, posed a single query for each topic and evaluated the relevance of a set of 10 search results [4]. Users were underg raduate students at a large university and were experienced Web searchers. The TREC Robust Track [6] collection was used which consists of a 3GB corpus of newswire text in English, 50 topics and a set of relevance judgments. Although 50 t opics were used in the Robust Track, only 4 were used in this study. This was related to the nature of the previous study and because we did not want to overburden our users. The topics that were used in this study are displayed in Table 1. Users were shown the title, description and narrative fields, but only the topic numbers and titles are displayed in Table 1. This tabl e includes the number of relevant documents in the corpus for each topic, as well as the difficulty of the topic [6]. The difficulty num ber is a rank that indicates how well the 2005 Robust Track participan ts did with the topics. For instance, Topic 374 was the least difficult topic out of the 50 original topics and was ranked  X 1. X  The Lemur Toolkit (http://www.lemurproject.org/), OKAPI BM25, the Libbow stop word list and the Porter stemme r were used for retrieval. 
Topic Title No. Relevant Difficulty 354 Journalist Risks 376 44 374 Nobel Prize Winners 278 1 408 Tropical Storms 183 23 Table 2 characterizes users X  queries. On average, users X  queries were about 3 terms long. Users entered the most unique queries for Topic 354 (i.e., 40 out of 81 queries were unique) and the fewest for topic 374. The number of users who used the title as their query is displayed in column four. Users entered more title queries for Topic 374 than for any other topic. Overall, 26 users used titles-as-queries for all 4 t opics. Column five shows that users X  queries for Topic 408 cont ained the most unique terms. Topic Length Mean (std.) No. Unique Queries No.  X  X itle X  354 2.73 (1.04) 40 34 28 374 3.30 (1.27) 21 56 18 408 2.95 (1.18) 38 31 33 448 3.09 (1.09) 33 29 25 The performance of the unique queries is presented in Table 3. This table presents precision at 20 (rounded down to the nearest .1). Although gmap is the offici al measure for the Robust track, it is a run level statistic, so is not suitable for the topic level analysis here. We choose to compute precision at 20 since it is accepted that users typically only search through the first 1-2 pages of results [3]. In this instance, we are interested in seeing whether having access to multiple queries for the same topic could possibly help users. The values for queries submitted for Topic 354 ranged from .00 to .65. Th e range of values was much smaller for Topic 374 and concentrated around the highest values, while the reverse was true for Topic 448. Table 3. Frequency of precision for unique queries per topic The average performances for each topic were .243, .486, .211 and .115, respectively. Although the performance m easures were not the same, the easiest topic according to the ranking in Table 1 performed the best in our study, while the most difficult topic performed the second best in our study. Our users X  queries were the least effective for Topic 448, even though it only represented a mid-range of difficultly. Finally, the performances of the title queries were about average fo r Topic 354 and 374 (.30 and .55), but were quite poor for Topi cs 408 and 448 (.10 and .00). Overall, it appears that having access to previous queries could help some users  X  the potential improvement gain for each topic was: +.65, +.40, +.65 and +.40, respectively. We combined unique terms from user s X  queries in various ways to see if different combinations could improve performance. To create queries for this comparison, we progressively combined the most frequently occurring terms in users X  queries for each topic. Query 1 was the term occurring the most frequently in users X  queries, while Query 2 was the combination of this term and the term occurring the second most frequently. We also created a  X  X uper X  query for each topic which contained each unique query term for that topic (e.g., 354 X  X  super query contained 28 terms). Figure 1 presents the average precision for each query, for each topic. Only queries contai ning the seven most frequently occurring terms for each topic are included. This figure also includes the median and best runs from the Robust Track [6]. In no case did progressive user queries outperform the best performing TREC run, but the performance of Query 3 for Topic 354 was very close to the best TREC run. For these queries, there is no apparent relationship between number of terms and performance; queries of different lengths performed the best for different topics. Super queries never performed best, although for Topic 448, for which users X  queri es were generally unsuccessful, the lengthier queries did perform better. For Topics 354 and 448, queries constructed from the most frequently used query terms performed much better than the TREC median. The goal of this study was to explore the potential benefits of using queries created by multiple users on retrieval performance for difficult topics. Our results provide some evidence that collaborative user queries can im prove retrieval for individual searchers and for difficult topics. Unique user queries generated a range of precision values which suggests that query recommendations could help those who initially pose unsuccessful queries. In this study a progressive combination of the most frequently occurring terms in users X  queries did not outperform the best runs from TREC. In some cases the progressive queries performed better than the TREC median, but there was no consistent pattern. Finally, this study only examined four topics, which limits the ge neralizability of the results. Currently, we are analyzing a second dataset collected from another experiment that contai ns queries for all 50 topics, although there are fewer queries per topic (5-15 as opposed to 81). [1] Belkin, N. J., Cool, C., Croft, W. B., &amp; Callan, J. P. (1993). [2] Ingwersen, P. (1996). Cognitive perspectives of information [3] Joachims, T., Granka, L., Pan, B., Hembrooke, H., &amp; Gay, [4] Kelly, D., Fu, X., &amp; Shah, C. (2007). Effects of rank and [5] Smyth, B., Balfe, E., Freyne, J., Briggs, P., Coyle, M., &amp; [6] Voorhees, E. M. (2006). Overview of the TREC 2005 
