 Processing large volumes of streaming data in near-real-time is becoming increasingly important as the Internet, sensor networks and network traffic grow. Online machine learning is a typical means of dealing with streaming data, since it allows the classification model to learn one instance of data at a time. Although many online learning methods have been developed since the development of the Perceptron al-gorithm, existing online methods assume that the number of classes is available in advance of classification process. However, this assumption is unrealistic for large scale or streaming data sets. This work proposes an online Chinese restaurant process (CRP) algorithm, which is an online and nonparametric algorithm, to tackle this problem. This work proposes a relaxing function as part of the prior and up-dates the parameters with the likelihood function in terms of the consistency between the true label information and predicted result. This work presents two Gibbs sampling al-gorithms to perform posterior inference. In the experiments, the online CRP is applied to three massive data sets, and compared with several online learning and batch learning al-gorithms. One of the data sets is obtained from Wikipedia, which comprises approximately two million documents. The experimental results reveal that the proposed online CRP performs well and efficiently on massive data sets. Finally, this work proposes two methods to update the hyperparam-eter  X  of the online CRP. The first method is based on the posterior distribution of  X  , and the second exploits the prop-erty of online learning, namely adapting to change, to adjust  X  dynamically.
 I.2.6 [ Artificial Intelligence ]: Learning X  Parameter Learn-ing ; F.1.2 [ Theory of Computation ]: Modes of Compu-tation X  Online computation Algorithms, Experimentation, Theory Adaptive Learning; Chinese Restaurant Process; Nonpara-metric; Online Learning
Online learning has attracted a significant amount of in-terest in the field of machine learning, and an important family of efficient and scalable machine learning algorithms exist for large-scale applications. Like massive data sets, streaming data sources are commonly important, particu-larly when data sets are generated in real time, as are many log files, sensor data, and network data. The distributions that underlie the streaming data typically change over time, so the predictive model should be adapted accordingly. One important property of online learning is that the true label of the instance is available after the prediction is made. The true label information can then be used to refine the predic-tive model, so that subsequent predictions will be closer to the true labels.

Although many online learning algorithms have been de-veloped over the past few decades, most online learning al-gorithms require the number of classes of the clasification model to be determined in advance of information process-ing. However, this requirement is unreasonable in real world applications, since the number of classes is usually unknown for massive data sets. Additionally, many application set-tings require models to be able to deal with a nonexhaustive training data set, from which some classes may be missing and therefore absent from the training data set. Detect-ing new classes from streaming or massive data sets is an important issue in the era of big data, motivating the de-velopment herein of a new algorithm to solve this problem. For example, the biosensing problem requires rapid identifi-cation of new, emerging classes of microorganisms, which are not represented in the initial training library. Detecting the sudden presence of a new class is an important element of an automated outbreak-identification strategy [11]. E-mail classification is another typical example of online learning. An e-mail system that can identify new classes that are not present in the existing class set and suggest possible labels could constitute an extension of an existing e-mail classifi-cation system.
Bayesian nonparametric (BNP) models provide an alter-n ative means of solving this problem; their complexity in-creases with the number of observed data instances. BNP models can incorporate prior knowledge into the model by placing a prior on an unbounded number of parameters. The Dirichlet process (DP), introduced by Ferguson [14], is a stochastic process that is frequently used as a prior in Bayesian nonparametric statistics. The idea of using a Dirichlet process as the prior of the mixing proportions of a simple distribution was first introduced by Antoniak [2]. The DP is an elegant alternative to parametric model selec-tion using the Dirichlet process mixture model, which com-prises a countably infinite number of components [2, 31, 28]. The Chinese restaurant process (CRP) [1] mixture model is one of the representations of the DP mixture model. The CRP induces an exchangeable distribution over partitions, so that the joint distribution is invariant to the order in which observations are assigned to clusters, and each obser-vation can be treated as the last in a sequence of observa-tions. This property yields a very useful representation in the inferences of DP mixture models [36, 28].

This work develops a nonparametric online learning algo-rithm that is called the online Chinese restaurant process. The proposed algorithm retains the ability of nonparametric models to automatically infer an adequate model complex-ity from data without determining the number of classes in advance. The online CRP is also an online learning algo-rithm, indicating that the online CRP is adapted as more data instances are observed. This work proposes a relax-ing function to represent part of the prior and updates the parameters according to the likelihood function in terms of the consistency between the true label information and the predicted result. The online CRP is influenced by the true label information, which is used to adjust the parameters of the model, so the online CRP relaxes the assumption of ex-changeability used by CRP. Unlike existing online learning algorithms, the online CRP is a nonparametric method, so its model complexity is determined by data. Additionally, the proposed algorithm allows the creation of new classes, and so is more practical and flexible model when applied to massive data sets. In the experiments, three massive data sets are used to assess the online CRP and compare it with several online learning and batch learning algorithms. One of the data sets is obtained from Wikipedia, and includes ap-proximately two million articles. The experimental results demonstrate that the online CRP works well and efficiently on the massive data sets.

The main contribution of this paper is to develop a non-parametric online learning algorithm. To the best of our knowledge, this is the first work that combines Bayesian nonparametric learning with online learning, allowing the classification model to grow with the data rather than re-quiring that the number of classes be determined in advance of the classification process. This work presents two Gibbs sampling algorithms for making posterior inferences. One of these algorithms is a collapsed Gibbs sampling algorithm, which is more efficient than the other and is used in the ex-periments. This work proposes two methods for updating hyperparameter  X  of the online CRP; the first one is based on the posterior distribution of  X  , while the second exploits the property of online learning, namely adapting to change, to adjust  X  dynamically.

The rest of this paper is organized as follows. Section 2 then reviews the Chinese restaurant process, and introduces online Chinese restaurant process algorithms. Next, Section 3 summarizes the results of several experiments. Section 4 discusses the experimental results and analyzes the online Chinese restaurant process. Section 5 draws conclusions.
The goal of clustering is to identify latent information in data, such that objects in the same cluster are more simi-lar to each other than objects from different clusters. The finite mixture model (FMM) is commonly used in cluster-ing applications. The FMM assumes the existence of finite models, each of which is associated with a parameter. Each data point is generated by one of the mixture models. In-ferring the parameters associated with the models and iden-tifying which model generated each data point yields the clustering of the data points. However, the FMM must de-termine the number of models in advance of the clustering. The usual trade off in model order selection problems arises: with too many components, the mixed models may overfit the data, while a mixture with too few components may not be sufficiently flexible to approximate the true underlying model [15]. The Bayesian nonparametric mixture model, which is called a Chinese restaurant process mixture or a Dirichlet process mixture, infers the number of clusters from the data and allows the number of clusters to grow as new data instances are observed [17].
This section presents the notation that will be used through-out this study. The notation is based on the metaphor of the CRP, which involves customers, tables and dishes. Cus-tomer x i corresponds to a data point, while the table is the cluster in the CRP or a class in the online CRP. Accordingly, the customers who sit at the same table correspond to data points in the same class. Meanwhile, the dish at table j is the parameter of the class j , denoted as  X  j , and the num-ber of customers who sit at table j is m j . The preference of customer x i for the dish served at table j is H ( x i which is the likelihood that x i belongs to class j . Unlike the CRP, the online CRP allows each customer to move to another table after he has first been assigned to a table. The assigned table and the final table at which customer x i sits are z i and y i , respectively.

The online CRP is a nonparametric method: the number of possible tables is infinite. In this work, the number of occupied tables is denoted as k . The base distribution and the concentration parameter are G o and  X  , respectively. Fi-nally, this work proposes a relaxing function g (  X  1 ,  X  to adjust the prior in the online CRP, in which  X  1 and  X  are regret rates, and f j and e j are used to track the misas-signment of the previous i  X  1 customers on table j . Finally, x  X  i is introduced to represent all of the customers except for customer i , and x  X  i,c represents all of the customers in class c except for customer i . The terms y  X  i and z  X  i similar meanings.
The Chinese restaurant process (CRP) [1], a discrete-time stochastic process, defines a distribution over partitions that embodies the assumed prior distribution over cluster struc-t ures [30]. The CRP receives the name from the following metaphor. Imagine a Chinese restaurant with an infinite number of tables each with infinite capacity, and a sequence of n customers who enter the restaurant and sit down. The first customer enters the restaurant and sits at the first ta-ble. The i th subsequent customer sits at an occupied table, or at the next unoccupied table as follows.

The CRP induces an exchangeable distribution over par-titions, so that the joint distribution is invariant in the order in which observations are assigned to clusters. Hence, each customer X  X  table assignment z i can be made by pretending that x i is the last person to sit down. As given by Equa-tion (1), the i th customer sits at an occupied table j with a probability that is proportional to the number of already seated customers m j , or sits at a new table with a proba-bility that is proportional to  X  . The tables are analogous to clusters, and customers to observations or data points. A larger m j is more likely to grow. Therefore, the CRP exhibits a clustering property due to a rich-gets-richer phe-nomenon. Additionally, as a prior on the number of tables, the CRP is a nonparametric model, meaning that the num-ber of occupied tables grows as more customers enter the restaurant.

The exact computation of posterior expectations for a DP mixture model is infeasible when the data include just a few observations. Markov chain methods enable the systematic computation of the posterior distribution of the parame-ters [12, 36, 26, 13, 28, 20]. Several Markov chain methods for sampling from distribution of a DP mixture model have been presented [28]. Markov chain Monte Carlo (MCMC) sampling methods can be slow to converge and their con-vergence can be difficult to diagnose. Variational inference is an alternative, giving deterministic approach to approxi-mate likelihoods and posteriors [6, 35].

The CRP offers great flexibility for clustering applica-tions, and has motivated many extensions. For example, the nested Chinese restaurant process (nCrp) [5, 4] extends the CRP to a hierarchy of partitions, allowing arbitrarily large branching factors. While exchangeability is commonly considered to be an advantageous property, much of the data in text, image and audio domains are not exchangeable. The distance-dependent Chinese restaurant process (dd-CRP) [3] relaxes the assumption of exchangeability, and provides a better fit to sequential data and network data. Socher et al. [33] further uses spectral clustering [32, 29] to reduce di-mensionality and cluster data using the dd-CRP on spectral space.
The metaphor of the online CRP is an extension of that of the CRP. The first customer sits at the first table, as in the CRP. When a subsequent customer enters the restau-rant, the table is assigned differently from that in the CRP. In the online CRP, a waiter tracks the dishes at all the ta-bles, and assigns the customer to a appropriate table based on the waiter X  X  prior knowledge of the dishes and the cus-tomer X  X  preference for dishes. However, the customer may be dissatisfied with the table assignment, and move to another table. The restaurant will be in chaos, if the customers move too frequently. To reduce the probability of misassignment, the waiter tracks the movements of customers to adjust the probabilities associated with the assignment of the next new customer to the various tables, and adjusts table dish infor-mation to avoid the misassignment. It is noted that table assignments for i th customer comprise two results, one is assigned by waiter, denoted as z i , and the other one is the final sitting table, represented as y i . For table j and cus-tomer i , f j and e j track the misassignment of the previous i  X  1 customers as shown in Equation (2), where 1I is an indicator function.
For table j , f j &gt; 0 indicates that the assignment to ta-ble j should be increased, since customers tend to move to table j . Conversely, the assignment to table j should be re-duced if e j &gt; 0, since the assignments to table j may cause the customers to move to other tables. Inspired by regret theory [25], this work proposes a relaxing function, Equa-tion (3), where  X  1 and  X  2 are regret rates, and the informa-tion about misassignment that is carried by f j and e j are viewed as prior knowledge in determining the distribution of table assignment probabilities.

Equation (4) presents the posterior distribution estima-tion of online CRP, in which the prior comprises the relaxing function and the prior of the CRP, and H ( x i ,  X  j ) denotes the likelihood that datum x i is a member of class j . Notably, the online CRP reduces to the CRP when f j = 0 and e j = 0 for all j . Although the metaphor of the online CRP is simi-lar to that of the CRP, several differences exist between the two processes. First, the CRP is an unsupervised learning method, and it is usually used in clustering applications. Conversely, the online CRP is an online algorithm, in which the label information y i is available after the prediction of x i is made. Second, the prior of the online CRP differs from that of the CRP. Third, the movement of the datum x i between classes simultaneously alters the parameters of the classes that are indexed as z i and y i in the online CRP.
Figure 1 shows the graphical model of the online CRP, in which the observed random variables include the datum x i and its corresponding label y i . The sampling process is as follows: Figure 1: Graphical Model of Online Chinese Restaurant P rocess
Equation (5) specifies that z i is a class indicator variable, referring to the predicted class of datum x i . For each da-tum x i , the predicted label z i can be sampled using the online CRP. The class parameter  X  z i is drawn from the base distribution, G 0 , and each data point x i is generated by a distribution F associated with parameter  X  z i . The proposed algorithm is an online algorithm, so the correct label infor-mation y i becomes available after the prediction is made. The model uses z i and y i to update class parameters.
Unlike most online learning algorithms, such as Percep-tron and Winnow [24], the online CRP is not a mistake-driven model, but updates parameters whenever correct or incorrect predictions have been made. If the prediction is in-correct, z i 6 = y i , then the likelihood that datum x i to class z i is overestimated. To adjust the estimate, a pseudo data point x  X  i =  X  b  X  x i is assumed to join class z i b &gt; 0 is a constant that specifies the weighting of the pseudo data point. Then, the model samples a new class parameter  X  i with the posterior probability of  X  z i that is conditional on x  X  i , x  X  i,z i ,  X  , G 0 and all of the pseudo data points that are associate with table z i . Conversely, the likelihood that datum x i belongs to class y i is underestimated, so a pseudo data point x  X  X  i = b  X  x i rather than x i is assumed to join class y . Similarly, the model samples a new class parameter  X  y i with the posterior probability of  X  y i that is conditional on x , x  X  i,y i ,  X  , G 0 and all of the pseudo data points that are associate with table y i . Conversely, if the prediction is cor-rect ( z i = y i ), then the model samples a new class parameter  X  i using the posterior probability of  X  z i that is conditional on x i , x  X  i,z i ,  X  , G 0 and all of the pseudo data points that are associated with table z i .

The graphical model that is presented in Figure 1 reveals that random variables z and  X  are latent variables. In this work, Gibbs sampling is conducted to carry out posterior inference. The Gibbs sampling involves iterations that al-ternatively draw samples from one of the variables while the others are kept fixed. Accordingly, the conditional poste-rior distributions of these variables must be derived. The conditional posterior distribution of  X  j is given above. The sampling of z i is based on the following equation, in which P ( z i = j | z  X  i , y i ,  X  ) is the prior, and P ( x i hood that  X  j generates datum x i : P ( z i = j | z  X  i , y  X  i , x i ,  X  j ,  X , G 0 )  X  P ( z
This study uses the prior of online CRP to represent P ( z j | z Hence, Equation (6) becomes Equation (4) in the online CRP. Algorithm 1 is the online CRP algorithm. Notably, the true label information is used to adjust model parame-ters, so the online CRP is not exchangeable.
 Algorithm 1: O nline Chinese Restaurant Process
Input : The dispersion parameter  X  and base
Initialize m s , f s , and e s as 0 for all s  X  N k  X  0 b  X  1 for i  X  1 to  X  do 5 G et a data x i 6 if k = 0 then 8 e lse 9 S ample z i from P ( z i = j | z  X  i , x i ,  X , G 0 ,  X  )  X  10 Get label information y i 11 if z i = y i then 12 S ample a new  X  z i from the posterior of  X  z i 15 S ample a new  X  z i from the posterior of  X  z i 16 Sample a new  X  y i from the posterior of  X  y i end
T he marginalization of some variables from a joint dis-tribution always reduces the variance, consistent with the Rao-Blackwell Theorem [21]. In a conjugate context, we can integrate analytically over  X  j , eliminating  X  j from the algo-rithm to simplify the sampling process. Then, for each data point x i , only z i has to be sampled using Equation (7), in which F (  X  j )  X  i is the posterior probability of  X  j conditional on x  X  i,j , the pseudo data points in table j and G 0 . Algorithm 2: C ollapsed Online Chinese Restaurant Process
Input : The dispersion parameter  X  and base
Initialize m s , f s , and e s as 0 for all s  X  N k  X  0 b  X  1 for i  X  1 to  X  do 5 G et a data x i 6 if k = 0 then 8 e lse 9 S ample z i from the distribution as listed in 10 Get label information y i 11 if z i = y i then 12 U pdate the sufficient statistics of table z i on 15 U pdate the sufficient statistics of table z i on 16 Update the sufficient statistics of table y i on end Algorithm 2 shows the collapsed sampling for the online C RP, and Algorithm 2 is used in the experiments herein. The proposed algorithm is an online algorithm, so the algo-rithm processes datum x i sequentially, as in Line 5. The first datum is assigned to the first class, as in Line 7. For subse-quent data, z i are sampled using Equation (7), as in Line 9. Since the number of classes is k , the sampling process must be conducted k times to estimate the posterior probabilities for all the k classes. The model should consider the proba-bility of assigning x i to a new class, labeled as k + 1 in the algorithm. The model selects the most likely class from the k + 1 classes, which is given by z i in the algorithm. When the predicted class is available, the model receives the true label information y i . The model compares z i with y i and updates parameters based on the results of the comparison. As in Line 12, the model updates the sufficient statistics of class z i with the added x i and increases the number of data within class z i by one when z i = y i . However, classes z y update the sufficient statistics of classes z i and y i with the addition of a pseudo data point when z i 6 = y i . Besides sufficient statistics, the algorithm updates the information about the number of data and misassignment. The above processes are found in Lines 15-19. Additionally, this work develops two methods for updating hyperparameter  X  of the online CRP. The first is based on the posterior distribution of  X  , while the second exploits the property of online learn-ing, namely adapting to change, to adjust  X  dynamically. They are described in the Appendix. In the experiments herein, the second method is used to adjust  X  dynamically with an initial value of one.
The implementation of the proposed online CRP assumes that the text of a document follows a multinomial distribu-tion, and that the parameters of the multinomial follow a Dirichlet distribution, such that the conjugate prior can be used to perform collapsed sampling as in Algorithm 2. In this work, three data sets are used to assess system perfor-mance and several methods are compared with the proposed algorithm.
The 20 Newsgroups, RCV1, and Wikipedia are all popu-lar data sets that are commonly used in text analysis exper-iments.
In the preprocessing stage, the stop words are removed from the data sets, since they fail to provide sufficient in-formation to be useful in the clustering task. Punctuation marks are removed and all English letters are converted to lower case. Finally, stemming is applied to the words.
C lassification F 1 metric is commonly used to assess the performance of text classification applications. However, the use of classification F 1 assumes that the number of classes is fixed. The proposed algorithm is a nonparametric method in which the number of classes is unbounded. Therefore, the classification F 1 metric could not be used in the experiments herein. The generated classes are compared using the clus-tering F 1 metric [27]. The clustering F 1 metric considers both precision and recall, which are computed over pairs of documents whose label assignments either agree or disagree. Similarly, one can obtain true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN) in terms of the clustering conditions of the pairs. Then, precision, recall and clustering F 1 metric can be obtained from the confusion matrix. Not only clustering F 1 metric, but also error rates are used as evaluation measures. The execution time is also recorded.
The experiments herein involve massive data sets, so clas-sification takes a relatively long time. Different approaches are used in the experiments on the 20 Newsgroups data set and the other two data sets. The 20 Newsgroups data set comprises approximately 20,000 documents, and five-fold cross-validation is used on this data set. Experimental re-sults are presented as average and standard deviation, and the mean plus or minus two standard deviations is presented in the tables. The RCV1 and Wikipedia data sets contain numerous documents, so uniform random sampling is used to select training data and testing data from these. The experiments on the two data sets are only conducted once, since each experiment takes a long time. All the methods are implemented using Matlab. The online CRP involves a parameter b that specifies the weighting of the pseudo data point, and its value is one in the experiments.

Hoi et al. [18] developed a library for online learning algo-rithms that is called LIBOL, which comprises a large family of recently developed state-of-the-art online learning algo-rithms for large-scale online classification tasks. The LIBOL provides binary and multi-class classification algorithms. The experiments involve multi-class data sets, so multi-class clas-sification algorithms are considered in the comparison of methods. Among these multi-class algorithms, the first-order algorithms include the online gradient descent (OGD) [38] algorithm, passive aggressive (PA) [8] algorithms, and the relaxed online maximum margin algorithm (ROMMA) [23]. The second-order algorithms include adaptive regularization of weight vectors (AROW) [10] algorithm, multi-class con-fidence weighted (CW) algorithms [9], and soft confidence-weighted learning (SCW) [19] algorithms. The LIBOL fur-ther includes variants of some of these algorithms, all of which are used in the experiments. Crammer et al. [8] de-veloped three variants of the passive-aggressive algorithms -PA, PA-I and PA-II. The LIBOL includes the multi-class ag-gressive ROMMA algorithm called aROMMA. Finally, the LIBOL includes two variants of SCW called SCW-I and SCW-II, respectively.

Besides LIBOL, this work uses additional methods in the experiments, including several variants of Perceptron and online logistic regression [37]. Perceptron is a typical online learning algorithm, and Freund and Schapire [16] proposed several variants of online Perceptron algorithms, including voted Perceptron, average Perceptron, last Perceptron and random Perceptron. This work uses the first three variants in the experiments. The Perceptron algorithm is well known to be able to be employed in a nonlinear way by means of the kernel trick, which depends on only the dot products be-tween the vectors in feature space, and selects the mapping such that these high-dimensional dot products can be com-puted in the original space by means of a kernel function. The kernelized algorithms of the variants of Perceptron are applied to the data sets. Although these variants of Percep-tron belong to batch learning algorithms, they can process data sequentially in the training phase, explaining why they are used in the experiments.

The algorithms that are implemented in LIBOL belong to online learning, while the variants of Perceptron belong to batch learning. Therefore, the experiments herein are conducted using online learning and batch learning settings. The online learning setting concerns the algorithms in the LIBOL, since these do not have a training phase, and they evaluate system performances from when they receive the first data point until they receive the last data point. They are online learning algorithms, so the models that are used in the algorithms can be continually adapted. In contrast, the algorithms of the variants of Perceptron comprise a training phase and a prediction phase, so the batch learning set-ting uses supervised learning approach to evaluate system performances, in which the algorithms adapt their models during the training phase, and use the trained model to pre-dict testing data without any further change to the model. The proposed online CRP can use online learning and batch learning settings, and we compare online CRP with the com-parison algorithms with the two settings.

The first data set to be used in the experiments is the 20 Newsgroups data set, which includes 20 categories and approximately 20,000 documents. Table 1 presents the ex-perimental results obtained using the online learning setting, and Table 2 lists the experimental results obtained using the batch learning setting. All of the algorithms complete classi-fication of the 20 Newsgroups data set. The second data set to be used is RCV1, which is massive. In a practical applica-tion setting, classification performance and execution time must both be considered. The methods in the LIBOL fail to complete the task owing to insufficient memory problem, so they are absent from the experimental results, and the batch learning setting is used in the experiments to com-pare the proposed online CRP with variants of Perceptron algorithms. As indicated in Table 2, the online CRP, the variants of Perceptron and their kernelized extensions can complete the classification in reasonable time. However, the kernelized extensions require the computation of the kernel matrix, which encounters the memory problem with RCV1 and Wikipedia data sets. Additionally, the execution time of the variants of Perceptron are all much longer than the proposed algorithm on the RCV1 and Wikipedia data sets. Therefore, the remaining experiments were performed using the online CRP and last Perceptron. Table 3 and Table 4 present the experimental results obtained using the RCV1 and Wikipedia data sets, respectively. Table 4 presents only the error rate, since clustering F 1 metric depends on pair-wise comparisons between all of the classification results. The number of documents in Wikipedia is approximately two millions, yielding a large number of pairwise combina-tions.
T able 1 reveals that the online CRP performs similar to some of the second-order algorithms in the LIBOL. In the LIBOL, the second-order online learning methods such as CW, SCW-I and SCW-II perform well on the 20 News-groups data set, but their execution times greatly exceed those of first-order methods. For the high-dimensional data sets, the calculation of covariance matrix is highly complex and therefore difficult. Table 2 and Table 4 show that the online CRP outperforms the variants of Perceptron on 20 Newsgroups and Wikipedia data sets.
 Last Perceptron outperforms the proposed method on the RCV1 data set as shown in Table 3, but its execution time is much longer. In the experiments on the compared meth-ods that involve binary classifiers, such as Perceptron vari-ants, the one-against-all technique is used to extend these methods to multi-class scenarios. Therefore, the number of classes should be available when a k -class problem is decom-posed into a series of two-class problems. In contrast, the proposed online CRP grows to accommodate the complexity of the data without requiring the number of classes in ad-vance, providing flexibility in processing massive or stream-ing data sets. To evaluate the proposed algorithm when used on a streaming data set, the proposed algorithm is applied to a sorted 20 Newsgroups data set, in which the documents are sorted by timestamps. The experimental results are al-most the same as those presented in Table 2.

The experiments involve balanced and imbalanced data sets, and the experimental results demonstrate that the pro-posed method can perform well on the two kinds of data sets. The experimental results also indicate that the proposed method can complete classification within reasonable time, even though the Wikipedia data set comprises two million documents. The proposed method is analyzed, compared with supervised learning methods, and the performance is assessed with different numbers of training documents. Fi-nally, the settings of the parameters are discussed. The proposed online CRP is an online learning algorithm. The prediction depends on the posterior probabilities of all the existing classes and a new class, so the model need re-tain sufficient statistics only for these classes. When the p rediction of a data point is completed, the model updates the required sufficient statistics rather than undergoing re-training. Thus, the online CRP can process the stream-ing data in almost real-time. Additionally, the model is memory-efficient, since only the model and a single data point have to be retained in memory.

The sampling process that is listed in Line 9 of Algo-rithm 2 shows that the computational cost of each sampling update is proportional to the number of classes, k . The value of k approaches  X  log( n ) asymptotically as n approaches infinity [2, 34]. Meanwhile, the worst case for the model to update parameters is z i 6 = y i , since the model should update the parameters of class z i and y i . The time com-plexity for updating model parameters is a constant, since the update involves only the sufficient statistics. Therefore, the total time complexity for processing n data points is O ( k  X  n ) = O (  X n log( n )). In most cases, the time complex-ity approaches O ( n ) since n  X  k . The experimental results are consistent with the analysis.
The proposed method is compared with supervised learn-ing methods on the three data sets using SVM, logistic re-gression, and Naive Bayes methods. Table 5 presents the experimental results on the 20 Newsgroups. Experiments on SVM are conducted using libsvm [7] with linear kernel. The above supervised learning methods require that model parameters to be determined in classifying documents, so a subset of the data is used as a validation set to determine ap-propriate values of parameters. Table 6 and Table 7 present the experimental results concerning RCV1 and Wikipedia data sets, respectively.

The experimental results reveal that SVM generally out-performs other methods, but it fails to classify the Wikipedia data set, owing to the computation of the kernel matrix. The proposed online CRP outperforms Naive Bayes, and is comparable to logistic regression. The above three super-vised learning methods require that the number of classes is known in advance of classification process. Although many differences exist between the proposed online CRP and the CRP, the proposed online CRP and the CRP share an im-portant property: both are nonparametric methods. The number of classes in the online CRP is determined by the data, and provides a practical and flexible model for dealing with massive data sets.
Experiments were conducted herein to determine whether the online CRP benefits from more training examples. Al-though the proposed method is an online learning algorithm, the online CRP can be very easily converted into a super-vised learning algorithm. We can use the online CRP to process available labeled examples sequentially, yielding a classification model. The classification model can then be used to classify the testing examples sequentially, and the classification performance can be evaluated.

In the experiments, 15,000 documents were randomly se-lected from the RCV1 data set as testing documents. To analyze further the impact of the number of training doc-uments on classification performance, various numbers of labeled examples are used to train the classification model. Figure 2 presents the experimental results, in which the test-ing error rates for various numbers of training documents are recorded. The experimental results indicate that the testing error generally declines as the number of training documents increases.
This section conducts experiments on 20 Newsgroups and evaluates performance with various settings of the parame-ters  X  1 ,  X  2 , b , and  X  . First experiments with various values of  X  1 and  X  2 are conducted and the performance is evaluated. The settings are simplified by setting  X  1 equal to  X  2 . When their values are less than 0 . 5, clustering F 1 metric is around 0 . 66. In contrast, increasing  X  1 and  X  2 significantly degrades performance. A small value of regret rate was preferred in the experiments. Second, experiments were performed us-ing various values of b , which specifies the weighting of the pseudo data point. The experimental results indicate that when b is ten, the best F 1 value, 0 . 6949, is obtained. If b is larger than 100 or less than 0 . 1, the performance will be poor. The above evaluation suggests the setting of param-eter b . Finally, the technique that is mentioned in the Ap-pendix is used to enable the model to automatically deter-mine  X  from the data. Experiments with two initial values, 1 and 1000, are conducted. The above experimental settings can evaluate whether the proposed scheme can automati-cally adjust  X  to maintain performance. The experimental results indicate that their performance is almost the same as that in Table 2. Therefore, even if the initial value of  X  is unreasonable, the proposed dynamic setting scheme can adjust  X  according to the data.
This work develops a nonparametric online learning al-gorithm that adjusts the parameters and complexity of the model as more data instances are observed. The online CRP is more flexible in dealing with massive and streaming data sets than are traditional online learning algorithms. This work proposes a new prior and model parameter updating mechanism that are based on the property of online learn-ing. The experimental results indicate that the online CRP performs well and efficiently on massive data sets. A simple mechanism for converting the online CRP into a supervised learning algorithm is also proposed, and experiments are performed with various numbers of training examples. The e xperimental results indicate that the online CRP benefits from more training examples, but the performance gain is small when the number of training examples exceeds a spe-cific threshold. This finding gives a direction for the future work: the online CRP with the stochastic gradient descent (SGD) trick can be used to deal with massive data sets when training time is an important issue. Central to the proposed method is nonparametric online learning approach, which we argue provides a more flexible and realistic means of dealing with streaming data sets. The nonparametric models make weaker assumptions and let the data  X  X peak for themselves X , and online learning allows the model to adapt to observed data.
This work was sponsored by Ministry of Economic Af-fairs, Taiwan, R.O.C. through project No. D352B23100 con-ducted by Industrial Technology Research Institute (ITRI). [1] D. Aldous. Exchangeability and related topics. In [2] C. E. Antoniak. Mixtures of dirichlet processes with [3] D. M. Blei and P. I. Frazier. Distance dependent [4] D. M. Blei, T. L. Griffiths, and M. I. Jordan. The [5] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. [6] D. M. Blei and M. I. Jordan. Variational inference for [7] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [8] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, [9] K. Crammer, M. Dredze, and A. Kulesza. Multi-class [10] K. Crammer, A. Kulesza, and M. Dredze. Adaptive [11] M. Dundar, F. Akova, A. Qi, and B. Rajwa. Bayesian [12] M. D. Escobar. Estimating normal means with a [13] M. D. Escobar and M. West. Bayesian density [14] T. S. Ferguson. A bayesian analysis of some [15] M. A. T. Figueiredo and A. K. Jain. Unsupervised [16] Y. Freund and R. E. Schapire. Large margin [17] S. J. Gershman and D. M. Blei. A tutorial on [18] S. C. Hoi, J. Wang, and P. Zhao. LIBOL: A Library [19] S. C. H. Hoi, J. Wang, and P. Zhao. Exact soft [20] H. Ishwaran and L. F. James. Gibbs sampling methods [21] S. M. Kay. Fundamentals of statistical signal [22] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [23] Y. Li and P. M. Long. The Relaxed Online Maximum [24] N. Littlestone. Learning quickly when irrelevant [25] G. Loomes and R. Sugden. Regret theory: An [26] S. N. MacEachern. Estimating normal means with a [27] C. D. Manning, P. Raghavan, and H. Schtze.
 [28] R. M. Neal. Markov chain sampling methods for [29] A. Y. Ng, M. I. Jordan, and Y. Weiss. On Spectral [30] J. Pitman. Combinatorial stochastic processes. [31] C. E. Rasmussen. The infinite gaussian mixture [32] J. Shi and J. Malik. Normalized Cuts and Image [33] R. Socher, A. L. Maas, and C. D. Manning. Spectral [34] E. B. Sudderth. Graphical models for visual object [35] M. J. Wainwright and M. I. Jordan. Graphical models, [36] M. West, P. M  X  uller, and M. D. Escobar. Hierarchical [37] F. Zhdanov and V. Vovk. Competitive online [38] M. Zinkevich. Online convex programming and
As in the CRP mixture, the concentration parameter  X  can be sampled from the posterior probability conditional on z as indicated in Equation (8). Equation (9) gives the likelihood that  X  generates z , where n is the number of data joining to table z i , m z i represents the number of people who sit at the table z i when x i joins it. If the prior distribution of  X  is available, we can sample  X  using Equation (8) and the observed data points.
 P ( z |  X  ) =
This work proposes to use a dynamic approach that is adapting to change to determine the value of  X  . The value of  X  specifies the probability of creating a new class. If the class prediction of x i is a new class, but the true class of x i is in one of the existing ones, the value of  X  should be reduced. In contrast, the value of  X  should be increased if the predicted class is in one of the existing classes, but the true class label is new. This approach is used in the experiments to adjust  X  dynamically.
