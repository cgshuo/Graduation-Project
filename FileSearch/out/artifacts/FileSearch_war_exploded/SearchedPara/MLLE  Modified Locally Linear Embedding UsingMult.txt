 The problem of nonlinear dimensionality reduction is to find the meaningful low-dimensional struc-ture hidden in high dimensional data. Recently, there have been advances in developing effective and efficient algorithms to perform nonlinear dimension reduction which include isometric mapping Isomap [7], locally linear embedding (LLE) [5] and its variations, manifold charting [2], Hessian LLE [1] and local tangent space alignment (LTSA) [9]. All these algorithms cover two common steps: learn the local geometry around each data point and nonlinearly map the high dimensional of these algorithms, however, are different both in learning local information and in constructing global embedding, though each of them solves an eigenvalue problem eventually. The effectiveness of the local geometry retrieved determines the efficiency of the methods.
 of each neighborhood in LLE [5]. LLE has many applications such as image classification, image straightforward implementation, and global optimization [6, 11]. It is however also reported that LLE may be not stable and may produce distorted embedding if the manifold dimension is larger tion weights is not well-determined, since the constrained least squares (LS) problem involved for determining the local weights may be ill-conditioned. A Tikhonov regularization is generally used for the ill conditions LS problem. However, a regularized solution may be not a good approximation to the exact solution if the regularization parameter is not suitably selected.
 geometric structure determined by multiple weight vectors is much stable and hence can be used to improve the standard LLE. The modified LLE named as MLLE uses multiple weight vectors for each point in reconstruction of lower dimensional embedding. It can stably retrieve the ideal isometric embedding approximately for an isometric manifold. MLLE has properties similar to LTSA both in measuring linear dependence of neighborhood and in constructing the (sparse) matrix whose smallest eigenvectors form the wanted lower dimensional embedding. It exploits the tight relations between LLE/MLLE and LTSA. Numerical examples given in this paper show the improvement and efficiency of MLLE. combination weights are determined by solving the constrained least squares problem { x combination properties totally, The low dimensional embedding T constructed by LLE tightly depends on the local weights. To G i =[ ...,x j error as x i  X  j  X  J given by w i = y i / 1 T k formulate the solution using the singular value decomposition (SVD) of G i .
 Theorem 2.1 Let G be a given matrix of k column vectors. Denote by y 0 the orthogonal projection of 1 is an optimal solution to min 1 T solve the regularized linear system replaced Figure 2: A 2 D data set (  X  -points) and computed coordinates (dot points) by LLE using different sets of optimal weight vectors (left two panels) or regularization weight vectors (right panel). first and then turns to the limit value w  X  = y 0 1 T y (dotted line) with different values of y 0 for the swiss-roll data. The left two panels show the metaphase phenomenon clearly, where y 0  X  0 . Therefore, w  X  can not be well approximated by w (  X  ) if  X  is not small enough. This partially explains the instability of LLE. weight vectors exist in that case. Figure 2 shows a small example of N =20 two-dimensional points two sets of exact optimal weight vectors and one set of weight vectors that solve the regularized set X and the computed sets within optimal affine transformation are large in the example. approximately optimal weight vectors w ( ) using the matrix V of left singular vectors correspond-ing to the s smallest singular values and bounds the combination errors Gw ( ) in terms of the minimum of Gw and the largest one of the s smallest singular values.
 Theorem 2.2 Let G  X  R m  X  k and  X  1 ( G )  X  ...  X   X  k ( G ) be the singular values of G . Denote where V is the eigenvector matrix of G corresponding to the s smallest right singular values,  X  = The Householder matrix is symmetric and orthogonal. It is given by H = I  X  2 hh T with vector h  X  X  s defined as follows. Let h 0 =  X  1 Note that w  X  can be very large when G is approximately singular. In that case, (1  X   X  ) w  X  domi-choice of  X  . We show an estimation of the condition number cond( W ) for the modified W below. Theorem 2.3 Let W =(1  X   X  ) w (  X  ) 1 T s + VH . Then cond( W )  X  (1 + than a single one. Though the exact optimal weight vector may be unique, multiple approximately optimal weight vectors exist by Theorem 2.2. We will use these weight vectors to determine an improved and more stable embedding. Below we show the details of the modified locally linear embedding using multiple local weight vectors.
 later.) Let w (1) i ,...,w ( s i ) i be s i  X  k linearly independent weight vectors, responding to the s i smallest right singular values,  X  i = 1  X  s Householder matrix that satisfies H i V T i 1 k i =  X  i 1 s i .
 with the constraint TT T = I . Denote by W i =(1  X   X  i ) w i (  X  ) 1 T s and let  X  W i  X  X  N  X  s i be the embedded matrix of W i into the N -dimensional space such that The cost function (3.5) can be rewritten as d eigenvectors of  X  corresponding to the 2nd to d +1 st smallest eigenvalues. 3.1 Determination of number s i of approximation optimal weight vectors data points are sampled from a d -dimensional manifold and the neighbor set is well selected, then  X  best choice. However because of noise and that the neighborhood is possibly not well selected,  X  between the number of weight vectors and the approximation to G i w  X  i . We suggest for a given  X &lt; 1 that is a threshold error. Here d can be over estimated to be d &gt;d . Obviously, s i depends on the parameter  X  monotonically. The smaller  X  is, the smaller s i is, and of course, the smaller the combination errors for the weight vectors used are. We use an adaptive as  X   X  1  X  ...  X   X   X  N . Then we set  X  to be the middle term of {  X  i } ,  X  =  X   X  curvatures and the neighbors are well selected,  X  i is smaller than  X  and s i = k  X  d . For those are used in constructing the local linear structures and the combination errors decrease. We summarize the Modified Locally linear Embedding (MLLE) algorithm as follows.

Algorithm MLLE (Modified Locally linear Embedding). The computational cost of MLLE is almost the same as that of LLE. The additional flops of MLLE Note that the most computationally expensive steps in both LLE and MLLE are the neighborhood selection and the computation of the d +1 eigenvectors of the alignment matrix  X  corresponding to cost of MLLE is ignorable. Consider the application of MLLE on an isometric manifold M = f ( X ) with open set  X   X  X  d and smooth function f . Assume that { x i } are sampled from M , x i = f (  X  i ) ,i =1 ,...,N .Wehave So we have that x i  X  j  X  J w i ,wehave x i  X  For the orthogonalized U of T  X  , i.e., T  X  = LU and UU T = I , since L = T  X  U T  X  X  d  X  d ,wehave that  X  d ( L )=  X  d ( T  X  ) and E ( U )  X  E ( T  X  ) / X  2 d ( T  X  ) . Note that  X  2 k So E ( U ) is always small and approximately achieves the minimum. Roughly speaking, MLLE can retrieve the isometric embedding. MLLE has similar properties similar to those of LTSA. In this section, we compare MLLE and LTSA in the linear dependence of neighbors and alignment matrices. For simplicity, we assume that r = d , i.e., k i  X  d weight vectors are used in MLLE for each neighbor set. 5.1 Linear dependence of neighbors.
 The total combination error of x i can be a measure of the linear dependence of the neighborhood N i . To compare it with the measure of linear dependence defined by LTSA, we denote by  X  x i = 1 | I In LTSA, the linear dependence of N i is measured by the total errors singular values. The MLLE-measure M LLE and the LTSA-measure LT SA of neighborhood linear dependence are similar, 5.2 Alignment matrices.
 Both MLLE and LTSA minimize a trace function of an alignment matrix  X  to obtain an embedding, min TT T = I trace( T  X  T T ) . The alignment matrix can be written in the same form discussion about distance of subspaces.) Theorem 5.1 Let G i =[  X  X  X  ,x j  X  x i ,  X  X  X  ] j  X  J i . Then dist(  X  W i ,  X  V i )  X  G i W i In this section, we present several numerical examples to illustrate the performance of MLLE algo-rithm. The test data sets include simulated date sets and real world examples.
 First, we compare Isomap, LLE, LTSA, and MLLE on the Swiss roll with a hole. The data points generated from a rectangle with a missing rectangle strip punched out of the center and then the resulting Swiss roll is not convex. We run these four algorithms with k =10 . In the top middle of and a warp on the rest of the embedding. As seen in the top right of Figure 3, there is a strong distortion on the computed coordinates by LLE. As we have shown in the bottom of Figure 3, LTSA and MLLE perform well.
 We now compare MLLE and LTSA for a 2 D manifold with 3 peaks embedded in 3 D space. We the interval [  X  1 . 5 , 1 . 5] and h ( t, s ) is defined by Figure 3: Left column: Swiss-roll data and generating coordinates with a missing rectangle. Middle column: computed results by Isomap and LTSA. Right column: results of LLE and MLLE. Figure 4: Left column:Plots of the 3 -peak data and the generating coordinates. Right column: Results of LTSA and MLLE.
 J ( t, s ) is orthonormal approximately. In the right of Figure 4, we plot the computed coordinates by LTSA and MLLE with k =12 . The deformations of the computed coordinates by LTSA near the peaks are prominent because the curvature of the 3-peak manifold varies very much. This bias can be reduced by the modified curvature model of LTSA proposed in [8]. MLLE can recover the generating parameter perfectly up to an affine transformation.
 Next, we consider a data set containing N = 4400 handwritten digits ( X 2 X - X 5 X ) with 1100 examples m = 256 dimensional vectors 2 . The data points are mapped into a 2-dimensional space using LLE and MLLE respectively. These experiments are shown in Figure 5. It is clear that MLLE performs respectively) are well clustered in the resulting embedding of MLLE.
 Finally, we consider application of MLLE and LLE on the real data set of 698 face images with variations of two pose parameters (left-right and up-down) and one lighting parameter. The image size is 64-by-64 pixel, and each image is converted to an m = 4096 dimensional vector. We apply MLLE with k =14 and d =3 on the data set. The first two coordinates of MLLE are plotted in coordinates, and display the corresponding images along each path. These components appear to capture well the pose and lighting variations in a continuous way.
 [1] D. Donoho and C. Grimes. Hessian Eigenmaps: new tools for nonlinear dimensionality reduc-Figure 5: Embedding results of N = 4400 handwritten digits by LLE(left) and MLLE(right). Figure 6: Images of faces mapped into the embedding described by the first two coordinates of MLLE, using the parameters k =14 and d =3 . [2] M. Brand. Charting a manifold. Advances in Neural Information Processing Systems , 15, MIT [3] Jihun Ham, Daniel D. Lee, Sebastian Mika, Bernhard Scholkopf. A kernel view of the dimen-[4] G. H. Golub and C. F Van Loan. Matrix Computations . Johns Hopkins University Press, [5] S. Roweis and L Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-[6] L. Saul and S. Roweis. Think globally, fit locally: unsupervised learning of nonlinear mani-[7] J Tenenbaum, V. De Silva and J. Langford. A global geometric framework for nonlinear [8] J. Wang, Z. Zhang and H. Zha. Adaptive Manifold Learning. Advances in Neural Information [9] Z. Zhang and H. Zha. Principal Manifolds and Nonlinear Dimensionality Reduction via Tan-[10] H. Zha and Z. Zhang. Spectral Analysis of Alignment in Manifold Learning. Submitted, 2006. [11] M. Vlachos, C. Domeniconi, D. Gunopulos, G. Kollios, and N. Koudas Non-Linear Di-
