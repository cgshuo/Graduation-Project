 Hugo Larochelle larocheh@iro.umontreal.ca Yoshua Bengio bengioy@iro.umontreal.ca Restricted Boltzmann Machines (RBMs) (Smolensky, 1986) are generative models based on latent (usually binary) variables to model an input distribution, and have seen their applicability grow to a large variety of problems and settings in the past few years. From binary inputs, they have been extended to model var-ious types of input distributions (Welling et al., 2005; Hinton et al., 2006). Conditional versions of RBMs have also been developed for collaborative filtering (Salakhutdinov et al., 2007) and to model motion cap-ture data (Taylor et al., 2006) and video sequences (Sutskever &amp; Hinton, 2007).
 RBMs have been particularly successful in classifica-tion problems either as feature extractors for text and image data (Gehler et al., 2006) or as a good initial training phase for deep neural network classifiers (Hin-ton, 2007). However, in both cases, the RBMs are merely the first step of another learning algorithm, ei-ther providing a preprocessing of the data or an initial-ization for the parameters of a neural network. When trained in an unsupervised fashion, RBMs provide no guarantees that the features implemented by their hid-den layer will ultimately be useful for the supervised task that needs to be solved. More practically, model selection can also become problematic, as we need to explore jointly the space of hyper-parameters of both the RBM (size of the hidden layer, learning rate, num-ber of training iterations) and the supervised learning algorithm that is fed the learned features. In partic-ular, having two separate learning phases (feature ex-traction, followed by classifier training) can be prob-lematic in an online learning setting.
 In this paper, we argue that RBMs can be used suc-cessfully as stand-alone non-linear classifiers along-side other standard classifiers like neural networks and Support Vector Machines, and not only as fea-ture extractors. We investigate training objectives for RBMs that are more appropriate for training clas-sifiers than the common generative objective. We describe Discriminative Restricted Boltzmann Ma-chines (DRBMs), i.e. RBMs that are trained more specifically to be good classification models, and Hy-brid Discriminative Restricted Boltzmann Machines (HDRBMs) which explore the space between discrim-inative and generative learning and can combine their advantages. We also demonstrate that RBMs can be successfully adapted to the common semi-supervised learning setting (Chapelle et al., 2006) for classifica-tion problems. Finally, the algorithms investigated in this paper are well suited for online learning on large datasets. Restricted Boltzmann Machines are undirected gener-ative models that use a layer of hidden variables to model a distribution over visible variables. Though they are most often trained to only model the inputs of a classification task, they can also model the joint distribution of the inputs and associated target classes (e.g. in the last layer of a Deep Belief Network in Hin-ton et al. (2006)). In this section, we will focus on such joint models.
 We assume given a training set D train = { ( x i , y i ) } , comprising for the i -th example an input vector x i and a target class y i  X  { 1 , . . . , C } . To train a generative model on such data we consider minimization of the negative log-likelihood An RBM with n hidden units is a parametric model of the joint distribution between a layer of hidden variables (referred to as neurons or features) h = ( h 1 , . . . , h n ) and the observed variables made of x = ( x 1 , . . . , x d ) and y , that takes the form where E ( y, x , h ) =  X  h T Wx  X  b T x  X  c T h  X  d T ~y  X  h T with parameters  X  = ( W , b , c , d , U ) and ~y = (1 Figure 2. For now, we consider for simplicity binary input variables, but the model can be easily gener-alized to non-binary categories, integer-valued, and continuous-valued inputs (Welling et al., 2005; Hinton et al., 2006). It is straightforward to show that where sigm is the logistic sigmoid. Equations 2 and 3 illustrate that the hidden units are meant to capture predictive information about the input vector as well as the target class. p ( h | y, x ) also has a similar form: When the number of hidden variables is fixed, an RBM can be considered a parametric model, but when it is allowed to vary with the data, it becomes a non-parametric model. In particular, Freund and Haus-sler (1994); Le Roux and Bengio (2008) showed that an RBM with enough hidden units can represent any distribution over binary vectors, and that adding hid-den units guarantees that a better likelihood can be achieved, unless the generated distribution already equals the training distribution.
 In order to minimize the negative log-likelihood (eq. 1), we would like an estimator of its gradient with respect to the model parameters. The exact gradient, for any parameter  X   X   X  can be written as follows: Though the first expectation is tractable, the second one is not. Fortunately, there exists a good stochastic approximation of this gradient, called the contrastive divergence gradient (Hinton, 2002). This approxima-tion replaces the expectation by a sample generated after a limited number of Gibbs sampling iterations, with the sampler X  X  initial state for the visible variables initialized at the training sample ( y i , x i ). Even when using only one Gibbs sampling iteration, contrastive divergence has been shown to produce only a small bias for a large speed-up in training time (Carreira-Perpi  X nan &amp; Hinton, 2005).
 Online training of an RBM thus consists in cy-cling through the training examples and updating the RBM X  X  parameters according to Algorithm 1, where the learning rate is controlled by  X  .
 Computing p ( y, x ) is intractable, but it is possible to compute p ( y | x ), sample from it, or choose the most probable class under this model. As shown in Salakhutdinov et al. (2007), for reasonable numbers of classes C (over which we must sum), this conditional distribution can be computed exactly and efficiently, by writing it as follows: p ( y | x ) = Algorithm 1 Training update for RBM over ( y, x ) using Contrastive Divergence Precomputing the terms c j + P i W ji x i and reusing for all classes y  X  permits to compute this conditional distribution in time O ( nd + nC ). In a classification setting, one is ultimately only inter-ested in correct classification, not necessarily to have a good p ( x ). Because our model X  X  p ( x ) can be in-appropriate, it can then be advantageous to optimize directly p ( y | x ) instead of p ( y, x ): We refer to RBMs trained according to L disc as Dis-criminative RBMs (DRBMs). Since RBMs (with enough hidden units) are universal approximators for binary inputs, it follows also that DRBMs are uni-versal approximators of conditional distributions with binary inputs.
 A DRBM can be trained by contrastive divergence, as has been done in conditional RBMs (Taylor et al., 2006), but since p ( y | x ) can be computed exactly, we can compute the exact gradient:  X  log p ( y i | x i ) where o yj ( x ) = c j + P k W jk x k + U jy . This gradient can be computed efficiently and then used in a stochas-tic gradient descent optimization. This discriminative approach has been used previously for fine-tuning the top RBM of a Deep Belief Network (Hinton, 2007). The advantage brought by discriminative training usu-ally depends on the amount of available training data. Smaller training sets tend to favor generative learn-ing and bigger ones favor discriminative learning (Ng &amp; Jordan, 2001). However, instead of solely rely-ing on one or the other perspective, one can adopt a hybrid discriminative/generative approach simply by combining the respective training criteria. Though this method cannot be interpreted as a maximum like-lihood approach for a particular generative model as in Lasserre et al. (2006), it proved useful here and elsewhere (Bouchard &amp; Triggs, 2004). In this paper, we used the following criterion:
L where the weight  X  of the generative criterion can be optimized (e.g., based on the validation set classifica-tion error). Here, the generative criterion can also be seen as a data-dependent regularizer for a DRBM. We will refer to RBMs trained using the criterion of equa-tion 5 as Hybrid DRBMs (HDRBMs).
 To train an HDRBM, we can use stochastic gradient descent and add for each example the gradient contri-bution due to L disc with  X  times the stochastic gradi-ent estimator associated with L gen for that example. A frequent classification setting is where there are few labeled training data but many unlabeled examples of inputs. Semi-supervised learning algorithms (Chapelle et al., 2006) address this situation by using the un-labeled data to introduce constraints on the trained model. For example, for purely discriminative models, these constraints are often imposed on the decision sur-face of the model. In the RBM framework, a natural constraint is to ask that the model be a good gener-ative model of the unlabeled data, which corresponds to the following objective: amples of inputs. To train on this objective, we can once again use a contrastive divergence approximation of the log-likelihood gradient: The contrastive divergence approximation is slightly different here. The first term can be computed in time O ( Cn + nd ), by noticing that it is equal to .
 One could either average the usual RBM gradient
E ( y i , x i , h ) for each class y (weighted by p ( y | x sample a y from p ( y | x i ) and only collect the gradient for that value of y . In the sampling version, the online training update for this objective can be described by replacing the statement y 0  X  y i with y 0  X  p ( y | x i ) in Algorithm 1. We used this version in our experiments. In order to perform semi-supervised learning, we can weight and combine the objective of equation 6 with those of equations 1, 4 or 5 where TYPE  X  { gen, disc, hybrid } . Online training according to this objective simply consists in apply-ing the appropriate update for each training example, based on whether it is labeled or not. As mentioned earlier, RBMs (sometimes also referred to as harmoniums (Welling et al., 2005)) have already been used successfully in the past to extract useful fea-tures for another supervised learning algorithm. One of the main contributions of this paper lies in the demonstration that RBMs can be used on their own without relying on another learning algorithm, and provide a self-contained framework for deriving com-petitive classifiers. In addition to ensuring that the features learned by the RBM X  X  hidden layer are dis-criminative, this approach facilitates model selection since the discriminative power of the hidden layer units (or features) can be tracked during learning by observ-ing the progression of classification error on a valida-tion set. It also makes it easier to tackle online learning problems relatively to approaches where learning fea-tures (hidden representation) and learning to classify are done in two separate phases (Hinton et al., 2006; Bengio et al., 2007).
 Gehler et al. (2006); Xing et al. (2005) have shown that the features learned by an RBM trained by ig-noring the labeled targets can be useful for retriev-ing documents or classifying images of objects. How-ever, in both these cases, the extracted features were linear in the input, were not trained discriminatively and had to be fed to another learning algorithm which ultimately performed classification. McCallum et al. (2006) presented Multi-Conditional Learning (MCL) 1 for harmoniums in order to introduce a discriminative component to harmoniums X  training, but the learned features still had to be fed to another learning algo-rithm.
 RBMs can also provide a good initialization for the pa-rameters of neural network classifiers (Hinton, 2007), however model selection issues arise, for instance when considering the appropriate number of learning up-dates and the magnitude of learning rates of each training phase. It has also been argued that the gen-erative learning aspect of RBM training was a key ele-ment to their success as good starting points for neural network training (Bengio et al., 2007), but the extent to which the final solution for the parameters of the neural network is influenced by generative learning is not well controlled. HDRBMs can be seen as a way of addressing this issue.
 Finally, though semi-supervised learning was never reported for RBMs before, Druck et al. (2007) in-troduced semi-supervised learning in hybrid genera-tive/discriminative models using a similar approach to the one presented in section 5. However, they worked with log-linear models, whereas the RBMs used here can perform non-linear classification. Log-linear mod-els depend much more on the discriminative quality of the features that are fed as input, whereas an RBM can learn useful features using their hidden variables, at the price of non-convex optimization. We present experiments on two classification problems: character recognition and text classification. In all ex-periments, we performed model selection on a valida-tion set before testing. For the different RBM models, model selection 2 consisted in finding good values for the learning rate  X  , the size of the hidden layer n and good weights for the different types of learning (gener-ative and semi-supervised weights). Also, the number of iterations over the training set was determined using early stopping according to the validation set classifi-cation error, with a look ahead of 15 iterations. 7.1. Character Recognition We evaluated the different RBM models on the prob-lem of classifying images of digits. The images were taken from the MNIST dataset, where we separated the original training set into training and validation sets of 50000 and 10000 examples and used the stan-dard test set of 10000 examples. The results are given in Table 1. The ordinary RBM model is trained generatively (to model ( x, y )), whereas RBM+NNet is an unsupervised RBM used to initialize a one-hidden layer supervised neural net (as in (Bengio et al., 2007)). We give as a comparison the results of a Gaus-sian kernel SVM and of a regular neural network (ran-dom initialization, one hidden layer, hyperbolic tan-gent hidden activation functions).
 First, we observe that a DRBM outperforms a genera-tive RBM. However, an HDRBM appears able to make the best out of discriminative and generative learning and outperforms the other models.
 We also experimented with a sparse version of the HDRBM model, since sparsity is known to be a good characteristic for features of images. Sparse RBMs were developed by Lee et al. (2008) in the context of deep neural networks. To introduce sparsity in the hidden layer of an RBM in Lee et al. (2008), after each iteration through the whole training set, the biases c in the hidden layer are set to a value that maintains the average of the conditional expected value of these neurons to an arbitrarily small value. This procedure tends to make the biases negative and large. We fol-low a different approach by simply subtracting a small constant  X  value, considered as an hyper-parameter 3 , from the biases after each update, which is more ap-propriate in an online setting or for large datasets. This sparse version of HDRBMs outperforms all the other RBM models, and yields significantly lower clas-Model Error RBM+NNet 1.41% SVM 1.40%
NNet 1.93% sification error then the SVM and the standard neural network classifiers. The performance achieved by the sparse HDRBM is particularly impressive when com-pared to reported performances for Deep Belief Net-works (1.25% in Hinton et al. (2006)) or of a deep neural network initialized using RBMs (around 1.2% in Bengio et al. (2007) and Hinton (2007)) for the MNIST dataset with 50000 training examples.
 The discriminative power of the HDRBM can be better understood by looking a the rows of the weight matrix W , which act as filter features. Figure 2 displays some of these learned filters. Some of them are spatially localized stroke detectors which can possibly be active for a wide variety of digit images, and others are much more specific to a particular shape of digit. 7.2. Document Classification We also evaluated the RBM models on the problem of classifying documents into their corresponding news-group topic. We used a version of the 20-newsgroup dataset 4 for which the training and test sets contain documents collected at different times, a setting that is more reflective of a practical application. The orig-inal training set was divided into a smaller training set and a validation set, with 9578 and 1691 examples respectively. The test set contains 7505 examples. We used the 5000 most frequent words for the binary input features. The results are given in Figure 3(a). We also provide the results of a Gaussian kernel SVM 5 and of a regular neural network for comparison.
 Once again, HDRBM outperforms the other RBM models. However, here the generatively trained RBM performs better then the DRBMs. The HDRBM also outperforms the SVM and neural network classifiers. In order to get a better understanding of how the HDRBM solves this classification problem, we first looked at the weights connecting each of the classes to the hidden neurons. This corresponds to the columns U  X  y of the weight matrix U . Figure 3(b) shows a sim-ilarity matrix M ( U ) for the weights of the different newsgroups, where M ( U ) y 1 y 2 = sigm( U T  X  y see that the HDRBM does not use different neurons for different newsgroups, but shares some of those neurons for newsgroups that are semantically related. Another interesting visualization of this characteristic is given in Figure 3(c), where the columns of U were projected on their two principal components. In both cases, we see that the HDRBM tends to share neurons for simi-lar topics, such as computer ( comp.* ), science ( sci.* ) and politics ( talk.politics.* ), or secondary topics such as sports ( rec.sports.* ) and other recreational activities ( rec.autos and rec.motorcycles ). Table 2 also gives the set of words used by the HDRBM to recognize some of the newsgroups. To obtain this table we proceeded as follows: for each newsgroup y , we looked at the 20 neurons with the largest weight among U  X  y , aggregated (by summing) the associated input-to-hidden weight vectors, sorted the words in de-creasing order of their associated aggregated weights and picked the first words according to that order. This procedure attempts to approximate the positive contribution of the words to the conditional probabil-ity of each newsgroup. 7.3. Semi-supervised Learning We evaluated our semi-supervised learning algorithm for the HDRBM on both the digit recognition and doc-ument classification problems. We also experimented with a version (noted MNIST-BI) of the MNIST dataset proposed by Larochelle et al. (2007) where background images have been added to MNIST digit images. This version corresponds to a much harder problem, but it will help to illustrate the advantage brought by semi-supervised learning in HDRBMs. The HDRBM trained on this data used truncated exponen-tial input units (see (Bengio et al., 2007)). In this semi-supervised setting, we reduced the size of the labeled training set to 800 examples, and used some of the remaining data to form an unlabeled dataset D unlab . The validation set was also reduced to 200 labeled examples. Model selection 6 covered all the parameters of the HDRBM as well as the unsuper-vised objective weight  X  of equation 7. For compar-ison purposes, we also provide the performance of a standard non-parametric semi-supervised learning al-gorithm based on function induction (Bengio et al., 2006b), which includes as a particular case or is very similar to other non-parametric semi-supervised learn-ing algorithms such as Zhu et al. (2003). We provide results for the use of a Gaussian kernel (NP-Gauss) and a data-dependent truncated Gaussian kernel (NP-Trunc-Gauss) used in Bengio et al. (2006b), which es-sentially outputs zero for pairs of inputs that are not near neighbors. The experiments on the MNIST and MNIST-BI (with background images) datasets used 5000 unlabeled examples and the experiment on 20-newsgroup used 8778. The results are given in Table 3, where we observe that semi-supervised learning consis-tently improves the performance of the HDRBM. The usefulness of non-parametric semi-supervised learning algorithms has been demonstrated many times in the past, but usually so on problems where the dimensionality of the inputs is low or the data lies on a much lower dimensional manifold. This is reflected in the result on MNIST for the non-parametric meth-ods. However, for high dimensional data with many factors of variation, these methods can quickly suffer from the curse of dimensionality, as argued by Bengio et al. (2006a). This is also reflected in the results for the MNIST-BI dataset which contains many factors of variation, and for the 20-newsgroup dataset where the input is very high dimensional.
 Finally, it is important to notice that semi-supervised learning in HDRBMs proceeds in an online fashion and hence could scale to very large datasets, unlike more standard non-parametric methods. 7.4. Relationship with Feed-forward Neural There are several similarities between discriminative RBMs and neural networks. In particular, the com-putation of p ( y | x ) could be implemented by a single layer neural network with softplus and softmax acti-Model Error RBM+NNet 26.8% SVM 32.8%
NNet 28.2% Model MNIST MNIST-BI 20-news HDRBM 9.73% 42.4% 40.5% Semi-sup HDRBM 8.04% 37.5% 31.8% NP-Gauss 10.60% 66.5% 85.0%
NP-Trunc-Gauss 7.49% 61.3% 82.6% vation functions in its hidden and output layers re-spectively, with a special structure in the output and hidden weights where the value of the output weights is fixed and many of the hidden layer weights are shared. The advantage of working in the framework of RBMs is that it provides a natural way to introduce gener-ative learning, which we used here to derive a semi-supervised learning algorithm. As mentioned earlier, a form of generative learning can be introduced in stan-dard neural networks simply by using RBMs to ini-tialize the hidden layer weights. However the extent to which the final solution for the parameters of the neural network is influenced by generative learning is not well controlled. This might explain the superior performance obtained by a HDRBM compared to a single hidden layer neural network initialized with an RBM (RBM+NNet in the tables). We argued that RBMs can and should be used as stand-alone non-linear classifiers alongside other stan-dard and more popular classifiers, instead of merely being considered as simple feature extractors. We eval-uated different training objectives that are more ap-propriate to train an RBM in a classification setting. These discriminative versions of RBMs integrate the process of discovering features of inputs with their use in classification, without relying on a separate classi-fier. This insures that the learned features are dis-criminative and facilitates model selection. We also presented a novel but straightforward semi-supervised learning algorithm for RBMs and demonstrated its usefulness for complex or high dimensional data. For future work, we would like to investigate the use of discriminative versions of RBMs in more challeng-ing settings such as in multi-task or structured out-put problems. The analysis of the target weights for the 20-newsgroup dataset seem to indicate that RBMs would be good at capturing the conditional sta-tistical relationship between multiple tasks or in the components in a complex target space. Exact com-putation of the conditional distribution for the tar-get is not tractable anymore, but there exists promis-ing techniques such as mean-field approximations that could estimate that distribution. Moreover, in the 20-newsgroup experiment, we only used 5000 words in input because generative training using Algorithm 1 does not exploit the sparsity of the input, unlike an SVM or a DRBM (since in that case the sparsity of the input makes the discriminative gradient sparse too). Motivated by this observation, we intend to explore ways to introduce generative learning in RBMs and HDRBMs which would be less computationally expen-sive when the input vectors are large but sparse. We thank Dumitru Erhan for discussions about sparse RBMs and anonymous reviewers for helpful comments.
