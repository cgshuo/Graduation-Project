 Automated singer identification is important in organising, brows-ing and retrieving data in large music databases. In this paper, we propose a novel scheme, called H ybrid S inger I dentifier (HSI), for automated singer recognition. HSI can effectively use multiple low-level features extracted from both vocal and non-vocal music segments to enhance the identification process with a hybrid archi-tecture and build profiles of individual singer characteristics based on statistical mixture models. Extensive experimental results con-ducted on a large music database demonstrate the superiority of our method over state-of-the-art approaches.
 H.3 [ Information Systems ]: Content Analysis and Indexing, In-formation Storage and Retrieval; J.5 [ Arts and Humanities ]: Mu-sic Algorithm, Experimentation, Measurement, Evaluation Music Retrieval, Singer Identification
With the explosive growth of online music repositories, tech-niques for content-based music retrieval are getting more attention in multimedia databases. Consequently, it has become an attrac-tive research topic, and many techniques have been developed to support automatic classification or recognition of music based on instrument, genre and other characteristics [11, 12, 14, 23]. With such techniques, users are provided with powerful functions for browsing and searching musical content in a large music database.  X  This research was partly supported by The Australian Research Council(ARC Discovery Grant DP0346004).
 Copyright 2006 ACM 1-59593-369-7/06/008 ... $ 5.00.

Techniques for automatic artist identification are becoming more and more important due to numerous potential applications includ-ing music indexing and retrieval, copyright management and music recommendation systems. The development of singer identifica-tion enables the effective management of music databases based on  X  X inger similarity X . With this technology, songs performed by a particular singer can be automatically clustered for easy manage-ment or exploration.

Currently, the most popular and naive approach to support singer identification is to manually embed artist information into the mu-sic database with the assistance of music professionals. The main weakness of the approach is that it requires a large amount of time and high domain expertise, which is very expensive. Moreover, in some cases, such as music downloaded from the Internet, descrip-tive information is often lacking and/or inconsistent. It is clear that songs performed by the same singer share certain audio charac-teristics; the singer X  X  voice is most likely to contain similar audio patterns over all songs (s)he perform. Singers also tend to per-form within a single genre, and so the audio characteristics of their work may contain common features (e.g. instrumentation). This suggests the feasibility of automatic singer identification based on audio content. By  X  X utomatic singer identification X  here we refer to the task of determining which singer is most likely singing a given song. The effectiveness of a solution heavily relies on its capability to capture salient information for separating one signal from another. While traditional s peech recognitio n techniques [2, 15] could be applied for this task, they are likely to perform well, because the vocal track is intertwined with the nonstationary back-ground signal played by different instruments. However, it is rare that we might acquire a pure solo voice track without instrumenta-tion (unless we had access to the original multi-track data for the song).

Several approaches have been recently proposed to apply statis-tical models or machine learning techniques for automatic singer classification/identification [12, 19, 23]. In general, these methods consist of two main steps: singer characteristic modelling based on solo voice and class label identification. In the singer char-acteristic modelling step, acoustic signal information is extracted to represent the music. Then specific mechanisms (i.e., statistical models or machine learning algorithms) are constructed to assign the songs to one of the pre-defined singer categories based on their extracted acoustic features. Unfor tunately, these approaches are unable to achieve acceptable classification accuracy. The main rea-sons are: 1) Like other kinds of multimedia objects such as images and videos, songs with vocal components contain many audio fea-tures. Thus, good performance for identification system cannot be expected by employing a single type of feature to represent mu-sic items, 2) No existing work h as addressed the underlying multi-feature integration model that can be used to explore the conjunc-tive effects among the different acoustic characteristics for effective singer identification, and 3) Most previous work simply ignored the influence of accompanying music. However, nonvocal components generally carry a large amount of information about music style and genre, which could be significantly useful to identify songs performed by certain artist.

Motivated by these concerns, this paper proposes a novel system, called H ybrid S inger I dentifier (HSI), for automated singer recog-nition in large music databases. In summary, the contributions of the paper include:
The remainder of this paper is organised as follows: Section 2 gives a brief overview of previous related work. In Section 3, we present our proposed architecture, its component modules and rel-evant algorithms. Section 4 reports the experimental study. Finally, we conclude the paper with the directions for future work in Sec-tion 5.
In recent years, there has been increasing interest in the devel-opment of frameworks for singer identification and associated top-ics. Among the earliest of such sy stems, Minnowmatch mainly fo-cuses on artist rather than singer identification using mel-frequency cepstral coefficients (MFCCs), which is a feature adapted from the classical speech recognition a nd speaker identification mech-anisms [21]. The best identification accuracy achieved on a small dataset, containing a 10 artist set, is approximately 70%. However, with a larger set of 21 artists, the best case accuracy significantly drops to 50%.

In [4], vocal music is used as an input to a speech recognition system, achieving a success rate of up to 80% in isolating vocal regions. In [3], the authors use a neural network trained on radio recordings to similarly segment songs into vocal and non-vocal re-gions. By focusing on voice regions alone, they improved artist identification by 15%. The system present here also attempts to perform segmentation of vocal regions prior to singer identifica-tion. After segmentation, the classifier uses features drawn from voice coding based on Linear Predictive Coding.

In [12], a novel scheme is designed and developed to automat-ically classify music objects according to their singers. First, the coefficients extracted from the output of the polyphase filters are used to compute the music features for segmentation. Based on these features, a music object can be decomposed into a sequence of notes (or phonemes). Then for each phoneme in the training set, its music feature is extracted and used to train a music classifier which can identify the singer of an unknown music object.
Zhang [23] developed a system for automatic singer identifica-tion which recognises the singer of a song by analysing the mu-sic signal. The proposed scheme follows the framework of com-mon speaker identification systems, but special efforts are made to distinguish the singing voice from instrumental sounds in a song. A statistical model is trained for each singer X  X  voice with typical song(s) of the singer. Then, for a song to be identified, the starting point of the singing voice is detected and a portion of the song is ex-cerpted from that point. Audio features are extracted and matched with singers X  voice models in the database. The song is assigned to the model having the best match. Accuracy rates of around 80% are achieved in a tiny database with 45 songs. Meanwhile, in [1], a singer identification method is developed based on the spectral en-velope estimation using composite transfer function (CTF), which is calculated from the instantaneous amplitude and frequency of the signal X  X  harmonic partials. Unfortunately, this method only exam-ines a very limited case in which audio samples only contain the singer X  X  voice, without accompaniment.

More recently, Tsai et al. proposed a solo voice modelling frame-work to capture singers X  vocal characteristics [19]. The technique firstly separates vocal from non-vocal regions and then models the singers X  vocal characteristics based on stochastic properties of the background music. However, the main weakness for the method is that it uses only a single type of acoustic feature for vocal portions (20 dimensional Mel-Frequency Ceptral coefficients) to profile dif-ferent singers. Based on their experiment on a small dataset of 230 popular music songs, the accuracy of identification is only 71%. At this point, it is the state-of-the-art in singer identification. Table 1 summarises the properties of each identification method. As discussed above, those methods either use single kind of acous-tic feature to represent music objects or simply investigate the singer identification from a speech-recognition angle. Moreover, all of them have been tested based on very small dataset. Consequently, it is difficult to evaluate their impact on real life applications. In this section, we present a new method, called H ybrid S inger I dentifier (HSI), to facilitate auto mated singer recognition in large music databases. The system, as illustrated in Figure 1, utilises a layering structure and consists of two major component layers -preprocessing module and singer modelling module. The ma-jor functionality of the preproce ssing module is to separate an in-coming song into vocal and non-vocal segments. The second layer includes multiple statistical si nger models, each corresponding to one artist. In particular, the statistical model for one singer consists of a series of Gaussian Mixture Models (GMMs), each constructed using one kind of acoustic feature. To identify a song, different fea-ture vectors are firstly extracted from vocal and nonvocal segments. The likelihood values being generated by each GMMs in one statis-tical singer model are combined to form an overall relevance score using a novel fusion scheme based on Logistic function. Thus, singer X  X  label is finally assigned to singer with the highest over-all relevance score. In the following subsections, we will present the submodules and corresponding algorithms in the system.
In the first identification process, vocal and non-vocal segments are identified and labelled via the preprocessing module. This pro-cess can be treated as a problem of vocal boundary detection and steps are illustrated in Algorithm 1. We use a learning based ap-proach with support vector machine (SVM). It consists of two pro-cesses: feature extraction and classification with SVM. When a song comes in, it is divided into many short time-frames of a pre-defined length (line 1.1). We set the length of frame to 0.5sec as it yields the best performance in our experiments. The acoustic features are calculated from each frame (line 1.2) and they include the Mel-Frequency Ceptral coefficients (MFCC), Spectral centroid, Spectral flux, Zero crossings, and Low energy.

After obtaining the features, the SVM is used to separate vocal from non-vocal frames (line 1.3). The use of SVM is motivated by the fact that they have demonstrated a strong effectiveness on various categorisation problems. The basic function of the SVM is that it can non-linearly map the input features into a high dimen-sional feature space; then a linear classifier is constructed to use optimal hyper-planes to separate positive patterns and negative pat-terns with maximum margin. The SVM performs well for binary classification, and the LIBSVM [5] is utilised in this work.
As mentioned before, in the second layer of our HSI system, there are numerous singer characteristic models, one for each singer. Each singer characteristic model is made up of two parts: feature extraction and multiple mixture models are built for capturing sta-tistical information using different acoustic features for a particular singer. Its structure is presented in Figure 2. To effectively represent the complex musical content of a specific artist X  X  song, we extract different features from both the vocal and non-vocal segments. The features consist of four different com-ponents: vocal timbre feature (VTF), vocal pitch feature (VPF), genre-based feature and instrument feature 1 . The VTF and VPF capture information of the vocal segments performed by the singer. The genre-based feature represents the music style. The instrument feature is used to model the characteristics of typical instrument configuration for songs performed by the artist. This is motivated by research results on perceptual study which indicates that music performed by certain singers typically contains similar instrument configuration [22]. The detail information is as follows:
Note that our method can be easily extended to consider more acoustic features.
For the purpose of effective singer identification, HSI constructs a statistical model for each singer based on multiple features. To achieve this, the individual features of the music signal are ex-tracted, and then, individual profiling models for one singer are built up based on each feature 2 . In our framework, singer profil-ing aims to capture statistical properties of different features with a finite numbers of mixture models. Thus, the probability of singer label s can be modelled as a random variable drawn from a proba-bility distribution for certain feature f . Given a parameter set estimated based on feature f , it is present as a mixture of multivari-ate component densities,
Where V f = { v 1 f , v 2 f , ..., v Tf } . Assume that Gaussian den-sity is used as multivariate component in this study, according to ance matrices individually. Also, p s f ( v tf |  X  s fj , bility of a singer label s based on feature f extracted from segment t and given data v tf , and can be easily calculated using Gaussian density function and associated parameters {  X  s fj ,  X  s
In HSI, an EM algorithm is used to determine a set of model parameters. This process of estimation is an iterative hill climbing procedure. The goal is to derive an optimal parameter set maximum likelihood estimation as
In this study, since four differen t features are extracted, the num-ber of profiling models for each singer is four.
At the first step,  X  s f is initialised with random values. Then, new model c  X  s f is obtained with the auxiliary function in E step, where and
For the M step, the parameters can be updated using estimation as below,
The updating procedure is repeated until the log-likelihood value is increased by less than a predefined threshold from one iteration to the next. Since HSI considers four different features, the overall training procedure will be rep eated four times, once for each fea-ture. After the training process is completed, the likelihood value generated based on feature f for input feature vector V f given as below, and we can derive an overall likelihood value based on various features for singer s , expressed as below, where w s f is the combination weights and C s is likelihood value combination function. L s can be used to quantify universal simi-larity distance between input song and singer label s .
In HSI, for each singer characteristic model, a logistic function is used as a combination function C s to derive an overall likeli-hood score. Logistic functions have been widely used in the statis-tical and machine learning community and play an important role in one of the most popular statistical algorithms -Logistic Regression (LR) [9, 20, 8]. The main reason for using LR to estimate param-eters is that few statistical assumptions are required for its use and
Algorithm 2 : Logistic regression training algorithm to de-termine weights of score fusion for singer s less computational cost in terms of training. In addition, the output of logistic functions can map any real value into probabilistic out-put in [0,1]. With logistic function, Eq. 10 can be reformulated as below,
Where y s =1 if this input object belongs to singer s , y otherwise, w s f is the weight for singer s  X  X  likelihood value gener-ated based on feature f . F is the size of input score and equals the number of features extracted in the first layer. L s denotes the overall relevancy score -conditional probability of singer s . Based on Eq. 11, the likelihood value occurring in the learning samples is,
Where M is the number of training examples. It is easy to see that the goal of the training process is to maximise the overall like-lihood value. Thus, the goal is to find W s to minimise the log loss of the model as below,
To achieve this, the sequential-update optimisation algorithm pro-posed by Collins et.al [6] 3 is applied and the modified version is shown in Algorithm 2 . The algorithm is equivalent to AdaBoost [7, 10]. The basic principle is that on each iteration t , the algorithm up-dates distribution q t,i to increase the weights of misclassified train-ing examples (lines 2.14 -2.15). To count the distribution between
For detail derivation, please refer to paper [6]. positive and negative learning examples, we revise the algorithm to give q t,i weight (lines 2.4 -2.9).
The goal of the HSI is to identify the singer of a song presented only as audio data. Given the architecture introduced previously, we describe the procedures of the whole system. To accomplish the singer identification, we have a music database and we know the singers of the songs in the database. In the training stage, we pro-cess the songs in the database, and get the features of each singer. According to the characteristics of the singers, the parameters for the multifeature statistical modelling module can be generated.
After the setup of the system, we can conduct the task of singer identification. The basic steps are shown in Algorithm 3 .Fora given music item, at the initial stage of the process, the system par-titions the song into vocal and non-vo cal segments (line 3.1). After that, the feature extraction proce dure generates four different kinds of features as described in Section 3.2.1. Next, the features are fed into statistical module for individual singer in the second layer of the HSI. The likelihood score based on a particular feature can be generated based on Eq. 9. Then, relevance scores, one from each singer statistical model, can be calculated with Equation 11(line 3.3). Those scores quantify the similarity between the incoming song and the singer label. In the final step, a singer X  X  label for the song can be assigned based on those relevance scores. Algorithm 3 : Algorithm for automatic singer identification
The HSI architecture presented in previous sections, enjoys sev-eral advantages over competing approaches:
In this section, we present an experimental study to evaluate the proposed method. We demonstrate the effectiveness of our method for large music databases by comparing it with the current best ap-proaches in the areas of accuracy of singer identification, scalability to accommodate different size of data, robustness against various kind of noise and efficiency in terms of the response time.
We first present the experimental settings for the performance evaluation, including competitive systems, testing datasets and per-formance metrics for evaluation. All methods have been imple-mented and tested on a Pentium III, 450MHz, PC running the Linux operating system. A demo system of our HSI system is also avail-able [16].
 Two datasets are used in our experimental study. The first dataset, Dataset I, contains 230 songs from 13 female and 10 male artists with 10 songs per artist. The dataset has been used in [19]. The second dataset, Dataset II, consists of 8000 songs covering 90 dif-ferent singers. This dataset was constructed from a CD collection of the authors. It includes 45 male singers (such as Van Morrison, Michael Jackson, Elton John, etc) and 45 female singers (such as Kylie Minogue, Madonna, Jennifer Lopez, etc). The length of each music item in the two datasets is set as 30 seconds and there is no overlap between the two datasets. For both datasets, the sound files are converted to 22050Hz, 16-bit, mono audio files. For singer identification, we use 20% of each dataset for training purposes and the remaining songs to evaluate the performance of all the schemes studied. In all experiments present below, there is no overlap be-tween training set and test set.

As discussed above, the main goal of the system is to identify the artist who performs an incoming song. The procedure is simi-lar to clustering with certain criteria. Thus, our evaluation method focuses on how accurate the identification process is with different approaches for a particular database. We use the accuracy as the metric for evaluation: accuracy = N C N ,where N C is the number of songs correctly identified and N is the number of songs for eval-uation. On the other hand, the average response time is applied to measure the efficiency of different techniques, which represents the time required for identifying a single query song.
In this section, we compare HSI with three existing methods in-cluding BER, LIU and TSAI. To ensure a fair comparison, we se-lected the same set of data for training, and use the rest for perfor-mance evaluation.
In this section, we first report comparative results on the accuracy of the various singer identification schemes. Table 2 summarises the results for the two datasets. The bottom two rows show how the BER and LIU performed. The experiments verify the fact that both of them achieve similar identification accuracy, which is the worst. Furthermore, although the TSAI technique achieves better performance than LIU and BER, it still suffers from low accuracy. This is because TSAI only considers MFCC based acoustic charac-teristic inside vocal segments of one song. In fact, the experimental results clearly demonstrate that HSI significantly outperforms the other three approaches. For example, from Table 2, it shows that compared to TSAI, the HSI method improves the identification pre-cision from 72.5% to 87.3% for dataset I and 62.1% to 76.2% for dataset II. In addition, this is a remarkable improvement over LIU. On average, around 35% improvement can be observed for both datasets.
 HSI has two advantages over the competitive schemes. First, HSI extracts both vocal and non-vocal features from the song. The usage of multiple features can result in more comprehensive sta-tistical models for songs by a particular singer and hence result in better identification effectiveness. Second, the weight for fusing likelihood scores derived via logistic regression can capture joint effects among various acoustic characteristics for singer identifica-tion. This naturally raises the question of how much accuracy can be improved by the scheme. In the last experiment, identification effectiveness of HSI with and without nonlinear likelihood value fusion weights is examined. The first two rows in Table 2 illus-trate the results. As expected, the nonlinear likelihood value fusion weights generation scheme presented in Section 3.3 plays an im-portant role in the whole identification procedure and can bring sig-nificant improvement in identification accuracy. From Table 2, we can see that the HSI with nonlinear likelihood value fusion weights offers much higher accuracy (about 12% accuracy improvement). With the scheme, we are able to construct a connection between the misclassification and its correction in the second layer of the system via Logistic regression.
 Table 2: Identification accuracy comparison. HSI-L denotes linear combination of likelihood score with same weight.
From the above experiment, we can see that the proposed HSI approach is superior to other methods in terms of accuracy. For large music databases, the response time is another concern of the system performance. Although the more features and extra deci-sion module in the HSI improve the accuracy, they introduce more cost overhead. In this experiment, we will show how this affects the time efficiency. Tests were run with 6400 songs and 180 query songs for Dataset I and Dataset II.

Table 3 shows the response time of query for different singer identification schemes over the two datasets. From the experimen-tal results summarised in the table, we can see that HSI achieved comparable results in terms of query speed against the other two methods, especially for large datasets. For example, the identifi-cation process over Dataset II with BER, LIU and TSAI required 1.775sec, 1.772sec, and 1.681sec respectively to finish in the mu-sic database containing 8000 songs. By contrast, it takes HSI only 1.086sec to complete whole process. There is only 29% saving. Another interesting observation is that, unlike the other three ap-proaches, the gap of response time for HSI between different sizes of data is much smaller and the time does not increase proportion-ally to the size (see the scalability experiment in next section).
Scalability is particularly important for large music databases, because such systems can potentially contain thousands of songs. As the number of music items increases, the performance of a system may degrade due to noise and more similar songs in the database. In this experiment, we compare the identification accu-racy and response time of the HSI system with the two other com-petitors using different sizes of music data.
For this set of experiments, we randomly pick 2000, 4000, 6000 and 8000 songs from dataset II. 20% of data are used for training and the rest is used for testing. Figure 3 shows the experimen-tal results for the four systems. We can see that the accuracy and query times for both LIU and BER degrade dramatically when the data size increases. This is because the larger data sizes affect the performance of the machine learning based classifiers in those sys-tems. While TSAI performs better, the improvement is very lim-ited. Compared to the above three approaches, HSI is very robust against the volume of data. There is no significant decrease in ac-curacy or increase query processing time with larger datasets, e.g. around 79% accuracy for large size of 2000 songs which increases only 4% for the same system with 8000 songs. This is because HSI is constructed using multiple feature from the songs and this enables HSI to capture the song characteristics more precisely. At the same time, conjunctive effects of multiple features are applied to enhance system performance via a nonlinear likelihood fusion weights generation scheme.
Human perception has a superi or capability to identify sound or music, even in the presence of moderate amounts of distortion. Also, real world applications often require singer identification un-der different constrained conditions. This property could be ex-tremely important in modern music database applications. This is because real data can typically contain noise and distortion. In this section, to study the robustness of different singer identification techniques, music data items are modified with different kinds of distortion as query examples and a series of experiments has been carried out to test the performance of different systems in the pres-ence of moderate amounts of noises and other kinds of distortions.
During the evaluation, we ran the same set of tests as in the pre-vious experiments. However, each query song is distorted and the results are compared against the results obtained from using a non-distorted query. We vary the levels and types of distortions to test the robustness of different schemes. Figure 4 summaries the accu-racy of three methods under various distortions for dataset II. The performance on dataset I shows a similar trend and we omit them due to space limitations.

The experimental results clearly demonstrate that compared with the other two approaches, the HSI emerges as the most robust tech-nique in terms of accuracy. It performs better than the competitors on all distortion cases. For example, it is shown that, HSI is robust to echo with 6s cropping, 60% volume amplification, 70% volume deamplification, 7sec delay time and 40dB SNR background noise on average 4 . In addition, the figures also show that HSI is fairly robust to different levels of noises and acoustic distortions.
With fast increasing music data from various emerging appli-cation domains, efficient and effective music information retrieval is becoming more important than ever. In this paper, we devel-oped a novel technique, called HSI, to facilitate singer identifica-tion in large music databases. Extensive experimental results based on large datasets demonstrate the superior effectiveness, scalabil-ity, efficiency and robustness of HSI over state-of-the-art systems. In the near future, we plan to evaluate the current framework on a larger dataset, develop advanced acoustic feature extraction meth-ods to gain further improvement on the effectiveness of the frame-work, and examine novel indexing structures to speed up the query processing cost. Adapting the system for other kinds of multimedia data is another promising research direction.
We would like to thank Professor Wei-Ho Tsai at Taipei Univer-sity of Technology, for kindly sharing his dataset with us. [1] M. Bartsch and G. Wakefield. Singing voice identification [2] C. Becchetti, L. Ricotti, and L. Ricotti. Speech Recognition . [3] A. Berenzweig, D. P. W. Ellis, and S. Lawrence. Using voice [4] A. L. Berenzweig and D. P. W. Ellis. Locating singing voice [5] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for [6] Michael Collins, Robert E. Schapire, and Yoram Singer. [7] Y. Freund and R. Schapire. A decision-theoretic [8] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of [9] Michael I. Jordan. Why the logistic function? a tutorial
The equation SNR dB =10 log 10 S N is used to calculate the signal-to-noise ratio, where S is the signal power, and N is the noise power in dB. [10] G. Lebanon and J. Lafferty. Boosting and maximum [11] Tao Li, Mitsunori Ogihara, and Qi Li. A comparative study [12] C. C. Liu and C. S. Huang. A singer identification technique [13] A. Livshin and X. Rodet. Musical instrument identification [14] Francois Pachet. Content management for electronic music [15] L. Rabiner and B. Juang. Fundamentals of Speech [16] J. Shen, J. Shepherd, B. Cui, and K.L. Tian. HSI: A novel [17] J. Sundberg. The Science of the Singing Voice . Illinios [18] T. Tolonen and M. Karjalainen. A computationally efficient [19] W. H. Tsai and H. M. Wang. Automatic singer recognition of [20] V. Vapnik. Statistical Learning Theory . John Wiley&amp;Sons, [21] B. Whitman, G. Flake, and S. Lawrence. Artist detection in [22] C. Xu, N.C. Maddage, and X. Shao. Automatic music [23] T. Zhang. Automatic singer identification. In Proc. of IEEE
