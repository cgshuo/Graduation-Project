 School of Computing and Information Sciences
Recently, many research efforts have been reported on developing efficient and effective techniques for analyzing large document collections. Among these efforts, nonnega-tive matrix factorization (NMF) has been shown to be useful for different document understanding problems, e.g., docu-ment clustering [40] and summarization [38]. The success of NMF is largely due to the newly discovered ability of NMF to solve challenging data mining and machine learning problems. In particular, NMF with the sum of squared error cost function is equivalent to a r elaxed K-means clustering, the most widely used unsupervised learning algorithm [8]. In addition, NMF with the I-divergence cost function is equiv-alent to probabilistic latent semantic indexing (PLSI) [22], another unsupervised learning method popularly used in text analysis [10], [14]. Furthermore, NMF is able to model widely varying data distributions and can do both hard and soft clustering simultaneously. Several variants of NMF with different forms of factorization and regularization have also been developed and applied to many document analysis tasks [11], [18], [38], [39].

Although NMF and its variants have shown their effective-ness in these tasks, they usually perform data clustering on all feature space. As we know, keyword (feature) selection can enhance and improve many document applications such as document categorization and automatic topic discovery. However most of existing keyword selection techniques are designed for supervised classification problems.
In this paper, we extends NMF to solve a novel problem of clustering with double labeling of important features and data points, which means that each data point is marked as belonging to one of the groups, and each feature and data point are also weighted to assess their importance respectively.

In particular, we first extends NMF to feature subset NMF which combines keyword selection and document clustering (topic discovery) together. The proposed extension incorpo-rates a weight matrix to indicate the importance of the key-words. It considers both theoretically and empirically feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering. The se-lected keywords are discriminant for different topics in a global perspective, unlike those obtained in co-clustering, which typically associate with one cluster strongly and are absent from other clusters. Also, the selected keywords are not linear combinations of words like those obtained in Latent Semantic Indexing (LSI) [17]: our selected words provide clear semantic meanings of the key features while LSI features combine different words together and are not easy to interpret.

We further extend feature subset NMF into a weighted version which assumes documents (data points) contribute differently to the clustering process, i.e., some documents are tightly related to certain topics, while some can be considered as outliers. Finally, we apply the proposed approaches in document understanding problems such as document clustering, summarization, and visualization. The comprehensive experiments demonstrate the effectiveness of our approaches.

The rest of the paper is organized as follows. Section II discusses the related work in NMF framework and various document understanding tasks. In Section III, we derive a generic theorem on the NMF algorithm. Section IV and Section V propose our (weighted) feature subsect NMF. An illustrative example is shown in Section VI, and compre-hensive experiments on document clustering, summarization, and visualization are conducted in Section VII. Section VIII concludes.
 A. NMF Framework NMF has been shown to be very useful for data clustering. Lee and Seung [24] proposed the NMF problem and showed that the NMF problem could be solved by a multiplicative update algorithm. In general, the NMF algorithm attempts to find the subspaces in which the majority of the data points lie. Let the input data matrix X =( x 1 ,...,x n ) contain the collection of n nonnegative data column vectors. The problem of NMF aims to factorize X into two nonnegative matrices, where
Similarly, there are other matrix factorizations which differ with standard NMF by the restrictions on the matrix factors and forms: We list them as follows.

Note that WFS-NMF is our proposed algorithm which extends NMF by incorporating a weight matrix to indicate the importance of the keywords and data points. The detail of the algorithm will be discussed in the following sections. A preliminary study of feature subset NMF which only considers the importance of keywords was presented as a two-page poster [37].

The relations between NMF and some of the other matrix factorization and clustering algorithms have been studied in [25]. In general, (1) Orthogonal NMF is equivalent to K-means clustering; (2) G-orthogonal NMF, semi-NMF and convex-NMF are identical to relaxed K-means clustering; (3) Tri-factorization with explic it orthogona lity constraints can be transformed into 2-factor NMF; (4) PLSI [22] (which is further developed into a more comprehensive Latent Dirich-let Allocation (LDA) model [1]) solves the problem of NMF with Kullback-Leibler divergence; (5) our proposed WFS-NMF combines clustering with double labeling of important features and samples by assi gning different weights to each row and column based on the weight matrix.
 B. Document Understanding Applications
There exist various document understanding applications in IR community. Here, we briefly review some popular tasks including document clustering, document summariza-tion, and visualization. In this paper, we also apply our proposed approaches to these three applications. tering has been extensively studied. Given a collection of documents, document clustering partitions them into differ-ent groups (called clusters) so that similar documents belong to the same group while the documents in different clusters are dissimilar. The problem of document clustering has been extensively studied. Traditional clustering techniques such as hierarchical and partitioning methods have been used in clustering documents (e.g. hierarchical agglomer-ative clustering (HAC) [12] an d K-means clustering [20]). Model-based clustering methods such as PLSI and the more comprehensive LDA have also been successfully applied to document clustering [22], [1]. Recently, matrix and graph based clustering algorithms have emerged as promising clustering approaches [39], and two representative examples of which are spectral clustering [34] and non-negative matrix factorization (NMF) [24], [40]. Co-clustering algorithms are then proposed which aim at clustering document and term simultaneously by making use of the dual relationship information [5], [7], [43]. Subspace clustering algorithms have also been developed for discovering low-dimensional clusters in high-dimension document space [26], [23]. summarization aims to generate a short summary for a collection of documents reflecting the major or query-relevant information. Existing summarization methods usually rank the sentences in the documents according to their salient scores calculated by a set of predefined liguistic features, such as term frequency-inverse sentence frequency (TF-ISF) [28], sentence or term position [41], and number of keywords [41]. Gong et al. [16] propose a generic method using latent semantic analys is (LSA) to select sentences with high ranking for summarization. Goldstein et al. [15] propose a maximal marginal relevance (MMR) method to summarize documents based on the cosine similarity between a query and a sentence and also the sentence and previously selected sentences. Other approaches include NMF based summarization [30], Conditional Random Field (CRF) based summarization [33], and hidden Markov model (HMM) based method [4]. In addition, graph-ranking based approaches have been proposed to summarize documents using the sentence relationship [13], the idea of which is similar to PageRank.
 cuses on displaying document relationships using various presentation techniques, which helps users to understand and navigate information easily. Some techniques have been developed to map the document collection into multivariate space. Typical systems for document visualization include the Galaxy of News [32], Jigsaw [35], and ThemeRiver [21].
In this paper, we extend the NMF model to allow unsu-pervised feature selection and data clustering and ranking to be conducted simultaneously. We apply the proposed approaches in three document understanding applications to demonstrate the effectiveness of the approaches for improv-ing document understanding.

In this paper, we will derive several algorithms for NMF problems. Here we first provide a generic theorem on the NMF algorithm. We will use this results repeatedly later.
For the following optimization problem where
P,Q,H  X  0 are constant matrices, the optimal solution for H is given by the following updating algorithm Theorem 1. If the algorithm converges, the converged solution satisfies the KKT condition.
 Proof . We minimize the Lagrangian function where  X  =(  X  ik ) is the Lagrangian multiplier to enforce H  X  0 . Setting the KKT complementarity slackness condition  X  ik H ik =0 becomes Now, when iterative solution of H convergees, it satisfies One can see Eq.(4) is identical to Eq.(3) either H ik =0 or not. This proves that the converged solution satisfies KKT condition.  X  Theorem 2 . The updating algorithm of Eq.(2) converges. Proof . We use the auxiliary function approach [24]. A function Z ( H,H ) is called an auxiliary function of J ( if it satisfies for any H,H .Define where we note that we require the global minimum. By construction, we have J ( H ( t ) )= Z ( H ( t ) ,H ( t ) Z ( decreasing (non-increasing). The key is to find (1) appropri-ate
Z ( H,H ) and (2) its global minimum.
 Using the following matrix inequality where H,P,Q  X  0 and P = P T ,Q = Q T ,wecanseethat is an auxiliary function of J ( H ) of Eq.(1). Now we solve Eq.(6) by identifying H ( t +1) = H and H ( t ) = H . Setting we obtain The second derivatives are which is a semi-positive definite matrix, ensuring the local optima of Eq.(8) obtained from Eq.(7) is the global minima for solving Eq.(6). Thus updating H using Eq.(8) will decrease J ( H ) . One can see Eq.(8) is identical to Eq.(2).  X  A. Objective
Let X = { x 1 ,  X  X  X  ,x n ) contains n documents with m keywords (features). In general, NMF factorizes the input nonnegative data matrix X into two nonnegative matrices, where G  X  R n  X  k + is the cluster indicator matrix for cluster-ing columns of X and F =( f 1 ,  X  X  X  ,f k )  X  R m  X  k + contains k cluster centroids.

In this paper, we propose a new objective to simultane-ously factorize X and rank the features in X as follows: where W  X  R m  X  m + which is a diagonal matrix indicating the weights of the rows (keywords or features) in X ,and  X  is a parameter (set to 0.7 empirically).
 B. Optimization
Minimizing Eq.(9) with respect to W , F ,and G ,hasa closed-form solution. We will optimize the objective with respect to one variable while fixing the other variables. This procedure repeats until convergence. 1) Computation of W : Optimizing Eq.(9) with respect to W is equivalent to optimizing J =
Now, from the KKT condition  X  X  1  X  X  i ) W i =0 , we obtain the following updating for-mula 2) Computation of G : Optimizing Eq.(9) with respect to G is equivalent to optimizing
Using the generic algorithm in Section III, we obtain the following updating formula 3) Computation of F : Optimizing Eq.(9) with respect to F is equivalent to optimizing
J ( F )= Tr [ WXX T  X  2 WXGF T + WFG T GF ] .
 Using the generic algorithm in Section III, we obtain the following updating formula 4) Algorithm Procedure: The detail procedure of FS-NMF is listed as Algorithm 1.
 Algorithm 1 FS-NMF Algorithm Description
In Section IV, different weights are assigned to the term features indicating the importance of the keywords, however all the documents are treated equally. This assumption does no longer hold in case that different documents are created with different importance. Thus, we extend our algorithm to a weighted version in which each document is also assigned a weight.

Similar to Eq.( 9), the objective of weighted FS-NMF can be written as: where we set W ij = a i b j . This becomes where  X ,  X  are two parameters with 0 &lt; X &lt; 1 , 0 &lt; X &lt; A. Optimization 1) Computation of W : Since W = ab T , we optimize a =( a 1 ,  X  X  X  ,a m ) first. Optimizing Eq.(13) with respect to a is equivalent to optimizing J This optimization has been analyzed in Section IV-B. The optimal solution for a is given by
We now optimize the objective Eq.(13) with respect to b =( b 1 ,  X  X  X  ,b n ) which is equivalent to optimizing J The optimal solution for b is given by 2) Computation of F : Let A = diag( a 1 ,a 2 ,...,a m ) and B =diag( b 1 ,b 2 ,...,b n ) . Optimizing Eq.(13) with respect to F is equivalent to optimizing = Tr ( X T AXB  X  2 G T BX T AF + F T AFG T BG ) . (16) Using the generic algorithm of Section III, we obtain 3) Computation of G : Using Eq.(16), the objective for G is J (
G )= Tr ( X T AXB  X  2 G T BX T AF + G T BGF T AF ) . (18) Using the generic algorithm of Section III, we obtain B. Algorithm Procedure
The detail procedure of WFS-NMF is listed as Algo-rithm 2.
 Algorithm 2 WFS-NMF Algorithm Description
In this section, we use a simple example to illustrate the process of weighting the keywords and data points using the proposed WFS-NMF algorithm.

An example dataset with six system log messages is presented in Table I, which is a subset of the Log data described in Section VII-A1. The six sample messages belong to two different clusters:  X  X tart X  and  X  X reate X .
In the data pre-processing step, the stop words and the words which only appear once are removed, and also stem-ming is performed. The following term-message matrix is obtained after pre-processing,
After the computation by WFS-NMF, the weights for the terms are
Thus the most important two keywords are  X  X tart X  and  X  X reate X , which is consistent with our perspective. Similarly, the weights for the messages are
Then we know S3 and S6 are not important words in discriminating the two clusters as they have the lowest weights. From the example, we clearly observe that the proposed approaches can discover key features and samples. A. Document Clustering First of all, we examine the clustering performance of FS-NMF and W-FS-NMF using four text datasets as described in Section VII-A1, and compare the results with seven widely used document clustering methods as described in Section VII-A2.
 1) Data Sets: Table II summarizes the characteristics of the datasets used in the experimetns. Detailed descriptions of the data sets are as follows. Language Processing(NLP), Robotics/Vision, Systems, and Theory. colleccted from several diff erent machines at Florida In-ternational University with different operating systems using logdump2td (an NT data collection tool). There are 9 categories of these messages, i.e., configuration, connection, create, depende ncy, other, report, request, start, and stop. collection contains documents collected from the Reuters newswire in 1987. It is a standard text cate-gorization benchmark and contains 135 categories. In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters-top 10. used for document clustering [2], [19]. The dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997. These documents are divided into 20 classes. News-groups. The 20 newsgroups dataset contains approxi-mately 20,000 articles evenly divided among 20 Usenet newsgroups. The raw text size is 26MB.

To pre-process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored. In all our experiments, we first select the top 1000 words by mutual information with class labels. The feature selection is done with the rainbow package [29]. 2) Implemented Baselines: We compare the clustering performance of FS-NMF and W-FS-NMF with the following most widely used document clustering methods. (1) K-means : Stardard K-means algorithm; (2) PCA-Km :PCA is firstly applied to reduce the data dimension followed by the K-means clustering; (3) LDA-Km [9]: an adaptive subspace clustering algorithm by integrating linear discrimi-nant analysis (LDA) and K-means clustering into a coherent process; (4) ECC : Euclidean co-clustering [3]; (5) MSRC : minimum squared residueco clustering [3]; (6) NMF : Non-negative matrix factorization [40]; (7) TNMF : Tri-factor matrix factorization [11]; (8) Ncut : Spectral Clustering with Normalized Cuts [42].

In these implemented baselines, (a) the K-means algo-rithm is one of the most widely used standard clustering algorithm; (b) LDA-Km and PCA-Km are two subspace clustering algorithms which identify clusters existing in the subspaces of the original data space; (c) Spectral Clustering with Normalized Cuts (Ncut) is also implemented since it has been shown that that weighted Kernel K-means is equivalent to the normalized cut [6]; (d) both ECC and MSRC are document co-clustering algorithms that are able to find blocks in a rectangle document-term matrix. Co-clustering algorithms generally perform implicit dimension reduction during clustering process. NMF has been shown to be effective in document clustering [40], and our methods are both based on the NMF framework. 3) Evaluation Measures: To measure the clustering per-formance, we use accuracy and normalized mutual infor-mation as our performance measures. Accuracy discovers the one-to-one relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class. It sums up the whole matching degree between all pair class-clusters. Its value is between [0 , 1] . Accuracy can be represented as: where C i denotes the i -th cluster, and L j is the j -th class. T (
C i ,L j ) is the number of entities which belong to class are assigned to cluster i . Accuracy computes the maximum sum of T ( C i ,L j ) for all pairs of clusters and classes, and these pairs have no overlaps. Generally, the greater accuracy means the better clustering performance.

Normalized mutual information (NMI) is another widely used performance evaluation measure for determining the quality of clusters [36]. For two random variables X and Y the NMI is defined as where I ( X,Y ) is the mutual information between X and Y ,and H ( X ) and H ( Y ) are the entropies of X and Y , respectively. Clearly, NMI ( X,X )=1 and this is the maximum possible value of NMI. Given a clustering result, NMI in Eq.( 21) is estimated as follows: where n i denotes the number of data points contained in the cluster C i (1  X  i  X  k ) ,  X  n j is the number of data points belonging to the j -th class ( 1  X  j  X  k ), and n ij denotes the number of data points that are in the intersection between the cluster C i and the j -th class. In general, the larger the NMI value, the better the clustering quality. 4) Clustering Results: Table III and Table IV show the accuracy and NMI evaluation results on the text datasets. From the experimental comparisons, we observe that: the rigid spherical clusters that the K-means clustering objective function attempts to capture [8]. clustering the rows and columns of the input docu-ments. Hence TNMF generally outperforms NMF. K-means. Note that spectral clustering can be viewed as a weighted version of Kernel K-means and hence it is able to discover arbitrarily shaped clusters. The experimental results of Ncut is similar to those of NMF. Note that it has also been that NMF is equivalent to spectral clustering [8]. NMF model and provide a good framework for weight-ing different terms and documents. Hence both of them generally outperform NMF and TNMF on the datasets. And in the meanwhile, important term features can be discovered by our algorithms. of different documents instead of treating them equally, the results of WFS-NMF achieves the best performance on most datasets.
 B. Document Summarization 1) Data Sets: We use the DUC benchmark datasets (DUC2002 and DUC2004) for generic document summa-rization tasks. Table V gives a brief description of the data sets. 2) Implemented Systems: In this experiment, we compare our algorithms for summarization with several most widely used document summarization methods as follows.  X  (1) DUCBest : the method developed by the team  X  (2) Random : selects sentences randomly for each doc- X  (3) Centroid : similar to MEAD algorithm proposed  X  (4) LexPageRank : a graph-based summarization  X  (5) LSA : conducts latent semantic analysis on terms by  X  (6) NMF : performs NMF on terms by sentences matrix
In order to use FS-NMF or WFS-NMF to conduct docu-ment summarization, we use the document-sentence matrix as the input data X , which can be generated from the document-term and sentence-term matrices, and now each feature (column) in X represents a sentence. Then the sentences can be ranked based on the sentence weights in W in both FS-NMF and WFS-NMF. Top-ranked sentences are included into the final summary. Since WFS-NMF weights both the samples and features, an alternative solution for document summarization is to factorize the sentence-term matrix generated from the original documents, and after computation the sentences are naturally ranked based on their assigned weights. Thus, we develop three new sum-marization methods as follows.  X  (7) FS-NMF : performs FS-NMF on document-sentence  X  (8) WFS-NMF-1 : similar to FS-NMF, performs WFS- X  (9) WFS-NMF-2 : performs WFS-NMF on sentence-3) Evaluation Methods: We use ROUGE [27] toolkit (version 1.5.5) to measure the summarization performance, which is widely applied by DUC for performance evaluation. It measures the quality of a summary by counting the unit overlaps between the candidate summary and a set of refer-ence summaries. Several automatic evaluation methods are implemented in ROUGE, such as ROUGE-N, ROUGE-W and ROUGE-SU. ROUGE-N is an n-gram recall computed as follows.
 where n is the length of the n-gram, and ref stands for the reference summaries. Count match ( gram n ) is the maximum number of n -grams co-occurring in a candidate summary and the reference summaries, and Count ( gram n ) is the number of n -grams in the reference summaries. ROUGE-W is based on weighted LCS and ROUGE-SU is based on skip-bigram plus unigram. Each of these evaluation methods in ROUGE can generate three scores (recall, precision and F-measure). As we have similar conclusions in terms of any of the three scores, for simplicity, in this paper, we only report the average F-measure scores generated by ROUGE-1, ROUGE-2, ROUGE-W and ROUGE-SU to compare the implemented systems.
 4) Summarization Evaluation: The experimental results are demonstrated in Table VI and Table VII. From the results, we have the following observations: C. Visualization
To evaluate the term features selected by our methods in document clustering and simultaneous keyword selection, in this set of experiments, we calculate the pairwise document similarity using the top 20 word features selected by differ-ent methods. We use CSTR dataset in this experiment, which contains four classes of text data. We compare the results of our FS-NMF and WFS-NMF algorithms with standard NMF and LSI, and Figure 1 demonstrates the document similarity matrix visually. Note that in the CSTR dataset, we order the documents based on their class labels.

From Figure 1, we have the following observations.
In this paper, we propose the weighed feature subset non-negative matrix factorization, which is an unsupervised approach to simultaneously cluster data points and select
