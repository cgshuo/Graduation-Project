 The so-called cocktail party problem refers to a situation where several sound sources are simul-sources from the measurement of the mixed signals. A standard method of solving the cocktail party problem is independent component analysis (ICA), which can be performed by a class of pow-erful algorithms. However, classical algorithms based on higher moments of the signal distribution [1] do not consider temporal correlations, i.e. data points corresponding to different time slices could be shuffled without a change in the results. But time order is important since most natural signal sources have intrinsic temporal correlations that could potentially be exploited. Therefore, some algorithms have been developed to take into account those temporal correlations, e.g. algorithms based on delayed correlations [2, 3, 4, 5] potentially combined with higher-order statistics [6], based on innovation processes [7], or complexity pursuit [8]. However, those methods are rather algorith-mic and most of them are difficult to interpret biologically, e.g. they are not online or not local or require a preprocessing of the data.
 Biological learning algorithms are usually implemented as an online Hebbian learning rule that trig-gers changes of synaptic efficacy based on the correlations between pre-and postsynaptic neurons. A Hebbian learning rule, like Oja X  X  learning rule [9], combined with a linear neuron model, has been shown to perform principal component analysis (PCA). Simply using a nonlinear neuron combined with Oja X  X  learning rule allows one to compute higher moments of the distributions which yields ICA if the signals have been preprocessed (whitening) at an earlier stage [1]. In this paper, we are s x y the theory of Molgedey and Schuster [4]. We will show that a linear neuron model combined with a Hebbian learning rule based on the joint firing rates of the pre-and postsynaptic neurons of different time delays performs ICA by exploiting the temporal correlations of the presynaptic inputs. 2.1 The problem mixed by a matrix C where x are the mixed signals recorded by a finite number of receptors (bold notation refers to a vector). We think of the receptors as presynaptic neurons that are connected via a weight matrix W to postsynaptic neurons. We consider linear neurons [9], so that the postsynaptic signals y can be written The aim is to find a learning rule that adjusts the appropriate weight matrix W to W  X  (* denotes the value at the solution) so that the postsynaptic signals y recover the independent sources s (Fig 1), i.e. y = P s where P is a permutation matrix with different multiplicative constants (the sources are recovered in a different order up to a multiplicative constant), which means that, neglecting P , To solve this problem we extend the theory of Molgedey and Schuster [4] in order to derive an online biological hebbian rule. 2.2 Theory of Molgedey and Schuster and generalization The paper of Molgedey and Schuster [4] focuses on the instantaneous correlation matrix but also the matrix M ij is symmetric, it has up to n ( n + 1) / 2 independent elements. However, the unknown mixing matrix C has potentially n 2 elements (for n sources and n detectors). Therefore, we need to evaluate two delayed correlation matrices M and  X  M with two different time delays defined as to get enough information about the mixing process [10].
 From equation 1, we obtain the relation M ij =  X  M = C  X   X  C T , we have It follows that C can be found from an eigenvalue problem. Since C is the mixing matrix, a simple algorithmic inversion allows Molgedey and Schuster to recover the original sources [4]. 2.3 Our learning rule In order to understand the putative neural mechanism performing ICA derived from the formalism developed above, we need to find an online learning rule describing changes of the synapses as  X  which defines the weight matrix W  X  at the solution.
 For the sake of simplicity, consider only one linear postsynaptic neuron. The generalization to many postsynaptic neurons is straightforward (see section 4). The output signal y of the neuron can be where  X  is one element of the diagonal matrix  X   X  X   X  1 .
 In order to solve this equation, we can use the following iterative update rule with update parameter  X  . The fixed point of this update rule is giving by (7), i.e. w = w  X  . Furthermore, multiplication of (7) If we insert the definition of M from (2), we obtain the following rule with a parameter  X  given by keep the same norm during iterations of (9).
 The rule 9 we derived is a batch-rule, i.e. it averages over all sample signals. We convert this rule into an online learning rule by taking a small learning rate  X  and using an online estimate of  X  . Note that the rule defined in (10) uses information on the correlated activity x y of pre-and postsy-naptic neurons as well as an estimate of the autocorrelation &lt; yy &gt; of the postsynaptic neuron.  X   X  is taken sufficiently long so as to average over a representative sample of the signals and |  X  | X  1 is a small learning rate. Stability properties of updates under rule (10) are discussed in section 4. A simple example of a cocktail party problem is shown in Fig 2 where two signals, a sinus and a ramp (saw-tooth signal), have been mixed. The learning rule converges to a correct set of synaptic Figure 2: A. Two periodic source signals, a sinus (thick solid line) and a ramp (thin solid line), are mixed into the presynaptic signals (dotted lines). B. The autocorrelation functions of the two source signals are shown (the sinus in thick solid line and the ramp in thin solid line). The sources are normalized so that  X (0) = 1 for both. C. The learning rule with  X  1 = 3 and  X  2 = 0 extracts the sinusoidal output signal (dashed) composed to the two input signals. In agreement with the calcula-The learning rule with  X  1 = 10 ,  X  2 = 0 , converges to the other signal (dashed line), i.e. the ramp, because  X  ramp (10) &gt;  X  sin (10) . Note that the signals have been rescalled since the learning rule recovers the signals up to a multiplicative factor. weights so that the postsynaptic signal recovers correctly one of the sources. Postsynaptic neurons with different combinations of  X  1 and  X  2 are able to recover different signals (see the section 4 on Stability). In the simulations, we find that the convergence is fast and the performance is very accu-rate and stable. Here we show only a two-sources problem for the sake of visual clarity. However, the rule can easily recover several mixed sources that have different temporal characteristics. Fig 3 shows an ICA problem with sources s ( t ) generated by an Ornstein-Uhlenbeck process of the form  X  s by different time constants. The learning rule is able to decouple these colored noise signals with gaussian amplitude distribution since they have different temporal correlations.
 Finally, Fig 4 shows an application with nine different sounds. We used 60 postsynaptic neurons with time delays  X  1 chosen uniformly in an interval [1,30ms] and  X  2 = 0 . Globally 52 of the 60 neurons recovered exactly 1 source (A, B) and the remaining 8 recovered mixtures of 2 sources (E). One postsynaptic neuron is recovering one of the sources depending on the source X  X  autocorrelation at time  X  1 and  X  2 (.i.e. the source with the biggest autocorrelation at time  X  1 since  X  2 = 0 for all neurons, see section Stability). A histogram (C) shows how many postsynaptic neurons recover postsynaptic neurons tuned to time delays, where the autocorrelation functions intersect (D, at time  X  = 3 ms and  X  2 = 0 ), cannot recover one of the sources precisely (E). Figure 3: A. The 3 source signals (solid lines generated with the equation  X  s different time constants, where  X  is some gaussian noise) are plotted together with the output signal (dashed). The learning rule is converging to one of the sources. B. Same as before, but only the one signal (solid) that was recovered is shown together with the neuronal output (dashed). Figure 4: Nine different sound sources from [11] were mixed with a random matrix. 60 postsynaptic to 30ms by steps of 0.5ms and  X  2 = 0 for all neurons. A. One source signal (below) is recovered by one of the postsynaptic neurons (above, for clarity reason, the output is shifted upward). B. postsynaptic neurons recovering each sources. D. Autocorrelation of the different sources. There are several sources with the biggest autocorrelation at time 3ms. E. The postsynaptic neuron tuned to a  X  1 = 3 ms and  X  2 = 0 (above) is not able to recover properly one of the sources even though it still performs well except for the low amplitude parts of the signal (below). In principle our online learning rule (10) could lead to several solutions corresponding to different fixed points of the dynamics. Fixed points will be denoted by w  X  = e k , which are by construction the row vectors of the decoupling matrix W  X  (see (5) and (7)). The rule 10 has two parameters, i.e. the delays  X  1 and  X  2 (the  X   X  is considered fixed). We assume that in our architecture, these delays characterize different properties of the postsynaptic neuron. Neurons with different choices of  X  1 and  X  2 will potentially recover different signals from the same mixture. The stability analysis will show which fixed point is stable depending on the autocorrelation functions of the signals and the delays  X  1 and  X  2 .
 calculation details) where  X (  X  ) ij = &lt; s i ( t ) s j ( t +  X  ) &gt; is the diagonal correlation matrix.  X  This stability relation is verified in the simulations. Fig 2 shows two signals with different autocor-relation functions. In this example, we chose  X  1 = 0 and  X (0) = I , i.e. the signals are normalized. positive learning rate. One of the algorithms most used to solve ICA is FastICA [1]. It is based on an approximation of negentropy and is purely spatial, i.e. it takes into account only the amplitude distribution of the signal, but not it X  X  temporal structure. Therefore we show an example (Fig. 5), where three signals generated by Ornstein-Uhlenbeck processes have the same spatial distribution but different time constants of the autocorrelation. With a spatial algorithm data points corresponding to different time slices can be shuffled without any change in the results. Therefore, it cannot solve this example. We tested our example with FastICA downloaded from [11] and it failed to recover the original sources (Fig. 5). However, to our surprise, FastICA could for very few trial solve this problem even though the convergence was not stable. Indeed, since FastICA algorithm is an iterative online algorithm, it takes the signals in the temporal order in which they arrive. Therefore temporal correlations can in some cases be taken into account even though this is not part of the theory of FastICA. We presented a powerful online learning rule that performs ICA by computing joint variations in the firing rates of pre-and postsynaptic neurons at different time delays. This is very similar to a standard Hebbian rule with exception of an additional factor  X  which is an online estimate of the different sources. Therefore properties varying between one postsynaptic neuron and the next could lead to different time delays used in the learning rule. We could assume that the time delays are intrinsic properties of each postsynaptic neuron due to for example the distance on the dendrites where the synapse is formed [12], i.e. due to different signal propagation time. The calculation of stability shows that a postsynaptic neuron will recover the signal with the biggest autocorrelation at the considered delay time or the smallest depending of the sign of the learning rates. We assume that for biological signals autocorrelation functions cross so that it X  X  possible with different postsynaptic neurons to recover all the signals. Figure 5: Two signals generated by an Ornstein-Uhlenbeck process are mixed. A. The signals have the same spatial distributions. B. The time constants of the autocorrelations are different. C. Our learning rule converges to an output (dashed line) recovering one of the signals source (solid line). D. FastICA (dashed line) doesn X  X  succeed to recover the sources (solid line).
 The algorithm assumes centered signals. However for a complete mapping of those signals online estimate of the mean firing rate and remove this mean from the original rates. This way the algorithm still holds taking neural rates as input.
 Hyvaerinen proposed an ICA algorithm [8] based on complexity pursuit. It uses the non-correlations has been removed. The update step of this algorithm has some similarities with our learning rule even though the approach is completely different since we want to exploit temporal correlations directly rather than formally removing them by a  X  X redictor X . We also do not assume pre-whitened data and are not considering nongaussianity.
 neurons looking at the precise timing of the spikes, i.e. Spike Timing Dependent Plasticity (STDP) [13, 14, 15]. Therefore a spike-based description of our algorithm is currently under study. Appendix: Stability calculation By construction, the row vectors { e k , k = 1,..,n } of W  X  = C  X  1 , the inverse of the mixing matrix, are solutions of the batch learning rule 9 (n is the number of sources). Assume one of these row vectors e T i , (i.e. a fixed point of the dynamic), and consider w = e i +  X  e j a small perturbation in (9) becomes: We can expand the terms on the righthand side to first order in  X  . Multiplying the stability expres-sion by e T j (here we can assume that e T j e j = 1 since the recovering of the sources are up to a multiplicative constant), we find: where  X (  X  ) ij = &lt; s i ( t ) s j ( t +  X  ) &gt; is the diagonal matrix.
 of the form (0,0,...,1,0,...) where the position of the  X 1 X  indicates the solution number 0. Therefore, we have e T i C  X (  X  ) C T e k =  X (  X  ) ik .
 The expression of stability becomes
