 In the knowledge discovery process, clustering is an estab-lished technique for grouping objects based on mutual sim-ilarity. However, in today X  X  applications for each object very many attributes are provided. As multiple concepts described by different attributes are mixed in the same data set, clusters do not appear in all dimensions. In these high dimensional data spaces, each object can be clustered in several projections of the data. However, recent clustering techniques do not succeed in detection of these orthogonal concepts hidden in the data. They either miss multiple con-cepts for each object by partitioning approaches or provide redundant clusters in very similar subspaces.

In this work we propose a novel clustering method aim-ing only at orthogonal concept detection in subspaces of the data. Unlike existing clustering approaches, OSCLU (Or-thogonal Subspace CLUstering) detects for each object the orthogonal concepts described by differing attributes while pruning similar concepts. Thus, each detected cluster in an orthogonal subspace provides novel information about the hidden structure of the data. Thorough experiments on real and synthetic data show that OSCLU yields substantial quality improvements over existing clustering approaches. Categories and Subject Descriptors: H.2.8 Database management: Database applications [Data mining] General Terms: Theory, Algorithms, Experimentation Keywords: data mining, high dimensional data, subspace clustering, orthogonal clustering, multi-view clustering
In the knowledge discovering process, clustering aims at detecting groups of similar objects while separating dissimi-lar ones. Traditional clustering approaches compute a parti-tion of the data, grouping each object in at most one cluster or detecting it as noise. However, it is not always the case that an object is part of only one cluster. Multiple mean-ingful groupings might exist for each object. The detection of such multiple clusters describing different views on each object is still an open challenge in recent applications.
In today X  X  applications, data is collected for multiple anal-ysis tasks. In most cases, databases contain objects specified by very many attributes. As one does not know the hidden structure of the data, one mixes up different measurements in one high dimensional database. Thus, each object can participate in various groupings reflected in different sub-sets of the attributes. For example, in customer segmenta-tion, objects are customers described by multiple attributes specifying their profile. A customer might be grouped by the attributes  X  X verage fruit consumption X  and  X  X port activ-ity X  with other  X  X ealthy people X  having high values in both of these attributes. The same customer might be a  X  X ock Fan X  which could be specified by high values in the attribute  X  X ttendance to rock concerts X  and low values in  X  X ttendance to classic concerts X  (cf. Fig. 1). We observe for each cus-tomer multiple possible behaviors which should be detected as clusters. Thus, clusters may overlap in their clustered objects, i.e. each object may be represented in multiple clusters. Furthermore, each behavior of a customer is de-scribed by specific attributes. Thus, meaningful clusters ap-pear only in these specific subspace projections of the data. While the attribute  X  X ttendance to rock concerts X  is useful for the distinction of musical interests, the attribute  X  X ruit consumption X  is irrelevant for grouping musical interests of customers.

We generalize these observations as they are not only ap-plicable to customer segmentation. In other applications, objects might be sensor nodes represented by multiple sensor measurements, or objects might be genes described by their expression level under multiple conditions. For each of these application scenarios, objects are described by very many at-tributes. For such high dimensional data, all objects seem to be unique in full space as distances grow alike due to the so called  X  X urse of dimensionality X . However, a common obser-vation is that each of the objects might be part of different groups in different subsets of attributes. In general, we call this an object that is part of multiple orthogonal concepts . All of these groupings are valid characterizations of the same objects by using different attributes in orthogonal subspaces . Thus, for the general case of high dimensional data, a con-cept is described by a subset of the dimensions. Hence we can substitute the detection of orthogonal concepts with the detection of orthogonal subspaces and their contained clus-ters. In our example, some healthy and unhealthy customers group together in another subspace and we can detect the or-thogonal concept that represents the customer X  X  taste of mu-sic. The same observation can be made in the other scenarios as well: Genes are controlling multiple functions (concepts) expressed only under specific conditions (relevant attributes for the concept), or sensors are measuring multiple concur-rent environmental events (concepts) specified by different sensor measurements (relevant attributes).
 Detection of Orthogonal Concepts
Formally, a concept is an interpretation of a group of ob-jects considering a set of relevant attributes (specific for this concept). The main characterization of a concept is given by its relevant dimensions as these dimensions provide the attributes in which the objects are grouped together. In Fig. 1 the concept  X  X aste of music X  is described by two di-mensions. A concept can contain several groups that are clearly separated in the relevant dimensions of the concept, like customers loving Rock or customers loving Classic in our previous example. Each object may be clustered by at most one cluster in the same subset of relevant attributes. However, it can be clustered in multiple orthogonal concepts having different sets of attributes. Considering the concept  X  X ealth status X , a  X  X ock Fan X  can be clustered with other customers to form a new grouping. There might exist mul-tiple meaningful groups for each object as it can be inter-preted in multiple different ways. Thus, orthogonal concepts provide for each object different groupings using no or only few shared attributes. Our novel OSCLU (Orthogonal Sub-space CLUstering) approach detects for each object multiple orthogonal concepts. Each detected cluster provides novel information, as we aim at detection of only clusters in or-thogonal subspaces. Thus, OSCLU prunes the detection of similar concepts by ignoring clusters in similar subspace pro-jections of already detected clusters.

Summing up, in our approach we aim at detection of only the orthogonal concepts fulfilling the following properties:
Following these properties, we propose a method for se-lection of orthogonal subspaces by using a similarity mea-sure on subspace projections. Our novel approach OSCLU chooses according to this similarity only the orthogonal sub-space to include novel concepts in this subspace projection into the result set. In addition, we propose a relaxation of the orthogonal subspaces to  X  X lmost orthogonal subspaces X . This generalization allows us to detect concepts sharing a certain amount of common dimensions. The attribute  X  X en-der X  for example could belong to several concepts. Relaxing to almost orthogonal subspaces includes more possible con-cepts in the result.

As each object might be present in multiple clusters, we have to ensure that each cluster adds sufficiently novel in-formation within its concept. Unlike most subspace cluster-ing techniques we prevent redundant information. For this purpose we introduce an interestingness measure for choos-ing only sufficiently distinguishing clusters from similar con-cepts. Furthermore, to select the most interesting clusters, we present an objective function that is based on multiple properties like size, dimensionality and density extracted out of the subspace clusters.
 Using both properties of orthogonal subspaces and most in-teresting clusters, OSCLU performs a global optimization of the result set. It ensures to include overlapping clusters to detect multiple concepts. Furthermore, it prunes similar subspaces and non-interesting clusters to ensure only the meaningful patterns in the result.
Different clustering paradigms have been proposed in the past decades. In this section, we review the main techniques and show their drawbacks in detection of orthogonal con-cepts in high dimensional data. Especially, we show the dif-ferences to our approach as none of the proposed techniques analyzes subspace projections to steer cluster detection in the direction of orthogonal concepts.
 Traditional Clustering
Traditional clustering approaches, aim at the detection of clustered objects using all attributes in the full data space. Several different clustering models, like partitioning or den-sity-based clustering, have been proposed. However, inde-pendent of the underlying clustering model, full space clus-tering approaches do not scale to high dimensional data spaces covering multiple different concepts. As clusters do not appear across all attributes, they are hidden by irrel-evant attributes [4]. Dimensionality reduction like PCA aims at discarding irrelevant, noisy dimensions. However, in many practical applications no globally irrelevant dimen-sions exist.
 Clustering in Subspace Projections
Recent research for clustering in high dimensional data has introduced a number of different approaches summa-rized in [16, 12]. The underlying mining task was named by the pioneers in this field subspace clustering or projected clustering . Their common goal is to detect clusters in ar-bitrary subspace projections of the data. Each cluster is associated with a set of relevant dimensions in which this pattern has been discovered. Differences of clustering ap-proaches in subspace projections have been shown in a re-cent evaluation study [15]. Projected clustering techniques detect disjoint subsets of objects similar to the partitioning approaches [14]. Thus, projected clustering misses to detect orthogonal concepts as it is unable to detect multiple clusters per object. In contrast, subspace clustering allows objects to be part of multiple clusters in arbitrary subspaces. However, due to the exponential number of possible subspaces, it re-sults in a huge number of redundant clusters. Although some recent approaches reduce redundancy [1, 2, 13], they output all maximal clusters as non-redundant results. Hence, simi-larity of the sets of dimensions is not taken into account, and thus, a set of objects might result in being part of clusters with very similar subspaces. Further reduction of the result set, taking similarity of subspaces into account, is essential for effective detection of only the orthogonal subspaces. Orthogonal Clustering
Recent extensions of traditional clustering techniques try to iteratively detect further orthogonal multi-view clusters [7, 17]. In each step these approaches transform the data space and thus force the traditional clustering algorithm to find novel clusters in orthogonalized spaces. In contrast to subspace clustering algorithms, they search for clusters in full space or in space transformations. By transforming the data they pose constraints on the cluster detection. How-ever, transformation hinders the interpretation of results as the original attributes are not directly used for description of detected concepts. Furthermore, due to the iterative pro-cedure, orthogonal clustering typically detects some of the already detected clusters multiple times [7] or is restricted to one alternative clustering for a given set of clusters [17]. Detection of only orthogonal concepts is hindered as redun-dant information does not provide any additional knowledge. Furthermore, restriction to a fixed number of alternative concepts does not provide knowledge about all hidden con-cepts.
 Multi-Source Clustering
In contrast to previous approaches, multi-source cluster-ing does not tackle the challenge of concept detection. One simply assumes to have the knowledge about the concepts provided by different sources [5]. Approaches in this par-adigm try to detect clusters in the given multi-source data which already provide the different views on the data. Thus, in contrast to our approach, they assume they know about the relevant dimensions for each concept. In general, this is not true for orthogonal concept detection as multiple con-cepts can be hidden in one high dimensional data source. Thus, multi-source clustering techniques are not able to de-tect multiple concepts in a single data source. Furthermore, they are also not able to detect concepts spreading across two or more data sources computing new subsets of relevant dimensions.
In this Section, we present our model for the detection of orthogonal concepts in subspaces of high dimensional data. Formally we map our contributions to an optimization prob-lem based on detected subspace clusters in the database. In contrast to subspace clustering, where all clusters are se-lected for the result set, we choose only a subset of most interesting clusters based on orthogonal subspaces. For this we make a distinction between the cluster definition and clustering definition. While the cluster model defines the properties that a set of objects O  X  DB and a set of di-mensions S  X  Dim have to fulfill to be a valid cluster C =( O, S ), the clustering model determines a set of clus-ters M = { C 1 ,...,C n } to be a valid clustering. The valid clustering for traditional subspace clustering is simply the set All that contains all subspace clusters. This set is highly redundant and hence in our model it is not a valid clustering. We want to generate a most informative clustering Opt  X  All so that the clusters in the result set represent the multi-ple concepts of the data without obfuscating this structure by redundant information. As motivated before, each object might be present in multiple clusters if the clusters describe different concepts and each cluster C  X  Opt has to provide novel information within its similar concepts. In short it is not allowed to group the same objects in similar concepts by several clusters. Therefore, we have to define
As a consequence overlapping clusters between different concepts are possible, in contrast to projected clustering. We solely have to check if the same objects are already de-scribed within similar concepts to filter out uninteresting clusters and to steer our cluster detection to the orthogonal subspaces. Thus, in a first step in Section 3.1 we define the notion of (almost) orthogonal concepts, to determine which concepts are similar to a selected one. In Section 3.2 we present the interestingness criterion, that each cluster has to fulfill to be an informative cluster within its similar con-cepts. In Section 3.3 we define our overall model for the optimal orthogonal clustering and show in Section 3.4 how the user can influence the clustering result. In Section 3.5 we prove that solving this model is NP-hard.
The data collected in today X  X  applications, are generated by different concepts which are mixed together. In an opti-mal setting the concepts, described by subspaces, share no dimensions and we can clearly distinguish between them. If we identify a concept in the subspace S all other subspaces T , which share at least some dimensions T  X  S =  X  , are sim-ilar to it and we can prune them. T cannot characterize a different concept because a dimension d  X  S  X  T is already covered by the concept in S and hence T does not detect a novel concept in this scenario. Hence, all subspaces that are similar to S are excluded from further consideration by the identification of S . This can be formalized by: A concept with the relevant subspace T is orthogonal to a concept in S if T/  X  coveredSubspaces 0 ( S ). The dimensions of T and S are disjoint and hence we can detect novel infor-mation in T . So our clustering model only has to identify clusters in subspaces which are orthogonal and prune the already covered subspaces.

However, this orthogonality definition is a too hard restric-tion for our clustering model. Many subspaces are prohib-ited for selection and hence the resulting clustering contains only low information. By definition each dimension appears in at most one concept. However, overlapping concepts are useful and expected in real life scenarios, e.g. the attribute  X  X ender X  in a customer database could appear in multiple concepts. For subspace clustering we need a relaxation of the orthogonality property.

A less hard restriction is realized by the idea of excluding lower dimensional projections of S . The subspace S is more meaningful for the representation of a concept than using the projections which contain fewer attributes. Hence, if we identify S as the relevant subspace for a concept each pro-jection is already described by this subspace. The subspaces similar to S can be defined by: By this definition we can find overlapping concepts, e.g. characterized by S 1 = { 1 , 2 } and S 2 = { 2 , 3 } . Neither of them is similar to the other concept and hence both of them could appear in the result set. This definition is related to the maximality property in other subspace clustering ap-proaches [1, 2], resulting in the same problems. Even if two subspaces share a high fraction of dimensions, e.g. 9 out of 10, they represent different concepts. Thus, similarity of subspaces is not yet modeled in an adequate way.

Our model of almost orthogonal concepts integrates the advantages of both models. We allow overlapping concepts, but we also avoid concepts with too many shared dimen-sions. Thus, we only include (almost) orthogonal concepts in the result and obtain a flexible model by generalizing both definitions to: coveredSubspaces  X  ( S )= { T  X  Dim || T  X  S | X   X   X | T |} with 0 &lt; X   X  1. For  X   X  0 we get the first, for  X  = 1 the second definition.

The idea of our clustering model is to avoid the grouping of the same objects in similar concepts by several clusters. Given a cluster C we have to determine the set of clusters that are in similar concepts. Because we use orthogonal subspaces for the orthogonal concept detection, we can de-termine these clusters by checking if their subspaces cover the subspace of C . We call this set the concept group of C which can be formalized by the following definition. Definition 1. Concept group The concept group of C =( O, S ) with respect to a set of clusters M = { C 1 ,...,C n } is defined as conceptGroup ( C, M )=
The concept group of C =( O, S ) contains all clusters that share at least a  X  -fraction of the dimensions of S . Checking the grouped objects O of C against the objects of its concept group is required to provide novel information within similar concepts. All other clusters, not in the concept group of C , do not need to be considered because they belong to other concepts. We permit such multiple concepts in our result. Let us consider the Figure 2 where the selected cluster C is in the subspace { 2 , 3 , 4 } .For  X   X  0 we have to com-pare C with all clusters in subspaces sharing at least one dimension. C has to group new objects w.r.t. these clusters because they all characterize similar concepts. The higher  X  the less subspaces are considered as similar and hence the more concepts are possible in the final clustering. The choice of  X  = 1 results in comparing C only to higher dimensional clusters C , which project to the subspace of C . For example the concept described by the subspace { 1 , 2 , 3 , 4 } subsumes the concept of C and thus C has to be checked against this subspace. Thereby, we see that the concept group is not symmetric but it tends to include more higher dimensional clusters. The concept group of a low dimensional cluster, that is in general less interesting, usually contains more clus-ters compared to the one of a higher dimensional cluster. Thus, for a low dimensional cluster it is more difficult to provide novel information and consequently to be included in the result set.
After defining the clusters which characterize similar con-cepts as C , we have to ensure that the cluster is interesting enough compared to these clusters. For our resulting clus-tering Opt  X  All , each cluster C  X  Opt has to fulfill this property. According to our motivation a cluster C =( O, S ) has to group new objects within the similar concepts. Hence we use the coverage of objects as a criterion for interesting-ness. For a clustering M = { C 1 ,...,C n } the coverage is defined as:
Because a strict partitioning of the clusters in similar con-cepts is not useful, i.e. we would enforce that each object of C is in no other cluster, we relax this property. We cal-culate the relative fraction of objects which are not covered by other clusters in similar concepts w.r.t. the whole cluster size.
 Definition 2. Global interestingness Given a cluster C =( O, S ) and a set of clusters M = {
C 1 ,...,C n } . The global interestingness of C with respect to M is First, we determine the clusters in similar concepts to the one of C and afterwards their objects are removed from O to obtain the newly covered objects of C . Only if I global ( C, M ) is larger than a given threshold  X  the cluster adds sufficiently new information to this concept.

Figure 3 illustrates this interestingness check. Let us as-sume that M contains the clusters C 7 to C 10 and possible further clusters in other subspaces (not within dimension 3). If we choose C = C 10 the concept group corresponds to {
C 7 ,C 8 ,C 9 } . The remaining clusters are not considered be-cause they represent other concepts. C 10 has to group new objects within the concept. However, most of the objects (29 out of 32) from C 10 are already covered by the other clusters and hence the information obtained by C 10 in this concept is small ( I global ( C, M )= 32  X  29 32 ). For a threshold  X &gt; cluster C 10 is regarded as redundant with respect to M .
The user is able to control the required interestingness of a cluster by variation of  X  . If the fraction of newly clustered objects is smaller than  X  we do not choose the cluster. For the extremal value  X  = 1 the clusters in similar concepts Figure 3: Example for the selection of an optimal orthogonal clustering must not overlap. For  X   X  0 a cluster is selected as long as not all objects are covered by other clusters. Consequently a high overlap is possible.

An important aspect of this model is that the interesting-ness is checked against several clusters within similar con-cepts. Unlike other models [1, 2], that make only a pairwise comparison of the object coverage, in our model all clusters from a similar concept are considered at the same time to evaluate the interestingness of the new cluster. If we did not check against several clusters, the cluster C 10 in Figure 3 would get a misleading high interestingness value. A pair-wise comparison of C 10 to C 7 or C 8 indicates a high fraction of newly clustered objects which is in fact not true.
Let us choose a clustering M  X  All . The global inter-estingness ensures that each cluster C  X  M results in an information gain within its concept by covering new ob-jects. Varying concepts are possible in M and considered by the definition. Thus, the proposed properties for a good clustering, mentioned at the beginning of the Section, are guaranteed.
 Definition 3. Orthogonal clustering The clustering M = { C 1 ,...,C n } is orthogonal iff
The clustering M = { C 1 ,C 2 ,C 7 ,C 8 ,C 9 } in our example from Fig. 3 is an orthogonal clustering, while the clustering M  X  X  C 10 } is not. However, the proposed definition alone is not yet sufficient to determine an optimal clustering Opt All . Several clusterings could fulfill the definition, e.g. the trivial clustering M =  X  . The user wants to get an overview of the clustering structure and seeks for the most informative clusters. We have to ensure that these clusters are selected.
While the global interestingness I global ( C, M )alwaysrates the cluster C with respect to a clustering M we now assess the interestingness of the cluster C on its own. This so called local interestingness should correspond to the user-specific notion of interesting clusters. Formally we have to define a function I local which maps each cluster C to the value I local ( C ). This function could include different aspects, as the dimensionality or the size of the clusters. A discussion of this function is presented in Section 3.4.

Both, the global and local interestingness are used to de-fine our optimal orthogonal clustering. With the global property we ensure that only informative clusters within similar concepts are selected. At the same time we want to maximize the sum of the local interestingness for the re-sulting clusters. By maximizing the local interestingness we get the most interesting clusters but also as many inter-esting clusters as possible (taking the orthogonal clustering constraint into account).
 Definition 4. Optimal orthogonal clustering ( OOC ) Given the set All of all possible subspace clusters, a cluster-ing Opt  X  All is an optimal orthogonal clustering iff with
In Figure 3 we show an overall example with  X  =0 . 5 and  X  =0 . 5. The clustering M 1 = { C 1 ,C 2 ,C 7 ,C 8 ,C 9 } orthogonal clustering, because each cluster covers a suffi-cient amount of new objects within its concept. Although C 1 and C 9 contain similar objects the overlap is permit-ted because different concepts are realized. The clustering M 1  X  X  C 10 } for example is not valid, because as shown in our previous example I global ( C 10 ,M 1 )= 32  X  29 32 &lt; X  . Ob-viously each subset of M 1 is also an orthogonal clustering but less informative than M 1 . Hence these subsets cannot be optimal clusterings. If we assume that the user is more interested in high dimensional clusters and chooses I local accordingly, the sum C  X  M 1 I local ( C ) would be maximal out of all orthogonal clusterings. Another orthogonal clus-tering like M 2 = { C 1 ,C 2 ,C 10 ,...,C 13 } which contains the one-dimensional projections from the second concept would therefore result in a lower value for the sum. As a con-sequence M 1 is preferred over M 2 and M 1 is the optimal clustering in this example.

Our model provides a selection of only interesting clusters in different and novel concepts. An overwhelming result size is prevented. As we use subspace clusters in our model, the interpretabiliy of the result set and the identification of the relevant attributes for each concept are guaranteed. Unlike other orthogonal clustering models we keep the original di-mensions and we use them for the orthogonality check. We steer the cluster selection to orthogonal subspaces.
Before we present our local interestingness function we set up our cluster definition. We use density-based clus-tering because it detects arbitrary shaped clusters even in noisy data [8]. The idea is to define clusters as dense areas separated by sparse areas. The density density S ( p )ofan object p in a subspace S is the number of objects in its  X  -neighborhood around it. To identify clusters based on this density we follow the definition from [10], with the mod-ification that the  X  -range is adjusted according to the di-mensionality of the subspace. Therefore, we adapt the opti-mal bandwidth for density estimation [18] to our clustering model. The value of  X  in a subspace with dimensionality d the  X  -range in the 1d subspace, n the database size and  X  the gamma function.

With the cluster definition we can define our user-specific local interestingness function. Three main properties char-acterize a subspace cluster C =( O, S ) in our cluster in-stantiation. The dimensionality | S | , the size | O | and the density. A very dense cluster shows small variation in the attribute values of the relevant dimensions and hence is more interesting than a sparse cluster. We use the mean density | p  X  O density S ( p ) over all objects within the cluster for this criterion.

Maximizing all measures at the same time is in general not possible, e.g. low dimensional clusters are usually larger than high dimensional clusters. Therefore our local interest-ingness function subsumes all measures and gives the user the flexibility to weight the measures dependent on the ap-plication. The local interestingness function used in our ex-periments is with C =( O, S ) and a + b + c =1.
In this Section we prove the NP-hardness of our optimal orthogonal clustering problem ( OOC ). For this we reduce the NP-complete SetP acking problem [9] to our model, i.e. SetP acking  X  P OOC . Given several finite sets O i the SetP acking problem seeks for the maximal number of dis-joint sets.
 Theorem 1. Computing OOC (Def. 4) is NP-hard.
 Proof.
 We show that SetP acking  X  p OOC .
 A. Input mapping: Each set O i is mapped to the cluster C i =( O i , { 1 } ). Furthermore we set  X   X  [0 ,..., 1],  X  =1 and I local ( C )= | S | (cf. Section 3.4, a =1 ,b = c =0). B. OOC generates a valid SetP acking solution: 1) The concept group contains all clusters: 2) Each orth. clustering M contains only disjoint sets: 3) Opt contains maximal number of such disjoint sets: (2) and (3)  X  Opt is a valid SetP acking solution  X  OOC is NP-hard
The optimal orthogonal clustering has global properties which increases the computational complexity. As we have already proven the problem is NP-hard and hence we can-not expect that an efficient algorithm exists. Furthermore, we cannot generate the huge set of all subspace clusters All in a first step and select the optimal subset afterwards. We develop an approximation algorithm (OSCLU) that incre-mentally adds further clusters to the result set. For efficient calculation we integrate the clustering process into the con-cept and cluster selection process. This means that not all clusters in all subspaces are generated but many subspaces are pruned based on already detected concepts/clusters. An important question is which subspaces should be clustered first and hence which clusters should be added at the begin-ning to the result set to prune many other subspaces.
Traditional bottom-up approaches that start with the low dimensional clusters are not useful for pruning based on our global interestingness criterion. As already mentioned the concept group of a cluster contains mainly higher dimen-sional clusters (cf. Fig. 2). Thus, a low dimensional cluster has to compare its object coverage against more clusters than a high dimensional cluster. A low dimensional clus-ter is more likely to be excluded from the result set than a high dimensional cluster. For this reason we use a top-down approach to add clusters to the final clustering.
Our algorithm, summarized in Algorithm 1, comprises three major contributions to avoid clustering of all sub-spaces. First we develop a ranking of the subspaces (all with the same dimensionality) without clustering them (lines 4-6). The ranking accounts for the similarity of the current subspace with already detected concepts. The greater the number of already detected similar concepts, the less inter-esting is the subspace. In a second step the ranking considers the possibility for a good clustering in a subspace based on efficient estimation. After ranking the subspaces we use the first subspace for clustering (line 8). If clusters were iden-tified we incrementally update the result set (line 10). We have to consider the global interestingness so that redun-dant clusters are not selected. Furthermore, a high local interestingness of the selected clusters should be ensured. Resorting the ranking and the possible pruning of further subspaces (line 11) based on the new clusters is performed in order to push novel concepts to the top. If all subspaces with the dimensionality dim are pruned or selected for clus-tering we decrease the dimensionality to realize the top-down approach.
 Algorithm 1 OSCLU (Orthogonal Subspace CLUstering) 1: result set M :=  X  ; 2: find initial dimensionality dim ; (Sec. 4.3) 3: WHILE( dim &gt; 0 ) 4: rank and prune subspaces based on (Sec. 4.1) 5: 1) subspace orthogonality score 6: 2) subspace quality score 7: WHILE( ranking not empty ) 8: choose best subspace for clustering; 9: IF (clusters found) 10: update result set M ; (Sec. 4.2) 11: resort ranking and prune; 12: dim = dim -1; 13: return result set M ;
As an additional step we present an efficient method that approximately identifies the highest dimensionality (of a sub-space) in which clusters are expected (line 2). This avoids to start our ranking in the full-space, where clusters are in general not present.
Clustering each subspace is not efficient since many sub-spaces can be pruned because of similar concepts that are already detected. We use two techniques to rank subspaces without clustering. The aim is to cluster only interesting and orthogonal subspaces. In our first approach we use the similarity of already discovered concepts for pruning and ranking. The greater the number of similar subspaces in the result set the higher is the possibility that new clusters in the current subspace cover the same concept and hence provide no novel information. We define the orthogonality score of a subspace S w.r.t. the current result set M as orthogonality score ( S, M )= |{ T  X  Dim | S  X  coveredSubspaces  X  ( T )  X  X  X  ( O, T )  X  M }| The definition is similar to the concept group, but only con-siders the subspaces. The higher the score the worst is a subspace because many similar concepts are already in the result set. The orthogonality score is the first criterion for ranking. Furthermore, all subspaces with a score greater than maxOrth are removed from the ranking. This param-eter can be controlled by the user and intuitively defines how detailed a concept is analyzed.

During the algorithm the result set M changes and hence the orthogonality score does so too. By this only the most informative subspaces are top ranked and hence are clus-tered. The clustering is concentrated to the orthogonal and novel concepts.

Our second approach makes use of subspace search [6, 11] for measuring the quality of subspaces. Usually subspace search is a stand-alone technique for identifying interesting subspaces. Each subspace is mapped to a quality value, a high value corresponds to a high possibility for a good clus-tering structure. Our trick is to use this technique within the clustering task. We guide our algorithm to cluster only the most interesting subspaces based on the calculated qualities. Therefore our ranking is extended such that all subspaces with the same orthogonality score are secondly ranked based on this qualities. In total, our ranking concentrates not only on novel concepts but also on high quality subspaces.
The subspace search method within our framework is eas-ily exchangeable and we can use techniques like RIS [11] or ENCLUS [6]. For efficiency reasons we develop an own technique which is based on our density-based cluster defi-nition (cf. Sec. 3.4). To have clusters in a subspace several objects must have a high density according to our density-based clustering model, i.e. for an object p the value of density S ( p ) is large. We use a strategy that randomly se-lects points and calculates their mean density. This method is efficient and a good indicator for the existence of clusters. Let Seeds be the set of randomly selected points, the quality score is then defined as
The higher the quality the better the subspace. As for the orthogonality score we introduce a minimum score minQual that each subspace has to fulfill to be maintained.
After ranking the subspaces based on the two scores, we select the first one and we cluster it according to our model. We get a list of resulting clusters New . We now have to check which clusters C  X  New should be included to our result set M . In a first step we analyze the global interest-ingness of the new clusters. For each cluster C  X  New we calculate I global ( C, M ). We distinguish two cases.
If I global ( C, M )  X   X  we directly add the cluster to M , i.e. the new result set is M := M  X  X  C } . The cluster C adds sufficiently new information. By this we ensure that in each step of the algorithm only informative clusters are selected. Please note that this procedure is a relaxation of Def. 3. We do not check the global interestingness of the remaining clusters in M which could be changed by selection of C . This recalculation would be too costly. However due to our top-down approach higher dimensional clusters are added first to M and these clusters are rarely removed by low dimensional clusters. Additionally, within the same dimensionalities our ranking tries to rank the best subspaces on top and hence these clusters are selected first.

If I global ( C, M ) &lt; X  we do not reject the cluster immedi-ately but we perform an additional improvement heuristic. We want to maximize the local interestingness in our model hence we check if it is possible to remove some clusters from M such that C is afterwards globally interesting and the sum of the local interestingness is increased. The algorithm which decides if C is included and which subset of M should be removed is presented in Alg. 2. First, we rank the clus-ters from conceptGroup ( C, M ) in decreasing order based on their local interestingness values. Second, we select the most interesting clusters which do not result in redundancy for C (set N ). The clusters which induce the redundancy are stored in R . At the end it holds that I global ( C, M \ i.e. C provides novel information with respect to the new set. If the local interestingness of C is greater than the one of R it is better (for maximizing the interestingness) to se-lect C and remove R from the result set M . The new result setisthen M := ( M \ R )  X  X  C } . Otherwise C is rejected and the set M remains unchanged.
 Algorithm 2 Cluster selection procedure 1: &lt;C 1 ,...,C n &gt; := ranking of conceptGroup ( C, M ) 2: N :=  X  , R :=  X  3: FOR( i:1 ...n ) 4: IF( I global ( C, N  X  X  C i } )  X   X  ) N := N  X  X  C i } 5: ELSE R := R  X  X  C i } 7: add C to M and remove R from M
By the incremental result construction we add only infor-mative clusters to our set and additionally try to maximize the interestingness of all selected clusters.
In general, full dimensional clusters are not identified in high dimensional databases. If we started our top-down ap-proach in full space we would analyze many uninteresting subspaces which are filtered out by our quality score crite-rion. For an efficiency boost we identify the first layer with interesting subspaces based on the idea of binary search. We start in the half-dimensional space (e.g. 5 in a 9d database) and use our subspace search estimator to calculate the qual-ities. If we identify a subspace with sufficiently high quality we directly jump up to the dimensionality between half and full space (e.g. from 5 to 7). If no interesting subspaces are found we accordingly jump to lower dimensional spaces (e.g. 5 to 3). For this new dimensionality we repeat the  X  X heck-and-jump X  procedure (with a half jump range) until we identify the highest dimensionality with interesting sub-spaces. This corresponds to the binary search procedure.
Overall, our algorithm comprises three contributions to obtain a good approximation of the optimal orthogonal clus-tering. The binary search technique supports the top-down approach by an efficient initialization. The ranking of sub-spaces yields a preference of orthogonal and interesting sub-spaces. By recalculating the ranking further subspaces can be pruned without clustering and novel concepts advance to the top. At last, the meaningful selection of new clusters to M results in an informative clustering. In the next Section we confirm this with an experimental analysis.
We evaluate the quality and efficiency of the OSCLU ap-proach compared to three variants of orthogonal clustering techniques ( Multi-View 1 and Multi-View 2 proposed in [7], and Altern. Clus. [17]), a recent non-redundant subspace clustering technique ( StatPC [13]) and a projected cluster-ing approach ( P3C [14]). For fair comparison we used a re-cent evaluation framework [15], additionally reimplemented both Multi-View approaches in this framework and used the original implementation for the alternative clustering [17]. Furthermore, for all algorithms we tried to find the optimal parameter settings for each data set.

In general, we perform our evaluation on data with mul-tiple hidden concepts. For both synthetic and real world data, we extend single concept data used in traditional clus-tering approaches such that each object is part of multiple concepts. Thus, for a high quality clustering each object has to be detected in multiple clusters. While traditional clustering approaches are well suited for data with only one hidden concept, we compare our approach against recent techniques designed for multiple hidden concepts. For scal-ability experiments, we generate synthetic data following a method proposed in [10, 2] to generate density-based clusters in arbitrary subspaces. In addition, our generator takes into account that objects can belong to multiple concepts. Thus, for each object we concatenated attribute values of different subspace clusters to a higher dimensional space with mul-tiple hidden concepts per object. Further on, we show the performance of OSCLU on two extended real world data sets (original iris and liver disorders are provided by the UCI repository [3]). We use the class labels in these data sets as one hidden concept of the data. In addition, we created multiple concepts per object by randomly concatenating ob-jects of different classes, resulting in one high dimensional data set.

To ensure comparability of evaluations, we measure run-times on identical machines with 2.33GHz Intel XEON CPU, 2 GB of main memory and JAVA 1.6 runtime environment. Furthermore, for comparable quality measurements we use the F 1 value that is used in evaluation of subspace and pro-jected clustering [2, 13, 14, 15]. In our case, it computes for each hidden cluster the harmonic mean of recall ( X  X re all objects of the hidden cluster detected? X ) and precision ( X  X ow accurately is the cluster detected? X ) values, respec-tively. Therefore, each hidden cluster is evaluated against one of the detected clusters which provides the highest F1 value. The F1 value of the whole clustering is simply the average of the F1 values for each hidden cluster.
Database size. In Figure 4(a) we analyze the quality of the clustering results with respect to the database size. While increasing the number of objects, we keep the num-ber of concepts fixed to three. We generate concepts with five relevant attributes such that overall we obtain a 15-dimensional data space. Our OSCLU algorithm yields the highest quality compared to all other algorithms, as we de-tect all hidden clusters in various concepts. The quality of OSCLU is independent of the database size and very robust. StatPC and P3C show good quality results, but also high fluctuating values which cannot reach the quality of OS-CLU. All three orthogonal clustering approaches, show only low and decreasing quality with respect to the database size. Their underlying k -means model tries to partition the data in each iteration of orthogonal cluster detection. Thus, it cannot cope with the fixed noise ratio in the data which is always assigned to some of the detected clusters and hence resulting in low quality clusterings.
The runtime with respect to the database size is presented in Fig. 4(b). The slopes of all curves are in the same range and the influence of the size on all algorithms becomes ap-parent. The two top quality approaches, our OSCLU model and StatPC, result in similar runtimes. Our redundancy checks and also our density-based model are very complex, but these aspects account for the high quality. The remain-ing algorithms are faster but we believe, that our runtime is still acceptable considering our high quality results. Es-pecially with increasing concept number, as presented in the next experiment, our model outperforms all other ap-proaches.
Number of concepts. The aim of our model is the de-tection of multiple concepts, which arise in real scenarios. Thus, in the next experiment we analyze the performance of the algorithm by increasing the number of concepts hid-den in a database. To scale the number of concepts, we use a simple data set with only 1000 objects as most of the algorithms showed comparable quality values in this range in the previous experiment. We vary the number of hidden concepts in Figure 5 from 1 to 5.
We show that OSCLU is able to detect clusters even if objects cluster in multiple concepts. It shows high quality even for a high number of hidden concepts. While tradi-tional clustering approaches aim at clustering single concept data, the alternative clustering approach is designed for two concepts and the multi-view approaches should detect even more than two. However, even these approaches cannot compete with our model. For the subspace and projected clustering approaches, increasing number of concepts makes it very hard to detect the hidden clusters. Especially the projected clustering approach P3C shows decreasing qual-ity, as each object belongs to at most one concept. Overall, StatPC and P3C are not able to detect the multiple hidden concepts per object, while OSCLU yields very high cluster-ing quality.

Noise percentage. In the previous experiments we showed that we outperform subspace and projected clustering ap-proaches as they cannot cope with multiple hidden concepts. Thus, in the following experiments we focus on a more de-tailed comparison of OSCLU against the orthogonal cluster-ing techniques detecting multi-view and alternative cluster-ings. First we analyze the effect of noisy data especially for high concept numbers. For the next experiment, illustrated in Figure 6, we generate data with five hidden concepts and vary the noise percentage. On such a difficult data setting, our OSCLU approach outperforms the other techniques. It can detect the clusters hidden in different concepts even in very noisy data sets. Both multi-view algorithms and the alternative clustering approach show again decreasing qual-ities.
As we aim at detection of multiple concepts we focus our evaluation for real world data also on scalability w.r.t. num-ber of concepts. We use single concept data obtained by UCI repository and extend them to multi concept data sets. As described in the experiment set-up, similar to synthetic data we can vary the complexity of data sets by including more and more hidden concepts. However, in contrast to the pre-vious experiments we use real world data distribution for the single concepts. We evaluate the effect of variable concept counts on the clustering quality, as for an increasing number of concepts it is more difficult for all algorithms to identify the hidden structure of the data. Figure 7: Quality on extended real world data sets with increasing number of concepts
In Figure 7 we show clustering quality on iris and liver dis-order data set. For the very simple case of only one concept (original UCI data sets), the quality is high for all algo-rithms. However, for increasing number of hidden concepts, quality dramatically drops for all competing approaches. Es-pecially, quality of the alternative clustering approach drops with more than two concepts, as it is designed for up to two concepts only. OSCLU shows significantly better per-formance as it keeps quality high, outperforming the com-peting approaches for multiple concept data. Although we set for higher number of concepts the optimal parameter value k such that the number of found clusters corresponds to the number of hidden clusters, the competing approaches are not able to detect all hidden concepts. Thus, our OS-CLU approach clearly outperforms all competing algorithms even for increasing number of concepts per object.
Additional to the experiments that compare OSCLU to existing methods, we analyze the flexibility of our model. As presented in Sec. 3.2 the user can control the output by changing the required interestingness. In Figure 8, we present the variation of the parameter  X  which controls the interestingness of each cluster in the result set. As intended by this parameter, higher values of  X  increase the required interestingness and hence less clusters are in the result. By varying the  X  parameter one can control the overall result set based on our global interestingness I global . We include a cluster only if the fraction of its newly clustered objects is at least  X  (cf. Def. 3). Furthermore, our OSCLU algorithm is not only able to detect orthogonal concepts, but in addi-tion it is very flexible by using a local interestingness I It allows the user to control the output dependent on the application or the current interestingness.
We introduced the OSCLU (Orthogonal Subspace CLUs-tering) approach. It overcomes major drawbacks of exist-ing approaches in the detection of multiple concepts hid-den in arbitrary subspace projections of the data. Our novel clustering model detects multiple concepts per ob-ject. It computes an optimal orthogonal clustering by en-suring non-redundancy and maximal interestingness of the resulting clustering. We show that our clustering model is NP-hard and propose an efficient approximative algorithm. We approximate the optimization problem by pruning simi-lar subspaces ensuring efficient cluster detection in only the orthogonal subspaces. Thus, our OSCLU approach is the first method for detection of multiple orthogonal concepts in subspaces of high dimensional data. It provides novel in-formation about the hidden structure of the data as each detected concept is described by orthogonal subspaces while OSCLU prunes redundant concepts. Thorough experiments demonstrate that OSCLU clearly outperforms existing sub-space clustering and orthogonal clustering algorithms while automatically reducing the output to only the orthogonal concepts hidden in the data.
This research was funded in part by the cluster of excel-lence on Ultra-high speed Mobile Information and Commu-nication (UMIC) of the DFG (German Research Foundation grant EXC 89). Furthermore, we thank the authors of [17] for providing us with their original implementation. [1] I. Assent, R. Krieger, E. M  X  uller, and T. Seidl. DUSC: [2] I. Assent, R. Krieger, E. M  X  uller, and T. Seidl. INSCY: [3] A. Asuncion and D. Newman. UCI machine learning [4] K. Beyer, J. Goldstein, R. Ramakrishnan, and [5] S. Bickel and T. Scheffer. Multi-view clustering. In [6] C.-H. Cheng, A. W. Fu, and Y. Zhang. Entropy-based [7] Y. Cui, X. Z. Fern, and J. G. Dy. Non-redundant [8] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [9] M. R. Garey and D. S. Johnson. Computers and [10] K. Kailing, H.-P. Kriegel, and P. Kr  X  oger. [11] K. Kailing, H.-P. Kriegel, P. Kr  X  oger, and S. Wanka. [12] H.-P. Kriegel, P. Kr  X  oger, and A. Zimek. Clustering [13] G. Moise and J. Sander. Finding non-redundant, [14] G. Moise, J. Sander, and M. Ester. P3C: A robust [15] E. M  X  uller, S. G  X  unnemann, I. Assent, and T. Seidl. [16] L. Parsons, E. Haque, and H. Liu. Subspace clustering [17] Z. Qi and I. Davidson. A principled and flexible [18] B. Silverman. Density Estimation for Statistics and
