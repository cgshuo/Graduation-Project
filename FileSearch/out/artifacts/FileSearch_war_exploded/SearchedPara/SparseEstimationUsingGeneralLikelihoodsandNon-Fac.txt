 of this problem is given by, where 2 R n m is a matrix whose columns signal vector . The cost function being minimized represents the ` solv e the alternati ve problem for the optimal representation requires the solution of up to m selection.
 One common strate gy is to replace k x k ` To implement this procedure, at the ( k + 1) -th iteration we compute Lasso estimator [14]. Ho we ver, given an appropriate selection for g ( ) , e.g., g ( x Lasso in approximating the solution of (1) or (2) [3].
 While certainly successful in practice, there remain fundamental limitations as to what can be achie ved using factorial penalties to approximate k x k rog ates for k x k tive reweighted ` Bayesian learning) and then deri ve a new iterati ve reweighted ` there will always exist cases where performance impro ves over standard ` scheme can never do worse than Lasso (assuming w (0) promising non-f actorial variant by starting with a plausible ` backw ards to determine the form and properties of the underlying penalty function. In general, iterati ve reweighted ` such as x contains empirical comparisons while Section 6 pro vides brief concluding remarks. learning (SBL) [15], which is based on the notion of automatic rele vance determination (ARD) where 2 R m the coef cients x and then performing what is commonly referred to as evidence maximization or type-II maximum lik elihood [10, 15]. Mathematically , this is equi valent to minimizing where using : Note that if any therefore sparse, with nonzero elements corresponding with the `rele vant' features. where assuming = and j x j , [ j x penalty function that only need have = to obtain equi valence with SBL; other selections may lead to better performance (more on this in Section 4 belo w).
 The analysis in [19 ] reveals that replacing k x k whereby all local minima are smoothed away [21]. Note that while basic ` has no local minima, the global minimum need not always correspond with the global solution to (1), unlik e when using g SBL ( x ) .
 conca vity , (6) can be optimized using a reweighted ` factorial case) using weights, Step II -Repeat until con vergence can be sho wn that a more rudimentary form of reweighted ` ally since it scales as O nm k x ( k +1) k intensi ve than the subsequent ` From a theoretical standpoint, ` Before proceeding, we dene spark() as the smallest number of linearly dependent columns in [5]. It follo ws then that 2 spark() n + 1 .
 Theor em 1. When applying iterati ve reweighted ` Theor em 2. Assume that spark() = n +1 and consider any instance where standard ` tion fails to nd some x dra wn from support set S with cardinality jSj &lt; ( n +1) with f W ( k +1) updated using (9), always succeeds but standard ` to the ` schemes. The follo wing is one such possibility .
 x a solution with k x k region. This is a very undesirable property since there are on the order of m BFS, where k x k favors degenerate BFS. We can accomplish this by constructing a reweighting scheme designed to avoid non-de generate BFS whene ver possible.
 construct weights using the projection of each basis vector weights to the zero-v alued x Cand es et al. [3] which uses w ( k +1) lar gest weight is only assigned when the associated coef cient goes to zero and k x ( k +1) k The reweighting option (10), which bears some resemblance to (9), also has some very desirable conte xt of reweighted ` with (10) is Moreo ver, because each weight w or inner -loop optimization as when using (9). One of the moti vating factors for using iterati ve reweighted ` incorporate alternati ve lik elihoods and priors. This section addresses three such examples. x compute the associated cost function or update rules no longer have closed-form expressions. and neuroimaging [22 ]. In this situation, we are presented with r signals Y , [ y that were produced by coef cient vectors X , [ x notation that x reco very problems (1) and (2) then become where d ( X ) , P m is a natural extension of the ` iteration j x ( k ) efcient matrix update analogous to (3) requires the solution of the more complicated weighted second-order cone (SOC) program Other selections such as the ` sparse logistic regression we would solv e where now y by iterati vely solving an ` does not require that we compute the full reweighted ` lend themselv es to an efcient partial (or greedy) update before recomputing the weights. can be used in the outer -loop. When (if) a x ed point is reached, the corresponding classier coef cients are chosen as the mode of p ( x j y ; ) .
 [4, 11]. There is currently no exibility in the model to remedy this problem. possible. Of course we can always substitute the reweighted ` conca ve favoring sparsity , while in the limit at becomes lar ge, it acts lik e a standard ` or (14). Both and can be tuned via cross-v alidation if desired.
 research; preliminary results are promising. A signal is then computed using y = x . We then attempted to reco ver x by applying non-negative ` g BU ( x ) ; and nally (iv) g ( x ) = represents the current state-of-the-art in reweighted ` Given w (0) ` ber . We observ e that standard non-ne gative ` efcac y of non-f actorial reweighting and the ability to handle constraints on x . For the second experiment, we used a randomly generated 50 100 dictionary for each trial with iid Gaussian entries as abo ve, and created 5 coef cient vectors X = [ x sparsity prole and iid Gaussian nonzero coef cients. We then generate the signal matrix Y = X the harder the reco very problem becomes. A total of ve algorithms modied to the simultaneous sparse approximation problem were tested using an ` sistent with the results in Figure 1. Other related simulation results are contained in [20]. p (success) reweighted ` models and minimized using efcient iterati ve reweighted ` more principled algorithm for minimizing it.
 and non-decreasing with respect to 0 , we can express it as where h ( z ) is dened as the conca ve conjug ate of h ( ) , log j I + T j [1]. We can then express g SBL ( x ) via
Minimizing over for x ed x and z , we get
Substituting this expression into (16) gives the representation for x ed z , solving (6) is a weighted ` To deri ve the weight update (9), we only need the optimal value of each z analysis will satisfy and z . We start by initializing z 1 = 2 compute the optimal z for x ed , which can be done analytically using By substituting (17) into (20) and dening w ( k +1) pre viously , only one iteration is actually required for the overall algorithm. Proof of Theorem 1 : Before we begin, we should point out that for ! 0 , the weight update can be set to zero for all future iterations. Otherwise w ( k +1) Moore-Penrose pseudoin verse and will be strictly nonzero.
 each subset of n columns of forms a basis in R n . The extension to the more general case is two possibilities. If k x ( k ) k next iteration regardless of f W ( k ) . In contrast, if k x ( k ) k evaluations of (9) with ! 0 , enforcing k x ( k +1) k Proof of Theorem 2 : For a x ed dictionary and coef cient vector x , we are assuming that scaling constants y = x 0 and x 0 ( i +1) i x 0 ( i ) , i = 1 ; : : : ; k x k 0 1 , the minimization problem Let x ` 1 , arg min each spark() &lt; n + 1 and to any x that satises k x k 0 &lt; spark() 1 . The more specic case addressed abo ve was only assumed to allo w direct application of [9, Theorem 6]. [1] S. Bo yd and L. Vandenber ghe, Con vex Optimization , Cambridge Uni versity Press, 2004. [3] E. Cand es, M. Wakin, and S. Bo yd,  X Enhancing sparsity by reweighted ` [5] D. Donoho and M. Elad,  X Optimally sparse representation in general (nonorthogonal) dictio-[7] B. Krishnapuram, L. Carin, M. Figueiredo, and A. Hartemink,  X Sparse multinomial logistic [10] R. Neal, Bayesian Learning for Neur al Networks , Springer -Verlag, Ne w York, 1996. [14] R. Tibshirani,  X Re gression shrinkage and selection via the Lasso,  X  Journal of the Royal [20] D. Wipf and S. Nag arajan,  X Iterati ve reweighted `
