
Xerox Innovation Group, Xerox Corporation, Rochester, NY, USA School of Computer Science, Florida International University, Miami, FL, USA 1. Introduction 1.1. Multi-way data ( tensor ) Multi-way data or tensors are generalizations of matrices and they can be represented as X  X  R of dimensions in each mode respectively. When m = 3, X is a three-way data as shown in Fig. 1 with three modes: data units, features, occasions. The c -th frontal slice of the three-way data is X clustering, the dataset can also be represented as a three-way dataset as author  X  terms  X  time . 1.2. Clustering multi-way data functions. Many clustering methods have been developed to cluster two-way input data matrices where clustering on three-way data, although the discussion can also be extended to multi-way data.
One way to cluster three-way data is to convert the three-way data into two-way matrices, either by datasets [1].
 their component matrices. Tensor factorization methods are mainly data reduction or compression methods. We note that there are generally two types of tensor decomposition models including: (1) Rank-1 Decomposition: Rank-1 decomposition can be thought as a multi-linear form of decomposition three vectors [27]. (2) Tucker Decomposition: The model, including HOSVD (Higher Order Singular Value Decomposition [ 31,34]) and 2DSVD [10], can be thought as the multi-wa y principal component These multi-way models have also been applied to many applications such as web link analysis [21], does not require any parameter tuning. Our approach simultaneously and iteratively fi nds subspaces which are the linear combination of original data dimensions. TriCluster discovers the subspaces by the most variances of data. 1.3. Adaptive subspace iteration
Developing effective clustering methods for high dimensional datasets is a challenging problem due at a speci fi c set of occasions.

For clustering two-day data matrices, there are many approaches to address the dimensionality curse by making the cluster structures more obvious in the lower dimensional spaces. One simple way is to perform dimensionality reduction techniques such as principal component analysis (PCA) and random clusters along the mixture of features [12]. Recently, many subspace clustering algorithms, where to be effective for clustering high dimensional two-way data matrices. 1.4. Organization of this paper
Despite signi fi cant progress made on subspace clustering for two-way data, few attempts have been models only deal with the subspace selection (data reduction) problems. In this paper, we propose a Subspace Iteration on Tensor). ASI-T model is a special version of HOSVD model. We show that the clustering algorithm is also equivalent to K-Means clustering and 2DSVD. More speci fi cally, ASI-T consists of two simultaneous steps: select the subspaces using 2DSVD (identifying the subspace data.

Our contributions can be summarized as follows:  X  Propose a new tensor factorization method, ASI-T, which is a special version of HOSVD with the  X  Show that ASI-T is simultaneously performing subspace selection by 2DSVD and data clustering  X  Show that ASI-T is an extension of two-way subspace clustering to three-way data.
A preliminary version of the work was presented as a 2-page poster at the ACM 17th Conference on Information and Knowledge Management [26]. In this journal submission, we included detailed Section 6 shows our experimental results on both synthetic three-way data and real-world three-way data. Finally Section 7 concludes. 2. An overview on tensor factorization models 2.1. Notations vectors are denoted by boldface lowercase letters, e.g. x ,wherethe i -th entry is x by boldface capital letter, e.g. X ,wherethe i -th row of matrix X is x X ,andthe ( i, j, l ) -th entry is x the last mode of X fi xed at l ,and X mode. The Kronecker product  X  is the operation between two matrices such that Kronecker product of of two entries from A and B respectively. X The notations used in the paper are summarized in Table 1. 2.2. Overview of factorization models
PCA and SVD: PCA and SVD are two widely used methods in matrix dimensionality reduction. PCA aims to fi nd the subspace projection of the data X by minimizing subspace. SVD minimizes the objective function as following is a diagonal matrix with the fi rst k corresponding eigenvalues alone the diagonal. Rank-1 Decomposition: The objective function for Rank-1 decomposition can be written as interactions). three-way data X by minimizing
HOSVD: HOSVD (High Order SVD) is also called Tuc ker3 tensor model w ith orthogona lity con-straints on components. Not like 2DSVD that does not compress the third mode, HOSVD treats every mode uniformly. It minimizes Parafac model (the sum of rank-1 tensors).
 ASI-T Model: ASI-T model is a special version of HOSVD model. It minimizes means there is only one entry with value 1 in each row, and other entries are all 0 s. 3. ASI-T model
In this section, we provide insights on ASI-T model. In particular, we show that (1) ASI-T is simultaneously performing subspace selection by 2DSVD and data clustering by K-Means; and (2) ASI-T is an extension of two-way subspace clustering to three-way data. 3.1. K-Means clustering and 2DSVD
ASI-T model is not only a particular version of HOSVD, but also a simultaneous process of K-Means data units in a iterative process.
 and 2DSVD on subspace identi fi cation, we fi rst formalize K-Means using matrix notations. K-Means clustering minimizes the within-clusters deviance by following cluster indication matrix that d Theorem 1. ASI-T is equivalent to simultaneous K-Means clustering and 2DSVD.
 Proof: The objective function in Eq. (4) of ASI-T can be transformed into Let Eq. (6) can be written as ASI-T is a model performing both Eqs (7) and (8). Note that Eq. (8) is the same as the K-Means and Z where p =1  X  X  X  k 2 , q =1  X  X  X  k 3 , F T F = I ,and G T G = I . This leads to a formulation of 2DSVD approximation as shown in Eq. (2). Thus ASI-T clusters data units using K-Means and compresses data using 2DSVD simultaneously. 3.2. Adaptive subspace clustering
As shown in Section 3.1, our ASI-T model simultaneously performs the following two steps: data reduction by 2DSVD and data clustering by K-Means. ASI-T can also be viewed as an extension of the dimension reduction is decoupled from the clustering process. Many adaptive subspace clustering approaches, where the subspace selection (or dimension reduction) and clustering are integrated in Formally many adaptive subspace clustering approaches PCA and K-Means clustering simultaneously to minimize the cluster centroid matrix.
 subspace for matricized three way data de fi ned by Kronecker product of G and F ,where G and F can with all ones. Y computed as the solution of the multivariate regression problem such that is projected cluster centroids on subspace ( G  X  F ) . 4. Algorithm description others iteratively until convergence. 4.1. Algorithm derivation can be written as Note that || X maximize || MX the subspace. In the following, we will specify how to update F , G ,and D .
 following where X taking the fi rst k 2 eigenvectors of X Update G : To update matrix G while fi xing the others, Eq. (12) can be written as where X taking the fi rst k 3 eigenvectors of X
Update D :Let Z = Y be written as independently such that where r =1 ,  X  X  X  ,n . 4.2. Algorithm procedure will monotonically decreases. Algorithm 1 lists the detailed algorithm steps. 5. An illustrating example the better clustering quality, we present an example in Table 2 to illustrate its advantage. and the other four authors (A5, A6, A7, and A8) belong to another area Data Mining .Wehave discriminating words of their own area. Thus the information in P4 is misleading. We observe that  X  We can convert the three-way dataset into a two-way dataset and then apply traditional clustering  X  The correct clustering result of this example can be obtained by using ASI-T. Moreover, ASI-T is  X  Running other two-way subspace clustering algorithms or co-clustering algorithms on the matrices  X  In addition, ASI-T can obtain more meaningful and interpretable clustering results by discovering 6. Experiments on both synthetic data and real-world data. 6.1. Dataset description 6.1.1. Synthetic data
The purpose of using synthetic data is that the detailed cluster structures are known, hence we can in some generated range. Thus cluster dimensions and noise dimensions are independent from each dimension of this cluster. The number of points that are assigned to each cluster are determined as following: From the range [ a, a + d ] where a, d &gt; 0, we choose k numbers { P 1 ,  X  X  X  ,P k is the number of clusters. Then, the number of points belonging to the cluster i is n P i of noise dimensions in mode 2 and mode 3. The last column provides the number of clusters. 6.1.2. Real data
The real datasets are extracted from the DBLP record fi le that can be downloaded at http://www. experiments. These researchers and their publications are divided into 9 different research areas: Learning , Networking ,and Natural Language Processing based on authors X  major activities in these areas. These different areas will serve as the ground-truth labels for our experimental comparison purpose. We conduct experiments on two such three-way datasets, one of which named DBLP1000 contains 20 who are randomly chosen from 1000 authors in 4 areas Data Mining , Software Engineering , Theory , in Table 4. The reason why we use both DBLP100 and DBLP1000 for experiments is that the clusters are more well-separated in DBLP100 compared to DBLP1000. Then we can have an insight on how different clustering algorithms perform differently on these two types of datasets. 6.2. Evaluation measures In order to compare the clustering performance, we use Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and Accuracy(ACC) as our performance measures. These measures would provide good insights on how the clustering results agree with the true label.

Normalized MutualInformation measures howclustering results share the information with the ground-between [0 , 1] . The NMI of the entire clustering solution is computed as:
TheRandIndexisde fi ned as the number of pairs of objects that are both located in the same cluster where a = both the class i and cluster j , V the labels.

Accuracy (ACC) discovers the one-to-one relationship between clusters and classes and measures the represented as: where C belong to class m are assigned to cluster k . Accuracy computes the maximum sum of T ( C clustering performance. 6.3. Comparison methods
The clustering performance of subspace clustering algorithm ASI-T is compared with a wide range of clustering algorithms:  X  Tensor factorization methods: We compare ASI-T with Rank-1 approximation method and HOSVD,  X  Two-way data clustering methods:  X  ClusterAgg: Run K-Means clustering on each frontal slice of the three-way array, and combine
We expect these comparisons would provide us with enough insights on the performance of ASI-T. In 6.4. Experimental results 6.4.1. ASI-T on synthetic data clustering performance of KMeans(sum), KMeans(ext), KMeans(pca), Rank-1, HOSVD, ClustserAgg, and ASI-T based on ARI, ACC and NMI on 5 synthetic datasets are shown in Table 5, and the results based on ACC and NMI are plotted in Fig. 2. The values of the best clustering performance based on each measure is highlighted with the bold number in Table 5. We observe that the best performance values are achieved by ASI-T among all these clustering methods with all measures on all synthetic when noise dimensions are added into the data. KMeans(sum) is better than KMeans(ext) since the and HOSVD obtain almost the second worst clustering performance. This is because they may lose labels could be generated. 6.4.2. ASI-T on real-world data
ASI-T on both DBLP datasets (DBLP100 and DBLP1000) takes within 30 iterations to converge. We measure is also highlighted with the bold number in Table 6. We observe that clustering performance of ASI-T on DBLP100 is among the best, although it does not obtain the best performance value in ARI measure. However, it achieves the obviously better clustering performance than other algorithms on DBLP1000 in all measures. In higher dimensions the advantages of ASI-T become more evident because the three-way subspace clustering of ASI-T can overcome the curse of dimensionality .Forthe same reason, KMeans(pca) and three co-clustering algorithms perform relatively better on DBLP1000 compared to their clustering p erformance on DBLP100. KMeans(ext) is still the worst clustering on DBLP1000.
 clusters that are obtained from B matrix by running ASI-T on DBLP100. We can see that Cluster 1 and Clustering 4 to Theory . The associated degrees of these words are decided by the corresponding in Cluster 3,  X  X ining X  and  X  X ata X  in Cluster 2.

The clustering results obtained from matrix C by running ASI-T on DBLP100 give the insight of how patterns in each year are correlated with each other. We found that 2004, 2005, 2006, and 2007 are have different properties. 1991, 1992, 1993, and 1994 tend to have smaller author clusters and word clusters, and contain more narrowly distributed words in each cluster. However, the words are more in these years are more and more active and interdisciplinary.

ASI-T computes the principle eigenvectors fromthe kroneckerproductofmatrices and itis computation 3.2 GHZ CPU, 4 GB RAM, and running Windows XP. One of future work is to scale our ASI-T model for large scale datasets. 7. Conclusion and future work few attempts have been made to develop subspace clustering algorithms on multi-way data. This paper effectiveness of ASI-T.
 relationships between the tensor factorization with probabilistic models.
 Acknowledgement The work is partially supported by NSF grants DMS-0915110 and CCF-0830659.
 References
