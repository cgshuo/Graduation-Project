 Recent years have witnessed an explosive growth of social networking on the Inter-net. Increasingly more social activities are expanding from face-to-face interaction to emailing, chatting, and posting in cyberspace. A significant advantage of an online social network service is that it transcends the barriers of geography. As a result, commercial websites such as Facebook, MySpace, and Orkut attract millions of users.
One of the most popular online social activities is sharing media such as images, videos, and blogs with others. Beyond the conventional social circle of friends, family members, and colleagues, many users choose to share their media assets publicly on media-sharing websites such as Flickr and YouTube. It has been observed that massive social links between users have been constructed based on their common interests on certain types of media. Specifically, users have formed social subcommunities, which are referred to as  X  X ocial groups X  or  X  X pecial interest groups, X  in media-sharing websites to share media collections and discuss information related to their common interests.
The common interest among group members leads to different stages of social inter-action between users. (1) Users first select and contribute their own media collections to the shared pool in (2) The other group members may view and comment on the shared media collections. (3) Based on their common interest and previously mentioned interactions, users can
It is clear that sharing media in suitable groups opens a new way to greatly expand one X  X  social network. As a result, the formation of groups has gained great popularity and attracted enormous number of users [Chen et al. 2008 ; Negoescu and Gatica-Perez 2008; Wang et al. 2009]. The major problem of the media-sharing process is that a user must manually assign each media in his/her collection to the appropriate group, which requires matching the subject of each media with the topics of various interest groups. This work is tedious and often prohibits users from exploiting the relevant group resources.

In this article, we propose new methods to integrate state-of-the-art visual con-tent recognition, novel collection context analysis, and active learning methods to make accurate group suggestions for each image in a user collection. An overview of the proposed methods is illustrated in Figure 1. Our system begins with analyzing the similarity between social groups and then clusters them into categories. Machine learning techniques are employed to produce initial group prediction for each image based on visual content and textual annotation analysis. We further leverage the re-lationship between the images in the same user collection to improve the prediction via a proposed technique called collection-based sparse label propagation. Further-more, an active learning scheme is readily facilitated by the sparse graph to select the most influential samples for relevance feedback, which could maximize the prediction enhancement without the need for retraining the classifier.

The remainder of the article is organized as follows: Section 2 reviews the related research work in the literature and theoretically highlights the contributions of our method. Section 3 introduces the automatic group categorization process, which es-timates the group similarity, selects the optimal cluster numbers, and clusters the group into relevant categories. In Section 4, initial group suggestions for each image are predicted independently based on image content and user annotations. Section 5 elaborates on leveraging the collection context to improve suggestion accuracy via the proposed collection-based sparse label propagation and the related relevance feed-back scheme. Finally, experiments on real-user image collections are designed and the performance of the new methods is compared with the state of the art in Section 6. Discussion on the proposed methods and future work conclude the article in Section 7. In the area of multimedia data mining and content analysis, tremendous efforts have been spent on categorizing and annotating images [Fan et al. 2005; Goh and Li 2005; Za  X   X ane et al 1998; Wu et al. 2009a]. However, direct inference from pure visual content remains a very challenging problem due to the well-known  X  X emantic gap X  between low-level visual features and high-level semantic concepts. The main reasons are twofold: (1) the relation of the visual content and semantic concepts is too complicated for most machine learning methods, and (2) the high-level semantic concepts may relate to certain knowledge or information that can be difficult or impossible to extract or infer using visual models alone. In this section, we give a brief review of the state-of-the-art techniques that are related to our research work and highlight the advantages of our proposed methods. In order to fill the semantic gap, researchers have proposed the use of contextual information, such as image annotations, capture location, and time, to provide more insight beyond the image content [Hays and Efros 2008; Pedro and Siersdorfer 2009; Joshi and Luo 2008; Yu and Luo 2008], Wang et al. [2008] proposed a new distance measure for image ranking by using both the image content and user annotations. Their method is trained on a large collection of web image data set and demonstrated superior performance overall content-based and text-based retrieval systems. Torralba et al. [2007] construct a web image data set of over 80 tiny images that relate to the nouns from WordNet. Using these images as the training set, they successfully built an image classification system using simple methods such as KNN. Jia et al. [2008] categorizes web image collections based on the surrounding text and uses them as training data for personal album annotation. One major drawback of using web images as training samples is that the annotations are often noisy and incomplete; Sorokin and Forsyth [2008] tried to overcome this problem by using Amazon Mechanical Turk to collect more reliable annotations from users. Google Image Labeler provides an interactive interface for users to annotate images online. Weinberger et al. [2008] built a probabilistic model and used KL divergence to find the ambiguous tags in the annotations. Li et al. [2008] introduces a new method to leverage the annotations from different users to discover tag relevance. Overall, manual inputs are still largely insufficient and impractical compared with the amount of web images on the Internet. Instead of enriching image annotations, visual content analysis is also employed in refining text-search results. Jing and Baluja [2008] proposed a visual rank method that combines text-based Pagerank and visual matching among retrieved images using local features.

Besides textual context information, geographical location also provides an extra cue about the image content. Yu et al. [2008] proposed a method to leverage the time and location information related to an image for content recognition. The basic idea is to statistically model the relationship of visual object and season-location context, for example,  X  X now is more likely to occur in photos taken in a Northeastern winter than in Florida-Caribbean areas. X  Their method unifies the low-level visual detection and season-location context in a probabilistic framework and shows significant im-provement over the baseline. Luo et al. [2008] exploited the precise GPS coordinates associated with geo-tagged images for event inference. Their method automatically extracts the satellite images corresponding to the consumer photos taken in the same region and uses them as an extra modality to improve event classification. Special interest groups attract increasingly more attention from the research commu-nity. Wang et al. [2009] demonstrated that Flickr group membership helps infer the subject of images. Their method shows that the group associations of images can be used as an inexplicit measure of visual similarity. Negoescu and Gatica-Perez [2008] conducted extensive statistical analysis on the relationship between image tags and groups. They further proposed to cluster the groups using user annotations. Chen et al. [2008] tried to recommend groups to images from a text search perspective. Their sys-tem first predicts the related query term for an image. Then it uses the query term for text searching and recommending the relevant groups. In information retrieval society, research works on recommendation systems usually focus on textual informa-tion [Chen et al. 2009; Jin et al. 2005] and cannot handle the semantic gap problem for multimedia data. To the best of our knowledge, little work has been reported on automatically suggesting groups based on both visual content and text annotations of images. In the last few years, researchers began to leverage the similarity between images for improving content understanding. Local Neighborhood Propagation ( LNP )[Wang and Zhang 2008] is a powerful semi-supervised machine learning technique, which assumes that data reside on a locally linear hyperplane. It propagates the labels of the training samples to the unlabeled ones by linear reconstruction analysis. For out-of-sample queries, it learns their labels from a linear combination of both labeled and unlabeled samples in a local neighborhood. Tang et al. [2008] extended the LNP to a kernel version and applied it on video categorization. It is worth noting that LNP and its variants are essentially instance-based learning techniques and could not use existing classifiers X  predictions directly. Considering the dissimilarity between samples from different classes, Cao et al. [2008] proposed a Bayes Label Propagation method. Their method selects samples with initial prediction confidence above a certain threshold as seeds and propagates their labels to neighbors from  X  X ame class X  and  X  X ifferent classes. X  Such neighborhood information is inferred using the distance distributions of data pairs from same and different classes, respectively. The potential problems of this approach are that selecting the right seeds requires fine-tuning the threshold and inferring the class difference from distance is equivalent to another classification problem, which is nontrivial [Yu et al. 2008]. Also aiming at modeling the dissimilarity, Tong and Jin [2007] proposed a technique called Mixed Label Propagation, which tries to preserve the similarity among samples from same classes and the dissimilarity between samples from different classes. However, their method is designed for two-class classification problems and cannot be easily extended to handle multiple class problems. Duan et al. [2009] also tried to utilize personal collection information by modeling personal style as an additional hidden variable in Coupled Probabilistic Latent Semantic Analysis. Their method works on multi-label problems and needs sufficient labels to build the personal style model, which is different from the multi-class problem that our work triestotackle. Active learning techniques have been widely adopted to boost the performance of image understanding and retrieval algorithms. Generally, it aims to find data that would maximize the retrieval performance with limited relevance feedback from users. Xu and Akella [2008] proposed a Bayesian logistic regression method for document retrieval, which selects text terms that reduce the original term space the most. Similar to our work, Joshi et al. [2008] designed a new strategy (BvSB) to select samples whose best and second-best category predictions are close. It outperforms widely used Entropy measure in several image categorization tasks. It is worth mentioning that existing active learning techniques require retraining the underlying classifiers by selecting the most informative data based on uncertainty sampling. Compared with the state-of-the-art techniques, our methods are novel in the following aspects. (1) To the best of our knowledge, our approach is the first to exploit user image collection information for social group recommendation. Unlike other instance-learning based method such as LNP [Wang and Zhang 2008] and K-NN, our method can obtain the initial prediction using any suitable classifiers, which is much more powerful. (2) The proposed collection-based sparse propagation algorithm leverages sparse coding techniques to exploit the similarity among images from the same user X  X  collection and improve prediction accuracy. In collection modeling, our approach does not assume a local linear plane and avoids the selection of neighborhood size K as in Wang and Zhang [2008], which is an open problem in practical applications. In label propagation, our method does not need to select seed samples as in Cao et al. [2008], which avoids parameter tuning and is more robust to outliers. (3) Our method also performs the propagation within the test data in the same col-lection while the other methods require storing all training samples and propagating labels from all of the training data to all the test data. Therefore, our method dramat-ically reduces both time and space complexity. (4) The new active learning method exploits both the relationship between samples within the same collection and their prediction uncertainty in a unified framework. Combined with the sparse label propagation technique, it achieves comparable per-formance to conventional relevance feedback schemes without retraining the group prediction classifiers. To the best of our knowledge, there is no public data set of user media collections and their group associations. Therefore, we designed an automatic process to collect real user images and its associated group information from Flickr.com, which hosts more than 2 billion images and 600,000 special interest groups.

In this article, a total of 16,145 user images from 24 groups are collected. The collected images and groups are related to popular visual concepts such as  X  X ortrait, X   X  X nimal, X  and  X  X rchitecture, X  similarly as in Chen et al. [2008]. Note that the images are from 727 unique users and, on average, each user shares more than 20 images in the groups, which shows the active user involvement in sharing among group members.
 In the rest of the article, an image is denoted as x . A group g hosts a set of images. Groups can be further clustered into semantic categories c , which will be elaborated in Section 3.3. Many social groups cover similar subjects/events and users tend to contribute an image to all of the groups that are relevant to its visual content. Specifically, among the images we collected, 25.2% of the images are shared by more than one group. Clearly, it would be desirable to cluster the groups into semantically meaningful categories.
In this article, our group clustering method begins with efficient evaluation of the similarity between groups. Because analyzing the visual similarity between images from two groups is computationally expensive, we employed SimRank [Jeh and Widom 2002] to infer the group similarity from the tag sets that annotate the images in the corresponding groups. Basically, we formulate the groups and annotations as two sets of nodes in a bipartite graph. Thus, one can use SimRank to estimate the similarity between groups and tags in the following way: we consider two groups are similar if they are annotated by similar tags and two tags are similar if they annotate similar groups . Mathematically, the similarity score for objects a and b in a homogeneous network is defined in the following way, Specifically, S ( a , b ) = 1for a = b and S ( a , b ) = 0for I ( a ) constant between 0 and 1, I ( a ) is the set of nodes linked to object a , the set, and I i ( a )isthe i th object in I ( a ).

To model the relationship between groups and tags, we build a bipartite graph , which consists of two sets of nodes: groups G and tags T . Each node represents either a group or a tag. We can add an undirected link between a group node g and a tag node t if t appears in the annotations of any image in g .
 To compute the similarity, for each pair of groups g and g X  ,wehave Similarly, for each pair of tags t and t X  ,wehave where I ( g ) is the set of tags that belong to group g , I ( t ) is the set of groups that are annotated by tag t and m indicates the iteration number.

By initializing the similarity between any pair of nodes as 0, one can iteratively update the similarity score for any pair of groups using (2) and (3) with the theoretical guarantee on converge [Jeh and Widom 2002].
 With the similarity scores between groups, one can categorize similar Flickr groups automatically using clustering algorithms such as Verma and Meila [2003]. However, selecting the optimal number of clusters N c remains an open problem for general clustering applications. In the group suggestion task, we found that users tend to share/contribute their images to all of the semantically related groups, which provides extra information for finding an optimal number of clusters.

Ideally, the groups should be clustered into categories that correspond to distinct semantic concepts. Therefore, the groups in the same cluster should share many images that are all related to the same semantic concept. Intuitively, a within-cluster sharing score S w c for a clustering scheme is defined as follows: where | g  X  g | is the number of images shared by two groups g and g X  and number of groups in the i th cluster c i .

On the other hand, the groups in different clusters should share few images when the clusters represent different concepts. Similarly, a between-cluster sharing score S can be defined as follows: Clearly, it is desirable to select the cluster number that maximizes the within cluster image sharing score and minimizes the between cluster image sharing score. Mathe-matically, the criterion can be formulated as follows: By applying a multi-way cut on the similarity matrix between groups, we partition them into N c clusters. Using (4), we select the number of clusters as 11 and obtain the group categories that contain the following semantic concepts, respectively: Flower, Animal, Architecture, People, Seashore, Night, Sky, Sunrise/Sunset, Red, Green, and Black/White . Figure 2 plots the number of images and the number of users contributing images to each category. Experiment in Section 6.1 provides a detailed explanation of the similarity estimation and clustering process. In this work, we extract four popular visual descriptors, tiny image [Torralba et al. 2007], color histogram, GIST [Oliva and Torralba 2001], and CEDD [Chatzichristofis and Boutalis 2008], to represent the image in visual feature spaces. (1) Tiny Images. Down-sampled (or tiny) images are trivial but useful features. Tiny (2) Color Histograms. Using CIE L  X  a  X  b color histograms has been very popular in (3) GIST Descriptor. GIST descriptors encode structural information and have suc-(4) CEDD Descriptor. Color and Edge Directivity Descriptor (CEDD) describes both User annotations have been proven to facilitate many content analysis applications [Ames and Naaman 2007]. Statistical analysis, such as probabilistic Latent Semantic Indexing (pLSI) [Hofmann 1999] and Latent Dirichlet Allocation (LDA) [Blei et al. 2003], have been used successfully in related areas. Different from classical methods in natural language processing, they model the words in articles as being generated by hidden topics. Recently, researchers have applied these methods in the image un-derstanding area to analyze user annotations alone [Negoescu and Gatica-Perez 2008]. Inspired by the above work, we adopt the idea of LDA [Blei et al. 2003; Griffiths and Steyvers 2004] to extract the hidden topics and produce a compact topic representation of the user annotations.

For each image I , its annotation set can be considered as a document d . Each word w in d is modeled as being generated by hidden topics Z
For a collection of images, their annotation set D shares T topics ( z corpus of unique words { 1 ,.., W } . In pLSI [Hofmann 1999] and LDA [Blei et al. 2003] models, P ( w | z ) is modeled as a T multinomial distribution of words while P ( z )isa D multinomial distribution of documents: Using an EM algorithm, pLSI directly solves for  X  and  X  [Griffiths and Steyvers 2004]. However, it has two drawbacks: (1) for a new document, the estimated parameters may not be suitable and (2) the EM process can suffer from local maxima and slow convergence.

LDA tries to provide a complete generative solution for the topic model. It further models P (  X  ) as generated by a Dirichlet process Dir (  X  often involves approximation methods such as variational inference.

Usually the number of topics | T | in the documents is much fewer than the number of words | W | . Using the topic assignment of annotations, we can represent the document in a compact and semantically meaningful topic space. The posterior distribution of topic assignment for each word can be obtained by maximizing the following term: If we assume the distribution of  X  is from another Dirichlet process Dir ( and  X  do not need to be explicitly estimated. We use the Gibbs sampling method to discover topics in the annotation sets of the images [Griffiths and Steyvers 2004]. Compared with other approaches, it is more memory and time efficient. The estimated topic assignments for each word in one annotation set form a T -dimension vector, which represents the image in the topic space. As we mentioned in Section 3, users often contribute images to all of the groups in the related group category. Accordingly, it would be desirable to suggest all the groups in the category that matches the visual content of the user image. Thus, the social group suggestion task is converted to a classification problem.

To leverage the complementary information in the extracted visual and textual features, three popular fusion strategies are considered in the classification process: (1) Feature-level fusion requires concatenation of features from both visual and tex-tual descriptors to form a monolithic feature vector, (2) Score-level fusion often uses the output scores from multiple classifiers across all of the features and feeds them to a meta-classifier, (3) Decision-level fusion trains a fusion classifier that takes the pre-diction labels of different classifiers for multiple modalities. In the category prediction task, feature-level fusion is preferred over the other two mainly because it uses only one layer of classifiers and, consequently, does not need extra computational power to handle the meta-classifier as the other two options. It is worth mentioning that using advanced fusion strategies such as Wu et al. [2009b] may further improve the prediction accuracy in our framework.

Theoretically, any classification method can be plugged into our framework to learn the subjects of different group categories. We empirically compared the performance of four state-of-the-art classifiers, K-NN, Support Vector Machine, Boosted Tree, and Random Forest, and, eventually, used SVMs to generate initial category prediction because of its superior performance. In this work, our motivation for leveraging the collection context arises from the fol-lowing two observations and quantitative analysis. (1) The images from the same user X  X  collection share much stronger similarity than those from different users X  collections. To quantitatively evaluate the difference, we compare the average Euclidean distance for image pairs from the same user and different users in each category in Figure 3. Clearly, images from the same user have smaller distances than those from different users. The underlying reason for this phe-nomenon may be that the users may shoot similar subjects, events, or have consistent personal preferences. (2) Similarly, we found that the digital photos from one user are more likely to focus on a few image categories rather than being diversified over all categories. In Figure 4, quantitative analysis on the 717 users in our data set verifies our intuition. It is clear that most of the users contribute their images to fewer than three categories. In accordance with these observations, it would be desirable to design a new method to leverage the similarity of images from the same user ollection and adaptively refine their category predictions. 5.2.1. Modeling the Collection with Graph Representation. Graph representation has been successfully applied in many data mining and machine learning applications [Belkin and Niyogi 2002; Roweis and Saul 2000; Wang and Zhang 2008]. As data points X are represented as graph nodes, the intrinsic relationship between data is depicted by graph edge W . Several techniques such as LNP [Wang and Zhang 2008], Locally Linear Embedding [Roweis and Saul 2000], and Laplacian Eigenmap [Belkin and Niyogi 2002], try to preserve the similarity between samples by finding the optimal W that minimizes the reconstruction loss: The common assumption of these techniques is that the data comply with a certain distribution in a local neighborhood, which is defined by K nearest neighbors or the neighbors whose distances are less than threshold  X  . However, such an assumption may be invalid and the selection of K and  X  remains an open problem.

To overcome these problems, we aim to find a sparse W , that is, one sample is re-constructed by a relatively small number of other samples in the data set. The proposed method has three advantages: (1) Recent findings in neural science suggest that the human vision system interprets images based on the sparse representation of the visual features [Rao et al. 2002]; (2) Theoretically, the sparse W does not make the local distribution assumption and provides an interpretive explanation of the correlation weights; (3) Practically, the shrinkage of coefficients in combining predictors often improves prediction accuracy [Hastie et al. 2003]. Although solving for the sparsest W is NP-hard, it can be approximated by this convex l 1 -norm minimization: or where  X  and s are constants that regulate the sparsity of W .

Considering our observation in Section 4.1, we further constrain that the samples should only be reconstructed by using similar ones in the same user X  X  collection. There-fore, we obtain our collection-base graph representation from the following optimization problem:
Compared with the original minimization problem, our objective function is more suitable for mining social media for two reasons. (1) Our solution for W only involves the images from the same user X  X  collection while the original form needs to calculate W based on all samples. Clearly, our method is much more efficient. (2) The similar images from the same user X  X  collection are more likely to be seman-tically correlated than images from different users. Therefore, the W we learned is more robust compared with the one learned from the noisy samples outside the user X  X  collection.

Solving the optimization Eq. (12) forms a quadratic programming problem. This op-timization problem could be solved by existing algorithms, such as LASSO [Tibshirani 1996] and modified Least Angle Regression [Efron et al. 2003]. 5.2.2. Sparse Label Propagation. Let us denote a classification process as F and its initial group suggestion for image set X as Y 0 . Then, we have: It is reasonable to assume that similar images from the same user X  X  collection should have similar predictions. Therefore, the predictions among images from the same user X  X  collection are propagated in the following iterative process: Note that is a diagonal matrix with where y 0 i , j is the initial prediction of sample x i for class j .

The definition in (14) indicates that describes the confidence of the original pre-diction. Therefore, W and jointly regulate that the refined prediction can be learned from visually similar samples with confident prediction. The final prediction for images would be refined by iterating Eq. (13) until convergence. Table I further describes the complete algorithm. Given the iterative process described previously, we prove that the final prediction converges to (1  X  )(1  X  W )  X  1 Y 0 .
 From Eq. (14), we can infer that: Considering 0  X   X  i , j ,w i , j &lt; 1, when t approaches infinity, we have It follows that Therefore, the final prediction is guaranteed to converge as shown in (19). In many image categorization tasks, the classifiers are trained on a labeled data set X and make predictions on the user collection X u . The system may select a few samples and asks the user for labeling. Conventional methods add the relevance feedback ( RF ) to the training pool and retrain the classifiers. It has been demonstrated that relevance feedback can significantly improve the performance in many documents and image retrieval tasks [Rui et al. 1998]. However, labeling many samples for relevance feedback is impractical due to the human effort involved.

As discussed in Section 3, most of the active learning techniques select data based on uncertainty sampling for relevance feedback and require retraining the classifiers. However, the relationship between samples within the same collection has not been fully exploited and the retraining process is often computationally expensive. To over-come these limits, we extend the collection-based sparse label propagation framework to incorporate active relevance feedback . Our method selects the most informative and influential samples, which maximizes the prediction enhancement from user feedback, without the costly retraining process.

Suppose the user provides feedback that one sample r is from class l , we denote the current prediction as Y t , change in prediction matrix as RF
Evidently, the new regulation matrix has the same elements as regulation matrix except the i th row, which is updated in the following way: Without retraining the classifier, we can propagate the new label to the rest in the collection: Intuitively, we try to select the optimal sample for relevance feedback, which maximizes the change in the refined prediction. Such an optimization problem can be formulated as follows: P ( r | l ) is the probability of that sample r is from class l and can be approximated by the prediction confidence of the classifier: The optimal sample for relevance feedback can be determined using (22) in O(N where N x is the number of test images and L is the number of categories.

If multiple samples are needed simultaneously for relevance feedback, maximizing the joint probability as in Eq. (22) become intractable. To solve that problem, we further propose a greedy selection algorithm in Table II. It iteratively finds the samples for relevance feedback based on the most likely labels that the samples might be assigned to in relevance feedback.
Group Similarity Estimation. As introduced in Section 3, we tried to infer the group similarity by analyzing the annotations and group associations via SimRank. To make the computation faster and more space efficient, we only keep scores that are more than 0.01.

In our experiments, we set  X  t and  X  g in Eqs. (2) and (3) as 0.8. Figure 5 shows the resulting group similarity matrix (rescaled for better visualization), which exhibits very clear cluster patterns along the diagonal.

Selecting Number of Clusters. Using the similarity matrix, categories of similar groups can be automatically inferred given the number of clusters N we choose the single-linkage clustering algorithm because of its robustness to initial-ization [Jain et al. 1999]. To select the optimal number of clusters, we try to minimize the intercluster overlap and maximize intracluster overlap between groups, which is defined by the score in (4). By computing the clustering scores for all possible cluster numbers, the results are obtained as in Figure 6. It is clear that setting the cluster number at 11 gives us the best performances.

Further evaluation indicates that the selected number of clusters obtained 11 cate-gories that contain highly correlated groups, as one can tell from the group names in each category in Table III.

After the categorization, we found that 8% of the original images are shared by more than one category and may contain vague subjects. The remaining 92% of the images form a nontrivial data set of 14,559 images, which is used in the rest of the experiments. To evaluate the performance of our methods, the experiments are conducted on the 14,559 images from 11 disjointed categories. The ground-truth information about the group category of each image is determined by the group category to which the owner of the image assigned it.

The unique property of our research is that we try to leverage user collection infor-mation. Therefore, instead of randomly selecting images from the entire data set, we construct our training set by randomly selecting 200 users and all of the images in their collections. The rest of the images are used for testing. All of the experiments are repeated 50 times.

Averaged suggestion accuracy is employed as the performance measurement. Basi-cally, the group suggestion for a test image is considered as correct if the ground-truth group assignment for the image is within the predicted category. Therefore, the predic-tion accuracy can be calculated in the following way: The curse of dimensionality has always been a critical problem for many vision and learning tasks. Directly concatenating all visual features into a long vector may lead to poor statistical modeling and extra computational cost. In order to bypass that problem, we try to preserve the most useful information in the four descriptors by projecting each of them to a 100-D space via Principal Component Analysis (PCA). A new PCA feature is constructed by concatenating the four projected features into a 400D vector.
In this experiment, we compare four original features and the PCA feature for the group suggestion task. Their performances are evaluated by the prediction accuracy of the corresponding SVM classifier built upon each feature.

Results in Table IV suggest that CEDD gives the best performance among the four original features. It is mainly because it incorporates both color and edge histogram information. The combined PCA feature performs the best because of the integration of all four visual features. It also requires much less computation time compared to high-dimensional features such as Tiny-Image, GIST, and Color Histogram. In this experiment, we further evaluate the performance of integrating visual and textual features as discussed in Section 4. The annotations of images can be mapped to a topic space using Latent Dirichlet Analysis. The parameters The number of topics | T | is set to 100 in the rest of experiments because using more topics would increase the computational complexity without significant performance improvement. Both the visual and textual features are concatenated to represent the image in a 500-D feature space. SVM classifiers are trained using visual features, textual features, and the combination of both. The results in Figure 7 indicate that the two modalities contain complementary information and mining the integrated representation can boost the performance. Theoretically, the proposed Collection-based Sparse Label Propagation (CSP) can improve the prediction of any baseline classifiers. To evaluate its performance, we implemented four baseline classifiers, that is, K-NN, Boosted Tree, Random Forest, and SVM. To verify the effectiveness of the sparsity constraint on W ,wealsoimple-mented a non-sparse variant of CSP, where the W is obtained using Eq. (9) instead of (10). Figure 8 shows that the performance of baseline classifiers, with CSP and its nonsparse version. Clearly, the suggestion accuracy is improved when the collection context is leveraged via CSP. Compared with the nonsparse variant, the sparsity con-straint leads to superior performance of CSP, which is consistent with the observation in pervious publications such as Hastie et al. [2003]. We further compared our method with Linear Neighborhood Propagation ( LNP ) be-cause it can handle the multi-class propagation problem as the discussed in Section 2. The neighborhood size K of LNP is empirically optimized to show its best performance. The average accuracy for each category is shown in Figure 9. Performance of SVM is also shown to demonstrate the performance improvement of CSP in each category. Several conclusions can be drawn from the results: (1) By applying CSP, the average accuracy is improved from 55% to 62% and the relative improvement is about 12.7%. It shows that the collection-level context is successfully leveraged in the collection-based spare propagation method. (2) In 10 out of 11 categories, our new method helps to make better prediction. It performs slightly worse than the baseline on the architecture im-ages. Upon reviewing the misclassified images, we find the culprit may be the visual diversity of buildings and the noisy background in some user collections. (3) Both CSP and the baseline classifier outperform LNP because LNP cannot exploit the collection information and the predictions from the trained group classifiers. To evaluate the collection-based sparse propagation with active relevance feedback (CSP-ARF), we implement a simulated relevance feedback process that provides ground-truth labels automatically for the selected samples in the following way.
The experiments begin with selecting image collections for training and testing as discussed in Section 6.2. CSP-ARF selects the most informative test images for rele-vance feedback, without knowing the ground truth. Because the ground-truth informa-tion about the images group affiliation is known, a human user X  X  relevance feedback is mimicked by providing the ground-truth information for the selected test images to the algorithm.

Based on the discussion in Section 5.4, we implemented two related techniques to compare with our active learning method. BvSB with retraining is selected because it demonstrates superior performance over the other state-of-the-art active learning techniques in Joshi et al. [2008]. Random relevance feedback with retraining is also implemented because of its efficiency and wide applications. Although CSP-ARF does not require retraining, we construct another method (CSP-ARF a retraining step in the algorithm before updating the prediction (in Table II, step 4) so that we can evaluate the potential performance boost after adding retraining.
Several conclusions can be drawn from the results in Figure 10: (1) Without ex-ploiting the collection information, both BvSB and random relevance feedback with retraining cannot provide significant performance improvement because the training set is dominated by samples that are not from the collection of the current user; (2) The Active Relevance Feedback (CSP-ARF) provides significant performance enhance-ment (19%) because it considers both the uncertainty of samples and their influence on other images in the same collection; and (3) Adding retraining to our active relevance feedback method (CSP-ARF + Retrain) slightly improve the accuracy at the expense of reconstructing the classifier. Overall, it is clear that, for collection data, our sparse propagation and active learning framework generates much more improvement than the traditional retraining method alone. Online social network services pose great opportunities and challenges for many re-search areas. In multimedia content analysis, automatic social group recommendation for images holds the promise to greatly expand one X  X  social network through media sharing. In this article, we propose to use the user image collection and active learning to collectively suggest social groups based on the image content and user annotations. Our method first automatically clusters similar social groups into categories by eval-uating their similarity via SimRank and clustering. State-of-the-art machine learning techniques can be used to build classifiers that generate the initial group suggestion for each image based on its text annotation and visual descriptors. Next, user collection information is leveraged to enhance the prediction accuracy through collection-based sparse label propagation. The sparse graph-based collection model can be readily ex-ploited to select the most influential and informative samples for active relevance feedback, which can be integrated with the label propagation process without the need for classifier retraining. The proposed methods have been tested on group suggestion tasks for real user collections and demonstrated superior performance over the state-of-the-art techniques.

The contributions of our work are three fold. (1) Instead of treating the test data independently as many other methods do, our (2) The strong relationship between images from the same user X  X  collection is quanti-(3) The new active learning method considers both the uncertainty of sample prediction
Several new research directions are worth exploring in the future: (1) Integrating the collection information in the learning process may lead to further improvement of the system; (2) Analyzing the social connections between users may provide further insight of their interests and potential subjects of their photo collections; (3) Leveraging the collection information in ranking tasks may also have broad applications; and (4) Enhancing the scalability of our method may lead to efficient application on a large-scale data set.

