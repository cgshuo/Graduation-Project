 In classification problems, to classify an unknown sample, the ( k
NN) method searches the training set for the k closest samples, known as its k nearest neighbors, and then classifies the unknown sample based on its nearest neighbors. One popular k NN method is the well known voting k
NN) rule proposed by Cover &amp; Hart [ 3 ]; in this method the unknown sample is assigned to the class represented by the majority of its the training set.
 k
NN algorithm is to employ a weighted algorithm in which a weight is applied to each of the k neighbors based on their distance to the query point; a greater weight is given to a closer neighbor; the query sample x is assigned to the class in which the weights of the representatives of the k nearest neighbors sum to the greatest value. Dudani [ 5 ] has proposed a distance weighted k
NN) rule by assigning the i th nearest neighbor x i a distanced-based weight w as Equation (1), where d k and d 1 respectively represent the maximum and minimum distances of the k nearest neighbors to the test sample x . Research has identified a number of advantages for k NN algorithm. (1) It is a non-parametric method and does not require a priori knowledge relating to prob-ability distributions for the classification problem; (2) it has been demonstrated that the error rate for k NN approaches is the Bayes error (i.e., theoretically minimum error) when both the number of training samples and the value of approximate to infinity [ 4 ]; (3) it can be implemented conveniently due to its simple algorithm.
 Due to these advantages, the k NN method has been successfully applied to real-world applications and becomes one of the most popular algorithms for clas-sification over several decades. It has been the subject of extensive development for use in Machine Learning (ML) and Data Mining (DM) [ 11 , 14 ]. Notwith-standing the inherent simplicity of the k NNrule,ithasbeenshowntobeone of the most useful and effective algorithms in DM where it has been considered to be one of the top 10 algorithms [ 20 ]. Moreover, the k support for classification problems and usually achieves very good performances in various research areas [ 9 , 17 ].
 Notwithstanding the positive benefits discussed, the traditional V-DW-k NN rule is not guaranteed to be the optimal method for implementation using only quantity and distance information contained in the neighborhood; Organizing the information contained in the neighborhood to generate effective and efficient decision rules to improve the classification performance of the method has remained an active research topic for several decades.
 Research has investigated the decision rules generated from the neighbors resulting in a number of improvements to the k NN method. The Cat-egorical Average Pattern (CAP) method proposed by Hotta and Kiyasu [ 13 ] uses the categorical k nearest neighbors of query sample to compute the local centers termed the categorical average pattern (CAP) for each class; the unseen query sample is classified to the class with the nearest CAP. The local mean-based nonparametric classifier proposed by Mitani and Hamamoto [ 18 ] shares the same idea with CAP, and it has excellent classification performance as com-pared to other state-of-the-art classifiers. Li [ 16 ] demonstrated improvements in the CAP classification method by introducing a notion of local probabilistic centers (LPC) to reduce the number of negative contributing samples. In LPC method, and the query sample is assigned to the class with the nearest LPC. These methods control an equal neighborhood size for each class, and only take the distances to neighborhood into account. Though the distance to the neighborhood center can partly reflect the distribution of the corresponding class around the query sample, it would be arguably better if the distribution of the neighborhood were taken into account. Moreover, it would reduce the negative influences of noisy samples if the k nearest neighbors were considered integrally instead of individually as is the case for the voting k NN or the weighted this paper we consider a query sample to be closely related to the distribution of its nearest neighbors and therefore analyze classification problems from the per-spective of local distribution. Based on this approach, we propose a comprehen-sive k NN method based on the local distribution termed the Local Distribution based k NN (LD-k NN). The LD-k NN method estimates the local distribution of each class around the query sample to achieve the probability of the query sample belonging to each class and the query sample is assigned to the class with the greatest posterior probability. As a k NN-type method, LD-k NN also performs the classification based on the neighborhood of the query sample. In LD-k NN, the local distribution is esti-mated from the neighborhood for each class and the classification tasks are performed by maximizing the posterior probability of each class based on the local distribution. 2.1 LD-kNN Formulation Let X be the event that a data sample x is equal to the specified sample X described by measurements made on a set of attributes, i.e. X : x = X .Let C be the hypothesis that a data sample x belongs to the specified class C , i.e. C : x  X  C . For classification problems, the purpose is to determine P ( x (abbreviated as P ( C | X )), the probability that C holds given the event X .In other words, we are looking for the probability that sample X belongs to class C , given that we know the attribute description of X . P ( C probability of C conditioned on X , it should be maximized with respect to the class for the class label of the sample X (denoted by  X  ) as Equation (2). of each class conditioned a query sample in local area. Let  X  ( X ) denote the neighborhood of sample X and let  X  ( X ) be the event that a sample x is in the neighborhood of X , i.e.  X  ( X ): x  X   X  ( X ). Since  X  ( X ) is the neighborhood of of conditional probability, we can get ditioned on X . That is to say, the probability of sample X belonging to class C is equal to the LPP of class C conditioned on X . On the other hand, By Bayes X  theorem [ 8 ], P ( C | X ) can be computed as Equation (4), where P ( C )and P ( X ) are the respective prior probabilities of C and X ,and P ( X | C ) is the posterior probability of X conditioned on C . Equation (4) can be extended to local conditions, where each item should be estimated under the local condition  X  ( X ). Then we get the Bayesian formula under local condition as Equation (5).
 The Bayesian classifier [ 10 ] maximizes P ( C | X ) according to formula (4) and estimates it in the whole dataset. While our method maximizes P ( C ing to formula (3) and (5), we estimate it in a local area around the query sample. Under the assumption that the near neighbors can represent the prop-erty of a query sample better than the more distant samples, estimating the LPP by formula (5) represents more reasonable than by formula (4).
 To maximize P ( C | ( X,  X  ( X ))) according to formula (5): as P ( X stant for all classes, only P ( X | ( C,  X  ( X ))) P ( C | Then, the optimization problem can be transformed to 2.2 Local Distribution Estimation Given an arbitrary query sample X and a distance metric, its can be obtained from the training set. In this paper, we call the set of the nearest neighbors k -neighborhood of sample X and denote it by  X  the optimization problem (6), the two items P ( X | ( C,  X  ( X ))) and P ( C which are relevant to the local distribution of class C should be estimated based on  X  ( X ) for each class.
 P ( C |  X  ( X )) derives the probability of a sample belonging to class C given that the sample is in the neighborhood of X . If there are N C in the k -neighborhood of X , then P ( C |  X  ( X )) can be estimated by P ( X | ( C,  X  ( X ))) derives the probability of a sample being equal to X given that the sample is from class C and is in the neighborhood of X ;thiscanbe regarded as the local probability distribution density of class C at point X for continuous attributes.
 attributes in our method; the estimation of P ( X | ( C,  X  ( X ))) becomes a prob-lem of probability density estimation in local area. In our method, we assume that the samples in the neighborhood follow a Gaussian distribution with a mean  X  and covariance matrix  X  defined by Equation (8). where d is the dimension of the data. So that, for  X  k ( X ) and a specified class C ,wehave where  X  C j and  X  C j respectively represent the mean and the covariance matrix of class C j in  X  k ( X ).
  X  ( X ) for each class. In our approach, to ensure the covariance matrix is positive definite, we take the naive assumption of local class conditional independence that an attribute on each class does not correlate with the other attributes in local area; that is, the covariance matrix ( X ) would be a diagonal matrix. If there are N j samples from class C j in  X  k ( X ), denoted by X parameters the mean (  X  C j ) and the covariance matrix ( X  through maximum likelihood estimation by the following Formulae (10) and (11) [ 15 ]. where diag (  X  ) converts a square matrix to a diagonal matrix with the same diagonal elements.
 from Formulae (10) and (11) into Equation (8) to estimate f ( X ;  X  then estimate P ( X | ( C j , X  k ( X ))) from Formula (9). 2.3 Classification Rules tion problem as defined in (6) can be transformed into an optimization problem finally formulated as shown in Formula (12). where N C is the total number of classes, f (  X  ),  X  C j and  X  mulae (8), (10) and (11) respectively.
 query sample by the LPP estimated from local distribution. This is calculated according to the Bayesian Theorem in the local area. The query sample is then labeled with the class having a maximum LPP. 2.4 Related Methods The traditional V-k NN classified the query sample only by the number of near-est neighbors for each class in the k -neighborhood (i.e. N Compared with the V-k NN rule, LD-k NN takes into account the local probabil-ity density around the query sample ( f ( X ;  X  C j ,  X  C For different classes, the local probability densities are not always the same and may play a significant role for classification.
 Another classification method related with LD-k NN is the Bayesian classifi-cation method. Bayesian classifier assigns the query sample to the class with the highest posterior probability, which is estimated through the global distribution. While LD-k NN estimates the posterior probability through the local distribu-tion around the query sample. Naive Bayesian Classification (NBC) method can be considered as a special case of LD-k NN with k approaching the size of the dataset. Thus, LD-k NN would be more effective and comprehensive for a special query sample.
 In actuality, the LD-k NN method may be viewed as a compromise between the nearest neighbor rule and the Bayesian method. The parameter the locality in LD-k NN; when parameter k is close to 1, LD-nearest neighbor rule. And when k is large and equal to the size of the dataset, the local area is extended to the whole dataset; in this case LD-a Bayesian classifier. Thus, LD-k NN may combine the advantages of the two classifiers and become a more effective and comprehensive classification method. As for CAP and LPC, they consider an equal number of nearest neighbors for each class and the classification is based on the nearest center. As presented in Equation (12), CAP and LPC use a constant N j for all classes and the other item ( f ( X ;  X  C j ,  X  C j )) is estimated only from the center of the N each class. Thus, CAP and LPC can be viewed as special cases of LD-3.1 The Datasets In our experimentation we have selected 15 real datasets from the well-known UCI-Irvine repository of machine learning datasets [ 1 ]. The selected datasets include six two-class problems and nine multi-class problems, and vary in terms of their domain, size, and complexity. The estimation of probability density is only for continuous attributes and we only take into account continuous attributes in our experiments. Table 1 summarizes the relevant information for these datasets; for more information, please turn to http://archive.ics.uci.edu/ml . 3.2 Experimental Settings Before classification, to prevent attributes with an initially large range from inducing bias by out-weighing attributes with initially smaller ranges, we use z -score normalization to linearly transform each of the numeric attributes of a dataset with mean value 0 and standard deviation 1 by v = and  X  A are the mean and standard deviation, respectively, of attribute A . classifiers to test the performance of alternative approaches and to provide a comparative analysis to evaluate the effectiveness of our LD-These competing classifiers include base classifiers (e.g. V-and NBC), and the state-of-the-art classifiers (e.g. CAP [ 13 ], LPC [ 16 ] and SVM [ 2 ]).
 between two samples in search of the nearest neighbors. In addition, the param-eter k in k NN-type classifiers indicates the number of nearest neighbors, we use the average number of nearest neighbors per class (denoted by kpc )toindicate the neighborhood size, i.e. kpc  X  N C nearest neighbors are searched, where N is the number of classes.
 sifier classifying previously unseen samples, the training samples and the test samples should be independent. In our research we use stratified 5-fold cross validation to estimate the misclassification rate of a classifier on each dataset. ing set with the remaining fold being used as the test set. The training and test sessions are performed 5 times with each session using a different test set and the corresponding training set. To avoid bias, the 5-fold cross validation process is applied to each dataset 10 times and the average misclassification rate (AMR) is calculated to evaluate the performance of the classifier. The parameter kpc is an important factor that can affect the performance of LD-kNN. If kpc is too small, the estimation of the local distribution may be unstable; however, if it is too large, there will be many distant neighbors that may have an adverse effect on the local distribution estimation. To investigate the influence of the parameter kpc on classification results for k NN-type classifiers, we tune the parameter kpc as an integer in the range 1 to 30 for each dataset, perform the classification tasks and achieve the corresponding AMR for each This procedure will guide us in the selection of parameter Fig. 1 shows the performance curves with respect to kpc of the five methods on several real datasets. Because different real datasets usually have different distributions, the curves of AMR with respect to the are usually different. These performance curves show that, on average the LD-k
NN method can be quite effective for these real problems, and validate that a modest kpc for LD-kNN can usually achieve a more effective performance. We use the lowest AMR with the corresponding kpc ranging from 1 to 30 to evaluate the performance of a k NN-type classifier. Then, following experimental testing we obtained a comparative performance for our posited approach when compared with the alternative approaches. The classification results on each dataset for all the classifiers are shown in Table 2 in terms of AMR with the corresponding standard deviations (stds).
 From the results in Table 2 we can see that LD-k NN offers the best per-formance on 5 datasets, more than all other classifiers; this is an improvement over the alternative classifiers. The overall average AMR and rank of LD-on these datasets are 17.08% and 2.13 respectively, lower than all other classi-fiers, which means that the proposed LD-k NN may be more effective than other classifiers for these datasets.
 and each other classifiers, we have performed a Wilcoxon signed rank test [ 12 ] between LD-k NN and each other classifiers. The p -values of the tests between LD-k
NN and V-k NN, DW-k NN, CAP, LPC, SVM and NBC are 0.0103, 0.0125, 0.0181, 0.0103, 0.1876 and 0.0001 respectively, all less than 0.05 except that of SVM. Combined with the result that the average AMR for the LD-is the lowest among these classifiers, it can be seen that the LD-can outperform other classifiers and be comparable with SVM in terms of AMR at the 5% significance level.
 To evaluate how well a particular method performs on average among all the problems taken into consideration we have addressed the issue of robustness. Fol-lowing the method designed by Friedman [ 7 ], we quantify the robustness of a clas-sifier m by the ratio r m of its error rate e m to the smallest error rate over all the methods being compared in a particular application (i.e. r The optimal method m* for that application will have the ratio with r and all other methods will have a greater ratio. The greater the value for this ratio, the worse the performance of the corresponding method is for that appli-cation among the comparative methods. Thus, the distribution of r method, over all the datasets, provides information concerning its robustness. We illustrate the distribution of r m for each method over the 15 datasets by box plots in Fig. 2 where it is clear that the spread of r m for LD-and close to point 1.0, which demonstrates that the LD-k extremely robustly over these datasets.
 From the above analysis, it can be seen that LD-k NN performs better than other classifiers in respect of the overall AMR. In considering the sifiers, the DW-k NN improves the performance over the traditional V-weighting; the CAP and the LPC has improved the k NN method by local cen-tering. The LD-k NN is a more comprehensive method and considers the nearest neighbor set integrally by local distribution; thus it is reasonable to conclude that among the k NN-type classifiers the LD-k NN performs best followed by CAP, LPC, DW-k NN and V-k NN.
 comparable performance with LD-k NN for certain classification problems; how-ever the performance of the LD-k NN is more robust to application than SVM; that is, SVM may perform effectively on certain datasets however it also performs badly on other datasets and is not as stable as the LD-k datasets. NBC performs badly in the experimental classification tasks principally due to the fact that the class conditional independence assumption is too severe in practical problems.
 as it is predicated on the Bayes theorem. Since the classification is based on maximum posterior probability, the LD-k NN classifier can in theory achieve the Bayes error rate. Additionally, As a k NN-type classifier, LD-the advantages of k NN method. Thus, it may be intuitively anticipated that LD-k NN can perform much more effectively than NBC and other classifiers in most cases. We have introduced the concept of local distribution to the classification. The proposed LD-k NN method essentially considers the neighbors of the query sample as several integral sets by the class labels and then estimates the local distribution of these integral sets to achieve the LPP for each class; then the query sample is classified based on the maximum LPP. This approach provides a simple mechanism for quantifying the probability of the query sample attached to each class and has been shown to present several advantages. The experimental results demonstrate the effectiveness and robust-ness for LD-k NN and show its potential superiority.
 bution. In our experiments, we assume that the local probability distributions of the instances for each class can be modeled as a Gaussian distribution. How-ever, the Gaussian distribution assumption may not be always appropriate for all practical problems; there are other probability distribution estimation methods available, such as Gaussian mixture model [ 19 ] and kernel density estimation [ 6 ]. Different local distribution estimation methods for LD-different results. For a particular classification problem in a specific domain of interest various methods may be tested to achieve good results; this represents a future direction for our research.

