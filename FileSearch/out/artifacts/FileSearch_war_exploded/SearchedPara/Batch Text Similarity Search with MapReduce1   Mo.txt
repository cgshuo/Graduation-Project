 Give a collection of query texts Y = { y 1 , y 2 , ..., y m } where y j isalsoatext record (1  X  j  X  m ). The task of batch text similarity search is retrieving all pairs threshold  X  j (0 &lt; X  j  X  1, 1  X  j  X  m ).
 Text similarity search, a.k.a. text query by example, has been applied widely. A lot of users submit text similarity search tasks every day, e.g. searching for similar webpages, looking for persons having common interests in social net-works. There is a wealth of studies on text similarity searching. However, there are still some challenges: 1. The popularity and development of information technology leads to the ex-2. The similarity thresholds change constantly according to users X  expectations. 3. Quick response to the users X  queries is highly in demand despite huge data In this paper, batch text similarity search with MapReduce is proposed to deal with the aforementioned challenges. MapReduce [2] is a new distributed programming model which can process large datasets in parallel fashion. The MapReduce framework may be a simple and effective approach to batch text similarity search due to its automatic load balancing and fault tolerance advan-tages. Distinguished from FuzzyJoin [3] which is the latest research achievement of set similarity join using MapReduce, batch text similarity join can support both online text similarity search and search with variable threshold effectively.
The main contributions of this work are as follows. 1. We propose the PLT inverted file which contains the text prefix, length, and 2. We propose an online batch text similarity searching algorithm Online-3. Experimental results on real dataset show that OBTS algorithm outperforms The rest of the paper is organized as follows. We review the related work on similarity text search in Section 2. In Section 3 we introduce some basic con-cepts and in Section 4 we give the structure of PLT inverted file . The detailed algorithm is described in Section 5. In Section 6, our algorithm is compared with the existing works and finally we conclude this work in Section 7. A large body of work has been devoted to the study of similarity search [4,5,6,7]. The problem of text similarity search has a lot to do with the problem of string joins [8,9] as well as set similarity joins [3,10]. An inverted index based probing method was proposed in [11]. Recently, se veral filters were proposed to decrease the number of candidate documents needed to be verified, such as prefix filter [12], length filter [10,4], positional filter and suffix filter [13].
Three MapReduce algorithms for text similarity search using vector are intro-duced in [14]. All the three algorithms can find out the top k similar documents of each query text. However, only the first one, which is in brute force fashion, is suitable for exact similarity search. Furthermore, this algorithm is inefficient because it needs to compute the similarity of every two documents.

A 3-stage approach is proposed in [3] for set-similarity joins. The R -S join case in set-similarity joins is much like similarity search. We can treat the relation R as a query and S as a text database. However, it is worth noting that this approach is inefficient, especially for large-scale datasets, because the 3-stage method have to be run from the beginning to the end when a new query arrives or the similarity threshold is changed. 3.1 Word Frequency Dictionary We introduce a dictionary containing the total word frequency of each word. A total frequency of a word stands for i ts occurrences in all records of a text database. The frequencies are ranked in ascending order in our dictionary. For example, there is a text database with just two texts: t 1 =( a, b, c, d )and t 2 = 3.2 Similarity Metrics There are several similarity functions available, such as Jaccard similarity func-tion, cosine similarity function, and overlap similarity function. Jaccard similar-ity and cosine similarity for two texts x , y are listed as follows: In our work, we denote | x | as the length of the text, measured by the number of words. | x  X  y | stands for the common words in x and y while | x  X  y | stands for | x | + | y | X  X  x  X  y | .
 There are internal relationships among all sorts of similarity functions [13]. As Jaccard similarity function is widely used in algorithm, we adopt it as our metric. In the example above, the similarity between the two texts t 1 and t 2 is 3 / 6=0 . 5. 3.3 Filters Lemma 1 (prefix filtering principle [12,13]). Provided that the words in each text are in accordance with the frequency dictionary word order, the p-prefix stands for the p words before x. Let t be the similarity threshold.

If jaccard ( x, y ) &gt;t ,then We can exploit MapReduce framework and Lemma 1 to construct prefix index, which will be discussed in detail in Section 5.
 Lemma 2 (length filter principle [10,4]). For texts x and y, x is longer than y. If jaccard ( x, y ) &gt;t , we have: We can construct a length fi lter by means of Lemma 2. Based on the filter, each text x i will only have to compare with the text y which satisfies: Length filter can further reduce the number of text candidates. The pruning effect is obvious especially when t is close to 1. Normally, there are vast amounts of data in a text database. Pairwise comparison between query texts and all the database t exts for similarity search is unrealistic. Normal method of using filters needs to process the database (including comput-ing the prefix of each text and so on) for each search, which is also inefficient. So we consider building an index. For the n eed of prefix filter an dlengthfilter,our index includes the information of prefix, length and similarity threshold, and it is called PLT inverted file.

According to Lemma 1, we find that when given a similarity threshold, the query text X  X  prefix shares at least one common word with the candidate database texts X  prefix. So the PLT inverted file needs to contain the prefix information of database texts.

According to Lemma 2, if the length of query text is l , for a given threshold t , query text only needs to match with the database texts whose length are in [ t  X  l, l/t ] interval. So the PLT inverted file need s to contain the length information of database texts.

Prefix length has something to do with similarity threshold. The higher the threshold, the longer the prefix length. So we need to calculate the maximum similarity threshold that makes one word appear in the prefix beforehand. Each a word, and each value contains the following three elements, i.e. id of the text whose prefix contains the key , the length of the text and the maximum similarity threshold that makes the key appear in the prefix. In this section, the algorithm OBTS is presented. OBTS includes two phases. In the first stage, the PLT inverted structure is generated for pruning. In Stage 2, the PLT inverted index structure r eceives the query text, searches in the PLT inverted file, and matches the quer y text and the text database. Finally it outputs the text pairs with a threshold no less than the given value. 5.1 Stage 1: Generate a Database Prefix Index The first stage, generating a database prefix index, is divided into three steps. Step 1 takes charge of ranking of word frequencies. In this step, after the statistics for each word frequency is accomplished, word frequencies are sorted in ascending order to generate a word frequency dicti onary. Step 2 gener ates vectors of all texts in the database according to the word frequency dictionary. Each text generates a new vector text in this step. In the final step, the PLT inverted file is generated.
 Overall Word Frequency Sort. This step consists of two MapReduce jobs. The first job computes the statistics o f word frequency. As shown in Algorithm 1, the input of Map function is the text in database while the output of Map function is used to aggregate the output in order to reduce the network data transmission from the Map function to the Reduce function. A k is obtained by accumulating of all the 1 s after the same token. Reducer accumulates all the k in the same token, and obtains overall word frequency f . The left part of Figure 1 shows the first job. We assume that there are only 4 texts in the database for convenience. Take Text 4 as an example. It contains three words, coming from Text 3 and Text 4 are aggregated for they are processed by the same Mapper. Reduce function makes a further aggregation, and all occurrences of the word D in the database are counted.

The second job is to sort words according to f . As shown in Algorithm 2, the input of Map function is the result of the first job. Map function swaps the positions of x and f in &lt;x , f&gt; , and MapReduce framework will automatically sort the pairs according to f . The number of reducer is set to 1 in order to ensure the correctness of sorting results in th e distributed environment. As shown in the right part of Figure 1, after grouped by key, all the words have been sorted by f . The output of Reduce function is the word frequency dictionary. Calculate the Vector Text. Algorithm 3 depicts that Map function receives texts, replaces the word with its position, and then sorts all the position number in the text, finally outputs the pair ( a, V )madeupbythe id and vector of x i . Take Text 4 in Figure 2 as an example. The positions of words D, M, and N in the dictionary are 8, 3, and 4, respectively. And it is changed to  X 3, 4, 8 X  after be sorted. In this stage we use Identity Reduce function to output its input directly.
 Generate the PLT Inverted File. As shown in Algorithm 4, the Map func-tion calculates the prefixes of all the vector texts. For each word in the prefix, it generates the pairs which contain the word, text id , length, and threshold value. MapReduce framework groups all the pairs according to word. Reduce function inserts its input into an index, and finally the PLT invert file is generated. In Figure 3, we take document 4 as an example, when given 0.9 as the threshold, prefix length remains to be 1. When the t hreshold is reduced to 0.6, the prefix threshold value of 0.5 and find that the prefix length is not changed. 5.2 Stage 2: Query Text Search The second stage, query text search, is divided into two steps. Firstly we trans-form the query texts into vector texts, and then we calculate the prefix for each vector text. For each word in the prefix , we search the PLT inverted file, match the texts in database which meet the requirements, and finally output the text pairs with the similarity no less than a given threshold. System continuously detects the coming of new query texts. On ce it is detected, the query texts will run the two steps above.

As shown in Algorithm 5, we calculate the prefix for each vector text gen-erating from query text in the Map function. For each word in the prefix, it outputs the pairs consisting of the word (key) and the content of vector text content (value). Reduce function retrie ves the PLT inverted file according to the key, returns the information of candidate text, including its id, length, threshold. After checking whether the length and threshold is desirable, we match the query text with the candidate, and output the text pairs whose similarity is no less than the given threshold. For example, in Figure 4, Let the given threshold be 0.75. For Text 5, we can see the prefix only has a token-1 as the text length is 3. Text 1 is a candidate by searching the PLT i nverted file. After the verification, it meets the requirements, with the similarity of 0.75. In this section we compare the performance of our algorithm with the fuzzyjoin in [3], whose source code is available at http://asterix.ics.uci.edu/. 6.1 Experimental Setting Experiments were run on a 5-node cluster. Each node had one Pentium(R) pro-cessor E6300 2.8 GHz with two cores, 4 GB of RAM, and 800 GB hard disks. Five nodes were all slave nodes and one of them was master node as well. We installed the Ubuntu 9.10 operating system, Java 1.6.0 12, Hadoop 0.20.1 [15]. In order to maximize the parallelism, the size of virtual memory of each map/reduce task was set to be 2 GB. 6.2 Experimental Dataset We used MEDLINE as our dataset. It is the U.S. National Library of Medicine X  X  bibliographic database, consisting of more than 11M articles from over 4.8K indexed titles. We only used a part of it. Our dataset contained 48K records and the total size was approximately 290 MB. The average length of each record was 6026 bytes. 6.3 Comparative Experiments In this subsection, we demonstrate that our algorithm had a better performance by a large number of experimental results.
 Influences of similarity threshold. We fixed the number of query texts to 60. Similarity threshold values were changed between 0.5 and 0.9 on the dataset of MEDLINE. The running time consisted of the time cost of two stages: generating the PLT inverted file, and similarity search.

Figure 5 shows the running time for searching MEDLINE containing 10K, 30K, 48K records. As the similarity threshold rose, time cost became lower mainly due to the following two reasons. Firstly, the prefixes of query texts became shorter with the rise of threshold, which made less items need to be searched in the PLT inverted file. Seco ndly, the rise also made less database texts in each item to be verified. We also noticed that the time cost for gen-erating the PLT inverted file did not incr ease linearly with the increase of the dataset size. It was because the time cost for launching MapReduce was con-stant. In order to show worst situations, we used 0.5 as the similarity threshold in experiments below.
 Influences of da tabase size. In this experiment, we also used 60 texts as a query. We selected 5K to 20K texts from the MEDLINE dataset. Figure 6 shows that our algorithm had a better performance. This was mainly due to two reasons. The first was less data was sent through the network in our algorithm, because we used vectors instead of the w ords. The second was that verification became easier for the use of vectors.
 Influences of Quer y Texts X  size. We fixed the number of database texts to 5K, so the time cost of generating the PLT inverted file was constant. Figure 7 shows that as the number of query texts increased, time cost of our algorithm rose slowly. For instance, when we change texts X  number from 200 to 300, only 8 extra seconds needed. Influences of Reducer Number. In this experiment, we used 500 texts as a query and 2000 texts as a database. Figure 8 shows that the time cost decreased significantly when the number of reducer s changed from 1 to 5, and insignificantly when it changed from 5 to 10. As the number of reducers increased, the processing capacity of reducer was improved. But the running time depended on the lowest reducer, not all the reducers finished their jobs at the same time. When the reducer number changed from 1 to 5, the former played a leading role, and when the reducer number changed from 5 to 10, the impact of the latter got larger. Speedup and scaleup. We calculated the relative speedup and scaleup in order to show the performance of our parallel algorithm.

For the speedup, we fixed the query texts X  size as well as the database size and varied the cluster size. Query texts X  number was set to 100, and database contained 2K texts. As shown in Figure 9, a dotted straight line shows the ideal speedup. For instance, if we changed cluster nodes from 1 to 3, it should be triple as fast. Both two algorithms did not reach the ideal speedup. Their limited speedup was due to two main reasons. Firstly, network data transmission traffic increased as more nodes added into the cluster. Secondly, the whole task was not equal divided, and the other reducers must wait for the lowest one.
For the scaleup, we increased the dataset (including query text and database) size and the cluster size together by the same factor. Let m bethesizeofquery text and n the size of database, time cost for matching each pair was p .Ifnofilter existed, time cost for matching would be m  X  n  X  p . If both the query text and the of the increase of cluster size, the final result would be m  X  n  X  p  X  t .Butthe start-up time for MapReduce is constant, so the time cost did not increase with t linearly. Figure 10 shows that our algorithm had a better expandability. Because when the cluster size grew larger, OBTS s pent the less time for processing the same dataset.
 Online Search. OBTS can handle the problem of batch similarity search with-out processing the database for each query. We used the whole 290MB dataset as the database. Figure 11 shows the advantage of this approach. The advantage is more outstanding at a higher frequency of query due to the existing of PLT inverted file. In this paper, we study the problem of online batch text similarity search using the MapReduce framework. We propose the PLT inverted file in order to avoid processing the database for every query. Based on the PLT inverted file, OBTS algorithm was proposed to support the real-time search. It can also support users to change the similarity threshold each time. We implemented OBTS in Hadoop and compared the performance with fuzzyjoin to show the advantages of OBTS.
