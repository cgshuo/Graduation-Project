 Relationship extraction is a task of recognizing a particular relationship between two or more entities in documents. However, a large amount of manually labeled data is demanded when the supervised learning methods are used to address this problem. But annotating training data is a very tedious and time consuming work [1]. Meanwhile, semi-supervised learning addresses this problem by combining a large amount of unla-beled data with a small set of labeled data to train a classifier, such as co-training, label propagation and so on.

Recently, label propagation, a graph-based s emi-supervised learning method, has increasingly attracted research attention [1,2,3]. The label propagation algorithm con-structs a graph with both labeled and unlabeled data. The seed nodes in the graph propa-gate their labels to neighbors according to the ir similarity. In label propagation process, the label distribution of initial labeled seeds are clamped in each iteration to replenish the label sources from these labeled data. W ith this spreading fro m labeled examples, the class boundaries are spread through edges with large weights and settle in gaps along edges with low weights.

Generally, many tasks of information extraction or natural language processing have a property that the data naturally consist of several views X  X isjoint subsets of features. For instance, web pages can be described by their contents or hyperlinks pointing to these pages [4]. A popular paradigm of multi-vi ew learning is the co-training algorithm, which splits all features into two subsets and trains two classifiers by the labeled seeds in each view. Each classifier classifies the unlabeled data in the unlabeled data pool and provides the other classifier with the few unlabeled examples as training seeds that receive the highest confidence from the first classifier.

In relation classification task, a semantic relationship can be represented with two different kinds of  X  X nforma tion X : the entity pair itself and the context surrounding it. Given an instance of data for relation classification as follows: where e 1 and e 2 are nouns or noun phrases and C pre , C mid ,and C post are the con-texts before, between, and after the nominal pairs. We split s into two parts: entity pair ( e 1 ,e 2 ) and contexts ( C pre ,C mid ,C post ) . Many features can be extracted from the two parts respectively and applied to learning.
 In this paper, we propose a label propaga tion based multi-view learning algorithm, Co-Label Propagation (Co-LP), which combines the information of the entity pair view and the context view. Let S = { s i | i =1 ,  X  X  X  ,u } be a set of sentence tuples. Let A = { a j | j =1 ,  X  X  X  ,n } and B = { b k | k =1 ,  X  X  X  ,m } be sets of entity pairs and contexts respectively. A entity pair a j occurs in S at least once with one or more context(s). A context b k signify at least one or more entity pair(s). The proposed Co-LP algorithm constructs three graphs: G A = &lt;A,E A &gt; , G B = &lt;B,E B &gt; and G
AB = &lt;A context view respectively, and E A , E B are edges that connect intra-view data points. Graphs G A and G B represent the similarities among data points in each view respec-tively. The inter-view graph G AB is a bipartite graph which describes the correlation between data points of two different views. The graph G AB uses the correlation of the entity pair a j and context b k to combine the label score of data points in the two views.
The remainder of this paper is organized as follows: In section 2, we outline related works of relation extraction and semi-supervised learning. In section 3, we present our Co-Label Propagation algorithm in detail. Section 4 presents some experiments and discussion of the results. Finally, in section 5, we discuss our conclusions. To leverage the unlabeled data in the training stage, semi-supervised learning has been applied to relation extraction task. As mentioned in previous section, Chen et al. ex-plored a graph based semi-supervised learning for relation extraction, which makes use of unlabeled data [5]. Niu et al. investigated label propagation for a word-sense disam-biguation task [6].

Co-Training is a semi-supervised, multi-view algorithm that uses the initial seeds to data. The examples on which each classifie r makes the most confident predictions are selected and added to the training set. Based on the new training set, a new classifier is learned in each view, and the whole proces s is repeated for several iterations.
The proposed Co-LP is based on label propagation which models an entire dataset as a weighted graph and propagates labels through the graph along its high-density areas [3]. Zhou et al. proposed another graph-based algorithm, the local and global consistency algorithm, in which the function at each node receives contribution from its neighbors in each step [2]. 3.1 Preliminaries We presume that each data point s has two views X  s = &lt;a,b&gt;  X  X here a and b denote data points constructed respectively from entity pair view and context view. Let u be the number of data points in the feature space built with all features. Similarly, n and m respectively signify the quantities of data points in the feature space generated with entity pair view and context view. More formally, the dataset S  X  A  X  B , | S | = u , |
A | = n , | B | = m ,where u  X  n, m , each example s  X  S is given as ( a, b ) . Actually, L = { l 1 ,l 2 ,  X  X  X  ,l c } is the set of labels and | L | = c .
 Let T A =( T A ij ,i,j =1 , 2 ,  X  X  X  ,n ) be an n  X  n similarity matrix constructed from A in which T A ij represents the similarity between a i and a j calculated from entity pair view. Let T B be defined similarly, as shown above from B .

Let W AB =( W AB ij ,i =1 , 2 ,  X  X  X  ,n ; j =1 , 2 ,  X  X  X  ,m ) be an n  X  m matrix defined as the correlation matrix between entity pair view and context view. In addition, T shows, where T AB ij denotes the normalized correlation between a i and b j . In addition, T normalized W AB as presented in Eq. 2. as l j in t -th round propagation. Let Y A 0 be initialized by the labeled data as distribution of data point b i and Y B 0 is initialized similarly as Y A 0 .Let Y be an u  X  c s  X  S .
 3.2 Intra-view and Inter-view Label Propagation In the proposed algorithm, we construct two intra-view graphs: G A = &lt;A,E A &gt; , G
B = &lt;B,E B &gt; ; one inter-view graph G AB = &lt;A the data point in entity pair view and where context view. G AB represent similarities between data points in different views. Following [1], we use both labeled and unlabeled nodes to create a fully connected intra-vi ew graph in each view. The edge between node i , j is weighted as where sim ij signifies the similarity of x i and x j calculated using some similarity mea-sure, and  X  is used to control the weight. As described in this paper, we set  X  as the average similarity between labeled examples from different classes.

In the first step of propagation, every node in each view receives a contribution from the linked nodes in the same view. The second step is to spread label scores among different views. We build an inter-view graph G AB = &lt;A  X  B, E AB &gt; between entity pair view and context view, which is used to amend the class distributions of each node. The weight of edge in E AB is given as W AB . In addition, T AB is row-normalized by Eq. 1 and T BA is line-normalized by Eq. 2. Table 1 presents the proposed algorithm concretely. 3.3 Rebalance the Label Distribution Using Label Bidding After the matrix Y is learned, the label bidding proces s is executed to rebalance the label distribution. When data classes are very clos e or when labeled data are very few, rebal-ancing the label distribution can improve the final classification performance. Initially, the number of each type of labels can be estimated from labeled data. In the label bid-ding process, the learned label score of each node in Y is regarded as a bid for the labels. For example, if Y il is the highest bid currently and class l has labels remained, then data point i is labeled as l . Then the data point i exits from bidding and the label number of class l subtracts 1. The second highest bid is processed if class l has no labels. This process will be repeated until all the labels are  X  X old X . 4.1 Evaluation of the Semantic Relation Classification Performance In this section, we present our empirical study using SemEval-2007 Task 04: Classi-fication of Semantic Relations between Nominals [7]. This dataset consists of seven semantic relations and every semantic relation is a separate binary classification task. Table 2 presents the number of positive and negative examples of each relation.
In our experiment, we first put the training set and test set of SemEval-07 Task 04 together; then we randomly select different percentages of data point as labeled seeds and others as unlabeled data. All data are pro jected into context and nominal pair views. To test the efficiency of inter-view label propagation algorithm, we also use the other algorithm named UnIVLP. This algorithm uses the same feature splitting and only prop-agates label scores in each view. In other words, UnIVLP merely skip over steps 2 and 3 portrayed in Table 1.
 Intra-View Similarity. To weight the similarity graph of each view, we use a fre-quently used measure, the cosine similarity measure (as Eq. 4), to calculate the similar-ity between any two data nodes in each view. Matrices T A and T B , are constructed and normalized as previous mentioned. Inter-View Similarity. Mutual information is an efficient measure of the relation be-tween two random variables. Therefore, we use mutual information (as Eq. 5) between a j and b k to measure the relevance between data points of different views. Actually, more, P ( a j ,b k ) is the joint probability distribution of the node pair. Features and View Splitting. In this experiment, as Table 3 shows, we use 13 features extracted from the SemEvald-07 Task 04 dat aset. Because of the property of this prob-lem, the features are divisible into two subsets: Context and Nominal pair. We put the first four features in Table 3 into the Nominal pair feature set; all other features are dis-tributed to the Context feature set. It is different general semantic relation classification tasks, we only used the surface token of nominals and contexts. Because we specifically examine testing the classification perform ance of the algorithm, the syntactic features of words are unimportant.
 Experiment Results. For semantic relations of all types, the proposed method is com-pared with the algorithms: 1) using label propagation, treating all features as one view (LP-ALL); 2) using label propagation in each view without inter-view propagation (UnIVLP); Figure 1 presents the average accuracy of seven relation types. It is apparent from Figure 1 that inter-view propagation can reduce the classification error in most cases. However, when the labeled seed percentage is 15%, Co-LP cannot beat UnIVLP, indi-cating that inter-view exchanging the label score cannot improve the performance of the classifier in these cases. Comparing UnIVLP with LP-ALL, it is apparent that UnIVLP works better than single view label propagation, although UnIVLP linearly combines results from separate views. Comparing Co-LP, UnIVLP with LP-ALL, the practice of regarding all features as two views always outperforms their treatment as a single view: Co-LP achieves the best accuracy. The refore, a classifier trained on one view cannot provide useful information to another classifier. One possible explanation of Co-LP outperforming LP-ALL is that the feature space is provided with a considerable amount of redundancy. This redundancy, in effect , improved the classification accuracy. Comparing Co-LP with UnIVLP, Co-LP employs the relation of different views; such information might improve th e classification accuracy. 4.2 Different Feature Splitting In this experiment, we specifically examine the sensitivity of Co-LP to feature split-ting. This will elucidate the dependence o f the proposed algorithm for feature split-ting. To this end, we split the feature randomly into two disjoint subsets and repeat the above semantic relation classification experiment on the same dataset. We compare the new feature splitting to the (nominal pair, context) feature splitting (Co-LP-NC) in the experiment presented above. We can observe from Figure 2 that, although we randomly split the feature (Co-LP-RP), Co-L P-RP still outperforms label propagation (LP-ALL) in most cases. With labeled data of 15 and 20 percent, Co-LP-RP even out-performs Co-LP-NC, but after 25 percent, Co-LP-NC works better than either of the other two algorithms. When we randomly label 35 percentages of data as seeds, the pro-posed algorithm with random projected features (Co-LP-RP) cannot beat the LP-ALL algorithm. 4.3 Robustness with Respect to the Inter-view Correlation Measure For studying the robustness of the proposed method to the correlation measures between different views, we use two similarity meas ures that are often used in the Natural Lan-guage Processing community: the matching coefficient and dice coefficient. The Eq. 6 and Eq. 7 respectively show the Matching co efficient and Dice coefficient. We use the same sampled data to test the sensitivity of Co-LP algorithm to the correlation measure.  X  Matching coefficient:  X  Dice coefficient: Figure 3 portrays the average accuracies of di fferent correlation measures: the mutual information measure (MI), matching coefficient (MC), and Dice coefficient (DC). We observe that the accuracy using DC closely re sembles that using MI. Furthermore, we can find that the accuracies of MI and DC are each better than that used MC. In this paper, we propose the Co-Label Propagation algorithm, which is based on the generalized cluster assumption. The experiment results show that our proposed algo-rithm can improve the performance of the label propagation algorithm, a well-known graph-based algorithm, on the SemEval-07 task 04 dataset. We also show that Co-LP works well with different splitting of feature set and choice of the inter-view correlation measure.

