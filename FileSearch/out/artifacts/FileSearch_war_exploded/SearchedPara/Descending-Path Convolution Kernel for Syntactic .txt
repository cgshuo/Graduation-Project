 Syntactic structure can provide useful features for many natural language processing (NLP) tasks such as semantic role labeling, coreference resolu-tion, temporal relation discovery, and others. How-ever, the choice of features to be extracted from a tree for a given task is not always clear. Convolu-tion kernels over syntactic trees (tree kernels) offer a potential solution to this problem by providing relatively efficient algorithms for computing sim-ilarities between entire discrete structures. These kernels use tree fragments as features and count the number of common fragments as a measure of similarity between any two trees.

However, conventional tree kernels are sensitive to pattern variations. For example, two trees in Fig-ure 1(a) sharing the same structure except for one terminal symbol are deemed at most 67% similar by the conventional tree kernel (PTK) (Moschitti, 2006). Yet one might expect a higher similarity given their structural correspondence.

The similarity is further attenuated by trivial structure changes such as the insertion of an ad-jective in one of the trees in Figure 1(a), which would reduce the similarity close to zero. Such an abrupt attenuation would potentially propel a model to memorize training instances rather than generalize from trends, leading towards overfitting.
In this paper, we describe a new kernel over syntactic trees that operates on descending paths through the tree rather than production rules as used in most existing methods. This representation is reminiscent of Sampson X  X  (2000) leaf-ancestor paths for scoring parse similarities, but here it is generalized over all ancestor paths, not just those from the root to a leaf. This approach assigns more robust similarity scores (e.g., 78% similarity in the above example) than other soft matching tree ker-nels, is faster than the partial tree kernel (Moschitti, 2006), and is less ad hoc than the grammar-based convolution kernel (Zhang et al., 2007). 2.1 Syntax-based Tree Kernels Syntax-based tree kernels quantify the similarity between two constituent parses by counting their common sub-structures. They differ in their defini-tion of the sub-structures.

Collins and Duffy (2001) use a subset tree (SST) representation for their sub-structures. In the SST representation, a subtree is defined as a subgraph with more than one node, in which only full pro-duction rules are expanded. While this approach is widely used and has been successful in many tasks, the production rule-matching constraint may be un-necessarily restrictive, giving zero credit to rules that have only minor structural differences. For example, the similarity score between the NPs in Figure 1(b) would be zero since the production rule is different (the overall similarity score is above-zero because of matching pre-terminals).

The partial tree kernel (PTK) relaxes the defi-nition of subtrees to allow partial production rule a) b) c) matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates a very large but sparse feature space. To Figure 1(b), the PTK generates fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a] [NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among others, for the second tree. This allows for partial matching  X  substructure (ii)  X  while also generating some fragments that violate grammatical intuitions.
Zhang et al. (2007) address the restrictiveness of SST by allowing soft matching of production rules. They allow partial matching of optional nodes based on the Treebank. For example, the rule NP  X  DT JJ NN indicates a noun phrase consisting of a determiner, adjective, and common noun. Zhang et al. X  X  method designates the JJ as optional, since the Treebank contains instances of a reduced version of the rule without the JJ node ( NP  X  DT NN ). They also allow node match-ing among similar preterminals such as JJ, JJR, and JJS, mapping them to one equivalence class.
Other relevant approaches are the spectrum tree (SpT) (Kuboyama et al., 2007) and the route kernel (RtT) (Aiolli et al., 2009). SpT uses a q-gram  X  a sequence of connected vertices of length q  X  as their sub-structure. It observes grammar rules by recording the orientation of edges: a  X  b  X  c is different from a  X  b  X  c. RtT uses a set of routes as basic structures, which observes grammar rules by
DT a Figure 2: A parse tree (left) and its descending paths according to Definition 1 ( l -length). recording the index of a neighbor node. 2.2 Temporal Relation Extraction Among NLP tasks that use syntactic informa-tion, temporal relation extraction has been draw-ing growing attention because of its wide applica-tions in multiple domains. As subtasks in Tem-pEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of fea-tures (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mir-roshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic informa-tion for temporal relation extraction on the Time-Bank (Pustejovsky et al., 2003) and the AQUAINT that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to rep-resent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal (Styler et al., 2014). In this paper, we also use syntactic structure-enriched temporal relation dis-covery as a vehicle to test our proposed kernel. Here we decribe the Descending Path Kernel (DPK).
Definition 1 (Descending Path) : Let T be a parse tree, v any non-terminal node in T , dv a descendant of v , including terminals. A descending path is the sequence of indexes of edges connecting v and dv , denoted by [ v  X  X  X  X  X  X  dv ] . The length l of a descending path is the number of connecting edges. When l = 0 , a descending path is the non-terminal node itself, [ v ]. Figure 2 illustrates a parse tree and its descending paths of different lengths.
Suppose that all descending paths of a tree T are indexed 1 ,  X  X  X  ,n , and path i ( T ) is the frequency of the i -th descending path in T . We represent T as a vector of frequencies of all its descending paths:  X ( T ) = ( path 1 ( T ) ,  X  X  X  ,path n ( T )) .
The similarity between any two trees T 1 and T 2 can be assessed via the dot product of their respec-tive descending path frequency vector representa-tions: K ( T 1 ,T 2 ) =  X   X ( T 1 ) ,  X ( T 2 )  X  .
Compared with the previous tree kernels, our descending path kernel has the following advan-tages: 1) the sub-structures are simplified so that they are more likely to be shared among trees, and therefore the sparse feature issues of previous kernels could be alleviated by this representation; 2) soft matching between two similar structures (e.g., NP  X  DT JJ NN versus NP  X  DT NN ) have high similarity without reference to any corpus or grammar rules;
Following Collins and Duffy (2001), we derive a recursive algorithm to compute the dot product of the descending path frequency vector represen-tations of two trees T 1 and T 2 : where N 1 and N 2 are the sets of nodes in T 1 and T 2 respectively, i indexes the set of possible paths, I path i ( n ) is an indicator function that is 1 iff the descending path i is rooted at node n or 0 other-wise. C ( n 1 ,n 2 ) counts the number of common descending paths rooted at nodes n 1 and n 2 : C ( n 1 ,n 2 ) can be computed in polynomial time by the following recursive rules: Rule 1: If n 1 and n 2 have different labels (e.g.,  X  X T X  versus  X  X N X ), then C ( n 1 ,n 2) = 0 ; Rule 2: Else if n 1 and n 2 have the same labels and are both pre-terminals (POS tags), then C ( n 1 ,n 2 ) = 1 + where term ( n ) is the terminal symbol under n ; Rule 3: Else if n 1 and n 2 have the same labels and they are not both pre-terminals, then: where children ( m ) are the child nodes of m . As in other tree kernel approaches (Collins and Duffy, 2001; Moschitti, 2006), we use a discount parameter  X  to control for the disproportionately large similarity values of large tree structures. Therefore, Rule 2 becomes: C ( n 1 ,n 2 ) = 1 + and Rule 3 becomes:
Note that Eq. (1) is a convolution kernel under the kernel closure properties described in Haus-sler (1999). Rules 1-3 show the equivalence be-tween the number of common descending paths rooted at nodes n 1 and n 2 , and the number of matching nodes below n 1 and n 2 .

In practice, there are many non-matching nodes, and most matching nodes will have only a few matching children, so the running time, as in SST, will be approximated by the number of matching nodes between trees. 3.1 Relationship with other kernels For a given tree, DPK will generate significantly fewer sub-structures than PTK, since it does not consider all ordered permutations of a production rule. Moreover, the fragments generated by DPK are more likely to be shared among different trees. For the number of corpus-wide fragments, it is
O  X  | N 1 || N 2 | b 15 2 0.25
O  X  2 | N 1 || N 2 | b 13 9 0.83
O  X  3 | N 1 || N 2 | b 36 15 0.65 Table 1: Comparison of the worst case computa-tional complexicity (  X  -the maximum branching factor) and kernel performance on the 3 examples from Figure 1. # Frag is the number of fragments, N ( Sim ) is the normalized similarity. Please see the online supplementary note for detailed frag-ments of example (a). possible that DPK  X  SST  X  PTK. In Table 1, given  X  = 1 , we compare the performance of 3 kernels on the three examples in Figure 1. Note that for more complicated structures, i.e., examples b and c, DPK generates fewer fragments than SST and PTK, with more shared fragments among trees. The complexity for all three kernels are at least O | N 1 || N 2 | since they share the pairwise summa-tion at the end of Equation 1. SST, due to its re-quirement of exact production rule matching, only takes one pass in the inner loop which adds a factor of  X  (the maximum branching factor of any pro-duction rule). DPK does a pairwise summation of children, which adds a factor of  X  2 to the com-plexity. Finally, the efficient algorithm for PTK is proved by Moschitti (2006) to contain a con-stant factor of  X  3 . Table 1 orders the tree kernels according by their listed complexity.

It may seem that the value of DPK is strictly in its ability to evaluate all paths, which is not explicitly accounted for by other kernels. However, another view of the DPK is possible by thinking of it as cheaply calculating rule production similarity by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category comparison is successfully completed, DPK looks at all paths that go through it and accumulates their similarity scores independent of ordering  X  in other words, it will ignore the ordering of the children in its pro-duction rule. This means, for example, that if the rule production NP  X  NN JJ DT were ever found in a tree, to DPK it would be indistinguishable from the common production NP  X  DT JJ NN , despite having inverted word order, and thus would have a maximal similarity score. SST and PTK would assign this pair a much lower score for having com-pletely different ordering, but we suggest that cases such as these are very rare due to the relatively strict word ordering of English. In most cases, the determiner of a noun phrase will be at the front, the nouns will be at the end, and the adjectives in the middle. So with small differences in production rules (one or two adjectives, extra nominal modifier, etc.) the PTK will capture similarity by compar-ing every possible partial rule completion, but the DPK can obtain higher and faster scores by just comparing one child at a time because the ordering is constrained by the language. This analysis does lead to a hypothesis for the general viability of the DPK, suggesting that in languages with freer word order it may give inflated scores to structures that are syntactically dissimilar if they have the same constituent components in different order.

Formally, Moschitti (2006) showed that SST is a special case of PTK when only the longest child sequence from each tree is considered. On the other end of the spectrum, DPK is a special case of PTK where the similarity between rules only considers child subsequences of length one. We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the C ONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: K C ( s 1 ,s 2 ) =  X  where s i is an instance object composed of flat fea-tures f i and a syntactic tree t i . A syntactic tree t i can have multiple representations, as in Bag Tree (BT), Path-enclosed Tree (PET), and Path Tree (PT). For the tree kernel K T , subset tree (SST) ker-nel was applied on each tree representation p . The final similarity score between two instances is the  X  -weighted sum of the similarities of all representa-tions, combined with the flat feature (FF) similarity as measured by a feature kernel K F (linear or poly-nomial). Here we replaced the SST kernel with DPK and tested two feature combinations FF+PET and FF+BT+PET+PT. To fine tune parameters, we used grid search by testing on the default develop-ment data. Once the parameters were tuned, we tested the system performance on the testing data, which was set up by the original system split. 4.2 Cleartk-TimeML We tested one sub-task from TempEval-2013  X  the extraction of temporal relations between an event and time expression within the same sen-tence. We obtained the training corpus (Time-Bank + AQUAINT) and testing data from the au-thors (Bethard, 2013). Since the original features didn X  X  contain syntactic features, we created a PET tree extractor for this system. The kernel setting was similar to equation (2), while there was only one tree representation, PET tree, P =1. A linear kernel was used as K F to evaluate the exact same flat features as used by the original system. We used the built-in cross validation to do grid search for tuning the parameters. The final system was tested on the testing data for reporting results. 4.3 Results and Discussion Results are shown in Table 2. The top section shows THYME results. For these experiments, the DPK is superior when a syntactically-rich PET representation is used. Using the full feature set of Miller et al. (2013), SST is superior to DPK and obtains the best overall performance. The bottom section shows results on TempEval-2013 data, for which there is little benefit from either tree kernel. Our experiments with THYME data show that DPK can capture something in the linguistically richer PET representation that the SST kernel can-not, but adding BT and PT representations decrease the DPK performance. As a shallow representation, BT does not have much in the way of descending paths for DPK to use. PT already ignores the pro-duction grammar by removing the inner tree nodes. DPK therefore cannot get useful information and may even get misleading cues from these two rep-Features K T P R F FF+PET DPK 0.756 0.667 0.708 FF+BT+ DPK 0.759 0.626 0.686 PET+PT SST 0.754 0.711 0.732 FF+PET DPK 0.328 0.263 0.292 Table 2: Comparison of tree kernel performance for temporal relation extraction on THYME and TempEval-2013 data. resentations. These results show that, while DPK should not always replace SST, there are represen-tations in which it is superior to existing methods. This suggests an approach in which tree representa-tions are matched to different convolution kernels, for example by tuning on held-out data.

For TempEval-2013 data, adding syntactic fea-tures did not improve the performance significantly (comparing F-score of 0.290 with 0.286 in Ta-ble 3). Probably, syntactic information is not a strong feature for all types of temporal relations on TempEval-2013 data. In this paper, we developed a novel convolution tree kernel (DPK) for measuring syntactic similar-ity. This kernel uses a descending path represen-tation in trees to allow higher similarity scores on partially matching structures, while being simpler and faster than other methods for doing the same. Future work will explore 1) a composite kernel which uses DPK for PET trees, SST for BT and PT, and feature kernel for flat features, so that different tree kernels can work with their ideal syntactic rep-resentations; 2) incorporate dependency structures for tree kernel analysis 3) applying DPK to other relation extraction tasks on various corpora. Thanks to Sean Finan for technically supporting the experiments. The project described was supported by R01LM010090 (THYME) from the National Library Of Medicine.
