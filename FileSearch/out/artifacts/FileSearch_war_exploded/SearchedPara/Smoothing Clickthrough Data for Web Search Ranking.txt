 Incorporating features extracted from clickthrough data (called clickthrough features ) has been demonstrated to significantly improve the performance of ranking models for Web search ap-plications. Such benefits, however, are severely limited by the data sparseness problem, i.e., many queries and documents have no or very few clicks. The ranker thus cannot rely strongly on clickthrough features for document ranking. This paper presents two smoothing methods to expand clickthrough data: query clus-tering via Random Walk on click graphs and a discounting me-thod inspired by the Good-Turing estimator. Both methods are evaluated on real-world data in three Web search domains. Expe-rimental results show that the ranking models trained on smoothed clickthrough features consistently outperform those trained on unsmoothed features. This study demonstrates both the impor-tance and the benefits of dealing with the sparseness problem in clickthrough data.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Clickthrough Data, Smoothing, Random Walk, Discounting, Learning to Rank, Web Search We consider the task of ranking We b search results, i.e., a set of retrieved Web documents (URLs) are ordered by relevance to a query issued by a user. In this paper we assume that the task is performed using a ranking model (also called ranker for short) that is learned on labeled training data, i.e., human-judged query-document pairs. The ranking model is a function that maps the feature vector of a query-documen t pair to a real-valued relevance score. Such a learned ranking model is shown to be superior to classical retrieval models [6, 11] largely due to its ability to inte-grate both traditional criteria such as TF-IDF and BM25 values, and non-traditional featur es such as hyperlinks.
 tiple text streams. Some of the most useful text streams for Web search are (1) a content stream consisting of all the title and body texts in a page, (2) an anchor stream consisting of all the anchor WH[WVRIDSDJH X  s incoming links, and (3) a clickthrough stream consisting of all the user queries that have click(s) on the docu-ment. Recent research shows that incorporating features extracted from the clickthrough stream (called clickthrough features ) could significantly improve the performance of ranking models for Web search because the clickthrough stream can provide complementa-ry information about a user  X  s intention [1].
 ness problem. Two related aspects are involved. First, for a query, users only click on a very limited number of documents, thus the clicks are not complete. We refer to it as the incomplete click problem. Second, for many queries and documents, no click at all is made by users. We call this the missing click problem. As a consequence, the clickthrough streams for most of documents are either short or empty. Although one can use such raw text streams to extract some clickthrough features as in previous stu-the following reasons: First, with incomplete clicks, the click-related features that we can generate for a document-query pair are also incomplete and unreliable. Second, no clickthrough fea-tures can be generated for pairs without clicks. In the rankers used in most previous studies [1, 6, 7], this is equivalent to assign-ing zero values for clickthrough features. In ranker training, the zero-valued features make a categorical difference between the documents with and without clicks, and severely penalize the documents without clicks. However, in reality, WKH X WUXH  X GLIIH r-ence between these documents may be much smaller because a document could be unclicked for a variety of reasons even if the document is relevant. problem of determining the frequency or probability of an unseen event, which has been well-studied in the context of estimating n -gram language models [8]. Various smoothing techniques have been proposed and successfully used to deal with this problem, including clustering (by grouping observations on similar n -grams) and discounting (by assigning some counts to unseen n -grams) [8, 14]. In the case of clickthrough data, we can consider a click for a document-query pair as an n -gram. Then clickthrough data can also be smoothed in two directions: by clustering similar queries or by assigning non-zero values to the clickthrough features of unclicked documents through discounting. In this paper, we pro-pose to perform query clustering via Random Walk on click graphs, and a discounting method inspired by the Good-Turing estimator [13]. The Random Walk method is intended to address the incomplete click problem. In some particular settings, such as image retrieval [9] and query classification [21], it has been shown that expanding clicks to similar documents and queries via Random Walk can lead to significant improvements. However, to our knowledge, no study has been carried out on general Web search applications showing a similar improvement. Our experi-ments show that the expanded clickthrough data is noisy, and it should be used with caution. Effective improvement is possible only when we extract those features that are robust to noise for ranking. Notice that documents and queries with no click cannot be enriched through Random Walk. present a discounting method to estimate the values of the click-through features for the documents without clicks.
 can significantly improve the retrieval effectiveness compared to the utilization of raw clickthrough data. In particular, the simple discounting method will prove to be effective on all the three test datasets. This series of experiments strongly indicate that sparse-ness is a crucial problem in clickthrough data, and an appropriate solution to this problem allows us to better take advantage of clickthrough data. formation on clickthrough data and rankers. Section 3 presents two smoothing techniques. Section 4 presents experiments. Re-lated work and conclusions are presented in Sections 5 and 6. In this section, we first describe the clickthrough data we use and the way a Web document is represented by a clickthrough stream. Then, we present the clickthrough features to be incorporated in ranking models. Finally, we review the ranking model used in our experiments. Notice that we focus on clickthrough features in this paper. The features extracted from other text streams will remain unchanged and be used in the same manner as before. Clickthrough data used in this study consists of a set of query sessions that were extracted from one-year log files of a commer-cial Web search engine. A query session contains a query issued by a user and a rank list of (top-10) links browsed by the same user (with or without click). Following the notations in [18], a query session is represented by a triplet ( q , r , c ) consisting of the query q , the ranking r presented to the user, and the set c of links (documents) the user clicked on. Figure 1 shows a query session IRU WKH TXHU\  X ZHE PHVVDJH  X  7KH GRFXPHQWV DQG DUH also recorded.
 back for Web search ranking in two different ways. The first ap-proach is to derive training data from clickthrough data directly [18, 19, 26]. In particular, [19] argued that relative preferences derived from clicks are reasonably accurate. For example, in Figure 1, the document #2 is assumed to be more relevant to the TXHU\  X ZHE PHVVDJH  X  WKDQ #1 because #2 is clicked, and #1, though ranked higher than #2, is not clicked. By doing so, one could derive a large amount of preference pairs. Then a ranking algorithm, such as LambdaRank [6 ], can be trained on such prefe-rence pairs. clickthrough data and incorporate them into a ranking model [1, 28]. Our approach belongs to this category. The method is based on the assumption that all the queries that have clicks on a docu-ment form a description of the document from XVHUV X  perspective. One can see an example of such a clickthrough stream in Figures 2 for the document  X  webmessenger.msn.com  X  . It consists of all the queries that have one or more clicks on the document. In Figure 2, each line in a clickthrough stream consists of a query and a click-through score Score ( d , q ), which can be considered as the impor-tance of the query q in describing the document d , similarly to the TF-IDF scores. The score can be derived from raw click informa-tion recorded in log files heuristically. In our experiments, one of the simplest functions that work well across all data sets is: where  X  (  X  ,  X  ) is the number of times that d is shown to the users when q is issued (so,  X  (  X  ,  X  ) is sometimes called the number of impressions ),  X  (  X  ,  X  ,  X  X  X  X  X  ) is the number of times that d is 1. Message Web Design  X  Home 2. MSN Web Messenger 3. High School Baseball Web 4. Send a Wireless Web Message 5. SprintPCS 2Way SMS 6. Email on the Web 7. USA MOBILITY 8. Message Boards  X  rootsweb.com 9. Yahoo! Messenger  X  Chat, Instant message 10. Yahoo! Message Boards  X  Home Figure 1. 7KHTXHU\VHVVLRQIRUWKHTXHU\ X ZHEPHVVDJH  X  Marked in bold are the links the user clicked on. Figure 2. Fragments of the clickthrough stream for the link http://webmessenger.msn.com clicked for q , and  X  (  X  ,  X  ,  X  X  X  X  _  X  X  X  X  X  ) is the number of times that d is the temporally last click of q in clickthrough data. For example, in Figure 1 the documents #2 and #8 are the clicks of the query, but only #2 is the last click. Here, the weight  X  is a scaling factor, empirically tuned (  X  = 0.2 in our experiments). Intuitively, if a document is the last click of a query, there is a higher chance that the user is satisfied by this document and no additional document is necessary. Therefore, we boost the score of the last-clicked documents in the above formula. In modern Web search engines, search results are ranked based on a large number of features extracted from query-document pairs. Since a document is described by multiple text streams, multiple sets of features can be extracted, one from each stream (with re-spect to the query). Therefore, using clickthrough data for rank-ing is equivalent to incorporating the clickthrough features, which are extracted from the clickthrough steam, in the ranking algo-rithm. As described in [1], during training, the ranker can be learned as before but with additional features. At runtime, the search engine would fetch the clickthrough features associated with the given query-document pair and determine a relevance score.
 tures we used in our experiments, and describes how their values are computed from the clickthrough scores of the matched queries (to an input query). Let us illustrate this by an example. Consider a clickthrough stream consisting of 4 query-score pairs, as follows. Now, given a 4-word input query A B C D, the values of the clickthrough features are as follows.
 Many rankers can be used to incorporate a set of features, such as RankSVM [18], or RankNet [7]. In this study, we will use Lamb-daRank. Details can be found in [6]. We only sketch it here. y ). x is a feature vector extracted from a query-document pair, where the document is represented by multiple text streams as described in Section 2.1. We use about 300-400 features extracted from content and anchor text streams, including dynamic ranking features such as term freque ncy and BM25 value and static fea-tures similar to PageRank, as well as a set of (44 or 30) click-through features. y is a human-judged relevance score, from 0 to 4, with 4 as the most relevant.
 x to a real value y that indicates the relevance of the document given the query. For example, a linear LambdaRank simply maps x to y with a learned weight vector w such that  X  =  X  X  X  . Several non linear functions are provided in LambdaRank. LambdaRank is particularly interesting to us due to the way w is learned. Typi-cally, w is optimized with respect to a cost function using numeri-cal methods if the cost function is smooth and its gradient with respect to w can be computed easily. In order for the ranker to achieve the best performance in document retrieval, the cost func-tion used should be the same as, or as close as possible to, the measure used to assess the final quality of the system. In Web search, Normalized Discounted Cumulative Gain (NDCG) [17] is widely used as quality measure. For a query q , NDCG is com-puted as follows: where  X  (  X  ) is the relevance level of the j -th document, and the normalization constant N i is chosen so that a perfect ordering would result in  X   X  =1 .Here L is the ranking truncation level at which NDCG is computed. The  X   X  are then averaged over a query set. However, NDCG, if it were to be used as a cost func-tion, is either flat or discontinuous everywhere. It thus presents particular challenges to most optimization approaches that require the computation of the gradient of the cost function.
 function whose gradients are specified by rules. These rules are called  X  -functions . Burges et al. [6] studied several  X  -functions that were designed with the NDCG cost function in mind. They showed that LambdaRank with the best  X  -function outperforms significantly a similar neural net ranker, RankNet [7], whose pa-rameters are optimized using the cost function based on cross-entropy. In this paper, we will use LambdaRank with a sigmoid StreamLength_w # of words in CS StreamLength_q # of queries in CS WordsFound Ratio between # of words in q that occur in Complete-Matches PerfectMatches Sum of the scores of the queries in CS that ExactPhrases Sum of the scores of the queries in CS that Occurrences_i Sum of the scores of the queries in CS that Bigrams Sum of the scores of the queries in CS that InorderBigrams Sum of the scores of the queries in CS that  X  X  Figure 3. Some clickthrough features used in ranking models, where q is the input query, containing N query words (stop words are removed); CS is the clickthrough stream that consists of a set of query-score pairs. function, as our ranker and we explore different ways to integrate clickthrough features in it. An analysis of the data sets in all the three search domains of our study reveals a severe sparseness problem of the clickthrough data. Take the Japanese training data as an example. Around 75% of 2.62 million samples (i.e., query-document pairs) do not have any click (see Figure 4). That is, th e clickthrough features of about 1.95 million samples are assigned a zero value (the missing click problem). For the rest of the data, the lengths of the clickthrough streams have a very skewed distribution, with a majority of sam-ples having very short (&lt; 5 queries) clickthrough streams, as illu-strated in Figure 4 (the incomplete click problem).
 hand, to the bias of the search results retrieved by an imperfect search engine (i.e., most users only see a few top, typically 10, search results and do not see the others), and on the other hand, to the incomplete clicks by the user even if many relevant documents are shown to the user. Both sparseness problems have their coun-terparts defined and studied in the machine learning research community. The missing click problem can be viewed as a par-ticular example of the missing data problem [22], and the incom-plete click problem is related to confidence-weighted learning presented in [10]. We will return to the related work in Section 5. imperfect and unreliable (in the sense that an unclicked document is not necessarily non-interesting). Instead of using the raw click-through data, a better approach is to derive a new clickthrough GDWD ZKLFK FRQWDLQV WKH  X H[SHFWHG  X  FOLFNWKURXJK IHDWXUHV LQ which the raw clickthrough features are generalized or expanded to other documents. This idea is very similar to smoothing in sta-tistical language modeling (SLM). Many studies showed that the model trained with expected coun ts can better capture the lan-guage usage than the raw counts. It can then be expected that a similar processing on raw clickthrough data could produce a simi-lar effect.
 two methods to smooth clickthrough data: clustering and dis-counting. Clustering techniques have been widely used in language model-ing to improve the reliability of probability estimation [5, 14, 15]. Consider a large text corpus containing N words in which a word w 1 occurs once and another word w 2 occurs twice. If a unigram model were built using maximum likelihood estimation without smoothing, the model would say that the probability that w curs in a new text, P ( w 2 ), is twice as large as that of w However, these probabilities are not reliable because they are estimated on few samples. Now suppose that we could group sim-ilar words into clusters. Assume that W 1 and W of w 1 and w 2 , respectively. If W 1 occurs 200 times, and W times, then one is more confident to say that P ( W large as P ( W 1 ). tures. Consider the StreamLength feature as an example. We would not be confident to say that a document d Length_q = 2, is twice as popular as a document d 2 , with Stream-Length_q = 1. However, if we could expand the stream with  X VLPLODU  X TXHULHVWKDWDUHOLNHO\WRFOLFNWKHVDPHGRFXPHQWEX W are not recorded in log data for some reason, and observe that the expanded streams of d 1 and d 2 are 200 and 100 in Stream-Length_q, respectively, then we are more confident to say that d is more popular. Similar idea applies to other clickthrough fea-tures.
 should have clicked on the document. One can use a similarity defined according to query terms. However, this would unlikely add very different queries into the click stream. Another solution is to exploit co-clicks: queries for which users have clicked on the same documents can be considered to be similar. This principle has been successfully used in several studies [e.g., 3, 9, 27]. We follow the same principle here, but use a different approach: in-stead of defining a static function of similarity according to the number of co-clicks, we use random walk to derive it dynamical-ly. This approach has been successfully used in [9]. Figure 5 gives an example that illustrates this idea.
 representation of clickthrough data. We use  X   X   X   X   X  =1 set of query nodes and  X  X   X   X   X  =1  X  a set of document nodes. We fur-ther define an  X   X   X  matrix  X  in which element  X  the click count associated with  X  X   X  ,  X   X   X  . This matrix can be norma-lized to be a query-to-documen t transition matrix, denoted by  X  , where  X   X  X  =  X  (1) (  X   X  |  X   X  ) is the probability that  X  one step. Similarly, we can normalize the transpose of  X  to be a document-to-query transition matrix, denoted by  X  , where  X   X  the probability of transiting from any node to any other node in  X  steps. There are various ways of evaluating query similarities based on a click graph, e.g. using hitting time [25]. In this work, we use a simple measure which is the probability that one query transits to another in 2 s steps; and the corresponding probability matrix is given by (  X  X  )  X  . Although longer transitions could be used, the most effective transitions are the first ones, and longer transitions also raise the problem of efficiency. So, in our experi-ments, we limit s to 1.
 Figure 4 . Length distribution of the clickthrough streams (with 6WUHDP/HQJWKBT X  LQWKH-DSDQHVH training data, where x -axis is the stream length, and y -axis is the number of training samples; Bars at x = 0 shows the number of documents without click. Black bars correspond to the raw click counts and grey bars to the smoothed counts using Random Walk. through stream expansion. For each query  X  in the original click-through stream, we select up to 8 similar, previously absent que-ries to be added into the expanded stream. A newly added similar query T X  must satisfy  X   X  2  X   X  X   X   X  X  X  &gt;  X  , where  X  is tuned empirical-ly on validation data (  X  =0.01 in all the experiments in Section 4). Alternatively, we can select similar queries if  X   X  2  X  Empirical experiments show no significant difference in using these two heuristics. In Figure 4, one can observe the effect of Random Walk smoothing. The length of clickthrough stream is generally increased and we can expect more reliable clickthrough features to be extracted. However, we observe that the number of streams of length 0 remains the same because they are not af-fected by Random Walk smoothing. The technique of discounting described in the next subsection aims to solve this problem. Many smoothing methods have been proposed in SLM to deal with unseen words [13, 20]. Our method is inspired by the Good-Turing estimator, which will be reviewed briefly.
 words which occur in the text exactly r times, so that Good-7XULQJ X  s estimate P GT for a probability of a word that oc-curred in the sample r times is where The procedure of replacing an empirical count r with an adjusted count r * is called discounting , and the ratio r * / r is a discount coef-ficient. When applying Good-Turing discounting to estimating n -gram language model probabilities, Katz [20] suggested not dis-counting high values of counts, c onsidering them as reliable. That is, for r &gt; k (typically k = 5), we have r * = r .
 quency r +1. Let us denote it by C r+1 . Then Equation (5) can be rewritten as: Good-Turing method to our case is to replace a raw click count, with its adjusted count according to Equation (5). However, this does not work here. While the clickthrough scores are derived from the raw click counts, the values of the clickthrough features are computed based on not only the clickthrough scores but also the specific words in the clickthrough stream, as illustrated in Figure 3. If we were to adjust the raw click counts, we would have expanded the clickthrough stream of a document to an infi-nitely large set by assigning a non-zero score to any possible query that does not have a click on the document. This would make most of the features, whose values are based on word or n -gram matching, meaningless. Therefore, instead of discounting raw click counts as in the Good-Turing estimator, we have devel-oped a heuristic method, inspired by the Good-Turing estimator, which directly discounts the clickthrough feature values. ple whose clickthrough stream is of length r , where the length is measured as the number of the queries that have click(s) on the document (i.e., StreamLength_q in Figure 3). Assume that the feature values f r ,for r &gt; 0, have been smoothed using the Random Walk based method described in Section 3.1. To address the miss-ing click problem, we only need to estimate an adjusted click-through feature value f 0 * . Obviously, we have f raw clickthrough features.
 sample whose clickthrough stream is of length 1. The sum of f tion (6), f 0 * is computed as where n 0 is the number of the samples whose clickthrough streams are empty. Notice that the average value of f 1 samples is  X  1  X  =1/  X  1  X   X  1,  X   X  1  X  =1 .Since  X  0  X  X  ure 4, we have  X  1  X   X  X  0  X  &gt;  X  0 =0 . That is, for each type of click-through features, Equation (7) assigns a very small non-zero con-stant if the feature is in a training sample whose clickthrough stream is empty (i.e., the raw feature value is zero). This will prevent the ranker from considering unclicked documents to be categorically different from clicked ones. As a consequence, the ranker can rely more on the smoothed features. Before we empiri-cally test the impact of the smoothing method in the next section, here is an example of illustrating why this simple method might work. been retrieved based on their co ntent streams. Now, we want to adjust their ranks based on the ir clickthrough streams (i.e., using their clickthrough features such as PerfectMatches in Figure 3). Assume that d 1 has a lot of clicks and d 2 has no click because d a new URL and we have not collected enough click data for d If PerfectMatches = 0 for both d 1 and d 2 , intuitively d ranked higher because the fact that q does not match any queries, collected previously, which have clicks on d 2 seems to provide a piece of evidence that d 1 might be irrelevant, whereas there is no evidence about the (ir)relevance of d 2 . Using the discounting smoothing method of Equation (7), d 2 would be ranked higher, in agreement with our intuition. Figure 5. Before expansion, document  X  3 has a clickthrough stream consisting of query  X  2 only; after expansion, the click-through stream is augmented with query  X  1 which has a similar click pattern as  X  2 . We evaluated the two smoothing methods in three Web search domains, namely (1) a person name search domain, which con-sists of only person name queries, (2) a long query domain, which consists of queries containing four or more words, and (3) a Japa-nese query domain, which consists of queries users submitted to the Japanese search market. The statistics of these data sets are shown in Tables 1 to 3. We chose English name queries and long queries for our experiments because we've collected large amounts of clickthrough data in these domains and we believe that the clickthrough features, if their values could be properly esti-mated, should lead to a significant improvement. We also eva-luated our methods on Japanese queries because our Japanese clickthrough data is an order of magnitude smaller than English log data, but the human-labeled Japanese training data is almost an order of magnitude larger than the training data sets in the first two domains. We expect that the different settings could help us know in what case our methods perform well.
 tain queries that are sampled from the query log files of a com-mercial Web search engine of two non-overlapping periods of time. We used the more recent one as test set, and split the older data set into two non-overlapping data sets: training and valida-Web search scenario, where the ranking models in use are usually trained on previously collected data. clickthrough features and other 374 features. These data are ex-tracted from the same en-click data, which is generated from 1-year query sessions as follows. Each query is associated to a set of documents (URLs) clicked for it, toge ther with the click counts. For each query-document pair, we computed the counts  X  (  X  ,  X  ) , We only kept the pairs with C ( d , q )  X  5 and we computed the clickthrough scores according to Equation (1). In the Japanese query experiments, we used 30 clickthrough features and other 263 features. The clickthrough data set, jp-click in Table 3, is generated from 1-year Japanese query sessions using the same procedure as that of en-click. performance of all the ranking models in our experiments is measured by NDCG on the test sets. We report NDCG scores at positions 1, 3 and 10, and the averaged NDCG score (Ave-NDCG), which is the arithmetic mean of the NDCG scores at 1 to 10. We also performed significance test, i.e., t-test with a signi-Section 4.2, the difference between any pair of different rankers is statistically significant. Table 4 shows the results of the name query experiments. All the human-labeled data sets (in Table 1) consist of only person name queries. Row 1 in Table 4 is the result of the baseline ranker which is a 2-layer LambdaRank model with 10 hidden nodes and a learning rate of 10 -5 , trained on name-train. It uses 374 features, i.e. without clickthrough features.
 meter setting, but with an additional set of 44 clickthrough fea-tures extracted from the raw data. We incorporated these click-through features as follows. For each document in the three hu-man-labeled data sets (i.e., name-train, name-valid and name-test), we built a clickthrough stream as shown in Figure 2. Then for each query-document pair, we extracted the 44 features by match-ing the query to the clickthrough stream of the document, and computed the values of these features, as described in Section 2.2. Finally, we appended these new clickthrough features to each query-document pair. clickthrough features have been smoothed using the Good-Turing inspired discounting method, described in Section 3.2.
 through features through Random Walk. Considering that the query clusters generated by Random Walk are noisy, we also con-sider using only a subset of the expanded clickthrough features that are the most reliable as follows: We grouped these features into two categories -query-dependent features and query-independent features. The feature values of the former have to be en-click aggregated 1-year clickthrough name-train human-labeled training data 5,752 85 name-valid human-labeled validation data 476 154 name-test human-labeled test data 4,370 84
Table 1. Data sets in the name query domain experiments, where # qry is number of queries, and # doc/qry is number of documents per query.
 en-click aggregated 1-year clickthrough lon g -train human-labeled training data 6,255 93 lon g -valid human-labeled validation data 532 159 long-test human-labeled test data 5,785 123 Table 2. Data sets in the long query domain experiments.
 jp-click aggregated 1-year clickthrough jp-train human-labeled training data 47,919 55 jp -valid human-labeled validation data 4,730 119 jp -test human-labeled test data 3,959 178
Table 3. Data sets in the Japanese query domain experiments. # Models NDCG@1 NDCG@3 NDCG@10 AveNDCG 1  X  -Rank-374 0.4981 0.5130 0.5716 0.5363 2  X  -Rank-418 0.5021 0.5163 0.5723 0.5381 3 2 + GT 0.5151 0.5240 0.5776 0.5452 4 2 + RW-44 0.5151 0.5198 0.5737 0.5409 5 2 + RW-02 0.5187 0.5275 0.5787 0.5472 6 3 + RW-44 0.5219 0.5242 0.5752 0.5448 7 3 + RW-02 0.5398 0.5403 0.5879 0.5595 Table 4 . Test results on name-test.  X  -Rank-374 is a ranker trained using LambdaRank with 374 features; GT stands for the discount-ing method inspired by the Good-Turing estimator; RW-44 is the query smoothing method based on Random Walk, using 44 click-through features, while RW-02 uses 2 clickthrough features. computed by matching query words to the words in a stream, such as WordsFound and CompleteMatches in Figure 3. Since similar queries are generated automatically, the expanded stream may contain arbitrary queries that are irrelevant to the document. Therefore, the quality of the query-dependent features is very sensitive to the quality of the clustering algorithm, which unfortu-nately is by no means satisfactory on noisy log data. The second category contains only two StreamLength features, as in Figure 3 (number of words and number of queries in the clickthrough stream). Their values can reflect the popularity of a document IURPXVHUV X SHUVSHFWLYHVLPLODUWR BrowseRank [23]. More impor-tantly, since the StreamLength features do not take into account any specific word in a stream, but simply measure its length, they are much more robust to noise. Row 4 is the model trained using all the 44 expanded clickthrough features, and Row 5 is the model trained using only the two StreamLength features.
 smoothed using the discounting method.
 corporating clickthrough features improves the ranker significant-ly (Row 2 vs. Row 1); (2) as expected, smoothing can further boost the ranking performance by a large margin in the name do-main experiments (Row 7 vs. Row 2); (3) interestingly, the dis-counting method, though simple, brings a substantial improve-ment; and (4) the Random Walk method works well (Row 7 vs. Row 3) but not all the expanded query-dependent clickthrough features are reliable and they should be used with care 1 Row 6). Overall, both smoothing methods work very well in the name domain experiments, and the combination of the two smoothing methods lead to a 4.0% relative improvement (or a 2.1% of absolute improvement) in AveNDCG (Row 7 vs. Row 2), which is very significant even LQXVHUV X SHUFHSWLRQ 2 .
 the models were built similarly to those in Table 4, except the parameters for the LambdaRank: here, we use a 2-layer Lambda-Rank models with 15 hidden nodes and a learning rate of 10 trained on long-train. The results are consistent with those in the name query experiments. The combined smoothing method still substantially outperforms the unsmoothed model by 1.3% in AveNDCG (Row 7 vs. Row 2). There is, however, one noticeable difference from the results in Table 4: The contribution of the Random Walk method, which only uses the two StreamLength features, is smaller than that of the discounting method in the long query experiments: We see in Table 5 that the discounting method contributed to a 0.82% improvement in AveNDCG (comparing results in Row 3 vs. Row 2), which is almost twice as large as that of the Random Walk method, which is 0.46% (Row 5 vs. Row 2). All the models are trained similarly to those in Table 4. The base-line ranker (Row 1) is a 2-layer LambdaRank models with 10 263 features. 30 clickthrough features are used. Comparing to the 
It is possible that a subset of the query-dependent features (after some transformation) is useful for ranking. We have not fully exploited each individual query-dependent feature, with differ-ent transformations. We leave it to future work.
A user study conducted by Microsoft Live Search (p.c.) shows that users start to sense the improvement of ranking when the 
NDCG improvement is larger than 0.5%. experimental settings of the other two search domains abovemen-tioned, in the Japanese experiments, the human-labeled training data is much larger and clickthrough data is much smaller. This difference leads to some changes in the results: the discounting method produces a smaller improvement than on two other data-sets and the Random Walk method fails to bring any improvement. This result suggests the following possible interpretations: (1) When the amount of human-judged document-query pairs is very large, the advantage of exploiting clickthrough data is reduced. (2) The smaller amount of clickthrough data leads to much noisier expansion by the Random Walk method. In this case, it is even better not to expand the data than to do it. Therefore, the impact of the Random Walk method is more subject to the amount of click-through data than that of the discounting method.
 gest: (1) Incorporating clickthrough features can improve the per-formance of rankers substantially. (2) Smoothing clickthrough features can reduce the sparseness problem of these features, and lead to some further, significant improvements. (3) The discount-ing method, inspired by the Good-Turing estimator, is simple and effective. It works very well across all the data sets we tested. (4) The Random Walk method also helps in some cases, but this de-pends more on the amount of raw clickthrough data. Although the sparseness problem of clickthrough data has been reported in many recent studies [1, 9, 26, 28], no effective solu-knowledge, the Random Walk method reported in [9] is perhaps the closest work to ours. However, [9] only tested the method on the application of image search, leaving it unclear whether it can be extended to general Web search. Clickthrough data plays a much more important role in ranki ng images than in ranking text documents in general Web search because the content text streams of images are usually much less informative. Radlinski et al. [26] argued that missing click is due to the ranking bias of a search engine and proposed an active learning method to collect more click data by modifying the original ranking list. Given the large amount of missing clicks, the extent to which the method could alleviate the missing click problem is questionable. # Models NDCG@1 NDCG@3 NDCG@10 AveNDCG 1  X  -Rank-374 0.4302 0.4341 0.4642 0.4456 2  X  -Rank-418 0.4486 0.4432 0.4697 0.4539 3 2 + GT 0.4520 0.4478 0.4728 0.4576 4 2 + RW-44 0.4507 0.4415 0.4675 0.4525 5 2 + RW-02 0.4538 0.4462 0.4710 0.4560 6 3 + RW-44 0.4473 0.4405 0.4667 0.4511 7 3 + RW-02 0.4563 0.4500 0.4748 0.4598 Table 5 . Test results on long-test. # Models NDCG @ 1NDCG @ 3NDCG @ 10 AveNDCG 1  X  -Rank-263 0.5555 0.5427 0.5418 0.5424 2  X  -Rank-293 0.5589 0.5503 0.5525 0.5515 3 2 + GT 0.5658 0.5542 0.5500 0.5528 4 2 + RW-30 0.5595 0.5456 0.5425 0.5448 5 2 + RW-02 0.5603 0.5482 0.5471 0.5482 6 3 + RW-30 0.5639 0.5518 0.5456 0.5490 7 3 + RW-02 0.5631 0.5537 0.5485 0.5517 Table 6. Test results on jp-test. the missing data problem that has been well-studied in the ma-chine learning community (e.g., [4, 16, 22]). Various heuristics have been proposed. In general, missing feature values are re-placed by integration over (e.g., the mean of) the corresponding features whose value is available, weighted (or discounted) by the appropriated distribution [2, 24]. Our discounting method is also a heuristic and shares some simi larities with them. One area of our future work is to explore the problem in a more principled way such as the work presented in [12] (though their method can-not be applied to our case directly), where missing data in density estimation problems are dealt with by seeking a maximum like-lihood solution using the exp ectation maximization algorithm. complete click problem. Incomplete click makes the feature val-ues unreliable for training. The Random Walk method tries to improve the reliability of features via smoothing the feature val-ues before training. An alternative strateg y is to take into account the uncertainty of feature values during training. Dredze et al. [10] introduced a class of online learning methods, called confidence-weighted learning , where a measure of confidence (reliability) of each feature is maintained during training so that each feature weight can be updated separately according to its confidence score. Clickthrough data have proven useful for document ranking in Web search. However, their sparseness prevents the ranker from strongly rely on these data. In this paper, we have presented two smoothing techniques for expanding clickthrough features: Dis-counting and Random Walk. We have demonstrated th at they lead to significant improvements compared to the utilization of raw clickthrough data. In particular, the discounting method is simple, robust, and effective. This work demonstrates both the importance and the benefits of dealing with the sparseness problem of click-through data. In our future work we will refine the smoothing techniques to reach the full potential of clickthrough data. [1] Agichtein, E., Brill, E. and Dumais, S. 2006. Improving web [2] Ahmad, S. and Tresp, V. 1993. Some solutions to the miss-[3] Baeza-Yates, R. and Tiberi, A. 2007. Extracti ng semantic [4] Bishop, C. M. 1995. Neural networks for pattern recognition. [5] Brown, P.F., Della Pietra, V. J., de Souza, P. V., Lai, J. C. [6] Burges, C. J., Ragno, R., &amp; Le, Q. V. 2006. Learning to rank [7] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., [8] Chen, S. and Goodman, J. 1998. An empirical study of [9] Craswell, N. and Szummer, M. 2007. Random walk on the [10] Dredze, M., Crammer, K. and Pereira, R. 2008. Confidence-[11] Gao, J., Qin, H., Xia, X. and Nie, J-Y. 2005. Linear discri-[12] Ghahramani, Z. and Jordan, M. I. 1994. Supervised learning [13] Good, I. J. 1953. The population frequencies of species and [14] Goodman, J. 2001. A bit of progress in language modeling [15] Goodman, J. and Gao, J. 2000. Language model size reduc-[16] Hastie, T., Tibshirani, R. and Friedman, J. 2001. The ele-[17] Jarvelin, K. and Kekalainen, J. 2000. IR evaluation methods [18] Joachims, T. 2002. Optimizing search engines using click-[19] Joachims, T., Granka, L., Pan, B., Hembrooke, H. and Gay, [20] Katz, S. M. 1987. Estimation of probabilities from sparse [21] Li, X., Wang, Y-Y. and Acero, A. 2008. Learning query [22] Little, R. J. A. and Rubin, D. B. 1987. Statistical analysis [23] Liu, Y., Gao, B., Liu, T., Zhang, Y., Ma, Z., He, S., and Li, [24] Lowe, D. and Webb, A. R. 1990. Exploit prior knowledge in [25] Mei, Q., Zhou, D. and Church, K. 2008. Query Suggestion [26] Radlinski, F., Kurup, M. and Joachims, T. 2007. Active ex-[27] Wen, J. Nie, J.Y. and Zhang, H. 2002. Query Clustering [28] Xue, G., Zeng, H-J., Chen, Z., Yu, Y., Ma, W-Y., Xi, W. and 
