 We address the query-performance-prediction task for entity retrieval; that is, retrieval effectiveness is estimated with no relevance judgements. First we show how to adapt state-of-the-art query-performance predictors proposed for doc-ument retrieval to the entity retrieval domain. We then present a novel predictor that is based on the cluster hy-pothesis. Evaluation performed with the INEX entity rank-ing track collections shows that our predictor can often out-perform the most effective predictors we experimented with.
Recently, it has been observed that for many user queries, named entities such as people, organizations and locations, better satisfy the user X  X  information need than full docu-ments [10]. Accordingly, there is a growing body of work on entity retrieval which deals with ranking entities by their presumed relevance to a query.

Entities are somewhat complex objects which are charac-terized by different properties such as name, type and as-sociated document. Several entity retrieval methods that exploit these properties have been proposed [1, 11].
Here we address the query-performance-prediction (QPP) task for entity retrieval. The goal is to estimate, without rel-evance judgements, the effectiveness of retrieval performed in response to a query. While there is a large body of work on QPP for document retrieval [2], there has been very little work on QPP for entity retrieval [9]. Yet, the same motiva-tion that triggered the development of predictors for docu-ment retrieval holds for entity retrieval. For example, alerts for ineffective retrieval can direct users to better formulate their queries.

We present a study of adapting state-of-the-art query-performance predictors, proposed for document retrieval, to the entity retrieval domain. In addition, we present a novel query-performance predictor for entity retrieval. The pre-dictor relies on retrieval scores of clustered entities, follow-ing a study of the cluster hypothesis for entity retrieval [12]. Evaluation performed with the INEX entity ranking track collections shows that our novel predictor can often outper-form the most effective predictors we experimented with.
Query-performance prediction methods proposed for doc-ument retrieval can be categorized to two groups [2]. Pre-retrieval predictors analyze the query using corpus-based term statistics [7]. Post-retrieval predictors analyze also the result list of top-retrieved documents [2]. We adapt the most effective of these predictors to the entity retrieval domain.
To the best of our knowledge, there is a single report of work on QPP for entity retrieval [9]. The entity-list comple-tion task was addressed where examples of relevant entities are provided. The most effective predictors used the descrip-tion and narrative of the (INEX) topic as well as information induced from the example entities. In contrast, we address the entity ranking task, and the predictors we study do not use entity feedback (i.e., examples) nor the topic X  X  narrative and description. We show that post-retrieval predictors out-perform pre-retrieval predictors, which was not the case in this work [9] that did not adapt state-of-the-art predictors proposed for document retrieval.
Our focus is on predicting retrieval performance for queries whose goal is finding entities of a particular type or class [10]. We use the datasets of the INEX entity ranking track [4, 5]. Each entity in the corpus is represented as a Wikipedia page associated with a set of categories which serve as the entity X  X  type. The entity ranking task queries are composed of a short keyword-based title and a set of categories represent-ing the query X  X  target type. Entities relevant to the query are expected to be associated with categories in the query X  X  target type, or with categories that are  X  X lose X  to those in the target type in the Wikipedia category graph.

Most entity retrieval methods utilize several properties of entities [14, 1, 11]. Typical properties are the document as-sociated with the entity (the Wikipedia page in our case), the entity type (the set of categories associated with the en-tity in our case), the entity name (the Wikipedia page title), etc. Accordingly, we study prediction methods that use in-formation induced from two properties which were found to be highly effective for retrieval [1, 11]; namely, the document associated with the entity and the entity type.
Specifically, the prediction methods that we present use t hree entity representations. The first is doc , under which an entity is represented by its associated document. The sec-ond representation, type , is the bag of terms that appear in the names of the categories that constitute the entity type. Unless otherwise stated, whenever the doc and type repre-sentations are used, we use the set of terms in the query title and the set of terms in the names of the categories which constitute the query target type, respectively. The third en-tity representation, score , is the retrieval score assigned to the entity. The score can rely on either (or both) properties of the entity (its associated document and its type).
Below we present query-performance prediction approaches, denoted P , which utilize the entity representations. We use P rep = r , where r  X  { doc, type, score } is the entity representa-tion used by P , to denote the resultant prediction methods.
Some of the predictors we explore utilize inter-entity simi-larity measures. The first measure, referred to as sim = doc , is the language-model-based similarity between the (Wikipedia) documents associated with the entities. The similarity be-tween documents x and y is exp(  X  CE ( p [0] x (  X  ) || p CE is the cross entropy measure; p [  X  ] z (  X  ) is the Dirichlet-smoothed unigram language model induced from z with the smoothing parameter  X  . The second inter-entity similarity measure is based on the entity type: sim = type . The mea-sure is the cosine similarity between the binary vectors that represent two entities in the category space. An entry in the vector is 1 if the corresponding category is associated with the entity and 0 otherwise.

To integrate a predictor which uses the doc entity repre-sentation (inter-entity similarity measure) with a predictor which uses the type representation (inter-entity similarity measure) we multiply the prediction values and denote the integration as rep = doc  X  type ( sim = doc  X  type ).
Pre-retrieval prediction methods analyze the query using corpus-based term statistics prior to retrieval. We adapt two highly effective pre-retrieval methods from document retrieval to the entity retrieval setting.

The first type of predictors is based on analyzing the in-verse document frequency ( IDF ) values of the set of terms in the query title; the doc entity representation is used. The re-sultant predictors are named A IDF rep = doc (cf., [3, 7]), where A  X  { avg, sum, max } is the aggregation type (average, sum-mation, maximization) of the terms X  IDF values.

We also use the IDF values of the set of terms that ap-pear in the names of the categories that constitute the query target type. The type entity representation is used yielding
The predictors just described quantify the discriminative power of the query by analyzing the IDF values of either its title or target type terms. Along the same lines, we study the A VarTF . IDF rep = doc predictor (cf. [16]) which measures for each query title term the variance of its tf-idf values across all the entity documents that contain it 1 .
E xperiments showed that using the VarTF . IDF predictors with the type entity representation yields poor prediction quality. Actual numbers are omitted as they convey no ad-ditional insight.
We now describe post-retrieval predictors that analyze the n most highly ranked entities in a result list retrieved by an entity retrieval method; n is a free parameter.
 Clarity. The Clarity prediction method [3], proposed for document retrieval, is based on the premise that the more focused the result list with respect to the corpus the more ef-fective the retrieval. Specifically, the KL divergence between a relevance language model [8] induced from the result list and a language model induced from the corpus is used to measure focus. For the entity retrieval task, we use the doc entity representation for Clarity computation. The resul-tant Clarity rep = doc predictor is the analogue of the Clarity predictor used for document retrieval [3]. Alternatively, the focus of the entity result list can be measured using the type entity representation, yielding the Clarity rep = type predictor. Particularly, a relevance language model induced from the bags of terms that represent the entity types is used. QF. Query feedback ( QF ) [17] is based on measuring the robustness of the result list. Specifically, a relevance model is constructed from the original result list and is used to re-trieve a second list from the corpus. The overlap between the two lists, measured by the number of documents which are at the l qf highest ranks of both lists, is used for prediction; l qf is a free parameter. Higher prediction value presum-ably attests to improved robustness of the result list, and therefore to increased retrieval effectiveness. For the entity retrieval task, we simply use the doc entity representation for QF computation. The resultant QF rep = doc predictor is the analogue of that used for document retrieval.
 WIG and NQC . The WIG [17] and NQC [13] methods measure the mean and standard deviation, respectively, of document retrieval scores in the result list. To apply WIG and NQC for the entity retrieval task, we use the score entity representation; i.e, the retrieval scores of entities in the result list are utilized. The resultant predictors are sult list indicates effective retrieval [2]. We measure the cohesion of the entity result list by the average similarity between two entities in the list using the doc and type inter-entity similarity measures. The resultant predictors are de-( AC ), which was proposed for document retrieval, measures the extent to which similar documents in the result list are assigned with similar retrieval scores. We use AC for the entity retrieval task as follows. First, the retrieval scores of the entities in the result list are normalized to have a zero mean and unit variance. Then, all entities in the list are assigned with a second score. This new ( X  X egularized X ) score is the weighted average of the original (normalized)
W e do not use the corpus-based retrieval score normaliza-tion as in the original implementations of WIG [17] and NQC [13]. Rather, we sum-normalize the entity retrieval score with respect to the scores of all entities in the result list following previous recommendations [13]. scores of the entity X  X  k n earest neighbors in the list; k is a free parameter. Nearest neighbors are determined using the inter-entity similarity measures ( doc or type ) which also serve for weighting. The prediction value is the Pearson correlation between the original (normalized) scores in the list and the new scores. The resultant predictors, which dif-fer by the inter-entity similarity measure employed, are de-predictors are based on the premise that  X  X imilar X  entities should be assigned with similar retrieval scores. This pre-diction principle is a manifestation of the cluster hypothesis which was recently explored for entity retrieval [12]. Max Cluster Score ( MCS ). The AC predictor can assign high prediction values to result lists with very low (yet sim-ilar) retrieval scores. The WIG predictor assigns a high prediction value if the entities X  scores at the top ranks of the list are high. However, WIG does not account for the ex-tent to which similar entities are assigned with similar scores. Hence, to conceptually leverage the strengths of the two ap-proaches, we present a novel prediction method ( MCS ).
The predictor uses nearest-neighbor clustering of the en-tity result list. Each entity and its k nearest neighbors in the list form a cluster. The score of a cluster is the geomet-ric mean of the normalized retrieval scores of its constituent entities [12]. 3 The maximal cluster score is the prediction ilarity measures, respectively, to create clusters. The predic-tion principle is that a result list which contains entities that are (i) similar to each other, and (ii) assigned with high re-trieval scores, is likely to be effective.
We performed experiments with the datasets of the INEX entity ranking track of 2007 and 2008 [4, 5]. These tracks used the English Wikipedia dataset from 2006. The tracks provide a total of 109 topics for the entity ranking task, which were originally used for training and testing. We use all of these queries in our experiments. 4 The data is pre-processed using Lucene 5 , including tokenization, stopword removal, and Porter stemming.

To measure prediction quality, we follow common prac-tice in work on QPP for document retrieval [2]. We use the Pearson correlation between the prediction values assigned to a set of queries by a predictor and the ground-truth av-erage precision (AP@1000) which is determined based on relevance judgements. 6
To set the values of free parameters of predictors, we ap-plied 100 tests of 2-fold cross validation performed over all
N ormalized retrieval scores are attained by a sum-normalization of the exponents of the original scores.
We did not use the 2009 dataset since there are too few queries for learning free-parameter values of predictors. http://lucene.apache.org/core/
The performance for queries of the 2008 track was orig-inally evaluated using extended inferred average precision (xinfAP) [15]. We found that the standard AP measure is 99 . 99% correlated with xinfAP for the retrieval methods we use. Hence, for consistency with the queries used in 2007, AP was used in all experiments. queries. The resultant average prediction quality is reported. Statistically significant differences of prediction quality are determined using the two-tailed paired t-test computed over the folds using a 95% confidence level. Prediction quality (measured using Pearson correlation) serves as the optimiza-tion criterion in the learning phase. The 2-fold procedure enables to have enough queries ( ~ 55) in both the train and test sets so as to compute Pearson correlation in a robust manner. The free-parameter values of each predictor X  X  ver-sion ( doc , type and doc  X  type ) were learned separately.
Clarity and QF use the RM1 relevance model [8] which is constructed from maximum likelihood estimates of the entities X  representations ( doc or type ). The exponent of the entities X  retrieval scores (described below) serve for entity weighting. The number of terms used by RM1, and the number of top-retrieved entities used to construct it, are set to values in { 10 , 50 , 100 } and { 25 , 50 , 100 } , respectively. QF X  X  l qf parameter is selected from { 5 , 10 , 20 , 30 , 40 , 50 } . The number of most highly ranked entities considered by WIG and NQC , n , is selected from { 5 , 10 , 20 , 30 , 40 , 50 , 100 } and { 10 , 20 , 30 , 40 , 50 , 100 , 500 } , respectively. For Cohesion , AC and MCS , n is set to values in { 10 , 50 , 100 } . The num-ber of nearest neighbors, k , used in the AC and MCS pre-dictors, is selected from { 4 , 9 } .

We predict the effectiveness of two lists, each contains 1000 entities, that are retrieved using effective methods [11]. The first, L D , is created by applying a standard language-model-based approach upon the doc representation of en-tities. The score of entity e , represented by document e with respect to query q is based on the cross entropy mea-scription of the inter-entity similarity measures in Section 3 for details regarding the language model notation used.) The second list, L D ; T , is created by re-ranking L D using a linear interpolation of two entity retrieval scores. The first is that used to create L D (i.e., S D ( e )). The second is an entity-type-based score, S T ( e ). Specifically, it is the mi-nus of the minimum (normalized) distance, over Wikipedia X  X  category graph, between a category in the query target type and a category among those associated with e . The interpo-lated score assigned to e is:  X  log exp( S D ( e )) P The rest of the technical details regarding the implementa-tion of the retrieval methods follow those in [11].
Table 1 presents the prediction quality numbers. Our first observation is that the most effective pre-retrieval predictors are outperformed by the most effective post-retrieval predic-tors, as reported for document retrieval [2]. Also, the Clarity predictors are less effective than most other post-retrieval predictors. QF , which is a state-of-the-art predictor for document retrieval, is outperformed (often substantially) by quite a few other post-retrieval predictors. WIG and NQC , which analyze retrieval scores, are highly effective, similarly to the case for document retrieval [2].

The Cohesion predictor posts poor prediction quality when using the doc inter-entity similarity measure. This finding is in accordance with those reported for document retrieval [2]. However, the prediction quality is relatively high when using the type inter-entity similarity measure. Thus, an en-tity result list which is cohesive in terms of the categories of the entities it contains is somewhat likely to be effective. In c ontrast to the case for Cohesion , for the AC predictor the doc inter-entity similarity measure is more effective than the type measure. This finding could potentially be attributed to the sparseness of the type measure. That is, in some cases an entity might not share categories with other entities in the list and hence the inter-entity similarity is 0. We use entity IDs to break similarity ties.

Predictors employed with both the rep = doc and rep = type representations are in most cases more effective when using the former than the latter. Yet, in quite a few cases (e.g., for maxIDF and Clarity ), using both representations ( rep = doc  X  type ) is superior to using either.

The prediction quality for almost all predictors is higher for the L D list than it is for the L D ; T list. Recall that L is a re-ranked version of L D created by interpolation of two entity scores. The first is based on the entity X  X  document and the second is based on the entity X  X  categories. However, the category-based information (distance in the Wikipedia category graph) is different than that used by the prediction methods (terms in categories X  names), and therefore the pre-diction quality for L D ; T might be lower. We hasten to point out, however, that some of the prediction quality numbers for L D ; T are quite high and competitive with those for L scores.
 Our novel MCS predictor is the most effective for the L
D list when using the doc inter-entity similarity measure tistically significant degree all other predictors. Further-significant degree all predictors except for WIG . For the L tively. While the former outperforms all predictors, except for WIG , to a statistically significant degree, it is outper-formed by WIG in a statistically significant manner. All in all, these findings attest to the merits of our MCS predictor that relies on the cluster hypothesis.
We presented a study of adapting query-performance pre-dictors proposed for document retrieval to the entity re-trieval domain. We also presented a novel predictor that relies on the cluster hypothesis and showed that it can often outperform the most effective predictors we studied.
We thank the reviewers for their comments. This paper is based on work supported in part by the Israel Science Foundation under grant no. 433/12 and by a Google faculty research award. Table 1: Prediction quality. The best result i n a column per a result list is boldfaced.  X  d  X ,  X  t  X  and  X   X   X  mark statistically significant differences
