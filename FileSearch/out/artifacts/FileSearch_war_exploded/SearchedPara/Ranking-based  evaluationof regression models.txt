 REGULAR PAPER Saharon Rosset  X  Claudia Perlich  X  Bianca Zadrozny Abstract We suggest the use of ranking-based evaluation measures for regres-sion models, as a complement to the commonly used residual-based evaluation. We argue that in some cases, such as the case study we present, ranking can be the main underlying goal in building a regression model, and ranking performance is the correct evaluation metric. However, even when ranking is not the contex-tually correct performance metric, the measures we explore still have significant advantages: They are robust against extreme outliers in the evaluation set; and they are interpretable. The two measures we consider correspond closely to non-parametric correlation coefficients commonly used in data analysis (Spearman X  X   X  and Kendall X  X   X  ); and they both have interesting graphical representations, which, similarly to ROC curves, offer useful various model performance views, in addi-tion to a one-number summary in the area under the curve. An interesting exten-sion which we explore is to evaluate models on their performance in  X  X artially X  ranking the data, which we argue can better represent the utility of the model in many cases. We illustrate our methods on a case study of evaluating IT Wallet size estimation models for IBM X  X  customers.
 Keywords Model evaluation  X  Evaluation robustness  X  Regression  X  Ranking correlation  X  Performance visualization 1 Introduction The standard approach to evaluating regression models on holdout data is through additive, residual-based loss functions, such as squared error loss or absolute loss. These measures are attractive from a statistical perspective, as they have likeli-hood interpretations, and from an engineering or scientific perspective because they often represent the  X  X rue X  cost of the prediction errors.
 models, through their success in ranking the test set observations in the correct or-der. There are several reasons why ranking-based evaluation of regression models is interesting: 2. Ranking-based measures are quite interpretable. The two main evaluation 3. Ranking-based evaluation is robust. That is, it is only mildly affected by errors nomenclature, describe the standard, residual-based evaluation approaches, define the concept of ranking-based evaluation, and introduce our suggested ranking-based measures. We analyze these measures, their interpretations, their statistical properties, and their visualizations in Sect. 3 . Section 4 is devoted to the topic of  X  X valuation robustness X  X  X e define an evaluation robustness measure and show that the residual-based additive methods are non-robust, while ranking-based mea-sures are very robust. We then consider the interesting extension of ranking-based evaluation to evaluate partial ranking in Sect. 5 . In Sects. 6 and 7 ,wepresent empirical results on our motivating case study and the direct mailing domain. 2 Residual-based and ranking-based evaluation of regression models In this paper, we concentrate on the model evaluation phase of supervised learning of regression models. We do not consider at all the process of model building , instead assuming that we are given a model  X  y = m ( x ) which predicts a response y  X  R as a function of a feature vector x . Assume further, we have a test set of performance of model m in predicting y . In reality, we may often have multiple models m 1 ( x ),..., m k ( x ) , and we evaluate them as a way of facilitating a model selection decision. We may also employ cross validation instead of pure holdout data for model evaluation. Our methods and results apply to these situations as well as we demonstrate in Sects. 6 and 7 . For simplicity of exposition, we stick here with the single model and holdout data case.
 on additive measures of error, depending on the residuals r i = y i  X  m ( x i ) : Some commonly used loss functions are: curves [ 2 , 10 ], where the model is evaluated according to its error rate at different levels of  X  X rror tolerance X  and using medians of the absolute deviations (MAD), rather than their mean, as the error measure: the use of ranking-based measures, which evaluate the performance of the scoring model m ( x ) in sorting the values of y from  X  X arge X  to  X  X mall. X  We assume, without loss of generality, that the test set is sorted in decreasing order of model scores, that is: 1 We rank the observed responses in the test set in decreasing order. Let s i be the rank of observation i in the original order: Ranking-based evaluation uses only these ranks in evaluating model performance. We consider here two ranking-based evaluation measures and their interpretations. We start by defining the following intuitive statistics: ordered incorrectly by the model m ( x ) . The second also considers these incorrect orderings but weighs them by the difference in their model ranks, a measure of the magnitude of error being committed.
 [ X  1 , 1 ] , where 1 corresponds to perfect model performance ( T , R = 0) and  X  1 corresponds to making all possible errors, thus attaining perfect reverse ranking. Thus, we re-scale: This, in fact, gives us exactly Kendall X  X   X   X  and Spearman X  X   X   X  [ 8 ] X  X he measures for non-parametric correlation prevalent in statistical data analysis tools. Here, we use the notation  X   X  and  X   X  to denote that these are the sample quantities (which [ 8 ] denoted by t , r , respectively), as compared to the  X  X opulation X  quantities:  X  = E  X   X  ,  X  = E  X   X  .
 corresponding to perfect ranking and 0 to inverse ranking. 3 Interpretations and visualizations In this section, we present some of the interpretations and visualization approaches that we can apply to our ranking evaluation measures  X   X  ,  X   X  . 3.1 Statistical properties of  X   X  Of the two measures we suggest,  X   X  is the more natural target for a statistical anal-ysis. First, we observe that  X   X  can be interpreted as the percentage of all pairs that are correctly ranked from the total of n ( n  X  1 )/ 2 pairs of observations [ 8 ]. Thus, E  X   X  is the probability of correctly ranking a randomly drawn pair of observations. the ROC curve [ 3 ]. Second, we are interested the distribution of  X   X  and  X   X  and the uncertainty inherent to them. The variances of  X   X  and  X   X  under the relevant null assumptions (  X  = 0and  X  = 0, respectively) are quite easy to calculate, and a normal approximation gives a hypothesis testing methodology for the assumption of no correlation [ 8 ]. For example, the variance of  X   X  turns out to be: where the H 0 model scores the data randomly compared to the true response. This leads to an approximate test for no correlation between the model scores and the true responses of the form: Reject H 0 at confidence level  X  if: the ranking by scores and by actual test set responses are independent X  X s of little interest in most cases for model evaluation. We expect any reasonable prediction model to create rankings that are indeed correlated with the rankings by response. uation measure given its sample value (the non-null case), as they represent the uncertainty in the model evaluation based on a single test set. The non-parametric nature of  X   X  allows us to write a general expression for its variance [ 9 ]: and  X  cc the probability of correctly ranking a random observation relative to two other random observations.
 sample means (which are consistent estimators), and after some algebra obtain [ 9 ]:
Var (  X   X ) = that are  X  X oncordant X  with observation i , that is, that their ranking relative to i in the test data agrees with the ranking by model scores (as plotted in Fig. 1 later). 1  X   X  confidence interval for  X  as: We are not aware of a similar calculation for  X   X  . 3.2 Visualizations Visualizations of ranking performance can often provide additional insights about model performances. We will use the data from the case study in Sect. 6 to illus-trate the different visualization approaches. The dataset was collected from a sur-vey of 500 firms reporting the amount of money allocated in 2003 to the purchase of IT goods. The 500 observations are considered the ground truth against which we evaluate an IBM internal model. Starting from the largest model prediction m ( x 1 ) , for each observation x i , we calculate the percentage of correctly ranked pairs ( y i , y j ) over all j = i in decreasing order. Figure 1 shows this percentage of correctly ranked pairs as a function of the rank. The area above the curve is the sum of the percent incorrectly ranked pairs, which is equal to 2 T / n ( n  X  1 ) . Therefore, the area under the curve equals  X   X  . The dashed line corresponds to a lo-cally optimal performance. A perfect model would show a constant performance of 100% correctly ranked pairs. But given that the model is not perfect and makes predictions that are sometimes too large or too small, even a perfect prediction for a particular observation with m ( x i ) = y i will have a number of inversely ranked pairs due to errors of the other predictions. The upper limit of the performance for a given prediction, keeping everything else constant, is therefore not 100% but determined by the model performance of predictions around it. The interpre-tation of the locally optimal performance is the highest achievable percentage of correctly ranked pairs if m ( x i ) could be placed arbitrarily, given all other model predictions.
 properties of the plot: (1) the distance of the local optimum from the 100% line, and (2) the distance of the actual performance from the local optimum. A particular region with a performance that on average remains very close to the This graph provides a number of relevant insights for model evaluation and analysis:  X  It shows the variability of the ranking performance for different prediction re- X  It identifies large outliers with less than 50% correct pairs. These predictions regression results into n  X  1 classification results, where we discretize the obser-1 &lt; i &lt; n .
 AUC is the probability that a pair of observations with opposite class labels is ranked correctly. Since AUC i only considers pairs with different class labels un-der cutoff i , the number of pairs used to calculate AUC i is equal to i ( n  X  i ) and the number of incorrectly ranked pairs under this cutoff is therefore (1-AUC i ) i ( n  X  i ) .
 boring pair ( k , k + 1) receives opposite class labels only if the cutoff is n  X  1 cutoffs. Given our definition in ( 2 ), this implies that ( n  X  i ) = R .
 as a function of the cutoff i would not have an area equal to  X   X  . In order to achieve a direct correspondence with  X   X  ,wehavetoallocate i ( n  X  i ) units to AUC i by rescaling the x -axis accordingly. Figure 2 shows such a transformation of the x -axis and has an area under the curve of  X   X  . This graph confirms our earlier notion that the model performs better in ranking large outcomes and worse in ranking small outcomes.
 predictions in decreasing order, we plot the cumulative inverse rank of the truth p emphasizes the model performance on the largest predictions that is shown in the bottom left of the graph. The model performance is bounded above by the optimal ranks p i = i j = 1 ( n  X  j + 1 ) and below by the cumulative worst (inverse) ranking w can be shown to be equal to n 3  X  n 2  X  n  X  R + n i = 1 i 2 exposing the relationships to  X   X  , for further details see [ 8 ]. 4 Evaluation robustness and ranking-based evaluation A commonly used definition of robustness in model fitting uses the concept of fitting breakdown point . A simplified version of the breakdown point definition of Hampel et al. ([ 7 , p. 98]) reads: point of 1 / n . Thus, this is a non-robust procedure X  X ne corrupted data point can affect the fitted model arbitrarily badly. Linear regression with absolute loss, on the other hand, has a breakdown point of  X  X lmost X  1 / 2. This is a robust fitting procedure, since as long as less than half of the data points are corrupted, we are guaranteed to remain  X  X easonably close X  to the uncorrupted solution. Consider [ 7 ], and references therein, for further details. As we will see later, absolute loss is not robust for evaluation.
 not aware of any work in the literature on that topic. Here, we suggest one such notion. Consider the following evaluation process: Inputs : Evaluation procedure : Definition 1 Let M = sup u  X  R n , v  X  R n L ( u , v ) (possibly M = X  ). The evaluation breakdown of an evaluation metric L , denoted R ( L ) is the smallest percentage of test data points that need to be arbitrarily corrupted to guarantee that for any c &lt; M and any test data we can get L ( m , y )&gt; c.
 Proposition 1 If L is:  X  a non-negative, additive function of the residuals, that is  X  L : R  X  R  X  R such  X  unbounded (that is, M = X  ) then R ( L ) = 1 / n for a test set of size n. That is, a single corrupted data point is enough to guarantee arbitrarily bad evaluation. Proof Because of the additivity, M = X  implies sup u  X  R L ( u ) = X  as well. Given a value c &lt;  X  ,let u 0 be such that L ( u 0 )&gt; c . Now, if we choose any evaluation score: bounded function of the residuals are all evaluation non-robust and have a break-down point of 1 / n for evaluation. This is less trivially also true of the area over the REC curve (AOC), suggested by Bi and Bennett [ 2 ] as a one-number sum-mary of model performance. The AOC can be bounded from below by an additive function: and the resulting lower bound is evaluation non-robust, which clearly leads to the AOC being evaluation non-robust as well.
 measure, since corrupting almost half of the data arbitrarily still guarantees that the median is in the uncorrupted half, and so is not significantly affected. It can be argued, though, that by ignoring completely the large absolute residuals, MAD is  X  X oo robust X  and does not penalize a model for making gross errors.
 stant M of Definition 1 is finite, and to get arbitrarily close to it, we clearly need to corrupt almost all our data, in some cases all data, to make sure that the order of all pairs is incorrect. More convincingly, we can derive an O ( 1 / n ) bound on the effect of any outlier on the overall model evaluation score (a property not shared by any of the residual-based methods, including MAD), as follows.
 arbitrarily, the change in both the measures  X   X  and  X   X  is O ( k 1 / n ) . Proof Denote the original label vector by y and the corrupted one by y  X  .Ifwe consider our two measures T , R as defined in ( 1 , 2 ), we can see that where the second calculation uses the equivalent formulation R = i i 2  X  i is i , proven by Kendall and Gibbons [ 8 ], and the fact that the change in the ranks of the corrupted observations is no more than n , while the ranks of the non-corrupted observations can be changed by at most k .
 5 Partial ranking evaluation In many marketing applications (like the case study we present in Sect. 6 ), we are only interested in separating the top k percentage of customers (who will be targeted for marketing) from the rest. The value of k to be used in practice depends on the number of customers who can be marketed within our resources. When there is uncertainty about the marketing budget and resources, we may aspire to do well in separating the top k percentage for multiple values of k , representing our  X  X rior X  over the distribution of likely resources. This gives us a partial ranking problem.
 probability, i.e., we believe that our targeting population is likely to be in: each with probability 1 / 4. The evaluation of model performance for such a mar-keting task should not really be affected by the correctness of the ranking within the lower half of actual wallet values, since these are all outside the possible uni-verse of targeting goals. However, we should certainly care if one of the low-wallet customers is ranked in the top 1% by our model, since that will define it as an in-teresting target for all values of k , when in fact it should not be for any value of k .
 the partial Spearman and Kendall correlation coefficients. We assume that we have an ordered set of p  X  1 percentiles defining p bins: For simplicity, we assume:  X  nk i is integer for all i , since all bins have to contain integer number of obser- X  We are interested in all cutoff points defined by k i with equal probability. Both of these assumptions can be relaxed, at a cost of complicating our notation and calculations somewhat. Further, define n i = n ( k i  X  k i  X  1 ) (with k 0 = 0and k vation with rank r (i.e., b ( r ) = min { j : k j  X  r / n } ). Definition 2 The partial Kendall ranking correlation for the given set of per-centiles is defined as:  X  p  X  to be in [ X  1 , 1 ] .
 Definition 3 The partial Spearman ranking correlation for the given set of per-centiles is defined as: where c p  X  is the appropriate constant to transform  X  p  X  to be in [ X  1 , 1 ] . duce to the expected reasonable measures in the extreme cases of only two bins (trying to separate  X  X igh X  from  X  X ow X  only with p = 2) and n bins (full ranking with p = n ), as follows.
 Proposition 3 Proof 1. Since we have only two bins, then: 2. If there are n bins, then b ( l ) = l for any l , and thus 6 Case study: evaluation of IT Wallet estimation models at IBM The wallet of a customer (or potential customer) is defined as the total amount that a customer spends in a certain product category in a given time frame. There are many possible uses for wallet estimates, which include targeting sales/marketing actions towards large wallet customers, detecting partial defection of customers, and rewarding sales representatives according to the share of the wallet of a cus-tomer that they attain. Recent marketing literature demonstrates that knowing the customer X  X  share of wallet is important for customer relationship management [ 5 ]. For example, in the credit card industry, the card issuing companies can calculate the wallet size using credit records from the major credit bureaus. For most indus-tries, however, no public wallet information is available at the customer level. In this case, a model that relates the available information about the customer to the wallet needs to be created. A common approach is to obtain actual wallet infor-mation for a random subset of customers through a survey and build a regression model.
 let of customers for information technology (IT) products, including heuristic ap-proaches and predictive modeling. Given the variety of models, there was a press-ing need for an objective comparison of their performance. For this reason, a sur-vey was conducted to obtain actual 2003 wallet figures for a set of 500 companies categories: hardware, software and services. In this case study, we do not discuss building models for wallet estimation, but restrict ourselves to the important prob-lem of evaluating and comparing models in a meaningful and robust fashion. 6.1 Advantages of ranking-based evaluation Figure 4 shows the distribution of IT Wallets obtained in the survey. Note that this distribution is long-tailed, that is, there are many companies with relatively evaluation measures such as mean squared error and mean absolute error can be greatly influenced by a small subset of companies that have very large wal-lets and for which the models are more likely to make larger absolute errors. On the other hand, measures such as median squared error can completely ignore the performance of the model on the companies with large IT Wallet size which are usually the most important customers. An approach that is often used to mit-igate the effects of a skewed distribution (especially in modeling) is to transform the numbers to a logarithmic scale. This approach, however, is usually not ad-equate for the evaluation of wallet models because doing well on the log scale (where differences correspond to ratios on the original dollar scale) does not nec-essarily correspond to good performance as intended by model users.
 ing the customers according to wallet size is all that is needed. To make this more concrete, let us assume that a certain budget of B dollars is available for targeting customers and that the cost of targeting a customer is fixed at c dollars. Then, we can target at most k = B / c customers. Given that the number of customers we mates is to target the customers with the top k wallet values. B and c can vary over time, and if we would like to assume nothing about their likely values, we need a complete ranking of customers to be able to threshold at any point. If we are willing to assume that B and c (and by extension k ) are limited to a small set of values, we may want to evaluate our models on their partial ranking performance, as discussed in Sect. 5 .
 sures is advantageous for at least two reasons. First, as demonstrated in Sect. 4 , they are robust, which is especially important when we have skewed distributions. Second, they evaluate the appropriate performance measure, at least for targeting applications. 6.2 Empirical robustness of different performance measures To illustrate the sensitiveness of different performance measures to a single ob-servation, we calculate the root mean squared error (RMSE), mean absolute er-ror (MAE), root mean squared error on the log scale (Log-RMSE), Kendall X  X   X   X  , and Spearman X  X   X   X  evaluation measures for one model (which we will denote by M 1 later) that predicts the software wallet for the companies included in the sur-vey. Then, we recalculate the measures excluding the single company that has pany whose estimated wallet is the furthest from the actual wallet. For ranking measures, we do an exhaustive search to determine the removal that leads to the biggest change. The results are shown in Table 1 .
 sure is affected by a single company. For RMSE and MAE, a single company is responsible for 49.8 and 25.9% of the total loss, respectively, while for Kendall X  X   X   X  and Spearman X  X   X   X  , no company is responsible for more than 2.05% of the to-tal correlation. These empirical results are in agreement with the theoretical re-sults from Sect. 4 showing that the ranking-based measures are more robust than residual-based measures.
 though the Log-RMSE measure is theoretically non-robust, the log transformation considerably reduces the effect of single companies because it makes the distribu-tion of wallets less skewed. Therefore, in practice, Log-RMSE is a robust measure. 6.3 Model comparison using different performance measures Here, we use residual-based and ranking-based performance measures to compare two existing IBM Software Wallet models, which in this study we refer to as M 1 and M 2 .
 man X  X   X   X  measures of the two models for the 500 companies included in the survey. According to these results, M 1 is slightly outperforming M 2 in RMSE and MAE, and it is greatly outperforming M 2 for the ranking-based measures (  X   X  and  X   X  )and for Log-RMSE.
 between these models are significant. In order to measure statistical significance, we create 100 bootstrap samples from the original survey of 500 companies. Using the bootstrap samples, we can obtain estimates of the standard deviation of the dif-ference between the performance of the two models for each measure. Assuming a gaussian distribution, we compute p -values using these estimates of the standard deviation and the actual difference in performance from the original sample. The last three columns of Table 2 summarize these results.
 tween the two models is statistically significant for the ranking-based measures and for Log-RMSE, but not statistically significant for the other residual-based measures. This is a consequence of the fact that we have a few extreme outliers in the test sample. The outliers considerably affect the residual-based measures on the original scale and make them unreliable and inappropriate for model com-parison. On the other hand, the ranking-based measures are much more stable and robust to the outliers, allowing us to be confident that model M 1 is indeed performing better than M 2 in terms of ranking.
 effect when we move to the log scale. Again, this illustrates the fact that even though Log-RMSE is theoretically non-robust, for most data encountered in prac-tice the log transformation is enough to considerably reduce the effect of outliers. This makes it a stable measure that is appropriate for model comparison. However, to metrics of interest in business applications. This is not the case for the ranking-based measures, which have a clear interpretation in targeting applications with uncertain budgets.
 it to the analytical expression ( 5 ). The numbers (for model M 2 ) are indeed very close: the empirical estimate is 0 . 00103 and the analytical estimate is 0 . 00097. 6.4 Visualizations In Sect. 3.2 , we already shown some visualizations for model M 1 of this case study. Figure 5 contrasts the graphs of the percent of correctly ranked pairs for both models. Recall that each plot has an area under the curve that equals  X   X  .We can make the following observations:  X  Both models provide better rankings for observation with larger predictions  X  Both models produce outliers that are strongly misplaced (probability of cor- X  The ranking-based model comparison in Sect. 6.3 identified M 1 as signifi-clearly verify the superiority of model M 1 . Finally, Fig. 7 compares the two visu-alizations that are related to  X   X  . The left graph shows the AUCs for each possible cutoff and the right graph presents the rank lifts of both models. Both graphs con-firm our earlier conclusions that M 1 is better in terms of its ranking performance  X   X  . A new insight from these graphs is that M 1 seems as good or superior to M 2 across all possible cutoffs. We calculated bootstrap confidence intervals for the difference in AUC at several cutoff points between 0.3 and 0.7 (not shown) and confirmed that the differences in AUC are significant at these cutoffs. 6.5 Partial ranking evaluation As we discussed in Sect. 5 , if we have a prior belief on the distribution of the percent of potential customers that will be targeted, we should evaluate our model not on the full ranking, but on the partial ranking problem induced by our prior. To demonstrate that effect, we also evaluated models M 1 and M 2 on their partial ranking performance using measures  X  p  X  and  X  p  X  with the bins defined by as in Sect. 5 .
 similarly to the conclusions in Table 2 , the partial ranking measures also show that M 1 is superior to M 2 , although the difference is not as significant in our bootstrap not seem critical if we use the partial or full ranking measures. However, the partial ranking measure is more appropriate when we have specific knowledge (or belief) about the tasks the models will be used for.
 7 Additional experiments To further illustrate the characteristics of the different evaluation measures, here we repeat the analysis done in the wallet case study using publicly available data from a similar domain. The dataset we use is the charitable donations dataset that was first used in the KDD Cup 1998 data mining competition and is now available at the UCI KDD archive [ 1 ]. This dataset consists of records of individuals who over a series of donation campaigns, as well as demographic information, such as income and age. The target prediction variable is the individual X  X  response to the most recent donation campaign.
 Only 5% of the individuals contributed in the current campaign and the donation amounts vary from US$ 1 to 500. Figure 8 shows the distribution of the donation amounts for the test individuals who have donated (excluding the seven donations of size US$ 200 X 500). Although this distribution is not as skewed as the wallet distribution, it is also long-tailed as expected for this type of data. ferent regression models for predicting the gift amounts of the people who donate using the 4843 donor examples in the training set. Accordingly, we test the models on the 4876 donor examples in the test set. Our choice of variables is the same for both models and based informally on the KDD98 winning submission of Georges and Milley [ 6 ]:  X  avggift : average dollar amount of gifts to date.  X  pgift : number of gifts/number of promotions received  X  lastgift : dollar amount of most recent gift.  X  ampergift : average dollar amount in responses to the last 22 promotions. data but are obtained by dividing ngiftall / numprom andbyaveraging RAMNT 3 to RAMNT 24 , respectively.
 is a simple least-squares linear regression model and the other is a neural network backpropagation.
 measures of the two models for the 4876 test examples. The standard deviations and p -values are calculated based on 100 bootstraps of the original test set, like we did in the wallet case study.
 network for the residual-based measures (RMSE, MAE, and Log-RMSE), where smaller is better. On the other hand, for the ranking-based measures (  X   X  and  X   X  ), where larger values are better, the neural network is outperforming linear regres-sion. These results clearly show that residual-based measures and ranking-based measures do not always agree and that we must take into account how the predic-tions will be used in practice when deciding between models.
 of the RMSE measure. This measure is the one most affected by outliers, so it is not surprising that it does not yield significant differences in performance for this data. While our analysis in Sect. 4 defines a sense in which MAE and Log-RMSE are also non-robust, it is easy to see that they are much less sensitive to the effect of outliers than RMSE, especially in the case of Log-RMSE.
 the conclusions we draw from residual-based and ranking-based measures. Fig-ures 9 and 10 provide details about the relative ranking performance of the two models. The first impression from the two graphs in Fig. 9 is a surprising degree of similarity between the models. Indeed, if we use the measures  X   X  ,  X   X  to measure the correlation between the rankings induced by the two models we get values of 0.9 and 0.984, respectively, indicating that the correlation between the two mod-els is close to perfect, and much higher than that of either of them and the true donations.
  X  Consider the far left end of both graphs in Fig. 9 . We see that in the top 5% pre- X  This conclusion is confirmed when we consider Fig. 10 , showing that the AUC  X  When we examine the actual donation predictions for the neural network and 8Conclusion In this paper, we have argued the usefulness of ranking-based evaluation ap-proaches for regression models, especially in the context of targeting campaigns, where they may often represent the true modeling task. Ranking-based methods are also more robust to outliers than traditional residual-based performance mea-sures.
 vide insights about local model performance and behavior of outliers. We pre-sented confidence intervals for one of the suggested ranking measures, which al-low us to assess the uncertainty in the evaluation score we get. We also suggested an extension of ranking-based evaluation to consider partial ranking, when we have a specific reason to concentrate on  X  X inned X  ranking performance. A further contribution of this work is the definition of evaluation robustness as a property of different evaluation measures.
 should be discarded completely in favor of ranking-based measures, we believe that the properties of the latter, as we have discussed them here, make them an important component in the regression evaluation toolbox.
 References Author Biographies
