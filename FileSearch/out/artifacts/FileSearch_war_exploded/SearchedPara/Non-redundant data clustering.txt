 REGULAR PAPER David Gondek  X  Thomas Hofmann Abstract Data clustering is a popular approach for automatically finding classes, concepts, or groups of patterns. In practice, this discovery process should avoid redundancies with existing knowledge about class structures or groupings, and reveal novel, previously unknown aspects of the data. In order to deal with this problem, we present an extension of the information bottleneck framework, called coordinated conditional information bottleneck , which takes negative relevance information into account by maximizing a conditional mutual information score subject to constraints. Algorithmically, one can apply an alternating optimization scheme that can be used in conjunction with different types of numeric and non-numeric attributes. We discuss extensions of the technique to the tasks of semi-supervised classification and enumeration of successive non-redundant cluster-ings. We present experimental results for applications in text mining and computer vision.
 Keywords Non-redundant clustering  X  Exploratory data mining  X  Information bottleneck 1 Introduction Data mining and knowledge discovery aim at finding concepts, patterns, relation-ships, regularities, and structures of interest in a given data set. However, in prac-tice it is rather atypical to be faced with data about which nothing is known al-ready. More typically, some knowledge has already been acquired in the past, possibly through precedent knowledge discovery processes. The goal then is more precisely to mine for concepts, relationships, etc. that will augment the existing knowledge and that are in some sense non-redundant and novel, relative to the available background knowledge. This leads to the general problem of how to subtract or factor out the background knowledge in a principled way, in order to augment it through additional data mining and exploration.
 of the concept of conditional mutual information as its cornerstone. We propose to quantify the information or knowledge gained by a data mining process in terms of how much new information it adds about relevant aspect of our data, conditioned on the already available knowledge. It is this conditioning that will provide a well-founded basis for the subtraction process we alluded to.
 tering, which deals with finding classes that are in some sense orthogonal to ex-isting knowledge. As far as the form of this knowledge is concerned, our focus is on problems that involve conditioning on one or more attributes or properties of instances. This includes conditioning on known classification schemes as a special case of particular relevance. There are many applications and problems that can be subsumed under this general setting, of which we enumerate a few here to provide some further motivation. known classification schemes, but rather discovers new ways in which to group documents. For instance, news stories may be clustered by geographic region as well as by topic. Assuming that documents are annotated by region, conditioning on this information may be valuable to enforce a clustering by topic. (ii) Clustering documents such that the found clusters are not correlated with the occurrence of certain terms. Using the above news story example, one may want to condition on the occurrence of certain geographic terms such as country and city names to introduce a bias that favors document clusters that are not based on geography. (iii) Grouping users for which transactional data has been collected in a way that group users in ways that are not correlated with stratifications based on gender or income. (iv) Finding non-dominant clustering solutions in arbitrary data sets. By first finding the dominant grouping structure and then conditioning on the latter, non-dominant clustering alternatives can be discovered.
 emphasis is on a common modeling framework and on the derivation of a general data mining methodology. We will only investigate specific instantiations in an exemplary manner in the experiment section. 2 Related work Our contribution is in line with a number of recent papers that have argued in favor of data mining techniques that are exploratory, and allow for user interaction and control (e.g., [8]): Techniques should allow users to interactively refine or modify queries based on the results of previous queries. They should furthermore allow users to specify prior knowledge and provide feedback in order to guide the search, both towards desired solutions and away from undesired solutions. Techniques constrained association rule mining [8].
 Constraints may take the form of cluster aggregate constraints [13, 15], e.g., by constraining clustering solutions to equal-sized clusters. Another line of work has focused on instance-level constraints [6, 14, 15]. As described in [14], these con-straints are informed by prior knowledge of the desired clustering and typically take the form of relations such as must-link and cannot-link , which are enforced from instance-level constraints proximity matrices and formal distance metrics, respectively. The common trait of these approaches is the assumption that prior knowledge takes the form of positive information about certain characteristics of a desired clustering solution.
 form of information about undesired solutions, were first formulated in [2]. An-other conceptually related framework has been presented in [4]. We will postpone a detailed discussion of these methods until the end of the following section. 3 Coordinated conditional information bottleneck 3.1 Preliminaries The mutual information I ( A ; B ) between two (discrete) random variables A , B is defined as where the sums are over the respective sample spaces for A and B .Wehaveuti-lized a generic notation for probability mass functions using subscripts involving random variables. Alternatively and equivalently one may define mutual informa-tion via conditional entropies as I ( A ; B ) = H ( A )  X  H ( A | B ) . A , B given a random variable C can be defined as 3.2 Setting and notation We use the following notation: x refers to objects or items, such as documents, that should be clustered, y to features that are considered relevant, e.g., word oc-currences in documents, c to clusters of objects, and z to available background knowledge. Uppercase letters X , Y , C , Z are used to denote the corresponding ran-dom variables. To simplify the presentation, we assume that background knowl-edge and features depend deterministically on the object, i.e., Y = Y ( X ) and Z = Z ( X ) .
 to clusters. The goal of data clustering is thus to find a stochastic mapping P C | X sumed to be given. Here, P C | X refers to the conditional distribution of C given object x to cluster c . 3.3 Conditional information bottleneck Given a particular choice for P C | X , we would like to quantify the amount of in-formation preserved in the clustering about the relevant features Y .However,we also need to take into account that we assume to have access to the background knowledge Z . A natural quantity to consider is the conditional mutual informa-tion I ( C ; Y | Z ) . It describes how much information C , Z convey jointly about relevant features Y compared to the information provided by Z alone. Finding an optimal clustering solution should involve maximizing I ( C ; Y | Z ) . gether. Cluster assignment probabilities should reflect the uncertainty with which objects are assigned to clusters. One way to accomplish this is to explicitly con-by the mutual information I ( C ; X ) between cluster and object identities. Here, I ( C ; X ) = 0, if objects are assigned to clusters completely at random, whereas I (
C ; X ) becomes maximal for non-stochastic mappings P C | X . I ( C ; X ) also can be given a well-known interpretation in terms of the channel capacity required for transmitting probabilistic cluster assignments over a communication channel [12]. following constrained optimization problem, the Conditional Information Bottle-neck (CIB), first introduced in [5]:
Stated in plain English, we are looking for probabilistic cluster assignments with a minimal fuzziness such that the relevant information jointly encoded in C , Z is maximal. 3.4 Coordinated CIB While the conditional information reflects much of the intuition behind non-redundant data mining, there still is a potential caveat in using Eq. ( 3 ): The def-inition of C may lack global coordination. That is, clustering solutions obtained for different values z may not be in correspondence. For instance, if Z can take a finite number of possible values, then the meaning of each cluster c is relative to a particular value z . The reason for this is that I ( C ; Y | Z ) only measures the information conveyed by C and Z in conjunction, but does not reflect how much relevant information C provides on its own, i.e., without knowing Z .Wecallthis problem the cluster coordination problem .
 problem is via the following proposition that we state here without proof. Proposition 1 Suppose Z and C are finite random variables and define pre-tained according to Eq. ( 3 ) . Then one can chose arbitrary permutations  X  z P Intuitively, this proposition states that by independently renumbering (i.e., per-affected.
 rization or a local refinement of the partition induced by Z . Generally, however, one is more interested in concepts or annotations C that are consistent across the whole domain of objects. We propose to address this problem by introducing an Conditional Information Bottleneck (CCIB) formulation: With I min &gt; 0 the CCIB favors clustering solutions that obey some global consis-tency across the sets X z . 3.5 Alternating optimization The formal derivation of an alternation scheme to compute an approximate solu-tion for the CCIB is somewhat involved, but leads to very intuitive re-estimation equations. As shown in the Appendix, one can compute probabilistic cluster as-signments according to the following formula: where we have dropped all subscripts, since the meaning of the probability mass functions is clear from the naming convention for the arguments. The scalars  X   X  0and  X   X  0 are Lagrange multipliers enforcing the two inequality constraints; equation is guaranteed to reach a fixed point corresponding to a local maximum of the CCIB criterion. 3.6 Relation to related work The CCIB formulation is an extension of the seminal work by Tishby et al. [12] on the information bottleneck (IB) framework. Among the different generalizations of IB proposed so far, our approach is most closely related to the IB with side information [2]. One way to formulate the latter is as the problem of minimizing I (
C ; Z ) subject to constraints on I ( C ; Y ) and I ( C ; X ) . The main disadvantage that we see in this procedure is the difficulty in adjusting the trade-off between minimizing redundancy between C and Z expressed by I ( C ; Z ) , and maximizing relevant information as expressed by the lower bound on I ( C ; Y ) . Notice that the e.g., I ( C ; Y ) may scale with the number of relevant features, while I ( C ; Z ) may scale with the cardinality of the state space of Z . In the CCIB formulation, this is taken into account by conditioning on the side information Z in I ( C ; Y | Z ) , which enforces non-redundancy without the need for an explicit term I ( C ; Z ) to penalize redundancy.
 of labeled data in order to address the coordination problem. The CIB objective in Eq. ( 3a ) may also be justified using the multivariate information bottleneck (MIB) framework [4]. Using the so-called L ( 1 ) principle, one may directly derive ( 3a ). The derivation follows that of the parallel information bottleneck, only using the assumption that Z is known a priori. However, the coordination problem is not addressed in [4]. In fact, the MIB formulation has a distinctly different goal from the one pursued with the CCIB method. 4 Finite sample considerations So far we have tacitly assumed that the joint distribution P XY Z is given. However, since this will rarely be the case in practice, it is crucial to be able to effectively deal with the finite sample case. Let us denote a sample set drawn i.i.d. by X n = { ( and likelihood maximization and then investigate particular parametric forms for scheme presented in the previous section. 4.1 Likelihood maximization A natural measure for the predictive performance of a model is the average condi-tional log-likelihood function Here,  X  P Y | C , Z is some approximation to the true distribution. This amounts to a  X  P | C , Z ( y i | c , z i ) . Asymptotically, one gets Provided that the estimation error (represented by the expected Kullback X  X eibler divergence) vanishes as n  X  X  X  , maximizing the log-likelihood with respect to P
C | X will thus asymptotically be equivalent to minimizing H maximizing the conditional information I ( C ; Y | Z ) .
 likelihood function Eq. ( 6 ) as the basis for computing a suitable approximation  X  P | C , Z . For instance, if the latter is parameterized by some parameter  X  , then one may compute the optimal parameter  X   X  as the one that maximizes L ( X n ) . 4.2 Categorical background knowledge Let us focus on the simplest case first, where Z is finite and its cardinality is small enough to allow estimating separate conditional feature distributions P Y | C , Z and P | Z for every combination c , z and every z , respectively. As discussed before, we can estimate the above probabilities by conditional maximum likelihood esti-mation. For concreteness, we present and discuss the resulting update equations and algorithm for the special case of a multinomial sampling model for Y ,which is of some importance, for example, in the context of text mining application. It is simple to derive similar algorithms for other sampling models such as Bernoulli, normal, or Poisson.
 sible Y -value and by n i the total number of observations for x i . For instance, n ij may denote the number of times the j th term occurs in the i th document in the context of text mining. Then, we can define the relevant empirical distributions by The maximum likelihood estimates for given probabilistic cluster assignments can be computed according to These equations need to be iterated in alternation with the re-estimation equation in Eq. ( 5 ), where the sum over y is replaced by a sum over the feature index j . We now discuss two special cases to illustrate the usefulness of the non-redundant data clustering algorithm. 4.2.1 Multinomial features We consider the case where features Y are distributed according to a multino-mial distribution where the feature count for instance x and feature j is denoted as n j ( x ) . We propose to use the empirical feature counts or maximum likelihood estimates as a plug-in estimator for P ( y | x ) and a uniform distribution over docu-ments, which produces the iterative update equations of Algorithm 1 [Eqs. (10) X  (17)].
 Algorithm 1 Update equations for multinomial Y , categorical Z 4.2.2 Binary features with Naive Bayes assumption b ( x ) represents feature j for instance x . We make the Naive Bayes assumption and model each feature as a Bernoulli trial. As in the previous case we assume a uniform distribution over documents and use the maximum likelihood estimates for P Y | X ( y | x ) . The updated equations in this case are presented in Algorithm 2 [Eqs. (18a) X (18h)]. 4.2.3 Gaussian features We also consider the case that Y is generated by Gaussians. In particular, assume Y is vector-valued, distributed according to a multivariate Gaussian distribution with covariance matrix  X  2 I for each c , z . The resulting update equations are shown in Algorithm 3 [Eqs. (19) X (26)].
 Algorithm 2 Update equations for binary Y , categorical Z 4.3 Continuous-valued background knowledge A more general case involves background knowledge consisting of a vector z  X  R d . This includes situations where Z might be a function of Y or might consists of a subset of the features (cf. Sect. 1 ). In order to obtain an estimate for P ( y | c , z ) one has to fit a regression model that predicts the relevant features Y from the background knowledge Z for every cluster. If the response variable Y is itself vector-valued, then we propose to fit regression models for every feature dimension separately.
 and sampling model of the feature variable Y . For instance, Y maybeamultivari-ate normal, a multinomial variable, or a vector of Bernoulli variables. In order to cover most cases of interest in a generic way, we propose to use of the framework of generalized linear models (GLMs) [7]. Since a detailed presentation of GLMs is beyond the scope of this paper, we only provide a brief outline of what is involved in this process: We assume that the conditional mean of Y can be written as a func-tion of C and Z in the following way E [ Y | C , Z ]=  X ( C , Z ) = h (  X , X ( C , Z ) ) , Algorithm 3 Update equations for Gaussian Y , categorical Z where h is the inverse link function and  X  is a vector of predictor variables. Tak-ing h = id results in standard linear regression based on the independent variables  X (
C , Z ) , but a variety of other (inverse) link functions can be used dependent on the application. The resulting iterative algorithm is shown as Algorithm 4 [Eqs. (27a) X (27d)].
 P ( y | c , z ) = P ( y | c , z ;  X ) requires to estimate  X  and  X  by maximizing the log-likelihood criterion in Eq. ( 6 ). The latter can be accomplished by standard model fitting algorithms for GLMs, which may themselves be iterative in nature. 4.4 Deterministic annealing and sequential methods We now address the issue of how to deal with the free parameters C max and I min of the CCIB or X  X quivalently X  X he Lagrange multipliers  X  and  X  . Notice that the constraint I ( C ; Y )  X  I min leads to a Lagrangian function that additively combines natural to directly set  X  which controls the trade-off between conditional and un-conditional information maximization. Since the I ( C ; Y ) term has been added to address the coordination problem, we will in practice typically chose  X   X  1. of the assignments such that hard clusterings are computed in the limit of  X   X  0. We consider two algorithmic approaches for setting  X  so as to obtain hard as-signments: A sequential clustering approach (also known as online clustering) as Algorithm 4 Update equations for Y distribution chosen via GLM link function h , continuous Z discussed in [11] where  X  = 0 and using a well-known continuation method, deterministic annealing [10].
 assignment and sequentially iterates through all instances in the data set, reassign-ing the instance to the cluster which maximizes the objective function score. The Jensen X  X hannon divergence, JS  X  is used to determine the change in the objective function score. The algorithm terminates when a local optimum is attained where no reassignments improve the objective function score.
 vantages: Conceptually, non-zero values for  X  avoid over-confidence in assigning objects to clusters and thus addresses the crucial problem of overfitting in learning from finite data. For instance, we may chose to select a value for  X  that maximizes the predictive performance on some held-out data set. The second advantage is algorithmic in nature. The proposed alternating scheme is sensitive with respect to the choice of initial values. As a result of that, convergence to poor local optima may be a nuisance in practice, a problem that plagues many similar alternating schemes such as k -means and mixture modeling and the sequential approach dis-cussed previously. However, a simple control strategy that starts with high entropy cluster assignments and then successively lowers the entropy of the assignments has proven to be a simple, yet effective tool in practice to improve the quality of the solutions obtained (cf. [10, 12]). In analogy of a physical system, one may think of  X  in terms of a computational temperature .
  X  =  X   X  according to some schedule, for instance an exponential schedule  X   X  b  X  with b &lt; 1. The process terminates, if the chosen  X  leads to a value for I ( C ; X ) that is close to the desired bound I max or if cluster assignments numerically reach hard assignments. Algorithm 5 Sequential method Algorithm 6 Deterministic annealing algorithm for non-redundant clustering 5 Experimental results 5.1 Synthetic data partitionings A and B with k = 2 are embedded in the data by associating A with m A = 8 of the features and B with m B = 4 of the features, making A the dominant partitioning. Noise is introduced by randomly flipping each binary feature with probability p noise = 0 . 1.
 the ability of the algorithms to discover partitioning B . The results are summarized in Table 1 . Here  X  X ean X  denotes the average precision of individual runs, whereas  X  X est X  is the average precision obtained by selecting the best out of 10 solutions based on the CCIB criterion and  X  X orst X  is the averaged precision obtained by selecting the worst solution. Discovered and target clustering have been aligned using the Hungarian method for an optimal matching. 5.1.1 Deterministic annealing versus sequential clustering As shown in Table 1 , the deterministic annealing algorithm is considerably more computationally efficient relative to the sequential algorithm, despite the need to anneal  X  . This is largely due to the fact that the deterministic annealing approach performs batch updates of all parameters whereas the sequential approach updates the solutions obtained by the deterministic annealing algorithm are considerably more consistent than those obtained by the sequential algorithm, as evinced by the much narrower range between best and worst performance for determinis-tic annealing versus the sequential algorithm. This insensitivity to initialization of P ( c | x ) leads to the substantially better average performance, as measured by Prec B , of deterministic annealing compared to the sequential algorithm. The per-formance of the algorithms is further investigated in Fig. 1 , which shows the sim-ilarity of solutions to both partitionings A and B as the number of features as-sociated with the dominant partitioning A increases. The deterministic annealing algorithm outperforms the sequential algorithm. Due to the efficiency gain and better average performance, we focus on the deterministic annealing algorithm for subsequent experiments. 5.1.2 Categorical and continuous Z Using synthetic data, we also investigate two types of background knowledge Z : The dominant classification itself Z = B (CCIB1) and the features associ-ated with the dominant classification (CCIB2). For comparison, we also consider Z = B when the coordination term is not used (CIB). In all cases, a multinomial distribution is assumed for the features Y . Both algorithms, CCIB1 and CCIB2, recover the target clustering with high accuracy, significantly outperforming CIB. The CCIB1 version using categorical side information slightly outperforms the GLM version CCIB2, which is due to feature noise. 5.2 Parameter sensitivity Experiments using the Information Bottleneck with Side Information suggested the solution found is dependent on the value for  X  . The range of  X  for which good solutions are found may in practice be somewhat narrow. We investigate the sensi-tivity of  X  for synthetic test sets. Synthetic data sets are generated with K = 2and 4. Experiments are performed for data sets where the dimensionality of Y is 12, 120, and 1200. Within the algorithm, we assume a Bernoulli distribution. As seen in Fig. 3 , the algorithm performs well for certain  X  . For the higher-dimensional sets, the algorithm obtains the desired solution exactly, which is to be expected as the high dimensionality prevents any spurious clusterings obtained due to the the desired solution. Further, the correct  X  varies substantially over the data sets considered and there is no  X  value that works for all sets. The performance of the sequential algorithm degrades more gracefully than that of the deterministic annealing approach. We conclude that the Information Bottleneck with Side Infor-mation requires particular settings of  X  that vary according to the data set under consideration.
 Bottleneck with Side Information, the  X  is used in the CCIB to control the weight as we do not want the coordination term to outweigh the conditional term, we consider 0  X   X   X  1. We find in practice that the CCIB is quite robust to a range of  X  . Results on the synthetic sets are shown in Fig. 4 . For the different dimensional-ities, the solutions obtained by the algorithm are insensitive with respect to  X  .The same  X  may be used over all cases and in practice we take  X  = 0 . 3. 5.3 Face database We consider a set of 369 face images with 40  X  40 grayscale pixels and gender annotations. We performed clustering with k = 2 clusters and a Gaussian noise model for the features. Initially, no background knowledge was used. All of 20 trials converged to the same clustering, suggesting that this clustering is the dom-inant structure in the data set. The precision score between the discovered cluster-ing and the gender classification was 0.5122, i.e., the overlap is close to random. Examining the centroids of each cluster in Fig. 5 shows the clustering which was obtained partitions the data into face-and-shoulder views and face-only views. ground knowledge and perform a second attempt. The resulting precision score showninTable 3 . Centroids for this clustering are shown in Fig. 6 and confirm that the dominant structure found in the previous attempt has been avoided, re-vealing lower-order structure that is more informative with respect to gender. 5.4 Text mining We evaluate performance on several real-world text data sets. Each may be par-titioned according to either one of two independent classification schemes. Ex-periments are performed using either one of these classification schemes as back-ground knowledge. An algorithm is considered successful if it finds a clustering not associated with the background knowledge, that is similar to the other classifi-cation scheme. Documents are represented by term frequency vectors that are as-sumed to follow a multinomial distribution. For all experiments described,  X  = 0 . 3 is used and k is set to the cardinality of the target categorization.
 consists of webpages collected from computer science departments and has a classification scheme based on page type: ( X  X ourse X ,  X  X aculty X ,  X  X roject X ,  X  X taff X ,  X  X tudent X ) as well source university: ( X  X ornell X ,  X  X exas X ,  X  X ashington X ,  X  X is-consin X ). Documents belonging to the  X  X isc X  and  X  X ther X  categories, as well as the  X  X epartment X  category that contained only four members, were removed, leaving 1087 pages remaining. Stopwords were removed, numbers were tok-enized, and only terms appearing in more than one document were retained, leav-ing 5650 terms.
 contains multiple labels for each document. We first select a number of topic labels Topic and region labels Region and then sample documents from the set of docu-ments having labels { Topic  X  Region } .Wetakeupto n documents from each com-bination of labels. For ease of evaluation, those documents which contain multiple labels from Topic or multiple labels from Region were excluded. We selected la-bels which would produce high numbers of eligible documents and generated the following sets: (i) RCV1-gmcat2  X  2: = Topic ={ MCAT (Markets), GCAT (Gov-ernment/Social) } and Region ={ UK, INDIA } : 1600 documents and 3295 terms. (ii) RCV1-ec5  X  6: 5 of the most frequent subcategories of the ECAT (Economics) and 6 of the most frequent country codes were chosen: 5362 documents and 4052 terms. (iii) RCV1-top7  X  9: ECAT (Economics), MCAT (Markets) and the 5 most frequent subcategories of the GCAT (General) topic and the 9 most frequent coun-try codes were chosen: 4345 documents and 4178 terms. As with the WebKB set, stopwords were removed, numbers were tokenized and a term frequency cutoff was used to remove low-frequency terms. Results of the CCIB method on the text data sets are shown in Table 4 . It is interesting to note that results improve as sets with more categories are considered. In all cases, the solutions found are more similar to the target classification than the known classification although the task appears to be more difficult for the RCV1-ec5  X  6set. 6 Extensions We now consider two extensions of the CCIB method: Semi-supervised learning with background knowledge and enumeration of successive non-redundant clus-terings. 6.1 Semi-supervised learning Up to this point, we have assumed that no supervised information, in the form of correctly-labeled instances, is available. We now address the question of whether semi-supervised learning may be enhanced by also avoiding redundancy with background knowledge. For example, one may wish to classify webpages accord-ing to a topic categorization. Typically in such settings, background knowledge of other categorization schemes (e.g., URL) is discarded. Here, we ask whether back-ground knowledge of undesired categorizations can in fact enhance classification along a desired categorization.
 rization. Performance is compared against EM augmented with a Naive Bayes model for labeled data, as described in [9]. Labeled data is incorporated into the CCIB framework by fixing the corresponding P ( c | x i ) for each labeled instance x . Results for varying  X  , the proportion of labeled data, are shown in Figs. 7  X  10 . Equal numbers of instances from each target category are labeled. The mean pre-cision over 10 random initializations is shown for each of the text data sets. The results show that using background information about undesired clusterings can significantly improve the classification performance. The performance gain varies with the proportion of instances labeled. Table 5 lists the average, minimum and maximum performance gains obtained for each data set. For the regime consid-ered, where the proportion of labeled data ranges from 0 to 10%, the CCIB method substantially outperforms EM. For proportion  X  = 0 . 05 of the data labeled, there is an average performance improvement of 29.14%. For  X  = 0 . 10, the average improvement is 20.51%. 6.2 Successive non-redundant clusterings dominant clustering, to find the next non-dominant clustering. That is, how can this technique be generalized to enumerate arbitrarily many clusterings where each clustering is non-redundant with respect the clusterings that have come before? At issue is the modeling of the known structure which in this case consists of two or more clusterings. We consider two approaches to representing this information: The cartesian product of the known clusterings may be taken and then supplied as categorical Z to the algorithm or the concatenated membership vectors may be treated as a binary vector and then supplied as continuous Z to the algorithm. is because by taking the cartesian product, one ignores the relation between differ-ent combinations within the cartesian product. For example, with known cluster memberships in A and B , no information is shared in estimating the Y distribution This is in contrast to the GLM approach which learns a function in Y over all ( a , b ) and so can share information between combinations.
 ings, A , B , and D . Using the number of features associated with each partitioning, m ity of the approaches to discover partitioning D . Results are shown in Table 6. We further consider the approaches as the data sets vary according to size and dimension. Figures 11a and b show performance as the number of instances and relative strength of partitions vary. The results show for this variety of settings, the cartesian product approach is in fact approximately as successful as the GLM approach while coming at a fraction of the computational cost. This rough equiva-lence holds up when the number of instances varies and when the relative strength of the desired clustering is varied. 7Conclusion We have presented a general information-theoretic framework for non-redundant clustering based on the idea of maximizing conditional mutual information rela-tive to given background knowledge. We have pointed out the cluster coordination problem and provided a way to deal with it in the Coordinated Conditional Infor-mation Bottleneck. A generic alternating optimization and special instantiations thereof have been derived and discussed, emphasizing the principled nature of our approach as well as its broad applicability. The method may be used for unsuper-vised clustering tasks in order to discover and enumerate non-redundant cluster-ings as well as for semi-supervised classification where there is the existence of an unrelated background categorization. Experiments on synthetic and real-world data sets have established the success of the approach. Appendix References
