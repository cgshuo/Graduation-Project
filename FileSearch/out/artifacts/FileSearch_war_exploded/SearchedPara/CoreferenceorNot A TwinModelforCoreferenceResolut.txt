 Coreference resolution aims to find multiple mentions of an entity (e.g., PERSON, ORGANIZA TION) in a document. In a typical machine learning-based coref-erence resolution system (Soon et al., 2001; Ng and Cardie, 2002b; Yang et al., 2003; Luo et al., 2004), a statistical model is learned from training data and is used to measure how lik ely an anaphor 1 is corefer -ential to a candidate antecedent. A related, but often overlook ed, problem is that the anaphor may be non-coreferential to any candidate, which arises from sce-narios such as an identified anaphor is truly generic and there does not exist an antecedent in the discourse con-text, or an anaphor is the first mention (relati ve to pro-cessing order) in a coreference chain.

In (Soon et al., 2001; Ng and Cardie, 2002b), the problem is treated by thresholding the scores re-turned by the coreference model. That is, if the max-imum coreference score is belo w a threshold, then the anaphor is deemed non-referential to any candidate an-tecedent. The threshold approach does not model non-coreferential events directly , and is by no means the op-timal approach to the problem. It also introduces a free parameter which has to be set by trial-and-error . As an impro vement, Ng and Cardie (2002a) and Ng (2004) train a separate model to classify an anaphor as either anaphoric or non-anaphoric . The output of this clas-sifier can be used either as a pre-filter (Ng and Cardie, 2002a) so that non-anaphoric anaphors will not be pre-cessed in the coreference system, or as a set of features in the coreference model (Ng, 2004). By rejecting any anaphor classified as non-anaphoric in coreference res-olution, the filtering approach is meant to handle non-anaphoric phrases (i.e., no antecedent exists in the dis-course under consideration), not the first mention in a coreference chain.

In this paper , coreference is vie wed as a process of sequential operations on anaphor mentions: an anaphor can either be link ed with its antecedent if the antecedent is available or present. If the anaphor , on the other hand, is discourse new (relati ve to the process order), then a new entity is created . Corresponding to the two types of operations, a twin-model is proposed to re-solv e coreferential relationships in a document. The first component is a statistical model measuring how lik ely an anaphor is coreferential to a candidate an-tecedent; The second one explicitly models the non-coreferential events. Both models are trained automat-ically and are used simultaneously in the coreference system. The twin-model coreference system is tested on the 2005 ACE (Automatic Content Extraction, see (NIST, 2005)) data and the best performance under both ACE-V alue and entity F-measure can be obtained without tuning a free parameter .

The rest of the paper is organized as follo ws. The twin-model is presented in Section 2. A maximum-entrop y implementation and features are then presented in Section 3. The experimental results on the 2005 ACE data is presented in Section 4. The proposed twin-model is compared with related work in Section 5 be-fore the paper is concluded. A phrasal reference to an entity is called a mention. A set of mentions referring to the same physical object is said to belong to the same entity . For example, in the follo wing sentence: (I) John said Mary was his sister . there are four mentions: John , Mary , his , and sister . John and his belong to the same entity since the y refer to the same person; So do Mary and sister . Furthermore, John and Mary are named mentions, sister is a nominal mention and his is a pronominal mention.

In our coreference system, mentions are processed sequentially , though not necessarily in chronological order . For a document with n mentions f m n g , at any time t ( t &gt; 1) , mention m have been processed and each mention is placed in one of N t ( N t ( t 1)) entities: E t = f e j : 1 j N t g . Inde x i in m cessed, not necessarily the order in which it appears in a document. The basic step is to extend E with m
Let us use the example in Figure 1 to illustrate how this is done. Note that Figure 1 contains one possible processing order for the four mentions in Example (I): first name mentions are processed, follo wed by nom-inal mentions, follo wed by pronominal mentions. At time t = 1 , there is no existing entity and the mention m fied by a solid rectangle). At time t = 2 , m is processed and a new entity containing Mary is cre-ated. At time t = 3 , the nominal mention m is processed. At this point, the set of existing entities m last step t = 4 , the pronominal mention his is link ed with the entity f John g .

The abo ve example illustrates how a sequence of coreference steps lead to a particular coreference result. Con versely , if the processing order is kno wn and fix ed, every possible coreference result can be decomposed and mapped to a unique sequence of such coreference steps. Therefore, if we can score the set of coreference sequences, we can score the set of coreference results as well.

In general, when determining if a mention m coreferential with any entity in E of actions: one is that m the entities; The other is that m with any. It is important to distinguish the two cases for the follo wing reason: if m entity e relationship by examining m conte xt; But if m entities, we need to consider m E . This observ ation leads us to propose the follo wing twin-model for coreference resolution.

The first model, P ( L j e entity e ing value 1 or 0, which represents positi ve and nega-tive coreferential relationship, respecti vely . The second model, on the other hand, P ( C j E on the past entities E random variable C is also binary: when C is 1 , it means that a new entity f m the second model measures the probability that m not coreferential to any existing entity . To avoid con-fusion in the subsequent presentation, the first model will be written as P The second model is written as P creation model.

For the time being, let X  s assume that we have the link and creation model at our disposal, and we will sho w how the y can be used to score coreference decisions.
Given a set of existing entities E by mentions f m there are N m create a new entity containing m tween e the creation action can be measured by P Each possible coreference outcome consists of n such actions f a scored by either the link model P ation model P tion a a sult corresponding to the action sequence is written as E drop f a
With this notation, the score for a coreference out-come E assigned to the corresponding action sequence f a and the best coreference result is the one with the high-est score:
Given n mentions, the number of all possible entity outcomes is the Bell Number (Bell, 1934): question. Thus, we organize hypotheses into a Bell Tree (Luo et al., 2004) and use a beam search with the follo wing pruning strate gy: first, a maximum beam size (typically 20) S is set, and we keep only the top S hy-potheses; Second, a relati ve threshold r (we use 10  X  5 ) is set to prune any hypothesis whose score divided by the maximum score falls belo w the threshold.
To give an concrete example, we use the example in Figure 1 again. The first step at t = 1 creates a new entity and is therefore scored by P the second step also creates an entity and is scored by P ever, links sister with f Mary g and is scored by P (1 jf Mary g , sister ) ; Similarly , the last step is scored by P coreference outcome is the product of the four num-bers: Other coreference results for these four mentions can be scored similarly . For example, if his at the last step is link ed with f Mary,sister g , the score would be: At testing time, (2) and (3) , among other possible out-comes, will be searched and compared, and the one with the highest score will be output as the coreference result.
 Examples in (2) and (3) indicate that the link model P inte grated coreference system and are applied simul-taneously at testing time. As will be sho wn in the next section, features in the creation model P be computed from their counterpart in the link model P ( j e j , m t ) under some mild constraints. So the two models X  training procedures are tightly coupled. This is dif ferent from (Ng and Cardie, 2002a; Ng, 2004) where their anaphoricty models are trained indepen-dently of the coreference model, and it is either used as a pre-filter , or its output is used as features in the coreference model. The creation model P proposed here bears similarity to the starting model in (Luo et al., 2004). But there is a crucial dif fer -ence: the starting model in (Luo et al., 2004) is an ad-hoc use of the link scores and is not learned auto-matically , while P P ( j E t , m t ) is covered in the next section. 3.1 Featur e Structur e To implement the twin model, we adopt the log linear or maximum entrop y (MaxEnt) model (Ber ger et al., 1996) for its flexibility of combining diverse sources of information. The two models are of the form:
P c ( C j E t , m t ) = where L and C are binary variables indicating either m new entity . Y ( e factors to ensure that P probabilities;  X  g the set of features functions are selected, algorithm such as impro ved iterati ve scaling (Ber ger et al., 1996) or sequential conditional generalized iterati ve scal-ing (Goodman, 2002) can be used to find the optimal parameter values of f  X 
Computing features f g model P given an entity e just need to characterize things such as lexical similar -ity, syntactic relationship, and/or semantic compatibil-ity of the two. It is, howe ver, very challenging to com-pute the features f h P tities E are processed. The problem exists because the decision of creating a new entity with m examining all preceding entities. There is no reason-able modeling assumption one can mak e to drop some entities in the conditioning.

To overcome the dif ficulty , we impose the follo w-ing constraints on the features of the link and creation model: (6) states that a feature in the link model is separable and can be written as a product of two functions: the first one, g (1) the conditioning part only; the second one, g (2) an indicator function depending on the prediction part L only . Lik e g (2) function. (7) implies that features in the creation model are also separable; Moreo ver, the conditioning part h i f g only depends on the function values of the set of link features f g (1) words, once f g (1) we can compute h paring m is a fairly mild constraint as non-binary features can be replaced by a set of binary features through quantiza-tion.

Ho w fast h (1) puted depends on how h (1)  X  as will be sho wn in Section 3.2, it boils down test-ing if any member in f g (1) zero; or counting how man y non-zero members there are in f g (1) erations that can be carried out quickly . Thus, the as-sumption (7) mak es it possible to compute efficiently h ( E t , m t , C ) . 3.2 Featur es in the Cr eation Model We describe features used in our coreference system. We will concentrate on features used in the creation model since those in the link model can be found in the literature (Soon et al., 2001; Ng and Cardie, 2002b; Yang et al., 2003; Luo et al., 2004). In particular , we sho w how features in the creation model can be computed from a set of feature values from the link model for a few example cate gories. Since g (2) h i ( ) g k ( , ) 3.2.1 Lexical Featur es
This set of features computes if two surf ace strings (spellings of two mentions) match each other , and are applied to name and nominal mentions only . For the link model, a lexical feature g (1) tains a mention matches m act, partial, or one is an acron ym of the other .
Since g ture used in the creation model is the disjunction of the values in the link model, or where _ is a binary  X  X r X  operator . The intuition is that if there is any mention in E probability to create a new entity with m low; Con versely , if none of the mentions in E m entity .

Take t = 2 in Figure 1 as an example. There is only one partially-established entity f John g , so E f
John g , and m feature g (1) and the corresponding string match feature in the cre-ation model is Disjunction is not the only operation we can use. Another possibility is counting how man y times m matches mentions in E where Q [ ] quantizes raw counts into bins. 3.2.2 Attrib ute Featur es
In the link model, features in this cate gory compare the properties of the current mention m entity e ever applicable, include gender , number , entity type, refle xivity of pronouns etc. Similar to what done in the lexical feature, we again synthesize a feature in the creation model by taking the disjunction of the corre-sponding set of feature values in the link model, or where g (1) m The intuition is that if there is an entity having the same property as the current mention, then the probability for the current mention to be link ed with the entity should be higher than otherwise; Con versely , if none of the en-tities in E the probability for the current mention to create a new entity ought to be higher .

Consider the gender attrib ute at t = 4 in Fig-ure 1. Let g (1) link model, assume that we kno w the gender of John , Mary and his . Then g (1) while g (1) fore, the gender feature for the creation model would be which means that there is at least one mention which has the same the gender of the current mention m 3.2.3 Distance Featur e
Distance feature needs special treatment: while it mak es sense to talk about the distance between a pair of mentions, it is not immediately clear how to compute the distance between a set of entities E m tween the entities and the current mention with respect to a  X  X ired X  link feature, as follo ws.

For a particular feature g (1) define the minimum distance to be where d ( m, m m or the number of interv ening mentions, or the number of sentences. The minimum distance  X  d ( E quantized and represented as binary feature in the cre-ation model. The idea here is to encode what is the nearest place where a feature fires.

Again as an example, consider the gender attrib ute at t = 4 in Figure 1. Assuming that d ( m, m t ) is the num-ber of tok ens. Since only John matches the gender of his , The number is then quantized and used as a binary fea-ture to encode the information that  X  X here is a mention whose gender matches the current mention within in a tok en distance range including 3. X 
In general, binary features in the link model which measure the similarity between an entity and a mention can be turned into features in the creation model in the same manner as described in Section 3.2.1 and 3.2.2. For example, syntactic features (Ng and Cardie, 2002b; Luo and Zitouni, 2005) can be computed this way and are used in our system. 4.1 Data and Ev aluation Metric We report the experimental results on ACE 2005 data (NIST, 2005). The dataset consists of 599 doc-uments from a rich and diversified sources, which in-clude newswire articles, web logs, and Usenet posts, transcription of broadcast news, broadcast con versa-tions and telephone con versations. We reserv e the last 16% documents of each source as the test set and use the rest of the documents as the training set. Statistics such as the number of documents, words, mentions and entities of this data split is tab ulated in Table 1. Table 1: Statistics of ACE 2005 data: number of docu-ments, words, mentions and entities in the training and test set.

The link and creation model are trained at the same time. Besides the basic feature cate gories described in Section 3.2, we also compute composite features by taking conjunctions of the basic features. Features are selected by their counts with a threshold of 8.
ACE-V alue is the official score reported in the ACE task and will be used to report our coreference system X  s performance. Its detailed definition can be found in the official evaluation document 3 . Since ACE-V alue is a weighted metric measuring a coreference system X  s rel-ative value, and it is not sensiti ve to certain type of errors (e.g., false-alarm entities if these entities con-tain correct mentions), we also report results using un-weighted entity F-measure. 4.2 Results To compare the proposed twin model with simple thresholding (Soon et al., 2001; Ng and Cardie, 2002b), Figure 2: Performance comparison between a thresh-olding baseline and the twin-model: lines with square points are the entity F-measure (x100) results; lines with triangle points are ACE-V alue (in %). Solid lines are baseline while dashed lines are twin-model. we first train our twin model. To simulate the thresh-olding approach, a baseline coreference system is cre-ated by replacing the creation model with a constant, i.e., where  X  is a number between 0 and 1. At testing time, a new entity is created with score  X  when The decision rule simply implies that if the scores be-tween the current mention m e 2 E t are belo w the threshold  X  , a new entity will be created.

Performance comparison between the baseline and the twin-model is plotted in Figure 2. X-axis is the threshold varying from 0.1 to 0.9 with a step size 0.1. Two metrics are used to compare the results: two lines with square data points are the entity F-measure results, and two lines with triangle points are ACE-V alue. Note that performances for the twin-model are constant since it does not use thresholding.

As sho wn in the graph, the twin-model (tw o dashed lines) always outperforms the baseline (tw o solid lines). A  X  X ad X  threshold impacts the entity F-measure much more than ACE-V alue, especially in the region with high threshold value. Note that a lar ge  X  will lead to more false-alarm entities. The graph suggests that ACE-V alue is much less sensiti ve than the un-weighted F-measure in measuring false-alarm errors. For exam-ple, at  X  = 0 . 9 , the baseline F-measure is 0.591 while the twin model F-measure is 0.848, a 43 . 5% dif ference; On the other hand, the corresponding ACE-V alues are 84.5% (baseline) vs. 88.4% (twin model), a mere 4 . 6% ACE-V alue discounts importance of nominal and pro-noun entities, so more nominal and pronoun entity er-rors are not reflected in the metric; Second, ACE-V alue does not penalize false-alarm entities if the y contain correct mentions. The problem associated with ACE-Value is the reason we include the entity F-measure re-sults.

Another interesting observ ation is that an optimal threshold for the entity F-measure is not necessarily op-timal for ACE-V alue, and vice versa:  X  = 0 . 3 is the best threshold for the entity F-measure, while  X  = 0 . 5 is optimal for ACE-V alue. This is highlighted in Ta-ble 2, where row  X  X -opt-F X  contains the best results op-timizing the entity F-measure (at  X  = 0 . 3 ), row  X  X -opt-AV X  contains the best results optimizing ACE-V alue (at  X  = 0 . 5 ), and the last line  X  X win-model X  contains the results of the proposed twin-model. It is clear from Table 2 that thresholding cannot be used to optimize the entity F-measure and ACE-V alue simultaneously . A sub-optimal threshold could be detrimental to an un-weighted metric such as the entity F-measure. The pro-posed twin model eliminates the need for threshold-ing, a benefit of using the principled creation model. In practice, the optimal threshold is a free parameter that has to be tuned every time when a task, dataset and model changes. Thus the proposed twin model is more portable when a task or dataset changes.
 Table 2: Comparison between the thresholding base-line and the twin model: optimal threshold depends on performance metric. The proposed twin-model outper -forms the baseline without tuning the free parameter . Some earlier work (Lappin and Leass, 1994; Kennedy and Bogurae v, 1996) use heuristic to determine whether a phrase is anaphoric or not. Bean and Rilof f (1999) extracts rules from non-anaphoric noun phrases and noun phrases patterns, which are then applied to test data to identify existential noun phrases. It is in-tended as as pre-filtering step before a coreference res-olution system is run. Ng and Cardie (2002a) trains a separate anaphoricity classifier in addition to a corefer -ence model. The anaphoricity classifier is applied as a filter and only anaphoric mentions are later considered by the coreference model. Ng (2004) studies what is the best way to mak e use of anaphoricity information and concludes that the constrained-based and globally-optimized approach works the best. Poesio et al. (2004) contains a good summary of recent research work on discourse new or anaphoricity . Luo et al. (2004) uses a start model to determine whether a mention is the first one in a coreference chain, but it is computed ad hoc without training. Nicolae and Nicolae (2006) con-structs a graph where mentions are nodes and an edge represents the lik elihood two mentions are in an entity , and then a graph-cut algorithm is emplo yed to produce final coreference results.

We tak e the vie w that determining whether an anaphor is coreferential with any candidate antecedent is part of the coreference process. But we do recog-nize that the disparity between the two types of events: while a coreferential relationship can be resolv ed by examining the local conte xt of the anaphor and its an-tecedent, it is necessary to compare the anaphor with all the preceding candidates before it can be declared that it is not coreferential with any. Thus, a creation component P type of events. A problem arising from the adoption of the creation model is that it is very expensi ve to have a conditional model depending on all preceding enti-ties E model and impose some reasonable constraints on the feature functions, which mak es it possible to synthe-size features in the creation model from those of the link model. The twin model components are intimately trained and used simultaneously in our coreference sys-tem. A twin-model is proposed for coreference resolution: one link component computes how lik ely a mention is coreferential with a candidate entity; the other compo-nent, called creation model, computes the probability that a mention is not coreferential with any candidate entity . Log linear or MaxEnt approach is adopted for building the two components. The twin components are trained and used simultaneously in our coreference system.

The creation model depends on all preceding enti-ties and is often expensi ve to compute. We impose some reasonable constraints on feature functions which mak es it feasible to compute efficiently the features in the creation model from a subset of link feature val-ues. We test the proposed twin-model on the ACE 2005 data and the proposed model outperforms a threshold-ing baseline. Moreo ver, it is observ ed that the optimal threshold in the baseline depends on performance met-ric, while the proposed model eliminates the need of tuning the optimal threshold.
 This work was partially supported by the Defense Ad-vanced Research Projects Agenc y under contract No. HR0011-06-2-0001. The vie ws and findings contained in this material are those of the authors and do not necessarily reflect the position or polic y of the U.S. government and no official endorsement should be in-ferred.

I would lik e to thank Salim Rouk os for helping to impro ve the writing of the paper . Suggestions and com-ments from three anon ymous revie wers are also grate-fully ackno wledged.
 Da vid L. Bean and Ellen Rilof f. 1999. Corpus-based identification of non-anaphoric noun phrases. In Proc. ACL .
 E.T . Bell. 1934. Exponential numbers. Amer . Math. Monthly , pages 411 X 419.
 Adam L. Ber ger , Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entrop y approach to natural language processing. Computational Lin-guistics , 22(1):39 X 71, March.
 Hal Daum  X  e III and Daniel Marcu. 2005. A lar ge-scale exploration of effecti ve global features for a joint entity detection and tracking model. In Proc. of HL T and EMNLP , pages 97 X 104, Vancouv er, British
Columbia, Canada, October . Association for Com-putational Linguistics.
 Joshua Goodman. 2002. Sequential conditional gener -alized iterati ve scaling. In Pro. of the 40th ACL . Christopher Kennedy and Branimir Bogurae v. 1996.
Anaphora for everyone: Pronominal anaphora reso-lution without a parser . In Proceedings of COLING-96 (16th International Confer ence on Computa-tional Linguistics) , Copenhagen,DK.
 Shalom Lappin and Herbert J. Leass. 1994. An algo-rithm for pronominal anaphora resolution. Compu-tational Linguistics , 20(4), December .
 Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual coreference resolution with syntactic fea-tures. In Proc. of Human Langua ge Technolo gy (HL T)/Empirical Methods in Natur al Langua ge Pro-cessing (EMNLP) .
 Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Rouk os. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proc. of ACL .
 Vincent Ng and Claire Cardie. 2002a. Identifying anaphoric and non-anaphoric noun phrases to im-pro ve coreference resolution. In Proceedings of COLING .
 Vincent Ng and Claire Cardie. 2002b . Impro ving ma-chine learning approaches to coreference resolution. In Proc. of ACL , pages 104 X 111.
 Vincent Ng. 2004. Learning noun phrase anaphoric-ity to impro ve conference resolution: Issues in rep-resentation and optimization. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL  X 04), Main Volume , pages 151 X 158, Barcelona, Spain, July .
 Cristina Nicolae and Gabriel Nicolae. 2006. BEST -CUT : A graph algorithm for coreference resolution.
In Proceedings of the 2006 Confer ence on Empiri-cal Methods in Natur al Langua ge Processing , pages 275 X 283, Sydne y, Australia, July . Association for Computational Linguistics.
 NIST. 2005. ACE 2005 evaluation. www .nist.go v/speech/tests/ace/ace05/inde x.htm. M. Poesio, O. Uryupina, R. Vieira, M. Ale xandro v-
Kabadjo v, and R. Goulart. 2004. Discourse-ne w de-tectors for definite description resolution: A surv ey and a preliminary proposal. In ACL 2004: Workshop on Refer ence Resolution and its Applications , pages 47 X 54, Barcelona, Spain, July .
 Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguis-tics , 27(4):521 X 544.
 Xiaofeng Yang, Guodong Zhou, Jian Su, and
Che w Lim Tan. 2003. Coreference resolution us-ing competition learning approach. In Proc. of the 41 st ACL .
