 The goal of dimensionality reduction is to obtain a low-dimensional representa-tion of high-dimensional data samples while preserving most of  X  X ntrinsic infor-mation X  contained in the original data. Once dimensionality reduction is carried out appropriately, the compact representation of the data can be used for various succeeding tasks such as visua lization and classification.

In supervised learning scenarios where data samples are accompanied with class labels, Fisher discriminant analysis (FDA) [1] is a popular dimensionality reduction method. FDA seeks an embedding transformation such that between-class scatter is maximized and within-cl ass scatter is minimized. FDA works very well if samples in each class are Gaussian with the common covariance structure. However, it tends to give undesired results if samples in a class form several separate clusters or there exist outliers [1]. To overcome this drawback, local FDA (LFDA) has been proposed [2], which locali zes the between-class and within-class scatter matrices. LFDA works well ev en when within-class multimodality or outliers exist. Furthermore, LFDA overcomes critical limitation of original FDA in dimensionality reduction X  X he dimension of the FDA embedding space should be less than the number of classes [1], while LFDA does not suffer from this restriction in general.

However, the performance of LFDA (and all other supervised dimensionality reduction methods) tend to be degraded when only a small number of labeled samples are available. Thus, the supervised methods overfit embedding spaces to the labeled samples. In such cases, it is effective to make use of unlabeled samples which are often available abundantly, i.e., semi-supervised learning .The book [3] showed through extensive simulations that principal component analysis (PCA), which is an unsupervised dimensionality reduction method for preserving the global data structure, works moderately well in semi-supervised learning scenarios.

Although PCA is reported to work well, it may not be the best choice in semi-supervised learning due to its unsupervised nature. In this paper, we propose a new semi-supervised dimensionality reduction method which smoothly bridges LFDA and PCA so that we can control our reliance on the global structure of unlabeled samples and information brought by (a small number of) labeled samples. We experimentally show that the proposed method, which we refer to as semi-supervised LFDA (SELF), compares favorably with other methods. Note that SELF maintains the same computational advantage of LFDA and PCA, i.e., a global solution can be analytically computed based on eigendecompositions. Therefore, SELF is still comput ationally efficient and reliable. In this section, we formulate the linear dimensionality reduction problem and give some mathematical backgrounds. 2.1 Formulation Let x i  X  R d ( i =1 , 2 ,...,n )be d -dimensional samples and let X  X  ( x a high-dimensional sample x  X  R d ,where r is the dimensionality of the reduced space. We focus on linear dimensionality reduction, i.e., using a d  X  r transfor-mation matrix T , an embedded representation z of a sample x is obtained as where denotes the transpose of a matrix or a vector.

Many dimensionality reduction techniques developed so far involve an opti-mization problem of the following form: Let {  X  k } d k =1 be the generalized eigenvectors a ssociated with the generalized eigenvalues {  X  k } d k =1 of the following generalized eigenvalue problem: We assume that the generali zed eigenvalues are sorted as  X  1  X   X  2  X  X  X  X  X  X   X  d and the generalized eigenv ectors are normalized as  X  k C  X  k =1for k =1 , 2 ,...,d . Note that this normalization is often automatically carried out by an eigensolver. Then a solution T OP T is analytically given as (  X  1 |  X  2 | X  X  X |  X  r ) (e.g., [1]):.
When addressing dimensionality reduction problems, we often face with a matrix of the following pairwise form [2]: where W is some n -dimensional matrix. Let D be the n -dimensional diagonal matrix with D i,i  X  n j =1 W i,j ,andlet L  X  D  X  W .Then S is expressed as S = XLX , which is positive semi-definite. 2.2 Principal Component Analysis (PCA) A fundamental unsupervised dimensionality reduction method is principal com-ponent analysis (PCA).
 Let S ( t ) be the total scatter matrix : where  X   X  1 n n i =1 x i . The PCA transformation matrix T PCA is defined as That is, PCA seeks a transformation matrix T such that scatter in the embed-ding space is maximized. A solution T PCA is given with C = S ( t ) and C = I d , where I d is the identity matrix on R d . 2.3 Locality-Preserving Projection (LPP) Another useful unsupervised dimensionality reduction technique is locality-preserving projection (LPP) [4].

Let A be the affinity matrix , i.e., the n -dimensional square matrix with A i,j if x are several different manners of defining A , e.g., based on nearest neighbors or the heat kernel. Through the paper, we use the local scaling heuristic [5] as the definition of the affinity matrix A , i.e.,  X  i is the local scaling around k -th nearest neighbor of x i . A heuristic choice of k = 7 has benn shown to be useful through extensive simulations [5,2].
Let S ( n ) and S ( l ) be the normalization matrix and the local scatter matrix defined by W That is, LPP seeks a transformation matrix T such that nearby data pairs in the original space R d are kept close in the embedding space R r . Thus, LPP tends to preserve the local structure of the data. A solution T LP P is given with C = S ( n ) 2.4 Fisher Discriminant Analysis (FDA) A popular supervised dimensionality reduction technique is Fisher discriminant analysis (FDA) [1]. When discussing supervised learning problems, we suppose label associated with the sample x i and c is the number of classes. Let n m be the number of labeled samples in class m  X  X  1 , 2 ,...,c } .

Let S ( b ) and S ( w ) be the between-class scatter matrix and the within-class scatter matrix : where  X  m  X  1 n as That is, FDA seeks a transformation matrix T such that between-class scatter is maximized and within-class scatter is minimized in the embedding space R r . Asolution T FDA is given with C = S ( b ) and C = S ( w ) .

The between-class scatter matrix S ( b ) has at most rank c  X  1 [1]. This implies that FDA allows us to obtain at most c  X  1 meaningful features; the remaining features found by FDA are arbitrary in the null space of S ( b ) .Thisisanessential limitation of FDA in dimensionality reduction. 2.5 Local Fisher Discriminant Analysis (LFDA) Local Fisher discriminant analysis (LFDA) is a supervised dimensionality reduc-tion method [2] which overcomes vulne rability of original FDA against within-class multimodality or outliers [1].

Let S ( lb ) and S ( lw ) be the local between-class scatter matrix and the local within-class scatter matrix defined by where W ( lb ) and W ( lw ) are the n -dimensional matrices with The LFDA transformation matrix T LF DA is defined as A LFDA seeks a transformation matrix T such that nearby data pairs in the same class are made close and the data pairs in different classes are made apart; far apart data pairs in the same class are not imposed to be close. Samples in different classes are separated from each other irrespective of their affinity values. When A i,j =1forall i, j (i.e., no locality), S ( lw ) and S ( lb ) are reduced to S ( w ) and S ( b ) [2]. Thus, LFDA could be regarded as a localized variant of FDA. The between-class scatter matrix S ( b ) hasatmostrank c  X  1, while its local counterpart S ( lb ) usually has full rank (given n  X  d ). Therefore, LFDA can be applied to dimensionality reduction into any dimensional spaces. In this section, we propose a new dimensionality reduction method for semi-supervised learning scenarios. From here on, we consider the case where, among unlabeled. 3.1 Basic Idea When only a small number of labeled samples are available, supervised dimen-sionality reduction methods tend to find embedding spaces which are overfit-ted to the labeled samples. In such situations, using unlabeled samples is often effective X  X ndeed, the book [3] showed through extensive simulations that PCA works well on the whole; our experimen tal results in Section 4 also show that PCA is sometimes better than LFDA. This means that preserving the global structure of all samples in an unsupervised manner can be better than strongly relying on class information provided by a small number of labeled samples.
Figure 1 depicts 2-dimensional 2-class examples; circle/triangle symbols de-note samples in positive/negative classes and filled/unfilled symbols denote la-beled/unlabeled samples; solid and dashed lines denote 1-dimensional embedding spaces found by LFDA and PCA, respectiv ely (onto which data samples will be projected). For the data set in Figure 1(a), both LFDA and PCA can find good embedding spaces which well separate unl abeled samples in different classes from each other. However, for the data set in Figure 1(b), LFDA finds an embedding space that is overfitted to the labeled samples. On the other hand, in the case of Figure 1(c), PCA does not work well due to its unsupervised nature.
The above result implies that LFDA and PCA can compensate for the weak-ness of each other, i.e., LFDA can utilize label information, while PCA can avoid overfitting. Our simulation results with benchmark data sets in Section 4 also show that LFDA and PCA work in a complementary manner. Motivated by these facts, we propose bridging LFDA and PCA so that we can smoothly control our reliance on the global structure of unlabeled samples and class information brought by labeled samples. We refer to the proposed method as semi-supervised LFDA (SELF).

The embedding transformations of LFDA and PCA can be analytically com-puted based on the eigendecompositions. So we combine the eigenvalue problems of LFDA and PCA and solve them together. This allows us to maintain the com-putational efficiency and reliability of LFDA and PCA. 3.2 Definition More specifically, we propose solving the following generalized eigenvalue prob-lem: where S ( rlb ) and S ( rlw ) are regularized local between-class scatter matrix and regularized local within-class scatter matrix defined by  X  (  X  [0 , 1]) is a trade-off parameter X  SELF is reduced to LFDA when  X  =0,and SELF is reduced to PCA when  X  = 1. In general, SELF inherits characteristics of both LFDA and PCA (this will be discussed in detail in Section 3.3). The solution of SELF can be computed in the same way as LFDA or PCA. 3.3 Properties First, we give an interpretation of S ( rlb ) .Thematrix S ( rlb ) can be expressed as where W ( rlb ) is the n -dimensional matrix with The first case in Eq.(18) is negative if  X &lt; A i,j n ( n that SELF tries to make sample pairs in the same class close if  X  is small, while it separates them from each other if  X  is large. Thus the local data structure in the same class tends to be preserved when  X  is small, but it is no longer preserved when  X  is large. The second case in Eq.(18) is always positive for any  X   X  [0 , 1], implying that SELF always tries to make sample pairs in different classes apart for any  X  . This would be natural in semi-supervised learning scenarios. The third case in Eq.(18) is always non-negative, implying that unlabeled samples are separated from each other for preserving the global data structure.
Next,wegiveaninterpretationof S ( rlw ) .When  X  =0, S ( rlw ) (= S ( lw ) ) could be ill-conditioned X  X his is crucial particularly when the dimension d of the original data space is larger than the number n of labeled samples. In such situations,  X  I d included in S ( rlw ) worksasa regularizer and SELF can avoid overfitting to the labeled sa mples. Therefore, SELF is regarded as a regularized variant of LFDA and would be more stable and reliable than original LFDA particularly when the number of labeled samples is small. Note that unlike Eq.(17), S ( rlw ) does not have a pairwise expression since I d can not be expressed in a pairwise form. 3.4 Numerical Examples For illustrating how SELF behaves, let us use the Olivetti face data set 1 .The data set consists of 400 gray-scale face images (40 people, 10 images per person); each image consists of 4096 (= 64  X  64) pixels and each pixel takes an integer value between 0 and 255 as the intensity level. In this simulation, we use the image samples of only 10 subjects (i.e., totally 100 images) for making the visu-alization results clear. We note that the result does not change essentially (but visually denser) when all 400 images are used.

Among 10 people used for the experim ents, 3 subjects are with glasses and other 7 are without glasses (see the top-left pictures of Figure 2). Our task is to embed the face images into a two-dimensional space so that the subjects with and without glasses are separated from each other. We treat 1 image per person as labeled (i.e., totally 3 faces with glasses and 7 faces without glasses) and the rest are treated as unlabeled . Since each class contains s everal different subjects, this data set is thought to possess within-class multimodality.

The embedded results are shown in Figu re 2, where circle/ triangle symbols are faces with/without glasses and filled/unfilled symbols are labeled/unlabeled samples. The figure shows that FDA and L FDA perfectly separate the labeled samples in different classes from each oth er. However, unlabeled samples tend to be mixed due to an overfitting phenomenon. PCA and LPP tend to mix the labeled samples in different classes due to the unsupervised nature. Consequently, unlabeled samples in different classes are also mixed. On the other hand, SELF with  X  =0 . 5 clearly separates the labeled samples in different classes from each other, and at the same time, it also nicel y separates the unlabeled samples in different classes from each other. We note that, in this visualization simulation, the result of SELF is not sensitive to the choice of the trade-off parameter  X  ; the results are almost unchanged for 0 . 01  X   X   X  0 . 99. In this section, we experimentally eval uate the performance of relevant dimen-sionality reduction methods using standard classification benchmark data sets.
The book [3] conducted systematic experiments for comparing semi-supervised learning methods. The res ults showed that each method performs very well for a particular type of data sets. However, at the same time, it tends to be poor for other kinds of data sets. Thus, the performance of semi-supervised learning methods is highly dependent on the type of data sets and there seems to be no single best method. On the other hand, 1-nearest neighbor classifier is shown to be stable for various data sets, although it may not be the best possible method in semi-supervised classification. For avoiding the bias caused by the choice of the learning methods, we decided to use the 1-nearest neighbor classifier in our experiments.

The misclassification rate is sometim es monotone increa sing as the dimen-sionality is reduced 2 . In such cases, if the best dimensionality is chosen, e.g., by cross-validation, the largest dimension is mostly chosen (i.e., no dimensionality reduction). Then we may not be able to compare the performance of dimensional-ity reduction methods in a meaningful way. Prefixing the reduced dimensionality r to some number is a possible option for avoiding the above problem, but the evaluation results can significantly depend on the choice of the dimensionality. Based on this argument, we decided to use the average misclassification rate over reduced dimensions (or equivalently the area under the classification error curve) as our error metric, which we believe to be reasonable in the current experiments.

First, we employ the benchmark data sets taken from the book [3], which consist of 9 semi-supervised data sets. We refer to them as the SSL data sets. We did not test the SSL8 and SSL9 data sets since they are too huge. Note that the SSL6 data mean and standard deviation of the misclassification rate over repetitions. Since we had a numerical problem when computin g LFDA, we slightly regularized it and consider SELF with  X  =0 . 001 as LFDA. The fulfillment of the cluster assumption [3] is described as  X  X A X , which is the corr ect classification rate by the 1-nearest-neighbor classifier when both training and test labels are used for classifying all the training and test samples. Note that CA is computed before dimensionality reduction is applied, so it represents the fulfillment of the cluster assumption of the original data samples. The larger the value of CA is, the more reliable the cluster assumption would be (although the values are coarse). When the number of labeled samples is 100 (see the upper half of the table), LFDA and PCA tend to work well in a complementary way X  X FDA works well if CA is small while PCA works well if CA is large. SELF with  X  =0 . 5 tends to make up the deficit of each method; mo reover it can outperform both LFDA and PCA for some cases. We also test  X  X ELF(CV) X , where  X  in SELF is chosen table show that SELF(CV) further improves the performance over SELF with  X  =0 . 5. LPP does not work so well on the whole. The combination of LFDA and LPP in a similar way (indicated by SELF X (CV) in the table) also does not perform as good as SELF(CV). We also tested the combination of LFDA, PCA, and LPP, but this did not further improve the performance over SELF so we omit the detail.

When the number of labeled samples is only 10 (see the lower half of Table 1), the difference of the performance amon g the methods shrinks but SELF(CV) is still slightly better than the other methods.

We also conducted similar experiments using the IDA data sets [6], where we randomly extracted labeled and unlabeled samples from the pool of all samples; we tested n = 100 , 30. The results are summarized in Table 2, showing that SELF(CV) still compares favorably with alternative methods.

Overall, SELFreg is shown to be a useful dimensionality reduction. Our approach to dimensionality reduction in this paper is called the filter ap-proach, i.e., the dimensionality reducti on procedure is independent of subsequent classification algorithms. Our experim ental results showed that the proposed method, SELF , works well when it is combined with the 1-nearest-neighbor classifier. An important fut ure direction is to develop a wrapper method of semi-supervised dimensionality reduction, which explicitly takes properties of subsequent classification algorithms i nto account. We expect that a wrapper ap-proach is promising in semi-supervised l earning since the per formance of elab-orate semi-supervised learning methods is highly dependent on the reliability of the assumption behind unlabeled samples such as the cluster or manifold structure [3].

In this paper, we focused on linear dimen sionality reduction. However, we can show that a non-linear variant of SELF is obtained by employing the standard kernel trick . This kernelized variant also allows us to reduce the dimensionality of non-vectorial structured data such as strings, trees, and graphs [7]. However, kernelized SELF shares the common diffi culty in kernel methods, i.e., how to choose the kernel functions. This needs to be investigated in the context of semi-supervised dimensionality reduction. In the future work, we will also explore semi-supervised dimensionality reduction of structured data using kernel SELF.
A remaining important issue to be discussed X  X hich is common to all semi-supervised learning techniques X  X s how to optimize tuning parameters. We may simply employ cross-validation for this purpose, but it has two potential prob-lems. The first problem is that the number of labeled samples is typically small in semi-supervised learning scenarios and thus cross-validation is not reliable [3]. Fortunately, our experiments showed that SELF is not so sensitive to the trade-off parameter  X  in small sample cases, but there is still room for further improvement. The second problem is tha t labeled samples and unlabeled sam-ples can have different (input) distributions. Such a situation is referred to as covariate shift in statistics and ordinary cross-validation is known to be signifi-cantly biased; importance-weighted cross-validation is unbiased under covariate shift [8]. In the future work, we will investigate how the covariate shift adaptation techniques could be employed in the context of semi-supervised dimensionality reduction.

Finally, it is important to compare the performance of the proposed method with other related methods, e.g., [9,10].
 Acknowledgments. The authors would like to thank members of T-PRIMAL (Tokyo PRobabilistic Inference and MAchine Learning) for their fruitful com-ments. MS acknowledges financial support from MEXT (Grant-in-Aid for Young Scientists 17700142 and Grant-in-Aid for Scientific Research (B) 18300057) and Tateishi Science and Technology Foundation.

