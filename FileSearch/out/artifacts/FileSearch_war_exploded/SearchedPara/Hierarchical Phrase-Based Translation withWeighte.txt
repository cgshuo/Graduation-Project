 University of Cambridge University of Vigo University of Cambridge University of Vigo University of Cambridge and alignment. The decoder is implemented with standard Weighted Finite-State Transducer (WFST) operations as an alternative to the well-known cube pruning procedure. We find that procedures, yielding further gains when applying long-span language models and Minimum defined by hierarchical rules. We show that shallow-n grammars, low-level rule catenation, language pairs. 1. Introduction
Hierarchical phrase-based translation (Chiang 2005) is one of the current promising approaches to statistical machine translation (SMT). Hiero SMT systems are based on probabilistic synchronous context-free grammars (SCFGs) whose translation rules can be extracted automatically from word-aligned parallel text. These grammars can produce a very rich space of candidate translations and, relative to simpler phrase-based systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident in translation between dissimilar languages, such as English and Chinese (Chiang 2005, 2007). Hiero is able to learn and apply complex patterns in movement and translation that are not possible with simpler systems. Hiero can also be used to good effect on  X  X impler X  problems, such as translation between English and Spanish (Iglesias et al. 2009c), even though there is not the same need for the full complexity of movement and translation. If gains in using Hiero are small, however, the computational and modeling complexity involved are difficult to justify. Such concerns would vanish if there were reliable methods to match Hiero complexity for specific translation problems. Loosely put, it would be a good thing if the complexity of a system was somehow proportional to the improvement in translation quality the system delivers.

Risk decoding is widely used to rescore and improve hypotheses produced by indi-vidual systems (Kumar and Byrne 2004; Tromble et al. 2008; de Gispert et al. 2009), and more aggressive system combination techniques which synthesize entirely new hypotheses from those of contributing systems can give even greater translation im-provements (Rosti et al. 2007; Sim et al. 2007). It is now commonplace to note that even the best available individual SMT system can be significantly improved upon by such techniques. This puts a burden on the underlying SMT systems which is somewhat unusual in NLP. The requirement is not merely to produce a single hypothesis that is as good as possible. Ideally, the SMT systems should generate large collections of candidate hypotheses that are simultaneously diverse and of good quality. certain limitations. Spurious ambiguity (Chiang 2005) was described as
This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all distinct hypotheses to a fixed depth by means of k -best hypothesis lists. If enumeration was not necessary, or if the lists could be arbitrarily deep, there might still be many duplicate derivations, but at least the hypothesis space would not be impoverished. 1997; Setiawan et al. 2009). For our purposes we say that overgeneration occurs when different derivations based on the same set of rules give rise to different translations. An example is given in Figure 1.
 synthesized from rules extracted from training data; a strong target language model, such as a high order n -gram, is typically relied upon to discard unsuitable hypotheses.
Overgeneration does complicate translation, however, in that many hypotheses are introduced only to be subsequently discarded. The situation is further complicated by search errors . Any search procedure which relies on pruning during search is at risk of search errors and the risk is made worse if the grammars tend to introduce many similar scoring hypotheses. In particular we have found that cube pruning is very prone to search errors, that is, the hypotheses produced by cube pruning are not the top scoring hypotheses which should be found under the Hiero grammar (Iglesias et al. 2009b). 506 problematic as the amount of parallel text grows. As the number of rules in the grammar increases, the grammars become more expressive, but the ability to search them does not improve. This leads to a widening gap between the expressive power of the grammar and the ability to search it to find good and diverse hypotheses.
 tended to address some of the limitations in its original formulation.

Lattice-based hierarchical translation We describe how the cube pruning procedure
Shallow-n grammars and additional nonterminal categories Nonterminals can be in-
With these refinements we find that hierarchical phrase-based translation can be effi-ciently carried out with no (or minimal) search errors in large-data tasks and can achieve state-of-the-art translation performance.
 lowing the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne (2006), and Graehl, Knight, and May (2008) elucidate other machine translation models, we can use WFST operations to make the operations of the Hiero decoder very clear. The simplicity of the analysis makes it possible to focus on the underlying grammars and avoid the complexities of heuristic search procedures. Once the decoder is formulated, implementation is mostly straightforward using standard WFST techniques developed for language processing (Mohri, Pereira, and Riley 2002). What difficulties arise are due to using finite state techniques with grammars which are not themselves finite state.
We will show, however, that the basic operations which need to be performed, such as extracting sufficient statistics for minimum error rate training, can be done relatively easily and naturally. 1.1 Overview
In Section 2 we describe HiFST, which is a hierarchical phrase-based translation system based on the OpenFST WFST libraries (Allauzen et al. 2007). We describe how trans-lation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used for parsing under Hiero. We also review some modeling issues needed for practical trans-lation, such as the efficient handling of source language deletions and the extraction of statistics for minimum error rate training. This requires running HiFST in  X  X lignment hypotheses.
 hierarchical phrase-based search space as defined by hierarchical translation rules. To efficiently explore the largest possible space and avoid pruning in search, we introduce ways to easily adapt the grammar to the reordering needs of each language pair. We describe the use of additional nonterminal categories to limit the degree of rule nesting, and can directly control the minimum or maximum span each translation rule can cover.
Chinese-to-English, and review translation results for Spanish-to-English and Finnish-to-English translation. In these experiments we contrast the performance of lattice-based and cube pruning hierarchical decoding and we measure the impact on processing time and translation performance due to changes in search parameters and grammar configurations. We demonstrate that it is easy and feasible to compute the marginal instead of the Viterbi probabilities when using WFSTs, and that this yields gains in translation performance. And finally, we show that lattice-based translation performs significantly better than k -best lists for the task of combining translation hypotheses generated from alternative morphological segmentations of the data via lattice-based
MBR decoding. 2. Hierarchical Translation and Alignment with WFSTs
Hierarchical phrase-based rules define a synchronous context-free grammar (CFG) and a particular search space of translation candidates. Table 1 shows the type of rules in-cluded in a standard hierarchical phrase-based grammar, where T denotes the terminals (words) and  X  is a bijective function that relates the source and target nonterminals of 508 each rule (Chiang 2007). This function is defined if there are at least two nonterminals, and for clarity of presentation may be omitted in general rule discussions. When  X  ,  X  {
T } + , that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair. related to CYK+ (Chappelier and Rajman 1998). Parsing follows the description of
Chiang (2005, 2007); it maintains back-pointers and employs hypothesis recombination without pruning. The underlying model is a probabilisitic synchronous CFG consisting of a set R = { R r } of rules R r : N r  X   X  r ,  X  r / p r
S  X  SX , SX . N denotes the set of nonterminal categories (examples are given in
Section 3), and p r denotes the rule probability, typically transformed to a cost c otherwise noted we use the tropical semiring, so c r =  X  log p (words), and the grammar builds parses based on strings  X  ,  X  in the CYK grid is specified by a nonterminal symbol and position in the CYK grid: ( N , x , y ), which spans s x + y  X  1 x on the source sentence.
 generation of translations is a second step that follows parsing. For this second step, we describe a method to construct word lattices with all possible translations that can be produced by the hierarchical rules. Construction proceeds by traversing the CYK grid s x from every derivation headed by N . These lattices also contain the translation scores on their arc weights.

Mohri, and Roark 2003). 2.1 Lattice Construction over the CYK Grid that cell.
 from the target side of the rule  X  r by concatenating lattices corresponding to the ele- X  pointer BP ( N , x , y , r , i ); in this case, the lattice used is where A ( t ), t  X  T returns a single-arc acceptor which accepts only the symbol t .The lattice L ( N , x , y ) is then built as the union of lattices corresponding to the rules in R ( N , x , y ): respectively, as described by Allauzen et al. (2007). If a rule R to the exit state of the lattice L ( N , x , y , r ) prior to the operation of Equation (3). 2.1.1 An Example of Phrase-based Translation. Figure 2 illustrates this process for a three-word source sentence s 1 s 2 s 3 under monotonic phrase-based translation. The left-hand side shows the state of the CYK grid after parsing using the rules R three standard phrases, that is, rules with only terminals ( R ( R , R 5 ). Arrows represent back-pointers to lower-level cells. We are interested in the uppermost S cell ( S , 1, 3), as it represents the search space of translation hypotheses covering the whole source sentence. Two rules ( R 4 , R 5
L ( S , 1, 3) will be obtained by the union of the two lattices found by the back-pointers of these two rules. This process is explicitly derived in the right-hand side of Figure 2. 2.1.2 An Example of Hierarchical Translation. Figure 3 shows a hierarchical scenario for the same sentence. Three rules, R 6 , R 7 , R 8 , are added to the example of Figure 2, thus providing two additional derivations. This makes use of sublattices already produced in the creation of L ( S ,1,3,5)and L ( X ,1,3,1)inFigure2;theseareshownwithin 510 2.2 A Procedure for Lattice Construction
Figure 4 presents an algorithm to build the lattice for every cell. The algorithm uses memoization: If a lattice for a requested cell already exists, it is returned (line 2); otherwise it is constructed via Equations (1) X (3). For every rule, each element of the target side (lines 3,4) is checked as terminal or nonterminal (Equation (2)). If it is a terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice
L ( N , x , y , r ) for each rule is built by Equation (1) (line 9). The lattice cell is then found by union of all the component rules (line 10, Equation (3)); this lattice is then reduced by standard WFST operations (lines 11, 12, 13). It is important at this point to remove any epsilon arcs which may have been introduced by the various WFST union, concatenation, and replacement operations (Allauzen et al. 2007).
 2.2.1 Delayed Translation. Equation (2) leads to the recursive construction of lattices in upper levels of the grid through the union and concatenation of lattices from lower levels. If Equations (1) and (3) are actually carried out over fully expanded word lattices, the memory required by the upper lattices will increase exponentially.
 effectively builds a skeleton for the desired lattice and delays the creation of the final
To make this exact, we define a function g ( N , x , y ) which returns a unique tag for each BP ( N , x , y , r , i ), these special arcs are introduced as pointers (Figure 5, left). Each still represents the entire search space of all translation hypotheses covering the span, however. Importantly, operations on these lattices X  X uch as lossless size reduction via determinization and minimization (Mohri, Pereira, and
Riley 2002) X  X an still be performed. Owing to the existence of multiple hierarchical rules which share the same low-level dependencies, these operations can greatly reduce the size of the skeleton lattice; Figure 5 shows the effect on the translation example. This process is carried out for the lattice at every cell, even at the lowest level where there are only sequences of word terminals. As stated, size reductions can be significant. Not all redundancy is removed, however, because duplicate paths may arise through the concatenation and union of sublattices with different spans.

A single FST replace operation (Allauzen et al. 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the complete target word lattice for the whole source sentence. The use of the lattice pointer arc was inspired by the  X  X azy evaluation X  techniques developed by Mohri, Pereira, and Riley (2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its implementation uses the infrastructure provided by the OpenFST libraries for delayed composition. 2.2.2 Top-level Pruning and Search Pruning. The final translation lattice be quite large after the pointer arcs are expanded. We therefore apply a word-based 512 language model via WFST composition (Allauzen et al. 2007) and perform likelihood-based pruning based on the combined translation and language model scores. We call this top-level pruning because it is performed over the topmost lattice.
 simple strategy is to monitor the number of states in the determinized lattices
If this number is above a threshold, we expand any pointer arcs and apply a word-based language model via composition. The resulting lattice is then reduced by likelihood-based pruning, after which the LM scores are removed. These pruning strategies can be very selective, for example allowing the pruning threshold to depend on the height of the cell in the grid. In this way the risk of search errors can be controlled. pruning, although different WFST realizations are required. For top-level pruning, a standard implementation as described by Allauzen et al. (2007) is appropriate. For search pruning, the WFST must allow for incomplete language model histories, because many sublattice paths are incomplete translation hypotheses which do not begin with a sentence-start marker. The language model acceptor is constructed so that initial substrings of length less than the language model order are assigned no weight under the language model. 2.2.3 Constrained Source Word Deletion. As a practical matter it can be useful to allow SMT systems to delete some source words rather than to enforce their translation. Deletions can be allowed in Hiero by including in the grammar a set of special deletion rules of the type: X  X  s,NULL . Unconstrained application of these rules can lead to overly large and complex search spaces, however. We therefore limit the number of consecutive source word deletions as we explore each cell of the CYK grid. This is done by standard composition with an unweighted transducer that maps any word to itself, and up to k NULL tokens to arcs. In Figure 6 this simple transducer for k = 1and k = 2 is drawn.
Composition of the lattice in each cell with this transducer filters out all translations with more than k consecutive deleted words. 2.3 Hierarchical Phrase-Based Alignment with WFSTs
We now describe a method to apply our decoder in alignment mode. The objective in alignment is to recover all the derivations which can produce a given translation. We do this rather than keep track of the rules used during translation, because we find it faster and more efficient first to generate translations and then, by running the system as an aligner with a constrained target space, to extract all the relevant derivations with their costs. As will be discussed in Section 2.3.1, this is useful for minimum error training, where the contribution of each feature to the overall hypothesis cost is required for system optimization.  X  s from all possible rule derivations to all possible translations, and then compose this transducer with an acceptor for the translations of interest. Creating this transducer which maps derivations to translations is not feasible for large translation grammars, so we instead keep track of rules as they are used to generate a particular translation output. We introduce two modifications into lattice construction over the CYK grid described in Section 2.2: 1. In each cell transducers are constructed which map rule sequences to the 2. As these transducers are built they are composed with acceptors for r in the input language (rule indices) and the symbol t in the output language (target words). The weight assigned to each arc is the same in alignment as in translation. With input symbols and target words in the output symbols. A simple example is given in Figure 7 where two rule derivations for the translation t transducer.
 references, we can discard non-desired translations via standard FST composition of 514 the lattice transducer with the given reference acceptor. In principle, this would be done in the uppermost cell of the CYK, once the complete source sentence has been covered.
However, keeping track of all possible rule derivations and all possible translations until the last cell may not be computationally feasible for many sentences. It is more desirable to carry out this filtering composition in lower-level cells while constructing the lattice over the CYK grid so as to avoid storing an increasing number of undesired translations and derivations in the lattice. The lattice in each cell should contain translations formed only from substrings of the references.
 strings of each target reference string. For instance, given the reference string t t , we build an acceptor for all substrings t i ... t j , where 1 to lattices in all cells ( x , y ) that do not span the whole sentence. In the uppermost cell we compose with the reference acceptor which accepts only complete reference strings. Given a lattice of target references, the unweighted substring acceptor is built as: 1. change all non-initial states into final states 2. add one initial state and add arcs from it to all other states t t 4 . The substring acceptor also accepts an empty string, accounting for those rules that delete source words, which in other words translate into NULL. In some instances the final composition with the reference acceptor might return an empty lattice. If this happens there is no rule sequence in the grammar that can generate the given source and target sentences simultaneously.
 tions to translations. These transducers are somewhat impoverished relative to parse trees and parse forests, which are more commonly used to encode this relationship. It is easy to map from a parse tree to one of these transducers but the reverse essentially requires re-parsing to recreate the tree structure. The structures of the parse trees asso-ciated with a translation are not needed by many algorithms, however. In particular, parameter optimization by MERT requires only the rules involved in translation. Our approach keeps only what is needed by such algorithms. This approach also has prac-translations. 2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are associated with hierarchical translation rules through a factoring into features within a log-linear model (Och and Ney 2002). We assume that we have a collection of K features and that the cost c r of each rule R r is c r = K k = 1  X  feature value for the r th rule and  X  k is the weight assigned to the k For a parse which makes use of the rules R r 1 ... R r N , its cost feature to the overall translation score for that parse. These are the quantities which need to be extracted from alignment lattices for use in procedures such as minimum error rate training for estimation of the feature weights  X  consistent with the total parse score. Further steps must be taken to factor this over-all score to identify the contribution due to individual features or translation rules.
We introduce a rule acceptor which accepts sequences of rule indices, such as the input sequences of the alignment transducer, and assigns weights in the form of
K-dimensional vectors. Each component of the weight vector corresponds to the feature value for that rule. Arcs have the form 0 R r / w r  X  X  X  0 where w of composition with this rule acceptor is given in Figure 9 to illustrate how feature scores are mapped to components of the weight vector. The same operations can be applied to the (unweighted) alignment transducer on a much larger scale to extract the statistics needed for minimum error rate training.
 that only the best rule derivation that generated each translation candidate is taken into account when extracting feature contributions for MERT. However, given the alignment transducer L , this could also be performed in the log semiring (marginal likelihoods), taking into account the feature contributions from all rule derivations, for each translation candidate. This would be adequate if the translation system also car-ried out decoding in the log semiring, an experiment which is partially explored in Section 4.4.
 cannot be calculated in this scheme, since the language model score cannot be factored in terms of rules. To obtain the language model contribution, we simply carry out
WFST composition (Allauzen et al. 2007) between an unweighted acceptor of the target 516 sentences and the n-gram language model used in translation. After determinization, the cost of each path in the acceptor is then the desired LM feature contribution. 3. Shallow-n Translation Grammars: Translation Search Space Refinements
In this section we describe shallow-n grammars in order to reduce Hiero overgeneration and adapt the grammar complexity to specific language pairs; the ultimate goal is to de-fine the most constrained grammar that is capable of generating the desired movement and translation, so that decoding can be performed without search errors.
Consider the example shown in Figure 10, which shows a hierarchical grammar defined the rule derivations for each tree are R 1 R 4 R 3 R 5 and R is shown the translation generated and the phrase-level alignment. Comparing the two trees and alignments, the leftmost tree makes use of more reordering when translating from source to target through the nested application of the hierarchical rules R
For some language pairs this level of reordering may be required in translation, but for other language pairs it may lead to overgeneration of unwanted hypotheses. Suppose the grammar in this example is modified as follows: 1. A nonterminal X 0 is introduced into hierarchical translation rules R 3 : X  X  X 0 s 3 ,t 5 X 0
R 4 : X  X  X 0 s 4 ,t 3 X 0 2. Rules for lexical phrases are applied to X 0
These modifications exclude parses in which hierarchical translation rules generate other hierarchical rules, except at the 0 th level which generate lexical phrases. Con-sequently the left most tree of Figure 10 cannot be generated and t allowable translation of s 1 s 2 s 3 s 4 . We call this a  X  X hallow-1 X  grammar: The maximum degree of rule nesting allowed is 1 and only the glue rule can be applied above this level.
 important aspects that can have a strong impact on the search space. A shallow-n translation grammar can be formally defined as: 1. the usual nonterminal S 2. a set of nonterminals { X 0 , ... , X N } 3. two glue rules: S  X  X N , X N and S  X  SX N , SX N 4. hierarchical translation rules for levels n = 1, ... , N : 5. translation rules which generate lexical phrases: 3.1 Avoiding Some Spurious Ambiguity
The added requirement in condition (4) in the definition of shallow-n grammars is included to avoid some instances in which multiple parses lead to the same translation.
It is not absolutely necessary but it can be added without any loss in representational capability. To see the effect of this constraint, consider the following example with a source sentence s 1 s 2 and a shallow-1 grammar defined by these four rules:
R would not be allowed under the constraint introduced here because it does not rewrite an X 1 to an X 0 . 3.2 Structured Long-Distance Movement
The basic formulation of shallow-n grammars allows only the upper-level nonterminal category S to act within the glue rule. This can prevent some useful long-distance movement, as might be needed to translate Arabic sentences in Verb-Subject-Object order into English. It often happens that the initial Arabic verb requires long-distance movement, but the subject which follows can be translated in monotonic order. For instance, consider the following Romanized Arabic sentence: (CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ... where the verb  X  X Alb X  must be translated into English so that it follows the translations of the five subsequent Arabic words  X  X lwzrA X  AlmjtmEyn Alywm fy dm$q X , which 518 are themselves translated monotonically. A shallow-1 grammar cannot generate this movement except in the relatively unlikely case that the five words following the verb can be translated as a single phrase.
 form movable groups of phrases. Additional nonterminals { successive generation of k nonterminals X N  X  1 in monotonic order for both languages, where K 1  X  k  X  K 2 . These act in the same manner as the glue rule does in the uppermost level. Applying M k nonterminals at the N X 1 level allows one hierarchical rule to perform a long-distance movement over the tree headed by M k .
 values of k for the successive productions of nonterminals X sible ways to formulate and constrain these grammars. If K is equivalent to the previous definition of shallow-n grammars, because monotonic production is only allowed by the glue rule of level N. If K search space defined by the grammar is greater than the standard shallow-n grammar as it includes structured long-distance movement. Finally, if K space is different from standard shallow-n as the n level is only used for long-distance movement.
 1. the usual nonterminal S 2. a set of nonterminals { X 0 , ... , X N } 3. a set of nonterminals { M K 1 , ... , M K 2 } for K 1 4. two glue rules: S  X  X N , X N and S  X  SX N , SX N 5. hierarchical translation rules for level N : 6. hierarchical translation rules for levels n = 1, ... , N 7. translation rules which generate lexical phrases: 8. rules which generate k nonterminals X N  X  1 : where I denotes the identity function that enforces monotonocity in the nonterminals.
For example, with a shallow-1 grammar, M 3 leads to the monotonic production of three nonterminals X 0 , which leads to the production of three lexical phrase pairs; these can be moved with a hierarchical rule of level 1. This is graphically represented by the leftmost tree in Figure 11. With a shallow-2 grammar, M 2 leads to the monotonic production of two nonterminals X 1 , a movement represented by the rightmost tree in Figure 11. This movement cannot be achieved with a shallow-1 grammar. 3.3 Minimum and Maximum Rule Span
It is useful to define two parameters which further control the application of hierarchical translation rules in generating the search space. Parameters hmax and hmin specify the maximum and minimum height at which any hierarchical translation rule can be applied in the CYK grid. In other words, a hierarchical rule can only be applied in cell ( x , y )if hmin  X  y  X  hmax . Note that these parameters can also be set independently for each nonterminal category. 3.4 Verb Movement Grammars for Arabic-to-English Translation Following the discussion which motivated this section, we wish to model movement of
Arabic verbs when translating into English. We add to the shallow-n grammars a verb restriction so that the hierarchical translation rules (5) apply only if the source language string  X  contains a verb. This encourages translations in which the Arabic verb is moved at the uppermost level N . 3.5 Grammars Used for SMT Experiments
We now define the hierarchical grammars for the translation experiments which we describe next.

Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3. These grammars do not incorporate
Shallow-1, K 1 = 1 , K 2 = 3: hierarchical rules with one nonterminal can reorder a 520
Shallow-1, K 1 = 1 , K 2 = 3, vo : hierarchical rules with one nonterminal can reorder a
Shallow-2, K 1 = 2 , K 2 = 3, vo : two levels of reordering with monotonic production 4. Translation Experiments In this section we report on hierarchical phrase-based translation experiments with
WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English trans-lation tasks; some results for other language pairs are summarized in Section 4.6.
Translation performance is evaluated using the BLEU score (Papineni et al. 2001) as implemented for the NIST 2009 evaluation. 1 The experiments are organized as follows: 4.1 Experimental Framework
For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08 (and MT09) Arabic Constrained Data track (  X  150M words per language). In addition to reporting results on the MT08 set itself, we make use of a development set mt02-05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form a validation set mt02-05-test .The mt02-05-tune set has 2,075 sentences.
 2008 evaluation; 2 this is approximately 250M words per language. We report translation results on the NIST MT08 set, a development set tune-nw , and a validation set test-nw .
These tuning and test sets contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT05. The tune-nw set has 1,755 sentences, and test-nw set is similar.
 2008). We extract hierarchical rules from the aligned parallel texts using the constraints developed by Chiang (2007). We further filter the extracted rules by count and pattern as described by Iglesias et al (2009a). The following features are extracted from the parallel data and used to assign scores to translation rules: source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). 4-gram language models estimated over the English side of the parallel text (for each language pair) and a 965 million word subset of monolingual data from the English
Gigaword Third Edition (LDC2007T07). These are the language models used if pruning is needed during search. The main language model is a zero-cutoff stupid-backoff (Brants et al. 2007) 5-gram language model, estimated using 6.6B words of English text from the English Gigaword corpus. These language models are converted to WFSTs as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct application of back-off weights. In tuning the systems, standard MERT (Och 2003) iterative parameter estimation under IBM BLEU is performed on the development sets. 4.2 Contrast between HiFST and Cube Pruning
We contrast two hierarchical phrase-based decoders. The first decoder, HCP, is a k -best decoder using cube pruning following the description by Chiang (2007); in our im-plementation, these k -best lists contain only unique hypotheses (Iglesias et al. 2009a), which are obtained by extracting the 10,000 best candidates from each cell (including the language model cost), using a priority queue to explore the cross-product of the k -best lists from the cells pointed by nonterminals. We find that deeper k -best lists yields better performance than use of a likelihood threshold parameter. The second decoder, HiFST, is a lattice-based decoder implemented with WFSTs as described earlier.
Hypotheses are generated after determinization under the tropical semiring so that scores assigned to hypotheses arise from a single minimum cost/maximum likelihood derivation.
 weights and the first-pass language model, hypotheses are written to disk. For HCP we save translations as 10,000-best lists, whereas HiFST generates word lattices. The first-pass results are then rescored with the main 5-gram language model. In this operation the first-pass language model scores are removed before the main language model scores are applied. We then perform MBR rescoring. For the n -best lists we rescore the top 1,000 hypotheses using the negative sentence-level BLEU score as the loss function (Kumar and Byrne 2004); we have found that using a deeper k -best list is impractically slow. For the HiFST lattices we use lattice-based MBR search procedures described by Tromble et al. (2008) in an implementation based on standard WFST operations (Allauzen et al. 2007). 4.2.1 Shallow-1 Arabic-to-English Translation. We translate Arabic-to-English with shallow-1 hierarchical decoding: as described in Section 3, nonterminals are allowed only to generate target language phrases. Table 2 shows results for mt02-05-tune , mt02-05-test ,and mt08 . In this experiment we use MERT to find optimized parameters for
HCP and we use these parameter values in HiFST as well. This allows for a close comparison of decoder behavior, independent of parameter optimization.
 a vs. b) is nearly identical. The most notable difference in the first-pass behavior of the decoders is their memory use. For example, for an input sentence of 105 words, HCP uses 1.2Gb memory whereas HiFST takes only 900Mb under the same conditions. To run HCP successfully requires cube pruning with the first-pass 4-gram language model.
By contrast, HiFST requires no pruning during lattice construction and the first pass language model is not applied until the lattice is fully built at the upper most cell of the 522
CYK grid. For this grammar, HiFST is able to produce exact translations without any search errors.
 we can compare their search errors on a sentence-by-sentence basis. A search error is assigned to one of the decoders if the other has found a hypothesis with lower cost. For mt02-05-tune , we find that in 18.5% of the sentences HiFST finds a hypothesis with lower cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any sentence. This is as expected: The HiFST decoder requires no pruning prior to applying the first-pass language model, so search is exact.
 yield better rescoring results than the k -best lists produced by HCP. This is the case for both 5-gram language model rescoring and MBR. In MT08 rescoring, HCP k -best lists yield an improvement of 0.8 BLEU relative to the first-pass baseline, whereas rescoring
HiFST lattices yield an improvement of 2.0 BLEU. The advantage of maintaining a large the gains from subsequent rescoring procedures.
 cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves much more efficient to process compact lattices containing many hypotheses rather than independently processing each distinct hypothesis in k -best form. 4.2.2 Fully Hierarchical Chinese-to-English Translation. We translate Chinese-to-English with full hierarchical decoding: nonterminals are allowed to generate other hierarchical rules in recursion.
 tion 2.2.2, so that only glue rules are allowed at upper levels of the CYK grid; this is applied in both HCP and HiFST.
 and contains more than 10,000 states. The log-likelihood pruning threshold relative to the best path in the sublattices is 9.0.
 and mt08 . The first two rows show results for HCP when using MERT parameters optimized over k -best lists produced by HCP (row a) and by HiFST (row b); in the latter case, we are tuning HCP parameters over the hypothesis list generated by HiFST. When measured over test-nw this gives a 0.3 BLEU improvement. HCP benefits from tuning over the HiFST hypotheses and we conclude that using the k -best list obtained by the HiFST decoder yields better parameters in optimization.
 with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis with a lower cost for any sentence, indicating that the described pruning strategy for
HiFST is much broader than that of HCP. Note that HCP search errors are more frequent translation; the larger the search space, the more likely it is that search errors will be introduced by the cube pruning algorithm.
 translation hypotheses contained in the lattices. Relative to the first-pass baseline in MT08, rescoring HiFST lattices yields a gain of 2.1 BLEU, compared to a gain of 0.7
BLEU with HCP k -best lists. 4.2.3 Reliability of n-gram Posterior Distributions. MBR decoding under linear BLEU (Tromble et al. 2008) is driven mainly by the presence of high posterior n -grams in the lattice; the low posterior n -grams have poor discriminatory power. In the following experiment, we show that high posterior n -grams are more likely to be found in the references, and that using the full evidence space of the lattice is much better than even very large k -best lists for computing posterior probabilities. Let denote the set of n -grams of order i in the first-pass translation 1-best, and let { w
For confidence threshold  X  ,let N i ,  X  = { w  X  N i : p ( w n -grams in N i with posterior probability greater than or equal to  X  , where p ( w the posterior probability of the n -gram w , that is, the sum of the posterior probabilities of all translations containing at least one occurrence of w . The precision at order i for threshold  X  is the proportion of n -grams in N i ,  X  found in the references:
The average per-sentence 4-gram precision at a range of posterior probability thresholds  X  is shown in Figure 12. The posterior probabilities are computed using either the full 524 lattice L or a k -best list of the specified size. The 4-gram precision of the 1-best trans-lations is approximately 0.35. At higher values of  X  , the reference precision increases considerably. Expanding the k -best list size from 1,000 to 10,000 hypotheses only slightly improves the precision but much higher precisions are observed when the full evidence space of the lattice is used. The improved precision results from more accurate estimates of n -gram posterior probabilites and emphasizes once more the advantage of lattice-based decoding and rescoring techniques. 4.3 Grammar Configurations and Search Parameters
We report translation performance and decoding speed as we vary hierarchical gram-mar depth and the constraints on low-level rule concatenation (see Section 3). Unless otherwise noted, hmin =1and hmax = 10 throughout (except for the  X  X  X  nonterminal category, where these constraints are not relevant). 4.3.1 Grammars for Arabic-to-English Translation. Table 4 reports Arabic-to-English trans-lation results using the alternative grammar configurations described in Section 3.5.
Results are shown in first-pass decoding (HiFST rows), and in rescoring with a larger 5-gram language model for the most promising configurations (+5g rows). Decoding time is reported for first-pass decoding only; rescoring time is negligible by comparison. not improve relative to a shallow-1 grammar, although decoding is much slower. This indicates that the additional hypotheses generated when allowing a hierarchical depth of two are not useful in Arabic-to-English translation. By contrast the shallow gram-mars that allow long-distance movement for verbs only (shallow-1 decoding time. Performance differences increase when the larger 5-gram is applied (Table 4, bottom). This is expected given that these grammars add valid translation candidates to the search space with similar costs; a language model is needed to select the good hypotheses among all those introduced. 4.3.2 Grammars for Chinese-to-English Translation. Table 5 shows contrastive results in
Chinese-to-English translation for full hierarchical and shallow-n ( n = 1, 2, 3) gram-mars. 3 Unlike Arabic-to-English translation, Chinese-to-English translation improves as the hierarchical depth of the grammar is increased (i.e., for larger n ). Decoding time also increases significantly. The shallow-1 grammar constraints which worked well for
Arabic-to-English translation are clearly inadequate for this task; performance degrades by approximately 1.0 BLEU relative to the full hierarchical grammar.
 nearly as good as that of the full hiero grammars; translation times are shorter and yield degradations of only 0.1 to 0.3 BLEU. Translation can be made significantly faster by constraining the shallow-3 search space with hmin = 9, 5, 2 for X tively; translation speed is reduced from 10.8 sec/word to 3.8 sec/word at a degradation of 0.2 to 0.3 BLEU relative to full Hiero.
 power in Chinese-to-English translation which is very similar to that of a full Hiero grammar. Each cell ( x , y ) is represented by a bigger set of nonterminals; this allows for more effective pruning strategies during lattice construction. We note also that hmax values greater than 10 yield little improvement. As shown in the five bottom rows of
Table 5, differences between grammar configurations tend to carry through after 5-gram rescoring. In summary, a shallow-3 grammar and filtering with hmin = 9, 5, 2 lead to a 0 . 4 degradation in BLEU relative to full Hiero. As a final contrast, the mixed-case NIST
BLEU-4 for the HiFST system on mt08 is 28.6. This result is obtained under the same evaluation conditions as the official NIST MT08 Constrained Training Track. 526 4.4 Marginalization Over Translation Derivations
As has been discussed earlier, the translation model in hierarchical phrase-based ma-chine translation allows for multiple derivations of a target language sentence. Each derivation corresponds to a particular combination of hierarchical rules and it has been argued that the correct approach to translation is to accumulate translation probability by summing over the scores of all derivations (Blunsom, Cohn, and Osborne 2008).
Computing this sum for each of the many translation candidates explored during de-coding is computationally difficult, however. For this reason the translation probability is commonly computed using the Viterbi max-derivation approximation. This is the approach taken in the previous sections in which translations scores were accumulated under the tropical semiring.
 to be computed efficiently. HiFST generates a translation lattice realized as a weighted transducer with output labels encoding words and input labels encoding the sequence of rules corresponding to a particular derivation, and the cost of each path in the lattice is the negative log probability of the derivation that generated the hypothesis. quence (Mohri 1997). When applied in the log semiring, this operator computes the sum of two paths with the same word sequence as x  X  y =  X  probabilities of alternative derivations can be summed.
 so it is still an approximation to the true translation probability. Computing the true translation probability would require the same operation to be repeated in every cell during decoding, which is very time consuming. Note that the translation lattice was generated with a language model and so the language model costs must be removed before determinization to ensure that only the derivation probabilities are included in the sum. After determinization, the language model is reapplied and the 1-best translation hypothesis can be extracted from the logarc determinized lattices. likelihoods) and the log semiring (marginal likelihoods). First-pass translation shows small gains in all sets: +0.3 and +0.4 BLEU for mt02-05-tune and mt02-05-test ,and+0.2 for mt08 . These gains show that the sum over alternative derivations can be easily obtained in HiFST simply by changing semiring and that these alternative derivations are beneficial to translation. The gains carry through to the large language model 5-gram rescoring stage but after LMBR the final BLEU scores are unchanged. The hypotheses selected by LMBR are in almost all cases exactly the same regardless of the choice of semiring. This may be due to the fact that our current marginalization procedure is only an approximation to the true marginal likelihoods, since the log semiring determiniza-tion operation is applied only in the uppermost cell of the CYK grid and MERT training is performed using regular Viterbi likelihoods.
 over derivations is beyond the scope of this paper. Our purpose here is to show how easily these operations can be done using WFSTs. 4.5 Combining Lattices Obtained from Alternative Morphological Decompositions
It has been shown that MBR decoding is a very effective way of combining translation hypotheses obtained from alternative morphological decompositions of the same source data. In particular, de Gispert et al. (2009) show gains for Arabic-to-English and Finnish-to-English when taking k -best lists obtained from two morphological decompositions of the source language. Here we extend this approach to the case of translation lat-tices and experiment with more than two alternative decompositions. We will show that working with translation lattices gives significant improvements relative to k -best lists.
 produced by translating one of the alternative morphological decompositions. The evidence space for MBR decoding is formed as the union of these lattices
The posterior probability of n -gram w in the union of lattices is computed as a simple 528 linear interpolation of the posterior probabilities according to the evidence space of each individual lattice so that where the interpolation parameters 0  X   X  i  X  1suchthat I i = 1 associated with each system in the combination and are optimized with respect to the tuning set. The system-specific posteriors required for the interpolation are com-puted as the sum is taken over the subset L ( i ) w = { E  X  L ( i ) paths with at least one occurrence of the n -gram w . These posterior probabilities are used in MBR decoding under the linear approximation to the BLEU score described in Tromble et al. (2008). We find that for system combination, decoding often produces output that is slightly shorter than required. A fixed per-word factor optimized on the tuning set is applied when computing the gain and this results in output with improved BLEU score and reduced brevity penalty.
 phological decompositions of the Arabic text (upper rows a, b, and c). For each decom-position an independent set of hierarchical rules is obtained from the respective parallel corpus alignments. The decompositions were generated by the MADA toolkit (Habash and Rambow 2005) with two alternative tokenization schemes, and by the Sakhr Arabic Morphological Tagger, developed by Sakhr Software in Egypt.
 hypotheses obtained from two or three decompositions. The table also shows a contrast between decoding the joint k -best lists (rows named MBR, with k = 1, 000) and decod-ing the unioned translation lattices (rows named LMBR). In line with the findings of to using any one segmentation alone. Interestingly, here we find further gains when applying lattice-based MBR instead of a k -best approach, obtaining consistent gains of 0.6 X 0.8 BLEU across all sets.
 score for a+b+c LMBR system in MT08 is 44.9. This result is obtained under the same evaluation conditions as the official NIST MT08 Constrained Training Track. the mixed-case BLEU-4 is 48.3, which ranks first in the Arabic-to-English NIST 2009
Constrained Data Track. 6 4.5.1 System Combination and Reference Precision. We have demonstrated that MBR de-coding of multiple lattices generated from alternative morphological segmentations leads to significant improvements in BLEU score. We now show that one reason for the improved performance is that lattice combination leads to better n -gram posterior probability estimates. To combine two equally weighted lattices terpolation weights are  X  1 =  X  2 = 1 2 ; Equation (8) simplifies as p ( w p ( w | L (2) )). Figure 13 plots average per-sentence reference precisions for the 4-grams in the MBR 1-best of systems a and b and their combination (labeled a+b) at a range of posterior probability thresholds 0  X   X   X  1. Systems a and b have similar precisions at all values of  X  , confirming that the optimal interpolation weights for this combination should be equal. The precision obtained using n -gram posterior probabilities computed 530 from the combined lattices is higher than that of the individual systems. A higher proportion of the n -grams assigned high posterior probability under the interpolated distribution are found in the references and this is one of the reasons for the large gains in BLEU in lattice-based MBR system combination. 4.6 European Language Translation
The HiFST described here has also been found to achieve competitive performance for other language pairs, such as Spanish-to-English and Finnish-to-English.
 2008 Workshop on Statistical Machine Translation (Callison-Burch et al. 2008) based on the Europarl corpus. For the official test2008 evaluation set we obtain a BLEU score of 34.2 using a shallow-1 grammar. Similarly to the Arabic case, deeper grammars are not found to improve scores for this task.
 corpus using 3,000 sentences from the Q4/2000 period for testing with a single ref-erence. In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereas the full hierarchical grammar only achieved 27.6. This is further evidence that full-hierarchical grammars are not appropriate in all instances. In this case we suspect that the use of Finnish words without morphological decomposition leads to data sparsity problems and complicates the task of learning complex translation rules. The lack of a large English language model suitable for this domain may also make it harder to select the right hypothesis when the translation grammar produces many more English alternatives. 5. Conclusions We have described two linked investigations into hierarchical phrase-based translation.
We investigate the use of weighted finite state transducers rather than k -best lists to represent the space of translation hypotheses. We describe a lattice-based Hiero de-coder, with which we find reductions in search errors, better parameter optimization, and improved translation performance. Relative to these reductions in search errors, direct generation of target language translation lattices also leads to further translation improvements through subsequent rescoring steps, such as MBR decoding and the application of large n -gram language models. These steps can be carried out easily via standard WFST operations.
 for alignment and feature extraction so that statistics needed for system optimization can be easily obtained and represented as transducers. In particular, we make use of a lattice-based representation of sequences of rule applications, which proves useful for minimum error rate training. In all instances we find that using lattices as compact representations of translation hypotheses offers clear modeling advantages. mars, structured long-distance movement, and constrained word deletion. We find that these techniques can be used to fit the complexity of Hiero translation systems to individual language pairs. In translation from Arabic into English, shallow grammars make it possible to explore the entire search space and to do so more quickly but with the same translation quality as the full Hiero grammar. Even in complex translation tasks, such as Chinese to English, we find significant speed improvements with minimal loss in performance using these methods. We take the view that it is better to perform exact search of a constrained space than to risk search errors in translation.

CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns 1968). X  We have taken this formulation as a starting point for the development of novel realizations of Hiero. Our motivation has mainly been practical in that we seek improved translation quality and efficiency through better models and algorithms. works (Woods 1970; Mohri 1997). Although this connection is beyond the scope of this paper, we do note that Hiero translation requires keeping track of two grammars, one based on the Hiero translation rules and the other based on n -gram language model probabilities. These two grammars have very different dependencies which suggests that a full implementation of Hiero translation such as we have addressed does not have a simple expression as an RTN.
 Acknowledgments References 532
 } @gts.tsc.uvigo.es Hierarchical phrase-based translation generates translation hypotheses via the application of hierar-chical rules in CYK parsing (Chiang, 2005). Cube pruning is used to apply language models at each cell of the CYK grid as part of the search for a k-best list of translation candidates (Chiang, 2005; Chiang, 2007). While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limita-tion. We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs). In ev-ery CYK cell we build a single, minimal word lattice containing all possible translations of the source sen-tence span covered by that cell. When derivations contain non-terminals, we use pointers to lower-level lattices for memory efficiency. The pointers are only expanded to the actual translations if prun-ing is required during search; expansion is otherwise only carried out at the upper-most cell, after the full CYK grid has been traversed.

We describe how this decoder can be easily im-plemented with WFSTs. For this we employ the OpenFST libraries (Allauzen et al., 2007). Using standard FST operations such as composition, ep-silon removal, determinization, minimization and shortest-path, we find this search procedure to be simpler to implement than cube pruning. The main modeling advantages are a significant reduction in search errors, a simpler implementation, direct gen-eration of target language word lattices, and better integration with other statistical MT procedures. We report translation results in Arabic-to-English and Chinese-to-English translation and contrast the per-formance of lattice-based and cube pruning hierar-chical decoding. 1.1 Related Work Hierarchical phrase-based translation has emerged as one of the dominant current approaches to statis-tical machine translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based transla-tion and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned paral-lel text. We summarize some extensions to the basic approach to put our work in context.
Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube prun-ing, as described by Chiang (2007). Li and Khudan-pur (2008) report significant improvements in trans-lation speed by taking unseen n-grams into account within cube pruning to minimize language model re-quests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices fol-lowing Chappelier et al. (1999).

Extensions to Hiero Several authors describe ex-tensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008).

Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Ara-bic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierar-chical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007).
WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacu-berta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008).

To our knowledge, this paper presents the first de-scription of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based transla-tion with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Sec-tion 3 reports translation experiments for Arabic-to-English and Chinese-to-English, and Section 4 con-cludes. The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappe-lier and Rajman, 1998). Parsing follows the de-scription of Chiang (2005; 2007), maintaining back-pointers and employing hypothesis recombination without pruning. The underlying model is a syn-chronous context-free grammar consisting of a set R = { R r } of rules R r : N  X  h  X  r ,  X  r i / p r , with  X  X lue X  rules, S  X  h X , X i and S  X  h S X , S X i . If a rule has probability p r , it is transformed to a cost c r ; here we use the tropical semiring, so c r =  X  log p r . N denotes a non-terminal; in this paper, N can be either S , X , or V (see section 3.2). T denotes the terminals (words), and the grammar builds parses based on strings  X ,  X   X  {{ S, X, V }  X  T } + . Each cell in the CYK grid is specified by a non-terminal symbol and position in the CYK grid: ( N, x, y ) ,
In effect, the source language sentence is parsed using a context-free grammar with rules N  X   X  . The generation of translations is a second step that follows parsing. For this second step, we describe a method to construct word lattices with all possible translations that can be produced by the hierarchical rules. Construction proceeds by traversing the CYK grid along the backpointers established in parsing. In each cell ( N, x, y ) in the CYK grid, we build a target language word lattice L ( N, x, y ) . This lat-tice contains every translation of s x + y  X  1 derivation headed by N . These lattices also contain the translation scores on their arc weights. The ultimate objective is the word lattice L (
S, 1 , J ) which corresponds to all the analyses that cover the source sentence s J we can apply a target language model to L ( S, 1 , J ) to obtain the final target language translation lattice (Allauzen et al., 2003).
 We use the approach of Mohri (2002) in applying WFSTs to statistical NLP. This fits well with the use of the OpenFST toolkit (Allauzen et al., 2007) to implement our decoder. 2.1 Lattice Construction Over the CYK Grid In each cell ( N, x, y ) , the set of rule indices used by the parser is denoted R ( N, x, y ) , i.e. for r  X  R ( N, x, y ) , N  X  h  X  r ,  X  r i was used in at least one derivation involving that cell.

For each rule R r , r  X  R ( N, x, y ) , we build a lat-tice L ( N, x, y, r ) . This lattice is derived from the target side of the rule  X  r by concatenating lattices corresponding to the elements of  X  r =  X  r If an  X  r forward. If  X  r ( pointer BP ( N, x, y, r, i ) ; in this case, the lattice used is L ( N  X  , x  X  , y  X  ) . Taken together,
L ( N, x, y, r, i ) = where A ( t ) , t  X  T returns a single-arc accep-tor which accepts only the symbol t . The lattice L (
N, x, y ) is then built as the union of lattices cor-responding to the rules in R ( N, x, y ) :
Lattice union and concatenation are performed using the  X  and  X  WFST operations respectively, as described by Allauzen et al.(2007). If a rule R r has L (
N, x, y, r ) prior to the operation of Equation 3. 2.1.1 An Example of Phrase-based Translation
Figure 1 illustrates this process for a three word source sentence s based translation. The left-hand side shows the state of the CYK grid after parsing using the rules R 1 to R R sent backpointers to lower-level cells. We are inter-ested in the upper-most S cell ( S, 1 , 3) , as it repre-sents the search space of translation hypotheses cov-ering the whole source sentence. Two rules ( R 4 , R 5 ) are in this cell, so the lattice L ( S, 1 , 3) will be ob-tained by the union of the two lattices found by the backpointers of these two rules. This process is ex-plicitly derived in the right-hand side of Figure 1. 2.1.2 An Example of Hierarchical Translation
Figure 2 shows a hierarchical scenario for the same sentence. Three rules, R 6 , R 7 , R 8 , are added to the example of Figure 1, thus providing two ad-ditional derivations. This makes use of sublattices already produced in the creation of L ( S, 1 , 3 , 5) and L (
X, 1 , 3 , 1) in Figure 1; these are within {} . 2.2 A Procedure for Lattice Construction Figure 3 presents an algorithm to build the lattice for every cell. The algorithm uses memoization: if a lattice for a requested cell already exists, it is re-turned (line 2); otherwise it is constructed via equa-tions 1,2,3. For every rule, each element of the tar-get side (lines 3,4) is checked as terminal or non-terminal (equation 2). If it is a terminal element (line 5), a simple acceptor is built. If it is a non-terminal (line 6), the lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice L ( N, x, y, r ) for each rule is built by equa-tion 1 (line 9). The lattice L ( N, x, y ) for this cell is then found by union of all the component rules (line 10, equation 3); this lattice is then reduced by standard WFST operations (lines 11,12,13). It is important at this point to remove any epsilon arcs which may have been introduced by the various WFST union, concatenation, and replacement oper-ations (Allauzen et al., 2007). 1 function buildFst(N,x,y) 2 if  X  L ( N, x, y ) return L ( N, x, y ) 3 for r  X  R ( N, x, y ) , R r : N  X  h  X  ,  X  i 4 for i = 1 ... |  X  | 5 if  X  6 else 7 ( N  X  , x  X  , y  X  ) = BP (  X  8 L ( N, x, y, r, i ) = buildFst ( N  X  , x  X  , y  X  ) 9 L ( N, x, y, r ) = N 10 L ( N, x, y ) = L 11 fstRmEpsilon L ( N, x, y ) 12 fstDeterminize L ( N, x, y ) 13 fstMinimize L ( N, x, y ) 14 return L ( N, x, y ) 2.3 Delayed Translation Equation 2 leads to the recursive construction of lat-tices in upper-levels of the grid through the union and concatenation of lattices from lower levels. If equations 1 and 3 are actually carried out over fully expanded word lattices, the memory required by the upper lattices will increase exponentially.

To avoid this, we use special arcs that serve as pointers to the low-level lattices. This effectively builds a skeleton of the desired lattice and delays the creation of the final word lattice until a single replacement operation is carried out in the top cell (
S, 1 , J ) . To make this exact, we define a function g (
N, x, y ) which returns a unique tag for each lattice in each cell, and use it to redefine equation 2. With the backpointer ( N  X  , x  X  , y  X  ) = BP ( N, x, y, r, i these special arcs are introduced as:
L ( N, x, y, r, i ) =
The resulting lattices L ( N, x, y ) are a mix of tar-get language words and lattice pointers (Figure 4, top). However each still represents the entire search space of all translation hypotheses covering the span. Importantly, operations on these lattices  X  such as lossless size reduction via determinization and minimization  X  can still be performed. Owing to the existence of multiple hierarchical rules which share the same low-level dependencies, these opera-tions can greatly reduce the size of the skeleton lat-tice; Figure 4 shows the effect on the translation ex-ample. This process is carried out for the lattice at every cell, even at the lowest level where there are only sequences of word terminals. As stated, size reductions can be significant. However not all redu-dancy is removed, since duplicate paths may arise through the concatenation and union of sublattices with different spans.

At the upper-most cell, the lattice L ( S, 1 , J ) con-tains pointers to lower-level lattices. A single FST replace operation (Allauzen et al., 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the com-plete target word lattice for the whole source sen-tence. The use of the lattice pointer arc was in-spired by the  X  X azy evaluation X  techniques developed by Mohri et al (2000). Its implementation uses the infrastructure provided by the OpenFST libraries for delayed composition, etc. 2.4 Pruning in Lattice Construction The final translation lattice L ( S, 1 , J ) can grow very large after the pointer arcs are expanded. We there-fore apply a word-based language model, via WFST composition, and perform likelihood-based prun-ing (Allauzen et al., 2007) based on the combined translation and language model scores.

Pruning can also be performed on sublattices during search. One simple strategy is to monitor the number of states in the determinized lattices L (
N, x, y ) . If this number is above a threshold, we expand any pointer arcs and apply a word-based lan-guage model via composition. The resulting lattice is then reduced by likelihood-based pruning, after which the LM scores are removed. This search prun-ing can be very selective. For example, the pruning threshold can depend on the height of the cell in the grid. In this way the risk of search errors can be controlled. We report experiments on the NIST MT08 Arabic-to-English and Chinese-to-English translation tasks. We contrast two hierarchical phrase-based decoders. The first decoder, Hiero Cube Pruning (HCP), is a k-best decoder using cube pruning implemented as de-scribed by Chiang (2007). In our implementation, k-best lists contain unique hypotheses. The second de-coder, Hiero FST (HiFST), is a lattice-based decoder implemented with Weighted Finite State Transduc-ers as described in the previous section. Hypotheses are generated after determinization under the trop-ical semiring so that scores assigned to hypotheses arise from single minimum cost / maximum likeli-hood derivations. We also use a variant of the k-best decoder which works in alignment mode: given an input k-best list, it outputs the feature scores of each hypothesis in the list without applying any pruning. This is used for Minimum Error Training (MET) with the HiFST system.

These two language pairs pose very different translation challenges. For example, Chinese-to-English translation requires much greater word movement than Arabic-to-English. In the frame-work of hierarchical translation systems, we have found that shallow decoding (see section 3.2) is as good as full hierarchical decoding in Arabic-to-English (Iglesias et al., 2009). In Chinese-to-English, we have not found this to be the case. Therefore, we contrast the performance of HiFST and HCP under shallow hierarchical decoding for Arabic-to-English, while for Chinese-to-English we perform full hierarchical decoding.

Both hierarchical translation systems share a common architecture. For both language pairs, alignments are generated over the parallel data. The following features are extracted and used in trans-lation: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the par-allel corpus and development sets used for each lan-guage pair are given in their respective section.
Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chi-ang (2007). For the HiFST system, we obtain a k-best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps.  X  Large-LM rescoring . We build sentence- X  Minimum Bayes Risk (MBR) . We rescore the 3.1 Building the Rule Sets We extract hierarchical phrases from word align-ments, applying the same restrictions as introduced by Chiang (2005). Additionally, following Iglesias et al. (2009) we carry out two rule filtering strate-gies:  X  we exclude rules with two non-terminals with  X  we consider only the 20 most frequent transla-
For each development set, this produces approx-imately 4.3M rules in Arabic-to-English and 2.0M rules in Chinese-to-English. 3.2 Arabic-to-English Translation We translate Arabic-to-English with shallow hierar-chical decoding, i.e. only phrases are allowed to be substituted into non-terminals. The rules used in this case are, in addition to the glue rules:
For translation model training, we use all allowed parallel corpora in the NIST MT08 Arabic track (  X  150M words per language). In addition to the MT08 set itself, we use a development set mt02-05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test . The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best lists of depth k=10000 (unique). Using deeper lists results in excessive memory and time requirements. In contrast, the WFST-based decoder, HiFST, re-quires no local pruning during lattice construction for this task and the language model is not applied until the lattice is fully built at the upper-most cell of the CYK grid.

Table 1 shows results for mt02-05-tune , mt02-05-test and mt08 , as measured by lowercased IBM BLEU and TER (Snover et al., 2006). MET param-eters are optimized for the HCP decoder. As shown in rows  X  X  X  and  X  X  X , results after MET are compara-ble.

Search Errors Since both decoders use exactly the same features, we can measure their search errors on a sentence-by-sentence basis. A search error is as-signed to one of the decoders if the other has found a hypothesis with lower cost. For mt02-05-tune , we find that in 18.5% of the sentences HiFST finds a hy-pothesis with lower cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any sentence. This is as expected: the HiFST decoder requires no pruning prior to applying the language model, so search is exact.

Lattice/k-best Quality Rescoring results are dif-ferent for cube pruning and WFST-based decoders. Whereas HCP improves by 0.9 BLEU, HiFST im-proves over 1.5 BLEU. Clearly, search errors in HCP not only affect the 1-best output but also the quality of the resulting k-best lists. For HCP, this limits the possible gain from subsequent rescoring steps such as large LMs and MBR.

Translation Speed HCP requires an average of 1.1 seconds per input word. HiFST cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves much more efficient to process com-pact lattices contaning many hypotheses rather than to independently processing each one of them in k-best form.
The mixed case NIST BLEU-4 for the HiFST sys-tem on mt08 is 42.9. This is directly comparable to the official MT08 Constrained Training Track eval-3.3 Chinese-to-English Translation We translate Chinese-to-English with full hierarchi-cal decoding, i.e. hierarchical rules are allowed to be substituted into non-terminals. We consider a maxi-mum span of 10 words for the application of hierar-chical rules and only glue rules are allowed at upper levels of the CYK grid.

For translation model training, we use all avail-250M words per language. In addition to the MT08 set itself, we use a development set tune-nw and a validation set test-nw . These contain a mix of the newswire portions of MT02 through MT05 and additional developments sets created by translation within the GALE program. The tune-nw set has 1,755 sentences.

Again, the HCP decoder employs k-best lists of depth k=10000. The HiFST decoder applies prun-ing in search as described in Section 2.4, so that any lattice in the CYK grid is pruned if it covers at least 3 source words and contains more than 10k states. The likelihood pruning threshold relative to the best path in the lattice is 9. This is a very broad threshold so that very few paths are discarded.
Improved Optimization Table 2 shows results for tune-nw , test-nw and mt08 , as measured by lower-cased IBM BLEU and TER. The first two rows show results for HCP when using MET parameters opti-mized over k-best lists produced by HCP (row  X  X  X ) and by HiFST (row  X  X  X ). We find that using the k-best list obtained by the HiFST decoder yields bet-ter parameters during optimization. Tuning on the HiFST k-best lists improves the HCP BLEU score, as well. We find consistent improvements in BLEU; TER also improves overall, although less consis-tently.

Search Errors Measured over the tune-nw devel-opment set, HiFST finds a hypothesis with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis with a lower cost for any sentence, indicating that the described pruning strat-egy for HiFST is much broader than that of HCP. Note that HCP search errors are more frequent for this language pair. This is due to the larger search space required in fully hierarchical translation; the larger the search space, the more search errors will be produced by the cube pruning k-best implemen-tation.
 Lattice/k-best Quality The lattices produced by HiFST yield greater gains in LM rescoring than the k-best lists produced by HCP. Including the subse-quent MBR rescoring, translation improves as much as 1.2 BLEU, compared to 0.7 BLEU with HCP. The mixed case NIST BLEU-4 for the HiFST sys-tem on mt08 is 27.8, comparable to official results in the UnConstrained Training Track of the NIST 2008 evaluation. The lattice-based decoder for hierarchical phrase-based translation described in this paper can be eas-ily implemented using Weighted Finite State Trans-ducers. We find many benefits in this approach to translation. From a practical perspective, the computational operations required can be easily car-ried out using standard operations already imple-mented in general purpose libraries. From a model-ing perspective, the compact representation of mul-tiple translation hypotheses in lattice form requires less pruning in hierarchical search. The result is fewer search errors and reduced overall memory use relative to cube pruning over k-best lists. We also find improved performance of subsequent rescor-ing procedures which rely on the translation scores. In direct comparison to k-best lists generated un-der cube pruning, we find that MET parameter opti-mization, rescoring with large language models, and MBR decoding, are all improved when applied to translations generated by the lattice-based hierarchi-cal decoder.
 This work was supported in part by the GALE pro-gram of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022. G. Iglesias supported by Spanish Government research grant BES-2007-15956 (project TEC2006-13694-C03-03).

