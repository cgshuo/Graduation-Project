 There has been recent interest in the design of statistical approaches for Korean mor-phological analysis. In a statistical approach, a morphological analysis model is de-rived mainly from the training corpus, without (or weakly) relying on dictionary or linguistic knowledge [Lee and Rim 2005; 2009; Lee 2011]. However, previous stud-ies have been based on generative models such as HMM without the use of various features, and these proposed models were applied to each eojeol , or Korean word, sepa-rately and not directly to a full sentence, thus requiring an additional model for appli-cation to full sentences. Discriminative models such as CRFs, which are widely used for many types of NLP problems, are clearly advantageous, as they provide a simple way of combining arbitrary overlapping and non-independent features, which are not easily (or cannot be) modeled in generative models. It is therefore of arguable interest to apply discriminative models for statistical Korean morphological analysis. In this article, we present a simple but novel two-stage approach using CRFs for Korean morphological analysis and POS tagging. Similar to methods used for Chinese, the proposed method mainly consists of morpheme segmentation and POS tagging steps. Our method is further augmented with an additional post-processing step for decomposing a compound morpheme. To the best of our knowledge, our work is the first to use compound morphemes in the discriminative approach to Korean morphological analysis. Experimental results consistently demonstrate the promise of our proposed method, which provides comparable performance to other state-of-the-art methods.
The remainder of this article is organized as follows. Section 2 describes other work related to this area. Section 3 describes the proposed method and the corpus process-ing method. Section 4 provides the experimental results, and finally, some concluding remarks and a description of future work are given in Section 5. Traditional studies on Korean morphological analysis have been based mostly on pars-ing methods, which include tabular parsing methods such as a CYK algorithm [Kim 1987; Kwon et al. 1999], two-level morphology [Kim et al. 1994; Koskenniemi 1983], and syllable-based analysis [Kang and Kim 1994]. Some studies have reduced the time complexity of morphological analysis by decreasing the frequency of dictionary lookups, which include the bidirectional longest match strategy [Choi and Lee 1993] and the use of partially pre-analyzed results [Yang and Kim 2000; Shim and Yang 2002]. Other works have handled the over-generation problem by filtering out implau-sible results extensively through linguistic knowledge, such as exclusive information [Lim et al. 1995] and the subsumption relation of morpheme structures [Kim et al. 1995]. To handle an unknown morpheme problem, Lee et al. [2002] proposed a syllable-based prediction model for unknown morphemes and suggested hybrid methods of integrating this model with a backbone HMM model. However, most early studies are not statistical approaches in the sense that their main components of morphological analysis necessarily rely on dictionary and linguistic knowledge.

Recent interest in designing statistical approaches for Korean morphological analysis has focused on  X  X tatistical methods X  where a morphological analysis model is derived mainly from a corpus without (or weakly) relying on any dictionary or linguis-tic knowledge. In Lee and Rim [2009], three processing models for Korean morphologi-cal analysis are proposed: lemmatization, morpheme segmentation, and POS tagging. First, a lemmatization model, called a syllable recovery model , is first applied to re-cover a lemma form from the surface form of a given eojeol . The recovered lemma form is then segmented and tagged using an HMM-based tagging model (either a morpheme unit or syllable unit ). A tagging model is further combined with the eojeol-unit model , which directly uses the pre-analyzed eojeol results. Instead of using the joint model in Lee and Rim [2009], Lee [2011] further extends the work in Lee and Rim [2009] by separately performing lemmatization from segmentation. For lemmatization, the author uses the transliteration-style model of Lee [2007], which is technically an advancement of the syllable recovery model in Lee and Rim [2009].

Some studies on applying discriminative approaches to analyzing Korean sentences have been recently conducted; however, most works have not focused on the full process of Korean morphological analysis and have rather been limited to the au-tomatic word-spacing problem [Lee and Kim 2013] based on the Pegasus algorithm [Shalev-Shwartz et al. 2007; Lee and Jang 2009] and the syntactic tagging of eojeols [Hong 2008]. Among existing works, two exceptions are the most similar to ours. One is the work in Kudo [2006], which applied Mecab, an open-source CRF toolkit, to Korean morphological analysis; however, no empirical evaluations or comparisons have been performed. Since Mecab is based on the Kudo framework [Kudo et al. 2004], the use of Mecab relies on a lexicon and unknown word processing, whereas our method does not use a lexicon or additional processing, and all models are derived from the POS-tagged corpus.

Another exception is the work in Shim [2011], which proposed syllable-based tagging using CRF. The work is largely similar to ours, as both use a discriminative approach like CRF for the full process of Korean morphological analysis. However, our work is different in that it handles the morphological variation between lemma form and sur-face from of a morpheme; that is, our method can be seen as a morpheme-level approach based on a compound morpheme which consists of two consequent morphemes, while Shim X  X  [2011] work belongs to a syllable-level approach based on a compound syllable which consists of two consequent syllables.

Extensive studies on applying discriminative approaches to the segmentation and tagging problems have been conducted for other languages. Kudo et al. [2004] first ap-plied CRFs to Japanese morphological analysis in which a lattice is first constructed using a lexicon. A Viterbi path over the lattice is then provided using CRF models, thereby joining segmentation and tagging into a single model, which is similar to the Semi-CRF method of Sarawagi and Cohen [2004]. Differing from Semi-CRF, however, their method relies on both a lexicon and additional unknown word processing, which are often unavailable when designing a statistical method. Neubig et al. [2011] use a pointwise approach for both word segmentation and POS tagging without the use of a sequential structure. For a pointwise approach, unlike in sequential tagging, classifi-cation is performed separately for each input word or character. In Chinese, two-stage approaches X  X ord segmentation and POS tagging X  X re the most popular [Peng et al. 2004; Xue 2003]. In Chinese POS tagging, Ng and Low [2004] demonstrate that a character-based tagger performs better than a word-based tagger.

Unlike Chinese, Korean has the unique feature of lemmatization, that is, the sur-face forms of Korean morphemes often differ from their lemma forms. To address the lemmatization issue, we preprocess an original POS-tagged corpus based on compound morphemes such that the lemma forms of all morphemes are the same as their surface forms. The use of compound morphemes enables us to apply a procedure similar to the ones used for Chinese in analyzing Korean sentences.

The idea of using compound morphemes was originally proposed in our previous paper [Na et al. 2012], which is written in Korean. This article is an extension of our previous work and contains detailed experiments and error analyses. Progress has been made using our morpheme-level approach, based on compound morphemes by Lee [2013], whose work is also written in Korean. Lee [2013] proposed a joint model of three Korean processing steps X  X ord spacing, morpheme segmentation, and POS tagging X  X n the single tagging stage using a richer set of features than ours and a structural SVM. Even in a nonspaced input sentence, his approach demonstrates state-of-the-art performance without performance loss when compared with word-spaced sentences. Our proposed method consists of the following three processing steps. (1) Morpheme segmentation based on CRF. (2) POS tagging based on CRF. (3) Post-processing compound morphemes based on either pre-analyzed patterns or a
Figure 1 describes the three processing steps used in the proposed method. First, in morpheme segmentation , we segment an input sentence into morphemes, where a morpheme unit is either atomic or compound , which will be defined in the next section. Second, in POS tagging , a POS tag is assigned to each morpheme. As such, the first two steps are exactly reminiscent of the word segmentation and tagging deployed in previous studies analyzing Chinese sentences. Third, in post-processing compound morpheme ,a compound morpheme , which consists of two or more atomic morphemes, is further decomposed into atomic morphemes . The third step relies on pre-analyzed patterns and a lattice HMM, both of which are constructed from the training corpus only. Morpheme segmentation produces two different types of morphemes: (1) atomic mor-phemes and (2) compound morphemes . A compound morpheme includes several atomic morphemes, taking consequent N syllables as its surface form.

Examples of atomic and compound morphemes include the following. (1) An atomic morpheme is the basic morpheme unit. (2) A compound morpheme is a morpheme consisting of two or more morphemes. The notation m / t is used for describing individual morphemes (e.g., ki/VV ), with mor-pheme m and POS tag t ,anda + notation is used to connect adjacent morphemes.
A detailed definition of a compound morpheme is presented in the next section.
Examples of Compound Morphemes. In the original POS-tagged corpus, a compound morpheme is not explicitly given, as an eojeol is usually annotated using atomic morphemes only. We therefore convert the original POS-tagged corpus for training a CRF model for morpheme segmentation by automatically extracting all compound morphemes.

Table I shows examples of the original POS-tagged corpus where compound mor-phemes are included. As shown in Table I, gass , kyeo , jul-lae ,and cheong-haet are compound morphemes, while their annotation includes the lemma forms of atomic morphemes only. In gass-da , the first syllable gass consists of two morphemes ga and at . Similarly, kyeo consists of two morphemes ki and wo , while jul-lae consists of two morphemes, ju and llae .

Definition of a Compound Tag. The remaining issue is how to define a compound tag , which is a tag for a compound morpheme. Formally, suppose that m is a compound morpheme consisting of k atomic morphemes, m 1 / t 1 ,  X  X  X  defined by k consequent tags t 1 ,  X  X  X  , t k . Three example schemes for defining a compound tag are given here. (1) t 1 : , only the first tag (e.g., haeng-hae-jin/VV ( haeng-ha/VV+wo/EC+ji/ (2) t 1  X  t k : , a combination of the first and last tags (e.g. haeng-hae-jin/VV-ETM ). (3) t 1 ,  X  X  X  , t k : , a combination of all component tags (e.g., haeng-hae-jin/VV
Among these three schemes, the first is simple but suffers from a large amount of ambiguity, and the set of tags of the third is too large, making learning inefficient.
In this article, we use a combination of the first and second schemes to give a com-pound tag, depending on whether the first morpheme, m 1 , is a verbal unit. That is, we use the second scheme if m 1 is a verbal unit and use the first scheme if it is not.
Automatically Extracting Compound Morphemes. To extract compound morphemes as previously defined, we convert the original POS-tagged corpus such that all of its morphemes are clearly segmented at the syllable level without the need to consider alphabet-level decomposition. To this end, given an eojeol , we apply the syllable-based alignment between its surface form and original lemma form if they are not the same. Herein, the lemma form of a word is defined as the combination of all lemma forms of the morphemes. For syllable-based alignment, we use a variant of the minimum-edit-distance alignment.

As an illustrative example, suppose that the original POS-tagged corpus is given as follows.

For the first eojeol, ak-swu-lul , its surface and lemma forms are the same, and there-fore alignment is not necessary. However, for the second eojeol, cheong-haet-da ,its lemma form is cheong-ha-at-da , which is not the same as its surface form. In this case, we apply a syllable-based alignment between cheong-haet-da and cheong-ha-at-da , which leads to the following alignment result. cheong-haet  X  cheong-ha-at da  X  da
Using this alignment result, we can extract cheong-haet as a compound morpheme, which consists of two morphemes: cheong-ha/VV+at/EP . In addition, as its first morpheme is a verbal unit, we therefore adopt the second scheme for determining its compound tag, which gives  X  X V  X  EP. X  As a result, the converted POS-tagged corpus is given as follows (the bold-faced entry indicates a compound morpheme).
 3.2.2. Morpheme Segmentation as Syllable Tagging. As is widely used in Chinese word segmentation, we use syllable tagging for morpheme segmentation. We use a BI tag scheme for tagging, where each syllable is tagged to either B or I. Herein, label B (begin) represents the first syllable of a morpheme, and label I (inside) indicates the syllable, with the exception of the first, that the morpheme is a part of, as defined in Roth and Yih [2005].

Table II summarizes the features used for morpheme segmentation. C cates a syllable located at a distance of i syllables after (before) the current syllable, and S i ( S  X  i ) indicates whether a space occurs just after C useful feature, as it is very likely that a morpheme is segmented immediately after a space. 3.2.3. POS Tagging. For POS tagging, we use a CRF-based method over the sequence of morphemes, which is widely used for other languages. For the features of POS tag-ging, we use the three surface forms of the previous morpheme ( W morpheme ( W 0 ), and the next morpheme ( W 1 ). Note that for a compound morpheme, its compound tag is used as the tagging unit for our POS tagging, and thus the number of output tags is much larger than the original number of given tags. In this step, we further decompose each compound morpheme into its atomic mor-phemes. The decomposition consists of two substeps: (1) using pre-analyzed patterns of the compound morphemes and (2) applying lattice HMM when a compound mor-pheme does not appear in the list of pre-analyzed patterns. In the following, we present further details of this approach. 3.3.1. Using Pre-analyzed Patterns Built while Extracting Compound Morphemes. The pre-analyzed patterns of compound morphemes are constructed during the corpus process-ing described in Section 3.2.1. Given a compound morpheme, all of its analysis results extracted from the training corpus (i.e., the sequences of its atomic morphemes and tags) are stored as its pre-analyzed patterns, for example, cheong-haet is extracted as a compound morpheme and one of its analysis results, cheong-ha/VV+at/EP , is added to the list of pre-analyzed patterns for cheong-haet .

Now, suppose that a compound morpheme is detected in a test sentence. We first check whether we have its pre-analyzed pattern. If a pre-analyzed pattern is avail-able, we simply present its stored analysis result as the final output. When we have one or more pre-analyzed patterns for a compound morpheme, we select the most frequent one. 3.3.2. Lattice HMM: Syllable Lattice as an Input. The use of pre-analyzed patterns enables us to present highly accurate analysis results. However, since the number of possible compound morphemes in the Korean language is too large, most compound morphemes will be unknown and do not appear in pre-analyzed patterns. To handle unknown compound morphemes, we further apply HMM-based tagging.

There is one issue in applying HMM-based tagging under our setting: a lemma form of an eojeol is not the same as its surface form, and thus various surface forms can be mapped to a single lemma form. However, the emission probabilities of HMM are usually trained based on lemma forms, not on surface forms. We therefore need to use the mapping between the surface forms and lemma forms before applying the emission probabilities.

To use lemma forms for HMM tagging, we apply lattice HMM , which takes a lattice as input, instead of a stream. To use a lattice HMM in our addressed problem, we first construct a syllable lattice for the surface form of a compound morpheme, where the edges are labeled with the lemma forms of the syllables. To obtain the lemma forms of the input syllables, we define a syllable mapping table between the surface and lemma forms (herein referred to as a syllable mapping table ), which stores all possible lemma forms for a syllable (or for a syllable sequence). Example entries of a syllable mapping table are { haet  X  ha-at } and { ge  X  geo-i , geos-i } (in addition, it is assumed that the original form of a syllable or syllable sequence can always be a lemma form). Using a syllable mapping table, we can convert an input sentence into a syllable lattice by creating an edge labeled with a lemma form of a syllable. In the following, we present the detailed procedure used in constructing a syllable lattice.

Construction of a Syllable Lattice. The construction of a syllable lattice from an input compound morpheme consists of two substeps: (1) construction of a base lattice and (2) extension of a base lattice , as presented in the following. (1) Construction of a Base Lattice . A base lattice is a graph resulting from directly (2) Extension of a Base Lattice .Let LT (  X  ) be a syllable mapping table, where LT ( (3) Example of a Lattice Construction . For a clearer understanding, an example of how
Decoding in a Syllable Lattice. To find the Viterbi path on an extended lattice, we use an extension of the decoding algorithm of hidden semi-Markov model (HSMM) [Yu 2010], which is straight-forwardly generalized to take a syllable lattice as an input. Owing to space limitation, the detailed algorithm is skipped herein.

Using Internal and Contextual Constraints on Lattice HMM. When applying HMM tagging on a syllable lattice, we have two types of constraints during decoding. First, the compound morpheme to be analyzed has a compound tag that provides the basic internal structure of the analysis. For a verbal morpheme, its compound tag consists of the first and last tags of the morpheme. Naturally, a constraint is given by putting the first and last tags of the HMM tagging result for the compound morpheme. We refer to a constraint from a compound tag as an internal constraint . Second, during the decoding step, we also have the surrounding context of a compound morpheme, the preceding and succeeding tags of which are available. We refer to a tag restriction of the surrounding context as a context constraint .

For example, take the input sentence  X  man-i haeng-hae-jin si-do  X  and its POS tag-ging result  X  man-i/MAG haeng-hae-jin/VV ETM si-do/NNG  X  (here, MAG, VV, ETM, and NNG indicate an adverb, verb, post-preposition, and noun, respectively), and sup-pose that haeng-hae-jin /VV ETM is a compound morpheme. In this example, the con-text constraint is that MAG and NNG are the preceding and succeeding tags of the compound morpheme, respectively. The internal constraint for the compound mor-pheme is that its first tag is VV and its last tag is ETM. As such, the tagging results are restricted to satisfy both the context and internal constraints. Once no tagging re-sults satisfying the constraints are available, we simply take the tagging result in the original Viterbi path without the use of any constraints. To evaluate the proposed CRF-based method, we use two different POS-tagged cor-pora, that is, SEJONG and ETRI. Table IV shows a summary of the basic statistics of both.

To learn the CRF models for the proposed method, we separate the training set from the test set for each tagged corpus, where we use 80% of the sentences for training and all remaining sentences for testing. A random splitting procedure is applied to select 80% of the sentences among the given corpora. All the parameters (i.e., feature weights for CRFs and transition/emission probabilities for lattice HMM) are estimated from the training set only.

For an evaluation, we basically use the F-measure at the morpheme level and the accuracy at the eojeol level. In the following, we provide the details of these evaluation measures.  X  F-measure (at the morpheme level) . This measurement is computed at the mor- X  Accuracy (at the eojeol level) . This metric indicates how many eojeols are cor-It should be noted that all the evaluation metrics, including Precision, Recall and Accuracy, are micro-averaged , where the numbers of both correctly and incorrectly matched ones are computed in all test sentences, not in each sentence separately. Tables V and VI show the experimental results of the proposed method at both the morpheme and eojeol levels.

When full-fledged components are used, the proposed method finally reports a 96.35% F-measure at the morpheme level, and 93.80% eojeol accuracy. The perfor-mances for ETRI are rather weak, showing a 91.39% F-measure at the morpheme level and an eojeol accuracy of 88.73%. 1 tion/Tagging. For further experiment, we also apply syllable-based POS tagging, moti-vated by the use of character-based POS tagging for Chinese word segmentation and POS tagging [Ng and Low 2004]. In syllable-based POS tagging, the output tag is ex-tended to B-t or I-t , where t is an arbitrary tag. Since B,I tags are still attached in an output tag, a morpheme is extracted through morpheme segmentation by referring to whether a given tag is a B-type or I-type. The resulting morpheme takes the output POS tag of the first syllable as its POS tag. As such, morpheme segmentation and POS tagging are performed jointly in syllable-based POS tagging.

Tables VII and VIII show the experimental results of syllable-based POS tagging compared with the results of a two-stage approach (i.e., the  X  X wo-stage X  row, which shows the results from the last row in Tables V and VI). As shown in Tables VII and VIII, syllable-based POS tagging is consistently better than in the two-stage method. For both SEJONG and ETRI, syllable-based tagging leads to more than a 1% abso-lute increase in eojeol accuracy. Overall, syllable-based POS tagging is better than a two-stage approach performing segmentation and morpheme-based POS tagging separately. 4.2.2. Error Analysis. Given the morphological analysis results, we manually check them and determine error types by referring to their corresponding results. Table IX shows some error cases and their corresponding error types. The bold face indicates syllable sequences that are incorrectly analyzed morphemes and their correspond-ing answers. Four different error types are reported: lemmatization error , compound morpheme segmentation error , tagging error ,and segmentation error .

In the lemmatization error case, the outputs produced by our method are correct in both segmentation and POS tagging; however, the lemma form of the verbal entries are incorrectly provided. To handle this lemmatization issue, we find it necessary to include the semantic structure of sentences beyond their syntactic information. An additional post-processing step, such as word sense disambiguation, could address this issue.

The second type of error is the compound morpheme segmentation error. This error primarily occurs in nominal compound morphemes because of the way we de-fine the compound tags for nouns. Unlike with verbal entries, the first tag is used only for nominal tags, without using other subsequent tags within them. Thus, the results suggest that our current definition of a compound tag needs to be extended to nominal ones.

Other types of error are found in segmentation and POS tagging, which are typi-cal in Korean morphological analysis. To handle these error types, it is necessary to use additional and contextual features or an improved machine learning method, as in Lee [2013].

During error analysis, we also find that the performance of our method is largely underestimated. Some segmentation errors turned out to be correct errors, although the results were different from our answers. Such cases are listed in Table IX and marked as the error type  X  X egmentation error (or correct). X  These types of errors are mostly found in compound nouns. In fact, the  X  X egmentation error (or correct) X  error type is one of the most frequent error types. A human evaluation would need to be included in our Korean morphological analysis to accurately estimate the performance of our method. 4.2.3. Comparison with Other Systems. Finally, we compare the proposed method with other systems. Table X shows the performance results of some existing systems carried on SEJONG, along with the final performance results of our method for SEJONG. our methods, the improvement of syllable-based POS tagging over word-based tagging is statistically significant at the 0.95 confidence level according to a nondirectional paired t-test.

Since we do not have our own implementations of these systems, we only provide the performance numbers herein. These numbers are therefore only meaningful as a reference and are not directly comparable, as the training/test sets of each system are somehow different (even when using the same type of dataset). Nevertheless, the proposed system shows comparable performances to other state-of-the-art methods, often achieving the best eojeol accuracy on the same type of dataset. In this article, we proposed a novel way to apply CRFs for Korean morpheme segmen-tation and POS tagging. Similar to methods used for Chinese, we cast the problem of Korean morphological analysis and POS tagging as two subsequent processes: a seg-mentation/tagging problem, which to the best of our knowledge is the first in the sense that segmentation/tagging is applied directly to the original input sentence without first performing lemmatization. The experimental results show that our discriminative approaches are promising, showing comparable performances to other state-of-the-art methods.

In the future, we would like to apply Semi-CRF for jointly modeling morpheme seg-mentation and POS tagging. It is also worth studying the use of phonetic features suggested in Shim and Yang [2002] under the setting of our discriminative framework and the use of pre-analyzed eojeol patterns [Yang and Kim 2000; Shin and Ock 2012].
