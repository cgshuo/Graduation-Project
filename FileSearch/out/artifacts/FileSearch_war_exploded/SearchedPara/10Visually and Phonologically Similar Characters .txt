 C.-L. LIU, M.-H. LAI, K.-W. TIEN, and Y.-H. CHUANG , National Chengchi University The studies about people using incorrect characters in Chinese words are related to the education, perception, recognition, and applications of the Chinese language 2 . Some Chinese words contain just one character, but most words comprise two or more characters. For instance,  X   X   X (hao3) 3 is a word that has just one character and means  X  X ood X  in English.  X   X  X   X  (yu3 yan2) is a word that is formed by two characters and means  X  X anguage X  in English. Experience indicates that the two most common causes for writing or typing incorrect Chinese words are due to phonological and visual similarity between the correct and the incorrect characters [Liu et al. 2009a, 2009b, 2009c]. For instance, one might use  X   X   X  (su4) in the place of  X   X   X (su4)in X   X  X  X   X (yan2 su4) because of the phonological similarity; one might use  X   X   X (shi1)for X   X   X  (lu3) in  X   X  X  X   X  (lu3 tu2) due to the visual similarity.

Manipulating the similarity between characters has served as an instrumental tech-nique in psycholinguistic studies into how people read and recognize Chinese charac-ters. Researchers in psycholinguistics investigate the cognition processes of Chinese readers [Kuo et al. 2004; Lee et al. 2006; Tsai et al. 2006], by measuring readers X  re-sponse times to words that have various numbers of  X  X eighbor X  words. The neighbors of a Chinese word include phonologically and visually similar characters.

Phonologically and visually similar characters are also useful for computer assisted language learning (CALL). In elementary schools in Taiwan, students may be re-quested to identify and correct  X  X rroneous words X  in test items, where, typically, an  X  X rroneous word X  contains an incorrect character that was introduced intentionally when teachers prepared the test items. Such tests are Incorrect Character Correction tests (ICC tests). It takes effort and time t o provide incorrect characters that are ap-propriate for different assessment purposes, and to make sure that the test items do not repeatedly use the same incorrect characters at the same time. We have built an environment for assisting the preparation of such test items [Liu et al. 2009a] by find-ing a way to offer phonologically and visually similar Chinese characters as candidates to serve as the incorrect characters [Liu and Lin 2008].

In addition, phonologically and visually similar characters can be applied to student modeling, optical character recognition (OCR), and information retrieval (IR) in Chi-nese. Bug libraries contain students X  reco rds of previous errors [Sison and Shimura 1998; Virvou et al. 2000], and are useful for modeling student behavior. Some algo-rithms for optical character recognition for printed Chinese and for written Chinese try to guess the input images based on confusion sets [Fan et al. 1995; Liu et al. 2004]. Characters in a confusion set are similar to each other visually, and they help the OCR programs to confine the search space for a given image. It would be possible to re-duce the computational costs and to increase recognition rates if we can pinpoint the confusion set of a character that is being recognized. The current confusion sets are hand-crafted clusters of visually similar c haracters. In recent years, it has become a common practice for IR service providers , such as Yahoo! and Google, to offer correc-tions when users enter queries that contai n incorrect words. For English queries, one may apply the Levenshtein distance to compute the edit distance between the spellings and employ the Soundex system to determine the degree of similarity between the pro-nunciations of words [cf., Croft et al. 2010; Manning et al. 2008]. These methods are not perfect but can catch similar English words in practice. The work reported in this article can be applied to find possible corrections for Chinese queries.

Some researchers state that there are more than 50,000 Chinese characters [HanDict 2010], although only thousands of characters are used in daily lives. In the People X  X  Republic of China, a government agency selected 7,000 popular Chinese characters and highlighted 3,500 characte rs among these 7,000 characters as the most frequently used characters in 1988 4 . In Taiwan, 5,401 characters were selected to be the most commonly used in daily lives in 1984 when the BIG5 code was formulated [Dict 2010].

Given that Chinese is used in different areas and in different countries in the world, it should not be surprising that not all people speak the  X  X tandard X  Mandarin. We will focus on the standards that are stated in specific lexicons in this research. Given a specific lexicon, it is relatively easy to judge whether two characters have the same or similar pronunciations based on their records, when we do not consider the phenomena of co-articulation. Although there are thousands of Chinese characters, these charac-ters are pronounced in only 420 different ways (cf., Lee [2009]). Interestingly, there are many fewer Chinese words that are pronounced exactly the same way than the number of Chinese characters that are pronounced exactly the same way. The problem of determining the pronunciation of Chinese characters becomes more complex if we consider tone sandhi [Chen 2000] in Chinese and if we consider the influences of sub-languages in the Chinese language family. We will discuss related issues in Section 2. In contrast, there were no obvious ways t o determine algorithmically whether two Chinese characters are visually similar yet. For instance,  X   X   X  (yuan2),  X   X   X  (yuan2), and  X   X   X  (xun1) are similar to each other in some ways, due to the presence of  X   X   X  (yuan2). Image processing techniques may be useful but are not perfectly practical, given the size of Chinese characters. A more important factor that affects the applica-bility of image processing methods is that many of the Chinese characters are similar to each other in subtle ways.  X   X   X  (yuan2) is contained in  X   X   X  (yuan2),  X   X   X  (yuan2), and  X   X   X  (xun1) in different sizes and at different positions.

We apply an extended version of the Cangjie codes [Cangjie 2010; Chu et al. 2010] to encode the layouts and details of traditional Chinese characters for computing vi-sually similar characters [Liu and Lin 2008; Liu et al. 2009a, 2009b, 2009c], and ex-tend the work to compare similar characters in simplified Chinese characters [Liu 2010]. Evidence observed in psycholinguistic studies [Feldman and Siok 1999; Lee et al. 2006; Yeh and Li 2002] offers a cognition-based support for the design of our ap-proach; namely, the use of shared components to define the visual similarity between Chinese characters.

The proposed method proves to be effectiv e in capturing incorrect words for both traditional [Liu et al. 2009a, 2009b, 2009c] and simplified Chinese [Liu 2010]. We col-lected and analyzed approximately 4,100 e rrors that were reported in published books, found in students X  compositions, or posted on the Internet. Each reported error is of a word which will be understood as a ppearing in its correct form as  X   X  X  X   X ; but which in the error may appear as  X   X  X   X , where  X   X   X  X susedinsteadof X   X   X . Namely, writing  X   X  X  X   X  X s X   X  X   X  is a reported error. We found th at 76% of the errors were related to phonological similarity and that 46% of the errors were related to visual similar-ity. More significantly, the dominance of the phonological factor was also observed in hand-written text, not just in electronic documents that were directly prepared on computers.

In experiments that aimed at reproducing t he collected errors, we ran our programs to select and recommend a list of candidates from more than 5,100 Chinese characters for the correct character, that is,  X   X   X , and we recorded the likelihood that the can-didate list actually included the incorrect character. Experimental results show that if the length of the candidate list is about 100, we achieved inclusion rates of about 97% for both traditional and simplified Chinese. If the length of the candidate list was shortened to 10, the average inclusion rates were 89% for the phonologically similar errors and 80% for the visually similar erro rs. We have also applied our algorithms for reproducing the reported e rrors to build an environment to assist teachers to prepare test items for ICC tests.

In this article, we integrate and extend the previous reports on the phonologically and visually similar characters in both traditional and simplified Chinese to capture errors in Chinese words. We go over some issues about phonological similarity in Chinese in Section 2, elaborate how we extend and apply the Cangjie codes to judge the visual similarity between Chinese characters in Section 3, explain how we acquired the reported errors and how we analyzed the phonological and visual influences on these errors in Section 4, present details about our experiments and discuss the observations in Section 5, show a real-world application of the proposed techniques to the authoring of test items for the ICC tests in Section 6, and review some of the design issues and experience in Section 7 before we summarize our work in Section 8.

Compared with the previous conference articles [Liu and Lin 2008; Liu et al. 2009a, 2009b, 2009c; Liu et al. 2010], we expanded the scale of experiments and discussions in terms of both depth and coverage. More specifically, we validated the reliability of the Web-based statistics by examining the data that we collected in 2009 and in 2010, compared the contribution of different sources of similar characters, explored the applications of alternative ranking methods, and exhibited the robustness of our approach by running our systems over new data sets. Chinese characters are single syllable. The pronunciation of a Chinese character in-volves the nucleus and a tone, where the nucleus contains a vowel that follows an optional consonant. In this article, we use the Hanyu pinyin method to denote the sound of Chinese characters, and show the tone with a digit that follows the symbol string for the sound. In Mandarin Chinese, there are four tones. (Some researchers include the fifth tone.)
Although Chinese is not an alphabetical language, it is shown that the pronun-ciations of characters affect how people write Chinese [Ziegler et al. 2000]]. The pronunciation of a Chinese character has two parts: sound and tone. Therefore, the phonological similarity between two characters may consider these two aspects, and we consider four categories of phonological similarity between two characters: s ame sound and s ame tone (SS), s ame sound and d ifferent tone (SD), si m ilar sound and s ame tone (MS), and si m ilar sound and d ifferent tone (MD).

We rely on the information provided in a lexicon [Dict 2010] to determine whether two characters have the same sound or the same tone. The judgment of whether two characters have a similar sound should consider the language experience of an individ-ual. An individual who lives in southern China and one who lives in northern China, for instance, might have quite different perceptions of similar sound. In this work, we resort to the confusion sets observed in a psycholinguistic study, conducted at the Academic Sinica in Taiwan, to obtain a list of confusion sets of vowels and consonants in Mandarin Chinese.

Some Chinese characters are heteronyms [cf., Fromkin et al. 2002]. Let C 1 and C 2 be two characters that have multiple pronunciations. If C 1 and C 2 share one of their pronunciations, we consider that C 1 and C 2 belong to the SS category. This principle applies when we consider phonological similarity in other categories.

With a lexicon and the list of confusion sets, our program can select a list of phonologically similar characters for a given character. Consider the example  X   X  X  X   X  (yan2 su4) that we mentioned in Section 1. We can find all of the characters that have exactly the same pronunciation with  X   X   X  (su4) based on the information provided by a lexicon. The SS list of  X   X   X  will include characters such as  X   X   X  (su4) and  X   X   X (su4).It is also easy to find the SD list for  X   X   X , and it includes  X   X   X (su1), X   X   X  (su2), and other characters. Based on the results of the psycholinguistic studies, we know that one might confuse the consonant /s/ with the consonant /sh/. Hence, the characters  X   X   X  ( sh u4) and  X   X   X ( sh u4) are in the MS list for  X   X   X ( s u4), and the characters  X   X   X ( sh u1),  X   X   X ( sh u3), and  X   X   X ( sh u3) are in the MD list for  X   X   X . Notice that the character  X   X   X  is in the MS and MD lists for  X   X   X  because it is a heteronym.
 Table I shows more pairs of the confusing phonemes that we used in our system. Note that phonological similarity is a symmetric relationship. Namely, when phoneme X is similar to phoneme Y , phoneme Y is similar to phoneme X . To help readers focus on the symbols of the phonemes, we underline the confusing phonemes of the example characters in boldface. Notice also that, although we do not explicitly provide examples in Table I, it is possible to change both the consonant and the vowel for a character to find a phonologically similar character. For instance,  X   X   X ( z ang1) is phonologically similar to  X   X   X ( zh eng1) because we can replace the consonant /zh/ and vowel /eng/ in  X   X   X  with /z/ and /ang/, respectively, and find  X   X   X .

One challenge in defining phonological similarity between characters is that a Chi-nese character may be pronounced in more than one way, and the actual pronunciation depends on the context. Tone sandhi [Chen 2000] is a frequently mentioned source of confusion. The most common example of the use of tone sandhi in Chinese is that the first third-tone character in words formed by two adjacent third-tone characters will be pronounced with the second tone. For example, although  X   X   X  (ni3) and  X   X   X  (hao3) are both third-tone characters,  X   X   X  X n X   X  X   X  is pronounced with the second tone in practice. Namely, native speakers usually pronounced  X   X  X   X  as ni2-hao3. At present, we ignore the influences of context when determining whether two characters are phonologically similar. (As we shall see in Section 5, doing so did not disturb the experimental results.)
Although we have confined our definition of phonological similarity to the context of the Mandarin Chinese, we would like to note that the influence of sublanguages within the Chinese language family will affect the perception of phonological similarity. Di-alects used in different areas in China, for example, Shanghai, Min, and Canton, share the same written forms with the Mandarin Chinese, but have quite different though related pronunciation systems. Hence, people living in different areas in China might perceive phonological similarity in different ways. The study in this direction, however, is beyond the scope of the current study. Figure 1 shows examples of visually similar Chinese characters. The first row contains five groups of visually similar traditional Chinese characters, and the second row con-tains five corresponding groups of simplified Chinese characters. The j th character (counted from left to right) in group ( i + 5) is the simplified form of the j th character in group i . Notice that the traditional and simplified forms of a character may be exactly the same.

The characters in group 1 differ subtly at t he stroke level, as do the characters in group 2. The characters in group 3 share the same components on their right sides. The shared components of the characters in group 4 and group 5 appear at different places within the characters.

Analogously, characters in group 6 differ subtly at the stoke level, as do the simpli-fied characters in group 7. Characters in group 8 share the components on their right sides. The shared components of the characters in group 9 and group 10 appear at different places with in the characters.

The radical of a Chinese character carries the main semantic information about the character [cf., Feldman et al. 1999], and lexicographers employ radicals to organize characters in Chinese dictionaries. Char acters that belong to the same radicals are placed in the same category, and are listed sequentially by the number of strokes. Hence, it is possible to employ the information about radicals to find visually similar characters. The characters in group 1 and group 2 have the radicals  X   X   X  (tian2) and  X   X   X  (yan2), respectively. Analogously, the simplified characters in group 6 and group 7 have the radicals  X   X   X  X nd X   X , respectively. ( X   X  is the simplified form of  X   X   X .) Notice that, although the radicals for group 2 and group 7 are obvious, those for group 1 and group 6 are not because  X   X   X  is not a standalone component in these groups.
Although radicals themselves provide information about the shared components of characters, the most saliently shared components of characters might not be the radicals of the characters. This problem occurs in both traditional and simplified Chinese. The shared component of the characters in group 3 is not the radical. The shared components of the characters in g roups 4, 8, and 9 are not the radicals for the characters in the groups either. In these cases, the shared components carry information about the pronunciations of the characters. Hence, those characters are listed under different radicals, though they do look similar in some ways.

In some cases, one may be interested in characters that share small elements in the characters, such as  X   X   X  (dai3) in group 5 and group 10. The shared elements in these two groups do not carry semantic or phonological information, and they are not the radicals either. It is also possible that a radical is written in different ways in the characters that have the same radical in a dictionary, for example,  X   X   X  (quan2) and  X   X   X  (bo2). These two characters are listed under the radical  X   X   X  (shui3). The radical appears literally in  X   X   X , but is written as  X   X   X  X n X   X   X .

Therefore, we cannot rely only on the information about radicals of characters in typical lexicons to find visually similar characters, and we will use the extended Cangjie codes as the basis to judge the degree of similarity between Chinese characters. The Cangjie input method is one of the most popular methods used for entering Chi-nese characters into computers. The designer of the Cangjie method selected a set of 24 basic elements occurring in characters, and proposed a set of rules to decompose Chinese characters according to these elements [Chu et al. 2010]. Because the Cangjie system is designed to help people enter Chinese characters into computers, the design of the Cangjie codes had aimed at allowing its users to recall the codes for Chinese characters as easy as possible. Namely, users must be able to easily figure out the Cangjie codes for the characters that they want to enter. Given the popularity of the Cangjije input method in a wide range of Chinese speaking communities, the Cangjie codes of Chinese characters have practically shown their strong links with the forma-tion of Chinese characters. This was an important motivation for us to try to define the similarity between two Chinese characters based on the degree of similarity between their Cangjie codes.
 Table II shows the Cangjie codes for the 1 3 characters listed in groups 1 to 4 in Figure 1 and for five other characters. The  X  X D X  column shows the identification num-ber for the characters, and we will refer to the i th character by c i ,where i is the ID. The  X  X C X  column shows the Chinese characters, and the  X  X angjie X  column shows the Cangjie codes. Each symbol in the Cangjie codes corresponds to a key on the keyboard, for example,  X   X   X  (tian2) and  X   X   X  (zhong1) collocate with  X  X  X  and  X  X  X , respectively. Information about the complete correspondence is available in Wikipedia 5 .
Using the Cangjie codes saves us from the need to apply image processing methods to determine the degrees of similarity between characters. Take the Cangjie codes for the characters in group 2 (c 5 ,c 6 ,andc 7 ) for example. See that the characters share a common component based on the shared substrings of the Cangjie codes (shown in boldface), that is,  X   X  X   X  (bu3 kou3). We may also find the shared component  X   X   X  (gou4, encoded by  X   X  X  X   X ) for the characters in group 3 (c 10 ,c 11 ,andc 12 ), the shared component  X   X   X  (li4, encoded by  X   X  X   X ) in c 15 and c 16 , and the shared component  X   X   X  (jing1, encoded by  X   X  X   X ) in c 16 and c 17 .

However, the original Cangjie codes are still lacking in some respects, in spite of their perceivable advantages. The Cangjie codes have been limited to contain no more than five keys, in order to maintain efficiency in inputting Chinese characters. Thus, users of the Cangjie input method must familiarize themselves with the principles for simplifying the Cangjie codes. While the simplified codes help to enhance the input ef-ficiency, they also introduce difficulties and ambiguities when we compare the original Cangjie codes for computing similar characters. The shared component  X   X   X  (yuan2) is encoded in three different ways in c 13 ,c 14 ,andc 15 , i.e.,  X   X  X  X  X   X  (kao2 yue4 shan1 jin1),  X   X  X  X   X , and  X   X  X   X . The prefix  X   X  X   X  X nc 16 and c 17 can represent  X   X   X (jing1),  X   X   X  (zheng4; e.g., in c 8 ), and  X   X   X (er4;e.g.,inc 9 ). Consequently, characters whose Cangjie codes include  X   X  X   X  may contain any of these three components, but c 8 ,c 9 , and c 16 do not really look alike. Not surprisingly, the Cangjie codes are also useful for capturing the similarities be-tween simplified Chinese characters. Using a structure similar to Table I, Table III shows the Cangjie codes for the characters listed in groups 6 to 9 in Figure 1 and five other characters.

Again, the Cangjie codes offer the possibility to determine the degrees of similarity between characters efficiently. It is possible to find that the characters c 23 ,c 24 ,and c 25 share a common component because their Cangjie codes share  X   X  X   X  (ge1 nu3). Using the common substrings (shown in boldface) of the Cangjie codes, we may also find the shared component  X   X   X  (gou1, encoded by  X   X  X   X ) for the characters in group 8(c 28 ,c 29 ,andc 30 ), the shared component  X   X   X  (yuan2, encoded by  X   X  X  X   X ) in c 31 and c 32 , the shared component  X   X   X  (li4, encoded by  X   X  X   X ) in c 33 and c 34 , and the shared component  X   X (jing1,encodedby X   X  X   X ) in c 34 and c 35 .

Similar to the problem of using the original Cangjie codes for traditional Chinese, we would encounter ambiguity problems when comparing the similarities between simplified Chinese characters. The shared component  X   X   X  X nc 32 and c 33 is encoded by  X   X  X  X   X  (kao3 yue4 ren2) and  X   X  X   X , respectively. The prefix  X   X  X   X (gong1yi1)inc 34 and c 35 can represent  X   X ,  X   X   X (yu2;e.g.,inc 26 ), and  X   X   X  (ma3; e.g., in c 27 ). Characters whose Cangjie codes include  X   X  X   X  may contain any of these three components, but c 26 ,c 27 ,and c 34 do not really look alike.

Given the observations reported in the pre vious subsection and in this present one, we augmented the original Cangjie codes by using the complete Cangjie codes and annotated each Chinese character with a layout identification that encodes the overall contours of the characters. Figure 2 shows the 12 possible layouts that are considered for the Cangjie codes for both traditional and simplified Chinese characters. Most of the layouts contain two or three small regions (called subareas henceforth), and the rectangles show individual subareas within a character. The subareas are assigned IDs, but to maintain readabil-ity of the figures, not all of the IDs for subareas are shown in Figure 2. From left to right and from top to bottom, each layout is assigned an identification number from 1 to 12. An example pair of characters, separated by a slash, is provided below each lay-out. A traditional Chinese character is on the left, and a simplified one is on the right. For example, the layout ID of  X   X  /  X   X  X s8. X   X   X  (gu4) is a traditional Chinese character, and has two parts, that is,  X   X   X (wei2)and X   X   X (gu3). X   X   X  (guo2) is a simplified Chinese character and has two parts, that is,  X   X   X  X nd X   X   X (yu4).

When Chinese characters are transformed from the traditional to simplified forms, the layout of the same characters may or may not be changed, and a more compre-hensive discussion about the significant change in the structures of the characters is available in Lee [2010b]. Hence, we may and may not use the traditional and simplified forms of the same character as a pair in Figure 2. Except for layouts 6, 7, 8, and 10, the pairs of characters shown under the layouts are the same characters in both traditional and simplified forms. For instance,  X   X   X (xie4)and X   X   X (xie4)are examples of layout 4, and  X   X   X  is the simplified form of  X   X   X . In contrast,  X   X   X  X nd X   X   X  are two different characters, but both are examples of layout 8. The traditional form of  X   X   X  X s X   X   X , which belongs to layout 9.

Researchers have come up with other ways to decompose individual Chinese char-acters. A team at the Shanghai Jiao-Tong University (SJTU) report an early attempt, and they consider five major ways to decompose Chinese characters [p. 1071, SJ-TUD 1988]. In this study, the SJTU team report detailed analysis of the compositions of Chinese characters. Based on their analysis,  X   X   X  (kou3) and  X   X   X  (mu4) are the most frequent components in Chinese characters [p. 1027, SJTUD 1988]. Juang et al. [2005] employ four relationships for components of Chinese characters, and Sun et al. [2002] six relationships. The Chinese Document Laboratory at the Academia Sinica in Taiwan considers 13 possible ways to decompose Chinese characters [CDL 2010]. Lee [2010b] proposes more than 30 possible layouts. In Unicode standard 4.0.1, 12 operators are considered to build Chinese characters from a set of building blocks [UNICODE 2010].

The layout of a character affects how people perceive the visual similarity between characters [Yeh and Li 2002]. For instance, c 16 ( X   X   X , jing4) in Table II is more similar to c 17 ( X   X   X , jing3) than to c 18 ( X   X   X , jing1), because the shared component of c 16 and c 17 is on the same side of the words. Overall, c 16 ,c 17 ,andc 18 are more similar to each other than to  X   X   X  (jing1), although they share  X   X   X  (jing1). We follow the style by which Chinese characters are decomposed in the Cangjie system, and rely on the expertise in Cangjie codes reported in Lee [2010a] to divide a Chinese character into subareas, which we showed in Figure 2.
 Table IV shows the extended codes for some of the characters listed in Table I. The ID column provides links between the characters listed in both Table II and Table IV. The CC column repeats the Chinese characters. The LID column shows the identifi-cations for the layouts of the characters. The columns with headings P1, P2, and P3 show parts of the extended Cangjie codes, where P i shows the i th part of the Cangjie codes, as indicated in Figure 2.

LIDs are useful for comparing the degree of similarity between characters. Consider the case that we want to determine whether  X   X   X  (jing4) is more similar to  X   X   X (jing3) than it is similar to  X   X   X  (jing1). Their extended Cangjie codes indicate that  X   X   X  X s a better answer for two reasons. First, both  X   X   X  X nd X   X   X  are examples of layout 2; and, second, the shared components reside in the same subarea, that is, P1, in  X   X   X  and  X   X   X .

We decide the extended Cangjie codes for the individual parts with the help of com-puter programs and subjective judgments. Starting from the original Cangjie codes, we can compute the most frequent substring s among the original Cangjie codes for all characters. This process is similar to the one which can be used to compute the fre-quencies of n -grams in corpora [cf., Jurafsky and Martin 2009]. Computing the most frequently appearing substrings in the original codes is not a complex task because the longest original Cangjie codes contain just five symbols.

Often, the frequent substrings are simplified codes for common components in Chi-nese characters, for example,  X   X   X  (yan2) and  X   X   X  (jing1). The complete codes for  X   X   X  and  X   X   X  should be  X   X  X  X  X   X  (bu3 yi1 yi1 kou3) and  X   X  X  X  X  X   X  (yi1 nu2 nu2 nu2 yi1), but they are simplified to  X   X  X   X  X nd X   X  X   X , respectively, in the original Cangjie codes. When the Cangjie codes are simplified,  X   X   X  has the same code as  X   X   X (zheng4) and  X   X   X  (er4), as we have illustrated by c8, c9, and c16 in Table II. The simplified Cangjie codes for  X   X   X  are the same as the Cangjie codes of  X   X ,whichisintheupper part of  X   X   X (gao1).

After finding the frequent substrings, we verify whether these frequent substrings are simplified codes for meaningful components, which, in our definition, form parts of one or more Chinese characters. For meaningful components, we replace the sim-plified codes with their complete codes. For instance, the Cangjie codes for  X   X   X (xu3) and  X   X   X  (jie2) are extended to contain  X   X  X  X  X   X  in Table IV, where we indicate the extended keys that did not belong to the original Cangjie codes in boldface and with a surrounding box. After recovering the dropped codes for  X   X   X , our programs will have the information necessary to be able to tell  X   X   X  X nd X   X  X part.

Although we have tried to employ computer programs to help us find the frequent substrings in as many instances as we can , the work to recover the simplified codes remained labor-intensive, and we had to devote particular attention to certain anom-alous cases at times. Fortunately, the process to implement the extended Cangjie codes proved to be worthwhile as we will show in the experimental studies.

Using a structure that is similar to Table IV, Table V shows the extended Cangjie codes for some of the simplified Chinese characters that we show in Table III. The  X  X D X  column provides links between the characters listed in both Table III and Table V.

In Table V, we recover the Cangjie codes for  X   X  (yan2) and  X   X  (jing1). Using  X   X  X  X   X  (ge1 gong1 su3), rather than  X   X  X   X , for  X   X  prevents us from confusing  X   X  with  X   X   X  (yue4). Similarly, using  X   X  X  X   X  (gong1 ren2 yi1), rather than  X   X  X   X , for  X   X  avoids the confusion of  X   X   X  (ma3) and  X   X   X (yu2)with X   X , e.g., c26, c27, and c34 in Table III.

Replacing simplified codes with complete codes not only helps us avoid incorrect matches but also helps us find matches that would be missed due to the simplification of the Cangjie codes. If we only use the origin al Cangjie codes in Table III, it is not easy to determine that c36 ( X   X   X , jing1) in Table III shares the component  X   X (jing1) with c34 ( X   X   X , jing4) and c35 ( X   X   X , jing3). In contrast, there is a chance to find the similarity with the extended Cangjie codes in Table V, given that all of the three Cangjie codes include  X   X  X  X   X (gong1ren2yi1).

Although most of the examples provided in Table V indicate that we expanded only the first part of the Cangjie codes for the simplified Chinese, it is possible that the other parts, that is, P2 and P3, may need to be extended too. Sample c37 shows such an example. The main differences between the origina l and the extended Cangjie codes are the degrees of detail about the structures of the Chinese characters. By recovering the details that were ignored in the original codes, our programs will be better equipped to find the similarities between characters.

We experiment with three different scoring methods to measure the visual simi-larity between two characters based on their extended Cangjie codes. Two of these methods were tried in our studies for traditional Chinese characters [Liu et al. 2009b, 2009c]]. The first method, denoted SC1, considers the total number of matched keys in the matched parts. Two parts are considered as matched as long as their contents are the same. They do not have to locate at the region within a character. Let c i denote the i character listed in Table V. We have SC1(c 33 ,c 34 ) = 2 because of the matched  X   X  X   X  (da4 shi1). Analogously, we have SC1(c 37 ,c 34 ) = 2 because of the matched  X   X  X   X . No-tice that, although  X   X   X  (yi1) is in the P1 of c 34 and in the P2 of c 37 , it is not considered a match because the P1 of c 34 and the P2 of c 37 do not match as a whole.

The second method, denoted SC2, includes the score of SC1 and considers the following conditions: (1) add one point if the matched parts locate at the same place in the characters and (2) if the first condition is met, an extra point will be added if the characters belong to the same layout. Hence, we have SC2(c 33 ,c 34 )=SC1(c 33 ,c 34 )+ 1+1=4because(1)thematched X   X  X   X  is the P2 of both characters and (2) c 33 and c 34 belong to the same layout. Assuming that c 34 belongs to layout 5, than SC2(c 33 ,c 34 ) would become 3. In contrast, we have SC2(c 37 ,c 34 ) = 2. No extra points were added for  X   X  X   X  in the Cangjie codes for c 37 and c 34 because  X   X  X   X  is not at the same position in the characters. The extra points consider the spatial influences of the matched parts on the perception of similarity.

While splitting the extended Cangjie codes into parts allows us to tell that c 33 is more similar to c 34 than to c 37 , it also creates a new barrier in computing similarity scores. An example of this problem is that SC2(c 35 ,c 36 ) = 0. This is because that  X   X  X  X   X (gong1ren2yi1)atP1inc 35 can match neither  X   X  X   X  X tP2nor X   X   X  X tP3 in c 36 .

To alleviate this problem, we consider SC3 which computes the similarity in three steps. First, we concatenate the parts of a Cangjie code for a character. Then, we compute the longest common subsequence (LCS) (cf., Cormen et al. [2009]) of the con-catenated codes of the two characters being compared, and compute a Dice coefficient (cf., Croft et al. [2010]) as the similarity. The Dice coefficients are used in many appli-cations, including defining the strength of the relatedness of two terms (or similarity of two strings) in natural language processing (cf., Manning and Sch  X  utze [1999]). Let X and Y denote the concatenated, extended Cangjie codes for two characters, and let Z be the LCS of X and Y . The similarity is defined by the following equation.
We compute another Dice coefficient between X and Y . The formula is the similar to Equation (1), except that we set Z to the longest common consecutive subsequence. We call this score Dice LCCS .Noticethat Dice LCCS  X  Dice LCS , Dice LCCS  X  1, and Dice LCS  X  1. Usingboth Dice LCS and Dice LCCS allows us to compute the visual similarity from two aspects. Finally, the SC3 of two characters is the sum of their SC2, 10  X  Dice LCCS ,and5  X  Dice LCS . We multiply the Dice coefficients with constants to make them as influential as the SC2 component in SC3. Since the LCCSs of two strings are generally quite shorter than the LCSs, we multiply Dice LCCS with a larger weight. The constants were not scientifically chosen, but were selected heuristically.
Using the extended Cangjie codes and a selected score function, we can select a list of visually similar characters for a given character. Using SC3, we can find that every character in the string  X   X  X ﹦ X  X  X  X  X  X  X  X   X  (jing4 jing1 qing1 jing4 jing4 jing1 yun2 qing1) is similar to  X   X   X  (jing1) in some way. Interestingly, each character in the string  X   X  X ﹦ X  X  X  X  X  X  X  X   X  belongs to diffe rent radicals:  X   X  X  X  X  X  X  X  X  X  X   X  (chuo4 shui3 che1 chi4 chuang2 cao3 mi4 qi4). One may verify that not all of the characters in the string are listed under the same radical as  X   X   X , so our approach offers chances to find visually similar characters tha t belong to different radicals. We discuss the main functions of these measures qualitatively, before we compare their effectiveness experimentally in Section 5.

Consider the problem of finding a set of  X  X isually similar characters X  for a given character within a Chinese word. Except resorting to the remembrance of human experts, how can we find similar character s from thousands of characters? A more important question may be what we mean by  X  X imilar X  characters. Certainly, some characters can look similar individually. However, when putting into the contexts of words, some words can become more a ttractive than others. For instance,  X   X   X  (hao4),  X   X   X (hao4),and X   X   X  (zhi4) are almost equally similar to each other in some ways. Each pair differs in only in one of their components. Replacing the radical component  X   X   X (gao4)in X   X   X  with the component  X   X   X  (tai2) will create  X   X   X . However,  X   X  X   X  (hao4 da4), and  X   X  X   X  (zhi4 da4) are not equally attractive for the writing of  X   X  X   X  (hao4 da4). Psycholinguistic evidence has shown that humans do not read text letter by letter for alphabetic languages or character by character for languages such as Chinese (e.g., Jackendoff [1995]). The contexts matter in determining the similarities.

As a result, the  X  X est X  similarity measure for computer software depend on the goals of the applications. Do we want to build a model for how humans judge visually-similar characters? Do we want to build a model for how human process confusing words in which some characters are visually similar? In this article, the target application is more closely related to the latter question. In another application that we are building for learning Chinese characters [Liu et al. 2011], we are more concerned with similarity between individual characters. Hence, the similarity measures that we presented in the previous section were just to find  X  X ood candidates X , and we will have another measure to compare these first-round candidates.

This is the main reason that we did not report experiments in which we carefully tuned the weights for SC3. The main function of SC3 in the current study was to find good first-round candidates.

Changing the current weights for Dice LCS and Dice LCCS also changes the order in the recommended characters. We illustrate the results of using three different sets of weights below. We show the alternative formulas for SC3, and the resulting recom-mended lists for  X   X   X  (yun4) and  X   X   X  (juan1) below. From left to right, recommended characters are listed in the order of descending scores. We do not show the Hanyu pingyin symbols of all of the listed characters to avoid congesting the page with the symbols for Chinese pronunciations.

We can see that there are differences in the lists when we adopted different sets of weights in SC3. However, most, if not all, visually similar characters are included in the lists. Hence, treating these as the first-round candidates, we were satisfied with the weights that we selected. A more practical mechanism to rank these candidate characters in the context of words will be introduced in Section 4.4. Due to that ranking mechanism, the resulting performances of different weights will not differ significantly for the current application, as long as we have chosen a satisfactory set of weights.

When we focus on just finding just visually similar characters, there will be no contextual information, that is, the words, available to rank the characters. In such cases, the weights certainly matter. Song et al. [2008] discuss related issues when they build a system for Chinese spelling checker. We [Liu et al. 2011] also face a similar problem when we need software to find characters that find Chinese characters that contain specific components. We provide information about our lexicons, the sources from which we obtained the reported errors in Chinese text, and our analyses of these reported errors in this section. For both traditional and simplified Chinese, we prepare a lexicon that provides infor-mation on the pronunciation and a database t hat contains the extended Cangjie codes for the characters. Our programs rely on these databases to generate lists of characters that are phonologically and visually similar to a given character.

It is not difficult to acquire lexicons that contain information about standard pro-nunciations for Chinese characters. As we stated in Section 2, the main problem is that it is not easy to predict how people in different areas in China and Taiwan actu-ally pronounce the characters. In the current study we employ the standards for Man-darin Chinese that are recorded in the lexicons and published by the official agency in Taiwan 6 . Experimental results reported in Section 5 will show that the ethnic back-ground and mother tones did not influence the performance of our methods very much (atmost1%).

With the procedure reported in Section 3.3, we built databases of extended Cangjie codes for both the traditional and the simplified Chinese. Our database for the tradi-tional Chinese was designed to contain 5,401 common characters in the BIG5 encoding system (between 0xa440 and 0xc67e), which was originally designed for the traditional Chinese. We will call this list of characters TCdict. We converted the traditional Chi-nese characters to their simplified counterparts and built the database of Cangjie codes for the simplified Chinese. Because two different traditional Chinese characters may be transformed to a common simplified form, this simplified list contains only 5,170 different characters, and we call this list of characters SCdict.

Count from the very first day of the conception of the main ideas, it took us a long time to develop the current TCdict and SCdic t. The original idea was published in Liu and Lin [2008], but we continued to try different ideas since then. With the help of the software, that we explained in Section 3.3, to analyze the frequent substrings of the original Cangjie codes, two graduate students (the third and the fourth authors) were able to come up with a good version of the extended Cangjie code for the 5,401 traditional Chinese characters in a couple of weeks. That initial version was modified once in a while afterward. The modification operations were motivated by results of sporadic tests we ran with some data (Elist and Jlist, to be explained in Section 4.2), so we used some new data (Wlist and Blist, also to be explained in Section 4.2) to examine the performance of our system.

We employed our experience with the traditional Chinese to build the first and only version of the extended Cangjie codes for the simplified Chinese characters in few weeks. Most of the work was conducted only by the second author. We did not run ex-periments for the simplified Chinese while we are building the extended codes. There-fore, the experimental results that we report in Section 5.6 were not already based on new data. We acquired five lists of reported errors in Chinese at different stages of our study. By 2009, we collected two lists of errors for traditional Chinese, and in 2010, we added two lists of errors for traditional Chinese and a list of errors for simplified Chinese.

All of these lists contained information about the observed errors. In order to facilitate our experiments, we saved the reported errors in a simple format. An item of a reported error contains three parts: th e correct word, the correct character that will be replaced, and the actual incorrect ch aracter. For instance, the correct way to write a type of banana is  X   X  X   X  (ba1 jiao1) and sometimes people use  X   X   X  (ba1) for  X   X   X  (ba1). In this case, we will maintain a data item  X   X  X  ,  X  ,  X   X  for this error. At the beginning of our study, we acquired t wo lists of reported errors for traditional Chinese. The first list was obtained from a book published by the Ministry of Education (MOE) in Taiwan [MOE 1996]. The second list was collected in 2008 from the written essays of students of the seventh and the eighth grades in a middle school in Taipei. The errors were entered into computers based on students X  writings, not including those characters that did not actually exis t and could not be entered. We call the first list of errors the Elist, and the second the Jlist. Elist and Jlist contain, respectively, 1,490 and 1,718 items of errors.

Twoormoredifferentwaystowritethesa me words incorrectly were listed in dif-ferent items and considered as two items. When the same character of a word can be written incorrectly in multiple ways, for example, writing  X   X  X   X  (ying4 fu4) as  X   X  X   X  (ying4 fu4) or  X   X  X   X  (ying4 fu4) in Jlist, we considered them different errors. Cases like these make a program difficult to find th e best actual incorrect character, as we will see in Sections 5.5 and 5.6.

Repeated or semantically related erro rs were treated as many times as the er-rors were committed by writers. Writing  X   X  X  X  X   X  (bian4 de2 geng4 hao3) as  X   X  X  X  X   X  (bian4 de1 geng4 hao3) and writing  X   X  X  X  X   X  (bian4 de2 geng4 qiang2) as  X   X  X  X  X   X  (bian4 de1 geng4 qiang2) can be con sidered repeated errors. Writing  X   X  X  X  X   X  X s X   X  X  X  X   X  and writing  X   X  X  X  X  X   X  (zuo4 de2 bu2 cuo4) as  X   X  X  X  X   X  (zuo4 de1 bu2 cuo4) can be considered related errors in lexical semantics. (These errors were observed in Jlist.) These decisions helped us preserve the original distribution of the reported errors. That is, we took the test data as they were and did not try to manipulate or change the reported incorrect Chinese words. However, this also allowed a larger influence of the repeated errors on the reported experiment results.

In order to conduct further experiments, we collected two more lists of errors for traditional Chinese in 2010. The main reason for obtaining these lists was to use them as extra test data for our Cangjie codes that were improved during 2008 and 2009. Since we had access to both Elist and Jlist while we were improving the extended Cangjie codes for TCList, we thought it would be necessary to have new test data that we had never seen before to examine the effectiveness of the improved codes.
The new datasets were acquired from independent sources. The first new list was collected from the Internet, 7 and the second new list came from errors discussed in a published book that was compiled by scholars [Tsay and Tsay 2003]. The first and the second lists contain 199 and 487 incorrect words, and we refer to these lists as Wlist and Blist, respectively.

In order to test whether our approach works for capturing errors in simplified Chi-nese, we searched the Internet for reported errors for simplified Chinese, and obtained two lists of errors. The first list 8 came from the entrance examinations for senior high schools in China, and the second list 9 contained errors that were observed at senior high schools in China. We used 160 and 524 errors from the former and the latter lists, respectively. Both of these lists of errors were produced by students at the senior high school levels, so we combined them into one list and refer to the combined list as Ilist.

We dropped some of the reported errors in our experiments because of the current scope of study. Some of the reported errors involved characters that did not belong to TCdict (for traditional Chinese) or SCdict (for simplified Chinese). Since we have extended the Cangjie codes for characters that were included only in TCdict for tradi-tional Chinese and in SCdict for simplified Chinese, we ignored reported errors that did not occur in either TCdict or SCdict. This reduced the sizes of the lists that we collected. Table VI shows the sizes of the original and the reduced lists, respectively, under and the Original and Reduced columns. In order to know the main reasons that caused the production of the observed errors, we asked two native speakers to classify the causes of these errors into three categories based on whether the errors were related to phonological similarity, visual similarity, or neither. Since the annotators did not always agree on their classifications, the final results are presented in five categories: P, V, N, D, and B in Table VII. P and V in-dicate that the annotators agreed on the types of errors to be related to, respectively, phonological and visual similarity. N indicates that both annotators believed that the errors were not due to phonological or visual similarity. D indicates that the annota-tors believed that the errors we re due to phonological or visual similarity, but they did not have a consensus on the category. B indicates the intersection of P and V, that is, errors that are related to both phonological and visual similarities. Table VII shows the percentages of errors in these categories.

We used the quantities of reported errors in the reduced lists as the denominators to compute the percentages in Table VII. Hence, 79.9% in the  X  X list X  row indicates that 1,314 ( = 1645  X  0.799) errors were classified as related to phonological similarity. To get 100% for a row in the table, we need to add P, V, N, and D, and subtract B from the total.
In all of these five lists, phonological similarity showed a dominant influence in respect to the visual similarity of the repor ted errors. Most of the reported errors were related to similar pronunciations, while the percentage of errors that were related to visual similarity depended on the lists of the reported errors. It should not be very surprising that the annotators may disagree sometimes.

The weighted proportion of phonologi cally related errors is 76.0%. Based on the statistics shown in Table VI and Table VII, this analysis considered 4,172 errors (the total of the errors in the reduced lists). Th e total number of errors that were related to similar pronunciation is 1333  X  0.672 + 1645  X  0.799 + 188  X  0.691 + 385  X  0.816 + 621  X  0.831 = 3170.25. The result of dividing 3170.25 by 4172 is 76.0%. Similarly, we can compute that the weighted proportion of visually related errors is (1333  X  0.661 + 1645  X  0.307 + 188  X  0.548 + 385  X  0.348 + 621  X  0.483)  X  4172 = 46.1%.

It is particularly noticeable that although the errors in Jlist were collected from written documents, the phonological factor still dominated. It is a common belief that the dominance of pronunciation-related errors in electronic documents occurs as a re-sult of the common habit of entering Chinese with pronunciation-based methods. The ratio between P and V, that is, P  X  V, for the Jlist challenges this popular belief and indicates that even though the errors occu rred during a writing process, rather than typing on computers, students still produced more pronunciation-related errors. Dis-tribution over error types is not as related to input method as one may have believed. Nevertheless, the observation might still be a result of students in Taiwan being so used to entering Chinese text with a pronunciation-based method that the organiza-tion of their mental lexicons is also pronunciation related. The P  X  Vratioforthe Ilist also supports this phenomenon, suggesting that the dominance of phonological influence may be a common phenomenon in the use of both traditional and simplified Chinese. The ratio for the Elist suggests that editors of the MOE book may have cho-sen the examples with a special viewpoint in their minds X  X hat of balancing pronun-ciation and composition relat ed errors. (The Blist is so short that we do not consider it representative in regard to this issue.)
It is worthwhile to note that a large pe rcentage of errors are related to either phonological or visual similarity in Chinese. The sum of the statistics under N and D columns indicates the proportion of errors that were related to neither visual nor phonological similarity. The weighted average of (N + D) for the five lists was just 7%. The lowness of this figure can be explained by the large percentage of phono-semantic compounds (xingsheng words,  X   X  X  X   X ) in Chinese. In this section, we examine the effectiveness of using Web-based statistics to differen-tiate correct and incorrect characters. The abundance of text material on the Internet allows people to treat the Web as a corpus 10 . When we send a query to Google, we will be informed of the estimated number of pages 11 (ENOPs) that possibly contain rele-vant information. If we put the query terms in quotation marks, we should find the Web pages that replicate the query forms in the exact sequence and with the same ad-jacency as those originally entered. Hence, it is possible for us to compare the ENOPs for two competing phrases for guessing the correct way of writing a word. For instance, at the time of this writing, Google reported 116,000 and 33,000 relevant pages, respec-tively, for  X  X trong tea X  and  X  X owerful tea X . (When conducting such advanced searches with Google, the quotation marks are needed to ensure the adjacency of the individual words.) Hence, X  X trong X  X ppearstobeabetterchoicetogowith X  X ea X . Thisisanidea similar to one of the approaches for computing collocations based on word frequencies [cf., Manning and Sch  X  utze 1999]. Although the idea may not work very well when using a small database, the size of the current Web should be large enough.
We ran experiments for only those items that the annotators were in consensus over the causes of the error. Hence, for instance, we had 1285( = 1333  X  (1-0.036), cf. Table VI and Table VII), 1515 ( = 1645  X  (1-0.079)), and 598( = 621  X  (1-0.037)) such words for Elist, Jlist, and Ilist, respectively. As the information available on the Web may change over time, we also have to note that the statistics reported in Table VIII were based on experiments conducted during April 2010.

Table VIII shows the results of our investigation. For each reported error, we sub-mitted the correct word and the incorrect wo rd to Google and considered that we had a correct result when we found that the ENO P for the correct word was larger than the ENOP for the incorrect word. If the ENOPs were equal, we recorded an ambiguous result; and when the ENOP for the incorrect word was larger, we recorded an incorrect event. We use C, A, and I to denote correct, ambiguous, and incorrect events, respec-tively, in the table. We record a correct result for the  X  X trong tea vs. powerful tea X  test, for instance.

The Web-based statistics did not work very well for the Elist and Jlist, but seemed to work well enough for Ilist. The most common reason for the errors is that certain words are confusing to the extent that the majority of the Web pages showed the incorrect words. Some of the errors are so common that even one of the Chinese input methods on Windows XP offered wrong words as possible choices, for example,  X   X  X   X   X  (xiong2 jiu1 jiu1; the correct one) vs.  X   X  X  X   X  (xiong2 jiu1 jiu1). It is also interesting to note that people may intentionally use incorrect words on some occasions; for instance, people may choose to write homophones in advertisements.

Another possible reason for the mistakes is that whether a word is correct depends on a larger context. For instance,  X   X  X   X  (xiao3 si1) is more popular than  X   X  X   X (xiao3 si1) because the former is a popular nickname. Unless we provided more contextual information about the queried words, checking only the ENOPs of  X   X  X   X  X nd X   X  X   X  would lead us to choose  X   X  X   X , which would be an incorre ct word if we meant to find the right way to write  X   X  X   X . Other difficult pairs of words to distinguish are X   X  X  X   X  (ji4 lu4) vs.  X   X   X   X  (ji4 lu4) and  X   X  X   X  (xu1 yao4) vs.  X   X  X  X   X  (xu1 yao4).
Yet another reason for having a large E NOP for the incorrect words was due to errors in segmenting Chinese character strings (cf., Ma and Chen [2003]). Consider a correct character string  X  X XYZ X . It is possible that  X  X Y X  happens to be an incorrect way to write a correct word. This is the case for having the counts for  X   X  X  X  X  X   X  (hua1 hai3 bin1 fen1) to contribute to the count for  X   X  X  X   X , which is an incorrect form of  X   X  X  X   X  (hai3 bin1).

A reason why the Web-based statistics worked for Ilist is that all of the correct words in Ilist contained four characters. None of the factors that we list above for explaining the errors that we observed from Elist and Ilist may reasonably apply in the case of four-character strings. Hence, Web-based statistics worked almost perfectly for Ilist.

We compared the statistics reported in Table VIII and our reports in Liu et al. [2009c] and found that the effectiveness for Elist and Jlist improved greatly. To under-stand this phenomenon, we examined the records of our experimental results. Let x and y be the ENOPs of the correct and incorrect words, respectively. When we reported cases in which our systems failed to identify the correct character, that is, the I cases, in 2009, many of the ratios of y against x were within approximately 5%. Namely, we had ( y / x )  X  1 . 05in many cases, indicating that the margins were very small in 2009. The differences between the ENOPs may have changed between 2009 and 2010, so making us to achieve the better performance reported in Table VIII.

The improvement shows that the reliability of the Web-based statistics may be im-proving as the correct usage of words incre ase. This also shows that our approach might not work very well if the majority o f Web-page authors do not use the charac-ters in standard ways. To avoid the problem of using only the raw values of ENOPs to judge the correctness of words, we will introduce an alternative method in the next section. We evaluate the effectiveness of using the phonologically and visually similar charac-ters to capture errors in Chinese words in this section.

In Section 5.1, we provide details about th e procedures for the experiments, and, in Section 5.2, we explain the definitions of inclusion rates that we used to evaluate the basic performance of our systems. In Section 5.3, we offer a comparison of the statistics about our experiments that were conducted in 2009 and 2010, in order to gather information about the reliability of Web-based statistics, which is crucial for the success and applicability of our systems in the long run. In Section 5.4, we report the inclusion rates of our systems on the five sources of test data. The experiments also show the robustness of our methods and data for processing test data that were not reported in previous conference articles. In Sections 5.5 through 5.7, we deepen and widen our investigation of the effectiveness of our systems by applying an alternative method to rank the candidate characters and by recommending a limited number of candidates characters based on two ranking mechanisms. We designed and employed the ICCEval proc edure for the evaluation task. We needed two types of data for the experiments. The information about the pronunciation and structures of the Chinese characters (Section 4.1) helped us generate lists of similar characters. We also needed reported errors (Section 4.2) so that we could evaluate whether the similar characters catch the reported errors.

At step 1, we created a list of characters based on the selection criterion, given the correct word and the correct character to b e replaced. We may choose to evaluate the effectiveness of phonologically or visually similar characters. For a given correct char-acter, ICCEval can generate characters that are in the SS, SD, MS, and MD categories for phonologically similar characters (Section 2). For visually similar characters, IC-CEval can select characters based on different score functions, i.e., SC1, SC2, and SC3 (Section 3.4). In addition, ICCEval can gener ate a list of characters that belong to the same r adical and have the same number of s trokes with the correct character. In the experimental results, we refer to t his type of similar characters as RS.

At step 2, we checked whether the selected list of characters indeed contained the actual incorrect character.

At step 3, for a correct word that people shou ld write, we replaced the correct char-acter with a character from the candidate list that was generated at step 1, submitted the incorrect word to Google AJAX Search API (or directly to the Google interface 12 , and extracted the ENOP of pages that cont ained the incorrect words. In an ordinary interaction with Google, an ENOP can be retrieved from the search results, and it typically follows the string  X  X esults 1-10 of about X  in the browser window. Using the Google AJAX Search API, we have only to parse the returned results using a simple method.

Larger ENOPs for incorrect words suggest that these words are incorrect words that people frequently used on their Web pages. Hence, we could rank the similar characters based on their ENOPs at step 4 and return the list.

Since the reported errors contained inf ormation about the actual incorrect ways to write the correct words (Section 4.2), we could check whether the actual incorrect characters were among the similar charact ers that our programs generated at step 2 (inclusion tests). We could also check whether the actual incorrect characters were ranked higher in the ranked lists (ranking tests).

Take the word  X   X  X  X  X   X  (he2 ai3 ke3 qin1) as an example. In the collected data, it was reported that people wrote this word as  X   X  X  X  X   X  (he2 ai3 ke3 qin1), that is, the second character was incorrect. Hence the t est item (correct word, correct character, actual incorrect character) is ( X   X  X  X  X   X ,  X   X   X ,  X   X   X ). Hoping to capture the error, ICCEval generated a list of possible substitutions for  X   X   X  at step 1. Depending on the categories of sources of errors, ICCEval ge nerated a list of characters. When aiming to test the effectiveness of visually simila r characters, we could ask ICCEval to apply SC3 to selected a list of alternatives for  X   X   X  from SCdict, and the results may include  X   X   X  (ai3),  X   X   X (ye4), X   X   X  (ge3), and other candidates. At step 2, we found that the actual incorrect character was included in the candidate list. At step 3, we created and submitted the query strings  X   X  X  X  X   X ,  X   X  X  X  X   X , and  X   X  X  X  X   X  to obtain the ENOPs for the candidates. If the ENOPs were, respectively, 571,000, 445,000, and 8,580, these candidates would be returned in the order of  X   X   X ,  X   X   X , and  X   X   X . As a result, the returned list contained the actual incorrect character  X   X   X , and placed  X   X   X  at the top of the ranked list.

Notice that we considered the contexts in which the incorrect characters appeared to rank the candidate characters. We did not rank the incorrect characters with the unigrams such as  X   X   X ,  X   X   X , and  X   X   X  alone. Instead, the candidates were ranked with the ENOPs of  X   X  X  X  X   X ,  X   X  X  X  X   X , and  X   X  X  X  X   X .

In addition, although this running example shows that we ranked the characters di-rectly with the ENOPs, we also tried to rank the list of alternatives with the pointwise mutual information [PMI; cf., Jurafsky and Martin 2009]: where X is the candidate character to replace the correct character and C is the correct word excluding the correct character to b e replaced. To compute the score for the replacement of  X   X   X  X ith X   X   X  X n X   X  X  X  X   X , X = X   X   X , C = X   X   X   X  X   X , and ( C  X  X )is  X   X  X  X  X   X . ( denotes a character to be replaced.) We chose to try the frequency-based method and PMI-based method because both are used to compute the strength of collocation in natural langu age processing [Manning and Sch  X  utze 1999].
It would demand a considerable amount of computation effort to find Pr( C )ingen-eral, if this is a required task. Fortunately, we do not have to consider the effect of Pr( C ) because it is a common denominator for all possible incorrect characters for a given incorrect word. Let X 1 and X 2 be two competing incorrect characters for the correct character. We can ignore Pr( C ) because of the following relationship. Equation (4). In our work, we approximate the probabilities used in Equation (4) by the correspond-ing frequencies. Namely, we replace Pr( C  X  X ) with the Web-based counts for ( C  X  X ), for example,  X   X  X  X  X   X ; and substitute Pr( X ) with the Web-based counts for X ,for example,  X   X   X . The counts were obtained with exactly the same mechanism that we used to obtain the ENOPs that we explained at the beginning of this section. Recall that we used only those errors for which the annotators had reached consensus on the causes of the errors. The errors that in volved characters that were not in TCdict and not SCdict were not considered in the current study either.

Given the errors in the lists in Table VI, we ran ICCEval , and measured the perfor-mance in two ways. First, we would like to have the candidate list (step 1 in ICCEval) include the actual incorrect character in the inclusion test. Second, we would prefer that the actual incorrect character be placed at the top of the ranked list (step 4 in ICCEval).

Assume that there were n items of errors in a given list and that the candidate lists for these n errors contained m of the n actual incorrect character. Then, we compute the inclusion rates in the following manner.
In order to compare whether it is easier to capture either the phonologically similar or the visually similar errors, we separate the test instances according to the annotators X  consensuses. Hence, we will provide separate inclusion rates for two types of error. When reporting the inclusion rates for phonologically similar errors, we use the number of phonologically-similar errors in place of n , and when reporting the inclusion rates for visually-similar errors, we use the number of visually-similar errors in place of n .

The inclusion rates are similar to the re call rates that are commonly-used in the field of information retrieval (e.g., Croft et al. [2010]). Yet, they are different. The recall rates measure how well a search engine retr ieves the correct answers for a query. In our experiments, each test has only one answer, that is, the actual incorrect character. Hence, we measure the probability whereby we would capture the actual incorrect character across the reported errors.
 In addition, we prefer to put the actual incorrect character higher in the ranked list. Hence, we analyzed the accumulative inclusion rates of the top k characters in the can-didate list. Assume that there were n items of errors in a given list. Also assume that ICCEval placed the actual incorrect characters at the j th position t j times in the n experiments, where the first position is the best position. The ratio ( t j  X  n) is the proba-bility that the actual incorrect character was ranked at the j th position in an individual test. Then, we compute the accumulative inclusion rate (AIR) in the following way.
If ICCEval returns the candidate list compl etely, then it will achieve the inclusion rate. If ICCEval returns only the top k candidates, it will achieve the accumulative inclusion rates, which are upper-bounded by the inclusion rate. In April 2010, we repeated the experiments that we conducted in March 2009. The main purpose was to inspect whether we would achieve performance of similar quality with Web-based statistics in 2009 and 2010. The experiments had two goals: (1) to have the candidate lists (step 1 in ICCEval) i nclude the incorrect characters in the reported errors, and (2) to put the actual i ncorrect characters at top of the ranked results (step 4). We used Elist and Jlist in the experiments.
 Because we kept the lists of candidate characters that we generated at step 1 in IC-CEval in 2009, we were able to resubmit the incorrect words at step 3 and collected the ENOPs to rank the incorrect characters in 2010. In 2009, we submitted our queries directly through the Google interface 13 and we repeated the same procedure in the experiments in 2010 again. (Submitting a large amount of queries directly to the stan-dard Google interface resulted in Google considering our programs to be a malicious attacking agent, and our computers might be blocked. Hence, we have switched to the Google AJAX search API for the other new experiments.)
Since we reused the candidate lists, the inclusion rates did not change over time, so we show the inclusion rates in Table IX. We show the inclusion rates of the candidate lists that were generated with different criteria, that is, SS or SC1 for Elist and Jlist. We did not run SC3 for this experiment because we did not have this score function in 2009. Using SS to recommend candidate characters, we captured 91.6% of the errors that were related to pronunciation in Elist. Since in Table VII, 67.2% of the errors in Elist was related to phonological similarity, using the SS list alone captured (91.6  X  67.2)%, that is, 61.6%, of all of the errors. Using SC1 to recommend candidate characters, we captured 74.5% of the errors that were caused by visual similarity, and that is (74.5  X  66.1)%, that is, 49.2%, of all of the errors in Elist. The Phone column shows the inclusion rates when we used the union of the recommend lists created with SS, SD, MS, and MD criteria to guess the actual incorrect characters that were caused by phonological similarity. The Visual column shows the inclusion rates when we used the union of the candidate lists created with SC1, SC2, and RS criteria to guess the ac-tual incorrect characters that were caused by visual similarity. The All column shows the inclusion rates when we used the union of the candidate lists created with SS, SD, MS, MD, SC1, SC2, and RS criteria to guess all of the actual incorrect characters.
Apparently, it was much easier to capture errors that were related to the phono-logical similarity. All together, we were able to capture more than 95.5% of the nearly 2,978 observed errors. (cf., Table VI and Table IX; 95.5 = (93.4  X  1333 + 97.3  X  1645)  X  (1333 + 1645)) Recall that SS stands for  X  X ame sound and same tone. X  SS is the most effective criterion to use to select a candidat e list that can capture the observed errors. Even so, candidate lists selected with other criteria were necessary to achieve 99% inclusion rates for the phonologically related errors.

In contrast, using the extended Cangjie code that we constructed manually in 2009 did not perform comparatively well for visually-related errors, although achieving an inclusion rate of 80.4% in 2009 was very encouraging. It is worth mentioning that the RS category was able to capture 4.8% of the visually similar errors. (cf., Table VI, Table VII, and Table IX; 80.4 = (82.0  X  1333  X  0.661 + 77.6  X  1645  X  0.307)  X  (1333  X  0.661 + 1645  X  0.307). We used only those errors that were caused by visual similarity in this calculation.)
Table X shows the accumulative inclusion rates up to the 10 th -ranked candidates for the errors in Elist. If we subtract the results for 2009 from the corresponding numbers for 2010 in the table, we will see that there is no significant difference between the data shown in the upper and the lower part of Table X. We also reran the tests for Jlist, and did not observe any significant differences in statistics for the ranked lists in 2009 and 2010, either. Web-based statistics therefore appeared to be very stable, for the task of ranking the candidate incorrect characters. We have changed several aspects of our system since we conducted the experiments re-ported in the previous subsection. We have built a new version of the extended Cangjie codes for the characters in TCdict and the first version of the extended Cangjie codes for the characters in the SCdict, using the procedure that we discussed in Section 3.3. We have also added a new score function, that is, SC3, that we did not have in 2009. Furthermore, we do not submit our queries to the ordinary Google interface anymore, as we explained in Section 5.3.

To make the results of the experiments with traditional Chinese more convincing, we used two new lists, Wlist and Blist that we did not know when we improved the Cangjie codes in TCdict. These two lists serve as unforeseen test instances for our programs and databases.

We ran ICCEval with Elist, Jlist, Wlist, Blist, and Ilist. The experiments were conducted for all categories of phonological and visual similarity. When using SS, SD, MS, MD, and RS as the selection criteria, we did not limit the number of candidate characters. Any characters that conform to the selection criteria were selected in the candidate list, L in ICCEval. When using SC1, SC2, and SC3 as the selection criteria, we limited the number of candidates to be no more than 30. We inspected samples of the candidate lists generated with SC1, SC2, and SC3, and found that the number of visually similar characters for a given character rarely exceeded 30. Hence this limit was chosen heuristically.

We considered only words that the native speakers were in consensus over the causes of errors. There is a limit on the maximum number of queries that one can submit to the Google AJAX API. As a consequence, we could not complete our exper-iments in a short time, and the ENOPs were obtained during March and April 2010. Table XI shows the inclusion rates that the candidate lists, generated with different crit at step 1, contained the incorrect characters in the reported errors. The columns have the same meaning as they have in Table IX. The rows show the statistics that we observed while using the lists, listed in Table VI, as the test data. For instance, we achieved an inclusion rate of 90.3% for the visually similar errors when we applied SC3 to generate the candidate lists for errors in the Wlist.

ICCEval and our databases worked well for traditional Chinese. Although we have slightly expanded the definitions of similar sound since 2009, the effectiveness of SS, SD, MS, and MD remain the same for Elist and Jlist in Table IX and Table XI. The statistics for RS did not change because we were using the same list in 2009 and in 2010. Statistics about SC1 and SC2 are slightly better in Table XI for both Elist and Jlist, but the improvements are not significant. Using the new score function, that is, SC3, and the new Cangjie codes significantly improved the inclusion rates for visually related errors. We were able to include 88.3% 14 of the visually similar errors with SC3. We were able to include approximately 10% more of the actual incorrect characters in experiments, when we used SC3 rather than SC1 or SC2 to generate the candidate lists.

Even though we had not seen the errors in Wlist and Blist 15 previously, our program had shown a robust performance. ICCEval achieved a comparable performance when running with Wlist than running with Elist and Jlist. When working with Blist, IC-CEval did not perform as well with phonologically similar errors, but showed a similar performance for visually similar errors. However, the change was not very significant, and the results reflected the preference of the experts who wrote the book from which we obtained Blist. The effectiveness of the SS lists dropped, while the effectiveness of the SD lists increased. We inspected the errors in Blist closely, and found many challenging instances X  X hose that native speakers found the incorrect words are be-coming more frequently used in practice. For this reason, we considered that ICCEval achieved reasonably well.

When running with Ilist, ICCEval achieved a performance similar to the one which it had achieved with Blist. Like the results observed in the other experiments, it is easier to find phonologically similar incorrect characters than visually similar ones. Using SS and SC3 as the selection criterion at step 1 in ICCEval were the most ef-fective criteria for phonologically and visually similar characters, respectively. The contribution of SD was quite significant for Ilist.

When we used the unions of the phonologically similar characters to compute the inclusion rates, we captured 98.6% of the phonologically similar errors for the five lists. The unions of the visually similar characters were also very effective, though capturing only about 90.5% of the visually similar errors. When we used the union of all of the candidate lists, we captured 97.4% of all the errors. These are the weighted averages of the inclusion rates which we calculated wi th a procedure similar to the one provided in the tenth footnote.

It is certainly desirable for applications to have the potential to capture all of the reported errors. However the inclusion rate s were achieved at different costs. For each reported error and the actual incorrect character of the error, ICCEval generated a candidate list at step 1. In an experiment that used a list of y errors, we would have y candidate lists. Hence, for a particular e xperiment, we can calculate the average length of such y candidate lists. Table XII shows the average lengths of the corre-sponding experiments that are reported in Table XI.
Clearly, longer candidate lists would increase the chances to achieve higher inclu-sion rates. Hence, it would be more preferable if a shorter candidate list can achieve the same inclusion rate as that of a longer candidate list. From this perspective, the statistics in Table XII show that SS is very effective for capturing phonologically-similar errors, as we were able to capture better than 89.8% of the phonologically-similar errors by an average of 12.2 characters. (12.2 is the weighted average of 11.3, 12.4, 11.8, 14.2, and 12.6.) Taking the union of SS, SD, MS, and MD lists to obtain the  X  X hone X  list, the average lengths increased from 12.2 to 60.0, but the inclusion rates increased from 89.8% only to 98.5%.

Using SC3 as the selection criterion for vis ually similar errors offered a significant improvement in both effectiveness and efficiency. The weighted average lengths of the candidate lists that were selected with SC1, SC2, and SC3 were 22.8, 25.7, and 25.4, respectively. The weighted average inclusion rates for SC1, SC2, and SC3 were 77.6%, 73.6%, and 88.6%, respectively. Using SC3 allows us to achieve higher inclusion rates with shorter candidate lists. We took the union of the SC1, SC2, SC3, and RS lists to form the Visual list, increased the average lengths of the candidate lists to 47.4, but increased the inclusion rates only marginally to 90.9%.
 To achieve the inclusion rates in the All column in Table XI, we would have to allow ICCEval to recommend 104.3 characters. Although a list of 104 characters will be too long to be practically useful, we have to keep in mind that we had reduced the search space from more than 5,100 characters to approximately 100 characters X  X hich is 98% for the compression rate. This point is part icularly important to bear in mind as we seek to applying our findings to help teachers select  X  X ttractive incorrect characters X  when authoring test items of the ICC tests. To make the candidate lists applicable, we wish to place the actual incorrect character high in the ranked list. This will help the efficiency in supporting computer-assisted test-item writing. Having shorter lists that contain relatively more confusing charac-ters may facilitate the data preparation for psycholinguistic studies as well.
Table XIII shows the results when we recommended only the leading ten candidates for the errors in Jlist. The table is divided into two parts. The upper part (with row heading  X  X requency X ) shows the results when we used the raw values of the ENOPs to rank the candidate characters, and the lower part (with row heading  X  X MI X ) shows the results when we used Equation (4) in Section 5.1 to rank the candidate characters. The column  X  X  i  X  shows the accumulative inclusion rates (AIRs) that we defined in Equation (6) in Section 5.2. The sub-row headings show the selection criteria that were used in the experiments. For instance, using SS as the criterion and ranking with the raw values of ENOPs, 55.1% of phonologically related errors were included if we offered only one candidate, 70.6% of the phone-related errors were included if we offered two candidates, etc. If we recommended only the top five candidates in SS (ranked with ENOPs), we captured the phonologically similar errors 84.3% of the time. For errors that were related to visual simila rity, recommending the top five candidates in SC3 (ranked with PMIs) would capture the a ctual incorrect characters 73.9% of the time. As we explained in Section 5.2, the AIRs must be smaller or equal to the inclusion rates of the individual experiments. In T able XIII, we copy the inclusion rates of the Jlist row in Table XI into the R num column.

The statistics listed in Table XIII show the effectiveness of our ranking mecha-nisms  X  both ENOPs and also PMIs. The difference (R num -R i ) is a good indicator of the degree of sacrifice required in the situation when we recommend only the top i candidate rather than the complete candidate lists. When we shortened the candidate lists to contain less than 10 characters, we did not sacrifice the inclusion rates significantly. When we recommended 10 characters, the differences (R num -R 10 ) were not large, especially when we considered that we would have to put forward much longer lists of candidate characters, for example, Table XII, to achieve R num .One exception to this observation is that providing the complete candidate lists that were selected with the SS criterion may be worthwhile. According to Table XII, suggesting an average of 12.4 characters achieved R num .
 Recall that using the union of the candidate lists, such as Phone and Visual in Table XII, helped us to achieve higher inclusion rates. Although higher inclusion rates are desirable, the detailed statistics in the Phone and Visual sub-rows in Table XIII shed light on the drawbacks of the union lists. If we present only the top k candi-dates to those who need the similar characters of a given character, the union of the lists might not provide better performance profiles than that of the individual lists, separately.

It is not very difficult to understand the potentially inferior performances of the union lists. Assume that the rank of the actual incorrect character is j in a list, say LL . This implies that there already are at least ( j  X  1) characters that are mistakenly considered as better candidates by the score functions. After we put the lists together and rank the joined lists, these ( j  X  1) characters still win against the actual incorrect character. In addition, other characters that were not in LL might be ranked higher than the actual incorrect character in the union. As a consequence, the rank of the actual incorrect character might not improve in the union lists.

Consider, in one particular test, two ranked lists SS = { A , B } and SD = { C , D } , where A , B , C ,and D are four different characters. Hence, we must have score ( A )  X  score( B ) and score( C )  X  score ( D ). Assume that B is the actual incorrect character, that score ( C )  X  score( B ), and that score( B )  X  score( D ). The union of SS and SD will be { A , C , B , D } . The rank of the actual incorrect character will drop from 2 in SS to 3 in the union. This is a situation in which the joined list might not outperform the best individual list.

However, it remains possible that the joined lists perform better than the individual ones. This could happen when the actual incorrect characters were included in only one of the individual lists and when the ranks of t he actual incorrect characters remain the same in the joined lists. If, in the previous example, score( B ) is larger than score( C ), than the joined lists will perform as well as SS . Moreover, if, in another test, we have SS = { E , F } , SD = { G , H } ,where E , F , G ,and H are different characters, and where G is the actual incorrect character, than the joined list will perform better than SS .
Given the reasons provided in the previous two paragraphs, we cannot tell whether or not the joined lists will perform better than the best performing individual lists.
Figure 3 shows four pairs of examples for the experiments with Jlist. It happened that the joined lists did not perform as well as the best-performing individual lists in the joined lists. The charts were drawn based on the statistics listed in Table XIII. For instance, the curve  X  X req-SS X  in the chart with the title  X  X list-Freq X  was based on the data in the sub-row  X  X S X  in the  X  X requency X  part in Table XIII. The performance profiles of SS lists dominated those of Phone lists, and the performance profiles of SC3 lists dominated those of Visual lists in these cases. When we ranked the candidate characters with PMIs, the results were similar, and are shown in the chart titled  X  X list-PMI. X 
It is interesting to explore whether we may improve the performance of the Phone list by not considering the characters that were in the MS and MD lists. We conducted such an experiment, and in the middle of Table XIII, the row SS + SD shows the AIRs of the union list of words that were formed by using the candidate characters originally in the SS and SD lists. We can compare the performances of the Phone list and the SS + SD list. Overall, the SS + SD list provides better, but not significantly better, performance.

In addition to ranking the candidate characters directly with their ENOPs, we also ranked the characters with their PMIs, shown in Equation (2) and Equation (4) in Section 5.1, and repeated the experiments with Elist and Jlist. The lower part of Table XIII shows the observed statistics. Qualitatively, the statistics in the upper and the lower parts of Table XIII do not show different trends: SS and SC3 remain to be the most effective methods to find phonologically and visually similar errors. Overall, finding phonologically similar errors is easier than finding visually similar errors. Pro-viding candidate lists that had only 10 characters achieved reasonable performances.
The most noticeable difference between the upper and the lower part of Table XIII is in the R 1 column. It appears that, if we would recommend only one candidate char-acter, using the raw values of ENOPs to rank will offer better inclusion rates than using PMI. Although this observation appears to be appropriate for the statistics in Table XIII that we collected from the experiments that used Jlist, this trend did not survive in our experiments with Elist.

Table XIV shows exactly the same sets of statistics as those shown in Table XIII. The only difference is that we used Elist, rather than Jlist, to repeat all of the experiments that we used to obtain Table XIII. Statistics in Table XIV indicate the same trends as those suggested by the most of statistics in Table XIII, so we do not repeat the same statements.

A major contribution of the statistics in Table XIV appears in Figure 4, which shows that using PMIs or ENOPs did not guarantee that there would be differences in the performances. We drew the left chart based on the data in Table XIII and the right chart based on data in Table XIV. The curves in the left chart show that using PMIs offered inferior performance than using the raw values of ENOPs, and the curves in the right chart show the opposite trend.

Although PMIs are frequently used to compute the co-occurrences of two events, including the collocation of words, examining Formula (4) discussed in Section 5.1 reveals an intuitive interpretation of the PMIs in our applications. The formula mea-sures the percentages of observ ing the incorrect words (i.e., C  X  X ) given that the candidate characters appeared (i.e., X in the formula; recall that this is the charac-ter that would replace the correct charac ter). Such percentages can be diluted if X happens to represent a high frequent character.

Similar to an experiment that we conducted for the Jlist, we also created the union list SS + SD for the experiments with Elist, and the results are shown in the middle of Table XIV. This time, the SS + SD list outperformed the Phone list by a margin by about 5%. However, the resulting performance profile of SS + SD is still inferior to that of SS. Interestingly, when we consider the top 10 candidate characters, the SS + SD outperformed not only the Phone but also the SS list marginally. Table XV shows the accumulative inclusion rates (AIRs) for the experiments for Ilist, which provides reported errors for simplified Chinese. The inclusion rates for the ex-periments for Ilist were presented in Table XI.

We use a format that is similar to the format for Tables XIII and XIV for Table XV, but we list only the AIRs for the top-five candidates. When we used the top-five candi-date characters, the AIRs were almost as good as the inclusion rates, which we listed in Table XI. Statistics in Table XV indicate that our system performed well for the simplified Chinese. We can find results similar to those that we presented for the ex-perimental results for Elist and Jlist. The candidate lists selected with the SS and SC3 criteria were the most effective in capturing phonologically and visually related errors. SD continued to serve as an instrumental complement for SS.

In contrast to what we observed in the experiments for Elist and Jlist, the Phone lists, which are the unions of SS, SD, MS, and MD lists, performed better than the best performing individual lists, that is, SS lists. The top-five candidate characters in the Phone lists captured 95.5% of the phonolo gically related errors on average. Anal-ogously, the Visual list, which is the union of SC1, SC2, SC3, and RS, captured 89.3% of the visually related errors. We explained, in Section 4.2, that we used Wlist and Blist as the new datasets to test how our system will perform with unforeseen data.

Table XVI and Table XVII provided in the Appendix show that the experimental results for Wlist and Blist were not different from the results for Elist and Jlist, which we discussed in Section 5.5. The inclusion rates were good, as we discussed in Section 5.4. Using the top 10 candidate characters enabled us to catch most of the errors that we were able to capture with the complete lists. Using PMI and ENOPs to rank the candidate characters achieved performance profiles of similar quality. SS lists and SC3 lists performed best if we had to use only one of the lists to capture the phonologically and the visually related errors, respectively. In addition, SD lists complemented the SS lists to find those phonologically related errors.
 With the capability to capture the actual errors that occurred while people typed and wrote Chinese, we can apply our techniques to computer assisted language learning and to the related fields that we mentioned in Section 1.

The most obvious application is to help teachers prepare test items for  X  X n-correct Character Correction X  tests (ICC tests). In such tests, students have to find and correct an incorrect Chinese chara cter in a given sentence, for example,  X   X  X  X   X  X  X  X  X  X  X  X  X   X  (Ming took part in the field trip yesterday. Xiao3 ming2 zuo2 tian1 can1 jia1 le1 xiao4 wai4 shi1 xing1). In this Chinese string,  X   X   X (shi1)is incorrect and should be changed to  X   X   X  (lu3) to make the statement correct. This is a very common type of test in the assessment of Chinese language proficiency.
To prepare such test items, there may be some characters which teachers wish to check if their students recognize i n their correct form or not, that is,  X   X   X  X n X   X  X   X  (lu3 xing2) in the previous example. When preparing the test items, teachers will have to figure out what might be the most appropriate incorrect character to use to stand in for the correct character. Depending on the level of difficulty and the purpose of the test, they may prefer incorrect characters that are visually similar or are phonologically similar to the correct character. For phonologically similar characters, teachers may prefer to select incorrect ch aracters that were recommended with the SS, SD, MS, and MD criteria. From this viewpoint, we should present candidate characters by their categories. The union lists are not the best choice.

Figure 5 shows a snapshot of the user interface of our prototype 16 that aims to help teachers prepare test items for ICC tests. In this example, a teacher requested errors that were visually similar ( X   X  X  X  X  X   X  in the figure; xing2 ti3 xiang1 si4) and errors that had the same sound and same tone ( X   X  X  X  X  X   X  in the figure; tong2 yin1 tong2 diao4), and our system returned only the top three candidates. This is a conserva-tive design, given that we are able to capture a large percentage of the actual incor-rect characters of previously observed errors with the top 10 candidates (Sections 5.5 to 5.7).

This authoring tool can be evaluated by how often the recommended characters are adopted by teachers in their test items. This style of evaluation is the same as what we have accomplished in Section 5.4 through Se ction 5.7. The correct words in the lists, in Table VI, serve as the target test items, and the actual incorrect characters are the teachers X  choices. From this viewpoint, we have conducted an evaluation with more than 4,100 individual tests. The observed inclusion rates, which were presented in Table XI, show that our system was able to offer candidate characters that included the teachers X  choices. The accumulate inclusion rates, which were presented from Table XIII to Table XVII, further indicate that, by providing no more than 10 candi-date characters in different categories of si milar characters, our system maintained its efficacy for assisting the compilation of test items for ICC tests.

In addition to assisting the preparation of test items for Chinese tests, we can em-ploy the lists of similar characters to automatic detection of errors in Chinese text (e.g., Zhang et al. [2000]). The statistics discussed in Section 4.3 show that a large portion of errors in Chinese texts are related to characters that have the same or similar pro-nunciations, and a previous work applied phonetic information for this error detection task based on related arguments [Huang et al. 2008]. Using both visually and phonet-ically similar characters along with statistical methods, we significantly improve the performances of Huang et al. X  X  system and two other systems that were reported in the literature [Wu et al. 2010].

We plan to offer a free and open Web-based service to the research community. The service will allow users to enter queries to se arch Chinese characters that meet certain conditions. With a minor change of the interface shown in Figure 5, we can offer psycholinguistic researchers the neighbor words (e.g., Lo and Hue [2008], Tsai et al. [2006]) of a given Chinese word for their studies. In fact, we are applying our system to support the design of educational games for cognition-based learning of Chinese characters (cf., Lee et al. [2010]). Moreover, our work can be used to find the suggested queries when users of search engines enter incorrect words [Croft et al. 2010, p. 197]. Although we are not experts in the recognition of Chinese characters either in printed (i.e., OCR) or in written form, we can help researchers to find the confusion sets for Chinese characters [Fan et al. 1995] more efficiently. The experimental results presented in Section 5 showed that it is relatively easy to capture errors that are related to phonological similarity. It is relatively harder to catch errors that are related to visual similarity.
 Using the information about the pronunciations of characters that are available in Chinese lexicons is very effective for reproducing phonologically-related errors. Se-lecting candidate characters with the SS and SD criteria was most fruitful. The main reason that our program did not catch the e rrors that were relate d to phonological similarity was that our lists of confusing phonemes (cf. Table I) did not contain the types of errors that actually occurred. This can happen if the types of errors are those which are not considered significant in psycholinguistic studies, but which can occur once in a while in reality.

Using the extended Cangjie codes proved to be the main reason why we could cap-ture a larger portion of the errors that are related to visual similarity, when we com-pare the performances of our systems that were implemented in 2007 and 2008. The decision to divide characters into subareas further improves our ability to find simi-lar characters. However, the steps demand subjective decisions, in which we observed how characters were divided [Lee 2010a], and these decisions will influence how well we find the incorrect characters. We discussed an example of the problem, that is, the  X   X  X  X   X  (gong1 ren2 yi1) problem, in Section 3.4. Another example is the question of how our programs may find the similarity between  X   X   X  (fu4) and  X   X   X  (fu2). According to Lee [2010a], the LIDs for  X   X   X  X nd X   X   X  are 4 and 3, respectively (cf., Section 3.3). Hence, the shared component  X   X   X  (fu2) of these two characters will be saved in two different ways:  X   X  X  X   X (yi1kou2tian1)atP1for X   X   X ; and  X   X  X   X  X nd X   X   X  X tP1and P2, respectively, for  X   X   X .

To alleviate the problem, we concatenated the substrings of Cangjie codes into one string and computed the Dice coefficient (Equation (1) in Section 3.4) of the concate-nated Cangjie codes for two characters. Th is strategy proved to be very important. Using SC3 to select visually similar characters outperformed SC2 and SC1 in all of our experiments.

Although we have achieved good experimental results, using Cangjie codes as the basis of defining the visual similarity between characters does not produce perfect re-sults. The original Cangjie codes may not reflect the complexity, for example, the num-ber of strokes, of a component in a character. A complex component can be represented with a simple Cangjie symbol, for example, the Cangjie code for  X   X   X  (fu2) is  X   X  X  X   X  (zhong1 zhong1 gong1). In contrast, a seemingly simple component can be represented with a longer sequence of Cangjie symbols, for example, the complete Cangjie code for  X   X   X (yu3)is X   X  X  X  X   X  (gong1 ge1 gong1 gong1). This phenomenon may mislead our score functions, that is, SC1, SC2, and SC3, which rely on the lengths of the matched Cangjie codes to determine the degree of similarity between characters.

A possible solution to this problem is to use our own Cangjie codes for the basic elements, but this strategy has its problems. For instance, we replaced the Cangjie code for  X   X   X (yan2)with X   X  X  X  X   X  (bu3 yi1 yi1 kou3) in c 5 ,c 6 ,andc 7 in Table IV. However, such an operation is extremely subjective and labor intensive. Although we changed the Cangjie codes for a limited number of elements, we cannot guarantee that we have done enough for all possible errors that are related to visual similarity. Moreover, we were not sure whether we were just trying to maximize the performance of our systems in the case of some particular lists of errors. This was the main reason that we collected Wlist and Blist for further experiments, after we had been using Elist and Jlist for an extended period of time. Fortunately, the experimental results for Wlist and Blist remained satisfactory.

Another problem that came up when we built the database of the extended Cangjie codes is the degree of detail to which we should recover the Cangjie codes. Consider this list of characters:  X   X   X (wu3), X   X   X  (lie4),  X   X   X  (li4),  X   X   X  (huo3), and  X   X   X (mai4). It is probably not easy for everyone to notice that they all share  X   X   X (xi4)somewherein-side them. To what degree do users pay attention to relatively small elements? Should we consider this factor when we design the score functions to measure the degree of similarity between two characters? This is a hard question for us. The best design may depend on the actual applications, for example, the needs of psycholinguistic ex-periments [Leck et al. 1995; Yeh and Li 2002].

So far, we have not touched upon the issue that the Cangjie codes do not provide a good mechanism for comparing the similarities between characters that consist of very ble II. Another group of similar characters are  X   X   X (tu3), X   X   X (shi4), X   X   X (gong1), X   X   X  (gan1), and  X   X   X  (qian1). Differences among these characters are at the stroke level, so we cannot rely on the Cangjie codes to find their similarities. For such characters, the Wubihua encoding method [Wubihua 2010] should be applied. The Wubihua encoding method assigns identification numbers to a selected set of strokes, for example,  X 1 X  for horizontal strokes and  X 2 X  for vertical stro kes. Because there is exactly one canonical way to write a Chinese character, that is, the standard order of the strokes that form the character, one can convert each of the strokes into their Wubihua digits, and use this sequence of digits to encode a Chinese character. The Wubihua codes for  X   X   X ,  X   X   X ,  X   X   X ,  X   X   X , and  X   X   X  are, respectively,  X 121 X ,  X 121 X ,  X 121 X ,  X 112 X  and  X 312 X ; and the Wu-bihua codes for  X   X   X ,  X   X   X ,  X   X   X , and  X   X   X  are, respectively,  X 25121 X ,  X 25121 X ,  X 25112 X , and  X 25112 X . Demanding an exact match between strings as the selection criterion, we can find that  X   X   X ,  X   X   X , and  X   X   X  are more similar to each other than to  X   X   X  X nd  X   X   X . By appropriately integrating the extended Cangjie codes and Wubihua codes, we will be able to extend our ability to find visually similar characters to a larger scope of characters.

For the study of incorrect Chinese characters, we have intentionally put aside an important class of errors at this moment. For written characters, people may write incorrect characters that look like corre ct characters, for example, writing  X   X   X (shi4) as  X   X   X . 17 These so-called pseudo-characters obey the formation principles of Chinese characters, but, in fact, do not belong to the language. These incorrect characters were not considered in the current study because we could not normally enter them into our files as they were not contained in the font files. Nevertheless, studying this type of errors may uncover possible ways that people memorize Chinese characters, and opens another door to the mental lexicons of Chinese learners.
 Song and his colleagues propose methods for automatic proofreading for simplified Chinese in Song et al. [2008]. They consider seven operators for building Chinese char-acters from their components and propose a set of rules for computing the similarity between Chinese characters. They then employ the similar characters with statistical information about language models to detect possible incorrect words. This line of work is very similar to the work presented in this article. It will be very interesting to compare the performances of Song et al. X  X  and our systems with some common test sets.

SJTUD [1988] provides not just a systematic way to decompose simplified Chinese characters, and it also lists the decompositions of 11,254 individual characters. It will be very interesting to compare the effectiveness for computing visually similar characters with the extended Cangjie codes and the decompositions in SJTUD [1988]. We found methods to reproduce the errors found in the writing of Chinese script. The methods utilized information about the pro nunciation of Chinese characters and the heuristics rules that were derived from observations in psycholinguistic studies to judge the degree of similarity between pronunciations. The methods also employed the extended Cangjie codes and score functions to determine the degree of visual sim-ilarity between characters.
 We evaluated our approach from three aspects. In Section 5.3, we compared the Web-based statistics to show the reliability of Web-based statistics. In Section 5.4, we showed that our approach could capture the incorrect character for a diverse scope of test data at satisfactory rates. In Sections 5.5 through 5.7, we applied and compared two different methods to rank the candidate characters in an attempt to capture the incorrect character with shorter lists of ca ndidate characters. The experiments were carried out with data that we presented in previous conference articles and some new data that covered both traditional and simplified Chinese.

In these experiments, it was found that 76% of these errors were related to phono-logical similarity and that 46% were related to visual similarity between characters. We showed that the Web-based statistics were reasonably stable when we compared the popularity of word usages by comparing the numbers of Web pages that contained the target words in both 2009 and 2010. Experimental results show that we were able to capture 97% of the 4,100 errors, when w e recommended 104 candidate characters. When we recommended only 10 candidate cha racters, we still caught more than 80% of the 4100 errors. The reported techniques are useful for applications that are related to Chinese, and, in particular, we showed a real-world application that can help teachers to author test items for  X  X ncorrect character correction X  tests for Chinese.
