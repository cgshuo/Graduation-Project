 Time series classification arises in diverse application. This paper develops new techniques based on hi-erarchical Bayesian generative models and the Fisher kernel derived from them. A key advantage of the new formulation is that, despite varying sequence lengths and sampling times, one can compute the Fisher information matrix. This avoids its common ad hoc replacement with the identity matrix. The latter strategy, of coordinates in the model parameter space.
 This work was motivated by the need to classify clinical longitudinal data on human motor and psychometric test performance. Clinical studies show that at the population level progressive slowing of walking and the individual patients based on such longitudinal data would improve medical care and planning for assistance. models that describe the population (consisting of many individuals) as a whole, and variations between the between-individual variability (the random effects ), and the additive noise variance are fit by maximum likelihood. The overall population model together with the covariance of the random effects comprise a set Bayesian procedure.
 Data Description The data for this study was drawn from the Oregon Brain Aging Study (OBAS) [2], a longitudinal study spanning up to fifteen years with roughly yearly assessment of subjects. For our work, we grouped the subjects into two classes: those who remain cognitively healthy through the course of the study (denoted normal ), and those who progress to mild cognitive impairment (MCI) or further to dementia impairment. We use 97 subjects from the normal group and 46 from the group that becomes impaired. Motor task data included the time (denoted as seconds ) and the number of steps (denoted as steps ) to walk 9 meters, and the number of times the subject can tap their forefinger, both dominant ( tappingD ) and non-dominant hands ( tappingN ) in 10 seconds. Psychometric test data include delayed-recall , which measures memory II in which the subject is graded on recall of a story told 15-20 minutes earlier. 2.1 Mixed-effect Regression Models parameters and  X  i n is zero-mean white Gaussian noise with (unknown) variance  X  2 . The superscript on the by the sum of a fixed and a random piece:  X  i =  X  +  X  i , where  X  i (called the random effect ), is assumed determines the model for the population as a whole, and the random effect  X  i accounts for the differences between individuals. This intuition is most precise for the case in which the model is linear in parameters denote the mixed-effect model parameters. The feature values, observation times, and observation noise are 2.2 Maximum Likelihood Fitting {  X , D ,  X  } by maximum likelihood. The likelihood of the data { t i , y i } given M is where  X  T  X ( t ) . The two green lines stand for  X  T  X ( t )  X  the observation noise.
 Q Maximization algorithm [6] with {  X  1 ,  X  2 ,  X  X  X  ,  X  k } considered as the latent variable: where M g stands for the model parameters estimated in previous step, and the expectation in the E-step is the linear mixed-effect model in Equation (1), the M-step can be given in a closed form. The details of the updating equations are given by Laird et al. [6].
 We use the linear mixed-effect model with polynomial basis functions  X ( t ) = [1 , t ] T . We trained separate mixed-effect models for each of the six measurements. For the four motor behavior measurements, we use the logarithm of data to reduce the skew of the residuals. Figure 1 shows the fit models for seconds and logical memory II , as the representatives of the six measurements. The plots show the fixed effect regression  X  T  X ( t ) (red curve), and the standard deviations arising from the random effects (green curves) and measurement noise (dashed black curve, see caption).
 The data are the blue spaghetti plots. The plots confirm that subjects that become impaired deteriorate faster than those who remain healthy.
 With multiple classes (or component subpopulations), it is natural to use a mixture of mixed-effect models. We have two components: one fit on the normal group (denoted M 0 ) and one fit on impaired group (denoted M 1 ), with M m = {  X  m , D m ,  X  m } , m = 0 , 1 . Here, we use {  X  0 , M 0 ,  X  1 , M 1 } to denote the parameters of this mixture, with  X  0 and  X  1 being the mixing proportions (prior) estimated from the training data.
 The overall generative process for any individual ( t i , y i ) is summarized 3.1 Fisher Kernel Background The Fisher kernel [4] provides a way to extract discriminative features from the generative model. For any  X  -parameterized model p ( x ;  X  ) , the Fisher kernel between x i and x j is defined as where I is the Fisher information matrix with the ( n, m ) entry linear classifier based on the Fisher kernel performs at least as well as the generative model. 3.2 Retaining the Fisher Information Matrix Where only sequence lengths differ, an empirical average can be used. However where observation times form an estimate by empirical averaging.
 our application, which uses a support vector machine (SVM), we found the difference cannot be neglected. mixed-effect model with polynomial basis functions the Fisher score components associated with higher order terms (such as slope and curvature) are far larger than the entries associated with lower order term (such as intercept). Without the proper normalization provided by the Fisher information matrix, the kernel hierarchical model allows proper calculation of the Fisher information matrix. 3.3 Hierarchical Fisher Kernel Our design of kernel is based on the generative hierarchy of mixture of mixed-effect models, in Figure 2. We We can thus build a standard Fisher kernel for the latent variables, and use it to induce a kernel on the observed data. Denoting the latent variables by v i , the Fisher kernel between v i and v j is where the Fisher score  X   X   X  log p ( v i ;  X   X ) is a column vector and I v is the well-defined Fisher information matrix for v : The kernel for y i and y j is the expectation of K ( v i , v j ) given the observation y i and y j . tions. This extension to the Fisher kernel, named hierarchical Fisher kernel (HFK), enables us to deal with elsewhere in the literature.
 Figure 2 with latent variable z i marginalized out 3 . The Fisher kernel for  X  is The kernel between y i and y j as the expectation of K (  X  i ,  X  j ) : The computational drawback is that the integral required to evaluate and I r do not have an analytical solution. In our experiments, we estimated the integral with Monte-Carlo sampling.
 generative process, as summarized in Figure 3 (middle panel), gives the probability for latent variables The Fisher kernel for the joint variable (  X  i , z i ) is where K m (  X  i ,  X  j ) is the Fisher kernel for  X  i associated with component m (= 0 , 1) The kernel for y i and y j is defined similarly as in Design A: where the integral can be evaluated analytically.
 Design C: f M = M m , m = 0 , 1 This design uses one mixed-effect component in-stead of the mixture as the generative model, as il-lustrated in Figure 3 (right panel). Although any single M m is not a satisfying generative model for the whole population, the resulting kernel is still useful for classification as follows. For either model, m = 0 , 1 , the Fisher score for the i th indi-vidual  X   X  ability p (  X  i ;  X  m ) responds to the change of param-eters  X  m . This is a discriminative feature vector since the likelihood of  X  i for individuals from dif-ferent group are likely to have different response to the change of parameters  X  m . The kernel between  X  i and  X  j is K m (  X  i ,  X  j ) defined in Equation (13).
 And then the kernel for y i and y j : Our experiments show that the kernel based on the impaired group is significantly better than others; we therefore use this kernel as the representative of Design C. It is easy to see that the designed kernel is a special case of Design A or Design B when  X  0 = 1 and  X  1 = 0 . 3.4 Related Models Marginalized Kernel Our HFK is related to the marginalized kernel (MK) proposed by Tsuda et. al. [10]. MK uses a distribution with discrete latent variable h (indicating the generating component) and observable x , which form a complete data pair x = ( h, x ) . The kernel for observable x i and x j is defined as where e K ( x i , x j ) is the joint kernel for complete data. Tsuda et. al. [10] uses the form: we re-write Equation (16) as (17). In our problem setting, this kernel does not exist due to the different lengths of time series. Probability Product Kernel We can get a family of kernels by employing various kernel designs probability product kernels [5] proposed by Jebara et. al. Performance Evaluation We use the empirical ROC curve ( detection rate vs. false alarm rate ) to eval-uate classifiers. We compare different classifiers using the area under the curve (AUC), and calculate the very close to those for logical memory II . The mixed-effect models for each feature were trained separately with order-1 polynomials (linear) as the basis functions. For each feature, the kernels are used in support vector machines (SVM) for classification, and the ROC is obtained by thresholding the classifier output with Classifiers For comparison, we also examined the following two classifiers. First, we consider the likeli-marize each individual i with the least-square fit coefficients for a d -degree polynomial regression model, cross validation in our experiment. The obtained kernel matrix G will be referred to as LSQ kernel . Results We first compare three HFK designs, using the ROC curves plotted in Figure 4 (upper row). On all four motor tests, Design A and Design B are very much comparable except on tappingD , on which but consistently better than other two designs. On logical memory II (story recall), the three designs have comparable performance. We thus use Design C as the representative of HFK, and compare it with the likelihood ratio classifier and SVM based on LSQ kernel, as shown in Figure 4 (lower row). On four motor the three classifiers have very much comparable performance. Fisher kernels derived from mixed-effect generative models retain the Fisher information matrix, and hence the proper invariance of the kernel under change of coordinates in the model parameter space. In additional experiments, classifiers constructed with the proper kernel out-perform those constructed with the identity matrix in place of the Fisher information on our data. For example, on seconds , the HKF (Deign C) achieves a AUC = 0.6873, with the p -value (Z-test) 0.0435.
 Our classifiers built with Fisher kernels derived from mixed-effect models outperform those based solely on the generative model (using likelihood ratio tests) for the motor task data, and are comparable on the psychometric tests. The hierarchical kernels also produce better classifiers than a standard SVM using the advantage for classification. The mixed-effect models capture both the population behavior (through  X  ), Figure 4: Comparison of classifiers. Upper row: Three HFK designs. The number in the parenthesis is the p -value (Z-test) for the null-hypothesis  X  X he AUC of Classifier 1 is the same as the AUC of Classifier 2 X . Upper row: Three HKF designs. p 1 : Design A vs. Design B, p 2 : Design C vs. Design A; Lower row: HFK &amp; other classifiers. p 1 : Design C vs. Likelihood ratio, p 2 : Design C vs. LSQ kernel. classifiers based only on the population model (  X  ) perform far worse than those presented here [7]. Acknowledgements This work was supported by Intel Corp. under the OHSU BAIC award. Milar Moore and to Robin Guariglia of the Layton Aging &amp; Alzheimer X  X  Disease Center gave invaluable help with data from the Oregon Brain Aging Study. We thank Misha Pavel, Tamara Hayes, and Nichole Carlson for helpful discussion.
