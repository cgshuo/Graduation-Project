 1. Introduction
Retrieval) tasks, MT (Machine Translation) systems, LM (Language Modeling), and many other applications in NLP benefit from reducing the number of word forms by applying a stemming method. Current word stemming methods Goldsmith with the same semantics but a different function in the sentence. The distinction between the same and different semantics is given by the particular task to which the stemmer is being applied. For example, the words friend and friendly may be consid-ered semantically equal for information retrieval but not for machine translation. The stemming results are often arbitrary as friend from friendship ). Creating correct morphological units would involve extra effort and might introduce errors, but having them is not always necessary. For the above mentioned tasks, it is sufficient to have a stem represented by a sequence of characters extracted from an input word that distinguishes meanings.

A large class of languages (in the linguistic typology, these languages are called synthetic languages) tends to modify the basic word form by adding prefixes and suffixes according to the function of the word in a sentence. These word forms usu-ally share the same basic meaning. Many stemming methods strip off these affixes. However, such a task is rather compli-cated in many cases, as illustrated by the following examples. The pairs of English words (A) shin and shining , (B) spar and pairs (A, B) consist of semantically different words, whereas words from the third pair (C) differ only in their verb tense. Stripping off the suffixes in A and B would be a stemming mistake, whereas in C it is a correct action. The same example can be made of the word pairs blue and blues or word and words . Again, the suffix s can mean two completely different words or just a different grammatical number. These examples illustrate that stemming algorithms cannot just strip off a known affix. It is instead necessary to decide in which case the affix can or cannot be stripped off.

In this article we describe a novel approach to stemming. The distinguishing property of the approach is the ability to provide very accurate stems (high precision) at the cost of a small decrease in the recall rate. This property constitutes the basis for the name of our stemmer: the High Precision Stemmer (HPS), where the word precision comes from preferring precision over recall. Our method works in a fully unsupervised manner (it does not require labeled data or any knowledge language families: Slavic, Uralic, Romance and Germanic. The Slavic languages are represented by Czech, Slovak, and Polish; the Uralic languages by Hungarian, the Romance languages by Spanish, and the Germanic languages are represented by English.
 The rest of this article is organized as follows. In Section 2 , we clarify some terms that are used throughout the article. Section 3 introduces state-of-the-art methods for stemming. Then we describe our algorithm in Section 4 . We explain our by comparing it with the morphologically annotated data and testing it on the IR and in language modeling tasks. The last drawing conclusions. 2. Definitions
Lexeme, lemma, stem : Throughout the article, we employ the terms lexeme , lemma and stem . To clarify these terms, we same lexeme. Lemma is one selected inflectional variant that is used to designate the lexeme. Lemmas have standardized morphological properties: it usually means that lemmas are words in the singular (nouns), masculine (nouns, adjectives), ignated by the lemma to speak .

The term stem has different meanings in linguistic sources. In some of them, a stem is defined as a part of a word with meaning that can create new words through different linguistic processes. According to Huddleston (1988) , stems can be of the word that stays the same for all the inflectional variants (e.g., daydream -s, daydream -ing).

In this article, we use a third definition that was outlined in the introduction. We are interested in a common part of a tion retrieval it is the part of the word that defines what a user looks for).

Stemming errors: We distinguish two basic types of stemming errors: understemming and overstemming . Understem-ming means that the word is not shortened enough and the resulting stem does not cover all variants of the word. Overstem-ming has the opposite meaning: the word is shortened too much and the resulting stem covers more lexemes.
Light and aggressive stemmers: Stemmers can be divided into light and aggressive stemmers. The light stemmers prefer creating too short a stem (for example, reducing the words durable and duration to dura ). The aggressive stemmers work the other way round. In disputable examples, their stemming is performed even at the risk of creating too short a stem (overstemming).

Inflectional morphology vs. derivation (linguistic process): As we noted in the Introduction, stemming tools usually with the lemma staying the same. Derivational affixes create new words with more or less related meaning. We can also clearly see that removing derivational affixes can be sometimes risky. The words blame -less and blame -ful share the mean-ing, however, they are antonyms. The question of whether stemmers should or should not remove derivational affixes is dif-ficult and we will address it in our experiments (see Section 5.4 ) and in the discussion (Section 6 ). used for similar purposes: to reduce the number of word forms in a text. The fundamental difference is the different kind of duction, is mostly task-oriented in NLP. Moreover, some stemmers also remove derivational affixes, whereas lemmatizers are restricted to inflections only. However, both stems and lemmas are intended for reducing the size of the dictionary.
Stemming and lemmatization thus can replace one another in some cases. Stemming cannot be used if the output is requested to be a valid word form of a language, just as lemmatization can be too weak for some tasks (e.g., vague and vaguely have different lemmas  X  in this case, the lemmas are the same as the words: vague , vaguely  X  which may be a prob-lem for IR). Another difference is that there are currently no means for training a lemmatizer in an unsupervised way: a labeled training corpus or set of manually created rules is needed. Moreover, stemmers are usually more semantically ori-ented: aggressive stemmers tend to join together semantically related lexemes. For example, runner and running may have one stem, run , but these would have two lemmas and two lexemes. A different example is familiar and unfamiliar . These would have one lemma (one lexeme) but usually two stems since the words have contradictory meaning. 3. State of the art and the statistical ones. Rule-based stemmers attempt to transform the word form to its base form by using a set of language-specific rules created manually by linguists. The statistical stemmers usually use unsupervised training to estimate the parameters of a stemming model. The basic qualitative difference is that the rule-based stemmers tend to be better at apply-ing rather complex linguistic rules. They are not limited to stripping off affixes, but they can also change the entire word a speaker of that particular language. On the other hand, statistical stemmers benefit from a large database of automatically meets the assumptions 3 that were made for the given statistical stemmer. However, they fail when the particular linguistic and foot and feet , although they are quite frequent in English). 3.1. Rule-based approaches
It needs only two steps for stemming a word according to predefined endings and transformation rules. This makes the algo-rithm very simple and very fast.
 ball . Snowball is a string-handling programming language developed by M. F. Porter. Stemming algorithms can be easily defined in this language. In addition, ANSI C or Java programs can be automatically generated. The framework is briefly described at http://snowball.tartarus.org , together with stemmers for several languages.
 improvement of about 46% by using the aggressive stemmer, and 42% by using the light stemmer in IR systems, compared with no stemming.
 terized by a complex morphology, thus two rule-based stemmers (light and aggressive) are used to improve IR. When com-pared to an IR scheme without stemming, the light stemmer was able to improve MAP by about 53% on average, and the aggressive stemmer, by about 67% on average. 3.2. Statistical approaches exhaustive survey can be found in Hammarstr X m and Borin (2011) , which provides a description and comparison of the dif-same-stem decisions level, which is defined as follows: Given two words, decide if they are affixations of the same lexeme .
The authors in Xu and Croft (1998) present a method that uses the word form co-occurrences in a corpus to upgrade or create a stemmer. Their work is based on the assumption that word variants (inflected forms of the same word) should occur close to each other (perhaps within a 100 word text window). To model this fact, a variant of expected mutual information is co-occurrence statistics. According to experiments, the authors show that this additional information enhances the quality of a stemming algorithm.

An interesting method for unsupervised stemming was described in Goldsmith (2001) . This method is based on the prin-ciple of MDL (Minimum Description Length). The algorithm tries to find the optimal breakpoint for each word. Each instance of a given word in a corpus uses the same breakpoint, which splits this word into stem and suffix. The model for the optimal distribution of breakpoints minimizes the number of bits to encode the whole collection of words (this is mathematically equal to minimizing the entropy of this collection). The MDL criterion causes breakpoints to segment the words into rela-tively common stems as well as common suffixes. This method is implemented as a framework called Linguistica 5 Goldsmith (2006) .

Automatic suffix discovery is investigated in Oard, Levow, and Cabezas (2001) . At first, the frequencies of each n -gram tracted from the frequency of the adequate suffix n -gram of the lower order n 1 (for example, the frequency of ing is sub-tracted from the frequency of ng ). The altered frequencies are consequently sorted and a threshold for the optimal number of suffixes for each length is chosen. It is computed by plotting the frequency rank ratios and finding the local extreme. The suffixes with a frequency higher than the threshold are then stored so as to be stripped off during the stemming process. The suffixes are processed starting from the longest ones.

In Bacchin, Ferro, and Melucci (2005) a new probabilistic model for word stemming is presented. The mutual relation between stems and suffixes is investigated. Two sets of substrings (prefixes and suffixes) are generated from the word lex-using the MLE (Maximum Likelihood Estimation) method. Three models for combinations of prefix and suffix probability estimations are defined. The stemmer selects the most probable split between stem and suffix given a chosen model. The authors experiment with several languages and measure retrieval performance in an IR system. The proposed algorithm pro-duces results just as good as those produced by Porter X  X  stemmer for these languages.

In Majumder et al. (2007) , YASS 6 stemmer was introduced. It is a simple approach based on word clustering. All the infor-These measures should approximate the morphological similarity between words. The lexicon is then clustered to discover morphologically related words (the equivalence classes). The authors present comparable results with rule-based stemmers approach improves results when compared with no stemming.

Another unsupervised approach to stemming was introduced in Paik, Pal, and Parui (2011b) . The method uses simple co-occurrence statistics reflecting how often word variants (sharing a common prefix of a given length) occur in the same doc-ument. A graph-based algorithm for merging morphologically similar words is then presented. The authors evaluate their stemmer on several languages, including European languages (Czech, Bulgarian, Hungarian, English) and Asian languages (Marathi, Bengali) in the context of IR. Stemmer outperforms YASS, XU stemmer Xu and Croft (1998) , and rule-based stemmers.

The novel graph-based stemmer GRAS (GRAph-based Stemmer) was introduced in Paik et al. (2011a) . Similarly to the approach of YASS, GRAS is focused only on lexical information about words. The stemmer also works only with the collection of distinct words (given by the text collection). The morphological relation is represented by a graph, where the words are treated as nodes and potentially related word pairs are connected by edges. Then the pivot nodes are identified. The idea is that pivots having many neighbors are likely to be potential roots. The authors perform retrieval experiments on seven lan-guages. According to the presented results, GRAS outperforms YASS, Linguistica, and stemmer by Oard et al. (2001) as well as the rule-based stemmer in all seven languages in the information retrieval task. For some languages, GRAS provides a more than 50% performance improvement in the IR task when compared with no stemming. 3.3. Stemmer evaluation
Stemmers are usually evaluated indirectly via a target application, e.g., measuring the improvement in IR with and with-out stemming. However, there have been some attempts how to measure a stemmer X  X  quality directly (without a target application).

One approach to direct measurement is described in Paice (1994) . In the article, stemming is compared with manually created groups of morphologically and semantically related word forms. They measure overstemming and understemming errors, using indices denoted by OI and UI . The indices are defined as the ratios between the number of incorrectly merged words and the total merges, and incorrectly not-merged words and the total merges. The test is designed not to take into account the frequencies of words. An error in a word with frequency 1 has the same impact on the indices as that involving a word with, e.g., frequency 100. They also consider only distinct words and stems, discarding context information. higher impact on the performance of a target application than some infrequently occurring words. Secondly, decisions about the stems of words may be context dependent (i.e., the group of morphologically and semantically related word forms may with one another (the OI and UI indices are not in the same order).
 4. The proposed stemming method tic information and remove the morphosyntactic information contained in words. The semantics should be an important clue to successful stemming. To model the semantic information, we use the findings from Charles (2000), Rubenstein and Goodenough (1965) , which claim that word meaning can be determined from its context. It is expected that the more similar two words are in meaning, the more similar contexts they usually share. This assumption was confirmed in these the semantic similarity of words by a statistical comparison of their contexts.
 mon prefix (they are semantically and lexically similar). The output of this method can be directly used as the stemming result by reducing all the words in a given cluster to their longest common prefix. However, this method alone does not yield the best results. Instead, we use it to automatically generate the training data for the second stage. these rules can decide whether and how to stem the word, depending on some conditions. These conditions can model cer-tain properties of the given word, for example, an occurrence of particular characters in the word, the presence of a certain suffix, etc. This motivation led us to treat the second stage as a classification problem, which naturally encodes the above-mentioned rules into features. We used the maximum entropy classifier that outputs stemming decisions for given words. As a training data, we employed the stemming examples generated from all clusters at the first stage. We also relied on two our approach should work on previously unseen data. The results in Section 5 verify both expectations. Our approach is very successful for both known (seen) and unknown (unseen) words.

A small set of manually prepared training data, another stemming algorithm, or a lemmatizer are viable ways of preparing the training data for the maximum entropy classifier. However, we believe that clustering based on semantic assumptions is the best approach, especially when no manually prepared training data are available. 4.1. Stage 1: Clustering Brown, deSouza, Mercer, Pietra, and Lai (1992) . The algorithm was originally developed to improve the language modeling task. In that task the clusters were created using the minimal mutual information loss scenario. After we manually observed the resulting clusters, it became apparent that they are semantically related. We believe that the semantic information comes from the principle of the algorithm to minimize the mutual information loss. As will be shown later, there is a direct connection between the similarity of neighboring words and the mutual information loss. The more similar are the neigh-boring words, the less mutual information is lost. A clustering method based on the similarity of neighboring words satisfies the conditions presented in Rubenstein and Goodenough (1965), Charles (2000) . In these studies, the words occurring in similar contexts (having similar neighbors) are observed to be semantically similar.

In our approach we take advantage of the MMI algorithm X  X  ability to find semantically related classes. At the same time we successfully reduce the computational costs by processing only words with a minimal (higher than a preset threshold) lexical similarity score. It is defined in the following subsection. 4.1.1. Lexical similarity
We define the lexical similarity between two words as the length of their longest common prefix normalized by the max-imum of their lengths: where LCP  X  w a ; w b  X  is the longest common prefix of words w a and w b .

It is expected that a word stem is related to the initial part of the word. Therefore, if two words are supposed to share a
In later stages of the clustering algorithm, the words are already members of some clusters. To compare two different clusters, we use the complete linkage algorithm : where the resulting similarity is calculated as the minimum similarity between any member of first cluster and any member of the second. 4.1.2. Description of the MMI algorithm and its proposed modifications
Let W denote the set of possible words (word vocabulary) and C denote the set of word clusters (class vocabulary). Note that in the following we make no distinction between class and cluster. Let m be a mapping function m : W ! C , which maps stemming problem.

The original MMI clustering Brown et al. (1992) is based on maximizing the average mutual information of adjacent clas-ses I  X  C L ; C R  X  where mutual information is defined as and P  X  c R  X  are determined using MLE (Maximum Likelihood Estimation). The superscripts L and R always denote the left side and right side word classes in a bigram, respectively.

However, there is no way to find such a partitioning m that maximizes the average mutual information over so many order to decrease the complexity to the order of W jj 3 . Such a complexity is however still very problematic. The iterative greedy algorithm merges two clusters into one cluster while maintaining a minimal mutual information loss. This means that in each step, it must find two clusters (clusters consist of already merged words or a single word) whose connection has a minimal impact on the mutual information of the whole training data. This can be formally described as follows: in in the i th step is derived from merging the two particular clusters c a and c b from the preceding step (the other clusters remaining unchanged) into a new cluster c ab : Note that the operator n denotes the set difference. The mapping m i that minimizes the mutual information loss compared to the preceding step can be expressed by the following formula: are instead limited to those which fulfill a minimal lexical similarity score.
 where Sc a ; c b  X  X  is the lexical similarity between clusters c a and c b (see Section 4.1.1 ).
 information loss.
 morphology of the analyzed language and it is not known in advance. Our solution is to repeat the process of merging clus-5 ). The complete clustering process is shown by the following simplified algorithm transcript 1 .

Algorithm 1. Find a word mapping m into morphologically related clusters straint. It is apparent that g is very likely to be much smaller than W jj , by several magnitudes. 4.2. Stage 2: Maximum entropy classifier we can use a supervised classifier while still the whole system remains unsupervised.
 the observation on the output of a process given by the knowledge x about y . y is a member of a finite set of all possible outputs Y and x is a member of a finite set of all possible pieces of knowledge X .
 edge) about the training data are captured by n real-valued feature functions f i  X  x ; y  X 2 0 ; 1 hi . training data. This can be formalized as of this feature given by the final model.
 formula (8) have exponential form and can be estimated as follows: where Z  X  x  X  X  P y 2 Y Q n by some algorithm for finding the global maximum of a function, such as IIS 7 or by some more sophisticated method, for example by OWL-GN. 8
In order to apply the maximum entropy classifier to the word stemming task (suffix stripping), we need to solve a few issues. Firstly, we define y as the length of a suffix of a given word (it is the suffix that is being stripped off) and to define a set of features which add constraints to the final model. We use four types of features, which are described in imum entropy classifier. 4.2.1. Variables for features
Before we describe the various features for the maximum entropy approach, we need to define a few variables. Firstly, let position a to position b .

Let the stem be the longest common prefix of a word group c (note that the groups are provided by stage 1 of our algo-rithm and may contain errors): where w is a given word for which we need to find a stem and w i are other words belonging to the same cluster c .
Then the suffix of a given word is the remaining part following the stem:
Now, we define an arbitrary ending of a word. It is simply a K character long ending of a word w : 4.2.2. Suffix length statistics
This feature represents the global distribution of suffixes according to the word length. The probability that an L character long word contains a suffix of length m is estimated using MLE: number of words with length L . The function # denotes the number of elements in a set. M is the maximum length of suffix to be stripped off.

The feature function is where m is the position in the word w where the word should be split between stem and suffix.
 The motivation for this feature is the assumption that the length of the suffixes depends on the length of the stems. It adds M  X  1 features to the maximum entropy classifier (one for every possible length of a suffix). 4.2.3. The probability of being a suffix
The most important feature (our experiments show that removing this feature from the feature set causes the highest is not a part of the stem) and therefore it needs to be stripped off. We can estimate the probability as follows: by the number of all times where the word w i ends with suff  X  w  X  .

The function adds M  X  1 features to the final classifier. 4.2.4. The probability of an n-gram X  X  standing before a suffix the characters that precede the suffix.
 denote an N -character substring ( N -gram) of the word w , which ends K characters before the end of the word. This means that this substring starts at the position L N K  X  1 and ends at the position L K , where L  X j W j . Then we define the probability calculated using MLE as the number of times where the N -gram is observed at the end of the stem, divided by the total num-ber of times where the N -gram is observed in a word.
 and 3). This means that this feature produces 3  X  M  X  1  X  features for the maximum entropy classifier. 4.2.5. Word length of feature function: where L ranges from 1 to L max . Thus, L max  X  M  X  1  X  features are added to the maximum entropy classifier.  X  X  5  X  L max  X  X  M  X  1  X  , where M is the maximum length of suffix to be stripped off. 5. Experimental results from the inflection removal point of view (Section 5.4 ). This experiment indicates how well the stemmer removes the inflec-tion of the word forms. The second perspective is the retrieval performance, which is the most frequently used way of mea-suring the performance of a stemmer (Section 5.5 ). Finally, stemmers are used to improve language modeling (Section 5.6 ).
Although the first and third experiments are not considered as traditional testing environments, they may uncover the degree of versatility of each tested stemmer. A versatile stemmer should cope well in variety of tasks. Also, successes and failures in different tasks reveal the properties of particular stemmer methods. For example, if a stemmer performs well may be useful when a similar task (that is know to work well with lemmas) is to be solved.
 stemmers) on the same data and with the same constraints and conditions. Experiments are conducted for several languages, namely Czech, Slovak, Polish, Hungarian, Spanish, and English. The settings of each stemmer are described in the following
Section 5.1 . 5.1. Stemmer settings about stop words was not taken into account in our experiments, in order to make our approach completely unsupervised.
The settings of stemmers are as follows:
HPS : The max length of suffix M was set to 3 for all languages, which means we classify into 4 classes (4 possible lengths of suffix, i.e., from 0 to 3 characters). The minimum similarity between two clusters (the stopping condition for clustering) was set to d  X  0 : 7 for Czech, Slovak, English, and Spanish. For Polish and Hungarian, d  X  0 : 6. Stemming is performed in two iterations for all languages (see Section 5.2 ).
 We implemented HPS on the Java X  platform and we used Konkol (2014) implementation of maximum entropy classifier.
GRAS : The suffix frequency cut-off coefficient (which is used to prune invalid suffix pairs) was set to 4. The cohesion threshold used to measure whether two nodes are morphologically related or not was set to 0.8. Both parameters are recommended by the authors of GRAS.

YASS : The clustering threshold value was set to 1 : 5 for all languages. This setting is recommended by the authors of this stemmer.

Linguistica : This does not require any special settings. However, the number of tokens used for training is limited in the only available implementation of this stemmer. The maximal amount of tokens is set to 5,000,000.

Rule-based stemmers : We used Porter X  X  stemmers for languages available via the Snowball framework. For Czech, we chose to use the light stemmer presented in Dolamic and Savoy (2009) . Despite the fact that, according to the authors, the aggressive stemmer performs slightly better in IR (our experiments agree with this), the results of the light stemmer on inflection removal are much better than the results of the aggressive one. For Polish and Slovak, we have not found any rule-based stemmers.
 Note that the d parameter for HPS is set empirically. By changing d it is possible to tune the aggressivity of the stemmer. The lower d is, the more aggressive a stemmer is created, because more words are grouped by a clustering. The parameter d can be perceived as the minimal ratio between the length of the stem and the length of the word. We recommend setting lower values of d for languages that tend to have long suffixes (e.g., Polish and Hungarian) and higher values of d for other languages. We recommend 0.6 as the lower value and 0.7 as the higher value. However, it is possible to tune the stemmer determined for training HPS.

We also created three new stemmers by extending GRAS, YASS, and Linguistica by our second stage of HPS (maximum entropy classifier). These stemmers are denoted by GRAS + HPS , YASS + HPS , and Linguistica + HPS , respectively. The origi-nal stemmers are used only for generating the training data for the classifier (in the same way as our modified MMI cluster-ing). All other settings remain unchanged. 5.2. Iterative stemming In our initial experiments, it turned out that some words with more complicated morphology remain understemmed. Repeated or iterative stemming proved to be beneficial for such words. The stage 2 algorithm is repeated more than once. During such a process, the understemmed words are shortened. However, there is a risk of overstemming (creating too short rate in particular.

Consider the following example, which deals with the complex inflectional morphology of Czech. The words mlad- X   X  (younger  X  1st case) and mlad- X   X  -mi (younger  X  7th case) share the same stem mlad with suffixes  X  X  and mi . In the second word, the suffix mi follows the suffix  X  X . In such a case, the second suffix mi may be removed during the first iteration derivation in English: fear-less-ly . The first iteration produces fear-less (the correct stem) and the second one fear (overstemmig).

The above mentioned examples show the advantages and drawbacks of iterative stemming. When applying this method, we should be particularly careful with derivational suffixes since the method may easily produce overstemming errors (see the example above). On the other hand, if we deal with inflectional suffixes, iterative stemming can only be beneficial. No inflectional suffix can change the lemma and thus it cannot change the meaning.

In our tests we experimented with the setting of the number of iterations. We found that the optimal value is two iter-we use iterations, the stemmer shortens the understemmed words but the well-stemmed words are left intact. 5.3. Training corpora We experimented with six languages, namely Czech (CZ), Slovak (SK), Polish (PL), Hungarian (HU), Spanish (ES), and English (EN). The stemmers were trained on the unlabeled corpora mentioned below. Since the implementation of the Linguistica stemmer limits the training size to 5,000,000 tokens, we used this limit for all stemmers. The exceptions are the tests with variable training data sizes. We used up to 15,000,000 tokens there.

Statistics for the corpora are presented in Table 1 . We distinguish between (word) tokens and words . Token refers to a sin-we count the total number of words and the number of words that occur in texts at least five times.
 5.4. Inflection removal experiments the presence of the two problematic attributes of the test mentioned in the state-of-the-art section, and mainly due to the lack of manually annotated data for languages other than English. Instead of groups of words that should have same stem, we use readily available groups of words sharing the same lemma. And instead of overstemming and understemming indices, lower the frequency of errors) and recall to understemming errors. Our values respect the frequencies of words and are com-puted directly from the texts (not from dictionaries), so they reflect the contexts.
 paring groups of words with the same stem with groups of words with the same lemma. Ideally, these should be equivalent. measures the ability of the stemmer to approximate lemmas. The score from the test thus should indicate the ability of the stemmer to replace a lemmatizer in applications where lemmatizers are working well, e.g., language models Brychc X n and
Konop X k (2011) , machine translation Koehn and Hoang (2007) , etc. Naturally, the score will be less related to applications which require aggressive stemming and working with derivational linguistic processes, e.g., information retrieval. Therefore, the test does not cover all aspect of stemming. However, we believe that a universal test of stemmers cannot be constructed, simply because stemming is always to some extent task dependent.
 consists of all words sharing the same stem with the word at its actual position (the result of the tested stemmer). The sec-ond group contains all words sharing the same lemma with the actual word (given by the lemmatized data). We calculate precision ( P ), recall ( R ) and their harmonic mean, the F -measure ( F m ):
Here, tp denotes the number of times the word in the stemmer group matched a word in the lemma group and fp denotes the number of times the stemmer group contained the wrong word. Finally, fn denotes the number of times the stemmer group missed the correct word. The higher the precision rate, the fewer times the stemmer produces an overstemming error. On the other hand, the higher the recall rate, the fewer the understemming results. The F -measure takes both these errors into account and thus can be used as a final evaluation of the stemming quality. 5.4.1. Test corpora
CZ : The data are part of the Prague Dependency Treebank 2.0. 13 The texts in the corpus consist of manually annotated arti-cles from several newspapers and journals in Czech.

SK : The manually annotated part of the Slovak National Corpus 14 mainly consists of artistic, journalistic, and professional texts.
 PL : The manually annotated part of the National Corpus of Polish. 15 HU : The manually annotated Hungarian texts of the Szeged Corpus 2.0. 16
ES : The Spanish texts are taken from the Ancora 2.0 17 Taul X , Mart X , and Recasens (2008) corpus, which is mainly oriented to journalism.
 EN : These texts are part of the Open American National Corpus (OANC). 18
Some statistics of the corpora are shown in Table 2 . 5.4.2. Results
This section presents the inflection removal results. Each stemmer was trained on the first 5,000,000 tokens from the appropriate corpus (Section 5.3 ) and then evaluated on the corresponding annotated corpus (Section 5.4.1 ). The results for our novel test are shown in Table 3 .

Before analyzing the results for the tested stemmers, we should note the following. As we explained in the Introduction, the goal of a stemming algorithm depends on the particular task at hand. Some aggressive stemmers usually remove deri-tion to the inflection removal task. Nevertheless, we also pointed out that removing derivational suffixes is risky whereas removing inflectional suffixes is safe. Thus, we can expect that a stemmer successful in this test should produce a low num-ber of overstemming results.
 If we use the F -measure ( F m ) as a quality measure for the inflection removal experiments, we can conclude the following. On all languages except Hungarian and Spanish, HPS yields the best results of all unsupervised stemmers. For Hungarian and Spanish, the results of HPS are similar to those of GRAS.
 On Czech and Slovak, the results of GRAS and YASS are similar. On other languages, GRAS performs much better than YASS. Linguistica performs the worst in this testing scenario. Furthermore, we can see that HPS always achieves one of the best precision rates. In contrast, GRAS always has one of the best recall rates.

In the tables we also show the results for the stand-alone first phase of HPS to see how large an improvement the second precision (except on English).
 Our maximum entropy extension also improves the other stemmers (GRAS + HPS, YASS + HPS and Linguistica + HPS). However, the combination of maximum entropy and the first phase of HPS is generally the best one. We assume this to data for the maximum entropy classifier. 5.4.3. The impact of the size of the training dataset In this section, we study how the quality of the stemming changes depending on the amount of training data available. The stemmers are evaluated on the same data as in the previous section, but different numbers of tokens being used for training. We start with a very small amount of training data (50,000 tokens) for each language, and continue up to 15,000,000 tokens, which we believe is a sufficient amount for all methods. The results are shown in Fig. 2 .
From the figures presented above, we can conclude that our stemmer excels in experiments for all sizes of training data and all tested languages. The F -measure results are better for Slavic languages (Czech, Slovak, Polish) in particular.
A very interesting and also important fact is that our stemmer gives very promising results even for an amount of training results of our stemmer do not improve significantly. Thus, we can state that 1,000,000 tokens are optimal for satisfactory results. This property is caused by the second stage of HPS, as we observe large improvements even for the other stemmers when they use this extension (GRAS + HPS, YASS + HPS and Linguistica + HPS).

As we expected, the quality of other stemmers (without the second stage of HPS) rises significantly with an increasing amount of training data. After a certain point (5M or 10M tokens) the results are not improved any more. The exception is the English (our less inflected language), where the performance of the stemmers decreases or stagnates. We believe this word forms can yield an increase in the recall rate but a very significant drop in precision (some stemmers start to overstem the words). The outcome is that the F -measure drops. Note that this problem is specific to the inflection removal experi-ments. However, in the information retrieval experiments (Section 5.5 ), this does not mean a definite retrieval drop. 5.5. IR experiments
In this section, we experiment with using different stemmers for the information retrieval task. We compare the results of our stemmer with those of other competitive stemmers in four languages. We use the open source search engine Terrier, 19 which implements state-of-the-art indexing and retrieval functionalities. Terrier is written in the Java X  platform and was developed by the School of Computing Science at the University of Glasgow.

In our experiments, we used the I(F)B2 model for term weighting. I(F)B2 denotes the model I(F) ( tf-itf model: term fre-Bernoulli processes) and with the assumption of the hypothesis H2 (the term frequency density is inversely related to the length of document). The derivation and definition of this function can be found in Amati and Van Rijsbergen (2002) . The ing models. 5.5.1. Corpora The evaluation presented in this section is based upon the collection built during the CLEF 20 evaluation campaign (CLEF Evaluation Package AdHoc News 2004 X 2008). Statistics of the corpora are shown in Table 4 .

The collection comprises queries which follow the guidelines of the TREC ad hoc task. Each query is structured into three answers) is provided for every query in the collection. In our experiments, we used the title and description parts only. 5.5.2. Results described in Majumder et al. (2007), Paik et al. (2011a) , the unsupervised stemmers YASS and GRAS should give as good results in the retrieval context as rule-based stemmers. Our experiments confirm this. However, we also present the retrieval experiments from a slightly different point of view.

In the real world, the indexing performed by an IR system is an iterative process. New data arise continually, and they need to be indexed. However, this fact is usually not taken into account when testing stemmers (YASS and GRAS). During the tests, it is expected that a stemmer is trained on all the data that are indexed. However, in reality, the new data are unseen by the stemmers. Retraining the stemmer is computationally expensive, although possible. However, this process has one big problem. The retrained stemmers are likely to stem some already seen words differently because in many stem-mers the stemming decisions are learned from all the data. The direct implication is that the complete data set needs to be data (data not seen during training). Taking these facts into account, we have decided to perform the retrieval experiments using stemmers trained on both types of data:
Seen : The stemmers are trained on the same data as they are used for indexing, which causes all words that are indexed to be known by the stemmers. This experiment allows comparisons to be made with the results in the original papers about competitive stemmers. Unfortunately, this test does not include results for Linguistica, since its implementation limits the training data size to 5,000,000 tokens only.

Unseen : The stemmers are trained on the data used for the inflection removal experiments described in Section 5.4.1 , which means that the indexed data are previously unseen by the stemmers. This way of testing corresponds to the scenario we introduced above. We used a completely different corpus for training the stemmers because we want to emphasize the ability of a stemmer to work with unseen data. However, we must note that in the scenario where the newly indexed data (the unseen data) are from the same domain, the difference between seen and unseen data probably will not be so big.

To calculate all performance scores, we used the TREC-EVAL program, which is the standard tool for the evaluation of TREC results using the standard NIST evaluation procedures.

Retrieval performance was measured using several measures. In the tables below, MAP denotes the Mean Average Preci-sion. R-prec is an R -precision measure that represents the precision at the R th position in retrieved documents for a query that has R relevant documents. The symbols P @ 5 and P @ 10 denote the precisions at fixed low levels of retrieved results (5 and 10 documents). The number of retrieved relevant documents is denoted by Rel-ret . The results are shown in Table 5 . 5.5.3. Significance test
In the preceding section, the stemmers were tested in the information retrieval task. There was, however, only a limited stemmers in terms of their stemming quality. The alternative hypothesis H 1 means that one stemmer is significantly better than the other one.
 The results of the significance testing are presented in Table 6 by the following symbols: Symbol  X  X &lt; X  X  if the row X  X  stemmer is worse than the column X  X  stemmer. The hypothesis H 0 is rejected. Symbol  X  X &gt; X  X  if the row X  X  stemmer is better than the column X  X  stemmer. The hypothesis H 0 is rejected. Symbol  X  X = X  X  if the row X  X  stemmer is equal to the column X  X  stemmer. The hypothesis H 0 is not rejected.
The p -value is calculated for two measures of performance: for average precision MAP (the first symbols in the table) and for R-precision (the second symbols in the table).

From Table 6 , we can deduce the behavior of all stemmers in the information retrieval context. In the case of seen data, it was discovered that all tested stemmers perform equally well (there is no significant difference between them).
In the case of unseen data, HPS (both phases), GRAS + HPS, YASS + HPS and rule-based stemmers perform the best. For less inflected languages (ES and EN) the performance of GRAS and YASS is on the same level, but for highly inflected languages (CZ and HU) their performance is significantly worse. This is expected behavior, because many word forms are previously unseen, leading to a significant OOV (out-of-vocabulary) rate. These facts prove that our second stage of HPS is able to work very well with unknown word forms.

In both the cases of seen and unseen data, all evaluated stemmers significantly improve the results of the IR system when compared with no stemming. If we summarize all these results, HPS (both phases), GRAS + HPS, YASS + HPS, and rule-based stemmers consistently give the best results even though HPS was not designed purely for the IR task, but rather to be a multi-purpose stemmer. GRAS and YASS perform slightly worse and Linguistica is the least efficient stemmer for the IR task. 5.6. Language models
This section presents experiments with the application of stemmers to language modeling. The purpose is to reveal the performance of stemmers in yet another scenario.

Language modeling is a crucial task in many areas of NLP. Speech recognition, optical character recognition, machine translation, information retrieval, and many other areas depend heavily on the quality of the language model that is being used. Each improvement in language modeling can also improve the particular job where the language model is used. Morphological information has already been proved to be useful in language modeling. For example, in Brychc X n and Konop X k (2011) , we use lemmatization and part-of-speech (POS) tags to significantly improve the perplexity of Czech and Slovak language models. In this section, we present a similar approach, but instead of lemmas we used stems, and instead of POS tags we used suffixes. 5.6.1. Architecture
We choose class-based language models as the architecture for incorporating the morphological information. We have derived two kinds of class-based models: Stem: word classes represent the words with the same stem.
 Inflection: words with the same inflection (suffix following the stem) are grouped into one class.

We use the modified Kneser X  X ey interpolation Chen and Goodman (1998) for smoothing the baseline n -gram language model as well as the stand-alone class-based models. The order ( n ) of all models is 3.
 These two class-based models are combined with the baseline model by bucketed linear interpolation Bahl, Jelinek, and Mercer (1983) : Dempster, Laird, and Rubin (1977) to calculate the optimal weights k k by maximizing the probability of the held-out data. In bucketed interpolation, the weights are functions of the frequency of word history. The main idea behind the interpolation is used. 5.6.2. Results
The performance of a language model is typically measured in terms of the perplexity of the model on the unseen test corpus. The perplexity can be seen as the confusion of the model. Lower perplexity means a better prediction ability of language model. It was shown by many authors that the reduction of perplexity often leads to improving whole system where the language model is used (for example machine translation in Brychc X n &amp; Konop X k (2014) or speech recognition in Watanabe, Iwata, Hori, Sako, &amp; Ariki (2011) ).

The stemmers were trained on the same data as during the inflection removal experiments (see Section 5.3 ), this means on 50k, 100k, 500k, 1M, 2M, 5M, 10M, and 15M tokens. Each language model was trained on 15M tokens. An additional 2M tokens were used as held-out data. Then, an additional 5M tokens were used to calculate the perplexity of the language model.
 Fig. 3 shows the improvement in perplexity when compared to the baseline language model.

The experiments with language modeling confirm the conclusions of the preceding experiments. Again, HPS obtains on average the highest perplexity drops. GRAS and YASS swapped the order of their results. In this test, YASS tends to perform better than GRAS. For smaller training data sizes, HPS has no competitor. The second stage of HPS again produces large improvements especially when little training data is used. 5.7. Performance tuning
In this section, we investigate some possible tweaks which decrease the computational costs of the stemming. These tweaks have virtually no impact on the stemming results.

Firstly, it must be remembered that our modification of maximum mutual information clustering (described in Section 4.1 ) gives accurate results only for words which occur frequently enough in a corpus. The infrequent words have a negligible impact on the average mutual information of the data, and their clustering may be very inaccurate, and can even decrease the quality of the whole stemmer. We recommend clustering only words with a frequency higher than some threshold (for Example 10) by the MMI algorithm. The remaining words should be clustered using only lexical information (presented in Section 4.1.1 ). Moreover, the complexity of the clustering increases with the square of the number of words being clustered. Thus, limiting the words being clustered has a positive impact on the processing time.

An additional acceleration of our algorithm is possible by incorporating just enough occurring word bigrams in the cal-culation of the average mutual information of the data. We recommend incorporating word bigrams that occur at least 2 or 3 stemmer stays almost the same.

We would like to point out that setting these parameters has no impact on the quality of the stemming results. Setting these parameters is not mandatory.

For the sake of a better grasp of these efficiencies, we also present the running times needed to train HPS, as well as that needed for the stemming itself. We implemented our HPS method on the Java X  platform, and tested it on a computer with a Core i7 3.4 GHz processor. Training the model on 5M tokens of Czech data (the same model used for the experiments with the above recommended settings, i.e., word frequency at least 10 and word pair frequency at least 2) takes about 36 min. The stemming of 10M tokens of text with this model takes about 33 s. These results clearly show that once we already have a trained model, the stemming itself is very fast. 6. Discussion
In Section 5 , we thoroughly tested our HPS from three different perspectives. To put the performance of HPS into the con-text of the state of the art, we also provided a comparison with other competitive stemmers (namely GRAS, YASS, Linguistica, and rule-based stemmers). In addition, we extended these competitive stemmers with our second stage of HPS (maximum entropy classifier). During our experiments, we also measured the amount of training data needed to give satisfactory results.

The first experiment (Section 5.4 ) was focused on evaluating how well the stemmers remove the inflection of words by comparing classes with the same stem with classes with the same lemma obtained from manually annotated data. HPS was at the top for all languages, only for Spanish and Hungarian did GRAS performed equally well.

The second experiment (Section 5.5 ) investigated the use in an information retrieval task. Retrieval effectiveness was tested on Czech, Hungarian, Spanish, and English, for both previously seen and unseen data. Significance testing was done to make the results more appropriate. It was discovered that our stemmer provides significantly better retrieval scores than competitive stemmers in the case of indexing new (unseen) data. HPS tends to be more suitable for languages with rich mor-phology (Czech, Hungarian) than other stemmers, because of the significant OOV (out-of-vocabulary) rate. For Spanish and English the significance testing proved that there is no significant difference between stemmers for both seen and unseen data. However, such results could have been easily predicted since the stemming of less inflected languages does not play as important a role in the IR task as the stemming of highly inflected languages.

The last experiment (Section 5.6 ) examined the effect of using stemming in language modeling. Class-based models were derived from stemming results and they were coupled with a baseline n -gram language model. Again, for highly inflected not play a key role here. It is interesting that GRAS, which in previous experiments performed very well, gives one of the often overstems the words.

We explain the superior performance of HPS by its building a stemmer in two stages. The first stage uses the idea of involving latent semantic information in the stemming task. Other approaches deal with the problem on a purely lexical level. By involving semantic information, our stemmer can better decide about the appropriateness of removing different suffixes. The suffixes that can alter the semantics of the words should be left intact in our approach. For example, compare semantic point of view, spar and spar-ing are completely different words. Naturally, semantic information retrieved from the statistical comparison of word contexts is not flawless. Nevertheless, the increased performance of HPS indicates that latent (1998) , the co-occurrence statistics were used to refine equivalence classes given by some aggressive stemmer, decreasing number of overstemming errors), our second stage goes deeper. It is not limited to work with aggressive stemmers only. The second stage of HPS extracts rules from the word clusters given by the first stage, and combines these rules using a maxi-mum entropy classifier. These rules were proved by our experiments to be very general and efficient, because HPS is able to stem previously unseen word forms.
 The preceding paragraph relates to the question of whether a stemmer should or should not remove derivational suffixes. IR but not for machine translation. Secondly, the question also relates to the semantics. We can generally say that we should remove a suffix only in cases where they do not change the meaning of the stemmed word. We believe that employing some semantic information is appropriate, given this perspective.

By taking all the results into account, HPS seems to be the most effective and most universal approach for stemming aimed at languages with rich morphology. GRAS is very efficient in IR tasks and even performs well in inflection removal experiments. YASS provides consistently good results and so it is also very universal. Linguistica was not as efficient as the other stemmers in our tests.

A very positive fact proved by our experiments is that HPS requires only a small amount of training data to give very sat-isfactory results. This property is caused by our second stage of HPS, the maximum entropy classifier. It was shown that this second stage also improves other stemmers: they are denoted by GRAS + HPS, YASS + HPS, and Linguistica + HPS, when sup-plemented by this second stage. In this regard, HPS has no competitor. Other stemmers perform poorly if they have little data of 1,000,000 tokens seem to be more than enough for training, because when increasing the amount of the training data, the needing only a small number of parameters to be estimated from the training data. The rules formulating the various endings of word forms are supposed to be common and often repeated in a corpus, and this is the reason why HPS performs well even with as small a training dataset as 50,000 tokens. These facts also explain why the performance of HPS does not improve as much as in the case of other stemmers with increasing amounts of training data.

It may seem that in an era of vast linguistic resources, such a property would be insignificant. However, we would like to point to the idea presented in Hammarstr X m and Borin (2011) . There are a huge number of languages that have a very lim-ited number of speakers. For such languages, rich linguistic resources are not obtainable. Our stemmer should be successful particularly in this area. We plan to target these languages in the future.

As stated at the beginning of our paper, precision is preferred over recall in our approach. Thus, it is possible to call our form better in the retrieval context than light stemmers. By more aggressive stemming, the recall rate is increased and the size of the storing index is decreased at the same time. However, it was not our aim to create an unsupervised stemmer focused solely on information retrieval, but to design a multi-purpose stemmer performing well in multiple scenarios with-out any modification. 7. Summary 7.1. Contributions The contributions of the paper are the following:
We present a new approach to stemming that has a very universal scope. It outperforms the state of the art in unsuper-vised ways of stemming in the inflection removal test, the information retrieval test with unseen data, and the language modeling task. In the information retrieval test with seen data, it provides comparable results with the state of the art. Our proposed method is by far the most effective one when little training data are available.
 We provide tests on four language families (six different languages).

In the article, we deal with several aspects of the stemming problem, such as the difference between removing inflectional and derivational suffixes, and the relation of stemming to lemmatization.
 7.2. Future work irregular verbs in English. In most languages, the verbs in different tenses often differ in large parts of the words. discover them. We plan to design more elaborate rules for finding candidates for words with the same stem. In this way, many of the current stemming mistakes can be resolved, thus enhancing the performance of our stemmer.
 mation loss algorithm. Note that we already experimented with semantic spaces for improving language modeling (see Brychc X n &amp; Konop X k (2014) ).
 our stemmer is suitable for them.
 leagues. In Habernal, Pt X c  X  ek, and Steinberger (2013), Habernal and Brychc X n (2013), Habernal, Pt X c  X  ek, and Steinberger (2014), Steinberger, Brychc X n, and Konkol (2014) , they discovered that our stemmer significantly improves sentiment anal-ysis. Our preliminary results indicate that HPS is also very useful (and significantly better than competing stemmers) in named entity recognition and machine translation tasks. 7.3. Conclusion art in unsupervised stemming. We successfully accomplished the goal of creating a multi-purpose stemming tool. Its design opens up possibilities for solving non-traditional tasks, such as approximating lemmas, improving language modeling or sen-timent analysis. However, it still provides very good results in the traditional task, information retrieval. additional information. A clustering method that discovers semantically related words creates the basis for learning the stemming rules. These rules successfully approximate the morphology of a language and can be used even for unknown (pre-viously unseen) word forms. Our experiments show that the stemming of unknown words is as effective as the stemming of known words, which is essentially one of the greatest advantages of our stemmer compared with other competitive stemmers.
 however, achieved with only 50,000 tokens for training, where other stemmers fail. This property could be very important, mainly for poor-resource languages.
 stemmers (namely GRAS, YASS, Linguistica as well as with rule-based stemmers) was done on several languages, including icant difference between the stemmers that have been tested. The stemming however play a key role for highly inflected languages, where our stemmer is significantly better than competing unsupervised stemmers and approximately on the same level as rule-based stemmers. Our HPS implementation is available at https://liks.fav.zcu.cz/HPS . Acknowledgements
Regional Development Fund (ERDF) and by project  X  X  X TIS  X  New Technologies for Information Society X  X , European Centre of Excellence, CZ.1.05/1.1.00/02.0090. Access to the MetaCentrum computing facilities provided under the program  X  X  X rojects of Large Infrastructure for Research, Development, and Innovations X  X  LM2010005, funded by the Ministry of Education,
Youth, and Sports of the Czech Republic, is highly appreciated. The access to the CERIT-SC computing and storage facilities provided under the programme Center CERIT Scientific Cloud, part of the Operational Program Research and Development for Innovations, reg. No. CZ. 1.05/3.2.00/08.0144 is acknowledged. We also thank the Czech News Agency for providing a huge number of texts in Czech.
 References
