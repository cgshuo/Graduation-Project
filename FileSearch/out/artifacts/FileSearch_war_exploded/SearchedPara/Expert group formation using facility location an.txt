 1. Introduction new direction in research and also poses some brand new challenges. Here, the key problem is how to assign a group of ex-several works have been made for conference paper-reviewer assignment ( Karimzadehgan &amp; Zhai, 2012; Karimzadehgan, Baykasoglu, Dereli, &amp; Das, 2007 ).

In real scenarios, while several and sometimes diverse skills are needed to perform a task successfully and completely, tion problem are:  X  because of implicit notion of expertise, a method is needed to transform the expertise documents (e.g. resume, profes-sional profile, etc.) of each expert into the set of his/her skills.
 of the assigned members in a complementary manner.
 member of the group individually be able to cover as many as possible the required skills of that project. taneously assign tasks to all the available experts with consideration of task-load balancing.
As a case study of the expert group formation problem, we consider the problem of review assignment. Review assign-ment is a common task that many people such as conference organizers, journal editors, and grant administrators would can be explicitly determined by some keywords or can be inferred from the abstract/body of the paper. Secondary, the re-be able to cover as many as possible aspects of the paper. Finally, each member of the program committee (i.e. each reviewer) can only be involved in the review process of a limited number of papers.
 the baseline language model proposed in Karimzadehgan et al. (2008) ), which computes the relevance score of each expert results collectively.

In this paper, we formalize the expert matching problem within the unified framework of Facility Location Analysis (FLA) paper as the desirable facilities to be placed as close as possible to their customers (i.e. aspects of papers).
We show that our proposed method can improve the performance of expert matching in comparison with the state-of-fine three problems that can be solved by the proposed frame work of FLA. These problems are modeled using the unified framework of facility location analysis. In these problems, given a set of N papers and M reviewers, each paper should be assigned to a group of exactly k reviewers. The above mentioned three problems are given below. paper are implicitly represented in the abstract of the paper and the skills of each reviewer can be inferred from the expertise document of that specific reviewer. Generally, the expertise document of an expert can be his/her resume but in this paper, we consider the concatenation of one X  X  publications as his/here expertise document. be assigned to a reviewer).
 paper and also the set of the relevant skills of each reviewer are explicitly determined (for example by using the ACM
Categories and Subject Descriptors 1 keywords for computer science related papers). Given a limited number of reviewers, (i.e. load balancing). this problem, we assume that the underlying aspects/skills of papers and reviewers are implicit and on the other hand, each expert has a limited capacity to review the assigned papers. The goal of this problem is to simultaneously maximize the coverage and the confidence of the assigned groups while the capacity condition is satisfied. optimality of the proposed solution.

The rest of the paper is organized as follows. We first discuss facility location problems in Section 2 and our proposed 2. Facility location analysis
Cheong, Kreveld, &amp; Overmars, 2008 ) concerning itself with mathematical modeling and solving problems which find the this problem is twofold: To minimize the total cost of opening those k facilities.
 To minimize the weighted distances from the customers locations to their closest facilities. said to be the communication cost of customer j . The sum of communication costs of all customers is the Communication by cost ( f i ). For a given solution, the cost of opening all the open facilities is called its Building Cost . main types of these problems are uncapacitated and capacitated facility location problems that can be useful to model the expertise matching problems. In this paper, we formally model the unconstraint (i.e. Problem 1 ) and constraint subsections, we give these problems as well as their approximate and exact solutions. 2.1. Uncapacitated Facility Location Analysis (UFLA) communication cost of the solution would be minimal. In this problem, an arbitrary number of customers can be assigned to a facility. In other words, there is no constraint on the assignment of customers to the facilities.
Given the set of facilities F , the set of customers C , the distance between each customer c the demand of each customer c j as demand ( c j ), and the building cost of each facility f facilities can be calculated as follows Gonzalez (2007) : munication Cost (the second summation).

Fig. 1 illustrates an instance of uncapacitated facility location problem in which the location of the customers and the which minimize the overall communication cost.

As an integer linear programming problem, the uncapacitated facility location problem can be represented by the opti-mization of the ILP problem indicated in Eq. (2) . In this integer linear program, U demand ( c j ) and D ( f i , c j ) are input parameters. U whether facility f i is a member of final selected facilities (i.e. set S ) or not. Constraint T be selected in final solution. X ( i , j ) is also a binary variable (according to constraint T assigned to facility f i in final solution or not. Constraint T facility and constraint T 3 indicates that customer c j can be assigned to facility f
The UFLA in general belongs to the class of NP-hard problems, which can be proved by reduction, for example, from the set mize it using Greedy Local Search (GLS), a.k.a. Hill Climbing, as shown in Algorithm 1 . Algorithm 1. Greedy Local Search for UFLA problem Input: F : (candidate facility locations), k (the cardinality of solution set) Output: S : top k facility locations selected facilities in S are an approximate solution for the problem. 2.2. Capacitated Facility Location Analysis (CFLA) distance between each customer c j and facility location f ing cost of each facility f i as cost ( f i ) and the capacity of each facility f because the number of customers is more than 3 2=6.
 While CFLA problem is in general NP-hard, various approximation algorithms ( Charikar &amp; Guha, 1999; Chudak &amp;
Williamson, 2005 ) are proposed for this problem. Specifically, Charikar and Guha in Charikar and Guha (1999) proposed programming, we chose to exactly solve the matching problem following the idea of linear programming. The explanation of our solution for constraint expert matching using the linear programming will be described in Section 3.4 . 3. Multi aspect expert matching
In this section, we describe how to model the expert matching problems using the facility location framework. The list of symbols used in this paper is represented in Table 1 .

Before describing the facility location framework for expert matching, we describe the author topic modeling method introduced in Karimzadehgan et al. (2008) , which is used for implicit topic matching problems (i.e. Problems 1 and 3 de-scribed in Section 1 ). 3.1. Expert topic modeling
The notion of related aspects of papers and reviewers in Problems 1 and 3 of expert matching (described in Section 1 )is fidence conditions. However, we can assume that the related aspects of a paper can be inferred from its abstract and the in this section, we describe a method to find a topic representation for each paper and each reviewer.
Following the idea of reviewer modeling introduced in Karimzadehgan et al. (2008) , we can assume that there is a space represented as the mixture of these topics. Let s =( s 1 , ... , s p ( w j s i ) is the probability of word w for the topic s Semantic Analysis (PLSA) ( Hofmann, 1999 ). Let R ={ r 1 , ... , r 1999 ) equals to: where V is the set of all words in the vocabulary, c ( w , r selection probability of topic s a for document r i .

EM algorithm can be used to compute the maximum likelihood estimation of all parameters including p ( s
After learning all the parameters, each expert e i can be represented using the topic vector s = p ( s a j h i ). Furthermore, using the estimated values of p ( w j s as s 0 j denoted by ( s 0 1 j , ... , s 0 Tj ) such that s 0 and the number of latent topics T , the set of papers and reviewers can be represented by the paper-topic A paper or reviewer to a topic.

Note that if the relevant topics of papers and reviewers are explicitly given using some keywords (i.e. the Problem 2of expert matching), then the paper-topic and reviewer-topic will be zero-one matrixes which can be defined as follows where aspect ( p j ) indicates the set of relevant aspects of paper p
After representing papers and reviewers using the above matrixes, now we can describe the matching algorithm for retriev-ing the k -top reviewers for a given paper.

Note that the proposed method for topic learning in this section is applicable, if we have enough papers for a candidate (or an specific section of the venue) in which his/her paper is published. For example, for reviewers who previously pub-pects/skills of them. But in this method, we can only infer the general aspects of a reviewer profile and this method is topics of each reviewer. 3.2. Modeling expert group formation as facility location placement
The expert group formation problem is concerned with the assignment of N papers to M reviewers such that the following conditions are satisfied: C 1 (Top-k retrieval) : Each paper p j should be assigned to a group of exactly k relevant reviewers.
C 2 (Maximal Coverage) : In an ideal matching, the group of reviewers assigned to paper p aspects/topics (i.e. required skills) of paper p j in a complementary manner.

C 3 (Maximal Confidence) : In an ideal matching, each reviewer e possible of the topics of paper p j .

While above conditions are common in all group formation problems, the following condition should also be satisfied in constraint group formation problems (i.e. Problems 2 and 3 described in Section 1 ).
 defined number of c i papers.

Following the general idea of facility location analysis, we can reduce each expert group formation problem to a facility location placement problem. These two problems are similar to each other in many aspects: should also be assigned to exactly k reviewers. 4 assigned to at least one relevant reviewer, then the maximal coverage condition can be satisfied. each reviewer for a given paper, then the maximal confidence condition can be satisfied. tion framework, the load balancing condition can be satisfied.

According to the above similarities between facility location placement and expert group formation problems, we can imagine each reviewer e i as a facility and each topic/aspect s of paper p ity/reviewer. Given the set of papers, reviewers, paper-topic matrix, reviewer-topic matrix and the number of reviewers for each paper, we should define the building and communication costs to complete the facility location framework. In the following subsections, we will define these costs for each of the group formation problems. 3.3. Implicit Aspects  X  Unconstraint Matching The first problem of expert matching is concerned the assignment of N papers to M reviewers such that the conditions
C , C 2 , and C 3 are satisfied. In this problem, we assume that each reviewer has infinite capacity. So, we can assign each of
N papers to all available reviewers independent of other papers. In other words, we can independently solve the matching problem for each paper.
 Using the topic modeling method described in Section 3.1 , we can infer the paper-topic A and reviewer-topic B matrixes.
UFLA is composed of two parts: 1. Building Cost : The building cost indicates the cost of opening a facility at a specific candidate location. solution.

In expert matching problem, the first part corresponds to the cost of selection of an expert e part indicates the cost of coverage for a topic of a paper p the building cost and communication cost in our framework as follows. 1. Building cost of assignment of reviewer e i to paper p where e i ! and p j ! correspond to the topic vector of reviewer e vector of paper p j (i.e. the j th row of the paper-topic matrix A ) and D  X  e value of these vectors.

Intuitively, if the topic distributions of vectors e i ! and p will be a very low building cost facility candidate for paper p 2. Communication cost of assignment of aspect a of paper p where e i ! is topic vector of reviewer e i , s aj is the weight of topic a in topic vector of paper p vector with all zero elements except for topic a .

In this case, if reviewer e i is able to cover skill a , then we expect that the weight of topic s (i.e. the reviewer) will be low.

According to the above definitions for building and communication costs, the objective function of unconstraint expert matching problem can be represented as follows: where S is the set of selected reviewers for paper p j and s that paper. 3.4. Explicit Aspects  X  Constraint Matching papers to M reviewers such that in addition to the conditions C satisfied.
 assignment problem, the relevant topics of each paper and reviewer can be described by some few keywords.
The constraint C 4 makes the matching problem very hard, indeed this matching problem belongs to the class of NP-hard problems; furthermore, in this problem the conditions C 2 2005 ) and also propose an exact linear programming solution for it.

Similar to Section 3.3 , in the CFLA framework of constraint expert matching, each aspect/topic of paper p a customer and each reviewer is considered as a candidate facility location.
 u = 1 if and only if, in the final solution, the paper p j is assigned to the reviewer e cates the assignment of topic a of paper p j (i.e. a customer) to reviewer e topic a of paper p j is assigned to reviewer e i , and finally, A paper p j is related to topic a .
Algorithm 2. Linear programming formulation of CFLA subject to
In the linear program given in Algorithm 2 , constraint T retrieval condition (i.e. the condition C 1 ).

Constraint T 2 indicates that the sum of elements of each column of U should be less than or equal with c of the i th reviewer); this means that reviewer e i can only be assigned to at most c balancing condition (i.e. the condition C 4 ).

Constraint T 3 indicates that for each related topic of paper p assigned. On the other hand, T 4 indicates that the topic a of the paper p signedtothereviewer e i .Inotherwords,ifthedecisionvariable X ( i , j , a ) = 1,thentheelement u The objective function in Algorithm 2 is the same as the objective function given in Eq. (4) with this difference that in same concept (i.e. the best assigned reviewer for topic a ).

Intuitively, by minimizing the objective function, both the coverage and the confidence conditions of the matching prob-lem (i.e. C 2 and C 3 conditions) can be satisfied. Firstly, paper p
On the other hand, minimization of the communication cost for the relevant topic a of paper p satisfy the coverage maximization condition (i.e. condition C building and communication cost as follow.

In above equations, aspect ( e i ) indicates the set of relevant topics for reviewer e paper p j . Intuitively, BCost ( i , j ) indicates the fraction of aspects of paper p
CCost ( i , j , a ) indicates whether the assigned reviewer r we prove that Algorithm 2 can produce the optimal coverage and confidence expert assignment. k = 0, then Algorithm 2 will produce the optimal converging groups.

Proof. Let U be the M N binary assignment matrix where u ij to assignment matrix U is the average value of coverage of each paper and can be computed as follows where j aspect ( p j ) j indicates the number of aspects of paper p all papers and Coverage ( p j ) is the coverage measure for paper p puted using the following equation.
 On the other hand, setting k =0in Algorithm 2 results in the following objective function.
By minimizing the above objective function, the value of decision variables X ( i , j , a ) are determined as follows. 1. if A ja = 0, then for all values of i the value of X ( i , j , a ) = 0. In other words, if paper p will not be assigned to any reviewer. 2. if A ja = 1, then only for exactly one value of i = i 1 values of i value of X ( i 1 , j , a ) = 0. In other words, for each relevant topic a of paper p
This assigned reviewer may be able to cover topic a or not (i.e. CCost ( i cover topic a then the value of objective function does not change; otherwise it increases by 1. According to the above cases, for each paper p j the value of equals the value of aspects of paper p j , which is not covered by the assigned group of reviewers (i.e. j NCaspect ( p tuting in Eq. (7) , we obtain the following equation.
 It is obvious by minimizing the values of OBJ 1 ( OBJ 1  X  p , the coverage measure will be maximized. h Algorithm 2 will produce the optimal confidence groups.
 to paper p j can be computed as follow Therefore, the average value of average confidence for all assignments according to the matrix U is like follows. where A , B , U are the paper-topic, the reviewer-topic and the assignment matrixes respectively, and k is the number of reviewers assigned for each paper. The above equation can be re-written as follows.
 On the other hand, by setting k = 1, the objective function of Algorithm 2 becomes as follows.
According to definition of BCost ( i , j ) (i.e. Eq. (5) ), we have:
On the other hand, Eq. (9) is true because the term B ia A and reviewer r i are relevant to topic a ).
 So, we have the following equality: According to Eqs. (8) and (10) , we have With simple modifications, we obtain: According to condition T 1 in Algorithm 2 , we have the following equation.
 So the following equation is always true:
According to the above equation, it is obvious that by minimizing the OBJ
The above theorems show that the linear programming solution can produce the optimal solution in terms of the cover-age and confidence measures. Parameter k can be used to make a trade-off between these measures. 3.5. Implicit Aspects  X  Constraint Matching ond problem but in this problem the topics/aspects of papers and reviewers are not predetermined. So, we use the topic modeling method introduced in Section 3.1 to infer the paper-topic and reviewer-topic matrixes. Fortunately, the linear programming method proposed in Algorithm 2 can be used to solve this matching problem. We only need to define the building and communication cost appropriately for implicit aspects. Similar to the method proposed for unconstraint implicit matching in Section 3.3 , we define the building and communication cost in the same way described in Eq. (4) . 3.6. Solving the integer linear program Once our problem is formulated, we can use many algorithms to solve it. In our experiments, we use the commercial ILOG CPLEX 12.5 package 7 to solve our reviewer matching problem.
 ILOG CPLEX optimizer is able to solve the linear programs with millions of constraints. Specifically, CPLEX uses the od with a Branch and-Bound algorithm, to solve the integer linear programs.

The idea of the Branch-and-Bound algorithm is to take a problem and decompose it into smaller problems such that a solution to a smaller problem is also a solution to the given problem. The algorithm recursively decomposes the original problem until it can be solved directly or is proven not to lead to an optimal solution.
In order to solve our integer linear program, using the LP relaxation, the original problem is transformed to a linear programming sub problem which is easy to solve optimality. This linear program without integer constraints is solved using the simplex algorithm. Branch and cut involves running a branch and bound algorithm and using cutting planes to tighten the linear programming relaxations. Specifically, when the optimal solution for the non-integer sub problem is the problem is divided into two sub problems: one is to explore values greater than or equal to the smallest integer greater than the current value, and the other is to explore values less than or equal to the next lesser integer. These new linear programs are then solved using the simplex method and the process repeats until a solution satisfying all the
Mitchell (2002) . 4. Related work
The problem of expert group formation has recently attracted lots of attention in information retrieval ( Karimzadeh-language modeling ( Balog, Azzopardi et al., 2009 ), voting model ( Macdonald, 2009 ), and person centric language mod-forums ( Pal &amp; Konstan, 2010 ).

The problem of multi aspect expert group formation is introduced by Karimzadehgan et al. (2008) . Specially, they con-methods are (1) the redundancy removal, (2) expert aspect modeling and (3) query aspect modeling ( Karimzadehgan et al., aspect query is segmented into semantically diverse parts such that each part can be considered as a single aspect query; sidered as the final answer.

The most effective method proposed in Karimzadehgan et al. (2008) is expert aspect modeling. Similar to our approach for the first problem of expert matching, it is based on learning a topic vector representation for experts (reviewers) and the queries (papers). Using these topic vectors, the Next Best greedy approach is utilized to form the optimal skill covering group. In this approach, the members of a group are selected step by step; At step n , a reviewer group.

In this equation, h i indicates the topic vector of the i th selected member of the group (specifically, h group after selection of the k th expert candidate.

The intuition behind this method is that h q gives a measure of which topic aspect is relevant; As a result, if h
Thus, the best r k is the one that works together with r 1 coverage given by the query. So, parameter r controls how much to rely on the previously picked reviewers r cover all topic aspects of the query.

In contrast with the Next Best greedy approach ( Karimzadehgan et al., 2008 ), our proposed method for implicit aspect inated at the following steps. However, in the Next Best greedy approach, a non-appropriate selected reviewer for a paper cannot be changed. On the other hand, the Next Best greedy approach cannot easily be extended for constraint matching problems (i.e. the second and third problems of expert matching). In contrast, our FLA framework for expert matching can be extended for constraint and explicit aspect matching problems.

The problem of constraint expert group formation (i.e. the second and third problems of expert matching) has been re-cently introduced in Karimzadehgan and Zhai (2012) . This problem can be considered as an extension of the paper-review 2007 ) concern only with the assignment of a paper to the relevant reviewers and ignore the coverage, confidence and load balancing conditions, Karimzadehgan and Zhai (2012) introduced the problem of multi-aspect constraint paper-review matching. They proposed an integer linear programming (ILP) method for this problem which is demonstrated in
Algorithm 3.
Algorithm 3. Integer linear programming method for constraint expert matching proposed in Karimzadehgan and Zhai (2012) subject to
In this algorithm, we used the same notation introduced in Table 1 and Algorithm 2 . In this algorithm, decision variable t 2 [0, k ] is an integer indicating the number of assigned reviewers of paper p outer sum).

In contrast with our proposed framework, this method cannot necessarily optimize the skill coverage of the assigned formance in comparison with our CFLA method. We use the methods proposed in Karimzadehgan et al. (2008, 2012) as our baseline algorithms and also use the same dataset to make the results comparable.

Recently, Tang et al. (2012) proposed a general framework based on the convex cost flow optimization for expert match-main concern is to find a group of experts in a social network which are able to contribute with each other easily. 5. Experiments
In this section, we present the test data and measures used for evaluating our methods. 5.1. Data set In our experiments, we use two datasets which are described bellow:
SIGIR dataset: We used the dataset introduced in Karimzadehgan et al. (2008) to evaluate our proposed methods. This dataset is used in several research papers (i.e. Karimzadehgan &amp; Zhai, 2012; Karimzadehgan et al., 2008; Tang et al., sidered as the prospective reviewers/experts. For modeling reviewers X  expertise, a profile is created for each author by tifying 25 major subtopics for these papers and then assignment of subtopics to all papers and the reviewers by a human edu/data/review.html .
 paper-reviewer dataset. We crawled multi aspect papers from PubMed
Med database are indexed based on a standard controlled vocabulary. Medical Subject Headings (MeSH) is a comprehensive dataset/ . 5.2. Evaluation measures
While the multi-aspect team formation problem can be cast as a retrieval problem, the traditional relevance-based pre-cision and recall measures cannot be directly applied to measure the matching performance, because they are unable to re-flect the coverage and confidence measures in the assigned groups. To measure the performance of our multi-aspect matching algorithms, we used the Coverage and Average Confidence measures defined in Karimzadehgan et al. (2008) .
Coverage score measures the number of different distinct topic aspects of a paper that are covered by the k assigned reviewers to that specific paper. Consider paper p j with n these reviewers: each reviewer of a paper is able to cover as many aspects as possible. The confidence of reviewer r as the fraction of aspects of paper p j that reviewer r i earlier, the Average Confidence measure is defined as follow: where k is the number of assigned reviewers and n r i indicates the number of topics/aspects of the paper that reviewer r cover. 5.3. Baseline methods
In the paper, the FLA framework of multi-aspect expert matching is compared with the methods proposed in Karimzadeh-ison of the proposed models, statistically significant improvements are measured using a Wilcoxon Signed-Rank test ( Wilcoxon, 1945 ) at the level of 0.05. 6. Experimental results In this section, an extensive set of experiments were conducted to address the following questions: UFLA approach? In Section 6.1 , we compare the performance of UFLA with various baseline algorithms proposed in
Karimzadehgan et al. (2008) . In particular, we compare two greedy approaches for expert matching namely, the Next Best ( Karimzadehgan et al., 2008 ) search and the UFLA (introduced in Section 3.3 ) strategies.
What is the impact of the building and communication cost on the coverage and confidence measures in our FLA frame-work? How good is the performance of the FLA framework for different values of parameter k ?
How good is the performance of the proposed framework for constraint expert matching problems in comparison with the heuristic methods proposed in Karimzadehgan and Zhai (2012) ? In Section 6.2 , we compare the performance of CFLA with the integer linear programming method proposed in Karimzadehgan and Zhai (2012) .

How scalable is the performance of the proposed framework for real scenarios? In Section 6.4 , we investigate the scala-bility of the proposed framework. 6.1. Implicit Aspects  X  Unconstraint Matching
In this section, we compare the UFLA method with (1) the language model retrieval model (LM), (2) Redundancy Removal fidence scores and the percentage of improvement for the UFLA method.

According to Table 3 , the performance of author topic modeling methods (i.e. Next Best and UFLA) are better than other baseline methods (except for Average confidence of Next Best on PubMed dataset) and also the coverage and especially the average confidence of the UFLA method is better than the Next Best search method.

Since the performance of the Next Best and the UFLA methods are dependent on the quality of the topic learning model, in associated with each reviewer from the golden set (i.e. in Eq. (3) , parameters p ( s tributions for each topic (i.e. the only unknown parameters in Eq. (3) are p ( w j s the new topic vectors. Table 4 indicates the coverage and average confidence for this experiment. According to Table 4 , by improving the quality of learned topics, the coverage and average confidence of both FLA and
Next Best methods is improved. However, the performance of UFLA is again better than the Next Best greedy matching. The improve the performance of matching.

To better understand the behavior of Next Best and the UFLA methods, we examine the impact of parameters r and k on performance of these methods for both datasets. As mentioned before, parameter r in Next Best method models the skill redundancy in the assigned groups and parameter k makes the balance between the building cost and commutation cost eters for the UFLA and Next Best algorithms.
 reviewer selection groups using UFLA method on SIGIR dataset. 6.2. Explicit Aspects  X  Constraint Matching
In this section, we compare our CFLA method with the baseline algorithms for constraint expert matching. The first base-line algorithm is the greedy approach proposed in Karimzadehgan and Zhai (2012) . In this method, first, the papers are to each reviewer and the number of reviewers assigned to each paper. If the review quota is reached, that reviewer is rithm 3 to match papers with reviewers. This method tries to globally maximize the number of covered aspects of the as-signed groups.

Before comparison with the baseline algorithms, we examine the effect of the building and the communication cost on the capacity of each reviewer is equal to 5. For each program committee size, we randomly select the specified number of experts from all available experts (i.e. 189 experts) and repeat each experiment 10 times and report the coverage and the average of confidence in Fig. 4 .
 scores and on the other hand, increasing parameter k (i.e. increasing the building cost and decreasing the communication cost in objective function of the CFLA) decreases the coverage score of the assigned groups. This experiment shows that by emphasizing communication cost in the objective function of the CFLA, the coverage score of assigned groups can be in-iment confirms the result of Theorems 1 and 2 mentioned in Section 3.4 . This experiment also shows that the coverage and the average confidence scores are contradicting constraints and parameter k can be used to make a trade off between these constraints. We found the same behavior on PubMed dataset and therefore we did not report the sensitivity results on Pub-
Med dataset. In experiments of this section, we use k = 0.5 to make a balance between the coverage and the average confi-dence measures.

In out first experiment, we compare the proposed CFLA method with the integer linear programming method committee sizes (i.e. number of available reviewers). Each experiment is repeated 30 times and the average of scores are the greedy approach for different sizes of the program committee for both datasets.
 addition, for all committee program sizes, the coverage score of the CFLA method is always better than the greedy and ILP less than 120 available reviewers for SIGIR dataset and less than 90 reviewers for PubMed dataset).
Table 6 indicates the average confidence score of the CFLA, ILP and the greedy approach for this experiment. According to average confidence score is improved. The performance of ILP and CFLA are almost the same and both are better than the greedy method. According to this experiment, the CFLA method can make expert groups with significantly better coverage the average confidence score is negligible.

In the next experiment, we fix the number of reviewers to 30 for SIGIR dataset and 45 for PubMed dataset, and vary the number of papers each reviewer can review while each paper is assigned to 3 reviewers. In this experiment, minimum capac-scores are shown in Fig. 6 .
As we increase the number of papers that each reviewer can review, we are also increasing the resources, and as a result, the performance of all algorithms becomes better. Also, comparing the CFLA method with the ILP and greedy approach, the performance of the CFLA method is significantly better than the greedy and ILP methods for all values of capacity of reviewers.

Table 7 indicates the average confidence scores for this experiment. The average confidence score of the CFLA and the ILP methods are almost the same but both are better than the greedy algorithm. This experiment also indicates that the CFLA tribute papers among available reviewers.

In the last experiment, we compare the performance of CFLA and ILP when very limited recourse (i.e. reviewers) are avail-able. In this experiment, for SIGIR dataset, the maximum number of reviewers is 10 for 73 papers and for PubMed dataset, the maximum number of reviewers is 22 for 231 papers. Again we randomly select reviewers and we repeat the sampling process for 30 times and get the average. Each paper gets three reviewers and the number of papers that each reviewer can each should get 44 papers. Fig. 7 indicates the coverage measure of CFLA and ILP methods. quality of matching for CFLA is always better than the ILP in terms of both the coverage and average confidence. 6.3. Implicit Aspects  X  Constraint Matching
In this section, we examine the quality of matching experts for the third problem of expert matching. In this case, the ments, we use k = 0.5 to make a balance between building and communication cost.

While the CFLA method can be directly applied to the probabilistic assignments of subtopics given by PLSA, intuitively, 10.

According to this figure, setting the cut-off equals 1 reduces the performance of both algorithms for SIGIR and PubMed for cut-off is near to the average number of required skills for each paper in the golden measure. Although, the coverage both coverage and average confidence measures for Implicit Aspect  X  Constraint Matching problem. 6.4. Scalability of FLA framework
Finally, we study the scalability of our CFLA framework. In the experiments reported so far, we have only evaluated our ference review assignments with larger number of submissions.
 for reviewers and 3 to 6 for papers). The same is done when we have 100 topics. The number of reviewers to be assigned to ers assigned to each paper and five is the capacity of each reviewer).

Fig. 9 a shows the runtime of the CFLA algorithm as the number of papers increases. The algorithm run on Intel CPU, 2.8 GHZ, with 8 GB memory. The runtime of the algorithm increases when we have a large number of papers and topics as expected. Given the computational complexity of the CFLA algorithm, this observation is intuitively expected; indeed, as we increase the number of papers, more time is needed to find the optimal assignment, because the number of variables is increased, as a result, the algorithm behaves exponentially in the number of variables. mal solution (i.e. gap = 0%) and secondly, the longer the program runs the smaller the gap becomes.
Since in most real conferences, the keyword list used for authors and reviewers usually does not have more than 50 key-
Since assignment of reviewers in a conference management system is usually not required to be run in real time, spending more time to get an optimal solution is worthwhile. Thus we can expect CFLA to be useful in a real application. 7. Conclusion a limited number of projects and the third problem is the combination of the first and the second problems. The assigned ysis is proposed in this paper to address these problems. As a case study, we consider the problem of multi-aspect review assignment which is a common task in conference and journal organizations. The optimality of the proposed method for re-generated dataset to compare the performance of the proposed framework with the state-of-the-art methods. Our experi-ments show that the FLA framework can significantly improve the performance of expert matching in terms of two perfor-mance measures.
 Acknowledgment for their helpful comments that help to improve the paper.
 References
