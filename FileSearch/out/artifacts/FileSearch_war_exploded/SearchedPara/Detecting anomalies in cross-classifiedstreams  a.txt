 REGULAR PAPER Deepak Agarwal Abstract We consider the problem of detecting anomalies in data that arise as multidimensional arrays with each dimension corresponding to the levels of a cat-egorical variable. In typical data mining applications, the number of cells in such arrays are usually large. Our primary focus is detecting anomalies by comparing information at the current time to historical data. Naive approaches advocated in the process control literature do not work well in this scenario due to the multiple testing problem X  X erforming multiple statistical tests on the same data produce excessive number of false positives. We use an empirical Bayes method which works by fitting a two-component Gaussian mixture to deviations at current time. The approach is scalable to problems that involve monitoring massive number of cells and fast enough to be potentially useful in many streaming scenarios. We show the superiority of the method relative to a naive  X  X er component error rate X  procedure through simulation. A novel feature of our technique is the ability to suppress deviations that are merely the consequence of sharp changes in the marginal distributions. This research was motivated by the need to extract critical application information and business intelligence from the daily logs that accom-pany large-scale spoken dialog systems. We illustrate our method on one such system.
 Keywords Multidimensional  X  Multiple testing  X  Hierarchical Bayesian  X  Active learning 1 Introduction Consider a computational model of streaming data where a block of records are simultaneously added to the database at regular time intervals (e.g. daily, hourly, etc.) [ 14 ]. Our focus is on detecting anomalous behavior by comparing data in the current block to some baseline model based on historic data. However, we are more interested in detecting anomalous patterns rather than in detecting unusual records. A powerful way to accomplish this is to monitor statistical measures (e.g. counts, mean, quantiles) computed for combinations of categorical attributes in the database. Considering such combinations gives rise to a multidimensional array at each time interval. Each dimension of such an array corresponds to the levels of a categorical variable. We note that the array need not necessarily be complete, i.e. only a subset of all possible cells might be of interest. A univariate measurement is attached to each cell of such an array. When the univariate cell measures are counts, such arrays are called contingency tables in statistics. Henceforth, we also refer to such arrays as cross-classified data streams. For instance, consider calls received at a call center and consider the two-dimensional array where the first dimension corresponds to the categorical variable  X  X aller intent X (reason for call) and the second dimension corresponds to the  X  X riginating location X  (state where the call originates). A call center manager is often interested in monitoring daily percentages of calls that are attached to the cells of such an array. This is an ex-ample of a two-dimensional cross-classified data stream that gets computed from call logs added to the database every day.
 of store locations for a retail enterprise. Detecting changes in cells might help for instance in efficient inventory management, provide knowledge of an emerging competitive threat. (b) Emergency room visits at several hospitals with different symptoms. The anomalies in this case might point to an adverse event like a dis-ease outbreak before it becomes an epidemic. (c) Detecting spikes in queries in several demographic segments received by a web portal like Yahoo!. This can potentially help in returning more relevant search results.
 is often crucial to monitor a large number of cells simultaneously for changes that take place relative to expected behavior. A system that can detect anomalies by comparison to historical data provides information which might lead to better planning, new business strategies and in some cases might even lead to financial benefits to corporations. However, the success of such a system critically depends on having resources to investigate the anomalies before taking action. Too many false positives would require additional resources, false negatives would defeat the purpose of building the system. Hence, there is need to have sound statistical methods that could achieve the right balance between false positives and false negatives. This is particularly important when monitoring data classified into a large number of cells due to the well-known multiple hypotheses testing problem. and data mining. The primary focus of several existing techniques is efficient processing of data to compute appropriate statistics (e.g. counts, quantiles, etc.), with change detection being done by using crude thresholds derived empirically or based on domain knowledge. For instance, Zhu and Shasha [ 18 ] describe efficient streaming algorithms in the context of multiple data streams to compute statis-tics of interest (e.g. pairwise correlations) with change being signalled using pre-specified rules. Non-parametric procedures based on Wilcoxon and Kolmogorov X  distribution of univariate data streams. In [ 17 ], the authors describe a technique to detect outliers when monitoring multiple streams by comparing current data to expected, the latter being computed using linear regression on past data. Our work, though related, has important differences. First, we are dealing with cross-classified data streams which introduce additional nuances. Second, we adjust for multiple testing which is ignored in [ 17 ].
 to adjust for sharp changes in the marginal statistics. Failure to do so may pro-duce anomalies which are direct consequences of changes in a small number of marginals. For instance, it is not desirable to produce anomalies which indicate a drop in sales volume for a large number of items in a store merely because there was a big drop in the overall sales volume due to bad weather. We accomplish this by adjusting for the marginal effects in our statistical framework.
 literature in statistics dating back to the 1950s. Broadly speaking, if multiple statis-tical tests are simultaneously performed on the same data, it tends to produce false positives even if nothing is amiss. This can be very serious in applications. Thus, if a call center manager is monitoring repair calls from different states, he might see false positives on normal days and stop using the system. Much of the early focus in multiple testing was on controlling the family wise error rates (FWER) (probability of at least one false detection). If K statistical tests are conducted simultaneously at per comparison error rate (PCER) of  X  (probability of false detection for each individual test), the FWER increases exponentially with K . Bonferroni type corrections which adjust the PCERs to  X / K achieving a FWER of  X  are generally used. However, such corrections may be unnecessarily con-servative. This is especially the case in data mining scenarios where K is large. An alternate approach have been proposed in [ 4 ] which uses shrinkage estimation in a hierarchical Bayesian framework in combination with decision theory. Later, Benjamini and Hochberg [ 16 ] proposed a method based on controlling the False Discovery Rate (FDR) (proportion of falsely detected signals) which is less strict than FWER and generally leads to gain in power compared to FWER approaches. In fact, controlling the FDR is better suited to high-dimensional problems that arise in data mining applications and has recently received a lot of attention in statistics, especially in genomics. Empirical and theoretical connections between Bayesian and FDR approaches have been studied in [ 8 , 11 ]. Another approach to tackle the curse of multiple testing is based on randomization [ 9 ] but might be computationally prohibitive in high dimensions. We take a hierarchical Bayesian approach in a decision theoretic framework similar in spirit to [ 4 ] but replace the normal prior with a two-component mixture as in [ 13 ]. We note that the multiple testing problem we consider is distinct from the traditional problem where all tests are performed on the same data; each test in our case is independent and uses its own data. The main goal in such large-scale screening problems is to provide the analyst with a set of interesting cases that are worth pursuing further. To accom-plish this, it is necessary to derive the null hypotheses empirically from the data instead of conducting significance tests that make sense for each individual case. An approach that detects deviations from a reference distribution obtained by fit-ting an appropriate statistical distribution to the histogram of observed residuals can achieve such an objective. A hierarchical Bayesian approach is particularly attractive in this context. In fact, a hierarchical Bayesian approach provides flex-ibility to account for additional features that might be present in some situations. For instance, if one of the dimensions corresponds to spatial locations, correla-tions induced due to geographic proximity are expected and could be easily ac-counted for. For a detailed introduction to hierarchical Bayesian models, we refer the reader to [ 3 ]. 1.1 Motivating application This research was motivated by the need to build a data mining tool which extracts information out of spoken dialog systems deployed at call centers. The data min-ing tool built to accomplish this is called the VoiceTone Daily News (VTDN) [ 6 ] and supplements a call center service called VoiceTone by automatically extract-ing critical service information and business intelligence from records of dialogs resulting from a customer calling an automated help desk. The Daily News uses the spoken dialog interaction logs to automatically detect interesting and unex-pected patterns and presents them in a daily web-based newsletter intended to resemble on-line news sites such as cnn.com or bbc.co.uk. The front page news items are provided with links to precomputed static plots and a drill down capabil-ity, powered by a query engine and equipped with dynamic visualization tools that enables a user to explore relevant data pertaining to news items in great detail. The data mining task in this application involves three challenging steps, namely, (a) extraction of relevant features from dialogues, (b) detect changes in these features, and (c) provide a flexible framework to explore the detected changes. Our focus in this paper is on task (b), for complete details on (a) and (c) we refer the reader to [ 6 ].
 a framework to detect anomalies in cross-classified data streams with potentially large number of cells. We correct for multiple testing using a hierarchical Bayesian model and suppress redundant alerts caused due to changes in the marginal dis-tributions. We empirically illustrate the superiority of our method by comparison to a PCER method and illustrate it on a novel application that arises in speech mining.
 problem followed by a brief description of the hierarchical Bayesian procedure called hbmix . Sections 3 and 4 describe our data in the context of the VTDN application. Section 5 compares hbmix to a PCER method through simulation fol-lowed by an illustration of hbmix on actual data in Sect. 6 . We end in Sect. 7 with discussion and scope for future work. 2 Theoretical framework For ease of exposition, we assume the multidimensional array consists of two cat-egorical variables with I and J levels respectively and note that generalization to higher dimensions is similar. In our discussion, we assume the array is complete. In practice, this is usually not the case but the theory still applies. Let the suffix ijt refer to the i th and j th levels of the first and second categorical variables re-spectively at time t .Let y ijt denote the observed value which is assumed to follow a Gaussian distribution. Often, some transformation of the original data might be needed to ensure this is approximately true. For instance, if we observe counts, a square root transformation is adequate, for proportions arc sine ensures approxi-mate normality. In general, the Box X  X ox transformation (( y + m ) p  X  1 )/ p with parameters m and p chosen to  X  X tabilize X  variance if it depends on the mean is recommended.
 changes in the marginal means. We show the difference between adjusting and not adjusting the margins by using a toy example. Consider a 2  X  2table,the levels of the row factor being A , B and the column factor being a , b respec-tively. We denote the four cell entries corresponding to ( Aa , Ab , Ba , Bb ) by served values be (25, 25, 75, 75). Then the raw changes are (  X  25,  X  25, 25, 25) which are all large. The deviations after adjusting for the changes in the row and columns means are (0, 0, 0, 0) producing no anomalies. Note that the significant values in the non-adjusted changes can be ascribed to a drop in the first row mean and a rise in the second row mean. Hence, non-adjusted cell changes contain redundant information. In such situations, adjusting for margins is desirable.
 explanation of change in all situations. For instance, consider a second scenario where the observed values are (50, 0, 50, 100). The raw and adjusted changes are (0,  X  50, 0, 50) and (25,  X  25,  X  25, 25), respectively. The raw changes in this case produce two alerts which pinpoint the culprit cells that caused deviations in the row means, the adjusted changes would alert all four cell entries. To summarize, adjusting the margins work well when changes in the marginal means can be at-tributed to some common cause affecting a large proportion of cells associated with the margins. Also, one byproduct is the automatic adjustment of seasonal ef-fects, holiday effects, etc., that affect the marginals, commonplace in applications. However, if the marginal drops/spikes could be attributed to a few specific cells and the goal is to find them, the unadjusted version is suitable. In our application, we track changes in the margins separately (using simple process control tech-niques) and run both adjusted and unadjusted versions but are careful in interpret-ing the results. In fact, the adjusted version detects changes in interactions among the levels of categorical variables which might be the focus of several applica-tions. For instance, in the emergency room example it is important to distinguish an anthrax attack from the onset of flu season. Since an anthrax attack is expected to be localized initially, it might be easier to identify the few culprit hospitals by adjusting for margins. Also, in higher dimensions one might want to adjust for higher order margins, which is routine in our framework.
 detected by comparing the observed values y ijt  X  X  with the corresponding posterior predictive distributions (expected distribution of data at time t based on historic data until t  X  1) which in our setup are Gaussian with means  X  ijt = E ( y ijt | H t  X  1 ) compute the posterior predictive distributions are discussed in Sect. 2.1 . variable X has a univariate normal distribution with mean m and variance  X  2 ). Here, u ijt represents the adjustment model and depends on an unknown parameter vector  X  t which is estimated from the data. The adjustment model encapsulates additional information known to the user at time t (but not incorporated in the baseline model). Our goal is to test for zero values of ijt  X  X . For marginal ad-justment, u ijt = u t + ur it + uc jt ( u t , ur it and uc jt are overall, row and column effects, respectively at t ). Here,  X  t = ( u t , ur t , uc t ) , which is estimated by us-ing the best linear unbiased estimators of the us based on e ijt = y ijt  X   X  ijt .Note that u ijt = 0 corresponds to the canonical no-adjustment model. Other adjustment models might be useful in practice. For instance, if a majority of streams being monitored experience an increase (decrease) due to some external factor (e.g. all items in a store register a drop in sales due to bad weather), u ijt =  X  t is a reason-able choice to adjust for this global change. Again, there may be situations where a certain subset of streams are affected by some common factor at time t .For instance, in monitoring emergency room visits, zip codes in a small geographic pocket may see a drop in visits due to doctors X  going on strike at a nearby hos-all zips affected by the strike and 0 otherwise) is a reasonable adjustment model. In general, the adjustment model provides a mathematical framework to adjust for information known to the analyst at time t and not incorporated in the baseline model. We believe this provides a useful mechanism to suppress anomalies that are not of interest to the analyst. In fact, with a feedback mechanism that provides labels to detected anomalies, one can potentially use this framework to refine the initial model and is similar in spirit to an active learning framework. More for-want to test multiple hypotheses ijt = 0( i = 1 ,..., I , j = 1 ,..., J ). The central idea of the hierarchical Bayesian method hbmix is to assume ijt  X  X  are random samples from some distribution G t . The form of G t may be known but depend on unknown parameters. For instance, author in [ 7 ] assumes G to be N ( X  0 t , X  2 t ) and discusses the important problem of eliciting prior probabilites for the unknown parameters. In [ 12 ], a non-parametric approach which assigns a Dirichlet process prior to G t is advocated but not pursued here due to compu-tational complexity. Following [ 13 ]and[ 8 ], we take a semi-parametric approach which assumes G t to be a mixture P t 1 ( = 0 ) + ( 1  X  P t ) N ( 0 , X  2 t ) ,i.e.apro-portion P t of cells does not change at time t while the remainder are drawn from a normal distribution. We assume a log-logistic prior for  X  2 t centered at the har-t = 0, we assume a uniform prior for P ) . The joint marginal likelihood of  X  component mixture densities and from Bayes rule the posterior distribution of ( P distribution of ijt conditional on ( P t , X  2 t ) is degenerate at 0 with probability Q ijt and with probability 1  X  Q ijt it follows N ( b ijt ,v 2 ijt ) where N ( x ; m , s 2 ) denotes density at x for a normal distribution with mean m and vari-ance s 2 . An empirical Bayes approach makes inference about ijt  X  X  by using plug-in estimates of the hyperparameters ( P t , X  2 t ) which are obtained as follows. Com-values of K , we use a data squashing technique [ 15 ]) and define the estimates as (  X 
P in the interval [ 0 . 95 , 0 . 99 ] . At time t = 0,  X  = 1. This exponential smoothing al-lows hyperparameters to evolve smoothly over time. In a fully Bayesian approach, inference is obtained by numerically integrating with respect to the posterior of ( P tribution of ijt depends directly on  X  ijt and indirectly on the other  X   X  X  through the posterior of the hyperparameters. Generally, such  X  X orrowing of strength X  makes the posterior means of ijt  X  X  regress or  X  X hrink X  toward each other and automati-cally builds in penalty for conducting multiple tests.
 Q (  X  Thus, the cell penalty increases monotonically with predictive variance. Also, the overall penalty of the procedure at time t depends on the hyperparameters which are estimated from data. In fact, replacing  X  2 ijt  X  X  by their harmonic mean  X  2 t in ( 2 ) gives us a constant A t which provides a good measure of the global penalty imposed by hbmix at time t . However, the loss assigned to false negatives by ( 2 ) does not depend on the magnitude of deviation of  X  X  from zero. Motivated by [ 4 ] and [ 13 ], we use a loss function where p  X  0, c ( &gt; 0) is a parameter which represents the cost of a false neg-ative relative to a false positive, C denotes change and N denotes no change. With p = 0, we recover ( 2 )and p = 1 gives us the loss function in [ 13 ]. In fact, p = 1 is a sensible choice for the VTDN application where missing a more important news item should incur a greater loss. In our application we assume c = 1 but remark other choices elicitated using domain knowledge are encour-aged. Having defined the loss function, the optimal action (called the Bayes rule) minimizes the posterior expected loss of . In our setup, we declare a change if E ( L ( C ,))  X  E ( L ( N ,)) &lt; 0 noting that the expression is a known function of hyperparameters and could be computed either by using plug-in estimates or numerical integration. 2.1 Calculating posterior predictive means and variances Two popular approaches used to capture history H t are sliding window and expo-nential smoothing . In the former, a window size w is fixed a priori and the distri-bution at t is assumed to depend only on data in the window [ t  X  1  X  w, t  X  1 ] . Extensive research on fast computational approaches to maintain summary statis-tics under this model have been done (see [ 1 ] for an overview). In an exponential smoothing model, a decay parameter  X   X  ( 0 , 1 ) is used to downweight historic data with the weights dropping exponentially in the past.
 predictive means and variances could be used to obtain  X  ijt  X  X  and  X  2 ijt  X  X . Also, to be useful in streaming scenarios, the chosen model should easily adapt to new data.
 dow to capture H t . We assume the cells are uncorrelated and for the ij th cell, y Then, the posterior predictive mean  X  ijt is the sample mean of y ijk  X  X  and the poste-by its estimator s 2 ij , the sample variance of y ijk  X  X . In order to adjust for seasonal effects, a separate sliding window is maintained for each season. 3 VoiceTone daily news We illustrate and evaluate hbmix on a customer care (BCC) application supported by VoiceTone (client X  X  identity not disclosed due to reasons of confidentiality). Before we describe the data, a high-level description of the features extracted are given below (see [ 6 ] for complete details).
 turns. A turn consists of a system prompt, the user response as recognized by the system, and any records associated with the system X  X  processing of that response. Each turn is mapped to one of a set of call types using BoosTexter X  X  member of the AdaBoost family of large-margin classifiers. A dialog ends when a goal is achieved by completing a transaction, for instance, or routing the user to an appropriate destination.
 number for the call ( ANI ), the number of turns in a dialog ( NTURNS ), the length of the call ( DURATION ), any final routing destination the call gets routed to ( RD ) and the final actionable call type ( FA C T ). This is the last call type the classifier obtained in the course of the system X  X  dialog with the user before routing. FACT and RD are primary features tracked by the  X  X aily News X  alert system. The FACT is our closest approximation to the caller X  X  intent. This is of particular interest to VoiceTone X  X  clients (banks, pharmacies, etc.), who want to know what their cus-tomers are calling about and how that is changing. 4 Data description for business customer care Due to proprietary nature of the data, all dates were translated by a fixed number of days, i.e. acutual date = date used in the analysis + x ,where x is not revealed. The news page for this application is updated on a daily basis. The system handles approximately 150 K  X 20 K care calls per day. Features tracked by hbmix include average call duration cross-classified by FACT  X  STATE (STATE where the calls originate are derived using ANI), RD  X  STATE, FACT  X  Hour-of-day, RD  X  STATE. The system is flexible enough to accept any new combination of variables to track. We present an analysis that tracks proportions for FACT  X  STATE. t , we only include cells that have occurred at least once in the historic window of length w which, for a window size of 10 days (we choose this by using a predictive loss criteria on initial training data) results in about 2900 categories being monitored on average. The system went live last week of January 2004. We use data ending April 2004 as our training set to choose an appropriate window size and to choose parameters for a simulation experiment discussed later. Finally, we run hbmix on data from May 2004 through January 2005.
 added to the database every day. For the ij th cell, p ij = number of calls in ij th cell/total number of calls. This multinomial structure induces negative correla-tions among cells. Under a multinomial model, the negative correlation between any pair of cells is the geometric mean of their odds ratio. This is high only if both odds ratio are large, i.e. if we have several big categories. From the train-ing data we compute the 95th percentile of the distribution of p  X  X  for each cell. The top few cells have values 0 . 07 , 0 . 05 , 0 . 04 , 0 . 03, which mean the correlation is approximately bounded below by  X  0 . 06. To ensure symmetry and approximate normality, we compute the score y ij = sin  X  1 ( p ij )/ ij sin  X  1 ( p ij ) with the normalization meant to preserve the multinomial structure. The top few cells after transformation have 95th percentile values of 0 . 012 , 0 . 009 , 0 . 009 , 0 . 008, which give a lower correlation bound of about  X  0 . 01. Hence, the assumption of cell independence seems reasonable in this case. 5 Simulation to evaluate hbmix Here, our goal is to compare the performance of hbmix with a naive PCER ap-proach for the BCC application. We take a simulation-based approach, i.e. we generate data whose statistical properties are close to that of our actual data dur-ing the training period, artificially inject anomalies and then score the two methods under consideration. simulate K streams ( K is the number of cells in our stream, we ignore the issue of adjusting for margins since it is not relevant for this experiment) at w + 1 time points introducing anomalies only at the last time point and compare the FDR and false negative rates based on several repititions of the experiment. Since the difference between FDR and false negative rate is not symmetric, we tweak the value of M 1 so that the false negative rate for PCER matches the one obtained for hbmix with c = 1. The tweaking is done using a bisection algorithm due to the monotonic dependence of false negative rate on M 1 . Simulation details are as follows:  X  Generate (  X   X  The cell variances  X  2  X  For each i ,simulate w + 1 observations as iid N ( X   X  At time w + 1, randomly select 100 streams, add  X  X nomalies X  generated from  X  Detect anomalies at w + 1using hbmix (we choose w = 10, p = 1 , c = 1)  X  The above steps are repeated 100 times, results are reported in Table 1 . ence increases with K . Also, the difference is statistically significant indicated by the significant t -statistics ( p -values were all close to 0.00) obtained using a two-sample t -test. For hbmix , we obtained similar results for both the empirical Bayes and full Bayes methods. Computational time for the full Bayes method is roughly 10 times slower (see Table 2 ) and hence we recommend empirical Bayes if the main goal is inference on  X  X . 6Dataanalysis In this section, we present results of our analyses on customer care from May 2004 to January 2005 for the combination FACT  X  State. We apply hbmix both adjusting and not adjusting for the marginal changes (call them adjusted hbmix and non-adjusted hbmix , respectively). In Fig. 1 , the top panel shows time series plots of A t for both versions of hbmix (horizontal gray line shows the constant threshold of three for PCER). As noted earlier, A t provides an estimate of the penalty built into hbmix at each time interval. The bottom panel shows the number of alerts obtained using the three procedures. The figure provides insights into the working of adjusted and non-adjusted hbmix relative to the PCER method. Large values of A t correspond to periods when the system is relatively stable producing a few alerts (e.g. mid-June through mid-July). In general, the PCER produces more alerts compared to hbmix . On a few days (the ones marked with dotted lines on the bottom panel of Fig. 1 ), adjusted hbmix drastically cuts down on the number of alerts relative to non-adjusted hbmix . These are days when a system failure caused a big increase in HANGUP rate triggering several related anomalies. The adjusted version always gives smaller number of alerts compared to PCER and it never produces more than a couple of extra alerts compared to the unadjusted version. In fact, there are about 30 days where the adjusted version produces one or two alerts when the unadjusted version produces none. These represent subtle changes in interactions. To illustrate the differences between adjusted and unadjusted hbmix , we investigate the alerts obtained on September 3 (we had other choices as well but believe this is sufficient to explain our ideas).
 do not point to anything for FACT, we notice a couple of spikes in the STATE vari-able for Maryland (3.2 X 7.4%) and Washington, DC (0.6 X 2.1%). There are eight alerts common to both versions of hbmix . Interestingly, these alerts are spatially clustered, concentrated to states that are geographically close to each other. There is one alert (an increase) that appear only with the unadjusted hbmix , namely, about Indicate (Service Line) in Maryland. One alert indicating increase in Ask (Cancel) in Connecticut is unique to the adjusted version. Figure 2 shows the dif-ference in the Indicate (Service Line) alert in Maryland using the adjusted and non-adjusted hbmix . The broken lines are the appropriate control limits about the historic mean. (For the marginals, the control limits are computed using PCER.) It provides an illustrative example of how the adjusted version works, the spike in Maryland when adjusted for reduce severity and the alert is dropped. Fig-ure 3 shows an example where adjusted hbmix produce the alert missed by the unadjusted one on September 3. Although marginal changes are well within their respective control limits, drops in Ask (Cancel) and Connecticut increase severity of the alert with the adjusted version.
 provide a useful metric to gauge its performance. For instance, in the current ap-plication, the ranks might determine the importance of news items and hence their positions on the news page. In fact, one can compare two detectors by looking at the discrepancy between the distribution of their ranks. If the distributions are similar, the two detectors are exchangeable (for an appropriate choice of thresh-olds). We compare the scores obtained by plugging p = 0 , 1 (call it p 0 , p 1) in ( 3 ) and the PCER (or threshold) based method. The correlation p 1  X  thresh was quite similar to p 0  X  thresh .InFig. 4 , we show results for unadjusted hbmix for two cor-relations, namely, p 1  X  p 0and p 1  X  thresh . (Corresponding figures for adjusted version omitted because they were similar.) We use two correlation measures, the Kendall X  X  rank correlation coefficient and the top-down correlation coefficient [ 10 ]. The latter measures correlation among the top-ranked items downweight-ing the low-ranked items by replacing the original ranks with Savage scores. As expected, p 1  X  p 0 is stronger than p 1  X  thresh . In fact, Kendall shows they are al-most indistinguishable. The top-down is generally smaller than Kendall and seems to peak (for p 1  X  p 0) during periods when the system was stable. Interestingly, Kendall X  X  correlation was close to 1 for both p 1  X  p 0and p 1  X  thresh during the short period spanning end of October to beginning of November when there was a system failure and the unadjusted version of hbmix produced a lot of alerts. To summarize, the threshold-based method produced rankings quite different from hbmix .For hbmix , the choice of loss function does not affect the overall rankings by much but does cause flip-flop among the top-ranked items, especially when the system is not completely stable. 7 Discussion We proposed a framework for detecting anomalies in massive cross-classified data streams. We described a method to reduce redundancy by adjusting for marginal changes. We solve the multiple testing problem using a hierarchical Bayesian model within a decision theoretic framework and prove the superiority of hbmix to a naive PCER method through simulation. We illustrate hbmix on a new speech mining application.
 parameter exponential family, which includes the Poisson, Binomial, Gamma, Gaussian as special cases. We are also working on methods to combine adjusted and unadjusted hbmix to automatically produce a parsimonious explanation of anomalies. For instance, in 2D, this could be done by testing for mean shifts in the distribution of individual row and column vectors using non-parametric quantile-based tests that are robust to outliers. Rows and columns that are subject to shifts relative to historic behavior would be the only ones that get adjusted. References Author Biography
