 We demonstrate the YaLi browser plug-in which discovers named entities in Web pages and provides background knowledge about them. The plug-in is implemented with two purposes. From a user perspective, it enriches the browsing experience with entities, help-ing users with their information needs. From the research perspec-tive, we aim to improve the methods that are used for named entity recognition and disambiguation (NERD) by leveraging the plug-in as an implicit crowdsourcing platform. YaLi tracks the system X  X  errors and the users X  corrections, and also gathers implicit training data for improving NERD accuracy.
 H.4 [ Information Systems Applications ]: Miscellaneous Crowdsourcing, Browser Plug-in, Named Entity Disambiguation, Named Entity Recognition
When users browse news or social media on the Web, often names of entities (e.g., people, places, companies, movies, songs, etc.) catch their attention, and users would then like to obtain ad-ditional information on these entities. Some news sites satisfy this need to some extent, by providing links to Wikipedia, but this fea-tures is limited to very prominent entities or relies on manually cu-rated links. The work presented here generates such links on the fly and fully automatically, whenever a user highlights a noun phrase in a Web page. The challenge that we face here is the ambiguity of names and phrases. For example,  X  X ashington X  most prominently refers to the capital of the USA, but it can have other meanings de-pending on context: the first American president, a film name, or the name of a state. To address this problem, we propose a browser plug-in that calls a server for named entity recognition and disam-biguation (NERD) in a real-time manner and transparently for the user. In this way we achieve two goals: 1) From a user perspective, we enrich the browsing experience by providing background knowledge about entities (from Wikipedi-a or knowledge bases) on demand in a pop-up frame.
 2) From a research perspective, we collect user feedbacks, as a form of implicit, user-transparent crowdsourcing, thus gather-ing statistics on name-entity pairs and implicit training data for improving NERD accuracy.

Crowdsourcing has become popular for many tasks such as im-age tagging, language translation, or recommendations. Platform-s such as Amazon Mechanical Turk ( www.mturk.com ) and Crowd-Flower ( crowdflower.com ) allow applications to submit micro-tasks to a worker community and perform the matchmaking between tasks and workers based on payment and profiles. Research projects such as [2, 5, 6] have leveraged crowdsourcing for tasks like entity co-reference resolution, image search, etc. However, users con-tribute only because they are paid and the topics of the input tasks are given to them by the application. In contrast, our approach adopts the rationale that users should voluntarily and transparently engage in NERD tasks, by embedding these tasks into the users X  daily browsing activities. We achieve this by the YaLi plug-in. No extra burden is imposed on users, and YaLi works on arbitrary Web pages so that users can freely choose their own topics of interest.
Contributions. We present the YaLi system which transparently harnesses NERD methods to help users with richer interpretation of Web pages. YaLi gathers user feedback to obtain implicitly la-beled data for training and further improvement of NERD perfor-mance. Unlike explicit crowdsourcing, YaLi does not impose any extra burdens on users and comes with the incentive of enhancing users X  browsing experience and their personal knowledge.
Figure 1 depicts the architecture of YaLi. YaLi integrates itself into the Google Chrome browser, listening for click or highlight events. When a user highlights a span of text, YaLi transfers both the highlighted text (mention) and its context to a NERD service, which returns the most likely entities for the highlighted text. The best entity candidates are shown in a pop-up frame next to the high-lighted text, including short descriptions of the entities. Users can also save interesting entities in a notebook for later reference. To do this, they have to select the correct entity in the candidate list, thus implicitly providing training data. YaLi uses AIDA [3] for entity disambiguation, which uses the Stanford NER Tagger [1] for mention recognition. AIDA uses the YAGO ( yago-knowledge.org ) knowledge base as a catalog of en-tities and their surface names, and returns Wikipedia identifiers for entities. When the highlighted phrase cannot be disambiguated by AIDA, YaLi tries to map it to Wikipedia directly by string match-ing. This is useful for new entities that are not registered in AI-DA but already have a Wikipedia article. If there is no match in Wikipedia either, the background server for YaLi remembers the highlighted phrase as a potentially new entity or as a new name for an existing entity. As evidence is collected from several users, this information is used to extend the back-end knowledge base.
The user feedback and implicit training data that YaLi gathers can be used for strengthening NERD methods in a number of ways.
Improving NER. The text spans highlighted by users are good indicators of entity mentions and also reflect the evolving interests of user communities. The spans can be compared to the mentions found by the Stanford NER tagger [1] employed in AIDA. Record-ing the cases when the NER tagger failed is valuable data for future re-training and improvement of the NER tagger. It is also valu-able input for extending the names dictionary that AIDA maintains. For example, phrases about entity roles such as  X  X he first American president X  (for George Washington) or  X  X he tenor sax player X  (in the context of a jazz band) can be easily picked up, and possibly added to the corresponding entities.

Improving statistics on name-entity pairs. Most NED meth-ods make uses, among other assets, of a prominence-based prior probability that a certain noun phrase denotes a specific entity [4]. YaLi can improve this prior by observing users X  click frequencies. An example is that  X  X ashington X  more frequently denotes Wash-ington DC, and less frequently the US state of Washington. More-over, as the entity associations in the user community change over time, it is crucial to estimate the prior in a dynamic manner rather than solely relying on static information such as link anchor texts in a Wikipedia snapshot. For example,  X  X andy X  most likely referred to the singer  X  X andy Denny X  until last winter, when  X  X urricane Sandy X  occurred. YaLi can track such trends and continuously im-prove the statistics for the NED prior.

Improving NED. YaLi increasingly collects labeled mention-entity pairs from the users X  browsing activities. This data can be used for re-training the parameters of NED methods. A concrete example are the various hyper-parameters (coefficients) that AIDA uses for combining a prominence-based prior, context similarity measures, and the coherence (semantic relatedness) among entities.
Providing ground-truth for NERD evaluation. YaLi contin-uously gathers ground-truth information on mention-entity pairs from the users X  clicks. This can serve as a growing basis for sys-tematically evaluating and comparing different NERD methods and systems, going way beyond the currently used benchmarks (e.g., the CoNLL X 03 corpus used in [3]). Figure 2 shows several screenshots to illustrate the demo.
Entity information. Highlighting a span of text triggers a small icon to be displayed next to it. To avoid the situation that users highlight the text casually, the icon will disappear if users continue browsing without clicking it. Once the user clicks the icon, a pop-up frame is shown and three situations are handled as shown in the No. 1 -3 screenshots respectively: 1) If the highlighted mention is disambiguated by AIDA, a list of entity candidates is returned. The best entity candidate is shown at the top, together with a short description and an image. For all candidates, the corresponding Wikipedia link is displayed and the first sentence from the Wikipedia page is provided as hovering text. If the user wants to save an entity for later refer-ence, she can choose the candidate she believes is correct. She can also click on  X  X one of the above is correct X . 2) If the disambiguation of AIDA fails but there is a matched entity in Wikipedia, that information is shown and the user can save the entry if she deems it correct. 3) When both of the above cases fail, a new entity is recorded into the notebook by the user.

Entity notebook. As shown in the No. 5 screenshot, a personal notebook is made available by YaLi, to edit or search the recorded entities. The first row shows the feedback that the disambigua-tion failed; the second row shows the case where AIDA failed but a match was found in Wikipedia. The third row records the user feedback about the mention  X  X ashington X  (see the No. 4 screen-shot). The generated best entity by AIDA is U.S. state but the user corrects it by choosing George Washington.

