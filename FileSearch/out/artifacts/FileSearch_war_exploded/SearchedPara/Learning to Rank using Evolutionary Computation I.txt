 Nowadays ranking function discovery approaches using Evo-lutionary Computation (EC), especially Genetic Program-ming (GP), have become an important branch in the Learn-ing to Rank for Information Retrieval (LR4IR) field. In-spired by the GP based learning to rank approaches, we pro-vide a series of generalized definitions and a common frame-work for the application of EC in learning to rank research. Besides, according to the introduced framework, we propose RankIP, a ranking function discovery approach using Im-mune Programming (IP). Experimental results demonstrate that RankIP evidently outperforms the baselines.

In addition, we study the differences between IP and GP in theory and experiments. Results show that IP is more suitable for LR4IR due to its high diversity.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Retrieval models General Terms: Algorithms, Experimentation, Theory. Keywords: Information retrieval, Page ranking, Learning to Rank, Immune programming, Evolutionary computation.
The performance of a search engine is mainly evaluated by the accuracy of its ranking results. As a matter of fact, the ranking problem is one of the most important issue in the Information Retrieval (IR) field.

In this area, the machine learning techniques, that is,  X  X earning to rank X , are becoming more widely used for the ranking problem of IR. Recently GP-based ranking function discovery approaches have become an important branch in learning to rank research, such as [3] and [9].

Meanwhile, many other EC related algorithms have been growing interest in recent years, for example, Immune Pro-gramming (IP) [7]. There are quite substantial similarities in these EC algorithms. Therefore, it would be very beneficial to the use of EC in the IR field, especially for the learning to rank problem, if a common framework of ranking function discovery using EC could be introduced.

Thus we provide a series of generic definitions, together with a common framework, for EC based learning to rank. Furthermore, RankIP, an approach based on IP for the dis-covery of the effective page ranking function, is proposed.
Experiments indicate that the use of RankIP leads to ef-fective ranking functions that significantly outperform the baselines, including Ranking SVM [5], RankBoost [4] and BM25 [8].

In this paper we also theoretically study the dissimilarities between IP and GP. In order to validate these arguments, we carry out some experiments. The results show that under the same conditions IP gains the advantage of GP.
For the ranking problem, a document d can be expressed by a series of feature values, formally, d = &lt;f 1 ,f 2 The problem of information retrieval can be formalized as follows. For a query q and a document collection D ,the optimal retrieval system should return a ranking that orders the documents in D according to their relevance to q .
Let Q be a query set, D be a document set, N d be a subset of the natural number set N . Given a query, the relevance judgement for a document is defined as a function rel ( Q ): D X  N d .

In the training phase, a ranking function should be gen-erated, which is supposed to associate a real number with a query and a document as their degree of relevance. For-mally, it is defined as rf ( Q ): D X  R . By estimating the ranking function values, the documents can be ordered. The order function is defined as o ( rf ( Q )) : D X  N o ,where N o = { 1 , 2 ,  X  X  X  , |D|} .Notethat o ( rf ( Q )) is an one-to-one function. Let o ( rf ( Q ))  X  1 : N o  X  X  be the inverse func-tion of o ( rf ( Q )). Obviously, o ( rf ( Q ))  X  1 is an one-to-one function as well. The following formula should be satisfied.
The optimum ranking function rf  X  canbeusedtoobtain the optimum document order, which satisfies Formula 2.  X  q  X  X  ,m,n  X  (1 .. |D| ): n&lt;m  X 
Let T be the training data set, R F be the ranking function set. Thetaskoflearningtorankistotrainaproperranking function rf which can get close to rf  X  in order to obtain a effective document order o . Thus the learning to rank algorithm can be regarded as a ranking function discovery process f : T X  X  F .
In EC based ranking function discovery algorithms, an individual expresses a ranking function, which is actually a composite function composed of some basic functions and terminals. Let P be the individual set. Given a training data set T , the fitness of a certain individual can be expressed as a real number. Thus the fitness function for the individuals is defined as F ( T ): P X  R .
 Let P g denotes the population at generation g . Obviously, P g  X  X  .Let P S g be a subpopulation of P g .From P S g a selecting operator s : P |P S g |  X  X   X  selects  X   X |P S g | viduals for variation that show a better fitness than others.
An evolutionary operator v : P  X   X  X   X  creates  X  off-springs out of the  X  selected individuals from population P . These generated individuals become members of pop-ulation P g +1 . In this way, population P g +1 can be fully generated. All of the evolutionary operators must guaran-tee that no syntactically incorrect programs are generated during the evolution process.
First of all, according to the formal definitions in Sec-tion 2.1, we think learning to rank is an optimization prob-lem essentially. That is, given a query set and correspond-ing document sets, what we try to find out is an optimum ranking function, which can rank the documents reasonably. In doing this, the loss function always plays a critical role. Currently, most of the learning to rank approaches are de-signed to optimize loss functions loosely related to the IR performance measures. By contrary, EC based approaches can optimize solutions by evaluating these IR performance measures directly in order to obtain a good ranking function.
Besides, according to the Darwin X  X  theory of evolution, there is no central organ controlling the evolutionary pro-cess in the nature. In this sense, it is easy to implement the EC algorithms as distributed ones, and they can be carried out on different computers distributively. For example, Dis-tributed Genetic Programming [1] have been proposed and applied in various fields. It is indeed an ideal character, for it allow to reduce the training time by using more distributed computers.
In general, three types of the data collections: training, validation and test, are adopted. The training data set T used to generate a series of candidate solutions B using EC algorithm, the validation set V helps in choosing good solu-tions that are not over-specialized for the training queries, and the test set E is the estimation data set for the rank-ing functions generated by the algorithm. The EC based Algorithm is given as Algorithm 1.

Note that the function getCandidates can be implemented in many ways. For example, Fan et al. [3] think of the best individuals per generations as the candidates. Besides, it is also rational to regard individuals in the last generation as the candidate solutions. In our experiments, we employ the same technology as Fan X  X .

Thus, once the function generateN extGeneration is im-plemented according to certain evolutionary principle, a com-plete algorithm is designed.
 Algorithm 1 EC Based Learning to Rank Framework Input: the training, validation and test sets: T , V and E Output: the best individual bestInd .
 Begin End
In Algorithm 1, the features abstracted in LETOR[6] are mainly adopted as the terminals. In addition, the constant set are also involved, including 0.1, 0.2, ..., 0.9, 1, 2, ..., 10.
In addition, we adapt 10 functions: + ,  X  ,  X  ,  X  ,min,max , sin, cos, log and sqrt . Since some functions need protected parameters, we design two protected mechanisms for the protected parameter x :(1)If x&lt; 0, then x := | x | .(2)If x =0,then x :=  X  ,where  X  is a real number close to zero. In our experiments  X  =0 . 000001.
EC-based learning to rank approaches usually regard the evaluation measures and their varieties as the fitness func-tions directly. In our experiments, we adopt three mea-sures: precision at position n ( P @ n ), Mean Average Pre-cision ( MAP ) and Normalized Discount Cumulative Gain (NDCG) at position n ( NDCG @ n ).

Note that P @ n and MAP can only handle cases with binary judgment:  X  X elevant X  or  X  X rrelevant X , while NDCG @ n can deal with multiple levels of relevance judgments.
As mentioned before, a validation set is used to help in choosing good solutions that are not over-specialized for the training queries. In [2], AV G  X  and SUM  X  formulae are mentioned.

Since the size of the training data collection is usually far greater than that of the validation data set, we do not think it appropriate to assume that the training and validation re-sults have the same weights, as shown in AV G  X  and SUM  X  According to our experiments, we propose two new similar methods:  X  -SUM  X  and  X  -SUM  X  2 expressed as Equation (3) and (4) respectively. argmax
The values of  X  and  X  are according to the size of training dataset T and validation dataset V respectively, that is,
Where both k and  X  are constants. The only difference between Equation (3) and (4) is that the former adopts stan-dard deviation  X  while the latter employs variance  X  2 .In our experiments, we let k =2and  X  =0 . 5.
IP is an extension of immune algorithms, particularly the clonal selection algorithm, inspired by the biological immune systems or their principles and mechanisms. Due to the similar structures between IP and traditional EC algorithms, nowadays it is regarded as a branch of EC. Algorithm 2 Immune Programming Operations Input: the replacement probability P r Output: the population at the next generation N Begin End
IP algorithm includes three immune operators: replace-ment, cloning and hypermutation. As the Algorithm 2 ex-pressed, firstly the replacement operator should execute with certain probability P r . If a new antibody cannot be gener-ated owing to the parameter P r , a high affinity antibody i is selected and considered for cloning with the probability P If the high-affinity antibody i cannot be cloned due to the parameter P c , it is submitted to the process of hypermuta-tion.

Thus, RankIP can be constructed directly by implement-ing the function generateN extGeneration in Algorithm 1 as Algorithm 2. Indeed, since both IP and GP are important varieties of EC, they have similar characters inevitably. However, there are some distinct differences: Table 1: Control Parameters for OHSUMED and TREC Data Collections in RankIP Population Size 500 100 100 Max Generation 100 100 100
Antibody Depth 6 8 8 Selecting Formula  X  -SUM  X  2  X  -SUM  X   X  -SUM  X  Diversity. In IP, the mutation rates are much higher. As Operator Selection. GP-based systems choose an alter-
Since the affinity function value had better be well dis-tributed over the range of number set (0 .. 1), we have to design a mapping function to transform the evaluation mea-sures, such as MAP , P @ n and NDCG @ n ,into(0 .. 1). This function value is considered as the affinity values for the antibodies. In our scheme, f ( x ) is a logarithm function, expressed formally as follows:
LETOR[6] data sets are adopted in our experiments. We compare the ranking accuracies of RankIP with those of three baseline methods: Ranking SVM, RankBoost and BM25. The ranking performances of both Ranking SVM and Rank-Boost are evaluated and reported in [6]. Table 1 shows the control parameters in RankIP.

We demonstrate the ranking performances of RankIP, Rank-ing SVM, RankBoost and BM25 in Table 2 and Figure 1. For OHSUMED data set we evaluate the measure MAP and NDCG @1 X 10 for comparison, while for TREC data set only MAP is employed. We can see that RankIP outperforms the three baseline methods in terms of almost all measures. Ranking SVM 0.4469 0.2564 0.3505 RankBoost 0.4403 0.2125 0.3835 Especially for NDCG @1, RankIP achieves more than 12% relative improvement.
Musilek et al. [7] demonstrate that, in their experiments, the convergence of IP is superior to GP evidently. However, their problem differs quite a bi t from learning t orank. Then, for our problem, is IP still preferable? We design some experiments to validate this assertion. The data collections are OHSUMED and TREC 2003 in LETOR 2.0. In GP, the probabilities of crossover, cloning and mutation are 0.5, 0.2 and 0.3 respectively. The results can be seen in Table 3. For OHSUMED data set, both IP and GP are effective enough. Actually, performances of all of the learning to rank approaches, including RankBoost, etc., improve slightly. We speculate OHSUMED is much easier to rank, thus even the traditional methods such as BM25 can provide a satisfactory result. However, for TREC 2003, IP receives a good estimate while GP is slightly better than BM25. We believe it is due to the low diversity of GP, though mutation probability 0.3 is fairly high.
 From Figure 2 we can conclude that in the training phase Table 3: Ranking accuracies in terms of MAP for using IP and GP Figure 2: MAP Distribution of the Best Individual per Generation for TREC 2003 GP converges to a local optimum point (or several points) prematurely. For example, for Fold1 and 3 in TREC 2003, in 100 best individuals per generations, two primary individu-als account for about 70%. In fact, there are only 19 and 30 candidate individuals respectively ready to be selected for these three data sets. For comparison, these figures reach 82 and 92 respectively in RankIP.
In order to boost the use of EC in the IR area, espe-cially for the learning to rank problem, we introduce a series of generalized definitions, as well as a common framework for ranking function discovery based on EC. Furthermore, according to this framework, an IP based approach, called RankIP, is presented.

We use LETOR 2.0 data collections to validate our ap-proach. Experiments show that performances of RankIP improves evidently compared with the baselines, including Ranking SVM, RankBoost and BM25.
 Finally, experiments show that IP has the superiority over GP for learning to rank problem under almost the same conditions due to IP X  X  high diversity.
