 The problem of building test collections is central to the de-velopment of information retrieval systems such as search engines. Starting with a few relevant  X  X uggets X  of informa-tion manually extracted from existing TREC corpora, we implement and test a methodology that finds and correctly assesses the vast majority of relevant documents found by TREC assessors X  X s well as up to four times more additional relevant documents. Our methodology produces highly ac-curate test collections that hold the promise of addressing the issues of scalability, reusability, and applicability. H.3 [ Information Systems ]: Information Storage and Re-trieval; H.3.4 [ Systems and Software ]: Performance Eval-uation Measurement, Performance
The key limitation of the Cranfield paradigm is that the information relevant to a topic is encoded by documents . Thus, assessing the performance of an information retrieval system requires effectively complete relevance judgments.
For large collections of documents and/or topics, it is im-practical to assess the relevance of each document to each topic. Instead, a small subset of the documents is chosen. A number of IR systems are used to find documents believed to be relevant to the topics. A  X  X ool X  of these documents is selected to be judged; only the relevance of these documents is assessed.

This gives rise to the following problems: 1. Scalability: The Cranfield paradigm and its vari-2. Reusability: For static collections, novel systems will 3. Applicability: It can be difficult to apply a test col-
Various attempts to address the issues described above have been proposed. Sampling techniques such as statAP [6], the Minimal Test Collection methodology [5] and their variants have been used extensively in various TREC tracks. A carefully chosen sample of documents is drawn from the pool, these documents are judged, and a statistical estimate of the true value of a performance measure over that pool is derived. Given that accurate estimates can be derived us-ing samples as small as roughly 5% of the entire pool, these methods permit the use of pools 20 times the size of stan-dard fully-judged pools. However, this is only a stop-gap measure. These methods cannot scale to collections the size of the web and do not address the issue of dynamic collec-tions. Furthermore, they reduce applicability in that the samples drawn and estimates obtained are typically tailored to specific evaluation measures, e.g. average precision. Crowd-sourcing relevance judgments, via, for example, Mechanical Turk, has also been proposed [1]. However, this too is only a stop-gap measure, allowing us to use collec-tions larger in roughly direct proportion to the relative ease (in time or cost) of crowd-sourced judgments vs. assessor judgments: if 10 to 100 crowd-sourced judgments can be obtained in the same time or at the same cost as 1 assessor judgment, then pools one to two orders of magnitude larger than standard pools can be contemplated. But this still does not scale to the web, or address the issue of dynamic collections. The use of click-through data has also been pro-posed [8], but this is only applicable to the web, and only for those topics and documents with sufficient  X  X licks X .
In order to address the inherent limitations of the Cran-field paradigm, we propose a test collection construction Figure 1: For a given query, selected documents are evaluated as  X  X elevant/nonrelevant X . Left: Tradi-tional TREC strategy for relevance. Right: Pro-posed nuggets methodology. methodology based on information nuggets . We refer to minimal, atomic units of relevant information as  X  X uggets X . Nuggets can range from simple answers, such as people X  X  names, to full sentences or paragraphs. In this model, asses-sors mark the information that is relevant contained within a document, rather than mark the documents containing relevant information. This relevant information is used to automatically assign relevance judgments to documents.
The Cranfield paradigm and its variants encode the infor-mation relevant to a topic by documents . Our hypothesis is that while the number of documents potentially relevant to a topic can be enormous, the amount of information relevant to a topic, the nuggets, is far, far smaller. Thus, collecting and encoding the relatively small set of nuggets, as opposed to the dynamically changing and effectively infinite set of documents , relevant to a topic will enable us to address the issues of scalability, reusability, and applicability.
Figure 1 graphically illustrates the differences between the traditional Cranfield-style methodology (left) and the nugget-based methodology proposed (right). The nuggets themselves are the relevant and useful pieces of information for a given topic X  X he information that the user seeks. As a set, they yield a natural encoding of the information rele-vant to a topic. In principle, if this set is complete, we can use the nuggets to infer the relevance of any document.
To build our test collection, we ask assessors to view doc-uments as before. However, rather than providing binary or graded relevance judgments, we instead ask the assessor to extract nuggets. These nuggets, the information relevant to a topic, are used to automatically infer the relevance of arbitrary documents in the collection.
Building a test collection in our framework consists of three distinct tasks (the rectangular boxes in Figure 1, right): (1) selecting documents from which to extract nuggets, (2) extracting nuggets from those documents, and (3) using the extracted nuggets to algorithmically create relevance judg-ments for any desired subset of the corpus. Each of these tasks could be performed in various ways; in this section, we document the decisions we made in implementing our methodology.
 must use a procedure to select the documents to be judged. Generally, documents retrieved by many systems and/or at higher ranks are preferred to documents retrieved by fewer systems and/or at lower ranks. We used the statAP selection mechanism because it has been shown to be an effective doc-ument selection procedure in previous TREC ad hoc tracks for system evaluation [6].
 our internal assessors (primarily graduate students engaged in IR research). For each relevant document in the sample, the assessor was asked to extract the relevant nuggets (see Figure 2 for the nugget extraction interface). They were in-structed to find the smallest piece of text that constitutes relevant information in and of itself. However, assessors were not restricted to text as it appears in the document; slight modifications of the text, e.g. co-reference disambiguation, deleting contextual stopwords, etc. were encouraged. In the end, the vast majority of nuggets collected were relevant information extracts encoded in the form of actual text con-tained in relevant documents.

Assessors were also given the option of adding topic key-words, which would be used later as a retrieval filter. If a topic has keywords associated with it, a document must contain at least one keyword to be considered relevant for that topic. For example, consider the topic  X  JFK assassi-nation  X . An assessor might add the keyword  X  Kennedy  X . If a document does not contain this term, it will not be con-sidered relevant.
 ogy, we implemented a text-based matching algorithm that automatically infers the relevance of documents given the nuggets extracted. According to the typical TREC defini-tion of relevance for ad hoc retrieval, a document is con-sidered relevant if it contains a single relevant piece of in-formation. Thus, if a document contains a piece of text sufficiently similar to a known relevant information nugget, then the document must necessarily be relevant. Therefore, each document was given a relevance score based on whether it matched any nugget.

The matching algorithm is based on a variant of shingle matching , which is often used in near-duplicate detection [4]. A shingle is a sequence of k consecutive words in a piece of text. For example, after stopwording, the nugget "John Kennedy was elected president in 1960" has the follow-ing shingles for k = 3: ( John Kennedy elected ), ( Kennedy elected president ), and ( elected president 1960 ).
Given the set of nuggets, we computed a relevance score for each document by (1) computing a score for each shingle, (2) combining these shingle scores to obtain a score for each nugget, and (3) combining these nugget scores to obtain a score for the document:
In order to show that our methodology produces use-ful test collections and addresses the issues of scalability, reusability, and applicability, we must show that our inferred relevance judgments are correct and that our system can return far more relevant documents than traditional meth-ods. To this end, we constructed two separate test collec-tions based on well-studied collections produced by previous TREC tracks.
 The first experiment uses ad hoc retrieval data from the TREC 8 ad hoc task: a collection built around a corpus of about 500,000 clean text newswire articles. It is considered to have effectively complete assessments, with an average of about 1,736 assessed documents for each of 50 queries. We refer to this data collectively (documents, judgments, queries, systems) as  X  X d hoc X .

The second experiment is based on data from the TREC09 web track diversity task, which uses the ClueWeb09 html collection of about one billion documents. It contains an average of only about 528 documents assessed per query. This data is referred to as  X  X eb X .

Using statAP sampling, we selected 200 documents for each query from each collection. Of these documents, we extracted nuggets from only those that had been judged rel-evant by TREC assessors. The TREC 8 ad hoc collection sample, denoted  X  X ampleAdHoc X , consists of approximately 11% of the documents assessed by TREC. The TREC09 web sample, denoted  X  X ampleWeb X , consists of approximately 38% of the documents assessed by TREC. On average, about 87 nuggets were extracted per query for the ad hoc sample, and about 62 nuggets were extracted per query for the web sample. (See table below.)
Given the nuggets extracted, we employ the matching al-gorithm to infer binary relevance for all documents not con-tained in our sample. The relevance judgments produced are referred to by the sample of documents from which nuggets were extracted, e.g.  X  X ampleAdHoc+InfRel(Nuggets) X  X efers to the judgments by TREC assessors of documents in the ad hoc sample, plus the judgments inferred for the remaining documents.

We demonstrate the correctness of our inferred judgments in two ways: by comparing our judgments to those pro-vided by TREC, and by verifying the inferred relevance of documents that were not assessed by TREC with indepen-dent human assessors. However, we must bear in mind that the notion of correctness of relevance judgments is some-what problematic. Inter-assessor disagreement [3, 9] is a well known phenomenon, as the question of relevance is am-biguous for many documents. Even if our inferences were perfect, we would still expect to see disagreement with our inferences and any other judgments.
After thresholding, we can compare our inferred judg-ments against the published TREC qrel, measuring preci-sion, recall, F1, etc. (See table below.) T runcated Result List M AP Prec ision Rec all F 1 SampleAdHoc+InfRel 0 .76 0 .88 0 .65 0 .75 SampleWeb+InfRel 0 .75 0 .88 0 .60 0 .71 For comparison, a previously published comparison of in-ferred relevance judgments to existing qrels [2] cites an F1 of 0.68 (compared with our F1=0.75). From this we conclude that the relevance judgments produced by our methodology are very similar to those produced by the TREC assessors.
To test the hypothesis that our method finds many ad-ditional relevant documents, as well as to further test the correctness of our inferred relevance judgments, we also val-idated the correctness of our relevance judgments for doc-uments from the web corpus not assessed by TREC. Our matching algorithm marked an average of about 400 ad-ditional documents as relevant per query. We selected a uniform random sample of about 80 per query and per-formed a validation of these inferred relevance assessments using Amazon X  X  Mechanical Turk service. 1 (See table be-low.) Note that the  X  X utside qrel X  numbers given below are maximum likelihood estimates of the number of documents in each category given the random sample that was judged. This experiment resulted in an agreement of 73.29% between the Mechanical Turk judges and our inferred assessments. Given the overall sample size of about 4000 documents, there is a 99.9% statistical confidence that the number of relevant documents outside the TREC qrel is at least 14,049.
Overall, even with nuggets extracted from a small sample of assessed documents, we were able to correctly infer rele-vance for a large number of relevant documents, with limited false positive mistakes and within the realm of inter-assessor disagreement.
Our methodology produces reliable relevance judgments at a substantially reduced cost. Based on reasonable as-sumptions, 2 we were able to construct SampleAdHoc in one-sixth the time and SampleWeb in half the time that TREC required to create the underlying collections.

Preliminary results with our nugget-based test collections indicate that the relevance judgments inferred from nuggets mturk.com. Each Mechanical Turk job was verified for quality: each job consisted of 30 documents, out of which 10 were verification documents with known TREC assess-ments. Turkers were required to correctly assess 70% of these 10 documents; otherwise, the job was not accepted. Furthermore, some jobs were performed by multiple judges. If a document had multiple assessments for a given query, the majority vote was used. In case of a tie, the document was discarded from measurement.
Private communication with a TREC organizer. are also accurate enough to perform reliable and reusable system evaluations; in our pilot study, we obtained system orderings with Kendall X  X   X  at or above 0.9 as compared to ground truth TREC assessments.

In future work, we intend to explore the application of nuggets to areas such as learning-to-rank, novelty and diver-sity, and so-called X  X nowledge engines X  X uch as Wolfram | Alpha. We also hope to develop new performance metrics that uses nugget matching to directly measure the information con-tent of the documents in a ranked list, rather than simply measuring the ranks at which relevant documents appear. Acknowledgment: This material is based upon work sup-ported by the National Science Foundation under Grant No. IIS-1017903. Any opinions, findings and conclusions or rec-ommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Na-tional Science Foundation (NSF). [1] 33rd ACM SIGIR Workshop on Crowdsourcing for [2] Javed A. Aslam and Emine Yilmaz. Inferring document [3] Peter Bailey, Nick Craswell, Ian Soboroff, Paul [4] Andrei Z. Broder. Identifying and filtering [5] Ben Carterette, James Allan, and Ramesh K.
 [6] Ben Carterette, Virgil Pavlu, Evangelos Kanoulas, [7] Steve Krenzel. Finding blurbs. Website. [8] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. [9] Ellen M. Voorhees. Variations in relevance judgments
