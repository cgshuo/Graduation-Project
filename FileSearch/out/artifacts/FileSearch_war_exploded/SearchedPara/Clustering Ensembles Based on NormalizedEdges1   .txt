 Cluster analysis is an important tool for exploratory data analysis, aiming to find homogeneous groups in a data set of unlabeled objects. Numerous algorithms have been and are being developed [2], [9], [10], such as the K-means (KM), the single-linkage (SL) or the average-linkage (AL) and the spectral clustering algorithms [14], [15].

However, clustering is inherently an ill-posed problem. All the previous meth-ods are designed with certain assumpti ons and favor some type of biases, and no single one is universally suitable for solving all the problems [19]. Hoping to exploit the strength of many individual clustering algorithms, people turn to clustering ensembles, seeking improvement over a single clustering algorithm in such aspects as robustness, novelty and scalability, et al.

Besides formal arguments on the effectiv eness of cluster ensembles [18], many combining algorithms have been propos ed, and their good performance further justified the use of cluster ensembles. A few examples are: methods based on hypergraph (CSPA, HGPA, and MCLA) [16] or bipartite graph partitioning [3], evidence accumulation using the CA matrix (EAC-SL and EAC-AL) [6], mix-ture model using a unified representation for multiple partitions [17], bagged clustering [11], and combination by plurality voting [1], [4], [7], [20].
Despite the primary success achieved b y those algorithms, they are far from ideal. To make the clustering ensembles pr actical and helpful for us, an effective algorithm is crucial, which is the focus of this paper. We first analyze the CA matrix, and discuss the problem of designing a proper algorithm for it in Sect. 2. Based on a novel concept of normalized edges as we have defined in this paper, we propose a hierarchical algorithm to find the final partition in Sect. 3. In Sect. 4, experiment results demonstrate the e ffectiveness of our proposed method. 2.1 The Co-Association (CA) Matrix In order to combine the multiple partitions of the data, one can first map the data to a new feature space as a way to accumulate the information provided by each partition. The CA matrix 1 C [6] is a newly constructed similarity matrix from multiple partitions of the original data. It takes the co-occurrence of pairs of patterns in the same clusters as votes for their association, with elements where n ij is the number of times the pair x i and x j is assigned to the same cluster among the N partitions. In fact, the CA matrix records the frequency that every pair of points is in the same cluster.

The CA matrix ( N = 30, and the number of clusters is fixed to 60 2 )ofthe 2-spirals data 3 (Fig. 1a) is shown in Fig. 1c. For comparison, the similarity matrix (normalized into the 0-1 scale) based on the Euclidean distance for the original patterns is plotted in Fig. 1b. Evidently, the similarity of pairs of points from different clusters is mostly much smaller in the CA matrix than that in the original similarity matrix, showing the CA matrix captures the global structure of the data. Thus, it is not surprising that the evidence accumulation method operating on the CA matrix [6] perfo rms well and that is why we use the CA matrix to combine the multiple partitions in our method. 2.2 A Proper Algorithm Despite the good discrimination ability of the CA matrix, improper clustering algorithms can still lead to bad results. For example, in [6] (e.g. table 2) and [17] (e.g. Fig. 9 and 10b), the authors found that, based on the CA values, the results of the consensus functions (i.e., SL, AL and CL) differ significantly, and the choice of a good consensus function is sensitive to the choice of the data set. So, does there exist a better consensus function? How can we find it?
Compared with ordinary similarity matrix using Euclidean distance, the CA matrix has special characteristics. Without loss of generality, we take the CA matrix (Fig. 1c) of the 2-spirals data as an example, part of which is highlighted in Fig. 1d. The similarity matrix of the data using Euclidean distance is shown in Fig. 1b. To construct a good algorithm, we believe that the following character-istics should be taken into account: 1) points from different clusters are always dissimilar, 2) a large percentage of pairs of points from the same cluster have very low similarity, and 3) if two points from the same cluster are dissimilar, then there always should be a path of some points (or just one point) between them who are successively similar. Referring to these features, we can also explore the reasons why the SL, AL and CL algorithms would have such performance in [6]. Based on the above analysis, we will customize a hierarchical clustering algorithm to operate on the CA matrix for combining multiple partitions, in hopes of discovering the true structure of the data more successfully and robustly. 3.1 Normalized Edges Our proposed hierarchical clustering algorithm is based on a novel concept of normalized edges for measuring the similarity between clusters. Treating all points of the data as a set of vertices, we can define an undirected and un-weighted graph. An edge exists between two points (or vertices), x i and x j ,if and only if their similarity is larger than a threshold  X  . In fact, this defines a threshold graph . For simplicity, we define a function edge between two points x i and x j , The notion of edges between two clusters C i and C j , edges ( C i ,C j ), is just the number of distinct edges connecting these two groups. Essentially, this can be used as a measure of the goodness of merging them in an agglomerative hier-archical clustering algorithm. However, this naive approach may work well only for well-separated and approximately equal-sized clusters.

A proper way to fix this problem is to normalize the number of edges between two clusters edges ( C i ,C j ), by dividing it by the (estimated) expected number of edges between them, which is inspired by goodness measure used in ROCK [8]. Hence, the number of normalized edges (NE) between two clusters, C i and C j ,is expected number of edges, in the clusters C i and C j respectively.

As in [8], we assume that every point in C i has n f (  X  ) i edges with other points in the cluster, then the total number of edges between points in the cluster is n i (each edge is counted twice). Thus the expected number of edges between n influence of  X  on the number of edges. Based on the analysis of Guha et al. [8], it is also defined as (1  X   X  ) / (1 +  X  )inthispaper. 3.2 Our Clustering Algorithm With the definition of normalized edges to measure the similarity between two clusters, we can use this measure to construct a new agglomerative hierarchical algorithm. To reduce the workload of calculation, we need not re-start calculating the similarity between clusters after eac h merging step. We just have to update the similarity between the merged and the other clusters. That is, if clusters C i and C j are merged into a new cluster C k ,thenwehave(for l = i, j and k ) edges ( C l ,C k )= edges ( C l ,C i )+ edges ( C l ,C j ), and n k = n i + n j . Thus we can calculate the normalized edges NE ( C l ,C k ) by definition. The proposed algorithm can be summarized as follows: Input: The similarity matrix (the CA matrix in this paper), the threshold  X  ,andthe number of clusters k .
 Initialization : To calculate edge between all pairs of points based on the similarity matrix. Repeat : 1. To merge two clusters, among all possible pairs of clusters, with the largest normalized edges ; 2. To update the edges and normalized edges between the merged clusters and the other clusters.
 Until : Only k clusters left or the number of edges between every pair of the remaining clusters becomes zero.

Certainly, we can speed up the algorithm by many methods used in the tra-ditional agglomerative clustering algorithms. The edge between every pair of points can be computed in O ( n 2 ) time, and the worst time complexity of the clustering algorithm is O ( n 2 log n ), just as the ROCK algorithm.
For the threshold  X  in our algorithm, we are still seeking a general way to de-termine it. Empirically, the algorithm worked well with  X  in the interval [0.1,0.4] for many data sets, and was not very sensitive to it (e.g., for all the data sets in the experiments of this paper, we fixed  X  to 0.30). We compared our algorithm with single runs of some well-known clustering al-gorithms and other ensemble ones, and the experiment results demonstrate the effectiveness of our method. 4.1 Data Sets, Algorithms and Parameters Selection We summarized the details of the data sets in Table 1, which had been adopted by other authors to test their ensemble algorithms [4], [6], [13], [17]. We compared the experiment results of our ensemble algorithm (denoted by CA-HNE) with single runs of following algorithms: KM, SL, AL, and spectral clustering algorithm (SC) [14], and following ensemble methods: CSPA, HGPA and MCLA [16], Boost-KM [7], EAC-SL and EAC-AL [6], and Latent-EM [17].
For the algorithms proposed by other authors, the parameters selection and other settings are the same as suggested in their papers. The multiple partitions of the data were obtained by running KM algorithm with random initialization of cluster centers.

We found that, using only 10 or 20 component partitions, our algorithm worked well for many data sets, while many other authors used a (much) larger number for their algorithm, e.g. 50 [6], 100 [4], 500 [12]. For simplicity, we gen-erated 30 partitions for our algorithm and 50 for all other ensemble ones. For our algorithm, k was chosen as a constant, usually larger than the true number of clusters of the data. The  X  X rue X  number of clusters of the data, was assumed to be known, as in [6], [17]. 4.2 Results, Comparison and Analysis Table 2 summarizes the mean error rates and standard deviations from 20 in-dependent runs of the different methods on the data sets. The error rates are obtained by matching the clustering results with the ground-truth information, taken as the known labeling of the real-world data sets or the perceptual group-ing of the artificial ones. Since all the clustering algorithms considered here do not detect outliers in the data, we ignore the noise points when calculating the error rates. Notice that the SL and AL algorithms give unvaried clustering results for each data set.

We can see that the evidence accumulation clustering (EAC-SL or EAC-AL) can sometimes discover the structure of the data successfully. However, which method will succeed depends heavily on the c hoice of the data sets. In general, the AL (SL) consensus function based on CA matrix is appropriate if standard AL (SL) agglomerative clustering method works well for the data, and vice versa [17]. This may be problematic sinc e sometimes the characteristic of the data is difficult to know, or is complex for standard agglomerative clustering (e.g. 3-paths ). However, our algorithm tackles this problem well. For our method, the mean error rates presented in Table 2 are all the best or comparable to the best, and the partitions of the complex image and 3-paths data sets are as good as we expected, see Fig. 2a and 2b. The exp eriments show the effectiveness of our algorithm: it gives the best (or comparable to the best) overall performance for all the data sets. It clusters all the chosen data sets reasonably, though they have hybrid or complex characteristic or are corrupted by noise. The CSPA algorithm does not perform well for these chosen data sets, though it is also based on the CA matrix. Again, this demonstrates the importance of the choice of algorithms for the final partition based on the CA matrix. Other ensemble methods HGPA, MCLA, Boost-KM, latent-EM still favor some type of biases, and do not perform well for other kinds of data sets.

Single runs of KM, SL, and AL algorithms result in good partitions when the data sets are suitable for them, but fail drastically otherwise (e.g. noise, hybrid structure). The SC algorithm gives reasonable partitions for some of the data sets, but fails for complex image and 3-paths .

