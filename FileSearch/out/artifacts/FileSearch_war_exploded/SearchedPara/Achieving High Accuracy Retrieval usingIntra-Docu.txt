 Most traditional ranking models roughly score the relevance of a given document by observing simple term statistics, such as the occurrence of query terms within the document or within the collection. Intuitively, the relative importance of query terms with regard to other individual non-query terms in a document can also be exploited to promote the ranks of documents in which the query is dedicated as the main topic. In this paper, we introduce a simple technique named intra-document term ranking , which involves ranking all the terms in a document according to their relative im-portance within that particular document. We demonstrate that the information regarding the rank positions of given query terms within the intra-document term ranking can be useful for enhancing the precision of top-retrieved results by traditional ranking models. Experiments are conducted on three standard TREC test collections.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Algorithms, Experimentation inter-document term ranking, precision at top ranks
With the rapid growth of Web document collection sizes in recent years, achieving high precision at the top of the retrieved result has become a major issue for search engine users [1]. In traditional IR ranking models [2, 3], the rele-vance of a document for a given query is scored primarily based on simple term statistics, such as the number of oc-currence of query terms within the document or within the whole document collection. Basically, the more query terms occur in a document, the higher the chance that the doc-ument would be relevant to the query and thus would be
To analyze the relative importance of individual terms that occurred in a given document, we rank the terms by their standard tf-idf weights. We use the term frequency normalized with the document length as the tf component, and the logarithm of the inverse document frequency as the idf component. We normalize the ranks in range 0 to 1 so that the term with highest tf-idf weight will be at rank 0 and the term with the lowest weight at rank 1.

Given this ranked list of terms within a document, we derive two document ranking heuristics based on the rank positions of query terms withi n the list. The first heuristic, which corresponds to our first hypothesis, ranks documents according to the average rank position of query terms. Here we basically assume that the higher the rank of the query term within a document, the more likely that the document is relevant. Given a query Q and the term ranking result of adocument D , the average rank position of query terms is calculated as follows: where q i represents i th query term, | Q | is the size of the query in words, and rank ( q i ,D ) is a function that returns the normalized term rank of q i in D .

The second heuristic, which corresponds to our second hypothesis, assumes that the more cohesive the rank po-sitions of query terms (i.e. have similar rank positions to each other) in a document, the higher the probability that the document is relevant. There are a number of ways to measure the cohesiveness of the term ranks. In this paper, we calculate such measure by the maximum rank difference of all pairs of query terms as follows: where diff ( q i ,q j ,D ) is a function that returns the absolute value of the difference in rank positions of q i and q j .For example, if three query terms were ranked at 0, 0.1, and 0.4, the maximum difference would be 0.3.
For test collections, we use the following standard TREC collections: AP88-90 (Associated Press news 1988-90), WT2g (2 gigabyte Web data), and Blogs06 (Web data used in TREC Blog Track 2006-08). For queries, we use the title field of the TREC topics 50-150 for AP88-90, 401-450 for WT2g, and 851-950 and 1001-1050 for Blogs06.

To investigate whether the two new heuristics can infer the true relevance for documents at top ranks of the retrieved list by traditional ranking models, we retrieved the top 20 doc-uments from the collections using query-likelihood language models [2] with dirichlet prior 1 (LM) and computed the cor-relation between the output value of either of the heuristics and the relevance. We observe that both have positive cor-relations with document relevance (0 . 21 for R 1 and 0 . 31 for R ). This result is consistent with our two hypotheses.
We validate the effectiveness of the two ranking heuristics in a re-ranking scheme. First, given a query, we generate a
We tuned  X  to be optimal for each collection (1000, 3000, and 5000 for AP88-90, WT2g, and Blogs06, respectively).
