 Olga Vechtomova * 1. Introduction
In the recent years, there has been a surge of interest in blogs (Web logs) among the general public. Many people see blogs as an opportunity to communicate to others their daily experiences, views, attitudes and opinions on various topics, such as current events, products, companies and other people. In this way a lot of information reflecting people X  X  personal senti-ments on various subjects has been accumulated on the Web. Knowing that opinionated information about many subjects exists on the Web, people may want to make use of it in various situations, for example, when choosing a product or service or making an investment decision. However, finding what others think is not always easy using the general-purpose search engines, which may retrieve many pages containing only factual information, such as sales/shopping sites and technical documentation.

In this paper we propose methods of retrieving blog posts containing opinions about an entity expressed in the query, such as a product, person, event or an abstract concept. The proposed methods consist of four main stages:  X  Collection pre-processing. The goal of this stage is to keep only the content-related text in each blog post.  X  Query processing. At this stage concepts are identified in the query by utilising Wikipedia, the query elements are  X  Topic-based document retrieval. The goal of this stage is to retrieve as many topically-relevant documents in response  X  Opinion-based re-ranking. The documents retrieved in the previous stage are re-ranked using one of the three proposed
The proposed opinion ranking methods rely on a lexicon of subjective words and phrases, gathered from a variety of man-the list of subjectivity clues by Wilson, Wiebe and Hoffman (2005) , etc. The first method uses the Kullback X  X eibler diver-gence (KLD) ( Losee, 1990 ) to weight subjective words, and factors these weights into the document score, the second method ranks documents based on the proximity of query terms to subjective words and the third uses a combination of the two. The proposed methods were evaluated on the topics of the opinion-finding task of the Blog tracks of 2007 ( Macdonald,
Relevance judgements in the opinion-finding task were performed on a 5-point scale:
Two types of relevance were defined in the task:
For each of the two types of relevance, the standard performance measures were calculated: Mean Average Precision (MAP), Precision at 10 retrieved documents (P10), and Precision at R , where R is the number of relevant documents for the given topic ( R -prec). In this paper, opinion-relevance measures will be denoted with subscript op (e.g., MAP relevance with subscript rel (e.g., MAP rel ).

While the general idea of faceted retrieval is not new, the paper proposes a novel method of facet identification in the query, expansion of each facet by means of Wikipedia, and use of facets in opinion retrieval. The proposed methods combine a number of components in opinion-based document ranking: facet distance, facet validation, KLD scores of subjective words, and distance between query terms and subjective words. The paper presents detailed evaluation of each contributing component, and discusses its impact on the effectiveness of opinion retrieval. An analysis of performance by query categories (person, event, product, organisation, media, geography and miscellaneous) is also presented, and shows the extent to which different types of queries benefit from the proposed opinion retrieval methods.

The rest of the paper is organised as follows: in Section 2 we review related work on opinion-finding, in Section 3 we describe the proposed methods, Section 4 presents the results of the evaluation, Section 5 contains the discussion and anal-ysis of the proposed methods, and Section 6 concludes the paper and outlines possible future research directions. 2. Related work The Blog track of TREC has served as a valuable platform for the development and evaluation of opinion retrieval methods. Most of the participants in the Blog opinion-finding task adopted a two-stage retrieval model. In the first stage, one of the standard IR methods was applied to retrieve the initial document set, and in the second stage, opinion-finding algorithms were used to re-rank this set. Many methods include collection  X  X  X leaning X , such as HTML tag removal, filtering of the text found in more than one blog, which is regarded as non-content-related text (Lee et al., 2008 ), removal of non-English blog filtering, e.g. (Jia et al., 2008 ). Query processing most commonly includes phrase identification and query expansion. For example, Wikipedia was used by (Jia et al., 2008 and Yang, 2008 ), WordNet (Fellbaum, 1998 ) and MiniPar (Lin, 1998 ) were used by (Jia, 2008 ) to identify phrases in the query. The sources for query expansion terms include Wikipedia, pseudo-rel-evance feedback on external collections (Jia et al., 2008; Yang, 2008 ), and the internal Blog collection (Lee et al., 2008 ).
There are two main approaches to opinion-based document ranking: classification approach and a lexicon-based ap-proach. SVM (Support Vector Machines) is commonly used in the classification based approaches, for example, by Jia done, for instance by Yang (2008) . Some methods rely on lexicons developed from several sources, for instance, Lee et al. (2008) use a combination of the lexicon from SentiWordNet (Esuli &amp; Sebastiani, 2006 ) with a lexicon learned from external product review corpora and through the pseudo-relevance feedback process. Amati, Ambrosi, Bianchi, Gaibisso, and Gambosi (2008) propose a method of generating a dictionary of subjective words and their weighting scheme based on information-theoretic measures.
 Here are brief descriptions of some of the opinion-based document ranking algorithms that showed high performance in
Blog 2008 track: One of the approaches proposed by He, Macdonald, Ounis, Peng, et al. (2008) weights words based on their opinion discriminating ability, and uses a query composed of top-weighted subjective words to calculate document match-ing scores, which are later combined with the topic-relevance document scores. Their other approach uses OpinionFinder (Wilson, Hoffmann et al., 2005 ) to identify subjective sentences, and calculate document scores based on the proximity of query terms to subjective sentences (He, Macdonald, &amp; Ounis, 2008 ). Lee et al. (2008) calculate a document score as the sum of opinion scores of subjective words occurring in it, normalised by document length. The opinion scoring method pro-posed by Yang (2008) integrates a number of factors, such as high-frequency subjective words learned from the training cor-pora, including Blog 2006 dataset and product reviews, low-frequency words occurring in opinionated documents, and or non-opinionated, then determine whether the sentences are related to the query based on their position in the document with respect to query terms and phrases, and calculate document scores using a number of approaches, such as the sum of SVM scores of the query-related sentences.

Some work on identifying topical subjectivity, i.e. opinions expressed about a target, has been done outside of the Blog track. For instance, Hurst and Nigam (2004) proposed a method of identifying sentences that are relevant to some topic and express opinion on it. They use a classifier, trained on hand-labelled documents to determine if a document is relevant to a topic, and if the classifier predicts the whole document as topically relevant, they apply the same classifier to predict topical relevance of each sentence. For the sentences predicted topically relevant, they apply sentiment analyzer, which re-lies on a set of heuristic rules and a hand-crafted domain-specific lexicon of subjective words, marked with positive or neg-ative polarity. They evaluated their classification method on a set of messages from online resources such as Usenet and online message boards in a specific domain. Their evaluation results show overall precision of 72%. Yi, Nasukawa, Bunescu, and Niblack (2003) proposed a method of extracting positive and negative opinions about specific features of a topic. By fea-ture terms they mean terms that have either a part-of or attribute-of relationships with the given topic or with a known fea-ture of the topic. Their method first determines candidate feature terms based on structural heuristics then narrows the selection using either the mixture language model, or the log-likelihood ratio. A pattern-dependent comparison is then made to a sentiment lexicon gathered from a variety of linguistic resources. The method was evaluated on two domains, digital camera and music review articles, using topic-relevance judgements performed by the authors, and achieved precision of 87% and recall of 56%.

Much research has been directed towards document classification by sentiment polarity ( Dave, Lawrence, &amp; Pennock, as either positive, or negative. Pang et al. (2002) evaluated several machine learning algorithms to classify film reviews as either containing positive or negative opinions. Dave et al. (2003) proposed and evaluated a number of algorithms for select-ing features for document classification by positive and negative sentiment using machine learning approaches. Turney (2002) developed an unsupervised algorithm for classifying reviews as positive or negative. He proposed to identify whether a phrase in a review has a positive or negative connotation by measuring its mutual information with the words  X  X  X xcellent X  and  X  X  X oor X . A review X  X  polarity is predicted from the average semantic orientation (positive or negative) of the phrases it contains. The method, evaluated on 410 reviews from Epinions in four different domains, showed accuracy between 66% and 84% depending on the domain. Hu and Liu (2004) developed a method of identifying frequent features of a specific re-view item, and finding opinion words from reviews by extracting adjectives most proximate to the terms representing fre-quent features. The Blog track has a separate polarity task, which in 2007 was defined as a classification task, i.e. the participants had to return unranked sets of documents with positive and negative opinions about the query target. In 2008, the polarity task was an ad-hoc style task, where participants had to return ranked sets of documents with positive and negative opinions on the query target. Overall the performance of track participants in this task was low, which means that this is still an open research problem. Polarity-based retrieval is beyond the scope of this paper, although the methods presented here could serve as the basis for developing polarity-based retrieval algorithms. 3. Methodology
Our approach to retrieving blog posts containing opinions about the concept expressed in the query is a two-stage pro-in the second stage, this document set is re-ranked using one of the opinion-finding methods described in this section. Any document retrieval model can be used to retrieve the initial document set, and in the evaluation section we report how the proposed opinion re-ranking methods perform with two different first-stage document retrieval methods: BM25 and one of the standard baselines provided by TREC 2008 Blog track organisers. In the following subsections we will describe our ap-proach, starting with the blog collection pre-processing (Section 3.1), query processing and expansion using Wikipedia (3.2), the building of a subjective lexicon (3.3) and, in subsequent sections, opinion-based document re-ranking methods. 3.1. Blog collection pre-processing
In all experiments reported in this paper, only the permalink component of the  X  X  X logs06 X  collection was used. The col-and corresponding closing tags where applicable, were replaced with newline characters. Other  X  X  X leaning X  steps included removing the remaining tags, and decoding URI-encoded strings and special characters. Then, each line, in which the number of hyperlinks constituted 50% or more of the total number of words in that line, was removed ( X  X  X lean50%+ X  method). The rationale was to remove parts of text, which were the least likely to have opinionated content. We assumed that such lines are more likely to contain advertisements, various website directories or blog navigation links. We compared this method to or more consecutive lines that start with a hyperlink, however, the  X  X  X lean50%+ X  method yielded better results in the docu-ment retrieval experiments than  X  X  X lean3+ X , as demonstrated in Table 1 . The table shows the results of BM25 runs using sin-gle terms from the Title section of Blog 2006, 2007 and 2008 opinion track topics. The runs were performed using the BM25 (Sp X rck Jones, Walker, &amp; Robertson, 2000) implemented in the Wumpus search system (B X ttcher &amp; Clarke, 2005 ). The BM25 tuning constants b and k 1 were set to 0.1 and 0.75, respectively, as these showed best results in the evaluation on Blog 2006
Runs marked with demonstrate statistically significant improvement at .05 level, at .02 level (paired t -test) compared to their corresponding runs based on the  X  X  X riginal X  collection.

Since the  X  X  X lean50%+ X  collection pre-processing method demonstrated best results, it was used for all subsequent runs reported in this paper. Stemming was not used in our methods, as it proved to deteriorate performance on the Blog 2006 dataset. 3.2. Query processing using Wikipedia
In the methods presented in this paper, we used only the Title section of TREC topics, as it most closely resembles queries submitted by users to Web search engines. In fact, topic titles used in Blog 2006 and 2007 tracks are selections of the actual queries submitted by users to a Web search engine, which were later supplemented with traditional TREC-style description and narrative sections by NIST assessors. Blog 2008 topics were created by NIST assessors, following the format of the topics in the earlier years. Our approach to query processing is based on recognition that each information need consists of one or more facets . We view a facet as an aspect of the topic about which the user wants to find information. Facet should not be physical object, event and person. A concept in turn, can be expressed in a language as a multiword unit, often referred to in  X  X  X ardi Gras X  represents one facet, consisting of one concept, which is lexically represented as a phrase. On the other hand, represented by one concept, which in turn is lexically expressed by a phrase. Another example is the title of Topic 944:  X  X  X p-era Software X  OR  X  X  X pera Browser X  OR  X  X  X pera Mobile X  OR  X  X  X pera Mini X . It may be argued that this topic title contains one facet or theme, i.e.  X  X  X pera X  software applications, which is represented by four concepts, each expressed by a phrase.
The method described below aims to achieve the following goals by utilising Wikipedia, the online collaborative encyclopaedia: 3.2.1. Identifying concepts in the topic titles
The method for identifying concepts by matching them to Wikipedia titles is described below, and is similar to the meth-od used in ( MacKinnon &amp; Vechtomova, 2008 ). Any part of the query that exactly matched a Wikipedia title was treated as a phrase in document retrieval (Section 3.2.3.4 ). First, we attempted to match the entire query of n words, then, if unsuccess-ful, every subphrase of size n 1, then every subphrase of size n 2 in the unmatched part of the query, and so on until we reached unigrams. The unmatched unigrams were always kept in the query. Stopwords were filtered out only from the single terms in the query. If a phrase that matched a Wikipedia title contained stopwords, they were not removed, such as in  X  X  X arch of the penguins X  (Topic 851).

To illustrate the process of identifying concepts in the query, consider the following example: the query  X  X  X usiness intel-ligence resources X  (Topic 898) was split into  X  X  X usiness intelligence X  and  X  X  X esources X , because Wikipedia has an article with the title  X  X  X usiness intelligence X . Similarly, the query  X  X  X pera software OR opera browser OR opera mobile OR opera mini X  (To-pic 944) was split into  X  X  X pera software X ,  X  X  X pera browser X ,  X  X  X pera mobile X  and  X  X  X pera mini X . The Boolean operator OR was removed because it is a stop word.

Inspection of the resulting queries showed that out of 150 topics from Blog 2006, 2007 and 2008 tracks, only the titles of six were incorrectly resolved into concepts. For instance the title  X  X  X ord Bell X  (Topic 949) was wrongly resolved as two sep-arate concepts:  X  X  X ord X  and  X  X  X ell X , because there was no article about the person Ford Bell in Wikipedia. 3.2.2. Grouping concepts into facets
Whether two concepts represent one facet or two is not easy to determine automatically, and may not always be agreed upon by humans. In an earlier example of Topic 1014,  X  X  X ax break X  and  X  X  X ybrid automobiles X  clearly represent two facets of the topic. The user does not simply want information regarding each of these concepts separately, but information which relates one to the other. According to the topic description, the user wants to  X  X  X ind opinions on the Federal tax break for purchasers of gasoline X  X lectric hybrid automobiles. X  Thus, a relevant document must cover both facets of this topic. On the other hand, in the example of Topic 944 ( X  X  X pera Software X  OR  X  X  X pera Browser X  OR  X  X  X pera Mobile X  OR  X  X  X pera Mini X ), we consider the four concepts to constitute one facet, and our reasoning is that they are not complementary, which is also evident by the searcher X  X  use of  X  X  X R X  operators. In other words, a document may only refer to one of these concepts and be deemed relevant by the searcher.

Ideally, in order to automatically attribute a concept to a facet, we need to determine whether it is close enough in mean-be mutually substituted without the loss of the overall text X  X  meaning. Linguistically, such  X  X  X losely X  related words could be synonyms, near-synonyms, abbreviations, alternate spelling forms, and in some cases hyponyms, co-hyponyms and hyper-nyms. While it is possible to implement sophisticated methods relying on machine-readable dictionaries and thesauri, we opted to use a much simpler method at this stage: if two or more concepts in the query have at least one word in common, they are considered to belong to one facet. For instance, out of the 150 topic titles in Blog 2006, 2007 and 2008 tracks, only one topic (944:  X  X  X pera Software X  OR  X  X  X pera Browser X  OR  X  X  X pera Mobile X  OR  X  X  X pera Mini X ) could be considered as contain-ing more than one concept per facet. We decided not to expend effort on implementing a more complex method than this because it is unlikely that the searchers use more than one concept, such as synonyms or alternate spelling forms, to rep-resent one facet. Facets are used in our opinion-based document re-ranking method as will be explained in Section 3.5. 3.2.3. Expanding facets with new concepts After the concepts and facets were identified in the user X  X  query, our next goal is to find related concepts for each facet.
Our approach is to use Wikipedia page redirects for this purpose. In Wikipedia, some pages are redirecting users to another (target) page, which is usually a more widely used, standard or formal term denoting the same concept. For example,  X  X  X in-ter Olympics X  (Topic 933) is redirected to  X  X  X inter Olympic Games X . Fig. 2 lists all pages that are redirected to it. Wikipedia anchor texts can also serve as potentially useful source of query expansion terms. For example, MacKinnon and
Vechtomova (2008) expanded queries in the Complex interactive Question Answering (CiQA) track of TREC, by first mapping the terms and phrases in a query to Wikipedia article titles, and then expanding them with anchor texts pointing to these articles. Elsas, Arguello, Callan, and Carbonell (2008) proposed another method of selecting and weighting anchor texts as QE terms, which showed performance gains in the distillation task of the Blog track. While anchor texts may be useful as QE terms in opinion retrieval, we did not explore their use in this paper, focusing instead on the use of page redirects. 3.2.3.1. Limited query expansion. The algorithm for the method consists of the steps presented in Fig. 3 .
Line 2 of the algorithm requires some clarification. Wikipedia has pages with the same title, which could be due to two reasons:  X  Multiple senses of a word or phrase. For example, at the time of this research there were three pages with the titles  X  Titles in Wikipedia are case-sensitive, and there exist some redirecting pages with the same title written in a different
Due to these reasons, a concept (called  X  X  X ser_concept X  in the algorithm) identified using the method described in Section each user_concept, so that we can use the titles of pages redirecting to this page as query expansion terms. We decided to matched, and two or more pages have equally the highest number of views, i.e. max (page_counter), we deem them ambig-do the same in cases when only one matching page was found (lines 12 X 15).
 Some examples are: Topic 1027 ( X  X  X AFTA X ), which was expanded with the title of the target page  X  X  X orth American Free
Trade Agreement X ; Topic 1045 ( X  X  X hina one child law X ), which was split into two facets  X  X  X hina X  and  X  X  X ne child law X  in the ited and full (Section 3.2.3.3 .) query expansion processes for the query  X  X  X AFTA X . 3.2.3.2. Expansion with phrase abbreviations. The next query processing step performed was to expand the facets with valid abbreviations of phrases. For each Wikipedia-matched phrase identified so far, either through the initial Wikipedia title phrase. For example, Topic 1013 ( X  X  X uropean Union Iceland X ) was split into two facets in the previous stage:  X  X  X uropean Un-ion X  and  X  X  X celand X . Abbreviation  X  X  X U X  was among the pages that redirect to  X  X  X uropean Union X , and was added to the facet containing it. Among the misses of this method was  X  X  X N commission on human rights X  (Topic 1008), for which the method produced a wrong abbreviation  X  X  X COHR X  instead of the correct  X  X  X NCHR X . 3.2.3.3. Full query expansion. The expansion method described above is rather conservative, and the overall number of expansion concepts found is small. We experimented with adding the titles of all other pages that redirect to the identified target page. In this method, all page titles in Fig. 2 redirecting to  X  X  X inter Olympic Games X , the target page for the query phrase  X  X  X inter Olympics X , would be added to the facet containing it. Fig. 4 shows the pages that would be selected for the query  X  X  X AFTA X  with limited and full QE methods. The method performed substantially worse than the limited QE meth-od, the reason being likely due to many low-quality and superficially related titles among redirecting pages. 3.2.3.4. QE evaluation. Table 2 presents MAP results of four types of queries:  X  X  X oQE-single X   X  the original single terms from topic titles were used;  X  X  X oQE-Wiki-phrases X   X  topic titles were matched to Wikipedia page titles as described in Section 3.2.1;  X  X  X imitedQE-Wiki-phrases X   X  limited query expansion was performed (Section 3.2.3.1 ), and valid abbreviations of page were added (Section 3.2.3.3 ). BM25 implemented in Wumpus was used for all runs. If the query contained phrases, identified using the methods described in Sections 3.2.1, 3.2.3.1 and 3.2.3.3, then only exactly matching phrases contributed to a document X  X  score, i.e. partially matching phrases were not considered in document scoring. For example, if the query is  X  X  X inter Olympics X , then only documents containing this exact phrase were retrieved. BM25 score for phrases was calculated by treating the phrase as an indivisible unit, in the same way as a single term. The frequency of the entire phrase in the collection was used to calculate idf (inverse document frequency) and the frequency in the current document was used to calculate tf (term frequency). In all our methods, weights of query expansion terms were calculated in the same way as for the original query terms. In runs  X  X  X oQE-Wiki-phrases X  and  X  X  X imitedQE-Wiki-phrases X  we also added all single terms from phrases to the query in order to increase recall. In  X  X  X ullQE-Wiki-phrases X  the same single terms as in the previous method were added, but single terms from other redirecting pages were not added, as there are many low-quality titles that are likely to deteriorate performance.

Runs marked with are statistically significant at .05 significance level, at .02 level (paired t -test) compared to their corresponding runs  X  X  X oQE-single X  runs. Mapping topic titles to Wiki phrases ( X  X  X oQE-Wiki-phrases X ) improves performance.
Limited query expansion method ( X  X  X imitedQE-Wiki-phrases X ) is generally better than  X  X  X oQE-Wiki-phrases X , except in Blog 2007 MAP op and Blog 2006 MAP rel . Full query expansion method is, however, worse than limited QE. Table 3 shows two examples of topics, which were respectively deteriorated and improved with query expansion.

Since  X  X  X imitedQE-Wiki-phrases X  was generally better than all other query types, we used it as our baseline in the subse-quent evaluation of the developed opinion-based re-ranking methods. 3.3. Building the subjective lexicon
The subjective lexicon used in the proposed opinion-based document re-ranking methods was compiled from a number of manually and automatically constructed lexical resources. The manual lexical resources used are Levin X  X  (1993) verb clas-ses, FrameNet ( Baker et al., 1998 ), Ballmer and Brennenstuhl X  X  (1981) speech act verbs, and Hatzivassilouglou and McKe-own X  X  (1997) subjective adjectives. We also used a large set of subjectivity clues compiled by Wilson and Wiebe et al. (2005) from both manually and automatically developed lexicons. 3.3.1. Levin X  X  verb classes Levin (1993) categorised English verbs into semantic classes. Verbs from the following classes were used in our method: 3.3.2. FrameNet Lexical units (verbs, phrasal verbs, adjectives, etc.) from the following frames were used: 3.3.3. Ballmer and Brennenstuhl (1981) speech act verbs
A few verbs and expressions were manually selected from the Emotion Model (e.g., blow up, burst out laughing, grumble about). 3.3.4. Hatzivassilouglou and McKeown X  X  subjective adjectives
A list of 1336 subjective adjectives manually composed by Hatzivassiloglou and McKeown (1997) was used, e.g., amusing, impressive, unreliable. 3.3.5. Wilson X  X  subjectivity clues
Wilson and Wiebe et al. (2005) compiled a collection of subjectivity clues from different sources, including manually developed lexicons, and automatically identified clues from annotated and unannotated corpora. Many of the words in Wil-contains 8221 lexical units, including different grammatical forms of some words. Each lexical unit is annotated with the 3.3.6. Subjective lexicon processing
After the removal of duplicates, the overall subjective lexicon gathered consisted of 6553 lexical units. For each verb and most of the phrasal verbs, whenever it made sense, we also added past tense, gerund ( X  X -ing X  form) and third person forms.
With all the grammatical word forms added, the total size of the subjective lexicon was 10447. 3.4. Window-based co-occurrence
Our approach to opinion-based re-ranking consists of adjusting the tf weights of query terms on the basis of their co-occurrence with subjective lexical units in fixed-size windows centered on the query term occurrences. The motivation ion about the topic related to the query term. The reason why we chose to use a fixed-size window instead of a natural lan-guage unit, such as a sentence, is twofold: first, a subjective word may not actually  X  X  X arget X  the query term occurrence in a sentence, but another word in a different sentence. For instance, a subjective adjective may modify a pronoun in a different query term, for instance, when expressing an opinion about a photo camera, a person may talk about the picture quality, rather than the camera directly:  X  X  X  bought a new camera. The picture quality is excellent. X  The second reason is practical: ary detection is a more difficult task than with, say, newswire articles: the former may use non-standard and ill-formed syn-tactic constructions without proper punctuation marks, and are typically in HTML format, which may not be always possible to convert to plain text correctly.

The window is defined as n words to the left and right of the query term occurrence in text. In cases where the distance between two instances of query term(s) in a document is less than n , the text span between these two query term in-stances is split in the middle, such that one half is attributed to the query term on the left, and the other half  X  to the query term on the right. This is done in order to avoid counting the same subjective word occurrence twice. In our exper-iments window size was set to 30, as it proved optimal on the training Blog 2006 topics compared to other window sizes (10, 20 and 40). 3.5. Opinion-based document ranking methods
Three opinion-based document re-ranking methods were developed:  X  KLD-based method, where document score depends on the Kullback X  X eibler divergence scores of the subjective words  X  Distance-based method, where distance between a query term occurrence and each of the subjective words co-occur- X  Combined method, which factors in both distance and the KLD score of each subjective word co-occurring with a query
The methods are described in detail in the following sections. All methods are used in the opinion re-ranking stage ing to which a document is down-ranked if it does not contain at least one concept from each query facet.
A document is considered to contain a facet if at least one concept (a phrase or single term) from that facet is found in the document. Consider Topic 1045 (Facet 1:  X  X  X hina X ; Facet 2:  X  X  X ne child law X ,  X  X  X ne-child policy X ). To be considered  X  X  X alid X  according to the facet validation algorithm, a document must contain  X  X  X hina X  and either  X  X  X ne child law X , or  X  X  X ne-child policy X . 3.5.1. KLD-based method the known relevant and opinionated documents than in the non-relevant or relevant but non-opinionated ones are more useful in predicting opinions. Based on this intuition, we calculated scores for the units in our subjective lexicon using the Kullback X  X eibler divergence (KLD).

The Kullback X  X eibler divergence measures the relative entropy between two probability distributions. It was defined in information theory ( Losee, 1990 ) and used in many information retrieval and natural language processing tasks, for example, in query expansion following pseudo-relevance feedback (Carpineto, De Mori, Romano, &amp; Bigi, 2001 ).
 The KLD score of a subjective lexical unit was calculated according to: where P R ( t )  X  probability of the subjective lexical unit t occurring in the relevant documents, and calculated as f f ( t )  X  frequency of occurrence of t in the relevant set, R  X  number of terms in the relevant set; P jective lexical unit t occurring in the non-relevant documents, and calculated as f of t in the non-relevant set, N  X  number of terms in the non-relevant set.

The Blog track 2006 and 2007 topics were used for calculating KLD scores of the subjective lexicon for the evaluation on 2008 topics, and 2006 topics only were used for calculating KLD scores for the evaluation on 2007 topics. The relevant set consisted of all the documents with the relevance judgments of 2 (negative opinion), 3 (mixed opinion) and 4 (positive opin-ion); the non-relevant set consisted of all the documents with the judgments of 0 (non-relevant) and 1 (relevant, but not opinionated).
 A KLD score was calculated for each lexical unit, not each of its grammatical word forms separately. Thus, in calculating total, KLD scores for 6553 lexical units were calculated. Lexical units with negative KLD scores were discarded. 3.5.1.2. Calculating document matching score. Our approach to the weighting of query term occurrences in a document con-sists of modifying the term frequency ( tf ) calculation in BM25. Instead of counting the actual frequency of a term X  X  occur-rence in the document to get tf , we calculate a pseudo-frequency ( pf ) value. In this approach, the contribution of each words in its window and proximity to other query terms. Specifically, if the query term t within a window of n words either side of it, we add the normalised KLD score of the subjective word to c(t query term t i does not have any subjective words in its window, c(t the query term to the nearest query term from a different query facet. The idea of pseudo-frequency weights was used earlier in our proximity-based document ranking method proposed in (Vechtomova &amp; Karamuftuoglu, 2008 ), which proved to be effective in an ad-hoc IR task. where c(t i )  X  contribution of the i th instance of the query term t to pf , FD( t the distance of t i to the nearest other query term/phrase from a different facet (Eq. (3)), KLD( s lexical unit s j ,| J |  X  the number of subjective lexical units occurring in the window of n words around t imum KLD score out of all 6553 subjective lexical units.
 where min dist(t i ,q )  X  the distance between t i and the nearest term/phrase q in the document D , where q belongs to a dif-ferent query facet than t .

The pseudo-frequency pf t of each query term t is calculated according to: where N  X  number of instances of the query term t in the document.

After pf is calculated for a query term, its Term Weight ( TW ) in the document is calculated in the same way as in the BM25 formula, with pf used instead of tf (Eq. (5)): where k 1  X  term frequency normalisation factor, which moderates the contribution of the weight of frequent terms. If k pf has no effect on the term weight, while the higher the value of k length normalisation factor, and is calculated in the same way as in the BM25 document ranking function, as expressed in Eq. (6).
 where b  X  tuning constant, DL  X  document length in word counts; AVDL  X  the average document length in the document collection.
 The Document Matching Score is calculated as the sum of weights of all query terms found in the document (Eq. (7)). where | Q |  X  the number of terms in the query.

The queries used in the opinion re-ranking stage are the same as used in the initial document retrieval stage (baseline), and contain single terms and phrases, identified according to the methods described in Section 3.2, plus the constituent non-stopwords of phrases. For instance, the query for Topic 851 (Title:  X  X  X arch of the Penguins X ) consists of:  X  X  X arch of the pen-guins X ,  X  X  X arch X ,  X  X  X enguins X . When calculating weights of single terms, the following rule was followed: if the occurrence of the single term t i in the document is not a part of the phrase occurrence, its c(t avoid double-counting the KLD and FD factors. The same rule was used in the methods described in Sections 3.5.2 and 3.5.3 .
For example, if a document contains 2 instances of the query phrase  X  X  X arch of the penguins X  and 3 separate instances of  X  X  X enguins X , the document matching score will be TW( X  X  X arch of the penguins X  X 2 instances]) + TW( X  X  X enguins X  X 3 instances]) calculated using Eq. (2) + TW( X  X  X arch X  X 2 instances]) + TW( X  X  X enguins X  [2 instances]) calculated using Eq. (8). 3.5.2. Proximity-based method
In proximity-based term weighting, we also used the idea of calculating a pseudo-frequency ( pf ) value for a query term in a document, which depends on its proximity to each of the instances of subjective lexical units within the window of n words (Eq. (9)). where dist(t i ,s j )  X  distance in number of non-stopwords between the query term t of subjective lexical units occurring within the window of n words around t
After c(t i ) is calculated, pf and the document matching score are calculated in the same way as described in Section 3.5.1 above. 3.5.3. Combined method
In this method we combined the KLD-based and distance-based term weighting methods as given in Eq. (10).
The document matching score is calculated in the same way as in the previous two methods. 4. Evaluation
The proposed methods were evaluated on 2007 and 2008 Blog track opinion-finding task topics. Two baselines were used for both evaluating our methods and for the initial document retrieval stage of our opinion retrieval algorithm: (1) BM25 implemented in Wumpus, using the limitedQE-Wiki-phrases method described in Section 3.2.3.
 (2) Baseline 4 provided by the organisers of the Blog track 2008, which facilitates cross-system comparison. The Blog 2008
In the Blog track, each run can contain up to 1000 retrieved and ranked blog posts per topic. The following naming con-ventions were used to denote the runs:  X  X  X LD X   X  runs using the KLD-based method (Section 3.5.1),  X  X  X ist X   X  runs using the proximity-based method (Section 3.5.2) and  X  X  X LD+dist X   X  runs using the combined method (Section 3.5.3). Suffix  X  X  X m25 X  was appended to the experimental runs based on the BM25 baseline  X  X  X imitedQE-Wiki-phrases X , and suffix  X  X  X 4 X  was ap-pended to the experimental runs based on Baseline 4. All experimental runs have facet distance (FD) and facet validation (FV) components.
 Table 4 presents the results of opinion-based re-ranking runs on Blog 2008 topics using the first baseline (BM25). The
Kullback X  X eibler divergence of subjective words in runs  X  X  X LD-FD-FV-subj-bm25 X  and  X  X  X LD+dist-FD-FV-subj-bm25 X  was cal-culated using Blog 2006 and 2007 topics. In the table, D MAP represents the difference in percentage of the run X  X  MAP from the baseline MAP. Runs marked with are statistically significant at .05 level; at .02 level (paired t -test). All experimental runs demonstrate statistically significant improvement in most measures. The combined method  X  X  X LD+dist-FD-FV-subj-bm25 X  performs slightly better than either  X  X  X LD-FD-FV-subj-bm25 X , or  X  X  X ist-bm25 X  methods in MAP and R-precision, how-ever, the other two runs are slightly better in Precision at 10 documents (P10).
 Table 5 shows the results of opinion-based runs on Blog 2007 topics, also using the BM25 baseline. Here, the Kullback X 
Leibler divergence of subjective words was calculated using only Blog 2006 topics. Again, all runs demonstrate statistically significant improvements over the baseline in most measures. The combined run  X  X  X LD+dist-FD-FV-subj-bm25 X  has the high-est results in topic-relevance measures, but  X  X  X LD-FD-FV-subj-bm25 X  has the highest MAP
Table 6 shows the opinion-based runs on Blog 2008 topics based on the standard Baseline 4 provided by the track organ-isers. Again, all experimental runs yield statistically significant improvement in most measures. Here,  X  X  X LD+dist-FD-FV-subj-b4 X  shows slightly higher MAP op and R-precision than the other two experimental runs, whereas they slightly outperform it in P10 op .

The proposed three opinion-based re-ranking methods compare favourably to other methods developed by the partici-pants of Blog 2007 and 2008 tracks. In Table 7 , we list the 10 best-performing opinion runs submitted by participants to Blog bold. The table reports the results in MAP op , P10 op and R-precision submitted a corresponding baseline run for their opinion runs, however, some participants did not separate opinion-ranking from topic-ranking features, and did not submit a baseline. D MAP where applicable. Two runs yielded higher MAP op than our methods, however, both of them do not have a corresponding baseline, therefore it is not clear how much of their performance is due to the opinion-finding features or other factors.
Our approaches show the best results among the two-stage retrieval methods, which first retrieve documents using to-pic-based ranking, and then re-rank them using opinion-based methods.

Table 8 compares our methods to the 10 best runs using the standard Baseline 4. Runs are ranked by D MAP ticipants X  results reported in this table are referenced from (Ounis et al., 2008 ). Our methods achieve the highest improve-ment in MAP op .

We also compare our methods using Blog 2007 topics to the 10 best opinion title-only runs submitted to Blog 2007 opin-are ranked by MAP op . One run achieved higher MAP op than our methods, but again this is a run that has no corresponding baseline. Our runs were the best among all two-stage retrieval methods. 5. Analysis and discussion 5.1. Topic-based analysis
Analysis of results by topics shows that the methods improve performance on a large number of topics over the baseline  X  X  X imited-QE-Wiki-phrase X . Among the 2008 topics,  X  X  X LD-FD-FV-subj-bm25 X ,  X  X  X ist-FD-FV-subj-bm25 X  and  X  X  X LD+dist-FD-FV-subj-bm25 X  improved the opinion-relevance Average Precision (AveP unchanged. Among the 2007 topics, the respective numbers of improved topics are 37, 35 and 38, with 2 topics unchanged. Fig. 6 shows the difference in AveP op between the experimental runs and the baseline on 2008 topics.

As an example of a document that benefited from the opinion re-ranking methods, consider document  X  X  X LOG06-20051208-132-0003415997 X  judged as 4 (relevant, containing positive opinion) for the Blog 2007 topic 935  X  X  X ozart X .
The document was retrieved at rank 315 in the baseline  X  X  X imitedQE-Wiki-phrases X  run, but was promoted to ranks 206, 181 and 194 in  X  X  X LD-FD-FV-subj-bm25 X ,  X  X  X ist-FD-FV-subj-bm25 X  and  X  X  X LD+dist-FD-FV-subj-bm25 X  respectively. An ex-cerpt from the document with the subjective lexicon underlined is shown in Fig. 7 .

As can be seen from the excerpt, none of the subjective words directly modify the opinion target of the query,  X  X  X ozart X , and its performance. Indeed, people frequently express opinions not directly about the target, but about related concepts.
This suggests that further improvements may be obtained by identifying related concepts and possibly performing a more sophisticated discourse analysis, aimed at determining whether the concepts are related contextually. 5.2. The effect of the method components on retrieval performance
We conducted runs with different combinations of components of the proposed methods in order to understand better their effect on the retrieval performance. The components used in each run are given in Table 10 .

In the run  X  X  X V-bm25 X  the contribution of each query term occurrence to pf is 1, regardless of whether it co-occurs with a subjective word in the surrounding window or not. In all the other runs, if a query term occurrence does not have at least 1 subjective word in its window, its contribution to pf is 0. Both runs  X  X  X ubj-bm25 X  and  X  X  X V-subj-bm25 X  use Eq. (11) in calcu-lating the contribution of each query term occurrence to pf : where | J |  X  the number of subjective lexical units occurring in the window of n words around t
The run  X  X  X D-FV-subj-bm25 X  uses Eq. (12):
The last three runs in Table 10 are the experimental runs presented in the previous section, and are given here for com-parison. Table 11 shows the results of the runs on Blog 2008 topics, while Table 12 contains the Blog 2007 results. Runs marked with are statistically significant at .05 level; at .02 level (paired t -test).

The facet validation (FV) component (run  X  X  X V-bm25 X ) and counting only query term instances near subjective words ( X  X  X ubj X  component, run  X  X  X ubj-bm25 X ) do not yield substantial improvements each on their own on 2008 topics, however the latter yields statistically significant improvements in all measures except R prec both components (run  X  X  X V-subj-bm25 X ) is more effective, yielding 5.24% improvement in MAP on 2007 topics over the baseline  X  X  X imitedQE-Wiki-phrase X .

By comparing the runs  X  X  X LD-FD-FV-subj-bm25 X ,  X  X  X ist-FD-FV-subj-bm25 X  and  X  X  X LD+dist-FD-FV-subj-bm25 X  to the run  X  X  X D-FV-subj-bm25 X , it is possible to understand how much  X  X  X LD X ,  X  X  X ist X  and  X  X  X LD+dist X  components contribute to perfor-mance. Specifically, when  X  X  X LD X  component is added (run  X  X  X LD-FD-FV-subj-bm25 X ), MAP component (distance from the query term occurrence to subjective words in its window, run  X  X  X ist-FD-FV-subj-bm25 X ) im-proves MAP op by 1.67% and 0.54% (not significant) on 2008 and 2007 topics respectively. Adding both KLD and dist compo-and 2007 topics respectively.

The facet distance (FD) component (run  X  X  X D-FV-subj-bm25 X ) improves performance by 0.23% and 0.98% over  X  X  X V-subj-bm25 X  on 2008 and 2007 topics respectively. In Blog 2008 there are only 10 topics that have more than 1 facet in the query, while in Blog 2007 there are 7 such topics. Table 13 shows results calculated based on only these topics. 5.3. The effect of topic type on performance ing to see whether the developed methods are equally effective in finding opinions about different entity types. We manually grouped the topics in Blog 2007 and 2008 into categories based on the type of opinion targets. The categories and their cor-responding topic identifiers are given in Table 14 .
 MAP op results by opinion target type of the  X  X  X imitedQE-Wiki-phrase X  baseline and the experimental runs are presented in
Fig. 8 . As can be seen from the graph, the category in which the experimental runs achieved the highest improvement over on average the highest MAP in all runs. Topics of types  X  X  X edia/Art X  and  X  X  X iscellaneous X  appear to have on average no or little benefit from the proposed opinion-finding methods. Topics in the category  X  X  X iscellaneous X  are mostly abstract con-cepts, for example,  X  X  X scar fashion X  (Topic 927),  X  X  X alk show hosts X  (1044),  X  X  X ax break for hybrid automobiles X  (Topic 1014) and  X  X  X niversal health care X  (Topic 1046). Topics, such as 927 and 1044, performed poorly because they use general talk show hosts. A relevant document is likely to refer to a talk show and its host by name and may not contain words  X  X  X alk show X  and  X  X  X ost X . One possible way to improve the performance of such topics is by expanding the query with the list of names of specific entities (talk shows and their hosts in this example), obtained for example from Wikipedia. As for topics, such as 1014 and 1046, they address rather complex subjects. Many people would express their thoughts and opinions on such topics by using more complex linguistic constructions that are not captured by our methods. 6. Conclusions
In this paper new methods of retrieving documents containing opinions expressed about an entity or entities specified by the user in the query were proposed. The main stages of the proposed methods are as follows: (1) Collection pre-processing. Our experiments demonstrate that this stage has a significant impact on the effectiveness of document retrieval from blogs in terms of both topic-and opinion-relevance. The major performance-improving steps at this stage include the removal of HTML tags, scripts and style definitions, and all lines where the hyperlinks account for 50% or more of the words. (2) Query processing. A method of building faceted queries by utilising Wikipedia was developed. The method consisted of the following steps: identifying concepts in the topic titles by matching them to Wikipedia article titles, grouping concepts into facets, and expanding each facet with new concepts by using Wikipedia article redirects and valid abbreviations.
The evaluation of different query processing levels demonstrates the merit of all three steps in query processing. Expand-ing queries using only target pages, redirected to from the Wikipedia page titles found in the query ( X  X  X imitedQE-Wiki-phrases X ) is better than expanding the query with other pages redirecting to the same targets ( X  X  X ullQE-Wiki-phrases X ). (3) Document retrieval. Retrieval of the initial document set using a topic-based ranking method, such as BM25. (4) Opinion-based document re-ranking. Three methods were proposed:  X  KLD-based method (KLD), using the Kullback X  X eibler divergence scores of the subjective words in the windows around query term occurrences.  X  Proximity-based method (dist), using distances between a query term occurrence and each of the co-occurring subjective words.  X  A method combining the previous two (KLD+dist).

In addition, all of these methods contain a Facet Distance component, which factors in the distance between query terms/ phrases from different facets, and a Facet Validation component, which down-ranks documents that do not contain at least one concept from each facet.

Evaluation demonstrates that the proposed methods are highly effective, and are among the best-performing methods developed by the Blog track 2007 and 2008 participants. Specifically, the proposed methods achieved the highest improve-ments over the standard baseline run provided by Blog 2008 organisers  X  X  X aseline 4 X  compared to other opinion-finding runs submitted by the participants.

Series of experiments were conducted to determine the effect of the major components (FV, FD, KLD and dist) on perfor-mance. The results indicate that all components, in general, have a positive effect on performance. However, the proximity of query terms to subjective words does not always improve the performance when used in conjunction with KL divergence of subjective words. Specifically,  X  X  X LD+dist-FD-FV-subj-bm25 X  yielded lower MAP bm25 X  on limitedQE-Wiki-phrase baseline (Blog 2007 topics).  X  X  X LD+dist-FD-FV-subj-bm25 X  and  X  X  X LD+dist-FD-FV-subj-b4 X  on the other hand, yielded higher MAP op and R-precision op topics) and Baseline 4.

An analysis of the methods X  performance by topic categories based on the type of entity expressed in the query was per-formed. It was found that the methods are most effective in finding opinions about events, products, geographical locations and people. They were least effective in finding opinions about entities in the category  X  X  X edia/art X , which included TV shows, films and books, and in the category  X  X  X iscellaneous X , which mostly contained abstract concepts.

Among the future extensions of this work, we think that there is a lot of potential in further utilising Wikipedia for finding concepts related to the opinion targets expressed in the query. People commonly express opinions about an entity indirectly, by referring to its related concepts, such as opinions about a composer are expressed by talking about his/her music, or opin-ions about a company are expressed by discussing its products and services. Our approach to facet-based query structuring and retrieval could be extended further and used to represent more complex queries. For instance, if somebody wants to find opinions about London as a travel destination, they are likely to be interested in opinions about places of interest, museums, hotels, restaurants, etc. A possible structured faceted query could be: Facet 1 ( X  X  X ondon X ) AND (Facet2 ( X  X  X useum X ,  X  X  X ritish museum X ,  X  X  X ational gallery X ,  X  X  X ate Modern X ) OR Facet 3 ( X  X  X estaurant X ,  X  X  X af X  X ,  X  X  X ub X )).
 References
