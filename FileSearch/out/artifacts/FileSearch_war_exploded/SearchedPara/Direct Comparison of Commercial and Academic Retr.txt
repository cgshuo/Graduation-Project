 H.3.3 [ Information Storage and Retrieval ]: Systems and Software X  performance evaluation Experimentation, Measurement Google Desktop, Indri, Baseline, Enterprise email retrieval
Searching for information has increasingly prevailed in people X  X  life. A new trend in the study of information re-trieval is the amplified interest in search over corporation and personal information collections. Recently released com-mercial desktop search engines are the achievements due to this trend. Comparing to the Web search engines, these desktop search engines are working on collections with rel-atively small size, and often rely on the machine power of a single desktop computer. These new developments provide an interesting opportunity for the evaluation of academic retrieval systems. Academic retrieval systems are systems developedinacademicenvironmentforresearchuse. Many of them are open source systems. For many years, due to various reasons like collection size and machine resources, commercial search engines and academic retrieval systems are studied and evaluated in non-interrelated environments. Now, since commercial desktop search engines and academic retrieval systems are working on collections with similar size and on comparable machine power, it becomes possible to directly compare their performance.

In this paper, we want to compare two untuned off-shelf retrieval systems. They are Google Desktop (GDS) v20051208 and Indri 2.0. GDS is a popular desktop search engine, which provides full text search on various types of files in a personal computer, including emails 1 . Google didn X  X  dis-close what is the retrieval model used in GDS. Indri 2.0, is a state-of-art retrieval system developed by University of Massachusetts Amherst and Carnegie Mellon University 2 . http://desktop.google.com/ http://www.lemurproject.org/indri/ It combines an inference network with a language-modeling approach, and has been widely used in TREC experiments. Thereasonthatweusethesesysteminanoff-shelfmanner without any tuning is because that is often what happens when most users use retrieval systems 3
Our research questions were: 1) Can commercial desktop search engines like GDS be employed in direct comparison with the state of art academic retrieval systems like Indri on TREC like platform? 2) How does GDS X  X  performance comparing to that of Indri in off-shelf manner? 3) Is there any limitation of GDS could be identified through our study? Our study was built on the email search tasks defined in Enterprise Track of TREC 2005 [1]. The email search task had two subtasks: searching for known emails in the collec-tion (called  X  X nown item search X ) and searching for emails discussing certain topics (called  X  X iscussion topic search X ). The collection was W3C email collection, which was based on the crawls at w3c.com website in June 2004. After remov-ing empty or duplicated emails, the W3C email collection consisted of 174,294 emails with average size 9.8KB. In our experiments, Indri was runni ng on a Dell 8300 computer, CPU 3.2GHz, Fedora 4 OS. GDS was running on a Dell 3000 computer, CPU 3.0GHz, Windows XP Pro OS.

In the known item search task, there were 125 evaluation topics, and the average length of queries was 5.42 words. Top 100 returned emails were evaluated against the relevant judgments provided by TREC. The measures included the mean reciprocal rank (MRR) of the correct answer, the frac-tion of topics with the correct answer in the top 10 returned emails (Success at 10 or S@10), the fraction of topics that found the correct answer in the returned 100 emails (S@inf).
In the discussion topic search task, there were 59 topics, each of which has a short title and a longer description. The average length of title was 3.65 words, whereas that of the description was 21.3 words. Top 1000 returned emails were evaluated against the ground truth provided by TREC. The measures were the mean average precision (MAP), R Precision (R-Prec), and Precision at top 10 (P@10).
We performed in total five differ ent retrieval runs for dis-cussion topic search using GDS and Indri: Indri-TitDes: queries used the titles and the descriptions, and the searches We acknowledge that our experiment is slightly bias to GDS since GDS is meant to search emails, whereas Indri is designed for much general types of documents. Tuning is generally needed for Indri to search emails effectively. Table 2: The Results of Discussion Topic Searches were on Indri; Indri-Tit: queries used the titles only, and the searches were on Indri; GDS-TitDes: queries used the titles and descriptions, and the searches were on GDS. GDS-Tit: queries used the titles only, and the searches were on GDS. GDS-Tit-Rev: queries used the terms in the titles but in reversed order, and the searches were on GDS.

To stick to the untuned off-shelf approach, we used Indri as a straightforward plain retrieval system without employ-ing any extra technique to boost the performance of Indri system, including various query expansion techniques. How-ever, after noticing that the word order of a query had im-pact in GDS searches, we performed all email searches on GDS with query terms in their original and reverse order respectively. We also list best result of TREC-2005 on the same tasks as  X  X REC-Best X  in tables to indicate the true state of art of the academic email search.
As shown in Table 1, in known item search, the two GDS runs (i.e.,GDS: with original query term order; GDS-Rev: with reverse term order) outperformed the Indri run. The relative MRR improvement of the two GDS runs over that of Indri run were 17.7% and 24.0% respectively. The difference was significant between GDS-Rev and Indri (two-tailed t-test resulted in p values of 0.046), and nearly significant between GDS and Indri (p = 0.076).

Interestingly, GDS-Rev achieved significant improvement ( p =0.012) over GDS when measured by S@10. This might indicate that the word order in GDS is really important. However, without knowing the actually query processing techniques in GDS, we cannot tell for sure.

The results for discussion topic search are in Table 2. It seems that GDS has trouble handling long queries. GDS-TitDes generated really low performance. The other two GDS runs (GDS-Tit and GDS-Tit-Rev) outperformed ei-ther of the two Indri runs, but the improvement was only significant when comparing the two GDS runs with Indri-TitDes under the measure of P@10. The p -values in two tailed paired t-test were 0.008 and 0.003 respectively. We also plotted the results of four runs Indri-Tit, Indri-TitDes, GDS-Tit, GDS-Tit-Rev into 11-points precision/recall graph(seeFigure1). TheFigureshowsthatGDSpreformed relative superior at high precision end over Indri runs, but almost no difference at high recall end.
 Figure 1: 11-points precision graphs showing per-formance on discussion topic search.

In both tasks, TREC-Best greatly outperformed all GDS and Indri runs. This shows the effect from training, and tuning, and also demonstrates the limitation of using off-shelf systems directly without any modification. GDS did demonstrate some limitations through our study. Firstly, it has problems in handling long queries. When searching for discussion topic with queries containing terms from titles and descriptions, which in average are about 25 words long, GDS was struggled in performing the search, and returned averagely only 0.07 relevant documents per topic. Secondly, although not being documented anywhere, it is apparent that the order of query terms does make no-ticeable or even significant difference on search results in GDS. Therefore, it is important to study further to estab-lish the optimal order of query terms in GDS.

Although not directly from our study, we identified several other issues with GDS. First, it doesn X  X  support explicitly assigning weights to query terms as those in Indri. GDS only allow user use repetitive words to emphasize some query words [2]. This limits GDS X  ability to provide more advanced control of the retrieval system. Second, according to GDS Help Center, GDS only indexes the first 10,000 words in a document. Although this did not make great impact to rather short email documents, it will limit GDS X  X  ability to handle long news documents that are common in TREC.
In this paper, based on TREC Enterprise email search tasks, we performed initial comparison between GDS, a com-mercial desktop search engine, and Indri, a widely used aca-demic retrieval system. Although GDS still have issues re-garding long queries, long documents, and the optimal se-quence of query terms, it generated comparable results to the plain untuned Indri system in both known email search task and discussion email search task. As a representative from popular commercial desktop search applications, GDS is a good baseline for establishing the direct comparison be-tween commercial and academic retrieval systems. [1] N. Craswell, A. P. de Vries, and I. Soboroff. Overview [2] T. Calishain and R. Dornfest. Google Hacks . O X  X eilly,
