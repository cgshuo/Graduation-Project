 YAOYONG LI and KALINA BONTCHEVA University of Sheffield, UK 1. INTRODUCTION Patents are an important vehicle for protecting intellectu al property and this importance is increasing in the current globalized and knowledge-based economy. When researching new product ideas or filing new pat ents people need to retrieve all relevant preexisting know-how and/or e xploit and enforce patents in their technological domain. Patent classificati on helps these tasks by classifying patents into different categories accordin g to the content of the invention and its claims, which makes it very useful for pate nt retrieval and exploitation.
 The IPC 1 is the most widely used patent classification scheme where pa tents are assigned categories corresponding to their main applic ation domain and topic. In contrast, the F-term classification system, devel oped by the Japanese Patent Office (JPO), classifies patents from multiple viewpo ints (e.g., materials used, operation, problems addressed, solutions proposed) [Schellner 2002]. In other words, each patent could be labelled with many differe nt F-term codes with respect to different aspects of its content. Since the F -term system adopts a much finer classification scheme than the IPC and it classifie s patents from multiple viewpoints, it complements the IPC by adding riche r information to facilitate patent retrieval and exploitation. For example , a so-called patents map can be constructed from the F-term codes of all patents in a given techno-logical domain, showing for example the problems addressed and the solutions they provided [Liu and Luo 2007]. Such patent maps help searc hers to survey easily all existing patents and to assess the advantages and disadvantages of their own patents against them [Iwayama et al. 2005].
 logical fields covered, and the intrinsic complexity of pate nt documents, auto-matic processing and classification is required. Machine le arning algorithms have been used successfully for text classification and info rmation retrieval [Cancedda et al. 2003; Lewis et al. 2004]. As patent informat ion processing is a subarea of text processing, machine learning can also be ap plied to patent in-formation processing and, in particular, to patent classifi cation and retrieval. This article applies a state-of-the-art machine learning a lgorithm, the Support Vector Machine (SVM) to F-term classification of patents.
 of documents. For example, a patent document is typically qu ite long, con-tains multiple required sections, and uses highly formaliz ed legal and tech-nical terminology. Patents are effectively semi-structur ed documents, with different aspects of the patent application being presente d in a predefined set of sections and subsections (e.g., prior art, patent claims , technical problem addressed and effect). In addition, F-term patent classific ation is different in nature from conventional document classification, where do cuments tend to be assigned only one class. In contrast, each patent is labelle d with many F-terms using information from the different sections, which makes automatic F-term classification a multiclass and multilabel problem. Anothe r important aspect of this problem is that the classification labels, the F-term s for example, are in hierarchical relationships with each other. Section 2 pres ents further discus-sion on the specifics of F-term classification. The point we wa nt to make here is that in order to achieve the best performance, one has to ad apt the machine learning algorithms to the specific requirements of F-term c lassification. method which achieve state-of-the-art results in classific ation and regres-sion tasks, including document classification (see e.g., [J oachims 1998; Li and Shawe-Taylor 2003]). SVMs have also been applied to the pate nt F-term clas-sification task. For instance, at NTCIR-5 [Tashiro et al. 200 5] reported an SVM-based system using the popular one-vs.-rest approach t o convert F-term classification into a binary classification problem. Since e ach F-term consists of two parts (a viewpoint and an element) [Tashiro et al. 2005 ] trained separate SVM classifiers for each F-term viewpoint and element. For ea ch patent docu-ment and F-term they applied these two SVM classifiers and com bined the two scores into a single number. They used the sigmoid function t o transform the SVM output (before thresholding) into a real number between 0 and 1 and also an optimization procedure to find the optimal values for the p arameters of the sigmoid function. A year later at NTCIR-6 there was another S VM-based sys-tem [Rikitoku 2007], which also uses the one-vs.-rest appro ach. An interesting aspect of this work is that they took into account the fact tha t the number of negative examples is significantly greater than the positiv e ones. In order to deal with this imbalance in the training data, [Rikitoku 200 7] randomly se-lect and remove many negative training examples, which not o nly improved accuracy but also reduced the training time. Both NTCIR syst ems used the standard SVM settings and did not attempt to adapt the learni ng algorithm to the hierarchical, multiclass nature of F-term classificati on. In addition to the default SVM settings, the authors did use other techniques, such as the sig-moid function, but they do not report explicitly what improv ement is obtained in this way. The key point though is the overall system result s on the NTCIR F-term classification data were worse than that achieved by o ther learning algorithms such as kNN [Iwayama et al. 2005; 2007; Murata et a l. 2007]. ument classification [Joachims 1998; Lewis et al. 2004] but p erformed worse on F-term classification, our conjecture is that the SVM algo rithm itself needs to be adapted to make better use of the specifics of patent clas sification. The motivation behind this article is to explore this idea in det ail and evaluate it against all other NTCIR-6 systems and thus against many othe r learning algo-rithms. The idea is viable because the SVM learning algorith m is quite flexible and includes many geometrical intuitions. This makes it ada ptable to differ-ent task-specific features, as shown by the techniques prese nted in this article. Therefore, the main novel aspect of this article is in explor ing successful adap-tations of SVM learning to the F-term classification task. In addition, we also investigate the improvements which can be gained from using the different sections of patent documents and other resources.
 F-term classification subtask at NTCIR-6 (see [Li et al. 2007 ]). We experi-mented with the uneven margins SVM which can deal with the imb alanced training data and also explored normalizing the SVM outputs to enable the comparison of results obtained from different SVM classifie rs. Our system achieved best results among all participating systems in tw o of the three met-rics used in the subtask. This article is an extension of our i nitial NTCIR-6 work [Li et al. 2007]. Here we present two more effective tech niques and their systematic evaluation.
 classification in general and the specific settings of the NTC IR-6 data which we use for evaluation. Section 3 describes our adaptations of t he SVM learning al-gorithm for F-term classification and other experimental de tails including the features used. Section 4 presents the experimental results showing the bene-fits of the proposed techniques. Section 5 discusses related work and Section 6 provides an overall discussion and draws some conclusions. 2. F-TERM CLASSIFICATION As discussed above, patent classification is a prerequisite for the efficient re-trieval and exploitation of know-how and there are two widel y used classifica-tion schemes: IPC and F-terms. In this section, we provide fu rther details on the F-term one, which is a two-level classification scheme. T he first level, de-noted as FI (or File Index), is an extension of IPC and refers t o the patent X  X  topic and domain. For example, FI 2C088 covers  X  X inball game machines (i.e., pachinko and the like) X , whereas theme 5J104 denotes the technologi-cal field of  X  X iphering device, decoding device and privacy c ommunication X . Each theme has a collection of viewpoints for specifying pos sible aspects of the patents within the given theme. Each viewpoint has a list of p ossible elements. Those viewpoints and the corresponding elements for one the me are encoded by the theme X  X  F-terms, which are the second level of the pate nt classification scheme. The viewpoints are different from one theme to anoth er. Each partic-ular viewpoint may consist of several elements, which are or ganized in a tree structure. Going back to our example, the theme 2C088 has the viewpoint AA for  X  X achine detail X , the viewpoint BA for  X  X rocessing of pachinko ball X , and the viewpoint BB for  X  X ard systems X . The viewpoint AA has elements such as AA01 :  X  X tandard pachinko games (i.e., vertical pinball machine s) X  and AA65 :  X  X pecial pachinko games X . Hence, F-terms under each theme h ave specializa-tion/generalization relations among them.
 assigned F-terms from that theme. A patent may have one or mor e themes and many F-terms for each of them.
 scripts used in the F-term classification subtask of NTCIR-6 . In fact, our system participated in the subtask and obtained excellent r esults in compari-son to other participants. In this article we report further developments and new techniques after our NTCIR-6 participation. In section 4 we will present the results of our systems at NTCIR-6 and the more recent resu lts. To help readers unfamiliar with the NTCIR-6 F-term classification t ask, we provide a brief description next. For more details, please see the ov erview article [Iwayama et al. 2007].
 ment of suitable F-terms to patent documents, given their th eme(s). It uses the Japanese 1993 X 1997 UPA (unexamined patent application s) for training and the 1998 X 1999 UPA patents for evaluation. The English tr anslation of the abstracts of these Japanese patent applications are also pr ovided and can be used as a surrogate text to aid classification. There are appr oximately 1,200 valid themes and each theme may have up to several hundred F-t erms. and 1920 training documents and 5F033 with 620 F-terms and 7314 training documents, respectively. In the formal test run, 108 themes were selected. The number of F-terms per theme is between one and 800s and the number of training documents for each theme is between one and 10,000s . 3. SVM-BASED SYSTEMS FOR F-TERM CLASSIFICATION 3.1 Extracting Features from the Patent Documents As discussed above, the NTCIR-6 patent classification subta sk used Japanese patent documents. It also provided the so-called PMGS docum ents 2 which in-clude a brief description (several words in most cases) of ea ch F-term and the hierarchical relations among the F-terms under each theme. Our experiments used those two types of information to derive features for tr aining the SVM machine learning algorithm.
 of sections and subsections, each of which addresses a speci fic aspect of the patent application. For example, typically patents would h ave an abstract con-taining a concise description of the invention. Another sec tion would describe the patent in detail with subsections describing the purpos e, function, and im-plementation of the invention. A patent document would also contain some bibliographical data such as the name and address of the appl icant and their associated company.
 into account the bibliographical data, although it would be worthwhile explor-ing the usefulness of this kind of information for patent cla ssification, since a particular applicant or company would typically be filing pa tents in the same domain and on related themes. We also ignore the patents X  cat egory codes, as this was the requirement of the NTCIR-6 patent classificatio n task. documents and classifying them into seven categories, name ly Abstract, Claim, Technological field, Problem purpose, Method, Effect function , and Implemen-tation example . We found that most patents use the same or very similar section titles, although there are some variations. The top 100 most frequent section titles from the training documents were classified m anually. These ti-tles cover more than 99.9% of the occurrences of all section t itles in the training corpus. The small number of other, unclassified section titl es are classified au-tomatically into one of the seven categories if their title c ontains one word from the category X  X  name. Section titles containing words from m ore than one cate-gory are classified as belonging to all categories with which they share a word. For example, a section entitled  X  X revious techniques and pr oblems solved X  is classified into the two categories: Technological field and Problem purpose. The Abstract and Claim categories contain the text from the two sections with the respective titles. The other four categories X  Technological field , Prob-lem purpose , Method and Effect function  X  X orrespond to subsections of the detailed description section. The category Implementation example covers im-plementational details, such as constitution of the invent ion and examples. material. Each description was regarded as an additional tr aining document and added as a positive example for that F-term, to be learnt b y its correspond-ing SVM classifier.
 analyser Chasen version 2.3.3. 3 From each document, we selected as feature terms all words whose part of speech tag was either noun (but n ot dependent noun, proper noun, or number noun), independent verb, indep endent adjec-tive, or unknown, following the approach of [Makita et al. 20 03]. All terms ap-pearing less than three times in the training corpus were rem oved. Next, we computed the tf  X  idf feature vectors [Joachims 1998] for each of the Japanese patent documents and F-term descriptions and finally normal ized these vec-tors so that they became the input to the SVM learning algorit hm. 3.2 SVM-Based Learning Algorithms Support Vector Machines (SVM) is a supervised machine learn ing algorithm and in its basic form it is used for binary classification. Giv en two classes, the SVM algorithm tries to find a hyperplane in the feature space t hat maximally separates the training examples belonging to the first class from those of the other class [Cristianini and Shawe-Taylor 2000]. Given a te st example, the SVM classifier computes a confidence score and then decides wh ether the ex-ample is in the first or second class by comparing the confidenc e scores with a threshold (which is 0 in the standard SVM). The confidence sco res themselves are useful in some applications, e.g., ranking the test exam ples. The SVM algo-rithm has been extended to cover multiclass classification a nd other problems with complex output labels (see e.g., [Cesa-Bianchi et al. 2 004; Tsochantaridis et al. 2004]). During training, the SVM algorithm solves a qu adratic opti-mization problem based on the training examples. In general , SVM classi-fiers tend to have better generalization capability on unsee n data than other distance-or similarity-based learning algorithms such as k-nearest neighbor (kNN) or decision trees.
 using SVMs for F-term classification did not manage to achiev e satisfactory results. The reason is that F-term classification is differe nt from conventional document classification in several aspects. First, F-term c lassification is a mul-ticlass, multilabel classification problem, where each pat ent would be assigned several different F-terms (or labels).
 ated differently. In the former, a classifier is learned and e valuated for each category, whereas in the latter each test documents is label led with many F-terms and evaluation is document-centric, otherwise, it counts how many correct F-terms are assigned to each document.
 F-term using the one-vs.-rest strategy (also used by [Rikit oku 2007; Tashiro et al. 2005]), which leads to imbalanced training data where the positive examples are vastly outnumbered by the negative ones. Howev er, previous work has demonstrated that the standard SVM algorithm canno t obtain good F-measure results on imbalanced training data (see [Lewis e t al. 2004; Li and Shawe-Taylor 2003]).
 nature of patents, where sections fall into one of the seven c ategories dis-cussed above and the F-term classifiers can also make use of th e PMGS short descriptions.
 among the F-terms within each theme, it is desirable that, if a document is not assigned the correct F-term then at least it is given an F-ter m which is closely related to the correct one. Consequently, in order to achiev e better results one needs to use a learning algorithm which takes into account th ese hierarchical relations between the classes.
 a highly specific multiclass, multilabel problem, it is nece ssary to adapt the standard SVM algorithm to the specifics of this task in order t o achieve the best possible performance.
 rate classifier is trained for each F-term belonging to a give n theme, using the NTCIR-6 training data. We used the one-vs.-rest approach, a ccording to which patents assigned the particular F-term are positive exampl es and those with-out it are negative ones. Then, given a patent document of thi s theme, we apply each of the F-term classifiers to it and obtain a confiden ce score of the appropriateness of each F-term for this document as well as a classification decision on whether or not the document has the F-term. In thi s way, a ranked list of F-terms is obtained based on the confidence scores. Fi nally, several mea-sures such as A-Precision, R-Precision, and F-measure are c omputed for the F-terms assigned to the document. Both A-Precision and R-Pr ecision are com-puted from the ranked list of F-terms, while F-measure is obt ained from the classification decisions of the F-term SVM classifiers for th e document. Those measures will be explained in more detail in section 4. Next w e focus on the techniques to adapt the standard binary SVM algorithm to the specifics of F-term classification. anced in the sense that the positive examples are vastly outn umbered by the negative ones. The experiments in Li and Shawe-Taylor [2003 ] showed that SVM with uneven margins can achieve higher F-measure than th e original SVM on such imbalanced training data. Consequently, here we experimented with SVM with uneven margins, instead of the standard SVM alg orithm. document, we had to compare the confidence scores of differen t SVM classi-fiers. To make a fair comparison, the confidence scores of diff erent SVM clas-sifiers should be in the same range. The range of the SVM confide nce scores relies on the support vectors, which are those training exam ples being close to the SVM hyperplane and determining the hyperplane. If each o f the SVM clas-sifiers only has genuine support vectors (corresponding to z ero slack variables) which have the SVM output as 1, then the outputs of those SVM cl assifiers would be in the same range and comparable. However, an SVM cla ssifier may potentially include many so-called bounded support vector s or outliers (corre-sponding to nonzero slack variables) (see Cristianini and S hawe-Taylor [2000]) which leads to SVM output less than 1. Hence the outputs of tho se SVMs may not be in the same range. In other words, in practice the confid ence scores from different SVM classifiers may not be comparable and it is advisable to normalize the confidence scores (see Goh et al. [2001]).
 where the x represents the feature vector of an example, w is the weight vector and b is the bias term in the SVM model. Our normalization involved two pa-rameters. One was the mean C s v of the confidence scores of all support vectors. The second one was the 2-norm C w b of the vector consisting of the weight vector and the bias term b of the SVM. The normalization was done by dividing the confidence score of each test example by both parameters C s v and C w b . By nor-malizing the SVM outputs with C w b , the outputs of different SVM classifiers for one feature vector (or document) would be normalized, wh ich facilitated the comparison of outputs between the different SVM classifiers (corresponding to different F-terms) on the same feature vector (correspon ding to one patent document). By normalizing the SVM X  X  output using the C s v , the effect of the outlier support vectors on the SVM X  X  output was alleviated. We then converted the normalized output into a value between 0 and 1 via the sigm oid function s ( x ) = 1 / (1 + exp (  X   X  x )), where  X  was set as 2.0 in our experiments. the patents with and without the F-term as positive and negat ive examples, respectively. In most cases the positive examples are rare i n comparison to the negative ones. On the other hand, since an F-term only rep resents one as-pect of the patent X  X  content, two patents which are differen t from each other in one aspect, F-term for example, may share some other aspects , such as other F-terms. Hence, it is possible that a patent from the negativ e examples for the first F-term may contain text which is similar or overlaps with text from a patent in the positive examples. Such similarity between n egative and pos-itive examples may pose difficulties for the learning algori thm as they would be hard to separate. Therefore, it may be helpful to remove th ose negative ex-amples that are too close to the positive ones in order to obta in a hyperplane which can clearly separate the two.
 which are very close to the positive ones. The first method is b ased on the shared F-terms between negative and positive examples. It e ntails the selec-tion and removal of those negative examples which share one o r more F-terms with the positive ones, because shared F-terms indicate pot entially shared or similar content. The second method employed a filtering tech nique during SVM training. In this case, we learn two SVM classifiers for ea ch F-term X  first an SVM classifier is learned from the entire training dat a, then a number of negative examples which are closest to the SVM hyperplane are removed from it, and then another SVM classifier is trained on the rema ining training data. It is the second SVM classifier that is then applied to th e test documents. it requires more training time than the first method which req uires only one classifier per F-term. On the other hand, in the second method the SVM clas-sifier is directly involved as a reference for selecting the n egative examples, while the first method uses the number of shared F-terms, whic h only gives a rough indication of the closeness between the negative and t he positive exam-ples. We can therefore expect that the second method may be mo re accurate than the first one, which was confirmed in our experiments, as d iscussed in section 4. ument. As discussed in section 3.1, each patent document consists o f several sections describing different aspects of the invention. At the same time, the F-terms are also describing different viewpoints. Therefo re, it is worthwhile experimenting whether for a given F-term, some patent secti ons have stronger discriminative capability than others. Therefore, simila r to Fujino and Isozaki [2007], we experimented with training a classifier on each se ction and then combining them (see section 4.2 for details on the combinati on methods and their effectiveness). investigate learning algorithms which take into account th e hierarchical rela-tions between the F-terms within a given theme. Therefore, w e carried out ex-periments with training and application of SVMs in a hierarc hical fashion. We use a variant of the standard SVM algorithm called H-SVM whic h is designed especially for hierarchical classification and obtains bet ter results than learn-ing without considering the relations among labels (see Ces a-Bianchi et al. [2004]).
 eralization relations among the F-terms, which effectivel y form a hierarchy. First an SVM classifier is trained for each of the most general F-terms, by se-lecting as positive examples all training documents which h ave assigned either the given F-term or a more specific one. For each less general F -term, an SVM classifier was trained on all documents which have assigned t he F-term itself or its parent F-term. During the classification of unseen dat a, H-SVM first classified the test documents using each F-term classifier. T hen we used two different ways in which to obtain the confidence score for eac h F-term. The first method was to use the confidence score of the F-term class ifier alone. The second method was to average the confidence scores of the F-te rm classifier itself and all the classifiers of its ancestor F-terms. Our ex periments demon-strated that the second method obtained better results than the first. Once the confidence scores for each of the F-terms are calculated, one can easily order the F-terms on that basis and thus complete the classifi cation decision for the given document. In comparison to the standard flat SVM classification, H-SVM takes into account the relations between the class lab els so one can then expect that if a document cannot be assigned the correct F-term, then H-SVM would have more tendency than the flat SVMs to assign an F -term which is closely related to the correct one.
 method was indeed more likely to assign a closely related F-t erm when uncer-tain, overall H-SVM performed much worse than the flat SVM whe n assigning the correct F-term. The main reason for the failure is that H-SVM is designed for assigning exactly one label per document whereas patent F-term classifica-tion is a multi-label task. For further details and experime ntal results, please see Li et al. [2007]. 4. EXPERIMENTS 4.1 Experimental Settings The different methods discussed in the previous section wer e evaluated on the data from the NTCIR-6 patent classification subtask. In all t he experiments, we used the dry-run data for theme 5J104 to develop the system, for example, determining optimal values of the parameters in the learnin g algorithms. The resulting system was run on the data from the formal run, whic h contains 108 themes.
 formal run, and applied to the corresponding test data. The o fficial evalua-tion scripts provided by the NTCIR organizers was run on the t est documents to obtain the evaluation results. The metrics reported by th ese scripts are A-Precision, R-Precision, and F1. Consequently the result s presented below can be compared directly to the official results of the patent classification sub-task at NTCIR-6 4 and in particular those in Iwayama et al. [2007]. examples, ranked by the scores measuring the relevance of th e test examples to a given category (or query). A-Precision is the mean of the Precisions at all Recall levels of the ranked test example list. R-Precision i s the mean of the Precisions at some predefined Recall levels. On the other han d, F-measure is computed from a particular subset of test examples which the system judges to be relevant to one category (or query), instead of a ranked list of all test examples. The F 1 is the harmonic mean of Precision and Recall with respect to the subset. Please refer to Li et al. [2007] for a more detai led explanation of these and other more generalized metrics for F-term classifi cation. and then ran our own implementation of the uneven margins SVM during classification (see Li and Shawe-Taylor [2003] for details) . We used all default settings of SVM light except for the parameter C , which was set to 2 in all our ex-periments after some preliminary experiments on the dry run data (for details see below). 4.2 Experimental Results This section presents the experimental results which prove the effectiveness of the methods presented in Section 3.2, which are aimed at ad apting the standard SVM classifier to the specifics of F-term patent clas sification. The standard SVM is used as the baseline and then the proposed met hods are ex-amined one by one against that baseline. The results are pres ented, including their assessment with some statistical tests. First we disc uss the results ob-tained by the methods used in our original NTCIR-6 participa ting system and then present the new results using the last two methods.
 SVM baseline, but instead we assess whether they bring any im provement in comparison to the results of our original NTCIR-6 system. Th e paired test for statistical significance is used to compare the results with and without each new technique in order to measure its impact. The results rep orted below are obtained on the data from the formal run, micro-averaged across all 108 themes.
 sures for each of the 108 themes in order to enable a more detai led compar-ison and analysis. As we can obtain 108 data points (correspo nding to the 108 themes), we performed some statistical significance tes ts for two experi-ments using the 108 samples. We report the paired t-test results which were calculated using a free online software. 6 In fact, we also experimented with the z-test 7 and the Wilcoxon matched-pairs signed-ranks test 8 , however we found that these two tests gave the same overall result as the t-test in all our experiments and consequently, due to space limitations, we only report here the results from the paired t-test. used only the patent abstracts, while others used the full te xt. The first set of experiments compares the classification performance usi ng different parts of the patent documents X  X he Abstract section only, the full text, and the full text plus the short descriptions of each F-term. In the latte r case, each F-term classifier was trained on the full text of the patent plus the d escription of that particular F-term, but without using the descriptions of al l other F-terms as negative examples.
 approach, described in section 3.2. The standard SVM implem entation in SVMLight was used with the default parameters discussed in t he previous subsection. The cost parameter C was set to 2 and that value was determined to be best by comparison against two other possible values (1 and 3) on the dry-run data, theme  X 5J104 X . Using the full text of the train ing patents we ob-tained A-Precision results of 0 . 4481 ( C = 1), 0 . 4530 ( C = 2), and 0 . 4365 ( C = 3). Therefore in the rest of the experiments we always use C = 2.
 108 themes, when using the different parts of the patent data , which show that the full text gave better results than the abstracts alone. M ore importantly, one can see that using the F-term descriptions as training data i s indeed helpful in improving the classification results.
 the 108 themes separately and performed the paired t-test on the 108 data points to compare two different settings. For the results of A-Precision us-ing abstracts only and those of using full text, the mean of th e latter minus the former equals 0 . 03568 and the 95% confidence interval of this difference is [0 . 0322 , 0 . 0391]. t = 20 . 43 and the two-tailed P value is less than 0.0001, showing that the difference is extremely statistically sig nificant. Comparing the A-precision results between the full text and the full te xt plus F-term, the mean of the latter minus the former is 0 . 00109 and the 95% confidence interval of this difference is [0 . 00079 , 0 . 00139]. t = 7 . 1906 and the two-tailed P value is less than 0.0001, proving that the difference is also extr emely statistically significant. As already discussed above, we also applied the z-test and the Wilcoxon matched-pairs signed-ranks test and they produce d the same con-clusions. We also applied these tests using R-Precision and F-measure and obtained the same conclusions: there is a statistically sig nificant difference in performance on the three types of patent data. However, it sh ould be noted that, although the difference is extremely statistically s ignificant in all cases, the actual differences vary for the different pairs of paren t data used and the different metrics. For example, the difference in F1 betwee n full text and full text plus F-term definitions is much less than that between th e abstract and full text only. Nevertheless, the difference is statistica lly significant in both cases, meaning that one setting was better than the other set ting for most of the themes.
 they were obtained from the application of the standard SVM w ithout using any of the adaptation techniques experimented with in this a rticle. The impact on results from using those methods will be presented next. SVM often achieved much better F1 scores than the standard SV M on imbal-anced data where the negative examples vastly outnumber the positive ones. Unlike the standard SVM model which treats the positive and n egative ex-ample as equal, the uneven margins SVM [Li and Shawe-Taylor 2 003] uses the uneven margin parameter  X  to adjust the ratio of the positive margin to the negative margin of the classification hyperplane in the f eature space. In our experiment we set the uneven margins parameter  X  to 0 . 5, where  X  = 1 . 0 corresponds to the standard SVM model.
 els for all F-terms, which is equivalent to the same shift of t he confidence scores in all SVM models, which means that this does not affect the ra nking order of those scores. Therefore the A-Precision and R-precision of the uneven margins SVM models are the same as those of the standard SVM, whereas F -measures including Precision, Recall, and F 1 are affected.
 against those of the standard SVM. We can see that the standar d SVM (  X  = 1) obtained much higher Precision in comparison to Recall. In c ontrast, the un-even margins SVM obtained balanced Precision and Recall and higher F 1 . By performing the t-test on the 108 data points of F 1 , we found that the mean of difference is 0.0482 and the 95% confidence interval of thi s difference is [0 . 0421 , 0 . 0542]. t = 15 . 75 and P &lt; 0 . 0001, meaning that the uneven margins SVM obtained statistically significantly better F 1 than the standard SVM. margins parameter  X  = 0 . 5.
 vector of the SVM model to facilitate the comparisons of the s cores of the dif-ferent SVM models for a given test document. Table III compar es the results with and without the weight vector normalization of the SVM m odel. We can see that the weight normalization did improve the results. H owever the dif-ference was not as big as expected, in particular for R-Preci sion, which was confirmed by the t-test results. The t-test showed that A-Pre cision improved significantly, as t = 10 . 71 and the two-tailed P value is less than 0.0001. How-ever, R-Precision did not improve significantly, as t = 1 . 0143 and P = 0 . 3128, and the 95% confidence interval of the difference is [  X  0 . 0008 , 0 . 0025]. ization technique. amined in order to select and remove those negative examples which are close to the positive ones. The first method is based on the number of F-terms shared between the negative and the positive examples. The other on e is to filter out the negative examples which are closest to the SVM hyperplan e in feature space in order to reduce the number of possible outliers and t o improve the performance of the SVM classifier. The first experiments are r un on theme 5J104 from the dry-run data to compare the two methods and als o to deter-mine how many negative examples need to be removed.
 amples in different rates on the dry run data (theme 5J104). T he filtering rate refers to the ratio between the removed negative examples an d all negative examples. The lower part presents the results obtained by th e shared F-term method for three values for the number of shared F-terms. We c an see that the best A-Precision results of the shared F-term method were gi ven by setting the shared number of F-terms to be greater than 10, which remo ves 19% of the negative examples on average. However, the best results of t his method were a bit worse than the best results of the SVM-based filtering me thod. was then used on the formal run data. We present the results us ing the fil-tering technique in Table V, together with the results witho ut filtering for comparison. The table also presents the results by removing some negative examples based on the shared F-terms with positive examples . It reported the results for the best setting tested on the theme  X 5J104 X , as shown in Table IV. First we can see that removing some negative exampl es based on the SVM learning indeed improved the results in all the three measures. By performing the t-test for A-Precision at the 95% confidence i nterval, a differ-ence of [0.0066, 0.0102] is obtained, t = 8 . 8489 and P &lt; 0 . 0001, meaning that the filtering technique significantly improved A-Precision . The same holds for R-Precision and F 1 as well. Second, the shared F-terms based filtering method obtained slightly worse overall performance on the 108 them es than those not using any filtering technique. To compare the results of the s hared F-terms method with those not using any filtering technique, we perfo rmed the t-test on the samples from the 108 themes and, for A-Precision, we ob tained the 95% confidence interval of the difference as [-0.005859, 0.0000 33], showing that the method obtained slightly better results for some themes but slightly worse re-sults for other themes. t = 1 . 9599 and P = 0 . 0526, meaning that the difference is not quite statistically significant. By comparing the res ults of the shared F-terms based filtering method and the SVM learning-based fil tering method, we also performed t-test and for A-Precision obtained the 95 % confidence in-terval of the difference as [-0.014140, -0.008352]. t = 7 . 7035 and P &lt; 0 . 0001, showing that the SVM learning-based filtering method obtain ed statistically significant better results than the shared F-term based meth od. tent of each patent document is divided into the seven catego ries: Abstract, Claim, Technological field, Problem purpose, Method, Effect function , and Implementation example . For each given F-term, we train an SVM classifier from the text belonging to each category. Consequently, the re are seven SVM classifiers for each F-term. We combined the confidence score s of those SVM classifiers by multiplying each of them with a weight and then summing them up, as showed in the following formula: where x represents a patent document, s i is the confidence score from the SVM classifier for category i , w i is the weight for the score s i , and n is the number of categories which is equal to 7 in our experiments.
 termine the weights w i . One approach is to employ another learning algorithm (Naive Bayes) to estimate them [Fujino and Isozaki 2007]. He re we experi-ment with three different methods. The first is to select the m ost suitable cate-gory (obtaining the highest A-Precision value) for each F-t erm using three-fold cross-validation on the training data, and then set the weig ht for that cate-gory as 1 and the weights for all other categories as 0. The sec ond method is to determine the weights by training a linear SVM from the con fidence scores for a given F-term. In another words, we learn the seven weigh ts using the SVM on the seven-dimensional input vectors, each of which co ntains the seven confidence scores. In the third method we used equal weights, namely setting every weight w i as 1. We compared the three methods on the dry-run theme 5J104. Table VI presents the results of the three methods, to gether with the baseline which used the full text without dividing the docum ent into different parts.
 baseline. Using an SVM classifier for weighting has much wors e results, show-ing that it is not possible to learn the correct weights from t he seven confidence scores. Selecting only one category for each F-term using cr oss-validation ob-tained slightly worse results than the baseline. Therefore we use the equal weighting for the combination of different confidence score s.
 mal run data (108 themes). As can be seen in Table VII, both A-P recision and R-Precision are improved, but the overall F 1 remains the same. By per-forming the t-test for A-Precision at the 95% confidence inte rval, the difference obtained is [0.0063, 0.0102], t = 8 . 4379 and P &lt; 0 . 0001, meaning that the com-bination technique significantly improved A-Precision. Th e same conclusion is applicable to R-Precision. However, for F 1 at the 95% confidence interval the difference is [-0.00199, 0.00203], t = 0 . 0201 and P = 0 . 984, meaning that there is no significant difference between the two F 1 results.
 techniques together, our system outperforms all best resul ts reported during the NTCIR-6 patent classification evaluation, according to all three evaluation metrics used (see Iwayama et al. [2007]). 5. RELATED WORK Most work in automatic F-term classification is reported wit hin the participat-ing systems in NTCIR-5 and NTCIR-6 [Iwayama et al. 2005, 2007 ]. Several popular learning algorithms such as kNN, Naive Bayes, maxim um entropy, and SVM have been applied to this problem. kNN has obtained go od results on this particular problem in comparison to other methods [Mur ata et al. 2007]. As already discussed in the introduction, the straightforw ard application of the default SVM algorithm failed to obtain results as good as kNN (see Tashiro et al. [2005]). Fujino and Isozaki [2007] adopted a two-laye r method, using the maximum entropy algorithm to learn one classifier for each of the several fields of the patent documents and employed Naive Bayes to learn the weights for combining the classifier results. It obtained one of the best runs in the NTCIR-6 patent classification subtask. Hashimoto and Yukawa [2007 ] investigated a classification method based on term weighting using Chi-squ are statistics. Un-fortunately it did not achieve as good performance as other m achine learning methods.
 see the start of section 3.2. Previous works on the applicati on of SVM to F-term patent classification were discussed in detail in sec tion 1, where we also motivated our approach and its novelty. 6. DISCUSSION AND CONCLUSIONS Support Vector Machines are a popular machine learning algo rithm, which achieves excellent performance in many applications, incl uding document clas-sification. Overall, patent F-term classification can be reg arded as a document classification problem. However, due to the unique aspects o f the problem, the direct application of the standard SVM cannot obtain as good results as other learning algorithms such as kNN. This article investigated several techniques to adapt the SVM algorithm and to tailor its application to pa tent F-term clas-sification.
 using the same metrics and settings as those used by the NTCIR -6 participa-tion systems. Therefore our experimental results can be com pared to those of the participating systems. In fact, we participated in the o riginal evaluation using some of the techniques examined here. During NTCIR-6, our system obtained the highest scores according to R-Precision and F-measure, and the second best A-Precision score.
 filtering out some negative examples and combination of the S VM classifiers learned from different parts of the patent documents. With t hese new methods added, our system obtained the highest scores according to a ll three metrics, in comparison against all participating systems. In additi on, our system also benefited from using the full patent text and the F-term descr iptions as extra training material.
 tion (see e.g. Lewis et al. [2004] and Li and Shawe-Taylor [20 03]), the best results on patent F-term classification are still much lower . This is proba-bly due to F-term classification being a much harder task, as d iscussed in Section 3.2.
 lem, while most learning algorithms are designed for single -label classifica-tion, which explains why H-SVM could not obtain good results (see Li et al. [2007] for detailed discussions about H-SVM). More importa nt, as each F-term reflects just one aspect of the patent, two patents could be di fferent in one aspect while being similar in other aspects, which further c omplicates their correct classification and hence poses a much more difficult c hallenge for the classification algorithms.
 ion. So far there has been little work that exploits the hiera rchical relations among the F-terms, while most treat them as independent of ea ch other. We believe that further performance gains could be achieved ex actly through ex-ploiting these relations in the learning algorithms.
 provides an interesting opportunity for the machine learni ng community to improve their methods and systems.
 We would like to thank the anonymous reviewers for their valu able com-ments and suggestions on improving the article, especially for the sugges-tion on experimenting with the filtering method based on shar ed F-terms (see Section 3.2).

