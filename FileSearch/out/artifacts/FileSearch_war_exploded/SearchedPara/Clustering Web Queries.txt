 Despite the wide applicability of clustering methods, their evaluation remains a problem. In this paper, we present a metric for the evaluation of clustering methods. The data set to be clustered is viewed as a sample from a larger pop-ulation, with clustering quality measured in terms of our predicted ability to discriminate between members of this population. We measure this property by training a classi-fier to recognize each cluster and measuring the accuracy of this classifier, normalized by a notion of expected accuracy. To demonstrate the applicability of this metric we apply it to Web queries. We investigated a commercially oriented data set of 1700 queries and a general data set of 4000 queries. Both sets are taken from the logs of a commercial Web search engine. Clustering is based on the contents of search engine result pages generated by executing the queries on the search engine from which they were taken. Multiple clustering al-gorithms are crossed with various weighting schemes to pro-duce multiple clusterings of each query set. Our metric is used evaluate these clusterings. The results on the commer-cially oriented data set are compared to two pre-existing manual labelings, and are also used in an ad clickthrough experiment.
 Experimentation, Algorithms I.5.3 [ Pattern Recognition ]: Clustering X  algorithms ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  clustering Clustering, Clustering Evaluation, Query Intent Detection
Despite their wide applicability, clustering methods suf-fer from notable problems, including the evaluation of re-sults. For example, consider the domain of text clustering where many clustering, methods have been applied success-fully ([15, 28, 35, 12, 16]). A major issue with clustering evaluation in text domains is that it is typically based on a comparison to ground truth labelings produced manually. Typical evaluation metrics such as normalized mutual in-formation ( NMI ), purity, and f-measures all equate higher similarity to ground truth with higher quality clustering re-sults. These metrics are reasonable measures of quality when one requires clusterings similar in form to the ground truth being used. Problems arise when two or more unrelated, but useful, ground truths exist. Metrics based on ground truth will identify a clustering result as  X  X ood X  with respect to at most one such ground truth. In the absence of ground truth; algorithm-specific objective functions are often used for evaluation. However, when such functions are used, it is often unclear how to compare results from clustering al-gorithms run on different weightings and/or representations of the same data set. In addition, objective functions neces-sarily have a bias favoring a specific kind of structure, and there are no assurances all useful labelings of a data set will exhibit this structure.

In this paper, we present a metric for evaluating the qual-ity of clustering results that does not require comparison to ground truth or the use of a specific clustering algorithm X  X  objective function. The key to our method is to place clus-tering in a larger framework: We view the data set to be clus-tered as a sample of a larger population, and the clustering will be applied in some (as yet unknown) task on the popula-tion. Regardless of the task, we can say that discrimination is a key property of clustering in this framework. We present a classifier based solution to measure the expected discrim-ination quality on the population. We train a classifier to recognize each cluster and take the accuracy of this classifier, normalized by a notion of expected accuracy, as a measure of the overall quality of the clustering. Classifier bias is dealt with by allowing potentially many classifiers to be applied to the same clustering, with only the best (most discrimi-nating) classifier used in the final measure. By measuring clustering quality in this way, not only are ground truths not required (we may find very different clusterings all with good quality) but the problems associated with comparing clusterings based on different weightings/representations of a data set are avoided.
We apply our evaluation technique in the context of Web query intent analysis. Classic efforts to detect user goals in Web search depend upon user surveys, interviews, and the manual inspection of query logs. Seminal work by Broder [6], Rose and Levinson [25] and Lee et al. [21] recognizes a fun-damental distinction between navigational queries and in-formational queries. The intent behind a navigational query is to locate a specific page or site on the Web, which is known or expected to exist. The intent behind an informa-tional query is to learn something about a specific topic, with a lesser regard for the source of that knowledge, provided that it is reliable. More recent efforts to improve sponsored search have recognized a distinction between commercial and non-commercial queries [9]. The intent behind a commercial query is the possible purchase of product or service, either immediately or in the future.

Queries may be classified along both dimensions. For ex-ample, the query  X  X niversity of Washington X  might be clas-sified as a navigational/non-commercial query, seeking the university X  X  home page; the query  X  X heap air tickets X  might be classified as an informational/commercial query, seeking discount travel from any source.

In this paper, we replace manual inspection of query logs with automatic inspection through clustering and the use of our clustering quality metric. While nothing can entirely replace user surveys and interviews for understanding user intent, Web query intent analysis is an ideal area to apply clustering methods and use our metric to select good clus-terings as multiple ground truth of intent are already in use (commercial/non-commercial and navigational/information-al). We may gain additional insight by applying clustering methods and examining the good clusterings (by our metric) that arise naturally.

In our initial experiments (Sections 2-6), we focus on bi-nary clusterings (which we call  X  X plits X ) in order to bet-ter compare our clustering results with manually labeled splits corresponding to the navigational/informational and commercial/non-commercial dimensions. We use ten clus-ters in our final experiment, reported in Section 7, where manual labels are not available. This second experiment provides a finer grained view of general Web queries.
In Section 2 our clustering algorithms and our commer-cially-oriented data set of search engine logs are detailed. In Section 3, we perform a standard analysis of our clustering splits, except that two manual labelings are used. We show that many of the splits learned by the clustering algorithms on this data set are similar to the manual commercial/non-commercial split, while none are strongly similar to the man-ual navigational/informational split. This section highlights the problem with using ground truth to evaluate clustering. In Section 4 we explain our new classification-based metric for assessing clustering quality, comparing them to a tra-ditional measure of internal similarity. We use this metric in Section 5 to select clustering results for further study, showing that two strong learnable splits exist, one of which is related to the manual commercial/non-commercial split, the other of which is not closely related to either manual split. Section 6 discusses an experiment to predict advertis-ing clickthroughs, applying the results of the previous clus-tering experiments. In these experiments, the best cluster-ings of each of the two strong learnable splits are incorpo-rated into models of ad clickthrough behavior to illustrate that they may reflect user intent and therefore have external validity. Section 7 details the results of the application of the clustering algorithms and weighting functions in Section 2 to a separate set of general Web queries. Our quality metric from Section 4 is used to select the result to examine. This result is shown to be an easily interpretable partitioning of Web queries, providing further evidence regarding the valid-ity of our metric. Section 8 gives our concluding discussion.
The problem of clustering Web queries has received recent attention. A comparison between manual query intent label-ings and unsupervised labelings was reported by Nettleton et al. [23]. In their work a Kohonen SOM was compared against Broder X  X  categories [6]. Baeza-Yates [2] discusses using k -means clustering on web query logs. In later work Baeza-Yates et al. [3] compare the results of Probabilistic La-tent Semantic Analysis [14] with categories taken from the Open Directory Project 1 . Beeferman and Berger [4] design a hierarchical clustering algorithm for web queries and URLs. Hosseini and Abolhassani [17] design another hierarchical al-gorithm for URLs and queries, which applies singular value decomposition before clustering.
In the following subsections we detail the data set for the experiments reported in Sections 2-6 including its prepro-cessing and our clustering algorithms.
The data set we used in our initial experiments is a sample of web search engine logs obtained from Microsoft adCen-ter, sampled over a few months. The data set was designed to investigate commercial search (as we do in Section 6) and includes a record of queries entered, ads displayed and ads clicked. Personally identifying information was removed from this data set. The data includes a sample of roughly 100 million search impressions, where an impression is de-fined as a single search result page. Queries are assumed to be in the English language. Any extra space at the be-ginning and end of the queries, and between words of the queries, were removed. All queries were case-normalized. We found about 27 million queries occurring only once in the impression file, mostly with no ads. Such queries were removed from the impression data. Impressions with a du-plicate combination of impression id and user session id were removed in order to filter out repeated queries from the same user. The queries subject to manual labeling were the same 1700 queries used in [1], selected through the following pro-cedure: the original impression file was sorted based on the time of the impression. Starting from an arbitrary point in the file (approximately 1 / 5 of the length of the file from the beginning), 1700 queries were selected for which the ad click frequency of the query was above 10. Because of this ad click based filtering, we refer to this set of 1700 as commercially-oriented . In Section 7 we will examine a data set without ad click filtering.

Each selected query was then manually labeled by three researchers in our lab. Working independently, each re-dmoz.org searcher labeled each query as either navigational or infor-mational, and as either commercial or non-commercial. In the case of disagreement, the majority opinion was taken. These labelings served as two equally valid interpretations of ground truth in our experiments, and will be referred to as manual splits throughout the rest of the paper.
For each query, we have two types of features available: i) the contents of a search engine result page ( SERP ) gener-ated by executing the query on a commercial search engine, and ii) the query-specific features extracted from the query string. Since the experiments in Section 6 are based on ad clickthrough, SERP content was filtered to remove ads to avoid tainting the clickthrough prediction experiment, leav-ing only the organic search results for feature generation. For the majority of the paper, Sections 3-5 and for the ad-ditional experiments in Section 7, we use only the SERP features. The query-specific features are used only in Sec-tion 6, when user behavior is considered.

Clustering algorithms typically require data sets of real vectors. SERPs may be converted to this form by first re-moving all tagging and other extraneous information. In this form, a SERP can be represented as an unordered collection of words. A feature vector may then be derived from this collection of words. The set of SERPs, one for each query, may be represented as a set of vectors, each one of the form where tf i , the term frequency of word i , is the number oc-currences of word i in the SERP. We apply the above trans-formation to the SERPs to generate tf vectors. We further remove all words entirely from the vectors that occur in less than 4 SERPs.

In addition to working directly with these raw term fre-quency vectors, we experiment with various weighting func-tions, each giving a different set of vectors. Weighting func-tions for text clustering most often follow a tf-idf scheme. In such schemes, the final weighted value for term i is some function of its term frequency ( tf ) multiplied by some func-tion of its inverse document frequency ( idf ). A length nor-malization is typically applied after all terms have been weighted, such that each vector is of unit Euclidian length. The set of weighting functions we used is summarized in Table 1. Each weighting function was applied to the tf vec-tors to produce eight different weighted data sets. We refer to each weighting method by its short name listed under the column Short in Table 1. The o weighting function is a variant of Okapi BM25, where b and k 1 are constants. Although normally learned from training data, here we as-sume values of b = 0 . 75 and k 1 = 1 . 2, which are typical for BM25. Several of the other weighting functions are simpli-fied variations of o . kt may be obtained by removing the idf component and setting b = 0. Kt and kti functions may be derived using other simplifications. The t function is the raw SERP vectors. log tf -log idf is a common weighting function in clustering research.  X  is binary weighting, 1 if a word occurs once or more, 0 otherwise. i is simple idf weighting. To our knowledge none of the variants of Okapi BM25 we used here have been previously applied to text clustering. However, the BM25 weighting is known to perform well for search, and many of the reasons for its success in that do-main may apply equally well here (and we shall see that it does).

All the clustering algorithms we selected produce a strict partitioning of a data set. We refer to such a partitioning as a clustering , and each group within the partition as a clus-ter . Each of the algorithms takes k , the number of clusters to return, as a parameter. For reasons detailed in the introduc-tion, our selection of clustering algorithms was not based on expected ground truth approximation. Instead, we focused on algorithms that are both relatively simple and have been widely applied to text clustering problems in prior research. Our selections were as follows: 1. k-means clustering ( kmeans ) using Lloyd X  X  method [22] 2. Normalized-Cut Spectral clustering ( spect ) [27] 3. UPGMA clustering ( upgma ) [29] 4. Single Link clustering ( slink ) [18] 5. Complete Link clustering ( clink ) [18] 6. Document clustering algorithms based on some of the
K -means using Lloyd X  X  method is a classic partitional clus-tering algorithm that looks for k centroids of a data set such that the total squared Euclidian distance of each data point to its nearest centroid is minimized. The spectral clustering algorithm we used is discussed in Shi and Malik [27].
UPGMA, single link, and complete link clustering are from the same family of agglomerative clustering algorithms. In each of these methods every point begins as a singleton cluster and clusters are progressively merged with the best similarity. In single link similarity between clusters is equal to the similarity of their closest points. In complete link the farthest points are used. In UPGMA the average similarity between all points is used.

We apply the objective functions from Zhao and Karypis in an agglomerative framework: every object begins in its own cluster. Clusters are merged iteratively such that the objective function is optimized for each individual step. The e1 function is based around minimizing the weighted simi-larity of cluster centroids from the centroid of the whole data set. The i2 function is essentially the k means objective function except any similarity metric may be used in the calculation. The i1 function is similar to UPGMA. The h1 function is i1 / e1 . Both g1 and g1p are graph-cut based objective functions. The g1 function is based on MinMaxCut [11], while the g1p function is a combination of g1 and e1 .

For all our clustering algorithms except k -means, cosine similarity was used. Our k -means method used squared Eu-clidian distance. As we used random starting points in our k -means algorithm it was not deterministic. To compensate for lack of determinism we ran it 20 times with each weighted data set and kept the best result by the k -means objective function for further analysis. Each k -means clustering was run till complete convergence.

The first step in our experiments consisted of clustering each weighted data set using each clustering algorithm de-tailed above, with two clusters as output. This resulted in 88 different binary clusterings, one for each (algorithm, weighted data set) pairing. Each of these will be referred in the form &lt; weighting &gt; -&lt; algorithm &gt; throughout the paper.
Before any attempt was made to evaluate the clusterings without ground truth, we were interested in determining which clusterings were similar, if any, to either manual split (in a typical experiment, this would be done to measure the quality of clustering algorithms). Our intention was to determine if the clusterings naturally fit the manual splits. Similarity to the manual splits was assessed using normal-ized mutual information ( NMI ). NMI between two discrete random variables X and Y is given by the equation I ( X ; Y ) is the mutual information between X and Y , defined as where H ( X ) is the entropy of X
NMI may be viewed as the reduction in uncertainty of X knowing Y  X  X  value. This value is symmetric and between 0 (independent) and 1 (perfectly associated). Here, X is a clustering and x is a cluster. Y is one of the two man-ual labelings and y is a category for that labeling. Let n be the size of the data set (the number of queries). Then we estimate p ( x ) = | x | n , p ( y ) = | y | n , and p ( x, y ) = Each of our 88 clusterings had its NMI with both manual splits computed. The top 5 NMI clusterings with respect to the manual commercial/non-commercial split are given in Table 2. Table 3 gives similar results for the manual navi-gational/informational split.

Table 2 shows that some of the clusterings share substan-tial similarity with the commercial/non-commercial split. A maximum NMI of 0 . 29432 was obtained by the weight-ing/algorithim combination kt-e1 .

The magnitude of similarity is more easily observed with keywords. We use a weighted pointwise mutual information formula to quantify the keyword strength of a term to a sin-gle cluster or category. Pointwise mutual information ( PMI ) Table 2: Top five NMIs for the commercial/non-commercial split.
 Table 3: Top five NMIs for the naviga-tional/informational split.
 is defined as Our weighted pointwise mutual information (WMPI) metric is defined as Here, x is a cluster from a clustering or a label category from a manual labeling; and y is a word. p ( x ) = | x | n and p ( y ) = data point having one or more occurrences of word y ; p ( x, y ) is the probability a data point has one or more occurrences of word y and belongs to the category or cluster x ( p ( x, y ) = where y occurs once or more.

WPMI rewards a word for being common in a category or cluster and for having high PMI. Table 4 shows the high-est WPMI words for each category in the commercial/non-commercial split and its two most similar clusterings from Table 2.

A commercial cluster is fairly obvious in both of the clus-tering results in Table 4. The non-commercial clusters also match the non-commercial category to a reasonable extent. We suggest that the very similar keywords but only moder-ate NMI indicate that the commercial clusters in our clus-tering results correspond to queries that evoke strongly com-mercial responses in search engines. This is different from the manual commercial/non-commercial split  X  in that split commercial was defined as the query itself having commer-cial intent. Many clustering results were similar to the com-mercial/non-commercial split. This suggests that commer-cial/non-commercial or some variation on that theme may be a natural split for commercially-oriented Web queries (at least from the standpoint of SERP features).

Table 3 shows that no clusterings shared significant NMI with the navigational vs. informational split. The highest NMI value obtained was 0 . 02636 ( i-i1 ). Despite little evi-dence in our clusterings, the navigational/informational split has been shown to exist quite strongly in practice [6, 25]. Further, in the following Section we show that SERPs of the data set do contain the necessary information to build a nav-igational/informational classifier (80.4% accuracy). Non-SERP text features of search engine logs may contain strong-er indications of the navigational/informational split. These features include dwell time  X  the latency between SERP display and the first click  X  and the shape of clickthrough curves [21].

We make a final observation in this section. In the pre-ceding experiment we had two manual splits  X  a naviga-tional/informational labeling and a commercial/non-com-mercial labeling. These were almost entirely unrelated (NMI of 0.0305). If we treat these as ground truth to evaluate the clustering algorithms we see significantly different results de-pending on the ground truth. This highlights the problem of using ground truth similarity as a quality measure when multiple ground truth are/may be valid. If we pick a certain ground truth as our standard of quality, we may or may not see any good results, regardless of how well it performs with respect to other ground truths.
From the previous section, it can be said ground truth metrics evaluate algorithms by determining if an algorithm can find a certain structure. This is not the same task as evaluating the clusterings themselves. Every clustering al-gorithm has an internal objective function which it attempts to optimize. These objective functions represent the algo-rithm X  X  notion of a good clustering; they do not use true labels, except in semisupervised situations. Variations on k -means [5, 22, 34] use deviation from centroids as quality measures. Likelihoods and model fitting are common ob-jective functions [10, 7]. Information bottleneck methods such as Gondek et al. X  X  [12] are based around preserving im-portant information in the clustering. SVM based objective functions have also been designed [31].

A specific objective function may be used as an evaluation metric to compare clusterings. Unfortunately, it is usually unclear how to compare objective function evaluations on clusterings based on different representations of the same data set. If we measure the average similarity of objects in the same cluster and use that value as a quality measure, we might get wildly different results for each weighting scheme (we show that this is the case later in the section). Also, ob-jective functions have a bias to certain structures, and there is no reason to believe in general that all useful clusterings may exhibit that same structure.

It is worth noting that selecting the number of clusters is related to evaluating clusterings (in that the clustering with the  X  X orrect X  number of clusters is better). There has been significant research on selecting the best number of clusters. Pelleg and Moore [24] use an information criterion approach to iteratively split a data set in to the best number of clus-ters. Hamerly and Elkan [13] perform something similar ex-cept confidence from a Gaussian fitness test is used to select the number of clusters. These two methods may be viewed as wrappers of single algorithms, and like standard algo-rithms their objective functions are not really comparable across clusterings on different weightings/representations of a data set. Gap statistic methods [30, 33], which select the number of clusters using some form of internal similarity, have a similar issue.

Information criterions (such as BIC [19]) may be applied to evaluate general clusterings of a single data set by balanc-ing the fitness of the clustering (to a certain model) versus the number of parameters used in the fit. Again though, if different representations of the same data set are used in a single experiment comparison in this method becomes unclear. Stability-based methods for comparing clusterings such as Lange et al. X  X  [20] might be applicable across mul-tiple algorithms, representations, and numbers of clusters, however, some research [26] has shown that stability may not be a suitable measure of clustering quality.

Caruana et al. [8] recognize very different clusterings may be useful for different tasks. They construct a tree of clus-terings based on their labeling similarities with encouraging results (the trees are useful for the tasks shown). However, evaluation in their method is a manual process, users must search through the tree. To automate the investigation of the tree would require some ground truth or specific objec-tive function, introducing their commensurate problems.
To avoid using ground truth and specific objective func-tions for evaluating clustering, we first consider the purpose of clustering. We have a data set. Often, the data set is a sample of some larger population (in our case, we have a sample of Web queries). If this data set is being clustered, we assume that the clustering will be applied in some (as yet unknown) task on the population. Nothing specific is known about the task. Knowing such limited details restricts how we can assess clustering quality. We can, however, say that that regardless of the specific task, being able to discrimi-nate members of the population will be valuable. We can use this discriminative ability as a measure of quality in the absence of a known task. While evaluating in this way does not ensure fitness for specific tasks, it can direct users to clusterings with a definable generic property that is likely useful in a number of tasks.

Ideally manual assessment would be used to estimate the discriminative ability of clustering on the population by ob-serving the clusters themselves. Those assessors would ex-amine the clusters and determine if they can tell them apart. However, there is a time expense associated with this method. Therefore, we opt to use classifiers in place of people. A classifier may be trained to recognize clusters in a cluster-ing. Using crossfold validation of the classifiers we obtain estimates of the discriminative ability of the clustering on the population in the form of classification accuracy.
We denote the ten-fold crossvalidation accuracy of a cer-tain classifier c as acc c . Different classifiers may yield very different acc c .

Unfortunately, it is easy to engineer a situation in which classification accuracy will be arbitrarily high despite the clustering being effectively worthless. This may happen in two ways. First, the data set may be reduced to a trivial representation before clustering. For this paper, we assume that representations are not trivialized in this manner. Sec-ondly, when one cluster contains most of the points in a data set, an extremely good classifier may be designed trivially by assigning all points to the largest cluster.

To factor cluster size balance into accuracy we define a trivial classifier . A trivial classifier is one that assigns all points to the largest cluster/category. The average cross-fold validation accuracy of a trivial classifier, denoted T computed as where X is a clustering or labeling.

Our measure of clustering quality, with respect to the clas-sifier c , then becomes
Our final measure of clustering quality, called normalized accuracy ( N a ), is then defined as where C is the set of all classifiers used. Note that only the best classifier is used in computing N a . This is appropriate, as we are only concerned with our ability to discriminate the clusters, not how the discrimination occurs or if certain clas-sifiers can not discriminate the clustering. Clusterings that maximize N a will be both balanced in terms of size distri-bution and have high accuracy under at least one classifier.
To show that N a can overcome the comparability limita-tions of objective functions between clusterings on different data representations we illustrate a correlation between N using a linear SVM and internal similarity . Internal similar-ity, or within cluster consistency, is a measure of similarity between points in a single cluster. Although optimal clus-terings rarely have the best internal similarity possible, it is generally regarded that higher internal similarity indicates a better clustering. The exact definition of internal similarity we use here, which we denote as ISIM-cos, is defined as where X is the clustering, x is a cluster, i and j are indi-vidual data points and cos( i, j ) is the cosine between i and j . This makes ISIM-cos the average cosine between points in the same cluster. A linear SVM was used with ISIM-cos as they have related biases. Note that in general there is no assurance that an objective function and a classifier will Table 5: The top five clustering N a s and the top N a s for each manual labeling.
 be correlated in the way we show for ISIM-cos and linear SVM. The experiment we present here is intended only to show that for reasonably matched classifiers and objective functions, comparison across data set representations is not a problem. Classifier/objective functions that do not match will obtain lower acc c s and not be used in the final com-putation of N a (only the most accurate classifier is used in Equation 8), so they will not damage the actual process of evaluating N a .

We computed N a and ISIM-cos for each of the 88 cluster-ings. We further took the two manual labelings and com-puted their N a and ISIM-cos with respect to each of our 8 weighted data sets. Figure 1 shows plots of N a vs. ISIM-cos for four of our weighted data sets. A positive correlation between N a and ISIM-cos is visible in each plot. The four other weighted data sets show similar trends but are omitted for space reasons. Notice that ISIM-cos values vary greatly across the four different weightings shown, a comparison us-ing ISIM-cos across these weightings is meaningless. Similar problems would be observable using a large number of objec-tive functions. However, the N a values across the weightings are of the same scale, suggesting N a can be used to compare between clusterings on different representations.
Finally, we note that as the number of clusters in a solu-tion increases, the accuracy of a trivial classifier will drop ( T a = 1 /k with k perfectly balanced clusters). This results in larger N a values as k increases unless accuracy drops to match. From our experiments, this does not happen, thus some penalty must be associated with larger number of clus-ters. However, since the number of clusters is fixed in each experiment, this extension is left for future research.
The N a values computed in the previous section were used to analyze our clusterings. We believe the computation of N a using a linear SVM is appropriate in this case as linear SVMs have been shown to work reasonably well in many text applications, although in general multiple classifiers could be used. Table five shows the top 5 N a clusterings. The top N a for each manual labeling is shown as well for comparison purposes. Table 6 shows the highest WPMI keywords for our clustering results. The clusterings are ordered from highest to lowest N a . Again, the two manual labelings are included for comparison purposes. Their positioning in Table 6 is based on their optimal N a .

The 22 trivial clustering results mentioned in Section 4 had N a close to one. There were also a number of non-trivial clusterings with low N a (such as kt-g1p in Table 6). Given their low N a , all of these were not considered further.
The high N a results proved interesting. Clusterings that strongly resemble the commercial/non-commercial labeling occur in the top results, namely i-e1 and Kt-g1p (the third and fourth lines of Table 6). They share NMIs of 0.146 and 0.055 respectively with the commercial/non-commercial manual labeling. Despite the NMIs, the keywords are very similar.

Three of the top five clusterings in Table 5 and 6 contain a cluster with  X  X ikipedia X  as the first keyword. Queries in these clusters have a strong informational flavor. We re-fer to these clustering as  X  X ikipedia splits X . Nonetheless, they have NMI close to zero with respect to the naviga-tional/information labeling and only a slightly positive NMI with respect to the commercial/non-commercial labeling.
The majority of higher N a clusterings were Wikipedia splits or commercial/non-commercial splits. The only other high N a clusterings contain a strong travel-related cluster and a strong media-related cluster. Such a clustering is il-lustrated by i-clink in Table 6; kti-clink had similar keywords and an N a of 1.30.

To further interpret our results, we performed meta clus-tering somewhat similar to that in Caruana et al. [8]. Each of the 88 clusterings and the 2 manual labelings formed the objects for clustering. Vectors of NMIs were used to repre-sent each object. The i th index of the j th object X  X  vector was the NMI between the i th and j th object. K -means clus-tering using Lloyd X  X  method was applied to this data set (as opposed to generating a tree of meta-clusters as in Caruana et al. [8]). For this experiment, we specified a value of k = 4 for the number of clusters to be returned. Results for other values were consistent.

To minimize confusion, we refer to a cluster of cluster-ings as a meta-cluster . All of the 22 trivial clusterings were put in to a single meta-cluster. A second meta-cluster con-tained only five low N a ( &lt; 1 . 05) clusterings. A third meta-cluster contained a pure collection of 34 of commercial/non-commercial clusterings. The average N a of the clusterings in this meta-cluster was 1.47. The final meta-cluster con-tained all the Wikipedia splits (15 of them), the two strong travel/media splits, all the miscellaneous splits (10 in total), and the two manual labelings. This meta-cluster had a lower average N a of 1.36 because of the weak miscellaneous splits.
We were surprised that the manual commercial/non-com-mercial labeling did not appear in the meta-cluster with the commercial/non-commercial clusterings. This outcome may further indicate that our clusters are somehow fundamen-tally different from the manual labeling.

The results of this section indicate that commercial/non-commercial split and Wikipedia split are two main learnable binary splits using commercial-oriented SERPs. Additional strong learnable binary splits may exist, examining this via other clustering algorithms and/or weighting methods is an avenue of further research. Increasing the number of clusters might also produce more informative, finer grained splits.
To determine if the splits our unsupervised evaluation method gave high scores to had external validity we used two of them to train ad clickthrough prediction models in this section. This experiment was selected as the data set was collected to investigate commercial behaviors. Recall Table 7: The clickthrough rates for categories in the manual commercial/non-commercial labeling and the best automatic commercial/non-commercial ( i-e1 ) and Wikipedia split (  X  -spect ).

Manual Non-Commercial 5 . 91% Commercial 24 . 50%  X  -spect A 6 . 95% B 20 . 05% that the labels for the strongest Wikipedia split came from  X  -spect , while the labels for the strongest automatic commer-cial/non-commercial split came from i-e1 . These two label-ings, and the commercial/non-commercial manual labeling, became the subject of the experiments in this Section. As in Section 4, clusterings are used to generate classifiers here, so we will refer to clusters as categories.
 Initially, a linear SVM classifier was built for each labeling. This was done using both the SERP features and the query features. To test these classifiers for clickthrough prediction properties a distinct test set of 45K unique queries, along with their SERP contents, was used (from the same source as the training data set [1]). For our purposes, the average clickthrough rate of a set of queries is the percentage of queries in that set that had an ad click. We first calculated the average clickthrough rate for the objects in the test set without classifying them and found a clickthrough rate of 10 . 03%. Then the test set was classified using the three classifiers. From these classifications we obtained average clickthrough rates for each category. Table 7 gives these values.

Examining Table 7 shows the unsupervised splits have real meaning in terms of ad clickthrough prediction. This pro-vides further evidence of our algorithm selecting good clus-terings (although the predictions are not as strong as the manual split, the unsupervised splits may well be suited for other tasks as well). It is interesting to note that the cat-egory with higher clickthrough rates in both unsupervised splits has commercial keywords, such as price , discount , and buy . Unsurprisingly, commercial keywords are associated with a greater frequency of ad clicks. We considered the task of clustering general Web query SERPs, as opposed to the commercially-oriented ones used in previous Sections. To do this, we created a sample data set from the same source as the data set in Section 2.1. The filtering procedure on the data set, the collection of the SERPs, and their conversion in to vectors was identical to the procedure described in Section 2.1, except the filtering based on ad clicks was removed, making the queries general as opposed to commercially-oriented. For this experiment we used 4000 Web queries.

All the weighting functions in Table 2 were applied to the vectors to produce eight weighted data sets, each of these of data sets was clustered using the clustering algorithms dis-cussed in Section 2.2. K-means and spectral clustering were omitted from this test as we desired a hierarchy of clusters and only the other nine methods produced such a structure. This gave 72 (clustering algorithm, weighted data set) pairs. Ten clusters were created in each clustering to give a finer grained view. We computed N a for each clustering result in the same manner as in the previous experiment. The best N a value of the 72 clustering results was obtained by the g 1 p clustering algorithm with o weighting. It scored 6 . 80 (a perfect result would be 10 in this case). The hierarchy of clusters obtained by this combination is illustrated in Fig-ure 7. Each rectangle is a cluster, with the number on left indicating the percentage of all SERPs in the cluster, and the words on the right being the top WPMI words for that cluster.

The results illustrated in Figure 7 scarcely need deep in-terpretation. Clusters corresponding to travel, lotteries, fi-nance, education, medicine, cars, shopping, games, enter-tainment and software are all apparent in the leaves of the hierarchy. The easy interpretability of the clustering our clustering quality metric considers best provides a further indication that the quality metric is a useful method for se-lecting meaningful clusterings in an entirely unsupervised fashion. Considering the best result in isolation, it may rep-resent a meaningful division of general Web query types and user intents, at least as reflected by search engine results.
Existing approaches to the evaluation of clustering meth-ods have several limitations. When clustering methods are compared using ground truth, different interpretations of ground truth will favor different algorithms. In other words, the best clustering method will vary from one problem to another. On the other hand, when using specific objective functions for evaluation, differing data representations and weighting functions confounds the comparison.

Our proposed metric views the target data set to be clus-tered as having been drawn from a larger source popula-tion, with the intention of applying the clustering to some as yet unknown task on that source population. Under this framework, we cluster objects using multiple representations and algorithms, treating the ability to discriminate between clusters as the property to maximize. Classification accu-racy, normalized by a form of expected accuracy, is used to measure this discriminative ability, and hence, the quality of a clustering. We call this quality normalized accuracy ( N
Normalized accuracy is comparable over different repre-sentations of the data set and does not depend on ground truth. In two distinct experiments on Web queries, we show that high N a clusterings have external validity. In the first experiment, high N a clusterings of commercially-oriented Web queries were applied to the problem of clickthrough prediction in commercial search. In the second experiment, the optimal N a clustering of a general Web query set is easily interpretable as a reasonable division of Web content.
One future direction for our research is to extend our met-ric to select the number of clusters. Investigating the met-ric X  X  results on other tasks is another avenue, as is using multiple classifiers. [1] A. Ashkan, C. L. A. Clarke, E. Agichtein, and Q. Guo. [2] R. Baeza-Yates. Applications of Web Query Mining. [3] R. Baeza-Yates, L. Calder  X an-Benavides, and [4] D. Beeferman and A. Berger. Agglomerative clustering [5] J. C. Bezdek. Pattern recognition with fuzzy objective [6] A. Broder. A taxomony of web search. ACM SIGIR [7] R. M. Castro, M. J. Coates, and R. D. Nowak.
 [8] R. Caruana, M. Elhawary, N. Nguyen, and C. Smith. [9] H. Dai, L. Zhao, Z. Nie, J. Wen, L. Wang, and Y. Li. [10] A. Dempster, N. Laird, and D. Rubin. Likelihood from [11] C. Ding, X. He, H. Zha, M. Gu, and H. Simon. [12] D. Gondek and T. Hofmann. Conditional information [13] G. Hamerly and C. Elkan. Learning the k in k-means. [14] T. Hofmann. Probabilistic latent semantic analysis. In [15] T. Hofmann. Probabilistic latent semantic indexing. In [16] J. Hu, L. Fang, Y. Cao, H. Zeng, H. Li, Q. Yang, [17] M. Hosseini and H. Abolhassani. Hierarchical [18] A. K. Jain, M. N. Murthy, and P. J. Flynn. Data [19] R. E. Kass and L. Wasserman. A reference Bayesian [20] T. Lange, M.L. Braun, V. Rother, and J.M. Buhmann. [21] U. Lee, Z. Liu, and J. Cho. Automatic identification of [22] S. Lloyd. Least squares quantization in PCM. IEEE [23] D. Nettleton, L. Calder  X an-Benavides, and [24] D. Pelleg and A. Moore. X-means: Extending k-means [25] D. E. Rose and D. Levinson. Understanding user goals [26] S. Ben-David, U. von Luxburg, and D. Pal. A Sober [27] J. Shi and J. Malik. Normalized cuts and image [28] N. Slonim and N. Tishby. Document clustering using [29] R. R. Snokal and P. H. Sneath. Numerical Taxonomy , [30] R. Tibshirani, G. Walter, and T. Hastie. Estimating [31] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. [32] W. Xu, X. Liu, Y. Gong. Document clustering based [33] M. Yan and K. Ye. Determining the Number of [34] B. Zhang, M. Hsu, and U. Dayal. Generalized [35] Y. Zhao and G. Karypis. Evaluation of hierarchical
