 While many multidimensional models of relevance have been posited, prior studies have been largely exploratory rather than confirmatory. Lacking a metho dological framework to quantify the relationships among factors or measure model fit to observed data, many past models could not be empirically tested or falsified. To enable more positivist experimentation, Xu and Chen [77] proposed a psychometric framework for multidimensional relevance modeling. However, we show their framework exhibits several methodological limitations which could call into question the validity of findings drawn from it. In this work, we identify and address these limitations, scale their methodology via crowdsourcing, and describe quality control methods from psychometrics which stand to benefit crowdsourcing IR studies in general. Methodology we descri be for relevance judging is expected to benefit both human-c entered and systems-centered IR. Experimentation, Human Factor s, Measurement, Reliability Relevance judgment; psyc hometrics; crowdsourcing. Despite relevance being 80 years ol d [13] and many attempts to define its contributing factors (for review see [11, 51, 61]), no conclusive results have been draw n and debate continues [38]. Xu and Chen [77] lamented in 2006 how little we still know about the factors influencing relevance judgments, writing: To address this, they proposed a novel statistical framework based upon psychometrics [14] for modeling relevance as a function of any number of contributing factors. In so doing, they sought to enable a new thrust of positivist relevance studies in which a solid foundation statistical hypothesis te sting would offer new traction on this old and thorny issue. While prior studies had posited a wide range of alternative factors with little resolution, factors could now be integrated and empirically assessed to determine the relative impact of each upon overall relevance. Moreover, the extensible framework permitted any newly hypothesized factors to be similarly incorporated and analyzed in order to test if their inclusion would enable the model to better explain observed data. While the overall framework and goa ls of Xu and Chen X  X  work continue to offer enduring value today, our review of the actual mechanics of their psychometric approach has revealed several methodological concerns which could threaten the validity of results derived from their framewo rk as originally proposed. The primary contribution of our paper is to delineate and rectify these methodological limitations such that their framework can live up to its full potential. As an innovative aspect of their work was introducing IR to psychometrics methodology, we expect that the novelty of psychometrics has cont ributed to limited adoption of their ideas. We provide a brief primer to help remedy this. Another contribution of our work is adapting this approach from a traditional interactive research design using student participants to a systems-oriented relevance judging approach using crowdsourced data colle ction. As such, our work straddles the traditional divide between user-centered and systems-centered IR research. Better understanding the factors influencing relevance decisions has potential to not only better explain end-user behavior and expectations, but al so offer new insights into oft-reported disagreement in systems-oriented relevance judging [3, 40, 44, 72]. Potential benefits in clude better understanding: 1) the type and importance of varying relevance criteria; 2) where search systems might best focus effort beyond topicality; 3) how multidimensional judging may yield more reliable overall relevance judgments; and 4) how multidimensional judgments can be effectively collected at scale to enable future systems-oriented evaluations beyond traditional Cranfield topicality [57]. Crowdsourced collection of subj ective data is now firmly-established in the behavioral sc iences, having been shown to faithfully reproduce many past findings [9]. As such, best practices for crowdsourcing from psychometrics may benefit not only user-centered IR studies, by increasing sample size and diversity, but also evaluation of IR systems, by increasing quality, dimensionality, and diversity of judgments. We might evaluate system effectiveness over multiple, weighted relevance dimensions, or maximize diversit y metrics over a distribution of subjective judgments for the same topical intent. We discuss well-established survey design techni ques for ensuring data validity with crowdsourcing. We also pos it that relevance judging might be more reliably crowdsourced by collecting multi-dimensional judgments, then aggregating acro ss dimensions at the individual level. For reproducibility, our st udy data can be obtained online Relevance definitions [51, 61, 63, 73] show a long-standing dichotomy between objective vs. s ubjective perspectives [7, 11, 38]. A growing shift from objective algorithm-oriented relevance to subjective relevance [11, 60 , 62] is emphasized by work inferring neuro-physiological relevance [31, 52]. In contrast with these experimentally advanced but not easily scalable approaches, a psychometrics approach can be crowdsourced. Since relevance is subjective, what are the factors that contribute to relevance judgments? We have known for decades that topicality [34, 51] alone is not sufficient [5, 12]. What other criteria are considered by user s? Early in 1960s, Rees and Schulz [58] suggested 40 variables related to relevance, Cuadra and Katter [21] found 38 factors contributing to relevance, Cool et al. [19] identified six facets of relevance judgments, and Taylor et al., [68] found that relevance criteria ch ange across search task stages. The list of proposed relevance judgments criteria is much longer, and the plurality of these proposals is problematic and carries a number of limitations. Firstly, there are typically many factors in each relevance model. It may be virtually impossible to ask users to assess all of these factors. Se condly, different studies seem to use synonyms or near-synonyms for the same criteria (i.e. utility and usefulness [29]). Thirdly, some factors overlap in their meaning (i.e. new, novel and recen t). Fourthly, often there is no distinction made between the effects of IR systems and documents. For example, the accessibility of a document [62] is a part of efficiency of access [11] and a property of the IR system. The relevance of the document should be defined by properties of the document itself, not the IR sy stem. Past work to address some of the above-mentioned problems include the work of Barry and Schamber [7], who compared their own studies conducted in two contexts (academic an d weather IR), found a significant overlap of criteria. Bateman [8] suggested that the major criteria were fairly stable across different situations though the set of criteria might change. More recently, Saracevic [62] in his synthesis of several decades of work proposes seven groups of relevance criteria: content, objects, validity, situational match, cognitive match, affective match, belief match. Despite such comprehensive attempts at improving our underst anding of relevance criteria, establishing them still remains a work-in-progress [38]. Most previous studies of relevanc e criteria relied on data collected from questionnaires in which ite ms were created based on a combination of intuition and prior research. In a few cases (e.g., [4]), criteria established in prior research were verified with users X  comments (e.g., from a concurrent or retrospective talk aloud protocol). Generally, there is a lack of approaches that ground subjective user responses in a theoretical framework. Building upon Grice X  X  framework of human communication [30], Xu and Chen [77] focus upon the five relevance criteria below: Topicality. Rooted in the heart of hum an document indexing [49], topicality combines aboutness [49] , relatedness [67], and topical relevance [28, 61]. Boyce [12] indicates that users first judge the topicality of document, and only then consider other factors. Novelty. Often, to be considered us eful, documents must inform the user beyond what they already know [43]. Studies by Harter [32] suggest that citations al ready known to users were not considered relevant due to not yielding a change cognitive state. Understandability. A document must be understood to be useful. Studies show across users X  expertis e levels that use of unfamiliar jargon significantly reduces determ ination of relevance [22, 67]. Scope addresses breadth and depth [69]. Levitin and Redman [45] argue that scope is im portant as it affects perception of document quality. Barry equates depth/sc ope and gives examples [6]. Intuitively, a user wants a document broad and specific enough to satisfy the given information need, yet without being so broad or specific that desired information is difficult to extract. Reliability. Typically, information must be perceived as accurate to be considered relevant. Pett y and Cacioppo [15] suggest that message receivers evaluate the quality of the message before accepting it. Reliability was consid ered key to relevance [39]. While we adopt these same five criteria in our study, prior findings [8 , 59, 67, 68 , 71] suppor t that people X  X  criteria to relevance judgment change in di fferent tasks, given different cognitive states as a task changes or the stages of a task are switched [62]. The methodolog y we propose for assessing relevance criteria is independent of the criteria chosen and search task, and it is critical to distinguish any choice of hypothesized multidimensional factors from how such a hypothesis is tested. We believe the primary contribution of both Xu and Chen X  X  work and our own is the methodology for positivist hypothesis testing rather than the actual findings regarding relevance. One may begin with a theory-driven hypothesis to test, or in data-driven hypothesis generation, empirically posit a set of factors, let the data speak for itself, and then seek an explanatory theory. Our decision to use the same five relevance criteria was intended to make our two studies ma ximally similar so that any differences in methodology could be easily discer ned by the reader. Secondarily, by preserving the same criteria across studies, we could most easily investigate whether the revised methodology we propose might lead to real differences in findings for these same criteria. A variety of recent work has in vestigated crowdsourcing methods for IR data collection [1], bot h for Cranfield-style relevance judging [35] and interactive IR studies [78] . Potential benefits include faster, easier, cheaper, and scalable data collection, increasing diversity of data available beyond that provided by traditional assessors and univers ity students, and the potential greater similarity between the crowd and typical IR system users. The primary challenge of crowds ourcing is ensuring data quality while not incurring greater cost outweighing the actual benefits [10] . While a variety of platform and incentive models exist, research has predominantly fo cused on the pay-based Amazon Mechanical Turk platform (AMT), which we adopt in this work. Typical quality control techniqu es include using gold-questions with known answers, and/or assigning the same task to multiple people and comparing agreement between their responses [35]. In this way, one can both asse ss whether individuals produced expected objective responses, a nd aggregate their responses to improve label quality. What thes e techniques largely fail to achieve, however, is qua lity control for subjective tasks. In fact, aggregation in such cases may actually eliminate valid diversity in responses that would be valuable to detect and model. Moreover, initial qualification tests do not verify ongoing behavior, and testable captchas are easily di stinguishable from subjective questions. Turkers have developed their own lingo of  X  X Cs X  (attention checks) and  X  X Cs X  (memory checks) to describe such easily identifiable quality controls vs. actual task work [50]. An early proposed technique from the HCI community was to design tasks to be sufficiently effortful that it is no easier to cheat than to complete an assigned task in good faith [42, 46]. Similarly, while one can try to make tasks engaging and fun [24] , we would prefer a methodology that does not require that work be made entertaining in order to be considered reliable. More systematic procedures, however, come from traditional survey design methodology which predates crow dsourcing. For example, we might pose nearly the same question twice, using slightly different wording, or negating the question when repeating it. For Likert scale questions, we can also check for constant neutral or near-neutral responses, which would pass the above checks. While there was early concern of data validity with behavioral crowdsourcing studies, mounting ev idence has shown consistency of findings with those yielded by traditional lab studies [9] . Psychometrics is the theory and methodology of psychological measurement of cognitive properties, covering techniques for both data collection and analysis [14]. Because cognitive traits are typically latent and cannot be measured directly [14, 55], one investigates the interrelati onships between observed (i.e., measurable) manifest variables from which properties of latent variables can be indirectly deduced [14]. Because observed data are expected to exhibit substantial measurement error, multiple interrogation techniques are typically applied with repetition [14]. Within psychometrics, structural equation modeling (SEM) [36] provides a well-established framework for modeling latent factors, their inter-relationships, and relationships to observed data. SEM derives from path analysis, invented by Sewall Wright in 1921 [76]. Social and behavioral scien ces begin using SEM in the early 1970s, where it has since become widely adopted. Under SEM, latent factors may be hypothesized a priori and/or emerge from the data through analysis. SEMs ar e defined by a set of equations between variables which must be solved from observed data. Linear models are common but not a limitation of SEM. Like graphical models, SEMs can be fully defined by an equivalent graphical representation: the path diagram (e.g., see Figures 2-3 ). Murphy briefly reviews SEM vs. graphical models [54]. Following path analysis nota tion [76], observed variables are shown in SEM by boxes, while ci rcles depict latent factors. Directed edges (i.e., factor loadings ) denote causal relationships in the pointed direction (regre ssion coefficients). A pointed-to variable is said to load on the factor pointing to it. Bi-directional edges denote correlation without causal interpretation. Edge weights for each case denote regression coefficients and covariance, respectively. Because we do not expect the model to perfectly explain observed data, a latent residual error term is typically associated with each observation and estimated with other model parameters. This la tent error term may also be depicted by a circle, or omitted and implicitly assumed. A model is completely parameterized by its factor loadings, factor variances or covariances, and the residual error terms. SEM begins with Exploratory Factor Analysis (EFA), in which statistical analys is proceeds without prior assumptions about the number of latent factors and rela tionships between latent factors and observed data (though we may have prior hypotheses). Model structure can be learned entirely from data, though the number of factors can also be individually fixed to impose a particular independence assumption. Star ting from some initial maximal number of possible factors, we fi rst assume all variables load on all factors. Statistical analysis is then employed to deduce both the number of factors to be kept and their associated edge weights. Those edge weights which are sufficiently low are then pruned from the model X  X  structure to reduce model complexity. Once the model X  X  structure is determined, Confirmatory Factor Analysis (CFA) is employed to assess that particular structure, re-estimating parameters and testing model fit as a function of data likelihood. Positivist significance testing enables a proposed model to be rejected for failing to explain observed data with sufficient likelihood. Since a m odel cannot be proven correct, but only falsified, alternative hypothes es (i.e., competing models) are typically compared relative to one another. While we can evaluate competing models which en code a lternative causation vs. correlation hypotheses, an experimenter must still proceed with care in distinguishing correlati on vs. causation relationships. Observed data are typically assumed to be continuous and normally distributed. Simulation st udies have shown that typical model sizes can be es timated via maximum likelihood with only about 200 observed instances [64]. Studies have shown that ordinal categorical data can still be accurately modeled as continuous, provided there are at least five categories and approximate normality, as in Xu and Chen X  X  study [77] and ours. Our psychometric methodology in cludes: data collection, modeling dimensions of relevanc e and their relationships, and inferring the significance of each dimension for overall relevance. Rather than presuppose any particular definition of relevance, or posing any direct questions about this often tacit concept, we induce relevance as a latent variab le. It is established based upon how well its inclusion better expl ains observed data. A strength of psychometric modeling is that it permits complex, latent factors to be robustly induced indirectly, as a data-driven, hierarchical combination of simpler factors which are more easily queried. The first step of psychometric anal ysis is to design a questionnaire (known as the  X  X nstrument X ) which is issued to participants as a survey. One new to conducting surv eys might assume this is as simple as listing one X  X  questions , and many na X ve crowdsourcing studies appear to do just that; when collected data (predictably) turns out to be poor, the research er simply blames  X  X pammers X . In contrast, it is well-known in social sciences, marketing, and human-computer interacti on fields that effectiv e survey design is a science, and how you ask a question strongly influences the quality of the ultimate answer you receive [17]. The goal of our questionnaire is to measure each of the relevance criteria (see Sectio n 2): topicality, novelty, understandability, scope, and reliability. Following best practices, each dimension is assessed using multiple questions (called  X  X tems X  in psychometrics). We adopt a seven-point Likert scale with anchors at: 1 (strongly disagree), 4 (neutral ), and 7 (strongl y agree). Items were created following esta blished principles [26]: 1. Content must reflect the inte nded psychological variable 2. Be straightforward and avoid complicated terms 3. Avoid leading or presumptive wording 4. Score scales should be  X  X alanced X  by including positively The final principle above suggests that the overall questionnaire should be positively or negatively  X  X eyed X  so that agreement and disagreement can be expected roughly equally. Not only does a balanced distribution help keep respondents cognitively engaged, and avoid skewing results by skew ed question polarity, but it also enables a method of quality control, as discussed below. It is not necessary that every factor ha ve equally balanced questions. Self-consistency is assessed by posing redundant pairs of highly-similar questions which each arti culate the same underlying query with slightly different wording [9]. For example,  X  X  think the information in this passage is wrong X  and  X  X  think some or all of the information in this passage is incorrect. X  In this case, we expect similar answers to each que stion pair and test for this. However, if all questions were similarly keyed, we could not detect an improper  X  X onstant X  re spondent who answered all such question-pairs positively or negatively. We thus also pose redundant pairs of opposit ely-keyed questions. For instance,  X  X t X  X  easy for me to understand most of the information in this passage X  and  X  X t X  X  difficult for me to understand most of the information in this passage X . When analyzing responses, the scale of negatively keyed question is reversed, then re sponses are tested for similarity as with the highly-simila r question pairs above. Another intentional aspect of th e above quality control design is making the task sufficiently effortful that it is no easier to  X  X heat X  than to answer the questionnaire in good faith [42]. To some degree, one might regard our posing multiple questions for each relevance dimension to be similar to the common crowdsourcing practice of posing the same question to multiple respondents and checking for agreement (i.e.,  X  X lurality X ) [35]. However, testing self-consistency of individual respondents supports subjective data collection for tasks in whic h respondents cannot be expected to agree with one another or any fixed gold-standard. A related question is how many questions to include in the survey? More questions could be more informative, but fatigue participants and increase time and cost of data collection. Kenny X  X  Rule offers an established rule -of-thumb for determining how many questions to use per factor for modeling:  X  X wo might be fine, three is better, f our is best, and anything more is gravy X  [76]. However, because some of our questions likely will not correlate as closely with factors as intended, standard practice is to over-generate questions, expecting some will be later pruned during cognitive interviewing, pilot testing, and EFA analysis (further discussed below). For example, while EFA analysis permits more than 3 questions to be retained, we keep only 3 questions given strength of factor loadings and Kenny X  X  Rule above. Prior work has suggested simple captchas [1] for quality control, e.g.,  X  X ow many paragraphs does this passage contain? X  Our pilot study included such a captcha but found it not useful, as later discussed in Section 4.3. Others have also suggested not requiring all questions to be answered and testing for this as a measure of participant effort [41]. However, this seemed unnecessary and inefficient given other controls we already had in place. Finally, the clarity and accuracy of our questionnaire was pre-tested before the actual pilot test using cognitive interviewing [17]. In particular, we asked ten graduate students (not authors of this study) to read a passage and then answer each question. They were then asked to to re-articulate each question in their own words and explain their answers. Based on this, some problems with initial questions were detected and fixed prior to the pilot. While this comes from survey de sign, Alsono has also suggested such pre-testing in general for crowdsourcing studies [1]. Our survey was built and hosted on Qualtrics and posted for data collection to Amazon X  X  Mechanical Turk (AMT) platform, with a brief summary of the task and an external link to the survey. We required workers to have a prior 95% approval rate; with best practices suggesting that such approval rating filtering is necessary but not sufficient in and of itself. We did not require any qualification test or exclude any workers by geographic region (many other studies restrict to U.S. participants as a proxy for English language competency or cultural familiarity). Each worker was allowed to comple te the survey once for $0.26 payment. While we wanted to bound completion time, there are reports of many AMT requesters setting unreasonably short time limits that anger workers [50]. We thus informed workers that they could also email us with their completion code and worker ID if necessary. Qualtrics pr ovided each respondent with a completion code to be entered into the AMT task form for payment. While prior work ha s reported seemingly fraudulent resubmission or alternation of codes [24], we did not observe this. Our study posed three search scenarios for consideration:  X  Health: Imagine that you or your friends are trying to make a plan for fast weight loss  X  Travel: Imagine that you will have a holiday in China for seven days  X  Technology: Imagine that you are writing a paper about the influence of smartphones on society. Participants self-selected the sear ch scenario to work on, and we pre-selected a set of documents to be judged for each scenario. Each participant was asked to read a randomly selected document and complete the que stionnaire for it. For each scenario, we wrote a short search query and submitted it to a commercial search engine: 1)  X  X ethods for rapid weight loss X ; 2)  X  X even day trip to China X ; and 3)  X  X mpact of smartphones on society X . Stratified sampling was then used over Google rankings to approximate decreasi ng relevance classes [16]. In addition to mitigating relevance bias, stratified sampling was also expected to yield a broad set of documents across relevance dimensions. A Webpage was rando mly selected from the top 10% of results, another from the next 10%, and so on, until 10 Webpages had been selected for each scenario. In a real search setting, the actual distribution of relevant vs. non-relevant documents observed by the user could vary greatly, which naturally could influence the use r X  X  relevance thresholds (e.g., being more liberal with few releva nt results, or more conservative when there are many). Just as Cranfield assessment assumes documents are judged indepe ndently when judging only topicality, we extend this inde pendence assumption to judging multidimensional criteria. Text from each Webpage was extracted and standardized to avoid any visual presentation effects. To pilot our study design, we r ecruited 86 participants from AMT to complete our survey. As discussed earlier, while our pilot included a captcha as a quality control measure, we found it to be subsumed by the opposit e question-pairs controls. In particular, respondents who missed the question-pairs controls typically passed the captcha, suggesting its low utility. Moreover, other prior work has suggested respondents may dislike such captchas [50], and so we discontinued using the captcha after our pilot. We also received worker comments suggesting that the initial time allotted (10 minutes) and payment ($0.05) were insufficient, and therefore we increased the working time to 25 minutes and basic payment to $0.1. We also decided to begin offering a bonus payment of $0.16 for responses passing quality a ssurance tests. Our main study collected surveys from 502 AMT workers. Topics were self-selected by workers (Health: 232, Travel: 93, Technology: 177), possibly reflect ing unassessed prior familiarity with the selected topic. We filtered out 118 responses (23.5%) which failed our quality control tests. Specifically, Likert responses to highly similar questions were required to be +/-1 of one another. Following prior work, we imposed a stricter criteria for opposite question-pairs, requir ing identical responses after scale inversion [9, 27]. Of the remaining 384 responses (Health: 173, Travel: 75, Technology: 136), we partitioned data into two sets: 150 responses were used fo r Exploratory Factor Analysis (EFA), and 234 participants for the Confirmatory Factor Analysis (CFA). This partition was chosen because 150 and 200 are the minimal recommended observations for applying EFA and CFA, respectively [75]. Many software packages are available for SEM (e.g., LISREL , SPSS and SAS ). We used freely-available R with the psych and sem packages (cran.r-project.org/web/packages/). Exploratory factor analysis (EFA) is utilized to determine: (a) the number of latent factors underlyi ng responses to the scal e items (i.e. survey questions); (b) the specific scale items that measure each factor; (c) the substantive label assigned to each factor; (d) the nature of correlations between the factors [33]. Regarding sample size, accepted practice is to perform Kaiser-Mayer-Olkin (KMO) Measure of Sampling Adequacy and Bartlett X  X  Test of Sphe ricity to ensure this sample supports valid EFA [75]. KMO  X  X ndicates the extent to which a correlation matrix actually contains factors or simply chance correlations between a small subset of variable s X . Tabachnick and Fidell [66] suggest that values of 0.60 an d higher are required. Bartlett X  X  (1950) test of Sphericity is used to estimate the zero correlation probabilities in the matrix. However, this test is very sensitive to sample size and so must also be applied with care[66]. We evaluated both KMO and Bartlett X  X  Test prior to EFA. The result of KMO was 0.870, with Bart lett X  X  Test yielding 2300.44 ( p &lt;0.001). Both values indicate that our own sample used satisfies the requisite assumptions for proceeding with EFA [33]. Recall that Section 2. 1 hypothesized 5 dimens ions of relevance: topicality, novelty, understandability, scope, and reliability. If these dimensions and questions we re well matched and crafted, we should observe expected correl ations. In partic ular, questions intended to interrogate a part icular dimension should have responses highly correlated with one another, and only weakly correlate with other questions. If so, we would then observe 5 clusters of correlated questions (one per dimension). Each hypothesized dimension would then comprise a distinct factor in our ultimate model. In simple terms, EFA enables us to empirically validate or refute these expectations. The first step in EFA is the initial extraction of the factors to be included in the model. Standard Maximum likelihood and principal axis factoring (PAF) methods were adopted for extracting factors [53]. PAF is a least-squares estimation of the latent factor model which min imizes the unweighted sum of the squares or ordinary least squares of the residual matrix [74]: S is the correlation matrix of observed sample.  X  is the model-fitted correlation matrix. s ij are the elements of matrix S, and  X  the elements of the matrix. Following factors extraction, rotation is employed to maximize high correlations between factors and variables and minimize low ones [66]. There are two kinds of rotations in EFA: orthogonal and oblique. Orthogonal rotation assumes no correlations exist between the resulting factors, while oblique rotation allows the factors correlated with others [33]. Given observed correlations, we adopt oblique (Promax) rotation. Our initial EFA was conducted in R assuming 30 latent factors. After applying PAF and oblique rotation, the resulting Scree plot for this initial EFA is shown in Figure 1. The top 30 Eigenvalues (marked by circles) are computed from the correlation matrix and ordered by decreasing value along the x-axis. The Scree plot of decreasing eigenvalues is used to identify where values roughly level-off [75]. To determine the num ber of factors to keep, we use parallel analysis [25]. A random dataset is generated with the same number of responses and variables as in our sample data. We then created a correlation matrix and computed its eigenvalues. Figure 1 augments the scree plot with an induced parallel analysis line marked by triangles. The red line shows the best intersection to the Eigenvalue plot based on parallel analysis. We should keep only as many factors as appear above it: five. Figure 1 . Revised scree plot showing parallel analysis results Given initial EFA supporting inclus ion of five factors in the model, EFA was then run again w ith the number of factors fixed to 5. Table 1 shows the Pearson correlation r observed between the five factors, ranging in [-0.25, 0.54]. Given standard levels of correlation defined as weak ( r =0.1), medium ( r =0.4), strong ( r =0.7), and very strong ( r =0.9) [18], the data is interpreted to show medium correlation among the five factors. Which questions (i.e., items) should be discarded due to weak correlation, or correlation with multiple factors? Standard EFA practice [75] is to discard questions with: 1) Weak factor loading &lt; 0.4; 2) Cross-loading &lt; 0.15 (difference in estimated loadings across multiple factors); and 3) Lack of logical agreement between question semantics (e.g., a question intended to be about novelty which highly correlated instead with reliability). Factor loading regression coefficients quantify direct effects of factors on items. Based on these criteria and Kenny X  X  Rule (Section 3.1), we keep three items per factor (e.g., R1-3 load on Reliability). Table 3 shows the five factors load ings for the items we kept, with standardized loadings between [0.52,0.94]. The final column, h 2 , denotes the final communality estimate : the proportion of variance accounted for by retained factors. A value of h indicates that an item is less strongly correlated with its corresponding factor [75]. The interpretability criterion guides us to expect that  X  X he manifest variables appear to cluster together in ways that seem logical and reasonable, given constructs that are being measured X  [33]. Table 3 shows that questions associated with each of the five factors included do cluster as expected. EFA is used in exploratory situations to discover a possible factor structure but not to validate it. To confirm the resulting factors found by EFA, CFA is employed following EFA to assess the goodness of fit between a candidate factor model vs. the actual relationships evidenced in the data [33]. In our model, relevance is repres ented as a latent factor atop the other relevance dime nsion factors. A hierarchical factor model is proposed, defined according to the following equations: where  X   X  is the matrix of the loadi ngs for endogenous variables; B is the matrix of causal path;  X  is the matrix of causal path from exogenous to endogenous;  X  and  X  are the residuals.  X  represent the exogenous and endogenous latent variables. Maximum likelihood is used to estimate the model para meters as follows: where  X  is the number of the observed variables;  X  is the estimated covariance matrix of the proposed model and S is the actual covariance matrix of the sample. CFA is a large sample technique. After discarding surv eys failing quality tests, 234 remained for analysis. This number exceeded the generally accepted minimum of 200 instances needed [56]. Figure 2 depicts the resulting structure of our proposed structural equation model (SEM) of multidimensional relevance. Latent measurement errors (not shown e xplicitly in the Figure) were assumed, modeled, and estimated in SEM. Observed data (responses to survey questions) ar e shown in square boxes, with induced factors shown in ovals. Directed edges connect the top-most latent factor, relevance , with the five factors selected from EFA (topicality, novelty, understandability, scop e, and reliability). These factors are each connected to their respective observed data. Edge weights quantify the inferred factor loading relationships. Table 2 shows the factor loadings resulting from CFA and their statistical significance. Standardized loadings shown here match those shown graphically in Figure 2 , but differ from the factor loadings shown in Table 3 , since EFA analysis shown there was exploratory to determine model st ructure, whereas CFA analysis shown here is used to confirm a specific model structure. Standardized loadings range from [-1.00,1.00] and show the strength of correlation. Unstandardized loadings determine if standardized loadings are statistically significant from a t-test. Figure 2. Our structural equation model for modeling relevance. Figure 3. Alternative first-order factor model without factors. Factor F 1 F 2 F 3 F 4 F Reliability --F Topicality .54 --F Scope .46 .34 --F Novelty .19 .19 -.25 --F Understandability .27 .26 .26 -.21 Table 2. Factor loadings for each survey question (i.e., item). Understandabilit y We also evaluate two baseline models. The Null Model effectively assumes observations are independent, with all covariance between questions fixed to 0 and the means and covariance left free. Our other baseline model, a first-order factor model ( Figure 3 ), posits no latent factors for rele vance dimensions, using a single relevance latent factor to explain observed data. Table 4 shows global fitness indices for our proposed model vs. the two alternative models. For th e proposed model, best  X  2 fit is achieved, and with the fewest degrees of freedom ( df ) as well. The root mean square error of approx imation (RMSEA) is 0.065, well below the standard acceptable level of 0.1 [2]. Standardized root mean-square residual (SMSR) is al so used to measure fit between model and data. We see our proposed model achieves SMSR of 0.0692, well below the ac ceptance level of 0.08 [37, 53]. Both Non-Normed Fit index (NNFI) a nd Comparative Fit Index (CFI) also exceed 0.9. This indicates that &gt; 90% covariance among variables is explained, yielding an acceptable model fit [48]. Figure 2 shows that topicality most strongly impacted relevance, followed by understandability a nd reliability. Scope weakly contributed, while novelty did no t contribute. Consequently, we studied excluding novelty from the m odel. This improved its fit to observed data: RMSEA was 0.055; Non-Normed Fit index (NNFI) was 0.96 and Comparative Fit Index (CFI) was 0.97. Improvement in  X   X  model fit was highly significant ( p &lt;&lt;0.001) Building upon the excellent framework proposed by Xu and Chen [77] for positivist investigation of multidimensional relevance, we now review key differences between studies and methodology. To begin, whereas their study as ked respondents to directly judge relevance, we do not. As always, there is the question of what is meant by  X  X elevance X ? How is the notion to be operationalized, and how do we expect respondents to interpret this term? With explicit judging, guide lines show wide variance: TREC criteria seem lenient in including any document one might cite in a comprehensive report, commercial guidelines are very conservative in restricting wh ich documents satisfy the upper echelons of graded relevance, and untrained judges tend to fall somewhere in the middle [40]. What is not clear is how much we learn about multidimensional releva nce at large when relevance is so specifically defined. Should we instead ask for relevance judgments without defining any cr iteria, permitti ng wide ranging interpretation? Should we collect judgments for many different operational contexts of releva nce decision-making to examine how models change as a function of search task and conditions? As with other forms of data-dri ven experimentation, we might seek explanatory models which generalize well across several tasks and information needs, yet expect specialized models to show better fit to observed data for specific search scenarios. Xu and Chen define their notion of relevance as  X  X ituational relevance X  (though their use of this term differs from that of others X  [11, 32, 62]). Regardless, this particular notion omits many other important aspects of relevance from prior studies over the decades [51, 62]. We can learn from their study how various factors interact with this partic ular notion of relevance, but how much can we learn about relevance in general? Their lab study asked participants to select one of four search scenarios and find as much information as possible through interactive search. After searching, each participant was asked to select two Webpages, each br owsed for at least one minute (verified in a user log) and complete a questionnaire for each. As noted earlier, this design yielded skewed data in which more relevant documents were selected . One might ask users to select an equal number of relevant and non-relevant documents, but such selection of non-relevant documen ts may seem unna tural. While this imbalance impacts how adequa tely their design handled non-relevant documents, there is a more fundamental methodological issue. With predominantly relevant documents, survey questions would tend to yield skewed Likert responses which violate underlying assumptions of the EFA and CFA analysis used. While both of our studies had participants self-select the search scenario, our participants were asked to complete the questionnaire for a randomly select ed document. Our intent was: 1) to avoid the above bias problem; 2) to be easier to scale via crowdsourcing, 3) to accommodate participant judging preferences (Sanderson cites Soboroff and Robertson reporting  X  X ssessors preferred to assess rather than search X  [60]); and 4) to resemble Cranfield-style topical relevance judging [57] which we posit such psychometric methods could usefully support. In particular, we believe psychometri c techniques can help generate reliable and large sets of judgments for evaluating search systems, and that multidimensional judgments could inform the long-entrenched issue of judging disagreements [3, 32, 40, 72]. In their study, only 72 document evaluations were used for EFA. This is problematic because covariation patterns could be unstable and might not represent the in tended population sufficiently. Kaiser-Mayer-Olkin (KMO) Measure of Sampling Adequacy and Bartlett X  X  Test of Sphericity ar e typically performed to ensure sufficient sample size. Moreover, while PCA was used to extract factors, PCA is not a valid EFA. While PCA can be used to reduce manifest variables to fewer synthetic variables, it is not appropriate for uncovering the factor structure which underlies a dataset [33]. In addition, despite showing correlations exist between the five factors considered, orthogonal rather than oblique rotation is still used. Finally, the criteria used to determine the factors in their structural equation model are not reported. For CFA, whereas path analysis [77] was used to investigate the relations between the five factors and relevance, path analysis assumes that no measurement error exists in the model. Thus, measurement error was not considered in their model to distinguish observed variables co mmon variance vs. their error variances [33]. Also, six latent factors  X  relevance and five dimensions of topicality, reliability, understandability, novelty, and scope  X  are each associated with a set of survey questions. Each latent factor is inferred based on responses to those questions. In the path analysis, however, these six factors are all treated as manifest variables; structurally, causal directed edges point from the five dimensional factors to relevance. Their modeling objective is to predict relevance from the other five factors, essentially a multiple regression task evaluated by R statistics. Finally, their propo sed model is not empirically compared to any alternative model (common A/B testing). In contrast, we focused entirely on relative model fit and not on prediction error; since we did not ask respondents to judge relevance, we could not assess how well our model predicted relevance, an apparent shortcoming. How might we evaluate prediction error? Each option ha s its own limitations: 1) collect relevance judgments as they did; 2) collect releva nce judgments in a separate questionnaire to avoid any interaction between relevance judging vs. other ques tions; 3) collect relevance judgments from trusted assessors or editors according to some particular relevance criteria; a nd 4) compare rank correlation between relevance predictions from the model vs. search engine ranking [16] from which documents were sampled (Section 3.2). With regard to modeling differen ces, we have already noted our hierarchical vs. their flat modeling of latent factors. Another structural difference is directed causation edges point in the opposite directions: from relevance to the five factors, and from each factor to its corresponding items (see Figure 2 ). On one hand, it seems more intuitive that novelty should (partially) cause relevance (their model), rather th an relevance causing novelty in our model. On the other hand, it is more intuitive that in our model that latent novelty should cause the responses to questions about novelty, as opposed to the other way around. Valid inferences may be drawn in both models, and we can let the data speak for itself in regard to how well each model fits the data. That said, further consideration of causality is warranted. With regard to findings, they showed that novelty and topicality contributed equally to explaining relevance judgment s, with lesser contributions from reliability and understandability. Scope did not appear contribute in any meaningf ul way to relevance. Our own findings, shown in Figure 2 , are rather different. What might have contributed to such different fi ndings between our two studies? Their sample of documents were self-selected by participants after interactive-search and biased toward relevance, whereas we assigned documents to be judged and control bias by stratified sampling from search engine results. People have been reported to judge their own search results differently than assigned documents [16]. Their participants were drawn from a university student population and ours from AMT. Our topics were different, intended to be more widely familiar to a distributed crowd. These topical differences may be si gnificant (e.g., novelty would presumably be more important fo r news-oriented search topics). Prior findings [8, 59, 67, 68, 71] support that people X  X  criteria to relevance judgment change in di fferent tasks, given different cognitive states as a task changes or the stages of a task are switched [62]. Some criteria may dominate in some domains (tasks) while being entirely dispensable in others. Understanding the nature of releva nce and the various factors that contribute to it is one of the most fundamental and long-standing research questions in information retrieval, yet one in which even today there seems to be little agreement about the number of factors or their relative import. We believe the positivist framework offered by Xu and Chen [73] offers our community an avenue for gaining significant traction, but that some of the mechanics of their originally proposed methodology require refinement to ensure valid co nclusions are drawn from studies based on their psychometric approach. While we resolve many of these concerns via revised methodology we have proposed, we have also discussed other questions that remain for future work. The potential of psychometrics methods for IR extends beyond informing our understanding of fa ctors contributing to end-user relevance judgments. For example, large-scale, multidimensional relevance judging could support more informative evaluation of IR systems beyond traditional Cranfield topicality. Moreover, multidimensional judgment data could yield new understanding of causes leading to disagreement in Cranfield relevance judgments. Prior studies to date have i nvestigated subjective relevance thresholds; varying interpretations of the underlying information need; human factors such as priming, fatigue and boredom; and issues in crowdsourcing like fraud or poor language skills. Complementing this, multidimensional relevance data could: 1) provide new insights into non-topical effects explaining disagreement in supposedly topica l judgments; 2) enable more robust inference of relevance judgments as a function of multiple factors; and 3) enable comparative studies of disagreement in topical judging vs. disagreement in judging other factors. Finally, proven quality control methods from psychometrics could enable more robust crowdsourcing data collection in general. With regard to future work, one might investigate wider relevance factors, search scenarios, users, and topics. We might model negative factors people use in making relevance decisions [29] as well as positive ones, or multi-stage relevance judging [29] rather than assuming a single decision point. Measurement error in survey responses might be estimat ed by deliberation time or other analytic data availabl e through instrumentation. As prior work has done in crowdsourcing [41], care fully controlled experiments could assess the relative import of different quality control tests or aggregation strategies, such as aggregating multiple factors at the individual-level rather than one dimensional judgments across individuals. One might then aggregate crowdsourced multi-criteria judgments co llected at the individual-level [20]. Cognitively, we still do not know much about how users integrate relevance criteria. Greisdorf's work [29] is the only one we are aware of attempting multi-stage modeling of relevance determinations by users. From th e systems-oriented perspective, there is an operational question of how IR systems detect and combine various sources of evidence regarding relevance to induce an overall document ranking. Systems can: 1) infer overall user relevance by observable behavi ors like clicks; 2) define and extract features approximating relevance criteria beyond topicality, such as authority or readability; and 3) learn weighting functions for combining features [47]. Eickhoff et al. [23] emphasize non-linear dependencies among such features, and provide pointers to other related work. Tsikrika and Lalmas [70] not only estimate overall rele vance from varying relevance criteria, but also infer relative strengths of the different criteria by decomposing overall relevance. The cognitive and systems-oriented IR literatures have been largely disjoint historically, yet there is clearly overlap fo r further investigation. Acknowledgments . The anonymous reviewers provided valuable feedback which strengthened this work. This study was supported in part by National Science Foundation grant No. 1253413, DARPA Award N66001-12-1-4256, and IMLS grant RE-04-13-0042-13. Any opinions, findings, and conclusions or recommendations expressed by the authors are entirely their own and do not represent those of the sponsoring agencies. [1] Alonso, O. 2013. Implementing crowdsourcing-based relevance [2] Anderson, J.C. and Gerbing, D.W. 1988. Structural equation [3] Bailey, P. et al. 2008. Relevance assessment: are judges [4] Balatsoukas, P. and Ruthven, I. 2012. An eye-tracking approach [5] Baokstein, A. 1979. Relevance. JASIS . 30, 5, 269 X 273. [6] Barry, C.L. 1994. User-defined relevance criteria: An [7] Barry, C.L. and Schamber, L. 19 98. Users X  criteria for relevance [8] Bateman, J. 1998. Changes in Relevance Criteria: A [9] Behrend, T.S. et al. 2011. The viability of crowdsourcing for [10] Blanco, R. et al. 2011. Rep eatable and Reliable Search System [11] Borlund, P. 2003. The concept of relevance in IR. JASIST . 54, [12] Boyce, B. 1982. Beyond topicality: A two stage view of [13] Bradford, S.C. 1934. Sources of information on specific [14] Browne, M.W. 2000. Psychometrics. Journal of the American [15] Cacioppo, J.T. and Petty, R.E. 1984. The Elaboration Likelihood [16] Chouldechova, A. and Mease, D. 2013. Differences in Search [17] Cognitive Interviewing: [18] Cohen, J. 1988. Statistical power analysis for the behavioral [19] Cool, C. et al. 1993. Character istics of Texts affecting relevance [20] Da Costa Pereira, C. et al. 2012. Multidimensional relevance: [21] Cuadra, C.A. and Katter, R.V. 1967. Opening the Black Box of [22] Dwyer, J. 2002. Communication in Business: Strategies and [23] Eickhoff, C. et al. 2013. Copulas for Information Retrieval. [24] Eickhoff, C. and Vries, A.P. de 2013. Increasing cheat [25] Franklin, S.B. et al. 1995. Parallel Analysis: a method for [26] Furr, M. 2011. Scale Construction and Psychometrics for Social [27] Goldberg, L.R. and Kilkowsk i, J.M. 1985. The prediction of [28] Green, R. 1995. Topical releva nce relationships. I. Why topic [29] Greisdorf, H. 2003. Relevance thresholds: a multi-stage [30] Grice, H.P. 1989. Studies in the way of words . Harvard [31] Gwizdka, J. 2014. News St ories Relevance Effects on Eye-[32] Harter, S.P. 1992. Psychologi cal relevance and information [33] Hatcher, L. 2013. Advanced statistics in research: reading, [34] Hj X rland, B. and Christensen, F.S. 2002. Work tasks and socio-[35] Hosseini, M. et al. 2012. On Aggregating Labels from Multiple [36] Hox, J.J. and Bechger, T.M. 2007. An introduction to structural [37] Hu, L. and Bentler, P.M. 1999. Cutoff criteria for fit indexes in [38] Huang, X. and Soergel, D. 2013. Relevance: An improved [39] Johnson, J.R. et al. 1981. Char acteristics of Errors in Accounts [40] Kazai, G. et al. 2012. An Anal ysis of Systematic Judging Errors [41] Kazai, G. et al. 2011. Crowdsourcing for Book Search [42] Kittur, A. et al. 2008. Crowdsourcing User Studies with [43] Lancaster, F.W. 1968. Information retrieval systems: [44] Lesk, M.E. and Salton, G. 1968. Relevance assessments and [45] Levitin, A. and Redman, T. 1995. Quality dimensions of a [46] Little, G. 2009. TurKit: Tools for iterative tasks on mechanical [47] Liu, T.-Y. 2009. Learning to Rank for Information Retrieval. [48] M, P. and Bonett, D.G. 1980. Significance tests and goodness of [49] Maron, M.E. 1977. On indexi ng, retrieval and the meaning of [50] Marshall, C.C. and Shipman, F.M. 2013. Experiences Surveying [51] Mizzaro, S. 1997. Releva nce: The whole history. JASIS . 48, 9, [52] Moshfeghi, Y. et al. 2013. Understanding Relevance: An fMRI [53] Mueller, R.O. and Hancock, G.R. 2008. Best practices in [54] Murphy, K.P. 2012. Machine Learning: A Probabilistic [55] Pearson -Modern Measurement: Theory, Principles, and [56] Principles and Practice of Structural Equation Modeling: Third [57] Proceedings of the International Conference on Scientific [58] Rees, A.M. and Schultz, D. G. 1967. A Field Experimental [59] Relevance as process: judgements in the context of scholarly [60] Sanderson, M. 2010. Test Collection Based Evaluation of [61] Saracevic, T. 2007. Relevance: A review of the literature and a [62] Saracevic, T. 2007. Relevance: A review of the literature and a [63] Schamber, L. 1994. Relevance and Information Behavior. [64] Scheines, R. et al. 1999. Bayesian estimation and testing of [66] Tabachnick, B.G. and Fidell, L.S. 2012. Using Multivariate [67] Tang, R. and Solomon, P. 199 8. Toward an understanding of the [68] Taylor, A.R. et al. 2007. Re lationships between categories of [69] The Social Construction of Me aning: An Alternative Perspective [70] Tsikrika, T. and Lalmas, M. 2007. Combining Evidence for [71] Vakkari, P. and Hakala, N. 2000. Changes in relevance criteria [72] Voorhees, E.M. 1998. Variations in Relevance Judgments and [73] Wilson, D. and Sperber, D. 2002. Relevance Theory. Handbook [74] De Winter, J.C.F. and Dodou , D. 2012. Factor recovery by [75] Worthington, R.L. and Whittaker, T.A. 2006. Scale [76] Wright, S. Correlation and causation . [77] Xu, Y. (Calvin) and Chen, Z. 2006. Relevance judgment: What [78] Zuccon , G. et al. 2013. Cr owdsourcing interactions: using 
