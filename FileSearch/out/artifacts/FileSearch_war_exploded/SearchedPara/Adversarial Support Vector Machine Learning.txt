 Many learning tasks such as spam filtering and credit card fraud detection face an active adversary that tries to avoid detection. For learning problems that deal with an active adversary, it is important to model the adversary X  X  attack strategy and develop robust learning models to mitigate the attack. These are the two objectives of this paper. We con-sider two attack models: a free-range attack model that per-mits arbitrary data corruption and a restrained attack model that anticipates more realistic attacks that a reasonable ad-versary would devise under penalties. We then develop opti-mal SVM learning strategies against the two attack models. The learning algorithms minimize the hinge loss while as-suming the adversary is modifying data to maximize the loss. Experiments are performed on both artificial and real data sets. We demonstrate that optimal solutions may be overly pessimistic when the actual attacks are much weaker than expected. More important, we demonstrate that it is pos-sible to develop a much more resilient SVM learning model while making loose assumptions on the data corruption mod-els. When derived under the restrained attack model, our optimal SVM learning strategy provides more robust overall performance under a wide range of attack parameters. I.5.1 [ Computing Methodologies ]: Pattern Recognition X  Models ; I.2.6 [ Computing Methodologies ]: Artificial In-telligence  X  Learning Theory, Algorithms adversarial learning, attack models, robust SVM
Many learning tasks, such as intrusion detection and spam filtering, face adversarial attacks. Adversarial exploits cre-ate additional challenges to existing learning paradigms. Gen-eralization of a learning model over future data cannot be achieved under the assumption that current and future data share identical properties, which is essential to the tradi-tional approaches. In the presence of active adversaries, data used for training in a learning system is unlikely to represent future data the system would observe. The dif-ference is not just simple random noise which most learning algorithms have already taken into consideration when they are designed. What typically flunk these learning algorithms are targeted attacks that aim to make the learning system dysfunctional by disguising malicious data that otherwise would be detected. Existing learning algorithms cannot be easily tailored to counter this kind of attack because there is a great deal of uncertainty in terms of how much the attacks would affect the structure of the sample space. Despite the sample size and distribution of malicious data given at train-ing time, we would need to make an educated guess about how much the malicious data would change, as sophisticated attackers adapt quickly to evade detection. Attack models, that foretell how far an adversary would go in order to breach the system, need to be incorporated into learning algorithms to build a robust decision surface. In this paper, we present two attack models that cover a wide range of attacks tai-lored to match the adversary X  X  motives. Each attack model makes a simple and realistic assumption on what is known to the adversary. Optimal SVM learning strategies are then derived against the attack models.

Some earlier work lays important theoretical foundations for problems in adversarial learning [15, 6, 20]. However, earlier work often makes strong assumptions such as un-limited computing resource and both sides having a com-plete knowledge of their opponents. Some proposes attack models that may not permit changes made to arbitrary sets of features [20]. In security applications, some existing re-search mainly explores practical means of defeating learning algorithms used in a given application domain [25, 19, 22]. Meanwhile, various learning strategies are proposed to fix application-specific weaknesses in learning algorithms [24, 21, 17], but only to find new doors open for future at-tacks [10, 22]. The main challenge remains as attackers con-tinually exploit unknown weaknesses of a learning system. Regardless of how well designed a learning system appears to be, there are always  X  X lind X  spots it fails to detect, lead-ing to escalating threats as the technical strengths on both sides develop. Threats are often divided into two groups, with one group aiming to smuggle malicious content past learning based detection mechanism, while the other trying to undermine the credibility of a learning system by raising both false positive and false negative rates [3]. The grey area in between is scarcely researched. In this work, we set ourselves free from handling application-specific attacks and addressing specific weaknesses of a learning algorithm. Our main contributions lie in the following three aspects: The rest of the paper is organized as follows. Section 2 presents the related work in the area of adversarial learning. Section 3 formally defines the problem. Section 4 presents the attack models and Section 5 derives the adversarial SVM models. Section 6 presents experimental results on both artificial and real data sets. Section 7 concludes our work and presents future directions.
Kearns and Li [15] provide theoretical upper bounds on tolerable malicious error rates for learning in the presence of malicious errors. They assume the adversary has unbounded computational resource. In addition, they assume the adver-sary has the knowledge of the target concept, target distri-butions, and internal states of the learning algorithm. They demonstrate that error tolerance needs not come at the ex-pense of efficiency or simplicity, and there are strong ties between learning with malicious errors and standard opti-mization problems.

Dalvi et al. [6] propose a game theoretic framework for learning problems where there is an optimal opponent. They define the problem as a game between two cost-sensitive op-ponents: a naive Bayes classifier and an adversary playing optimal strategies. They assume all parameters of both play-ers are known to each other and the adversary knows the ex-act form of the classifier. Their adversary-aware algorithm makes predictions according to the class that maximizes the conditional utility. Finding optimal solutions remains to be computational intensive, which is typical in game theory.
Lowed and Meek [20] point out that assuming the adver-sary has perfect knowledge of the classifier is unrealistic. Instead they suggest the adversary can confirm the mem-bership of an arbitrary instance by sending queries to the classifier. They also assume the adversary has available an adversarial cost function over the sample space that maps samples to cost values. This assumption essentially means the adversary needs to know the entire feature space to issue optimal attacks. They propose an adversarial classifier re-verse engineering (ACRE) algorithm to learn vulnerabilities of given learning algorithms.

Adversarial learning problems are often modeled as games played between two opponents. Br  X  uckner and Scheffer model adversarial prediction problems as Stackelberg games [5]. To guarantee optimality, the model assumes adversaries behave rationally. However, it does not require a unique equilib-rium. Kantarcioglu et al. [14] treat the problem as a sequen-tial Stackelberg game. They assume the two players know each other X  X  payoff function. They use simulated anneal-ing and genetic algorithm to search for a Nash equilibrium. Later on such an equilibrium is used to choose optimal set of attributes that give good equilibrium performance. Im-proved models in which Nash strategies are played have also been proposed [4, 18].

Other game theoretic models play zero-sum minimax strate-gies. Globerson and Roweis [11] consider a problem where some features may be missing at testing time. This is related to adversarial learning in that the adversary may simply delete highly weighted features in malicious data to increase its chance to evade detection. They develop a game theoretic framework in which classifiers are constructed to be optimal in the worst case scenario. Their idea is to prevent assign-ing too much weight on any single feature. They use the support vector machine model which optimally minimizes the hinge loss when at most K features can be deleted. El Ghaoui et al [9] apply a minimax model to training data bounded by hyper-rectangles. Their model minimizes the worst-case loss over data in given intervals. Other robust learning algorithms for handling classification-time noise are also proposed [16, 23, 7, 8].
 Our work differs from the existing ones in several respects. First of all, we do not make strong assumptions on what is known to either side of the players. Second, both wide-range attacks and targeted attacks are considered and in-corporated into the SVM learning framework. Finally, the robustness of the minimax solutions against attacks over a wide range of parameters is investigated.
Denote a sample set by { ( x i ,y i )  X  ( X , Y ) } n i =1 the i th sample and y i  X  { X  1 , 1 } is its label, X  X  R d dimensional feature space, n is the total number of samples. We consider an adversarial learning problem where the ad-versary modifies malicious data to avoid detection and hence achieves his planned goals. The adversary has the freedom to move only the malicious data ( y i = 1) in any direction by adding a non-zero displacement vector  X  i to x i | y i =1 ample, in spam-filtering the adversary may add good words to spam e-mail to defeat spam filters. On the other hand, adversary will not be able to modify legitimate e-mail.
We make no specific assumptions on the adversary X  X  knowl-edge of the learning system. Instead, we simply assume there is a trade-off or cost of changing malicious data. For exam-ple, a practical strategy often employed by an adversary is to move the malicious data in the feature space as close as possible to where the innocuous data is frequently observed. However, the adversary can only alter a malicious data point so much that its malicious utility is not completely lost. If the adversary moves a data point too far away from its own class in the feature space, the adversary may have to sacri-fice much of the malicious utility of the original data point. For example, in the problem of credit card fraud detection, an attacker may choose the  X  X ight X  amount to spent with a stolen credit card to mimic a legitimate purchase. By doing so, the attacker will lose some potential profit.
We present two attack models X  free-range and restrained , each of which makes a simple and realistic assumption about how much is known to the adversary. The models differ in their implications for 1) the adversary X  X  knowledge of the in-nocuous data, and 2) the loss of utility as a result of changing the malicious data. The free-range attack model assumes the adversary has the freedom to move data anywhere in the feature space. The restrained attack model is a more conservative attack model. The model is built under the in-tuition that the adversary would be reluctant to let a data point move far away from its original position in the feature space. The reason is that greater displacement often entails loss of malicious utility.
The only knowledge the adversary needs is the valid range of each feature. Let x max .j and x min .j be the largest and the smallest values that the j th feature of a data point x x  X  X an take. For all practical purposes, we assume both x .j and x min .j are bounded. For example, for a Gaussian distribution, they can be set to the 0.01 and 0.99 quantiles. The resulting range would cover most of the data points and discard a few extreme values. An attack is then bounded in the following form: where C f  X  [0 , 1] controls the aggressiveness of attacks. C f = 0 means no attacks, while C f = 1 corresponds to the most aggressive attacks involving the widest range of permitted data movement.

The great advantage of this attack model is that it is suffi-ciently general to cover all possible attack scenarios as far as data modification is concerned. When paired with a learning model, the combination would produce good performance against the most severe attacks. However, when there are mild attacks, the learning model becomes too  X  X aranoid X  and its performance suffers accordingly. Next, we present a more realistic model for attacks where significant data alteration is penalized.
Let x i be a malicious data point the adversary aims to alter. Let x t i , a d -dimensional vector, be a potential target to which the adversary would like to push x i . The adversary chooses x t i according to his estimate of the innocuous data distribution. Ideally, the adversary would optimize x each x i to minimize the cost of changing it and maximize the goal it can achieve. Optimally choosing x t i is desired, but often requires a great deal of knowledge about the fea-ture space and sometimes the inner working of a learning algorithm [6, 20]. More realistically, the adversary can set x i to be the estimated centroid of innocuous data, a data point sampled from the observed innocuous data, or an ar-tificial data point generated from the estimated innocuous data distribution. Note that x t i could be a rough guess if the adversary has a very limited knowledge of the innocu-ous data, or a very accurate one if the adversary knows the exact make up of the training data.

In most cases, the adversary cannot change x i to x desired since x i may lose too much of its malicious utility. Therefore, for each attribute j in the d -dimensional feature space, we assume the adversary adds  X  ij to x ij where Furthermore, we place an upper bound on the amount of displacement for attribute j as follows: 0  X  ( x t ij  X  x ij )  X  ij  X  1  X  C  X  where C  X   X  [0 , 1] is a constant modeling the loss of malicious utility as a result of the movement  X  ij . This attack model specifies how much the adversary can push x ij towards x t based on how far apart they are from each other. The term allowed to be at most. When C  X  is fixed, the closer x ij to x t ij , the more x ij is allowed to move towards x centage wise. The opposite is also true. The farther apart x ij and x t ij , the smaller |  X  ij | will be. For example, when x one is positive and the other is negative, then no movement is permitted (that is,  X  ij = 0) when C  X  = 1. This model balances between the needs of disguising maliciousness of data and retaining its malicious utility in the mean time. ( x ij  X  x ij )  X  ij  X  0 ensures  X  ij moves in the same direction after the data has been modified. C  X  sets how much mali-cious utility the adversary is willing to sacrifice for breaking through the decision boundary. A larger C  X  means smaller loss of malicious utility, while a smaller C  X  models greater loss of malicious utility. Hence a larger C  X  leads to less ag-gressive attacks while a smaller C  X  leads to more aggress attacks.

The attack model works great for well-separated data as shown in Figure 1(a). When data from both classes are near the separation boundary as shown in Figure 1(b), slightly changing attribute values would be sufficient to push the data across the boundary. In this case, even if C  X  is set to 1, the attack from the above model would still be too aggres-sive compared with what is needed. We could allow C  X  &gt; 1 to further reduce the aggressiveness of attacks, however, for simplicity and more straightforward control, we instead ap-ply a discount factor C  X  to | x t ij  X  x ij | directly to model the severeness of attacks: 0  X  ( x t ij  X  x ij )  X  ij  X  C  X  1  X  where C  X   X  [0 , 1]. A large C  X  gives rise to a greater amount of data movement, and a small C  X  sets a narrower limit on data movement. Combining these two cases, the restrained-attack model is given as follows: 0  X  ( x t ij  X  x ij )  X  ij  X  C  X  1  X  C  X  Figure 1: Data well separated and data cluttered near separating boundary.
We now present an adversarial support vector machine model (AD-SVM) against each of the two attack models discussed in the previous section. We assume the adversary cannot modify the innocuous data. Note that this assump-tion can be relaxed to model cases where the innocuous data may also be altered.
We first consider the free-range attack model. The hinge loss model is given as follows: h ( w,b,x i ) = s.t. where  X  i is the displacement vector for x i , and denote component-wise inequality.

Following the standard SVM risk formulation, we have
Combining cases for positive and negative instances, this is equivalent to: argmin
Note that the worst case hinge loss of x i is obtained when  X  is chosen to minimize its contribution to the margin, that is,
This is a disjoint bilinear problem with resect to w and  X  Here, we are interested in discovering optimal assignment to  X  with a given w . We can reduce the bilinear problem to the following asymmetric dual problem over u i  X  R d , v i  X  R where d is the dimension of the feature space: g or g s.t. ( u i  X  v i ) = 1 2 (1 + y i ) w
The SVM risk minimization problem can be rewritten as follows: argmin
Adding a slack variable and linear constraints to remove the non-differentiality of the hinge loss, we can rewrite the problem as follows: s.t.  X  i  X  0
With the restrained attack model, we modify the hinge loss model and solve the problem following the same steps: h ( w,b,x i ) = s.t. ( x t i  X  x i )  X   X  i 0 where  X  i denotes the modification to x i , is component-wise inequality, and  X  denotes component-wise operations.
The worst case hinge loss is obtained by solving the fol-lowing minimization problem: f i = min  X 
Let We reduce the bilinear problem to the following asymmetric dual problem over u i  X  R d , v i  X  R d where d is the dimension of the feature space:
The SVM risk minimization problem can be rewritten as follows:
After removing the non-differentiality of the hinge loss, we can rewrite the problem as follows:
We test the AD-SVM models on both artificial and real data sets. In our experiments, we investigate the robustness of the AD-SVM models as we increase the severeness of the attacks. We let x t i be the centroid of the innocuous data in our AD-SVM model against restrained attacks. We also tried setting x t i to a random innocuous data point in the training or test set, and the results are similar. Due to space limitations, we do not report the results in the latter cases.
Attacks on the test data used in the experiments are sim-ulated using the following model: where x  X  i is an innocuous data point randomly chosen from the test set, and f attack &gt; 0 sets a limit for the adversary to move the test data toward the target innocuous data points. By controlling the value of f attack , we can dictate the severity of attacks in the simulation. The actual attacks on the test data are intentionally designed not to match the attack models in AD-SVM so that the results are not biased. For each parameter C f , C  X  and C  X  in the attack models considered in AD-SVM, we tried different values as f attack increases. This allows us to test the robustness of our AD-SVM model in all cases where there are no attacks and attacks that are much more severe than the model has anticipated. We compare our AD-SVM model to the stan-dard SVM and one-class SVM models. We implemented our AD-SVM algorithms in CVX X  X  package for specifying and solving convex programs [12]. Experiments using SVM and one-class SVM are implemented using Weka [13].
We generate two artificial data sets from bivariate normal distributions with specified means and covariance matrices. Data in the first data set is well separated. The second data set consists of data more cluttered near the separating boundary. All results are averaged over 100 random runs.
Figure 2 illustrates the data distributions when different levels of distortion are applied to the malicious data by set-ting f attack to 0 (original distribution), 0.3, 0.5, 0.7, and 1.0. As can be observed, as f attack increases, the malicious data points are moved more aggressively towards innocuous data. Figure 2: Data distributions of the first data set after attacks. f attack varies from 0 (no attack) to 1.0 (most aggressive). Plain  X + X  marks the original positive data points,  X + X  with a central black square marks positive data points after alteration, and  X   X   X  represents negative data.

Table 1 lists the predictive accuracy of our AD-SVM algo-rithm with the free-range attack model, the standard SVM algorithm, and the one-class SVM algorithm. AD-SVM clearly outperforms both SVM and one-class SVM when it assumes reasonable adversity ( C f  X  [0 . 1 , 0 . 5]). When there is mild attack or no attack at all, AD-SVM with more ag-gressive free-range assumptions ( C f  X  [0 . 5 , 0 . 9]) suffers great performance loss as we expect from such pessimistic model.
Compared to the free-range attack model, the restrained attack model works much more consistently across the entire spectrum of the learning and attack parameters. Here C  X  reflects the aggressiveness of attacks in our AD-SVM learn-ing algorithm. Table 2 shows the classification results as C decreases, from less aggressive ( C  X  = 0 . 9) to very aggressive ( C  X  = 0 . 1). Clearly, the most impressive results are lined up along the diagonal when the assumptions on the attacks made in the learning model match the real attacks. The results of our AD-SVM in the rest of the experiments are mostly superior to both SVM and one-class SVM too. This relax the requirement of finding the best C  X  . Regardless of what C  X  value is chosen, our model delivers solid perfor-mance.
Figure 3 illustrates the distributions of our second arti-ficial data set under different levels of attacks. Malicious data points can be pushed across the boundary with little modification. We again consider both the free-range and the restrained attack models. Similar conclusions can be drawn: restrained AD-SVM is more robust than free-range AD-SVM; AD-SVMs in general cope much better with mild adversarial attacks than standard SVM and one-class SVM models.

Table 3 lists the predictive accuracy of our AD-SVM algo-rithm with the free-range attack model on the second data set. The results of the standard SVM algorithm and the one-class SVM algorithm are also listed. The free-range model is overly pessimistic in many cases, which overshadows its re-silience against the most severe attacks. For the restrained attack model, since the two classes are not well separated originally, C  X  is used (not combined with C  X  ) to reflect the aggressiveness of attacks in AD-SVM. A larger C  X  is more aggressive while a smaller C  X  assumes mild attacks. Table 4 shows the classification results as C  X  increases, from less ag-gressive ( C  X  = 0 . 1) to very aggressive ( C  X  = 0 . 9).
The restrained AD-SVM model still manages to improve the predictive accuracy compared to SVM and one-class SVM, although the improvement is much less impressive. This is understandable since the data set is generated to make it harder to differentiate between malicious and in-nocuous data, with or without attacks. The model suffers no performance loss when there are no attacks. Figure 3: Data distributions of the second data set after attacks. f attack varies from 0 (none) to 1.0 (most aggressive). Plain X + X  marks the original pos-itive data points,  X + X  with a central black square is for positive data points after alteration, and  X   X   X  rep-resents negative data.
We also test our AD-SVM model on two real datasets: spam base taken from the UCI data repository [2], and web spam taken from the LibSVM website [1].

In the spam base data set, the spam concept includes ad-vertisements, make money fast scams, chain letters, etc. The spam collection came from the postmaster and individuals who had filed spam. The non-spam e-mail collection came from filed work and personal e-mails [2]. The dataset con-sists of 4601 total number of instances, among which 39.4% is spam. There are 57 attributes and one class label. We divide the data sets into equal halves, with one half T r training and the other half T s for test only. Learning mod-els are built from 10% of random samples selected from T r The results are averaged over 10 random runs.
 We took the second data set from the LibSVM website [1]. According to the website, the web spam data is the subset used in the Pascal Large Scale Learning Challenge. All pos-itive examples were kept in the data set while the negative examples were created by randomly traversing the Internet starting at well known web-sites. They treat continuous n bytes as a word and use word count as the feature value and normalize each instance to unit length. We use their unigram data set in which the number of features is 254. The total number of instances is 350,000. We again divide the data set into equal halves for training and test. We use 2% of the samples in the training set to build the learning models and report the results averaged over 10 random runs.
Table 5 and Table 6 show the results on the spam base data set. AD-SVM, with both the free-range and the re-strained attack models, achieved solid improvement on this data set. C  X  alone is used in the restrained learning model. Except for the most pessimistic cases, AD-SVM suffers no performance loss when there are no attacks. On the other hand, it achieved much more superior classification accuracy than SVM and one-class SVM when there are attacks.
Table 7 and Table 8 illustrate the results on the web spam data set. Unlike the spam base data set where data is well separated, web spam data is more like the second artificial data set. The AD-SVM model exhibits similar classification performance as on the second artificial data set. The free-range model is too pessimistic when there are no attacks, while the restrained model performs consistently better than SVM and one-class SVM and, more importantly, suffers no loss when there are no attacks. We use C  X  alone in our learning model. Which parameter, C  X  or C  X  , to use in the restrained attack model can be determined through cross validation on the initial data. Next subsection has a more detailed discussion on model parameters.
The remaining question is how to set the parameters in the attack models. The AD-SVM algorithms proposed in this paper assume either a free-range attack model or a re-strained attack model. In reality we might not know the exact attack model or the true utility function of the at-tackers. However, as Tables 1 X 8 demonstrate, although the actual attacks may not match what we have anticipated, our AD-SVM algorithm using the restrained attack model exhibits overall robust performance by setting C  X  or C  X  ues for more aggressive attacks. If we use the restrained attack model, choosing C  X   X  0 . 5 ( C  X   X  0 . 5) consistently re-turns robust results against all f attack values. If we use the free-range attack model in AD-SVM, we will have to set pa-rameter values to avoid the very pessimistic results for mild attacks. Hence choosing C f  X  0 . 3 in general returns good classification results against all f attack values.
As a general guideline, the baseline of C f , C  X  or C  X  has to be chosen to work well against attack parameters sug-gested by domain experts. This can be done through cross-validation for various attack scenarios. From there, we grad-ually increase C f or C  X  , or decrease in the case of C  X  best value of C f , C  X  or C  X  is reached right before perfor-free-range attack model is used in the learning model. C increases as attacks become more aggressive. restrained attack model is used in the learning model. C increases as attacks become more aggressive. mance deteriorates. Also note that it is sufficient to set only one of C  X  and C  X  while fixing the other to 1. Furthermore, C , C  X  and C  X  do not have to be a scalar parameter. In many applications, it is clear some attributes can be changed while others cannot. A C f , C  X  / C  X  parameter vector would help enforce these additional rules.
Adversarial attacks can lead to severe misrepresentation of real data distributions in the feature space. Learning al-gorithms lacking the flexibility of handling the structural change in the samples would not cope well with attacks that modify data to change the make up of the sample space. We present two attack models and an adversarial SVM learning model against each attack model. We demonstrate that our adversarial SVM model is much more resilient to adversarial attacks than standard SVM and one-class SVM models. We also show that optimal learning strategies derived to counter overly pessimistic attack models can produce unsatisfactory results when the real attacks are much weaker. On the other hand, learning models built on restrained attack models per-form more consistently as attack parameters vary. One fu-ture direction for this work is to add cost-sensitive metrics into the learning models. Another direction is to extend the single learning model to an ensemble in which each base learner handles a different set of attacks.
This work was partially supported by Air Force Office of Scientific Research MURI Grant FA9550-08-1-0265, Na-tional Institutes of Health Grant 1R01LM009989, National Science Foundation (NSF) Grant Career-CNS-0845803, and NSF Grants CNS-0964350, CNS-1016343, CNS-1111529. [1] LIBSVM Data: Classification, Regression, and [2] UCI Machine Learning Repository , 2012. [3] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and [4] M. Bruckner and T. Scheffer. Nash equilibria of static [5] M. Bruckner and T. Scheffer. Stackelberg games for [6] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and [7] O. Dekel and O. Shamir. Learning to classify with [8] O. Dekel, O. Shamir, and L. Xiao. Learning to classify [9] L. El Ghaoui, G. R. G. Lanckriet, and G. Natsoulis. [10] P. Fogla and W. Lee. Evading network anomaly [11] A. Globerson and S. Roweis. Nightmare at test time: [12] M. Grant and S. Boyd. CVX: Matlab software for [13] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [14] M. Kantarcioglu, B. Xi, and C. Clifton. Classifier [15] M. Kearns and M. Li. Learning in the presence of [16] G. R. G. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, [17] Z. Li, M. Sanghi, Y. Chen, M.-Y. Kao, and B. Chavez. [18] W. Liu and S. Chawla. Mining adversarial patterns [19] D. Lowd. Good word attacks on statistical spam [20] D. Lowd and C. Meek. Adversarial learning. In [21] J. Newsome, B. Karp, and D. X. Song. Polygraph: [22] R. Perdisci, D. Dagon, W. Lee, P. Fogla, and [23] C. H. Teo, A. Globerson, S. T. Roweis, and A. J. [24] K. Wang, J. J. Parekh, and S. J. Stolfo. Anagram: A [25] G. L. Wittel and S. F. Wu. On attacking statistical
