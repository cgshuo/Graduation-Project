 Conversation double pivots recomme nd target items related to a source item, based on co -mentions of source and target items in online forums. We deployed severa l variants on the drupal.org site that supports the Drupal open source community, and assessed them through clickthrough rates. A similarity metric based on correlation of mentions rather than mere co-occurrence reduced the problem of over-recommending th e most popular modules, but additional corrections for recency and uniqueness of mentions were not helpful. Detection of more module mentions in recommendations, even though the detection algorithm then had more false positives. Recommendati ons based on conversation co-mention were more effective than those based on co-installation, because co-installation data only led to recommendations of complementary modules and not substitutes. Recommendations based on co-mention were more effe ctive than those based on text similarity matching for navi gating from the most popular modules, but less effective than text matching for less popular modules. H.3.3 [ Information Systems ]: Information Search and Retrieval  X  information filtering, selection process. Algorithms, Design Conversation double pivot, evalua tion, item-item recommender, Drupal People rely on recommender systems when they need to allocate their limited attention to only a few things that might interest them the most. Successful recommender systems have been reported in several application domains, such as movies [1], Usenet articles [9], and consumer products [7]. Typically, recommender systems produce personalized predicted ratings or top-N lists [2]. Anot her use case is to recommend non-personalized sets of related items on each item page, as part of a search or browsing process among pages for items of potential interest. For example, Amazon sugge sts, on the page for an item, various other items that might be of interest. The suggested items may be complementary ( X  X uying a razor? Perhaps you X  X  like blades X ) or they may be substitutes ( X  X erhaps you X  X  like this other brand of razors instead X ). The recommendations on any particular page are the same for all visito rs X  X hey are personalized only to the extent that different user s will visit diffe rent pages. Presentation of related-items while browsing is well-suited to the domain of software module re commendations. Many software platforms allow third party cont ributed add-on extensions, or modules. For example, Perl, Python, and other programming languages have add-on software lib raries or modules available. Browsers like Firefox offer ad d-ons, portals like iGoogle offer plug-in  X  X adgets X , and FaceBook offers  X  X pps X  that users can potential interest in alternative, substitute modules (perhaps of higher quality) and complementary modules that work with the one currently being considered. While it is possible to imagine systematic taste differences (one user prefers text interfaces; another GUIs), personalization ba sed just on which module page the user is currently visiting is likely to be quite effective. Because modules are typically accompanied by short descriptive text, content-based filters can compare the descriptions of modules to assess similarity, but the descriptions provide little clue as to quality. Collaborative data, indicators of other people X  X  assessments of items, can be used to assess both similarity and quality. Those indicators can be explicit, based on, for example, five-star or up/down ratings. They may also be implicit, based on purchases, link citations [3], link clickthrough [6], or time spent reading [8]. In the case of soft ware modules, we have several potential additional implicit indicators: user downloads, installations on running systems, and mentions of modules in public discussion forums. Terveen et al mined Usenet conv ersations for mentions of URLs [13]. Drenner et al explored the automatic detection of mentions of movies in forum discussions , and the suggestion of related conversations on movie pages [4]. In our previous work [14], we applied the idea to software modules, introduced the idea of a conversation double pivot from page s to conversations and back to other modules mentioned in th e same discussions, and showed that it could be used to genera te at least some recommendations for most Drupal and Perl modules . The double pivot technique is closely related to the technique of item-item collaborative filtering [11], where two items are related if they are highly rated (or purchased) by the same people. It is also analogous to co-citation analysis in bibliometrics, where tw o papers are related if they are cited in the same other paper [12]. This paper reports on a tuning and assessment process for conversation double pivots. We depl oyed them on drupal.org, the official portal for the Drupal open source community. Drupal is a popular Web Content Management software platform that has more than 4,000 contributed mo dules. Drupal.org receives about 600,000 pageviews from 100,000 visito rs on a typical day; it has an average of 200 new incoming conversation threads daily. Figure 1 shows pivots enabled on a typical drupal.org module page. Area B of Figure 1 shows the related module recommendations to the module on the current page. 
Figure 1. Drupal.org module page with (A) pivot to related We conducted a four-stage assessm ent and tuning process. First, holding fixed the algorithm for detecting mentions of a module in a conversation, we conducted a bake-off comparing four alternative algorithms for using the detected m odule co-mentions to produce recommendations, correct ing for popularity, recency, and uniqueness. Next, we assessed the utility of the conversation pivot and double pivot blocks by comparing the clickthrough rate for the winning version of the recommender from stage 1 to an alternative block of similar size that was already deployed elsewhere on the drupal.org site. Th ird, we adjusted the algorithm for detecting mentions of a module by adding a number of  X  X liases X  that people often use as shorthand to refer to modules, which nearly tripled the number of mentions detected, while introducing only a modest number of false positives. In Stage 3 we assessed the change in performance resulting from the changed detection algorithm. Finally, we compared the performance of the tuned reco mmender based on conversation double pivots to four other re commender algorithms: one based on co-popularity of installation of modules and three variations of a content similarity algorithm that compared the texts describing the modules. Figure 2 summariz es the stages of tuning and assessment. To assess conversation double pi vots as a recommender system for software modules, we deployed the alternative algorithms on the live drupal.org site and measur ed the actual clickthrough rates. Higher clickthrough rates were cons idered indicators of better recommendations. Usage was tracked with Google Analytics, which naturally filters out visits from most automated crawlers and keeps track of user sessions. Unique views for a particular module page were counted, as the number of distinct sessions in which the page for that module was displayed. For each module, each session in which the user clicked on at least one of the recommended links to other modules in the  X  X elated Projects X  block counted as a recommended module clickthrough; similarly, one or more clicks in the relate d discussions block counted as a single conversation clickt hrough for that module. We also attempted to subjectively code the recommended modules as either non-relevant, re levant substitute modules, or as relevant complementary modules. We found, however, that complement vs. substitute judgments require significant domain knowledge about a large number of Drupal modules, and that even the relevance/non-relevance criteria depend considerably on the task scenario that one imagines for a user of the recommendations. We were unable to achieve sufficient inter-coder reliability and do not report the relevance assessments here. In the first stage of tuning, we implemented and tested four variations of an item-item recomme nder system [4] that all use the same conversation co-mention da ta but process it slightly differently. Each algorithm co mputes a square item-item similarity matrix whose rows and columns are the sets of software modules. Items with the highe st similarity scores are recommended. The conversation mention data th at all algorithms work from is a matrix M with one row for each software module and one column for each conversation thread. We treat the thread as the unit of analysis rather than the indivi dual message because we thought that when two modules are menti oned anywhere in a thread, even if not in the same message, it i ndicates a relationship that would be useful in recommending modules. M it = 1 if thread t mentions items and threads that will be used to illustrate the algorithms. The actual matrix contains 1,645 rows for the software modules and more than 47,000 columns for threads in the forums on the drupal.org site as of November 16, 2008. 
Table 4. Conversation mentions matrix M X  corrected for The first version of the similarity function, as reported previously [14], simply counts co-occurrences. That is: Table 2 shows the matrix Co-occur that would be generated from the detected item mentions from Ta ble 1. Module S1 is not related to S2 because they are never referenced in the same thread, but is somewhat related to S3 because one thread mentions both S1 and S3. One problem with using the Co-o ccur similarity score is over-recommendation of popular modules, what we might call the  X  X arry Potter syndrome X . Since so many people have purchased Harry Potter, a na X ve implementation of  X  X eople Who Bought This Book Also Bought... X  at Amazon would cause Harry Potter to be recommended on almost every other book page. This is problematic because people are often aware already of the most common books or software modules and do not need them to be recommended. There is an a dditional problem when item similarity is based on co-mention in conversations. When two less popular modules are mentioned in a conversation thread, we speculate that there is often an interesting relationship between them, that they are either complements or substitutes for each other. When a popular module is mentioned in a conversation, however, we speculate that it is more often an incidental reference that does not necessari ly indicate a strong relationship between it and the other modules mentioned in the thread. Our second similarity function corrects for the Harry Potter syndrome by assigning scores base d on whether the target occurs more frequently in conversations that mention the source item than in other conversations. The scoring function is simply the Pearson correlation coefficient [11] between two rows of the mention matrix M. Recall that, for any matrix X with I rows and T columns, corr(X) is an IxI matrix giving the correlation coefficients: Thus, for our second similarity algorithm, we have Correlation corr(M) ij . Table 3 shows an example, using the module detection data from Table 1. Note that S2 and S3 are positively correlated, but S1 and S3 are negatively related even though they have one co-mention. H1a: Correlation produces bette r recommendations than Co-occur. Our third similarity function takes into account the recency of conversations. The idea is to hi ghlight "hotspot" modules that received a lot of attention in recent discussions. Modules not discussed recently could be due to abandonment, and thus should be given less weight in recomme ndations. It also corrects the potential problem th at newly created modules would get overshadowed by older modules th at have accumulated mentions over time. The score function of the Recency algorithm uses the same correlation coefficient function as earlier, but on a different mention matrix M X , where:  X   X   X  X   X  X ax X 0, X 1 X  That is, we apply a degradation function on the original mention matrix M that decreases the value of mentions linearly from 1 to 0 in 600 days. Age(t) is the number of days since the last time thread t was updated. Table 4 shows the original conversation mentions matrix M after correction for recency, where T1 through T4 were last updated today, 60 days ago, 2 years ago, and 300 days ago respectively. Thus, for our third similarity algorithm, we have Recency corr(M X ) ij . Table 5 shows the double pivot under M X , where S2 and S3 have a higher relevance score than in Correlation because T4 X  X  mention of S1 and S3 together became weaker after 300 days. H1b: Recency produces be tter recommendations than Correlation. Our last similarity function takes into account the  X  X niqueness X  of module mentions in conversa tions. Intuitively, when a conversation mentions too many modules, those modules are not likely to have as strong relevancy to each other as those from a conversation mentioning only a few modules, which are somewhat  X  X niquely X  related. Th e score function also uses the same correlation coefficient function, but modifies the mention matrix M, where: That is, the weight of each modul e mention is normalized by the total number of mentions in the same conversation. Table 6 shows the original conversation mentions matrix M corrected for uniqueness. Thus, for our last si milarity algorithm, we have Uniqueness ij = corr(M X ) ij . Table 7 shows the double pivot under M X  X . 
Table 6. Conversation mentions matrix M X  X  corrected for H1c: Uniqueness produces be tter recommendations than Correlation . The four alternative double pivot algorithms generated quite different recommendation result sets, even though they used the same conversation co-mentions . For the top-100 most popular modules, we calculate d average overlap of the top-five recommended modules from the four algorithms. As shown in Table 8, all the algorithms produced distinct results. The algorithms were deployed on the live drupal.org site. We concurrently tested clickthrough for all four algorithms from Nov. 16 to Dec. 3, 2008. Each page view was randomly assigned to display results from one of the four alternatives. In stage 2, we tested to see whether conversation pivots were a better use of screen real estate th an other possible uses of the same space. The version we tested was the Correlation algorithm (a winner of the Stage 1 bakeoff, as will be described in the results section). The comparison baseline was the  X  X ecent conversations X  block that simply displayed links to the ten most recent threads in the drupal.org forums. It was al ready displayed on the drupal.org homepage, but not on module pages. H2: Conversation pivot and double pivot blocks will receive more clickthroughs than the  X  X ecent conversations X  block. First, we measured clickthrough on the pivot blocks for 36 days between Dec. 4, 2008 and Jan. 8, 2009, using the Correlation algorithm. Then, we stopped displa ying pivots blocks and instead displayed the  X  X ecent conversations X  block on the module pages for the next 11 days between Jan. 9 and Jan. 19, 2009. Finally, we switched back to displaying the pivot block for another 17 days between Jan. 19 and Feb. 5, 2009. The conversation mention matrix M is constructed by a detection program that automatically infers references to modules as they are naturally expressed in message s, albeit with some error. The initial detection algorithm was crafted to minimize false positives, as we initially thought that false positives were more harmful than false negatives for conversation double pivot recommendations. The algorithm detected a menti on if a module X  X  complete title appeared exactly in the message, immediately preceded or followed by the word  X  X odule X . In addition, if a known alias for a module appeared exactly in a me ssage, a mention was detected without requiring the adjacent wo rd  X  X odule X . For example, most authors refer to the Drupal modul e titled  X  X onten t Construction Kit (CCK) X  as  X  X CK X  rather than using the full title. Using domain knowledge, we manually bui lt a small list of such commonly used aliases. We ran the detection program on the complete drupal.org forums history as of Feb.5, 2009. Of 588,246 messages, 75,266 contained at least one module reference. 1,645 of the 4,199 modules were cited in at least one thread and th ose had a median of 4 references. The total number of detected module references was 89,216. To measure precision and recall of the detection program, one of the authors manually coded module mentions in all 4200 messages posted on drupal.org foru ms between Nov. 20 and Nov. 30, 2008. 1 Those 4200 messages referenced a total of 472 modules, and each module had an average of 3 mentions. Using the 4200 coded messages, we calculated precision and recall as 85.6% and 27.1% respect ively. Recall was very low because of the conservative detection algorithm. For example, the 34 th most popular module accordi ng to download statistics,  X  X rinter, e-mail and PDF versions X , was detected only six times in forum conversations, because it had no alias in our initial list, and people rarely referred to it using the long title. This raised the question of whether improved recall for the module mention detection algorith m, at the expense of some precision, would improve the effectiveness of the double pivot module recommendations. To increase recall, we used a few more aggressive string matchi ng heuristics, such as:  X  If a module title has more than one word, then match on the  X  If a module name is a complex word such as  X  X mageCache X ,  X  Detect references to the modules X  drupal.org URLs and  X  If a module X  X  title has two words, such as  X  X rganic Groups X ,  X  Use additional aliases we found, such as  X  X F X  for Applying those new heuristics to the detection program, we found 2,603 modules, a 58% increase, cite d in at least one thread. The total number of module mentions detected increased to 216,325, raising recall from 27.1% to 84.1%. Precision dropped only slightly, from 85.6% to 84.4%. We ran the Correlation double pivot algorithm again using the new detection program. We called it Correlation-HR because it differed from the original Correlation algorithm only by using the high recall module detections. The top-five recommended items lists for Correlation and Corr elation-HR had only a 27.4% overlap: they generated quite different results. H3. Correlation-HR produces bett er recommendations than the original Correlation. The pivots block continued to be displayed on the drupal.org website from Feb 5. through April 24, but the Google Analytics tracking of clickthroughs was turned off during that period. We first tracked the original Correlation double pivot from April 24-29, 2009, in case there had been a change in clickthroughs from the previous tracking period. Then we switched to Correlation-HR from April 29 -May.4. Because we found that clickthrough rates on weekends were different from rates on weekdays, we made sure that both periods included three weekdays and a weekend. We also continued to track Correlation-HR from May 5-26. To check reliability, the other author randomly coded 100 messages. The two raters agreed if they applied the same set of module labels. The agreement was 92%, Cohen's kappa was 0.85 Our last stage compared the Correlation-HR algorithm to other recommender approaches. The first alternative approach was based on co-installation, with the assumption that modules installed on the same site were somewhat related. Drupal.org keeps track of module installation statistics on live sites, if the hosting sites choose to share it. One daily snapshot of such installation data on Feb. 4, 2009 c onsisted of 225,962 sites with an average of 13 modules installed on each site. The module installation matrix N has N is =1 if module i is installed on site s , and N is =0 otherwise. The scoring function was simply the correlation coefficient between two rows of N. Thus, we have Corr-install ij =corr(N) ij . We speculate that discussion co-m entions might indicate stronger relevance than co-installations because a Drupal site might install many unrelated modules. Moreover, we observe that substitute modules that offer similar functi onality should rarely be installed together, and so Corr-install recommendations will miss the recommending of substitutes. H4a: Correlation-HR produces better recommendations then Corr-install. The second alternative recommende r used a Vector Space Model [10] content similarity algorithm to compare the texts describing the modules. We used the open source tool  X  X pacheSolr More Like This X  2 (SOLR) to generate the recommended items list using module title texts and module desc ription texts, sometimes up to several paragraphs, provided by the module authors. SOLR gives higher weight to terms appearing in the title than the description. SOLR has an option to use only a limited number of terms, those with the highest tf-idf weights. That strips off unimportant noisy terms that might harm the quality of the similarity results. We used three instantiations of th e SOLR algorithm with the terms limits set to 10, 30, and 100, and named them as SOLR-10, SOLR-30, and SOLR-100 respectiv ely. Table 9 shows that the three instantiations generated quite different result sets. Corr.-
SOLR-
SOLR-We observed that not all Drupal modules had good text descriptions and hypothesized that that would hamper the performance of the text-based algorithms. H4b. Correlation-HR produces better recommendations than SOLR-10, SOLR-30 and SOLR-100. http://lucene.apache.org/solr/ The alternative algorithms and Correlation-HR were deployed on the drupal.org site from May 27 to July 22. Each pageview was randomly assigned to display results from one of the five alternative algorithms. Table 10 shows that in the stage 1 bakeoff, Correlation and Uniqueness had the highest c lickthrough rates. Except for Correlation compared to Uniquene ss, all other differences were statistically significant (pai rwise comparisons of population proportions, p &lt; .05). Thus, the results support H1a: a ll three correlation coefficient based algorithms performed better than Co-occur, in both clickthroughs and relevance outcome s. Surprisingly, however, the corrections for recency an d uniqueness did not improve performance as predicted by H1b and H1c. We declared Correlation the winner of the stage 1 bakeoff since it had a slightly higher clickthrough than Uniqueness, although the difference was tiny. Figure 3 shows the daily clickthrough for the testing period of the stage 2 comparison. With the pivo ts block displa yed, clickthrough was 0.18% for the 5 recommended conversation items, 0.28% for the 5 recommended modules, and 0.45% for the overall 10 links. During the 11-day comparison period with the  X  X ew Forum Posts X  block displayed instead , the overall clickthrough dropped to 0.14%. When the pivots block was turned back on, the overall clickthrough partially recovered to 0.34%, consisting of 0.14% from recommended conversations and 0.20% from recommended modules. The differences betw een overall clickthrough rates before and after each switch were statistically significant. (pairwise comparisons of population proportions; p &lt; .05). H2 was supported. The third stage compared the Correlation algorithm with and without the higher recall de tector of module mentions. Clickthrough with the original Corre lation algorithm in the 5 days from April 24-29, 2009 was 0.64%. In the five days after switching to Correlation-HR, clickthrough jumped to 1.07%, more than a 50% increase. Clickthroughs remained relatively stable for the next 21 days, using Correlation-HR. The Correlation-HR algorithm was not able to fill all five of its recommendation slots on pages for modules that were not co-mentioned with at least five ot her modules in the Drupal forums. Figure 4 shows the percentage of possible recommendation slots that the Correlation-HR algorithm produced. Modules are ranked by popularity. For each decile, the bar shows the percentage of the possible recommendation slots (five for each of the source modules) that were filled by th e algorithm. Thus, the most popular April 24 -29 Correlation 289,866 0.64% April 29  X  May 4 Corr-HR 315,301 1.07% May 5  X  May 26 Corr-HR 1,481,350 1.05% recommendation slots filled, while the least popular tenth had only 11% of the possible recommendation slots filled. Correlation-HR had significantly higher clickthrough than Corr-install, as shown in Table 12 (1.12% &gt; 0.80%, pairwise comparisons of population proportions; p &lt; .05). H4a was supported. Contrary to H4b, we found that Correlation-HR had significantly lower clickthrough than SOLR-10 and SOLR-30. Note that even though pageviews were randomly assigned to the five algorithms, Correlation-HR ha d fewer registered pageviews. This occurred because when Correlation-HR made no module recommendations, our code did not generate a Google Analytics event tracking code. In order to correct for that, we estimate the number of pageviews for such modules with algorithm Correlation-HR as equal to the number of pageviews for those modules with SOLR-10. With th at correction, the clickthrough rate on Correlation-HR recomm endations goes down to 1.04% overall, about the same as SOLR-100. In a post-hoc analysis, however, we found that Corr-HR had a higher clickthrough rate than any of the SOLR algorithms on the more popular modules. The top 206 modules garnered half of all page views, with the remaining 4,006 getting the other half. On these modules, Corr-HR had a clickthrough rate of 0.91%, while SOLR-10 had 0.82%, as shown in Ta ble 13. Interestingly, all the algorithms had higher clickthr ough rates on recommendations from the pages for the less popular modules than for the more popular modules. Even Correla tion-HR, while it recommended fewer items, had a higher clickt hrough rate from pages for less popular modules than more popular modules. We explored how the co-mention of modules in c onversations can be detected and used as a resource for software module recommendation. We found that the correlation coefficient algorithm worked better than merely counting co-occurrences. We attribute that to the fact that it corrects for the Harry Potter syndrome, over-recommendation of popular modules. The additional corrections for discussion recency did not work as well as expected. We speculate that it leads to unstable results: a random recent conversation on two remotely related modules might override strong relevancies established over a long period of time. Our results suggest that reduction of variance from larger numbers of conversations is more va luable in this domain, even if the extra conversations are old. We also found that detection of more module mentions in recommendations, even though the detection algorithm then had more false positives. Contrary to our original argument in [14] that precision of the detection al gorithm is most critical, we found that high recall was vital to the double pivot algorithm X  X  performance. Improving recall has another side effect, too: it picks out more mentions of obscure, unpopular m odules. If an obscure module is mentioned only once or twice, always together with a popular module, the correlation algorithm will cause the obscure module to be displayed at the top of the recommendation list for the popular module. Thus, improving recall could lead to over-recommendation of unpopular modules. In post-hoc analysis of Stage 4, however, we did not find a difference in probability of clickthrough to recommendations of more or less popular algorithms. Clickthrough rates increased more than tenfold from the beginning to the end of the tuning process. Overall, this suggests that clickthroughs were indeed sensitive to the quality of recommendations and that develope rs of recommenders should be prepared to tune their recommenders over time. recommendations occurred in the three months between Stage 2 and Stage 3, a time during which the recommendation block was installed and the algorithm wa s unchanged, but clickthroughs were not tracked. One possible explanation is that users became accustomed to the presence of the recommendations block and looked at it more often. This expl anation is consistent with the apparent slow rise in clickthroughs in the first, long part of Figure 3 during December, 2008. When recommendations were relatively stable over time, people may have begun to use them as re-finding aids: someone who forgot the name of a module but remembered the page from wh ich it was recommended might click through again from the reme mbered page. Another possible explanation is that other change s were made to the display of module pages, which could have affected the population of recommendations. A final possibility is that we changed the Google Analytics tracking functions used between stage 2 and stage 3 X  X hile Google X  X  documen tation indicates that the numbers reported in stages 2 a nd 3 should be comparable, it is possible that there was some underlying change that we are not aware of. Conversation double pivots were more effective than the recommender based on co-installation. Perhaps this was because most sites only installed a few popular modules, and those popular modules were calculated as rela ted even though they were not useful recommendations. Even so , co-installation can still be Table 13. Stage 4 clickthrough results by module popularity 
More popular modules 
Less popular modules * Pageviews for Corr-HR include estimated pageviews for modules where pageviews were not tracked because no recommendations were made. valuable in the sense that it can distinguish substitutes from complements: if two modules are related but rarely found in co-installation, then they are almost certainly substitutes. We suspect that recommending substitutes would be especially valuable to users: a common use case would be searching for a module that performs some function, finding one , and navigating from there to substitutes that are of higher quality. Conversation double pivots performed better than the text-based matching algorithm on pages for the more popular items where conversation mentions were plen tiful. The best text-matching algorithm, however, performed better on recommendations from the less popular module page s, and better overall. Somewhat surprisingly, users followed the recommendation links much more often from less popular module pages. There are two plausible explanations. One is that more users who visit the popular pages already know they want those modules, and are just visiting to actually download them, while more visitors to less popular modules are still exploring to find modules that will meet their needs. Another is that, while exploring, more users find that the popular modules are just what they need and stop exploring, while visitors to pages for le ss popular modules find that they need to keep looking. There are other small fixes that could be applied to refine the recommenders. For example, we could remove the extremely popular and unpopular modules from a ll result sets, because they are not likely to be high quality recommendations anyway (unpopular items because they may be of low quality; popular items because everyone knows about them already). Also, rather than displaying the top five items, we could randomly display five of the top ten recommendations, in order to provide more variety. Overall, the results suggest that there is great value in tuning module recommendations. They also suggest that a hybrid approach may be better than any one algorithm alone. Conversation co-mentions may be better for modules that are mentioned a lot, and text matc hing better for other modules. Perhaps the best approach, in the end, would be a semi-automatic recommender: use the automatic al gorithms only to suggest a list of recommended candidates, and then have the users pick, collectively, the related modules to recommend. This is the direction we are heading to in our future work. Our final reflection goes back to the basic question: what module recommendations are really useful for different users and usage scenarios? Novice users might wa nt to see more popular modules; veterans might be more adventur ous and prefer those remotely related modules that might insp ire them for new applications. Some users might like to see re lated modules, but others might prefer to use the five recommended links as shortcuts to the most popular modules. This question needs to be addressed in future work. To conclude, we assessed convers ation co-mentions as resources for software module recommenda tion, and found they were indeed useful. We hope to see more applications in other domains that mine recommendations from conversation. This work was supported by the National Science Foundation under award IIS-0812042. We thank Kieran Lal, Gerhard Killesreiter, and other drupal.org infrastructure team members for their effort deploying pivots on the production server. We thank Michael Hess, Nancy Douyan and Nathan Oostendorp for their advice and help. [1] Bennett, J., and Lanning, J. 2007. The Netflix Prize. In [2] Breese, J., Heckerma n, D., and Kadie, C. 1998. Empirical [3] Brin, S., and Page, L. 1998. Th e anatomy of a large-scale [4] Drenner, S., Harper, M., Fra nkowski, D., Riedl, J., and [5] Herlocker, J. L., Konstan, J. A. , Terveen, L. G., and Riedl, J. [6] Joachims, T., Granka , L., Pan, B., Hembrooke, H., and Gay, [7] Linden, G., Smith, B., a nd York, J. 2003. Amazon.com [8] Morita, M. and Shinoda, Y. 1994. Information filtering based [9] Resnick, P., Iacovou, N., Such ak, M., Bergstrom, P., and [10] Salton, G., Wong, A., and Yang, C. S. 1975. A vector space [11] Sarwar, B., Karypis, G., Kons tan, J., and Reidl, J. 2001. [12] Small, H. 1973. Co-citation in th e scientific literature: A new [13] Terveen, L., Hill, W., Amento, B., McDonald, D., and [14] Zhou, D., Oostendorp, N., Hess, M., and Resnick, P. 2008. 
