 Roy Fox royf@cs.huji.ac.il Naftali Tishby tishby@cs.huji.ac.il School of Computer Science and Engineering The Hebrew University Jerusalem 91904, Israel 1.1. Passive POMDPs Planning Planning in Partially Observable Markov Decision Processes (POMDPs) is an important task in rein-forcement learning, which models an agent X  X  interac-tion with its environment as a discrete-time stochastic process. The environment goes through a sequence of world states W 1 ,...,W n in a finite domain W . These states are hidden from the agent except for an observa-tion O t in a finite domain O , distributed by  X  ( O t | W In the standard POMDP, the agent then chooses an action, which affects the next world state and incurs a cost. Here we consider Passive POMDPs , in which the action affects the cost, but not the world state. We assume that the world itself is a Markov Chain, with states governed by a time-independent transition probability function p ( W t | W t  X  1 ) and an initial distri-bution P 1 ( W 1 ).
 The agent maintains an internal memory state M t in a finite domain M . In each step the memory state is up-dated from the previous memory state and the current observation, according to a memory-state transition policy . Figure 1 summarizes the stochastic process. The agent X  X  goal is to minimize the average expected cost of its actions. In this paper we take the agent X  X  memory state to represent the action, and define a cost function d : W  X M  X  R on the world and memory states. The planning task is then to minimize given the model parameters P 1 , p ,  X  and d . A Passive POMDP can be viewed as an HMM in which inference quality is measured by a cost function. Ex-amples of Passive POMDPs include various gambling scenarios, such as the stock exchange or horse rac-ing, where the betting does not affect the world state. In some settings, the reward depends directly on the amount of information that the agent has on the world state (Kelly gambling, see Cover &amp; Thomas, 2006). When the agent is unbounded it has a simple deter-ministic optimal inference policy. It can maintain a ity of the world state W t given the observable history O ( t ) = O 1 ,...,O t . The belief is a minimal sufficient statistic of O ( t ) for W t , and therefore keeps all the rel-evant information. It can be computed sequentially by a forward algorithm, starting with B 1 ( W 1 | O 1 P ( W 1 )  X  ( O 1 | W 1 ), and at each step updating normalized to be a probability vector.
 1.2. Information Constraints The sufficiency of the exact belief allows the agent to minimize the external cost, but it incurs significant internal costs. The amount of information which the agent needs to keep in memory can be large, and even each observation can be more than the agent can grasp. Anyway, not all of this information is equally useful in reducing external costs.
 In general, the agent X  X  information-processing capacity may be bounded in two ways: 1. The capacity of the agent X  X  memory may limit its 2. The capacity of the channel from the agent X  X  sen-The requirement that the agent keeps sufficient statis-tics and exact beliefs is unrealistic. Rather, the agent X  X  memory M t must be a statistic of O ( t ) which is not suf-ficient, but is still  X  X ood X  in the sense that it keeps the external cost low. We also want it to be  X  X inimal X  for that level of quality, in terms of information-processing rates, so that the agent keeps only information which is useful enough. For each step individually, this is exactly the notion captured by rate-distortion theory, and here we have a sequential extension of it. The main results of this paper are the formulation of the setting described above, and the introduction of an efficient and simple algorithm to solve it. We prove that the algorithm converges to a local optimum, and demonstrate in simulations the tradeoff of memory and sensing intrinsic to this setting. The application of our results to previously studied problems, and a compar-ison to existing algorithms, are left for future work. This paper is organized as follows. In section 2 we for-mulate out setting in information-theoretical terms. In section 3 we solve the problem for one step by finding a variational principle and an efficient optimization al-gorithm. In section 4 we analyze the complete sequen-tial problem and introduce an algorithm to solve it. In section 5 we show two simulations of our solution. 1.3. Related Work Unconstrained planning in Passive POMDPs is eas-ily done by maintaining the exact belief, and choos-ing each action to minimize the subjective expected cost. Planning in general POMDPs is harder, in one aspect due to the size of the belief space. Many algo-rithms plan efficiently but approximately by focusing on a subset of this space.
 Several works do so by optimizing a finite-state con-troller of a given size (Poupart &amp; Boutilier, 2003; Am-ato et al., 2010). The belief represented by each state of the controller is then the posterior probability of the world state given that memory state. A different approach is to explicitly select a subset of beliefs, and use them to guide the iterations (Pineau et al., 2003). Another is to reduce the dimension of the belief space to its principle components (Roy &amp; Gordon, 2002). In this paper we present the novel setting of planning in Passive POMDPs which is constrained by informa-tion capacities. This setting allows treatment of rein-forcement learning in an information-theoretic frame-work. It may also provide a principled method for be-lief approximation in general POMDPs. With a fixed action policy the POMDP becomes a Passive POMDP, and a bounded inference policy can be computed. This reduces the belief space, which in turn guides the ac-tion planning. This decoupling is similar to Chrisman (1992), and will be explored in future work.
 Some research treats POMDPs where the cost is the D
KL between the distributions of the next world state when it is controlled and uncontrolled (Todorov, 2006; Kappen et al., 2009). This has interesting analogies to our setting. Our information-rate constraints define, in effect, components of the cost which are the D KL be-tween the distribution of the next memory state and its marginals (see section 3.1). Tishby &amp; Polani (2011) combine similar information-rate constraints of per-ception and action together. Future work will explore and exploit this symmetry in the special case where the memory information rate is unconstrained. Assume that the model parameters P 1 , p ,  X  and d are given. The agent strives to find an inference policy q ( n ) such that the average expected cost satisfies for the minimal D possible. However, the agent oper-ates under capacity constraints on the channels from M t  X  1 and O t to M t . The external cost d parallels the distortion in rate-distortion theory, while the internal costs are information rates. The agent actually needs to minimize a combination of these costs.
 Note that the agent will generally have some infor-mation on the next observation even before seeing it, i.e. M t  X  1 and O t will not be independent. The agent therefore has some freedom in choosing what part of the information common to M t  X  1 and O t it remem-bers, and what part it forgets and observes anew. The average information rate in both channels com-bined cannot exceed their total capacity, that is In addition, in each step the portion of the above in-formation that is absent from O t may only be passed on the memory channel, and so Similarly, information absent from M t  X  1 is subject to the sensory channel capacity The distortion constraint and the three information-rate constraints together form the problem of inference-planning in Passive POMDPs ( Problem 1 ). The emergence of three information-rate constraints for two channels is similar in spirit to multiterminal source coding (Berger, 1977). In their terminology, the agent needs to implement in each M t a lossy coding of the correlated sources M t  X  1 and O t , under capac-ity constraints, so as to minimize an average expected distortion. The main difference is that here we chose to allow the encoding not to be distributed, in keeping with the ability of memory to interact with perception in biological agents (Laeng &amp; Endestad, 2012). 3.1. Variational Principle Before we consider the long-term planning required of the agent in Problem 1, we focus on the choice of q n in the final step, given the other transitions, that is, given the joint distribution of M n  X  1 , W n and O n . We distribution of M n  X  1 and W n , and have We are interested in the rate-distortion region which includes all points ( R M ,R S ,D ) which are achievable , that is, for which there exists some q n ( M n | M n  X  1 ,O with For any information-rate pair ( R M ,R S ), the minimal achievable D lies on the boundary D  X   X  rate-distortion region. When  X  n and q n are clear from context, we refer to these quantities as D , I C , I M , I and D  X  . We find D  X  ( R M ,R S ) by minimizing the ex-pected distortion under information-rate constraints. The minimum exists because all our formulas are con-tinuous, and the solution space for q n is closed. marginals of q n ( M n | M n  X  1 ,O n ). We expand the terms of the problem using these conditional probability dis-tributions, to have under normalization constraints. 1 We may waive the constraints of non-negative probabilities, which will es-sentially never be active as we shall see later. Also note that we optimize over q n and  X  q n as distinct parame-ters. This is justified by theorem 1 which states that, at the optimum,  X  q n are indeed the marginals of q n . Let the Lagrange multipliers for the constraints be  X  C ,  X 
M and  X  S , and their sum  X  =  X  C +  X  M +  X  S . Leav-Lagrangian will be In the following analysis, several expectations of this function will be useful:  X  G  X   X  G q  X  G  X  where H is the entropy function. The Lagrangian of the problem, up to normalization terms and additive constants, can now be written as
L 1 ( q n ,  X  q n ;  X  n , X  C , X  M , X  S ) = G  X  3.2. Properties of the One-Step Lagrangian Theorem 1. For any fixed  X  n , L 1 is convex in q n and  X  q n . L 1 is strictly convex in parameters which are conditional on m n  X  1 and o n with Pr  X  n ( m n  X  1 ,o n and at the minimum these satisfy where Z n is a normalizing partition function, and  X  q n ( M n ) = X  X  q n ( M n | O n ) =  X  q Proof. For any fixed  X  n , L 1 is convex since all its terms are convex. Non-zero terms only involve m n  X  1 and o rameters, the distortion terms are linear, and the in-formation terms strictly convex. The unique feasible extremum of L 1 is then the global minimum. Differen-tiating by each parameter gives equations 1 and 2. If follows from theorem 1 that complementary slack-ness conditions are sufficient for optimality. Ta-ble 1 shows these conditions, the information rates ( R
M ,R S ) where the solution meets the boundary, and a subgradient of the boundary at that point. For ex-ample, if the minimum of L 1 with  X  M =  X  S = 0 sat-isfies I C  X  I M + I S , then for any information-rate pair in the interval [( I C  X  X  S , I S ) , ( I M , I C  X  X  minimal achievable distortion is D and (  X   X  C ,  X   X  C ) is a subgradient of the boundary.
 Theorem 2. For any joint belief  X  n , the boundary D ous and convex. Any point ( R M ,R S ,D ) on the bound-ary at which (  X   X  M ,  X   X  S ) is a subgradient, is achieved by minimizing L 1 for multipliers (  X  Proof. Let transitions q n and q 0 n achieve the rate-distortion boundary at ( R M ,R S ,D ) and ( R by equations 2 and the convexity of the Kullback-Leibler divergence, the transition  X q n + (1  X   X  ) q 0 (over-)achieves the rate-distortion constraints distortion region is therefore convex, and so is its boundary. The boundary is continuous by the continuity of the problem.
 For a positive information-rate pair ( R M ,R S ), hav-ing M n independent of M n  X  1 and O n makes all information-rate constraints inactive. This satisfies the Slater condition, and the multipliers detailed in the theorem are then the Karush-Kuhn-Tucker multi-pliers necessary for q n to be optimal.
 Corollary 3. Let D  X  C , D  X  M and D  X  S be the boundaries of the rate-distortion regions obtained by keeping each two of the three information-rate constraints. Then D  X  is their maximum. 3.3. Optimization Algorithm An algorithm which alternatingly minimizes L 1 over each parameter with the others fixed, in the style of Blahut-Arimoto (Cover &amp; Thomas, 2006), will allow us to find the minimum.
 Theorem 4. Algorithm 1 converges 2 monotonically to the global minimum of L 1 .
 Proof. L 1 is non-increasing in each iteration and is bounded from below, which guarantees its monotonic convergence. That is Algorithm 1 Last-Step Optimization Input: P 1 ,p, X ,d, X  C , X  M , X  S , X  n Output: optimal q n r  X  0
Initialize some suggestion for q r n repeat until q r n converges But q r +1 n is the unique minimum of the continuous Lagrangian. This implies that q r n also converges to of the Lagrangian X  X  derivatives, they are all 0 at this solution. 4.1. Variational Principle Returning to the entire process of Problem 1, the se-quence of joint beliefs  X  (2 ,n ) =  X  2 ,..., X  n depends re-with  X  1 given as the independent distribution of M 0 and W 1 .
 Adding the constraints of equation 3 with multipliers  X 
L up to normalization terms and additive constants. Solving L n is much more difficult than L 1 . L n is not convex, and each step may affect all future steps. Intu-itively, remembering some feature of the sample in one step is less rewarding if this information is discarded in a future step, and vice versa. This leads to L n having many local minima. 4.2. Local Optimization Algorithm Nevertheless, Problem 1 still has some structure which can be insightful to explore. In particular, it has some interesting similarities to the standard POMDP plan-ning problem. Differentiating L n by q t we now get with where ~ X  n = 0. q t now depends on the future through the multiplier vector ~ X  t . Note how the expectation d ( W t ,M t ).
 L n is linear in each  X  t , and at the optimum must in fact be constant in every non-trivial component of  X  t . This gives us a recursive formula for computing ~ X  t  X  1 0 &lt;  X  t ( M t  X  1 ,W t ) &lt; 1, we have Note that equation 5 is a linear backward recursion for ~ X  t . The multipliers ~  X  t come from the constraints that  X  t is a probability distribution function. It has no consequence, however, since it is independent of M t  X  1 , and is normalized out when ~ X  t  X  1 is used to compute q t  X  1 in equation 4.
 At this point, we can introduce the following general-ization of algorithm 1, which finds the optimal transi-tion q t , given the joint belief  X  t and the policy suffix q Algorithm 2 One-Step Optimization Output: optimal q t r  X  0
Initialize some suggestion for q r t repeat until q r t converges This is a forward-backward algorithm. In each iter-ation we compute  X  ( t +1 ,n ) =  X  t +1 ,..., X  n recursively backward. The algorithm is guaranteed to converge monotonically to an optimal solution, since L n is still strictly convex in each q t separately. In fact, all our theorems and proofs regarding algorithm 1 carry over to this generalization. 4.3. Joint-Belief MDP Expanding the recursion of ~ X  t in equation 5 to a closed form, and disregarding ~  X  t , we find that for 1 &lt; t  X  n and consistent parameters 3 If we extend the recursion by another step to define ~ X  , we get that our minimization target is The minimization can be looked at as the cost-to-go given the joint belief  X  before step t . Importantly, the recursive formula 5, when minimized over q ( t,n ) , is a Bellman equation. It contains a recursive term which is the expected future cost, and other terms which are the expected immediate costs, internal and external, of implementing q t in step t .
 This suggests viewing our problem as a joint-belief MDP. Here the states are the joint beliefs  X  t , the ac-tions are q t , and the next state always follows deter-ministically according to equation 3. This determinism allows us to use a time-dependent policy q ( n ) , rather than a state-dependent one, and will prove useful in finding a solution.
 The belief space of a standard POMDP can be looked at as the state space of a belief MDP, with the same actions and observations, and a linear transition func-tion. If memory states are approximate beliefs, then our model is more like a further abstraction, where the MDP state space is the set of distributions over the belief space. Table 2 summarizes the main differences between this joint-belief MDP and the belief-MDP rep-resentation of discrete-action finite-horizon POMDPs. One important difference is in the structure of the value function. The expected cost L n  X  t +1 of a fixed policy suffix q ( t,n ) consists of some linear terms of ex-pected distortion, but also some strictly convex terms. The latter all take the form of a Kullback-Leibler di-vergence between q t 0 , for some t 0  X  t , and a marginal  X  q , the latter depending on  X  t through equations 2 and the recursion 3.
 That this cost is not linear makes the representation of the value function a challenge, but a greater difficulty is the size of the policy space, which is finite in discrete-action finite-horizon POMDPs, but continuous here. Minimizing over it does not yield a piecewise-linear function of the joint belief, although it is still contin-uous, and the convex mixing of policies shows that it is still concave 4 . It is unclear how to finitely represent the resulting value function in our case. 4.4. Bounded Planning Algorithm Perhaps surprisingly, the determinism of the joint-belief MDP allows us to define a local criterion for optimality. Together with iterations of algorithm 2 which make local improvements, this will guarantee convergence to a local optimum.
 Our algorithm is a simple forward-backward algo-rithm, with a building block (algorithm 2) which is itself forward-backward. In each iteration we compute recursively forward the joint beliefs  X  ( n ) for the current policy q ( n ) . Then we compute recursively backward a which is locally optimal for  X  t . The criterion for opti-mality is that in each step we can use either q 0 ( t +1 ,n ) from the previous step or q ( t +1 ,n ) from the previous it-eration, and whichever leads to a lower cost is chosen. Theorem 5. Algorithm 3 converges monotonically to within of L  X  is also within of a local minimum of Algorithm 3 Passive POMDP Bounded Planning Input:  X  1 ,p, X ,d, X  C , X  M , X  S ,n Output: locally optimal q ( n ) r  X  0
Initialize some suggestion for q r ( n ) repeat until L n ( q r ( n ) ;  X  1 ) converges the bounded-inference-planning problem (section 2), in the sense that for any 1  X  t  X  n , the global minimum Proof. In iteration r , q r ( n ) from the previous iteration is feasible for q r +1 ( n ) . Therefore the cost of q r increasing in r and converges monotonically to a limit L .
 and let q  X  t achieve the global optimum given q r ( t  X  1) q where Where algorithm 3 runs algorithm 2, it can initial-ize q t to q r t from the previous iteration. This may speed up each iteration, particularly when the algo-rithm has nearly converged. In addition, when run-ning algorithm 3 with different sets of multipliers, it converges much faster if each run is initialized with the previous result. Empirically, this also leads to much better local minima if the runs are sorted in order of decreasing multipliers.
 5.1. Symmetric Channel Figure 2 shows the boundary of the rate-distortion region for the 30-step sequential symmetric channel problem. The domains W , O and M are all binary. The agent observes the state correctly with probabil-ity 0.8. The state remains the same for the next step independently with probability 0.8. The distortion is the delta function.
 The boundary consists of three parts as in corollary 3. They have  X  M = 0 (left),  X  M =  X  S = 0 (middle) and  X 
S = 0 (right). Empirically, taking  X  C = 0 is never feasible, as no optimal solution ever has I C  X I M + I S . To clarify this further, figure 3 shows a colored contour map of the boundary. The lower the distortion, the higher the required information rates. The tradeoff between memory and perception is illustrated by the negative slope of the contours.
 5.2. Kelly Gambling Three horses are running in 10 races. Each horse has a fitness rating f i  X  X  1 , 2 , 3 } , and the winning horse is determined by softmax, i.e. horse i wins with proba-bility proportional to exp( f i ). Between the races, the fitness of each horse may independently grow by 1, with probability 0 . 1 if it is not maxed out, or drop by 1, with probability 0 . 1 if it is not depleted. Each horse keeps its fitness with the remaining probability. The only observations are side races performed be-fore each race: 2 random horses compete (with soft-max) and the identities of the winner and the loser are announced. The memory state is a model of the world, consisting of the presumed fitness  X  f i of each horse. The log-optimal proportional gambling strategy is used (Kelly gambling, see Cover &amp; Thomas, 2006), betting on horse i a fraction of the wealth proportional to exp(  X  f i ). Each bet is double-or-nothing, and the dis-tortion is the expected log return on the portfolio. Figure 4 shows the contour map, which is not convex in this instance. We have presented the problem of planning in Pas-sive POMDPs with information-rate constraints. This problem takes the form of a sequential version of rate-distortion theory, and accordingly we were able to pro-vide algorithms which globally optimize each step in-dividually. Unfortunately, the full problem is not con-vex, and we expect that it has very hard instance sets. Nevertheless, typical instances with some locality in their transitions and observations are expected to be easier. We have introduced an efficient and simple algorithm for finding a local minimum, and have used it to illustrate the problem with two simulations. In doing so, we have demonstrated the emergence of a memory-perception tradeoff in the problem.
 Our work has been motivated by the problem of plan-ning in general POMDPs, which may benefit from be-lief approximation which is principled by information theory. The application of our current results to this problem is left for future work. This project is supported in part by the MSEE DARPA Project and by the Gatsby Charitable Foun-dation.
 Amato, Christopher, Bernstein, Daniel S., and Zilber-stein, Shlomo. Optimizing fixed-size stochastic con-trollers for POMDPs and decentralized POMDPs. Autonomous Agents and Multi-Agent Systems , 2010. Berger, Toby. Multiterminal source coding. The Infor-mation Theory Approach to Communications , 1977. Chrisman, Lonnie. Reinforcement learning with per-ceptual aliasing: The perceptual distinctions ap-proach. In Proceedings of the tenth national con-ference on Artificial intelligence , 1992.
 Cover, Thomas M. and Thomas, Joy A. Elements of
Information Theory . Wiley Series in Telecommuni-cations and Signal Processing. 2006.
 Kappen, Bert, G  X omez, Vicen  X c, and Opper, Manfred.
Optimal control as a graphical model inference prob-lem. CoRR , 2009.
 Laeng, Bruno and Endestad, Tor. Bright illusions re-duce the eye X  X  pupil. Proceedings of the National Academy of Sciences , 2012.
 Pineau, Joelle, Gordon, Geoffrey J., and Thrun, Se-bastian. Point-based value iteration: An anytime algorithm for POMDPs. In IJCAI , 2003.
 Poupart, Pascal and Boutilier, Craig. Bounded finite state controllers. In NIPS , 2003.
 Roy, Nicholas and Gordon, Geoffrey J. Exponential family PCA for belief compression in POMDPs. In NIPS , 2002.
 Tishby, Naftali and Polani, Daniel. Information theory of decisions and actions. In Cutsuridis, Vassilis, Hus-sain, Amir, and Taylor, John G. (eds.), Perception-Action Cycle , pp. 601 X 636. Springer, 2011.
 Todorov, Emanuel. Linearly-solvable Markov decision
