 When generating query recommendations for a user, a natu-ral approach is to try and leverage not only the user X  X  most recently submitted query, or reference query, but also in-formation about the current search context, such as the user X  X  recent search interactions. We focus on two important classes of queries that make up search contexts: those that address the same information need as the reference query (on-task queries), and those that do not (off-task queries). We analyze the effects on query recommendation perfor-mance of using contexts consisting of only on-task queries, only off-task queries, and a mix of the two. Using TREC Session Track data for simulations, we demonstrate that on-task context is helpful on average but can be easily over-whelmed when off-task queries are interleaved X  X  common situation according to several analyses of commercial search logs. To minimize the impact of off-task queries on recom-mendation performance, we consider automatic methods of identifying such queries using a state of the art search task identification technique. Our experimental results show that automatic search task identification can eliminate the effect of off-task queries in a mixed context.

We also introduce a novel generalized model for generat-ing recommendations over a search context. While we only consider query text in this study, the model can handle in-tegration over arbitrary user search behavior, such as page visits, dwell times, and query abandonment. In addition, it can be used for other types of recommendation, including personalized web search.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  search process, query formulation Query recommendation, context-aware recommendation, search task identification Figure 1: A search context with interleaved tasks.
Query recommendation is a common tool used by search engines to assist users in reformulating queries. When an information need, or task , requires multiple searches, the se-quence of queries form a context around which new queries can be recommended. Figure 1 illustrates a series of queries issued by a user consisting of two tasks: 1) finding infor-mation about the history of black powder firearms and 2) preparing for the GMAT standardized test. Given this se-quence, our goal is to generate a list of query suggestions with respect to the most recently submitted query, or refer-ence query , which is X  X lack powder inventor X  X n this example. Notice, however, that the user has interleaved the two tasks such that no two adjacent queries are part of the same task. If we use the entire context to generate recommendations, two of the queries will be off-task with respect to the ref-erence query and three (including the reference query) will be on-task . This paper explores the effects that on-and off-task contexts have on query recommendation. While previ-ous work has considered task-aware query recommendation over logged user data, we are not aware of any work that systematically explores the effects of on-task, off-task, and mixed contexts on recommendation performance.

Though the example in Figure 1 may seem an extreme case, consider that Lucchese et al. found 74% of web queries were part of multi-tasking search sessions [15] in a three-month sample of AOL search logs; Jones and Klinkner ob-served that 17% of tasks were interleaved in a 3-day sample of Yahoo! web searches [10]; and Liao et al. [14] found 30% of sessions contained multiple tasks and 5% of sessions con-tained interleaved tasks in a sample of half a billion sessions extracted from Bing search logs. In addition, in a labeled sample of 503 AOL user sessions, we found 44% of search tasks consisted of two or more queries (see Figure 2), but there was only a 45% chance that any two adjacent queries Figure 2: Distribution of tasks lengths observed in a labeled sample of the AOL query log. were part of the same task. Figure 3 shows the likelihood of seeing n tasks in any sequence of x queries, e.g., 10-query sequences typically consist of 3 X 7 search tasks. This means that a context consisting of the most recent n queries is very likely to consist of sub-contexts for several disjoint tasks, none of which may be a part of the same task as the refer-ence query.

The goal of this paper is to better understand the effects of on-task, off-task, and mixed contexts on query recommen-dation quality. We also present and analyze several methods for handling mixed contexts. We address four questions con-cerning query recommendation:
RQ1. How does on-task context affect query recommen-
RQ2. How does off-task context affect query recommen-
RQ3. How does mixed context (on-and off-task queries)
RQ4. How do the following three methods affect query To answer these questions, we perform a number of exper-iments using simulated search sequences derived from the TREC Session Track. For recommendation, we rely on ran-dom walks over a query flow graph formed from a subset of the 2006 AOL query log. We measure query recommenda-tion performance by the quality of the results returned for a recommendation, focusing primarily on mean reciprocal rank (MRR). Our results show that on-task context is usu-ally helpful, while off-task and mixed contexts are extremely harmful. However, automatic search task identification is a reliable way of detecting and discarding off-task queries.
There are four primary contributions of this paper: (1) an
Likelihood of seeing n tasks Figure 3: The distribution of seeing n tasks in a sequence of x queries as observed in a labeled sample of the AOL query log. analysis of task-aware query recommendation demonstrating the usefulness of on-task query context, (2) an analysis of the impact of automatic search task identification on task-aware recommendation, in which we show the state of the art works very well, (3) an open source data set for evaluat-ing context-aware query recommendation, and (4) a gener-alized model of combining recommendations across a search context, regardless of the recommendation algorithm.
Huang, Chien, and Oyang [9] introduced a search log-based query recommendation algorithm that extracts sug-gestions from search sessions in a query log that appeared similar to the user X  X  current session, thereby incorporating the surrounding search context. They found it outperformed methods that extract suggestions from retrieved documents in many aspects.

Filali et al. [8] presented a probabilistic model for gener-ating rewrites based on an arbitrarily long user search his-tory. Their model interpolates the same-task similarity of a rewrite candidate to the reference query with the aver-age similarity of that candidate to all on-task queries from a user X  X  history, weighted by each query X  X  similarity to the reference query. They found that giving some weight to the history, but most weight to the reference query, did best on a set of manually and automatically labeled data.
Liao et al. [14] explored the effect of task-trails on three applications, including query recommendation. They com-pare two session-based, two task-based, and two single-query recommendation models and found they retrieve comple-mentary sets of suggestions, though the task-based models provided the higher quality suggestions. To identify tasks, they used an SVM model using features similar to Jones and Klinkner [10], and the weighted connected components clus-tering method described by Lucchese et al. [15]. Unlike Liao et al., we focus on the effects of on-and off-task context in various mixtures and evaluate on publicly accessible data. T heir work is complementary to ours and our findings sup-port theirs concerning the quality of recommendations using on-task recommendations.

Cao et al. [4] introduce a context-aware recommendation system that converts a series of user queries into concept sequences and builds a suffix tree of these from a large query log. To produce recommendations, a concept sequence is looked up in the suffix tree and the common next queries are given as suggestions.

Cao et al. [5] explored an efficient way to train a very large variable length Hidden Markov Model (vlHMM), which con-siders sequences of queries and clicks in order to produce query and URL recommendations as well as document re-ranking. The authors trained the vlHMM on a large com-mercial search log. However, they did not analyze the ef-fects of on-and off-task contexts on recommendation and the vlHMM technique is tied to large search logs, limiting its portability.

Boldi et al. [2] presented a technique for building a query reformulation graph over user sessions, called a query flow graph. They considered two edge weighting techniques: one uses the threshold weight output by a same-task classifier and the other uses the observed likelihood of one query be-ing submitted after the other. They looked at two appli-cations of query flow graphs: (1) identifying sequences of queries that share a common search task and (2) generating query recommendations. The query suggestion component involves random walks and can be configured to consider the most recent n queries.

Several groups have investigated automatic methods for segmenting and clustering user search interactions into search tasks [2, 10, 13, 15, 18]. Most of these depend on ma-chine learning algorithms X  X ogistic regression, support vec-tor machines, and decision trees X  X rained over large amounts of manually or automatically labeled user data to classify a pair of queries as belonging to the same task or not. We decided to use the Lucchese et al. method [15], which is a heuristic model that depends on lexical and semantic fea-tures of the pair of queries being classified. This method is the most recent and was shown to out-perform machine learned models on a sample of AOL data. The other meth-ods are, however, equally applicable to the analysis we con-duct.
We only consider one query recommendation algorithm based largely on the query flow graph (QFG) work of Boldi et al. [2] and the term-query graph (TQGraph) work of Bonchi et al. [3]. We use a query flow graph G in which the vertices V are queries from a query log L and the edges E represent reformulation probabilities. For any vertex v  X  V , the weights of all outgoing edges must sum to 1. A reformu-lation is defined as an ordered pair of queries ( q i ; q that the pair occurs in a user search session in that order, though not necessarily adjacent. A session is defined to be the maximal sequence of queries and result clicks such that no more than t seconds separate any two adjacent events. The outgoing edges of v are normalized across all sessions and users in L .
 While we do not require reformulations to be adjacent, Boldi et al. did. By considering all reformulations X  X djacent and otherwise X  X ithin a session, we expand the coverage of G beyond using adjacent query reformulations only but avoid incorporating as many off-task reformulations as when the scope is a user X  X  entire search history. We assume that reformulations in which q i and q j are from different tasks, q will be an infrequent follower of q i , and therefore statistically insignificant among q i  X  X  outgoing edges. In addition, Boldi et al. used a thresholded and normalized chaining probability for the edge weights, but we do not due to the sparseness of our data (the AOL query logs).

To generate recommendations, we rely on a slight adapta-tion of the query flow graph, called the term-query graph [3]. This adds a layer to the QFG that consists of all terms that occur in queries in the QFG, each of which points to the queries in which it occurs. Given a query q , we find recom-mendations by first producing random walk scores over all queries in Q for each term t  X  q .

To compute the random walk with restart for a given term t , we must first create a vector v of length | V | (i.e., with one element per node in Q ). Each element corresponding to a query that contains t is set to 1 and all others are set to 0. This is our initialization vector . Next, we must select the probability, c , of restarting our random walk to one of the queries in our initialization vector. Given the adjacency matrix of G , A , and a vector u that is initially set to v , we then compute the following until convergence: After convergence, the values in u are the random walk scores of each corresponding query q  X  for t . We denote this as the term-level recommendation score b r term ( q  X  | t ).
One issue with using the random walk score for a query is that it favors frequent queries. To address this, Boldi et al. used the geometric mean r term of the random walk score b r term and its normalized score r t erm . Given an initial query q , the scores for an arbitrary query q  X  can be computed by: where b r uniform ( q  X  ) is the random walk score produced for q when the initialization vector v is uniform.

The final query recommendation vector is computed us-ing a component-wise product of the random walk vectors for each term in the query. Specifically, for each query q  X  V , we compute the query-level recommendation score r query ( q  X  | q ) as follows:
In this section, we introduce formal definitions of general and task-based contexts as well as the automatic search task identification algorithm used for the experiments.
The recommendation models described above, and recom-mendation algorithms in general, that generate suggestions with respect to a single query can be easily extended to handle additional contextual information. The basic idea is simple: when generating recommendations for a query, con-s ider the search context consisting of the m most recently submitted queries from the user, weighting the influence of each according to some measure of importance. Many func-tions can be used to measure the importance of a context query. The two we consider in this paper are how far back in a user X  X  search history a query occurs and whether the query appears to be related to a user X  X  current task. However, oth-ers may include whether a context query was quickly aban-doned or spell corrected, how many results the user visited, the time of day they were visited, and other behavioral as-pects. In this section, we introduce a generalized model that makes it easier for us to talk about the various importance functions we are interested in and can be used with addi-tional functions explored in future work.

Assume that we have a search context C that contains all the information about a user X  X  search behavior related to a sequence of m queries, with the m th query, C [ m ], be-ing the most recently submitted query. Also assume that we have a set of n importance functions,  X  1 ;  X  2 ; : : : ;  X  corresponding weights  X  1 ;  X  2 ; : : : ;  X  n that tell us how much weight to give to each of the importance functions. We will represent corresponding functions and weights as tuples in a set F = {h  X  1 ;  X  1 i h  X  2 ;  X  2 i : : : ; h  X  n ;  X  n context-aware recommendation score for a query suggestion q as follows: r Each importance function  X  , takes three parameters: i; m; C , where i is the position of the query within context C for which importance is being measured and m is the position of the reference query. In this work, the reference query is always the last query of C , but the model does not make the assumption that this is always the case. The r context recommendation scoring function scores q  X  with respect to each query in the context ( C [ i ]) and adds that score to the overall score with some weight that is computed as the lin-ear combination of the importance function values for that query.
One of the importance functions we consider in this paper is a decaying function, where queries earlier in a user X  X  con-text are considered less important than more recent queries. As such, queries submitted more recently have a greater in-fluence on recommendation scores. This has the intuitive interpretation that users are less interested in older queries, otherwise they would not have moved on to new queries.
Boldi et al. [2] discussed a decay weighting method for entries in the random walk initialization vector ( v in Eq. 1). They proposed that each query in a search context receive a weight proportional to  X  d , where d is the distance in query count from the current query. For example, the second most recent query would get a weight of  X  1 , because it X  X  one query away from the most recent query.

While the Boldi et al. method is specific to recommenda-tions using random walks, we can transfer their exponential decay function to our model as follows:
While decaying the influence of queries earlier in a search context is a natural importance function, we are also inter-ested in functions that incorporate the degree to which a query is on the same task as the reference query. It is rea-sonable to assume (an assumption we test) that queries from a search context that are part of the same task should be more helpful in the recommendation process than queries that are not.

As we have stated earlier, Lucchese et al. observed that 74% of web queries are part of multi-task search sessions [15] while Jones and Klinkner found that 17% of tasks are inter-leaved in web search [10]. Using a labeled sample of the AOL query log, we observed an exponential decrease in the likeli-hood that the previous m queries are part of the same task as m increases (see Figure 3). This suggests that using the m most recent queries as the the search context for generating recommendations will likely introduce off-topic information, causing recommendations that seem out of place. There-fore, it may be beneficial to identify which queries from that context share the same task as the reference query.
Formally, given a search context C with m queries, we define a task context T to be the maximal subset of queries in C that share a common task to C [ m ]: T  X  C |  X  i  X  [1 ; m ] ; C [ i ]  X  T  X  X  X  sametask ( i; m; C ) &gt;  X  where sametask ( i; m; C ) is a function that outputs a pre-diction in the range [0,1] as to how likely C [ i ] and C [ m ] are to be part of the same search task given C and  X  is the decision threshold.

Once we have T , the natural question to pose is how do we use it? One method would be to treat T just as C and use it with the r decay function, i.e., r decay ( q  X  | T ). However, it may be that the off-task context is still useful, just not as useful as T . To support both of these points of view, we can use the following hard task recommendation scoring functions: where  X  can be used to give more or less weight to the task context and taskdist is the number of on-task queries be-tween C [ i ] and C [ j ]. If we set  X  = 1, we only use the task context, whereas with  X  = 0, we ignore the task context altogether. If we use  X  = 0 : 5, we use some of the task in-formation, but still allow the greater context to have a pres-ence. Note that we have left off the parameters to decay and sametask in Eq. 16 for readability.

This approach may work well if one is comfortable with setting a hard threshold  X  on the output of the sametask function. If, however, we want to provide a mechanism by which we use the output of sametask as a confidence, we can use the following soft task recommendation scoring func-tions: Here,  X  smooths between using and not using the same-task scores to dampen the decay weights. As before, we have left off the parameters to sametask and decay in Eq. 12.
Two additional models we consider are both variations of what we call firm task recommendation, as they combine aspects of both the hard and soft task models. The first, called firmtask1 , behaves similarly to the soft task model, except that the weight given to any queries with a same task score at or below the threshold  X  are weighted as 0. The second, called firmtask2 , is identical to the hard task model, except that the task classification score is used in addition to the taskdecay weight. Mathematically, the firm task recommendation models are described by: firmtask1 ( i; j; C ) = firmtask2 ( i; j; C ) = Note that unlike the hard task model, the decay component of the firmtask1 model is affected by every query, not just those above the threshold.

For example, suppose we have a context C with five queries, q 1 ; : : : ; q 5 . Relative to the reference query, q pose applying sametask to each query produces the same-T = [ q 1 ; q 4 ; q 5 ]. Using  X  = 0 : 8, notice in Figure 4 how the importance weight of each query in the context changes be-tween using only the decay function (a.) and setting  X  = 1 for the task-aware recommendations (b. X  X .). Note that when  X  = 0, the hard, firm, and soft task recommendation scores are equivalent (they all reduce to using the decay-only scor-ing function).

There are two primary differences between using hard-and soft task recommendation. First, hard task recommen-dation does not penalize on-task queries that occur prior to a sequence of off-task queries, e.g. in Figure 4, we see that q is on-task and hardtask treats it as the first query in a sequence of three:  X  n  X  1 = 0 : 8 2 . Conversely, the soft task recommendation model treats q 1 as the first in a sequence of five:  X  m  X  1 = 0 : 8 4 .

Second, soft task recommendation can only down-weight a query X  X  importance weight, unlike-hard task recommenda-tion, which we saw can significantly increase the weight of an on-task query further back in the context. At the same time, however, soft task recommendation only allows a query to Figure 4: An example of the degree to which each q uery in a context contributes (right column) given its predicted same-task score (top row) for: (a.) de-cay only, (b.) soft task, (c.) firm task-1, (d.) firm task-2, and (e) hard task recommendation. We set  X  = 0 : 8 for all, and  X  = 1 ;  X  = 0 : 2 for b. X  X . be given a zero weight if its same-task score is zero. The two firm task models balance these aspects in different ways.
In Section 3, we discussed how to use information about tasks for query recommendation, but we did not say how to generate the scores. We use the search task identification heuristic described by Luccehse et al. [15]. In deciding if two queries q i and q j are part of the same task, we calculate two similarity measures: a lexical and a semantic score, defined as follows.
 The lexical scoring function s lexical is the average of the Jaccard coefficient between term trigrams extracted from the two queries and one minus the Levenshtein edit distance of the two queries. The score is in the range [0,1]. Two queries that are lexically very similar X  X nes where a single term has been added, removed, or reordered, or queries that have been spell corrected X  X hould have an s lexical score close to one.

The semantic scoring function s semantic is made of up two components. The first, s wikipedia ( q i ; q j ), creates the vectors v and v j consisting of the tf  X  idf scores of every Wikipedia document relative to q i and q j , respectively. The func-tion then returns the cosine similarity between these two vectors. The second component, s wiktionary ( q i ; q j ) is simi-larly computed, but over Wiktionary entries. We then set s As with the lexical score, the range of the semantic score is [0,1].

The combined similarity score, s , is defined as follows: We can define a same-task scoring function to use s directly, as follows: Alternatively, we can run one extra step: single-link clus-tering over the context C using s as the similarity measure. Clustering allows us to boost the similarity score between two queries that are only indirectly related. Similar to Liao et al. [14], our choice of clustering follows the results of Luc-chese et al. [15], who describe a weighted connected compo-nents algorithm that is equivalent to single-link clustering with a cutoff of  X  . After clustering, we use the notation K
C [ q ] to represent the cluster or task associated with query q in context C ; if two queries q; q  X   X  C are part of the same task, then K C [ q ] = K C [ q  X  ], otherwise K C [ q ] 6 = K Figure 5: Examples of on-task/off-task segmenta-t ions using sametask 1 and sametask 2 scoring. The reference query, q 5 , sits in the bolded center node. Note that the edge ( q 1 ; q 5 ) goes from 0.2 using sametask 1 to 0.6 under sametask 2 due to q 1  X  X  strong similarity to q 3 , which has a similarity score of 0.6 with q 5 . scoring function that uses task clustering is the following: sametask 2 ( i; j; C ) = max Note that sametask 2 will return the highest similarity be-tween C [ i ] and any member of C [ j ] X  X  tasks, excluding C [ i ]. Figure 5 illustrates a case in which sametask 2 improves over sametask 1 ; note, however, that sametask 2 can also be harmful when an off-task query is found to be similar to an on-task query.
In this section, we describe the data, settings, and methodology used for the experiments.
We extracted query reformulations from the 2006 AOL query log, which includes more than 10 million unique queries making up 21 million query instances submitted by 657,426 users between March X  X pril 2006. Considering all ordered pairs from a 30-query sliding window across sessions with a maximum timeout of 26 minutes, we ex-tracted 33,218,915 distinct query reformulations to construct a query flow graph (compared to 18,271,486 if we used only adjacent pairs), ignoring all dash ( X - X ) queries, which corre-spond to queries that AOL scrubbed or randomly replaced. The inlink and outlink counts of the nodes in the graph both have a median of 2 and a mean of about 5. If we were to use only adjacent reformulations from the logs, the median would be 1 and the mean just under 2.
We used the 2010 and 2011 TREC Session Track [11, 12] data to generate task contexts. The 2010 track data con-tains 136 judged sessions, each with two queries (totaling 272 queries), covering three reformulation types: drift, spe-cialization, and generalization relative to the first search. We ignore the reformulation type. The 2011 track data con-sists of 76 variable length sessions, 280 queries (average of 3.7 queries per session), and 62 judged topics. Several top-ics have multiple corresponding sessions. In total, we use all 212 judged sessions from both years. The relevance judg-ments in both cases are over the ClueWeb09 collection. Our goal is to provide recommendations to retrieve documents relevant to the last query in each session, thus we mark the last query as the reference query.

Each session constitutes a single task, and henceforth we refer to the sessions as tasks. Since the TREC data consists of single tasks, we need some way of simulating the case that multiple tasks are interleaved. We describe our approach for this next.
To answer our four research questions, we use the follow-ing set of experiments. Throughout all of these experiments, the baseline is to use only the reference query for generating query recommendations.

Experiment 1. For RQ1, which seeks to understand the effect of including relevant context on recommendation performance, we use each task T from the TREC Session Track and recommend suggestions using the most recent m queries for m = [1 ; | T | ]. If incorporating context is helpful, then we should see an improvement as m increases. Note that m = 1 is the case in which only the reference query is used.

Experiment 2. To address RQ2, which asks how off-task context affects recommendation performance, we mod-ify the experiment described above to consider a context of m = [1 ; | T | ] queries such that queries 2 X  | T | are off-task. To capture the randomness of off-task queries, we evaluate over R random samples of off-task contexts (each query is inde-pendently sampled from other tasks, excluding those with the same TREC Session Track topic) for each task T and each value of m &gt; 1. If off-task context is harmful, we should see a worsening trend in performance as m increases.
Experiment 3. To address RQ3, which asks how query recommendation performance is affected by a context that is a mix of on-and off-task queries, we rely on a simulation of mixed contexts. As we saw in Figure 3, the probability that a sequence of m queries share the same task decreases exponentially as m increases, and so the mixed context as-sumed in RQ3 is realistic if not typical. We simulate mixed contexts by taking each task T of length n and considering a context window of length m = [1 ; n + R ], where R is the number of off-task queries to add into the context. The last query in the context q m always corresponds to the last query q in T . Queries q 1 ; : : : ; q m  X  1 consist of a mix of the queries from T and other tasks from the TREC Session Track. The queries from T will always appear in the same order, but not necessarily adjacent.
 To incorporate noise, we initially set C = []. We select R off-task queries as follows: first, we randomly select an off-topic task, O , from the TREC Session Track and take the first R queries from that task. If | O | &lt; R , we randomly selected an addition off-topic task and concatenate its first R  X  X  O | queries to O . We continue the process until | O | = | R | . We now randomly interleave T and O , the only rule being that T n  X  X he reference query X  X ust be the last query in C (an easy rule to adhere to by simply removing T n before the randomized interleaving, and then concatenating it to the end). For a given value of R , we can perform many randomizations and graph the effect of using the most recent n + R queries to perform query recommendation.

Experiment 4. The final research question, RQ4, asks how mixed contexts should be used in the query recommen-d ation process. We have limited ourselves to consider three possibilities: (a.) using only the reference query (i.e., our baseline throughout these experiments), (b.) using the most recent n + R queries (i.e., the results from Experiment 3), or (c.) incorporating same-task classification scores. Exper-iment 4 concentrates on (c.) and analyzes the effect of in-corporating same-task classification scores during the search context integration process. This is where we will compare the task-aware recommendation models described in the pre-vious section.
For the query recommendation using the TQGraph, we used a restart probability of c = 0 : 1, as was found to be optimal by Bonchi et al. [3]. Note that they refer to the restart value  X  , where c = 1  X   X  . To increase the speed of our recommendation, we only stored the 100,000 top scoring random walk results for each term. Bonchi et al. [3] found this to have no or very limited effects on performance when used with c = 0 : 1.

For task classification, we used the parameters found op-timal by Lucchese et al. [15]:  X  = 0 : 2 (used during task clustering) and  X  = 0 : 5 (used to weight the semantic and lexical features). We also set  X  =  X  since  X  is used in much the same way in the task-aware recommendation models.
To evaluate recommendations, we retrieved documents from ClueWeb09 using the default query likelihood model implemented in Indri 5.3 [17]. 1 We removed spam by us-ing the Fusion spam score dataset [6] at a 75th percentile, meaning we only kept the least spammy 25% of documents. 2
In this section, we cover the results of each of the experi-ments described in Section 5.3. We then discuss the meaning of our findings as well as their broader implications.
In all experiments, we measured recommendation perfor-mance using the mean reciprocal rank (MRR) of ClueWeb09 document retrieved for the top scored recommendation av-eraged over the 212 TREC Session Track tasks. We found similar trends using normalized discounted cumulative gain (nDCG) and precision at 3, 5, and 10. There are several ways one can calculate relevance over the document sets re-trieved for recommendations, such as count any document retrieved in the top ten for any of the context queries as non-relevant (rather harsh), indifferently (resulting in duplicate documents), or by removing all such documents from the re-sult lists of recommendations. We elected to go with the last as it is a reasonable behavior to expect from a context-aware system. We removed documents retrieved for any query in the context, not just those that are on-task. This is a very conservative evaluation and is reflected in the performance metrics.

Experiment 1. With this experiment, our aim was to quantify the effect of on-task query context on recommen-dation quality. Focusing on the top line with circles in Fig-ure 6, the MRR of the top scored recommendation averaged over the 212 tasks performs better than using only the ref-http://www.lemurproject.org/i ndri/ http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/ Figure 6: The effect of adding on-task (blue circles) a nd off-task (red triangles) queries versus only the reference query on recommendation MRR (black squares). MRR is calculated on the top scoring rec-ommendation.
 Figure 7: The per session effect of on-and off-task c ontext on the change in MRR of the top scoring recommendation. The y -axis shows the difference between the MRR of using context and using only the reference query. A higher value means context improved MRR. Note that 145 tasks were removed as neither on-nor off-task context had an effect. The bars are not necessarily aligned between the two plots and should not be compared. erence query (middle line with squares). To generate these scores, we used the r decay model with  X  = 0 : 8, as set by Boldi et al. [2] in their decay function. For each value of m , if a particular task T has fewer than m queries, the value at | T | is used. The MRR scores are low because for a large number of tasks, none of the methods provide any useful rec-ommendations. We performed evaluations where such tasks were ignored and found that the MRR does indeed increase and the relationship between the methods plotted stays the same. However, in order to ensure comparability with future work, we elected to report on all tasks.

While performance is better on average in Figure 6, the top bar chart in Figure 7 breaks the performance down by the TREC search tasks and we can see that there are many tasks for which on-task context is very helpful, as well as several where it hurts. Note that some of the tasks are not displayed for readability.

Experiment 2. The goal of the second experiment was to ascertain the effect of off-task context on query recom-mendation. We generated 50 random off-task contexts for each task and report the micro-average across all trials. The b ottom line with triangles in Figure 6 shows that adding off-task queries under the r decay model with  X  = 0 : 8 rapidly decreases recommendation performance for low values of m before more or less leveling off around m = 5 (it still de-creases, but much slower). Its performance is well below that of the baseline of using only the reference query, mak-ing it clear that off task context is extremely detrimental.
Turning to the bottom plot in Figure 7, we see that off-task context has an almost entirely negative effect (there is an ever so slight increase in performance for the task rep-resented by the far left bar). Interestingly, for the severely compromised tasks on the far right, the effect is not as nega-tive as when on-task context hurts. We have not conducted a full analysis to understand this phenomena, but one possi-ble cause is the averaging over 50 trials that takes place for the off-task contexts. We leave investigations into this for future work.

Experiment 3. With Experiment 3, we wanted to un-derstand the effect of mixed contexts X  X onsisting of both on-and off-task queries X  X n query recommendation perfor-mance. As explained earlier, the experiment explores the performance of tasks when R noisy queries are added to the entire set of on-task queries. The bottom line with trian-gles in Figure 8 shows just this, using r decay with  X  = 0 : 8. The far left point, where R = 0, lines up with the far right point of the on-task line in Figure 6. We randomly gener-ated 50 noisy contexts per task for each value of R . The solid line shows the micro-averaged MRR over all tasks X  samples. The dotted lines on either side show the minimum and max-imum values for the micro-average MRR on a set of 1,000 sub-samples (with replacement) of the original 50. As you can see, the bounds indicate relatively low variance of the micro-average across the 212 tasks. There are still certain tasks for which performance is very high or very low (that is, the bounds on the micro-average do not inform us of the variance among tasks).

An important observation from this experiment is that performance dips below that of the baseline when even a sin-gle off-task query is mixed in. This is quite startling when you consider that the chances of three queries (at R = 1, all contexts are of at least length three) in a row belonging to a single task are below 30% (see Figure 3) and that roughly 40% of tasks in the wild are of length three or more (see Fig-ure 2). These results clearly show that blindly incorporating mixed context is a poor method of incorporating context.
Experiment 4. In the final experiment, we hoped to de-termine the effects of using recommendation models that consider the reference query only, the entire context, or the entire context, but in a task-aware manner. The first two were addressed in the previous experiments, where we learned that using the reference query is more effective than blindly using the entire context. Figure 8 shows the re-sults of using the models we introduced in Section 4.3. We used the same randomizations as in Experiment 3 and like-wise generated the minimum and maximum bounds around each model X  X  performance line. For these experiments, sametask 1 scores were used to produce same-task scores. We also performed the experiment using sametask 2 and found it was comparable. We used  X  = 1 for all task-aware models; setting it to anything less resulted in an extreme degradation of performance.

There are several interesting observations. First, the firm-Figure 8: The effect of adding off-task queries to a t ask context on MRR when same task classification is used and is not used versus only using the refer-ence query (black squares). The sametask 1 scoring method is used for all task-aware recommendation models. MRR is calculated on the top scoring rec-ommendation. task models performed best, though it is likely that the performance of the r firmtask1 model (the top line with x X  X ) would decrease with larger amounts of noise because the de-cay function depends on the length of the context, not the number of queries predicted to be on-task. Thus, for on-task queries occurring early on in very large contexts, the decay weight will effectively be 0. You may notice that this model also increases for a bit starting at R = 2. This is likely due to the decay function used: since every query in the context, and not just the on-task queries, count toward the distances between an on-task query and the reference query under r firmtask1 , on-task queries are actually down-weighted els. The graph indicates that this change in weighting is helpful. This also suggests that setting  X  differently may improve the performance of the other models.

The r firmtask2 model (diamonds) comes in at a close second and narrowly outperforms r hardtask (circles). All three mod-els outperform the baselines X  X sing only the reference query (squares) and r decay over the entire context (triangles). The r softtask model, however, performs rather poorly. While it can offer some improvement over using just the reference query for a small amount of noise, once the noise level reaches four off-task queries, it is not longer viable. It does, however, outperform the decay model applied in a task-unaware manner.

Another interesting point is that the performance of the task-aware models is actually better at R = 0 than if the known tasks are used. The likely explanation is that the same-task scores prevent on-task queries that are quite dif-ferent from the reference query from affecting the final rec-ommendation. These kinds of queries may introduce more noise since their only recommendation overlap with the ref-erence query may be generic queries, such as  X  X oogle X . This is not always the case, however. For example, one task con-sists of the queries [ X  X lan greenspan X ,  X  X ongest serving Fed-eral Reserve Chairman X  X . The first query is detected to be shown in Figure 10. Reciprocal rank (RR) values of 0 are left blank. Figure 10: An example of a randomly generated mixed context along with the same-task scores. The top query (No. 5) is the reference query. The bolded queries are on-task. off-task , however, it is imperative to generate decent recom-mendations since the reference query generates generic Fed-eral Reserve-related queries and not ones focused on Alan Greenspan.

Overall, though, the same-task classification with a thresh-old  X  = 0 : 2 worked well. The same-task classification preci-sion relative to the positive class was on average 80%. The precision relative to the negative class varied across each noise level, but was on average 99%. The average accuracy was 93%. The decent same-task classification is why the three models at the top of Figure 8 are so flat.
The results of our experiments demonstrate not only the usefulness of on-task context, but also the extreme impact of off-task and mixed contexts. The results from Experiment 4 suggest that the appropriate model is one that balances a hard threshold to remove any influence from context queries predicted to be off-task, and to weight the importance of the remaining queries by both their distance to the reference query and by the confidence of the same-task classification. Based on the results, we recommend the r firmtask2 model, since its performance will be consistent regardless of how far back in a user X  X  history we go, unlike r firmtask1 .
We did not see any substantial effects from using task clus-tering, as Liao et al. [14] used. However, other task iden-tification schemes may perform differently; after all, as we saw in Experiment 4, our task identification method actually caused slight improvements over using the true tasks.
To get a feel for the quality of the recommendations pro-duced generally with the AOL query logs and specifically by different models, consider the randomly generated mixed context in Figure 10. The top five recommendations from three methods for this context are shown in Figure 9. No-tice that using only the reference query produces popular queries, none of which are related to the context in the least. Meanwhile, blindly using the context produces suggestions that are swamped by the off-task queries. This is in stark contrast to using the hard task model, which suggests four decent looking suggestions, three of which have non-zeros reciprocal rank values.

Another observation from this example is the amount of noise: very popular queries such as  X  X oogle X  appear in the suggestion lists. This is in part due to not finely tuning the various parameters of the underlying recommendation algorithm we used X  X omething we avoided since the recom-mendation algorithm was not the focus of this work. It is likely also due to the age and scope of the AOL query log, which is not large compared to the commercial logs com-monly used in query recommendation research. Nonetheless, the context-aware recommendation models we presented in Section 4 are compatible with any recommendation algo-rithm that supplies a sufficiently long, scored list of sugges-tions. We leave the investigation as to whether performance follows for future work.

Many of the tasks for which task-aware recommenda-tion performed well involved specialization reformulations. Some examples include: heart rate  X  slow fast heart rate , bobcat tractor  X  bobcat tractor attachment , disneyland ho-tel  X  disneyland hotel reviews , and elliptical trainer  X  elliptical trainer benefits . One possible reason for this is that incorpo-rating recommendations for a context query that is a subset of the reference query focuses the final recommendations on the most important concepts.

None of the experiments used notions of temporal sessions or complete user histories. We did this mainly because the mixed contexts were generated and not from actual user logs where context windows could be tied to temporal bound-aries, e.g., two-day windows. We believe that by focusing on factors such as on-and off-task queries, we struck at the core questions in this space. We leave testing whether the re-sults from these experiments port to real user data to future work, but we believe they will, especially given the results of related studies, such as those conducted by Liao et al. [14] and Filali et al. [8].
In this work, we investigated four key research questions surrounding context-aware query recommendation and the effects of on-and off-task recommendation: (RQ1) How does on-task context affect recommendation quality relative to using only the most recently used query? (RQ2) How does off-task context affect recommendation quality? (RQ3) How does mixed context consisting of on-and off-task queries affect recommendation quality? and (RQ4) How do task-aware recommendation models affect performance given a mixed context?
We designed four sets of experiments that used random walks over a term-query graph of the 2006 AOL query logs for recommendations and the 2010 X 2011 TREC Session Track data for tasks. We evaluated all models using micro-averaged mean reciprocal rank (MRR) over the 212 TREC tasks and used 50 trials when randomization was needed. The results of our experiments show that on-task context is, on average, very helpful, but can sometimes degrade per-formance substantially. On the other hand, off-task context o nly degrades performance. We found that leveraging mixed contexts, when used without regard of the task correspond-ing to each of its constituent queries, reduced the quality of recommendations. Finally, we demonstrated the effective-ness of four task-aware models, which rely on a state-of-the-art search task identification algorithm. Three of the four models out-performed using only the most recently submit-ted query when up to ten off-task queries were added. Re-sults suggest that two of those models would maintain their accuracy at large levels of noise since their weighting schemes completely ignore any query classified as off-task.
In many ways these results are not surprising: incorpo-rating relevant queries improves query accuracy just as rel-evance feedback improves document ranking. What is sur-prising, though, is the extraordinary sensitivity of these ap-proaches to off-task queries. Even making a single mistake and including an off-task query can shift many approaches in the wrong direction, away from relevant documents. This work takes a methodical look at the relative impact of on-and off-task queries and provides a deeper understanding of their inter-relationships than had been reported previously.
In addition, our findings rely on publicly accessible datasets, which eases others X  efforts in validating and reproducing our results. Furthermore, it allows others to compare directly with our results if they so desire. The simulated contexts and corresponding recommendations used in our experiments are available on our website. 3
There are many avenues of future work. One element to explore is the portability of our findings to real user search behavior over many more tasks. Another is to con-sider other recommendation algorithms, including ones not dependent on query logs [1] and ones that use other user behavior [7, 19]. We only considered the effectiveness of query recommendations to address the current task, but other measures of suggestion quality exists [16], such as at-tractiveness and diversity; exploring these other evaluations would provide a fuller picture of the value of task-aware rec-ommendation. Other future directions include expanding the analysis to other applications, such as website or task recommendation [20].
This work was supported in part by the Center for In-telligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. [1] S. Bhatia, D. Majumdar, and P. Mitra. Query sugges-[2] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gio-[3] F. Bonchi, R. Perego, F. Silvestri, H. Vahabi, and http://ciir.cs.umass.edu/down loads/ task-aware-query-recommendation [4] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and [5] H. Cao, D. Jiang, J. Pei, E. Chen, and H. Li. Towards [6] G. Cormack, M. Smucker, and C. Clarke. Efficient and [7] S. Cucerzan and R. W. White. Query suggestion based [8] K. Filali, A. Nair, and C. Leggetter. Transitive history-[9] C. Huang, L. Chien, and Y. Oyang. Relevant term sug-[10] R. Jones and K. L. Klinkner. Beyond the session time-[11] E. Kanoulas, P. Clough, B. Carterette, and M. Sander-[12] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and [13] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, [14] Z. Liao, Y. Song, L.-w. He, and Y. Huang. Evaluating [15] C. Lucchese, S. Orlando, R. Perego, F. Silvestri, and [16] Z. Ma, Y. Chen, R. Song, T. Sakai, J. Lu, and J. Wen. [17] D. Metzler and W. Croft. Combining the language [18] F. Radlinski and T. Joachims. Query chains: learning [19] Y. Song and L.-w. He. Optimal rare query suggestion [20] G. Tolomei, S. Orlando, and F. Silvestri. Towards
