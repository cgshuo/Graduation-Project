 Challenging the implicit reliance on documen t collections, this pap er discusses the pros and cons of using query logs rather than documen t collections, as self-con tained sources of data in textual information extraction. The di erences are quan ti ed as part of a large-scale study on extracting prominen t attributes or quan ti able prop erties of classes (e.g., top speed , pric e and fuel consumption for CarMo del ) from unstructured text. In a head-to-head qualitativ e com-parison, a ligh tweigh t extraction metho d pro duces class at-tributes that are 45% more accurate on average, when ac-quired from query logs rather than Web documen ts.
 H.3.1 [ Information Storage and Retriev al ]: Con ten t Analysis and Indexing; I.2.7 [ Arti cial Intelligence ]: Nat-ural Language Pro cessing; I.2.6 [ Arti cial Intelligence ]: Learning; H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al Algorithms, Exp erimen tation Kno wledge acquisition, class attribute extraction, textual data sources, query logs
To acquire useful kno wledge in the form of entities and relationships among those entities, existing work in infor-mation extraction taps on a variet y of textual data sources. Whether domain-sp eci c (e.g., collections of medical articles or job announcemen ts) or general-purp ose (e.g., news cor-pora or the Web), textual data sources are alw ays assumed Con tributions made during internships at Google.
 Table 1: Textual documen ts vs. queries as data sources for information extraction to be available as documen t collections [12]. This reliance on documen t collections is by no means a weakness. On the con trary , the availabilit y of larger documen t collections is instrumen tal in the trend towards large-scale information extraction. But as extraction exp erimen ts on terab yte-sized documen t collections become less rare [4], they have yet to capitalize on an alternativ e resource of textual information (i.e., searc h queries) that millions of users generate daily , as they nd information through Web searc h.

Table 1 compares documen t collections and query logs as poten tial sources of textual data for information extraction. On average, documen ts have textual con ten t of higher qual-ity, con vey information directly in natural language rather than through sets of keyw ords, and con tain more raw tex-tual data. In con trast, queries are usually ambiguous, short, keyw ord-based appro ximations of often-undersp eci ed user information needs. An intriguing asp ect of queries is, how-ever, their abilit y to indirectly capture human kno wledge, precisely as they inquire about what is already kno wn. In-deed, users form ulate their queries based on the common-sense kno wledge that they already possess at the time of the searc h. Therefore, searc h queries play two roles sim ultane-ously . In addition to requesting new information, they also indirectly con vey kno wledge in the pro cess. If kno wledge is generally prominen t or relev ant, people will eventually ask about it [13], esp ecially as the num ber of users and the quan-tity and breadth of the available kno wledge increase, as it is the case with the Web as a whole. Query logs con vey kno wl-edge through requests that may be answ ered by kno wledge asserted in exp ository text of documen t collections. This pap er is the rst comparison of Web documen ts and Web query logs as separate, self-sucien t data sources for in-formation extraction, through a large-scale study on extract-ing prominen t attributes or quan ti able prop erties of classes (e.g., top speed , pric e and fuel consumption for CarMo del ) from unstructured text. The attributes corresp ond to useful relations among classes, whic h is a step beyond mining in-stances of a xed target relation that is speci ed in adv ance. More imp ortan tly, class attributes have sev eral applications. In kno wledge acquisition, they represen t building blo cks to-wards the app ealing, and yet elusiv e goal of constructing large-scale kno wledge bases automatically [17]. They also constitute topics (e.g., radius , surfac e gravity , orbital velo c-ity etc.) to be suggested automatically , as human con tribu-tors man ually add new entries (e.g., for a newly disco vered celestial body) to resources suc h as Wikip edia [16]. In open-domain question answ ering, the attributes are useful in ex-panding and calibrating existing answ er type hierarc hies [9] towards frequen t information needs. In Web searc h, the re-sults returned to a query that refers to a named entity (e.g., Pink Floyd ) can be augmen ted with a compilation of speci c facts, based on the set of attributes extracted in adv ance for the class to whic h the named entity belongs. Moreo ver, the original query can be re ned into seman tically-justi ed query suggestions, by concatenating it with one of the top extracted attributes for the corresp onding class (e.g., Pink Floyd albums for Pink Floyd ).

The remainder of the pap er is structured as follo ws. Sec-tion 2 introduces a metho d for extracting quan ti able at-tributes of arbitrary classes from query logs and Web docu-men ts. The metho d relies on a small set of linguistically mo-tivated extraction patterns to extract candidate attributes from sen tences in documen ts, and from entries in query logs resp ectiv ely. Section 4 is the rst head-to-head comparison of the qualit y of information (in this case, class attributes) extracted from documen t collections vs. query logs. Results are describ ed comparing attributes extracted from appro xi-mately 100 million Web documen ts vs. 50 million queries.
The extraction metho d is designed to be simple, general and generic, allo wing for robustness on large amoun ts of noisy data, the abilit y to operate on a wide range of open-domain target classes, and most imp ortan tly ensuring a fair, apple-to-apple comparison of results obtained from query logs vs. Web documen ts. As sho wn in Figure 1, given a set of target classes, the extraction metho d iden ti es rele-vant sen tences and queries, collects candidate attributes for various instances of the classes, and ranks the candidate at-tributes within eac h class.
The linguistic pro cessing of documen t collections is lim-ited to tok enization, sen tence boundary detection and part-of-sp eech tagging. Comparativ ely, the queries from query logs are not pre-pro cessed in any way. Thus, the input data source is available in the form of part-of-sp eech tagged doc-umen t sen tences with documen t collections, or query strings in isolation of other queries in the case of query logs.
Follo wing the view that a class is a placeholder for a set of instances that share similar attributes or prop erties [7], a target class (e.g., HeavenlyBo dy ) for whic h attributes must be extracted is speci ed through a set of represen tativ e in-stances (e.g., Venus , Uranus , Sirius etc.). It is straigh tfor-ward to obtain high-qualit y sets of instances that belong to a common, arbitrary class by either a) acquiring a reasonably large set of instances through bootstrapping from a small set of man ually speci ed instances [2]; or b) selecting instances from available lexicons, gazetteers and Web-deriv ed lists of names; or c) acquiring the instances automatically from a large text collection (including the Web), based on the class name alone [19]; or d) selecting prominen t clusters of in-stances from distributionally similar phrases acquired from a large text collection [10]; or e) simply assem bling instance sets man ually , from Web-based lists.
For robustness and scalabilit y, a small set of linguistically-motiv ated patterns extract poten tial pairs of a class instance and an attribute from the textual data source. Although the patterns are the same, their matc hing onto text is sligh tly di eren t on documen t sen tences vs. queries.

With documen t sen tences, eac h pattern is matc hed par-tially against the text, allo wing other words to occur around the matc h. When a pattern matc hes a sen tence, the outer boundaries of the matc h are chec ked and computed heuris-tically based on the part of speech tags. For example, one of the extraction patterns matc hes the sen tence \Human activ-ity has a e cted Earth's surfac e temp eratur e during the last 130 years" via the instance Earth , pro ducing the candidate attribute surfac e temp eratur e . In con trast, although the sen-tence \The market shar e of Franc e Telecom for local trac was 80.9% in Decemb er 2002" matc hes one of the patterns via the instance Franc e , it does not pro duce any attribute because the instance is part of a longer sequence of prop er nouns, namely Franc e Telecom .

With queries, patterns are matc hed fully , with no addi-tional words allo wed around the matc h and with no addi-tional chec ks. Thus, the outer boundaries of the candidate attributes are appro ximated trivially through an extremit y of the query , pro ducing the candidate attributes size and downlo ad full version from the queries size of venus and downlo ad full version of mdk2 resp ectiv ely.

With the exception of how the patterns are matc hed onto text (i.e., fully vs. partially), the extraction metho d operates iden tically on both documen ts vs. queries.
A candidate attribute selected for an instance from the input text (e.g., diameter for Jupiter from the rst sen tence, or atmospher e for earth from the rst query in Figure 1) is in turn a candidate attribute of the class(es) to whic h the instances belong. For example, diameter and atmospher e be-come asso ciated to the class C 1 in Figure 1 because Jupiter and Earth are instances of that class. The score of a candi-date attribute A within a class C is higher if the attribute is asso ciated to more of the instances I of C :
Candidates sim ultaneously asso ciated to man y classes are either less useful because they are generic (e.g., history , meaning , de nition ), or incorrect because they are extracted from constructs that occur frequen tly in natural language sen tences (e.g., case and position extracted from the second and sixth sen tence of Figure 1 resp ectiv ely). An alternativ e scoring form ula demotes suc h attributes accordingly:
The scores determine the relativ e ranking of candidate at-tributes within a class. The rank ed list is passed through a lter that aims at reducing the num ber of attributes that are seman tically close to one another, thus increasing the div er-sity and usefulness of the overall list of attributes for that class. For the sak e of simplicit y, we prefer a fast heuristic that ags attributes as poten tially redundan t if they have a low edit distance to, or share the same head word with, an-other attribute already encoun tered in the list. With mo d-erate e ort and added complexit y, this heuristic could be com bined with one of the popular seman tic distance metrics based on WordNet. After discarding redundan t attributes, the resulting rank ed lists of attributes constitute the output of the extraction metho d. Tw o sets of exp erimen ts acquire attributes separately from Web documen ts main tained by and searc h queries submit-ted to the Google searc h engine. The documen t collection (D) consists of appro ximately 100 million Web documen ts in English, as available in a Web rep ository snapshot in 2006. The textual portion of the documen ts is cleaned of html , to-kenized, split into sen tences and part-of-sp eech tagged using the TnT tagger [3].

The collection of queries (Q) is a random sample of fully-anon ymized queries in English submitted by Web users in 2006. The sample con tains around 50 million unique queries. Eac h query is accompanied by its frequency of occurrence in the logs.

The rst graph in Figure 2 sho ws the distribution of the queries from the random sample, according to the num ber of words in eac h query . Despite the di erences in the distribu-tions of unique (dotted line) vs. all (solid line) queries, the rst graph in Figure 2 con rms that most searc h queries in Q are relativ ely short. Therefore, the amoun t of input data that is actually usable by the extraction metho d from query logs is only a fraction of the available 50 million queries, since an attribute cannot be extracted for a given class un-less it occurs together with a class instance in an input query , whic h is a condition that is less likely to be satis ed in the case of short queries.
The second graph in Figure 2 sho ws the distribution of the input Web documen ts, according to the num ber of words after documen ts were cleaned of html tags. As exp ected, the distribution of documen ts is quite di eren t from that of queries, as illustrated by the two graphs in Figure 2. First, the possible range of the num ber of words is much wider in the case of Web documen ts, since documen ts are signi -can tly longer than queries. Consequen tly, the percen tage of documen ts having any given length is quite small, regard-less of the length. Second, longer documen ts tend to occur less frequen tly, throughout the entire range of the documen t length.
The target classes selected for exp erimen ts are eac h spec-i ed as an (incomplete) set of represen tativ e instances, de-tails on whic h are given in Table 2. The num ber of given instances varies from 50 (for Carto onChar acter ) to 1500 (for Actor ), with a median of 197 instances per class. The classes also di er with resp ect to the domain of interest (e.g., Health for Drug vs. Entertainmen t for Movie ), instance capitaliza-tion (e.g., instances in BasicF ood usually occur in text in lower rather than upp er case), and conceptual type (e.g., abstraction for Religion vs. group for SoccerT eam vs. activ-ity for Vide oGame ).

Quan titativ ely, the selected target classes also exhibit great variation from the point of view of their popularit y within query logs, measured by the sum of the frequencies of the input queries that fully matc h any of the instances of eac h class (e.g., the queries san francisc o for City , or harvar d for University ). As sho wn in Figure 3, the corresp onding frequency sums per target class vary considerably , ranging between 65,556 (for Wine ) and 29,361,706 (for Comp any ). Therefore, we choose what we feel to be a large enough num-ber of classes (20) to prop erly ensure varied exp erimen tation on sev eral dimensions, while taking into accoun t the time in-tensiv e nature of man ual accuracy judgmen ts often required in the evaluation of information extraction systems [2, 4].
Multiple lists of attributes are evaluated for eac h class, corresp onding to the com bination of the use of one of the two ranking functions (frequency-based or normalized) on either Web documen ts (e.g., D-fr eq ) or query logs (e.g., Q-Table 3: Correctness lab els for the man ual assess-men t of attributes norm ). To remo ve any undesirable psyc hological bias to-wards higher-rank ed attributes during the assessmen t, the elemen ts of eac h list to be evaluated are sorted alphab eti-cally into a merged list.

A human judge man ually assigns a correctness lab el to eac h attribute of the merged list within its resp ectiv e class. Similarly to metho dology previously prop osed to evaluate answ ers to De nition questions [21], an attribute is vital if it must be presen t in an ideal list of attributes of the target class; okay if it pro vides useful but non-essen tial information; and wrong if it is incorrect. Thus, a correctness lab el is man ually assigned to a total of 5,859 attributes extracted for the 20 target classes, in a pro cess that con rms that evaluation of information extraction metho ds can be quite time consuming.

To compute the overall precision score over a given rank ed list of extracted attributes, the correctness lab els are con-verted to numeric values as sho wn in Table 3. Precision at some rank N in the list is thus measured as the sum of the assigned values of the rst N candidate attributes, divided by N .
For a formal analysis of qualitativ e performance, Table 4 pro vides a detailed picture of precision scores for eac h of the twenty target classes. For completeness, the scores in the table capture precision at the very top of the extracted lists of attributes (rank 5) as well as over a wider range of those lists (ranks 10 through 50).
 Tw o conclusions can be dra wn after insp ecting the results. First, the qualit y of the results varies among classes. At the lower end, the precision for the class Wine is below 0.40 at rank 5. At the higher end, the attributes for Comp any are very good, with precision scores above 0.90 even at rank 20. Second, documen ts and queries are not equally useful in class attribute extraction. The attributes extracted from documen ts are better at the very top of the list (rank 5) for the class SoccerT eam and at all ranks for City . However, the large ma jorit y of the classes have higher precision scores when the attributes are extracted from queries rather than documen ts. The di erences in qualit y are particularly high for classes like HeavenlyBo dy , CarMo del , BasicF ood , Flower and Mountain . To better quan tify the qualit y gap, the last rows of Table 4 sho w the precision computed as an aver-age over all classes, rather than for eac h class individually . Consisten tly over all computed ranks, the precision is about 45% better on average when using queries rather than doc-Table 5: Top attributes extracted with normalized ranking for various classes from Web documen ts (D) vs. query logs (Q) umen ts. This is the most imp ortan t result of the pap er. It sho ws that query logs represen t a comp etitiv e resource against documen t collections in class attribute extraction.
As an alternativ e to Table 4, Table 5 illustrates the top attributes extracted from text for a few of the target classes. Documen ts pro duce more spurious items, as indicated by a more frequen t presence of attributes that are deemed wrong , suc h as bowl for BasicF ood , mg for Drug , or temple for Heav-enlyBo dy . The highest-rank ed attributes acquired from query logs are relativ ely more useful, particularly for the rst three classes sho wn in Table 5.
 Table 6: Impact of extraction with normalized rank-ing from a fth vs. half vs. all of the Web documen ts
The precision results con rm and quan tify the qualitativ e adv antage of query logs over documen ts, in the task of at-tribute extraction. However, the exp erimen ts do not tak e into accoun t the fact that it is more likely for an extraction pattern to matc h a portion of a documen t rather than a query , simply because a documen t con tains more raw text. Other things being equal, although the percen tage of spuri-ous attributes among all extracted attributes is exp ected to be similar when extracted from the 100 million documen ts vs. 50 million query logs, the absolute num ber of suc h spu-rious attributes is exp ected to be higher from documen ts. Although it is not really intuitiv e that using too many input documen ts could result in lower precision due to an over-whelming num ber of spurious attributes, additional exp eri-men ts verify whether that may be the case. Table 6 com-pares the precision at various ranks as an average over all classes, when attributes are extracted ( D-norm ) from 20%, 50% or 100% of the available input Web documen ts. The table sho ws that, in fact, using few er documen ts does not impro ve precision, whic h instead degrades sligh tly.
Figure 4 pro vides a graphical comparison of precision from all Web documen ts vs. query logs, at all ranks from 1 through 50. Besides the head-to-head comparison of the two types of data sources, the graphs sho w the added bene t of norm alized (as opp osed to freq uency-based) ranking, whic h is more ap-classes from Web documen ts (D) vs. query logs (Q) paren t in the case of attributes extracted from documen ts ( D-norm vs. D-fr eq ).
Since the ideal, complete set of items to be extracted is usually not available, most studies on Web information ex-traction are forced to forgo the evaluation of recall and focus instead on measuring precision [4]. Similarly , the man ual enumeration of the complete set of attributes of eac h target class, to measure recall, is unfeasible. As a tractable alterna-tive to evaluating recall, the attributes extracted from docu-men ts (with D-fr eq or D-norm ) that were man ually judged as vital during the evaluation of precision are temp orarily con-sidered as a reference set for measuring the relativ e recall. Giv en this reference set of attributes and a list of attributes acquired from query logs, the evaluation of the latter con-sists in automatically verifying whether eac h attribute from query logs is an exact, case-insensitiv e string matc h of one of the attributes in the reference set. Therefore, the scores computed as an average over all target classes in Table 7 represen t lower bounds on relativ e recall rather than actual relativ e recall values, since extracted attributes that are se-Table 7: Coverage of the list of attributes extracted with normalized ranking from query logs, relativ e to the set of vital attributes extracted from Web documen ts man tically equiv alen t but lexically di eren t to one of the attributes in the reference set (e.g., plural forms, di eren t spelling, synon yms etc.) unfairly receiv e no credit. The per-formance varies by class, with relativ e recall values in the range from 0.03 (for City ) to 0.25 (for HeavenlyBo dy ) at rank 10. Similarly , recall values vary from 0.11 (for Wine ) to 0.50 (for BasicF ood ) at rank 50.
In terms of scale and general goals, our work ts into a broader trend towards large-scale information extraction. Previous studies rely exclusiv ely on large documen t collec-tions, for mining pre-sp eci ed types of relations suc h as InstanceOf [15], Person-AuthorOf-In vention [11], Compan y-HeadquartersIn-Lo cation [2] or Coun try-CapitalOf-Cit y [4] from text. In con trast, we explore the role of both documen t collections and query logs in extracting an open, rather than pre-sp eci ed type of information, namely class attributes. A related recen t approac h [18] pursues the goal of unrestricted relation disco very from text.

Our extracted attributes are relations among objects in the given class, and objects or values from other, \hidden" classes. Determining the type of the \hidden" argumen t of eac h attribute (e.g., Person and Location for the attributes chief executive oc er and headquarters of the class Com-pany ) is beyond the scop e of this pap er. Nev ertheless, the lists of extracted attributes have direct bene ts in gauging existing metho ds for harv esting pre-sp eci ed seman tic rela-tions [4, 14], towards the acquisition of relations that are of real-w orld interest to a wide set of Web users, e.g., towards nding mechanisms of action for Drugs and health bene ts for BasicF ood .

Query logs have been a natural candidate in e orts to impro ve the qualit y of information retriev al, either directly through re-ranking of retriev ed documen ts [23, 22, 1] and query expansion [6], or indirectly through the dev elopmen t of spelling correction mo dels [8]. [13] were the rst to ex-plore query logs as a resource for acquiring explicit relations, but evaluated their approac h on a very small set of target classes, without a comparison to traditional documen t-based metho ds. Suc h a comparativ e study is highly useful, if not necessary , before further explorations based on query logs.
In [5], the acquisition of attributes and other kno wledge relies on Web users who explicitly specify it by hand. In con trast, we may think of our approac h as Web users im-plicitly giving us the same type of information, outside of any systematic attempts to collect kno wledge of general use from the users.

The metho d prop osed in [20] applies lexico-syn tactic pat-terns to text within a small collection of Web documen ts. The resulting attributes are evaluated through a notion of question answ erabilit y, wherein an attribute is judged to be valid if a question can be form ulated about it. More pre-cisely , evaluation consists in users man ually assessing how natural the resulting candidate attributes are, when placed in a wh-question. Comparativ ely, our evaluation is stricter. Indeed, man y attributes, suc h as long term uses and users for the class Drugs , are mark ed as wrong in our evaluation, although they would easily pass the question answ erabilit y test (e.g., \What are the long term uses of Prilose c?" ) used in [20].
Con rming our intuition that Web query logs as a whole mirror a signi can t amoun t of kno wledge presen t within Web documen ts, the exp erimen tal results of this pap er introduce query logs as a valuable resource in textual information ex-traction. Somewhat surprisingly , a robust metho d for ex-tracting class attributes pro duces signi can tly better results when applied to query logs rather than Web documen ts, thus holding the promise of a new path in researc h in information extraction. Ongoing work includes a mo del for com bining the two types of data sources while accoun ting for the di er-ence in their variabilit y, a weakly sup ervised metho d based on seeds rather than patterns, and exploration of the role of query logs in other information extraction tasks. [1] E. Agic htein, E. Brill, and S. Dumais. Impro ving Web [2] E. Agic htein and L. Gra vano. Sno wball: Extracting [3] T. Bran ts. TnT -a statistical part of speech tagger. In [4] M. Cafarella, D. Downey , S. Soderland, and [5] T. Chklo vski and Y. Gil. An analysis of kno wledge [6] H. Cui, J. Wen, J. Nie, and W. Ma. Probabilistic [7] D. Dowty, R. Wall, and S. Peters. Intr oduction to [8] M. Li, M. Zhu, Y. Zhang, and M. Zhou. Exploring [9] X. Li and D. Roth. Learning question classi ers. In [10] D. Lin and P. Pantel. Concept disco very from text. In [11] L. Lita and J. Carb onell. Instance-based question [12] R. Mo oney and R. Bunescu. Mining kno wledge from [13] M. Pasca and B. Van Durme. What you seek is what [14] P. Pantel and M. Pennacc hiotti. Espresso: Lev eraging [15] P. Pantel and D. Ravichandran. Automatically [16] M. Rem y. Wikip edia: The free encyclop edia. Online [17] L. Schubert. Turing's dream and the kno wledge [18] Y. Shin yama and S. Sekine. Preemptiv e information [19] K. Shinzato and K. Torisa wa. Acquiring hyponym y [20] K. Tokunaga, J. Kazama, and K. Torisa wa. Automatic [21] E. Voorhees. Evaluating answ ers to de nition [22] G. Wang, T. Chua, and Y. Wang. Extracting key [23] Z. Zhuang and S. Cucerzan. Re-ranking searc h results
