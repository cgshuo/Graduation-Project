 Nicolas Pe  X  cheux 1  X  Alexandre Allauzen 1  X  Jan Niehues 2  X  Franc  X ois Yvon 1 Abstract In Statistical Machine Translation (SMT), the constraints on word reorderings have a great impact on the set of potential translations that is explored during search. Notwithstanding computational issues, the reordering space of a SMT system needs to be designed with great care: if a larger search space is likely to yield better translations, it may also lead to more decoding errors, because of the added ambiguity and the interaction with the pruning strategy. In this paper, we study the reordering search space, using a state-of-the art translation system, where all reorderings are represented in a permutation lattice prior to decoding. This allows us to directly explore and compare different reordering schemes and oracle settings. We also study in detail a rule-based preordering system, varying the length and number of rules, the tagset used, as well as contrasting with purely combinatorial subsets of permutations. We carry out experiments on three language pairs in both directions: English-French, a close language pair; English-German and English-Czech, two much more challenging pairs. We show that even though it might be desirable to design better reordering spaces, model and search errors seem to be the most important issues. Therefore, improvements of the reordering space should come along with improvements of the associated models to be really effective. Reordering is known to be a critical issue for statistical machine translation and the reordering complexity for a language pair can be considered as a relevant indicator of the difficulty to automatically translate from one into the other (Birch et al. 2008 ).

When translating a source sentence, most machine translation systems, either explicitly or implicitly, have to choose the order in which they will process the source sentence to compute its translation, thereby inducing a reordering of the source words which reflects the target word order. In order to correctly generate the word order, two main problems have to be solved: the identification of a restricted number of possible reorderings and the numerical evaluation of their appropriate-combinatorial set of all possible permutations. Even for short sentences, this set contains too much ambiguity and an overwhelming number of linguistically meaningless reorderings. It is therefore necessary to rely on methods that filter this space so as to meet the two following conflicting goals: (a) the search space should be large enough to contain good translation hypotheses; (b) yet small enough to be rapidly explored. This first problem thus amounts to identify appropriate reordering constraints , which will help to shape the set of permutations of the source that will actually be considered. The second is to design reordering models that can assign numerical scores to candidate permutations, so that the most correct word order(s) will receive high scores. Those include distance-based models, lexicalized reordering models (Tillmann et al. 2004 ) or hierarchical lexicalized models (Galley and Manning 2008 ) among many others.

In this work we mainly focus on analyzing the first issue. While improvements on the reordering models are likely to benefit the overall translation performance, it is less obvious to what extent the reordering constraints are currently impacting the translation process. Indeed, in addition to computational issues, there is a tradeoff when building the reordering space of a machine translation system. On the one hand, a larger space is more likely to contain a permutation that can yield a relevant translation. On the other hand, it may also cause more decoding errors, because of both the ambiguity of natural languages and the necessary pruning of the search space. It is then of great help to understand the current limits of an SMT regarding the reordering space. Thus, the main questions we address in this work are: how good are the current reordering search spaces? how to design them? is it important that they contain the exact needed reorderings or good approximations would be enough? are the models able to make use of the best reorderings from the search space? to what extent would the overall system benefit from much better reordering spaces? Therefore, we investigate several ways to generate the reordering space, in order to evaluate how the SMT system can benefit from a larger/better reordering space. In addition, by studying monotonic as well as various oracle-like reordering spaces, we compute lower and upper bounds on the possible reordering space design, also giving insights on the complex influence of search and model errors on the translation quality.
Various constraints on admissible permutations have been proposed in the past including IBM (Berger et al. 1996 ), MJ (Kumar and Byrne 2005 ) or ITG (Wu 1997 ). Those constraints have been compared in terms of performance (Zens and Ney 2003 ; Zens et al. 2004 ) or in oracle settings (Dreyer et al. 2007 ; Wisniewski and Yvon 2013 ). Other approaches include linguistically motivated rules that are automatically learned (Crego and Marin  X  o 2006 ; Niehues and Kolss 2009 ; Herrmann et al. 2013a ). To the best of our knowledge, these two families of approaches, purely combinatorial on the one hand and empirically learned on the other, have never been systematically compared. In this work, we use a rule-based reordering system in which reordering rules are extracted during the training phase (Sect. 3.2 ), considering word factors instead of surface word in an attempt to mitigate sparsity issues. We study in detail the effectiveness of the rule-based approach in defining an accurate search space, and show that linguistically motivated constraints define can be used to define a compact search space, and yet, improve the translation quality.
In the phrase-based approach, word reorderings can be divided in two tightly intertwined types: local reorderings that take place within phrases; and longer reorderings of those phrases. The additional use of pre-ordering methods is introduced eg. in (Xia and McCord 2004 ; Collins et al. 2005 ; Tromble and Eisner 2009 ; Genzel 2010 ): in this approach, source sentences are reordered in a preprocessing step to match the target word order and then fed into the standard phrase-based pipeline. This further complexifies the analysis of the reorderings that are actually considered in translation. Finally, because of pruning, only a restricted part of the search space is effectively explored. In this paper, we use a state-of-the-art n -gram SMT system (Crego et al. 2011 ), described in Sect. 2 , that splits reordering and decoding into two separate steps. Reorderings of the source sentence are compactly encoded in a permutation lattice, the reordering space , that is then translated in a monotonic fashion. This two-step approach allows us to study the reordering space that is explored and then to assess its impact on the whole translation process. This controlled framework also enables to directly compare the size and the coverage of the different reordering spaces. Therefore, even though we only consider one specific phrase-based architecture, we believe that most conclusions would carry over for other phrase-based systems, that mostly use the same reordering mechanisms, and even for hierarchical phrase-based systems Auli et al. ( 2009 ).
 Evaluation is carried out for three language pairs (French-English, German-English and Czech-English in both directions) that differ by the range of the involved reorderings. We measure the impact on the system performance as well as oracle decoding to better understand the potentials of the different reordering spaces as well as the influence of search and model errors on translation quality. We find that while there is ample room for improving the reordering space, this problem might not be the main issue, since search and model errors would prevent the SMT system to fully benefit from a more accurate search space.

The rest of this study is organized as follows. In Sect. 2 , we present the n -gram -based approach and its peculiarities. Among them, the rule-based method for source reordering is described in Sect. 3 , while Sect. 4 explains how to build the reordering space explored by the SMT system and how to derive oracle-like reorderings. In Sect. 5 , multiple experimental comparisons are carried out to assess the impact of the reordering space on translation performance. All our experiments use N CODE , an open source SMT toolkit, 1 which achieved state-of-the-art performance in recent evaluation campaigns (Callison-Burch et al. 2012 ; Bojar et al. 2013 , 2014 ). N CODE implements the bilingual n -gram approach to SMT (Casacuberta and Vidal 2004 ; Marin  X  o et al. 2006 ; Crego and Marin  X  o 2006 ) that is closely related to the standard phrase-based approach (Zens et al. 2002 ). In this approach, the translation of a source sentence f into a target sentence e is decomposed into two steps: a source reordering step and a monotonic translation step. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n -gram assumption to factor the joint probability of a sentence pair into a product of conditional probabilities involving bilingual atomic units called tuples :in other words, the translation model is a conventional n -gram model of synchronized segments.
 N CODE uses a set of feature functions embedded in a log-linear model (Och and Ney 2002 ) that is similar to standard phrase-based systems (see Crego et al. 2011 for details). The best translation is selected by solving the following program: where K feature functions f f k ; k  X  1 ... K g ) are weighted by a set of coefficients f k k g , Z f is a normalizing factor and a denotes the set of hidden variables corre-sponding to the reordering and segmentation of the source sentence. Along with the n-gram translation model and the target n-gram language model , 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann et al. 2004 ; Crego et al. 2011 ) aimed at predicting the orientation of the next translation unit; a  X  X  X eak X  X  distance-based distortion model ; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase and the corresponding weights ( k k ) are estimated during a tuning phase on held-out development data. The models that have a direct impact on the selected reordering are the monolingual and bilingual n -gram models, the lexicalized reordering models and the distortion model.

During training, source sentencesre first reordered so as to match the target word order by unfolding the word alignments. In a nutshell, unfolding aims to reorder the source words so as to remove all crossing alignment links; additional heuristic rules handle the movements of non-aligned words on the source side and make the procedure deterministic (see details in Crego et al. 2005 ). Unfolding is performed as follows: the target sentence is first segmented in K segments of consecutive words e  X  e 1 ... e k ... e K such that for each segment e k , if a word f aligns with one word in e , it is only aligned with words in e k , i.e. if f k  X f f 2 f j9 e 2 e k ;  X  f ; e  X 2 a g is the set of source words aligned with e k , then 8 f 2 f k ; 8 e 2 e ;  X  f ; e  X 2 a ) e 2 e k .This is the same as for standard phrase extraction, except that the source words need not be consecutive. One can then output the reordered source words ~ f  X  f 1 ... f k ... f K (using monotonic order withing each f k ) 2 and the tuple sequence f X  f k ; e k  X g k . Figure 1 displays a simple example, where the word politicians is moved to the start of the sentence. Unaligned words on the target side, such as l 0 in Fig. 1 , cause problems as the search does needs input words to generate units; they are consequently attached to the neighbor tuple which maximizes IBM model 1 lexical probabilities (de Gispert and Marin  X  o 2006 ). Tuples are then extracted in such a way that a unique segmentation of the bilingual corpus is achieved. A n -gram translation model and optional word factor models are then estimated over the training corpus composed of tuple sequences, using modified Kneser-Ney smoothing (Chen and Goodman 1998 ).

During decoding, the source sentence is first reordered so as to reproduce the word order modifications introduced during the tuple extraction process, i.e. to best match the target word order. This generates a word lattice containing the most promising source permutations. This lattice represents the reordering space that is then searched for the best candidate translation. As exhaustive search is intractable, N
CODE uses a beam search strategy based on stacks. As future cost estimation is problematic for multiple n -gram models, N CODE uses one stack per hypothesis translating the same input words , in contrast to the same number of words as in standard phrase-based systems. Thus the memory footprint of the decoding algorithm directly depends on the number of nodes in the reordering lattice. 3.1 Reorderings We have thus far used the term word reordering , even though the definition of how the words  X  X  X ove X  X  during translation is not trivial, as translation is not word-to-word. In fact, standard phrase-based approaches first segment the sentences in phrases and only consider reorderings of those phrases, while local  X  X  X ord moves X  X  are implicitly included withing the phrases. 3 In this work, we are interested in understanding the reorderings that are considered by the overall system, thus the focus on the permutations at the level of words. Word reorderings can be inferred from word alignments, which indeed originate from word-to-word translation models (Berger et al. 1996 ). It is however not straightforward to induce a permutation from many-to-many alignments and several heuristics, that differ by many subtle details, have been used for evaluating reordering (Birch 2011 )or preordering techniques (Tromble and Eisner 2009 ; Khalilov and Sima X  X n 2012 ; Neubig et al. 2012 ). In this work, we directly make use of the unfolding procedure to obtain word reorderings. As explained above (see Fig. 1 ), unfolding the alignment links directly results in a permutation, that we call the unfolded reordering .

Intuitively, a reordering occurs when some words move away from their initial position. In general, a global permutation can be decomposed in many local reorderings. Let S n be the set of permutations of f 1 ; ... ; n g for some integer n and let r 2 S n be a permutation r  X  X  r 1 ... r n  X  with 8 i ; r i 2f 1 ; ... ; n g . We define a reordering of r to be any subsequence r  X  i : j  X  r i ... r j of r with j j i j [ 1 such that: i.e. f r k g i k j  X f i ; ... ; j g . A reordering is said to be minimal if it is minimal for this property, i.e if it doesn X  X  (strictly) contain any reordering. Spans r  X  i : j correspond to the smallest (non trivial) ones to be reordered in order to recover r . It is easy to see that any permutation can be segmented in an unique way, where each segment is either a fixed point or a minimal reordering. For example, the unfolded reordering r  X  41235678 in Fig. 1 contains only one minimal reordering ( r  X  4123) and four fixed points. Any reordering p can be mapped to a (unique) permutation p 2 S j p j by renumbering, i.e. 8 k ; p k  X  p k min  X  p  X  , where min  X  p  X  is the smallest integer in p . Let R n 2 S n be the set of minimal reorderings of 1 ; ... ; n for n 2. The number of minimal reorderings r n  X j R n j , can be computed recursively as: where r  X  1  X  X  1 is a mathematical convenience. 3.2 Reordering rules extraction Reordering rules are automatically learned during the unfolding procedure. Let w  X  w 1 w 2 ... w n be a source sentence and t  X  t 1 t 2 ... t n the associated tag sequence. Let w r  X  w r 1 w r 2 ... w r n be the reordered sentence produced by the unfolding procedure where r  X  r 1 ... r n 2 S n . A reordering rule is extracted for any minimal reordering r  X  i : j of r . Rules then have the following form: where r  X  i : j is the induced permutation in S j j i  X  1 j obtained by renumbering r  X  i : j as described above (see Sect. 3.1 ). An example is in Fig. 1 , where only one rule: would be extracted.

Note that it would also be conceivable to also extract rules t  X  i : j ! r  X  i : j for non-minimal reorderings (subject to j j i j [ 1) in a way similar to the phrase extraction heuristic in M OSES ; preliminary experiments showed a slight drop in performance for this variant, which is not explored further in this paper.

To filter out alignment noise and limit the size of the reordering space, rules may be pruned according to a maximum cost threshold (maxcost). The cost of a rule is defined by: where t is any tag sequence, r 2 S j t j is a permutation and the counts are computed on the training data. Since this cost is the negative logarithm of a conditional ratio, a coarser tagset might be more heavily pruned than a fine-grain one, resulting in a smaller set of extracted rules; in principle, the optimal threshold thus depends on the granularity of the tagset, as well as on the translation direction. In our experiments, we use a default value of 4 for the parameter maxcost.

Rules may be also pruned according to their length (by default 10). Preliminary experiments show that further increasing this limit hardly makes any difference in performance. In fact, long rules are too sparse to possibly generalize beyond the training set. Long range reorderings are thus explicitly excluded from the model. Note that in standard phrase-based systems, the maximal reordering span, i.e. the distortion limit is usually even set to a smaller value than ours. 4 3.3 Alternative tagsets In (Crego and Marin  X  o 2006 ), rewriting rules are built using Part-of-speech (POS), rather than surface word forms in order to increase their generalization power. However, any word factor may possibly be used. To investigate different levels of generalization and the relevance of syntactic word factors, different tagsets are introduced.  X  One single tag (one): The tagset consist of one single tag. This means that the reordering rules are extracted and applied independently of any syntactic or contextual information. This results in a system which reduces the possible reorderings to all the ones observed in the training data.  X  Universal POS (ups): The tagset is reduced to 12 simple language-independent categories, in an attempt to limit the sparsity of the extracted rules. In this work we use the universal POS tagset described in (Petrov et al. 2012 ). For under-resourced languages, universal POS can be projected by cross-lingual transfer or learned from partial annotations (Li et al. 2012 ;Ta  X  ckstro  X  m et al. 2013 ;
Wisniewski et al. 2014 ), thereby relaxing the need for a POS tagger.  X  Enhanced POS (e50pos): The POS tags are lexicalized for the 50 most frequent words, resulting in more specific rules. Enhanced tags are closely related to lexicalized rules (Huang and Pendus 2013 ).  X  Brown classes (classes): Statistical word classes were found to be a good approximation for Part-of-Speech tags when a POS tagger is not available. In (Ramanathan and Visweswariah 2012 ), word clusters perform worse than POS, but still do reasonably well in a preordering setting. Durrani et al. ( 2014 ) report some gains when using word clusters, in addition to POS and morphological tags, in an Operation Sequence N-gram model. In this work, we compute statistical word clusters using the method of Brown et al. ( 1992 ).  X  Plain words (words): We use the surface word to build the rules, resulting in high-specific fully lexicalized rules with less generalization power. The reordering space explored during decoding can be generated in many different ways. In standard phrase-based SMT, all possible reorderings of source segments that do not result in a word move above a distortion threshold is implicitly used (see Lopez ( 2009 ) for a detailed account). In this work, the generation of the reordering space is controlled by a set of rewriting rules that non-deterministically reorder the source words. In this section, we detail the procedure use to generate the reordering lattice used in our rule-based system, as well as variants considered in our experiments. 4.1 Reordering lattice generation A permutation lattice (Crego 2008 ) is an acyclic weighted Finite State Automaton (FSA) L  X h V ; E ; R ; w i , where V is a set of nodes, E a set of edges, the alphabet R  X f 1 ; ... ; n g , w : E ! R a weight function, which generates (or recognize) a language L  X  L  X  S n , i.e. in which any path corresponds to a permutation of f 1 ; ... ; n g . For some subsets of permutations, a lattice encodes an exponential number of permutations with a polynomial number of nodes and edges. A strong property of permutation lattices is that all incoming paths that reach a node cover the same word indices.

The permutation lattice is built incrementally for any sentence w with corresponding tags t as follows. The monotonic path forms the initial lattice. Then for each segment [ i : j ] and each rule t  X  i : j ! r ; the lattice is expanded by adding the subpath r  X  X  i : j  X  : Figure 2 displays an example. This is performed in a parallel fashion so that rewriting rules do not interfere with each other. Applying the reordering rules finally results in a finite-state graph that represents the reordering space . This lattice may be weighted, using for example the probability of reordering rules as, in Herrmann et al. ( 2013b ); this allows us to include the lattice path score as a feature in the log-linear combination of Eq. ( 1 ). We have not pursued such developments in this study, as previous experiments did not show any improvement of performance.

In principle, one can design any set of permutation constraints and encode them in a lattice. In practice, the number of nodes in the lattice must remain reasonable (polynomial) in the number of words in the sentence. 5 To assess whether constraining the reorderings to those observed in the data is appropriate, the rule-based approach is compared with MaxJump (MJ) constraints (Kumar and Byrne 2005 ). In MJ-i , a word move cannot exceed i positions. 6 This is equivalent to using a rule-based system where all possible rules up to size i  X  1 would be considered. 4.2 Oracle unfolded reordering At training time, source sentences are deterministically reordered to enable the tuple extraction and the estimation of the models. During decoding, one would ideally like to process source sentences in their correct target order, i.e. the unfolded reordering defined in Sect. 2 . For unseen data, this oracle can be derived from forced alignments between a source sentence and the corresponding reference (see Fig. 2 d for an example). In that sense, the best reordering constraints should be the ones that generate lattices containing the unfolded reordering as the only option. We refer to this oracle-like reordering as the unfolded reordering . 7 4.3 What is the best reordering in a lattice? As explained above, the unfolded reordering can be considered as the best possible reordering. However for unseen data, this oracle reordering usually requires long range moves and/or permutations that were not observed in the training data. In our experiments (Sect. 5.7 ), only about 20 X 60 % of the test unfolded reorderings are actually reachable by our rule based system, depending on the translation direction and the setting used. Therefore it is also of interest to study the properties of the best reordering the system can explore. This best reordering can be defined as the path in the reordering lattice leading to the best translation. Such definition would however make the best reordering depend on the whole SMT system, including the pruning strategy and the translation models. Instead, we follow Herrmann et al. ( 2013b )by defining the best reordering as the one closest to the unfolded reordering. This approximation assumes that the best order is the one that most closely matches the target reference order, which is reasonable as most of the automatic metrics also rely on a similar assumption.
 Finding the closest permutation requires to define a metric over permutations. Among many choices (Deza and Huang 1998 ), two metrics have been shown to be useful when assessing reordering accuracy: the Kendall X  X  s (Isozaki et al. 2010a ; Birch et al. 2010 ; Talbot et al. 2011 ; Neubig et al. 2012 ) and the fragmentation chunk (or fuzzy reordering) (Banerjee and Lavie 2005 ; Talbot et al. 2011 ; Neubig et al. 2012 ). In this work, we use the Kendall X  X  s which proved to correlate strongly with human fluency judgment (Birch et al. 2010 ). The Kendall X  X  s metric (Kendall 1962 ) counts the number of pairwise disagreements between two permutations r ; p 2 S n where 1 cond is the indicator function with value 1 if cond is true and 0 otherwise. It is also the minimum number of swaps between two adjacent symbols needed to transform one permutation into the other, so the distance is also sometimes called the bubble-sort distance. The Kendall X  X  s is usually normalized, so a value of 1 indicates a maximum disagreement In the following, we explain how to efficiently search a lattice and find the closest Kendall X  X  s permutation to the unfolded reordering. First observe that where id is the identity permutation. Up to relabeling, the problem is then to find the permutation in a lattice L with the minimal number of inversions: arg min where w  X  k ; S  X  X  element from an integer set S 2 2 n . The number of inversions thus decompose as a sum of local functions that only depend on the set of already permuted integers. As observed above, in a permutation lattice L  X  X  V ; E ; R ; w  X  , each node v 2 V cor-responds to a set of integers S v . Each edge e 2 E leaving node v with integer label k will have a contribution w  X  k ; S v  X  to the total number of inversions of any path in the lattice leading to e . Therefore, L can be weighted with w  X  k ; S v  X  and the conven-tional shortest path algorithm can infer the best reordering.

Figure 2 displays an example of a reordering lattice containing three paths, with respective Kendall X  X  s to the unfolded reordering equal to 1, 2 and 8. The best path has just one arc of non-null weight between states 1 and 2, where the first word ( k  X  1  X  of the unfolded reordering leaves state 1 covering the second word ( S 1  X f 2 g ); likewise the second best bath has one non-null edge of weight 2 between nodes 2 and 5, since this edge corresponds again to k  X  1, with a coverage equal to S 2  X f 2 ; 3 g .

As many paths in the lattice may have the smallest distance to the unfolded reordering (i.e. the arg min in Eq. ( 7 ) may not be unique), the shortest path is in fact a sub-lattice of the original one. In our experiments, we found however the best reordering lattices to have about only 1.1 path on average. 4.4 Metrics Given our assumptions, the reordering space should be the smallest one containing the unfolded reordering. Therefore, as a quality measure on reordering constraints, we define the coverage on some test set as the number of time the reordering space contains the reference reordering. On the other hand, we compute the size of the reordering space as the number of paths 8 as well as the number of edges in the reordering lattice, as this last number closely relates to the decoding complexity.
This study primarily focuses on the overall translation performance, in relationship to the order in which the source has been translated. Our main translation quality metric is therefore BLEU (Papineni et al. 2002 ). As BLEU however provides little insight from the perspective of the reordering quality, we also report Kendall X  X  s metric to separately evaluate reorderings as in (Birch and Osborne 2010 ).  X  X  Appendix  X  X  also presents results with two additional metrics that were designed to better take into account reordering issues : BEER (Stanojevic  X  and Sima X  X n 2014 ) and RIBES (Isozaki et al. 2010a ). 5.1 Experimental setup Our experimental setup is based on the WMT 9 evaluation campaign shared task. We consider the following language pairs (on both directions): English-French, English-German and English-Czech. For training, the N EWS C OMMENTARY corpus provided by the organizers of WMT X 12 (Callison-Burch et al. 2012 ) is used; newstest2009 and newstest2010 are used for tuning and testing, 10 respectively. Table 1 contains various basic statistics regarding these corpora.

In-house text processing tools are used for the tokenization (De  X  chelotte et al. 2008 ) in a  X  X  X rue-case X  X  scheme. As German is morphologically rich, the German source side is normalized using a specific preprocessing scheme (Allauzen et al. 2010 ; Durgar El-Kahlout and Yvon 2010 ) which aims at reducing the lexical redundancy by normalizing the orthography, neutralizing most inflections and splitting complex compounds. The English side of the parallel corpora is POS-tagged with Wapiti (Lavergne et al. 2010 ), while for French and German we use the TreeTagger (Schmid 1994 ) and for Czech the open-source tool M ORPHO D I T A 11 (Strakova  X  et al. 2014 ). In the last case, only the first two characters of the fifteen-letter Prague Dependency Treebank tags are used, resulting in 67 possible POS tags. For all languages, we also use the mappings from Petrov et al. ( 2012 ) to project to the Universal Tagset.
 Word alignments and the 50 word classes 12 are computed using MGIZA ?? 13 and MKCLS 14 with default settings, using, for English-French and English-German all the parallel data described in (Allauzen et al. 2013 ), and, for English-Czech, the E
For each task, a 4-gram language model is estimated using the target side of the training data. We use N CODE with the default setting and an additional bilingual factor model based on POS tags. 15 The beam size is set to 25 for KB-MIRA tuning Cherry and Foster ( 2012 ) and to 50 when decoding, a parameter setting that worked well in previous experiments. All results are averaged over 3 runs to control for optimizer instability (Clark, et al. 2011 ). Approximate randomization tests for multiple optimizer samples to assess statistical significance are carried out using
Oracles are computed using the linear approximation to the BLEU score introduced by Tromble et al. ( 2008 ): using a first order Taylor-series approximation to the corpus log  X  BLEU  X  gain leads to the following sentence level gain function: for a reference e and an hypothesis e 0 , where n -gram  X  e  X  is the set of n -grams in e and # g  X  e 0  X  is the number of times a n -gram g appears in e 0 . As this sentence-level approximation decomposes into a sum of local functions, we can efficiently find the maximum gain path in the lattice. The parameters of Eq. ( 8 ) are chosen using h  X  X  4 p r n 1  X  1 for n 2f 1 ; ... ; 4 g (Tromble et al. 2008 ), where the unigram precision p , the precision ratio r and the length bonus h 0 are chosen so as to maximize corpus-level BLEU. We found p  X  0 : 4, r  X  0 : 8 and h 0  X  1 to yield good performance.
 5.2 Coverage, generalization and complexity of the rule-based approach A first question is the coverage and the generalization power of the rule-based approach. Figure 3 displays, for each reordering size n , the ratio between the number of reorderings 17 observed in the data and the total number of possible reorderings of that size (i.e. r n as defined in Sect. 3.1 ). We also vary the values of the cost-based filtering threshold: when maxcost  X 1 , all the reorderings observed in the training data are considered. In this case, almost all the possible reorderings appear in the data for the rules up to length 5. The ratio then quickly decreases to zero as the size exceeds 8. Moreover, the comparison between maxcost  X  1 and maxcost  X  4 shows that the exact value of the threshold has a small impact on the coverage and this trend is observed for all translation directions.

Figure 4 characterizes the complexity of the reorderings for three conditions: (a) the reorderings observed in the training data; (b) the reordering in test data; (c) the test reorderings that are not captured 18 by the rule-based approach ( miss ). The complexity of reorderings, as a function of their size, is described with three indicators: the proportion of extracted permutations that are in the ITG family, the average normalized Kendall X  X  s and the normalized fragmentation chunk distance to the identity permutation. Statistics are computed at the level of rules (rather than sentences): this is because rules decompose sentences into chunks that are reordered independently. For long sentences with many independent local reorderings, the properties of each local reordering are more relevant than considering the sentence as a whole. For instance, a non-ITG sentence may exhibit several local ITG reorderings, in addition to the non-ITG one(s). As shown in Fig. 4 , the complexity measures for train, test and missed reorderings are nearly identical. Therefore, reorderings that are not captured by the rule-based approach cannot be characterized by their complexity. For small size reorderings, we observe the missed reordering to have a slightly lower ITG ratio, a phenomena that quickly vanishes, as in fact almost all large test reorderings are missed.

Figure 5 displays the number of missed reorderings on the test data as a function of the size. We also vary the tagsets used to build the reordering rules. 19 For English-French, most reorderings concern moderate size spans (2 to 4) and half of the reorderings spread over only two words. Long range reorderings (i.e. more than ten word) are rare: if some correspond to genuine linguistic patterns, such as the alternation between active and passive voice, most of these permutations are due to alignment errors, mistranlations or complex constructions. In contrast, for English-Czech and even more so for English-German, the rule length is more evenly spread, with many medium size (5 X 10) reorderings as well a significant number of long range reorderings, which cannot be fully attributed to alignment errors. These results are in line with the numbers in Table 1 , which suggest that French-English is the easiest pair (having the shortest average reordering length), and that German-English is the hardest (with longer reorderings and fewer monotonous alignments), Czech-English being in-between with a large number of monotone reorderings, yet a much larger average reordering length than French.

Figure 5 enables to distinguish three types of reorderings, the proportion of which varies depending the language pairs:  X  short range reorderings, corresponding to permutations involving 2 X 4 words; these are accurately captured by the rule-based approach and the number of misses is accordingly very small; in this case, it makes sense to use a syntactic context to make sure they fire only in likely positions.  X  medium-size reorderings (permutations of 5 X 10 words): most of the test situations are observed in training (as acknowledged by the small number of misses when we only use one single POS tag); the rule-based approach is less successful here, and many misses are observed when syntactic constraints are introduced;  X  long-range reorderings (involving more than 10 words): most of these are missed, even with the most general tagset. These means that most of these test permutations are not seen in training and suggests that learning such long permutations is useless.
 Figure 5 also gives some insights regarding the generalization power of the tagsets introduced in Sect. 3.3 . With fully lexicalized rules, the coverage of test reorderings is rather poor: for instance, more than half of the swaps are missed, for all translation directions. We also note that class-based rules are always worst than rules based on linguistic tags. Additionally, for reordering size greater than 5, whatever the tagset, half of the test reordering are missed. This means that the reordering rules, as used in this work, are only useful for very small range reorderings. Differences between tagsets thus mainly impact such reorderings and enable to vary the trade-off between coverage and ambiguity. 5.3 From monotone to rule-based reordering: impact on MT performance In this section, we assess the impact of the rule-based approach in terms of MT performance. For this purpose, Table 2 reports BLEU scores on test data for several reordering spaces of varying  X  X  X uality X  X . Additional figures for BEER and RIBES metrics are provided in  X  X  Appendix  X  X  (see Table 5 ). Note that results obtained with those metrics are consistent with our observations based on BLEU scores throughout. The first reordering space only considers the original source sentence order (monotone). The second uses our rule-based approach to create a reordering lattice (rules). The remaining four are oracle-like reordering spaces that will be detailed in Sects. 5.4 and 5.5 . An example of sentence translation for these configurations is in Fig. 6 . Table 2 also gives the best possible BLEU scores (oracle decoding) for the six conditions. 20
For English-French and English-German, we can observe BLEU improvements from monotone to lattice reordering, as one would expect. For English-French, the increase is as high as 3 BLEU points, which illustrates the importance of taking word moves into account during translation, even for closely related languages. In Fig. 6 for example, the monotone translation fails to invert president X  X  and spokesman , resulting in a mistranslation (meaning  X  X  X y the president, spokesman, Radim Ochvat X  X ). For English-German, gains are however much lower, especially for en ! de (only about a half BLEU point). This suggests that our reordering system does not succeed in predicting the German word order. Finally and perhaps more surprisingly, there is no gain at all for English-Czech in neither direction, which indicates that our rule-base might not be particularly adapted for capturing ordering variation for this particular language pair. 21 Two main explanations may account for this negative result. Either the reordering mechanism is not expressive enough to generate good reordering variants in the search space, or the model is not able to recognize these better paths. We will see in Sect. 5.7 (Table 3 ) that for only about 30 X 40 % percent of the sentences, the correct order is encoded in the lattice. However, oracle decoding shows that in all cases, even for the most challenging translation directions, the reordering lattice contains much better reorderings than the monotonic order which could be exploited to achieve better BLEU scores. This suggests that model and/or search errors are largely responsible for the lack of improvement observed when moving from a monotone to an enriched reordering space.

Table 2 also shows that BLEU scores for en ! de , and even more so for en ! cs , are much worse than in the other translation direction. Assuming that the reordering complexity is more or less symmetric, the difference here may be due to the complex morphology of German and Czech, which is difficult to generate when translating from English.

The high oracle BLEU scores in Table 2 finally suggest that larger gains may be achieved by improving the translation models than by increasing the size of the reordering space. Note however that oracle BLEU scores may be overly optimistic and a large part of these gains may be due to over-fitting the BLEU metric. An illustration is in Fig. 6 , with a mumbo-jumbo oracle translation. 5.4 Oracle reorderings, an upper bound on MT performance To better understand the impact of model and search errors, we carry out additional experiments with two informed reordering spaces. Results are again in Table 2 . The best configuration refers to the situation where the reordering space only contains the best(s) reordering as defined in Sect. 4.3 , unfo denotes the forced unfolded reordering. Table 2 shows that for all translation directions, the prior knowledge of the best(s) reordering is actually useful, with particularly large gains for French to English. The gap between unfo and aug and the corresponding oracle conditions indicates that model and search errors cause the system to often miss good reordering paths in the lattice, and reveals quantitatively the impact of these errors in decoding.

As also noted by Herrmann et al. ( 2013b ), the best reordering in the lattice is not necessarily the one leading to the best translation. In fact, alignment errors may yield low quality unfolded reorderings, which will then affect the best reordering approximation. Moreover, the resulting word order is somewhat artificial and may not correspond to an optimal matching of phrase pairs. 22 Oracle decoding gives a quantitative illustration as oracle decoding with best(s) reordering is only half way between monotone and lattice-based oracle decoding. This means that, in many cases, an even better reordering would yield a larger improvement. On the other hand, giving hints about a given reference translation (here information about its order) biases the system towards the order of that reference. So the oracle-like reordering results may be slightly more optimistic than the actual performance a real system could ever achieve.

Previous experiments have delivered an upper bound of the performance for a constrained reordering space. It is also interesting to contrast the best achievable reordering with best conceivable one, as this gives information about the quality of the approximation of the reordering by the generation mechanism (Herrmann et al. 2013b ). In addition, this also gives an upper bound for any reordering mechanism that would be used to generate the reordering space. As shown in Table 2 , all language directions benefit from knowing the unfolded reordering, sometimes by a wide margin, with however some disparities between language directions. The difference between the best reordering and the unfolded one measures the improvements that could be obtained by relaxing the reordering constraints (assuming no model/search errors), while the difference between the lattice and the unfolded cases measures the improvements that could be obtained by designing a better reordering space and a better model score function.
 The observed gaps in performance, up to 4 BLEU points when translating into English from French or German shows that there is indeed room for improvement. The improvements for en ! cs are however not so clear. In other words,  X  X  X olving X  X  the reordering problem at decoding time has only a slight effect on performance for this language direction. Therefore, the reordering constraints might currently not be the main limitation of our system for this language pair. Note finally that the reordering length is unbounded in the unfolded reordering case, hence the lack of long range reorderings in our model can not be the main explanation. 5.5 Discrimination of the unfolded reordering In this section, we design two additional experiments where we simulate a  X  X  X ompetition X  X  between some of the previous lattices.

In the first one, the rule-based reordering space is augmented with the unfolded reordering (line denoted by ( aug ) in Table 2 ). In this setting, the reordering space now contains the  X  X  X xpected X  X  reordering, as well as many alternative permuations. This experiment allows us to further understand how well the decoding system is able to find the unfolded reordering in the lattice. Table 2 shows that there is almost no difference with regular lattice decoding, except for de ! en . Therefore, it seems that one of the main issue with reordering is not the lack of good reorderings in the search space, but rather the failure to select these permutations, due to models and/ or search errors.

It should be noted that the unfolded reordering does not always result in a better translation. For instance, in Fig. 6 , the lattice translation is valid though different from the reference. However, the translation in the unfolded reordering condition is mistranslated as the initial sentence is in the passive voice (the translated sentence means  X  X  X resident Radim X  X  spokesman has been announced by the Ochvat meeting X  X ). This illustrates the situation where a seemingly optimal reordering leads to a poor translation. Oracle decoding for the augmented lattice gives a more quantitative analysis: results in Table 2 for oracle decoding with the augmented lattice are always superior to the ones obtained with just the unfolded reordering. This means that in many cases, the search space contains a better reordering than the unfolded one.

The second experiment is a face-to-face comparison of the unfolded and the best reorderings (see line duel in Table 2 ). This experiment aims at understanding whether the decoder could be able to choose the unfolded reordering in the absence the ambiguity introduced by many competing spurious reorderings. For each source sentence, a duel permutation lattice is built containing only the unfolded and the best reorderings. Table 2 reveals that for English-French and English-Czech, the scores are only slightly better than the ones with the best reorderings, suggesting that the decoder would only choose the unfolded reordering if forced to. One explanation is that the unfolded permutation may contain long-range reorderings that are severely penalized by the distortion mode; this is nonetheless revealing of the pitfalls of the scoring function. The improvement is minimal for English-Czech, for which the scoring function seems to be the less appropriate. Oracle results also show that in some cases, the best reorderings might be easier to use than the unfolded ones, as the scores slightly increase.
 Note that in this paper, we have not attempted to sort model from search errors. In future work, we plan to evaluate the oracle coverage using the lattices that are effectively explored during decoding (i.e. after beam search pruning), to better understand the relative contributions of these types of errors. However, our duel setting already suggests that model errors play an important role, as it is unlikely to observe many search errors when only two alternative reorderings are competing. 5.6 Reordering space when tuning We finally explore the importance of the reordering space during the tuning step. The bottom part of Table 2 shows that tuning, then decoding with the augmented lattice could yield some improvements; these are particularly significant for English-German. This means that the tuning benefits from seeing the unfolded reordering as a possible candidate. The effect is all the more important when tuning and decoding with the duel lattices, with an increase of about one BLEU point for all directions. We must then mitigate the conclusions of previous paragraphs: the decoder is indeed able to select the unfolded reordering, but has to be trained with lattices containing good non-monotone paths. 23
Unfortunately, tuning with an augmented lattice, while decoding with the rule-based one, as is the case in real-world scenario, actually harms performance, as shown in the last line of Table 2 . 5.7 Reordering space tradeoff Table 3 reports reordering space size, coverage, oracle and decoding scores 24 when varying the rule filtering threshold. We observe that while the number of rules is almost twice as large for en ! de than for en ! fr , the generated reordering spaces are comparable in size, but with a much lower coverage for en ! de . English-Czech has the smallest reordering space; yet the rule coverage is higher than for English-German. Interestingly, when compared with English-German, English-Czech has almost twice as many monotonic translations; 25 yet, we also observe much lower BLEU scores both when using N CODE or oracle decoding, indicating again that reordering is only part of the complexity of this language pair.

By relaxing the rules pruning, we see large increases in the size of the reordering space, in coverage and in oracle BLEU. For English-French, in regular test condition, we observe a slight degradation of the BLEU scores, even though the size of the search space drastically increases. This shows the importance of the trade-off regarding the design of the reordering space, as noisy reorderings may introduce spurious, but plausible, alternatives. This is however not the case for the other language pairs, for which the BLEU scores do not change much as the reordering space increases, and with it the unfolded reordering coverage. Again, en ! cs is the most challenging translation pair, where the best performance are obtained for the monotone condition, reflecting again the inability of our systems to take advantage of a richer search space.

Table 3 also displays the average Kendall X  X  s distance from N CODE hypotheses to the unfolded permutations as well as the number of times the unfolded reordering is actually used by the decoder (which corresponds to a null value for Kendall X  X  s ). For English-German and English-Czech, we observed that the unfolded coverage of the search space increases when relaxing the filtering strategy; this is however not reflected in the final reordering chosen by the decoder, which remains the same, and even slightly decreases (for maxcost  X 1 ). Improving the reordering search space does not seem to benefit the decoder in making better reordering decisions for those language pairs.

It might be surprising, at a first glance, to see that using low cost rules does have a small effect on the reordering space. For instance, for en ! de , using a threshold of 2 selects 101 K rules, which however generate lattices that have only two paths on average. Similarly, the 53 K en ! cs rules do not significantly increase the number of paths with respect to the monotone condition. This shows that most of the extracted reordering rules do not generalize to the test data, mainly because the corresponding tag sequences are observed too rarely. However, even useless, they are not harmful as they never fire. 5.8 Using alternative tagsets Figure 7 displays the results when building the reordering rules over different tagsets and when using different filtering thresholds. It is interesting to see different behaviors between languages pairs, irrespective of the translation direction. For both English-French and English-German, using fully lexicalized rules, as predicted in Sect. 5.2 , performs significantly worst, with improvement however over the monotonic case. For English-French, we see little differences between the other tags, and, as previously noted, larger reordering spaces, corresponding to higher filtering thresholds slightly degrade the performance. It is surprising to see that for fr ! en , the rules built on the word classes slightly outperform the other tags, as this was not predictable from Fig. 5 a. Word classes however do not perform so well for English-German for which differences between tagsets are larger. In this case, smaller tagsets seem to perform better, probably because they enable a better generalization.

As previously observed, the case of en ! cs is peculiar, as we do not observe any quantifiable score change when varying the parameters. Note, however, that this result rules out a possible explanation that the cs ! en direction would be penalized by a large POS tagset, as when using the smaller Universal tagset no changes is observed. Finally, the results for cs ! en show a surprising outlier when using word classes and a very specific threshold parameter, with about one BLEU point improvement with respect to other conditions. 26
In general, the competitive results obtained with the coarse-grain tagset and the automatic word classes show that they can be used as a workaround for under-resourced language, as for a new language pair, one would not been able to predict which tagset should be used anyways. 5.9 Comparison with MJ-i Table 4 finally reports the results 27 of a head-to-head comparison between MJ-i constraints and the rule-based approach. The MJ reordering spaces are several orders of magnitude larger than their ruled-based counterparts but yield the same or significantly lower results. This warrants the use of linguistically motivated rules, instead of allowing all local permutations, and corroborates the trade-off discussed earlier. Training time is also an issue here: for en ! fr , the tuning step with MJ-3 constraints is twenty times longer than maxlen  X  4. The reordering problem has been addressed in many ways since the advent of machine translation. Researchers tried to solve this problem via new approaches, varying the modeling strategies, or by restricting the possible word reordering operations. In this work, we are interested in how the reordering space is defined, either explicitly or implicitly, and in methods that could help to understand the importance and the impact of reordering space design.

Early work on word reordering constraints includes ITG constraints (Wu 1997 ) and IBM constraints (Berger et al. 1996 ), which are compared in Zens and Ney ( 2003 ). Goh et al. ( 2011 ) partition sentences into several clauses and restrict word reordering to occur within clauses. The definition of the reordering space is also closely related to the generative mechanisms used in SMT. In phrase-based SMT (Zens et al. 2002 ) local reorderings are modeled within phrases, which may then be reordered according to some constraints, e.g. a simple distortion limit on words. Other constraints on phrases include ITG constraints (Zens et al. 2004 ; Feng et al. 2010 ; Cherry et al. 2012 ) and MJ constraints (Kumar and Byrne 2005 ). 28 Syntax-based MT systems handle the reordering problem by embedding syntactic analysis in the decoding process (Wu 1997 ; Yamada and Knight 2001 ; Galley et al. 2004 ). Finally, the hierarchical approach or Chiang ( 2005 ) is mainly motivated by the recursive nature of reorderings. However, Auli et al. ( 2009 ) showed that the search space explored by phrase-based and hierarchical-based models are very close. All these approaches generally fail to handle long range reorderings, hence the motivation of approaches that rearrange the source sentence in a target-like word order before translating and that handle reorderings at the word level, as ours.
This line of work has been pioneered by Xia and McCord ( 2004 ), who automatically learn reordering rules from source and target language dependency trees. Many subsequent approaches have proposed to manually design reordering rules based on syntactic or dependency parse trees (Collins et al. 2005 ; Xu et al. 2009 ; Carpuat et al. 2010 ; Isozaki et al. 2010b ), or to automatically learn them (Zhang et al. 2007 ; Li et al. 2007 ; Khalilov et al. 2009 ; Elming and Habash 2009 ; Genzel 2010 ; Dyer and Resnik 2010 ; Khalilov and Sima X  X n 2011 ; Lerner and Petrov 2013 ). As source parse trees are not always available, other approaches cast the preordering directly as a permutation modeling problem (Tromble and Eisner 2009 ; Visweswariah et al. 2011 ) or infer the parse trees automatically from parallel text (DeNero and Uszkoreit 2011 ; Neubig et al. 2012 ). Note that these approaches require high-quality manual word alignments. However, Visweswariah et al. ( 2013 ) propose an approach that jointly improves alignment and reordering in the presence of noisy alignments.

Another widely-used approach is to automatically learn shallow reordering rules based on POS tags or syntactic chunks (Rottmann and Vogel 2007 ; Zhang et al. 2007 ; Crego and Habash 2008 ; Niehues and Kolss 2009 ). Herrmann et al. ( 2013a ) further combine POS based reordering on the morphosyntactic level and syntax tree-based on the constituent level. Alternatively, Costa-jussa ` and Fonollosa ( 2006 ) cast the word reordering problem as a translation task, using word class information to translate the original source sentence into the reordered source sentence.
In some cases, pre-ordering fails to improve translation performance (Howlett and Dras 2010 ). These authors investigate in detail several factors to understand when preordering may be useful; one reason being that it enables to better match the inner mechanism of phrase-based SMT (Zwarts and Dras 2006 ).

In the majority of previous works, only one deterministic preordering of the source sentence is computed: this is because preordering is used in a preprocessing step, which is then followed by the whole translation pipeline, inducing further reorderings. In contrast, in our approach, all the possible reorderings are computed once and for all in the reordering step, whose single goal is to generate the reordering space; the selection of the best reordering path is then left to the decoder.
Bisazza and Federico ( 2013b ) claim that long-range reorderings issues should not be attributed to the deficiencies of existing reordering models , but rather to too coarse definition of the reordering search space . They introduce a word after word reordering similar to the preordering model of Visweswariah et al. ( 2011 )to dynamically shape the search space while decoding with a very high distortion limit (Bisazza and Federico 2013a ). This approach enables to achieve fast decoding and performance improvements for the reordering of verbs in Arabic to English translation.

Oracle experiments are a valuable method for analyzing different aspects of machine translation, e.g. identify translation errors in the phrase-table (Wisniewski et al. 2010 ), or to perform failure analysis (Wisniewski and Yvon 2013 ). Sokolov et al. ( 2012 ) describe efficient methods to find the best translation hypothesis in a lattice and apply them to compare the lattices explored by M OSES and N CODE . In this work, we compute the oracle on the full search lattice. Another line of study is to assess the limitations induced by various reordering constraints. Dreyer et al. ( 2007 ) compute a lower bounds of the best achievable BLEU score using dynamic programming techniques for IBM and ITG constraints, while Sokolov et al. ( 2012 ) show a very limited influence of the distortion limit both on the decoder and on the oracle quality. Wisniewski and Yvon ( 2013 ) study in details various reordering constraints, including the distortion limit, IBM and MJ-i constraints. We share the main conclusions of these studies: the scoring functions (or models) seem to be the main limitation for phrase-based systems, while they are expressive enough to achieve higher translation performance. As for oracle-like reorderings, Khalilov and Sima X  X n ( 2012 ) introduce an upper bound, similar to our unfolded reordering, and show potential improvement for preordering performance, albeit limited when considering tree structure constraints.

Auli et al. ( 2009 ) explore induction errors in the search space of phrase-based and hierarchical phrase-based model (HPBT), and promote the use of reference reachability metric, which corresponds to our notion of coverage. They only consider different reordering spaces by varying the distortion limit and show that both types of model explore almost similar search spaces, and mostly differ by the way they score derivations. In contrast to previous work, we make use of many complementary approaches to assess the importance of the reordering space, by studying and comparing jointly the actual performance, the oracle best possible performance and the search space size, both for rule-lattice based and for oracle reordering spaces.

The most similar work to ours is certainly the study of Herrmann et al. ( 2013b ), which also contains oracle experiments aimed to analyze the potential of the preordering approach and the impact of various restriction of the reordering space. Based on oracle results for the English-German pair that are in line with our own findings, the authors suggest that closing the observed gap between the unfolded and rule-based reorderings could yield significant improvements in performance. Based on our experiments, notably the comparison between lattice and augmented reordering spaces (Sect. 5.5 ), we are enclined to mitigate these conclusions: in fact, little gains will be obtained from such endeavours unless decisive progresses are made to reordering models.

Note finally that this study extends our own previous work (Pe  X  cheux et al. 2014 ) in several ways: (a) experiments on English-Czech, a challenging translation language pair; (b) a detailed study of the rule-based approach and its efficiency; (c) additional oracle-like conditions which shed light on the importance of model/ search errors. 29 In this work, we have compared the search space generated by different reordering rules as well as local permutation constraints. Linguistically motivated reordering rules lead to a much smaller search space and improve the translation quality, and only moderately depends on the abstraction used to generalize rules beyond purely lexical patterns. However, this simple rule-based approach is only effective for small range reorderings, and other techniques would be needed to generate more accurate reordering spaces, in particular for English-Czech. To assess the potential of a better reordering search space, we use a n -gram SMT tool that decorrelates reordering and decoding; but our results are more general and hold for any system for which the reordering space could be encoded in a lattice prior to decoding. This framework allows us to specifically study the the impact of the reordering space on the overall translation performance. We find that there is a large room of improvement by designing a better reordering space. This improvement is however less substantial for English to Czech, the most difficult translation direction, suggesting that the reordering search space is not the only critical issue in a system design; for this particular language pair, the complexity of Czech morphology also contributes to make to make SMT very challenging. However, there is little hope to generate reordering spaces composed of solely a few good reordering candidates. We showed that because of model/search errors, simply adding a good reordering in the search space would not be enough. Therefore, improving the reordering space should come with improvements on the reordering models if one wants to expect some gains.

It is worth mentioning that, in this work, we aim to understand the importance and the expressiveness of the reordering decoding search space , all other things being equal. Word order differences between languages intervene however at many other levels in a statistical machine translation system. The importance of the reordering search space has to be conditioned on the fact we are using a (particular) phrase-based approach relying on alignment links. What we claim is that currently, at least for the systems and language pairs studied in this work, the main sources of errors, even from the reordering point of view, can not be attributed to the decoding search space design. This does not negate that word ordering issues might still play a critical role, in particular, it presides over tuple extraction, which is at the root of phrase-based approaches like ours. In addition, our system is plagued by alignment error which intervene at various levels, affecting the system performance as well as our analysis. We plan to further study the impact of word alignment noise on the reorderings.
 See Tables 5 , 6 and 7 . References
